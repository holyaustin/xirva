[{"id": "2012.00192", "submitter": "Anand Jayarajan", "authors": "Anand Jayarajan, Kimberly Hau, Andrew Goodwin, Gennady Pekhimenko", "title": "LifeStream: A High-Performance Stream Processing Engine for Periodic\n  Streams", "comments": null, "journal-ref": null, "doi": "10.1145/3445814.3446725", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hospitals around the world collect massive amounts of physiological data from\ntheir patients every day. Recently, there has been an increase in research\ninterest to subject this data to statistical analysis to gain more insights and\nprovide improved medical diagnoses. Such analyses require complex computations\non large volumes of data, demanding efficient data processing systems. This\npaper shows that currently available data processing solutions either fail to\nmeet the performance requirements or lack simple and flexible programming\ninterfaces. To address this problem, we propose \\emph{LifeStream}, a\nhigh-performance stream processing engine for physiological data. LifeStream\nhits the sweet spot between ease of programming by providing a rich temporal\nquery language support and performance by employing optimizations that exploit\nthe periodic nature of physiological data. We demonstrate that LifeStream\nachieves end-to-end performance up to $7.5\\times$ higher than state-of-the-art\nstreaming engines and $3.2\\times$ than hand-optimized numerical libraries on\nreal-world datasets and workloads.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 00:54:57 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 01:44:53 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Jayarajan", "Anand", ""], ["Hau", "Kimberly", ""], ["Goodwin", "Andrew", ""], ["Pekhimenko", "Gennady", ""]]}, {"id": "2012.00217", "submitter": "Roel Van Beeumen", "authors": "Roel Van Beeumen and Khaled Z. Ibrahim and Gregory D. Kahanamoku-Meyer\n  and Norman Y. Yao and Chao Yang", "title": "Enhancing Scalability of a Matrix-Free Eigensolver for Studying\n  Many-Body Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.dis-nn cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In [Van Beeumen, et. al, HPC Asia 2020,\nhttps://www.doi.org/10.1145/3368474.3368497] a scalable and matrix-free\neigensolver was proposed for studying the many-body localization (MBL)\ntransition of two-level quantum spin chain models with nearest-neighbor $XX+YY$\ninteractions plus $Z$ terms. This type of problem is computationally\nchallenging because the vector space dimension grows exponentially with the\nphysical system size, and averaging over different configurations of the random\ndisorder is needed to obtain relevant statistical behavior. For each eigenvalue\nproblem, eigenvalues from different regions of the spectrum and their\ncorresponding eigenvectors need to be computed. Traditionally, the interior\neigenstates for a single eigenvalue problem are computed via the\nshift-and-invert Lanczos algorithm. Due to the extremely high memory footprint\nof the LU factorizations, this technique is not well suited for large number of\nspins $L$, e.g., one needs thousands of compute nodes on modern high\nperformance computing infrastructures to go beyond $L = 24$. The matrix-free\napproach does not suffer from this memory bottleneck, however, its scalability\nis limited by a computation and communication imbalance. We present a few\nstrategies to reduce this imbalance and to significantly enhance the\nscalability of the matrix-free eigensolver. To optimize the communication\nperformance, we leverage the consistent space runtime, CSPACER, and show its\nefficiency in accelerating the MBL irregular communication patterns at scale\ncompared to optimized MPI non-blocking two-sided and one-sided RMA\nimplementation variants. The efficiency and effectiveness of the proposed\nalgorithm is demonstrated by computing eigenstates on a massively parallel\nmany-core high performance computer.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 02:10:14 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Van Beeumen", "Roel", ""], ["Ibrahim", "Khaled Z.", ""], ["Kahanamoku-Meyer", "Gregory D.", ""], ["Yao", "Norman Y.", ""], ["Yang", "Chao", ""]]}, {"id": "2012.00365", "submitter": "Dominik Stra{\\ss}el", "authors": "Dominik Strassel, Philipp Reusch and Janis Keuper", "title": "Python Workflows on HPC Systems", "comments": "9 pages with 7 figures, submitted and accepted at the PyHPC Workshop\n  at SuperComputing 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent successes and wide spread application of compute intensive machine\nlearning and data analytics methods have been boosting the usage of the Python\nprogramming language on HPC systems. While Python provides many advantages for\nthe users, it has not been designed with a focus on multi-user environments or\nparallel programming - making it quite challenging to maintain stable and\nsecure Python workflows on a HPC system. In this paper, we analyze the key\nproblems induced by the usage of Python on HPC clusters and sketch appropriate\nworkarounds for efficiently maintaining multi-user Python software\nenvironments, securing and restricting resources of Python jobs and containing\nPython processes, while focusing on Deep Learning applications running on GPU\nclusters.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 09:51:12 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Strassel", "Dominik", ""], ["Reusch", "Philipp", ""], ["Keuper", "Janis", ""]]}, {"id": "2012.00400", "submitter": "Felix Lorenz", "authors": "Felix Lorenz, Morgan Geldenhuys, Harald Sommer, Frauke Jakobs, Carsten\n  L\\\"uring, Volker Skwarek, Ilja Behnke, Lauritz Thamsen", "title": "A Scalable and Dependable Data Analytics Platform for Water\n  Infrastructure Monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With weather becoming more extreme both in terms of longer dry periods and\nmore severe rain events, municipal water networks are increasingly under\npressure. The effects include damages to the pipes, flash floods on the streets\nand combined sewer overflows. Retrofitting underground infrastructure is very\nexpensive, thus water infrastructure operators are increasingly looking to\ndeploy IoT solutions that promise to alleviate the problems at a fraction of\nthe cost. In this paper, we report on preliminary results from an ongoing joint\nresearch project, specifically on the design and evaluation of its data\nanalytics platform. The overall system consists of energy-efficient sensor\nnodes that send their observations to a stream processing engine, which\nanalyzes and enriches the data and transmits the results to a GIS-based\nfrontend. As the proposed solution is designed to monitor large and critical\ninfrastructures of cities, several non-functional requirements such as\nscalability, responsiveness and dependability are factored into the system\narchitecture. We present a scalable stream processing platform and its\nintegration with the other components, as well as the algorithms used for data\nprocessing. We discuss significant challenges and design decisions, introduce\nan efficient data enrichment procedure and present empirical results to\nvalidate the compliance with the target requirements. The entire code for\ndeploying our platform and running the data enrichment jobs is made publicly\navailable with this paper.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 11:02:31 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 15:20:32 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Lorenz", "Felix", ""], ["Geldenhuys", "Morgan", ""], ["Sommer", "Harald", ""], ["Jakobs", "Frauke", ""], ["L\u00fcring", "Carsten", ""], ["Skwarek", "Volker", ""], ["Behnke", "Ilja", ""], ["Thamsen", "Lauritz", ""]]}, {"id": "2012.00472", "submitter": "Martin Kleppmann", "authors": "Martin Kleppmann, Heidi Howard", "title": "Byzantine Eventual Consistency and the Fundamental Limits of\n  Peer-to-Peer Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sybil attacks, in which a large number of adversary-controlled nodes join a\nnetwork, are a concern for many peer-to-peer database systems, necessitating\nexpensive countermeasures such as proof-of-work. However, there is a category\nof database applications that are, by design, immune to Sybil attacks because\nthey can tolerate arbitrary numbers of Byzantine-faulty nodes. In this paper,\nwe characterize this category of applications using a consistency model we call\nByzantine Eventual Consistency (BEC). We introduce an algorithm that guarantees\nBEC based on Byzantine causal broadcast, prove its correctness, and demonstrate\nnear-optimal performance in a prototype implementation.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 13:24:09 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Kleppmann", "Martin", ""], ["Howard", "Heidi", ""]]}, {"id": "2012.00740", "submitter": "K. R. Jayaram", "authors": "K. R. Jayaram, Archit Verma, Ashish Verma, Gegi Thomas and Colin\n  Sutcher-Shepard", "title": "MYSTIKO : : Cloud-Mediated, Private, Federated Gradient Descent", "comments": "IEEE CLOUD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning enables multiple, distributed participants (potentially on\ndifferent clouds) to collaborate and train machine/deep learning models by\nsharing parameters/gradients. However, sharing gradients, instead of\ncentralizing data, may not be as private as one would expect. Reverse\nengineering attacks on plaintext gradients have been demonstrated to be\npractically feasible. Existing solutions for differentially private federated\nlearning, while promising, lead to less accurate models and require nontrivial\nhyperparameter tuning. In this paper, we examine the use of additive\nhomomorphic encryption (specifically the Paillier cipher) to design secure\nfederated gradient descent techniques that (i) do not require addition of\nstatistical noise or hyperparameter tuning, (ii) does not alter the final\naccuracy or utility of the final model, (iii) ensure that the plaintext model\nparameters/gradients of a participant are never revealed to any other\nparticipant or third party coordinator involved in the federated learning job,\n(iv) minimize the trust placed in any third party coordinator and (v) are\nefficient, with minimal overhead, and cost effective.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 18:58:51 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Jayaram", "K. R.", ""], ["Verma", "Archit", ""], ["Verma", "Ashish", ""], ["Thomas", "Gegi", ""], ["Sutcher-Shepard", "Colin", ""]]}, {"id": "2012.00825", "submitter": "Elvis Rojas", "authors": "Elvis Rojas, Albert Njoroge Kahira, Esteban Meneses, Leonardo Bautista\n  Gomez, Rosa M Badia", "title": "A Study of Checkpointing in Large Scale Training of Deep Neural Networks", "comments": null, "journal-ref": "2020 International Conference on High Performance Computing &\n  Simulation (HPCS20)", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep learning (DL) applications are increasingly being deployed on HPC\nsystems, to leverage the massive parallelism and computing power of those\nsystems for DL model training. While significant effort has been put to\nfacilitate distributed training by DL frameworks, fault tolerance has been\nlargely ignored. In this work, we evaluate checkpoint-restart, a common fault\ntolerance technique in HPC workloads. We perform experiments with three\nstate-of-the-art DL frameworks common in HPC Chainer, PyTorch, and TensorFlow).\nWe evaluate the computational cost of checkpointing, file formats and file\nsizes, the impact of scale, and deterministic checkpointing. Our evaluation\nshows some critical differences in checkpoint mechanisms and exposes several\nbottlenecks in existing checkpointing implementations. We provide discussion\npoints that can aid users in selecting a fault-tolerant framework to use in\nHPC. We also provide takeaway points that framework developers can use to\nfacilitate better checkpointing of DL workloads in HPC.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 20:53:17 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 17:03:54 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Rojas", "Elvis", ""], ["Kahira", "Albert Njoroge", ""], ["Meneses", "Esteban", ""], ["Gomez", "Leonardo Bautista", ""], ["Badia", "Rosa M", ""]]}, {"id": "2012.00854", "submitter": "Tim Roughgarden", "authors": "Tim Roughgarden", "title": "Transaction Fee Mechanism Design for the Ethereum Blockchain: An\n  Economic Analysis of EIP-1559", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DC cs.DS econ.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  EIP-1559 is a proposal to make several tightly coupled additions to\nEthereum's transaction fee mechanism, including variable-size blocks and a\nburned base fee that rises and falls with demand. This report assesses the\ngame-theoretic strengths and weaknesses of the proposal and explores some\nalternative designs.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 21:48:57 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Roughgarden", "Tim", ""]]}, {"id": "2012.00912", "submitter": "Yuan Meng", "authors": "Yuan Meng, Sanmukh Kuppannagari, Rajgopal Kannan, Viktor Prasanna", "title": "DYNAMAP: Dynamic Algorithm Mapping Framework for Low Latency CNN\n  Inference", "comments": "Published in ACM/SIGDA FPGA '21", "journal-ref": null, "doi": "10.1145/3431920.3439286", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most of the existing work on FPGA acceleration of Convolutional Neural\nNetwork (CNN) focus on employing a single strategy (algorithm, dataflow, etc.)\nacross all the layers. Such an approach does not achieve optimal latency on\ncomplex and deep CNNs. Emerging CNNs have diverse per-layer computation\ncharacteristics including parallelism, arithmetic intensity, locality, and\nmemory footprint. Per-layer strategy selection and fine-grained tuning are\nrequired to achieve low end-to-end latency. However, specialized hardware\nmodules dedicated to each layer limit the per-layer utilization and adversely\naffect end-to-end latency. In this paper, we address these problems by an\nalgorithm-architecture co-optimization framework, DYNAMAP, consisting of (1) a\nunified hardware overlay that can be reused across layers, supporting dynamic\nmapping of all three families of popular convolution algorithms, and further\nallowing flexible dataflow switching to maximize hardware utilization for each\nlayer; (2) a novel software Design Space Exploration (DSE) flow that customizes\nthe hardware overlay and chooses optimal strategy mapping. We show that the\nalgorithm mapping space increases exponentially with network depth, and while\nthe optimal algorithm selection problem is NP-hard in general, by exploiting\nthe series-parallel structure of CNN models, we demonstrate a polynomial-time\nsolution for optimal algorithm mapping. DYNAMAP is optimized for any CNN,\nincluding those having diverse computation and memory requirements across the\nlayers. We demonstrate DYNAMAP using two state-of-the-art CNNs - GoogleNet and\nInception-V4. The generated accelerators achieve up to $2.8\\times$ and\n$1.4\\times$ speedups, respectively, wrt inference latency compared with the\nstate-of-the-art FPGA implementations.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 00:58:42 GMT"}, {"version": "v2", "created": "Sat, 13 Mar 2021 07:19:56 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Meng", "Yuan", ""], ["Kuppannagari", "Sanmukh", ""], ["Kannan", "Rajgopal", ""], ["Prasanna", "Viktor", ""]]}, {"id": "2012.00941", "submitter": "Xiangqiang Gao", "authors": "Xiangqiang Gao, Rongke Liu (Senior Member, IEEE), and Aryan Kaushik\n  (Member, IEEE)", "title": "Virtual Network Function Placement in Satellite Edge Computing with a\n  Potential Game Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Satellite networks, as a supplement to terrestrial networks, can provide\neffective computing services for Internet of Things (IoT) users in remote\nareas. Due to the resource limitation of satellites, such as in computing,\nstorage, and energy, a computation task from a IoT user can be divided into\nseveral parts and cooperatively accomplished by multiple satellites to improve\nthe overall operational efficiency of satellite networks. Network function\nvirtualization (NFV) is viewed as a new paradigm in allocating network\nresources on-demand. Satellite edge computing combined with the NFV technology\nis becoming an emerging topic. In this paper, we propose a potential game\napproach for virtual network function (VNF) placement in satellite edge\ncomputing. The VNF placement problem aims to maximize the number of allocated\nIoT users, while minimizing the overall deployment cost. We formulate the VNF\nplacement problem with maximum network payoff as a potential game and analyze\nthe problem by a game-theoretical approach. We implement a decentralized\nresource allocation algorithm based on a potential game (PGRA) to tackle the\nVNF placement problem by finding a Nash equilibrium. Finally, we conduct the\nexperiments to evaluate the performance of the proposed PGRA algorithm. The\nsimulation results show that the proposed PGRA algorithm can effectively\naddress the VNF placement problem in satellite edge computing.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 03:06:20 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 09:08:08 GMT"}, {"version": "v3", "created": "Tue, 6 Apr 2021 10:44:17 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Gao", "Xiangqiang", "", "Senior Member, IEEE"], ["Liu", "Rongke", "", "Senior Member, IEEE"], ["Kaushik", "Aryan", "", "Member, IEEE"]]}, {"id": "2012.00953", "submitter": "Benjamin Smith", "authors": "Benjamin Smith", "title": "Ship Detection: Parameter Server Variant", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning ship detection in satellite optical imagery suffers from false\npositive occurrences with clouds, landmasses, and man-made objects that\ninterfere with correct classification of ships, typically limiting class\naccuracy scores to 88\\%. This work explores the tensions between customization\nstrategies, class accuracy rates, training times, and costs in cloud based\nsolutions. We demonstrate how a custom U-Net can achieve 92\\% class accuracy\nover a validation dataset and 68\\% over a target dataset with 90\\% confidence.\nWe also compare a single node architecture with a parameter server variant\nwhose workers act as a boosting mechanism. The parameter server variant\noutperforms class accuracy on the target dataset reaching 73\\% class accuracy\ncompared to the best single node approach. A comparative investigation on the\nsystematic performance of the single node and parameter server variant\narchitectures is discussed with support from empirical findings.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 03:39:24 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Smith", "Benjamin", ""]]}, {"id": "2012.01594", "submitter": "Wei Emma Zhang", "authors": "Wei Emma Zhang, Quan Z. Sheng, Adnan Mahmood, Dai Hoang Tran, Munazza\n  Zaib, Salma Abdalla Hamad, Abdulwahab Aljubairy, Ahoud Abdulrahmn F. Alhazmi,\n  Subhash Sagar, and Congbo Ma", "title": "The 10 Research Topics in the Internet of Things", "comments": "10 pages. IEEE CIC 2020 vision paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the term first coined in 1999 by Kevin Ashton, the Internet of Things\n(IoT) has gained significant momentum as a technology to connect physical\nobjects to the Internet and to facilitate machine-to-human and\nmachine-to-machine communications. Over the past two decades, IoT has been an\nactive area of research and development endeavours by many technical and\ncommercial communities. Yet, IoT technology is still not mature and many issues\nneed to be addressed. In this paper, we identify 10 key research topics and\ndiscuss the research problems and opportunities within these topics.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 23:33:23 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Zhang", "Wei Emma", ""], ["Sheng", "Quan Z.", ""], ["Mahmood", "Adnan", ""], ["Tran", "Dai Hoang", ""], ["Zaib", "Munazza", ""], ["Hamad", "Salma Abdalla", ""], ["Aljubairy", "Abdulwahab", ""], ["Alhazmi", "Ahoud Abdulrahmn F.", ""], ["Sagar", "Subhash", ""], ["Ma", "Congbo", ""]]}, {"id": "2012.01636", "submitter": "Jianyu Niu", "authors": "Jianyu Niu and Chen Feng", "title": "Leaderless Byzantine Fault Tolerant Consensus", "comments": "13 page, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Byzantine fault tolerant (BFT) consensus has recently gained much attention\nbecause of its intriguing connection with blockchains. Several state-of-the-art\nBFT consensus protocols have been proposed in the age of blockchains such as\nTendermint [5], Pala [9], Streamlet [8], HotStuff [23], and Fast-HotStuff [17].\nThese protocols are all leader-based (i.e., protocols run in a series of views,\nand each view has a delegated node called the leader to coordinate all\nconsensus decisions). To make progress, leader-based BFT protocols usually rely\non view synchronization, which is an ad-hoc way of rotating the leader and\nsynchronizing nodes to the same view with the leader for enough overlap time.\nHowever, many studies and system implementations show that existing methods of\nview synchronization are complicated and bug-prone [2], [15], [16], [19]. In\nthis paper, we aim to design a leaderless Byzantine fault tolerant (LBFT)\nprotocol, in which nodes simply compete to propose blocks (containing a batch\nof clients' requests) without the need of explicit coordination through view\nsynchronization. LBFT also enjoys several other desirable features emphasized\nrecently by the research community, such as the chain structure, pipelining\ntechniques, and advanced cryptography [5], [6], [9], [17], [23]. With these\nefforts, LBFT can achieve both good performance (e.g., O(n)or O(nlog(n))\nmessage complexity) and prominent simplicity.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 01:53:52 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Niu", "Jianyu", ""], ["Feng", "Chen", ""]]}, {"id": "2012.01659", "submitter": "EPTCS", "authors": "Hans-J\\\"org Kreowski (University of Bremen, Department of Computer\n  Science, Bremen, Germany), Aaron Lye (University of Bremen, Department of\n  Computer Science, Bremen, Germany)", "title": "Graph Surfing in Reaction Systems from a Categorial Perspective", "comments": "In Proceedings GCM 2020, arXiv:2012.01181", "journal-ref": "EPTCS 330, 2020, pp. 71-87", "doi": "10.4204/EPTCS.330.5", "report-no": null, "categories": "cs.DC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-based reaction systems were recently introduced as a generalization of\nthe intensely studied set-based reaction systems. They deal with simple\nedge-labeled directed graphs, and dynamic semantics of graph-based reaction\nsystems is defined by graph surfing as a novel kind of graph transformation\nwhere, in a single surf step, reactions are applied to a subgraph of a given\nbackground graph yielding a successor subgraph. In this paper, we propose a\ncategorical approach to reaction systems so that a wider spectrum of data\nstructures becomes available on which reaction systems can be based. In this\nway, many types of graphs, hypergraphs, and graph-like structures are covered.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 02:28:11 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Kreowski", "Hans-J\u00f6rg", "", "University of Bremen, Department of Computer\n  Science, Bremen, Germany"], ["Lye", "Aaron", "", "University of Bremen, Department of\n  Computer Science, Bremen, Germany"]]}, {"id": "2012.01686", "submitter": "Matthew Daggitt Dr", "authors": "Matthew L. Daggitt, Timothy G. Griffin", "title": "Dynamic Asynchronous Iterations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many problems can be solved by iteration by multiple participants\n(processors, servers, routers etc.). Previous mathematical models for such\nasynchronous iterations assume a single function being iterated by a fixed set\nof participants. We will call such iterations static since the system's\nconfiguration does not change. However in several real-world examples, such as\ninter-domain routing, both the function being iterated and the set of\nparticipants change frequently while the system continues to function. In this\npaper we extend Uresin & Dubois's work on static iterations to develop a model\nfor this class of \"dynamic\" or \"always on\" asynchronous iterations. We explore\nwhat it means for such an iteration to be implemented correctly, and then prove\ntwo different conditions on the set of iterated functions that guarantee the\nfull asynchronous iteration satisfies this new definition of correctness. These\nresults have been formalised in Agda and the resulting library is publicly\navailable.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 03:43:28 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 02:38:12 GMT"}, {"version": "v3", "created": "Mon, 19 Jul 2021 06:01:48 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Daggitt", "Matthew L.", ""], ["Griffin", "Timothy G.", ""]]}, {"id": "2012.01752", "submitter": "Louis Esperet", "authors": "Nicolas Bousquet, Louis Esperet, Fran\\c{c}ois Pirot", "title": "Distributed algorithms for fractional coloring", "comments": "16 pages, 2 figures. Full version of a paper accepted at SIROCCO 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study fractional coloring from the angle of distributed\ncomputing. Fractional coloring is the linear relaxation of the classical notion\nof coloring, and has many applications, in particular in scheduling. It was\nproved by Hasemann, Hirvonen, Rybicki and Suomela (2016) that for every real\n$\\alpha>1$ and integer $\\Delta$, a fractional coloring of total weight at most\n$\\alpha(\\Delta+1)$ can be obtained deterministically in a single round in\ngraphs of maximum degree $\\Delta$, in the LOCAL model of computation. However,\na major issue of this result is that the output of each vertex has unbounded\nsize. Here we prove that even if we impose the more realistic assumption that\nthe output of each vertex has constant size, we can find fractional colorings\nof total weight arbitrarily close to known tight bounds for the fractional\nchromatic number in several cases of interest. More precisely, we show that for\nany fixed $\\epsilon > 0$ and $\\Delta$, a fractional coloring of total weight at\nmost $\\Delta+\\epsilon$ can be found in $O(\\log^*n)$ rounds in graphs of maximum\ndegree $\\Delta$ with no $K_{\\Delta+1}$, while finding a fractional coloring of\ntotal weight at most $\\Delta$ in this case requires $\\Omega(\\log \\log n)$\nrounds for randomized algorithms and $\\Omega( \\log n)$ rounds for deterministic\nalgorithms. We also show how to obtain fractional colorings of total weight at\nmost $2+\\epsilon$ in grids of any fixed dimension, for any $\\epsilon>0$, in\n$O(\\log^*n)$ rounds. Finally, we prove that in sparse graphs of large girth\nfrom any proper minor-closed family we can find a fractional coloring of total\nweight at most $2+\\epsilon$, for any $\\epsilon>0$, in $O(\\log n)$ rounds.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 08:37:14 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 10:57:46 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Bousquet", "Nicolas", ""], ["Esperet", "Louis", ""], ["Pirot", "Fran\u00e7ois", ""]]}, {"id": "2012.01823", "submitter": "Jan Strohschein", "authors": "Jan Strohschein, Andreas Fischbach, Andreas Bunte, Heide\n  Faeskorn-Woyke, Natalia Moriz, Thomas Bartz-Beielstein", "title": "Cognitive Capabilities for the CAAI in Cyber-Physical Production Systems", "comments": null, "journal-ref": null, "doi": "10.1007/s00170-021-07248-3", "report-no": null, "categories": "cs.AI cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the cognitive module of the cognitive architecture for\nartificial intelligence (CAAI) in cyber-physical production systems (CPPS). The\ngoal of this architecture is to reduce the implementation effort of artificial\nintelligence (AI) algorithms in CPPS. Declarative user goals and the provided\nalgorithm-knowledge base allow the dynamic pipeline orchestration and\nconfiguration. A big data platform (BDP) instantiates the pipelines and\nmonitors the CPPS performance for further evaluation through the cognitive\nmodule. Thus, the cognitive module is able to select feasible and robust\nconfigurations for process pipelines in varying use cases. Furthermore, it\nautomatically adapts the models and algorithms based on model quality and\nresource consumption. The cognitive module also instantiates additional\npipelines to test algorithms from different classes. CAAI relies on\nwell-defined interfaces to enable the integration of additional modules and\nreduce implementation effort. Finally, an implementation based on Docker,\nKubernetes, and Kafka for the virtualization and orchestration of the\nindividual modules and as messaging-technology for module communication is used\nto evaluate a real-world use case.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 10:55:56 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Strohschein", "Jan", ""], ["Fischbach", "Andreas", ""], ["Bunte", "Andreas", ""], ["Faeskorn-Woyke", "Heide", ""], ["Moriz", "Natalia", ""], ["Bartz-Beielstein", "Thomas", ""]]}, {"id": "2012.01882", "submitter": "Uri Meir", "authors": "Uri Meir", "title": "Comparison Graphs: a Unified Method for Uniformity Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Distribution testing can be described as follows: $q$ samples are being drawn\nfrom some unknown distribution $P$ over a known domain $[n]$. After the\nsampling process, a decision must be made about whether $P$ holds some\nproperty, or is far from it. The most studied problem in the field is arguably\nuniformity testing, where one needs to distinguish the case that $P$ is uniform\nover $[n]$ from the case that $P$ is $\\epsilon$-far from being uniform (in\n$\\ell_1$). In the classic model, it is known that\n$\\Theta\\left(\\sqrt{n}/\\epsilon^2\\right)$ samples are necessary and sufficient\nfor this task. This problem was recently considered in various restricted\nmodels that pose, for example, communication or memory constraints. In more\nthan one occasion, the known optimal solution boils down to counting collisions\namong the drawn samples (each two samples that have the same value add one to\nthe count), an idea that dates back to the first uniformity tester, and was\ncoined the name \"collision-based tester\".\n  In this paper, we introduce the notion of comparison graphs and use it to\nformally define a generalized collision-based tester. Roughly speaking, the\nedges of the graph indicate the tester which pairs of samples should be\ncompared (that is, the original tester is induced by a clique, where all pairs\nare being compared). We prove a structural theorem that gives a sufficient\ncondition for a comparison graph to induce a good uniformity tester. As an\napplication, we develop a generic method to test uniformity, and devise\nnearly-optimal uniformity testers under various computational constraints. We\nimprove and simplify a few known results, and introduce a new constrained model\nin which the method also produces an efficient tester.\n  The idea behind our method is to translate computational constraints of a\ncertain model to ones on the comparison graph, which paves the way to finding a\ngood graph.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 12:57:17 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Meir", "Uri", ""]]}, {"id": "2012.01968", "submitter": "Jung Ho Ahn", "authors": "Sangpyo Kim and Wonkyung Jung and Jaiyoung Park and Jung Ho Ahn", "title": "Accelerating Number Theoretic Transformations for Bootstrappable\n  Homomorphic Encryption on GPUs", "comments": "12 pages, 13 figures, to appear in IISWC 2020", "journal-ref": null, "doi": "10.1109/IISWC50251.2020.00033", "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Homomorphic encryption (HE) draws huge attention as it provides a way of\nprivacy-preserving computations on encrypted messages. Number Theoretic\nTransform (NTT), a specialized form of Discrete Fourier Transform (DFT) in the\nfinite field of integers, is the key algorithm that enables fast computation on\nencrypted ciphertexts in HE. Prior works have accelerated NTT and its inverse\ntransformation on a popular parallel processing platform, GPU, by leveraging\nDFT optimization techniques. However, these GPU-based studies lack a\ncomprehensive analysis of the primary differences between NTT and DFT or only\nconsider small HE parameters that have tight constraints in the number of\narithmetic operations that can be performed without decryption. In this paper,\nwe analyze the algorithmic characteristics of NTT and DFT and assess the\nperformance of NTT when we apply the optimizations that are commonly applicable\nto both DFT and NTT on modern GPUs. From the analysis, we identify that NTT\nsuffers from severe main-memory bandwidth bottleneck on large HE parameter\nsets. To tackle the main-memory bandwidth issue, we propose a novel\nNTT-specific on-the-fly root generation scheme dubbed on-the-fly twiddling\n(OT). Compared to the baseline radix-2 NTT implementation, after applying all\nthe optimizations, including OT, we achieve 4.2x speedup on a modern GPU.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 14:47:03 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Kim", "Sangpyo", ""], ["Jung", "Wonkyung", ""], ["Park", "Jaiyoung", ""], ["Ahn", "Jung Ho", ""]]}, {"id": "2012.02076", "submitter": "Jinyan Wang", "authors": "Jinhuan Duan, Xianxian Li, Shiqi Gao, Jinyan Wang and Zili Zhong", "title": "SSGD: A safe and efficient method of gradient descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.CV cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the vigorous development of artificial intelligence technology, various\nengineering technology applications have been implemented one after another.\nThe gradient descent method plays an important role in solving various\noptimization problems, due to its simple structure, good stability and easy\nimplementation. In multi-node machine learning system, the gradients usually\nneed to be shared. Shared gradients are generally unsafe. Attackers can obtain\ntraining data simply by knowing the gradient information. In this paper, to\nprevent gradient leakage while keeping the accuracy of model, we propose the\nsuper stochastic gradient descent approach to update parameters by concealing\nthe modulus length of gradient vectors and converting it or them into a unit\nvector. Furthermore, we analyze the security of super stochastic gradient\ndescent approach. Our algorithm can defend against attacks on the gradient.\nExperiment results show that our approach is obviously superior to prevalent\ngradient descent approaches in terms of accuracy, robustness, and adaptability\nto large-scale batches.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 17:09:20 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 04:33:08 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Duan", "Jinhuan", ""], ["Li", "Xianxian", ""], ["Gao", "Shiqi", ""], ["Wang", "Jinyan", ""], ["Zhong", "Zili", ""]]}, {"id": "2012.02131", "submitter": "Zaid Alzaid", "authors": "Zaid ALzaid, Xin Yuan, Saptarshi Bhowmik", "title": "Multi-Path Routing on the Jellyfish Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The Jellyfish network has recently be proposed as an alternate to the\nfat-tree network as the interconnect for data centers and high performance\ncomputing clusters. Jellyfish adopts a random regular graph as its topology and\nhas been showed to be more cost-effective than fat-trees. Effective routing on\nJellyfish is challenging. It is known that shortest path routing and equal-cost\nmulti-path routing (ECMP) do not work well on Jellyfish. Existing schemes use\nvariations of k-shortest path routing (KSP). In this work, we study two routing\ncomponents for Jellyfish: path selection that decides the paths to route\ntraffic, and routing mechanisms that decide which path to be used for each\npacket. We show that the performance of the existing KSP can be significantly\nimproved by incorporating two heuristics, randomization and edge-disjointness.\nWe evaluate a range of routing mechanisms including traffic oblivious and\ntraffic adaptive schemes and identify an adaptive routing scheme that has\nsignificantly higher performance than others including the Universal Globally\nAdaptive Load-balance (UGAL) routing.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 18:10:06 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["ALzaid", "Zaid", ""], ["Yuan", "Xin", ""], ["Bhowmik", "Saptarshi", ""]]}, {"id": "2012.02328", "submitter": "Vijay Janapa Reddi", "authors": "Vijay Janapa Reddi, David Kanter, Peter Mattson, Jared Duke, Thai\n  Nguyen, Ramesh Chukka, Kenneth Shiring, Koan-Sin Tan, Mark Charlebois,\n  William Chou, Mostafa El-Khamy, Jungwook Hong, Michael Buch, Cindy Trinh,\n  Thomas Atta-fosu, Fatih Cakir, Masoud Charkhabi, Xiaodong Chen, Jimmy Chiang,\n  Dave Dexter, Woncheol Heo, Guenther Schmuelling, Maryam Shabani, Dylan Zika", "title": "MLPerf Mobile Inference Benchmark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  MLPerf Mobile is the first industry-standard open-source mobile benchmark\ndeveloped by industry members and academic researchers to allow\nperformance/accuracy evaluation of mobile devices with different AI chips and\nsoftware stacks. The benchmark draws from the expertise of leading mobile-SoC\nvendors, ML-framework providers, and model producers. In this paper, we\nmotivate the drive to demystify mobile-AI performance and present MLPerf\nMobile's design considerations, architecture, and implementation. The benchmark\ncomprises a suite of models that operate under standard models, data sets,\nquality metrics, and run rules. For the first iteration, we developed an app to\nprovide an \"out-of-the-box\" inference-performance benchmark for computer vision\nand natural-language processing on mobile devices. MLPerf Mobile can serve as a\nframework for integrating future models, for customizing quality-target\nthresholds to evaluate system performance, for comparing software frameworks,\nand for assessing heterogeneous-hardware capabilities for machine learning, all\nfairly and faithfully with fully reproducible results.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 23:29:03 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 14:34:51 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Reddi", "Vijay Janapa", ""], ["Kanter", "David", ""], ["Mattson", "Peter", ""], ["Duke", "Jared", ""], ["Nguyen", "Thai", ""], ["Chukka", "Ramesh", ""], ["Shiring", "Kenneth", ""], ["Tan", "Koan-Sin", ""], ["Charlebois", "Mark", ""], ["Chou", "William", ""], ["El-Khamy", "Mostafa", ""], ["Hong", "Jungwook", ""], ["Buch", "Michael", ""], ["Trinh", "Cindy", ""], ["Atta-fosu", "Thomas", ""], ["Cakir", "Fatih", ""], ["Charkhabi", "Masoud", ""], ["Chen", "Xiaodong", ""], ["Chiang", "Jimmy", ""], ["Dexter", "Dave", ""], ["Heo", "Woncheol", ""], ["Schmuelling", "Guenther", ""], ["Shabani", "Maryam", ""], ["Zika", "Dylan", ""]]}, {"id": "2012.02536", "submitter": "Basilis Mamalis", "authors": "Basilis Mamalis and Marios Perlitis", "title": "Energy Balanced Two-level Clustering for Large-Scale Wireless Sensor\n  Networks based on the Gravitational Search Algorithm", "comments": "11 pages", "journal-ref": "International Journal of Advanced Computer Science and\n  Applications, Vol. 10, No. 12, 2019", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Organizing sensor nodes in clusters is an effective method for energy\npreservation in a Wireless Sensor Network (WSN). Throughout this research work\nwe present a novel hybrid clustering scheme, that combines a typical gradient\nclustering protocol with an evolutionary optimization method that is mainly\nbased on the Gravitational Search Algorithm (GSA). The proposed scheme aims at\nimproved performance over large in size networks, where classical schemes in\nmost cases lead to non-efficient solutions. It first creates suitably balanced\nmultihop clusters, in which the sensors energy gets larger as coming closer to\nthe cluster head (CH). In the next phase of the proposed scheme a suitable\nprotocol based on the GSA runs to associate sets of cluster heads to specific\ngateway nodes for the eventual relaying of data to the base station (BS). The\nfitness function was appropriately chosen considering both the distance from\nthe cluster heads to the gateway nodes and the remaining energy of the gateway\nnodes, and it was further optimized in order to gain more accurate results for\nlarge instances. Extended experimental measurements demonstrate the efficiency\nand scalability of the presented approach over very large WSNs, as well as its\nsuperiority over other known clustering approaches presented in the literature.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 11:36:25 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Mamalis", "Basilis", ""], ["Perlitis", "Marios", ""]]}, {"id": "2012.02701", "submitter": "Sebastian Siebertz", "authors": "Simeon Kublenz, Sebastian Siebertz, Alexandre Vigny", "title": "Constant round distributed domination on graph classes with bounded\n  expansion", "comments": "Paper accepted at SIROCCO 2021, implemented reviews, corrected an\n  error in Lemma 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We show that the dominating set problem admits a constant factor\napproximation in a constant number of rounds in the LOCAL model of distributed\ncomputing on graph classes with bounded expansion. This generalizes a result of\nCzygrinow et al. for graphs with excluded topological minors.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 16:15:42 GMT"}, {"version": "v2", "created": "Sat, 13 Mar 2021 16:31:18 GMT"}, {"version": "v3", "created": "Mon, 28 Jun 2021 14:44:35 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Kublenz", "Simeon", ""], ["Siebertz", "Sebastian", ""], ["Vigny", "Alexandre", ""]]}, {"id": "2012.02732", "submitter": "Gyeong-In Yu", "authors": "Woosuk Kwon, Gyeong-In Yu, Eunji Jeong, Byung-Gon Chun", "title": "Nimble: Lightweight and Parallel GPU Task Scheduling for Deep Learning", "comments": "In NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL) frameworks take advantage of GPUs to improve the speed of\nDL inference and training. Ideally, DL frameworks should be able to fully\nutilize the computation power of GPUs such that the running time depends on the\namount of computation assigned to GPUs. Yet, we observe that in scheduling GPU\ntasks, existing DL frameworks suffer from inefficiencies such as large\nscheduling overhead and unnecessary serial execution. To this end, we propose\nNimble, a DL execution engine that runs GPU tasks in parallel with minimal\nscheduling overhead. Nimble introduces a novel technique called ahead-of-time\n(AoT) scheduling. Here, the scheduling procedure finishes before executing the\nGPU kernel, thereby removing most of the scheduling overhead during run time.\nFurthermore, Nimble automatically parallelizes the execution of GPU tasks by\nexploiting multiple GPU streams in a single GPU. Evaluation on a variety of\nneural networks shows that compared to PyTorch, Nimble speeds up inference and\ntraining by up to 22.34$\\times$ and 3.61$\\times$, respectively. Moreover,\nNimble outperforms state-of-the-art inference systems, TensorRT and TVM, by up\nto 2.81$\\times$ and 1.70$\\times$, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 17:25:46 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Kwon", "Woosuk", ""], ["Yu", "Gyeong-In", ""], ["Jeong", "Eunji", ""], ["Chun", "Byung-Gon", ""]]}, {"id": "2012.02925", "submitter": "Weicheng Xue", "authors": "Weicheng Xue, Charles W. Jackson and Christoper J. Roy", "title": "An Improved Framework of GPU Computing for CFD Applications on\n  Structured Grids using OpenACC", "comments": "43 pages, 27 figures", "journal-ref": null, "doi": "10.1016/j.jpdc.2021.05.010", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is focused on improving multi-GPU performance of a research CFD\ncode on structured grids. MPI and OpenACC directives are used to scale the code\nup to 16 GPUs. This paper shows that using 16 P100 GPUs and 16 V100 GPUs can be\n30$\\times$ and 70$\\times$ faster than 16 Xeon CPU E5-2680v4 cores for three\ndifferent test cases, respectively. A series of performance issues related to\nthe scaling for the multi-block CFD code are addressed by applying various\noptimizations. Performance optimizations such as the pack/unpack message\nmethod, removing temporary arrays as arguments to procedure calls, allocating\nglobal memory for limiters and connected boundary data, reordering non-blocking\nMPI I\\_send/I\\_recv and Wait calls, reducing unnecessary implicit derived type\nmember data movement between the host and the device and the use of GPUDirect\ncan improve the compute utilization, memory throughput, and asynchronous\nprogression in the multi-block CFD code using modern programming features.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 02:25:56 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Xue", "Weicheng", ""], ["Jackson", "Charles W.", ""], ["Roy", "Christoper J.", ""]]}, {"id": "2012.03112", "submitter": "Juan G\\'omez-Luna", "authors": "Onur Mutlu, Saugata Ghose, Juan G\\'omez-Luna, Rachata Ausavarungnirun", "title": "A Modern Primer on Processing in Memory", "comments": "arXiv admin note: substantial text overlap with arXiv:1903.03988", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern computing systems are overwhelmingly designed to move data to\ncomputation. This design choice goes directly against at least three key trends\nin computing that cause performance, scalability and energy bottlenecks: (1)\ndata access is a key bottleneck as many important applications are increasingly\ndata-intensive, and memory bandwidth and energy do not scale well, (2) energy\nconsumption is a key limiter in almost all computing platforms, especially\nserver and mobile systems, (3) data movement, especially off-chip to on-chip,\nis very expensive in terms of bandwidth, energy and latency, much more so than\ncomputation. These trends are especially severely-felt in the data-intensive\nserver and energy-constrained mobile systems of today. At the same time,\nconventional memory technology is facing many technology scaling challenges in\nterms of reliability, energy, and performance. As a result, memory system\narchitects are open to organizing memory in different ways and making it more\nintelligent, at the expense of higher cost. The emergence of 3D-stacked memory\nplus logic, the adoption of error correcting codes inside the latest DRAM\nchips, proliferation of different main memory standards and chips, specialized\nfor different purposes (e.g., graphics, low-power, high bandwidth, low\nlatency), and the necessity of designing new solutions to serious reliability\nand security issues, such as the RowHammer phenomenon, are an evidence of this\ntrend. This chapter discusses recent research that aims to practically enable\ncomputation close to data, an approach we call processing-in-memory (PIM). PIM\nplaces computation mechanisms in or near where the data is stored (i.e., inside\nthe memory chips, in the logic layer of 3D-stacked memory, or in the memory\ncontrollers), so that data movement between the computation units and memory is\nreduced or eliminated.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 19:59:49 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Mutlu", "Onur", ""], ["Ghose", "Saugata", ""], ["G\u00f3mez-Luna", "Juan", ""], ["Ausavarungnirun", "Rachata", ""]]}, {"id": "2012.03140", "submitter": "Anup Joshi", "authors": "Prasad Jayanti, Anup Joshi", "title": "Recoverable Mutual Exclusion with Abortability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in non-volatile main memory (NVRAM) technology have spurred\nresearch on designing algorithms that are resilient to process crashes. This\npaper is a fuller version of our conference paper \\cite{jayanti:rmeabort},\nwhich presents the first Recoverable Mutual Exclusion (RME) algorithm that\nsupports abortability. Our algorithm uses only the read, write, and CAS\noperations, which are commonly supported by multiprocessors. It satisfies FCFS\nand other standard properties.\n  Our algorithm is also adaptive. On DSM and Relaxed-CC multiprocessors, a\nprocess incurs $O(\\min(k, \\log n))$ RMRs in a passage and $O(f+ \\min(k, \\log\nn))$ RMRs in an attempt, where $n$ is the number of processes that the\nalgorithm is designed for, $k$ is the point contention of the passage or the\nattempt, and $f$ is the number of times that $p$ crashes during the attempt. On\na Strict CC multiprocessor, the passage and attempt complexities are $O(n)$ and\n$O(f+n)$.\n  Attiya et al. proved that, with any mutual exclusion algorithm, a process\nincurs at least $\\Omega(\\log n)$ RMRs in a passage, if the algorithm uses only\nthe read, write, and CAS operations \\cite{Attiya:lbound}. This lower bound\nimplies that the worst-case RMR complexity of our algorithm is optimal for the\nDSM and Relaxed CC multiprocessors.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 23:18:21 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Jayanti", "Prasad", ""], ["Joshi", "Anup", ""]]}, {"id": "2012.03185", "submitter": "Diego Ram\\'irez-Romero", "authors": "Pedro Montealegre, Diego Ram\\'irez-Romero, Iv\\'an Rapaport", "title": "Compact Distributed Interactive Proofs for the Recognition of Cographs\n  and Distance-Hereditary Graphs", "comments": "20 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present compact distributed interactive proofs for the recognition of two\nimportant graph classes, well-studied in the context of centralized algorithms,\nnamely complement reducible graphs and distance-hereditary graphs. Complement\nreducible graphs (also called cographs) are defined as the graphs not\ncontaining a four-node path $P_4$ as an induced subgraph. Distance-hereditary\ngraphs are a super-class of cographs, defined as the graphs where the distance\n(shortest paths) between any pair of vertices is the same on every induced\nconnected subgraph.\n  First, we show that there exists a distributed interactive proof for the\nrecognition of cographs with two rounds of interaction. More precisely, we give\na $\\mathsf{dAM}$ protocol with a proof size of $\\mathcal{O}(\\log n)$ bits that\nuses shared randomness and recognizes cographs with high probability. Moreover,\nour protocol can be adapted to verify any Turing-decidable predicate restricted\nto cographs in $\\mathsf{dAM}$ with certificates of size $\\mathcal{O}(\\log n)$.\n  Second, we give a three-round, $\\mathsf{dMAM}$ interactive protocol for the\nrecognition of distance-hereditary graphs, still with a proof size of\n$\\mathcal{O}(\\log n)$ bits and also using shared randomness.\n  Finally, we show that any one-round (denoted $\\mathsf{dM}$) or two-round,\n$\\mathsf{dMA}$ protocol for the recognition of cographs or distance-hereditary\ngraphs requires certificates of size $\\Omega(\\log n)$ bits. Moreover, we show\nthat any constant-round $\\mathsf{dAM}$ protocol using shared randomness\nrequires certificates of size $\\Omega(\\log \\log n)$.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 05:10:40 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Montealegre", "Pedro", ""], ["Ram\u00edrez-Romero", "Diego", ""], ["Rapaport", "Iv\u00e1n", ""]]}, {"id": "2012.03214", "submitter": "Jin-woo Lee", "authors": "Jin-woo Lee, Jaehoon Oh, Sungsu Lim, Se-Young Yun, Jae-Gil Lee", "title": "TornadoAggregate: Accurate and Scalable Federated Learning via the\n  Ring-Based Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning has emerged as a new paradigm of collaborative machine\nlearning; however, many prior studies have used global aggregation along a star\ntopology without much consideration of the communication scalability or the\ndiurnal property relied on clients' local time variety. In contrast, ring\narchitecture can resolve the scalability issue and even satisfy the diurnal\nproperty by iterating nodes without an aggregation. Nevertheless, such\nring-based algorithms can inherently suffer from the high-variance problem. To\nthis end, we propose a novel algorithm called TornadoAggregate that improves\nboth accuracy and scalability by facilitating the ring architecture. In\nparticular, to improve the accuracy, we reformulate the loss minimization into\na variance reduction problem and establish three principles to reduce variance:\nRing-Aware Grouping, Small Ring, and Ring Chaining. Experimental results show\nthat TornadoAggregate improved the test accuracy by up to 26.7% and achieved\nnear-linear scalability.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 08:39:05 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Lee", "Jin-woo", ""], ["Oh", "Jaehoon", ""], ["Lim", "Sungsu", ""], ["Yun", "Se-Young", ""], ["Lee", "Jae-Gil", ""]]}, {"id": "2012.03257", "submitter": "Liekang Zeng", "authors": "Liekang Zeng, Xu Chen, Zhi Zhou, Lei Yang, Junshan Zhang", "title": "CoEdge: Cooperative DNN Inference with Adaptive Workload Partitioning\n  over Heterogeneous Edge Devices", "comments": "Accepted by IEEE/ACM Transactions on Networking, Nov. 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in artificial intelligence have driven increasing intelligent\napplications at the network edge, such as smart home, smart factory, and smart\ncity. To deploy computationally intensive Deep Neural Networks (DNNs) on\nresource-constrained edge devices, traditional approaches have relied on either\noffloading workload to the remote cloud or optimizing computation at the end\ndevice locally. However, the cloud-assisted approaches suffer from the\nunreliable and delay-significant wide-area network, and the local computing\napproaches are limited by the constrained computing capability. Towards\nhigh-performance edge intelligence, the cooperative execution mechanism offers\na new paradigm, which has attracted growing research interest recently. In this\npaper, we propose CoEdge, a distributed DNN computing system that orchestrates\ncooperative DNN inference over heterogeneous edge devices. CoEdge utilizes\navailable computation and communication resources at the edge and dynamically\npartitions the DNN inference workload adaptive to devices' computing\ncapabilities and network conditions. Experimental evaluations based on a\nrealistic prototype show that CoEdge outperforms status-quo approaches in\nsaving energy with close inference latency, achieving up to 25.5%~66.9% energy\nreduction for four widely-adopted CNN models.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 13:15:52 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Zeng", "Liekang", ""], ["Chen", "Xu", ""], ["Zhou", "Zhi", ""], ["Yang", "Lei", ""], ["Zhang", "Junshan", ""]]}, {"id": "2012.03270", "submitter": "Taehyeon Kim", "authors": "Taehyeon Kim, Sangmin Bae, Jin-woo Lee, Seyoung Yun", "title": "Accurate and Fast Federated Learning via Combinatorial Multi-Armed\n  Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning has emerged as an innovative paradigm of collaborative\nmachine learning. Unlike conventional machine learning, a global model is\ncollaboratively learned while data remains distributed over a tremendous number\nof client devices, thus not compromising user privacy. However, several\nchallenges still remain despite its glowing popularity; above all, the global\naggregation in federated learning involves the challenge of biased model\naveraging and lack of prior knowledge in client sampling, which, in turn, leads\nto high generalization error and slow convergence rate, respectively. In this\nwork, we propose a novel algorithm called FedCM that addresses the two\nchallenges by utilizing prior knowledge with multi-armed bandit based client\nsampling and filtering biased models with combinatorial model averaging. Based\non extensive evaluations using various algorithms and representative\nheterogeneous datasets, we showed that FedCM significantly outperformed the\nstate-of-the-art algorithms by up to 37.25% and 4.17 times, respectively, in\nterms of generalization accuracy and convergence rate.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 14:05:14 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Kim", "Taehyeon", ""], ["Bae", "Sangmin", ""], ["Lee", "Jin-woo", ""], ["Yun", "Seyoung", ""]]}, {"id": "2012.03399", "submitter": "Sayaka Kamei", "authors": "Sayaka Kamei and S\\'ebastien Tixeuil", "title": "An Asynchronous Maximum Independent Set Algorithm by Myopic Luminous\n  Robots on Grids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of constructing a maximum independent set with mobile\nmyopic luminous robots on a grid network whose size is finite but unknown to\nthe robots. In this setting, the robots enter the grid network one-by-one from\na corner of the grid, and they eventually have to be disseminated on the grid\nnodes so that the occupied positions form a maximum independent set of the\nnetwork. We assume that robots are asynchronous, anonymous, silent, and they\nexecute the same distributed algorithm. In this paper, we propose two\nalgorithms: The first one assumes the number of light colors of each robot is\nthree and the visible range is two, but uses additional strong assumptions of\nport-numbering for each node. To delete this assumption, the second one assumes\nthe number of light colors of each robot is seven and the visible range is\nthree. In both algorithms, the number of movements is $O(n(L+l))$ steps where\n$n$ is the number of nodes and $L$ and $l$ are the grid dimensions.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 00:30:00 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Kamei", "Sayaka", ""], ["Tixeuil", "S\u00e9bastien", ""]]}, {"id": "2012.03461", "submitter": "Lei Wang", "authors": "Lei Wang, Xin Liu, and Yin Zhang", "title": "A Distributed and Secure Algorithm for Computing Dominant SVD Based on\n  Projection Splitting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose and study a distributed and secure algorithm for\ncomputing dominant (or truncated) singular value decompositions (SVD) of large\nand distributed data matrices. We consider the scenario where each node\nprivately holds a subset of columns and only exchanges ''safe'' information\nwith other nodes in a collaborative effort to calculate a dominant SVD for the\nwhole matrix. In the framework of alternating direction methods of multipliers\n(ADMM), we propose a novel formulation for building consensus by equalizing\nsubspaces spanned by splitting variables instead of equalizing themselves. This\ntechnique greatly relaxes feasibility restrictions and accelerates convergence\nsignificantly, while at the same time yielding simple subproblems. We design\nseveral algorithmic features, including a low-rank multiplier formula and\nmechanisms for controlling subproblem solution accuracies, to increase the\nalgorithm's computational efficiency and reduce its communication overhead.\nMore importantly, the possibility appears remote, if possible at all, for a\nmalicious node to uncover the data stored in another node through shared\nquantities available in our algorithm, which is not the case in existing\ndistributed or parallelized algorithms. We present the convergence analysis\nresults, including a worst-case complexity estimate, and extensive experimental\nresults indicating that the proposed algorithm, while safely guarding data\nprivacy, has a strong potential to deliver a cutting-edge performance,\nespecially when communication costs are high.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 05:52:18 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 12:34:07 GMT"}, {"version": "v3", "created": "Mon, 14 Jun 2021 13:34:05 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Wang", "Lei", ""], ["Liu", "Xin", ""], ["Zhang", "Yin", ""]]}, {"id": "2012.03550", "submitter": "Hao Li", "authors": "Hao Li, Zixuan Li, Kenli Li, Jan S. Rellermeyer, Lydia Y. Chen, Keqin\n  Li", "title": "SGD_Tucker: A Novel Stochastic Optimization Strategy for Parallel Sparse\n  Tucker Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sparse Tucker Decomposition (STD) algorithms learn a core tensor and a group\nof factor matrices to obtain an optimal low-rank representation feature for the\n\\underline{H}igh-\\underline{O}rder, \\underline{H}igh-\\underline{D}imension, and\n\\underline{S}parse \\underline{T}ensor (HOHDST). However, existing STD\nalgorithms face the problem of intermediate variables explosion which results\nfrom the fact that the formation of those variables, i.e., matrices Khatri-Rao\nproduct, Kronecker product, and matrix-matrix multiplication, follows the whole\nelements in sparse tensor. The above problems prevent deep fusion of efficient\ncomputation and big data platforms. To overcome the bottleneck, a novel\nstochastic optimization strategy (SGD$\\_$Tucker) is proposed for STD which can\nautomatically divide the high-dimension intermediate variables into small\nbatches of intermediate matrices. Specifically, SGD$\\_$Tucker only follows the\nrandomly selected small samples rather than the whole elements, while\nmaintaining the overall accuracy and convergence rate. In practice,\nSGD$\\_$Tucker features the two distinct advancements over the state of the art.\nFirst, SGD$\\_$Tucker can prune the communication overhead for the core tensor\nin distributed settings. Second, the low data-dependence of SGD$\\_$Tucker\nenables fine-grained parallelization, which makes SGD$\\_$Tucker obtaining lower\ncomputational overheads with the same accuracy. Experimental results show that\nSGD$\\_$Tucker runs at least 2$X$ faster than the state of the art.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 09:38:49 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 18:21:39 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Li", "Hao", ""], ["Li", "Zixuan", ""], ["Li", "Kenli", ""], ["Rellermeyer", "Jan S.", ""], ["Chen", "Lydia Y.", ""], ["Li", "Keqin", ""]]}, {"id": "2012.03576", "submitter": "Yan Li", "authors": "Yan Li, Bo An, Junming Ma, Donggang Cao, Yasha Wang, Hong Mei", "title": "SpotTune: Leveraging Transient Resources for Cost-efficient\n  Hyper-parameter Tuning in the Public Cloud", "comments": "11 pages, accepted by ICDCS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hyper-parameter tuning (HPT) is crucial for many machine learning (ML)\nalgorithms. But due to the large searching space, HPT is usually time-consuming\nand resource-intensive. Nowadays, many researchers use public cloud resources\nto train machine learning models, convenient yet expensive. How to speed up the\nHPT process while at the same time reduce cost is very important for cloud ML\nusers. In this paper, we propose SpotTune, an approach that exploits transient\nrevocable resources in the public cloud with some tailored strategies to do HPT\nin a parallel and cost-efficient manner. Orchestrating the HPT process upon\ntransient servers, SpotTune uses two main techniques, fine-grained cost-aware\nresource provisioning, and ML training trend predicting, to reduce the monetary\ncost and runtime of HPT processes. Our evaluations show that SpotTune can\nreduce the cost by up to 90% and achieve a 16.61x performance-cost rate\nimprovement.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 10:32:39 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Li", "Yan", ""], ["An", "Bo", ""], ["Ma", "Junming", ""], ["Cao", "Donggang", ""], ["Wang", "Yasha", ""], ["Mei", "Hong", ""]]}, {"id": "2012.03665", "submitter": "Justin Ormont", "authors": "Phuong Pham, Vivek Jain, Lukas Dauterman, Justin Ormont, Navendu Jain", "title": "DeepTriage: Automated Transfer Assistance for Incidents in Cloud\n  Services", "comments": null, "journal-ref": "KDD '20: Proceedings of the 26th ACM SIGKDD International\n  Conference on Knowledge Discovery & Data Mining August 2020. Pages 3281-3289", "doi": "10.1145/3394486.3403380", "report-no": null, "categories": "cs.DC cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As cloud services are growing and generating high revenues, the cost of\ndowntime in these services is becoming significantly expensive. To reduce loss\nand service downtime, a critical primary step is to execute incident triage,\nthe process of assigning a service incident to the correct responsible team, in\na timely manner. An incorrect assignment risks additional incident reroutings\nand increases its time to mitigate by 10x. However, automated incident triage\nin large cloud services faces many challenges: (1) a highly imbalanced incident\ndistribution from a large number of teams, (2) wide variety in formats of input\ndata or data sources, (3) scaling to meet production-grade requirements, and\n(4) gaining engineers' trust in using machine learning recommendations. To\naddress these challenges, we introduce DeepTriage, an intelligent incident\ntransfer service combining multiple machine learning techniques - gradient\nboosted classifiers, clustering methods, and deep neural networks - in an\nensemble to recommend the responsible team to triage an incident. Experimental\nresults on real incidents in Microsoft Azure show that our service achieves\n82.9% F1 score. For highly impacted incidents, DeepTriage achieves F1 score\nfrom 76.3% - 91.3%. We have applied best practices and state-of-the-art\nframeworks to scale DeepTriage to handle incident routing for all cloud\nservices. DeepTriage has been deployed in Azure since October 2017 and is used\nby thousands of teams daily.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 03:10:11 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Pham", "Phuong", ""], ["Jain", "Vivek", ""], ["Dauterman", "Lukas", ""], ["Ormont", "Justin", ""], ["Jain", "Navendu", ""]]}, {"id": "2012.03692", "submitter": "Ohad Ben-Baruch", "authors": "Ohad Ben-Baruch and Srivatsan Ravi", "title": "Separation and Equivalence results for the Crash-stop and Crash-recovery\n  Shared Memory Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Linearizability, the traditional correctness condition for concurrent data\nstructures is considered insufficient for the non-volatile shared memory model\nwhere processes recover following a crash. For this crash-recovery shared\nmemory model, strict-linearizability is considered appropriate since, unlike\nlinearizability, it ensures operations that crash take effect prior to the\ncrash or not at all.\n  This work formalizes and answers the question of whether an implementation of\na data type derived for the crash-stop shared memory model is also\nstrict-linearizable in the crash-recovery model. This work presents a rigorous\nstudy to prove how helping mechanisms, typically employed by non-blocking\nimplementations, is the algorithmic abstraction that delineates linearizability\nfrom strict-linearizability. Our first contribution formalizes the\ncrash-recovery model and how explicit process crashes and recovery introduces\nfurther dimensionalities over the standard crash-stop shared memory model. We\nmake the following technical contributions: (i) we prove that\nstrict-linearizability is independent of any known help definition; (ii) we\nthen present a natural definition of help-freedom to prove that any\nobstruction-free, linearizable and help-free implementation of a total object\ntype is also strict-linearizable; (iii) finally, we prove that for a large\nclass of object types, a non-blocking strict-linearizable implementation cannot\nhave helping. Viewed holistically, this work provides the first precise\ncharacterization of the intricacies in applying a concurrent implementation\ndesigned for the crash-stop model to the crash-recovery model, and vice-versa.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 13:48:35 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Ben-Baruch", "Ohad", ""], ["Ravi", "Srivatsan", ""]]}, {"id": "2012.03736", "submitter": "Joseph Corneli", "authors": "Raymond Puzio, Paola Ricaurte, Charles Jeffrey Danoff, Charlotte\n  Pierce, Analua Dutka-Chirichetti, Vitor Bruno, Hermano Cintra, Joseph Corneli", "title": "Patterns, anticipation and participatory futures", "comments": "29 pages, 5 figures; submitted to Futures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SY eess.SY", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Patterns embody repeating phenomena, and, as such, they are partly but not\nfully detachable from their context. 'Design patterns' and 'pattern languages'\nare established methods for working with patterns. They have been applied in\narchitecture, software engineering, and other design fields, but have so far\nseen little application in the field of future studies. We reimagine futures\ndiscourse and anticipatory practices using pattern methods. We focus\nspecifically on processes for coordinating distributed projects, integrating\nmultiple voices, and on play that builds capability to face what's yet to come.\nOne of the advantages of the method as a whole is that it deals with local\nknowledge and does not subsume everything within one overall 'global' strategy,\nwhile nevertheless offering a way to communicate between contexts and\ndisciplines.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 11:16:45 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Puzio", "Raymond", ""], ["Ricaurte", "Paola", ""], ["Danoff", "Charles Jeffrey", ""], ["Pierce", "Charlotte", ""], ["Dutka-Chirichetti", "Analua", ""], ["Bruno", "Vitor", ""], ["Cintra", "Hermano", ""], ["Corneli", "Joseph", ""]]}, {"id": "2012.03747", "submitter": "Huiping Zhuang", "authors": "Huiping Zhuang, Zhiping Lin, Kar-Ann Toh", "title": "Accumulated Decoupled Learning: Mitigating Gradient Staleness in\n  Inter-Layer Model Parallelization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decoupled learning is a branch of model parallelism which parallelizes the\ntraining of a network by splitting it depth-wise into multiple modules.\nTechniques from decoupled learning usually lead to stale gradient effect\nbecause of their asynchronous implementation, thereby causing performance\ndegradation. In this paper, we propose an accumulated decoupled learning (ADL)\nwhich incorporates the gradient accumulation technique to mitigate the stale\ngradient effect. We give both theoretical and empirical evidences regarding how\nthe gradient staleness can be reduced. We prove that the proposed method can\nconverge to critical points, i.e., the gradients converge to 0, in spite of its\nasynchronous nature. Empirical validation is provided by training deep\nconvolutional neural networks to perform classification tasks on CIFAR-10 and\nImageNet datasets. The ADL is shown to outperform several state-of-the-arts in\nthe classification tasks, and is the fastest among the compared methods.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 11:52:55 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Zhuang", "Huiping", ""], ["Lin", "Zhiping", ""], ["Toh", "Kar-Ann", ""]]}, {"id": "2012.03913", "submitter": "Andr\\'e Gaul", "authors": "Andr\\'e Gaul, J\\\"org Liesen", "title": "Centrality of nodes in Federated Byzantine Agreement Systems", "comments": "24 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The federated Byzantine agreement system (FBAS) is a consensus model\nintroduced by Mazi\\`eres in 2016 where the participating nodes conceptually\nform a network, with links between them being established by each node\nindividually and thus in a decentralized way. An important question is whether\nthese decentralized decisions lead to an overall decentralized network. The\nlevel of (de-)centralization in a network can be assessed using centrality\nmeasures. In this paper we consider three different approaches for obtaining\ncentrality measures for the nodes in an FBAS. Two of them are based on adapting\nwell-known measures based on graphs and hypergraphs to the FBAS context. Since\nthe network structure of an FBAS can be more complex than (usual) graphs or\nhypergraphs, we also develop a new, problem-adapted centrality measure. This\nnew measure is based on the intactness of nodes, which is an important\ningredient of the FBAS model. We illustrate advantages and disadvantages of the\nthree approaches on several computed examples. We have implemented all\ncentrality measures and performed all computations in the Python package\nStellar Observatory.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 18:42:39 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Gaul", "Andr\u00e9", ""], ["Liesen", "J\u00f6rg", ""]]}, {"id": "2012.04061", "submitter": "Abolfazl Hashemi", "authors": "Rudrajit Das, Anish Acharya, Abolfazl Hashemi, Sujay Sanghavi,\n  Inderjit S. Dhillon, Ufuk Topcu", "title": "Faster Non-Convex Federated Learning via Global and Local Momentum", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose \\texttt{FedGLOMO}, the first (first-order) FL\nalgorithm that achieves the optimal iteration complexity (i.e matching the\nknown lower bound) on smooth non-convex objectives -- without using clients'\nfull gradient in each round. Our key algorithmic idea that enables attaining\nthis optimal complexity is applying judicious momentum terms that promote\nvariance reduction in both the local updates at the clients, and the global\nupdate at the server. Our algorithm is also provably optimal even with\ncompressed communication between the clients and the server, which is an\nimportant consideration in the practical deployment of FL algorithms. Our\nexperiments illustrate the intrinsic variance reduction effect of\n\\texttt{FedGLOMO} which implicitly suppresses client-drift in heterogeneous\ndata distribution settings and promotes communication-efficiency. As a prequel\nto \\texttt{FedGLOMO}, we propose \\texttt{FedLOMO} which applies momentum only\nin the local client updates. We establish that \\texttt{FedLOMO} enjoys improved\nconvergence rates under common non-convex settings compared to prior work, and\nwith fewer assumptions.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 21:05:31 GMT"}, {"version": "v2", "created": "Sat, 12 Dec 2020 00:16:40 GMT"}, {"version": "v3", "created": "Fri, 19 Feb 2021 22:57:58 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Das", "Rudrajit", ""], ["Acharya", "Anish", ""], ["Hashemi", "Abolfazl", ""], ["Sanghavi", "Sujay", ""], ["Dhillon", "Inderjit S.", ""], ["Topcu", "Ufuk", ""]]}, {"id": "2012.04063", "submitter": "Christian Makaya", "authors": "Christian Makaya, Amalendu Iyer, Jonathan Salfity, Madhu Athreya, M\n  Anthony Lewis", "title": "Cost-effective Machine Learning Inference Offload for Edge Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.RO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Computing at the edge is increasingly important since a massive amount of\ndata is generated. This poses challenges in transporting all that data to the\nremote data centers and cloud, where they can be processed and analyzed. On the\nother hand, harnessing the edge data is essential for offering data-driven and\nmachine learning-based applications, if the challenges, such as device\ncapabilities, connectivity, and heterogeneity can be mitigated. Machine\nlearning applications are very compute-intensive and require processing of\nlarge amount of data. However, edge devices are often resources-constrained, in\nterms of compute resources, power, storage, and network connectivity. Hence,\nlimiting their potential to run efficiently and accurately state-of-the art\ndeep neural network (DNN) models, which are becoming larger and more complex.\nThis paper proposes a novel offloading mechanism by leveraging installed-base\non-premises (edge) computational resources. The proposed mechanism allows the\nedge devices to offload heavy and compute-intensive workloads to edge nodes\ninstead of using remote cloud. Our offloading mechanism has been prototyped and\ntested with state-of-the art person and object detection DNN models for mobile\nrobots and video surveillance applications. The performance shows a significant\ngain compared to cloud-based offloading strategies in terms of accuracy and\nlatency.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 21:11:02 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Makaya", "Christian", ""], ["Iyer", "Amalendu", ""], ["Salfity", "Jonathan", ""], ["Athreya", "Madhu", ""], ["Lewis", "M Anthony", ""]]}, {"id": "2012.04158", "submitter": "Hailiang Zhao", "authors": "Hailiang Zhao, Shuiguang Deng, Zijie Liu, Zhengzhe Xiang, Jianwei Yin", "title": "Placement is not Enough: Embedding with Proactive Stream Mapping on the\n  Heterogenous Edge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Edge computing is naturally suited to the applications generated by Internet\nof Things (IoT) nodes. The IoT applications generally take the form of directed\nacyclic graphs (DAGs), where vertices represent interdependent functions and\nedges represent data streams. The status quo of minimizing the makespan of the\nDAG motivates the study on optimal function placement. However, current\napproaches lose sight of proactively mapping the data streams to the physical\nlinks between the heterogenous edge servers, which could affect the makespan of\nDAGs significantly. To solve this problem, we study both function placement and\nstream mapping with data splitting simultaneously, and propose the algorithm\nDPE (Dynamic Programming-based Embedding). DPE is theoretically verified to\nachieve the global optimality of the embedding problem. The complexity analysis\nis also provided. Extensive experiments on Alibaba cluster trace dataset show\nthat DPE significantly outperforms two state-of-the-art joint function\nplacement and task scheduling algorithms in makespan by 43.19% and 40.71%,\nrespectively.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 01:46:31 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Zhao", "Hailiang", ""], ["Deng", "Shuiguang", ""], ["Liu", "Zijie", ""], ["Xiang", "Zhengzhe", ""], ["Yin", "Jianwei", ""]]}, {"id": "2012.04172", "submitter": "Hong-Ning Dai Prof.", "authors": "Xiaoyun Li, Zibin Zheng, Hong-Ning Dai", "title": "When Services Computing Meets Blockchain: Challenges and Opportunities", "comments": "15 pages, 5 figures", "journal-ref": "Journal of Parallel and Distributed Computing, 2021", "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Services computing can offer a high-level abstraction to support diverse\napplications via encapsulating various computing infrastructures. Though\nservices computing has greatly boosted the productivity of developers, it is\nfaced with three main challenges: privacy and security risks, information silo,\nand pricing mechanisms and incentives. The recent advances of blockchain bring\nopportunities to address the challenges of services computing due to its\nbuild-in encryption as well as digital signature schemes, decentralization\nfeature, and intrinsic incentive mechanisms. In this paper, we present a survey\nto investigate the integration of blockchain with services computing. The\nintegration of blockchain with services computing mainly exhibits merits in two\naspects: i) blockchain can potentially address key challenges of services\ncomputing and ii) services computing can also promote blockchain development.\nIn particular, we categorize the current literature of services computing based\non blockchain into five types: services creation, services discovery, services\nrecommendation, services composition, and services arbitration. Moreover, we\ngeneralize Blockchain as a Service (BaaS) architecture and summarize the\nrepresentative BaaS platforms. In addition, we also outline open issues of\nblockchain-based services computing and BaaS.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 02:27:43 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Li", "Xiaoyun", ""], ["Zheng", "Zibin", ""], ["Dai", "Hong-Ning", ""]]}, {"id": "2012.04402", "submitter": "Chirag Kumar", "authors": "Ketan Rajawat, Chirag Kumar", "title": "A Primal-Dual Framework for Decentralized Stochastic Optimization", "comments": "31 pages, 6 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the decentralized convex optimization problem, where multiple\nagents must cooperatively minimize a cumulative objective function, with each\nlocal function expressible as an empirical average of data-dependent losses.\nState-of-the-art approaches for decentralized optimization rely on gradient\ntracking, where consensus is enforced via a doubly stochastic mixing matrix.\nConstruction of such mixing matrices is not straightforward and requires\ncoordination even prior to the start of the optimization algorithm. This paper\nputs forth a primal-dual framework for decentralized stochastic optimization\nthat obviates the need for such doubly stochastic matrices. Instead, dual\nvariables are maintained to track the disagreement between neighbors. The\nproposed framework is flexible and is used to develop decentralized variants of\nSAGA, L-SVRG, SVRG++, and SEGA algorithms. Using a unified proof, we establish\nthat the oracle complexity of these decentralized variants is $O(1/\\epsilon)$,\nmatching the complexity bounds obtained for the centralized variants.\nAdditionally, we also present a decentralized primal-dual accelerated SVRG\nalgorithm achieving $O(1/\\sqrt{\\epsilon})$ oracle complexity, again matching\nthe bound for the centralized accelerated SVRG. Numerical tests on the\nalgorithms establish their superior performance as compared to the\nvariance-reduced gradient tracking algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 12:45:56 GMT"}, {"version": "v2", "created": "Sun, 13 Dec 2020 12:11:04 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Rajawat", "Ketan", ""], ["Kumar", "Chirag", ""]]}, {"id": "2012.04432", "submitter": "Yi Liu", "authors": "Yi Liu, Xingliang Yuan, Ruihui Zhao, Yifeng Zheng, Yefeng Zheng", "title": "RC-SSFL: Towards Robust and Communication-efficient Semi-supervised\n  Federated Learning System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning (FL) is an emerging decentralized artificial intelligence\nparadigm, which promises to train a shared global model in high-quality while\nprotecting user data privacy. However, the current systems rely heavily on a\nstrong assumption: all clients have a wealth of ground truth labeled data,\nwhich may not be always feasible in the real life. In this paper, we present a\npractical Robust, and Communication-efficient Semi-supervised FL (RC-SSFL)\nsystem design that can enable the clients to jointly learn a high-quality model\nthat is comparable to typical FL's performance. In this setting, we assume that\nthe client has only unlabeled data and the server has a limited amount of\nlabeled data. Besides, we consider malicious clients can launch poisoning\nattacks to harm the performance of the global model. To solve this issue,\nRC-SSFL employs a minimax optimization-based client selection strategy to\nselect the clients who hold high-quality updates and uses geometric median\naggregation to robustly aggregate model updates. Furthermore, RC-SSFL\nimplements a novel symmetric quantization method to greatly improve\ncommunication efficiency. Extensive case studies on two real-world datasets\ndemonstrate that RC-SSFL can maintain the performance comparable to typical FL\nin the presence of poisoning attacks and reduce communication overhead by $2\n\\times \\sim 4 \\times $.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 14:02:56 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Liu", "Yi", ""], ["Yuan", "Xingliang", ""], ["Zhao", "Ruihui", ""], ["Zheng", "Yifeng", ""], ["Zheng", "Yefeng", ""]]}, {"id": "2012.04436", "submitter": "Yi Liu", "authors": "Yi Liu, Ruihui Zhao, Jiawen Kang, Abdulsalam Yassine, Dusit Niyato,\n  Jialiang Peng", "title": "Towards Communication-efficient and Attack-Resistant Federated Edge\n  Learning for Industrial Internet of Things", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Edge Learning (FEL) allows edge nodes to train a global deep\nlearning model collaboratively for edge computing in the Industrial Internet of\nThings (IIoT), which significantly promotes the development of Industrial 4.0.\nHowever, FEL faces two critical challenges: communication overhead and data\nprivacy. FEL suffers from expensive communication overhead when training\nlarge-scale multi-node models. Furthermore, due to the vulnerability of FEL to\ngradient leakage and label-flipping attacks, the training process of the global\nmodel is easily compromised by adversaries. To address these challenges, we\npropose a communication-efficient and privacy-enhanced asynchronous FEL\nframework for edge computing in IIoT. First, we introduce an asynchronous model\nupdate scheme to reduce the computation time that edge nodes wait for global\nmodel aggregation. Second, we propose an asynchronous local differential\nprivacy mechanism, which improves communication efficiency and mitigates\ngradient leakage attacks by adding well-designed noise to the gradients of edge\nnodes. Third, we design a cloud-side malicious node detection mechanism to\ndetect malicious nodes by testing the local model quality. Such a mechanism can\navoid malicious nodes participating in training to mitigate label-flipping\nattacks. Extensive experimental studies on two real-world datasets demonstrate\nthat the proposed framework can not only improve communication efficiency but\nalso mitigate malicious attacks while its accuracy is comparable to traditional\nFEL frameworks.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 14:11:32 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Liu", "Yi", ""], ["Zhao", "Ruihui", ""], ["Kang", "Jiawen", ""], ["Yassine", "Abdulsalam", ""], ["Niyato", "Dusit", ""], ["Peng", "Jialiang", ""]]}, {"id": "2012.04553", "submitter": "Keval Vora", "authors": "Kasra Jamshidi and Keval Vora", "title": "Pattern Morphing for Efficient Graph Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Graph mining applications analyze the structural properties of large graphs,\nand they do so by finding subgraph isomorphisms, which makes them\ncomputationally intensive. Existing graph mining techniques including both\ncustom graph mining applications and general-purpose graph mining systems,\ndevelop efficient execution plans to speed up the exploration of the given\nquery patterns that represent subgraph structures of interest.\n  In this paper, we step beyond the traditional philosophy of optimizing the\nexecution plans for a given set of patterns, and exploit the sub-structural\nsimilarities across different query patterns. We propose Pattern Morphing, a\ntechnique that enables structure-aware algebra over patterns to accurately\ninfer the results for a given set of patterns using the results of a completely\ndifferent set of patterns that are less expensive to compute. Pattern morphing\n\"morphs\" (or converts) a given set of query patterns into alternative patterns,\nwhile retaining full equivalency. It is a general technique that supports\nvarious operations over matches of a pattern beyond just counting (e.g.,\nsupport calculation, enumeration, etc.), making it widely applicable to various\ngraph mining applications like Motif Counting and Frequent Subgraph Mining.\nSince pattern morphing mainly transforms query patterns before their\nexploration starts, it can be easily incorporated in existing general-purpose\ngraph mining systems. We evaluate the effectiveness of pattern morphing by\nincorporating it in Peregrine, a recent state-of-the-art graph mining system,\nand show that pattern morphing significantly improves the performance of\ndifferent graph mining applications.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 16:46:31 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Jamshidi", "Kasra", ""], ["Vora", "Keval", ""]]}, {"id": "2012.04705", "submitter": "Srinivas Kota Reddy", "authors": "Kota Srinivas Reddy and Nikhil Karamchandani", "title": "Structured Index Coding Problem and Multi-access Coded Caching", "comments": "42 pages, single column", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.CO math.IT", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Index coding and coded caching are two active research topics in information\ntheory with strong ties to each other. Motivated by the multi-access coded\ncaching problem, we study a new class of structured index coding problems\n(ICPs) which are formed by the union of several symmetric ICPs. We derive upper\nand lower bounds on the optimal server transmission rate for this class of ICPs\nand demonstrate that they differ by at most a factor of two. Finally, we apply\nthese results to the multi-access coded caching problem to derive better bounds\nthan the state of the art.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 19:47:10 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 03:50:29 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Reddy", "Kota Srinivas", ""], ["Karamchandani", "Nikhil", ""]]}, {"id": "2012.04854", "submitter": "Jer Shyuan Ng", "authors": "Jer Shyuan Ng, Wei Yang Bryan Lim, Sahil Garg, Zehui Xiong, Dusit\n  Niyato, Mohsen Guizani, and Cyril Leung", "title": "Collaborative Coded Computation Offloading: An All-pay Auction Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the amount of data collected for crowdsensing applications increases\nrapidly due to improved sensing capabilities and the increasing number of\nInternet of Things (IoT) devices, the cloud server is no longer able to handle\nthe large-scale datasets individually. Given the improved computational\ncapabilities of the edge devices, coded distributed computing has become a\npromising approach given that it allows computation tasks to be carried out in\na distributed manner while mitigating straggler effects, which often account\nfor the long overall completion times. Specifically, by using polynomial codes,\ncomputed results from only a subset of devices are needed to reconstruct the\nfinal result. However, there is no incentive for the edge devices to complete\nthe computation tasks. In this paper, we present an all-pay auction to\nincentivize the edge devices to participate in the coded computation tasks. In\nthis auction, the bids of the edge devices are represented by the allocation of\ntheir Central Processing Unit (CPU) power to the computation tasks. All edge\ndevices submit their bids regardless of whether they win or lose in the\nauction. The all-pay auction is designed to maximize the utility of the cloud\nserver by determining the reward allocation to the winners. Simulation results\nshow that the edge devices are incentivized to allocate more CPU power when\nmultiple rewards are offered instead of a single reward.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 04:16:22 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Ng", "Jer Shyuan", ""], ["Lim", "Wei Yang Bryan", ""], ["Garg", "Sahil", ""], ["Xiong", "Zehui", ""], ["Niyato", "Dusit", ""], ["Guizani", "Mohsen", ""], ["Leung", "Cyril", ""]]}, {"id": "2012.04857", "submitter": "Jin-woo Lee", "authors": "Jin-woo Lee, Jaehoon Oh, Yooju Shin, Jae-Gil Lee, Se-Young Yoon", "title": "Accurate and Fast Federated Learning via IID and Communication-Aware\n  Grouping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning has emerged as a new paradigm of collaborative machine\nlearning; however, it has also faced several challenges such as non-independent\nand identically distributed(IID) data and high communication cost. To this end,\nwe propose a novel framework of IID and communication-aware group federated\nlearning that simultaneously maximizes both accuracy and communication speed by\ngrouping nodes based on data distributions and physical locations of the nodes.\nFurthermore, we provide a formal convergence analysis and an efficient\noptimization algorithm called FedAvg-IC. Experimental results show that,\ncompared with the state-of-the-art algorithms, FedAvg-IC improved the test\naccuracy by up to 22.2% and simultaneously reduced the communication time to as\nsmall as 12%.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 04:30:38 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Lee", "Jin-woo", ""], ["Oh", "Jaehoon", ""], ["Shin", "Yooju", ""], ["Lee", "Jae-Gil", ""], ["Yoon", "Se-Young", ""]]}, {"id": "2012.04880", "submitter": "Somali Chaterji", "authors": "Karthick Shankar, Pengcheng Wang, Ran Xu, Ashraf Mahgoub, Somali\n  Chaterji", "title": "JANUS: Benchmarking Commercial and Open-Source Cloud and Edge Platforms\n  for Object and Anomaly Detection Workloads", "comments": "Appeared at the IEEE Cloud 2020 conference. 10 pages", "journal-ref": "\"JANUS: Benchmarking Commercial and Open-Source Cloud and Edge\n  Platforms for Object and Anomaly Detection Workloads,\" IEEE International\n  Conference on Cloud Computing (IEEE Cloud), pp. 1--10, Oct 18-24, 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With diverse IoT workloads, placing compute and analytics close to where data\nis collected is becoming increasingly important. We seek to understand what is\nthe performance and the cost implication of running analytics on IoT data at\nthe various available platforms. These workloads can be compute-light, such as\noutlier detection on sensor data, or compute-intensive, such as object\ndetection from video feeds obtained from drones. In our paper, JANUS, we\nprofile the performance/$ and the compute versus communication cost for a\ncompute-light IoT workload and a compute-intensive IoT workload. In addition,\nwe also look at the pros and cons of some of the proprietary deep-learning\nobject detection packages, such as Amazon Rekognition, Google Vision, and Azure\nCognitive Services, to contrast with open-source and tunable solutions, such as\nFaster R-CNN (FRCNN). We find that AWS IoT Greengrass delivers at least 2X\nlower latency and 1.25X lower cost compared to all other cloud platforms for\nthe compute-light outlier detection workload. For the compute-intensive\nstreaming video analytics task, an opensource solution to object detection\nrunning on cloud VMs saves on dollar costs compared to proprietary solutions\nprovided by Amazon, Microsoft, and Google, but loses out on latency (up to 6X).\nIf it runs on a low-powered edge device, the latency is up to 49X lower.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 06:07:26 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Shankar", "Karthick", ""], ["Wang", "Pengcheng", ""], ["Xu", "Ran", ""], ["Mahgoub", "Ashraf", ""], ["Chaterji", "Somali", ""]]}, {"id": "2012.04883", "submitter": "Sharareh Alipour", "authors": "Sharareh Alipour, Ehsan Futuhi, Shayan Karimi", "title": "On Distributed Algorithms for Minimum Dominating Set problem, from\n  theory to application", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a distributed algorithm for the minimum dominating\nset problem. For some especial networks, we prove theoretically that the\nachieved answer by our proposed algorithm is a constant approximation factor of\nthe exact answer. This problem arises naturally in social networks, for example\nin news spreading, avoiding rumor spreading and recommendation spreading. So we\nimplement our algorithm on massive social networks and compare our results with\nthe state of the art algorithms. Also, we extend our algorithm to solve the\n$k$-distance dominating set problem and experimentally study the efficiency of\nthe proposed algorithm.\n  Our proposed algorithm is fast and easy to implement and can be used in\ndynamic networks where the edges and vertices are added or deleted constantly.\nMore importantly, based on the experimental results the proposed algorithm has\nreasonable solutions and running time which enables us to use it in distributed\nmodel practically.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 06:17:21 GMT"}, {"version": "v2", "created": "Sun, 3 Jan 2021 12:00:55 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Alipour", "Sharareh", ""], ["Futuhi", "Ehsan", ""], ["Karimi", "Shayan", ""]]}, {"id": "2012.04930", "submitter": "Alexandra Angerd", "authors": "Alexandra Angerd, Keshav Balasubramanian, Murali Annavaram", "title": "Distributed Training of Graph Convolutional Networks using Subgraph\n  Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern machine learning techniques are successfully being adapted to data\nmodeled as graphs. However, many real-world graphs are typically very large and\ndo not fit in memory, often making the problem of training machine learning\nmodels on them intractable. Distributed training has been successfully employed\nto alleviate memory problems and speed up training in machine learning domains\nin which the input data is assumed to be independently identical distributed\n(i.i.d). However, distributing the training of non i.i.d data such as graphs\nthat are used as training inputs in Graph Convolutional Networks (GCNs) causes\naccuracy problems since information is lost at the graph partitioning\nboundaries.\n  In this paper, we propose a training strategy that mitigates the lost\ninformation across multiple partitions of a graph through a subgraph\napproximation scheme. Our proposed approach augments each sub-graph with a\nsmall amount of edge and vertex information that is approximated from all other\nsub-graphs. The subgraph approximation approach helps the distributed training\nsystem converge at single-machine accuracy, while keeping the memory footprint\nlow and minimizing synchronization overhead between the machines.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 09:23:49 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Angerd", "Alexandra", ""], ["Balasubramanian", "Keshav", ""], ["Annavaram", "Murali", ""]]}, {"id": "2012.04982", "submitter": "Manisha Luthra", "authors": "Manisha Luthra, Sebastian Hennig, Kamran Razavi, Lin Wang and Boris\n  Koldehofe", "title": "Operator as a Service: Stateful Serverless Complex Event Processing", "comments": "10 pages, Published in the Proceedings of the IEEE International\n  Conference on Big Data", "journal-ref": "2020 IEEE International Conference on Big Data (Big Data)", "doi": "10.1109/BigData50022.2020.9378142", "report-no": null, "categories": "cs.NI cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Complex Event Processing (CEP) is a powerful paradigm for scalable data\nmanagement that is employed in many real-world scenarios such as detecting\ncredit card fraud in banks. The so-called complex events are expressed using a\nspecification language that is typically implemented and executed on a specific\nruntime system. While the tight coupling of these two components has been\nregarded as the key for supporting CEP at high performance, such dependencies\npose several inherent challenges as follows. (1) Application development atop a\nCEP system requires extensive knowledge of how the runtime system operates,\nwhich is typically highly complex in nature. (2) The specification language\ndependence requires the need of domain experts and further restricts and\nsteepens the learning curve for application developers. In this paper, we\npropose CEPLESS, a scalable data management system that decouples the\nspecification from the runtime system by building on the principles of\nserverless computing. CEPLESS provides operator as a service and offers\nflexibility by enabling the development of CEP application in any specification\nlanguage while abstracting away the complexity of the CEP runtime system. As\npart of CEPLESS, we designed and evaluated novel mechanisms for in-memory\nprocessing and batching that enables the stateful processing of CEP operators\neven under high rates of ingested events. Our evaluation demonstrates that\nCEPLESS can be easily integrated into existing CEP systems like Apache Flink\nwhile attaining similar throughput under a high scale of events (up to 100K\nevents per second) and dynamic operator update in up to 238 ms.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 11:19:29 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 18:58:30 GMT"}, {"version": "v3", "created": "Mon, 28 Jun 2021 12:27:14 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Luthra", "Manisha", ""], ["Hennig", "Sebastian", ""], ["Razavi", "Kamran", ""], ["Wang", "Lin", ""], ["Koldehofe", "Boris", ""]]}, {"id": "2012.05070", "submitter": "Manisha Luthra", "authors": "Manisha Luthra, Johannes Pfannm\\\"uller, Boris Koldehofe, Jonas\n  H\\\"ochst, Artur Sterz, Rhaban Hark and Bernd Freisleben", "title": "Efficient Complex Event Processing in Information-centric Networking at\n  the Edge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Information-centric Networking (ICN) is an emerging Internet architecture\nthat offers promising features, such as in-network caching and named data\naddressing, to support the edge computing paradigm, in particular\nInternet-of-Things (IoT) applications. ICN can benefit from Complex Event\nProcessing (CEP), which is an in-network processing paradigm to specify and\nperform efficient query operations on data streams. However, integrating CEP\ninto ICN is a challenging task due to the following reasons: (1) typical ICN\narchitectures do not provide support for forwarding and processing continuous\ndata streams; (2) IoT applications often need short response times and require\nrobust event detection, which both are hard to accomplish using existing CEP\nsystems.\n  In this article, we present a novel network architecture, called INetCEP, for\nefficient CEP-based in-network processing as part of ICN. INetCEP enables\nefficient data processing in ICN by means of (1) a unified communication model\nthat supports continuous data streams, (2) a meta query language for CEP to\nspecify data processing operations in the data plane, and (3) query processing\nalgorithms to resolve the specified operations. Our experimental results for\ntwo IoT use cases and datasets show that INetCEP offers very short response\ntimes of up to 73 {\\mu}s under high workload and is more than 15X faster in\nterms of forwarding events than the state-of-the-art CEP system Flink.\nFurthermore, the delivery and processing of complex queries is around 32X\nfaster than Flink and more than 100X faster than a naive pull-based reference\napproach, while maintaining 100% accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 14:22:34 GMT"}, {"version": "v2", "created": "Sun, 13 Dec 2020 22:26:49 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Luthra", "Manisha", ""], ["Pfannm\u00fcller", "Johannes", ""], ["Koldehofe", "Boris", ""], ["H\u00f6chst", "Jonas", ""], ["Sterz", "Artur", ""], ["Hark", "Rhaban", ""], ["Freisleben", "Bernd", ""]]}, {"id": "2012.05105", "submitter": "Xuejie Wang", "authors": "Honghao Gao and Xuejie Wang and Xiaojin Ma and Wei Wei and Shahid\n  Mumtaz", "title": "Com-DDPG: A Multiagent Reinforcement Learning-based Offloading Strategy\n  for Mobile Edge Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The development of mobile services has impacted a variety of\ncomputation-intensive and time-sensitive applications, such as recommendation\nsystems and daily payment methods. However, computing task competition\ninvolving limited resources increases the task processing latency and energy\nconsumption of mobile devices, as well as time constraints. Mobile edge\ncomputing (MEC) has been widely used to address these problems. However, there\nare limitations to existing methods used during computation offloading. On the\none hand, they focus on independent tasks rather than dependent tasks. The\nchallenges of task dependency in the real world, especially task segmentation\nand integration, remain to be addressed. On the other hand, the multiuser\nscenarios related to resource allocation and the mutex access problem must be\nconsidered. In this paper, we propose a novel offloading approach, Com-DDPG,\nfor MEC using multiagent reinforcement learning to enhance the offloading\nperformance. First, we discuss the task dependency model, task priority model,\nenergy consumption model, and average latency from the perspective of server\nclusters and multidependence on mobile tasks. Our method based on these models\nis introduced to formalize communication behavior among multiple agents; then,\nreinforcement learning is executed as an offloading strategy to obtain the\nresults. Because of the incomplete state information, long short-term memory\n(LSTM) is employed as a decision-making tool to assess the internal state.\nMoreover, to optimize and support effective action, we consider using a\nbidirectional recurrent neural network (BRNN) to learn and enhance features\nobtained from agents' communication. Finally, we simulate experiments on the\nAlibaba cluster dataset. The results show that our method is better than other\nbaselines in terms of energy consumption, load status and latency.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 15:22:47 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Gao", "Honghao", ""], ["Wang", "Xuejie", ""], ["Ma", "Xiaojin", ""], ["Wei", "Wei", ""], ["Mumtaz", "Shahid", ""]]}, {"id": "2012.05239", "submitter": "Manisha Luthra", "authors": "Manisha Luthra, Boris Koldehofe, Jonas H\\\"ochst, Patrick Lampe, Ali\n  Haider Rizvi, Ralf Kundel and Bernd Freisleben", "title": "INetCEP: In-Network Complex Event Processing for Information-Centric\n  Networking", "comments": "arXiv admin note: text overlap with arXiv:2012.05070", "journal-ref": "2019 ACM/IEEE Symposium on Architectures for Networking and\n  Communications Systems (ANCS), Cambridge, United Kingdom, 2019, pp. 1-13", "doi": "10.1109/ANCS.2019.8901877", "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emerging network architectures like Information-centric Networking (ICN)\noffer simplicity in the data plane by addressing named data. Such flexibility\nopens up the possibility to move data processing inside network elements for\nhigh-performance computation, known as in-network processing. However, existing\nICN architectures are limited in terms of data plane programmability due to the\nlack of (i) in-network processing and (ii) data plane programming abstractions.\nSuch architectures can benefit from Complex Event Processing (CEP), an\nin-network processing paradigm to efficiently process data inside the data\nplane. Yet, it is extremely challenging to integrate CEP because the current\ncommunication model of ICN is limited to consumer-initiated interaction that\ncomes with significant overhead in a number of requests to process continuous\ndata streams. In contrast, a change to producer-initiated interaction, as\nfavored by CEP, imposes severe limitations for request-reply interactions. In\nthis paper, we propose an in-network CEP architecture, INetCEP that supports\nunified interaction patterns (consumer- and producer-initiated). In addition,\nwe provide a CEP query language and facilitate CEP operations while increasing\nthe range of applications that can be supported by ICN. We provide an\nopen-source implementation and evaluation of INetCEP over an ICN architecture,\nNamed Function Networking, and two applications: energy forecasting in smart\nhomes and a disaster scenario.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 14:56:01 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 18:52:48 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Luthra", "Manisha", ""], ["Koldehofe", "Boris", ""], ["H\u00f6chst", "Jonas", ""], ["Lampe", "Patrick", ""], ["Rizvi", "Ali Haider", ""], ["Kundel", "Ralf", ""], ["Freisleben", "Bernd", ""]]}, {"id": "2012.05266", "submitter": "Lorenzo Valerio", "authors": "Lorenzo Valerio, Andrea Passarella, Marco Conti", "title": "Optimising cost vs accuracy of decentralised analytics in fog computing\n  environments", "comments": "Submitted to IEEE TNSE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The exponential growth of devices and data at the edges of the Internet is\nrising scalability and privacy concerns on approaches based exclusively on\nremote cloud platforms. Data gravity, a fundamental concept in Fog Computing,\npoints towards decentralisation of computation for data analysis, as a viable\nalternative to address those concerns. Decentralising AI tasks on several\ncooperative devices means identifying the optimal set of locations or\nCollection Points (CP for short) to use, in the continuum between full\ncentralisation (i.e., all data on a single device) and full decentralisation\n(i.e., data on source locations). We propose an analytical framework able to\nfind the optimal operating point in this continuum, linking the accuracy of the\nlearning task with the corresponding network and computational cost for moving\ndata and running the distributed training at the CPs. We show through\nsimulations that the model accurately predicts the optimal trade-off, quite\noften an intermediate point between full centralisation and full\ndecentralisation, showing also a significant cost saving w.r.t. both of them.\nFinally, the analytical model admits closed-form or numeric solutions, making\nit not only a performance evaluation instrument but also a design tool to\nconfigure a given distributed learning task optimally before its deployment.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 19:05:44 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 12:58:11 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Valerio", "Lorenzo", ""], ["Passarella", "Andrea", ""], ["Conti", "Marco", ""]]}, {"id": "2012.05326", "submitter": "Aur\\'elien Bellet", "authors": "Edwige Cyffers, Aur\\'elien Bellet", "title": "Privacy Amplification by Decentralization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Analyzing data owned by several parties while achieving a good trade-off\nbetween utility and privacy is a key challenge in federated learning and\nanalytics. In this work, we introduce a novel relaxation of local differential\nprivacy (LDP) that naturally arises in fully decentralized protocols, i.e.,\nwhen participants exchange information by communicating along the edges of a\nnetwork graph. This relaxation, that we call network DP, captures the fact that\nusers have only a local view of the decentralized system. To show the relevance\nof network DP, we study a decentralized model of computation where a token\nperforms a walk on the network graph and is updated sequentially by the party\nwho receives it. For tasks such as real summation, histogram computation and\noptimization with gradient descent, we propose simple algorithms on ring and\ncomplete topologies. We prove that the privacy-utility trade-offs of our\nalgorithms significantly improve upon LDP, and in some cases even match what\ncan be achieved with methods based on trusted/secure aggregation and shuffling.\nOur experiments illustrate the superior utility of our approach when training a\nmachine learning model with stochastic gradient descent.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 21:33:33 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 14:33:33 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Cyffers", "Edwige", ""], ["Bellet", "Aur\u00e9lien", ""]]}, {"id": "2012.05396", "submitter": "Yemao Xu Mr", "authors": "Yemao Xu, Dezun Dong, Yawei Zhao, Weixia Xu, Xiangke Liao", "title": "SSD-SSD: Communication sparsification for distributed deep learning\n  training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intensive communication and synchronization cost for gradients and parameters\nis the well-known bottleneck of distributed deep learning training. Based on\nthe observations that Synchronous SGD (SSGD) obtains good convergence accuracy\nwhile asynchronous SGD (ASGD) delivers a faster raw training speed, we propose\nSeveral Steps Delay SGD (SSD-SGD) to combine their merits, aiming at tackling\nthe communication bottleneck via communication sparsification. SSD-SGD explores\nboth global synchronous updates in the parameter servers and asynchronous local\nupdates in the workers in each periodic iteration. The periodic and flexible\nsynchronization makes SSD-SGD achieve good convergence accuracy and fast\ntraining speed. To the best of our knowledge, we strike the new balance between\nsynchronization quality and communication sparsification, and improve the\ntrade-off between accuracy and training speed. Specifically, the core\ncomponents of SSD-SGD include proper warm-up stage, steps delay stage, and our\nnovel algorithm of global gradient for local update (GLU). GLU is critical for\nlocal update operations to effectively compensate the delayed local weights.\nFurthermore, we implement SSD-SGD on MXNet framework and comprehensively\nevaluate its performance with CIFAR-10 and ImageNet datasets. Experimental\nresults show that SSD-SGD can accelerate distributed training speed under\ndifferent experimental configurations, by up to 110%, while achieving good\nconvergence accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 01:32:11 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 03:35:05 GMT"}, {"version": "v3", "created": "Fri, 9 Apr 2021 04:53:29 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Xu", "Yemao", ""], ["Dong", "Dezun", ""], ["Zhao", "Yawei", ""], ["Xu", "Weixia", ""], ["Liao", "Xiangke", ""]]}, {"id": "2012.05430", "submitter": "Saigopal Thota", "authors": "Saigopal Thota, Mridul Jain, Nishad Kamat, Saikiran Malikireddy,\n  Pruthvi Raj Eranti, Albin Kuruvilla", "title": "Building Graphs at a Large Scale: Union Find Shuffle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Large scale graph processing using distributed computing frameworks is\nbecoming pervasive and efficient in the industry. In this work, we present a\nhighly scalable and configurable distributed algorithm for building connected\ncomponents, called Union Find Shuffle (UFS) with Path Compression. The scale\nand complexity of the algorithm are a function of the number of partitions into\nwhich the data is initially partitioned, and the size of the connected\ncomponents. We discuss the complexity and the benchmarks compared to similar\napproaches. We also present current benchmarks of our production system,\nrunning on commodity out-of-the-box cloud Hadoop infrastructure, where the\nalgorithm was deployed over a year ago, scaled to around 75 Billion nodes and\n60 Billions linkages (and growing). We highlight the key aspects of our\nalgorithm which enable seamless scaling and performance even in the presence of\nskewed data with large connected components in the size of 10 Billion nodes\neach.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 03:00:39 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 18:39:07 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Thota", "Saigopal", ""], ["Jain", "Mridul", ""], ["Kamat", "Nishad", ""], ["Malikireddy", "Saikiran", ""], ["Eranti", "Pruthvi Raj", ""], ["Kuruvilla", "Albin", ""]]}, {"id": "2012.05439", "submitter": "Yuping Fan", "authors": "Yuping Fan, Zhiling Lan, Paul Rich, William E. Allcock, Michael E.\n  Papka, Brian Austin, David Paul", "title": "Scheduling Beyond CPUs for HPC", "comments": "Accepted by HPDC 2019", "journal-ref": "Proceedings of the 28th ACM International Symposium on\n  High-Performance Parallel and Distributed Computing (HPDC'19), 2019", "doi": "10.1145/3307681.3325401", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High performance computing (HPC) is undergoing significant changes. The\nemerging HPC applications comprise both compute- and data-intensive\napplications. To meet the intense I/O demand from emerging data-intensive\napplications, burst buffers are deployed in production systems. Existing HPC\nschedulers are mainly CPU-centric. The extreme heterogeneity of hardware\ndevices, combined with workload changes, forces the schedulers to consider\nmultiple resources (e.g., burst buffers) beyond CPUs, in decision making. In\nthis study, we present a multi-resource scheduling scheme named BBSched that\nschedules user jobs based on not only their CPU requirements, but also other\nschedulable resources such as burst buffer. BBSched formulates the scheduling\nproblem into a multi-objective optimization (MOO) problem and rapidly solves\nthe problem using a multi-objective genetic algorithm. The multiple solutions\ngenerated by BBSched enables system managers to explore potential tradeoffs\namong various resources, and therefore obtains better utilization of all the\nresources. The trace-driven simulations with real system workloads demonstrate\nthat BBSched improves scheduling performance by up to 41% compared to existing\nmethods, indicating that explicitly optimizing multiple resources beyond CPUs\nis essential for HPC scheduling.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 03:58:36 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Fan", "Yuping", ""], ["Lan", "Zhiling", ""], ["Rich", "Paul", ""], ["Allcock", "William E.", ""], ["Papka", "Michael E.", ""], ["Austin", "Brian", ""], ["Paul", "David", ""]]}, {"id": "2012.05600", "submitter": "Daniel Kelly", "authors": "Daniel Kelly, Frank G Glavin, Enda Barrett", "title": "Serverless Computing: Behind the Scenes of Major Platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Serverless computing offers an event driven pay-as-you-go framework for\napplication development. A key selling point is the concept of no back-end\nserver management, allowing developers to focus on application functionality.\nThis is achieved through severe abstraction of the underlying architecture the\nfunctions run on. We examine the underlying architecture and report on the\nperformance of serverless functions and how they are effected by certain\nfactors such as memory allocation and interference caused by load induced by\nother users on the platform. Specifically, we focus on the serverless offerings\nof the four largest platforms; AWS Lambda, Google Cloud Functions, Microsoft\nAzure Functions and IBM Cloud Functions}. In this paper, we observe and\ncontrast between these platforms in their approach to the common issue of \"cold\nstarts\", we devise a means to unveil the underlying architecture serverless\nfunctions execute on and we investigate the effects of interference from load\non the platform over the time span of one month.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 11:40:57 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Kelly", "Daniel", ""], ["Glavin", "Frank G", ""], ["Barrett", "Enda", ""]]}, {"id": "2012.05622", "submitter": "Naram Mhaisen", "authors": "Naram Mhaisen, Alaa Awad, Amr Mohamed, Aiman Erbad, Mohsen Guizani", "title": "Analysis and Optimal Edge Assignment For Hierarchical Federated Learning\n  on Non-IID Data", "comments": null, "journal-ref": null, "doi": "10.1109/TNSE.2021.3053588", "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Distributed learning algorithms aim to leverage distributed and diverse data\nstored at users' devices to learn a global phenomena by performing training\namongst participating devices and periodically aggregating their local models'\nparameters into a global model. Federated learning is a promising paradigm that\nallows for extending local training among the participant devices before\naggregating the parameters, offering better communication efficiency. However,\nin the cases where the participants' data are strongly skewed (i.e., non-IID),\nthe local models can overfit local data, leading to low performing global\nmodel. In this paper, we first show that a major cause of the performance drop\nis the weighted distance between the distribution over classes on users'\ndevices and the global distribution. Then, to face this challenge, we leverage\nthe edge computing paradigm to design a hierarchical learning system that\nperforms Federated Gradient Descent on the user-edge layer and Federated\nAveraging on the edge-cloud layer. In this hierarchical architecture, we\nformalize and optimize this user-edge assignment problem such that edge-level\ndata distributions turn to be similar (i.e., close to IID), which enhances the\nFederated Averaging performance. Our experiments on multiple real-world\ndatasets show that the proposed optimized assignment is tractable and leads to\nfaster convergence of models towards a better accuracy value.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 12:18:13 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 13:41:44 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Mhaisen", "Naram", ""], ["Awad", "Alaa", ""], ["Mohamed", "Amr", ""], ["Erbad", "Aiman", ""], ["Guizani", "Mohsen", ""]]}, {"id": "2012.06001", "submitter": "Zaoxing Liu", "authors": "Zaoxing Liu, Hun Namkung, Anup Agarwal, Antonis Manousis, Peter\n  Steenkiste, Srinivasan Seshan, Vyas Sekar", "title": "Sketchy With a Chance of Adoption: Can Sketch-Based Telemetry Be Ready\n  for Prime Time?", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sketching algorithms or sketches have emerged as a promising alternative to\nthe traditional packet sampling-based network telemetry solutions. At a high\nlevel, they are attractive because of their high resource efficiency and\naccuracy guarantees. While there have been significant recent advances in\nvarious aspects of sketching for networking tasks, many fundamental challenges\nremain unsolved that are likely stumbling blocks for adoption. Our contribution\nin this paper is in identifying and formulating these research challenges\nacross the ecosystem encompassing network operators, platform\nvendors/developers, and algorithm designers. We hope that these serve as a\nnecessary fillip for the community to enable the broader adoption of\nsketch-based telemetry.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 22:28:51 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Liu", "Zaoxing", ""], ["Namkung", "Hun", ""], ["Agarwal", "Anup", ""], ["Manousis", "Antonis", ""], ["Steenkiste", "Peter", ""], ["Seshan", "Srinivasan", ""], ["Sekar", "Vyas", ""]]}, {"id": "2012.06012", "submitter": "Bruno Magalhaes", "authors": "Bruno Magalhaes, Felix Sch\\\"urmann", "title": "Efficient Distributed Transposition Of Large-Scale Multigraphs And\n  High-Cardinality Sparse Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-based representations underlie a wide range of scientific problems.\n  Graph connectivity is typically represented as a sparse matrix in the\nCompressed Sparse Row format. Large-scale graphs rely on distributed storage,\nallocating distinct subsets of rows to compute nodes.\n  Efficient matrix transpose is an operation of high importance, providing the\nreverse graph pathways and a column-ordered matrix view. This operation is well\nstudied for simple graph models. Nevertheless, its resolution for multigraphs\nand higher-cardinality connectivity matrices is unexistent.\n  We advance state-of-the-art distributed transposition methods by providing a\ntheoretical model, algorithmic details, MPI-based implementation and proof of\nmathematical soundness for such complex models. Benchmark results demonstrate\nideal and almost ideal scaling properties for perfectly- and\nheterogeneously-balanced datasets, respectively\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 23:04:56 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Magalhaes", "Bruno", ""], ["Sch\u00fcrmann", "Felix", ""]]}, {"id": "2012.06021", "submitter": "Chavit Denninnart", "authors": "Shangrui Wu, Chavit Denninnart, Xiangbo Li, Yang Wang, Mohsen Amini\n  Salehi", "title": "Descriptive and Predictive Analysis of Aggregating Functions in\n  Serverless Clouds: the Case of Video Streaming", "comments": null, "journal-ref": "IEEE HPCC 2020", "doi": null, "report-no": null, "categories": "cs.DC cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Serverless clouds allocate multiple tasks (e.g., micro-services) from\nmultiple users on a shared pool of computing resources. This enables serverless\ncloud providers to reduce their resource usage by transparently aggregate\nsimilar tasks of a certain context (e.g., video processing) that share the\nwhole or part of their computation. To this end, it is crucial to know the\namount of time-saving achieved by aggregating the tasks. Lack of such knowledge\ncan lead to uninformed merging and scheduling decisions that, in turn, can\ncause deadline violation of either the merged tasks or other following tasks.\nAccordingly, in this paper, we study the problem of estimating execution-time\nsaving resulted from merging tasks with the example in the context of video\nprocessing. To learn the execution-time saving in different forms of merging,\nwe first establish a set of benchmarking videos and examine a wide variety of\nvideo processing tasks -- with and without merging in place. We observed that\nalthough merging can save up to 44% in the execution-time, the number of\npossible merging cases is intractable. Hence, in the second part, we leverage\nthe benchmarking results and develop a method based on Gradient Boosting\nDecision Tree (GBDT) to estimate the time-saving for any given task merging\ncase. Experimental results show that the method can estimate the time-saving\nwith the error rate of 0.04, measured based on Root Mean Square Error (RMSE).\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 23:37:01 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Wu", "Shangrui", ""], ["Denninnart", "Chavit", ""], ["Li", "Xiangbo", ""], ["Wang", "Yang", ""], ["Salehi", "Mohsen Amini", ""]]}, {"id": "2012.06035", "submitter": "Chulhong Min", "authors": "Chulhong Min, Akhil Mathur, Alessandro Montanari, Utku Gunay Acer,\n  Fahim Kawsar", "title": "SensiX: A Platform for Collaborative Machine Learning on the Edge", "comments": "14 pages, 13 firues, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG eess.SP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The emergence of multiple sensory devices on or near a human body is\nuncovering new dynamics of extreme edge computing. In this, a powerful and\nresource-rich edge device such as a smartphone or a Wi-Fi gateway is\ntransformed into a personal edge, collaborating with multiple devices to offer\nremarkable sensory al eapplications, while harnessing the power of locality,\navailability, and proximity. Naturally, this transformation pushes us to\nrethink how to construct accurate, robust, and efficient sensory systems at\npersonal edge. For instance, how do we build a reliable activity tracker with\nmultiple on-body IMU-equipped devices? While the accuracy of sensing models is\nimproving, their runtime performance still suffers, especially under this\nemerging multi-device, personal edge environments. Two prime caveats that\nimpact their performance are device and data variabilities, contributed by\nseveral runtime factors, including device availability, data quality, and\ndevice placement. To this end, we present SensiX, a personal edge platform that\nstays between sensor data and sensing models, and ensures best-effort inference\nunder any condition while coping with device and data variabilities without\ndemanding model engineering. SensiX externalises model execution away from\napplications, and comprises of two essential functions, a translation operator\nfor principled mapping of device-to-device data and a quality-aware selection\noperator to systematically choose the right execution path as a function of\nmodel accuracy. We report the design and implementation of SensiX and\ndemonstrate its efficacy in developing motion and audio-based multi-device\nsensing systems. Our evaluation shows that SensiX offers a 7-13% increase in\noverall accuracy and up to 30% increase across different environment dynamics\nat the expense of 3mW power overhead.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 23:06:56 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Min", "Chulhong", ""], ["Mathur", "Akhil", ""], ["Montanari", "Alessandro", ""], ["Acer", "Utku Gunay", ""], ["Kawsar", "Fahim", ""]]}, {"id": "2012.06054", "submitter": "Razin Farhan Hussain", "authors": "Razin Farhan Hussain, Alireza Pakravan, Mohsen Amini Salehi", "title": "Analyzing the Performance of Smart Industry 4.0 Applications on Cloud\n  Computing Systems", "comments": null, "journal-ref": "IEEE International Conference on High Performance Computing and\n  Communications (HPCC 2020)", "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cloud-based Deep Neural Network (DNN) applications that make\nlatency-sensitive inference are becoming an indispensable part of Industry 4.0.\nDue to the multi-tenancy and resource heterogeneity, both inherent to the cloud\ncomputing environments, the inference time of DNN-based applications are\nstochastic. Such stochasticity, if not captured, can potentially lead to low\nQuality of Service (QoS) or even a disaster in critical sectors, such as Oil\nand Gas industry. To make Industry 4.0 robust, solution architects and\nresearchers need to understand the behavior of DNN-based applications and\ncapture the stochasticity exists in their inference times. Accordingly, in this\nstudy, we provide a descriptive analysis of the inference time from two\nperspectives. First, we perform an application-centric analysis and\nstatistically model the execution time of four categorically different DNN\napplications on both Amazon and Chameleon clouds. Second, we take a\nresource-centric approach and analyze a rate-based metric in form of Million\nInstruction Per Second (MIPS) for heterogeneous machines in the cloud. This\nnon-parametric modeling, achieved via Jackknife and Bootstrap re-sampling\nmethods, provides the confidence interval of MIPS for heterogeneous cloud\nmachines. The findings of this research can be helpful for researchers and\ncloud solution architects to develop solutions that are robust against the\nstochastic nature of the inference time of DNN applications in the cloud and\ncan offer a higher QoS to their users and avoid unintended outcomes.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 00:18:05 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Hussain", "Razin Farhan", ""], ["Pakravan", "Alireza", ""], ["Salehi", "Mohsen Amini", ""]]}, {"id": "2012.06065", "submitter": "Anindya Bijoy Das", "authors": "Anindya Bijoy Das and Aditya Ramamoorthy", "title": "Coded sparse matrix computation schemes that leverage partial stragglers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Distributed matrix computations over large clusters can suffer from the\nproblem of slow or failed worker nodes (called stragglers) which can dominate\nthe overall job execution time. Coded computation utilizes concepts from\nerasure coding to mitigate the effect of stragglers by running 'coded' copies\nof tasks comprising a job; stragglers are typically treated as erasures. While\nthis is useful, there are issues with applying, e.g., MDS codes in a\nstraightforward manner. Several practical matrix computation scenarios involve\nsparse matrices. MDS codes typically require dense linear combinations of\nsubmatrices of the original matrices which destroy their inherent sparsity.\nThis is problematic as it results in significantly higher worker computation\ntimes. Moreover, treating slow nodes as erasures ignores the potentially useful\npartial computations performed by them. Furthermore, some MDS techniques also\nsuffer from significant numerical stability issues. In this work we present\nschemes that allow us to leverage partial computation by stragglers while\nimposing constraints on the level of coding that is required in generating the\nencoded submatrices. This significantly reduces the worker computation time as\ncompared to previous approaches and results in improved numerical stability in\nthe decoding process. Exhaustive numerical experiments on Amazon Web Services\n(AWS) clusters support our findings.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 01:08:13 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Das", "Anindya Bijoy", ""], ["Ramamoorthy", "Aditya", ""]]}, {"id": "2012.06128", "submitter": "Qin Wang", "authors": "Qin Wang, Jiangshan Yu, Shiping Chen, Yang Xiang", "title": "SoK: Diving into DAG-based Blockchain Systems", "comments": "Full version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Blockchain plays an important role in cryptocurrency markets and technology\nservices. However, limitations on high latency and low scalability retard their\nadoptions and applications in classic designs. Reconstructed blockchain systems\nhave been proposed to avoid the consumption of competitive transactions caused\nby linear sequenced blocks. These systems, instead, structure\ntransactions/blocks in the form of Directed Acyclic Graph (DAG) and\nconsequently re-build upper layer components including consensus, incentives,\n\\textit{etc.} The promise of DAG-based blockchain systems is to enable fast\nconfirmation (complete transactions within million seconds) and high\nscalability (attach transactions in parallel) without significantly\ncompromising security. However, this field still lacks systematic work that\nsummarises the DAG technique. To bridge the gap, this Systematization of\nKnowledge (SoK) provides a comprehensive analysis of DAG-based blockchain\nsystems. Through deconstructing open-sourced systems and reviewing academic\nresearches, we conclude the main components and featured properties of systems,\nand provide the approach to establish a DAG. With this in hand, we analyze the\nsecurity and performance of several leading systems, followed by discussions\nand comparisons with concurrent (scaling blockchain) techniques. We further\nidentify open challenges to highlight the potentiality of DAG-based solutions\nand indicate their promising directions for future research.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 05:13:18 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 02:11:31 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Wang", "Qin", ""], ["Yu", "Jiangshan", ""], ["Chen", "Shiping", ""], ["Xiang", "Yang", ""]]}, {"id": "2012.06171", "submitter": "Alexandru Iosup", "authors": "Sherif Sakr, Angela Bonifati, Hannes Voigt, Alexandru Iosup, Khaled\n  Ammar, Renzo Angles, Walid Aref, Marcelo Arenas, Maciej Besta, Peter A.\n  Boncz, Khuzaima Daudjee, Emanuele Della Valle, Stefania Dumbrava, Olaf\n  Hartig, Bernhard Haslhofer, Tim Hegeman, Jan Hidders, Katja Hose, Adriana\n  Iamnitchi, Vasiliki Kalavri, Hugo Kapp, Wim Martens, M. Tamer \\\"Ozsu, Eric\n  Peukert, Stefan Plantikow, Mohamed Ragab, Matei R. Ripeanu, Semih Salihoglu,\n  Christian Schulz, Petra Selmer, Juan F. Sequeda, Joshua Shinavier, G\\'abor\n  Sz\\'arnyas, Riccardo Tommasini, Antonino Tumeo, Alexandru Uta, Ana Lucia\n  Varbanescu, Hsiang-Yun Wu, Nikolay Yakovets, Da Yan, Eiko Yoneki", "title": "The Future is Big Graphs! A Community View on Graph Processing Systems", "comments": "12 pages, 3 figures, collaboration between the large-scale systems\n  and data management communities, work started at the Dagstuhl Seminar 19491\n  on Big Graph Processing Systems, to be published in the Communications of the\n  ACM", "journal-ref": null, "doi": "10.1145/3434642", "report-no": null, "categories": "cs.DC cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Graphs are by nature unifying abstractions that can leverage\ninterconnectedness to represent, explore, predict, and explain real- and\ndigital-world phenomena. Although real users and consumers of graph instances\nand graph workloads understand these abstractions, future problems will require\nnew abstractions and systems. What needs to happen in the next decade for big\ngraph processing to continue to succeed?\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 07:35:07 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Sakr", "Sherif", ""], ["Bonifati", "Angela", ""], ["Voigt", "Hannes", ""], ["Iosup", "Alexandru", ""], ["Ammar", "Khaled", ""], ["Angles", "Renzo", ""], ["Aref", "Walid", ""], ["Arenas", "Marcelo", ""], ["Besta", "Maciej", ""], ["Boncz", "Peter A.", ""], ["Daudjee", "Khuzaima", ""], ["Della Valle", "Emanuele", ""], ["Dumbrava", "Stefania", ""], ["Hartig", "Olaf", ""], ["Haslhofer", "Bernhard", ""], ["Hegeman", "Tim", ""], ["Hidders", "Jan", ""], ["Hose", "Katja", ""], ["Iamnitchi", "Adriana", ""], ["Kalavri", "Vasiliki", ""], ["Kapp", "Hugo", ""], ["Martens", "Wim", ""], ["\u00d6zsu", "M. Tamer", ""], ["Peukert", "Eric", ""], ["Plantikow", "Stefan", ""], ["Ragab", "Mohamed", ""], ["Ripeanu", "Matei R.", ""], ["Salihoglu", "Semih", ""], ["Schulz", "Christian", ""], ["Selmer", "Petra", ""], ["Sequeda", "Juan F.", ""], ["Shinavier", "Joshua", ""], ["Sz\u00e1rnyas", "G\u00e1bor", ""], ["Tommasini", "Riccardo", ""], ["Tumeo", "Antonino", ""], ["Uta", "Alexandru", ""], ["Varbanescu", "Ana Lucia", ""], ["Wu", "Hsiang-Yun", ""], ["Yakovets", "Nikolay", ""], ["Yan", "Da", ""], ["Yoneki", "Eiko", ""]]}, {"id": "2012.06256", "submitter": "Ionut Anghel", "authors": "Tudor Cioara, Claudia Pop (Antal), Razvan Zanc, Ionut Anghel, Marcel\n  Antal, Ioan Salomie", "title": "Smart Grid Management using Blockchain: Future Scenarios and Challenges", "comments": "Accepted and presented at: 19th RoEduNet Conference: Networking in\n  Education and Research, December 11-12, 2020", "journal-ref": "2020 19th RoEduNet Conference: Networking in Education and\n  Research (RoEduNet)", "doi": "10.1109/RoEduNet51892.2020.9324874", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Decentralized management and coordination of energy systems are emerging\ntrends facilitated by the uptake of the Internet of Things and Blockchain\noffering new opportunities for more secure, resilient, and efficient energy\ndistribution. Even though the use of distributed ledger technology in the\nenergy domain is promising, the development of decentralized smart grid\nmanagement solutions is in the early stages. In this paper, we define a layered\narchitecture of a blockchain-based smart grid management platform featuring\nenergy data metering and tamper-proof registration, business enforcement via\nsmart contracts, and Oracle-based integration of high computational services\nsupporting the implementation of future grid management scenarios. Three such\nscenarios are discussed from the perspective of their implementation using the\nproposed blockchain platform and associated challenges: peer to peer energy\ntrading, decentralized management, and aggregation of energy flexibility and\noperation of community oriented Virtual Power Plants.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 11:44:52 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Cioara", "Tudor", "", "Antal"], ["Pop", "Claudia", "", "Antal"], ["Zanc", "Razvan", ""], ["Anghel", "Ionut", ""], ["Antal", "Marcel", ""], ["Salomie", "Ioan", ""]]}, {"id": "2012.06281", "submitter": "Mohammad Dashti", "authors": "Mohammad Dashti, Alexandra Fedorova", "title": "Trash Talk: Accelerating Garbage Collection on Integrated GPUs is\n  Worthless", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systems integrating heterogeneous processors with unified memory provide\nseamless integration among these processors with minimal development\ncomplexity. These systems integrate accelerators such as GPUs on the same die\nwith CPU cores to accommodate running parallel applications with varying levels\nof parallelism. Such integration is becoming very common on modern chip\narchitectures, and it places a burden (or opportunity) on application and\nsystem programmers to utilize the full potential of such integrated chips. In\nthis paper we evaluate whether we can obtain any performance benefits from\nrunning garbage collection on integrated GPU systems, and discuss how difficult\nit would be to realize these gains for the programmer.\n  Proliferation of garbage-collected languages running on a variety of\nplatforms from handheld mobile devices to data centers makes garbage collection\nan interesting target to examine on such platforms and can offer valuable\nlessons for other applications. We present our analysis of running garbage\ncollection on integrated systems and find that the current state of these\nsystems does not provide an advantage for accelerating such a task. We build a\nframework that allows us to offload garbage collection tasks on integrated GPU\nsystems from within the JVM. We identify dominant phases of garbage collection\nand study the viability of offloading them to the integrated GPU. We show that\nperformance advantages are limited, partly because an integrated GPU has\nlimited advantage in memory bandwidth over the CPU, and partly because of\ncostly atomic operations.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 12:30:39 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Dashti", "Mohammad", ""], ["Fedorova", "Alexandra", ""]]}, {"id": "2012.06473", "submitter": "Mich\\`ele Weiland", "authors": "Michele Weiland and Bernhard Homoelle", "title": "Usage Scenarios for Byte-Addressable Persistent Memory\n  inHigh-Performance and Data Intensive Computing", "comments": null, "journal-ref": null, "doi": "10.1007/s11390-020-0776-8", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Byte-addressable persistent memory (B-APM) presents a new opportunity to\nbridge the performance gap between main memory and storage. In this paper, we\npresent the usage scenarios for this new technology, based on the capabilities\nof Intel's DCPMM. We outline some of the basic performance characteristics of\nDCPMM, and explain how it can be configured and used to address the needs of\nmemory and I/O intensive applications in the HPC and data intensive domains.\nTwo decision trees are presented to advise on the configuration options for\nB-APM; their use is illustrated with two examples. We show that the flexibility\nof the technology has the potential to be truly disruptive, not only because of\nthe performance improvements it can deliver, but also because it allows systems\nto cater for wider range of applications on homogeneous hardware.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 10:35:52 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Weiland", "Michele", ""], ["Homoelle", "Bernhard", ""]]}, {"id": "2012.06480", "submitter": "Youry Khmelevsky", "authors": "Chris Mazur, Jesse Ayers, Gaetan Hains, and Youry Khmelevsky", "title": "Machine Learning Prediction of Gamer's Private Networks", "comments": "17 pages, 3 figures, FTC 2020 conference (preprint version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The Gamer's Private Network (GPN) is a client/server technology created by\nWTFast for making the network performance of online games faster and more\nreliable. GPN s use middle-mile servers and proprietary algorithms to better\nconnect online video-game players to their game's servers across a wide-area\nnetwork. Online games are a massive entertainment market and network latency is\na key aspect of a player's competitive edge. This market means many different\napproaches to network architecture are implemented by different competing\ncompanies and that those architectures are constantly evolving. Ensuring the\noptimal connection between a client of WTFast and the online game they wish to\nplay is thus an incredibly difficult problem to automate. Using machine\nlearning, we analyzed historical network data from GPN connections to explore\nthe feasibility of network latency prediction which is a key part of\noptimization. Our next step will be to collect live data (including\nclient/server load, packet and port information and specific game state\ninformation) from GPN Minecraft servers and bots. We will use this information\nin a Reinforcement Learning model along with predictions about latency to alter\nthe clients' and servers' configurations for optimal network performance. These\ninvestigations and experiments will improve the quality of service and\nreliability of GPN systems.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 01:19:08 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Mazur", "Chris", ""], ["Ayers", "Jesse", ""], ["Hains", "Gaetan", ""], ["Khmelevsky", "Youry", ""]]}, {"id": "2012.06539", "submitter": "Stanis{\\l}aw Szufa", "authors": "Dariusz Stolicki, Stanis{\\l}aw Szufa, Nimrod Talmon", "title": "Pabulib: A Participatory Budgeting Library", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MA", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We describe the PArticipatory BUdgeting LIBrary website (in short, Pabulib),\nwhich can be accessed via http://pabulib.org/, and which is a library of\nparticipatory budgeting data. In particular, we describe the file format (.pb)\nthat is used for instances of participatory budgeting.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 08:23:28 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Stolicki", "Dariusz", ""], ["Szufa", "Stanis\u0142aw", ""], ["Talmon", "Nimrod", ""]]}, {"id": "2012.06554", "submitter": "Do Le Quoc", "authors": "Robert Krahn and Donald Dragoti and Franz Gregor and Do Le Quoc and\n  Valerio Schiavoni and Pascal Felber and Clenimar Souza and Andrey Brito and\n  Christof Fetzer", "title": "TEEMon: A continuous performance monitoring framework for TEEs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Trusted Execution Environments (TEEs), such as Intel Software Guard\neXtensions (SGX), are considered as a promising approach to resolve security\nchallenges in clouds. TEEs protect the confidentiality and integrity of\napplication code and data even against privileged attackers with root and\nphysical access by providing an isolated secure memory area, i.e., enclaves.\nThe security guarantees are provided by the CPU, thus even if system software\nis compromised, the attacker can never access the enclave's content. While this\napproach ensures strong security guarantees for applications, it also\nintroduces a considerable runtime overhead in part by the limited availability\nof protected memory (enclave page cache). Currently, only a limited number of\nperformance measurement tools for TEE-based applications exist and none offer\nperformance monitoring and analysis during runtime.\n  This paper presents TEEMon, the first continuous performance monitoring and\nanalysis tool for TEE-based applications. TEEMon provides not only fine-grained\nperformance metrics during runtime, but also assists the analysis of\nidentifying causes of performance bottlenecks, e.g., excessive system calls.\nOur approach smoothly integrates with existing open-source tools (e.g.,\nPrometheus or Grafana) towards a holistic monitoring solution, particularly\noptimized for systems deployed through Docker containers or Kubernetes and\noffers several dedicated metrics and visualizations. Our evaluation shows that\nTEEMon's overhead ranges from 5% to 17%.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 18:33:23 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Krahn", "Robert", ""], ["Dragoti", "Donald", ""], ["Gregor", "Franz", ""], ["Quoc", "Do Le", ""], ["Schiavoni", "Valerio", ""], ["Felber", "Pascal", ""], ["Souza", "Clenimar", ""], ["Brito", "Andrey", ""], ["Fetzer", "Christof", ""]]}, {"id": "2012.06607", "submitter": "Jan Verschelde", "authors": "Jan Verschelde", "title": "Parallel Software to Offset the Cost of Higher Precision", "comments": "The paper corresponds to a talk given by the author at the HILT 2020\n  Workshop on Safe Languages and Technologies for Structured and Efficient\n  Parallel and Distributed/Cloud Computing, 16-17 November 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC cs.NA cs.SC math.AG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hardware double precision is often insufficient to solve large scientific\nproblems accurately. Computing in higher precision defined by software causes\nsignificant computational overhead. The application of parallel algorithms\ncompensates for this overhead. Newton's method to develop power series\nexpansions of algebraic space curves is the use case for this application.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 19:23:55 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Verschelde", "Jan", ""]]}, {"id": "2012.06646", "submitter": "Andrew Kassen", "authors": "Andrew Kassen and Varun Shankar and Aaron L Fogelson", "title": "A fine-grained parallelization of the immersed boundary method", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NA math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present new algorithms for the parallelization of Eulerian-Lagrangian\ninteraction operations in the immersed boundary method. Our algorithms rely on\ntwo well-studied parallel primitives: key-value sort and segmented reduce. The\nuse of these parallel primitives allows us to implement our algorithms on both\ngraphics processing units (GPUs) and on other shared memory architectures. We\npresent strong and weak scaling tests on problems involving scattered points\nand elastic structures. Our tests show that our algorithms exhibit near-ideal\nscaling on both multicore CPUs and GPUs.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 21:35:32 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Kassen", "Andrew", ""], ["Shankar", "Varun", ""], ["Fogelson", "Aaron L", ""]]}, {"id": "2012.06670", "submitter": "Yuya Ong", "authors": "Yuya Jeremy Ong, Yi Zhou, Nathalie Baracaldo, Heiko Ludwig", "title": "Adaptive Histogram-Based Gradient Boosted Trees for Federated Learning", "comments": "11 pages with 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning (FL) is an approach to collaboratively train a model\nacross multiple parties without sharing data between parties or an aggregator.\nIt is used both in the consumer domain to protect personal data as well as in\nenterprise settings, where dealing with data domicile regulation and the\npragmatics of data silos are the main drivers. While gradient boosted tree\nimplementations such as XGBoost have been very successful for many use cases,\nits federated learning adaptations tend to be very slow due to using\ncryptographic and privacy methods and have not experienced widespread use. We\npropose the Party-Adaptive XGBoost (PAX) for federated learning, a novel\nimplementation of gradient boosting which utilizes a party adaptive histogram\naggregation method, without the need for data encryption. It constructs a\nsurrogate representation of the data distribution for finding splits of the\ndecision tree. Our experimental results demonstrate strong model performance,\nespecially on non-IID distributions, and significantly faster training run-time\nacross different data sets than existing federated implementations. This\napproach makes the use of gradient boosted trees practical in enterprise\nfederated learning.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 23:01:35 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Ong", "Yuya Jeremy", ""], ["Zhou", "Yi", ""], ["Baracaldo", "Nathalie", ""], ["Ludwig", "Heiko", ""]]}, {"id": "2012.06706", "submitter": "Yuhao Zhou", "authors": "Yuhao Zhou, Ye Qing, and Jiancheng Lv", "title": "Communication-Efficient Federated Learning with Compensated\n  Overlap-FedAvg", "comments": "15 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Petabytes of data are generated each day by emerging Internet of Things\n(IoT), but only few of them can be finally collected and used for Machine\nLearning (ML) purposes due to the apprehension of data & privacy leakage, which\nseriously retarding ML's growth. To alleviate this problem, Federated learning\nis proposed to perform model training by multiple clients' combined data\nwithout the dataset sharing within the cluster. Nevertheless, federated\nlearning introduces massive communication overhead as the synchronized data in\neach epoch is of the same size as the model, and thereby leading to a low\ncommunication efficiency. Consequently, variant methods mainly focusing on the\ncommunication rounds reduction and data compression are proposed to reduce the\ncommunication overhead of federated learning. In this paper, we propose\nOverlap-FedAvg, a framework that parallels the model training phase with model\nuploading & downloading phase, so that the latter phase can be totally covered\nby the former phase. Compared to vanilla FedAvg, Overlap-FedAvg is further\ndeveloped with a hierarchical computing strategy, a data compensation mechanism\nand a nesterov accelerated gradients~(NAG) algorithm. Besides, Overlap-FedAvg\nis orthogonal to many other compression methods so that they can be applied\ntogether to maximize the utilization of the cluster. Furthermore, the\ntheoretical analysis is provided to prove the convergence of the proposed\nOverlap-FedAvg framework. Extensive experiments on both conventional and\nrecurrent tasks with multiple models and datasets also demonstrate that the\nproposed Overlap-FedAvg framework substantially boosts the federated learning\nprocess.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 02:50:09 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 08:43:15 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Zhou", "Yuhao", ""], ["Qing", "Ye", ""], ["Lv", "Jiancheng", ""]]}, {"id": "2012.06739", "submitter": "Sandeep Chinchali", "authors": "Sandeep Chinchali, Evgenya Pergament, Manabu Nakanoya, Eyal Cidon,\n  Edward Zhang, Dinesh Bharadia, Marco Pavone, and Sachin Katti", "title": "Sampling Training Data for Continual Learning Between Robots and the\n  Cloud", "comments": "International Symposium on Experimental Robotics (ISER) 2020, Malta", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.DC cs.LG cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Today's robotic fleets are increasingly measuring high-volume video and LIDAR\nsensory streams, which can be mined for valuable training data, such as rare\nscenes of road construction sites, to steadily improve robotic perception\nmodels. However, re-training perception models on growing volumes of rich\nsensory data in central compute servers (or the \"cloud\") places an enormous\ntime and cost burden on network transfer, cloud storage, human annotation, and\ncloud computing resources. Hence, we introduce HarvestNet, an intelligent\nsampling algorithm that resides on-board a robot and reduces system bottlenecks\nby only storing rare, useful events to steadily improve perception models\nre-trained in the cloud. HarvestNet significantly improves the accuracy of\nmachine-learning models on our novel dataset of road construction sites, field\ntesting of self-driving cars, and streaming face recognition, while reducing\ncloud storage, dataset annotation time, and cloud compute time by between\n65.7-81.3%. Further, it is between 1.05-2.58x more accurate than baseline\nalgorithms and scalably runs on embedded deep learning hardware. We provide a\nsuite of compute-efficient perception models for the Google Edge Tensor\nProcessing Unit (TPU), an extended technical report, and a novel video dataset\nto the research community at https://sites.google.com/view/harvestnet.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 05:52:33 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Chinchali", "Sandeep", ""], ["Pergament", "Evgenya", ""], ["Nakanoya", "Manabu", ""], ["Cidon", "Eyal", ""], ["Zhang", "Edward", ""], ["Bharadia", "Dinesh", ""], ["Pavone", "Marco", ""], ["Katti", "Sachin", ""]]}, {"id": "2012.06925", "submitter": "Leighton Wilson", "authors": "Leighton Wilson, Nathan Vaughn, and Robert Krasny", "title": "A GPU-Accelerated Fast Summation Method Based on Barycentric Lagrange\n  Interpolation and Dual Tree Traversal", "comments": "31 pages, 36 figures", "journal-ref": null, "doi": "10.1016/j.cpc.2021.108017", "report-no": null, "categories": "physics.comp-ph cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present the barycentric Lagrange dual tree traversal (BLDTT) fast\nsummation method for particle interactions. The scheme replaces well-separated\nparticle-particle interactions by adaptively chosen particle-cluster,\ncluster-particle, and cluster-cluster approximations given by barycentric\nLagrange interpolation at proxy particles on a Chebyshev grid in each cluster.\nThe BLDTT is kernel-independent and the approximations can be efficiently\nmapped onto GPUs, where target particles provide an outer level of parallelism\nand source particles provide an inner level of parallelism. We present an\nOpenACC GPU implementation of the BLDTT with MPI remote memory access for\ndistributed memory parallelization. The performance of the GPU-accelerated\nBLDTT is demonstrated for calculations with different problem sizes, particle\ndistributions, geometric domains, and interaction kernels, as well as for\nunequal target and source particles. Comparison with our earlier\nparticle-cluster barycentric Lagrange treecode (BLTC) demonstrates the superior\nperformance of the BLDTT. In particular, on a single GPU for problem sizes\nranging from $N$=1E5 to 1E8, the BLTC has $O(N\\log N)$ scaling, while the BLDTT\nhas $O(N)$ scaling. In addition, MPI strong scaling results are presented for\nthe BLTC and BLDTT using $N$=64E6 particles on up to 32 GPUs.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 00:09:10 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Wilson", "Leighton", ""], ["Vaughn", "Nathan", ""], ["Krasny", "Robert", ""]]}, {"id": "2012.06959", "submitter": "Chenhao Xie", "authors": "Chenhao Xie, Jieyang Chen, Jesun S Firoz, Jiajia Li, Shuaiwen Leon\n  Song, Kevin Barker, Mark Raugas, Ang Li", "title": "Fast and Scalable Sparse Triangular Solver for Multi-GPU Based HPC\n  Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Designing efficient and scalable sparse linear algebra kernels on modern\nmulti-GPU based HPC systems is a daunting task due to significant irregular\nmemory references and workload imbalance across the GPUs. This is particularly\nthe case for Sparse Triangular Solver (SpTRSV) which introduces additional\ntwo-dimensional computation dependencies among subsequent computation steps.\nDependency information is exchanged and shared among GPUs, thus warrant for\nefficient memory allocation, data partitioning, and workload distribution as\nwell as fine-grained communication and synchronization support. In this work,\nwe demonstrate that directly adopting unified memory can adversely affect the\nperformance of SpTRSV on multi-GPU architectures, despite linking via fast\ninterconnect like NVLinks and NVSwitches. Alternatively, we employ the latest\nNVSHMEM technology based on Partitioned Global Address Space programming model\nto enable efficient fine-grained communication and drastic synchronization\noverhead reduction. Furthermore, to handle workload imbalance, we propose a\nmalleable task-pool execution model which can further enhance the utilization\nof GPUs. By applying these techniques, our experiments on the NVIDIA multi-GPU\nsupernode V100-DGX-1 and DGX-2 systems demonstrate that our design can achieve\non average 3.53x (up to 9.86x) speedup on a DGX-1 system and 3.66x (up to\n9.64x) speedup on a DGX-2 system with 4-GPUs over the Unified-Memory design.\nThe comprehensive sensitivity and scalability studies also show that the\nproposed zero-copy SpTRSV is able to fully utilize the computing and\ncommunication resources of the multi-GPU system.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 04:35:29 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Xie", "Chenhao", ""], ["Chen", "Jieyang", ""], ["Firoz", "Jesun S", ""], ["Li", "Jiajia", ""], ["Song", "Shuaiwen Leon", ""], ["Barker", "Kevin", ""], ["Raugas", "Mark", ""], ["Li", "Ang", ""]]}, {"id": "2012.07112", "submitter": "Moumita Mondal", "authors": "Moumita Mondal, Sruti Gan Chaudhuri, Punyasha Chatterjee", "title": "Uniform Scattering of Robots on Alternate Nodes of a Grid", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a distributed algorithm to uniformly scatter the\nrobots along a grid, with robots on alternate nodes of this grid distribution.\nThese homogeneous, autonomous mobile robots place themselves equidistant apart\non the grid, which can be required for guarding or covering a geographical area\nby the robots. The robots operate by executing cycles of the states\n\"look-compute-move\". In the look phase, it looks to see the position of the\nother robots; in the compute phase, it computes a destination to move to; and\nthen in the move phase, it moves to that computed destination. They do not\ninteract by message passing and can recollect neither the past actions nor the\nlooked data from the previous cycle, i.e., oblivious. The robots are\nsemi-synchronous, anonymous and have unlimited visibility. Eventually, the\nrobots uniformly distribute themselves on alternate nodes of a grid, leaving\nthe adjacent nodes of the grid vacant. The algorithm presented also assures no\ncollision or deadlock among the robots.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 17:36:45 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Mondal", "Moumita", ""], ["Chaudhuri", "Sruti Gan", ""], ["Chatterjee", "Punyasha", ""]]}, {"id": "2012.07113", "submitter": "Moumita Mondal", "authors": "Moumita Mondal, Sruti Gan Chaudhuri, Ayan Dutta, Krishnendu\n  Mukhopadhyaya, Punyasha Chatterjee", "title": "Uniform Circle Formation By Oblivious Swarm Robots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the circle formation problem by multiple autonomous\nand homogeneous disc-shaped robots (also known as fat robots). The goal of the\nrobots is to place themselves on the periphery of a circle. Circle formation\nhas many real-world applications, such as boundary surveillance. This paper\naddresses one variant of such problem { uniform circle formation, where the\nrobots have to be equidistant apart. The robots operate by executing cycles of\nthe states wait-look-compute-move. They are oblivious, indistinguishable,\nanonymous, and do not communicate via message passing. First, we solve the\nuniform circle formation problem while assuming the robots to be transparent.\nNext, we address an even weaker model, where the robots are non-transparent and\nhave limited visibility. We propose novel distributed algorithms to solve these\nvariants. Our presented algorithms in this paper are proved to be correct and\nguarantee to prevent collision and deadlock among the swarm of robots.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 17:38:00 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Mondal", "Moumita", ""], ["Chaudhuri", "Sruti Gan", ""], ["Dutta", "Ayan", ""], ["Mukhopadhyaya", "Krishnendu", ""], ["Chatterjee", "Punyasha", ""]]}, {"id": "2012.07183", "submitter": "Beomyeol Jeon", "authors": "Beomyeol Jeon, S.M. Ferdous, Muntasir Raihan Rahman, Anwar Walid", "title": "Privacy-preserving Decentralized Aggregation for Federated Learning", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning is a promising framework for learning over decentralized\ndata spanning multiple regions. This approach avoids expensive central training\ndata aggregation cost and can improve privacy because distributed sites do not\nhave to reveal privacy-sensitive data. In this paper, we develop a\nprivacy-preserving decentralized aggregation protocol for federated learning.\nWe formulate the distributed aggregation protocol with the Alternating\nDirection Method of Multiplier (ADMM) and examine its privacy weakness. Unlike\nprior work that use Differential Privacy or homomorphic encryption for privacy,\nwe develop a protocol that controls communication among participants in each\nround of aggregation to minimize privacy leakage. We establish its privacy\nguarantee against an honest-but-curious adversary. We also propose an efficient\nalgorithm to construct such a communication pattern, inspired by combinatorial\nblock design theory. Our secure aggregation protocol based on this novel group\ncommunication pattern design leads to an efficient algorithm for federated\ntraining with privacy guarantees. We evaluate our federated training algorithm\non image classification and next-word prediction applications over benchmark\ndatasets with 9 and 15 distributed sites. Evaluation results show that our\nalgorithm performs comparably to the standard centralized federated learning\nmethod while preserving privacy; the degradation in test accuracy is only up to\n0.73%.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 23:45:42 GMT"}, {"version": "v2", "created": "Mon, 28 Dec 2020 23:31:54 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Jeon", "Beomyeol", ""], ["Ferdous", "S. M.", ""], ["Rahman", "Muntasir Raihan", ""], ["Walid", "Anwar", ""]]}, {"id": "2012.07339", "submitter": "Jiangshan Yu Dr", "authors": "Ermyas Abebe, Yining Hu, Allison Irvin, Dileban Karunamoorthy,\n  Vinayaka Pandit, Venkatraman Ramakrishna, Jiangshan Yu", "title": "Verifiable Observation of Permissioned Ledgers", "comments": "Full report of ICBC'21 version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Permissioned ledger technologies have gained significant traction over the\nlast few years. For practical reasons, their applications have focused on\ntransforming narrowly scoped use-cases in isolation. This has led to a\nproliferation of niche, isolated networks that are quickly becoming data and\nvalue silos. To increase value across the broader ecosystem, these networks\nmust seamlessly integrate with existing systems and interoperate with one\nanother. A fundamental requirement for enabling crosschain communication is the\nability to prove the validity of the internal state of a ledger to an external\nparty. However, due to the closed nature of permissioned ledgers, their\ninternal state is opaque to an external observer. This makes consuming and\nverifying states from these networks a non-trivial problem.\n  This paper addresses this fundamental requirement for state sharing across\npermissioned ledgers. In particular, we address two key problems for external\nclients: (i) assurances on the validity of state in a permissioned ledger and\n(ii) the ability to reason about the currency of state. We assume an\nadversarial model where the members of the committee managing the permissioned\nledger can be malicious in the absence of detectability and accountability. We\npresent a formalization of the problem for state sharing and examine its\nsecurity properties under different adversarial conditions. We propose the\ndesign of a protocol that uses a secure public ledger for providing guarantees\non safety and the ability to reason about time, with at least one honest member\nin the committee. We then provide a formal security analysis of our design and\na proof of concept implementation based on Hyperledger Fabric demonstrating the\neffectiveness of the proposed protocol.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 08:43:19 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 08:38:17 GMT"}, {"version": "v3", "created": "Fri, 12 Mar 2021 11:53:05 GMT"}, {"version": "v4", "created": "Mon, 10 May 2021 00:46:06 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Abebe", "Ermyas", ""], ["Hu", "Yining", ""], ["Irvin", "Allison", ""], ["Karunamoorthy", "Dileban", ""], ["Pandit", "Vinayaka", ""], ["Ramakrishna", "Venkatraman", ""], ["Yu", "Jiangshan", ""]]}, {"id": "2012.07450", "submitter": "Xu Chen", "authors": "Qiong Wu and Xu Chen and Zhi Zhou and Junshan Zhang", "title": "FedHome: Cloud-Edge based Personalized Federated Learning for In-Home\n  Health Monitoring", "comments": "Accepted by IEEE Transactions on Mobile Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In-home health monitoring has attracted great attention for the ageing\npopulation worldwide. With the abundant user health data accessed by Internet\nof Things (IoT) devices and recent development in machine learning, smart\nhealthcare has seen many successful stories. However, existing approaches for\nin-home health monitoring do not pay sufficient attention to user data privacy\nand thus are far from being ready for large-scale practical deployment. In this\npaper, we propose FedHome, a novel cloud-edge based federated learning\nframework for in-home health monitoring, which learns a shared global model in\nthe cloud from multiple homes at the network edges and achieves data privacy\nprotection by keeping user data locally. To cope with the imbalanced and\nnon-IID distribution inherent in user's monitoring data, we design a generative\nconvolutional autoencoder (GCAE), which aims to achieve accurate and\npersonalized health monitoring by refining the model with a generated\nclass-balanced dataset from user's personal data. Besides, GCAE is lightweight\nto transfer between the cloud and edges, which is useful to reduce the\ncommunication cost of federated learning in FedHome. Extensive experiments\nbased on realistic human activity recognition data traces corroborate that\nFedHome significantly outperforms existing widely-adopted methods.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 12:04:44 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Wu", "Qiong", ""], ["Chen", "Xu", ""], ["Zhou", "Zhi", ""], ["Zhang", "Junshan", ""]]}, {"id": "2012.07626", "submitter": "Linshan Jiang", "authors": "Linshan Jiang, Rui Tan, Xin Lou, Guosheng Lin", "title": "On Lightweight Privacy-Preserving Collaborative Learning for Internet of\n  Things by Independent Random Projections", "comments": "arXiv admin note: substantial text overlap with arXiv:1902.05197", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.LG cs.NI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The Internet of Things (IoT) will be a main data generation infrastructure\nfor achieving better system intelligence. This paper considers the design and\nimplementation of a practical privacy-preserving collaborative learning scheme,\nin which a curious learning coordinator trains a better machine learning model\nbased on the data samples contributed by a number of IoT objects, while the\nconfidentiality of the raw forms of the training data is protected against the\ncoordinator. Existing distributed machine learning and data encryption\napproaches incur significant computation and communication overhead, rendering\nthem ill-suited for resource-constrained IoT objects. We study an approach that\napplies independent random projection at each IoT object to obfuscate data and\ntrains a deep neural network at the coordinator based on the projected data\nfrom the IoT objects. This approach introduces light computation overhead to\nthe IoT objects and moves most workload to the coordinator that can have\nsufficient computing resources. Although the independent projections performed\nby the IoT objects address the potential collusion between the curious\ncoordinator and some compromised IoT objects, they significantly increase the\ncomplexity of the projected data. In this paper, we leverage the superior\nlearning capability of deep learning in capturing sophisticated patterns to\nmaintain good learning performance. The extensive comparative evaluation shows\nthat this approach outperforms other lightweight approaches that apply additive\nnoisification for differential privacy and/or support vector machines for\nlearning in the applications with light to moderate data pattern complexities.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 12:44:37 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Jiang", "Linshan", ""], ["Tan", "Rui", ""], ["Lou", "Xin", ""], ["Lin", "Guosheng", ""]]}, {"id": "2012.07755", "submitter": "Archit Patke", "authors": "Archit Patke, Saurabh Jha, Haoran Qiu, Jim Brandt, Ann Gentile, Joe\n  Greenseid, Zbigniew Kalbarczyk, Ravishankar Iyer", "title": "Application-aware Congestion Mitigation for High-Performance Computing\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  High-performance computing (HPC) systems frequently experience congestion\nleading to significant application performance variation. However, the impact\nof congestion on application runtime differs from application to application\ndepending on their network characteristics (such as bandwidth and latency\nrequirements). We leverage this insight to develop Netscope, an automated\nML-driven framework that considers those network characteristics to dynamically\nmitigate congestion. We evaluate Netscope on four Cray Aries systems, including\na production supercomputer on real scientific applications. Netscope has a\nlower training cost and accurately estimates the impact of congestion on\napplication runtime with a correlation between 0.7and 0.9 for common scientific\napplications. Moreover, we find that Netscope reduces tail runtime variability\nby up to 14.9 times while improving median system utility by 12%.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 17:51:46 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 02:09:52 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Patke", "Archit", ""], ["Jha", "Saurabh", ""], ["Qiu", "Haoran", ""], ["Brandt", "Jim", ""], ["Gentile", "Ann", ""], ["Greenseid", "Joe", ""], ["Kalbarczyk", "Zbigniew", ""], ["Iyer", "Ravishankar", ""]]}, {"id": "2012.07915", "submitter": "Yili Hong", "authors": "Li Xu, Thomas Lux, Tyler Chang, Bo Li, Yili Hong, Layne Watson, Ali\n  Butt, Danfeng Yao, and Kirk Cameron", "title": "Prediction of High-Performance Computing Input/Output Variability and\n  Its Application to Optimization for System Configurations", "comments": "29 pages, 8 figures", "journal-ref": "Quality Engineering, 2021", "doi": null, "report-no": null, "categories": "cs.DC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance variability is an important measure for a reliable high\nperformance computing (HPC) system. Performance variability is affected by\ncomplicated interactions between numerous factors, such as CPU frequency, the\nnumber of input/output (IO) threads, and the IO scheduler. In this paper, we\nfocus on HPC IO variability. The prediction of HPC variability is a challenging\nproblem in the engineering of HPC systems and there is little statistical work\non this problem to date. Although there are many methods available in the\ncomputer experiment literature, the applicability of existing methods to HPC\nperformance variability needs investigation, especially, when the objective is\nto predict performance variability both in interpolation and extrapolation\nsettings. A data analytic framework is developed to model data collected from\nlarge-scale experiments. Various promising methods are used to build predictive\nmodels for the variability of HPC systems. We evaluate the performance of the\nmethods by measuring prediction accuracy at previously unseen system\nconfigurations. We also discuss a methodology for optimizing system\nconfigurations that uses the estimated variability map. The findings from\nmethod comparisons and developed tool sets in this paper yield new insights\ninto existing statistical methods and can be beneficial for the practice of HPC\nvariability management. This paper has supplementary materials online.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 19:56:52 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Xu", "Li", ""], ["Lux", "Thomas", ""], ["Chang", "Tyler", ""], ["Li", "Bo", ""], ["Hong", "Yili", ""], ["Watson", "Layne", ""], ["Butt", "Ali", ""], ["Yao", "Danfeng", ""], ["Cameron", "Kirk", ""]]}, {"id": "2012.07984", "submitter": "A. Ben Hamza", "authors": "Lorenzo Luciano, Imre Kiss, Peter William Beardshear, Esther Kadosh,\n  and A. Ben Hamza", "title": "WISE: A Computer System Performance Index Scoring Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The performance levels of a computing machine running a given workload\nconfiguration are crucial for both users and providers of computing resources.\nKnowing how well a computing machine is running with a given workload\nconfiguration is critical to making proper computing resource allocation\ndecisions. In this paper, we introduce a novel framework for deriving computing\nmachine and computing resource performance indicators for a given workload\nconfiguration. We propose a workload/machine index score (WISE) framework for\ncomputing a fitness score for a workload/machine combination. The WISE score\nindicates how well a computing machine is running with a specific workload\nconfiguration by addressing the issue of whether resources are being stressed\nor sitting idle wasting precious resources. In addition to encompassing any\nnumber of computing resources, the WISE score is determined by considering how\nfar from target levels the machine resources are operating at without maxing\nout. Experimental results demonstrate the efficacy of the proposed WISE\nframework on two distinct workload configurations.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 22:36:36 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Luciano", "Lorenzo", ""], ["Kiss", "Imre", ""], ["Beardshear", "Peter William", ""], ["Kadosh", "Esther", ""], ["Hamza", "A. Ben", ""]]}, {"id": "2012.07990", "submitter": "Changwan Hong", "authors": "Ajay Brahmakshatriya, Yunming Zhang, Changwan Hong, Shoaib Kamil,\n  Julian Shun, Saman Amarasinghe", "title": "Compilation Techniques for Graph Algorithms on GPUs", "comments": "This paper appears in International Symposium on Code Generation and\n  Optimization (CGO) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of graph programs depends highly on the algorithm, the size\nand structure of the input graphs, as well as the features of the underlying\nhardware. No single set of optimizations or one hardware platform works well\nacross all settings. To achieve high performance, the programmer must carefully\nselect which set of optimizations and hardware platforms to use. The GraphIt\nprogramming language makes it easy for the programmer to write the algorithm\nonce and optimize it for different inputs using a scheduling language. However,\nGraphIt currently has no support for generating high performance code for GPUs.\nProgrammers must resort to re-implementing the entire algorithm from scratch in\na low-level language with an entirely different set of abstractions and\noptimizations in order to achieve high performance on GPUs.\n  We propose GG, an extension to the GraphIt compiler framework, that achieves\nhigh performance on both CPUs and GPUs using the same algorithm specification.\nGG significantly expands the optimization space of GPU graph processing\nframeworks with a novel GPU scheduling language and compiler that enables\ncombining graph optimizations for GPUs. GG also introduces two performance\noptimizations, Edge-based Thread Warps CTAs load balancing (ETWC) and\nEdgeBlocking, to expand the optimization space for GPUs. ETWC improves load\nbalancing by dynamically partitioning the edges of each vertex into blocks that\nare assigned to threads, warps, and CTAs for execution. EdgeBlocking improves\nthe locality of the program by reordering the edges and restricting random\nmemory accesses to fit within the L2 cache. We evaluate GG on 5 algorithms and\n9 input graphs on both Pascal and Volta generation NVIDIA GPUs, and show that\nit achieves up to 5.11x speedup over state-of-the-art GPU graph processing\nframeworks, and is the fastest on 66 out of the 90 experiments.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 22:44:03 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2021 00:30:49 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Brahmakshatriya", "Ajay", ""], ["Zhang", "Yunming", ""], ["Hong", "Changwan", ""], ["Kamil", "Shoaib", ""], ["Shun", "Julian", ""], ["Amarasinghe", "Saman", ""]]}, {"id": "2012.08003", "submitter": "Mahdi Zamani", "authors": "Mihai Christodorescu, Wanyun Catherine Gu, Ranjit Kumaresan, Mohsen\n  Minaei, Mustafa Ozdayi, Benjamin Price, Srinivasan Raghuraman, Muhammad Saad,\n  Cuy Sheffield, Minghua Xu, Mahdi Zamani", "title": "Towards a Two-Tier Hierarchical Infrastructure: An Offline Payment\n  System for Central Bank Digital Currencies", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Digital payments traditionally rely on online communications with several\nintermediaries such as banks, payment networks, and payment processors in order\nto authorize and process payment transactions. While these communication\nnetworks are designed to be highly available with continuous uptime, there may\nbe times when an end-user experiences little or no access to network\nconnectivity.\n  The growing interest in digital forms of payments has led central banks\naround the world to explore the possibility of issuing a new type of\ncentral-bank money, known as central bank digital currency (CBDC). To\nfacilitate the secure issuance and transfer of CBDC, we envision a CBDC design\nunder a two-tier hierarchical trust infrastructure, which is implemented using\npublic-key cryptography with the central bank as the root certificate authority\nfor generating digital signatures, and other financial institutions as\nintermediate certificate authorities. One important design feature for CBDC\nthat can be developed under this hierarchical trust infrastructure is an\noffline capability to create secure point-to-point offline payments through the\nuse of authorized hardware. An offline capability for CBDC as digital cash can\ncreate a resilient payment system for consumers and businesses to transact in\nany situation.\n  We propose an offline payment system (OPS) protocol for CBDC that allows a\nuser to make digital payments to another user while both users are temporarily\noffline and unable to connect to payment intermediaries (or even the Internet).\nOPS can be used to instantly complete a transaction involving any form of\ndigital currency over a point-to-point channel without communicating with any\npayment intermediary, achieving virtually unbounded throughput and real-time\ntransaction latency.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 23:22:56 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Christodorescu", "Mihai", ""], ["Gu", "Wanyun Catherine", ""], ["Kumaresan", "Ranjit", ""], ["Minaei", "Mohsen", ""], ["Ozdayi", "Mustafa", ""], ["Price", "Benjamin", ""], ["Raghuraman", "Srinivasan", ""], ["Saad", "Muhammad", ""], ["Sheffield", "Cuy", ""], ["Xu", "Minghua", ""], ["Zamani", "Mahdi", ""]]}, {"id": "2012.08079", "submitter": "Victor A. Melent'ev", "authors": "A.F. Zadorozhny and V.A. Melent'ev", "title": "On the compatibility of the topologies of parallel tasks and computing\n  systems", "comments": null, "journal-ref": null, "doi": "10.1088/1742-6596/1864/1/012103", "report-no": null, "categories": "cs.DC cs.DM math.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Aspects of compatibility of topologies of parallel computing systems and\ntasks are investigated. The introduction of appropriate indexes based on the\noriginal topological model of parallel computations and on the nontraditional\ndescription of a graph by its projections is proposed and elucidated. On the\nexample of hypercubic computing system (CS) and tasks with ring and star\ninformation topologies, we demonstrate determining the indexes and their use in\na comparative analysis of the applicability of interconnect with a given\ntopology for solving tasks with the same and different types of information\ntopologies.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 04:09:53 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Zadorozhny", "A. F.", ""], ["Melent'ev", "V. A.", ""]]}, {"id": "2012.08313", "submitter": "Sebastian M\\\"uller", "authors": "Sebastian M\\\"uller, Andreas Penzkofer, Darcy Camargo, Olivia Saa", "title": "On Fairness in Voting Consensus Protocols", "comments": "conference paper, SAI Computing21. arXiv admin note: text overlap\n  with arXiv:2006.00928", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Voting algorithms have been widely used as consensus protocols in the\nrealization of fault-tolerant systems. These algorithms are best suited for\ndistributed systems of nodes with low computational power or heterogeneous\nnetworks, where different nodes may have different levels of reputation or\nweight. Our main contribution is the construction of a fair voting protocol in\nthe sense that the influence of the eventual outcome of a given participant is\nlinear in its weight. Specifically, the fairness property guarantees that any\nnode can actively participate in the consensus finding even with low resources\nor weight. We investigate effects that may arise from weighted voting, such as\nloss of anonymity, centralization, scalability, and discuss their relevance to\nprotocol design and implementation.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 14:17:48 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["M\u00fcller", "Sebastian", ""], ["Penzkofer", "Andreas", ""], ["Camargo", "Darcy", ""], ["Saa", "Olivia", ""]]}, {"id": "2012.08336", "submitter": "Bing Luo", "authors": "Bing Luo, Xiang Li, Shiqiang Wang, Jianwei Huang, Leandros Tassiulas", "title": "Cost-Effective Federated Learning Design", "comments": "Accepted in IEEE INFOCOM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) is a distributed learning paradigm that enables a\nlarge number of devices to collaboratively learn a model without sharing their\nraw data. Despite its practical efficiency and effectiveness, the iterative\non-device learning process incurs a considerable cost in terms of learning time\nand energy consumption, which depends crucially on the number of selected\nclients and the number of local iterations in each training round. In this\npaper, we analyze how to design adaptive FL that optimally chooses these\nessential control variables to minimize the total cost while ensuring\nconvergence. Theoretically, we analytically establish the relationship between\nthe total cost and the control variables with the convergence upper bound. To\nefficiently solve the cost minimization problem, we develop a low-cost\nsampling-based algorithm to learn the convergence related unknown parameters.\nWe derive important solution properties that effectively identify the design\nprinciples for different metric preferences. Practically, we evaluate our\ntheoretical results both in a simulated environment and on a hardware\nprototype. Experimental evidence verifies our derived properties and\ndemonstrates that our proposed solution achieves near-optimal performance for\nvarious datasets, different machine learning models, and heterogeneous system\nsettings.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 14:45:11 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Luo", "Bing", ""], ["Li", "Xiang", ""], ["Wang", "Shiqiang", ""], ["Huang", "Jianwei", ""], ["Tassiulas", "Leandros", ""]]}, {"id": "2012.08409", "submitter": "Sebastian Steinau", "authors": "Sebastian Steinau and Kevin Andrews and Manfred Reichert", "title": "Enacting Coordination Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  With the rise of data-centric process management paradigms, interdependent\nprocesses, such as artifacts or object lifecycles, form a business process\nthrough their interactions. Coordination processes may be used to coordinate\nthese interactions, guiding the overall business process towards a meaningful\ngoal. A coordination process model specifies coordination constraints between\nthe interdependent processes in terms of semantic relationships. At run-time,\nthese coordination constraints must be enforced by a coordination process\ninstance. As the coordination of multiple interdependent processes is a complex\nendeavor, several challenges need to be fulfilled to achieve optimal process\ncoordination. For example, processes must be allowed to run asynchronously and\nconcurrently, taking their complex relations into account. This paper\ncontributes the operational semantics of coordination processes, which enforces\nthe coordination constraints at run-time. Coordination processes form complex\nstructures to adequately represent processes and their relations, specifically\nsupporting many-to-many relationships. Based on these complex structures,\nmarkings and process rules allow for the flexible enactment of the\ninterdependent processes while fulfilling all challenges. Coordination\nprocesses represent a sophisticated solution to the complex problem of\ncoordinating interdependent, concurrently running processes.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 18:47:46 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Steinau", "Sebastian", ""], ["Andrews", "Kevin", ""], ["Reichert", "Manfred", ""]]}, {"id": "2012.08545", "submitter": "Eliu Huerta", "authors": "E. A. Huerta, Asad Khan, Xiaobo Huang, Minyang Tian, Maksim Levental,\n  Ryan Chard, Wei Wei, Maeve Heflin, Daniel S. Katz, Volodymyr Kindratenko,\n  Dawei Mu, Ben Blaiszik and Ian Foster", "title": "Accelerated, Scalable and Reproducible AI-driven Gravitational Wave\n  Detection", "comments": "17 pages, 5 figures; v2: 12 pages, 6 figures. Accepted to Nature\n  Astronomy. See also the Behind the Paper blog in Nature Astronomy\n  \"https://astronomycommunity.nature.com/posts/from-disruption-to-sustained-innovation-artificial-intelligence-for-gravitational-wave-astrophysics\"", "journal-ref": "Nat Astron (2021)", "doi": "10.1038/s41550-021-01405-0", "report-no": null, "categories": "gr-qc astro-ph.IM cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of reusable artificial intelligence (AI) models for wider use\nand rigorous validation by the community promises to unlock new opportunities\nin multi-messenger astrophysics. Here we develop a workflow that connects the\nData and Learning Hub for Science, a repository for publishing AI models, with\nthe Hardware Accelerated Learning (HAL) cluster, using funcX as a universal\ndistributed computing service. Using this workflow, an ensemble of four openly\navailable AI models can be run on HAL to process an entire month's worth\n(August 2017) of advanced Laser Interferometer Gravitational-Wave Observatory\ndata in just seven minutes, identifying all four all four binary black hole\nmergers previously identified in this dataset and reporting no\nmisclassifications. This approach combines advances in AI, distributed\ncomputing, and scientific data infrastructure to open new pathways to conduct\nreproducible, accelerated, data-driven discovery.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 19:00:29 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 21:44:56 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Huerta", "E. A.", ""], ["Khan", "Asad", ""], ["Huang", "Xiaobo", ""], ["Tian", "Minyang", ""], ["Levental", "Maksim", ""], ["Chard", "Ryan", ""], ["Wei", "Wei", ""], ["Heflin", "Maeve", ""], ["Katz", "Daniel S.", ""], ["Kindratenko", "Volodymyr", ""], ["Mu", "Dawei", ""], ["Blaiszik", "Ben", ""], ["Foster", "Ian", ""]]}, {"id": "2012.08565", "submitter": "Michael Zhang", "authors": "Michael Zhang, Karan Sapra, Sanja Fidler, Serena Yeung and Jose M.\n  Alvarez", "title": "Personalized Federated Learning with First Order Model Optimization", "comments": "ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While federated learning traditionally aims to train a single global model\nacross decentralized local datasets, one model may not always be ideal for all\nparticipating clients. Here we propose an alternative, where each client only\nfederates with other relevant clients to obtain a stronger model per\nclient-specific objectives. To achieve this personalization, rather than\ncomputing a single model average with constant weights for the entire\nfederation as in traditional FL, we efficiently calculate optimal weighted\nmodel combinations for each client, based on figuring out how much a client can\nbenefit from another's model. We do not assume knowledge of any underlying data\ndistributions or client similarities, and allow each client to optimize for\narbitrary target distributions of interest, enabling greater flexibility for\npersonalization. We evaluate and characterize our method on a variety of\nfederated settings, datasets, and degrees of local data heterogeneity. Our\nmethod outperforms existing alternatives, while also enabling new features for\npersonalized FL such as transfer outside of local data distributions.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 19:30:29 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 06:58:37 GMT"}, {"version": "v3", "created": "Thu, 28 Jan 2021 01:39:36 GMT"}, {"version": "v4", "created": "Fri, 26 Mar 2021 23:13:26 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhang", "Michael", ""], ["Sapra", "Karan", ""], ["Fidler", "Sanja", ""], ["Yeung", "Serena", ""], ["Alvarez", "Jose M.", ""]]}, {"id": "2012.08616", "submitter": "Haider Al-Lawati", "authors": "Haider Al-Lawati and Stark C. Draper", "title": "Anytime Minibatch with Delayed Gradients", "comments": "Accepted for publication in the IEEE Transaction on Signal and\n  Information Processing over Networks", "journal-ref": null, "doi": "10.1109/TSIPN.2020.3044955", "report-no": null, "categories": "cs.DC math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Distributed optimization is widely deployed in practice to solve a broad\nrange of problems. In a typical asynchronous scheme, workers calculate\ngradients with respect to out-of-date optimization parameters while the master\nuses stale (i.e., delayed) gradients to update the parameters. While using\nstale gradients can slow the convergence, asynchronous methods speed up the\noverall optimization with respect to wall clock time by allowing more frequent\nupdates and reducing idling times. In this paper, we present a variable\nper-epoch minibatch scheme called Anytime Minibatch with Delayed Gradients\n(AMB-DG). In AMB-DG, workers compute gradients in epochs of a fixed time while\nthe master uses stale gradients to update the optimization parameters. We\nanalyze AMB-DG in terms of its regret bound and convergence rate. We prove that\nfor convex smooth objective functions, AMB-DG achieves the optimal regret bound\nand convergence rate. We compare the performance of AMB-DG with that of Anytime\nMinibatch (AMB) which is similar to AMB-DG but does not use stale gradients. In\nAMB, workers stay idle after each gradient transmission to the master until\nthey receive the updated parameters from the master while in AMB-DG workers\nnever idle. We also extend AMB-DG to the fully distributed setting. We compare\nAMB-DG with AMB when the communication delay is long and observe that AMB-DG\nconverges faster than AMB in wall clock time. We also compare the performance\nof AMB-DG with the state-of-the-art fixed minibatch approach that uses delayed\ngradients. We run our experiments on a real distributed system and observe that\nAMB-DG converges more than two times.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 21:08:29 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Al-Lawati", "Haider", ""], ["Draper", "Stark C.", ""]]}, {"id": "2012.08626", "submitter": "Roberto Casadei PhD", "authors": "Giorgio Audrito, Roberto Casadei, Ferruccio Damiani, Mirko Viroli", "title": "Computation Against a Neighbour", "comments": "50 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent works in contexts like the Internet of Things (IoT) and large-scale\nCyber-Physical Systems (CPS) propose the idea of programming distributed\nsystems by focussing on their global behaviour across space and time. In this\nview, a potentially vast and heterogeneous set of devices is considered as an\n\"aggregate\" to be programmed as a whole, while abstracting away the details of\nindividual behaviour and exchange of messages, which are expressed\ndeclaratively. One such a paradigm, known as aggregate programming, builds on\ncomputational models inspired by field-based coordination. Existing models such\nas the field calculus capture interaction with neighbours by a so-called\n\"neighbouring field\" (a map from neighbours to values). This requires ad-hoc\nmechanisms to smoothly compose with standard values, thus complicating\nprogramming and introducing clutter in aggregate programs, libraries and\ndomain-specific languages (DSLs). To address this key issue we introduce the\nnovel notion of \"computation against a neighbour\", whereby the evaluation of\ncertain subexpressions of the aggregate program are affected by recent\ncorresponding evaluations in neighbours. We capture this notion in the\nneighbours calculus (NC), a new field calculus variant which is shown to\nsmoothly support declarative specification of interaction with neighbours, and\ncorrespondingly facilitate the embedding of field computations as internal DSLs\nin common general-purpose programming languages -- as exemplified by a Scala\nimplementation, called ScaFi. This paper formalises NC, thoroughly compares it\nwith respect to the classic field calculus, and shows its expressiveness by\nmeans of a case study in edge computing, developed in ScaFi.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 21:33:55 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Audrito", "Giorgio", ""], ["Casadei", "Roberto", ""], ["Damiani", "Ferruccio", ""], ["Viroli", "Mirko", ""]]}, {"id": "2012.08660", "submitter": "Sen Lin", "authors": "Sen Lin, Mehmet Dedeoglu and Junshan Zhang", "title": "Accelerating Distributed Online Meta-Learning via Multi-Agent\n  Collaboration under Limited Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online meta-learning is emerging as an enabling technique for achieving edge\nintelligence in the IoT ecosystem. Nevertheless, to learn a good meta-model for\nwithin-task fast adaptation, a single agent alone has to learn over many tasks,\nand this is the so-called 'cold-start' problem. Observing that in a multi-agent\nnetwork the learning tasks across different agents often share some model\nsimilarity, we ask the following fundamental question: \"Is it possible to\naccelerate the online meta-learning across agents via limited communication and\nif yes how much benefit can be achieved? \" To answer this question, we propose\na multi-agent online meta-learning framework and cast it as an equivalent\ntwo-level nested online convex optimization (OCO) problem. By characterizing\nthe upper bound of the agent-task-averaged regret, we show that the performance\nof multi-agent online meta-learning depends heavily on how much an agent can\nbenefit from the distributed network-level OCO for meta-model updates via\nlimited communication, which however is not well understood. To tackle this\nchallenge, we devise a distributed online gradient descent algorithm with\ngradient tracking where each agent tracks the global gradient using only one\ncommunication step with its neighbors per iteration, and it results in an\naverage regret $O(\\sqrt{T/N})$ per agent, indicating that a factor of\n$\\sqrt{1/N}$ speedup over the optimal single-agent regret $O(\\sqrt{T})$ after\n$T$ iterations, where $N$ is the number of agents. Building on this sharp\nperformance speedup, we next develop a multi-agent online meta-learning\nalgorithm and show that it can achieve the optimal task-average regret at a\nfaster rate of $O(1/\\sqrt{NT})$ via limited communication, compared to\nsingle-agent online meta-learning. Extensive experiments corroborate the\ntheoretic results.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 23:08:36 GMT"}, {"version": "v2", "created": "Sat, 19 Dec 2020 19:26:22 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Lin", "Sen", ""], ["Dedeoglu", "Mehmet", ""], ["Zhang", "Junshan", ""]]}, {"id": "2012.08679", "submitter": "Jin Wang", "authors": "Jin Wang, Jia Hu, and Geyong Min", "title": "Online Service Migration in Edge Computing with Incomplete Information:\n  A Deep Recurrent Actor-Critic Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-access Edge Computing (MEC) is an emerging computing paradigm that\nextends cloud computing to the network edge (e.g., base stations, MEC servers)\nto support resource-intensive applications on mobile devices. As a crucial\nproblem in MEC, service migration needs to decide where to migrate user\nservices for maintaining high Quality-of-Service (QoS), when users roam between\nMEC servers with limited coverage and capacity. However, finding an optimal\nmigration policy is intractable due to the highly dynamic MEC environment and\nuser mobility. Many existing works make centralized migration decisions based\non complete system-level information, which can be time-consuming and suffer\nfrom the scalability issue with the rapidly increasing number of mobile users.\nTo address these challenges, we propose a new learning-driven method, namely\nDeep Recurrent Actor-Critic based service Migration (DRACM), which is\nuser-centric and can make effective online migration decisions given incomplete\nsystem-level information. Specifically, the service migration problem is\nmodeled as a Partially Observable Markov Decision Process (POMDP). To solve the\nPOMDP, we design an encoder network that combines a Long Short-Term Memory\n(LSTM) and an embedding matrix for effective extraction of hidden information.\nWe then propose a tailored off-policy actor-critic algorithm with a clipped\nsurrogate objective for efficient training. Results from extensive experiments\nbased on real-world mobility traces demonstrate that our method consistently\noutperforms both the heuristic and state-of-the-art learning-driven algorithms,\nand achieves near-optimal results on various MEC scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 00:16:24 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 11:11:29 GMT"}, {"version": "v3", "created": "Sun, 11 Apr 2021 10:53:50 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Wang", "Jin", ""], ["Hu", "Jia", ""], ["Min", "Geyong", ""]]}, {"id": "2012.08809", "submitter": "Qinglong Chang", "authors": "Dingwei Li, Qinglong Chang, Lixue Pang, Yanfang Zhang, Xudong Sun,\n  Jikun Ding, Liang Zhang", "title": "More Industry-friendly: Federated Learning with High Efficient Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although many achievements have been made since Google threw out the paradigm\nof federated learning (FL), there still exists much room for researchers to\noptimize its efficiency. In this paper, we propose a high efficient FL method\nequipped with the double head design aiming for personalization optimization\nover non-IID dataset, and the gradual model sharing design for communication\nsaving. Experimental results show that, our method has more stable accuracy\nperformance and better communication efficient across various data\ndistributions than other state of art methods (SOTAs), makes it more\nindustry-friendly.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 09:12:37 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Li", "Dingwei", ""], ["Chang", "Qinglong", ""], ["Pang", "Lixue", ""], ["Zhang", "Yanfang", ""], ["Sun", "Xudong", ""], ["Ding", "Jikun", ""], ["Zhang", "Liang", ""]]}, {"id": "2012.08866", "submitter": "N Zhou", "authors": "Naweiluo Zhou, Yiannis Georgiou, Li Zhong, Huan Zhou, Marcin\n  Pospieszny", "title": "Container Orchestration on HPC Systems", "comments": "Zhou N, Georgiou Y, Zhong L, Zhou H, Pospieszny M. Container\n  Orchestration on HPC Systems. Inproceedings: 2020 IEEE International\n  Conference on Cloud Computing (CLOUD); 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Containerisation demonstrates its efficiency in application deployment in\ncloud computing. Containers can encapsulate complex programs with their\ndependencies in isolated environments, hence are being adopted in HPC clusters.\nHPC workload managers lack micro-services support and deeply integrated\ncontainer management, as opposed to container orchestrators (e.g. Kubernetes).\nWe introduce Torque-Operator (a plugin) which serves as a bridge between HPC\nworkload managers and container Orchestrators.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 11:14:14 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 16:23:54 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Zhou", "Naweiluo", ""], ["Georgiou", "Yiannis", ""], ["Zhong", "Li", ""], ["Zhou", "Huan", ""], ["Pospieszny", "Marcin", ""]]}, {"id": "2012.08979", "submitter": "Tobias Pfandzelter", "authors": "Tobias Pfandzelter, David Bermbach", "title": "Edge (of the Earth) Replication: Optimizing Content Delivery in Large\n  LEO Satellite Communication Networks", "comments": "Accepted for the 21st IEEE/ACM International Symposium on Cluster,\n  Cloud and Internet Computing (CCGrid '21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large low earth orbit (LEO) satellite networks such as SpaceX's Starlink\nconstellation promise to deliver low-latency, high-bandwidth Internet access\nwith global coverage. As an alternative to terrestrial fiber as a global\nInternet backbone, they could potentially serve billions of Internet-connected\ndevices. Currently, operators of CDNs exploit the hierarchical topology of the\nInternet to place points-of-presence near users, yet this approach is no longer\npossible when the topology changes to a single, wide-area, converged access and\nbackhaul network.\n  In this paper, we explore the opportunities of points-of-presence for CDNs\nwithin the satellite network itself, as it could provide better access latency\nfor users while reducing operational costs for the satellite Internet service\nproviders. We propose four strategies for selecting points-of-presence in\nsatellite constellations that we evaluate through extensive simulation. In one\ncase, we find that replicating web content within satellites can reduce\nbandwidth use in the constellation by 93% over an approach without replication\nin the network, while storing just 0.01% of all content in individual\nsatellites.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 14:15:33 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 16:21:32 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Pfandzelter", "Tobias", ""], ["Bermbach", "David", ""]]}, {"id": "2012.09086", "submitter": "Carlos Baquero", "authors": "Carlos Baquero", "title": "Causality is Graphically Simple", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Events in distributed systems include sending or receiving messages, or\nchanging some state in a node. Not all events are related, but some events can\ncause and influence how other, later events, occur. For instance, a reply to a\nreceived mail message is influenced by that message, and maybe by other prior\nmessages also received. This article brings an introduction to classic\ncausality tracking mechanisms and covers some more recent developments. The\npresentation is supported by a new graphical notation that allows an intuitive\ninterpretation of the causality relations described.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 17:09:09 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Baquero", "Carlos", ""]]}, {"id": "2012.09102", "submitter": "Kerem \\\"Ozfatura", "authors": "Emre Ozfatura and Kerem Ozfatura and Deniz Gunduz", "title": "FedADC: Accelerated Federated Learning with Drift Control", "comments": "Accepted to ISIT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) has become de facto framework for collaborative\nlearning among edge devices with privacy concern. The core of the FL strategy\nis the use of stochastic gradient descent (SGD) in a distributed manner. Large\nscale implementation of FL brings new challenges, such as the incorporation of\nacceleration techniques designed for SGD into the distributed setting, and\nmitigation of the drift problem due to non-homogeneous distribution of local\ndatasets. These two problems have been separately studied in the literature;\nwhereas, in this paper, we show that it is possible to address both problems\nusing a single strategy without any major alteration to the FL framework, or\nintroducing additional computation and communication load. To achieve this\ngoal, we propose FedADC, which is an accelerated FL algorithm with drift\ncontrol. We empirically illustrate the advantages of FedADC.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 17:49:37 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 17:55:10 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Ozfatura", "Emre", ""], ["Ozfatura", "Kerem", ""], ["Gunduz", "Deniz", ""]]}, {"id": "2012.09108", "submitter": "Paolo Notaro", "authors": "Paolo Notaro, Jorge Cardoso, and Michael Gerndt", "title": "A Systematic Mapping Study in AIOps", "comments": null, "journal-ref": "International Workshop on Artificial Intelligence for IT\n  Operations (AIOPS) 2020", "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  IT systems of today are becoming larger and more complex, rendering their\nhuman supervision more difficult. Artificial Intelligence for IT Operations\n(AIOps) has been proposed to tackle modern IT administration challenges thanks\nto AI and Big Data. However, past AIOps contributions are scattered,\nunorganized and missing a common terminology convention, which renders their\ndiscovery and comparison impractical. In this work, we conduct an in-depth\nmapping study to collect and organize the numerous scattered contributions to\nAIOps in a unique reference index. We create an AIOps taxonomy to build a\nfoundation for future contributions and allow an efficient comparison of AIOps\npapers treating similar problems. We investigate temporal trends and classify\nAIOps contributions based on the choice of algorithms, data sources and the\ntarget components. Our results show a recent and growing interest towards\nAIOps, specifically to those contributions treating failure-related tasks\n(62%), such as anomaly detection and root cause analysis.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 09:05:20 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Notaro", "Paolo", ""], ["Cardoso", "Jorge", ""], ["Gerndt", "Michael", ""]]}, {"id": "2012.09252", "submitter": "Homayoun Valafar", "authors": "Homayoun Valafar, Okan K. Ersoy, Faramarz Valafar", "title": "Distributed Global Optimization (DGO)", "comments": "7 pages", "journal-ref": "Proceedings of International Conference on Neural Networks\n  (ICNN'96) June 1996", "doi": "10.1109/ICNN.1996.548951", "report-no": null, "categories": "cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new technique of global optimization and its applications in particular to\nneural networks are presented. The algorithm is also compared to other global\noptimization algorithms such as Gradient descent (GD), Monte Carlo (MC),\nGenetic Algorithm (GA) and other commercial packages. This new optimization\ntechnique proved itself worthy of further study after observing its accuracy of\nconvergence, speed of convergence and ease of use. Some of the advantages of\nthis new optimization technique are listed below: 1. Optimizing function does\nnot have to be continuous or differentiable. 2. No random mechanism is used,\ntherefore this algorithm does not inherit the slow speed of random searches. 3.\nThere are no fine-tuning parameters (such as the step rate of G.D. or\ntemperature of S.A.) needed for this technique. 4. This algorithm can be\nimplemented on parallel computers so that there is little increase in\ncomputation time (compared to linear increase) as the number of dimensions\nincreases. The time complexity of O(n) is achieved.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 20:37:55 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Valafar", "Homayoun", ""], ["Ersoy", "Okan K.", ""], ["Valafar", "Faramarz", ""]]}, {"id": "2012.09511", "submitter": "Jan Gmys", "authors": "Jan Gmys", "title": "Solving large permutation flow-shop scheduling problems on\n  GPU-accelerated supercomputers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Makespan minimization in permutation flow-shop scheduling is a well-known\nhard combinatorial optimization problem. Among the 120 standard benchmark\ninstances proposed by E. Taillard in 1993, 23 have remained unsolved for almost\nthree decades. In this paper, we present our attempts to solve these instances\nto optimality using parallel Branch-and-Bound tree search on the\nGPU-accelerated Jean Zay supercomputer. We report the exact solution of 11\npreviously unsolved problem instances and improved upper bounds for 8\ninstances. The solution of these problems requires both algorithmic\nimprovements and leveraging the computing power of peta-scale high-performance\ncomputing platforms. The challenge consists in efficiently performing parallel\ndepth-first traversal of a highly irregular, fine-grained search tree on\ndistributed systems composed of hundreds of massively parallel accelerator\ndevices and multi-core processors. We present and discuss the design and\nimplementation of our permutation-based B&B and experimentally evaluate its\nparallel performance on up to 384 V100 GPUs (2 million CUDA cores) and 3840 CPU\ncores. The optimality proof for the largest solved instance requires about 64\nCPU-years of computation -- using 256 GPUs and over 4 million parallel search\nagents, the traversal of the search tree is completed in 13 hours, exploring\n339$\\times 10^{12}$ nodes.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 11:20:47 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Gmys", "Jan", ""]]}, {"id": "2012.09579", "submitter": "Prashant Singh", "authors": "Prashant Singh, Mona Mohamed Elamin, Salman Toor", "title": "Towards Smart e-Infrastructures, A Community Driven Approach Based on\n  Real Datasets", "comments": "Accepted - 2020 IEEE Green Technologies Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  e-Infrastructures have powered the successful penetration of e-services\nacross domains, and form the backbone of the modern computing landscape.\ne-Infrastructure is a broad term used for large, medium and small scale\ncomputing environments. The increasing sophistication and complexity of\napplications have led to even small-scale data centers consisting of thousands\nof interconnects. However, efficient utilization of resources in data centers\nremains a challenging task, mainly due to the complexity of managing physical\nnodes, network equipment, cooling systems, electricity, etc. This results in a\nvery strong carbon footprint of this industry. In recent years, efforts based\non machine learning approaches have shown promising results towards reducing\nenergy consumption of data centers. Yet, practical solutions that can help data\ncenter operators in offering energy efficient services are lacking. This\nproblem is more visible in the context of medium and small scale data center\noperators (the long tail of e-infrastructure providers). Additionally, a\ndisconnect between solution providers (machine learning experts) and data\ncenter operators has been observed. This article presents a community-driven\nopen source software framework that allows community members to develop better\nunderstanding of various aspects of resource utilization. The framework\nleverages machine learning models for forecasting and optimizing various\nparameters of data center operations, enabling improved efficiency, quality of\nservice and lower energy consumption. Also, the proposed framework does not\nrequire datasets to be shared, which alleviates the extra effort of organizing,\ndescribing and anonymizing data in an appropriate format.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 13:38:11 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Singh", "Prashant", ""], ["Elamin", "Mona Mohamed", ""], ["Toor", "Salman", ""]]}, {"id": "2012.09646", "submitter": "Alberto Parravicini", "authors": "Alberto Parravicini, Arnaud Delamare, Marco Arnaboldi, Marco D.\n  Santambrogio", "title": "DAG-based Scheduling with Resource Sharing for Multi-task Applications\n  in a Polyglot GPU Runtime", "comments": "10 pages, to be published in 2021 IEEE International Parallel and\n  Distributed Processing Symposium (IPDPS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  GPUs are readily available in cloud computing and personal devices, but their\nuse for data processing acceleration has been slowed down by their limited\nintegration with common programming languages such as Python or Java. Moreover,\nusing GPUs to their full capabilities requires expert knowledge of asynchronous\nprogramming. In this work, we present a novel GPU run time scheduler for\nmulti-task GPU computations that transparently provides asynchronous execution,\nspace-sharing, and transfer-computation overlap without requiring in advance\nany information about the program dependency structure. We leverage the GrCUDA\npolyglot API to integrate our scheduler with multiple high-level languages and\nprovide a platform for fast prototyping and easy GPU acceleration. We validate\nour work on 6 benchmarks created to evaluate task-parallelism and show an\naverage of 44% speedup against synchronous execution, with no execution time\nslowdown compared to hand-optimized host code written using the C++ CUDA Graphs\nAPI.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 14:54:07 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 10:11:15 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Parravicini", "Alberto", ""], ["Delamare", "Arnaud", ""], ["Arnaboldi", "Marco", ""], ["Santambrogio", "Marco D.", ""]]}, {"id": "2012.09680", "submitter": "Davide Ferrari", "authors": "Davide Ferrari, Angela Sara Cacciapuoti, Michele Amoretti and Marcello\n  Caleffi", "title": "Compiler Design for Distributed Quantum Computing", "comments": null, "journal-ref": "IEEE Transactions on Quantum Engineering, vol. 2, pp. 1-20, 2021", "doi": "10.1109/TQE.2021.3053921", "report-no": null, "categories": "quant-ph cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In distributed quantum computing architectures, with the network and\ncommunications functionalities provided by the Quantum Internet, remote quantum\nprocessing units (QPUs) can communicate and cooperate for executing\ncomputational tasks that single NISQ devices cannot handle by themselves. To\nthis aim, distributed quantum computing requires a new generation of quantum\ncompilers, for mapping any quantum algorithm to any distributed quantum\ncomputing architecture. With this perspective, in this paper, we first discuss\nthe main challenges arising with compiler design for distributed quantum\ncomputing. Then, we analytically derive an upper bound of the overhead induced\nby quantum compilation for distributed quantum computing. The derived bound\naccounts for the overhead induced by the underlying computing architecture as\nwell as the additional overhead induced by the sub-optimal quantum compiler --\nexpressly designed through the paper to achieve three key features, namely,\ngeneral-purpose, efficient and effective. Finally, we validate the analytical\nresults and we confirm the validity of the compiler design through an extensive\nperformance analysis.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 15:48:32 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Ferrari", "Davide", ""], ["Cacciapuoti", "Angela Sara", ""], ["Amoretti", "Michele", ""], ["Caleffi", "Marcello", ""]]}, {"id": "2012.09861", "submitter": "Homayoun Valafar", "authors": "Homayoun Valafar, Okan K. Ersoy, Farmaraz Valafar", "title": "Parallel Implementation of Distributed Global Optimization (DGO)", "comments": "6 pages published in pdpa 98. arXiv admin note: text overlap with\n  arXiv:2012.09252", "journal-ref": "Parallel and Distributed Processing Techniques and Applications,\n  July 13-16, 1998", "doi": null, "report-no": null, "categories": "cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel implementations of distributed global optimization (DGO) [13] on\nMP-1 and NCUBE parallel computers revealed an approximate O(n) increase in the\nperformance of this algorithm. Therefore, the implementation of the DGO on\nparallel processors can remedy the only draw back of this algorithm which is\nthe O(n2) of execution time as the number of the dimensions increase. The speed\nup factor of the parallel implementations of DGO is measured with respect to\nthe sequential execution time of the identical problem on SPARC IV computer.\nThe best speed up was achieved by the SIMD implementation of the algorithm on\nthe MP-1 with the total speedup of 126 for an optimization problem with n = 9.\nThis optimization problem was distributed across 128 PEs of Mas-Par.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 20:46:33 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Valafar", "Homayoun", ""], ["Ersoy", "Okan K.", ""], ["Valafar", "Farmaraz", ""]]}, {"id": "2012.10146", "submitter": "Conor McMenamin", "authors": "Conor McMenamin and Vanesa Daza and Matteo Pontecorvi", "title": "Achieving State Machine Replication without Honest Players", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DC cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing standards for player characterisation in tokenised state machine\nreplication protocols depend on honest players who will always follow the\nprotocol, regardless of possible token increases for deviating. Given the\never-increasing market capitalisation of these tokenised protocols, honesty is\nbecoming more expensive and more unrealistic. As such, this out-dated player\ncharacterisation must be removed to provide true guarantees of safety and\nliveness in a major stride towards universal trust in state machine replication\nprotocols and a new scale of adoption. As all current state machine replication\nprotocols are built on these legacy standards, it is imperative that a new\nplayer model is identified and utilised to reflect the true nature of players\nin tokenised protocols, now and into the future.\n  To this effect, we propose the ByRa player model for state machine\nreplication protocols. In the ByRa model, players either attempt to maximise\ntheir tokenised rewards, or behave adversarially. This merges the fields of\ngame theory and distributed systems, an intersection in which tokenised state\nmachine replication protocols exist, but on which little formalisation has been\ncarried out. In the ByRa model, we identify the properties of strong incentive\ncompatibility in expectation and fairness that all protocols must satisfy in\norder to achieve state machine replication. We then provide Tenderstake, a\nprotocol which provably satisfies these properties, and by doing so, achieves\nstate machine replication in the ByRa model.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 10:13:35 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 14:35:54 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["McMenamin", "Conor", ""], ["Daza", "Vanesa", ""], ["Pontecorvi", "Matteo", ""]]}, {"id": "2012.10172", "submitter": "Thibault Rieutord", "authors": "Emmanuelle Anceaume, Antonella Pozzo, Thibault Rieutord, Sara\n  Tucci-Piergiovanni", "title": "On Finality in Blockchains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There exist many forms of Blockchain finality conditions, from deterministic\nto probabilistic terminations. To favor availability against consistency in the\nface of partitions, most blockchains only offer probabilistic eventual\nfinality: blocks may be revoked after being appended to the blockchain, yet\nwith decreasing probability as they sink deeper into the chain. Other\nblockchains favor consistency by leveraging the immediate finality of\nConsensus-a block appended is never revoked-at the cost of additional\nsynchronization. In this paper, we focus on necessary and sufficient conditions\nto implement a blockchain with deterministic eventual finality, which ensures\nthat selected main chains at different processes share a common increasing\nprefix. This is a much weaker form of finality that allows us to provide a\nsolution in an asynchronous system subject to unlimited number of byzantine\nfailures. We study stronger forms of eventual finality as well and show that it\nis unfortunately impossible to provide a bounded displacement. By bounded\ndisplacement we mean that the (unknown) number of blocks that can be revoked\nfrom the current blockchain is bounded. This problem reduces to consensus or\neventual consensus depending on whether the bound is known or not. We also show\nthat the classical selection mechanism, such as in Bitcoin, that appends blocks\nat the longest chain is not compliant with a solution to eventual finality.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 11:21:33 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Anceaume", "Emmanuelle", ""], ["Pozzo", "Antonella", ""], ["Rieutord", "Thibault", ""], ["Tucci-Piergiovanni", "Sara", ""]]}, {"id": "2012.10253", "submitter": "Enis Karaarslan Dr.", "authors": "E. Karaarslan, E.Konacakl{\\i}", "title": "Data Storage in the Decentralized World: Blockchain and Derivatives", "comments": "33 pages, 10 figures", "journal-ref": "In Gulsecen S., Sharma S., Akadal E.(Eds.), Who Runs The World:\n  DATA (pp. 37-69). Istanbul, Istanbul University Press (2020)", "doi": "10.26650/B/ET06.2020.011.03", "report-no": null, "categories": "cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We have entered an era where the importance of decentralized solutions has\nbecome more obvious. Blockchain technology and its derivatives are distributed\nledger technologies that keep the registry of data between peers of a network.\nThis ledger is secured within a successive over looping cryptographic chain.\nThe accomplishment of the Bitcoin cryptocurrency proved that blockchain\ntechnology and its derivatives could be used to eliminate intermediaries and\nprovide security for cyberspace. However, there are some challenges in the\nimplementation of blockchain technology. This chapter first explains the\nconcept of blockchain technology and the data that we can store therein. The\nmain advantage of blockchain is the security services that it provides. This\nsection continues by describing these services.. The challenges of blockchain;\nblockchain anomalies, energy consumption, speed, scalability, interoperability,\nprivacy and cryptology in the age of quantum computing are described. Selected\nsolutions for these challenges are given. Remarkable derivatives of blockchain,\nwhich use different solutions (directed acyclic graph, distributed hash table,\ngossip consensus protocol) to solve some of these challenges are described.\nThen the data storage in blockchain and evolving data solutions are explained.\nThe comparison of decentralized solutions with the lcentralized database\nsystems is given. A multi-platform interoperable scalable architecture (MPISA)\nis proposed. In the conclusion we include the evolution assumptions of data\nstorage in a decentralized world.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 14:09:43 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Karaarslan", "E.", ""], ["Konacakl\u0131", "E.", ""]]}, {"id": "2012.10280", "submitter": "Stef Roos", "authors": "Yuup van Engelshoven, Stefanie Roos", "title": "The Merchant: Avoiding Payment Channel Depletion through Incentives", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GT", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Payment channels networks drastically increase the throughput and hence\nscalability of blockchains by performing transactions \\emph{off-chain}. In an\noff-chain payment, parties deposit coins in a channel and then perform\ntransactions without invoking the global consensus mechanism of the blockchain.\nHowever, the transaction value is limited by the capacity of the channel, i.e.,\nthe amount of funds available on a channel. These funds decrease when a\ntransaction is sent and increase when a transaction is received on the channel.\nRecent research indicates that there is an imbalance between sending and\nreceiving transactions, which leads to channel depletion in the sense that one\nof these operations becomes impossible over time due to the lack of available\nfunds.\n  We incentivize the balanced use of payment channels through fees. Whereas the\ncurrent fee model depends solely on the transaction value, our fee policies\nencourage transactions that have a positive effect on the balance in a channel\nand discourage those that have a negative effect. This paper first defines\nnecessary properties of fee strategies. Then, it introduces two novel fees\nstrategies that provably satisfy all necessary properties. Our extensive\nsimulation study reveals that these incentives increase the effectiveness of\npayments by $8\\%$ to $19\\%$.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 14:56:30 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["van Engelshoven", "Yuup", ""], ["Roos", "Stefanie", ""]]}, {"id": "2012.10333", "submitter": "Sai Praneeth Karimireddy", "authors": "Sai Praneeth Karimireddy, Lie He, Martin Jaggi", "title": "Learning from History for Byzantine Robust Optimization", "comments": "ICML 2021. v2 contains stronger theory; v3 fixes some errors in the\n  proof", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Byzantine robustness has received significant attention recently given its\nimportance for distributed and federated learning. In spite of this, we\nidentify severe flaws in existing algorithms even when the data across the\nparticipants is identically distributed. First, we show realistic examples\nwhere current state of the art robust aggregation rules fail to converge even\nin the absence of any Byzantine attackers. Secondly, we prove that even if the\naggregation rules may succeed in limiting the influence of the attackers in a\nsingle round, the attackers can couple their attacks across time eventually\nleading to divergence. To address these issues, we present two surprisingly\nsimple strategies: a new robust iterative clipping procedure, and incorporating\nworker momentum to overcome time-coupled attacks. This is the first provably\nrobust method for the standard stochastic optimization setting. Our code is\nopen sourced at https://github.com/epfml/byzantine-robust-optimizer.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 16:22:32 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 05:33:29 GMT"}, {"version": "v3", "created": "Tue, 29 Jun 2021 08:07:48 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Karimireddy", "Sai Praneeth", ""], ["He", "Lie", ""], ["Jaggi", "Martin", ""]]}, {"id": "2012.10413", "submitter": "Mikhail Nesterenko", "authors": "Kendric Hood, Joseph Oglio, Mikhail Nesterenko, and Gokarna Sharma", "title": "Partitionable Asynchronous Cryptocurrency Blockchain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider operation of blockchain-based cryptocurrency in case of\npartitioning. We define the Partitionable Blockchain Consensus Problem. The\nproblem may have an interesting solution if the partitions proceed\nindependently by splitting accounts. We prove that this problem is not solvable\nin the asynchronous system. The peers in the two partitions may not agree on\nthe last jointly mined block or, alternatively, on the starting point of\nindependent concurrent computation. We introduce a family of detectors that\nenable a solution. We establish the relationship between detectors. We present\nthe algorithm that solves the Partitionable Blockchain Consensus Problem using\nour detectors. We extend our solution to multiple splits, message loss and to\npartition merging. We simulate and evaluate the performance of detectors,\ndiscuss the implementation of the detectors and future work.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 18:07:02 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Hood", "Kendric", ""], ["Oglio", "Joseph", ""], ["Nesterenko", "Mikhail", ""], ["Sharma", "Gokarna", ""]]}, {"id": "2012.10526", "submitter": "Srini Bhagavan", "authors": "Srini Bhagavan, Saravanan Balasubramanian, Prasad Reddy Annem, Thuan\n  Ngo, Arun Soundararaj", "title": "Achieving Operational Scalability Using Razee Continuous Deployment\n  Model and Kubernetes Operators", "comments": "9 pages, 18 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advancements in the cloud computing domain have resulted in huge\nstrides toward simplifying the procurement of hardware and software for diverse\nneeds. By moving enterprise workloads to managed cloud offerings (private,\npublic, hybrid), customers are delegating mundane tasks and labor-intensive\nmaintenance activities related to network connectivity, procurement of cloud\nresource, application deployment, software patches, and upgrades, etc., This\noften translates to benefits such as high availability and reduced cost. The\npopularity of container and micro-services-based deployment has made Kubernetes\nthe de-facto standard to deliver applications. However, even with Kubernetes\norchestration, cloud service providers frequently have operational scalability\nissues due to lack of Continuous Integration and Continuous Deployment (CICD)\nautomation and increased demand for human operators when managing a large\nnumber of software deployments across multiple data centers/availability zones.\nKubernetes solves this in a novel way by creating and managing custom\napplications using Operators. Agile methodology advocates incremental CICD\nwhich are adopted by cloud providers. However, ironically, it is this same\ncontinuous delivery feature of application updates, Kubernetes cluster\nupgrades, etc., that is also a bane to cloud providers. In this paper, we will\ndemonstrate the use of IBM open-source project Razee as a scalable continuous\ndeployment framework to deploy open-source RStudio and Nginx Operators. We will\ndiscuss how IBM Watson SaaS application Operator, Blockchain applications, and\nKubernetes resources updates, etc., can be deployed similarly and the use of\nOperators to perform application life cycle management. We assert that using\nRazee in conjunction with Operators on Kubernetes simplifies application life\ncycle management and increases scalability.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 21:37:37 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Bhagavan", "Srini", ""], ["Balasubramanian", "Saravanan", ""], ["Annem", "Prasad Reddy", ""], ["Ngo", "Thuan", ""], ["Soundararaj", "Arun", ""]]}, {"id": "2012.10557", "submitter": "Romil Bhardwaj", "authors": "Romil Bhardwaj, Zhengxu Xia, Ganesh Ananthanarayanan, Junchen Jiang,\n  Nikolaos Karianakis, Yuanchao Shu, Kevin Hsieh, Victor Bahl, Ion Stoica", "title": "Ekya: Continuous Learning of Video Analytics Models on Edge Compute\n  Servers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Video analytics applications use edge compute servers for the analytics of\nthe videos (for bandwidth and privacy). Compressed models that are deployed on\nthe edge servers for inference suffer from data drift, where the live video\ndata diverges from the training data. Continuous learning handles data drift by\nperiodically retraining the models on new data. Our work addresses the\nchallenge of jointly supporting inference and retraining tasks on edge servers,\nwhich requires navigating the fundamental tradeoff between the retrained\nmodel's accuracy and the inference accuracy. Our solution Ekya balances this\ntradeoff across multiple models and uses a micro-profiler to identify the\nmodels that will benefit the most by retraining. Ekya's accuracy gain compared\nto a baseline scheduler is 29% higher, and the baseline requires 4x more GPU\nresources to achieve the same accuracy as Ekya.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 00:29:22 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Bhardwaj", "Romil", ""], ["Xia", "Zhengxu", ""], ["Ananthanarayanan", "Ganesh", ""], ["Jiang", "Junchen", ""], ["Karianakis", "Nikolaos", ""], ["Shu", "Yuanchao", ""], ["Hsieh", "Kevin", ""], ["Bahl", "Victor", ""], ["Stoica", "Ion", ""]]}, {"id": "2012.10560", "submitter": "Mark Taylor", "authors": "Mark Taylor", "title": "TOPCAT Visualisation over the Web", "comments": "4 pages, 1 figure, to appear in proceedings of ADASS XXX; at\n  submission time, some examples at\n  https://andromeda.star.bristol.ac.uk:8080/plotserv/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC astro-ph.IM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The desktop GUI catalogue analysis tool TOPCAT, and its command-line\ncounterpart STILTS, offer among other capabilities visual exploration of\nlocally stored tables containing millions of rows or more. They offer many\nvariations on the theme of scatter plots, density maps and histograms, which\ncan be navigated interactively. These capabilities have now been extended to a\nclient-server model, so that a plot server can be run close to the data\nstorage, and remote lightweight HTML/JavaScript clients can configure and\ninteract with plots based on that data. The interaction can include\npan/zoom/rotate navigation, identifying individual points, and potentially\nsubset selection. Since only the pixels and not the row data are transmitted to\nthe client, this enables flexible remote visual exploration of large tables at\nrelatively low bandwidth. The web client can request any of the plot options\navailable from TOPCAT/STILTS. Possible applications include web-based\nvisualisations of static datasets too large to transmit, visual previews of\narchive search results, service-configured arrays of plots for complex\ndatasets, and embedding visualisations of local or remote tables into Jupyter\nnotebooks.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 00:47:25 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Taylor", "Mark", ""]]}, {"id": "2012.10690", "submitter": "Bruno Jos\\'e Olivieri de Souza", "authors": "Bruno Olivieri, Marcelo Paulon and Markus Endler", "title": "GrADyS: Exploring movement awareness for efficient routing in\n  Ground-and-Air Dynamic Sensor Networks", "comments": "16 pages presenting project GrADyS with drones/uav decentralized\n  coordination and BLE relay protocol. Tech report series Monografias em\n  Ci\\^encia da Computa\\c{c}\\~ao, November 2020, Dep. Inform\\'atica PUC-Rio,\n  ISSN 0103-9741", "journal-ref": null, "doi": null, "report-no": "MCC02/20", "categories": "cs.NI cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Several situations exist where a geographic region of some size needs to be\nscanned or monitored through many sensors. Still, it is either absolutely\nimpossible or prohibitively expensive to deploy and maintain wireless\ncommunication infrastructure for the distributed sensors. Either because the\nregion is hidden behind walls, not easily accessible, hard to get through, or\ninfected with some lethal bacteria or virus transmitter. In this case, the best\nis to scatter (disposable) sensors in the region and let them transmit the\ncollected sensor data by wireless means to an overflying UAV/drone. Which then\nphysically hauls the collected data from the monitored area to a central base\nstation that functions as a gateway to the Internet. The project GrADyS aims to\nresearch two sets of problems regarding such data collection. The former aims\nto coordinate several autonomous UAVs in a distributed manner to collect the\ngenerated data while relying only on ad-hoc communication. The latter aims to\ndevelop routing protocols to mesh networks Bluetooth Mesh's Low Power Nodes.\nBoth research lines already present preliminary results that are presented in\nthis paper.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 14:01:59 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2021 12:59:36 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Olivieri", "Bruno", ""], ["Paulon", "Marcelo", ""], ["Endler", "Markus", ""]]}, {"id": "2012.10840", "submitter": "Angela Wang", "authors": "Matthew T. Dearing, Xiaoyan Wang", "title": "Analyzing the Performance of Graph Neural Networks with Pipe Parallelism", "comments": "Proceedings of the conference MLSys'21 Workshop on Graph Neural\n  Networks and Systems (GNNSys'21), San Jose, CA, USA, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many interesting datasets ubiquitous in machine learning and deep learning\ncan be described via graphs. As the scale and complexity of graph-structured\ndatasets increase, such as in expansive social networks, protein folding,\nchemical interaction networks, and material phase transitions, improving the\nefficiency of the machine learning techniques applied to these is crucial. In\nthis study, we focus on Graph Neural Networks (GNN) that have found great\nsuccess in tasks such as node or edge classification and link prediction.\nHowever, standard GNN models have scaling limits due to necessary recursive\ncalculations performed through dense graph relationships that lead to memory\nand runtime bottlenecks. While new approaches for processing larger networks\nare needed to advance graph techniques, and several have been proposed, we\nstudy how GNNs could be parallelized using existing tools and frameworks that\nare known to be successful in the deep learning community. In particular, we\ninvestigate applying pipeline parallelism to GNN models with GPipe, introduced\nby Google in 2018.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 04:20:38 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 16:59:33 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Dearing", "Matthew T.", ""], ["Wang", "Xiaoyan", ""]]}, {"id": "2012.10846", "submitter": "Sweta Kumari", "authors": "Hagit Attiya, Sweta Kumari, and Noa Schiller", "title": "Optimal Resilience in Systems that Mix Shared Memory and Message Passing", "comments": "22 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate the minimal number of failures that can partition a system\nwhere processes communicate both through shared memory and by message passing.\nWe prove that this number precisely captures the resilience that can be\nachieved by algorithms that implement a variety of shared objects, like\nregisters and atomic snapshots, and solve common tasks, like randomized\nconsensus, approximate agreement and renaming. This has implications for the\nm&m-model and for the hybrid, cluster-based model.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 04:55:47 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Attiya", "Hagit", ""], ["Kumari", "Sweta", ""], ["Schiller", "Noa", ""]]}, {"id": "2012.10936", "submitter": "Yihao Xue", "authors": "Yihao Xue, Chaoyue Niu, Zhenzhe Zheng, Shaojie Tang, Chengfei Lv, Fan\n  Wu, Guihai Chen", "title": "Toward Understanding the Influence of Individual Clients in Federated\n  Learning", "comments": "Accepted at AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Federated learning allows mobile clients to jointly train a global model\nwithout sending their private data to a central server. Extensive works have\nstudied the performance guarantee of the global model, however, it is still\nunclear how each individual client influences the collaborative training\nprocess. In this work, we defined a new notion, called {\\em Fed-Influence}, to\nquantify this influence over the model parameters, and proposed an effective\nand efficient algorithm to estimate this metric. In particular, our design\nsatisfies several desirable properties: (1) it requires neither retraining nor\nretracing, adding only linear computational overhead to clients and the server;\n(2) it strictly maintains the tenets of federated learning, without revealing\nany client's local private data; and (3) it works well on both convex and\nnon-convex loss functions, and does not require the final model to be optimal.\nEmpirical results on a synthetic dataset and the FEMNIST dataset demonstrate\nthat our estimation method can approximate Fed-Influence with small bias.\nFurther, we show an application of Fed-Influence in model debugging.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 14:34:36 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2021 08:15:12 GMT"}, {"version": "v3", "created": "Tue, 13 Apr 2021 02:19:12 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Xue", "Yihao", ""], ["Niu", "Chaoyue", ""], ["Zheng", "Zhenzhe", ""], ["Tang", "Shaojie", ""], ["Lv", "Chengfei", ""], ["Wu", "Fan", ""], ["Chen", "Guihai", ""]]}, {"id": "2012.11120", "submitter": "Lijun Sun", "authors": "L. J. Sun (1 and 2), Q. Yang (1), X. Chen (3), Z. X. Chen (2) ((1)\n  College of Information Science and Technology, Qingdao University of Science\n  & Technology, Qingdao, China, (2) Shandong Provincial Key Laboratory of\n  Network Based Intelligent Computing, University of Jinan, China, (3) School\n  of Informatics, University of Edinburgh, Edinburgh, UK)", "title": "RC-chain: Reputation-based Crowdsourcing Blockchain for Vehicular\n  Networks", "comments": null, "journal-ref": null, "doi": "10.1016/j.jnca.2020.102956", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  As the commercial use of 5G technologies has grown more prevalent, smart\nvehicles have become an efficient platform for delivering a wide array of\nservices directly to customers. The vehicular crowdsourcing service (VCS), for\nexample, can provide immediate and timely feedback to the user regarding\nreal-time transportation information. However, different sources can generate\nspurious information towards a specific service request in the pursuit of\nprofit. Distinguishing trusted information from numerous sources is the key to\na reliable VCS platform. This paper proposes a solution to this problem called\n\"RC-chain\", a reputation-based crowdsourcing framework built on a blockchain\nplatform (Hyperledger Fabric). We first establish the blockchain-based platform\nto support the management of crowdsourcing trading and user-reputation\nevaluating activities. A reputation model, the Trust Propagation \\& Feedback\nSimilarity (TPFS), then calculates the reputation values of participants and\nreveals any malicious behavior accordingly. Finally, queueing theory is used to\nevaluate the blockchain-based platform and optimize the system performance. The\nproposed framework was deployed on the IBM Hyperledger Fabric platform to\nobserve its real-world running time, effectiveness, and overall performance.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 04:59:00 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Sun", "L. J.", "", "1 and 2"], ["Yang", "Q.", ""], ["Chen", "X.", ""], ["Chen", "Z. X.", ""]]}, {"id": "2012.11188", "submitter": "Tom Tseng", "authors": "Tom Tseng, Laxman Dhulipala, Julian Shun", "title": "Parallel Index-Based Structural Graph Clustering and Its Approximation", "comments": null, "journal-ref": null, "doi": "10.1145/3448016.3457278", "report-no": null, "categories": "cs.DB cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SCAN (Structural Clustering Algorithm for Networks) is a well-studied, widely\nused graph clustering algorithm. For large graphs, however, sequential SCAN\nvariants are prohibitively slow, and parallel SCAN variants do not effectively\nshare work among queries with different SCAN parameter settings. Since users of\nSCAN often explore many parameter settings to find good clusterings, it is\nworthwhile to precompute an index that speeds up queries.\n  This paper presents a practical and provably efficient parallel index-based\nSCAN algorithm based on GS*-Index, a recent sequential algorithm. Our parallel\nalgorithm improves upon the asymptotic work of the sequential algorithm by\nusing integer sorting. It is also highly parallel, achieving logarithmic span\n(parallel time) for both index construction and clustering queries.\nFurthermore, we apply locality-sensitive hashing (LSH) to design a novel\napproximate SCAN algorithm and prove guarantees for its clustering behavior.\n  We present an experimental evaluation of our algorithms on large real-world\ngraphs. On a 48-core machine with two-way hyper-threading, our parallel index\nconstruction achieves 50--151$\\times$ speedup over the construction of\nGS*-Index. In fact, even on a single thread, our index construction algorithm\nis faster than GS*-Index. Our parallel index query implementation achieves\n5--32$\\times$ speedup over GS*-Index queries across a range of SCAN parameter\nvalues, and our implementation is always faster than ppSCAN, a state-of-the-art\nparallel SCAN algorithm. Moreover, our experiments show that applying LSH\nresults in faster index construction while maintaining good clustering quality.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 09:07:44 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 21:51:50 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Tseng", "Tom", ""], ["Dhulipala", "Laxman", ""], ["Shun", "Julian", ""]]}, {"id": "2012.11328", "submitter": "Antonio Ferrara", "authors": "Vito Walter Anelli, Yashar Deldjoo, Tommaso Di Noia, Antonio Ferrara,\n  Fedelucio Narducci", "title": "FedeRank: User Controlled Feedback with Federated Recommender Systems", "comments": "Accepted for publishing at 43rd European Conference on Information\n  Retrieval (ECIR 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recommender systems have shown to be a successful representative of how data\navailability can ease our everyday digital life. However, data privacy is one\nof the most prominent concerns in the digital era. After several data breaches\nand privacy scandals, the users are now worried about sharing their data. In\nthe last decade, Federated Learning has emerged as a new privacy-preserving\ndistributed machine learning paradigm. It works by processing data on the user\ndevice without collecting data in a central repository. We present FedeRank\n(https://split.to/federank), a federated recommendation algorithm. The system\nlearns a personal factorization model onto every device. The training of the\nmodel is a synchronous process between the central server and the federated\nclients. FedeRank takes care of computing recommendations in a distributed\nfashion and allows users to control the portion of data they want to share. By\ncomparing with state-of-the-art algorithms, extensive experiments show the\neffectiveness of FedeRank in terms of recommendation accuracy, even with a\nsmall portion of shared user data. Further analysis of the recommendation\nlists' diversity and novelty guarantees the suitability of the algorithm in\nreal production environments.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 22:26:54 GMT"}, {"version": "v2", "created": "Tue, 29 Dec 2020 09:25:24 GMT"}, {"version": "v3", "created": "Wed, 20 Jan 2021 10:28:21 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Anelli", "Vito Walter", ""], ["Deldjoo", "Yashar", ""], ["Di Noia", "Tommaso", ""], ["Ferrara", "Antonio", ""], ["Narducci", "Fedelucio", ""]]}, {"id": "2012.11355", "submitter": "Javad Hassannataj Joloudari", "authors": "Fatemeh Rahmani, Javad Hassannataj Joloudari, Shahab Shamshirband,\n  Seyedakbar Mostafavi", "title": "Game theory and Evolutionary-optimization methods applied to resource\n  allocation problems in emerging computing environments: A survey", "comments": "We would like to withdraw this preprint version as we already\n  uploaded the wrong version with technical issues", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Today's intelligent computing environments, including Internet of Things,\ncloud computing and fog computing, allow many organizations around the world to\noptimize their resource allocation regarding time and energy consumption. Due\nto the sensitive conditions of utilizing resources by users and the real-time\nnature of the data, a comprehensive and integrated computing environment has\nnot yet been able to provide a robust and reliable capability for proper\nresource allocation. Although, traditional methods of resource allocation in a\nlow-capacity hardware resource system are efficient for small-scale resource\nproviders, for a complex system in the conditions of dynamic computing\nresources and fierce competition in obtaining resources, they do not have the\nability to develop and adaptively manage the conditions optimally. To solve\nthis problem, computing intelligence techniques try to optimize resource\nallocation with minimal time delay and energy consumption. Therefore, the\nobjective of this research is a comprehensive and systematic survey on resource\nallocation problems using computational intelligence methods under Game Theory\nand Evolutionary-optimization in emerging computing environments, including\ncloud, fog and Internet of Things according to the latest scientific-research\nachievements.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 14:07:42 GMT"}, {"version": "v2", "created": "Sat, 20 Feb 2021 19:42:57 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Rahmani", "Fatemeh", ""], ["Joloudari", "Javad Hassannataj", ""], ["Shamshirband", "Shahab", ""], ["Mostafavi", "Seyedakbar", ""]]}, {"id": "2012.11396", "submitter": "Enzo Rucci", "authors": "Marina Mor\\'an and Javier Balladini and Dolores Rexachs and Enzo Rucci", "title": "Towards Management of Energy Consumption in HPC Systems with Fault\n  Tolerance", "comments": "This is the accepted version of the manuscript that was sent to\n  review to 2020 IEEE Biennial Congress of Argentina (ARGENCON) (ISBN\n  978-1-7281-5957-7/20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-performance computing continues to increase its computing power and\nenergy efficiency. However, energy consumption continues to rise and finding\nways to limit and/or decrease it is a crucial point in current research. For\nhigh-performance MPI applications, there are rollback recovery based fault\ntolerance methods, such as uncoordinated checkpoints. These methods allow only\nsome processes to go back in the face of failure, while the rest of the\nprocesses continue to run. In this article, we focus on the processes that\ncontinue execution, and propose a series of strategies to manage energy\nconsumption when a failure occurs and uncoordinated checkpoints are used. We\npresent an energy model to evaluate strategies and through simulation we\nanalyze the behavior of an application under different configurations and\nfailure time. As a result, we show the feasibility of improving energy\nefficiency in HPC systems in the presence of a failure.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 14:55:55 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Mor\u00e1n", "Marina", ""], ["Balladini", "Javier", ""], ["Rexachs", "Dolores", ""], ["Rucci", "Enzo", ""]]}, {"id": "2012.11430", "submitter": "Nela Bosner", "authors": "Nela Bosner", "title": "Parallel Prony's method with multivariate matrix pencil approach and its\n  numerical aspect", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC cs.NA", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Prony's method is a standard tool exploited for solving many imaging and data\nanalysis problems that result in parameter identification in sparse exponential\nsums $$f(k)=\\sum_{j=1}^{T}c_{j}e^{-2\\pi i\\langle t_{j},k\\rangle},\\quad k\\in\n\\mathbb{Z}^{d},$$ where the parameters are pairwise different $\\{\nt_{j}\\}_{j=1}^{M}\\subset [0,1)^{d}$, and $\\{ c_{j}\\}_{j=1}^{M}\\subset\n\\mathbb{C}\\setminus \\{ 0\\}$ are nonzero. The focus of our investigation is on a\nProny's method variant based on a multivariate matrix pencil approach. The\nmethod constructs matrices $S_{1}$, \\ldots , $S_{d}$ from the sampling values,\nand their simultaneous diagonalization yields the parameters $\\{\nt_{j}\\}_{j=1}^{M}$. The parameters $\\{ c_{j}\\}_{j=1}^{M}$ are computed as the\nsolution of an linear least squares problem, where the matrix of the problem is\ndetermined by $\\{ t_{j}\\}_{j=1}^{M}$. Since the method involves independent\ngeneration and manipulation of certain number of matrices, there is intrinsic\ncapacity for parallelization of the whole computation process on several\nlevels. Hence, we propose parallel version of the Prony's method in order to\nincrease its efficiency. The tasks concerning generation of matrices is divided\namong GPU's block of threads and CPU, where heavier load is put on the GPU.\nFrom the algorithmic point of view, the CPU is dedicated to the more complex\ntasks. With careful choice of algorithms solving the subtasks, the load between\nCPU and GPU is balanced. Besides the parallelization techniques, we are also\nconcerned with some numerical issues, and we provide detailed numerical\nanalysis of the method in case of noisy input data. Finally, we performed a set\nof numerical tests which confirm superior efficiency of the parallel algorithm\nand consistency of the forward error with the results of numerical analysis.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 15:35:42 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Bosner", "Nela", ""]]}, {"id": "2012.11702", "submitter": "Mehrnoosh Shafiee", "authors": "Mehrnoosh Shafiee, Javad Ghaderi", "title": "Scheduling Coflows with Dependency Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Applications in data-parallel computing typically consist of multiple stages.\nIn each stage, a set of intermediate parallel data flows (Coflow) is produced\nand transferred between servers to enable starting of next stage. While there\nhas been much research on scheduling isolated coflows, the dependency between\ncoflows in multi-stage jobs has been largely ignored. In this paper, we\nconsider scheduling coflows of multi-stage jobs represented by general DAGs\n(Directed Acyclic Graphs) in a shared data center network, so as to minimize\nthe total weighted completion time of jobs. This problem is significantly more\nchallenging than the traditional coflow scheduling, as scheduling even a single\nmulti-stage job to minimize its completion time is shown to be NP-hard.\n  In this paper, we propose a polynomial-time algorithm with approximation\nratio of $O(\\mu\\log(m)/\\log(\\log(m)))$, where $\\mu$ is the maximum number of\ncoflows in a job and $m$ is the number of servers. For the special case that\nthe jobs' underlying dependency graphs are rooted trees, we modify the\nalgorithm and improve its approximation ratio. To verify the performance of our\nalgorithms, we present simulation results using real traffic traces that show\nup to $53 \\%$ improvement over the prior approach. We conclude the paper by\nproviding a result concerning an optimality gap for scheduling coflows with\ngeneral DAGs.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 21:59:24 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Shafiee", "Mehrnoosh", ""], ["Ghaderi", "Javad", ""]]}, {"id": "2012.11731", "submitter": "Richard Olaniyan", "authors": "Richard Olaniyan and Muthucumaru Maheswaran", "title": "A Fast Edge-Based Synchronizer for Tasks in Real-Time Artificial\n  Intelligence Applications", "comments": "11 pages, 20 figures, journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time artificial intelligence (AI) applications mapped onto edge\ncomputing need to perform data capture, process data, and device actuation\nwithin given bounds while using the available devices. Task synchronization\nacross the devices is an important problem that affects the timely progress of\nan AI application by determining the quality of the captured data, time to\nprocess the data, and the quality of actuation. In this paper, we develop a\nfast edge-based synchronization scheme that can time align the execution of\ninput-output tasks as well compute tasks. The primary idea of the fast\nsynchronizer is to cluster the devices into groups that are highly synchronized\nin their task executions and statically determine few synchronization points\nusing a game-theoretic solver. The cluster of devices use a late notification\nprotocol to select the best point among the pre-computed synchronization points\nto reach a time aligned task execution as quickly as possible. We evaluate the\nperformance of our synchronization scheme using trace-driven simulations and we\ncompare the performance with existing distributed synchronization schemes for\nreal-time AI application tasks. We implement our synchronization scheme and\ncompare its training accuracy and training time with other parameter server\nsynchronization frameworks.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 23:02:21 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Olaniyan", "Richard", ""], ["Maheswaran", "Muthucumaru", ""]]}, {"id": "2012.11890", "submitter": "Geraldo Francisco De Oliveira Junior", "authors": "Nastaran Hajinazar, Geraldo F. Oliveira, Sven Gregorio, Jo\\~ao Dinis\n  Ferreira, Nika Mansouri Ghiasi, Minesh Patel, Mohammed Alser, Saugata Ghose,\n  Juan G\\'omez-Luna, Onur Mutlu", "title": "SIMDRAM: A Framework for Bit-Serial SIMD Processing Using DRAM", "comments": "Extended abstract of the full paper to appear in ASPLOS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.ET", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Processing-using-DRAM has been proposed for a limited set of basic operations\n(i.e., logic operations, addition). However, in order to enable the full\nadoption of processing-using-DRAM, it is necessary to provide support for more\ncomplex operations. In this paper, we propose SIMDRAM, a flexible\ngeneral-purpose processing-using-DRAM framework that enables massively-parallel\ncomputation of a wide range of operations by using each DRAM column as an\nindependent SIMD lane to perform bit-serial operations. SIMDRAM consists of\nthree key steps to enable a desired operation in DRAM: (1) building an\nefficient majority-based representation of the desired operation, (2) mapping\nthe operation input and output operands to DRAM rows and to the required DRAM\ncommands that produce the desired operation, and (3) executing the operation.\nThese three steps ensure efficient computation of any arbitrary and complex\noperation in DRAM. The first two steps give users the flexibility to\nefficiently implement and compute any desired operation in DRAM. The third step\ncontrols the execution flow of the in-DRAM computation, transparently from the\nuser. We comprehensively evaluate SIMDRAM's reliability, area overhead,\noperation throughput, and energy efficiency using a wide range of operations\nand seven diverse real-world kernels to demonstrate its generality. Our results\nshow that SIMDRAM provides up to 5.1x higher operation throughput and 2.5x\nhigher energy efficiency than a state-of-the-art in-DRAM computing mechanism,\nand up to 2.5x speedup for real-world kernels while incurring less than 1% DRAM\nchip area overhead. Compared to a CPU and a high-end GPU, SIMDRAM is 257x and\n31x more energy-efficient, while providing 93x and 6x higher operation\nthroughput, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 09:14:28 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Hajinazar", "Nastaran", ""], ["Oliveira", "Geraldo F.", ""], ["Gregorio", "Sven", ""], ["Ferreira", "Jo\u00e3o Dinis", ""], ["Ghiasi", "Nika Mansouri", ""], ["Patel", "Minesh", ""], ["Alser", "Mohammed", ""], ["Ghose", "Saugata", ""], ["G\u00f3mez-Luna", "Juan", ""], ["Mutlu", "Onur", ""]]}, {"id": "2012.12178", "submitter": "Jisung Park", "authors": "Jisung Park, Myungsuk Kim, Myoungjun Chun, Lois Orosa, Jihong Kim, and\n  Onur Mutlu", "title": "Reducing Solid-State Drive Read Latency by Optimizing Read-Retry\n  (Extended Abstract)", "comments": "Extended abstract of the full paper to appear in ASPLOS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  3D NAND flash memory with advanced multi-level cell techniques provides high\nstorage density, but suffers from significant performance degradation due to a\nlarge number of read-retry operations. Although the read-retry mechanism is\nessential to ensuring the reliability of modern NAND flash memory, it can\nsignificantly increase the read latency of an SSD by introducing multiple retry\nsteps that read the target page again with adjusted read-reference voltage\nvalues. Through a detailed analysis of the read mechanism and rigorous\ncharacterization of 160 real 3D NAND flash memory chips, we find new\nopportunities to reduce the read-retry latency by exploiting two advanced\nfeatures widely adopted in modern NAND flash-based SSDs: 1) the CACHE READ\ncommand and 2) strong ECC engine. First, we can reduce the read-retry latency\nusing the advanced CACHE READ command that allows a NAND flash chip to perform\nconsecutive reads in a pipelined manner. Second, there exists a large\nECC-capability margin in the final retry step that can be used for reducing the\nchip-level read latency. Based on our new findings, we develop two new\ntechniques that effectively reduce the read-retry latency: 1) Pipelined\nRead-Retry (PR$^2$) and 2) Adaptive Read-Retry (AR$^2$). PR$^2$ reduces the\nlatency of a read-retry operation by pipelining consecutive retry steps using\nthe CACHE READ command. AR$^2$ shortens the latency of each retry step by\ndynamically reducing the chip-level read latency depending on the current\noperating conditions that determine the ECC-capability margin. Our evaluation\nusing twelve real-world workloads shows that our proposal improves SSD response\ntime by up to 31.5% (17% on average) over a state-of-the-art baseline with only\nsmall changes to the SSD controller.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 17:16:17 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 10:49:28 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Park", "Jisung", ""], ["Kim", "Myungsuk", ""], ["Chun", "Myoungjun", ""], ["Orosa", "Lois", ""], ["Kim", "Jihong", ""], ["Mutlu", "Onur", ""]]}, {"id": "2012.12420", "submitter": "Xinwei Zhang", "authors": "Xinwei Zhang, Wotao Yin, Mingyi Hong, Tianyi Chen", "title": "Hybrid Federated Learning: Algorithms and Implementation", "comments": "Appeared in NeurIPS-SpicyFL 2020, the Best Student Paper Award", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) is a recently proposed distributed machine learning\nparadigm dealing with distributed and private data sets. Based on the data\npartition pattern, FL is often categorized into horizontal, vertical, and\nhybrid settings. Despite the fact that many works have been developed for the\nfirst two approaches, the hybrid FL setting (which deals with partially\noverlapped feature space and sample space) remains less explored, though this\nsetting is extremely important in practice. In this paper, we first set up a\nnew model-matching-based problem formulation for hybrid FL, then propose an\nefficient algorithm that can collaboratively train the global and local models\nto deal with full and partial featured data. We conduct numerical experiments\non the multi-view ModelNet40 data set to validate the performance of the\nproposed algorithm. To the best of our knowledge, this is the first formulation\nand algorithm developed for the hybrid FL.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 23:56:03 GMT"}, {"version": "v2", "created": "Tue, 29 Dec 2020 03:40:33 GMT"}, {"version": "v3", "created": "Thu, 18 Feb 2021 03:53:03 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Zhang", "Xinwei", ""], ["Yin", "Wotao", ""], ["Hong", "Mingyi", ""], ["Chen", "Tianyi", ""]]}, {"id": "2012.12501", "submitter": "Deniz Alt{\\i}nb\\\"uken", "authors": "Hussam Abu-Libdeh, Deniz Alt{\\i}nb\\\"uken, Alex Beutel, Ed H. Chi,\n  Lyric Doshi, Tim Kraska, Xiaozhou (Steve) Li, Andy Ly, Christopher Olston", "title": "Learned Indexes for a Google-scale Disk-based Database", "comments": "4 pages, Presented at Workshop on ML for Systems at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  There is great excitement about learned index structures, but understandable\nskepticism about the practicality of a new method uprooting decades of research\non B-Trees. In this paper, we work to remove some of that uncertainty by\ndemonstrating how a learned index can be integrated in a distributed,\ndisk-based database system: Google's Bigtable. We detail several design\ndecisions we made to integrate learned indexes in Bigtable. Our results show\nthat integrating learned index significantly improves the end-to-end read\nlatency and throughput for Bigtable.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 05:56:45 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Abu-Libdeh", "Hussam", "", "Steve"], ["Alt\u0131nb\u00fcken", "Deniz", "", "Steve"], ["Beutel", "Alex", "", "Steve"], ["Chi", "Ed H.", "", "Steve"], ["Doshi", "Lyric", "", "Steve"], ["Kraska", "Tim", "", "Steve"], ["Xiaozhou", "", "", "Steve"], ["Li", "", ""], ["Ly", "Andy", ""], ["Olston", "Christopher", ""]]}, {"id": "2012.12544", "submitter": "Letian Zhao", "authors": "Letian Zhao, Rui Xu, Tianqi Wang, Teng Tian, Xiaotian Wang, Wei Wu,\n  Chio-in Ieong, Xi Jin", "title": "BaPipe: Exploration of Balanced Pipeline Parallelism for DNN Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The size of deep neural networks (DNNs) grows rapidly as the complexity of\nthe machine learning algorithm increases. To satisfy the requirement of\ncomputation and memory of DNN training, distributed deep learning based on\nmodel parallelism has been widely recognized. We propose a new pipeline\nparallelism training framework, BaPipe, which can automatically explore\npipeline parallelism training methods and balanced partition strategies for DNN\ndistributed training. In BaPipe, each accelerator calculates the forward\npropagation and backward propagation of different parts of networks to\nimplement the intra-batch pipeline parallelism strategy. BaPipe uses a new load\nbalancing automatic exploration strategy that considers the parameters of DNN\nmodels and the computation, memory, and communication resources of accelerator\nclusters. We have trained different DNNs such as VGG-16, ResNet-50, and GNMT on\nGPU clusters and simulated the performance of different FPGA clusters. Compared\nwith state-of-the-art data parallelism and pipeline parallelism frameworks,\nBaPipe provides up to 3.2x speedup and 4x memory reduction in various\nplatforms.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 08:57:39 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 05:58:54 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Zhao", "Letian", ""], ["Xu", "Rui", ""], ["Wang", "Tianqi", ""], ["Tian", "Teng", ""], ["Wang", "Xiaotian", ""], ["Wu", "Wei", ""], ["Ieong", "Chio-in", ""], ["Jin", "Xi", ""]]}, {"id": "2012.12578", "submitter": "Ali Dauda Baba", "authors": "Ali Baba Dauda, Mohammed Sani Adam, Muhammad Ahmad Mustapha, Audu Musa\n  Mabu, Suleiman Mustafa", "title": "Soap serialization effect on communication nodes and protocols", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although serialization improves the transmission of data through utilization\nof bandwidth, but its impact at the communication systems is not fully\naccounted. This research used Simple Object Access Protocol (SOAP) Web services\nto exchange serialized and normal messages via Hypertext Transfer Protocol\n(HTTP) and Java Messaging System (JMS). We implemented two web services as\nserver and client endpoints and transmitted SOAP messages as payload. We\nanalyzed the effect of unserialized and serialized messages on the computing\nresources based on the response time and overhead at both server and client\nendpoints. The analysis identified the reasons for high response time and\ncauses for overhead. We provided some insights on the resources utilization and\ntrade-offs when choosing messaging format or transmission protocol. This study\nis vital in resource management in edge computing and data centers.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 10:09:56 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Dauda", "Ali Baba", ""], ["Adam", "Mohammed Sani", ""], ["Mustapha", "Muhammad Ahmad", ""], ["Mabu", "Audu Musa", ""], ["Mustafa", "Suleiman", ""]]}, {"id": "2012.12745", "submitter": "Faten Alenizi", "authors": "Faten Alenizi, Omer Rana", "title": "Minimising Delay and Energy in Online Dynamic Fog Systems", "comments": null, "journal-ref": "10th International Conference on Advances in Computing and\n  Information Technology (ACITY 2020)", "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The increasing use of Internet of Things (IoT) devices generates a greater\ndemand for data transfers and puts increased pressure on networks.\nAdditionally, connectivity to cloud services can be costly and inefficient. Fog\ncomputing provides resources in proximity to user devices to overcome these\ndrawbacks. However, optimisation of quality of service (QoS) in IoT\napplications and the management of fog resources are becoming challenging\nproblems. This paper describes a dynamic online offloading scheme in vehicular\ntraffic applications that require execution of delay-sensitive tasks. This\npaper proposes a combination of two algorithms: dynamic task scheduling (DTS)\nand dynamic energy control (DEC) that aim to minimise overall delay, enhance\nthroughput of user tasks and minimise energy consumption at the fog layer while\nmaximising the use of resource-constrained fog nodes. Compared to other\nschemes, our experimental results show that these algorithms can reduce the\ndelay by up to 80.79% and reduce energy consumption by up to 66.39% in fog\nnodes. Additionally, this approach enhances task execution throughput by\n40.88%.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 15:25:12 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Alenizi", "Faten", ""], ["Rana", "Omer", ""]]}, {"id": "2012.12868", "submitter": "Matan Rusanovsky", "authors": "Matan Rusanovsky, Ohad Ben-Baruch, Danny Hendler and Pedro Ramalhete", "title": "A Flat-Combining-Based Persistent Stack for Non-Volatile Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.OS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Flat combining (FC) is a synchronization paradigm in which a single thread,\nholding a global lock, collects requests by multiple threads for accessing a\nconcurrent data structure and applies their combined requests to it. Although\nFC is sequential, it significantly reduces synchronization overheads and cache\ninvalidations and thus often provides better performance than that of lock-free\nimplementations. The recent emergence of non-volatile memory (NVM) technologies\nincreases the interest in the development of persistent (a.k.a. durable or\nrecoverable) objects. These are objects that are able to recover from system\nfailures and ensure consistency by retaining their state in NVM and fixing it,\nif required, upon recovery. Of particular interest are detectable objects that,\nin addition to ensuring consistency, allow recovery code to infer if a failed\noperation took effect before the crash and, if it did, obtain its response. In\nthis work, we present the first FC-based persistent object. Specifically, we\nintroduce a detectable FC-based implementation of a concurrent LIFO stack\nobject. Our empirical evaluation establishes that thanks to the usage of flat\ncombining, the novel stack algorithm requires a much smaller number of costly\npersistence instructions than competing algorithms and is therefore able to\nsignificantly outperform them.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 18:34:45 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Rusanovsky", "Matan", ""], ["Ben-Baruch", "Ohad", ""], ["Hendler", "Danny", ""], ["Ramalhete", "Pedro", ""]]}, {"id": "2012.13063", "submitter": "Chengxi Li", "authors": "Chengxi Li, Gang Li, Pramod K. Varshney", "title": "Decentralized Federated Learning via Mutual Knowledge Transfer", "comments": "Published in IEEE Internet of Things Journal", "journal-ref": null, "doi": "10.1109/JIOT.2021.3078543", "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the problem of decentralized federated learning\n(DFL) in Internet of things (IoT) systems, where a number of IoT clients train\nmodels collectively for a common task without sharing their private training\ndata in the absence of a central server. Most of the existing DFL schemes are\ncomposed of two alternating steps, i.e., model updating and model averaging.\nHowever, averaging model parameters directly to fuse different models at the\nlocal clients suffers from client-drift especially when the training data are\nheterogeneous across different clients. This leads to slow convergence and\ndegraded learning performance. As a possible solution, we propose the\ndecentralized federated earning via mutual knowledge transfer (Def-KT)\nalgorithm where local clients fuse models by transferring their learnt\nknowledge to each other. Our experiments on the MNIST, Fashion-MNIST, CIFAR-10,\nand CIFAR-100 datasets reveal that the proposed Def-KT algorithm significantly\noutperforms the baseline DFL methods with model averaging, i.e., Combo and\nFullAvg, especially when the training data are not independent and identically\ndistributed (non-IID) across different clients.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 01:43:53 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 06:42:58 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Li", "Chengxi", ""], ["Li", "Gang", ""], ["Varshney", "Pramod K.", ""]]}, {"id": "2012.13548", "submitter": "Shubhendra Singhal Mr.", "authors": "Shubhendra Pal Singhal", "title": "Graph500 from OCaml-Multicore Perspective", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  OCaml is an industrial-strength, multi-paradigm programming language, widely\nused in industry and academia. OCaml was developed for solving numerical and\nscientific problems involving large scale data-intensive operations and one\nsuch classic application set is Graph Algorithms, which are a core part of most\nanalytics workloads. In this paper, we aim to implement the graph benchmarks\nalong with the performance analysis. Graph500 is one such serious benchmark\nwhich aims at developing data intensive applications requiring extreme\ncomputational power. We try to implement Graph Construction, BFS, Shortest-Path\nproblems using the desired specifications and rules posed by graph500. This\npaper aims at providing a clear direction of choices of several data structures\nused, algorithms developed and pose a reason behind every step of program. The\nfirst few sections of the paper discusses a formal approach to the problem with\na small guide for starters in OCaml. The latter sections describe the\nalgorithms in detail with the possibilities of future exploration and several\nmistakes which we committed or encountered whilst approaching the solution. All\nperformance metrics were tested on Intel(R) Xeon(R) Gold 5120 CPU @ 2.20GHz 24\ncore machine. Every section talks about the initial performance failures\nencountered, which will help analyse and prioritise our preferred\nimplementation from a performance perspective.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2020 09:11:18 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Singhal", "Shubhendra Pal", ""]]}, {"id": "2012.13608", "submitter": "Gauri Joshi", "authors": "Gauri Joshi and Dhruva Kaushal", "title": "Synergy via Redundancy: Adaptive Replication Strategies and Fundamental\n  Limits", "comments": "Accepted for publication in the IEEE/ACM Transactions on Networking", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT math.IT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The maximum possible throughput (or the rate of job completion) of a\nmulti-server system is typically the sum of the service rates of individual\nservers. Recent work shows that launching multiple replicas of a job and\ncanceling them as soon as one copy finishes can boost the throughput,\nespecially when the service time distribution has high variability. This means\nthat redundancy can, in fact, create synergy among servers such that their\noverall throughput is greater than the sum of individual servers. This work\nseeks to find the fundamental limit of the throughput boost achieved by job\nreplication and the optimal replication policy to achieve it. While most\nprevious works consider upfront replication policies, we expand the set of\npossible policies to delayed launch of replicas. The search for the optimal\nadaptive replication policy can be formulated as a Markov Decision Process,\nusing which we propose two myopic replication policies, MaxRate and AdaRep, to\nadaptively replicate jobs. In order to quantify the optimality gap of these and\nother policies, we derive upper bounds on the service capacity, which provide\nfundamental limits on the throughput of queueing systems with redundancy.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2020 17:39:20 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Joshi", "Gauri", ""], ["Kaushal", "Dhruva", ""]]}, {"id": "2012.13618", "submitter": "Sepideh Maleki", "authors": "Sepideh Maleki, Udit Agarwal, Martin Burtscher, Keshav Pingali", "title": "BiPart: A Parallel and Deterministic Multilevel Hypergraph Partitioner", "comments": "Accepted for publication at PPoPP21", "journal-ref": null, "doi": "10.1145/3437801.3441611", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Hypergraph partitioning is used in many problem domains including VLSI\ndesign, linear algebra, Boolean satisfiability, and data mining. Most versions\nof this problem are NP-complete or NP-hard, so practical hypergraph\npartitioners generate approximate partitioning solutions for all but the\nsmallest inputs. One way to speed up hypergraph partitioners is to exploit\nparallelism. However, existing parallel hypergraph partitioners are not\ndeterministic, which is considered unacceptable in domains like VLSI design\nwhere the same partitions must be produced every time a given hypergraph is\npartitioned.\n  In this paper, we describe BiPart, the first deterministic, parallel\nhypergraph partitioner. Experimental results show that BiPart outperforms\nstate-of-the-art hypergraph partitioners in runtime and partition quality while\ngenerating partitions deterministically.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2020 19:50:11 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Maleki", "Sepideh", ""], ["Agarwal", "Udit", ""], ["Burtscher", "Martin", ""], ["Pingali", "Keshav", ""]]}, {"id": "2012.13757", "submitter": "Yuan Wang", "authors": "Yuan Wang, Hideaki Ishii, Fran\\c{c}ois Bonnet, Xavier D\\'efago", "title": "Resilient Consensus Against Epidemic Malicious Attacks", "comments": "14 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.DC cs.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper addresses novel consensus problems for multi-agent systems\noperating in a pandemic environment where infectious diseases are spreading.The\ndynamics of the diseases follows the susceptible-infected-recovered (SIR)\nmodel, where the infection induces faulty behaviors in the agents and affects\ntheir state values. To ensure resilient consensus among the noninfectious\nagents, the difficulty is that the number of infectious agents changes over\ntime. We assume that a high-level policy maker announces the level of infection\nin real-time, which can be adopted by the agents for their preventative\nmeasures. It is demonstrated that this problem can be formulated as resilient\nconsensus in the presence of the socalled mobile malicious models, where the\nmean subsequence reduced (MSR) algorithms are known to be effective. We\ncharacterize sufficient conditions on the network structures for different\npolicies regarding the announced infection levels and the strength of the\npandemic.Numerical simulations are carried out for random graphs to verify the\neffectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 15:04:11 GMT"}, {"version": "v2", "created": "Sun, 25 Apr 2021 20:53:45 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Wang", "Yuan", ""], ["Ishii", "Hideaki", ""], ["Bonnet", "Fran\u00e7ois", ""], ["D\u00e9fago", "Xavier", ""]]}, {"id": "2012.13806", "submitter": "Danilo Pianini", "authors": "Danilo Pianini, Roberto Casadei, Mirko Viroli, Stefano Mariani, Franco\n  Zambonelli", "title": "Time-Fluid Field-Based Coordination through Programmable Distributed\n  Schedulers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emerging application scenarios, such as cyber-physical systems (CPSs), the\nInternet of Things (IoT), and edge computing, call for coordination approaches\naddressing openness, self-adaptation, heterogeneity, and deployment\nagnosticism. Field-based coordination is one such approach, promoting the idea\nof programming system coordination declaratively from a global perspective, in\nterms of functional manipulation and evolution in \"space and time\" of\ndistributed data structures called fields. More specifically regarding time, in\nfield-based coordination (as in many other distributed approaches to\ncoordination) it is assumed that local activities in each device are regulated\nby a fair and unsynchronised fixed clock working at the platform level. In this\nwork, we challenge this assumption, and propose an alternative approach where\nscheduling is programmed in a natural way (along with usual field-based\ncoordination) in terms of causality fields, each enacting a programmable\ndistributed notion of a computation \"cause\" (why and when a field computation\nhas to be locally computed) and how it should change across time and space.\nStarting from low-level platform triggers, such causality fields can be\norganised into multiple layers, up to high-level, collectively-computed time\nabstractions, to be used at the application level. This reinterpretation of\ntime in terms of articulated causality relations allows us to express what we\ncall \"time-fluid\" coordination, where scheduling can be finely tuned so as to\nselect the triggers to react to, generally allowing to adaptively balance\nperformance (system reactivity) and cost (resource usage) of computations. We\nformalise the proposed scheduling framework for field-based coordination in the\ncontext of the field calculus, discuss an implementation in the aggregate\ncomputing framework, and finally evaluate the approach via simulation on\nseveral case studies.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 20:24:29 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 17:09:58 GMT"}, {"version": "v3", "created": "Fri, 4 Jun 2021 08:46:10 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Pianini", "Danilo", ""], ["Casadei", "Roberto", ""], ["Viroli", "Mirko", ""], ["Mariani", "Stefano", ""], ["Zambonelli", "Franco", ""]]}, {"id": "2012.13846", "submitter": "Pan He", "authors": "Keke Zhai, Pan He, Tania Banerjee, Anand Rangarajan, and Sanjay Ranka", "title": "SparsePipe: Parallel Deep Learning for 3D Point Clouds", "comments": "Accepted in 2020 IEEE 27th International Conference on High\n  Performance Computing, Data, and Analytics (HiPC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose SparsePipe, an efficient and asynchronous parallelism approach for\nhandling 3D point clouds with multi-GPU training. SparsePipe is built to\nsupport 3D sparse data such as point clouds. It achieves this by adopting\ngeneralized convolutions with sparse tensor representation to build expressive\nhigh-dimensional convolutional neural networks. Compared to dense solutions,\nthe new models can efficiently process irregular point clouds without densely\nsliding over the entire space, significantly reducing the memory requirements\nand allowing higher resolutions of the underlying 3D volumes for better\nperformance.\n  SparsePipe exploits intra-batch parallelism that partitions input data into\nmultiple processors and further improves the training throughput with\ninter-batch pipelining to overlap communication and computing. Besides, it\nsuitably partitions the model when the GPUs are heterogeneous such that the\ncomputing is load-balanced with reduced communication overhead.\n  Using experimental results on an eight-GPU platform, we show that SparsePipe\ncan parallelize effectively and obtain better performance on current point\ncloud benchmarks for both training and inference, compared to its dense\nsolutions.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 01:47:09 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Zhai", "Keke", ""], ["He", "Pan", ""], ["Banerjee", "Tania", ""], ["Rangarajan", "Anand", ""], ["Ranka", "Sanjay", ""]]}, {"id": "2012.13891", "submitter": "Chen Wang", "authors": "Gaoyang Liu, Xiaoqiang Ma, Yang Yang, Chen Wang, Jiangchuan Liu", "title": "Federated Unlearning", "comments": "To be published in Proc.of 29th IEEE/ACM IWQoS, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) has recently emerged as a promising distributed\nmachine learning (ML) paradigm. Practical needs of the \"right to be forgotten\"\nand countering data poisoning attacks call for efficient techniques that can\nremove, or unlearn, specific training data from the trained FL model. Existing\nunlearning techniques in the context of ML, however, are no longer in effect\nfor FL, mainly due to the inherent distinction in the way how FL and ML learn\nfrom data. Therefore, how to enable efficient data removal from FL models\nremains largely under-explored. In this paper, we take the first step to fill\nthis gap by presenting FedEraser, the first federated unlearning methodology\nthat can eliminate the influence of a federated client's data on the global FL\nmodel while significantly reducing the time used for constructing the unlearned\nFL model.The basic idea of FedEraser is to trade the central server's storage\nfor unlearned model's construction time, where FedEraser reconstructs the\nunlearned model by leveraging the historical parameter updates of federated\nclients that have been retained at the central server during the training\nprocess of FL. A novel calibration method is further developed to calibrate the\nretained updates, which are further used to promptly construct the unlearned\nmodel, yielding a significant speed-up to the reconstruction of the unlearned\nmodel while maintaining the model efficacy. Experiments on four realistic\ndatasets demonstrate the effectiveness of FedEraser, with an expected speed-up\nof $4\\times$ compared with retraining from the scratch. We envision our work as\nan early step in FL towards compliance with legal and ethical criteria in a\nfair and transparent manner.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 08:54:37 GMT"}, {"version": "v2", "created": "Sun, 21 Feb 2021 10:08:04 GMT"}, {"version": "v3", "created": "Thu, 6 May 2021 04:50:42 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Liu", "Gaoyang", ""], ["Ma", "Xiaoqiang", ""], ["Yang", "Yang", ""], ["Wang", "Chen", ""], ["Liu", "Jiangchuan", ""]]}, {"id": "2012.13900", "submitter": "Ruiyuan Wu", "authors": "Ruiyuan Wu, Anna Scaglione, Hoi-To Wai, Nurullah Karakoc, Kari\n  Hreinsson, and Wing-Kin Ma", "title": "Federated Block Coordinate Descent Scheme for Learning Global and\n  Personalized Models", "comments": "31 pages, 5 figures. Codes available at this url\n  {https://github.com/REIYANG/FedBCD}. To appear in AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In federated learning, models are learned from users' data that are held\nprivate in their edge devices, by aggregating them in the service provider's\n\"cloud\" to obtain a global model. Such global model is of great commercial\nvalue in, e.g., improving the customers' experience. In this paper we focus on\ntwo possible areas of improvement of the state of the art. First, we take the\ndifference between user habits into account and propose a quadratic\npenalty-based formulation, for efficient learning of the global model that\nallows to personalize local models. Second, we address the latency issue\nassociated with the heterogeneous training time on edge devices, by exploiting\na hierarchical structure modeling communication not only between the cloud and\nedge devices, but also within the cloud. Specifically, we devise a tailored\nblock coordinate descent-based computation scheme, accompanied with\ncommunication protocols for both the synchronous and asynchronous cloud\nsettings. We characterize the theoretical convergence rate of the algorithm,\nand provide a variant that performs empirically better. We also prove that the\nasynchronous protocol, inspired by multi-agent consensus technique, has the\npotential for large gains in latency compared to a synchronous setting when the\nedge-device updates are intermittent. Finally, experimental results are\nprovided that corroborate not only the theory, but also show that the system\nleads to faster convergence for personalized models on the edge devices,\ncompared to the state of the art.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 09:39:28 GMT"}, {"version": "v2", "created": "Sun, 31 Jan 2021 00:18:59 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Wu", "Ruiyuan", ""], ["Scaglione", "Anna", ""], ["Wai", "Hoi-To", ""], ["Karakoc", "Nurullah", ""], ["Hreinsson", "Kari", ""], ["Ma", "Wing-Kin", ""]]}, {"id": "2012.13995", "submitter": "Xiaoyu Cao", "authors": "Xiaoyu Cao, Minghong Fang, Jia Liu, Neil Zhenqiang Gong", "title": "FLTrust: Byzantine-robust Federated Learning via Trust Bootstrapping", "comments": "To appear in NDSS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Byzantine-robust federated learning aims to enable a service provider to\nlearn an accurate global model when a bounded number of clients are malicious.\nThe key idea of existing Byzantine-robust federated learning methods is that\nthe service provider performs statistical analysis among the clients' local\nmodel updates and removes suspicious ones, before aggregating them to update\nthe global model. However, malicious clients can still corrupt the global\nmodels in these methods via sending carefully crafted local model updates to\nthe service provider. The fundamental reason is that there is no root of trust\nin existing federated learning methods.\n  In this work, we bridge the gap via proposing FLTrust, a new federated\nlearning method in which the service provider itself bootstraps trust. In\nparticular, the service provider itself collects a clean small training dataset\n(called root dataset) for the learning task and the service provider maintains\na model (called server model) based on it to bootstrap trust. In each\niteration, the service provider first assigns a trust score to each local model\nupdate from the clients, where a local model update has a lower trust score if\nits direction deviates more from the direction of the server model update.\nThen, the service provider normalizes the magnitudes of the local model updates\nsuch that they lie in the same hyper-sphere as the server model update in the\nvector space. Our normalization limits the impact of malicious local model\nupdates with large magnitudes. Finally, the service provider computes the\naverage of the normalized local model updates weighted by their trust scores as\na global model update, which is used to update the global model. Our extensive\nevaluations on six datasets from different domains show that our FLTrust is\nsecure against both existing attacks and strong adaptive attacks.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 18:43:39 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Cao", "Xiaoyu", ""], ["Fang", "Minghong", ""], ["Liu", "Jia", ""], ["Gong", "Neil Zhenqiang", ""]]}, {"id": "2012.14086", "submitter": "Mohamed Aymen Saied", "authors": "Leila Abdollahi Vayghan, Mohamed Aymen Saied, Maria Toeroe, Ferhat\n  Khendek", "title": "A Kubernetes Controller for Managing the Availability of Elastic\n  Microservice Based Stateful Applications", "comments": "Paper submitted to the Journal of Systems and Software", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The architectural style of microservices has been gaining popularity in\nrecent years. In this architectural style, small and loosely coupled modules\nare deployed and scaled independently to compose cloud-native applications.\nCarrier-grade service providers are migrating their legacy applications to a\nmicroservice based architecture running on Kubernetes which is an open source\nplatform for orchestrating containerized microservice based applications.\nHowever, in this migration, service availability remains a concern. Service\navailability is measured as the percentage of time the service is provisioned.\nHigh Availability (HA) is achieved when the service is available at least\n99.999% of the time. In this paper, we identify possible architectures for\ndeploying stateful microservice based applications with Kubernetes and evaluate\nKubernetes from the perspective of availability it provides for its managed\napplications. The results of our experiments show that the repair actions of\nKubernetes cannot satisfy HA requirements, and in some cases cannot guarantee\nservice recovery. Therefore, we propose an HA State Controller which integrates\nwith Kubernetes and allows for application state replication and automatic\nservice redirection to the healthy microservice instances by enabling service\nrecovery in addition to the repair actions of Kubernetes. Based on experiments\nwe evaluate our solution and compare the different architectures from the\nperspective of availability and scaling overhead. The results of our\ninvestigations show that our solution can improve the recovery time of stateful\nmicroservice based applications by 50%.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 04:03:06 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Vayghan", "Leila Abdollahi", ""], ["Saied", "Mohamed Aymen", ""], ["Toeroe", "Maria", ""], ["Khendek", "Ferhat", ""]]}, {"id": "2012.14132", "submitter": "Marcin Copik", "authors": "Marcin Copik, Grzegorz Kwasniewski, Maciej Besta, Michal Podstawski,\n  Torsten Hoefler", "title": "SeBS: A Serverless Benchmark Suite for Function-as-a-Service Computing", "comments": "Extended version of the paper accepted at Middleware 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Function-as-a-Service (FaaS) is one of the most promising directions for the\nfuture of cloud services, and serverless functions have immediately become a\nnew middleware for building scalable and cost-efficient microservices and\napplications. However, the quickly moving technology hinders reproducibility,\nand the lack of a standardized benchmarking suite leads to ad-hoc solutions and\nmicrobenchmarks being used in serverless research, further complicating\nmetaanalysis and comparison of research solutions. To address this challenge,\nwe propose the Serverless Benchmark Suite: the first benchmark for FaaS\ncomputing that systematically covers a wide spectrum of cloud resources and\napplications. Our benchmark consists of the specification of representative\nworkloads, the accompanying implementation and evaluation infrastructure, and\nthe evaluation methodology that facilitates reproducibility and enables\ninterpretability. We demonstrate that the abstract model of a FaaS execution\nenvironment ensures the applicability of our benchmark to multiple commercial\nproviders such as AWS, Azure, and Google Cloud. Our work facilities\nexperimental evaluation of serverless systems, and delivers a standardized,\nreliable and evolving evaluation methodology of performance, efficiency,\nscalability and reliability of middleware FaaS platforms.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 08:01:54 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 01:08:18 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Copik", "Marcin", ""], ["Kwasniewski", "Grzegorz", ""], ["Besta", "Maciej", ""], ["Podstawski", "Michal", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2012.14169", "submitter": "Yannic Maus", "authors": "Magn\\'us M. Halld\\'orsson, Fabian Kuhn, Yannic Maus, Tigran Tonoyan", "title": "Efficient Randomized Distributed Coloring in CONGEST", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Distributed vertex coloring is one of the classic problems and probably also\nthe most widely studied problems in the area of distributed graph algorithms.\nWe present a new randomized distributed vertex coloring algorithm for the\nstandard CONGEST model, where the network is modeled as an $n$-node graph $G$,\nand where the nodes of $G$ operate in synchronous communication rounds in which\nthey can exchange $O(\\log n)$-bit messages over all the edges of $G$. For\ngraphs with maximum degree $\\Delta$, we show that the $(\\Delta+1)$-list\ncoloring problem (and therefore also the standard $(\\Delta+1)$-coloring\nproblem) can be solved in $O(\\log^5\\log n)$ rounds. Previously such a result\nwas only known for the significantly more powerful LOCAL model, where in each\nround, neighboring nodes can exchange messages of arbitrary size. The best\nprevious $(\\Delta+1)$-coloring algorithm in the CONGEST model had a running\ntime of $O(\\log\\Delta + \\log^6\\log n)$ rounds. As a function of $n$ alone, the\nbest previous algorithm therefore had a round complexity of $O(\\log n)$, which\nis a bound that can also be achieved by a na\\\"{i}ve folklore algorithm. For\nlarge maximum degree $\\Delta$, our algorithm hence is an exponential\nimprovement over the previous state of the art.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 10:09:39 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 11:33:04 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Halld\u00f3rsson", "Magn\u00fas M.", ""], ["Kuhn", "Fabian", ""], ["Maus", "Yannic", ""], ["Tonoyan", "Tigran", ""]]}, {"id": "2012.14219", "submitter": "Antoine Kaufmann", "authors": "Hejing Li, Jialin Li, Keon Jang, Antoine Kaufmann", "title": "Reproducible Host Networking Evaluation with End-to-End Simulation", "comments": "15 pages, 10 figures, under submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networking researchers are facing growing challenges in evaluating and\nreproducing results for modern network systems. As systems rely on closer\nintegration of system components and cross-layer optimizations in the pursuit\nof performance and efficiency, they are also increasingly tied to specific\nhardware and testbed properties. Combined with a trend towards heterogeneous\nhardware, such as protocol offloads, SmartNICs, and in-network accelerators,\nresearchers face the choice of either investing more and more time and\nresources into comparisons to prior work or, alternatively, lower the standards\nfor evaluation.\n  We aim to address this challenge by introducing SimBricks, a simulation\nframework that decouples networked systems from the physical testbed and\nenables reproducible end-to-end evaluation in simulation. Instead of\nreinventing the wheel, SimBricks is a modular framework for combining existing\ntried-and-true simulators for individual components, processor and memory, NIC,\nand network, into complete testbeds capable of running unmodified systems. In\nour evaluation, we reproduce key findings from prior work, including dctcp\ncongestion control, NOPaxos in-network consensus acceleration, and the Corundum\nFPGA NIC.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 13:03:04 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Li", "Hejing", ""], ["Li", "Jialin", ""], ["Jang", "Keon", ""], ["Kaufmann", "Antoine", ""]]}, {"id": "2012.14294", "submitter": "Alaa Awad Abdellatif", "authors": "Alaa Awad Abdellatif, Lutfi Samara, Amr Mohamed, Aiman Erbad, Carla\n  Fabiana Chiasserini, Mohsen Guizani, Mark Dennis O'Connor, and James Laughton", "title": "I-Health: Leveraging Edge Computing and Blockchain for Epidemic\n  Management", "comments": "A version of this paper has been submitted in IEEE Internet of Things\n  Journal. arXiv admin note: text overlap with arXiv:2006.10843", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DC cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Epidemic situations typically demand intensive data collection and management\nfrom different locations/entities within a strict time constraint. Such demand\ncan be fulfilled by leveraging the intensive and easy deployment of the\nInternet of Things (IoT) devices. The management and containment of such\nsituations also rely on cross-organizational and national collaboration. Thus,\nthis paper proposes an Intelligent-Health (I-Health) system that aims to\naggregate diverse e-health entities in a unique national healthcare system by\nenabling swift, secure exchange and storage of medical data. In particular, we\ndesign an automated patients monitoring scheme, at the edge, which enables the\nprompt discovery, remote monitoring, and fast emergency response for critical\nmedical events, such as emerging epidemics. Furthermore, we develop a\nblockchain optimization model that aims to optimize medical data sharing\nbetween different health entities to provide effective and secure health\nservices. Finally, we show the effectiveness of our system, in adapting to\ndifferent critical events, while highlighting the benefits of the proposed\nI-Health system.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 23:41:00 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Abdellatif", "Alaa Awad", ""], ["Samara", "Lutfi", ""], ["Mohamed", "Amr", ""], ["Erbad", "Aiman", ""], ["Chiasserini", "Carla Fabiana", ""], ["Guizani", "Mohsen", ""], ["O'Connor", "Mark Dennis", ""], ["Laughton", "James", ""]]}, {"id": "2012.14332", "submitter": "Mario Michael Krell", "authors": "Sourabh Kulkarni and Mario Michael Krell and Seth Nabarro and Csaba\n  Andras Moritz", "title": "Hardware-accelerated Simulation-based Inference of Stochastic\n  Epidemiology Models for COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.AR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Epidemiology models are central in understanding and controlling large scale\npandemics. Several epidemiology models require simulation-based inference such\nas Approximate Bayesian Computation (ABC) to fit their parameters to\nobservations. ABC inference is highly amenable to efficient hardware\nacceleration. In this work, we develop parallel ABC inference of a stochastic\nepidemiology model for COVID-19. The statistical inference framework is\nimplemented and compared on Intel Xeon CPU, NVIDIA Tesla V100 GPU and the\nGraphcore Mk1 IPU, and the results are discussed in the context of their\ncomputational architectures. Results show that GPUs are 4x and IPUs are 30x\nfaster than Xeon CPUs. Extensive performance analysis indicates that the\ndifference between IPU and GPU can be attributed to higher communication\nbandwidth, closeness of memory to compute, and higher compute power in the IPU.\nThe proposed framework scales across 16 IPUs, with scaling overhead not\nexceeding 8% for the experiments performed. We present an example of our\nframework in practice, performing inference on the epidemiology model across\nthree countries, and giving a brief overview of the results.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 03:01:59 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Kulkarni", "Sourabh", ""], ["Krell", "Mario Michael", ""], ["Nabarro", "Seth", ""], ["Moritz", "Csaba Andras", ""]]}, {"id": "2012.14363", "submitter": "Carl Pearson", "authors": "Carl Pearson, Kun Wu, I-Hsin Chung, Jinjun Xiong, Wen-Mei Hwu", "title": "TEMPI: An Interposed MPI Library with a Canonical Representation of\n  CUDA-aware Datatypes", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MPI derived datatypes are an abstraction that simplifies handling of\nnon-contiguous data in MPI applications. These datatypes are recursively\nconstructed at runtime from primitive Named Types defined in the MPI standard.\nMore recently, the development and deployment of CUDA-aware MPI implementations\nhas encouraged the transition of distributed high-performance MPI codes to use\nGPUs. Such implementations allow MPI functions to directly operate on GPU\nbuffers, easing integration of GPU compute into MPI codes. This work first\npresents a novel datatype handling strategy for nested strided datatypes, which\nfinds a middle ground between the specialized or generic handling in prior\nwork. This work also shows that the performance characteristics of\nnon-contiguous data handling can be modeled with empirical system measurements,\nand used to transparently improve MPI_Send/Recv latency. Finally, despite\nsubstantial attention to non-contiguous GPU data and CUDA-aware MPI\nimplementations, good performance cannot be taken for granted. This work\ndemonstrates its contributions through an MPI interposer library, TEMPI. TEMPI\ncan be used with existing MPI deployments without system or application\nchanges. Ultimately, the interposed-library model of this work demonstrates\nMPI_Pack speedup of up to 242000x and MPI_Send speedup of up to 59000x compared\nto the MPI implementation deployed on a leadership-class supercomputer. This\nyields speedup of more than 917x in a 3D halo exchange with 3072 processes.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 17:01:39 GMT"}, {"version": "v2", "created": "Tue, 29 Dec 2020 23:12:35 GMT"}, {"version": "v3", "created": "Tue, 5 Jan 2021 01:22:51 GMT"}, {"version": "v4", "created": "Mon, 11 Jan 2021 19:43:20 GMT"}, {"version": "v5", "created": "Wed, 13 Jan 2021 14:01:49 GMT"}, {"version": "v6", "created": "Thu, 21 Jan 2021 22:44:17 GMT"}, {"version": "v7", "created": "Fri, 16 Apr 2021 15:44:44 GMT"}, {"version": "v8", "created": "Tue, 20 Apr 2021 22:33:04 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Pearson", "Carl", ""], ["Wu", "Kun", ""], ["Chung", "I-Hsin", ""], ["Xiong", "Jinjun", ""], ["Hwu", "Wen-Mei", ""]]}, {"id": "2012.14368", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu, Faeze Ebrahimian, Jerry Li, Dan Alistarh", "title": "Byzantine-Resilient Non-Convex Stochastic Gradient Descent", "comments": "V1.5 polishes writing and V2 rewrites the experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.DS cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study adversary-resilient stochastic distributed optimization, in which\n$m$ machines can independently compute stochastic gradients, and cooperate to\njointly optimize over their local objective functions. However, an\n$\\alpha$-fraction of the machines are $\\textit{Byzantine}$, in that they may\nbehave in arbitrary, adversarial ways. We consider a variant of this procedure\nin the challenging $\\textit{non-convex}$ case. Our main result is a new\nalgorithm SafeguardSGD which can provably escape saddle points and find\napproximate local minima of the non-convex objective. The algorithm is based on\na new concentration filtering technique, and its sample and time complexity\nbounds match the best known theoretical bounds in the stochastic, distributed\nsetting when no Byzantine machines are present.\n  Our algorithm is very practical: it improves upon the performance of all\nprior methods when training deep neural networks, it is relatively lightweight,\nand it is the first method to withstand two recently-proposed Byzantine\nattacks.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 17:19:32 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 17:25:48 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Ebrahimian", "Faeze", ""], ["Li", "Jerry", ""], ["Alistarh", "Dan", ""]]}, {"id": "2012.14453", "submitter": "Amirhossein Reisizadeh", "authors": "Amirhossein Reisizadeh, Isidoros Tziotis, Hamed Hassani, Aryan\n  Mokhtari, Ramtin Pedarsani", "title": "Straggler-Resilient Federated Learning: Leveraging the Interplay Between\n  Statistical Accuracy and System Heterogeneity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning is a novel paradigm that involves learning from data\nsamples distributed across a large network of clients while the data remains\nlocal. It is, however, known that federated learning is prone to multiple\nsystem challenges including system heterogeneity where clients have different\ncomputation and communication capabilities. Such heterogeneity in clients'\ncomputation speeds has a negative effect on the scalability of federated\nlearning algorithms and causes significant slow-down in their runtime due to\nthe existence of stragglers. In this paper, we propose a novel\nstraggler-resilient federated learning method that incorporates statistical\ncharacteristics of the clients' data to adaptively select the clients in order\nto speed up the learning procedure. The key idea of our algorithm is to start\nthe training procedure with faster nodes and gradually involve the slower nodes\nin the model training once the statistical accuracy of the data corresponding\nto the current participating nodes is reached. The proposed approach reduces\nthe overall runtime required to achieve the statistical accuracy of data of all\nnodes, as the solution for each stage is close to the solution of the\nsubsequent stage with more samples and can be used as a warm-start. Our\ntheoretical results characterize the speedup gain in comparison to standard\nfederated benchmarks for strongly convex objectives, and our numerical\nexperiments also demonstrate significant speedups in wall-clock time of our\nstraggler-resilient method compared to federated learning benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 19:21:14 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Reisizadeh", "Amirhossein", ""], ["Tziotis", "Isidoros", ""], ["Hassani", "Hamed", ""], ["Mokhtari", "Aryan", ""], ["Pedarsani", "Ramtin", ""]]}, {"id": "2012.14542", "submitter": "Ajay Singh", "authors": "Ajay Singh, Trevor Brown, Ali Mashtizadeh", "title": "NBR: Neutralization Based Reclamation", "comments": "Accepted in PPoPP2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Safe memory reclamation (SMR) algorithms suffer from a trade-off between\nbounding unreclaimed memory and the speed of reclamation. Hazard pointer (HP)\nbased algorithms bound unreclaimed memory at all times, but tend to be slower\nthan other approaches. Epoch based reclamation (EBR) algorithms are faster, but\ndo not bound memory reclamation. Other algorithms follow hybrid approaches,\nrequiring special compiler or hardware support, changes to record layouts,\nand/or extensive code changes. Not all SMR algorithms can be used to reclaim\nmemory for all data structures.\n  We propose a new neutralization based reclamation (NBR) algorithm that is\nfaster than the best known EBR algorithms and achieves bounded unreclaimed\nmemory. It is non-blocking when used with a non-blocking operating system (OS)\nkernel, and only requires atomic read, write and CAS. NBR is straightforward to\nuse with many different data structures, and in most cases, require similar\nreasoning and programmer effort to two-phased locking. NBR is implemented using\nOS signals and a lightweight handshaking mechanism between participating\nthreads to determine when it is safe to reclaim a record. Experiments on a\nlock-based binary search tree and a lazy linked list show that NBR\nsignificantly outperforms many state of the art reclamation algorithms. In the\ntree NBR is faster than next best algorithm, DEBRA by upto 38% and HP by upto\n17%. And, in the list NBR is 15% and 243% faster than DEBRA and HP,\nrespectively.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 00:32:48 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 21:20:03 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Singh", "Ajay", ""], ["Brown", "Trevor", ""], ["Mashtizadeh", "Ali", ""]]}, {"id": "2012.14713", "submitter": "Mohan Liyanage", "authors": "Mohan Liyanage, Farooq Dar, Rajesh Sharma, Huber Flores", "title": "Edge Computing Enabled by Unmanned Autonomous Vehicles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pervasive applications are revolutionizing the perception that users have\ntowards the environment. Indeed, pervasive applications perform resource\nintensive computations over large amounts of stream sensor data collected from\nmultiple sources. This allows applications to provide richer and deep insights\ninto the natural characteristics that govern everything that surrounds us. A\nkey limitation of these applications is that they have high energy footprints,\nwhich in turn hampers the quality of experience of users. While cloud and edge\ncomputing solutions can be applied to alleviate the problem, these solutions\nare hard to adopt in existing architecture and far from become ubiquitous.\nFortunately, cloudlets are becoming portable enough, such that they can be\ntransported and integrated into any environment easily and dynamically. In this\narticle, we investigate how cloudlets can be transported by unmanned autonomous\nvehicles (UAV)s to provide computation support on the edge. Based on our study,\nwe develop GEESE, a novel UAVbased system that enables the dynamic deployment\nof an edge computing infrastructure through the cooperation of multiple UAVs\ncarrying cloudlets. By using GEESE, we conduct rigorous experiments to analyze\nthe effort to deliver cloudlets using aerial, ground, and underwater UAVs. Our\nresults indicate that UAVs can work in a cooperative manner to enable edge\ncomputing in the wild.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 11:44:46 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Liyanage", "Mohan", ""], ["Dar", "Farooq", ""], ["Sharma", "Rajesh", ""], ["Flores", "Huber", ""]]}, {"id": "2012.14757", "submitter": "Ioannis Vardas", "authors": "Ioannis Vardas, Manolis Ploumidis and Manolis Marazakis", "title": "Improving the Performance and Resilience of MPI Parallel Jobs with\n  Topology and Fault-Aware Process Placement", "comments": "21 pages, 8 figures, added Acknowledgements section", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  HPC systems keep growing in size to meet the ever-increasing demand for\nperformance and computational resources. Apart from increased performance,\nlarge scale systems face two challenges that hinder further growth: energy\nefficiency and resiliency. At the same time, applications seeking increased\nperformance rely on advanced parallelism for exploiting system resources, which\nleads to increased pressure on system interconnects. At large system scales,\nincreased communication locality can be beneficial both in terms of application\nperformance and energy consumption. Towards this direction, several studies\nfocus on deriving a mapping of an application's processes to system nodes in a\nway that communication cost is reduced. A common approach is to express both\nthe application's communication patterns and the system architecture as graphs\nand then solve the corresponding mapping problem. Apart from communication\ncost, the completion time of a job can also be affected by node failures. Node\nfailures may result in job abortions, requiring job restarts. In this paper, we\naddress the problem of assigning processes to system resources with the goal of\nreducing communication cost while also taking into account node failures. The\nproposed approach is integrated into the Slurm resource manager. Evaluation\nresults show that, in scenarios where few nodes have a low outage probability,\nthe proposed process placement approach achieves a notable decrease in the\ncompletion time of batches of MPI jobs. Compared to the default process\nplacement approach in Slurm, the reduction is 18.9% and 31%, respectively for\ntwo different MPI applications.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 14:09:52 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 11:03:30 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Vardas", "Ioannis", ""], ["Ploumidis", "Manolis", ""], ["Marazakis", "Manolis", ""]]}, {"id": "2012.14790", "submitter": "Seyed Mehdi Hosseini Motlagh", "authors": "Seyed Mehdi Hosseini Motlagh", "title": "Thermal Safety and Real-Time Predictability on Heterogeneous Embedded\n  SoC Platforms", "comments": "This is the Ph.D. dissertation of the author", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recent embedded systems are designed with high-performance System-on-Chips\n(SoCs) to satisfy the computational needs of complex applications widely used\nin real life, such as airplane controllers, autonomous driving automobiles,\nmedical devices, drones, and hand-held devices. Modern SoCs integrate\nmulti-core CPUs and various types of accelerators including GPUs and DSPs.\nUncontrolled heat dissipation is one of the main sources of interference that\ncan adversely affect the reliability and real-time performance of\nsafety-critical applications. The mechanisms currently available to protect\nSoCs from overheating, such as frequency throttling or core shutdown, may\nexacerbate the problem as they cause unpredictable delay and deadline misses.\nDynamic changes in ambient temperature further increase the difficulty of\nsolving this problem.\n  This dissertation addresses the challenges caused by thermal interference in\nreal-time mixed-criticality systems built with heterogeneous embedded SoC\nplatforms. We propose a novel thermal-aware system framework with analytical\ntiming and thermal models to guarantee safe execution of real-time tasks under\nthe thermal constraints of a multi-core CPU/GPU integrated SoC. For\nmixed-criticality tasks, the proposed framework bounds the heat generation of\nthe system at each criticality level and provides different levels of assurance\nagainst ambient temperature changes. In addition, we propose a data-driven\nthermal parameter estimation scheme that is directly applicable to MCSs built\nwith commercial-off-the-shelf multi-core processors to obtain a precise thermal\nmodel without using special measurement instruments or access to proprietary\ninformation. The practicality and effectiveness of our solutions have been\nevaluated using real SoC platforms and our contributions will help develop\nsystems with thermal safety and real-time predictability.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 15:05:22 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Motlagh", "Seyed Mehdi Hosseini", ""]]}, {"id": "2012.14792", "submitter": "Thomas Amestoy", "authors": "Thomas Amestoy (IETR), Wassim Hamidouche (IETR), Cyril Bergeron,\n  Daniel Menard (IETR)", "title": "Quality-Driven Dynamic VVC Frame Partitioning for Efficient Parallel\n  Processing", "comments": null, "journal-ref": "27th IEEE International Conference on Image Processing (ICIP\n  2020), Oct 2020, Abu Dhabi, United Arab Emirates. pp.3129-3133", "doi": "10.1109/ICIP40778.2020.9190928", "report-no": null, "categories": "cs.DC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  VVC is the next generation video coding standard, offering coding capability\nbeyond HEVC standard. The high computational complexity of the latest video\ncoding standards requires high-level parallelism techniques, in order to\nachieve real-time and low latency encoding and decoding. HEVC and VVC include\ntile grid partitioning that allows to process simultaneously rectangular\nregions of a frame with independent threads. The tile grid may be further\npartitioned into a horizontal sub-grid of Rectangular Slices (RSs), increasing\nthe partitioning flexibility. The dynamic Tile and Rectangular Slice (TRS)\npartitioning solution proposed in this paper benefits from this flexibility.\nThe TRS partitioning is carried-out at the frame level, taking into account\nboth spatial texture of the content and encoding times of previously encoded\nframes. The proposed solution searches the best partitioning configuration that\nminimizes the trade-off between multi-thread encoding time and encoding quality\nloss. Experiments prove that the proposed solution, compared to uniform TRS\npartitioning, significantly decreases multi-thread encoding time, with slightly\nbetter encoding quality.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 15:07:04 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Amestoy", "Thomas", "", "IETR"], ["Hamidouche", "Wassim", "", "IETR"], ["Bergeron", "Cyril", "", "IETR"], ["Menard", "Daniel", "", "IETR"]]}, {"id": "2012.14847", "submitter": "Raazesh Sainudiin", "authors": "Raazesh Sainudiin and Warwick Tucker and Tilo Wiklund", "title": "Scalable Multivariate Histograms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DC math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We give a distributed variant of an adaptive histogram estimation procedure\npreviously developed by the first author. The procedure is based on regular\npavings and is known to have numerous appealing statistical and arithmetical\nproperties. The distributed version makes it possible to process data sets\nsignificantly bigger than previously. We provide prototype implementation under\na permissive license.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 16:51:53 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Sainudiin", "Raazesh", ""], ["Tucker", "Warwick", ""], ["Wiklund", "Tilo", ""]]}, {"id": "2012.14971", "submitter": "Zheng Chen", "authors": "Zheng Chen and Erik G. Larsson", "title": "Consensus-Based Distributed Computation of Link-Based Network Metrics", "comments": "5 pages, 2 figures, accepted for publication in IEEE Signal\n  Processing Letters", "journal-ref": null, "doi": "10.1109/LSP.2021.3050056", "report-no": null, "categories": "cs.DC cs.SI eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Average consensus algorithms have wide applications in distributed computing\nsystems where all the nodes agree on the average value of their initial states\nby only exchanging information with their local neighbors. In this letter, we\nlook into link-based network metrics which are polynomial functions of\npair-wise node attributes defined over the links in a network. Different from\nnode-based average consensus, such link-based metrics depend on both the\ndistribution of node attributes and the underlying network topology. We propose\na general algorithm using the weighted average consensus protocol for the\ndistributed computation of link-based network metrics and provide the\nconvergence conditions and convergence rate analysis.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 23:12:22 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2021 15:59:58 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Chen", "Zheng", ""], ["Larsson", "Erik G.", ""]]}, {"id": "2012.15027", "submitter": "Yuqing Zhu", "authors": "Yuqing Zhu", "title": "When Load Rebalancing Does Not Work for Distributed Hash Table", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Distributed hash table (DHT) is the foundation of many widely used storage\nsystems, for its prominent features of high scalability and load balancing.\nRecently, DHT-based systems have been deployed for the Internet-of-Things (IoT)\napplication scenarios. Unfortunately, such systems can experience a breakdown\nin the scale-out and load rebalancing process. This phenomenon contradicts with\nthe common conception of DHT systems, especially about its scalability and load\nbalancing features. In this paper, we investigate the breakdown of DHT-based\nsystems in the scale-out process. We formulate the load rebalancing problem of\nDHT by considering the impacts of write workloads and data movement. We show\nthat, the average network bandwidth of each node and the intensity of the\naverage write workload are the two key factors that determine the feasibility\nof DHT load rebalancing. We theoretically prove that load rebalancing is not\nfeasible for a large DHT system under heavy write workloads in a node-by-node\nscale-out process.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 03:58:10 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Zhu", "Yuqing", ""]]}, {"id": "2012.15198", "submitter": "Sangho Yeo Student", "authors": "Sangho Yeo, Minho Bae, Minjoong Jeong, Oh-kyoung Kwon, Sangyoon Oh", "title": "Crossover-SGD: A gossip-based communication in distributed deep learning\n  for alleviating large mini-batch problem and enhancing scalability", "comments": "Under review as a journal paper at CCPE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Distributed deep learning is an effective way to reduce the training time of\ndeep learning for large datasets as well as complex models. However, the\nlimited scalability caused by network overheads makes it difficult to\nsynchronize the parameters of all workers. To resolve this problem,\ngossip-based methods that demonstrates stable scalability regardless of the\nnumber of workers have been proposed. However, to use gossip-based methods in\ngeneral cases, the validation accuracy for a large mini-batch needs to be\nverified. To verify this, we first empirically study the characteristics of\ngossip methods in a large mini-batch problem and observe that the gossip\nmethods preserve higher validation accuracy than AllReduce-SGD(Stochastic\nGradient Descent) when the number of batch sizes is increased and the number of\nworkers is fixed. However, the delayed parameter propagation of the\ngossip-based models decreases validation accuracy in large node scales. To cope\nwith this problem, we propose Crossover-SGD that alleviates the delay\npropagation of weight parameters via segment-wise communication and load\nbalancing random network topology. We also adapt hierarchical communication to\nlimit the number of workers in gossip-based communication methods. To validate\nthe effectiveness of our proposed method, we conduct empirical experiments and\nobserve that our Crossover-SGD shows higher node scalability than\nSGP(Stochastic Gradient Push).\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 15:39:13 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Yeo", "Sangho", ""], ["Bae", "Minho", ""], ["Jeong", "Minjoong", ""], ["Kwon", "Oh-kyoung", ""], ["Oh", "Sangyoon", ""]]}, {"id": "2012.15295", "submitter": "Yuankun Fu", "authors": "Yuankun Fu, Fengguang Song", "title": "SDN helps Big Data to optimize access to data", "comments": null, "journal-ref": "Big Data and Software Defined Networks, March, 2018", "doi": "10.1049/PBPC015E_ch14", "report-no": null, "categories": "cs.DC cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter introduces the state-of-the-art in the emerging area of\ncombining High Performance Computing (HPC) with Big Data Analysis. To\nunderstand the new area, the chapter first surveys the existing approaches to\nintegrating HPC with Big Data. Next, the chapter introduces several\noptimization solutions that focus on how to minimize the data transfer time\nfrom computation-intensive applications to analysis-intensive applications as\nwell as minimizing the end-to-end time-to-solution. The solutions utilize SDN\nto adaptively use both high speed interconnect network and high performance\nparallel file systems to optimize the application performance. A computational\nframework called DataBroker is designed and developed to enable a tight\nintegration of HPC with data analysis. Multiple types of experiments have been\nconducted to show different performance issues in both message passing and\nparallel file systems and to verify the effectiveness of the proposed research\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 19:33:13 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Fu", "Yuankun", ""], ["Song", "Fengguang", ""]]}, {"id": "2012.15321", "submitter": "Yubo Qin", "authors": "Yubo Qin, Ivan Rodero, Anthony Simonet, Charles Meertens, Daniel\n  Reiner, James Riley, Manish Parashar", "title": "Leveraging User Access Patterns and Advanced Cyberinfrastructure to\n  Accelerate Data Delivery from Shared-use Scientific Observatories", "comments": "10 pages, 13 figures, 5 tables, Future Generation Computer Systems\n  journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the growing number and increasing availability of shared-use instruments\nand observatories, observational data is becoming an essential part of\napplication workflows and contributor to scientific discoveries in a range of\ndisciplines. However, the corresponding growth in the number of users accessing\nthese facilities coupled with the expansion in the scale and variety of the\ndata, is making it challenging for these facilities to ensure their data can be\naccessed, integrated, and analyzed in a timely manner, and is resulting\nsignificant demands on their cyberinfrastructure (CI).\n  In this paper, we present the design of a push-based data delivery framework\nthat leverages emerging in-network capabilities, along with data pre-fetching\ntechniques based on a hybrid data management model. Specifically, we analyze\ndata access traces for two large-scale observatories, Ocean Observatories\nInitiative (OOI) and Geodetic Facility for the Advancement of Geoscience\n(GAGE), to identify typical user access patterns and to develop a model that\ncan be used for data pre-fetching. Furthermore, we evaluate our data\npre-fetching model and the proposed framework using a simulation of the Virtual\nData Collaboratory (VDC) platform that provides in-network data staging and\nprocessing capabilities. The results demonstrate that the ability of the\nframework to significantly improve data delivery performance and reduce network\ntraffic at the observatories' facilities.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 20:52:00 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Qin", "Yubo", ""], ["Rodero", "Ivan", ""], ["Simonet", "Anthony", ""], ["Meertens", "Charles", ""], ["Reiner", "Daniel", ""], ["Riley", "James", ""], ["Parashar", "Manish", ""]]}, {"id": "2012.15351", "submitter": "Mohsen Toorani", "authors": "Mohsen Toorani, Christian Gehrmann", "title": "A Decentralized Dynamic PKI based on Blockchain", "comments": "19 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The central role of the certificate authority (CA) in traditional public key\ninfrastructure (PKI) makes it fragile and prone to compromises and operational\nfailures. Maintaining CAs and revocation lists is demanding especially in\nloosely-connected and large systems. Log-based PKIs have been proposed as a\nremedy but they do not solve the problem effectively. We provide a general\nmodel and a solution for decentralized and dynamic PKI based on a blockchain\nand web of trust model where the traditional CA and digital certificates are\nremoved and instead, everything is registered on the blockchain. Registration,\nrevocation, and update of public keys are based on a consensus mechanism\nbetween a certain number of entities that are already part of the system. Any\nnode which is part of the system can be an auditor and initiate the revocation\nprocedure once it finds out malicious activities. Revocation lists are no\nlonger required as any node can efficiently verify the public keys through\nwitnesses.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 22:45:23 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Toorani", "Mohsen", ""], ["Gehrmann", "Christian", ""]]}, {"id": "2012.15422", "submitter": "Konstantinos Kallas", "authors": "Shivam Handa (MIT), Konstantinos Kallas (University of Pennsylvania),\n  Nikos Vasilakis (MIT), Martin Rinard (MIT)", "title": "An Order-Aware Dataflow Model for Parallel Unix Pipelines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a dataflow model for modelling parallel Unix shell pipelines. To\naccurately capture the semantics of complex Unix pipelines, the dataflow model\nis order-aware, i.e., the order in which a node in the dataflow graph consumes\ninputs from different edges plays a central role in the semantics of the\ncomputation and therefore in the resulting parallelization. We use this model\nto capture the semantics of transformations that exploit data parallelism\navailable in Unix shell computations and prove their correctness. We\nadditionally formalize the translations from the Unix shell to the dataflow\nmodel and from the dataflow model back to a parallel shell script. We implement\nour model and transformations as the compiler and optimization passes of a\nsystem parallelizing shell pipelines, and use it to evaluate the speedup\nachieved on 47 pipelines.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 03:25:29 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 19:23:18 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Handa", "Shivam", "", "MIT"], ["Kallas", "Konstantinos", "", "University of Pennsylvania"], ["Vasilakis", "Nikos", "", "MIT"], ["Rinard", "Martin", "", "MIT"]]}, {"id": "2012.15438", "submitter": "Jacob Nelson", "authors": "Jacob Nelson, Ahmed Hassan, and Roberto Palmieri", "title": "Bundled References: An Abstraction for Highly-Concurrent Linearizable\n  Range Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present bundled references, a new building block to provide linearizable\nrange query operations for highly concurrent linked data structures. Bundled\nreferences allow range queries to traverse a path through the data structure\nthat is consistent with the target atomic snapshot and is made of the minimal\namount of nodes that should be accessed to preserve linearizability. We\nimplement our technique into a skip list, a binary search tree, and a linked\nlist data structure. Our evaluation reveals that in mixed workloads, our design\nimproves upon the state-of-the-art techniques by 3.9x for a skip list and 2.1x\nfor a binary search tree. We also integrate our bundled data structure into the\nDBx1000 in-memory database, yielding up to 20% gain over the same competitors.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 04:11:11 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Nelson", "Jacob", ""], ["Hassan", "Ahmed", ""], ["Palmieri", "Roberto", ""]]}, {"id": "2012.15443", "submitter": "Nikos Vasilakis", "authors": "Nikos Vasilakis (CSAIL, MIT), Jiasi Shen (CSAIL, MIT), Martin Rinard\n  (CSAIL, MIT)", "title": "Automatic Synthesis of Parallel and Distributed Unix Commands with\n  KumQuat", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present KumQuat, a system for automatically synthesizing parallel and\ndistributed versions of Unix shell commands. KumQuat follows a\ndivide-and-conquer approach, decomposing commands into (i) a parallel mapper\napplying the original command to produce partial results, and (ii) an ordered\ncombiner that combines the partial results into the final output. KumQuat\nsynthesizes the combiner by applying repeated rounds of exploration; at each\nround, it compares the results of the synthesized program with those from the\nsequential program to discard invalid candidates. A series of refinements\nimprove the performance of both the synthesis component and the resulting\nsynthesized programs. For 98.2% of Unix commands from real pipelines, KumQuat\neither synthesizes a combiner (92.2%) or reports that a combiner is not\nsynthesizable (7.8%), offering an average speedup of 7.8$\\times$ for the\nparallel version and 3.8$\\times$ for the distributed version.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 04:31:26 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Vasilakis", "Nikos", "", "CSAIL, MIT"], ["Shen", "Jiasi", "", "CSAIL, MIT"], ["Rinard", "Martin", "", "CSAIL, MIT"]]}, {"id": "2012.15469", "submitter": "Tianyi Chen", "authors": "Tianyi Chen, Ziye Guo, Yuejiao Sun and Wotao Yin", "title": "CADA: Communication-Adaptive Distributed Adam", "comments": "OPT2020: NeurIPS Workshop on Optimization for Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent (SGD) has taken the stage as the primary\nworkhorse for large-scale machine learning. It is often used with its adaptive\nvariants such as AdaGrad, Adam, and AMSGrad. This paper proposes an adaptive\nstochastic gradient descent method for distributed machine learning, which can\nbe viewed as the communication-adaptive counterpart of the celebrated Adam\nmethod - justifying its name CADA. The key components of CADA are a set of new\nrules tailored for adaptive stochastic gradients that can be implemented to\nsave communication upload. The new algorithms adaptively reuse the stale Adam\ngradients, thus saving communication, and still have convergence rates\ncomparable to original Adam. In numerical experiments, CADA achieves impressive\nempirical performance in terms of total communication round reduction.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 06:52:18 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Chen", "Tianyi", ""], ["Guo", "Ziye", ""], ["Sun", "Yuejiao", ""], ["Yin", "Wotao", ""]]}, {"id": "2012.15545", "submitter": "Md Ferdous Pervej", "authors": "Md Ferdous Pervej and Shih-Chun Lin", "title": "Vehicular Network Slicing for Reliable Access and Deadline-Constrained\n  Data Offloading: A Multi-Agent On-Device Learning Approach", "comments": "Submitted for possible journal publication, 15 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.MA cs.SY eess.SP eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Efficient data offloading plays a pivotal role in computational-intensive\nplatforms as data rate over wireless channels is fundamentally limited. On top\nof that, high mobility adds an extra burden in vehicular edge networks (VENs),\nbolstering the desire for efficient user-centric solutions. Therefore, unlike\nthe legacy inflexible network-centric approach, this paper exploits a\nsoftware-defined flexible, open, and programmable networking platform for an\nefficient user-centric, fast, reliable, and deadline-constrained offloading\nsolution in VENs. In the proposed model, each active vehicle user (VU) is\nserved from multiple low-powered access points (APs) by creating a noble\nvirtual cell (VC). A joint node association, power allocation, and distributed\nresource allocation problem is formulated. As centralized learning is not\npractical in many real-world problems, following the distributed nature of\nautonomous VUs, each VU is considered an edge learning agent. To that end,\nconsidering practical location-aware node associations, a joint radio and power\nresource allocation non-cooperative stochastic game is formulated. Leveraging\nreinforcement learning's (RL) efficacy, a multi-agent RL (MARL) solution is\nproposed where the edge learners aim to learn the Nash equilibrium (NE)\nstrategies to solve the game efficiently. Besides, real-world map data, with a\npractical microscopic mobility model, are used for the simulation. Results\nsuggest that the proposed user-centric approach can deliver remarkable\nperformances in VENs. Moreover, the proposed MARL solution delivers\nnear-optimal performances with approximately 3% collision probabilities in case\nof distributed random access in the uplink.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 11:15:10 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Pervej", "Md Ferdous", ""], ["Lin", "Shih-Chun", ""]]}, {"id": "2012.15589", "submitter": "Binbin Guo", "authors": "Binbin Guo, Yuan Mei, Danyang Xiao, Weigang Wu, Ye Yin, Hongli Chang", "title": "PFL-MoE: Personalized Federated Learning Based on Mixture of Experts", "comments": "10 pages, 5 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Federated learning (FL) is an emerging distributed machine learning paradigm\nthat avoids data sharing among training nodes so as to protect data privacy.\nUnder coordination of the FL server, each client conducts model training using\nits own computing resource and private data set. The global model can be\ncreated by aggregating the training results of clients. To cope with highly\nnon-IID data distributions, personalized federated learning (PFL) has been\nproposed to improve overall performance by allowing each client to learn a\npersonalized model. However, one major drawback of a personalized model is the\nloss of generalization. To achieve model personalization while maintaining\ngeneralization, in this paper, we propose a new approach, named PFL-MoE, which\nmixes outputs of the personalized model and global model via the MoE\narchitecture. PFL-MoE is a generic approach and can be instantiated by\nintegrating existing PFL algorithms. Particularly, we propose the PFL-MF\nalgorithm which is an instance of PFL-MoE based on the freeze-base PFL\nalgorithm. We further improve PFL-MF by enhancing the decision-making ability\nof MoE gating network and propose a variant algorithm PFL-MFE. We demonstrate\nthe effectiveness of PFL-MoE by training the LeNet-5 and VGG-16 models on the\nFashion-MNIST and CIFAR-10 datasets with non-IID partitions.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 12:51:14 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Guo", "Binbin", ""], ["Mei", "Yuan", ""], ["Xiao", "Danyang", ""], ["Wu", "Weigang", ""], ["Yin", "Ye", ""], ["Chang", "Hongli", ""]]}, {"id": "2012.15592", "submitter": "Marcin Copik", "authors": "Marcin Copik, Alexandru Calotoiu, Tobias Grosser, Nicolas Wicki, Felix\n  Wolf, Torsten Hoefler", "title": "Extracting Clean Performance Models from Tainted Programs", "comments": "Accepted at PPoPP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance models are well-known instruments to understand the scaling\nbehavior of parallel applications. They express how performance changes as key\nexecution parameters, such as the number of processes or the size of the input\nproblem, vary. Besides reasoning about program behavior, such models can also\nbe automatically derived from performance data. This is called empirical\nperformance modeling. While this sounds simple at the first glance, this\napproach faces several serious interrelated challenges, including expensive\nperformance measurements, inaccuracies inflicted by noisy benchmark data, and\noverall complex experiment design, starting with the selection of the right\nparameters. The more parameters one considers, the more experiments are needed\nand the stronger the impact of noise. In this paper, we show how taint\nanalysis, a technique borrowed from the domain of computer security, can\nsubstantially improve the modeling process, lowering its cost, improving model\nquality, and help validate performance models and experimental setups.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 12:59:19 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Copik", "Marcin", ""], ["Calotoiu", "Alexandru", ""], ["Grosser", "Tobias", ""], ["Wicki", "Nicolas", ""], ["Wolf", "Felix", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2012.15675", "submitter": "Xiaorui Sun", "authors": "Sebastian Forster and Gramoz Goranci and Yang P. Liu and Richard Peng\n  and Xiaorui Sun and Mingquan Ye", "title": "Minor Sparsifiers and the Distributed Laplacian Paradigm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study distributed algorithms built around edge contraction based vertex\nsparsifiers, and give sublinear round algorithms in the $\\textsf{CONGEST}$\nmodel for exact mincost flow, negative weight shortest paths, maxflow, and\nbipartite matching on sparse graphs. For the maxflow problem, this is the first\nexact distributed algorithm that applies to directed graphs, while the previous\nwork by [Ghaffari et al. SICOMP'18] considered the approximate setting and\nworks only for undirected graphs. For the mincost flow and the negative weight\nshortest path problems, our results constitute the first exact distributed\nalgorithms running in a sublinear number of rounds. These algorithms follow the\ncelebrated Laplacian paradigm, which numerically solve combinatorial graph\nproblems via series of linear systems in graph Laplacian matrices.\n  To enable Laplacian based algorithms in the distributed setting, we develop a\nLaplacian solver based upon the subspace sparsifiers of [Li, Schild FOCS'18].\nWe give a parallel variant of their algorithm that avoids the sampling of\nrandom spanning trees, and analyze it using matrix martingales. Combining this\nvertex reduction recursively with both tree and elimination based\npreconditioners leads to an algorithm for solving Laplacian systems on $n$\nvertex graphs to high accuracy in $O(n^{o(1)}(\\sqrt{n}+D))$ rounds. The round\ncomplexity of this distributed solver almost matches the lower bound of\n$\\widetilde{\\Omega}(\\sqrt{n}+D)$.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 15:52:28 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2021 18:01:01 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Forster", "Sebastian", ""], ["Goranci", "Gramoz", ""], ["Liu", "Yang P.", ""], ["Peng", "Richard", ""], ["Sun", "Xiaorui", ""], ["Ye", "Mingquan", ""]]}, {"id": "2012.15731", "submitter": "Jayneel Gandhi", "authors": "Pradeep Fernando, Irina Calciu, Jayneel Gandhi, Aasheesh Kolli and Ada\n  Gavrilovska", "title": "Persistence and Synchronization: Friends or Foes?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Emerging non-volatile memory (NVM) technologies promise memory speed\nbyte-addressable persistent storage with a load/store interface. However,\nprogramming applications to directly manipulate NVM data is complex and\nerror-prone. Applications generally employ libraries that hide the low-level\ndetails of the hardware and provide a transactional programming model to\nachieve crash-consistency. Furthermore, applications continue to expect\ncorrectness during concurrent executions, achieved through the use of\nsynchronization. To achieve this, applications seek well-known ACID guarantees.\nHowever, realizing this presents designers of transactional systems with a\nrange of choices in how to combine several low-level techniques, given target\nhardware features and workload characteristics.\n  In this paper, we provide a comprehensive evaluation of the impact of\ncombining existing crash-consistency and synchronization methods for achieving\nperformant and correct NVM transactional systems. We consider different\nhardware characteristics, in terms of support for hardware transactional memory\n(HTM) and the boundaries of the persistence domain (transient or persistent\ncaches). By characterizing persistent transactional systems in terms of their\nproperties, we make it possible to better understand the tradeoffs of different\nimplementations and to arrive at better design choices for providing ACID\nguarantees. We use both real hardware with Intel Optane DC persistent memory\nand simulation to evaluate a persistent version of hardware transactional\nmemory, a persistent version of software transactional memory, and undo/redo\nlogging. Through our empirical study, we show two major factors that impact the\ncost of supporting persistence in transactional systems: the persistence domain\n(transient or persistent caches) and application characteristics, such as\ntransaction size and parallelism.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 03:28:42 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Fernando", "Pradeep", ""], ["Calciu", "Irina", ""], ["Gandhi", "Jayneel", ""], ["Kolli", "Aasheesh", ""], ["Gavrilovska", "Ada", ""]]}, {"id": "2012.15762", "submitter": "Michael Whittaker", "authors": "Michael Whittaker, Ailidani Ailijiang, Aleksey Charapko, Murat\n  Demirbas, Neil Giridharan, Joseph M. Hellerstein, Heidi Howard, Ion Stoica,\n  Adriana Szekeres", "title": "Scaling Replicated State Machines with Compartmentalization [Technical\n  Report]", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State machine replication protocols, like MultiPaxos and Raft, are a critical\ncomponent of many distributed systems and databases. However, these protocols\noffer relatively low throughput due to several bottlenecked components.\nNumerous existing protocols fix different bottlenecks in isolation but fall\nshort of a complete solution. When you fix one bottleneck, another arises. In\nthis paper, we introduce compartmentalization, the first comprehensive\ntechnique to eliminate state machine replication bottlenecks.\nCompartmentalization involves decoupling individual bottlenecks into distinct\ncomponents and scaling these components independently. Compartmentalization has\ntwo key strengths. First, compartmentalization leads to strong performance. In\nthis paper, we demonstrate how to compartmentalize MultiPaxos to increase its\nthroughput by 6x on a write-only workload and 16x on a mixed read-write\nworkload. Unlike other approaches, we achieve this performance without the need\nfor specialized hardware. Second, compartmentalization is a technique, not a\nprotocol. Industry practitioners can apply compartmentalization to their\nprotocols incrementally without having to adopt a completely new protocol.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 17:38:39 GMT"}, {"version": "v2", "created": "Fri, 1 Jan 2021 19:29:17 GMT"}, {"version": "v3", "created": "Sun, 16 May 2021 17:48:04 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Whittaker", "Michael", ""], ["Ailijiang", "Ailidani", ""], ["Charapko", "Aleksey", ""], ["Demirbas", "Murat", ""], ["Giridharan", "Neil", ""], ["Hellerstein", "Joseph M.", ""], ["Howard", "Heidi", ""], ["Stoica", "Ion", ""], ["Szekeres", "Adriana", ""]]}, {"id": "2012.15800", "submitter": "David Doty", "authors": "David Doty, Mahsa Eftekhari, Eric Severson", "title": "A stable majority population protocol using logarithmic time and states", "comments": "We combined this paper with arXiv:2011.07392 and have a new updated\n  version at arXiv:2106.10201", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study population protocols, a model of distributed computing appropriate\nfor modeling well-mixed chemical reaction networks and other physical systems\nwhere agents exchange information in pairwise interactions, but have no control\nover their schedule of interaction partners. The well-studied *majority*\nproblem is that of determining in an initial population of $n$ agents, each\nwith one of two opinions $A$ or $B$, whether there are more $A$, more $B$, or a\ntie. A *stable* protocol solves this problem with probability 1 by eventually\nentering a configuration in which all agents agree on a correct consensus\ndecision of $A$, $B$, or $T$, from which the consensus cannot change. We\ndescribe a protocol that solves this problem using $O(\\log n)$ states ($\\log\n\\log n + O(1)$ bits of memory) and optimal expected time $O(\\log n)$. The\nnumber of states $O(\\log n)$ is known to be optimal for the class of stable\nprotocols that are \"output dominant\" and \"monotone\". These are two natural\nconstraints satisfied by our protocol, making it state-optimal for that class.\nWe use, and develop novel analysis of, a key technique called a \"fixed\nresolution clock\" due to Gasieniec, Stachowiak, and Uznanski, who showed a\nmajority protocol using $O(\\log n)$ time and states that has a positive\nprobability of error. Our protocol is *nonuniform*: the transition function has\nthe value $\\left \\lceil {\\log n} \\right \\rceil$ encoded in it. We show that the\nprotocol can be modified to be uniform, while increasing the state complexity\nto $\\Theta(\\log n \\log \\log n)$.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 18:22:13 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 03:48:04 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Doty", "David", ""], ["Eftekhari", "Mahsa", ""], ["Severson", "Eric", ""]]}, {"id": "2012.15831", "submitter": "Baturalp Buyukates", "authors": "Baturalp Buyukates and Sennur Ulukus", "title": "Timely Communication in Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.LG cs.NI eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a federated learning framework in which a parameter server (PS)\ntrains a global model by using $n$ clients without actually storing the client\ndata centrally at a cloud server. Focusing on a setting where the client\ndatasets are fast changing and highly temporal in nature, we investigate the\ntimeliness of model updates and propose a novel timely communication scheme.\nUnder the proposed scheme, at each iteration, the PS waits for $m$ available\nclients and sends them the current model. Then, the PS uses the local updates\nof the earliest $k$ out of $m$ clients to update the global model at each\niteration. We find the average age of information experienced by each client\nand numerically characterize the age-optimal $m$ and $k$ values for a given\n$n$. Our results indicate that, in addition to ensuring timeliness, the\nproposed communication scheme results in significantly smaller average\niteration times compared to random client selection without hurting the\nconvergence of the global learning task.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 18:52:08 GMT"}, {"version": "v2", "created": "Sat, 13 Mar 2021 16:59:25 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Buyukates", "Baturalp", ""], ["Ulukus", "Sennur", ""]]}]