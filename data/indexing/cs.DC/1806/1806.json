[{"id": "1806.00039", "submitter": "Andy Edmonds", "authors": "Andy Edmonds, Chris Woods, Ana Juan Ferrer, Juan Francisco Ribera,\n  Thomas Micheal Bohnert", "title": "Blip: JIT and Footloose On The Edge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge environments offer a number of advantages for software developers\nincluding the ability to create services which can offer lower latency, better\nprivacy, and reduced operational costs than traditional cloud hosted services.\nHowever large technical challenges exist, which prevent developers from\nutilising the Edge; complexities related to the heterogeneous nature of the\nEdge environment, issues with orchestration and application management and\nlastly, the inherent issues in creating decentralised distributed applications\nwhich operate at a large geographic scale. In this conceptual and architectural\npaper we envision a solution, Blip, which offers an easy to use programming and\noperational environment which addresses the these issues. It aims to remove the\ntechnical barriers which will inhibit the wider adoption Edge application\ndevelopment. This paper validates the Blip concept by demonstrating how it will\ndeliver on the advantages of the Edge for a familiar scenario.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 18:23:26 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Edmonds", "Andy", ""], ["Woods", "Chris", ""], ["Ferrer", "Ana Juan", ""], ["Ribera", "Juan Francisco", ""], ["Bohnert", "Thomas Micheal", ""]]}, {"id": "1806.00329", "submitter": "Ashraf Shahin", "authors": "Thanaa S. Alnusairi, Ashraf A. Shahin, Yassine Daadaa", "title": "Binary PSOGSA for Load Balancing Task Scheduling in Cloud Environment", "comments": null, "journal-ref": "International Journal of Advanced Computer Science and\n  Applications(IJACSA), 9(5), 2018", "doi": "10.14569/IJACSA.2018.090535", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In cloud environments, load balancing task scheduling is an important issue\nthat directly affects resource utilization. Unquestionably, load balancing\nscheduling is a serious aspect that must be considered in the cloud research\nfield due to the significant impact on both the back end and front end.\nWhenever an effective load balance has been achieved in the cloud, then good\nresource utilization will also be achieved. An effective load balance means\ndistributing the submitted workload over cloud VMs in a balanced way, leading\nto high resource utilization and high user satisfaction. In this paper, we\npropose a load balancing algorithm, Binary Load Balancing-Hybrid Particle Swarm\nOptimization and Gravitational Search Algorithm (Bin-LB-PSOGSA), which is a\nbio-inspired load balancing scheduling algorithm that efficiently enables the\nscheduling process to improve load balance level on VMs. The proposed algorithm\nfinds the best Task-to-Virtual machine mapping that is influenced by the length\nof submitted workload and VM processing speed. Results show that the proposed\nBin-LB-PSOGSA achieves better VM load average than the pure Bin-LB-PSO and\nother benchmark algorithms in terms of load balance level.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 13:09:22 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Alnusairi", "Thanaa S.", ""], ["Shahin", "Ashraf A.", ""], ["Daadaa", "Yassine", ""]]}, {"id": "1806.00469", "submitter": "Wei-Ting Chang", "authors": "Wei-Ting Chang, Ravi Tandon", "title": "On the Capacity of Secure Distributed Matrix Multiplication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix multiplication is one of the key operations in various engineering\napplications. Outsourcing large-scale matrix multiplication tasks to multiple\ndistributed servers or cloud is desirable to speed up computation. However,\nsecurity becomes an issue when these servers are untrustworthy. In this paper,\nwe study the problem of secure distributed matrix multiplication from\ndistributed untrustworthy servers. This problem falls in the category of secure\nfunction computation and has received significant attention in the cryptography\ncommunity. However, the fundamental limits of information-theoretically secure\nmatrix multiplication remain an open problem. We focus on\ninformation-theoretically secure distributed matrix multiplication with the\ngoal of characterizing the minimum communication overhead. The capacity of\nsecure matrix multiplication is defined as the maximum possible ratio of the\ndesired information and the total communication received from $N$ distributed\nservers. In particular, we study the following two models where we want to\nmultiply two matrices $A\\in\\mathbb{F}^{m\\times n}$ and $B\\in\\mathbb{F}^{n\\times\np}$: $(a)$ one-sided secure matrix multiplication with $\\ell$ colluding\nservers, in which $B$ is a public matrix available at all servers and $A$ is a\nprivate matrix. $(b)$ fully secure matrix multiplication with $\\ell$ colluding\nservers, in which both $A$ and $B$ are private matrices. The goal is to\nsecurely multiply $A$ and $B$ when any $\\ell$ servers can collude. For model\n$(a)$, we characterize the capacity as\n$C_{\\text{one-sided}}^{(\\ell)}=(N-\\ell)/N$ by providing a secure matrix\nmultiplication scheme and a matching converse. For model $(b)$, we propose a\nnovel scheme that lower bounds the capacity, i.e.,\n$C_{\\text{fully}}^{(\\ell)}\\geq (\\lceil \\sqrt{N}-\\ell \\rceil)^2/(\\lceil\n\\sqrt{N}-\\ell \\rceil+\\ell)^2$.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 17:59:16 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Chang", "Wei-Ting", ""], ["Tandon", "Ravi", ""]]}, {"id": "1806.00588", "submitter": "Xing Shi", "authors": "Xing Shi, Shizhen Xu, Kevin Knight", "title": "Fast Locality Sensitive Hashing for Beam Search on GPU", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a GPU-based Locality Sensitive Hashing (LSH) algorithm to speed up\nbeam search for sequence models. We utilize the winner-take-all (WTA) hash,\nwhich is based on relative ranking order of hidden dimensions and thus\nresilient to perturbations in numerical values. Our algorithm is designed by\nfully considering the underling architecture of CUDA-enabled GPUs\n(Algorithm/Architecture Co-design): 1) A parallel Cuckoo hash table is applied\nfor LSH code lookup (guaranteed O(1) lookup time); 2) Candidate lists are\nshared across beams to maximize the parallelism; 3) Top frequent words are\nmerged into candidate lists to improve performance. Experiments on 4\nlarge-scale neural machine translation models demonstrate that our algorithm\ncan achieve up to 4x speedup on softmax module, and 2x overall speedup without\nhurting BLEU on GPU.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 06:18:15 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Shi", "Xing", ""], ["Xu", "Shizhen", ""], ["Knight", "Kevin", ""]]}, {"id": "1806.00751", "submitter": "Pengcheng Yao", "authors": "Pengcheng Yao", "title": "An Efficient Graph Accelerator with Parallel Data Conflict Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-specific computing with the support of dedicated accelerator has\ngreatly boosted the graph processing in both efficiency and energy.\nNevertheless, their data conflict management is still sequential in essential\nwhen some vertex needs a large number of conflicting updates at the same time,\nleading to prohibitive performance degradation. This is particularly true for\nprocessing natural graphs.\n  In this paper, we have the insight that the atomic operations for the vertex\nupdating of many graph algorithms (e.g., BFS, PageRank and WCC) are typically\nincremental and simplex. This hence allows us to parallelize the conflicting\nvertex updates in an accumulative manner. We architect a novel graphspecific\naccelerator that can simultaneously process atomic vertex updates for massive\nparallelism on the conflicting data access while ensuring the correctness. A\nparallel accumulator is designed to remove the serialization in atomic\nprotection for conflicting vertex updates through merging their results in\nparallel. Our implementation on Xilinx Virtex UltraScale+ XCVU9P with a wide\nvariety of typical graph algorithms shows that our accelerator achieves an\naverage throughput by 2.36 GTEPS as well as up to 3.14x performance speedup in\ncomparison with state-of-the-art ForeGraph (with single-chip version).\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 08:34:34 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Yao", "Pengcheng", ""]]}, {"id": "1806.00760", "submitter": "Yu Huang", "authors": "Yu Huang", "title": "Efficient Time-Evolving Stream Processing at Scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-evolving stream datasets exist ubiquitously in many real-world\napplications where their inherent hot keys often evolve over times.\nNevertheless, few existing solutions can provide efficient load balance on\nthese time-evolving datasets while preserving low memory overhead. In this\npaper, we present a novel grouping approach (named FISH), which can provide the\nefficient time-evolving stream processing at scale. The key insight of this\nwork is that the keys of time-evolving stream data can have a skewed\ndistribution within any bounded distance of time interval. This enables to\naccurately identify the recent hot keys for the real-time load balance within a\nbounded scope. We therefore propose an epoch-based recent hot key\nidentification with specialized intra-epoch frequency counting (for maintaining\nlow memory overhead) and inter-epoch hotness decaying (for suppressing\nsuperfluous computation). We also propose to heuristically infer the accurate\ninformation of remote workers through computation rather than communication for\ncost-efficient worker assignment. We have integrated our approach into Apache\nStorm. Our results on a cluster of 128 nodes for both synthetic and real-world\nstream datasets show that FISH significantly outperforms state-of-the-art with\nthe average and the 99th percentile latency reduction by 87.12% and 76.34% (vs.\nW-Choices), and memory overhead reduction by 99.96% (vs. Shuffle Grouping).\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 10:08:42 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Huang", "Yu", ""]]}, {"id": "1806.00762", "submitter": "Xianliang Li", "authors": "Xianliang Li", "title": "Scaling Up Large-Scale Graph Processing for GPU-Accelerated\n  Heterogeneous Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Not only with the large host memory for supporting large scale graph\nprocessing, GPU-accelerated heterogeneous architecture can also provide a great\npotential for high-performance computing. However, few existing heterogeneous\nsystems can exploit both hardware advantages to enable the scale-up performance\nfor graph processing due to the limited CPU-GPU transmission efficiency.\n  In this paper, we investigate the transmission inefficiency problem of\nheterogeneous graph systems. Our key insight is that the transmission\nefficiency for heterogeneous graph processing can be greatly improved by simply\niterating each subgraph multiple times (rather than only once in prior work) in\nthe GPU, further enabling to obtain the improvable efficiency of heterogeneous\ngraph systems by enhancing GPU processing capability. We therefore present\nSeraph, with the highlights of {\\em pipelined} subgraph iterations and {\\em\npredictive} vertex updating, to cooperatively maximize the effective\ncomputations of GPU on graph processing. Our evaluation on a wide variety of\nlarge graph datasets shows that Seraph outperforms state-of-the-art\nheterogeneous graph systems by 5.42x (vs. Graphie) and 3.05x (vs. Garaph).\nFurther, Seraph can be significantly scaled up over Graphie as fed with more\ncomputing power for large-scale graph processing.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 10:17:08 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Li", "Xianliang", ""]]}, {"id": "1806.00764", "submitter": "Yehia Elkhatib PhD", "authors": "Luis M. Vaquero and Felix Cuadrado and Yehia Elkhatib and Jorge\n  Bernal-Bernabe and Satish N. Srirama and Mohamed Faten Zhani", "title": "Research Challenges in Nextgen Service Orchestration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fog/edge computing, function as a service, and programmable infrastructures,\nlike software-defined networking or network function virtualisation, are\nbecoming ubiquitously used in modern Information Technology infrastructures.\nThese technologies change the characteristics and capabilities of the\nunderlying computational substrate where services run (e.g. higher volatility,\nscarcer computational power, or programmability). As a consequence, the nature\nof the services that can be run on them changes too (smaller codebases, more\nfragmented state, etc.). These changes bring new requirements for service\norchestrators, which need to evolve so as to support new scenarios where a\nclose interaction between service and infrastructure becomes essential to\ndeliver a seamless user experience. Here, we present the challenges brought\nforward by this new breed of technologies and where current orchestration\ntechniques stand with regards to the new challenges. We also present a set of\npromising technologies that can help tame this brave new world.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 10:38:01 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 08:30:08 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Vaquero", "Luis M.", ""], ["Cuadrado", "Felix", ""], ["Elkhatib", "Yehia", ""], ["Bernal-Bernabe", "Jorge", ""], ["Srirama", "Satish N.", ""], ["Zhani", "Mohamed Faten", ""]]}, {"id": "1806.00777", "submitter": "Zhao Jin", "authors": "Jin Zhao", "title": "Efficient Two-Level Scheduling for Concurrent Graph Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapidly growing demand of graph processing in the real scene, they\nhave to efficiently handle massive concurrent jobs. Although existing work\nenable to efficiently handle single graph processing job, there are plenty of\nmemory access redundancy caused by ignoring the characteristic of data access\ncorrelations. Motivated such an observation, we proposed two-level scheduling\nstrategy in this paper, which enables to enhance the efficiency of data access\nand to accelerate the convergence speed of concurrent jobs. Firstly,\ncorrelations-aware job scheduling allows concurrent jobs to process the same\ngraph data in Cache, which fundamentally alleviates the challenge of CPU\nrepeatedly accessing the same graph data in memory. Secondly, multiple\npriority-based data scheduling provides the support of prioritized iteration\nfor concurrent jobs, which is based on the global priority generated by\nindividual priority of each job. Simultaneously, we adopt block priority\ninstead of fine-grained priority to schedule graph data to decrease the\ncomputation cost. In particular, two-level scheduling significantly advance\nover the state-of-the-art because it works in the interlayer between data and\nsystems.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 12:10:34 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Zhao", "Jin", ""]]}, {"id": "1806.00788", "submitter": "Paolo Missier", "authors": "Nicholas Tucci and Jacek Cala and Jannetta Steyn and Paolo Missier", "title": "Design and evaluation of a genomics variant analysis pipeline using GATK\n  Spark tools", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scalable and efficient processing of genome sequence data, i.e. for variant\ndiscovery, is key to the mainstream adoption of High Throughput technology for\ndisease prevention and for clinical use. Achieving scalability, however,\nrequires a significant effort to enable the parallel execution of the analysis\ntools that make up the pipelines. This is facilitated by the new Spark versions\nof the well-known GATK toolkit, which offer a black-box approach by\ntransparently exploiting the underlying Map Reduce architecture. In this paper\nwe report on our experience implementing a standard variant discovery pipeline\nusing GATK 4.0 with Docker-based deployment over a cluster. We provide a\npreliminary performance analysis, comparing the processing times and cost to\nthose of the new Microsoft Genomics Services.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 13:12:06 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Tucci", "Nicholas", ""], ["Cala", "Jacek", ""], ["Steyn", "Jannetta", ""], ["Missier", "Paolo", ""]]}, {"id": "1806.00834", "submitter": "Jonathan Marbaniang", "authors": "Jonathan Marbaniang, Shekhar Bhandakkar, Sathya Peri", "title": "Garbage Collection in Concurrent Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Garbage Collection in concurrent data structures, especially lock-free ones,\npose multiple design and consistency challenges. In this instance, we consider\nthe case of concurrent sets. A set is a collection of elements, where the\nelements are ordered and distinct. These two invariants are always maintained\nat every point in time. Currently, multiple implementations of concurrent sets\nalready exist. LazyList, Hand-over-hand List and Harris List are some of the\nwell-known implementations. However none of these implementations employ, or\nare concerned with garbage collection of deleted nodes. Instead each\nimplementation ignores deleted nodes or depends on the garbage collector of the\nlanguage to handle them. Additionally, Garbage collection in concurrent lists,\nthat use optimistic traversals or that are lock-free, is not trivial. For\nexample, in LazyList and Harris List, they allow a thread to traverse a node or\na sequence of nodes after these nodes have already been removed from the list,\nand hence possibly deleted. If deleted nodes are to be reused, this will\npotentially lead to the ABA problem. Moreover, some languages like C++ do not\nhave an inbuilt garbage collector. Some constructs like Shared Pointers provide\na limited garbage collection facility, but it degrades performance by a large\nscale. Integrating Shared Pointers into a concurrent code is also not a trivial\ntask. In this paper, we propose a new representation of a concurrent set,\nGCList, which employs inbuilt garbage collection. We propose a novel garbage\ncollection scheme that implements in-built memory reclamation whereby it reuses\ndeleted nodes from the list. We propose both lock-based and lock-free\nimplementations of GCList. The garbage collection scheme works in parallel with\nthe Set operations.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 17:00:12 GMT"}, {"version": "v2", "created": "Sat, 21 Jul 2018 09:19:31 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Marbaniang", "Jonathan", ""], ["Bhandakkar", "Shekhar", ""], ["Peri", "Sathya", ""]]}, {"id": "1806.00885", "submitter": "Zaid Alali", "authors": "Ehab Ababneh, Zaid Al-Ali, Sangtae Ha, Richard Han, Eric Keller", "title": "Elasticizing Linux via Joint Disaggregation of Memory and Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a set of operating system primitives which provides\na scaling abstraction to cloud applications in which they can transparently be\nenabled to support scaled execution across multiple physical nodes as resource\nneeds go beyond that available on a single machine. These primitives include\nstretch, to extend the address space of an application to a new node, push and\npull, to move pages between nodes as needed for execution and optimization, and\njump, to transfer execution in a very lightweight manner between nodes. This\njoint disaggregation of memory and computing allows for transparent elasticity,\nimproving an application's performance by capitalizing on the underlying\ndynamic infrastructure without needing an application re-write. We have\nimplemented these primitives in a Linux 2.6 kernel, collectively calling the\nextended operating system, ElasticOS. Our evaluation across a variety of\nalgorithms shows up to 10x improvement in performance over standard network\nswap.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 21:35:11 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Ababneh", "Ehab", ""], ["Al-Ali", "Zaid", ""], ["Ha", "Sangtae", ""], ["Han", "Richard", ""], ["Keller", "Eric", ""]]}, {"id": "1806.00907", "submitter": "Beibei Si", "authors": "Beibei Si", "title": "A Structure-aware Approach for Efficient Graph Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of the big data, graph are processed in an iterative manner,\nwhich incrementally described in the form of graph in big data applications.\nMost currently, graph processing methods treat the underlying map data as black\nboxes. However, as shown in experimental evaluation, graph structures often\nhave diversity, different graph processing methods are very sensitive to the\ngraph structure and show different performance for different data sets. Based\non this, a graph processing method for graph structure analysis is proposed in\nthis paper: (1) This paper calculates the vertex activity of a graph according\nto the in-degree and out-degree, and divide the corresponding vertices into the\nhot or cold partitions; (2) According to the change of graph structure caused\nby partial vertex convergence after iteration, this paper reclassifies the\npartitions, divides the lower active vertices into cold partition and reduces\nthe frequency of calculation, which thereby reducing the cache miss rate and\nthe I/O overhead caused by active vertices as well; (3) The partition with\nhighest vertex status degree are given a priority calculation in this paper. In\ndetail, more pronounced and more frequent vertices have higher processing\npriority. In this way, the convergence speed of the graph vertices is\naccelerated, and the running time of the graph algorithm in the big data\nenvironment is reduced. Our experiments show that compared with the latest\nsystem, the proposed method can double the performance of different graph\nalgorithms and data sets.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 00:49:50 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Si", "Beibei", ""]]}, {"id": "1806.00939", "submitter": "Qian Yu", "authors": "Qian Yu, Songze Li, Netanel Raviv, Seyed Mohammadreza Mousavi Kalan,\n  Mahdi Soltanolkotabi, Salman Avestimehr", "title": "Lagrange Coded Computing: Optimal Design for Resiliency, Security and\n  Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a scenario involving computations over a massive dataset stored\ndistributedly across multiple workers, which is at the core of distributed\nlearning algorithms. We propose Lagrange Coded Computing (LCC), a new framework\nto simultaneously provide (1) resiliency against stragglers that may prolong\ncomputations; (2) security against Byzantine (or malicious) workers that\ndeliberately modify the computation for their benefit; and (3)\n(information-theoretic) privacy of the dataset amidst possible collusion of\nworkers. LCC, which leverages the well-known Lagrange polynomial to create\ncomputation redundancy in a novel coded form across workers, can be applied to\nany computation scenario in which the function of interest is an arbitrary\nmultivariate polynomial of the input dataset, hence covering many computations\nof interest in machine learning. LCC significantly generalizes prior works to\ngo beyond linear computations. It also enables secure and private computing in\ndistributed settings, improving the computation and communication efficiency of\nthe state-of-the-art. Furthermore, we prove the optimality of LCC by showing\nthat it achieves the optimal tradeoff between resiliency, security, and\nprivacy, i.e., in terms of tolerating the maximum number of stragglers and\nadversaries, and providing data privacy against the maximum number of colluding\nworkers. Finally, we show via experiments on Amazon EC2 that LCC speeds up the\nconventional uncoded implementation of distributed least-squares linear\nregression by up to $13.43\\times$, and also achieves a\n$2.36\\times$-$12.65\\times$ speedup over the state-of-the-art straggler\nmitigation strategies.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 03:26:22 GMT"}, {"version": "v2", "created": "Wed, 13 Jun 2018 16:47:17 GMT"}, {"version": "v3", "created": "Wed, 2 Jan 2019 12:37:25 GMT"}, {"version": "v4", "created": "Mon, 1 Apr 2019 19:09:28 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Yu", "Qian", ""], ["Li", "Songze", ""], ["Raviv", "Netanel", ""], ["Kalan", "Seyed Mohammadreza Mousavi", ""], ["Soltanolkotabi", "Mahdi", ""], ["Avestimehr", "Salman", ""]]}, {"id": "1806.01087", "submitter": "Sourya Dey", "authors": "Sourya Dey, Diandian Chen, Zongyang Li, Souvik Kundu, Kuan-Wen Huang,\n  Keith M. Chugg and Peter A. Beerel", "title": "A Highly Parallel FPGA Implementation of Sparse Neural Network Training", "comments": "An abridged version of this work was accepted as a short paper (4\n  pages) at ReConFig: 2018 International Conference on Reconfigurable Computing\n  and FPGAs. This is the full version of this work", "journal-ref": "Int. Conf. Reconfigurable Computing and FPGAs (ReConFig), pp. 1-4,\n  2018", "doi": "10.1109/RECONFIG.2018.8641739", "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate an FPGA implementation of a parallel and reconfigurable\narchitecture for sparse neural networks, capable of on-chip training and\ninference. The network connectivity uses pre-determined, structured sparsity to\nsignificantly reduce complexity by lowering memory and computational\nrequirements. The architecture uses a notion of edge-processing, leading to\nefficient pipelining and parallelization. Moreover, the device can be\nreconfigured to trade off resource utilization with training time to fit\nnetworks and datasets of varying sizes. The combined effects of complexity\nreduction and easy reconfigurability enable significantly greater exploration\nof network hyperparameters and structures on-chip. As proof of concept, we show\nimplementation results on an Artix-7 FPGA.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 19:12:29 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2018 21:42:05 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Dey", "Sourya", ""], ["Chen", "Diandian", ""], ["Li", "Zongyang", ""], ["Kundu", "Souvik", ""], ["Huang", "Kuan-Wen", ""], ["Chugg", "Keith M.", ""], ["Beerel", "Peter A.", ""]]}, {"id": "1806.01103", "submitter": "Raphael Polig", "authors": "Raphael Polig, Kubilay Atasu, Laura Chiticariu, Christoph Hagleitner,\n  H. Peter Hofstee, Frederick R. Reiss, Eva Sitaridi, Huaiyu Zhu", "title": "Giving Text Analytics a Boost", "comments": null, "journal-ref": "IEEE Micro ( Volume: 34, Issue: 4, July-Aug. 2014 ) p. 6-14", "doi": "10.1109/MM.2014.69", "report-no": null, "categories": "cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The amount of textual data has reached a new scale and continues to grow at\nan unprecedented rate. IBM's SystemT software is a powerful text analytics\nsystem, which offers a query-based interface to reveal the valuable information\nthat lies within these mounds of data. However, traditional server\narchitectures are not capable of analyzing the so-called \"Big Data\" in an\nefficient way, despite the high memory bandwidth that is available. We show\nthat by using a streaming hardware accelerator implemented in reconfigurable\nlogic, the throughput rates of the SystemT's information extraction queries can\nbe improved by an order of magnitude. We present how such a system can be\ndeployed by extending SystemT's existing compilation flow and by using a\nmulti-threaded communication interface that can efficiently use the bandwidth\nof the accelerator.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 09:58:59 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Polig", "Raphael", ""], ["Atasu", "Kubilay", ""], ["Chiticariu", "Laura", ""], ["Hagleitner", "Christoph", ""], ["Hofstee", "H. Peter", ""], ["Reiss", "Frederick R.", ""], ["Sitaridi", "Eva", ""], ["Zhu", "Huaiyu", ""]]}, {"id": "1806.01104", "submitter": "Anirudh Seshadri", "authors": "Dhanasekar, Anirudh Seshadri, Sudharshan Srinivasan, Suryanarayanan,\n  Akash Sridhar", "title": "Consolidating the innovative concepts towards Exascale computing for\n  Co-Design of Co-Applications ll: Co-Design Automation - Workload\n  Characterization", "comments": "Revised Submission 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many-core co-design is a complex task in which application complexity design\nspace, heterogeneous many-core architecture design space, parallel programming\nlanguage design space, simulator design space and optimizer design space should\nget integrated through a binding process and these design spaces, an ensemble\nof what is called many-core co-design spaces. It is indispensable to build a\nco-design automation process to dominate over the co-design complexity to cut\ndown the turnaround time. The co-design automation is frame worked to\ncomprehend the dependencies across the many-core co-design spaces and devise\nthe logic behind these interdependencies using a set of algorithms. The\nsoftware modules of these algorithms and the rest from the many-core co-design\nspaces interact to crop up the power-performance optimized heterogeneous\nmany-core architecture specific for the simultaneous execution of co\napplications without space-time sharing. It is essential that such co-design\nautomation has a built-in user-customizable workload generator to benchmark the\nemerging many-core architecture. This customizability benefits the generation\nof complex workloads with the desired computation complexity, communication\ncomplexity, control flow complexity, and locality of reference, specified under\na distribution and established on quantitative models. In addition, the\ncustomizable workload model aids the generation of what is called computational\nand communication surges. None of the current day benchmark suites encompasses\napplications and kernels that can match the attributes of customizable workload\nmodel proposed in this paper. Aforementioned concepts are exemplified in, the\ncase study supported by simulation results gathered from the simulator.\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2018 09:47:51 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Dhanasekar", "", ""], ["Seshadri", "Anirudh", ""], ["Srinivasan", "Sudharshan", ""], ["Suryanarayanan", "", ""], ["Sridhar", "Akash", ""]]}, {"id": "1806.01105", "submitter": "Philippos Papaphilippou", "authors": "Philippos Papaphilippou", "title": "Performance tuning for deep learning on a many-core processor (master\n  thesis)", "comments": "A dissertation submitted to the University of Cambridge on June 16,\n  2017 in partial fulfilment of the requirements for the degree of Master of\n  Philosophy in Advanced Computer Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) are becoming very successful and popular\nfor a variety of applications. The Loki many-core processor architecture is\nvery promising for achieving specialised hardware performance and efficiency\nwhile being a general purpose solution. Loki combines many simple cores with\nincreased control for the programmer. This freedom can be exploited to produce\nmuch more efficient code than in conventional multiprocessors but it also\ncreates a very big design space for possible optimisations. In this project, I\nexplore possible optimisations for a CNN application, their portability on\ndifferent Loki-specific configurations, convolution parameters and inputs.\nFinally, I investigate the potential for adaptive algorithms for further\nperformance increase.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 11:01:28 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Papaphilippou", "Philippos", ""]]}, {"id": "1806.01106", "submitter": "Alejandro Linares-Barranco A. Linares-Barranco", "authors": "A. Rios-Navarro, R. Tapiador-Morales, A. Jimenez-Fernandez, M.\n  Dominguez-Morales, C. Amaya and A. Linares-Barranco", "title": "Performance evaluation over HW/SW co-design SoC memory transfers for a\n  CNN accelerator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many FPGAs vendors have recently included embedded processors in their\ndevices, like Xilinx with ARM-Cortex A cores, together with programmable logic\ncells. These devices are known as Programmable System on Chip (PSoC). Their ARM\ncores (embedded in the processing system or PS) communicates with the\nprogrammable logic cells (PL) using ARM-standard AXI buses. In this paper we\nanalyses the performance of exhaustive data transfers between PS and PL for a\nXilinx Zynq FPGA in a co-design real scenario for Convolutional Neural Networks\n(CNN) accelerator, which processes, in dedicated hardware, a stream of visual\ninformation from a neuromorphic visual sensor for classification. In the PS\nside, a Linux operating system is running, which recollects visual events from\nthe neuromorphic sensor into a normalized frame, and then it transfers these\nframes to the accelerator of multi-layered CNNs, and read results, using an\nAXI-DMA bus in a per-layer way. As these kind of accelerators try to process\ninformation as quick as possible, data bandwidth becomes critical and\nmaintaining a good balanced data throughput rate requires some considerations.\nWe present and evaluate several data partitioning techniques to improve the\nbalance between RX and TX transfer and two different ways of transfers\nmanagement: through a polling routine at the userlevel of the OS, and through a\ndedicated interrupt-based kernellevel driver. We demonstrate that for longer\nenough packets, the kernel-level driver solution gets better timing in\ncomputing a CNN classification example. Main advantage of using kernel-level\ndriver is to have safer solutions and to have tasks scheduling in the OS to\nmanage other important processes for our application, like frames collection\nfrom sensors and their normalization.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 08:54:15 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Rios-Navarro", "A.", ""], ["Tapiador-Morales", "R.", ""], ["Jimenez-Fernandez", "A.", ""], ["Dominguez-Morales", "M.", ""], ["Amaya", "C.", ""], ["Linares-Barranco", "A.", ""]]}, {"id": "1806.01107", "submitter": "Amir Yazdanbakhsh", "authors": "Amir Yazdanbakhsh, Hajar Falahati, Philip J. Wolfe, Kambiz Samadi, Nam\n  Sung Kim, and Hadi Esmaeilzadeh", "title": "GANAX: A Unified MIMD-SIMD Acceleration for Generative Adversarial\n  Networks", "comments": "Proceedings of the 45th International Symposium on Computer\n  Architecture (ISCA), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) are one of the most recent deep\nlearning models that generate synthetic data from limited genuine datasets.\nGANs are on the frontier as further extension of deep learning into many\ndomains (e.g., medicine, robotics, content synthesis) requires massive sets of\nlabeled data that is generally either unavailable or prohibitively costly to\ncollect. Although GANs are gaining prominence in various fields, there are no\naccelerators for these new models. In fact, GANs leverage a new operator,\ncalled transposed convolution, that exposes unique challenges for hardware\nacceleration. This operator first inserts zeros within the multidimensional\ninput, then convolves a kernel over this expanded array to add information to\nthe embedded zeros. Even though there is a convolution stage in this operator,\nthe inserted zeros lead to underutilization of the compute resources when a\nconventional convolution accelerator is employed. We propose the GANAX\narchitecture to alleviate the sources of inefficiency associated with the\nacceleration of GANs using conventional convolution accelerators, making the\nfirst GAN accelerator design possible. We propose a reorganization of the\noutput computations to allocate compute rows with similar patterns of zeros to\nadjacent processing engines, which also avoids inconsequential multiply-adds on\nthe zeros. This compulsory adjacency reclaims data reuse across these\nneighboring processing engines, which had otherwise diminished due to the\ninserted zeros. The reordering breaks the full SIMD execution model, which is\nprominent in convolution accelerators. Therefore, we propose a unified\nMIMD-SIMD design for GANAX that leverages repeated patterns in the computation\nto create distinct microprograms that execute concurrently in SIMD mode.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 22:54:29 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Yazdanbakhsh", "Amir", ""], ["Falahati", "Hajar", ""], ["Wolfe", "Philip J.", ""], ["Samadi", "Kambiz", ""], ["Kim", "Nam Sung", ""], ["Esmaeilzadeh", "Hadi", ""]]}, {"id": "1806.01108", "submitter": "Ellis Giles", "authors": "Ellis Giles, Kshitij Doshi, Peter Varman", "title": "Hardware Transactional Persistent Memory", "comments": "16 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emerging Persistent Memory technologies (also PM, Non-Volatile DIMMs, Storage\nClass Memory or SCM) hold tremendous promise for accelerating popular\ndata-management applications like in-memory databases. However, programmers now\nneed to deal with ensuring the atomicity of transactions on Persistent Memory\nresident data and maintaining consistency between the order in which processors\nperform stores and that in which the updated values become durable.\n  The problem is specially challenging when high-performance isolation\nmechanisms like Hardware Transactional Memory (HTM) are used for concurrency\ncontrol. This work shows how HTM transactions can be ordered correctly and\natomically into PM by the use of a novel software protocol combined with a\nPersistent Memory Controller, without requiring changes to processor cache\nhardware or HTM protocols. In contrast, previous approaches require significant\nchanges to existing processor microarchitectures. Our approach, evaluated using\nboth micro-benchmarks and the STAMP suite compares well with standard\n(volatile) HTM transactions. It also yields significant gains in throughput and\nlatency in comparison with persistent transactional locking.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 17:14:01 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Giles", "Ellis", ""], ["Doshi", "Kshitij", ""], ["Varman", "Peter", ""]]}, {"id": "1806.01109", "submitter": "Fuyuan Xiao", "authors": "Fuyuan Xiao and Masayoshi Aritsugi", "title": "An adaptive parallel processing strategy in complex event processing\n  systems over data streams", "comments": "6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient matching of incoming events of data streams to persistent queries\nis fundamental to event stream processing systems. These applications require\ndealing with high volume and continuous data streams with fast processing time\non distributed complex event processing (CEP) systems. Therefore, a\nwell-managed parallel processing technique is needed for improving the\nperformance of the system. However, the specific properties of pattern\noperators in the CEP systems increase the difficulties of the parallel\nprocessing problem. To address these issues, a parallelization model and an\nadaptive parallel processing strategy are proposed for the complex event\nprocessing by introducing a histogram, and utilizing the probability and queue\ntheory. The proposed strategy can estimate the optimal event splitting policy,\nwhich can suit the most recent workloads conditions such that the selected\npolicy has the least expected waiting time for further processing the arriving\nevents. The proposed strategy can keep the CEP system running fast under the\nvariation of the time window sizes of operators and input rates of streams.\nFinally, the utility of our work is demonstrated through the experiments on the\nStreamBase system.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 09:37:59 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Xiao", "Fuyuan", ""], ["Aritsugi", "Masayoshi", ""]]}, {"id": "1806.01110", "submitter": "Nikolay Malitsky", "authors": "Nikolay Malitsky, Ralph Castain, and Matt Cowan", "title": "Spark-MPI: Approaching the Fifth Paradigm of Cognitive Applications", "comments": "Deep Learning, HPC, PMIx", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decade, the fourth paradigm of data-intensive science rapidly\nbecame a major driving concept of multiple application domains encompassing and\ngenerating large-scale devices such as light sources and cutting edge\ntelescopes. The success of data-intensive projects subsequently triggered the\nnext generation of machine learning approaches. These new artificial\nintelligent systems clearly represent a paradigm shift from data processing\npipelines towards the fifth paradigm of composite cognitive applications\nrequiring the integration of Big Data processing platforms and HPC\ntechnologies. The paper addresses the existing impedance mismatch between\ndata-intensive and compute-intensive ecosystems by presenting the Spark-MPI\napproach based on the MPI Exascale Process Management Interface (PMIx). The\napproach is demonstrated within the context of hybrid MPI/GPU ptychographic\nimage reconstruction pipelines and distributed deep learning applications.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 00:21:40 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Malitsky", "Nikolay", ""], ["Castain", "Ralph", ""], ["Cowan", "Matt", ""]]}, {"id": "1806.01116", "submitter": "Huichen Yang", "authors": "Dan Andresen, William Hsu, Huichen Yang, and Adedolapo Okanlawon", "title": "Machine Learning for Predictive Analytics of Compute Cluster Jobs", "comments": "7 pages, CSC'18 - The 16th Int'l Conf on Scientific Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of predicting whether sufficient memory and CPU\nresources have been requested for jobs at submission time. For this purpose, we\nexamine the task of training a supervised machine learning system to predict\nthe outcome - whether the job will fail specifically due to insufficient\nresources - as a classification task. Sufficiently high accuracy, precision,\nand recall at this task facilitates more anticipatory decision support\napplications in the domain of HPC resource allocation. Our preliminary results\nusing a new test bed show that the probability of failed jobs is associated\nwith information freely available at job submission time and may thus be usable\nby a learning system for user modeling that gives personalized feedback to\nusers.\n", "versions": [{"version": "v1", "created": "Sun, 20 May 2018 03:21:12 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Andresen", "Dan", ""], ["Hsu", "William", ""], ["Yang", "Huichen", ""], ["Okanlawon", "Adedolapo", ""]]}, {"id": "1806.01117", "submitter": "Navjot Kukreja", "authors": "Navjot Kukreja, Jan H\\\"uckelheim, Gerard J. Gorman", "title": "Backpropagation for long sequences: beyond memory constraints with\n  constant overheads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Naive backpropagation through time has a memory footprint that grows linearly\nin the sequence length, due to the need to store each state of the forward\npropagation. This is a problem for large networks. Strategies have been\ndeveloped to trade memory for added computations, which results in a sublinear\ngrowth of memory footprint or computation overhead. In this work, we present a\nlibrary that uses asynchronous storing and prefetching to move data to and from\nslow and cheap stor- age. The library only stores and prefetches states as\nfrequently as possible without delaying the computation, and uses the optimal\nRevolve backpropagation strategy for the computations in between. The memory\nfootprint of the backpropagation can thus be reduced to any size (e.g. to fit\ninto DRAM), while the computational overhead is constant in the sequence\nlength, and only depends on the ratio between compute and transfer times on a\ngiven hardware. We show in experiments that by exploiting asyncronous data\ntransfer, our strategy is always at least as fast, and usually faster than the\npreviously studied \"optimal\" strategies.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 12:59:40 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Kukreja", "Navjot", ""], ["H\u00fcckelheim", "Jan", ""], ["Gorman", "Gerard J.", ""]]}, {"id": "1806.01214", "submitter": "Ivan Geffner", "authors": "Ittai Abraham, Danny Dolev, Ivan Geffner, Joseph Y. Halpern", "title": "Implementing Mediators with Asynchronous Cheap Talk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mediator can help non-cooperative agents obtain an equilibrium that may\notherwise not be possible. We study the ability of players to obtain the same\nequilibrium without a mediator, using only cheap talk, that is, nonbinding\npre-play communication. Previous work has considered this problem in a\nsynchronous setting. Here we consider the effect of asynchrony on the problem,\nand provide upper bounds for implementing mediators. Considering asynchronous\nenvironments introduces new subtleties, including exactly what solution concept\nis most appropriate and determining what move is played if the cheap talk goes\non forever. Different results are obtained depending on whether the move after\nsuch \"infinite play\" is under the control of the players or part of the\ndescription of the game.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 16:55:07 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Abraham", "Ittai", ""], ["Dolev", "Danny", ""], ["Geffner", "Ivan", ""], ["Halpern", "Joseph Y.", ""]]}, {"id": "1806.01270", "submitter": "Kai Rothauge", "authors": "Alex Gittens, Kai Rothauge, Shusen Wang, Michael W. Mahoney, Jey\n  Kottalam, Lisa Gerhardt, Prabhat, Michael Ringenburg, Kristyn Maschhoff", "title": "Alchemist: An Apache Spark <=> MPI Interface", "comments": "Accepted for publication in Concurrency and Computation: Practice and\n  Experience, Special Issue on the Cray User Group 2018. arXiv admin note: text\n  overlap with arXiv:1805.11800", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB physics.data-an stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Apache Spark framework for distributed computation is popular in the data\nanalytics community due to its ease of use, but its MapReduce-style programming\nmodel can incur significant overheads when performing computations that do not\nmap directly onto this model. One way to mitigate these costs is to off-load\ncomputations onto MPI codes. In recent work, we introduced Alchemist, a system\nfor the analysis of large-scale data sets. Alchemist calls MPI-based libraries\nfrom within Spark applications, and it has minimal coding, communication, and\nmemory overheads. In particular, Alchemist allows users to retain the\nproductivity benefits of working within the Spark software ecosystem without\nsacrificing performance efficiency in linear algebra, machine learning, and\nother related computations.\n  In this paper, we discuss the motivation behind the development of Alchemist,\nand we provide a detailed overview its design and usage. We also demonstrate\nthe efficiency of our approach on medium-to-large data sets, using some\nstandard linear algebra operations, namely matrix multiplication and the\ntruncated singular value decomposition of a dense matrix, and we compare the\nperformance of Spark with that of Spark+Alchemist. These computations are run\non the NERSC supercomputer Cori Phase 1, a Cray XC40.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 23:25:29 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Gittens", "Alex", ""], ["Rothauge", "Kai", ""], ["Wang", "Shusen", ""], ["Mahoney", "Michael W.", ""], ["Kottalam", "Jey", ""], ["Gerhardt", "Lisa", ""], ["Prabhat", "", ""], ["Ringenburg", "Michael", ""], ["Maschhoff", "Kristyn", ""]]}, {"id": "1806.01305", "submitter": "Arman Zaribafiyan", "authors": "Takeshi Yamazaki, Shunji Matsuura, Ali Narimani, Anushervon\n  Saidmuradov, Arman Zaribafiyan", "title": "Towards the Practical Application of Near-Term Quantum Computers in\n  Quantum Chemistry Simulations: A Problem Decomposition Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the aim of establishing a framework to efficiently perform the practical\napplication of quantum chemistry simulation on near-term quantum devices, we\nenvision a hybrid quantum--classical framework for leveraging problem\ndecomposition (PD) techniques in quantum chemistry. Specifically, we use PD\ntechniques to decompose a target molecular system into smaller subsystems\nrequiring fewer computational resources. In our framework, there are two levels\nof hybridization. At the first level, we use a classical algorithm to decompose\na target molecule into subsystems, and utilize a quantum algorithm to simulate\nthe quantum nature of the subsystems. The second level is in the quantum\nalgorithm. We consider the quantum--classical variational algorithm that\niterates between an expectation estimation using a quantum device and a\nparameter optimization using a classical device. We investigate three popular\nPD techniques for our hybrid approach: the fragment molecular-orbital (FMO)\nmethod, the divide-and-conquer (DC) technique, and the density matrix embedding\ntheory (DMET). We examine the efficacy of these techniques in correctly\ndifferentiating conformations of simple alkane molecules. In particular, we\nconsider the ratio between the number of qubits for PD and that of the full\nsystem; the mean absolute deviation; and the Pearson correlation coefficient\nand Spearman's rank correlation coefficient. Sampling error is introduced when\nexpectation values are measured on the quantum device. Therefore, we study how\nthis error affects the predictive performance of PD techniques. The present\nstudy is our first step to opening up the possibility of using quantum\nchemistry simulations at a scale close to the size of molecules relevant to\nindustry on near-term quantum hardware.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 18:11:59 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Yamazaki", "Takeshi", ""], ["Matsuura", "Shunji", ""], ["Narimani", "Ali", ""], ["Saidmuradov", "Anushervon", ""], ["Zaribafiyan", "Arman", ""]]}, {"id": "1806.01360", "submitter": "Mostafa Kishani", "authors": "Mostafa Kishani, Reza Eftekhari, and Hossein Asadi", "title": "Evaluating Impact of Human Errors on the Availability of Data Storage\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the effect of incorrect disk replacement\nservice on the availability of data storage systems. To this end, we first\nconduct Monte Carlo simulations to evaluate the availability of disk subsystem\nby considering disk failures and incorrect disk replacement service. We also\npropose a Markov model that corroborates the Monte Carlo simulation results. We\nfurther extend the proposed model to consider the effect of automatic disk\nfail-over policy. The results obtained by the proposed model show that\noverlooking the impact of incorrect disk replacement can result up to three\norders of magnitude unavailability underestimation. Moreover, this study\nsuggests that by considering the effect of human errors, the conventional\nbelieves about the dependability of different RAID mechanisms should be\nrevised. The results show that in the presence of human errors, RAID1 can\nresult in lower availability compared to RAID5.\n", "versions": [{"version": "v1", "created": "Sat, 26 May 2018 08:32:39 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Kishani", "Mostafa", ""], ["Eftekhari", "Reza", ""], ["Asadi", "Hossein", ""]]}, {"id": "1806.01430", "submitter": "Yoji Yamato", "authors": "Yoji Yamato, Tatsuya Demizu, Hirofumi Noguchi and Misao Kataoka", "title": "Study of Automatic GPU Offloading Technology for Open IoT", "comments": "6 pages, in Japanese, 5 figures, IEICE Technical Report, SC2018-10,\n  June 2018", "journal-ref": "IEICE Technical Report, SC2018-10, June 2018. (c) 2018 IEICE", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  IoT technologies have been progressed. Now Open IoT concept has attracted\nattentions which achieve various IoT services by integrating horizontal\nseparated devices and services. For Open IoT era, we have proposed the Tacit\nComputing technology to discover the devices with necessary data for users on\ndemand and use them dynamically. However, existing Tacit Computing does not\ncare about performance and operation cost. Therefore in this paper, we propose\nan automatic GPU offloading technology as an elementary technology of Tacit\nComputing which uses Genetic Algorithm to extract appropriate offload loop\nstatements to improve performances. We evaluate a C/C++ matrix manipulation to\nverify effectiveness of GPU offloading and confirm more than 35 times\nperformances within 1 hour tuning time.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 23:31:29 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Yamato", "Yoji", ""], ["Demizu", "Tatsuya", ""], ["Noguchi", "Hirofumi", ""], ["Kataoka", "Misao", ""]]}, {"id": "1806.01556", "submitter": "Haomiao Wang Mr.", "authors": "Haomiao Wang, Prabu Thiagaraj, Oliver Sinnen", "title": "Combining Multiple Optimised FPGA-based Pulsar Search Modules Using\n  OpenCL", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Field-Programmable Gate Arrays (FPGAs) are widely used in the central signal\nprocessing design of the Square Kilometre Array (SKA) as acceleration hardware.\nThe frequency domain acceleration search (FDAS) module is an important part of\nthe SKA1-MID pulsar search engine. To develop for a yet to be finalised\nhardware, for cross-discipline interoperability and to achieve fast\nprototyping, OpenCL as a high-level FPGA synthesis approach is employed to\ncreate the sub-modules of FDAS. The FT convolution and the harmonic-summing\nplus some other minor sub-modules are elements in the FDAS module that have\nbeen well-optimised separately before. In this paper, we explore the design\nspace of combining well-optimised designs, dealing with the ensuing need to\ntrade-off and compromise. Pipeline computing is employed to handle multiple\ninput arrays at high speed. The hardware target is to employ multiple high-end\nFPGAs to process the combined FDAS module. The results show interesting\nconsequences, where the best individual solutions are not necessarily the best\nsolutions for the speed of a pipeline where FPGA resources and memory bandwidth\nneed to be shared. By proposing multiple buffering techniques to the pipeline,\nthe combined FDAS module can achieve up to 2x speedup over implementations\nwithout pipeline computing. We perform an extensive experimental evaluation on\nmultiple FPGA boards (Arria 10) hosted in a workstation and compare to a\ntechnology comparable mid-range GPU.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 08:40:01 GMT"}, {"version": "v2", "created": "Fri, 29 Jun 2018 00:14:41 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Wang", "Haomiao", ""], ["Thiagaraj", "Prabu", ""], ["Sinnen", "Oliver", ""]]}, {"id": "1806.01611", "submitter": "Kiril Dichev", "authors": "Kiril Dichev and Kirk Cameron and Dimitrios Nikolopoulos", "title": "Energy-efficient localised rollback after failures via data flow\n  analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exascale systems will suffer failures hourly. HPC programmers rely mostly on\napplication-level checkpoint and a global rollback to recover. In recent years,\ntechniques reducing the number of rolling back processes have been implemented\nvia message logging. However, the log-based approaches have weaknesses, such as\nbeing dependent on complex modifications within an MPI implementation, and the\nfact that a full restart may be required in the general case. To address the\nlimitations of all log-based mechanisms, we return to checkpoint-only\nmechanisms, but advocate data-flow-driven recovery (DFR), a fundamentally\ndifferent approach relying on analysis of the data flow of iterative codes, and\nthe well-known concept of data-flow graphs. We demonstrate the effectiveness of\nDFR for an MPI stencil code to optimise rollback and reduce the overall energy\nconsumption by 10-12 % on idling nodes during localised rollback. We also\nprovide large-scale estimates for the energy savings of DFR compared to global\nrollback, which for stencil codes increase as n square for a process count n.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 11:17:29 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Dichev", "Kiril", ""], ["Cameron", "Kirk", ""], ["Nikolopoulos", "Dimitrios", ""]]}, {"id": "1806.01683", "submitter": "Kamel Abdelouahab", "authors": "Kamel Abdelouahab and Maxime Pelcat and Jocelyn Serot and Fran\\c{c}ois\n  Berry", "title": "Accelerating CNN inference on FPGAs: A Survey", "comments": "Cloning our HAL submission in ArXiv, Technical Report - Universite\n  Clermont Auvergne, January 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) are currently adopted to solve an ever\ngreater number of problems, ranging from speech recognition to image\nclassification and segmentation. The large amount of processing required by\nCNNs calls for dedicated and tailored hardware support methods. Moreover, CNN\nworkloads have a streaming nature, well suited to reconfigurable hardware\narchitectures such as FPGAs. The amount and diversity of research on the\nsubject of CNN FPGA acceleration within the last 3 years demonstrates the\ntremendous industrial and academic interest. This paper presents a\nstate-of-the-art of CNN inference accelerators over FPGAs. The computational\nworkloads, their parallelism and the involved memory accesses are analyzed. At\nthe level of neurons, optimizations of the convolutional and fully connected\nlayers are explained and the performances of the different methods compared. At\nthe network level, approximate computing and datapath optimization methods are\ncovered and state-of-the-art approaches compared. The methods and tools\ninvestigated in this survey represent the recent trends in FPGA CNN inference\naccelerators and will fuel the future advances on efficient hardware deep\nlearning.\n", "versions": [{"version": "v1", "created": "Sat, 26 May 2018 12:24:49 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Abdelouahab", "Kamel", ""], ["Pelcat", "Maxime", ""], ["Serot", "Jocelyn", ""], ["Berry", "Fran\u00e7ois", ""]]}, {"id": "1806.01775", "submitter": "Fuqiang Liu", "authors": "F. Liu, C. Liu and F.Bi", "title": "A Memristor based Unsupervised Neuromorphic System Towards Fast and\n  Energy-Efficient GAN", "comments": "8 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV cs.ET cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning has gained immense success in pushing today's artificial\nintelligence forward. To solve the challenge of limited labeled data in the\nsupervised learning world, unsupervised learning has been proposed years ago\nwhile low accuracy hinters its realistic applications. Generative adversarial\nnetwork (GAN) emerges as an unsupervised learning approach with promising\naccuracy and are under extensively study. However, the execution of GAN is\nextremely memory and computation intensive and results in ultra-low speed and\nhigh-power consumption. In this work, we proposed a holistic solution for fast\nand energy-efficient GAN computation through a memristor-based neuromorphic\nsystem. First, we exploited a hardware and software co-design approach to map\nthe computation blocks in GAN efficiently. We also proposed an efficient data\nflow for optimal parallelism training and testing, depending on the computation\ncorrelations between different computing blocks. To compute the unique and\ncomplex loss of GAN, we developed a diff-block with optimized accuracy and\nperformance. The experiment results on big data show that our design achieves\n2.8x speedup and 6.1x energy-saving compared with the traditional GPU\naccelerator, as well as 5.5x speedup and 1.4x energy-saving compared with the\nprevious FPGA-based accelerator.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 02:45:38 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 06:00:55 GMT"}, {"version": "v3", "created": "Fri, 30 Aug 2019 02:32:16 GMT"}, {"version": "v4", "created": "Sun, 8 Sep 2019 08:46:12 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Liu", "F.", ""], ["Liu", "C.", ""], ["Bi", "F.", ""]]}, {"id": "1806.01926", "submitter": "Johan Pouwelse", "authors": "Quinten Stokkink, Johan Pouwelse", "title": "Deployment of a Blockchain-Based Self-Sovereign Identity", "comments": "2018 IEEE International Conference on Blockchain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital identity is unsolved: after many years of research there is still no\ntrusted communication over the Internet. To provide identity within the context\nof mutual distrust, this paper presents a blockchain-based digital identity\nsolution. Without depending upon a single trusted third party, the proposed\nsolution achieves passport-level legally valid identity. This solution for\nmaking identities Self-Sovereign, builds on a generic provable claim model for\nwhich attestations of truth from third parties need to be collected. The claim\nmodel is then shown to be both blockchain structure and proof method agnostic.\nFour different implementations in support of these two claim model properties\nare shown to offer sub-second performance for claim creation and claim\nverification. Through the properties of Self-Sovereign Identity, legally valid\nstatus and acceptable performance, our solution is considered to be fit for\nadoption by the general public.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 20:19:17 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Stokkink", "Quinten", ""], ["Pouwelse", "Johan", ""]]}, {"id": "1806.02030", "submitter": "Amanda Bienz", "authors": "Amanda Bienz, William D. Gropp, and Luke N. Olson", "title": "Improving Performance Models for Irregular Point-to-Point Communication", "comments": "8 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel applications are often unable to take full advantage of emerging\nparallel architectures due to scaling limitations, which arise due to\ninter-process communication. Performance models are used to analyze the sources\nof communication costs. However, traditional models for point-to-point\ncommunication fail to capture the full cost of many irregular operations, such\nas sparse matrix methods. In this paper, a node-aware based model is presented.\nFurthermore, the model is extended to include communication queue search time\nas well as an additional parameter estimating network contention. The resulting\nmodel is applied to a variety of irregular communication patterns throughout\nmatrix operations, displaying improved accuracy over traditional models.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 06:49:15 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Bienz", "Amanda", ""], ["Gropp", "William D.", ""], ["Olson", "Luke N.", ""]]}, {"id": "1806.02284", "submitter": "Michele Dolfi", "authors": "Peter W J Staar, Michele Dolfi, Christoph Auer, Costas Bekas", "title": "Corpus Conversion Service: A Machine Learning Platform to Ingest\n  Documents at Scale", "comments": "Accepted paper at KDD 2018 conference", "journal-ref": null, "doi": "10.1145/3219819.3219834", "report-no": null, "categories": "cs.DL cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few decades, the amount of scientific articles and technical\nliterature has increased exponentially in size. Consequently, there is a great\nneed for systems that can ingest these documents at scale and make the\ncontained knowledge discoverable. Unfortunately, both the format of these\ndocuments (e.g. the PDF format or bitmap images) as well as the presentation of\nthe data (e.g. complex tables) make the extraction of qualitative and\nquantitive data extremely challenging. In this paper, we present a modular,\ncloud-based platform to ingest documents at scale. This platform, called the\nCorpus Conversion Service (CCS), implements a pipeline which allows users to\nparse and annotate documents (i.e. collect ground-truth), train\nmachine-learning classification algorithms and ultimately convert any type of\nPDF or bitmap-documents to a structured content representation format. We will\nshow that each of the modules is scalable due to an asynchronous microservice\narchitecture and can therefore handle massive amounts of documents.\nFurthermore, we will show that our capability to gather ground-truth is\naccelerated by machine-learning algorithms by at least one order of magnitude.\nThis allows us to both gather large amounts of ground-truth in very little time\nand obtain very good precision/recall metrics in the range of 99\\% with regard\nto content conversion to structured output. The CCS platform is currently\ndeployed on IBM internal infrastructure and serving more than 250 active users\nfor knowledge-engineering project engagements.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 09:44:07 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Staar", "Peter W J", ""], ["Dolfi", "Michele", ""], ["Auer", "Christoph", ""], ["Bekas", "Costas", ""]]}, {"id": "1806.02397", "submitter": "Amit Gajbhiye", "authors": "Amit Gajbhiye and Shailendra Singh", "title": "Resource Provisioning and Scheduling Algorithm for Meeting Cost and\n  Deadline-Constraints of Scientific Workflows in IaaS Clouds", "comments": "15 pages, 8 figures, This work is done in the year 2015 when the\n  first author was part of NITTTR, Bhopal, India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Infrastructure as a Service model of cloud computing is a desirable platform\nfor the execution of cost and deadline constrained workflow applications as the\nelasticity of cloud computing allows large-scale complex scientific workflow\napplications to scale dynamically according to their deadline requirements.\nHowever, scheduling of these multitask workflow jobs in a distributed computing\nenvironment is a computationally hard multi-objective combinatorial\noptimization problem. The critical challenge is to schedule the workflow tasks\nwhilst meeting user quality of service (QoS) requirements and the application's\ndeadline. The existing research work not only fails to address this challenge\nbut also do not incorporate the basic principles of elasticity and\nheterogeneity of computing resources in cloud environment. In this paper, we\npropose a resource provisioning and scheduling algorithm to schedule the\nworkflow applications on IaaS clouds to meet application deadline constraints\nwhile optimizing the execution cost. The proposed algorithm is based on the\nnature-inspired population based Intelligent Water Drop (IWD) optimization\nalgorithm. The experimental results in the simulated environment of CloudSim\nwith four real-world workflow applications demonstrates that IWD algorithm\nschedules workflow tasks with optimized cost within the specified deadlines.\nMoreover, the IWD algorithm converges fast to near optimal solution.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 19:30:14 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Gajbhiye", "Amit", ""], ["Singh", "Shailendra", ""]]}, {"id": "1806.02508", "submitter": "Chen Chen", "authors": "Chen Chen, Qizhen Weng, Wei Wang, Baochun Li, Bo Li", "title": "Semi-Dynamic Load Balancing: Efficient Distributed Learning in\n  Non-Dedicated Environments", "comments": null, "journal-ref": null, "doi": "10.1145/3419111.3421299", "report-no": null, "categories": "cs.DC cs.AI cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) models are increasingly trained in clusters with\nnon-dedicated workers possessing heterogeneous resources. In such scenarios,\nmodel training efficiency can be negatively affected by stragglers -- workers\nthat run much slower than others. Efficient model training requires eliminating\nsuch stragglers, yet for modern ML workloads, existing load balancing\nstrategies are inefficient and even infeasible. In this paper, we propose a\nnovel strategy called semi-dynamic load balancing to eliminate stragglers of\ndistributed ML workloads. The key insight is that ML workers shall be\nload-balanced at iteration boundaries, being non-intrusive to intra-iteration\nexecution. We develop LB-BSP based on such an insight, which is an integrated\nworker coordination mechanism that adapts workers' load to their instantaneous\nprocessing capabilities by right-sizing the sample batches at the\nsynchronization barriers. We have custom-designed the batch sizing algorithm\nrespectively for CPU and GPU clusters based on their own characteristics.\nLB-BSP has been implemented as a Python module for ML frameworks like\nTensorFlow and PyTorch. Our EC2 deployment confirms that LB-BSP is practical,\neffective and light-weight, and is able to accelerating distributed training by\nup to $54\\%$.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 04:15:58 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 14:37:47 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Chen", "Chen", ""], ["Weng", "Qizhen", ""], ["Wang", "Wei", ""], ["Li", "Baochun", ""], ["Li", "Bo", ""]]}, {"id": "1806.02596", "submitter": "Gregor Stefan Bankhamer", "authors": "Gregor Bankhamer and Robert Els\\\"asser and Dominik Kaaser and\n  Matja\\v{z} Krnc", "title": "Positive Aging Admits Fast Asynchronous Plurality Consensus", "comments": null, "journal-ref": null, "doi": "10.1145/3382734.3406506", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study distributed plurality consensus among $n$ nodes, each of which\ninitially holds one of $k$ opinions. The goal is to eventually agree on the\ninitially dominant opinion. We consider an asynchronous communication model in\nwhich each node is equipped with a random clock. Whenever the clock of a node\nticks, it may open communication channels to a constant number of other nodes,\nchosen uniformly at random or from a list of constantly many addresses acquired\nin previous steps. The tick rates and the delays for establishing communication\nchannels (channel delays) follow some probability distribution. Once a channel\nis established, communication between nodes can be performed instantaneously.\n  We consider distributions for the waiting times between ticks and channel\ndelays that have constant mean and the so-called positive aging property. In\nthis setting, asynchronous plurality consensus is fast: if the initial bias\nbetween the largest and second largest opinion is at least $\\sqrt{n}\\log n$,\nthen after $O(\\log\\log_\\alpha k\\cdot\\log k+\\log\\log n)$ time all but a $1/\n\\text{polylog } n$ fraction of nodes have the initial plurality opinion. Here\n$\\alpha$ denotes the initial ratio between the largest and second largest\nopinion. After additional $O(\\log n)$ steps all nodes have the same opinion\nw.h.p., and this result is tight.\n  If additionally the distributions satisfy a certain density property, which\nis common in many well-known distributions, we show that consensus is reached\nin $O(\\log \\log_\\alpha k + \\log \\log n)$ time for all but $n/\\text{polylog } n$\nnodes, w.h.p. This implies that for a large range of initial configurations\npartial consensus can be reached significantly faster in this asynchronous\ncommunication model than in the synchronous setting.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 10:08:26 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 12:34:41 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Bankhamer", "Gregor", ""], ["Els\u00e4sser", "Robert", ""], ["Kaaser", "Dominik", ""], ["Krnc", "Matja\u017e", ""]]}, {"id": "1806.02638", "submitter": "Michail Theofilatos", "authors": "Othon Michail, Paul G. Spirakis, Michail Theofilatos", "title": "Fast Approximate Counting and Leader Election in Populations", "comments": "16 pages, 5 figures, to be published in SIROCCO 2018 proceedings -\n  Brief Announcement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problems of leader election and population size counting for\npopulation protocols: networks of finite-state anonymous agents that interact\nrandomly under a uniform random scheduler. We show a protocol for leader\nelection that terminates in $O(\\log_m(n) \\cdot \\log_2 n)$ parallel time, where\n$m$ is a parameter, using $O(\\max\\{m,\\log n\\})$ states. By adjusting the\nparameter $m$ between a constant and $n$, we obtain a single leader election\nprotocol whose time and space can be smoothly traded off between $O(\\log^2 n)$\nto $O(\\log n)$ time and $O(\\log n)$ to $O(n)$ states. Finally, we give a\nprotocol which provides an upper bound $\\hat{n}$ of the size $n$ of the\npopulation, where $\\hat{n}$ is at most $n^a$ for some $a>1$. This protocol\nassumes the existence of a unique leader in the population and stabilizes in\n$\\Theta{(\\log{n})}$ parallel time, using constant number of states in every\nnode, except the unique leader which is required to use $\\Theta{(\\log^2{n})}$\nstates.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 12:21:29 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Michail", "Othon", ""], ["Spirakis", "Paul G.", ""], ["Theofilatos", "Michail", ""]]}, {"id": "1806.02698", "submitter": "Antonio Libri", "authors": "Antonio Libri, Andrea Bartolini, Luca Benini", "title": "DiG: Enabling Out-of-Band Scalable High-Resolution Monitoring for\n  Data-Center Analytics, Automation and Control (Extended)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data centers are increasing in size and complexity, and we need scalable\napproaches to support their automated analysis and control. Performance\ncounters and power consumption are their key \"vital signs\". State-of-the-Art\n(SoA) monitoring systems provide built-in tools to collect performance\nmeasurements, and custom solutions to get insight on their power consumption.\nHowever, with the increase in measurement resolution (in time and space) and\nthe ensuing huge amount of measurement data to handle, new challenges arise,\nsuch as bottlenecks on the network bandwidth, storage and software overhead on\nthe monitoring units. To face these challenges we propose a novel monitoring\nplatform for data centers, which enables real-time high-resolution profiling\n(i.e., all available performance counters and the entire signal bandwidth of\nthe power consumption at the plug - sampling up to 20us - with an error below\n1%) and analytics, both at the edge (node-level analysis) and on a centralized\nunit (cluster-level analysis). The monitoring infrastructure is completely\nout-of-band, scalable, technology agnostic and low cost, and it is already\ninstalled in a SoA high-performance compute cluster (i.e., D.A.V.I.D.E. - 18th\nin Green500 November 2017).\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 14:19:30 GMT"}, {"version": "v2", "created": "Wed, 17 Jul 2019 15:20:12 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Libri", "Antonio", ""], ["Bartolini", "Andrea", ""], ["Benini", "Luca", ""]]}, {"id": "1806.02939", "submitter": "Xingyu Zhou", "authors": "Xingyu Zhou and Jian Tan and Ness Shroff", "title": "Flexible Load Balancing with Multi-dimensional State-space Collapse:\n  Throughput and Heavy-traffic Delay Optimality", "comments": "26 pages 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Heavy traffic analysis for load balancing policies has relied heavily on the\ncondition of state-space collapse onto a single-dimensional line in previous\nworks. In this paper, via Lyapunov-drift analysis, we rigorously prove that\neven under a multi-dimensional state-space collapse, steady-state heavy-traffic\ndelay optimality can still be achieved for a general load balancing system.\nThis result directly implies that achieving steady-state heavy-traffic delay\noptimality simply requires that no server is idling while others are busy at\nheavy loads, thus complementing and extending the result obtained by diffusion\napproximations. Further, we explore the greater flexibility provided by\nallowing a multi-dimensional state-space collapse in designing new load\nbalancing policies that are both throughput optimal and heavy-traffic delay\noptimal in steady state. This is achieved by overcoming various technical\nchallenges, and the methods used in this paper could be of independent\ninterest.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 01:37:13 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Zhou", "Xingyu", ""], ["Tan", "Jian", ""], ["Shroff", "Ness", ""]]}, {"id": "1806.03103", "submitter": "Katina Kralevska", "authors": "Katina Kralevska and Danilo Gligoroski", "title": "An Explicit Construction of Systematic MDS Codes with Small\n  Sub-packetization for All-Node Repair", "comments": null, "journal-ref": "https://ieeexplore.ieee.org/document/8512021 2018", "doi": "10.1109/DASC/PiCom/DataCom/CyberSciTec.2018.00066", "report-no": "Published in IEEE DataCom 2018", "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An explicit construction of systematic MDS codes, called HashTag+ codes, with\narbitrary sub-packetization level for all-node repair is proposed. It is shown\nthat even for small sub-packetization levels, HashTag+ codes achieve the\noptimal MSR point for repair of any parity node, while the repair bandwidth for\na single systematic node depends on the sub-packetization level. Compared to\nother codes in the literature, HashTag+ codes provide from 20% to 40% savings\nin the average amount of data accessed and transferred during repair.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 12:03:50 GMT"}, {"version": "v2", "created": "Mon, 25 Jun 2018 21:02:41 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Kralevska", "Katina", ""], ["Gligoroski", "Danilo", ""]]}, {"id": "1806.03124", "submitter": "Xu Chen", "authors": "Xu Chen and Wenzhong Li and Sanglu Lu and Zhi Zhou and Xiaoming Fu", "title": "Efficient Resource Allocation for On-Demand Mobile-Edge Cloud Computing", "comments": "Xu Chen,Wenzhong Li,Sanglu Lu,Zhi Zhou,and Xiaoming Fu, \"Efficient\n  Resource Allocation for On-Demand Mobile-Edge Cloud Computing,\" IEEE\n  Transactions on Vehicular Technology, June 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile-edge cloud computing is a new paradigm to provide cloud computing\ncapabilities at the edge of pervasive radio access networks in close proximity\nto mobile users. Aiming at provisioning flexible on-demand mobile-edge cloud\nservice, in this paper we propose a comprehensive framework consisting of a\nresource-efficient computation offloading mechanism for users and a joint\ncommunication and computation (JCC) resource allocation mechanism for network\noperator. Specifically, we first study the resource-efficient computation\noffloading problem for a user, in order to reduce user's resource occupation by\ndetermining its optimal communication and computation resource profile with\nminimum resource occupation and meanwhile satisfying the QoS constraint. We\nthen tackle the critical problem of user admission control for JCC resource\nallocation, in order to properly select the set of users for resource demand\nsatisfaction. We show the admission control problem is NP-hard, and hence\ndevelop an efficient approximation solution of a low complexity by carefully\ndesigning the user ranking criteria and rigourously derive its performance\nguarantee. To prevent the manipulation that some users may untruthfully report\ntheir valuations in acquiring mobile-edge cloud service, we further resort to\nthe powerful tool of critical value approach to design truthful pricing scheme\nfor JCC resource allocation. Extensive performance evaluation demonstrates that\nthe proposed schemes can achieve superior performance for on-demand mobile-edge\ncloud computing.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 12:53:13 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Chen", "Xu", ""], ["Li", "Wenzhong", ""], ["Lu", "Sanglu", ""], ["Zhou", "Zhi", ""], ["Fu", "Xiaoming", ""]]}, {"id": "1806.03157", "submitter": "Joberto Martins", "authors": "Pedro Moraes, Rafael Reale (IFBA), Joberto Martins", "title": "A Publish/Subscribe QoS-aware Framework for Massive IoT Traffic\n  Orchestration", "comments": "https://lrsm.ibisc.univ-evry.fr/Advance2018/", "journal-ref": null, "doi": "10.5281/zenodo.1098298", "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet of Things (IoT) application deployment requires the allocation of\nresources such as virtual machines, storage, and network elements that must be\ndeployed over distinct infrastructures such as cloud computing, Cloud of Things\n(CoT), datacenters and backbone networks. For massive IoT data acquisition, a\ngateway-based data aggregation approach is commonly used featuring sensor/\nactuator seamless access and providing cache/ buffering and preprocessing\nfunctionality. In this perspective , gateways acting as producers need to\nallocate network resources to send IoT data to consumers. In this paper, it is\nproposed a Publish/-Subscribe (PubSub) quality of service (QoS) aware framework\n(PSIoT-Orch) that orchestrates IoT traffic and allocates network resources\nbetween aggregates and consumers for massive IoT traffic. PSIoT-Orch schedules\nIoT data flows based on its configured QoS requirements. Additionally , the\nframework allocates network resources (LSP/ bandwidth) over a controlled\nbackbone network with limited and constrained resources between IoT data users\nand consumers. Network resources are allocated using a Bandwidth Allocation\nModel (BAM) to achieve efficient network resource allocation for scheduled IoT\ndata streams. PSIoT-Orch adopts an ICN (Information-Centric Network) PubSub\narchitecture approach to handle IoT data transfers requests among framework\ncomponents. The proposed framework aims at gathering the inherent advantages of\nan ICN-centric approach using a PubSub message scheme while allocating\nresources efficiently keeping QoS awareness and handling restricted network\nresources (bandwidth) for massive IoT traffic.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 13:53:33 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Moraes", "Pedro", "", "IFBA"], ["Reale", "Rafael", "", "IFBA"], ["Martins", "Joberto", ""]]}, {"id": "1806.03210", "submitter": "Aleksey Charapko", "authors": "Murat Demirbas, Aleksey Charapko, Ailidani Ailijiang", "title": "Does The Cloud Need Stabilizing?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last decade has witnessed rapid proliferation of cloud computing. While\neven the smallest distributed programs (with 3-5 actions) produce many\nunanticipated error cases due to concurrency involved, it seems short of a\nmiracle these web-services are able to operate at those vast scales. In this\npaper, we explore the factors that contribute most to the high-availability of\ncloud computing services and examine where self-stabilization could fit in that\npicture.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 15:09:21 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Demirbas", "Murat", ""], ["Charapko", "Aleksey", ""], ["Ailijiang", "Ailidani", ""]]}, {"id": "1806.03224", "submitter": "Burt Holzman", "authors": "Mine Altunay, W. David Dagenhart, Stuart Fuess, Burt Holzman, Jim\n  Kowalkowski, Dmitry Litvintsev, Qiming Lu, Parag Mhashilkar, Alexander\n  Moibenko, Marc Paterno, Panagiotis Spentzouris, Steven Timm, Anthony Tiradani", "title": "Intelligently-automated facilities expansion with the HEPCloud Decision\n  Engine", "comments": "Accepted for 18th IEEE/ACM International Symposium on Cluster, Cloud\n  and Grid Computing (CCGrid 2018)", "journal-ref": null, "doi": null, "report-no": "FNAL CONF-18-051-CD", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The next generation of High Energy Physics experiments are expected to\ngenerate exabytes of data---two orders of magnitude greater than the current\ngeneration. In order to reliably meet peak demands, facilities must either plan\nto provision enough resources to cover the forecasted need, or find ways to\nelastically expand their computational capabilities. Commercial cloud and\nallocation-based High Performance Computing (HPC) resources both have explicit\nand implicit costs that must be considered when deciding when to provision\nthese resources, and to choose an appropriate scale. In order to support such\nprovisioning in a manner consistent with organizational business rules and\nbudget constraints, we have developed a modular intelligent decision support\nsystem (IDSS) to aid in the automatic provisioning of resources---spanning\nmultiple cloud providers, multiple HPC centers, and grid computing federations.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 15:38:58 GMT"}, {"version": "v2", "created": "Mon, 11 Jun 2018 14:33:30 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Altunay", "Mine", ""], ["Dagenhart", "W. David", ""], ["Fuess", "Stuart", ""], ["Holzman", "Burt", ""], ["Kowalkowski", "Jim", ""], ["Litvintsev", "Dmitry", ""], ["Lu", "Qiming", ""], ["Mhashilkar", "Parag", ""], ["Moibenko", "Alexander", ""], ["Paterno", "Marc", ""], ["Spentzouris", "Panagiotis", ""], ["Timm", "Steven", ""], ["Tiradani", "Anthony", ""]]}, {"id": "1806.03365", "submitter": "Michael Dinitz", "authors": "Michael Dinitz and Magn\\'us M. Halld\\'orsson and Calvin Newport", "title": "Distributed Algorithms for Minimum Degree Spanning Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The minimum degree spanning tree (MDST) problem requires the construction of\na spanning tree $T$ for graph $G=(V,E)$ with $n$ vertices, such that the\nmaximum degree $d$ of $T$ is the smallest among all spanning trees of $G$. In\nthis paper, we present two new distributed approximation algorithms for the\nMDST problem. Our first result is a randomized distributed algorithm that\nconstructs a spanning tree of maximum degree $\\hat d = O(d\\log{n})$. It\nrequires $O((D + \\sqrt{n}) \\log^2 n)$ rounds (w.h.p.), where $D$ is the graph\ndiameter, which matches (within log factors) the optimal round complexity for\nthe related minimum spanning tree problem. Our second result refines this\napproximation factor by constructing a tree with maximum degree $\\hat d = O(d +\n\\log{n})$, though at the cost of additional polylogarithmic factors in the\nround complexity. Although efficient approximation algorithms for the MDST\nproblem have been known in the sequential setting since the 1990's, our results\nare first efficient distributed solutions for this problem.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 22:21:53 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Dinitz", "Michael", ""], ["Halld\u00f3rsson", "Magn\u00fas M.", ""], ["Newport", "Calvin", ""]]}, {"id": "1806.03377", "submitter": "Deepak Narayanan", "authors": "Aaron Harlap, Deepak Narayanan, Amar Phanishayee, Vivek Seshadri,\n  Nikhil Devanur, Greg Ganger, Phil Gibbons", "title": "PipeDream: Fast and Efficient Pipeline Parallel DNN Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PipeDream is a Deep Neural Network(DNN) training system for GPUs that\nparallelizes computation by pipelining execution across multiple machines. Its\npipeline parallel computing model avoids the slowdowns faced by data-parallel\ntraining when large models and/or limited network bandwidth induce high\ncommunication-to-computation ratios. PipeDream reduces communication by up to\n95% for large DNNs relative to data-parallel training, and allows perfect\noverlap of communication and computation. PipeDream keeps all available GPUs\nproductive by systematically partitioning DNN layers among them to balance work\nand minimize communication, versions model parameters for backward pass\ncorrectness, and schedules the forward and backward passes of different inputs\nin round-robin fashion to optimize \"time to target accuracy\". Experiments with\nfive different DNNs on two different clusters show that PipeDream is up to 5x\nfaster in time-to-accuracy compared to data-parallel training.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 23:18:08 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Harlap", "Aaron", ""], ["Narayanan", "Deepak", ""], ["Phanishayee", "Amar", ""], ["Seshadri", "Vivek", ""], ["Devanur", "Nikhil", ""], ["Ganger", "Greg", ""], ["Gibbons", "Phil", ""]]}, {"id": "1806.03498", "submitter": "Elad Michael Schiller (PhD)", "authors": "Shlomi Dolev, Thomas Petig and Elad Michael Schiller", "title": "Self-Stabilizing and Private Distributed Shared Atomic Memory in\n  Seldomly Fair Message Passing Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of privately emulating shared memory in message-passing\nnetworks. The system includes clients that store and retrieve replicated\ninformation on N servers, out of which e are malicious. When a client access a\nmalicious server, the data field of that server response might be different\nthan the value it originally stored. However, all other control variables in\nthe server reply and protocol actions are according to the server algorithm.\nFor the coded atomic storage (CAS) algorithms by Cadambe et al., we present an\nenhancement that ensures no information leakage and malicious fault-tolerance.\n  We also consider recovery after the occurrence of transient faults that\nviolate the assumptions according to which the system is to behave. After their\nlast occurrence, transient faults leave the system in an arbitrary state (while\nthe program code stays intact). We present a self-stabilizing algorithm, which\nrecovers after the occurrence of transient faults. This addition to Cadambe et\nal. considers asynchronous settings as long as no transient faults occur. The\nrecovery from transient faults that bring the system counters (close) to their\nmaximal values may include the use of a global reset procedure, which requires\nthe system run to be controlled by a fair scheduler. After the recovery period,\nthe safety properties are provided for asynchronous system runs that are not\nnecessarily controlled by fair schedulers.\n  Since the recovery period is bounded and the occurrence of transient faults\nis extremely rare, we call this design criteria self-stabilization in the\npresence of seldom fairness. Our self-stabilizing algorithm uses a bounded\nstorage during asynchronous executions (that are not necessarily fair). To the\nbest of our knowledge, we are the first to address privacy and\nself-stabilization in the context of emulating atomic shared memory in\nnetworked systems.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 16:08:54 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Dolev", "Shlomi", ""], ["Petig", "Thomas", ""], ["Schiller", "Elad Michael", ""]]}, {"id": "1806.03791", "submitter": "Lingjiao Chen", "authors": "Lingjiao Chen and Hongyi Wang and Jinman Zhao and Dimitris\n  Papailiopoulos and Paraschos Koutris", "title": "The Effect of Network Width on the Performance of Large-batch Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed implementations of mini-batch stochastic gradient descent (SGD)\nsuffer from communication overheads, attributed to the high frequency of\ngradient updates inherent in small-batch training. Training with large batches\ncan reduce these overheads; however, large batches can affect the convergence\nproperties and generalization performance of SGD. In this work, we take a first\nstep towards analyzing how the structure (width and depth) of a neural network\naffects the performance of large-batch training. We present new theoretical\nresults which suggest that--for a fixed number of parameters--wider networks\nare more amenable to fast large-batch training compared to deeper ones. We\nprovide extensive experiments on residual and fully-connected neural networks\nwhich suggest that wider networks can be trained using larger batches without\nincurring a convergence slow-down, unlike their deeper variants.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 03:29:17 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Chen", "Lingjiao", ""], ["Wang", "Hongyi", ""], ["Zhao", "Jinman", ""], ["Papailiopoulos", "Dimitris", ""], ["Koutris", "Paraschos", ""]]}, {"id": "1806.03845", "submitter": "Pietro  Hiram Guzzi", "authors": "Pietro H Guzzi, Marianna Milano, Pierangelo Veltri, Mario Cannataro", "title": "HetNetAligner: Design and Implementation of an algorithm for\n  heterogeneous network alignment on Apache Spark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The importance of the use of networks to model and analyse biological data\nand the interplay of bio-molecules is widely recognised. Consequently, many\nalgorithms for the analysis and the comparison of networks (such as alignment\nalgorithms) have been developed in the past. Recently, many different\napproaches tried to integrate into a single model the interplay of different\nmolecules, such as genes, transcription factors and microRNAs. A possible\nformalism to model such scenario comes from node coloured networks (or\nheterogeneous networks) implemented as node/ edge-coloured graphs.\nConsequently, the need for the introduction of alignment algorithms able to\nanalyse heterogeneous networks arises. To the best of our knowledge, all the\nexisting algorithms are not able to mine heterogeneous networks. We propose a\ntwo-step alignment strategy that receives as input two heterogeneous networks\n(node-coloured graphs) and a similarity function among nodes of two networks\nextending the previous formulations. We first build a single alignment graph.\nThen we mine this graph extracting relevant subgraphs. Despite this simple\napproach, the analysis of such networks relies on graph and subgraph\nisomorphism and the size of the data is still growing. Therefore the use of\nhigh-performance data analytics framework is needed. We here present\nHetNetAligner a framework built on top of Apache Spark. We also implemented our\nalgorithm, and we tested it on some selected heterogeneous biological networks.\nPreliminary results confirm that our method may extract relevant knowledge from\nbiological data reducing the computational time.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 07:47:27 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Guzzi", "Pietro H", ""], ["Milano", "Marianna", ""], ["Veltri", "Pierangelo", ""], ["Cannataro", "Mario", ""]]}, {"id": "1806.03901", "submitter": "Rana Faisal Munir", "authors": "Rana Faisal Munir, Alberto Abell\\'o, Oscar Romero, Maik Thiele,\n  Wolfgang Lehner", "title": "A Cost-based Storage Format Selector for Materialization in Big Data\n  Frameworks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern big data frameworks (such as Hadoop and Spark) allow multiple users to\ndo large-scale analysis simultaneously. Typically, users deploy Data-Intensive\nWorkflows (DIWs) for their analytical tasks. These DIWs of different users\nshare many common parts (i.e, 50-80%), which can be materialized to reuse them\nin future executions. The materialization improves the overall processing time\nof DIWs and also saves computational resources. Current solutions for\nmaterialization store data on Distributed File Systems (DFS) by using a fixed\ndata format. However, a fixed choice might not be the optimal one for every\nsituation. For example, it is well-known that different data fragmentation\nstrategies (i.e., horizontal, vertical or hybrid) behave better or worse\naccording to the access patterns of the subsequent operations.\n  In this paper, we present a cost-based approach which helps deciding the most\nappropriate storage format in every situation. A generic cost-based storage\nformat selector framework considering the three fragmentation strategies is\npresented. Then, we use our framework to instantiate cost models for specific\nHadoop data formats (namely SequenceFile, Avro and Parquet), and test it with\nrealistic use cases. Our solution gives on average 33% speedup over\nSequenceFile, 11% speedup over Avro, 32% speedup over Parquet, and overall, it\nprovides upto 25% performance gain.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 10:58:04 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Munir", "Rana Faisal", ""], ["Abell\u00f3", "Alberto", ""], ["Romero", "Oscar", ""], ["Thiele", "Maik", ""], ["Lehner", "Wolfgang", ""]]}, {"id": "1806.03915", "submitter": "Cesar A. Uribe", "authors": "Pavel Dvurechensky, Darina Dvinskikh, Alexander Gasnikov, C\\'esar A.\n  Uribe, Angelia Nedi\\'c", "title": "Decentralize and Randomize: Faster Algorithm for Wasserstein Barycenters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the decentralized distributed computation of discrete approximations\nfor the regularized Wasserstein barycenter of a finite set of continuous\nprobability measures distributedly stored over a network. We assume there is a\nnetwork of agents/machines/computers, and each agent holds a private continuous\nprobability measure and seeks to compute the barycenter of all the measures in\nthe network by getting samples from its local measure and exchanging\ninformation with its neighbors. Motivated by this problem, we develop, and\nanalyze, a novel accelerated primal-dual stochastic gradient method for general\nstochastic convex optimization problems with linear equality constraints. Then,\nwe apply this method to the decentralized distributed optimization setting to\nobtain a new algorithm for the distributed semi-discrete regularized\nWasserstein barycenter problem. Moreover, we show explicit non-asymptotic\ncomplexity for the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 11:28:59 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2018 18:12:30 GMT"}, {"version": "v3", "created": "Thu, 20 Feb 2020 03:21:45 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Dvurechensky", "Pavel", ""], ["Dvinskikh", "Darina", ""], ["Gasnikov", "Alexander", ""], ["Uribe", "C\u00e9sar A.", ""], ["Nedi\u0107", "Angelia", ""]]}, {"id": "1806.04075", "submitter": "Qian Lin", "authors": "Hao Zhou, Ming Chen, Qian Lin, Yong Wang, Xiaobin She, Sifan Liu, Rui\n  Gu, Beng Chin Ooi, Junfeng Yang", "title": "Overload Control for Scaling WeChat Microservices", "comments": "This version has updated the adaptive admission control algorithm by\n  merging the errata published in v2", "journal-ref": "ACM SoCC (2018) 149-161", "doi": "10.1145/3267809.3267823", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective overload control for large-scale online service system is crucial\nfor protecting the system backend from overload. Conventionally, the design of\noverload control is ad-hoc for individual service. However, service-specific\noverload control could be detrimental to the overall system due to intricate\nservice dependencies or flawed implementation of service. Service developers\nusually have difficulty to accurately estimate the dynamics of actual workload\nduring the development of service. Therefore, it is essential to decouple the\noverload control from service logic. In this paper, we propose DAGOR, an\noverload control scheme designed for the account-oriented microservice\narchitecture. DAGOR is service agnostic and system-centric. It manages overload\nat the microservice granule such that each microservice monitors its load\nstatus in real time and triggers load shedding in a collaborative manner among\nits relevant services when overload is detected. DAGOR has been used in the\nWeChat backend for five years. Experimental results show that DAGOR can benefit\nhigh success rate of service even when the system is experiencing overload,\nwhile ensuring fairness in the overload control.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 15:58:30 GMT"}, {"version": "v2", "created": "Fri, 21 Dec 2018 05:22:21 GMT"}, {"version": "v3", "created": "Mon, 24 Dec 2018 04:29:41 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Zhou", "Hao", ""], ["Chen", "Ming", ""], ["Lin", "Qian", ""], ["Wang", "Yong", ""], ["She", "Xiaobin", ""], ["Liu", "Sifan", ""], ["Gu", "Rui", ""], ["Ooi", "Beng Chin", ""], ["Yang", "Junfeng", ""]]}, {"id": "1806.04090", "submitter": "Zachary Charles", "authors": "Hongyi Wang, Scott Sievert, Zachary Charles, Shengchao Liu, Stephen\n  Wright, Dimitris Papailiopoulos", "title": "ATOMO: Communication-efficient Learning via Atomic Sparsification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed model training suffers from communication overheads due to\nfrequent gradient updates transmitted between compute nodes. To mitigate these\noverheads, several studies propose the use of sparsified stochastic gradients.\nWe argue that these are facets of a general sparsification method that can\noperate on any possible atomic decomposition. Notable examples include\nelement-wise, singular value, and Fourier decompositions. We present ATOMO, a\ngeneral framework for atomic sparsification of stochastic gradients. Given a\ngradient, an atomic decomposition, and a sparsity budget, ATOMO gives a random\nunbiased sparsification of the atoms minimizing variance. We show that recent\nmethods such as QSGD and TernGrad are special cases of ATOMO and that\nsparsifiying the singular value decomposition of neural networks gradients,\nrather than their coordinates, can lead to significantly faster distributed\ntraining.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 16:23:14 GMT"}, {"version": "v2", "created": "Sun, 24 Jun 2018 03:49:12 GMT"}, {"version": "v3", "created": "Thu, 8 Nov 2018 20:04:34 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Wang", "Hongyi", ""], ["Sievert", "Scott", ""], ["Charles", "Zachary", ""], ["Liu", "Shengchao", ""], ["Wright", "Stephen", ""], ["Papailiopoulos", "Dimitris", ""]]}, {"id": "1806.04207", "submitter": "Shi Pu", "authors": "Shi Pu and Alfredo Garcia", "title": "Swarming for Faster Convergence in Stochastic Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.MA cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a distributed framework for stochastic optimization which is\ninspired by models of collective motion found in nature (e.g., swarming) with\nmild communication requirements. Specifically, we analyze a scheme in which\neach one of $N > 1$ independent threads, implements in a distributed and\nunsynchronized fashion, a stochastic gradient-descent algorithm which is\nperturbed by a swarming potential. Assuming the overhead caused by\nsynchronization is not negligible, we show the swarming-based approach exhibits\nbetter performance than a centralized algorithm (based upon the average of $N$\nobservations) in terms of (real-time) convergence speed. We also derive an\nerror bound that is monotone decreasing in network size and connectivity. We\ncharacterize the scheme's finite-time performances for both convex and\nnon-convex objective functions.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 19:24:59 GMT"}, {"version": "v2", "created": "Mon, 6 Aug 2018 20:02:49 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Pu", "Shi", ""], ["Garcia", "Alfredo", ""]]}, {"id": "1806.04211", "submitter": "Alice Niemeyer", "authors": "Stephen Linton, Gabriele Nebe, Alice Niemeyer, Richard Parker, Jon\n  Thackray", "title": "A parallel algorithm for Gaussian elimination over finite fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.RA cs.DC cs.NA math.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe a parallel Gaussian elimination algorithm for\nmatrices with entries in a finite field. Unlike previous approaches, our\nalgorithm subdivides a very large input matrix into smaller submatrices by\nsubdividing both rows and columns into roughly square blocks sized so that\ncomputing with individual blocks on individual processors provides adequate\nconcurrency. The algorithm also returns the transformation matrix, which\nencodes the row operations used. We go to some lengths to avoid storing any\nunnecessary data as we keep track of the row operations, such as block columns\nof the transformation matrix known to be zero. The algorithm is accompanied by\na concurrency analysis which shows that the improvement in concurrency is of\nthe same order of magnitude as the number of blocks. An implementation of the\nalgorithm has been tested on matrices as large as $1 000 000\\times 1 000 000$\nover small finite fields.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 10:33:55 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Linton", "Stephen", ""], ["Nebe", "Gabriele", ""], ["Niemeyer", "Alice", ""], ["Parker", "Richard", ""], ["Thackray", "Jon", ""]]}, {"id": "1806.04328", "submitter": "Ali Mashreghi", "authors": "Ali Mashreghi, Valerie King", "title": "Broadcast and minimum spanning tree with $o(m)$ messages in the\n  asynchronous CONGEST model", "comments": "Submitted to DISC 2018 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide the first asynchronous distributed algorithms to compute broadcast\nand minimum spanning tree with $o(m)$ bits of communication, in a graph with\n$n$ nodes and $m$ edges. For decades, it was believed that $\\Omega(m)$ bits of\ncommunication are required for any algorithm that constructs a broadcast tree.\nIn 2015, King, Kutten and Thorup showed that in the KT1 model where nodes have\ninitial knowledge of their neighbors' identities it is possible to construct\nMST in $\\tilde{O}(n)$ messages in the synchronous CONGEST model. In the CONGEST\nmodel messages are of size $O(\\log n)$. However, no algorithm with $o(m)$\nmessages were known for the asynchronous case. Here, we provide an algorithm\nthat uses $O(n^{3/2} \\log^{3/2} n)$ messages to find MST in the asynchronous\nCONGEST model. Our algorithm is randomized Monte Carlo and outputs MST with\nhigh probability. We will provide an algorithm for computing a spanning tree\nwith $O(n^{3/2} \\log^{3/2} n)$ messages. Given a spanning tree, we can compute\nMST with $\\tilde{O}(n)$ messages.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 04:35:05 GMT"}, {"version": "v2", "created": "Sun, 27 Jan 2019 04:22:56 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Mashreghi", "Ali", ""], ["King", "Valerie", ""]]}, {"id": "1806.04506", "submitter": "Yang Li", "authors": "Yang Li, Di Wang, Saugata Ghose, Jie Liu, Sriram Govindan, Sean James,\n  Eric Peterson, John Siegler, Rachata Ausavarungnirun, and Onur Mutlu", "title": "Techniques for Efficiently Handling Power Surges in Fuel Cell Powered\n  Data Centers: Modeling, Analysis, Results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fuel cells are a promising power source for future data centers, offering\nhigh energy efficiency, low greenhouse gas emissions, and high reliability.\nHowever, due to mechanical limitations related to fuel delivery, fuel cells are\nslow to adjust to sudden increases in data center power demands, which can\nresult in temporary power shortfalls. To mitigate the impact of power\nshortfalls, prior work has proposed to either perform power capping by\nthrottling the servers, or to leverage energy storage devices (ESDs) that can\ntemporarily provide enough power to make up for the shortfall while the fuel\ncells ramp up power generation. Both approaches have disadvantages: power\ncapping conservatively limits server performance and can lead to service level\nagreement (SLA) violations, while ESD-only solutions must significantly\noverprovision the energy storage device capacity to tolerate the shortfalls\ncaused by the worst-case (i.e., largest) power surges, which greatly increases\nthe total cost of ownership (TCO).\n  We propose SizeCap, the first ESD sizing framework for fuel cell powered data\ncenters, which coordinates ESD sizing with power capping to enable a\ncost-effective solution to power shortfalls in data centers. SizeCap sizes the\nESD just large enough to cover the majority of power surges, but not the\nworst-case surges that occur infrequently, to greatly reduce TCO. It then uses\nthe smaller capacity ESD in conjunction with power capping to cover the power\nshortfalls caused by the worst-case power surges. As part of our new flexible\nframework, we propose multiple power capping policies with different degrees of\nawareness of fuel cell and workload behavior, and evaluate their impact on\nworkload performance and ESD size. Using traces from Microsoft's production\ndata center systems, we demonstrate that SizeCap significantly reduces the ESD\nsize without violating any SLAs.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 13:39:31 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Li", "Yang", ""], ["Wang", "Di", ""], ["Ghose", "Saugata", ""], ["Liu", "Jie", ""], ["Govindan", "Sriram", ""], ["James", "Sean", ""], ["Peterson", "Eric", ""], ["Siegler", "John", ""], ["Ausavarungnirun", "Rachata", ""], ["Mutlu", "Onur", ""]]}, {"id": "1806.04528", "submitter": "Martin Pil\\'at", "authors": "\\v{S}t\\v{e}p\\'an Balcar, Martin Pil\\'at", "title": "Online Parallel Portfolio Selection with Heterogeneous Island Model", "comments": "8 pages, submitted to ICTAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an online parallel portfolio selection algorithm based on the\nisland model commonly used for parallelization of evolutionary algorithms. In\nour case each of the islands runs a different optimization algorithm. The\ndistributed computation is managed by a central planner which periodically\nchanges the running methods during the execution of the algorithm -- less\nsuccessful methods are removed while new instances of more successful methods\nare added.\n  We compare different types of planners in the heterogeneous island model\namong themselves and also to the traditional homogeneous model on a wide set of\nproblems. The tests include experiments with different representations of the\nindividuals and different duration of fitness function evaluations. The results\nshow that heterogeneous models are a more general and universal computational\ntool compared to homogeneous models.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 13:57:31 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Balcar", "\u0160t\u011bp\u00e1n", ""], ["Pil\u00e1t", "Martin", ""]]}, {"id": "1806.04544", "submitter": "Gabriele D'Angelo", "authors": "Gabriele D'Angelo, Stefano Ferretti, Moreno Marzolla", "title": "A Blockchain-based Flight Data Recorder for Cloud Accountability", "comments": "1st Workshop on Cryptocurrencies and Blockchains for Distributed\n  Systems (CryBlock 2018)", "journal-ref": null, "doi": "10.1145/3211933.3211950", "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many companies rely on Cloud infrastructures for their computation,\ncommunication and data storage requirements. While Cloud services provide some\nbenefits, e.g., replacing high upfront costs for an IT infrastructure with a\npay-as-you-go model, they also introduce serious concerns that are notoriously\ndifficult to address. In essence, Cloud customers are storing data and running\ncomputations on infrastructures that they can not control directly. Therefore,\nwhen problems arise -- violations of Service Level Agreements, data corruption,\ndata leakage, security breaches -- both customers and Cloud providers face the\nchallenge of agreeing on which party is to be held responsible. In this paper,\nwe review the challenges and requirements for enforcing accountability in Cloud\ninfrastructures, and argue that smart contracts and blockchain technologies\nmight provide a key contribution towards accountable Clouds.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 14:08:25 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["D'Angelo", "Gabriele", ""], ["Ferretti", "Stefano", ""], ["Marzolla", "Moreno", ""]]}, {"id": "1806.04780", "submitter": "Yuanhao Wei", "authors": "Naama Ben-David, Guy E. Blelloch, Michal Friedman, Yuanhao Wei", "title": "Delay-Free Concurrency on Faulty Persistent Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-volatile memory (NVM) promises persistent main memory that remains\ncorrect despite loss of power. This has sparked a line of research into\nalgorithms that can recover from a system crash. Since caches are expected to\nremain volatile, concurrent data structures and algorithms must be redesigned\nto guarantee that they are left in a consistent state after a system crash, and\nthat the execution can be continued upon recovery. However, the prospect of\nredesigning every concurrent data structure or algorithm before it can be used\nin NVM architectures is daunting.\n  In this paper, we present a construction that takes any concurrent program\nwith reads, writes and CASs to shared memory and makes it persistent, i.e., can\nbe continued after one or more processes fault and have to restart. Importantly\nthe converted algorithm has constant computational delay (preserves instruction\ncounts on each process within a constant factor), as well as constant recovery\ndelay (a process can recover from a fault in a constant number of\ninstructions). We show this first for a simple transformation, and then present\noptimizations to make it more practical, allowing for a tradeoff for better\nconstant factors in computational delay, for sometimes increased recovery\ndelay. We also provide an optimized transformation that works for any\nnormalized lock-free data structure, thus allowing more efficient constructions\nfor a large class of concurrent algorithms. We experimentally evaluate our\ntransformations by applying them to a queue.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 21:52:00 GMT"}, {"version": "v2", "created": "Thu, 17 Jan 2019 22:25:52 GMT"}, {"version": "v3", "created": "Thu, 18 Jun 2020 21:18:57 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Ben-David", "Naama", ""], ["Blelloch", "Guy E.", ""], ["Friedman", "Michal", ""], ["Wei", "Yuanhao", ""]]}, {"id": "1806.04859", "submitter": "H{\\aa}kan Grahn", "authors": "Max Danielsson, H{\\aa}kan Grahn, Thomas Sievert, Jim Rasmusson", "title": "Comparing Two Generations of Embedded GPUs Running a Feature Detection\n  Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Graphics processing units (GPUs) in embedded mobile platforms are reaching\nperformance levels where they may be useful for computer vision applications.\nWe compare two generations of embedded GPUs for mobile devices when running a\nstate-of-the-art feature detection algorithm, i.e., Harris-Hessian/FREAK. We\ncompare architectural differences, execution time, temperature, and frequency\non Sony Xperia Z3 and Sony Xperia XZ mobile devices. Our results indicate that\nthe performance soon is sufficient for real-time feature detection, the GPUs\nhave no temperature problems, and support for large work-groups is important.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 06:08:13 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Danielsson", "Max", ""], ["Grahn", "H\u00e5kan", ""], ["Sievert", "Thomas", ""], ["Rasmusson", "Jim", ""]]}, {"id": "1806.05135", "submitter": "Andrea Borghesi", "authors": "Andrea Borghesi, Andrea Bartolini, Michela Milano, Luca Benini", "title": "Pricing Schemes for Energy-Efficient HPC Systems: Design and Exploration", "comments": null, "journal-ref": "The International Journal of High Performance Computing\n  Applications. Volume: 33 issue: 4, page(s): 716-734 , 2019", "doi": "10.1177/1094342018814593", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy efficiency is of paramount importance for the sustainability of HPC\nsystems. Energy consumption limits the peak performance of supercomputers and\naccounts for a large share of total cost of ownership. Consequently, system\nowners and final users have started exploring mechanisms to trade off\nperformance for power consumption, for example through frequency and voltage\nscaling.\n  However, only a limited number of studies have been devoted to explore the\neconomic viability of performance scaling solutions and to devise pricing\nmechanisms fostering a more energy-conscious usage of resources, without\nadversely impacting return-of-investment on the HPC facility. We present a\nparametrized model to analyze the impact of frequency scaling on energy and to\nassess the potential total cost benefits for the HPC facility and the user. We\nevaluate four pricing schemes, considering both facility manager and the user\nperspectives. We then perform a design space exploration considering current\nand near-future HPC systems and technologies.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 16:33:39 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Borghesi", "Andrea", ""], ["Bartolini", "Andrea", ""], ["Milano", "Michela", ""], ["Benini", "Luca", ""]]}, {"id": "1806.05300", "submitter": "Doug Woos", "authors": "Doug Woos, Zachary Tatlock, Michael D. Ernst, Thomas E. Anderson", "title": "A Graphical Interactive Debugger for Distributed Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing and debugging distributed systems is notoriously difficult. The\ncorrectness of a distributed system is largely determined by its handling of\nfailure scenarios. The sequence of events leading to a bug can be long and\ncomplex, and it is likely to include message reorderings and failures. On\nsingle-node systems, interactive debuggers enable stepping through an execution\nof the program, but they lack the ability to easily simulate failure scenarios\nand control the order in which messages are delivered.\n  Oddity is a graphical, interactive debugger for distributed systems. It\nbrings the power of traditional step-through debugging---fine-grained control\nand observation of a program as it executes---to distributed systems. It also\nenables exploratory testing, in which an engineer examines and perturbs the\nbehavior of a system in order to better understand it, perhaps without a\nspecific bug in mind. A programmer can directly control message and failure\ninterleaving. Oddity supports time travel, allowing a developer to explore\nmultiple branching executions of a system within a single debugging session.\nAbove all, Oddity encourages distributed systems thinking: rather than assuming\nthe normal case and attaching failure handling as an afterthought, distributed\nsystems should be developed around the certainty of message loss and node\nfailure.\n  Graduate and undergraduate students used Oddity in two distributed systems\nclasses. Usage tracking and qualitative surveys showed that students found\nOddity useful for both debugging and exploratory testing.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 23:34:00 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Woos", "Doug", ""], ["Tatlock", "Zachary", ""], ["Ernst", "Michael D.", ""], ["Anderson", "Thomas E.", ""]]}, {"id": "1806.05358", "submitter": "Dong Yin", "authors": "Dong Yin, Yudong Chen, Kannan Ramchandran, Peter Bartlett", "title": "Defending Against Saddle Point Attack in Byzantine-Robust Distributed\n  Learning", "comments": "ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study robust distributed learning that involves minimizing a non-convex\nloss function with saddle points. We consider the Byzantine setting where some\nworker machines have abnormal or even arbitrary and adversarial behavior. In\nthis setting, the Byzantine machines may create fake local minima near a saddle\npoint that is far away from any true local minimum, even when robust gradient\nestimators are used. We develop ByzantinePGD, a robust first-order algorithm\nthat can provably escape saddle points and fake local minima, and converge to\nan approximate true local minimizer with low iteration complexity. As a\nby-product, we give a simpler algorithm and analysis for escaping saddle points\nin the usual non-Byzantine setting. We further discuss three robust gradient\nestimators that can be used in ByzantinePGD, including median, trimmed mean,\nand iterative filtering. We characterize their performance in concrete\nstatistical settings, and argue for their near-optimality in low and high\ndimensional regimes.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 04:15:59 GMT"}, {"version": "v2", "created": "Fri, 14 Sep 2018 19:12:59 GMT"}, {"version": "v3", "created": "Thu, 24 Jan 2019 20:02:41 GMT"}, {"version": "v4", "created": "Wed, 29 Jul 2020 04:52:23 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Yin", "Dong", ""], ["Chen", "Yudong", ""], ["Ramchandran", "Kannan", ""], ["Bartlett", "Peter", ""]]}, {"id": "1806.05583", "submitter": "Yang Zhang", "authors": "Yang Zhang, Zehui Xiong, Dusit Niyato, Ping Wang, and Zhu Han", "title": "Market-Oriented Information Trading in Internet of Things (IoT) for\n  Smart Cities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet of Things (IoT) technology is a fundamental infrastructure for\ninformation transmission and integration in smart city implementations to\nachieve effective fine-grained city management, efficient operations, and\nimproved life quality of citizens. Smart-city IoT systems usually involve high\nvolume and variety of information circulation. The information lifecycle also\ninvolves many parties, stakeholders, and entities such as individuals,\nbusinesses, and government agencies with their own objectives which needed to\nbe incentivized properly. As such, recent studies have modeled smart-city IoT\nsystems as a market, where information is treated as a commodity by market\nparticipants. In this work, we first present a general information-centric\nsystem architecture to analyze smart-city IoT systems. We then discuss features\nof market-oriented approaches in IoT, including market incentive, IoT service\npattern, information freshness, and social impacts. System design chanlenges\nand related work are also reviewed. Finally, we optimize information trading in\nsmart-city IoT systems, considering direct and indirect network externalities\nin a social domain.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 14:39:56 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Zhang", "Yang", ""], ["Xiong", "Zehui", ""], ["Niyato", "Dusit", ""], ["Wang", "Ping", ""], ["Han", "Zhu", ""]]}, {"id": "1806.05701", "submitter": "Anson Kahng", "authors": "Bernhard Haeupler and D Ellis Hershkowitz and Anson Kahng and Ariel D.\n  Procaccia", "title": "Computation-Aware Data Aggregation", "comments": "Changed the introduction and title; this is the ITCS camera-ready\n  version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data aggregation is a fundamental primitive in distributed computing wherein\na network computes a function of every nodes' input. However, while compute\ntime is non-negligible in modern systems, standard models of distributed\ncomputing do not take compute time into account. Rather, most distributed\nmodels of computation only explicitly consider communication time.\n  In this paper, we introduce a model of distributed computation that considers\n\\emph{both} computation and communication so as to give a theoretical treatment\nof data aggregation. We study both the structure of and how to compute the\nfastest data aggregation schedule in this model. As our first result, we give a\npolynomial-time algorithm that computes the optimal schedule when the input\nnetwork is a complete graph. Moreover, since one may want to aggregate data\nover a pre-existing network, we also study data aggregation scheduling on\narbitrary graphs. We demonstrate that this problem on arbitrary graphs is hard\nto approximate within a multiplicative $1.5$ factor. Finally, we give an\n$O(\\log n \\cdot \\log \\frac{\\mathrm{OPT}}{t_m})$-approximation algorithm for\nthis problem on arbitrary graphs, where $n$ is the number of nodes and\n$\\mathrm{OPT}$ is the length of the optimal schedule.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 18:40:11 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2018 17:32:06 GMT"}, {"version": "v3", "created": "Mon, 5 Nov 2018 19:07:41 GMT"}, {"version": "v4", "created": "Tue, 12 Nov 2019 20:09:55 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Haeupler", "Bernhard", ""], ["Hershkowitz", "D Ellis", ""], ["Kahng", "Anson", ""], ["Procaccia", "Ariel D.", ""]]}, {"id": "1806.05931", "submitter": "Kim J.L. Nevelsteen PhD", "authors": "Kim Nevelsteen and Martin Wehlou", "title": "IPSME- Idempotent Publish/Subscribe Messaging Environment", "comments": "7 pages", "journal-ref": null, "doi": "10.1145/3458307.3460966", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The integration (interoperability) of highly disparate systems is an open\ntopic of research in many domains. A common approach for getting two highly\ndisparate systems to be interoperable, is through an agreed-upon protocol\n(e.g., via standardization) or by employing a common framework. The problem of\nintegrating systems arises when many of these protocols/frameworks come into\nexistence. Both, agreeing on protocols/frameworks and creating mappings between\nprotocols takes time and effort. An interoperability solution must be scalable\nand should not require stakeholders to adapt to major changes in their system\ni.e., systems should not need to be re-engineered as other systems are added,\nremoved or replaced in the integration.\n  IPSME is introduced as a solution for integrating highly disparate systems.\nIPSME decouples the dependencies between interacting participants.\nInteroperability is achieved through dynamic translations, avoiding the need\nfor agreed-upon protocols or frameworks. Scalability is achieved by not having\na limitation on the number of messaging environments or the topological\norganization thereof. IPSME is minimally invasive and through a network effect\nreduces the overall complexity of integrating many systems to linear. IPSME has\nbeen evaluated and thus far been tested in three use cases.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 12:35:34 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 09:49:31 GMT"}, {"version": "v3", "created": "Fri, 23 Apr 2021 18:34:58 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Nevelsteen", "Kim", ""], ["Wehlou", "Martin", ""]]}, {"id": "1806.05971", "submitter": "Wissem Abbes", "authors": "Wissem Abbes, Zied Kechaou, Adel M. Alimi", "title": "An Enhanced BPSO based Approach for Service Placement in Hybrid Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the challenges of competition and the rapidly evolving market,\ncompanies need to be innovative and agile, particularly in regard of web\napplications as used by customers. Nowadays, hybrid cloud stands as an\nattractive solution as organizations tend to use a combination of private and\npublic cloud implementations, in accordance with their appropriate needs to\nprofitably apply the available resources and speed of execution. In such a\ncase, deploying the new applications would certainly entail opting for placing\nand consecrating some components to the private cloud option, while reserving\nsome others to the public cloud option. In this respect, our primary goal in\nthis paper consists in minimizing the extra costs likely to be incurred by\napplying the public cloud related options, along with those costs involved in\nmaintaining communication between the private cloud system and the public cloud\nframework. As for our second targeted objective, it lies in reducing the\ndecision process relating to the execution time, necessary for selecting the\noptimal service placement solution. For this purpose, a novel Binary Particle\nSwarm Optimization (BPSO) based approach is proposed, useful for an effective\nservice placement optimization within hybrid cloud to take place. Using a real\nbenchmark, the experimental results appear to reveal that our proposed approach\nreached results that outperform those documented in the state of the art both\nin terms of cost and time.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 22:08:02 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Abbes", "Wissem", ""], ["Kechaou", "Zied", ""], ["Alimi", "Adel M.", ""]]}, {"id": "1806.06032", "submitter": "Yogesh Simmhan", "authors": "Shreyas Badiger, Shrey Baheti and Yogesh Simmhan", "title": "VIoLET: A Large-scale Virtual Environment for Internet of Things", "comments": "To appear in the Proceedings of the 24TH International European\n  Conference On Parallel and Distributed Computing (EURO-PAR), August 27-31,\n  2018, Turin, Italy, europar2018.org. Selected as a Distinguished Paper for\n  presentation at the Plenary Session of the conference", "journal-ref": "Euro-Par 2018: Parallel Processing. Euro-Par 2018. Lecture Notes\n  in Computer Science, vol 11014", "doi": "10.1007/978-3-319-96983-1_22", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  IoT deployments have been growing manifold, encompassing sensors, networks,\nedge, fog and cloud resources. Despite the intense interest from researchers\nand practitioners, most do not have access to large-scale IoT testbeds for\nvalidation. Simulation environments that allow analytical modeling are a poor\nsubstitute for evaluating software platforms or application workloads in\nrealistic computing environments. Here, we propose VIoLET, a virtual\nenvironment for defining and launching large-scale IoT deployments within cloud\nVMs. It offers a declarative model to specify container-based compute resources\nthat match the performance of the native edge, fog and cloud devices using\nDocker. These can be inter-connected by complex topologies on which\nprivate/public networks, and bandwidth and latency rules are enforced. Users\ncan configure synthetic sensors for data generation on these devices as well.\nWe validate VIoLET for deployments with > 400 devices and > 1500 device-cores,\nand show that the virtual IoT environment closely matches the expected compute\nand network performance at modest costs. This fills an important gap between\nIoT simulators and real deployments.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 16:07:34 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Badiger", "Shreyas", ""], ["Baheti", "Shrey", ""], ["Simmhan", "Yogesh", ""]]}, {"id": "1806.06043", "submitter": "Jarno Rantaharju", "authors": "Ed Bennett, Mark Dawson, Michele Mesiti, Jarno Rantaharju", "title": "AVX-512 extension to OpenQCD 1.6", "comments": "9 pages, 4 figures and 4 tables. Presented at The 36th Annual\n  International Symposium on Lattice Field Theory (Lattice 2018), 22-28 July,\n  2018, Michigan State University, East Lansing, Michigan, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "hep-lat cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We publish an extension of openQCD-1.6 with AVX-512 vector instructions using\nIntel intrinsics. Recent Intel processors support extended instruction sets\nwith operations on 512-bit wide vectors, increasing both the capacity for\nfloating point operations and register memory. Optimal use of the new\ncapabilities requires reorganising data and floating point operations into\nthese wider vector units. We report on the implementation and performance of\nthe AVX-512 OpenQCD extension on clusters using Intel Knights Landing and Xeon\nScalable (Skylake) CPUs. In complete HMC trajectories with physically relevant\nparameters we observe a performance increase of 5% to 10%.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 16:46:04 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 12:07:53 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Bennett", "Ed", ""], ["Dawson", "Mark", ""], ["Mesiti", "Michele", ""], ["Rantaharju", "Jarno", ""]]}, {"id": "1806.06185", "submitter": "Jianli Pan", "authors": "Jianli Pan, Jianyu Wang, Austin Hester, Ismail Alqerm, Yuanni Liu,\n  Ying Zhao", "title": "EdgeChain: An Edge-IoT Framework and Prototype Based on Blockchain and\n  Smart Contracts", "comments": "14 pages, 13 figures, and 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emerging Internet of Things (IoT) is facing significant scalability and\nsecurity challenges. On the one hand, IoT devices are \"weak\" and need external\nassistance. Edge computing provides a promising direction addressing the\ndeficiency of centralized cloud computing in scaling massive number of devices.\nOn the other hand, IoT devices are also relatively \"vulnerable\" facing\nmalicious hackers due to resource constraints. The emerging blockchain and\nsmart contracts technologies bring a series of new security features for IoT\nand edge computing. In this paper, to address the challenges, we design and\nprototype an edge-IoT framework named \"EdgeChain\" based on blockchain and smart\ncontracts. The core idea is to integrate a permissioned blockchain and the\ninternal currency or \"coin\" system to link the edge cloud resource pool with\neach IoT device' account and resource usage, and hence behavior of the IoT\ndevices. EdgeChain uses a credit-based resource management system to control\nhow much resource IoT devices can obtain from edge servers, based on\npre-defined rules on priority, application types and past behaviors. Smart\ncontracts are used to enforce the rules and policies to regulate the IoT device\nbehavior in a non-deniable and automated manner. All the IoT activities and\ntransactions are recorded into blockchain for secure data logging and auditing.\nWe implement an EdgeChain prototype and conduct extensive experiments to\nevaluate the ideas. The results show that while gaining the security benefits\nof blockchain and smart contracts, the cost of integrating them into EdgeChain\nis within a reasonable and acceptable range.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 05:13:56 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Pan", "Jianli", ""], ["Wang", "Jianyu", ""], ["Hester", "Austin", ""], ["Alqerm", "Ismail", ""], ["Liu", "Yuanni", ""], ["Zhao", "Ying", ""]]}, {"id": "1806.06204", "submitter": "Shengguo Li", "authors": "Shengguo Li, Jie Liu, Yunfei Du", "title": "A New High Performance and Scalable SVD algorithm on Distributed Memory\n  Systems", "comments": "25 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a high performance implementation of \\texttt{Zolo-SVD}\nalgorithm on distributed memory systems, which is based on the polar\ndecomposition (PD) algorithm via the Zolotarev's function (\\texttt{Zolo-PD}),\noriginally proposed by Nakatsukasa and Freund [SIAM Review, 2016]. Our\nimplementation highly relies on the routines of ScaLAPACK and therefore it is\nportable. Compared with the other PD algorithms such as the QR-based\ndynamically weighted Halley method (\\texttt{QDWH-PD}), \\texttt{Zolo-PD} is\nnaturally parallelizable and has better scalability though performs more\nfloating-point operations. When using many processes, \\texttt{Zolo-PD} is\nusually 1.20 times faster than \\texttt{QDWH-PD} algorithm, and\n\\texttt{Zolo-SVD} can be about two times faster than the ScaLAPACK routine\n\\texttt{\\texttt{PDGESVD}}. These numerical experiments are performed on\nTianhe-2 supercomputer, one of the fastest supercomputers in the world, and the\ntested matrices include some sparse matrices from particular applications and\nsome randomly generated dense matrices with different dimensions. Our\n\\texttt{QDWH-SVD} and \\texttt{Zolo-SVD} implementations are freely available at\nhttps://github.com/shengguolsg/Zolo-SVD.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 08:12:11 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Li", "Shengguo", ""], ["Liu", "Jie", ""], ["Du", "Yunfei", ""]]}, {"id": "1806.06294", "submitter": "Sergi Abadal", "authors": "Sergi Abadal, Albert Mestres, Josep Torrellas, Eduard Alarc\\'on,\n  Albert Cabellos-Aparicio", "title": "Medium Access Control in Wireless Network-on-Chip: A Context Analysis", "comments": "To appear in IEEE Communications Magazine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wireless on-chip communication is a promising candidate to address the\nperformance and efficiency issues that arise when scaling current\nNetwork-on-Chip (NoC) techniques to manycore processors. A Wireless\nNetwork-on-Chip (WNoC) can serve global and broadcast traffic with ultra-low\nlatency even in thousand-core chips, thus acting as a natural complement of\nconventional and throughput-oriented wireline NoCs. However, the development of\nMedium Access Control (MAC) strategies needed to efficiently share the wireless\nmedium among the increasing number of cores remains as a considerable challenge\ngiven the singularities of the environment and the novelty of the research\narea. In this position paper, we present a context analysis describing the\nphysical constraints, performance objectives, and traffic characteristics of\nthe on-chip communication paradigm. We summarize the main differences with\nrespect to traditional wireless scenarios, to then discuss their implications\non the design of MAC protocols for manycore WNoCs, with the ultimate goal of\nkickstarting this arguably unexplored research area.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 21:07:44 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Abadal", "Sergi", ""], ["Mestres", "Albert", ""], ["Torrellas", "Josep", ""], ["Alarc\u00f3n", "Eduard", ""], ["Cabellos-Aparicio", "Albert", ""]]}, {"id": "1806.06541", "submitter": "Jung Ho Ahn", "authors": "Daejin Jung and Sunjung Lee and Wonjong Rhee and Jung Ho Ahn", "title": "Partitioning Compute Units in CNN Acceleration for Statistical Memory\n  Traffic Shaping", "comments": "4 pages, 6 figures, appears at IEEE Computer Architecture Letters", "journal-ref": "IEEE Computer Architecture Letters ( Volume: 17, Issue: 1,\n  Jan.-June 1 2018 )", "doi": "10.1109/LCA.2017.2773055", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design complexity of CNNs has been steadily increasing to improve\naccuracy. To cope with the massive amount of computation needed for such\ncomplex CNNs, the latest solutions utilize blocking of an image over the\navailable dimensions and batching of multiple input images to improve data\nreuse in the memory hierarchy. While there has been numerous works on\nmaximizing data reuse, only a few studies have focused on the memory bottleneck\ncaused by limited bandwidth. Bandwidth bottleneck can easily occur in CNN\nacceleration as CNN layers have different sizes with varying computation needs\nand as batching is typically performed over each CNN layer for an ideal data\nreuse. In this case, the data transfer demand for a layer can be relatively low\nor high compared to the computation requirement of the layer, and hence\ntemporal fluctuations in memory access can be induced eventually causing\nbandwidth problems. In this paper, we first show that there exists a high\ndegree of fluctuation in memory access to computation ratio depending on CNN\nlayers and functions in the layer being processed by the compute units (cores),\nwhere the units are tightly synchronized to maximize data reuse. Then we\npropose a strategy of partitioning the compute units where the cores within\neach partition process a batch of input data synchronously to maximize data\nreuse but different partitions run asynchronously. As the partitions stay\nasynchronous and typically process different CNN layers at any given moment,\nthe memory access traffic sizes of the partitions become statistically\nshuffled. Thus, the partitioning of compute units and asynchronous use of them\nmake the total memory access traffic size be smoothened over time. We call this\nsmoothing statistical memory traffic shaping, and we show that it can lead to\n8.0 percent of performance gain on a commercial 64-core processor when running\nResNet-50.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 08:11:38 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Jung", "Daejin", ""], ["Lee", "Sunjung", ""], ["Rhee", "Wonjong", ""], ["Ahn", "Jung Ho", ""]]}, {"id": "1806.06576", "submitter": "Jiawen Sun", "authors": "Jiawen Sun, Hans Vandierendonck and Dimitrios S. Nikolopoulos", "title": "VEBO: A Vertex- and Edge-Balanced Ordering Heuristic to Load Balance\n  Parallel Graph Processing", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph partitioning drives graph processing in distributed, disk-based and\nNUMA-aware systems. A commonly used partitioning goal is to balance the number\nof edges per partition in conjunction with minimizing the edge or vertex cut.\nWhile this type of partitioning is computationally expensive, we observe that\nsuch topology-driven partitioning nonetheless results in computational load\nimbalance. We propose Vertex- and Edge-Balanced Ordering (VEBO): balance the\nnumber of edges and the number of unique destinations of those edges. VEBO\noptimally balances edges and vertices for graphs with a power-law degree\ndistribution. Experimental evaluation on three shared-memory graph processing\nsystems (Ligra, Polymer and GraphGrind) shows that VEBO achieves excellent load\nbalance and improves performance by 1.09x over Ligra, 1.41x over Polymer and\n1.65x over GraphGrind, compared to their respective partitioning algorithms,\naveraged across 8 algorithms and 7 graphs.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 09:55:45 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Sun", "Jiawen", ""], ["Vandierendonck", "Hans", ""], ["Nikolopoulos", "Dimitrios S.", ""]]}, {"id": "1806.06580", "submitter": "Massimo Cafaro", "authors": "Massimo Cafaro, Italo Epicoco and Marco Pulimeno", "title": "Mining frequent items in unstructured P2P networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale decentralized systems, such as P2P, sensor or IoT device networks\nare becoming increasingly common, and require robust protocols to address the\nchallenges posed by the distribution of data and the large number of peers\nbelonging to the network. In this paper, we deal with the problem of mining\nfrequent items in unstructured P2P networks. This problem, of practical\nimportance, has many useful applications. We design P2PSS, a fully\ndecentralized, gossip--based protocol for frequent items discovery, leveraging\nthe Space-Saving algorithm. We formally prove the correctness and theoretical\nerror bound. Extensive experimental results clearly show that P2PSS provides\nvery good accuracy and scalability, also in the presence of highly dynamic P2P\nnetworks with churning. To the best of our knowledge, this is the first\ngossip--based distributed algorithm providing strong theoretical guarantees for\nboth the Approximate Frequent Items Problem in Unstructured P2P Networks and\nfor the frequency estimation of discovered frequent items.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 10:13:19 GMT"}, {"version": "v2", "created": "Wed, 20 Jun 2018 10:00:14 GMT"}, {"version": "v3", "created": "Mon, 2 Jul 2018 11:49:32 GMT"}, {"version": "v4", "created": "Tue, 16 Oct 2018 18:44:30 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Cafaro", "Massimo", ""], ["Epicoco", "Italo", ""], ["Pulimeno", "Marco", ""]]}, {"id": "1806.06728", "submitter": "Cristian Galleguillos", "authors": "Cristian Galleguillos (1 and 2), Zeynep Kiziltan (2), Alessio Netti\n  (2), Ricardo Soto (1) ((1) Pontificia Universidad Cat\\'olica de Valpara\\'iso,\n  Valpara\\'iso, Chile., (2) University of Bologna, Bologna, Italy)", "title": "AccaSim: a Customizable Workload Management Simulator for Job\n  Dispatching Research in HPC Systems", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present AccaSim, a simulator for workload management in HPC systems.\nThanks to AccaSim's scalability to large workload datasets, support for easy\ncustomization, and practical automated tools to aid experimentation, users can\neasily represent various real HPC systems, develop novel advanced dispatchers\nand evaluate them in a convenient way across different workload sources.\nAccaSim is thus an attractive tool for conducting job dispatching research in\nHPC systems.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 14:23:46 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Galleguillos", "Cristian", "", "1 and 2"], ["Kiziltan", "Zeynep", ""], ["Netti", "Alessio", ""], ["Soto", "Ricardo", ""]]}, {"id": "1806.06749", "submitter": "Samo Gerk\\v{s}i\\v{c}", "authors": "Samo Gerk\\v{s}i\\v{c}, Bo\\v{s}tjan Pregelj, Matija Perne", "title": "FPGA acceleration of Model Predictive Control for Iter Plasma current\n  and shape control", "comments": "21st IEEE Real Time Conference, Colonial Williamsburg, 9-15 June\n  2018, Woodlands Conference Center, Poster1 518", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A faster implementation of the Quadratic Programming (QP) solver used in the\nModel Predictive Control scheme for Iter Plasma current and shape control was\ndeveloped for Xilinx Field-Programmable Gate Array (FPGA) platforms using a\nhigh-level synthesis approach. The QP solver is based on the dual Fast Gradient\nMethod (dFGM). The dFGM is essentially an iterative algorithm, where\nmatrix-vector arithmetic operations within the main iteration loop may be\nparallelized. This type of parallelism is not well-suited to standard\nmulti-core processors because the number of operations to be spread among\nprocessing threads is relatively small considering the time-scale of thread\nscheduling. The FPGA implementation avoids this issue, but it requires specific\ntechniques of code optimization in order to achieve faster solver execution.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 16:15:26 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Gerk\u0161i\u010d", "Samo", ""], ["Pregelj", "Bo\u0161tjan", ""], ["Perne", "Matija", ""]]}, {"id": "1806.06826", "submitter": "Manuel Parra-Royon", "authors": "Manuel Parra-Royon and Ghislain Atemezing and J.M. Ben\\'itez", "title": "Semantics of Data Mining Services in Cloud Computing", "comments": "In-depth review. Fixed mistakes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years with the rise of Cloud Computing, many companies providing\nservices in the cloud, are empowering a new series of services to their\ncatalogue, such as data mining and data processing, taking advantage of the\nvast computing resources available to them. Different service definition\nproposals have been put forward to address the problem of describing services\nin Cloud Computing in a comprehensive way. Bearing in mind that each provider\nhas its own definition of the logic of its services, and specifically of data\nmining services, it should be pointed out that the possibility of describing\nservices in a flexible way between providers is fundamental in order to\nmaintain the usability and portability of this type of Cloud Computing\nservices. The use of semantic technologies based on the proposal offered by\nLinked Data for the definition of services, allows the design and modelling of\ndata mining services, achieving a high degree of interoperability. In this\narticle a schema for the definition of data mining services on cloud computing\nis presented considering all key aspects of service, such as prices,\ninterfaces, Software Level Agreement, instances or data mining workflow, among\nothers. The new schema is based on Linked Data, and it reuses other schemata\nobtaining a better and more complete definition of the services. In order to\nvalidate the completeness of the scheme, a series of data mining services have\nbeen created where a set of algorithms such as Random Forest or K-Means are\nmodeled as services. In addition, a dataset has been generated including the\ndefinition of the services of several actual Cloud Computing data mining\nproviders, confirming the effectiveness of the schema.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 17:03:56 GMT"}, {"version": "v2", "created": "Sun, 24 Jun 2018 21:37:17 GMT"}, {"version": "v3", "created": "Fri, 5 Oct 2018 17:10:24 GMT"}, {"version": "v4", "created": "Mon, 14 Jan 2019 11:16:02 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Parra-Royon", "Manuel", ""], ["Atemezing", "Ghislain", ""], ["Ben\u00edtez", "J. M.", ""]]}, {"id": "1806.06917", "submitter": "Patrick Diehl", "authors": "Patrick Diehl, Prashant K. Jha, Hartmut Kaiser, Robert Lipton, and\n  Martin Levesque", "title": "An asynchronous and task-based implementation of Peridynamics utilizing\n  HPX -- the C++ standard library for parallelism and concurrency", "comments": null, "journal-ref": null, "doi": "10.1007/s42452-020-03784-x", "report-no": null, "categories": "cs.DC physics.comp-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  On modern supercomputers, asynchronous many task systems are emerging to\naddress the new architecture of computational nodes. Through this shift of\nincreasing cores per node, a new programming model with the focus on handle the\nfine-grain parallelism of this increasing amount of cores per computational\nnode is needed. Asynchronous Many Task (AMT) run time systems represent an\nemerging paradigm for addressing fine-grain parallelism since they handle the\nincreasing amount of threads per node and concurrency. HPX, a open source C++\nstandard library for parallelism and concurrency, is one AMT which is confirm\nwith the C++ standard. Which means that HPX's Application Programming Interface\n(API) is confirm with its definition by the C++ standard committee. For example\nfor the concept of futurization the hpx:future can be replaced by std::future\nwithout breaking the API. Peridynamics is a non-local generalization of\ncontinuum mechanics tailored to address discontinuous displacement fields\narising in fracture mechanics. As many non-local approaches, peridynamics\nrequires considerable computing resources to solve practical problems. This\npaper investigates the implementation of a peridynamics EMU nodal\ndiscretization in an asynchronous task-based fashion. The scalability of\nasynchronous task-based implementation is to be in agreement with theoretical\nestimations. In addition, to the scalabilty the code is convergent for implicit\ntime integration and recovers theoretical solutions. Explicit time integration,\nconvergence results are presented to showcase the agreement of results with\ntheoretical claims in previous works.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 20:26:01 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 21:17:12 GMT"}, {"version": "v3", "created": "Sat, 30 Nov 2019 02:05:02 GMT"}, {"version": "v4", "created": "Tue, 31 Mar 2020 18:37:06 GMT"}, {"version": "v5", "created": "Wed, 5 Aug 2020 01:42:34 GMT"}, {"version": "v6", "created": "Wed, 28 Oct 2020 16:23:54 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Diehl", "Patrick", ""], ["Jha", "Prashant K.", ""], ["Kaiser", "Hartmut", ""], ["Lipton", "Robert", ""], ["Levesque", "Martin", ""]]}, {"id": "1806.06978", "submitter": "Isaac Sheff", "authors": "Isaac Sheff, Xinwen Wang, Andrew C. Myers, Robbert van Renesse", "title": "A Web of Blocks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Blockchains offer a useful abstraction: a trustworthy, decentralized log of\ntotally ordered transactions. Traditional blockchains have problems with\nscalability and efficiency, preventing their use for many applications. These\nlimitations arise from the requirement that all participants agree on the total\nordering of transactions. To address this fundamental shortcoming, we introduce\nCharlotte, a system for maintaining decentralized, authenticated data\nstructures, including transaction logs. Each data structurestructure -- indeed,\neach block -- specifies its own availability and integrity properties, allowing\nCharlotte applications to retain the full benefits of permissioned or\npermissionless blockchains. In Charlotte, a block can be atomically appended to\nmultiple logs, allowing applications to be interoperable when they want to,\nwithout inefficiently forcing all applications to share one big log. We call\nthis open graph of interconnected blocks a blockweb. We allow new kinds of\nblockweb applications that operate beyond traditional chains. We demonstrate\nthe viability of Charlotte applications with proof-of-concept servers running\ninteroperable blockchains. Using performance data from our prototype, we\nestimate that when compared with traditional blockchains, Charlotte offers\nmultiple orders of magnitude improvement in speed and energy efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 23:01:41 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Sheff", "Isaac", ""], ["Wang", "Xinwen", ""], ["Myers", "Andrew C.", ""], ["van Renesse", "Robbert", ""]]}, {"id": "1806.07060", "submitter": "Flavio Vella", "authors": "Marco Cianfriglia, Flavio Vella, Cedric Nugteren, Anton Lokhmotov,\n  Grigori Fursin", "title": "A model-driven approach for a new generation of adaptive libraries", "comments": "New detailed analysis will be provided", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC cs.MS cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient high-performance libraries often expose multiple tunable parameters\nto provide highly optimized routines. These can range from simple loop unroll\nfactors or vector sizes all the way to algorithmic changes, given that some\nimplementations can be more suitable for certain devices by exploiting hardware\ncharacteristics such as local memories and vector units. Traditionally, such\nparameters and algorithmic choices are tuned and then hard-coded for a specific\narchitecture and for certain characteristics of the inputs. However, emerging\napplications are often data-driven, thus traditional approaches are not\neffective across the wide range of inputs and architectures used in practice.\nIn this paper, we present a new adaptive framework for data-driven applications\nwhich uses a predictive model to select the optimal algorithmic parameters by\ntraining with synthetic and real datasets. We demonstrate the effectiveness of\na BLAS library and specifically on its matrix multiplication routine. We\npresent experimental results for two GPU architectures and show significant\nperformance gains of up to 3x (on a high-end NVIDIA Pascal GPU) and 2.5x (on an\nembedded ARM Mali GPU) when compared to a traditionally optimized library.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 06:17:38 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Cianfriglia", "Marco", ""], ["Vella", "Flavio", ""], ["Nugteren", "Cedric", ""], ["Lokhmotov", "Anton", ""], ["Fursin", "Grigori", ""]]}, {"id": "1806.07081", "submitter": "Van Sy Mai", "authors": "Van Sy Mai and Eyad H. Abed", "title": "Distributed Optimization over Directed Graphs with Row Stochasticity and\n  Constraint Regularity", "comments": "14 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with an optimization problem over a network of agents, where\nthe cost function is the sum of the individual objectives of the agents and the\nconstraint set is the intersection of local constraints. Most existing methods\nemploying subgradient and consensus steps for solving this problem require the\nweight matrix associated with the network to be column stochastic or even\ndoubly stochastic, conditions that can be hard to arrange in directed networks.\nMoreover, known convergence analyses for distributed subgradient methods vary\ndepending on whether the problem is unconstrained or constrained, and whether\nthe local constraint sets are identical or nonidentical and compact. The main\ngoals of this paper are: (i) removing the common column stochasticity\nrequirement; (ii) relaxing the compactness assumption, and (iii) providing a\nunified convergence analysis. Specifically, assuming the communication graph to\nbe fixed and strongly connected and the weight matrix to (only) be row\nstochastic, a distributed projected subgradient algorithm and its variation are\npresented to solve the problem for cost functions that are convex and Lipschitz\ncontinuous. Based on a regularity assumption on the local constraint sets, a\nunified convergence analysis is given that can be applied to both unconstrained\nand constrained problems and without assuming compactness of the constraint\nsets or an interior point in their intersection. Further, we also establish an\nupper bound on the absolute objective error evaluated at each agent's available\nlocal estimate under a nonincreasing step size sequence. This bound allows us\nto analyze the convergence rate of both algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 07:40:26 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Mai", "Van Sy", ""], ["Abed", "Eyad H.", ""]]}, {"id": "1806.07258", "submitter": "Daniele Cesarini", "authors": "Daniele Cesarini, Andrea Bartolini, Pietro Bonf\\`a, Carlo Cavazzoni\n  and Luca Benini", "title": "COUNTDOWN: a Run-time Library for Performance-Neutral Energy Saving in\n  MPI Applications", "comments": "14 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power and energy consumption is becoming key challenges to deploy the first\nexascale supercomputer successfully. Large-scale HPC applications waste a\nsignificant amount of power in communication and synchronization-related idle\ntimes. However, due to the time scale at which communication happens,\ntransitioning in low power states during communication's idle times may\nintroduce unacceptable overhead in applications' execution time. In this paper,\nwe present COUNTDOWN, a runtime library, supported by a methodology and\nanalysis tool for identifying and automatically reducing the power consumption\nof the computing elements during communication and synchronization. COUNTDOWN\nsaves energy without imposing significant time-to-completion increase by\nlowering CPUs power consumption only during idle times for which power state\ntransition overhead are negligible. This is done transparently to the user,\nwithout requiring labor-intensive and error-prone application code\nmodifications, nor requiring recompilation of the application. We test our\nmethodology in a production Tier-0 system. For the NAS benchmarks, COUNTDOWN\nsaves between 6% and 50% energy, with a time-to-solution penalty lower than 5%.\nIn a complete production --- Quantum ESPRESSO --- for a 3.5K cores run,\nCOUNTDOWN saves 22.36% energy, with a performance penalty below 3%. Energy\nsaving increases to 37% with a performance penalty of 6.38%, if the application\nis executed without communication tuning.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 14:03:47 GMT"}, {"version": "v2", "created": "Thu, 23 May 2019 14:04:57 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Cesarini", "Daniele", ""], ["Bartolini", "Andrea", ""], ["Bonf\u00e0", "Pietro", ""], ["Cavazzoni", "Carlo", ""], ["Benini", "Luca", ""]]}, {"id": "1806.07300", "submitter": "James Browne", "authors": "James Browne, Tyler M. Tomita, Disa Mhembere, Randal Burns, Joshua T.\n  Vogelstein", "title": "Forest Packing: Fast, Parallel Decision Forests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Machine learning has an emerging critical role in high-performance computing\nto modulate simulations, extract knowledge from massive data, and replace\nnumerical models with efficient approximations. Decision forests are a critical\ntool because they provide insight into model operation that is critical to\ninterpreting learned results. While decision forests are trivially\nparallelizable, the traversals of tree data structures incur many random memory\naccesses and are very slow. We present memory packing techniques that\nreorganize learned forests to minimize cache misses during classification. The\nresulting layout is hierarchical. At low levels, we pack the nodes of multiple\ntrees into contiguous memory blocks so that each memory access fetches data for\nmultiple trees. At higher levels, we use leaf cardinality to identify the most\npopular paths through a tree and collocate those paths in cache lines. We\nextend this layout with out-of-order execution and cache-line prefetching to\nincrease memory throughput. Together, these optimizations increase the\nperformance of classification in ensembles by a factor of four over an\noptimized C++ implementation and a actor of 50 over a popular R language\nimplementation.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 15:11:17 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Browne", "James", ""], ["Tomita", "Tyler M.", ""], ["Mhembere", "Disa", ""], ["Burns", "Randal", ""], ["Vogelstein", "Joshua T.", ""]]}, {"id": "1806.07334", "submitter": "Jun Guo", "authors": "Jun Guo and Hamid Jafarkhani", "title": "Movement-efficient Sensor Deployment in Wireless Sensor Networks with\n  Limited Communication Range", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a mobile wireless sensor network (MWSN) consisting of multiple\nmobile sensors or robots. Three key factors in MWSNs, sensing quality, energy\nconsumption, and connectivity, have attracted plenty of attention, but the\ninteraction of these factors is not well studied. To take all the three factors\ninto consideration, we model the sensor deployment problem as a constrained\nsource coding problem. %, which can be applied to different coverage tasks,\nsuch as area coverage, target coverage, and barrier coverage. Our goal is to\nfind an optimal sensor deployment (or relocation) to optimize the sensing\nquality with a limited communication range and a specific network lifetime\nconstraint. We derive necessary conditions for the optimal sensor deployment in\nboth homogeneous and heterogeneous MWSNs. According to our derivation, some\nsensors are idle in the optimal deployment of heterogeneous MWSNs. Using these\nnecessary conditions, we design both centralized and distributed algorithms to\nprovide a flexible and explicit trade-off between sensing uncertainty and\nnetwork lifetime. The proposed algorithms are successfully extended to more\napplications, such as area coverage and target coverage, via properly selected\ndensity functions. Simulation results show that our algorithms outperform the\nexisting relocation algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 16:41:19 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Guo", "Jun", ""], ["Jafarkhani", "Hamid", ""]]}, {"id": "1806.07530", "submitter": "Raj Gaire", "authors": "Raj Gaire, Chigulapalli Sriharsha, Deepak Puthal, Hendra Wijaya,\n  Jongkil Kim, Prateeksha Keshari, Rajiv Ranjan, Rajkumar Buyya, Ratan K.\n  Ghosh, R.K. Shyamasundar and Surya Nepal", "title": "Internet of Things (IoT) and Cloud Computing Enabled Disaster Management", "comments": "Submitted for the book titled \"Integration of Cyber-Physical Systems,\n  Cloud, and Internet of Things\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Disaster management demands a near real-time information dissemina-tion so\nthat the emergency services can be provided to the right people at the right\ntime. Recent advances in information and communication technologies enable\ncollection of real-time information from various sources. For example, sensors\ndeployed in the fields collect data about the environment. Similarly, social\nnetworks like Twitter and Facebook can help to collect data from people in the\ndisaster zone. On one hand, inadequate situation awareness in disasters has\nbeen identified as one of the primary factors in human errors with grave\nconsequences such as loss of lives and destruction of critical infrastructure.\nOn the other hand, the growing ubiquity of social media and mobile devices, and\npervasive nature of the Internet-of-Things means that there are more sources of\noutbound traffic, which ultimately results in the creation of a data deluge,\nbeginning shortly after the onset of disaster events, leading to the problem of\ninformation tsunami. In addition, security and privacy has crucial role to\novercome the misuse of the system for either intrusions into data or overcome\nthe misuse of the information that was meant for a specified purpose. .... In\nthis chapter, we provide such a situation aware application to support disaster\nmanagement data lifecycle, i.e. from data ingestion and processing to alert\ndissemination. We utilize cloud computing, Internet of Things and social\ncomputing technologies to achieve a scalable, effi-cient, and usable\nsituation-aware application called Cloud4BigData.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 03:00:29 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Gaire", "Raj", ""], ["Sriharsha", "Chigulapalli", ""], ["Puthal", "Deepak", ""], ["Wijaya", "Hendra", ""], ["Kim", "Jongkil", ""], ["Keshari", "Prateeksha", ""], ["Ranjan", "Rajiv", ""], ["Buyya", "Rajkumar", ""], ["Ghosh", "Ratan K.", ""], ["Shyamasundar", "R. K.", ""], ["Nepal", "Surya", ""]]}, {"id": "1806.07764", "submitter": "Lixing Chen", "authors": "Lixing Chen, Pan Zhou, Liang Gao, Jie Xu", "title": "Adaptive Fog Configuration for the Industrial Internet of Things", "comments": null, "journal-ref": null, "doi": "10.1109/TII.2018.2846549", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Industrial Fog computing deploys various industrial services, such as\nautomatic monitoring/control and imminent failure detection, at the Fog Nodes\n(FNs) to improve the performance of industrial systems. Much effort has been\nmade in the literature on the design of fog network architecture and\ncomputation offloading. This paper studies an equally important but much less\ninvestigated problem of service hosting where FNs are adaptively configured to\nhost services for Sensor Nodes (SNs), thereby enabling corresponding tasks to\nbe executed by the FNs. The problem of service hosting emerges because of the\nlimited computational and storage resources at FNs, which limit the number of\ndifferent types of services that can be hosted by an FN at the same time.\nConsidering the variability of service demand in both temporal and spatial\ndimensions, when, where, and which services to host have to be judiciously\ndecided to maximize the utility of the Fog computing network. Our proposed Fog\nconfiguration strategies are tailored to battery-powered FNs. The limited\nbattery capacity of FNs creates a long-term energy budget constraint that\nsignificantly complicates the Fog configuration problem as it introduces\ntemporal coupling of decision making across the timeline. To address all these\nchallenges, we propose an online distributed algorithm, called Adaptive Fog\nConfiguration (AFC), based on Lyapunov optimization and parallel Gibbs\nsampling. AFC jointly optimizes service hosting and task admission decisions,\nrequiring only currently available system information while guaranteeing\nclose-to-optimal performance compared to an oracle algorithm with full future\ninformation.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 14:30:10 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Chen", "Lixing", ""], ["Zhou", "Pan", ""], ["Gao", "Liang", ""], ["Xu", "Jie", ""]]}, {"id": "1806.07840", "submitter": "Xu Chen", "authors": "En Li and Zhi Zhou and Xu Chen", "title": "Edge Intelligence: On-Demand Deep Learning Model Co-Inference with\n  Device-Edge Synergy", "comments": "ACM SIGCOMM Workshop on Mobile Edge Communications, Budapest,\n  Hungary, August 21-23, 2018. https://dl.acm.org/authorize?N665473", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.CV cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the backbone technology of machine learning, deep neural networks (DNNs)\nhave have quickly ascended to the spotlight. Running DNNs on\nresource-constrained mobile devices is, however, by no means trivial, since it\nincurs high performance and energy overhead. While offloading DNNs to the cloud\nfor execution suffers unpredictable performance, due to the uncontrolled long\nwide-area network latency. To address these challenges, in this paper, we\npropose Edgent, a collaborative and on-demand DNN co-inference framework with\ndevice-edge synergy. Edgent pursues two design knobs: (1) DNN partitioning that\nadaptively partitions DNN computation between device and edge, in order to\nleverage hybrid computation resources in proximity for real-time DNN inference.\n(2) DNN right-sizing that accelerates DNN inference through early-exit at a\nproper intermediate DNN layer to further reduce the computation latency. The\nprototype implementation and extensive evaluations based on Raspberry Pi\ndemonstrate Edgent's effectiveness in enabling on-demand low-latency edge\nintelligence.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 16:56:54 GMT"}, {"version": "v2", "created": "Thu, 21 Jun 2018 03:36:27 GMT"}, {"version": "v3", "created": "Fri, 14 Sep 2018 02:37:07 GMT"}, {"version": "v4", "created": "Thu, 27 Dec 2018 11:49:55 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Li", "En", ""], ["Zhou", "Zhi", ""], ["Chen", "Xu", ""]]}, {"id": "1806.07984", "submitter": "Tobias Weinzierl", "authors": "Dominic E. Charrier and Benjamin Hazelwood and Tobias Weinzierl", "title": "Enclave Tasking for Discontinuous Galerkin Methods on Dynamically\n  Adaptive Meshes", "comments": null, "journal-ref": null, "doi": "10.1137/19M1276194", "report-no": null, "categories": "cs.MS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-order Discontinuous Galerkin (DG) methods promise to be an excellent\ndiscretisation paradigm for partial differential equation solvers by combining\nhigh arithmetic intensity with localised data access. They also facilitate\ndynamic adaptivity without the need for conformal meshes. A parallel evaluation\nof DG's weak formulation within a mesh traversal is non-trivial, as dependency\ngraphs over dynamically adaptive meshes change, as causal constraints along\nresolution transitions have to be preserved, and as data sends along MPI domain\nboundaries have to be triggered in the correct order. We propose to process\nmesh elements subject to constraints with high priority or, where needed,\nserially throughout a traversal. The remaining cells form enclaves and are\nspawned into a task system. This introduces concurrency, mixes memory-intensive\nDG integrations with compute-bound Riemann solves, and overlaps computation and\ncommunication. We discuss implications on MPI and show that MPI parallelisation\nimproves by a factor of three through enclave tasking, while we obtain an\nadditional factor of two from shared memory if grids are dynamically adaptive.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 14:09:42 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 04:40:26 GMT"}, {"version": "v3", "created": "Mon, 24 Feb 2020 08:17:01 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Charrier", "Dominic E.", ""], ["Hazelwood", "Benjamin", ""], ["Weinzierl", "Tobias", ""]]}, {"id": "1806.07985", "submitter": "Grey Ballard", "authors": "Grey Ballard and Koby Hayashi and Ramakrishnan Kannan", "title": "Parallel Nonnegative CP Decomposition of Dense Tensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The CP tensor decomposition is a low-rank approximation of a tensor. We\npresent a distributed-memory parallel algorithm and implementation of an\nalternating optimization method for computing a CP decomposition of dense\ntensor data that can enforce nonnegativity of the computed low-rank factors.\nThe principal task is to parallelize the matricized-tensor times Khatri-Rao\nproduct (MTTKRP) bottleneck subcomputation. The algorithm is computation\nefficient, using dimension trees to avoid redundant computation across MTTKRPs\nwithin the alternating method. Our approach is also communication efficient,\nusing a data distribution and parallel algorithm across a multidimensional\nprocessor grid that can be tuned to minimize communication. We benchmark our\nsoftware on synthetic as well as hyperspectral image and neuroscience dynamic\nfunctional connectivity data, demonstrating that our algorithm scales well to\n100s of nodes (up to 4096 cores) and is faster and more general than the\ncurrently available parallel software.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 13:52:12 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Ballard", "Grey", ""], ["Hayashi", "Koby", ""], ["Kannan", "Ramakrishnan", ""]]}, {"id": "1806.08054", "submitter": "Jiaxiang Wu", "authors": "Jiaxiang Wu, Weidong Huang, Junzhou Huang, Tong Zhang", "title": "Error Compensated Quantized SGD and its Applications to Large-scale\n  Distributed Optimization", "comments": "Accepted by ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale distributed optimization is of great importance in various\napplications. For data-parallel based distributed learning, the inter-node\ngradient communication often becomes the performance bottleneck. In this paper,\nwe propose the error compensated quantized stochastic gradient descent\nalgorithm to improve the training efficiency. Local gradients are quantized to\nreduce the communication overhead, and accumulated quantization error is\nutilized to speed up the convergence. Furthermore, we present theoretical\nanalysis on the convergence behaviour, and demonstrate its advantage over\ncompetitors. Extensive experiments indicate that our algorithm can compress\ngradients by a factor of up to two magnitudes without performance degradation.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 03:21:24 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Wu", "Jiaxiang", ""], ["Huang", "Weidong", ""], ["Huang", "Junzhou", ""], ["Zhang", "Tong", ""]]}, {"id": "1806.08082", "submitter": "Khaled Ammar", "authors": "Khaled Ammar and Tamer Ozsu", "title": "Experimental Analysis of Distributed Graph Systems", "comments": "Volume 11 of Proc. VLDB Endowment", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper evaluates eight parallel graph processing systems: Hadoop, HaLoop,\nVertica, Giraph, GraphLab (PowerGraph), Blogel, Flink Gelly, and GraphX (SPARK)\nover four very large datasets (Twitter, World Road Network, UK 200705, and\nClueWeb) using four workloads (PageRank, WCC, SSSP and K-hop). The main\nobjective is to perform an independent scale-out study by experimentally\nanalyzing the performance, usability, and scalability (using up to 128\nmachines) of these systems. In addition to performance results, we discuss our\nexperiences in using these systems and suggest some system tuning heuristics\nthat lead to better performance.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 06:45:33 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Ammar", "Khaled", ""], ["Ozsu", "Tamer", ""]]}, {"id": "1806.08092", "submitter": "Kartik Lakhotia", "authors": "Kartik Lakhotia, Sourav Pati, Rajgopal Kannan, Viktor Prasanna", "title": "GPOP: A cache- and work-efficient framework for Graph Processing Over\n  Partitions", "comments": "23 pages, 7 figures, 4 tables", "journal-ref": null, "doi": "10.1145/3293883.3299108", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Past decade has seen the development of many shared-memory graph processing\nframeworks, intended to reduce the effort of developing high performance\nparallel applications. However many of these frameworks, based on\nVertex-centric or Edge-centric paradigms suffer from several issues, such as\npoor cache utilization, irregular memory accesses, heavy use of synchronization\nprimitives and theoretical inefficiency, that deteriorate overall performance\nand scalability.\n  Recently, we proposed a cache and memory efficient partition-centric paradigm\nfor computing PageRank. In this paper, we generalize this approach to develop a\nnovel Graph Processing Over Partitions (GPOP) framework that is\ncache-efficient, scalable and work-efficient. GPOP induces locality in memory\naccesses by increasing granularity of execution to vertex subsets called\n'partitions', thereby dramatically improving the cache performance of a variety\nof graph algorithms. It achieves high scalability by enabling completely lock\nand atomic free computation. GPOP's built-in analytical performance model\nenables it to use a hybrid of source and partitioncentric communication modes\nin a way that ensures work-efficiency each iteration, while simultaneously\nboosting high bandwidth sequential memory accesses.\n  We extensively evaluate the performance of GPOP for a variety of graph\nalgorithms, using several large datasets. We observe that GPOP incurs up to 9x,\n6.8x and 5.5x less L2 cache misses compared to Ligra, GraphMat and Galois,\nrespectively. In terms of execution time, GPOP is upto 19x, 9.3x and 3.6x\nfaster than Ligra, GraphMat and Galois respectively.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 07:19:54 GMT"}, {"version": "v2", "created": "Tue, 1 Jan 2019 04:58:56 GMT"}, {"version": "v3", "created": "Tue, 19 Nov 2019 23:40:49 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Lakhotia", "Kartik", ""], ["Pati", "Sourav", ""], ["Kannan", "Rajgopal", ""], ["Prasanna", "Viktor", ""]]}, {"id": "1806.08206", "submitter": "Tangliu Wen", "authors": "Tangliu Wen", "title": "Proving Linearizability Using Reduction", "comments": "the co-authors of the paper require me to withdraw it", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lipton's reduction theory provides an intuitive and simple way for deducing\nthe non-interference properties of concurrent programs, but it is difficult to\ndirectly apply the technique to verify linearizability of sophisticated\nfine-grained concurrent data structures. In this paper, we propose three\nreduction-based proof methods that can handle such data structures. The key\nidea behind our reduction methods is that an irreducible operation can be\nviewed as an atomic operation at a higher level of abstraction. This allows us\nto focus on the reduction properties of an operation related to its abstract\nsemantics. We have successfully applied the methods to verify 11 concurrent\ndata structures including the most challenging ones: the Herlihy and Wing\nqueue, the HSY elimination-based stack, and the time-stamped queue, and the\nlazy list. Our methods inherit intuition and simplicity of Lipton's reduction,\nand concurrent data structures designers can easily and quickly learn to use\nthe methods.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 12:43:22 GMT"}, {"version": "v2", "created": "Wed, 29 Aug 2018 08:59:18 GMT"}, {"version": "v3", "created": "Thu, 30 Aug 2018 01:02:50 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Wen", "Tangliu", ""]]}, {"id": "1806.08266", "submitter": "Mohamad Moussa", "authors": "Mohamad Moussa and Marek Rychlik", "title": "Beyond RAID 6 --- an Efficient Systematic Code Protecting Against\n  Multiple Errors, Erasures, and Silent Data Corruption", "comments": "51 pages, Intellectual Property", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a replacement for RAID 6, based on a new linear, systematic code,\nwhich detects and corrects any combination of $E$ errors (unknown location) and\n$Z$ erasures (known location) provided that $Z+2E \\leq 4$. We investigate some\nscenarios for error correction beyond the code's minimum distance, using list\ndecoding. We describe a decoding algorithm with quasi-logarithmic time\ncomplexity, when parallel processing is used: $\\approx O(\\log N)$ where $N$ is\nthe number of disks in the array (similar to RAID 6).\n  By comparison, the error correcting code implemented by RAID 6 allows error\ndetection and correction only when $(E,Z)=(1,0)$, $(0,1)$, or $(0,2)$. Hence,\nwhen in degraded mode (i.e., when $Z \\geq 1$), RAID 6 loses its ability for\ndetecting and correcting random errors (i.e., $E=0$), leading to data loss\nknown as silent data corruption. In contrast, the proposed code does not\nexperience silent data corruption unless $Z \\geq 3$.\n  The aforementioned properties of our code, the relative simplicity of\nimplementation, vastly improved data protection, and low computational\ncomplexity of the decoding algorithm, make our code a natural successor to RAID\n6. As this code is based on the use of quintuple parity, this justifies the\nname PentaRAID for the RAID technology implementing the ideas of the current\npaper.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 14:29:47 GMT"}, {"version": "v2", "created": "Thu, 5 Jul 2018 20:21:20 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Moussa", "Mohamad", ""], ["Rychlik", "Marek", ""]]}, {"id": "1806.08658", "submitter": "Behrooz Razeghi", "authors": "Behrooz Razeghi, Slava Voloshynovskiy, Sohrab Ferdowsi and Dimche\n  Kostadinov", "title": "Privacy-Preserving Identification via Layered Sparse Code Design:\n  Distributed Servers and Multiple Access Authorization", "comments": "EUSIPCO 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.DB cs.DC cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new computationally efficient privacy-preserving identification\nframework based on layered sparse coding. The key idea of the proposed\nframework is a sparsifying transform learning with ambiguization, which\nconsists of a trained linear map, a component-wise nonlinearity and a privacy\namplification. We introduce a practical identification framework, which\nconsists of two phases: public and private identification. The public untrusted\nserver provides the fast search service based on the sparse privacy protected\ncodebook stored at its side. The private trusted server or the local client\napplication performs the refined accurate similarity search using the results\nof the public search and the layered sparse codebooks stored at its side. The\nprivate search is performed in the decoded domain and also the accuracy of\nprivate search is chosen based on the authorization level of the client. The\nefficiency of the proposed method is in computational complexity of encoding,\ndecoding, \"encryption\" (ambiguization) and \"decryption\" (purification) as well\nas storage complexity of the codebooks.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 18:06:55 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Razeghi", "Behrooz", ""], ["Voloshynovskiy", "Slava", ""], ["Ferdowsi", "Sohrab", ""], ["Kostadinov", "Dimche", ""]]}, {"id": "1806.08895", "submitter": "Nguyen Vo", "authors": "Nguyen Vo, Kyumin Lee, Thanh Tran", "title": "MRAttractor: Detecting Communities from Large-Scale Graphs", "comments": "Full paper accepted at IEEE Big Data 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting groups of users, who have similar opinions, interests, or social\nbehavior, has become an important task for many applications. A recent study\nshowed that dynamic distance based Attractor, a community detection algorithm,\noutperformed other community detection algorithms such as Spectral clustering,\nLouvain and Infomap, achieving higher Normalized Mutual Information (NMI) and\nAdjusted Rand Index (ARI). However, Attractor often takes long time to detect\ncommunities, requiring many iterations. To overcome the drawback and handle\nlarge-scale graphs, in this paper we propose MRAttractor, an advanced version\nof Attractor to be runnable on a MapReduce framework. In particular, we (i)\napply a sliding window technique to reduce the running time, keeping the same\ncommunity detection quality; (ii) design and implement the Attractor algorithm\nfor a MapReduce framework; and (iii) evaluate MRAttractor's performance on\nsynthetic and real-world datasets. Experimental results show that our algorithm\nsignificantly reduced running time and was able to handle large-scale graphs.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2018 02:38:26 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Vo", "Nguyen", ""], ["Lee", "Kyumin", ""], ["Tran", "Thanh", ""]]}, {"id": "1806.08901", "submitter": "Dingwen Tao", "authors": "Dingwen Tao, Sheng Di, Xin Liang, Zizhong Chen, Franck Cappello", "title": "Optimizing Lossy Compression Rate-Distortion from Automatic Online\n  Selection between SZ and ZFP", "comments": "14 pages, 9 figures, first revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With ever-increasing volumes of scientific data produced by HPC applications,\nsignificantly reducing data size is critical because of limited capacity of\nstorage space and potential bottlenecks on I/O or networks in writing/reading\nor transferring data. SZ and ZFP are the two leading lossy compressors\navailable to compress scientific data sets. However, their performance is not\nconsistent across different data sets and across different fields of some data\nsets: for some fields SZ provides better compression performance, while other\nfields are better compressed with ZFP. This situation raises the need for an\nautomatic online (during compression) selection between SZ and ZFP, with a\nminimal overhead. In this paper, the automatic selection optimizes the\nrate-distortion, an important statistical quality metric based on the\nsignal-to-noise ratio. To optimize for rate-distortion, we investigate the\nprinciples of SZ and ZFP. We then propose an efficient online, low-overhead\nselection algorithm that predicts the compression quality accurately for two\ncompressors in early processing stages and selects the best-fit compressor for\neach data field. We implement the selection algorithm into an open-source\nlibrary, and we evaluate the effectiveness of our proposed solution against\nplain SZ and ZFP in a parallel environment with 1,024 cores. Evaluation results\non three data sets representing about 100 fields show that our selection\nalgorithm improves the compression ratio up to 70% with the same level of data\ndistortion because of very accurate selection (around 99%) of the best-fit\ncompressor, with little overhead (less than 7% in the experiments).\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2018 03:40:39 GMT"}, {"version": "v2", "created": "Sat, 5 Jan 2019 14:26:30 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Tao", "Dingwen", ""], ["Di", "Sheng", ""], ["Liang", "Xin", ""], ["Chen", "Zizhong", ""], ["Cappello", "Franck", ""]]}, {"id": "1806.09199", "submitter": "Yuan Chen", "authors": "Yuan Chen, Soummya Kar, and Jos\\'e M. F. Moura", "title": "The Internet of Things: Secure Distributed Inference", "comments": null, "journal-ref": null, "doi": "10.1109/MSP.2018.2842097", "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growth in the number of devices connected to the Internet of Things (IoT)\nposes major challenges in security. The integrity and trustworthiness of data\nand data analytics are increasingly important concerns in IoT applications.\nThese are compounded by the highly distributed nature of IoT devices, making it\ninfeasible to prevent attacks and intrusions on all data sources. Adversaries\nmay hijack devices and compromise their data. As a result, reactive\ncountermeasures, such as intrusion detection and resilient analytics, become\nvital components of security. This paper overviews algorithms for secure\ndistributed inference in IoT.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jun 2018 19:50:31 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Chen", "Yuan", ""], ["Kar", "Soummya", ""], ["Moura", "Jos\u00e9 M. F.", ""]]}, {"id": "1806.09265", "submitter": "Faria Kalim", "authors": "Faria Kalim, Shadi A. Noghabi", "title": "Ben\\'e: On Demand Cost-Effective Scaling at the Edge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge computing has become increasingly popular across many domains and\nenterprises. However, given the locality constraint of edges (i.e., only\nclose-by edges are useful), multiplexing diverse workloads becomes challenging.\nThis results in poor resource utilization in edge resources that are\nprovisioned for peak demand. A simple way to allow multiplexing is through\nmicro-data centers, that bring computation close to the users while supporting\ndiverse workloads throughout the data, along with edges. In this paper, we\nargue for a hybrid approach of dedicated edge resources within an enterprise\nand on demand resources in micro-data centers that are shared across entities.\nWe show that this hybrid approach is an effective and cost-efficient way for\nscaling workloads and removes the need for over-provisioning dedicated\nresources per enterprise. Moreover, compared to a scaling approach that uses\nonly the edges across enterprises, micro-data centers also form a trusted third\nparty that can maintain important quality of service guarantees such as data\nprivacy, security, and availability.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 03:06:07 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Kalim", "Faria", ""], ["Noghabi", "Shadi A.", ""]]}, {"id": "1806.09466", "submitter": "Vaneet Aggarwal", "authors": "Abubakr Alabbasi and Vaneet Aggarwal", "title": "Optimized Video Streaming over Cloud: A Stall-Quality Trade-off", "comments": "15 pages. arXiv admin note: substantial text overlap with\n  arXiv:1703.08348", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As video-streaming services have expanded and improved, cloud-based video has\nevolved into a necessary feature of any successful business for reaching\ninternal and external audiences. In this paper, video streaming over\ndistributed storage is considered where the video segments are encoded using an\nerasure code for better reliability. There are multiple parallel streams\nbetween each server and the edge router. For each client request, we need to\ndetermine the subset of servers to get the data, as well as one of the parallel\nstream from each chosen server. In order to have this scheduling, this paper\nproposes a two-stage probabilistic scheduling. The selection of video quality\nis also chosen with a certain probability distribution. With these parameters,\nthe playback time of video segments is determined by characterizing the\ndownload time of each coded chunk for each video segment. Using the playback\ntimes, a bound on the moment generating function of the stall duration is used\nto bound the mean stall duration. Based on this, we formulate an optimization\nproblem to jointly optimize the convex combination of mean stall duration and\naverage video quality for all requests, where the two-stage probabilistic\nscheduling, probabilistic video quality selection, bandwidth split among\nparallel streams, and auxiliary bound parameters can be chosen. This non-convex\nproblem is solved using an efficient iterative algorithm. Evaluation results\nshow significant improvement in QoE metrics for cloud-based video as compared\nto the considered baselines.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 06:59:11 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Alabbasi", "Abubakr", ""], ["Aggarwal", "Vaneet", ""]]}, {"id": "1806.09528", "submitter": "Paolo Viviani", "authors": "Paolo Viviani, Maurizio Drocco, Marco Aldinucci", "title": "Pushing the boundaries of parallel Deep Learning -- A practical approach", "comments": "12 pages, 4 figures, 61 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work aims to assess the state of the art of data parallel deep neural\nnetwork training, trying to identify potential research tracks to be exploited\nfor performance improvement. Beside, it presents a design for a practical C++\nlibrary dedicated at implementing and unifying the current state of the art\nmethodologies for parallel training in a performance-conscious framework,\nallowing the user to explore novel strategies without departing significantly\nfrom its usual work-flow.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 15:30:33 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Viviani", "Paolo", ""], ["Drocco", "Maurizio", ""], ["Aldinucci", "Marco", ""]]}, {"id": "1806.09582", "submitter": "Seyedmohammad Salehi", "authors": "Seyedmohammad Salehi, Li Li, Chien-Chung Shen, Leonard Cimini, John\n  Graybeal", "title": "Traffic Differentiation in Dense WLANs with CSMA/ECA-DR MAC Protocol", "comments": "This work has been accepted by IEEE VTC 2018 Fall conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In today's WLANs, scheduling of packet transmissions solely relies on the\ncollision and success a station may experience. To better support traffic\ndifferentiation in dense WLANs, in this paper, we propose a distributed\nreservation mechanism for the Carrier Sense Multiple Access Extended Collision\nAvoidance (CSMA/ECA) MAC protocol, termed CSMA/ECA-DR, based on which stations\ncan collaboratively achieve higher network performance. In addition, proper\nContention Window (CW) will be chosen based on the instantaneously estimated\nnumber of active contenders in the network. Simulation results from dense\nscenarios with traffic differentiation demonstrate that CSMA/ECA-DR can greatly\nimprove the efficiency of WLANs for traffic differentiation even with large\nnumbers of contenders.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 17:26:15 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Salehi", "Seyedmohammad", ""], ["Li", "Li", ""], ["Shen", "Chien-Chung", ""], ["Cimini", "Leonard", ""], ["Graybeal", "John", ""]]}, {"id": "1806.09847", "submitter": "Mohamad Ahmadi", "authors": "Mohamad Ahmadi and Fabian Kuhn and Shay Kutten and Anisur Rahaman\n  Molla and Gopal Pandurangan", "title": "The Communication Cost of Information Spreading in Dynamic Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the message complexity of distributed information\nspreading (a.k.a gossip or token dissemination) in adversarial dynamic\nnetworks, where the goal is to spread $k$ tokens of information to every node\non an $n$-node network. We consider the amortized (average) message complexity\nof spreading a token, assuming that the number of tokens is large. Our focus is\non token-forwarding algorithms, which do not manipulate tokens in any way other\nthan storing, copying, and forwarding them.\n  We consider two types of adversaries that arbitrarily rewire the network\nwhile keeping it connected: the adaptive adversary that is aware of the status\nof all the nodes and the algorithm (including the current random choices), and\nthe oblivious adversary that is oblivious to the random choices made by the\nalgorithm. The central question that motivates our work is whether one can\nachieve subquadratic amortized message complexity for information spreading.\n  We present two sets of results depending on how nodes send messages to their\nneighbors: (1) Local broadcast: We show a tight lower bound of $\\Omega(n^2)$ on\nthe number of amortized local broadcasts, which is matched by the naive\nflooding algorithm, (2) Unicast: We study the message complexity as a function\nof the number of dynamic changes in the network. To facilitate this, we\nintroduce a natural complexity measure for analyzing dynamic networks called\nadversary-competitive message complexity where the adversary pays a unit cost\nfor every topological change. Under this model, it is shown that if $k$ is\nsufficiently large, we can obtain an optimal amortized message complexity of\n$O(n)$. We also present a randomized algorithm that achieves subquadratic\namortized message complexity when the number of tokens is not large under an\noblivious adversary.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 08:53:19 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Ahmadi", "Mohamad", ""], ["Kuhn", "Fabian", ""], ["Kutten", "Shay", ""], ["Molla", "Anisur Rahaman", ""], ["Pandurangan", "Gopal", ""]]}, {"id": "1806.10036", "submitter": "Zuchao Zhang", "authors": "Z. Zhang, B. Xiao, Z. Ji, Y. Wang, P. Wang", "title": "The application of precision time protocol on EAST timing system", "comments": "4 pages, 6 figures, 21st IEEE Real Time Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The timing system focuses on synchronizing and coordinating each subsystem\naccording to the trigger signals. A new prototype timing slave node based on\nprecision time protocol has been developed by using ARM STM32 platform. The\nproposed slave timing module is tested and results show that the\nsynchronization accuracy between slave nodes is in sub-microsecond range.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jun 2018 22:24:37 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Zhang", "Z.", ""], ["Xiao", "B.", ""], ["Ji", "Z.", ""], ["Wang", "Y.", ""], ["Wang", "P.", ""]]}, {"id": "1806.10037", "submitter": "Ayush Singhal", "authors": "Ayush Singhal, Rakesh Pant, Pradeep Sinha", "title": "AlertMix: A Big Data platform for multi-source streaming data", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The demand for stream processing is increasing at an unprecedented rate. Big\ndata is no longer limited to processing of big volumes of data. In most\nreal-world scenarios, the need for processing stream data as it comes can only\nmeet the business needs. It is required for trading, fraud detection, system\nmonitoring, product maintenance and of course social media data such as Twitter\nand YouTube videos. In such cases, a \"too late architecture\" that focuses on\nbatch processing cannot realize the use cases. In this article, we present an\nend to end Big data platform called AlertMix for processing multi-source\nstreaming data. Its architecture and how various Big data technologies are\nutilized are explained in this work. We present the performance of our platform\non real live streaming data which is currently handled by the platform.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 22:53:42 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Singhal", "Ayush", ""], ["Pant", "Rakesh", ""], ["Sinha", "Pradeep", ""]]}, {"id": "1806.10113", "submitter": "Nicolas Guil", "authors": "A.J. L\\'azaro-Mu\\~noz, J.M. Gonz\\'alez-Linares, J. G\\'omez-Luna, N.\n  Guil", "title": "Improving tasks throughput on accelerators using OpenCL command\n  concurrency", "comments": "This paper is an extension of the work \"Efficient OpenCL-based\n  concurrent tasks offloading on accelerators\" published in Procedia Computer\n  Science (ICCS 2017), Vol. 108, pp. 1353-2357, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A heterogeneous architecture composed by a host and an accelerator must\nfrequently deal with situations where several independent tasks are available\nto be offloaded onto the accelerator. These tasks can be generated by\nconcurrent applications executing in the host or, in case the host is a node of\na computer cluster, by applications running on other cluster nodes that are\nwilling to offload tasks in the accelerator connected to the host. In this work\nwe show that a runtime scheduler that selects the best execution order of a\ngroup of tasks on the accelerator can significantly reduce the total execution\ntime of the tasks and, consequently, increase the accelerator use. Our solution\nis based on a temporal execution model that is able to predict with high\naccuracy the execution time of a set of concurrent tasks launched on the\naccelerator. The execution model has been validated in AMD, NVIDIA, and Xeon\nPhi devices using synthetic benchmarks. Moreover, employing the temporal\nexecution model, a heuristic is proposed which is able to establish a\nnear-optimal tasks execution ordering that significantly reduces the total\nexecution time, including data transfers.The heuristic has been evaluated with\nfive different benchmarks composed of dominant kernel and dominant transfer\nreal tasks. Experiments indicate the heuristic is able to find always an\nordering with a better execution time than the average of every possible\nexecution order and, most times, it achieves a near-optimal ordering (very\nclose to the execution time of the best execution order) with a negligible\noverhead. Concretely, our heuristic obtains, on average for all the devices,\nbetween 84\\% and 96\\% of the improvement achieved by the best execution order.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 17:18:03 GMT"}, {"version": "v2", "created": "Sun, 1 Jul 2018 20:11:47 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["L\u00e1zaro-Mu\u00f1oz", "A. J.", ""], ["Gonz\u00e1lez-Linares", "J. M.", ""], ["G\u00f3mez-Luna", "J.", ""], ["Guil", "N.", ""]]}, {"id": "1806.10254", "submitter": "Nuno Pregui\\c{c}a", "authors": "Nuno Pregui\\c{c}a", "title": "Conflict-free Replicated Data Types: An Overview", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet-scale distributed systems often replicate data at multiple\ngeographic locations to provide low latency and high availability, despite node\nand network failures. Geo-replicated systems that adopt a weak consistency\nmodel allow replicas to temporarily diverge, requiring a mechanism for merging\nconcurrent updates into a common state. Conflict-free Replicated Data Types\n(CRDT) provide a principled approach to address this problem. This document\npresents an overview of Conflict-free Replicated Data Types research and\npractice, organizing the presentation in the aspects relevant for the\napplication developer, the system developer and the CRDT developer.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 00:09:59 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Pregui\u00e7a", "Nuno", ""]]}, {"id": "1806.10773", "submitter": "Yang Yang", "authors": "Yang Yang and Marius Pesavento and Symeon Chatzinotas and Bj\\\"orn\n  Ottersten", "title": "Successive Convex Approximation Algorithms for Sparse Signal Estimation\n  with Nonconvex Regularizations", "comments": "submitted to IEEE Journal of Selected Topics in Signal Processing,\n  special issue in Robust Subspace Learning", "journal-ref": null, "doi": "10.1109/JSTSP.2018.2877584", "report-no": null, "categories": "cs.LG cs.DC cs.IR math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a successive convex approximation framework for\nsparse optimization where the nonsmooth regularization function in the\nobjective function is nonconvex and it can be written as the difference of two\nconvex functions. The proposed framework is based on a nontrivial combination\nof the majorization-minimization framework and the successive convex\napproximation framework proposed in literature for a convex regularization\nfunction. The proposed framework has several attractive features, namely, i)\nflexibility, as different choices of the approximate function lead to different\ntype of algorithms; ii) fast convergence, as the problem structure can be\nbetter exploited by a proper choice of the approximate function and the\nstepsize is calculated by the line search; iii) low complexity, as the\napproximate function is convex and the line search scheme is carried out over a\ndifferentiable function; iv) guaranteed convergence to a stationary point. We\ndemonstrate these features by two example applications in subspace learning,\nnamely, the network anomaly detection problem and the sparse subspace\nclustering problem. Customizing the proposed framework by adopting the\nbest-response type approximation, we obtain soft-thresholding with exact line\nsearch algorithms for which all elements of the unknown parameter are updated\nin parallel according to closed-form expressions. The attractive features of\nthe proposed algorithms are illustrated numerically.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 05:21:16 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Yang", "Yang", ""], ["Pesavento", "Marius", ""], ["Chatzinotas", "Symeon", ""], ["Ottersten", "Bj\u00f6rn", ""]]}, {"id": "1806.10853", "submitter": "Vaneet Aggarwal", "authors": "Eric Friedlander and Vaneet Aggarwal", "title": "Generalization of LRU Cache Replacement Policy with Applications to\n  Video Streaming", "comments": "Accepted to ACM TOMPECS, Jun 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Caching plays a crucial role in networking systems to reduce the load on the\nnetwork and is commonly employed by content delivery networks (CDNs) in order\nto improve performance. One of the commonly used mechanisms, Least Recently\nUsed (LRU), works well for identical file sizes. However, for asymmetric file\nsizes, the performance deteriorates. This paper proposes an adaptation to the\nLRU strategy, called gLRU, where the file is sub-divided into equal-sized\nchunks. In this strategy, a chunk of the newly requested file is added in the\ncache, and a chunk of the least-recently-used file is removed from the cache.\nEven though approximate analysis for the hit rate has been studied for LRU, the\nanalysis does not extend to gLRU since the metric of interest is no longer the\nhit rate as the cache has partial files. This paper provides a novel\napproximation analysis for this policy where the cache may have partial file\ncontents. The approximation approach is validated by simulations. Further, gLRU\noutperforms the LRU strategy for a Zipf file popularity distribution and\ncensored Pareto file size distribution for the file download times. Video\nstreaming applications can further use the partial cache contents to help the\nstall duration significantly, and the numerical results indicate significant\nimprovements (32\\%) in stall duration using the gLRU strategy as compared to\nthe LRU strategy. Furthermore, the gLRU replacement policy compares favorably\nto two other cache replacement policies when simulated on MSR Cambridge Traces\nobtained from the SNIA IOTTA repository.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 09:44:56 GMT"}, {"version": "v2", "created": "Mon, 24 Jun 2019 01:56:07 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Friedlander", "Eric", ""], ["Aggarwal", "Vaneet", ""]]}, {"id": "1806.10929", "submitter": "Yvonne Anne Pignolet", "authors": "Thomas Locher and Sebastian Obermeier and Yvonne-Anne Pignolet", "title": "When Can a Distributed Ledger Replace a Trusted Third Party?", "comments": "Published at IEEE Blockchain 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The functionality that distributed ledger technology provides, i.e., an\nimmutable and fraud-resistant registry with validation and verification\nmechanisms, has traditionally been implemented with a trusted third party. Due\nto the distributed nature of ledger technology, there is a strong recent trend\ntowards using ledgers to implement novel decentralized applications for a wide\nrange of use cases, e.g., in the financial sector and sharing economy. While\nthere can be several arguments for the use of a ledger, the key question is\nwhether it can fully replace any single trusted party in the system as\notherwise a (potentially simpler) solution can be built around the trusted\nparty. In this paper, we introduce an abstract view on ledger use cases and\npresent two fundamental criteria that must be met for any use case to be\nimplemented using a ledger-based approach without having to rely on any\nparticular party in the system. Moreover, we evaluate several ledger use cases\nthat have recently received considerable attention according to these criteria,\nrevealing that often participants need to trust each other despite using a\ndistributed ledger. Consequently, the potential of using a ledger as a\nreplacement for a trusted party is limited for these use cases.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 13:00:06 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Locher", "Thomas", ""], ["Obermeier", "Sebastian", ""], ["Pignolet", "Yvonne-Anne", ""]]}, {"id": "1806.10939", "submitter": "Rohitash Chandra", "authors": "Rohitash Chandra, R. Dietmar M\\\"uller, Danial Azam, Ratneel Deo,\n  Nathaniel Butterworth, Tristan Salles, Sally Cripps", "title": "Multi-core parallel tempering Bayeslands for basin and landscape\n  evolution", "comments": null, "journal-ref": "Geochemistry, Geophysics, Geosystems, 2019", "doi": "10.1029/2019GC008465", "report-no": null, "categories": "physics.geo-ph cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Bayesian paradigm is becoming an increasingly popular framework for\nestimation and uncertainty quantification of unknown parameters in geo-physical\ninversion problems. Badlands is a basin and landscape evolution forward model\nfor simulating topography evolution at a large range of spatial and time\nscales. Our previous work presented Bayeslands that used the Bayesian paradigm\nto make inference for unknown parameters in the Badlands model using Markov\nchain Monte Carlo (MCMC) sampling. Bayeslands faced challenges in convergence\ndue to multi-modal posterior distributions in the selected parameters of\nBadlands. Parallel tempering is an advanced MCMC method suited for irregular\nand multi-modal posterior distributions. In this paper, we extend Bayeslands\nusing parallel tempering (PT-Bayeslands) with high performance computing to\naddress previous limitations in parameter space exploration in the context of\nthe computationally expensive Badlands model. Our results show that\nPT-Bayeslands not only reduces the computation time, but also provides an\nimprovement of the sampling for multi-modal posterior distributions. This\nprovides an improvement over Bayeslands which used single chain MCMC that face\ndifficulties in convergence and can lead to misleading inference. This\nmotivates its usage in large-scale basin and landscape evolution models.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2018 03:39:27 GMT"}, {"version": "v2", "created": "Sat, 20 Jul 2019 03:30:02 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Chandra", "Rohitash", ""], ["M\u00fcller", "R. Dietmar", ""], ["Azam", "Danial", ""], ["Deo", "Ratneel", ""], ["Butterworth", "Nathaniel", ""], ["Salles", "Tristan", ""], ["Cripps", "Sally", ""]]}, {"id": "1806.11128", "submitter": "Justin Deters", "authors": "Justin Deters, Jiaye Wu, Yifan Xu, I-Ting Angelina Lee", "title": "A NUMA-Aware Provably-Efficient Task-Parallel Platform Based on the\n  Work-First Principle", "comments": "14 pages, 9 figures", "journal-ref": "2018 IEEE International Symposium on Workload Characterization\n  (IISWC), Raleigh, NC, USA, 2018, pp. 59-70", "doi": "10.1109/IISWC.2018.8573486", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Task parallelism is designed to simplify the task of parallel programming.\nWhen executing a task parallel program on modern NUMA architectures, it can\nfail to scale due to the phenomenon called work inflation, where the overall\nprocessing time that multiple cores spend on doing useful work is higher\ncompared to the time required to do the same amount of work on one core, due to\neffects experienced only during parallel executions such as additional cache\nmisses, remote memory accesses, and memory bandwidth issues. It's possible to\nmitigate work inflation by co-locating the computation with the data, but this\nis nontrivial to do with task parallel programs. First, by design, the\nscheduling for task parallel programs is automated, giving the user little\ncontrol over where the computation is performed. Second, the platforms tend to\nemploy work stealing, which provides strong theoretical guarantees, but its\nrandomized protocol for load balancing does not discern between work items that\nare far away versus ones that are closer. In this work, we propose NUMA-WS, a\nNUMA-aware task parallel platform engineering based on the work-first\nprinciple. By abiding by the work-first principle, we are able to obtain a\nplatform that is work efficient, provides the same theoretical guarantees as\nthe classic work stealing scheduler, and mitigates work inflation. Furthermore,\nwe implemented a prototype platform by modifying Intel's Cilk Plus runtime\nsystem and empirically demonstrate that the resulting system is work efficient\nand scalable.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 18:00:42 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2018 17:02:41 GMT"}, {"version": "v3", "created": "Wed, 5 Sep 2018 18:29:45 GMT"}, {"version": "v4", "created": "Mon, 7 Jan 2019 15:46:26 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Deters", "Justin", ""], ["Wu", "Jiaye", ""], ["Xu", "Yifan", ""], ["Lee", "I-Ting Angelina", ""]]}, {"id": "1806.11190", "submitter": "M. Hadi Amini", "authors": "M. Hadi Amini, Javad Mohammadi, Soummya Kar", "title": "Fully Distributed Cooperative Charging for Plug-in Electric Vehicles in\n  Constrained Power Networks", "comments": "7 pages, 4 figures, conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plug-in Electric Vehicles (PEVs) play a pivotal role in transportation\nelectrification. The flexible nature of PEVs' charging demand can be utilized\nfor reducing charging cost as well as optimizing the operating cost of power\nand transportation networks. Utilizing charging flexibilities of geographically\nspread PEVs requires design and implementation of efficient optimization\nalgorithms. To this end, we propose a fully distributed algorithm to solve the\nPEVs' Cooperative Charging with Power constraints (PEV-CCP). Our solution\nconsiders the electric power limits that originate from physical\ncharacteristics of charging station, such as on-site transformer capacity\nlimit, and allows for containing charging burden of PEVs on the electric\ndistribution network. Our approach is also motivated by the increasing load\ndemand at the distribution level due to additional PEV charging demand. Our\nproposed approach distributes computation among agents (PEVs) to solve the\nPEV-CCP problem in a distributed fashion through an iterative interaction\nbetween neighboring agents. The structure of each agent's update functions\nensures an agreement on a price signal while enforcing individual PEV\nconstraints. In addition to converging towards the globally-optimum solution,\nour algorithm ensures the feasibility of each PEV's decision at each iteration.\nWe have tested performance of the proposed approach using a fleet of PEVs.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 21:03:05 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Amini", "M. Hadi", ""], ["Mohammadi", "Javad", ""], ["Kar", "Soummya", ""]]}, {"id": "1806.11448", "submitter": "Martin Henze", "authors": "Martin Henze, Roman Matzutt, Jens Hiller, Erik M\\\"uhmer, Jan Henrik\n  Ziegeldorf, Johannes van der Giet, Klaus Wehrle", "title": "Complying with Data Handling Requirements in Cloud Storage Systems", "comments": "14 pages, 11 figures; revised manuscript, accepted for publication in\n  IEEE Transactions on Cloud Computing", "journal-ref": null, "doi": "10.1109/TCC.2020.3000336", "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In past years, cloud storage systems saw an enormous rise in usage. However,\ndespite their popularity and importance as underlying infrastructure for more\ncomplex cloud services, today's cloud storage systems do not account for\ncompliance with regulatory, organizational, or contractual data handling\nrequirements by design. Since legislation increasingly responds to rising data\nprotection and privacy concerns, complying with data handling requirements\nbecomes a crucial property for cloud storage systems. We present PRADA, a\npractical approach to account for compliance with data handling requirements in\nkey-value based cloud storage systems. To achieve this goal, PRADA introduces a\ntransparent data handling layer, which empowers clients to request specific\ndata handling requirements and enables operators of cloud storage systems to\ncomply with them. We implement PRADA on top of the distributed database\nCassandra and show in our evaluation that complying with data handling\nrequirements in cloud storage systems is practical in real-world cloud\ndeployments as used for microblogging, data sharing in the Internet of Things,\nand distributed email storage.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 14:40:03 GMT"}, {"version": "v2", "created": "Sun, 7 Jun 2020 18:45:40 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Henze", "Martin", ""], ["Matzutt", "Roman", ""], ["Hiller", "Jens", ""], ["M\u00fchmer", "Erik", ""], ["Ziegeldorf", "Jan Henrik", ""], ["van der Giet", "Johannes", ""], ["Wehrle", "Klaus", ""]]}, {"id": "1806.11509", "submitter": "Chengbo Yang", "authors": "Chengbo Yang", "title": "An Efficient Dispatcher for Large Scale GraphProcessing on OpenCL-based\n  FPGAs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High parallel framework has been proved to be very suitable for graph\nprocessing. There are various work to optimize the implementation in FPGAs, a\npipeline parallel device. The key to make use of the parallel performance of\nFPGAs is to process graph data in pipeline model and take advantage of on-chip\nmemory to realize necessary locality process. This paper proposes a modularize\ngraph processing framework, which focus on the whole executing procedure with\nthe extremely different degree of parallelism. The framework has three\ncontributions. First, the combination of vertex-centric and edge-centric\nprocessing framework can been adjusting in the executing procedure to\naccommodate top-down algorithm and bottom-up algorithm. Second, owing to the\npipeline parallel and finite on-chip memory accelerator, the novel edge-block,\na block consist of edges vertex, achieve optimizing the way to utilize the\non-chip memory to group the edges and stream the edges in a block to realize\nthe stream pattern to pipeline parallel processing. Third, depending to the\nanalysis of the block structure of nature graph and the executing\ncharacteristics during graph processing, we design a novel conversion\ndispatcher to change processing module, to match the corresponding exchange\npoint.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 11:14:38 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Yang", "Chengbo", ""]]}, {"id": "1806.11536", "submitter": "Amirhossein Reisizadeh", "authors": "Amirhossein Reisizadeh, Aryan Mokhtari, Hamed Hassani, Ramtin\n  Pedarsani", "title": "An Exact Quantized Decentralized Gradient Descent Algorithm", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2019.2932876", "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of decentralized consensus optimization, where the\nsum of $n$ smooth and strongly convex functions are minimized over $n$\ndistributed agents that form a connected network. In particular, we consider\nthe case that the communicated local decision variables among nodes are\nquantized in order to alleviate the communication bottleneck in distributed\noptimization. We propose the Quantized Decentralized Gradient Descent (QDGD)\nalgorithm, in which nodes update their local decision variables by combining\nthe quantized information received from their neighbors with their local\ninformation. We prove that under standard strong convexity and smoothness\nassumptions for the objective function, QDGD achieves a vanishing mean solution\nerror under customary conditions for quantizers. To the best of our knowledge,\nthis is the first algorithm that achieves vanishing consensus error in the\npresence of quantization noise. Moreover, we provide simulation results that\nshow tight agreement between our derived theoretical convergence rate and the\nnumerical results.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 17:02:54 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 23:27:11 GMT"}, {"version": "v3", "created": "Fri, 2 Aug 2019 01:10:39 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Reisizadeh", "Amirhossein", ""], ["Mokhtari", "Aryan", ""], ["Hassani", "Hamed", ""], ["Pedarsani", "Ramtin", ""]]}, {"id": "1806.11546", "submitter": "Arezoo Khatibi", "authors": "Arezoo Khatibi and Omid Khatibi", "title": "Efficient Multichannel in XML Wireless Broadcast Stream", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we recommend the use of multi-channel for XML data in wireless\nbroadcasting. First we divide XML data into information units as bucket, then\nextract path information (XPath) for any unit and build an index tree from the\ndata path. Finally, make wireless data stream with merging parts of index tree\nand parts of XML data in multichannel XML. Then, create a protocol that allows\nmobile users access to the wireless XML stream generated with our method. We\nstudy 11 channels in server side and 3 orthogonal channels in client side.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 18:14:08 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Khatibi", "Arezoo", ""], ["Khatibi", "Omid", ""]]}, {"id": "1806.11547", "submitter": "Philip Colangelo", "authors": "Philip Colangelo, Nasibeh Nasiri, Asit Mishra, Eriko Nurvitadhi,\n  Martin Margala, Kevin Nealis", "title": "Exploration of Low Numeric Precision Deep Learning Inference Using Intel\n  FPGAs", "comments": "To Appear In The 26th IEEE International Symposium on\n  Field-Programmable Custom Computing Machines", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CNNs have been shown to maintain reasonable classification accuracy when\nquantized to lower precisions. Quantizing to sub 8-bit activations and weights\ncan result in accuracy falling below an acceptable threshold. Techniques exist\nfor closing the accuracy gap of limited numeric precision typically by\nincreasing computation. This results in a trade-off between throughput and\naccuracy and can be tailored for different networks through various\ncombinations of activation and weight data widths. Hardware architectures like\nFPGAs provide the opportunity for data width specific computation through\nunique logic configurations leading to highly optimized processing that is\nunattainable by full precision networks. Ternary and binary weighted networks\noffer an efficient method of inference for 2-bit and 1-bit data respectively.\nMost hardware architectures can take advantage of the memory storage and\nbandwidth savings that come along with smaller datapaths, but very few\narchitectures can take advantage of limited numeric precision at the\ncomputation level. In this paper, we present a hardware design for FPGAs that\ntakes advantage of bandwidth, memory, power, and computation savings of limited\nnumerical precision data. We provide insights into the trade-offs between\nthroughput and accuracy for various networks and how they map to our framework.\nFurther, we show how limited numeric precision computation can be efficiently\nmapped onto FPGAs for both ternary and binary cases. Starting with Arria 10, we\nshow a 2-bit activation and ternary weighted AlexNet running in hardware that\nachieves 3,700 images per second on the ImageNet dataset with a top-1 accuracy\nof 0.49. Using a hardware modeler designed for our low numeric precision\nframework we project performance most notably for a 55.5 TOPS Stratix 10 device\nrunning a modified ResNet-34 with only 3.7% accuracy degradation compared with\nsingle precision.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 22:00:31 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Colangelo", "Philip", ""], ["Nasiri", "Nasibeh", ""], ["Mishra", "Asit", ""], ["Nurvitadhi", "Eriko", ""], ["Margala", "Martin", ""], ["Nealis", "Kevin", ""]]}, {"id": "1806.11552", "submitter": "Li Lin", "authors": "Li Lin, Xiaofei Liao", "title": "Echo: An Edge-Centric Code Offloading System with Quality of Service\n  Guarantee", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Code offloading is promising to accelerate mobile applications and save\nenergy of mobile devices by shifting some computation to cloud. However,\nexisting code offloading systems suffer from a long communication delay between\nmobile devices and cloud. To address this challenge, in this paper, we consider\nto deploy edge nodes in the proximity of mobile devices, and study how they\nbenefit code offloading. We design an edge-centric code offloading system,\ncalled Echo, over a three-layer computing hierarchy consisting of mobile\ndevices, edge and cloud. A critical problem needs to be addressed by Echo is to\ndecide which method should be offloaded to which computing platform (edge or\ncloud). Different from existing offloading systems that let mobile devices\nindividually make offloading decisions, Echo implements a centralized decision\nengine at the edge node. This edge-centric design can fully exploit the limited\nhardware resources at the edge to provide an offloading service with Quality of\nService guarantee. Furthermore, we propose some novel mechanisms, e.g., lazy\nobject transmission and differential object update, to further improve system\nperformance. The results of a small-scale real deployment and trace-driven\nsimulations show that Echo significantly outperforms existing\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 16:38:44 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Lin", "Li", ""], ["Liao", "Xiaofei", ""]]}, {"id": "1806.11553", "submitter": "Arezoo Khatibi", "authors": "Arezoo Khatibi and Omid Khatibi", "title": "An Improved Energy-Aware Clustering Method for the Regional Queries in\n  the Internet of Things", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We will offer a method to improve energy efficient consumption for processing\nqueries on the Internet of Things. We focused on an energy efficient\nhierarchical clustering index tree such that we can facilitate time-correlated\nregion queries in the I.O.T (Internet of Things). We try to improve clustering\nand make a change on its proposed index tree. We try to do this by optimizing\nthe query processing. We improve clustering to increase the accuracy of the\nInternet of Things and prevent the network from disconnecting. In the article\nthat we have chosen, there is a heterogeneous cluster which means there exists\na large data difference in the two ends of a cluster. Also, it often happens\nthat the same information is sent to the base station by two overlapping\nclusters; therefore, we save energy by eliminating duplicated data.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 17:57:26 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Khatibi", "Arezoo", ""], ["Khatibi", "Omid", ""]]}, {"id": "1806.11555", "submitter": "Marcelo Fernandes", "authors": "Matheus F. Torquato and Marcelo A. C. Fernandes", "title": "High-Performance Parallel Implementation of Genetic Algorithm on FPGA", "comments": "27 pages, 16 figures", "journal-ref": null, "doi": "10.1007/s00034-019-01037-w", "report-no": null, "categories": "cs.DC cs.AI cs.AR eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genetic Algorithms (GAs) are used to solve search and optimization problems\nin which an optimal solution can be found using an iterative process with\nprobabilistic and non-deterministic transitions. However, depending on the\nproblem's nature, the time required to find a solution can be high in\nsequential machines due to the computational complexity of genetic algorithms.\nThis work proposes a parallel implementation of a genetic algorithm on\nfield-programmable gate array (FPGA). Optimization of the system's processing\ntime is the main goal of this project. Results associated with the processing\ntime and area occupancy (on FPGA) for various population sizes are analyzed.\nStudies concerning the accuracy of the GA response for the optimization of two\nvariables functions were also evaluated for the hardware implementation.\nHowever, the high-performance implementation proposes in this paper is able to\nwork with more variable from some adjustments on hardware architecture.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 18:30:27 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Torquato", "Matheus F.", ""], ["Fernandes", "Marcelo A. C.", ""]]}, {"id": "1806.11558", "submitter": "Peter Zaspel", "authors": "Helmut Harbrecht, Peter Zaspel", "title": "A scalable H-matrix approach for the solution of boundary integral\n  equations on multi-GPU clusters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider the solution of boundary integral equations by\nmeans of a scalable hierarchical matrix approach on clusters equipped with\ngraphics hardware, i.e. graphics processing units (GPUs). To this end, we\nextend our existing single-GPU hierarchical matrix library hmglib such that it\nis able to scale on many GPUs and such that it can be coupled to arbitrary\napplication codes. Using a model GPU implementation of a boundary element\nmethod (BEM) solver, we are able to achieve more than 67 percent relative\nparallel speed-up going from 128 to 1024 GPUs for a model geometry test case\nwith 1.5 million unknowns and a real-world geometry test case with almost 1.2\nmillion unknowns. On 1024 GPUs of the cluster Titan, it takes less than 6\nminutes to solve the 1.5 million unknowns problem, with 5.7 minutes for the\nsetup phase and 20 seconds for the iterative solver. To the best of the\nauthors' knowledge, we here discuss the first fully GPU-based\ndistributed-memory parallel hierarchical matrix Open Source library using the\ntraditional H-matrix format and adaptive cross approximation with an\napplication to BEM problems.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 07:39:20 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Harbrecht", "Helmut", ""], ["Zaspel", "Peter", ""]]}]