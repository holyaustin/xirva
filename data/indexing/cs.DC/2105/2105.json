[{"id": "2105.00027", "submitter": "Weile Wei", "authors": "Weile Wei, Eduardo D'Azevedo, Kevin Huck, Arghya Chatterjee, Oscar\n  Hernandez, Hartmut Kaiser", "title": "Memory Reduction using a Ring Abstraction over GPU RDMA for Distributed\n  Quantum Monte Carlo Solver", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cond-mat.mtrl-sci cond-mat.str-el cond-mat.supr-con", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific applications that run on leadership computing facilities often\nface the challenge of being unable to fit leading science cases onto\naccelerator devices due to memory constraints (memory-bound applications). In\nthis work, the authors studied one such US Department of Energy\nmission-critical condensed matter physics application, Dynamical Cluster\nApproximation (DCA++), and this paper discusses how device memory-bound\nchallenges were successfully reduced by proposing an effective \"all-to-all\"\ncommunication method -- a ring communication algorithm. This implementation\ntakes advantage of acceleration on GPUs and remote direct memory access (RDMA)\nfor fast data exchange between GPUs.\n  Additionally, the ring algorithm was optimized with sub-ring communicators\nand multi-threaded support to further reduce communication overhead and expose\nmore concurrency, respectively. The computation and communication were also\nanalyzed by using the Autonomic Performance Environment for Exascale (APEX)\nprofiling tool, and this paper further discusses the performance trade-off for\nthe ring algorithm implementation. The memory analysis on the ring algorithm\nshows that the allocation size for the authors' most memory-intensive data\nstructure per GPU is now reduced to 1/p of the original size, where p is the\nnumber of GPUs in the ring communicator. The communication analysis suggests\nthat the distributed Quantum Monte Carlo execution time grows linearly as\nsub-ring size increases, and the cost of messages passing through the network\ninterface connector could be a limiting factor.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 18:11:54 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 13:43:52 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Wei", "Weile", ""], ["D'Azevedo", "Eduardo", ""], ["Huck", "Kevin", ""], ["Chatterjee", "Arghya", ""], ["Hernandez", "Oscar", ""], ["Kaiser", "Hartmut", ""]]}, {"id": "2105.00039", "submitter": "Ahmad Hesam", "authors": "Ahmad Hesam, Lukas Breitwieser, Fons Rademakers, Zaid Al-Ars", "title": "GPU Acceleration of 3D Agent-Based Biological Simulations", "comments": "8 pages, 12 figures", "journal-ref": null, "doi": "10.1109/IPDPSW52791.2021.00040", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Researchers in biology are faced with the tough challenge of developing\nhigh-performance computer simulations of their increasingly complex agent-based\nmodels. BioDynaMo is an open-source agent-based simulation platform that aims\nto alleviate researchers from the intricacies that go into the development of\nhigh-performance computing. Through a high-level interface, researchers can\nimplement their models on top of BioDynaMo's multi-threaded core execution\nengine to rapidly develop simulations that effectively utilize parallel\ncomputing hardware. In biological agent-based modeling, the type of operations\nthat are typically the most compute-intensive are those that involve agents\ninteracting with their local neighborhood. In this work, we investigate the\ncurrently implemented method of handling neighborhood interactions of cellular\nagents in BioDynaMo, and ways to improve the performance to enable large-scale\nand complex simulations. We propose to replace the kd-tree implementation to\nfind and iterate over the neighborhood of each agent with a uniform grid method\nthat allows us to take advantage of the massively parallel architecture of\ngraphics processing units (GPUs). We implement the uniform grid method in both\nCUDA and OpenCL to address GPUs from all major vendors and evaluate several\ntechniques to further improve the performance. Furthermore, we analyze the\nperformance of our implementations for models with a varying density of\nneighboring agents. As a result, the performance of the mechanical interactions\nmethod improved by up to two orders of magnitude in comparison to the\nmultithreaded baseline version. The implementations are open-source and\npublicly available on Github.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 18:37:51 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Hesam", "Ahmad", ""], ["Breitwieser", "Lukas", ""], ["Rademakers", "Fons", ""], ["Al-Ars", "Zaid", ""]]}, {"id": "2105.00069", "submitter": "Neil McGlohon", "authors": "Neil McGlohon and Christopher D. Carothers", "title": "Unbiased Deterministic Total Ordering of Parallel Simulations with\n  Simultaneous Events", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the area of discrete event simulation (DES), event simultaneity occurs\nwhen any two events are scheduled to happen at the same point in simulated\ntime. Simulation determinism is the expectation that the same semantically\nconfigured simulation will be guaranteed to repeatedly reproduce identical\nresults. Since events in DES are the sole mechanism for state change, ensuring\nconsistent real-time event processing order is crucial to maintaining\ndeterminism. This is synonymous with finding a consistent total ordering of\nevents.\n  In this work, we extend the concept of virtual time to utilize an\narbitrary-length series of tie-breaking values to preserve determinism in\nparallel, optimistically executed simulations without imposing additional bias\ninfluencing the ordering of otherwise incomparable events. Furthermore, by\nchanging the core pseudo-random number generator seed at initialization,\ndifferent orderings of events incomparable by standard virtual time can be\nobserved, allowing for fair probing of other potential simulation outcomes. We\nimplement and evaluate this extended definition of virtual time in the\nRensselaer Optimistic Simulation System (ROSS) with three simulation models and\ndiscuss the importance of deterministic event ordering given the existence of\nevent ties.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 20:12:46 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["McGlohon", "Neil", ""], ["Carothers", "Christopher D.", ""]]}, {"id": "2105.00110", "submitter": "Paul Burkhardt", "authors": "Paul Burkhardt", "title": "Triangle Centrality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Triangle centrality is introduced for finding important vertices in a graph\nbased on the concentration of triangles surrounding each vertex. An important\nvertex in triangle centrality is at the center of many triangles, and therefore\nit may be in many triangles or none at all.\n  We give optimal algorithms that compute triangle centrality in $O(m^{3/2})$\ntime and $O(m+n)$ space. Using fast matrix multiplication it takes\n$n^{\\omega+o(1)}$ time where $\\omega$ is the matrix product exponent.\n  On a Concurrent Read Exclusive Write (CREW) Parallel Random Access Memory\n(PRAM) machine, we give a near work-optimal algorithm that takes $O(\\log n)$\ntime using $O(m\\sqrt{m})$ CREW PRAM processors. In MapReduce, we show it takes\nfour rounds using $O(m\\sqrt{m})$ communication bits, and is therefore optimal.\n  We also give a deterministic algorithm to find the triangle neighborhood and\ntriangle count of each vertex in $O(m\\sqrt{m})$ time and $O(m+n)$ space. It can\nbe also easily be computed in $O(m\\bar\\delta(G))$ expected time, where\n$\\bar\\delta(G)$ is the average graph degeneracy and is related to the\narboricity. We leave it as an open problem to deterministically compute\ntriangle neighbors in $O(m\\bar\\delta(G))$ time and $O(m+n)$ space.\n  Our empirical results demonstrate that triangle centrality uniquely\nidentified central vertices thirty-percent of the time in comparison to five\nother well-known centrality measures, while being asymptotically faster to\ncompute on sparse graphs than all but the most trivial of these other measures.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 22:29:10 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Burkhardt", "Paul", ""]]}, {"id": "2105.00129", "submitter": "Rafael Ferreira da Silva", "authors": "Tain\\~a Coleman, Henri Casanova, Rafael Ferreira da Silva", "title": "WfChef: Automated Generation of Accurate Scientific Workflow Generators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scientific workflow applications have become mainstream and their automated\nand efficient execution on large-scale compute platforms is the object of\nextensive research and development. For these efforts to be successful, a solid\nexperimental methodology is needed to evaluate workflow algorithms and systems.\nA foundation for this methodology is the availability of realistic workflow\ninstances. Dozens of workflow instances for a few scientific applications are\navailable in public repositories. While these are invaluable, they are limited:\nworkflow instances are not available for all application scales of interest. To\naddress this limitation, previous work has developed generators of synthetic,\nbut representative, workflow instances of arbitrary scales. These generators\nare popular, but implementing them is a manual, labor-intensive process that\nrequires expert application knowledge. As a result, these generators only\ntarget a handful of applications, even though hundreds of applications use\nworkflows in production.\n  In this work, we present WfChef, a framework that fully automates the process\nof constructing a synthetic workflow generator for any scientific application.\nBased on an input set of workflow instances, WfChef automatically produces a\nsynthetic workflow generator. We define and evaluate several metrics for\nquantifying the realism of the generated workflows. Using these metrics, we\ncompare the realism of the workflows generated by WfChef generators to that of\nthe workflows generated by the previously available, hand-crafted generators.\nWe find that the WfChef generators not only require zero development effort\n(because it is automatically produced), but also generate workflows that are\nmore realistic than those generated by hand-crafted generators.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 00:34:55 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Coleman", "Tain\u00e3", ""], ["Casanova", "Henri", ""], ["da Silva", "Rafael Ferreira", ""]]}, {"id": "2105.00174", "submitter": "Hao Wang", "authors": "Qing Yang, Hao Wang, Taotao Wang, Shengli Zhang, Xiaoxiao Wu, Hui Wang", "title": "Blockchain-Based Decentralized Energy Management Platform for\n  Residential Distributed Energy Resources in A Virtual Power Plant", "comments": null, "journal-ref": "Applied Energy, 2021", "doi": "10.1016/j.apenergy.2021.117026", "report-no": null, "categories": "eess.SY cs.CR cs.DC cs.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The advent of distributed energy resources (DERs), such as distributed\nrenewables, energy storage, electric vehicles, and controllable loads,\n\\rv{brings} a significantly disruptive and transformational impact on the\ncentralized power system. It is widely accepted that a paradigm shift to a\ndecentralized power system with bidirectional power flow is necessary to the\nintegration of DERs. The virtual power plant (VPP) emerges as a promising\nparadigm for managing DERs to participate in the power system. In this paper,\nwe develop a blockchain-based VPP energy management platform to facilitate a\nrich set of transactive energy activities among residential users with\nrenewables, energy storage, and flexible loads in a VPP. Specifically, users\ncan interact with each other to trade energy for mutual benefits and provide\nnetwork services, such as feed-in energy, reserve, and demand response, through\nthe VPP. To respect the users' independence and preserve their privacy, we\ndesign a decentralized optimization algorithm to optimize the users' energy\nscheduling, energy trading, and network services. Then we develop a prototype\nblockchain network for VPP energy management and implement the proposed\nalgorithm on the blockchain network. By experiments using real-world\ndata-trace, we validated the feasibility and effectiveness of our algorithm and\nthe blockchain system. The simulation results demonstrate that our\nblockchain-based VPP energy management platform reduces the users' cost by up\nto 38.6% and reduces the overall system cost by 11.2%.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 06:04:27 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 06:52:05 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Yang", "Qing", ""], ["Wang", "Hao", ""], ["Wang", "Taotao", ""], ["Zhang", "Shengli", ""], ["Wu", "Xiaoxiao", ""], ["Wang", "Hui", ""]]}, {"id": "2105.00243", "submitter": "Yue Tan", "authors": "Yue Tan, Guodong Long, Lu Liu, Tianyi Zhou, Qinghua Lu, Jing Jiang,\n  and Chengqi Zhang", "title": "FedProto: Federated Prototype Learning over Heterogeneous Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The heterogeneity across devices usually hinders the optimization convergence\nand generalization performance of federated learning (FL) when the aggregation\nof devices' knowledge occurs in the gradient space. For example, devices may\ndiffer in terms of data distribution, network latency, input/output space,\nand/or model architecture, which can easily lead to the misalignment of their\nlocal gradients. To improve the tolerance to heterogeneity, we propose a novel\nfederated prototype learning (FedProto) framework in which the devices and\nserver communicate the class prototypes instead of the gradients. FedProto\naggregates the local prototypes collected from different devices, and then\nsends the global prototypes back to all devices to regularize the training of\nlocal models. The training on each device aims to minimize the classification\nerror on the local data while keeping the resulting local prototypes\nsufficiently close to the corresponding global ones. Through experiments, we\npropose a benchmark setting tailored for heterogeneous FL, with FedProto\noutperforming several recent FL approaches on multiple datasets.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 13:21:56 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 02:05:32 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Tan", "Yue", ""], ["Long", "Guodong", ""], ["Liu", "Lu", ""], ["Zhou", "Tianyi", ""], ["Lu", "Qinghua", ""], ["Jiang", "Jing", ""], ["Zhang", "Chengqi", ""]]}, {"id": "2105.00339", "submitter": "Saeed Khorram", "authors": "Saeed Khorram, Xiao Fu, Mohamad H. Danesh, Zhongang Qi, Li Fuxin", "title": "Stochastic Block-ADMM for Training Deep Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose Stochastic Block-ADMM as an approach to train deep\nneural networks in batch and online settings. Our method works by splitting\nneural networks into an arbitrary number of blocks and utilizes auxiliary\nvariables to connect these blocks while optimizing with stochastic gradient\ndescent. This allows training deep networks with non-differentiable constraints\nwhere conventional backpropagation is not applicable. An application of this is\nsupervised feature disentangling, where our proposed DeepFacto inserts a\nnon-negative matrix factorization (NMF) layer into the network. Since\nbackpropagation only needs to be performed within each block, our approach\nalleviates vanishing gradients and provides potentials for parallelization. We\nprove the convergence of our proposed method and justify its capabilities\nthrough experiments in supervised and weakly-supervised settings.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 19:56:13 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Khorram", "Saeed", ""], ["Fu", "Xiao", ""], ["Danesh", "Mohamad H.", ""], ["Qi", "Zhongang", ""], ["Fuxin", "Li", ""]]}, {"id": "2105.00440", "submitter": "Izack Cohen", "authors": "Ilan Reuven Cohen, Izack Cohen, Iyar Zaks", "title": "Weighted completion time minimization for capacitated parallel machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the weighted completion time minimization problem for capacitated\nparallel machines, which is a fundamental problem in modern cloud computing\nenvironments. We study settings in which the processed jobs may have varying\nduration, resource requirements and importance (weight). Each server (machine)\ncan process multiple concurrent jobs up to its capacity.\n  Due to the problem's $\\mathcal{NP}$-hardness, we study heuristic approaches\nwith provable approximation guarantees. We first analyze an algorithm that\nprioritizes the jobs with the smallest volume-by-weight ratio. We bound its\napproximation ratio with a decreasing function of the ratio between the highest\nresource demand of any job to the server's capacity.\n  Then, we use the algorithm for scheduling jobs with resource demands equal to\nor smaller than 0.5 of the server's capacity in conjunction with the classic\nweighted shortest processing time algorithm for jobs with resource demands\nhigher than 0.5. We thus create a hybrid, constant approximation algorithm for\ntwo or more machines. We also develop a constant approximation algorithm for\nthe case with a single machine. This research is the first, to the best of our\nknowledge, to propose a polynomial-time algorithm with a constant approximation\nratio for minimizing the weighted sum of job completion times for capacitated\nparallel machines.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 10:31:04 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Cohen", "Ilan Reuven", ""], ["Cohen", "Izack", ""], ["Zaks", "Iyar", ""]]}, {"id": "2105.00560", "submitter": "Brad Calder", "authors": "Anna Berenberg and Brad Calder", "title": "Deployment Archetypes for Cloud Applications", "comments": "Under review for publication at ACM Computing Surveys", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This is a survey paper that explores six Cloud-based deployment archetypes\nfor Cloud applications and the tradeoffs between them to achieve high\navailability, low end-user latency, and acceptable costs. These are (1) Zonal,\n(2) Regional, (3) Multi-Regional, (4) Global, (5) Hybrid, and (6) Multi-Cloud\ndeployment archetypes. The goal is to classify cloud applications into a set of\ndeployment archetypes and deployment models that tradeoff their needs around\navailability, latency, and geographical constraints with a focus on serving\napplications. This enables application owners to better examine the tradeoffs\nof each deployment model and what is needed for achieving the availability and\nlatency goals for their application.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 22:02:50 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Berenberg", "Anna", ""], ["Calder", "Brad", ""]]}, {"id": "2105.00562", "submitter": "Saeed Vahidian", "authors": "Saeed Vahidian and Mahdi Morafah and Bill Lin", "title": "Personalized Federated Learning by Structured and Unstructured Pruning\n  under Data Heterogeneity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The traditional approach in FL tries to learn a single global model\ncollaboratively with the help of many clients under the orchestration of a\ncentral server. However, learning a single global model might not work well for\nall clients participating in the FL under data heterogeneity. Therefore, the\npersonalization of the global model becomes crucial in handling the challenges\nthat arise with statistical heterogeneity and the non-IID distribution of data.\nUnlike prior works, in this work we propose a new approach for obtaining a\npersonalized model from a client-level objective. This further motivates all\nclients to participate in federation even under statistical heterogeneity in\norder to improve their performance, instead of merely being a source of data\nand model training for the central server. To realize this personalization, we\nleverage finding a small subnetwork for each client by applying hybrid pruning\n(combination of structured and unstructured pruning), and unstructured pruning.\nThrough a range of experiments on different benchmarks, we observed that the\nclients with similar data (labels) share similar personal parameters. By\nfinding a subnetwork for each client ...\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 22:10:46 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 00:43:37 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Vahidian", "Saeed", ""], ["Morafah", "Mahdi", ""], ["Lin", "Bill", ""]]}, {"id": "2105.00578", "submitter": "Erik Boman", "authors": "Seher Acer, Erik G Boman, Christian A Glusa, and Sivasankaran\n  Rajamanickam", "title": "Sphynx: a parallel multi-GPU graph partitioner for distributed-memory\n  systems", "comments": "To appear in Parallel Computing", "journal-ref": null, "doi": null, "report-no": "SAND2021-0352-O", "categories": "cs.DC cs.DM cs.MS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph partitioning has been an important tool to partition the work among\nseveral processors to minimize the communication cost and balance the workload.\nWhile accelerator-based supercomputers are emerging to be the standard, the use\nof graph partitioning becomes even more important as applications are rapidly\nmoving to these architectures. However, there is no distributed-memory\nparallel, multi-GPU graph partitioner available for applications. We developed\na spectral graph partitioner, Sphynx, using the portable, accelerator-friendly\nstack of the Trilinos framework. In Sphynx, we allow using different\npreconditioners and exploit their unique advantages. We use Sphynx to\nsystematically evaluate the various algorithmic choices in spectral\npartitioning with a focus on the GPU performance. We perform those evaluations\non two distinct classes of graphs: regular (such as meshes, matrices from\nfinite element methods) and irregular (such as social networks and web graphs),\nand show that different settings and preconditioners are needed for these graph\nclasses. The experimental results on the Summit supercomputer show that Sphynx\nis the fastest alternative on irregular graphs in an application-friendly\nsetting and obtains a partitioning quality close to ParMETIS on regular graphs.\nWhen compared to nvGRAPH on a single GPU, Sphynx is faster and obtains better\nbalance and better quality partitions. Sphynx provides a good and robust\npartitioning method across a wide range of graphs for applications looking for\na GPU-based partitioner.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 23:47:28 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Acer", "Seher", ""], ["Boman", "Erik G", ""], ["Glusa", "Christian A", ""], ["Rajamanickam", "Sivasankaran", ""]]}, {"id": "2105.00613", "submitter": "Barak Shoshany", "authors": "Barak Shoshany", "title": "A C++17 Thread Pool for High-Performance Scientific Computing", "comments": "15 pages, source code available at\n  https://github.com/bshoshany/thread-pool", "journal-ref": null, "doi": "10.5281/zenodo.4742687", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a modern C++17-compatible thread pool implementation, built from\nscratch with high-performance scientific computing in mind. The thread pool is\nimplemented as a single lightweight and self-contained class, and does not have\nany dependencies other than the C++17 standard library, thus allowing a great\ndegree of portability. In particular, our implementation does not utilize\nOpenMP or any other high-level multithreading APIs, and thus gives the\nprogrammer precise low-level control over the details of the parallelization,\nwhich permits more robust optimizations. The thread pool was extensively tested\non both AMD and Intel CPUs with up to 40 cores and 80 threads. This paper\nprovides motivation, detailed usage instructions, and performance tests.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 03:04:49 GMT"}, {"version": "v2", "created": "Sat, 8 May 2021 16:12:52 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Shoshany", "Barak", ""]]}, {"id": "2105.00725", "submitter": "Siamak Taati", "authors": "Siamak Taati", "title": "Reversible cellular automata in presence of noise rapidly forget\n  everything", "comments": "To appear in the Proceedings of AUTOMATA 2021, published in the\n  OASIcs series", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.ET cs.IT math.IT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider reversible and surjective cellular automata perturbed with noise.\nWe show that, in the presence of positive additive noise, the cellular\nautomaton forgets all the information regarding its initial configuration\nexponentially fast. In particular, the state of a finite collection of cells\nwith diameter n becomes indistinguishable from pure noise after O(log n) time\nsteps. This highlights the seemingly unavoidable need for irreversibility in\norder to perform scalable reliable computation in the presence of noise.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 10:15:11 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Taati", "Siamak", ""]]}, {"id": "2105.00766", "submitter": "Xiong Wang", "authors": "Xiong Wang, Jiancheng Ye, John C.S. Lui", "title": "Joint D2D Collaboration and Task Offloading for Edge Computing: A Mean\n  Field Graph Approach", "comments": "IWQoS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile edge computing (MEC) facilitates computation offloading to edge\nserver, as well as task processing via device-to-device (D2D) collaboration.\nExisting works mainly focus on centralized network-assisted offloading\nsolutions, which are unscalable to scenarios involving collaboration among\nmassive users. In this paper, we propose a joint framework of decentralized D2D\ncollaboration and efficient task offloading for a large-population MEC system.\nSpecifically, we utilize the power of two choices for D2D collaboration, which\nenables users to beneficially assist each other in a decentralized manner. Due\nto short-range D2D communication and user movements, we formulate a mean field\nmodel on a finite-degree and dynamic graph to analyze the state evolution of\nD2D collaboration. We derive the existence, uniqueness and convergence of the\nstate stationary point so as to provide a tractable collaboration performance.\nComplementing this D2D collaboration, we further build a Stackelberg game to\nmodel users' task offloading, where edge server is the leader to determine a\nservice price, while users are followers to make offloading decisions. By\nembedding the Stackelberg game into Lyapunov optimization, we develop an online\noffloading and pricing scheme, which could optimize server's service utility\nand users' system cost simultaneously. Extensive evaluations show that our D2D\ncollaboration can mitigate users' workloads by $73.8\\%$ and task offloading can\nachieve high energy efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 11:49:59 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 11:20:28 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Wang", "Xiong", ""], ["Ye", "Jiancheng", ""], ["Lui", "John C. S.", ""]]}, {"id": "2105.00831", "submitter": "Lodovico Giaretta", "authors": "Daniel Garcia Bernal, Lodovico Giaretta, Sarunas Girdzijauskas, Magnus\n  Sahlgren", "title": "Federated Word2Vec: Leveraging Federated Learning to Encourage\n  Collaborative Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large scale contextual representation models have significantly advanced NLP\nin recent years, understanding the semantics of text to a degree never seen\nbefore. However, they need to process large amounts of data to achieve\nhigh-quality results. Joining and accessing all these data from multiple\nsources can be extremely challenging due to privacy and regulatory reasons.\nFederated Learning can solve these limitations by training models in a\ndistributed fashion, taking advantage of the hardware of the devices that\ngenerate the data. We show the viability of training NLP models, specifically\nWord2Vec, with the Federated Learning protocol. In particular, we focus on a\nscenario in which a small number of organizations each hold a relatively large\ncorpus. The results show that neither the quality of the results nor the\nconvergence time in Federated Word2Vec deteriorates as compared to centralised\nWord2Vec.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 15:39:02 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Bernal", "Daniel Garcia", ""], ["Giaretta", "Lodovico", ""], ["Girdzijauskas", "Sarunas", ""], ["Sahlgren", "Magnus", ""]]}, {"id": "2105.00842", "submitter": "Panagiotis Diamantoulakis", "authors": "Pavlos S. Bouzinis, Panagiotis D. Diamantoulakis, George K.\n  Karagiannidis", "title": "Wireless Federated Learning (WFL) for 6G Networks -- Part I: Research\n  Challenges and Future Trends", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Conventional machine learning techniques are conducted in a centralized\nmanner. Recently, the massive volume of generated wireless data, the privacy\nconcerns and the increasing computing capabilities of wireless end-devices have\nled to the emergence of a promising decentralized solution, termed as Wireless\nFederated Learning (WFL). In this first of the two parts paper, we present the\napplication of WFL in the sixth generation of wireless networks (6G), which is\nenvisioned to be an integrated communication and computing platform. After\nanalyzing the key concepts of WFL, we discuss the core challenges of WFL\nimposed by the wireless (or mobile communication) environment. Finally, we shed\nlight to the future directions of WFL, aiming to compose a constructive\nintegration of FL into the future wireless networks.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 18:10:05 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Bouzinis", "Pavlos S.", ""], ["Diamantoulakis", "Panagiotis D.", ""], ["Karagiannidis", "George K.", ""]]}, {"id": "2105.00843", "submitter": "Leila Ismail Prof.", "authors": "Huned Materwala and Leila Ismail", "title": "Performance and Energy-Aware Bi-objective Tasks Scheduling for Cloud\n  Data Centers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cloud computing enables remote execution of users tasks. The pervasive\nadoption of cloud computing in smart cities services and applications requires\ntimely execution of tasks adhering to Quality of Services (QoS).\n  However, the increasing use of computing servers exacerbates the issues of\nhigh energy consumption, operating costs, and environmental pollution.\nMaximizing the performance and minimizing the energy in a cloud data center is\nchallenging. In this paper, we propose a performance and energy optimization\nbi-objective algorithm to tradeoff the contradicting performance and energy\nobjectives. An evolutionary algorithm-based multi-objective optimization is for\nthe first time proposed using system performance counters. The performance of\nthe proposed model is evaluated using a realistic cloud dataset in a cloud\ncomputing environment. Our experimental results achieve higher performance and\nlower energy consumption compared to a state of the art algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 08:55:57 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Materwala", "Huned", ""], ["Ismail", "Leila", ""]]}, {"id": "2105.00847", "submitter": "Enis Karaarslan Dr.", "authors": "Melih Birim, H\\\"useyin Emre Ari, Enis Karaarslan", "title": "GoHammer Blockchain Performance Test Tool", "comments": "3 pages, 4 figures, Journal of Emerging Computer Technologies", "journal-ref": "Journal of Emerging Computer Technologies, 1(2), 31-33 (2021)", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Decentralized services are increasingly being developed and their\nDecentralized applications are increasingly developed but their performance\nmetrics are not tested enough. The total number of transactions that can be\nsupported by the blockchain network and the performance effects of selecting\ndifferent consensus protocols, using different block intervals and block size\nshould be tested. There are some blockchain performance tools but most are\nbuilt for specific blockchain frameworks and require complex configuration. The\nGoHammer tool is developed to provide an easy to use, flexible test tool for\nthe Ethereum/Quorum blockchain frameworks. Transaction per second (TPS) values\nand several performance metrics will be tested. This tool is also a part of the\nseries of tools that can be integrated with Tubu-io. This tool will help in\ndeveloping more efficient decentralized systems and will affect decreasing the\ncosts of developing decentralized application projects.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 16:10:55 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Birim", "Melih", ""], ["Ari", "H\u00fcseyin Emre", ""], ["Karaarslan", "Enis", ""]]}, {"id": "2105.00852", "submitter": "Abdu Saif", "authors": "Qazwan Abdullah, Nor Shahida Mohd Shah, Mahathir Mohamad, Muaammar\n  Hadi Kuzman Ali, Nabil Farah, Adeb Salh, Maged Aboali, Mahmod Abd Hakim\n  Mohamad, Abdu Saif", "title": "Real-time Autonomous Robot for Object Tracking using Vision System", "comments": null, "journal-ref": "www.solidstatetechnology.us Solid State Technology Volume: 63\n  Issue: 6 Publication Year: 2020", "doi": null, "report-no": null, "categories": "cs.DC cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Researchers and robotic development groups have recently started paying\nspecial attention to autonomous mobile robot navigation in indoor environments\nusing vision sensors. The required data is provided for robot navigation and\nobject detection using a camera as a sensor. The aim of the project is to\nconstruct a mobile robot that has integrated vision system capability used by a\nwebcam to locate, track and follow a moving object. To achieve this task,\nmultiple image processing algorithms are implemented and processed in\nreal-time. A mini-laptop was used for collecting the necessary data to be sent\nto a PIC microcontroller that turns the processes of data obtained to provide\nthe robot's proper orientation. A vision system can be utilized in object\nrecognition for robot control applications. The results demonstrate that the\nproposed mobile robot can be successfully operated through a webcam that\ndetects the object and distinguishes a tennis ball based on its color and\nshape.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 18:33:47 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Abdullah", "Qazwan", ""], ["Shah", "Nor Shahida Mohd", ""], ["Mohamad", "Mahathir", ""], ["Ali", "Muaammar Hadi Kuzman", ""], ["Farah", "Nabil", ""], ["Salh", "Adeb", ""], ["Aboali", "Maged", ""], ["Mohamad", "Mahmod Abd Hakim", ""], ["Saif", "Abdu", ""]]}, {"id": "2105.00872", "submitter": "Shuo Wan", "authors": "Shuo Wan, Jiaxun Lu, Pingyi Fan, Yunfeng Shao, Chenghui Peng and\n  Khaled B. letaief", "title": "Convergence Analysis and System Design for Federated Learning over\n  Wireless Networks", "comments": "15 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.MA cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) has recently emerged as an important and promising\nlearning scheme in IoT, enabling devices to jointly learn a model without\nsharing their raw data sets. However, as the training data in FL is not\ncollected and stored centrally, FL training requires frequent model exchange,\nwhich is largely affected by the wireless communication network. Therein,\nlimited bandwidth and random package loss restrict interactions in training.\nMeanwhile, the insufficient message synchronization among distributed clients\ncould also affect FL convergence. In this paper, we analyze the convergence\nrate of FL training considering the joint impact of communication network and\ntraining settings. Further by considering the training costs in terms of time\nand power, the optimal scheduling problems for communication networks are\nformulated. The developed theoretical results can be used to assist the system\nparameter selections and explain the principle of how the wireless\ncommunication system could influence the distributed training process and\nnetwork scheduling.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 02:33:29 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Wan", "Shuo", ""], ["Lu", "Jiaxun", ""], ["Fan", "Pingyi", ""], ["Shao", "Yunfeng", ""], ["Peng", "Chenghui", ""], ["letaief", "Khaled B.", ""]]}, {"id": "2105.00964", "submitter": "Alex Sim", "authors": "Elizabeth Copps, Huiyi Zhang, Alex Sim, Kesheng Wu, Inder Monga, Chin\n  Guok, Frank W\\\"urthwein, Diego Davila, Edgar Fajardo", "title": "Analyzing scientific data sharing patterns for in-network data caching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The volume of data moving through a network increases with new scientific\nexperiments and simulations. Network bandwidth requirements also increase\nproportionally to deliver data within a certain time frame. We observe that a\nsignificant portion of the popular dataset is transferred multiple times to\ndifferent users as well as to the same user for various reasons. In-network\ndata caching for the shared data has shown to reduce the redundant data\ntransfers and consequently save network traffic volume. In addition, overall\napplication performance is expected to improve with in-network caching because\naccess to the locally cached data results in lower latency. This paper shows\nhow much data was shared over the study period, how much network traffic volume\nwas consequently saved, and how much the temporary in-network caching increased\nthe scientific application performance. It also analyzes data access patterns\nin applications and the impacts of caching nodes on the regional data\nrepository. From the results, we observed that the network bandwidth demand was\nreduced by nearly a factor of 3 over the study period.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 16:01:18 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Copps", "Elizabeth", ""], ["Zhang", "Huiyi", ""], ["Sim", "Alex", ""], ["Wu", "Kesheng", ""], ["Monga", "Inder", ""], ["Guok", "Chin", ""], ["W\u00fcrthwein", "Frank", ""], ["Davila", "Diego", ""], ["Fajardo", "Edgar", ""]]}, {"id": "2105.01109", "submitter": "Damian S. Steiger", "authors": "Thomas H\\\"aner, Damian S. Steiger, Torsten Hoefler, Matthias Troyer", "title": "Distributed Quantum Computing with QMPI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DC cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Practical applications of quantum computers require millions of physical\nqubits and it will be challenging for individual quantum processors to reach\nsuch qubit numbers. It is therefore timely to investigate the resource\nrequirements of quantum algorithms in a distributed setting, where multiple\nquantum processors are interconnected by a coherent network. We introduce an\nextension of the Message Passing Interface (MPI) to enable high-performance\nimplementations of distributed quantum algorithms. In turn, these\nimplementations can be used for testing, debugging, and resource estimation. In\naddition to a prototype implementation of quantum MPI, we present a performance\nmodel for distributed quantum computing, SENDQ. The model is inspired by the\nclassical LogP model, making it useful to inform algorithmic decisions when\nprogramming distributed quantum computers. Specifically, we consider several\noptimizations of two quantum algorithms for problems in physics and chemistry,\nand we detail their effects on performance in the SENDQ model.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 18:30:43 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["H\u00e4ner", "Thomas", ""], ["Steiger", "Damian S.", ""], ["Hoefler", "Torsten", ""], ["Troyer", "Matthias", ""]]}, {"id": "2105.01196", "submitter": "Patryk Orzechowski", "authors": "Pawe{\\l} Renc, Patryk Orzechowski, Aleksander Byrski, Jaros{\\l}aw\n  W\\k{a}s, and Jason H. Moore", "title": "EBIC.JL -- an Efficient Implementation of Evolutionary Biclustering\n  Algorithm in Julia", "comments": "9 pages, 11 figures", "journal-ref": null, "doi": "10.1145/3449726.3463197", "report-no": null, "categories": "cs.LG cs.AI cs.DC cs.NE q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biclustering is a data mining technique which searches for local patterns in\nnumeric tabular data with main application in bioinformatics. This technique\nhas shown promise in multiple areas, including development of biomarkers for\ncancer, disease subtype identification, or gene-drug interactions among others.\nIn this paper we introduce EBIC.JL - an implementation of one of the most\naccurate biclustering algorithms in Julia, a modern highly parallelizable\nprogramming language for data science. We show that the new version maintains\ncomparable accuracy to its predecessor EBIC while converging faster for the\nmajority of the problems. We hope that this open source software in a\nhigh-level programming language will foster research in this promising field of\nbioinformatics and expedite development of new biclustering methods for big\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 22:30:38 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Renc", "Pawe\u0142", ""], ["Orzechowski", "Patryk", ""], ["Byrski", "Aleksander", ""], ["W\u0105s", "Jaros\u0142aw", ""], ["Moore", "Jason H.", ""]]}, {"id": "2105.01231", "submitter": "Xin Zhang", "authors": "Xin Zhang, Jia Liu, Zhengyuan Zhu, and Elizabeth S. Bentley", "title": "GT-STORM: Taming Sample, Communication, and Memory Complexities in\n  Decentralized Non-Convex Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Decentralized nonconvex optimization has received increasing attention in\nrecent years in machine learning due to its advantages in system robustness,\ndata privacy, and implementation simplicity. However, three fundamental\nchallenges in designing decentralized optimization algorithms are how to reduce\ntheir sample, communication, and memory complexities. In this paper, we propose\na \\underline{g}radient-\\underline{t}racking-based \\underline{sto}chastic\n\\underline{r}ecursive \\underline{m}omentum (GT-STORM) algorithm for efficiently\nsolving nonconvex optimization problems. We show that to reach an\n$\\epsilon^2$-stationary solution, the total number of sample evaluations of our\nalgorithm is $\\tilde{O}(m^{1/2}\\epsilon^{-3})$ and the number of communication\nrounds is $\\tilde{O}(m^{-1/2}\\epsilon^{-3})$, which improve the\n$O(\\epsilon^{-4})$ costs of sample evaluations and communications for the\nexisting decentralized stochastic gradient algorithms. We conduct extensive\nexperiments with a variety of learning models, including non-convex logistical\nregression and convolutional neural networks, to verify our theoretical\nfindings. Collectively, our results contribute to the state of the art of\ntheories and algorithms for decentralized network optimization.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 00:44:48 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 04:28:42 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Zhang", "Xin", ""], ["Liu", "Jia", ""], ["Zhu", "Zhengyuan", ""], ["Bentley", "Elizabeth S.", ""]]}, {"id": "2105.01266", "submitter": "Kevin Jin", "authors": "Ayaz Hafiz, Kevin Jin", "title": "Architecture of a Flexible and Cost-Effective Remote Code Execution\n  Engine", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Oftentimes, there is a need to experiment with different programming\nlanguages and technologies when designing software applications. Such\nexperiments must be reproducible and share-able within a team workplace, and\nmanual effort should be minimized for setting up/tearing down said experiments.\n  This paper solves this problem by presenting a cloud-based web service for\nremote code execution, that is easily extensible to support any number of\nprogramming languages and libraries. The service provides a fast, reproducible\nsolution for small software experiments and is amenable to collaboration in a\nworkplace (via sharable permalinks). The service is designed as a distributed\nsystem to reliably support a large number of users, and efficiently manage\ncloud-hosting costs with predictive auto-scaling while minimizing SLA\nviolations.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 03:13:54 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Hafiz", "Ayaz", ""], ["Jin", "Kevin", ""]]}, {"id": "2105.01281", "submitter": "Chengliang Zhang Dr", "authors": "Chengliang Zhang, Junzhe Xia, Baichen Yang, Huancheng Puyang, Wei\n  Wang, Ruichuan Chen, Istemi Ekin Akkus, Paarijaat Aditya, Feng Yan", "title": "Citadel: Protecting Data Privacy and Model Confidentiality for\n  Collaborative Learning with SGX", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advancement of machine learning (ML) and its growing awareness, many\norganizations who own data but not ML expertise (data owner) would like to pool\ntheir data and collaborate with those who have expertise but need data from\ndiverse sources to train truly generalizable models (model owner). In such\ncollaborative ML, the data owner wants to protect the privacy of its training\ndata, while the model owner desires the confidentiality of the model and the\ntraining method which may contain intellectual properties. However, existing\nprivate ML solutions, such as federated learning and split learning, cannot\nmeet the privacy requirements of both data and model owners at the same time.\n  This paper presents Citadel, a scalable collaborative ML system that protects\nthe privacy of both data owner and model owner in untrusted infrastructures\nwith the help of Intel SGX. Citadel performs distributed training across\nmultiple training enclaves running on behalf of data owners and an aggregator\nenclave on behalf of the model owner. Citadel further establishes a strong\ninformation barrier between these enclaves by means of zero-sum masking and\nhierarchical aggregation to prevent data/model leakage during collaborative\ntraining. Compared with the existing SGX-protected training systems, Citadel\nenables better scalability and stronger privacy guarantees for collaborative\nML. Cloud deployment with various ML models shows that Citadel scales to a\nlarge number of enclaves with less than 1.73X slowdown caused by SGX.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 04:17:29 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Zhang", "Chengliang", ""], ["Xia", "Junzhe", ""], ["Yang", "Baichen", ""], ["Puyang", "Huancheng", ""], ["Wang", "Wei", ""], ["Chen", "Ruichuan", ""], ["Akkus", "Istemi Ekin", ""], ["Aditya", "Paarijaat", ""], ["Yan", "Feng", ""]]}, {"id": "2105.01316", "submitter": "Jack Tanner", "authors": "Jack Tanner, Roshaan Khan", "title": "Technology Review of Blockchain Data Privacy Solutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This objective of this report is to review existing enterprise blockchain\ntechnologies - EOSIO powered systems, Hyperledger Fabric and Besu, Consensus\nQuorum, R3 Corda and Ernst and Young's Nightfall - that provide data privacy\nwhile leveraging the data integrity benefits of blockchain. By reviewing and\ncomparing how and how well these technologies achieve data privacy, a snapshot\nis captured of the industry's current best practices and data privacy models.\nMajor enterprise technologies are contrasted in parallel to EOSIO to better\nunderstand how EOSIO can evolve to meet the trends seen in enterprise\nblockchain privacy. The following strategies and trends were generally observed\nin these technologies:\n  Cryptography: the hashing algorithm was found to be the most used\ncryptographic primitive in enterprise or changeover privacy solutions.\nCoordination via on-chain contracts - a common strategy was to use a shared\npublicly ledger to coordinate data privacy groups and more generally managed\nidentities and access control.\n  Transaction and contract code sharing: there was a variety of different\nlevels of privacy around the business logic (smart contract code) visibility.\nSome solutions only allowed authorised peers to view code while others made\nthis accessible to everybody that was a member of the shared ledger.\n  Data migrations for data privacy applications: significant challenges exist\nwhen using cryptographically stored data in terms of being able to run system\nupgrades.\n  Multiple blockchain ledgers for data privacy: solutions attempted to create a\nnew private blockchain for every private data relationship which was eventually\nabandoned in favour of one shared ledger with private data\ncollections/transactions that were anchored to the ledger with a hash in order\nto improve scaling.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 06:56:42 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Tanner", "Jack", ""], ["Khan", "Roshaan", ""]]}, {"id": "2105.01372", "submitter": "Mattia Bianchi", "authors": "Mattia Bianchi, Wicak Ananduta, Sergio Grammatico", "title": "The distributed dual ascent algorithm is robust to asynchrony", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distributed dual ascent is an established algorithm to solve strongly\nconvex multi-agent optimization problems with separable cost functions, in the\npresence of coupling constraints. In this paper, we study its asynchronous\ncounterpart. Specifically, we assume that each agent only relies on the\noutdated information received from some neighbors. Differently from the\nexisting randomized and dual block-coordinate schemes, we show convergence\nunder heterogeneous delays, communication and update frequencies. Consequently,\nour asynchronous dual ascent algorithm can be implemented without requiring any\ncoordination between the agents.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 09:01:49 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Bianchi", "Mattia", ""], ["Ananduta", "Wicak", ""], ["Grammatico", "Sergio", ""]]}, {"id": "2105.01641", "submitter": "Silviu Craciunas", "authors": "Mohammadreza Barzegaran, Niklas Reusch, Luxi Zhao, Silviu S.\n  Craciunas, Paul Pop", "title": "Real-Time Guarantees for Critical Traffic in IEEE 802.1Qbv TSN Networks\n  with Unscheduled and Unsynchronized End-Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-Sensitive Networking (TSN) aims to extend the IEEE 802.1Q Ethernet\nstandard with real-time and time-aware capabilities. Each device's transmission\nof time-critical frames is done according to a so-called Gate Control List\n(GCL) schedule via the timed-gate mechanism described in IEEE 802.1Qbv. Most\nschedule generation mechanisms for TSN have a constraining assumption that both\nswitches and end-systems in the network must have at least the TSN capabilities\nrelated to scheduled gates and time synchronization. However, many TSN networks\nuse off-the-shelf end-systems, e.g., for providing sensor data, which are not\nscheduled and/or synchronized.\n  In this paper, we propose a more flexible scheduling strategy that considers\na worst-case delay analysis within the scheduling synthesis step, leveraging\nthe solution's optimality to support TSN networks with unscheduled and\nunsynchronized end-systems while still being able to guarantee bounded latency\nfor critical messages. Our method enables real-world systems that feature\noff-the-shelf microcontrollers and sensor nodes without TSN capabilities\nconnected to state-of-the-art TSN networks to communicate critical messages in\na real-time fashion. We evaluate our approach using both synthetic and\nreal-world test cases, comparing it with existing scheduling mechanisms.\nFurthermore, we use OMNET++ to validate the generated GCL schedules.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 17:37:06 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Barzegaran", "Mohammadreza", ""], ["Reusch", "Niklas", ""], ["Zhao", "Luxi", ""], ["Craciunas", "Silviu S.", ""], ["Pop", "Paul", ""]]}, {"id": "2105.01707", "submitter": "Haidar Harmanani", "authors": "Sherif G. Aly, Haidar Harmanani, Rajendra K. Raj, and Sanaa\n  Sharafeddine", "title": "ABET Accreditation: A Way Forward for PDC Education", "comments": null, "journal-ref": "EduPar-21: 11th NSF/TCPP Workshop on Parallel and Distributed\n  Computing Education, May 2021", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  With parallel and distributed computing (PDC) now wide-spread, modern\ncomputing programs must incorporate PDC within the curriculum. ACM and IEEE\nComputer Society's Computer Science curricular guidelines have recommended\nexposure to PDC concepts since 2013. More recently, a variety of initiatives\nhave made PDC curricular content, lectures, and labs freely available for\nundergraduate computer science programs. Despite these efforts, progress in\nensuring computer science students graduate with sufficient PDC exposure has\nbeen uneven.\n  This paper discusses the impact of ABET's revised criteria that have required\nexposure to PDC to achieve accreditation for computer science programs since\n2018. The authors reviewed 20 top ABET-accredited computer science programs and\nanalyzed how they covered the required PDC components in their curricula. Using\ntheir own institutions as case studies, the authors examine in detail how three\ndifferent ABET-accredited computer science programs covered PDC using different\napproaches, yet meeting the PDC requirements of these ABET criteria. The paper\nalso shows how ACM/IEEE Computer Society curricular guidelines for computer\nengineering and software engineering programs, along with ABET accreditation\ncriteria, can cover PDC.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 18:58:18 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Aly", "Sherif G.", ""], ["Harmanani", "Haidar", ""], ["Raj", "Rajendra K.", ""], ["Sharafeddine", "Sanaa", ""]]}, {"id": "2105.01798", "submitter": "Emna Baccour", "authors": "Emna Baccour, Naram Mhaisen, Alaa Awad Abdellatif, Aiman Erbad, Amr\n  Mohamed, Mounir Hamdi, Mohsen Guizani", "title": "Pervasive AI for IoT Applications: Resource-efficient Distributed\n  Artificial Intelligence", "comments": "Survey paper submitted to IEEE COMSAT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Artificial intelligence (AI) has witnessed a substantial breakthrough in a\nvariety of Internet of Things (IoT) applications and services, spanning from\nrecommendation systems to robotics control and military surveillance. This is\ndriven by the easier access to sensory data and the enormous scale of\npervasive/ubiquitous devices that generate zettabytes (ZB) of real-time data\nstreams. Designing accurate models using such data streams, to predict future\ninsights and revolutionize the decision-taking process, inaugurates pervasive\nsystems as a worthy paradigm for a better quality-of-life. The confluence of\npervasive computing and artificial intelligence, Pervasive AI, expanded the\nrole of ubiquitous IoT systems from mainly data collection to executing\ndistributed computations with a promising alternative to centralized learning,\npresenting various challenges. In this context, a wise cooperation and resource\nscheduling should be envisaged among IoT devices (e.g., smartphones, smart\nvehicles) and infrastructure (e.g. edge nodes, and base stations) to avoid\ncommunication and computation overheads and ensure maximum performance. In this\npaper, we conduct a comprehensive survey of the recent techniques developed to\novercome these resource challenges in pervasive AI systems. Specifically, we\nfirst present an overview of the pervasive computing, its architecture, and its\nintersection with artificial intelligence. We then review the background,\napplications and performance metrics of AI, particularly Deep Learning (DL) and\nonline learning, running in a ubiquitous system. Next, we provide a deep\nliterature review of communication-efficient techniques, from both algorithmic\nand system perspectives, of distributed inference, training and online learning\ntasks across the combination of IoT devices, edge devices and cloud servers.\nFinally, we discuss our future vision and research challenges.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 23:42:06 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Baccour", "Emna", ""], ["Mhaisen", "Naram", ""], ["Abdellatif", "Alaa Awad", ""], ["Erbad", "Aiman", ""], ["Mohamed", "Amr", ""], ["Hamdi", "Mounir", ""], ["Guizani", "Mohsen", ""]]}, {"id": "2105.01803", "submitter": "Zhe Yang", "authors": "Zhe Yang, Klara Nahrstedt, Hongpeng Guo, Qian Zhou", "title": "DeepRT: A Soft Real Time Scheduler for Computer Vision Applications on\n  the Edge", "comments": "Accepted by the Sixth ACM/IEEE Symposium on Edge Computing, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ubiquity of smartphone cameras and IoT cameras, together with the recent\nboom of deep learning and deep neural networks, proliferate various computer\nvision driven mobile and IoT applications deployed on the edge. This paper\nfocuses on applications which make soft real time requests to perform inference\non their data - they desire prompt responses within designated deadlines, but\noccasional deadline misses are acceptable. Supporting soft real time\napplications on a multi-tenant edge server is not easy, since the requests\nsharing the limited GPU computing resources of an edge server interfere with\neach other. In order to tackle this problem, we comprehensively evaluate how\nlatency and throughput respond to different GPU execution plans. Based on this\nanalysis, we propose a GPU scheduler, DeepRT, which provides latency guarantee\nto the requests while maintaining high overall system throughput. The key\ncomponent of DeepRT, DisBatcher, batches data from different requests as much\nas possible while it is proven to provide latency guarantee for requests\nadmitted by an Admission Control Module. DeepRT also includes an Adaptation\nModule which tackles overruns. Our evaluation results show that DeepRT\noutperforms state-of-the-art works in terms of the number of deadline misses\nand throughput.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 00:08:17 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Yang", "Zhe", ""], ["Nahrstedt", "Klara", ""], ["Guo", "Hongpeng", ""], ["Zhou", "Qian", ""]]}, {"id": "2105.01833", "submitter": "Peter Robinson", "authors": "Christian Konrad, Sriram V. Pemmaraju, Talal Riaz, Peter Robinson", "title": "The Complexity of Symmetry Breaking in Massive Graphs", "comments": "A preliminary version of this paper appeared in DISC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to understand the complexity of symmetry breaking\nproblems, specifically maximal independent set (MIS) and the closely related\n$\\beta$-ruling set problem, in two computational models suited for large-scale\ngraph processing, namely the $k$-machine model and the graph streaming model.\nWe present a number of results. For MIS in the $k$-machine model, we improve\nthe $\\tilde{O}(m/k^2 + \\Delta/k)$-round upper bound of Klauck et al. (SODA\n2015) by presenting an $\\tilde{O}(m/k^2)$-round algorithm. We also present an\n$\\tilde{\\Omega}(n/k^2)$ round lower bound for MIS, the first lower bound for a\nsymmetry breaking problem in the $k$-machine model. For $\\beta$-ruling sets, we\nuse hierarchical sampling to obtain more efficient algorithms in the\n$k$-machine model and also in the graph streaming model. More specifically, we\nobtain a $k$-machine algorithm that runs in $\\tilde{O}(\\beta\nn\\Delta^{1/\\beta}/k^2)$ rounds and, by using a similar hierarchical sampling\ntechnique, we obtain one-pass algorithms for both insertion-only and\ninsertion-deletion streams that use $O(\\beta \\cdot n^{1+1/2^{\\beta-1}})$ space.\nThe latter result establishes a clear separation between MIS, which is known to\nrequire $\\Omega(n^2)$ space (Cormode et al., ICALP 2019), and $\\beta$-ruling\nsets, even for $\\beta = 2$. Finally, we present an even faster 2-ruling set\nalgorithm in the $k$-machine model, one that runs in\n$\\tilde{O}(n/k^{2-\\epsilon} + k^{1-\\epsilon})$ rounds for any $\\epsilon$, $0\n\\le \\epsilon \\le 1$.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 02:09:44 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Konrad", "Christian", ""], ["Pemmaraju", "Sriram V.", ""], ["Riaz", "Talal", ""], ["Robinson", "Peter", ""]]}, {"id": "2105.01976", "submitter": "Nimish Shah", "authors": "Nimish Shah, Wannes Meert, Marian Verhelst", "title": "GRAPHOPT: constrained optimization-based parallelization of irregular\n  graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sparse, irregular graphs show up in various applications like linear algebra,\nmachine learning, engineering simulations, robotic control, etc. These graphs\nhave a high degree of parallelism, but their execution on parallel threads of\nmodern platforms remains challenging due to the irregular data dependencies.\nThe parallel execution performance can be improved by efficiently partitioning\nthe graphs such that the communication and thread synchronization overheads are\nminimized without hurting the utilization of the threads. To achieve this, this\npaper proposes GRAPHOPT, a tool that models the graph parallelization as a\nconstrained optimization problem and uses the open Google OR-Tools solver to\nfind good partitions. Several scalability techniques are developed to handle\nlarge real-world graphs with millions of nodes and edges. Extensive experiments\nare performed on the graphs of sparse matrix triangular solves (linear algebra)\nand sum-product networks (machine learning). GRAPHOPT achieves an average\nspeedup of 1.4x and 6.1x over the Intel Math Kernel Library and a commonly-used\nheuristic partitioning technique, respectively, demonstrating the effectiveness\nof the constrained optimization-based graph parallelization.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 11:01:27 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Shah", "Nimish", ""], ["Meert", "Wannes", ""], ["Verhelst", "Marian", ""]]}, {"id": "2105.02019", "submitter": "Blesson Varghese", "authors": "Hyunho Ahn and Munkyu Lee and Cheol-Ho Hong and Blesson Varghese", "title": "ScissionLite: Accelerating Distributed Deep Neural Networks Using\n  Transfer Layer", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Industrial Internet of Things (IIoT) applications can benefit from leveraging\nedge computing. For example, applications underpinned by deep neural networks\n(DNN) models can be sliced and distributed across the IIoT device and the edge\nof the network for improving the overall performance of inference and for\nenhancing privacy of the input data, such as industrial product images.\nHowever, low network performance between IIoT devices and the edge is often a\nbottleneck. In this study, we develop ScissionLite, a holistic framework for\naccelerating distributed DNN inference using the Transfer Layer (TL). The TL is\na traffic-aware layer inserted between the optimal slicing point of a DNN model\nslice in order to decrease the outbound network traffic without a significant\naccuracy drop. For the TL, we implement a new lightweight down/upsampling\nnetwork for performance-limited IIoT devices. In ScissionLite, we develop\nScissionTL, the Preprocessor, and the Offloader for end-to-end activities for\ndeploying DNN slices with the TL. They decide the optimal slicing point of the\nDNN, prepare pre-trained DNN slices including the TL, and execute the DNN\nslices on an IIoT device and the edge. Employing the TL for the sliced DNN\nmodels has a negligible overhead. ScissionLite improves the inference latency\nby up to 16 and 2.8 times when compared to execution on the local device and an\nexisting state-of-the-art model slicing approach respectively.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 12:38:58 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Ahn", "Hyunho", ""], ["Lee", "Munkyu", ""], ["Hong", "Cheol-Ho", ""], ["Varghese", "Blesson", ""]]}, {"id": "2105.02093", "submitter": "Pierluigi Crescenzi", "authors": "Amos Korman and Pierluigi Crescenzi", "title": "Public Communication can Facilitate Low-Risk Coordination under\n  Surveillance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Consider a sub-population of rebels that wish to initiate a revolution. In\norder to avoid initializing a failed revolution, rebels would first strive to\nestimate their relative \"power\", which is often correlated with their fraction\nin the population. However, and especially in non-democratic countries, rebels\nrefrain from disclosing themselves. This poses a significant challenge for\nrebels: estimating their fraction in the population while minimizing the risk\nof being identified as rebels. This paper introduces a distributed computing\nframework aiming to study this question. Our main takeaway message is that the\ncommunication pattern has a crucial role in achieving such a task.\nSpecifically, we show that relying on the inherent noise in the communication,\n\"public communication\", characterized by the fact that each message announced\nby an individual can be viewed by all its neighbors, allows rebels to estimate\ntheir fraction in the population while keeping a negligible risk of each rebel\nbeing identified as such. The suggested estimation protocol, inspired by\nhistorical events, is extremely simple and can be executed covertly even under\nextreme conditions of surveillance. Conversely, we show that under peer-to-peer\ncommunication, protocols of similar simplicity are either inefficient or\nnon-covert.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 14:45:11 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Korman", "Amos", ""], ["Crescenzi", "Pierluigi", ""]]}, {"id": "2105.02315", "submitter": "Marco Serafini", "authors": "Marco Serafini, Hui Guan", "title": "Scalable Graph Neural Network Training: The Case for Sampling", "comments": "9 pages, 2 figures", "journal-ref": "ACM SIGOPS Operating Systems Review, Volume 55, Issue 1, July\n  2021, pp 68-76", "doi": "10.1145/3469379.3469387", "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph Neural Networks (GNNs) are a new and increasingly popular family of\ndeep neural network architectures to perform learning on graphs. Training them\nefficiently is challenging due to the irregular nature of graph data. The\nproblem becomes even more challenging when scaling to large graphs that exceed\nthe capacity of single devices. Standard approaches to distributed DNN\ntraining, such as data and model parallelism, do not directly apply to GNNs.\nInstead, two different approaches have emerged in the literature: whole-graph\nand sample-based training.\n  In this paper, we review and compare the two approaches. Scalability is\nchallenging with both approaches, but we make a case that research should focus\non sample-based training since it is a more promising approach. Finally, we\nreview recent systems supporting sample-based training.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 20:44:10 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Serafini", "Marco", ""], ["Guan", "Hui", ""]]}, {"id": "2105.02420", "submitter": "Joshua Daymude", "authors": "Joshua J. Daymude and Andr\\'ea W. Richa and Christian Scheideler", "title": "The Canonical Amoebot Model: Algorithms and Concurrency Control", "comments": "45 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.ET cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The amoebot model abstracts active programmable matter as a collection of\nsimple computational elements called amoebots that interact locally to\ncollectively achieve tasks of coordination and movement. Since its introduction\n(SPAA 2014), a growing body of literature has adapted its assumptions for a\nvariety of problems; however, without a standardized hierarchy of assumptions,\nprecise systematic comparison of results under the amoebot model is difficult.\nWe propose the canonical amoebot model, an updated formalization that\ndistinguishes between core model features and families of assumption variants.\nA key improvement addressed by the canonical amoebot model is concurrency. Much\nof the existing literature implicitly assumes amoebot actions are isolated and\nreliable, reducing analysis to the sequential setting where at most one amoebot\nis active at a time. However, real programmable matter systems are concurrent.\nThe canonical amoebot model formalizes all amoebot communication as message\npassing, leveraging adversarial activation models of concurrent executions.\nUnder this granular treatment of time, we take two complementary approaches to\nconcurrent algorithm design. In the first, using hexagon formation as a case\nstudy, we establish a set of sufficient conditions that guarantee an\nalgorithm's correctness under any concurrent execution, embedding concurrency\ncontrol directly in algorithm design. In the second, we present a concurrency\ncontrol protocol that uses locks to convert amoebot algorithms that terminate\nin the sequential setting and satisfy certain conventions into algorithms that\nexhibit equivalent behavior in the concurrent setting. These complementary\napproaches to concurrent algorithm design under the canonical amoebot model\nopen new directions for distributed computing research on programmable matter\nand form a rigorous foundation for connections to related literature.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 03:36:25 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Daymude", "Joshua J.", ""], ["Richa", "Andr\u00e9a W.", ""], ["Scheideler", "Christian", ""]]}, {"id": "2105.02919", "submitter": "Birenjith Sasidharan", "authors": "Birenjith Sasidharan, Anoop Thomas", "title": "Coded Gradient Aggregation: A Tradeoff Between Communication Costs at\n  Edge Nodes and at Helper Nodes", "comments": "A shorter version accepted in 2021 IEEE International Symposium on\n  Information Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing amount of data generated at the edge/client nodes and the\nprivacy concerns have resulted in learning at the edge, in which the\ncomputations are performed at edge devices and are communicated to a central\nnode for updating the model. The edge nodes have low bandwidth and may be\navailable only intermittently. There are helper nodes present in the network\nthat aid the edge nodes in the communication to the server. The edge nodes\ncommunicate the local gradient to helper nodes which relay these messages to\nthe central node after possible aggregation. Recently, schemes using repetition\ncodes and maximum-distance-separable (MDS) codes, respectively known as Aligned\nMDS Coding (AMC) scheme and Aligend Repetition Coding (ARC) scheme, were\nproposed. It was observed that in AMC scheme the communication between edge\nnodes and helper nodes is optimal but with an increased cost of communication\nbetween helper and master. An upper bound on the communication cost between\nhelpers and master was obtained. In this paper, a tradeoff between\ncommunication costs at edge nodes and helper nodes is established with the help\nof pyramid codes, a well-known class of locally repairable codes. The\ncommunication costs at both the helper nodes and edge nodes are exactly\ncharacterized. Using the developed technique, the exact communication cost at\nhelper nodes can be computed for the scheme using MDS codes. In the end, we\nprovide two improved aggregation strategies for the existing AMC and ARC\nschemes, yielding significant reduction in communication cost at helpers,\nwithout changing any of the code parameters.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 19:30:54 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Sasidharan", "Birenjith", ""], ["Thomas", "Anoop", ""]]}, {"id": "2105.02937", "submitter": "Jan Kalbantner", "authors": "J. Kalbantner, K. Markantonakis, D. Hurley-Smith, C. Shepherd, B.\n  Semal", "title": "A DLT-based Smart Contract Architecture for Atomic and Scalable Trading", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Distributed Ledger Technology (DLT) has an enormous potential but also\ndownsides. One downside of many DLT systems, such as blockchain, is their\nlimited transaction throughput that hinders their adoption in many use cases\n(e.g., real-time payments). State channels have emerged as a potential solution\nto enhance throughput by allowing transactions to process off-chain. While\ncurrent proposals can increase scalability, they require high collateral and\nlack support for dynamic systems that require asynchronous state transitions.\nAdditionally, the latency of channel initialisations can cause issues\nespecially if fast interactions are required. In this paper, we propose an\natomic, scalable and privacy-preserving protocol that enables secure and\ndynamic updates. We develop a smart contract-based Credit-Note System (CNS)\nthat allows participants to lock funds before a state channel initialisation,\nwhich enhances flexibility and efficiency. We formalise our model using the\nUniversal Composability (UC) framework and demonstrate that it achieves the\nstated design goals of privacy, scalability, and atomicity. Moreover, we\nimplement a dispute process in the state channel to counter availability\nattacks. Finally, we analyse the protocol in the context of an asynchronous\nsmart grid-based marketplace.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 20:24:09 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Kalbantner", "J.", ""], ["Markantonakis", "K.", ""], ["Hurley-Smith", "D.", ""], ["Shepherd", "C.", ""], ["Semal", "B.", ""]]}, {"id": "2105.02957", "submitter": "Piyush Yadav", "authors": "Piyush Yadav, Dhaval Salwala, Edward Curry", "title": "VID-WIN: Fast Video Event Matching with Query-Aware Windowing at the\n  Edge for the Internet of Multimedia Things", "comments": "22 pages, 24 figures, 9 tables, Journal accepted in IEEE Internet of\n  Things Journal", "journal-ref": null, "doi": "10.1109/JIOT.2021.3075336", "report-no": null, "categories": "cs.CV cs.DC cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Efficient video processing is a critical component in many IoMT applications\nto detect events of interest. Presently, many window optimization techniques\nhave been proposed in event processing with an underlying assumption that the\nincoming stream has a structured data model. Videos are highly complex due to\nthe lack of any underlying structured data model. Video stream sources such as\nCCTV cameras and smartphones are resource-constrained edge nodes. At the same\ntime, video content extraction is expensive and requires computationally\nintensive Deep Neural Network (DNN) models that are primarily deployed at\nhigh-end (or cloud) nodes. This paper presents VID-WIN, an adaptive 2-stage\nallied windowing approach to accelerate video event analytics in an edge-cloud\nparadigm. VID-WIN runs parallelly across edge and cloud nodes and performs the\nquery and resource-aware optimization for state-based complex event matching.\nVID-WIN exploits the video content and DNN input knobs to accelerate the video\ninference process across nodes. The paper proposes a novel content-driven\nmicro-batch resizing, queryaware caching and micro-batch based utility\nfiltering strategy of video frames under resource-constrained edge nodes to\nimprove the overall system throughput, latency, and network usage. Extensive\nevaluations are performed over five real-world datasets. The experimental\nresults show that VID-WIN video event matching achieves ~2.3X higher throughput\nwith minimal latency and ~99% bandwidth reduction compared to other baselines\nwhile maintaining query-level accuracy and resource bounds.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 10:08:40 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Yadav", "Piyush", ""], ["Salwala", "Dhaval", ""], ["Curry", "Edward", ""]]}, {"id": "2105.02972", "submitter": "Karla Vargas", "authors": "Carlos L\\'opez, Sergio Rajsbaum, Michel Raynal, Karla Vargas", "title": "Leader Election in Arbitrarily Connected Networks with Process Crashes\n  and Weak Channel Reliability", "comments": "24 pages, 4 figures, 2 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A channel from a process p to a process q satisfies the ADD property if there\nare constants K and D, unknown to the processes, such that in any sequence of K\nconsecutive messages sent by p to q, at least one of them is delivered to q at\nmost D time units after it has been sent. This paper studies implementations of\nan eventual leader, namely, an {\\Omega} failure detector, in an arbitrarily\nconnected network of eventual ADD channels, where processes may fail by\ncrashing. It first presents an algorithm that assumes that processes initially\nknow n, the total number of processes, sending messages of size O( log n).\nThen, it presents a second algorithm that does not assume the processes know n.\nEventually the size of the messages sent by this algorithm is also O( log n).\nThese are the first implementations of leader election in the ADD model. In\nthis model, only eventually perfect failure detectors were considered, sending\nmessages of size O(n log n).\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 21:21:23 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["L\u00f3pez", "Carlos", ""], ["Rajsbaum", "Sergio", ""], ["Raynal", "Michel", ""], ["Vargas", "Karla", ""]]}, {"id": "2105.02977", "submitter": "Lene Kristian Bryngemark", "authors": "Lene Kristian Bryngemark, David Cameron, Valentina Dutta, Thomas\n  Eichlersmith, Balazs Konya, Omar Moreno, Geoffrey Mullier, Florido Paganelli,\n  Ruth P\\\"ottgen, Fuzzy Rogers, Andrii Salnikov, Paul Weakliem", "title": "Building a Distributed Computing System for LDMX: Challenges of creating\n  and operating a lightweight e-infrastructure for small-to-medium size\n  accelerator experiments", "comments": "10 pages, 4 figures, Submitted to 25th International Conference on\n  Computing in High-Energy and Nuclear Physics (vCHEP 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "hep-ex cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Particle physics experiments rely extensively on computing and data services,\nmaking e-infrastructure an integral part of the research collaboration.\nConstructing and operating distributed computing can however be challenging for\na smaller-scale collaboration. The Light Dark Matter eXperiment (LDMX) is a\nplanned small-scale accelerator-based experiment to search for dark matter in\nthe sub-GeV mass region. Finalizing the design of the detector relies on\nMonte-Carlo simulation of expected physics processes. A distributed computing\npilot project was proposed to better utilize available resources at the\ncollaborating institutes, and to improve scalability and reproducibility. This\npaper outlines the chosen lightweight distributed solution, presenting\nrequirements, the component integration steps, and the experiences using a\npilot system for tests with large-scale simulations. The system leverages\nexisting technologies wherever possible, minimizing the need for software\ndevelopment, and deploys only non-intrusive components at the participating\nsites. The pilot proved that integrating existing components can dramatically\nreduce the effort needed to build and operate a distributed e-infrastructure,\nmaking it attainable even for smaller research collaborations.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 21:47:43 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 22:08:23 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Bryngemark", "Lene Kristian", ""], ["Cameron", "David", ""], ["Dutta", "Valentina", ""], ["Eichlersmith", "Thomas", ""], ["Konya", "Balazs", ""], ["Moreno", "Omar", ""], ["Mullier", "Geoffrey", ""], ["Paganelli", "Florido", ""], ["P\u00f6ttgen", "Ruth", ""], ["Rogers", "Fuzzy", ""], ["Salnikov", "Andrii", ""], ["Weakliem", "Paul", ""]]}, {"id": "2105.03009", "submitter": "Fernando Koch", "authors": "Hugo Vaz Sampaio, Fernando Koch, Carlos Becker Westphall, Ricardo do\n  Nascimento Boing, Rene Nolio Santa Cruz", "title": "Autonomic Management of Power Consumption with IoT and Fog Computing", "comments": "14 pages, 7 figures, 11 tables, preprint submitted to Elsevier", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a system for Autonomic Management of Power Consumption in setups\nthat involve Internet of Things (IoT) and Fog Computing. The Central IoT (CIoT)\nis a Fog Computing based solution to provide advanced orchestration mechanisms\nto manage dynamic duty cycles for extra energy savings. The solution works by\nadjusting Home (H) and Away (A) cycles based on contextual information, like\nenvironmental conditions, user behavior, behavior variation, regulations on\nenergy and network resources utilization, among others. Performance analysis\nthrough a proof of concept present average energy savings of 58.4%, reaching up\nto 61.51% when augmenting with a scheduling system and variable long sleep\ncycles (LS). However, there is no linear relation increasing LS time and more\nsavings. The significance of this research is to promote autonomic management\nas a solution to develop more energy efficient buildings and smarter cities,\ntowards sustainable goals.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 00:09:31 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 16:05:43 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Sampaio", "Hugo Vaz", ""], ["Koch", "Fernando", ""], ["Westphall", "Carlos Becker", ""], ["Boing", "Ricardo do Nascimento", ""], ["Cruz", "Rene Nolio Santa", ""]]}, {"id": "2105.03201", "submitter": "Tobias Wegner", "authors": "Tobias Wegner, Mario Lassnig, Peer Ueberholz, Christian Zeitnitz", "title": "Simulation and evaluation of cloud storage caching for data intensive\n  science", "comments": "17 pages, 8 figures, submitted to the Springer Journal \"Computing and\n  Software for Big Science\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common task in scientific computing is the derivation of data. This\nworkflow extracts the most important information from large input data and\nstores it in smaller derived data objects. The derived data objects can then be\nused for further analysis tasks.\n  Typically, those workflows use distributed storage and computing resources. A\nstraightforward configuration of storage media would be low cost tape storage\nand higher cost disk storage. The large, infrequently accessed input data is\nstored on tape storage. The smaller, frequently accessed derived data is stored\non disk storage. In a best case scenario, the large input data is only accessed\nvery infrequently and in a well planned pattern. However, practice shows that\noften the data has to be processed continuously and unpredictably. This can\nsignificantly reduce tape storage performance. A common approach to counter\nthis is storing copies of the large input data on disk storage.\n  This contribution evaluates an approach that uses cloud storage resources to\nserve as a flexible cache or buffer depending on the computational workflow.\nThe proposed model is elaborated for the case of continuously processed data.\nFor the evaluation, a simulation was developed, which can be used to evaluate\nmodels related to storage and network resources.\n  We show that using commercial cloud storage can reduce the on-premises disk\nstorage requirements, while maintaining an equal throughput of jobs. Moreover,\nthe key metrics of the model are discussed and an approach is described that\nuses the simulation to assist with the decision process of using commercial\ncloud storage. The goal is to investigate approaches and propose new evaluation\nmethods to overcome the future data challenges.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 12:20:54 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Wegner", "Tobias", ""], ["Lassnig", "Mario", ""], ["Ueberholz", "Peer", ""], ["Zeitnitz", "Christian", ""]]}, {"id": "2105.03217", "submitter": "Bart{\\l}omiej Przybylski", "authors": "Bart{\\l}omiej Przybylski, Pawe{\\l} \\.Zuk, Krzysztof Rzadca", "title": "Data-driven scheduling in serverless computing to reduce response time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Function as a Service (FaaS), a serverless computing variant, customers\ndeploy functions instead of complete virtual machines or Linux containers. It\nis the cloud provider who maintains the runtime environment for these\nfunctions. FaaS products are offered by all major cloud providers (e.g. Amazon\nLambda, Google Cloud Functions, Azure Functions); as well as standalone\nopen-source software (e.g. Apache OpenWhisk) with their commercial variants\n(e.g. Adobe I/O Runtime or IBM Cloud Functions). We take the bottom-up\nperspective of a single node in a FaaS cluster. We assume that all the\nexecution environments for a set of functions assigned to this node have been\nalready installed. Our goal is to schedule individual invocations of functions,\npassed by a load balancer, to minimize performance metrics related to response\ntime. Deployed functions are usually executed repeatedly in response to\nmultiple invocations made by end-users. Thus, our scheduling decisions are\nbased on the information gathered locally: the recorded call frequencies and\nexecution times. We propose a number of heuristics, and we also adapt some\ntheoretically-grounded ones like SEPT or SERPT. Our simulations use a\nrecently-published Azure Functions Trace. We show that, compared to the\nbaseline FIFO or round-robin, our data-driven scheduling decisions\nsignificantly improve the performance.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 12:44:44 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Przybylski", "Bart\u0142omiej", ""], ["\u017buk", "Pawe\u0142", ""], ["Rzadca", "Krzysztof", ""]]}, {"id": "2105.03371", "submitter": "Haoyu Ren", "authors": "Haoyu Ren, Darko Anicic, Thomas Runkler", "title": "The Synergy of Complex Event Processing and Tiny Machine Learning in\n  Industrial IoT", "comments": "Accepted by The 15th ACM International Conference on Distributed and\n  Event-based Systems (DEBS) 2021", "journal-ref": null, "doi": "10.1145/3465480.3466928", "report-no": null, "categories": "cs.DC cs.AI cs.DB cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Focusing on comprehensive networking, big data, and artificial intelligence,\nthe Industrial Internet-of-Things (IIoT) facilitates efficiency and robustness\nin factory operations. Various sensors and field devices play a central role,\nas they generate a vast amount of real-time data that can provide insights into\nmanufacturing. The synergy of complex event processing (CEP) and machine\nlearning (ML) has been developed actively in the last years in IIoT to identify\npatterns in heterogeneous data streams and fuse raw data into tangible facts.\nIn a traditional compute-centric paradigm, the raw field data are continuously\nsent to the cloud and processed centrally. As IIoT devices become increasingly\npervasive and ubiquitous, concerns are raised since transmitting such amount of\ndata is energy-intensive, vulnerable to be intercepted, and subjected to high\nlatency. The data-centric paradigm can essentially solve these problems by\nempowering IIoT to perform decentralized on-device ML and CEP, keeping data\nprimarily on edge devices and minimizing communications. However, this is no\nmean feat because most IIoT edge devices are designed to be computationally\nconstrained with low power consumption. This paper proposes a framework that\nexploits ML and CEP's synergy at the edge in distributed sensor networks. By\nleveraging tiny ML and micro CEP, we shift the computation from the cloud to\nthe power-constrained IIoT devices and allow users to adapt the on-device ML\nmodel and the CEP reasoning logic flexibly on the fly without requiring to\nreupload the whole program. Lastly, we evaluate the proposed solution and show\nits effectiveness and feasibility using an industrial use case of machine\nsafety monitoring.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 14:58:48 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Ren", "Haoyu", ""], ["Anicic", "Darko", ""], ["Runkler", "Thomas", ""]]}, {"id": "2105.03374", "submitter": "Jan Ruh", "authors": "Jan Ruh, Wilfried Steiner and Gerhard Fohler", "title": "Clock Synchronization in Virtualized Distributed Real-Time Systems using\n  IEEE 802.1AS and ACRN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtualization of distributed real-time systems enables the consolidation of\nmixed-criticality functions on a shared hardware platform thus easing system\nintegration. Time-triggered communication and computation can act as an enabler\nof safe hard real-time systems. A time-triggered hypervisor that activates\nvirtual CPUs according to a global schedule can provide the means to allow for\na resource efficient implementation of the time-triggered paradigm in\nvirtualized distributed real-time systems. A prerequisite of time-triggered\nvirtualization for hard real-time systems is providing access to a global time\nbase to VMs as well as to the hypervisor. A global time base is the result of\nclock synchronization with an upper bound on the clock synchronization\nprecision.\n  We present a formalization of the notion of time in virtualized real-time\nsystems. We use this formalization to propose a virtual clock condition that\nenables us to test the suitability of a virtual clock for the design of\nvirtualized time-triggered real-time systems. We discuss and model how\nvirtualization, in particular resource consolidation versus resource\npartitioning, degrades clock synchronization precision. Finally, we apply our\ninsights to model the IEEE~802.1AS clock synchronization protocol and derive an\nupper bound on the clock synchronization precision of IEEE 802.1AS. We present\nour implementation of a dependent clock for ACRN that can be synchronized to a\ngrandmaster clock. The results of our experiments illustrate that a type-1\nhypervisor implementing a dependent clock yields native clock synchronization\nprecision. Furthermore, we show that the upper bound derived from our model\nholds for a series of experiments featuring native as well as virtualized\nsetups.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 10:59:32 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 06:56:22 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Ruh", "Jan", ""], ["Steiner", "Wilfried", ""], ["Fohler", "Gerhard", ""]]}, {"id": "2105.03545", "submitter": "Jared Coleman", "authors": "Jared Coleman, Evangelos Kranakis, Danny Krizanc, Oscar Morales Ponce", "title": "The Pony Express Communication Problem", "comments": "14 pages, 3 figures to be published in IWOCA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a new problem which we call the Pony Express problem. n robots\nwith differing speeds are situated over some domain. A message is placed at\nsome commonly known point. Robots can acquire the message either by visiting\nits initial position, or by encountering another robot that has already\nacquired it. The robots must collaborate to deliver the message to a given\ndestination. The objective is to deliver the message in minimum time. In this\npaper we study the Pony Express problem on the line where n robots are\narbitrarily deployed along a finite segment. The robots have different speeds\nand can move in both directions. We are interested in both offline centralized\nand online distributed algorithms. In the online case, we assume the robots\nhave limited knowledge of the initial configuration. In particular, the robots\ndo not know the initial positions and speeds of the other robots nor even their\nown position and speed. They do, however, know the direction on the line in\nwhich to find the message and have the ability to compare speeds when they\nmeet.\n  First, we study the Pony Express problem where the message is initially\nplaced at one endpoint of a segment and must be delivered to the other\nendpoint. We provide an O(n log n) running time offline algorithm as well as an\noptimal online algorithm. Then we study the Half-Broadcast problem where the\nmessage is at the center and must be delivered to either one of the endpoints\nof the segment [-1,1]. We provide an offline algorithm running in O(n^2 log n)\ntime and we provide an online algorithm that attains a competitive ratio of 3/2\nwhich we show is the best possible. Finally, we study the Broadcast problem\nwhere the message is at the center and must be delivered to both endpoints of\nthe segment [-1,1]. Here we give an FPTAS in the offline case and an online\nalgorithm that attains a competitive ratio of 9/5, which we show is tight.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 00:46:56 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Coleman", "Jared", ""], ["Kranakis", "Evangelos", ""], ["Krizanc", "Danny", ""], ["Ponce", "Oscar Morales", ""]]}, {"id": "2105.03559", "submitter": "Qiu HouMing", "authors": "Houming Qiu, Kun Zhu, Nguyen Cong Luong, Changyan Yi, Dusit Niyato,\n  Dong In Kim", "title": "Applications of Auction and Mechanism Design in Edge Computing: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge computing as a promising technology provides lower latency, more\nefficient transmission, and faster speed of data processing since the edge\nservers are closer to the user devices. Each edge server with limited resources\ncan offload latency-sensitive and computation-intensive tasks from nearby user\ndevices. However, edge computing faces challenges such as resource allocation,\nenergy consumption, security and privacy issues, etc. Auction mechanisms can\nwell characterize bidirectional interactions between edge servers and user\ndevices under the above constraints in edge computing. As demonstrated by the\nexisting works, auction and mechanism design approaches are outstanding on\nachieving optimal allocation strategy while guaranteeing mutual satisfaction\namong edge servers and user devices, especially for scenarios with scarce\nresources. In this paper, we introduce a comprehensive survey of recent\nresearches that apply auction approaches in edge computing. Firstly, a brief\noverview of edge computing including three common edge computing paradigms,\ni.e., cloudlet, fog computing and mobile edge computing, is presented. Then, we\nintroduce fundamentals and backgrounds of auction schemes commonly used in edge\ncomputing systems. After then, a comprehensive survey of applications of\nauction-based approaches applied for edge computing is provided, which is\ncategorized by different auction approaches. Finally, several open challenges\nand promising research directions are discussed.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 02:21:09 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Qiu", "Houming", ""], ["Zhu", "Kun", ""], ["Luong", "Nguyen Cong", ""], ["Yi", "Changyan", ""], ["Niyato", "Dusit", ""], ["Kim", "Dong In", ""]]}, {"id": "2105.03572", "submitter": "Bin Cao", "authors": "Bin Cao, Zixin Wang, Long Zhang, Daquan Feng, Mugen Peng, Lei Zhang", "title": "Blockchain Systems, Technologies and Applications: A Methodology\n  Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past decade, blockchain has shown a promising vision greatly to build\nthe trust without any powerful third party in a secure, decentralized and\nsalable manner. However, due to the wide application and future development\nfrom cryptocurrency to Internet of Things, blockchain is an extremely complex\nsystem enabling integration with mathematics, finance, computer science,\ncommunication and network engineering, etc. As a result, it is a challenge for\nengineer, expert and researcher to fully understand the blockchain process in a\nsystematic view from top to down. First, this article introduces how blockchain\nworks, the research activity and challenge, and illustrates the roadmap\ninvolving the classic methodology with typical blockchain use cases and topics.\nSecond, in blockchain system, how to adopt stochastic process, game theory,\noptimization, machine learning and cryptography to study blockchain running\nprocess and design blockchain protocol/algorithm are discussed in details.\nMoreover, the advantage and limitation using these methods are also summarized\nas the guide of future work to further considered. Finally, some remaining\nproblems from technical, commercial and political views are discussed as the\nopen issues. The main findings of this article will provide an overview in a\nmethodology perspective to study theoretical model for blockchain fundamentals\nunderstanding, design network service for blockchain-based mechanisms and\nalgorithms, as well as apply blockchain for Internet of Things, etc.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 03:19:11 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Cao", "Bin", ""], ["Wang", "Zixin", ""], ["Zhang", "Long", ""], ["Feng", "Daquan", ""], ["Peng", "Mugen", ""], ["Zhang", "Lei", ""]]}, {"id": "2105.03573", "submitter": "Brett Fazio", "authors": "Brett Fazio, Ellie Kozlowski, Dylan Ochoa, Blake Robertson, Idel\n  Martinez", "title": "Survey of Parallel A* in Rust", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A* is one of the most popular Best First Search (BFS) techniques for graphs.\nIt combines the cost-based search of Breadth First Search with a computed\nheuristic for each node to attempt to locate the goal path faster than\ntraditional Breadth First Search or Depth First Search techniques. However, A*\nis a sequential algorithm. The standard implementation only runs in one thread.\nThere are a few attempts to get A* to leverage multiple threads. Centralized\n(SPA*) and Decentralized (DPA*, HDA*) methods are the most standard attempts,\nwith the most unique and modern method being massively-parallel A* (MPA* or\nGA*). We will attempt an implementation of each in Rust to determine if there\nis a performance boost, and which one has the best performance.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 03:20:10 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Fazio", "Brett", ""], ["Kozlowski", "Ellie", ""], ["Ochoa", "Dylan", ""], ["Robertson", "Blake", ""], ["Martinez", "Idel", ""]]}, {"id": "2105.03617", "submitter": "Masahito Ohue", "authors": "Masahito Ohue, Yutaka Akiyama", "title": "MEGADOCK-GUI: a GUI-based complete cross-docking tool for exploring\n  protein-protein interactions", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM cs.DC q-bio.MN q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information on protein-protein interactions (PPIs) not only advances our\nunderstanding of molecular biology but also provides important clues for target\nselection in drug discovery and the design of PPI inhibitors. One of the\ntechniques used for computational prediction of PPIs is protein-protein docking\ncalculations, and a variety of software has been developed. However, a friendly\ninterface for users who are not sufficiently familiar with the command line\ninterface has not been developed so far. In this study, we have developed a\ngraphical user interface, MEGADOCK-GUI, which enables users to easily predict\nPPIs and protein complex structures. In addition to the original 3-D molecular\nviewer and input file preparation functions, MEGADOCK-GUI is software that can\nautomatically perform complete cross-docking of $M$ vs. $N$ proteins. With\nMEGADOCK-GUI, various applications related to the prediction of PPIs, such as\nensemble docking that handles multiple conformations of proteins and screening\nof binding partner proteins that bind to specific proteins, can now be easily\nperformed.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 07:32:06 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Ohue", "Masahito", ""], ["Akiyama", "Yutaka", ""]]}, {"id": "2105.03638", "submitter": "Ryota Eguchi", "authors": "Ryota Eguchi, Naoki Kitamura, and Taisuke Izumi", "title": "Fast Neighborhood Rendezvous", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the rendezvous problem, two computing entities (called \\emph{agents})\nlocated at different vertices in a graph have to meet at the same vertex. In\nthis paper, we consider the synchronous \\emph{neighborhood rendezvous problem},\nwhere the agents are initially located at two adjacent vertices. While this\nproblem can be trivially solved in $O(\\Delta)$ rounds ($\\Delta$ is the maximum\ndegree of the graph), it is highly challenging to reveal whether that problem\ncan be solved in $o(\\Delta)$ rounds, even assuming the rich computational\ncapability of agents. The only known result is that the time complexity of\n$O(\\sqrt{n})$ rounds is achievable if the graph is complete and agents are\nprobabilistic, asymmetric, and can use whiteboards placed at vertices. Our main\ncontribution is to clarify the situation (with respect to computational models\nand graph classes) admitting such a sublinear-time rendezvous algorithm. More\nprecisely, we present two algorithms achieving fast rendezvous additionally\nassuming bounded minimum degree, unique vertex identifier, accessibility to\nneighborhood IDs, and randomization. The first algorithm runs within\n$\\tilde{O}(\\sqrt{n\\Delta/\\delta} + n/\\delta)$ rounds for graphs of the minimum\ndegree larger than $\\sqrt{n}$, where $n$ is the number of vertices in the\ngraph, and $\\delta$ is the minimum degree of the graph. The second algorithm\nassumes that the largest vertex ID is $O(n)$, and achieves $\\tilde{O}\\left(\n\\frac{n}{\\sqrt{\\delta}} \\right)$-round time complexity without using\nwhiteboards. These algorithms attain $o(\\Delta)$-round complexity in the case\nof $\\delta = {\\omega}(\\sqrt{n} \\log n)$ and $\\delta = \\omega(n^{2/3} \\log^{4/3}\nn)$ respectively.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 08:38:05 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Eguchi", "Ryota", ""], ["Kitamura", "Naoki", ""], ["Izumi", "Taisuke", ""]]}, {"id": "2105.03649", "submitter": "Amar Shrestha", "authors": "Amar Shrestha, Haowen Fang, Daniel Patrick Rider, Zaidao Mei and Qinru\n  Qiu", "title": "In-Hardware Learning of Multilayer Spiking Neural Networks on a\n  Neuromorphic Processor", "comments": "6 pages, 5 figures, accepted for Design Automation Conference (DAC)\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DC cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although widely used in machine learning, backpropagation cannot directly be\napplied to SNN training and is not feasible on a neuromorphic processor that\nemulates biological neuron and synapses. This work presents a spike-based\nbackpropagation algorithm with biological plausible local update rules and\nadapts it to fit the constraint in a neuromorphic hardware. The algorithm is\nimplemented on Intel Loihi chip enabling low power in-hardware supervised\nonline learning of multilayered SNNs for mobile applications. We test this\nimplementation on MNIST, Fashion-MNIST, CIFAR-10 and MSTAR datasets with\npromising performance and energy-efficiency, and demonstrate a possibility of\nincremental online learning with the implementation.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 09:22:21 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Shrestha", "Amar", ""], ["Fang", "Haowen", ""], ["Rider", "Daniel Patrick", ""], ["Mei", "Zaidao", ""], ["Qiu", "Qinru", ""]]}, {"id": "2105.03725", "submitter": "Geraldo Francisco De Oliveira Junior", "authors": "Geraldo F. Oliveira and Juan G\\'omez-Luna and Lois Orosa and Saugata\n  Ghose and Nandita Vijaykumar and Ivan Fernandez and Mohammad Sadrosadati and\n  Onur Mutlu", "title": "DAMOV: A New Methodology and Benchmark Suite for Evaluating Data\n  Movement Bottlenecks", "comments": "Our open source software is available at\n  https://github.com/CMU-SAFARI/DAMOV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data movement between the CPU and main memory is a first-order obstacle\nagainst improving performance, scalability, and energy efficiency in modern\nsystems. Computer systems employ a range of techniques to reduce overheads tied\nto data movement, spanning from traditional mechanisms (e.g., deep multi-level\ncache hierarchies, aggressive hardware prefetchers) to emerging techniques such\nas Near-Data Processing (NDP), where some computation is moved close to memory.\nOur goal is to methodically identify potential sources of data movement over a\nbroad set of applications and to comprehensively compare traditional\ncompute-centric data movement mitigation techniques to more memory-centric\ntechniques, thereby developing a rigorous understanding of the best techniques\nto mitigate each source of data movement.\n  With this goal in mind, we perform the first large-scale characterization of\na wide variety of applications, across a wide range of application domains, to\nidentify fundamental program properties that lead to data movement to/from main\nmemory. We develop the first systematic methodology to classify applications\nbased on the sources contributing to data movement bottlenecks. From our\nlarge-scale characterization of 77K functions across 345 applications, we\nselect 144 functions to form the first open-source benchmark suite (DAMOV) for\nmain memory data movement studies. We select a diverse range of functions that\n(1) represent different types of data movement bottlenecks, and (2) come from a\nwide range of application domains. Using NDP as a case study, we identify new\ninsights about the different data movement bottlenecks and use these insights\nto determine the most suitable data movement mitigation mechanism for a\nparticular application. We open-source DAMOV and the complete source code for\nour new characterization methodology at https://github.com/CMU-SAFARI/DAMOV.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 16:02:53 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 17:03:34 GMT"}, {"version": "v3", "created": "Thu, 20 May 2021 15:57:09 GMT"}, {"version": "v4", "created": "Tue, 6 Jul 2021 17:05:36 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Oliveira", "Geraldo F.", ""], ["G\u00f3mez-Luna", "Juan", ""], ["Orosa", "Lois", ""], ["Ghose", "Saugata", ""], ["Vijaykumar", "Nandita", ""], ["Fernandez", "Ivan", ""], ["Sadrosadati", "Mohammad", ""], ["Mutlu", "Onur", ""]]}, {"id": "2105.03789", "submitter": "Jingji Chen", "authors": "Jingji Chen, Xuehai Qian", "title": "Kudu: An Efficient and Scalable Distributed Graph Pattern Mining Engine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes Kudu, a general distributed execution engine with a\nwell-defined abstraction that can be integrated with various existing\nsingle-machine graph pattern mining (GPM) systems. With this approach, the\nprogramming interfaces and codes based on existing GPM systems do not change\nand Kudu can transparently enable the distributed execution. The key novelty is\nextendable embedding which can express pattern enumeration algorithm and enable\nfine-grained task scheduling. To enable efficient scheduling, we propose a\nnovel BFS-DFS hybrid exploration method that generates sufficient concurrent\ntasks without incurring high memory consumption. The computation and\ncommunication of Kudu can be further optimized with several effective\ntechniques. We implemented two scalable distributed GPM systems by porting\nAutomine and GraphPi on Kudu. Our evaluation shows that Kudu-based systems\nsignificantly outperform state-of-the-art graph partition-based GPM systems by\nup to three orders of magnitude, achieve similar or even better performance\ncompared with the fastest graph replication-based systems, and scale to large\ndatasets with graph partitioning.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 21:58:29 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Chen", "Jingji", ""], ["Qian", "Xuehai", ""]]}, {"id": "2105.03814", "submitter": "Juan G\\'omez-Luna", "authors": "Juan G\\'omez-Luna, Izzat El Hajj, Ivan Fernandez, Christina Giannoula,\n  Geraldo F. Oliveira, Onur Mutlu", "title": "Benchmarking a New Paradigm: An Experimental Analysis of a Real\n  Processing-in-Memory Architecture", "comments": "Our open source software is available at\n  https://github.com/CMU-SAFARI/prim-benchmarks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many modern workloads, such as neural networks, databases, and graph\nprocessing, are fundamentally memory-bound. For such workloads, the data\nmovement between main memory and CPU cores imposes a significant overhead in\nterms of both latency and energy. A major reason is that this communication\nhappens through a narrow bus with high latency and limited bandwidth, and the\nlow data reuse in memory-bound workloads is insufficient to amortize the cost\nof main memory access. Fundamentally addressing this data movement bottleneck\nrequires a paradigm where the memory system assumes an active role in computing\nby integrating processing capabilities. This paradigm is known as\nprocessing-in-memory (PIM).\n  Recent research explores different forms of PIM architectures, motivated by\nthe emergence of new 3D-stacked memory technologies that integrate memory with\na logic layer where processing elements can be easily placed. Past works\nevaluate these architectures in simulation or, at best, with simplified\nhardware prototypes. In contrast, the UPMEM company has designed and\nmanufactured the first publicly-available real-world PIM architecture.\n  This paper provides the first comprehensive analysis of the first\npublicly-available real-world PIM architecture. We make two key contributions.\nFirst, we conduct an experimental characterization of the UPMEM-based PIM\nsystem using microbenchmarks to assess various architecture limits such as\ncompute throughput and memory bandwidth, yielding new insights. Second, we\npresent PrIM, a benchmark suite of 16 workloads from different application\ndomains (e.g., linear algebra, databases, graph processing, neural networks,\nbioinformatics).\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 01:57:47 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 17:05:02 GMT"}, {"version": "v3", "created": "Thu, 17 Jun 2021 20:58:40 GMT"}, {"version": "v4", "created": "Tue, 6 Jul 2021 16:39:26 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["G\u00f3mez-Luna", "Juan", ""], ["Hajj", "Izzat El", ""], ["Fernandez", "Ivan", ""], ["Giannoula", "Christina", ""], ["Oliveira", "Geraldo F.", ""], ["Mutlu", "Onur", ""]]}, {"id": "2105.04086", "submitter": "Guangyao Zhou", "authors": "Guangyao Zhou, Wenhong Tian, Rajkumar Buyya", "title": "Deep Reinforcement Learning-based Methods for Resource Scheduling in\n  Cloud Computing: A Review and Future Directions", "comments": "18 pages,9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As the quantity and complexity of information processed by software systems\nincrease, large-scale software systems have an increasing requirement for\nhigh-performance distributed computing systems. With the acceleration of the\nInternet in Web 2.0, Cloud computing as a paradigm to provide dynamic,\nuncertain and elastic services has shown superiorities to meet the computing\nneeds dynamically. Without an appropriate scheduling approach, extensive Cloud\ncomputing may cause high energy consumptions and high cost, in addition that\nhigh energy consumption will cause massive carbon dioxide emissions. Moreover,\ninappropriate scheduling will reduce the service life of physical devices as\nwell as increase response time to users' request. Hence, efficient scheduling\nof resource or optimal allocation of request, that usually a NP-hard problem,\nis one of the prominent issues in emerging trends of Cloud computing. Focusing\non improving quality of service (QoS), reducing cost and abating contamination,\nresearchers have conducted extensive work on resource scheduling problems of\nCloud computing over years. Nevertheless, growing complexity of Cloud\ncomputing, that the super-massive distributed system, is limiting the\napplication of scheduling approaches. Machine learning, a utility method to\ntackle problems in complex scenes, is used to resolve the resource scheduling\nof Cloud computing as an innovative idea in recent years. Deep reinforcement\nlearning (DRL), a combination of deep learning (DL) and reinforcement learning\n(RL), is one branch of the machine learning and has a considerable prospect in\nresource scheduling of Cloud computing. This paper surveys the methods of\nresource scheduling with focus on DRL-based scheduling approaches in Cloud\ncomputing, also reviews the application of DRL as well as discusses challenges\nand future directions of DRL in scheduling of Cloud computing.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 03:07:35 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Zhou", "Guangyao", ""], ["Tian", "Wenhong", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "2105.04087", "submitter": "Pengcheng Ren", "authors": "Pengcheng Ren and Tongjiang Yan", "title": "Latency Analysis of Consortium Blockchained Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A decentralized federated learning architecture is proposed to apply to the\nBusinesses-to-Businesses scenarios by introducing the consortium blockchain in\nthis paper. We introduce a model verification mechanism to ensure the quality\nof local models trained by participators. To analyze the latency of the system,\na latency model is constructed by considering the work flow of the\narchitecture. Finally the experiment results show that our latency model does\nwell in quantifying the actual delays.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 03:14:52 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Ren", "Pengcheng", ""], ["Yan", "Tongjiang", ""]]}, {"id": "2105.04357", "submitter": "Alejandro Ranchal-Pedrosa", "authors": "Alejandro Ranchal-Pedrosa and Vincent Gramoli", "title": "Agreement in the presence of disagreeing rational players: The Huntsman\n  Protocol", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel Byzantine consensus protocol among $n$ players is\nproposed for the partially synchronous model. In particular, by assuming that\nstandard cryptography is unbreakable, and that\n$n>\\max\\bigl(\\frac{3}{2}k+3t,2(k+t)\\bigr)$, this protocol is an equilibrium\nwhere no coalition of $k$ rational players can coordinate to increase their\nexpected utility regardless of the arbitrary behavior of up to $t$ Byzantine\nplayers. We show that a baiting strategy is necessary and sufficient to solve\nthis, so-called rational agreement problem. First, we show that it is\nimpossible to solve this rational agreement problem without implementing a\nbaiting strategy, a strategy that rewards rational players for betraying its\ncoalition, by exposing undeniable proofs of fraud. Second, we propose the\nHuntsman protocol that solves the rational agreement problem by building recent\nadvances in the context of accountable Byzantine agreement in partial\nsynchrony. This protocol finds applications in distributed ledgers where\nplayers are incentivized to steal assets by leading other players to a\ndisagreement on two distinct decisions where they ``double spend''.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 13:42:27 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Ranchal-Pedrosa", "Alejandro", ""], ["Gramoli", "Vincent", ""]]}, {"id": "2105.04449", "submitter": "Hongzhi Chen", "authors": "Hongzhi Chen, Changji Li, Chenguang Zheng, Chenghuan Huang, Juncheng\n  Fang, James Cheng, Jian Zhang", "title": "G-Tran: Making Distributed Graph Transactions Fast", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Graph transaction processing raises many unique challenges such as random\ndata access due to the irregularity of graph structures, low throughput and\nhigh abort rate due to the relatively large read/write sets in graph\ntransactions. To address these challenges, we present G-Tran -- an RDMA-enabled\ndistributed in-memory graph database with serializable and snapshot isolation\nsupport. First, we propose a graph-native data store to achieve good data\nlocality and fast data access for transactional updates and queries. Second,\nG-Tran adopts a fully decentralized architecture that leverages RDMA to process\ndistributed transactions with the MPP model, which can achieve high performance\nby utilizing all computing resources. In addition, we propose a new MV-OCC\nimplementation with two optimizations to address the issue of large read/write\nsets in graph transactions. Extensive experiments show that G-Tran achieves\ncompetitive performance compared with other popular graph databases on\nbenchmark workloads.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 15:18:27 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 11:13:11 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Chen", "Hongzhi", ""], ["Li", "Changji", ""], ["Zheng", "Chenguang", ""], ["Huang", "Chenghuan", ""], ["Fang", "Juncheng", ""], ["Cheng", "James", ""], ["Zhang", "Jian", ""]]}, {"id": "2105.04555", "submitter": "Jaehoon Koo", "authors": "Jaehoon Koo, Prasanna Balaprakash, Michael Kruse, Xingfu Wu, Paul\n  Hovland, Mary Hall", "title": "Customized Monte Carlo Tree Search for LLVM/Polly's Composable Loop\n  Optimization Transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI cs.DC cs.LG cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Polly is the LLVM project's polyhedral loop nest optimizer. Recently,\nuser-directed loop transformation pragmas were proposed based on LLVM/Clang and\nPolly. The search space exposed by the transformation pragmas is a tree,\nwherein each node represents a specific combination of loop transformations\nthat can be applied to the code resulting from the parent node's loop\ntransformations. We have developed a search algorithm based on Monte Carlo tree\nsearch (MCTS) to find the best combination of loop transformations. Our\nalgorithm consists of two phases: exploring loop transformations at different\ndepths of the tree to identify promising regions in the tree search space and\nexploiting those regions by performing a local search. Moreover, a restart\nmechanism is used to avoid the MCTS getting trapped in a local solution. The\nbest and worst solutions are transferred from the previous phases of the\nrestarts to leverage the search history. We compare our approach with random,\ngreedy, and breadth-first search methods on PolyBench kernels and ECP proxy\napplications. Experimental results show that our MCTS algorithm finds pragma\ncombinations with a speedup of 2.3x over Polly's heuristic optimizations on\naverage.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 21:57:39 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Koo", "Jaehoon", ""], ["Balaprakash", "Prasanna", ""], ["Kruse", "Michael", ""], ["Wu", "Xingfu", ""], ["Hovland", "Paul", ""], ["Hall", "Mary", ""]]}, {"id": "2105.04663", "submitter": "Yuanzhong Xu", "authors": "Yuanzhong Xu, HyoukJoong Lee, Dehao Chen, Blake Hechtman, Yanping\n  Huang, Rahul Joshi, Maxim Krikun, Dmitry Lepikhin, Andy Ly, Marcello\n  Maggioni, Ruoming Pang, Noam Shazeer, Shibo Wang, Tao Wang, Yonghui Wu,\n  Zhifeng Chen", "title": "GSPMD: General and Scalable Parallelization for ML Computation Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present GSPMD, an automatic, compiler-based parallelization system for\ncommon machine learning computation graphs. It allows users to write programs\nin the same way as for a single device, then give hints through a few\nannotations on how to distribute tensors, based on which GSPMD will parallelize\nthe computation. Its representation of partitioning is simple yet general,\nallowing it to express different or mixed paradigms of parallelism on a wide\nvariety of models.\n  GSPMD infers the partitioning for every operator in the graph based on\nlimited user annotations, making it convenient to scale up existing\nsingle-device programs. It solves several technical challenges for production\nusage, such as static shape constraints, uneven partitioning, exchange of halo\ndata, and nested operator partitioning. These techniques allow GSPMD to achieve\n50% to 62% compute utilization on 128 to 2048 Cloud TPUv3 cores for models with\nup to one trillion parameters.\n  GSPMD produces a single program for all devices, which adjusts its behavior\nbased on a run-time partition ID, and uses collective operators for\ncross-device communication. This property allows the system itself to be\nscalable: the compilation time stays constant with increasing number of\ndevices.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 20:54:58 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Xu", "Yuanzhong", ""], ["Lee", "HyoukJoong", ""], ["Chen", "Dehao", ""], ["Hechtman", "Blake", ""], ["Huang", "Yanping", ""], ["Joshi", "Rahul", ""], ["Krikun", "Maxim", ""], ["Lepikhin", "Dmitry", ""], ["Ly", "Andy", ""], ["Maggioni", "Marcello", ""], ["Pang", "Ruoming", ""], ["Shazeer", "Noam", ""], ["Wang", "Shibo", ""], ["Wang", "Tao", ""], ["Wu", "Yonghui", ""], ["Chen", "Zhifeng", ""]]}, {"id": "2105.04700", "submitter": "Alexandre Nolin", "authors": "Magn\\'us M. Halld\\'orsson, Alexandre Nolin, Tigran Tonoyan", "title": "Ultrafast Distributed Coloring of High Degree Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a new randomized distributed algorithm for the $\\Delta+1$-list\ncoloring problem. The algorithm and its analysis dramatically simplify the\nprevious best result known of Chang, Li, and Pettie [SICOMP 2020]. This allows\nfor numerous refinements, and in particular, we can color all $n$-node graphs\nof maximum degree $\\Delta \\ge \\log^{2+\\Omega(1)} n$ in $O(\\log^* n)$ rounds.\nThe algorithm works in the CONGEST model, i.e., it uses only $O(\\log n)$ bits\nper message for communication. On low-degree graphs, the algorithm shatters the\ngraph into components of size $\\operatorname{poly}(\\log n)$ in $O(\\log^*\n\\Delta)$ rounds, showing that the randomized complexity of $\\Delta+1$-list\ncoloring in CONGEST depends inherently on the deterministic complexity of\nrelated coloring problems.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 22:49:48 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Halld\u00f3rsson", "Magn\u00fas M.", ""], ["Nolin", "Alexandre", ""], ["Tonoyan", "Tigran", ""]]}, {"id": "2105.04720", "submitter": "Renan Souza", "authors": "Renan Souza, V\\'itor Silva, Alexandre A. B. Lima, Daniel de Oliveira,\n  Patrick Valduriez, Marta Mattoso", "title": "Distributed In-memory Data Management for Workflow Executions", "comments": "26 pages, 14 figures, PeerJ Computer Science (2021)", "journal-ref": null, "doi": "10.7717/peerj-cs.527", "report-no": null, "categories": "cs.DC cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Complex scientific experiments from various domains are typically modeled as\nworkflows and executed on large-scale machines using a Parallel Workflow\nManagement System (WMS). Since such executions usually last for hours or days,\nsome WMSs provide user steering support, i.e., they allow users to run data\nanalyses and, depending on the results, adapt the workflows at runtime. A\nchallenge in the parallel execution control design is to manage workflow data\nfor efficient executions while enabling user steering support. Data access for\nhigh scalability is typically transaction-oriented, while for data analysis, it\nis online analytical-oriented so that managing such hybrid workloads makes the\nchallenge even harder. In this work, we present SchalaDB, an architecture with\na set of design principles and techniques based on distributed in-memory data\nmanagement for efficient workflow execution control and user steering. We\npropose a distributed data design for scalable workflow task scheduling and\nhigh availability driven by a parallel and distributed in-memory DBMS. To\nevaluate our proposal, we develop d-Chiron, a WMS designed according to\nSchalaDB's principles. We carry out an extensive experimental evaluation on an\nHPC cluster with up to 960 computing cores. Among other analyses, we show that\neven when running data analyses for user steering, SchalaDB's overhead is\nnegligible for workloads composed of hundreds of concurrent tasks on shared\ndata. Our results encourage workflow engine developers to follow a parallel and\ndistributed data-oriented approach not only for scheduling and monitoring but\nalso for user steering.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 00:16:47 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 01:30:04 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Souza", "Renan", ""], ["Silva", "V\u00edtor", ""], ["Lima", "Alexandre A. B.", ""], ["de Oliveira", "Daniel", ""], ["Valduriez", "Patrick", ""], ["Mattoso", "Marta", ""]]}, {"id": "2105.04731", "submitter": "Wadii Boulila Prof.", "authors": "Yosra Hajjaji, Wadii Boulila, Imed Riadh Farah", "title": "An improved tile-based scalable distributed management model of massive\n  high-resolution satellite images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The amount of remote sensing (RS) data has increased at an unexpected scale,\ndue to the rapid progress of earth-observation and the growth of satellite RS\nand sensor technologies. Traditional relational databases attend their limit to\nmeet the needs of high-resolution and large-scale RS Big Data management. As a\nresult, massive RS data management is currently one of the most imperative\ntopics. To address this problem, this paper describes a distributed\narchitecture for big RS data storage based on a unified metadata file, pyramid\nmodel, and Hilbert curve for data composition and indexing using NoSQL\ndatabases (i.e, Apache Hbase). In this paper, a Hadoop-based framework in\nAzureInsight cloud platform is designed to manage massive RS data in a parallel\nand distributed way. Experimental results prove that our method has the\npotential to overcome the weakness of traditional methods. The proposed model\nis suitable for massive high-resolution image data management.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 01:00:17 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Hajjaji", "Yosra", ""], ["Boulila", "Wadii", ""], ["Farah", "Imed Riadh", ""]]}, {"id": "2105.04851", "submitter": "Shi Pu", "authors": "Kun Huang and Shi Pu", "title": "Improving the Transient Times for Distributed Stochastic Gradient\n  Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG cs.MA cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the distributed optimization problem where $n$ agents each\npossessing a local cost function, collaboratively minimize the average of the\n$n$ cost functions over a connected network. Assuming stochastic gradient\ninformation is available, we study a distributed stochastic gradient algorithm,\ncalled exact diffusion with adaptive stepsizes (EDAS) adapted from the Exact\nDiffusion method and NIDS and perform a non-asymptotic convergence analysis. We\nnot only show that EDAS asymptotically achieves the same network independent\nconvergence rate as centralized stochastic gradient descent (SGD) for\nminimizing strongly convex and smooth objective functions, but also\ncharacterize the transient time needed for the algorithm to approach the\nasymptotic convergence rate, which behaves as\n$K_T=\\mathcal{O}\\left(\\frac{n}{1-\\lambda_2}\\right)$, where $1-\\lambda_2$ stands\nfor the spectral gap of the mixing matrix. To the best of our knowledge, EDAS\nachieves the shortest transient time when the average of the $n$ cost functions\nis strongly convex and each cost function is smooth. Numerical simulations\nfurther corroborate and strengthen the obtained theoretical results.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 08:09:31 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Huang", "Kun", ""], ["Pu", "Shi", ""]]}, {"id": "2105.04880", "submitter": "Yiming Wang", "authors": "Yiming Wang, Dongxia Chang, Zhiqiang Fu and Yao Zhao", "title": "Consistent Multiple Graph Embedding for Multi-View Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph-based multi-view clustering aiming to obtain a partition of data across\nmultiple views, has received considerable attention in recent years. Although\ngreat efforts have been made for graph-based multi-view clustering, it remains\na challenge to fuse characteristics from various views to learn a common\nrepresentation for clustering. In this paper, we propose a novel Consistent\nMultiple Graph Embedding Clustering framework(CMGEC). Specifically, a multiple\ngraph auto-encoder(M-GAE) is designed to flexibly encode the complementary\ninformation of multi-view data using a multi-graph attention fusion encoder. To\nguide the learned common representation maintaining the similarity of the\nneighboring characteristics in each view, a Multi-view Mutual Information\nMaximization module(MMIM) is introduced. Furthermore, a graph fusion\nnetwork(GFN) is devised to explore the relationship among graphs from different\nviews and provide a common consensus graph needed in M-GAE. By jointly training\nthese models, the common latent representation can be obtained which encodes\nmore complementary information from multiple views and depicts data more\ncomprehensively. Experiments on three types of multi-view datasets demonstrate\nCMGEC outperforms the state-of-the-art clustering methods.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 09:08:22 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Wang", "Yiming", ""], ["Chang", "Dongxia", ""], ["Fu", "Zhiqiang", ""], ["Zhao", "Yao", ""]]}, {"id": "2105.04909", "submitter": "Petr Kuznetsov", "authors": "Luciano Freitas de Souza, Petr Kuznetsov, Thibault Rieutord, Sara\n  Tucci-Piergiovanni", "title": "Accountability and Reconfiguration: Self-Healing Lattice Agreement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An accountable distributed system provides means to detect deviations of\nsystem components from their expected behavior. It is natural to complement\nfault detection with a reconfiguration mechanism, so that the system could heal\nitself, by replacing malfunctioning parts with new ones. In this paper, we\ndescribe a framework that can be used to implement a large class of accountable\nand reconfigurable replicated services. We build atop the fundamental lattice\nagreement abstraction lying at the core of storage systems and\ncryptocurrencies.\n  Our asynchronous implementation of accountable lattice agreement ensures that\nevery violation of consistency is followed by an undeniable evidence of\nmisbehavior of a faulty replica. The system can then be seamlessly reconfigured\nby evicting faulty replicas, adding new ones and merging inconsistent states.\nWe believe that this paper opens a direction towards asynchronous\n\"self-healing\" systems that combine accountability and reconfiguration.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 09:54:59 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["de Souza", "Luciano Freitas", ""], ["Kuznetsov", "Petr", ""], ["Rieutord", "Thibault", ""], ["Tucci-Piergiovanni", "Sara", ""]]}, {"id": "2105.04937", "submitter": "Takeshi Fukaya", "authors": "Takeshi Fukaya, Koki Ishida, Akie Miura, Takeshi Iwashita, Hiroshi\n  Nakashima", "title": "Accelerating the SpMV kernel on standard CPUs by exploiting the\n  partially diagonal structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse Matrix Vector multiplication (SpMV) is one of basic building blocks in\nscientific computing, and acceleration of SpMV has been continuously required.\nIn this research, we aim for accelerating SpMV on recent CPUs for sparse\nmatrices that have a specific sparsity structure, namely a diagonally\nstructured sparsity pattern. We focus a hybrid storage format that combines the\nDIA and CSR formats, so-called the HDC format. First, we recall the importance\nof introducing cache blocking techniques into HDC-based SpMV kernels. Next,\nbased on the observation of the cache blocked kernel, we present a modified\nversion of the HDC formats, which we call the M-HDC format, in which partial\ndiagonal structures are expected to be more efficiently picked up. For these\nSpMV kernels, we theoretically analyze the expected performance improvement\nbased on performance models. Then, we conduct comprehensive experiments on\nstate-of-the-art multi-core CPUs. By the experiments using typical matrices, we\nclarify the detailed performance characteristics of each SpMV kernel. We also\nevaluate the performance for matrices appearing in practical applications and\ndemonstrate that our approach can accelerate SpMV for some of them. Through the\npresent paper, we demonstrate the effectiveness of exploiting partial diagonal\nstructures by the M-HDC format as a promising approach to accelerating SpMV on\nCPUs for a certain kind of practical sparse matrices.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 11:04:01 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Fukaya", "Takeshi", ""], ["Ishida", "Koki", ""], ["Miura", "Akie", ""], ["Iwashita", "Takeshi", ""], ["Nakashima", "Hiroshi", ""]]}, {"id": "2105.04966", "submitter": "Andrei Tonkikh", "authors": "Petr Kuznetsov, Yvonne-Anne Pignolet, Pavel Ponomarev, Andrei Tonkikh", "title": "Permissionless and Asynchronous Asset Transfer [Technical Report]", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most modern asset transfer systems use consensus to maintain a totally\nordered chain of transactions. It was recently shown that consensus is not\nalways necessary for implementing asset transfer. More efficient, asynchronous\nsolutions can be built using reliable broadcast instead of consensus. This\napproach has been originally used in the closed (permissioned) setting. In this\npaper, we extend it to the open (permissionless) environment. We present\nPastro, a permissionless and asynchronous asset-transfer implementation, in\nwhich quorum systems, traditionally used in reliable broadcast, are replaced\nwith a weighted Proof-of-Stake mechanism. Pastro tolerates a dynamic adversary\nthat is able to adaptively corrupt participants based on the assets owned by\nthem into account.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 12:03:55 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Kuznetsov", "Petr", ""], ["Pignolet", "Yvonne-Anne", ""], ["Ponomarev", "Pavel", ""], ["Tonkikh", "Andrei", ""]]}, {"id": "2105.05001", "submitter": "Zhao Song", "authors": "Baihe Huang, Xiaoxiao Li, Zhao Song, Xin Yang", "title": "FL-NTK: A Neural Tangent Kernel-based Framework for Federated Learning\n  Convergence Analysis", "comments": "ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Federated Learning (FL) is an emerging learning scheme that allows different\ndistributed clients to train deep neural networks together without data\nsharing. Neural networks have become popular due to their unprecedented\nsuccess. To the best of our knowledge, the theoretical guarantees of FL\nconcerning neural networks with explicit forms and multi-step updates are\nunexplored. Nevertheless, training analysis of neural networks in FL is\nnon-trivial for two reasons: first, the objective loss function we are\noptimizing is non-smooth and non-convex, and second, we are even not updating\nin the gradient direction. Existing convergence results for gradient\ndescent-based methods heavily rely on the fact that the gradient direction is\nused for updating. This paper presents a new class of convergence analysis for\nFL, Federated Learning Neural Tangent Kernel (FL-NTK), which corresponds to\noverparamterized ReLU neural networks trained by gradient descent in FL and is\ninspired by the analysis in Neural Tangent Kernel (NTK). Theoretically, FL-NTK\nconverges to a global-optimal solution at a linear rate with properly tuned\nlearning parameters. Furthermore, with proper distributional assumptions,\nFL-NTK can also achieve good generalization.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 13:05:53 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Huang", "Baihe", ""], ["Li", "Xiaoxiao", ""], ["Song", "Zhao", ""], ["Yang", "Xin", ""]]}, {"id": "2105.05071", "submitter": "Michael Feldmann M. Sc.", "authors": "Michael Feldmann, Andreas Padalkin, Christian Scheideler, Shlomi Dolev", "title": "Accelerating Amoebots via Reconfigurable Circuits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an extension to the geometric amoebot model that allows amoebots\nto form so-called \\emph{circuits}. Given a connected amoebot structure, a\ncircuit is a subgraph formed by the amoebots that permits the instant\ntransmission of signals. We show that such an extension allows for\nsignificantly faster solutions to a variety of problems related to programmable\nmatter. More specifically, we provide algorithms for leader election,\nconsensus, compass alignment, chirality agreement and shape recognition. Leader\nelection can be solved in $\\Theta(\\log n)$ rounds, w.h.p., consensus in $O(1)$\nrounds and both, compass alignment and chirality agreement, can be solved in\n$O(\\log n)$ rounds, w.h.p. For shape recognition, the amoebots have to decide\nwhether the amoebot structure forms a particular shape. We show how the\namoebots can detect a parallelogram with linear and polynomial side ratio\nwithin $\\Theta(\\log{n})$ rounds, w.h.p. Finally, we show that the amoebots can\ndetect a shape composed of triangles within $O(1)$ rounds, w.h.p.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 14:25:30 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Feldmann", "Michael", ""], ["Padalkin", "Andreas", ""], ["Scheideler", "Christian", ""], ["Dolev", "Shlomi", ""]]}, {"id": "2105.05079", "submitter": "Tahseen Khan", "authors": "Tahseen Khan, Wenhong Tian, Rajkumar Buyya", "title": "Machine Learning (ML)-Centric Resource Management in Cloud Computing: A\n  Review and Future Directions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing has rapidly emerged as model for delivering Internet-based\nutility computing services. In cloud computing, Infrastructure as a Service\n(IaaS) is one of the most important and rapidly growing fields. Cloud providers\nprovide users/machines resources such as virtual machines, raw (block) storage,\nfirewalls, load balancers, and network devices in this service model. One of\nthe most important aspects of cloud computing for IaaS is resource management.\nScalability, quality of service, optimum utility, reduced overheads, increased\nthroughput, reduced latency, specialised environment, cost effectiveness, and a\nstreamlined interface are some of the advantages of resource management for\nIaaS in cloud computing. Traditionally, resource management has been done\nthrough static policies, which impose certain limitations in various dynamic\nscenarios, prompting cloud service providers to adopt data-driven,\nmachine-learning-based approaches. Machine learning is being used to handle a\nvariety of resource management tasks, including workload estimation, task\nscheduling, VM consolidation, resource optimization, and energy optimization,\namong others. This paper provides a detailed review of challenges in ML-based\nresource management in current research, as well as current approaches to\nresolve these challenges, as well as their advantages and limitations. Finally,\nwe propose potential future research directions based on identified challenges\nand limitations in current research.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 08:03:58 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Khan", "Tahseen", ""], ["Tian", "Wenhong", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "2105.05080", "submitter": "Federica Filippini", "authors": "Federica Filippini, Danilo Ardagna, Marco Lattuada, Edoardo Amaldi,\n  Michele Ciavotta, Maciek Riedl, Katarzyna Materka, Pawe{\\l} Skrzypek,\n  Fabrizio Magugliani, Marco Cicala", "title": "ANDREAS: Artificial intelligence traiNing scheDuler foR accElerAted\n  resource clusterS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Intelligence (AI) and Deep Learning (DL) algorithms are currently\napplied to a wide range of products and solutions. DL training jobs are highly\nresource demanding and they experience great benefits when exploiting AI\naccelerators (e.g., GPUs). However, the effective management of GPU-powered\nclusters comes with great challenges. Among these, efficient scheduling and\nresource allocation solutions are crucial to maximize performance and minimize\nData Centers operational costs. In this paper we propose ANDREAS, an advanced\nscheduling solution that tackles these problems jointly, aiming at optimizing\nDL training runtime workloads and their energy consumption in accelerated\nclusters. Experiments based on simulation demostrate that we can achieve a cost\nreduction between 30 and 62% on average with respect to first-principle methods\nwhile the validation on a real cluster shows a worst case deviation below 13%\nbetween actual and predicted costs, proving the effectiveness of ANDREAS\nsolution in practical scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 14:36:19 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Filippini", "Federica", ""], ["Ardagna", "Danilo", ""], ["Lattuada", "Marco", ""], ["Amaldi", "Edoardo", ""], ["Ciavotta", "Michele", ""], ["Riedl", "Maciek", ""], ["Materka", "Katarzyna", ""], ["Skrzypek", "Pawe\u0142", ""], ["Magugliani", "Fabrizio", ""], ["Cicala", "Marco", ""]]}, {"id": "2105.05085", "submitter": "Heejin Park", "authors": "Heejin Park, Felix Xiaozhu Lin", "title": "TinyStack: A Minimal GPU Stack for Client ML", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  TinyStack is a novel way for deploying GPU-accelerated computation on mobile\nand embedded devices. It addresses the high complexity of a modern GPU stack.\nWithout an overhaul of the stack, TinyStack provides a static, fast path for an\napp to push its computation to GPU. It records GPU executions on the full GPU\nstack ahead of time and replays the executions with only a small replayer on\nnew input at run time. TinyStack addresses challenges in capturing key CPU/GPU\ninteractions and GPU states, working around proprietary GPU internals, and\npreventing replay divergence. The resultant replayer is a drop-in replacement\nof the original GPU stack. It is tiny (as few as 50 KB executable), robust\n(replaying long executions without divergence), portable (running in a POSIX\nOS, in TEE, or on baremetal), and quick to launch (speeding up startup by up to\ntwo orders of magnitude). We have implemented TinyStack and tested it with a\nvariety of ML frameworks, GPU programming APIs, and integrated GPUs.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 07:55:19 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Park", "Heejin", ""], ["Lin", "Felix Xiaozhu", ""]]}, {"id": "2105.05408", "submitter": "Mahsa Eftekhari Hesari", "authors": "David Doty, Mahsa Eftekhari", "title": "A survey of size counting in population protocols", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The population protocol model describes a network of $n$ anonymous agents who\ncannot control with whom they interact. The agents collectively solve some\ncomputational problem through random pairwise interactions, each agent updating\nits own state in response to seeing the state of the other agent. They are\nequivalent to the model of chemical reaction networks, describing abstract\nchemical reactions such as $A+B \\rightarrow C+D$, when the latter is subject to\nthe restriction that all reactions have two reactants and two products, and all\nrate constants are 1. The counting problem is that of designing a protocol so\nthat $n$ agents, all starting in the same state, eventually converge to states\nwhere each agent encodes in its state an exact or approximate description of\npopulation size $n$. In this survey paper, we describe recent algorithmic\nadvances on the counting problem.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 02:57:39 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Doty", "David", ""], ["Eftekhari", "Mahsa", ""]]}, {"id": "2105.05574", "submitter": "Yannic Maus", "authors": "Alkida Balliu, Keren Censor-Hillel, Yannic Maus, Dennis Olivetti,\n  Jukka Suomela", "title": "Locally Checkable Labelings with Small Messages", "comments": "fixed typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A rich line of work has been addressing the computational complexity of\nlocally checkable labelings (LCLs), illustrating the landscape of possible\ncomplexities. In this paper, we study the landscape of LCL complexities under\nbandwidth restrictions. Our main results are twofold. First, we show that on\ntrees, the CONGEST complexity of an LCL problem is asymptotically equal to its\ncomplexity in the LOCAL model. An analog statement for general (non-LCL)\nproblems is known to be false. Second, we show that for general graphs this\nequivalence does not hold, by providing an LCL problem for which we show that\nit can be solved in $O(\\log n)$ rounds in the LOCAL model, but requires\n$\\tilde{\\Omega}(n^{1/2})$ rounds in the CONGEST model.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 10:50:43 GMT"}, {"version": "v2", "created": "Sun, 16 May 2021 09:47:30 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Balliu", "Alkida", ""], ["Censor-Hillel", "Keren", ""], ["Maus", "Yannic", ""], ["Olivetti", "Dennis", ""], ["Suomela", "Jukka", ""]]}, {"id": "2105.05575", "submitter": "Yannic Maus", "authors": "Yannic Maus", "title": "Distributed Graph Coloring Made Easy", "comments": "SPAA 2021 v2: removed typo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a deterministic CONGEST algorithm to compute an\n$O(k\\Delta)$-vertex coloring in $O(\\Delta/k)+\\log^* n$ rounds, where $\\Delta$\nis the maximum degree of the network graph and $1\\leq k\\leq O(\\Delta)$ can be\nfreely chosen. The algorithm is extremely simple: Each node locally computes a\nsequence of colors and then it \"tries colors\" from the sequence in batches of\nsize $k$. Our algorithm subsumes many important results in the history of\ndistributed graph coloring as special cases, including Linial's color reduction\n[Linial, FOCS'87], the celebrated locally iterative algorithm from [Barenboim,\nElkin, Goldenberg, PODC'18], and various algorithms to compute defective and\narbdefective colorings. Our algorithm can smoothly scale between these and also\nsimplifies the state of the art $(\\Delta+1)$-coloring algorithm. At the cost of\nlosing the full algorithm's simplicity we also provide a $O(k\\Delta)$-coloring\nalgorithm in $O(\\sqrt{\\Delta/k})+\\log^* n$ rounds. We also provide improved\ndeterministic algorithms for ruling sets, and, additionally, we provide a tight\ncharacterization for one-round color reduction algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 10:50:56 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 09:18:09 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Maus", "Yannic", ""]]}, {"id": "2105.05720", "submitter": "Abhinav Jangda", "authors": "Abhinav Jangda, Jun Huang, Guodong Liu, Amir Hossein Nodehi Sabet,\n  Saeed Maleki, Youshan Miao, Madanlal Musuvathi, Todd Mytkowicz, Olli Sarikivi", "title": "CoCoNet: Co-Optimizing Computation and Communication for Distributed\n  Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern deep learning workloads run on distributed hardware and are difficult\nto optimize -- data, model, and pipeline parallelism require a developer to\nthoughtfully restructure their workload around optimized computation and\ncommunication kernels in libraries such as cuBLAS and NCCL. The logical\nseparation between computation and communication leaves performance on the\ntable with missed optimization opportunities across abstraction boundaries. To\nexplore these opportunities, this paper presents CoCoNet, which consists of a\ncompute language to express programs with both computation and communication, a\nscheduling language to apply transformations on such programs, and a compiler\nto generate high performance kernels. Providing both computation and\ncommunication as first class constructs enables new optimizations, such as\noverlapping or fusion of communication with computation. CoCoNet allowed us to\noptimize several data, model and pipeline parallel workloads in existing deep\nlearning systems with very few lines of code. We show significant improvements\nafter integrating novel CoCoNet generated kernels.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 15:13:43 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 01:04:11 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Jangda", "Abhinav", ""], ["Huang", "Jun", ""], ["Liu", "Guodong", ""], ["Sabet", "Amir Hossein Nodehi", ""], ["Maleki", "Saeed", ""], ["Miao", "Youshan", ""], ["Musuvathi", "Madanlal", ""], ["Mytkowicz", "Todd", ""], ["Sarikivi", "Olli", ""]]}, {"id": "2105.05734", "submitter": "Julian Matschinske", "authors": "Julian Matschinske, Julian Sp\\\"ath, Reza Nasirigerdeh, Reihaneh\n  Torkzadehmahani, Anne Hartebrodt, Bal\\'azs Orb\\'an, S\\'andor Fej\\'er, Olga\n  Zolotareva, Mohammad Bakhtiari, B\\'ela Bihari, Marcus Bloice, Nina C Donner,\n  Walid Fdhila, Tobias Frisch, Anne-Christin Hauschild, Dominik Heider, Andreas\n  Holzinger, Walter H\\\"otzendorfer, Jan Hospes, Tim Kacprowski, Markus\n  Kastelitz, Markus List, Rudolf Mayer, M\\'onika Moga, Heimo M\\\"uller,\n  Anastasia Pustozerova, Richard R\\\"ottger, Anna Saranti, Harald HHW Schmidt,\n  Christof Tschohl, Nina K Wenke, Jan Baumbach", "title": "The FeatureCloud AI Store for Federated Learning in Biomedicine and\n  Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine Learning (ML) and Artificial Intelligence (AI) have shown promising\nresults in many areas and are driven by the increasing amount of available\ndata. However, this data is often distributed across different institutions and\ncannot be shared due to privacy concerns. Privacy-preserving methods, such as\nFederated Learning (FL), allow for training ML models without sharing sensitive\ndata, but their implementation is time-consuming and requires advanced\nprogramming skills. Here, we present the FeatureCloud AI Store for FL as an\nall-in-one platform for biomedical research and other applications. It removes\nlarge parts of this complexity for developers and end-users by providing an\nextensible AI Store with a collection of ready-to-use apps. We show that the\nfederated apps produce similar results to centralized ML, scale well for a\ntypical number of collaborators and can be combined with Secure Multiparty\nComputation (SMPC), thereby making FL algorithms safely and easily applicable\nin biomedical and clinical environments.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 15:31:46 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Matschinske", "Julian", ""], ["Sp\u00e4th", "Julian", ""], ["Nasirigerdeh", "Reza", ""], ["Torkzadehmahani", "Reihaneh", ""], ["Hartebrodt", "Anne", ""], ["Orb\u00e1n", "Bal\u00e1zs", ""], ["Fej\u00e9r", "S\u00e1ndor", ""], ["Zolotareva", "Olga", ""], ["Bakhtiari", "Mohammad", ""], ["Bihari", "B\u00e9la", ""], ["Bloice", "Marcus", ""], ["Donner", "Nina C", ""], ["Fdhila", "Walid", ""], ["Frisch", "Tobias", ""], ["Hauschild", "Anne-Christin", ""], ["Heider", "Dominik", ""], ["Holzinger", "Andreas", ""], ["H\u00f6tzendorfer", "Walter", ""], ["Hospes", "Jan", ""], ["Kacprowski", "Tim", ""], ["Kastelitz", "Markus", ""], ["List", "Markus", ""], ["Mayer", "Rudolf", ""], ["Moga", "M\u00f3nika", ""], ["M\u00fcller", "Heimo", ""], ["Pustozerova", "Anastasia", ""], ["R\u00f6ttger", "Richard", ""], ["Saranti", "Anna", ""], ["Schmidt", "Harald HHW", ""], ["Tschohl", "Christof", ""], ["Wenke", "Nina K", ""], ["Baumbach", "Jan", ""]]}, {"id": "2105.06050", "submitter": "Fernando Koch", "authors": "Wesley dos Reis Bezerra and Fernando Luiz Koch and Carlos Becker\n  Westphall", "title": "Models of Computing as a Service and IoT: an analysis of the current\n  scenario with applications using LPWAN", "comments": null, "journal-ref": "Revista de Sistemas de Informacao da FSMA, 25:56-65, 2020", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work provides the basis to understand and select Cloud Computing models\napplied for the development of IoT solutions using Low-Power Wide Area Network\n(LPWAN). Cloud Computing paradigm has transformed how the industry implement\nsolution, through the commoditization of shared IT infrastructures. The advent\nof massive Internet of Things (IoT) and related workloads brings new challenges\nto this scenario demanding malleable configurations where the resources are\ndistributed closer to data sources. We introduce an analysis of existing\nsolution architectures, along with an illustrative case from where we derive\nthe lessons, challenges, and opportunities of combining these technologies for\na new generation of Cloud-native solutions.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 02:42:44 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Bezerra", "Wesley dos Reis", ""], ["Koch", "Fernando Luiz", ""], ["Westphall", "Carlos Becker", ""]]}, {"id": "2105.06068", "submitter": "Volodymyr Polosukhin", "authors": "Keren Censor-Hillel, Dean Leitersdorf, Volodymyr Polosukhin", "title": "On Sparsity Awareness in Distributed Computations", "comments": "To appear in SPAA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extract a core principle underlying seemingly different fundamental\ndistributed settings, showing sparsity awareness may induce faster algorithms\nfor problems in these settings. To leverage this, we establish a new framework\nby developing an intermediate auxiliary model weak enough to be simulated in\nthe CONGEST model given low mixing time, as well as in the recently introduced\nHYBRID model. We prove that despite imposing harsh restrictions, this\nartificial model allows balancing massive data transfers with high bandwidth\nutilization. We exemplify the power of our methods, by deriving shortest-paths\nalgorithms improving upon the state-of-the-art.\n  Specifically, we show the following for graphs of $n$ nodes:\n  A $(3+\\epsilon)$ approximation for weighted APSP in\n$(n/\\delta)\\tau_{mix}\\cdot 2^{O(\\sqrt\\log n)}$ rounds in the CONGEST model,\nwhere $\\delta$ is the minimum degree of the graph and $\\tau_{mix}$ is its\nmixing time. For graphs with $\\delta=\\tau_{mix}\\cdot 2^{\\omega(\\sqrt\\log n)}$,\nthis takes $o(n)$ rounds, despite the $\\Omega(n)$ lower bound for general\ngraphs [Nanongkai, STOC'14].\n  An $(n^{7/6}/m^{1/2}+n^2/m)\\cdot\\tau_{mix}\\cdot 2^{O(\\sqrt\\log n)}$-round\nexact SSSP algorithm in the CONGNEST model, for graphs with $m$ edges and a\nmixing time of $\\tau_{mix}$. This improves upon the algorithm of [Chechik and\nMukhtar, PODC'20] for significant ranges of values of $m$ and $ \\tau_{mix}$.\n  A CONGESTED CLIQUE simulation in the CONGEST model improving upon the\nstate-of-the-art simulation of [Ghaffari, Kuhn, and SU, PODC'17] by a factor\nproportional to the average degree in the graph.\n  An $\\tilde O(n^{5/17}/\\epsilon^9)$-round algorithm for a $(1+\\epsilon)$\napproximation for SSSP in the HYBRID model. The only previous $o(n^{1/3})$\nround algorithm for distance approximations in this model is for a much larger\nfactor [Augustine, Hinnenthal, Kuhn, Scheideler, Schneider, SODA'20].\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 04:04:49 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Censor-Hillel", "Keren", ""], ["Leitersdorf", "Dean", ""], ["Polosukhin", "Volodymyr", ""]]}, {"id": "2105.06075", "submitter": "Joachim Neu", "authors": "Joachim Neu, Ertem Nusret Tas, David Tse", "title": "The Availability-Accountability Dilemma and its Resolution via\n  Accountability Gadgets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Byzantine fault tolerant (BFT) consensus protocols are traditionally\ndeveloped to support reliable distributed computing. For applications where the\nprotocol participants are economic agents, recent works highlighted the\nimportance of accountability: the ability to identify participants who provably\nviolate the protocol. We propose to evaluate the security of an accountable\nprotocol in terms of its liveness resilience, the minimum number of Byzantine\nnodes when liveness is violated, and its accountable safety resilience, the\nminimum number of accountable Byzantine nodes when safety is violated. We\ncharacterize the optimal tradeoffs between these two resiliences in different\nnetwork environments, and identify an availability-accountability dilemma: in\nan environment with dynamic participation, no protocol can simultaneously be\naccountably-safe and live. We provide a resolution to this dilemma by\nconstructing an optimally-resilient accountability gadget to checkpoint a\nlongest chain protocol, such that the full ledger is live under dynamic\nparticipation and the checkpointed prefix ledger is accountable. Our\naccountability gadget construction is black-box and can use any BFT protocol\nwhich is accountable under static participation. Using HotStuff as the black\nbox, we implemented our construction as a protocol for the Ethereum 2.0 beacon\nchain, and our Internet-scale experiments with more than 4000 nodes show that\nthe protocol can achieve the required scalability and has better latency than\nthe current solution Gasper, while having the advantage of being provably\nsecure. To contrast, we demonstrate a new attack on Gasper.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 04:45:04 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Neu", "Joachim", ""], ["Tas", "Ertem Nusret", ""], ["Tse", "David", ""]]}, {"id": "2105.06145", "submitter": "Xiaojun Dong", "authors": "Xiaojun Dong, Yan Gu, Yihan Sun, Yunming Zhang", "title": "Efficient Stepping Algorithms and Implementations for Parallel Shortest\n  Paths", "comments": null, "journal-ref": null, "doi": "10.1145/3350755.3400227", "report-no": null, "categories": "cs.DS cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we study the single-source shortest-path (SSSP) problem with\npositive edge weights, which is a notoriously hard problem in the parallel\ncontext. In practice, the $\\Delta$-stepping algorithm proposed by Meyer and\nSanders has been widely adopted. However, $\\Delta$-stepping has no known\nworst-case bounds for general graphs. The performance of $\\Delta$-stepping also\nhighly relies on the parameter $\\Delta$. There have also been lots of\nalgorithms with theoretical bounds, such as Radius-stepping, but they either\nhave no implementations available or are much slower than $\\Delta$-stepping in\npractice.\n  We propose a stepping algorithm framework that generalizes existing\nalgorithms such as $\\Delta$-stepping and Radius-stepping. The framework allows\nfor similar analysis and implementations of all stepping algorithms. We also\npropose a new ADT, lazy-batched priority queue (LaB-PQ), that abstracts the\nsemantics of the priority queue needed by the stepping algorithms. We provide\ntwo data structures for LaB-PQ, focusing on theoretical and practical\nefficiency, respectively. Based on the new framework and LaB-PQ, we show two\nnew stepping algorithms, $\\rho$-stepping and $\\Delta^*$-stepping, that are\nsimple, with non-trivial worst-case bounds, and fast in practice.\n  The stepping algorithm framework also provides almost identical\nimplementations for three algorithms: Bellman-Ford, $\\Delta^*$-stepping, and\n$\\rho$-stepping. We compare our code with four state-of-the-art\nimplementations. On five social and web graphs, $\\rho$-stepping is 1.3--2.5x\nfaster than all the existing implementations. On two road graphs, our\n$\\Delta^*$-stepping is at least 14\\% faster than existing implementations,\nwhile $\\rho$-stepping is also competitive. The almost identical implementations\nfor stepping algorithms also allow for in-depth analyses and comparisons among\nthe stepping algorithms in practice.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 08:50:39 GMT"}, {"version": "v2", "created": "Sun, 20 Jun 2021 22:52:59 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Dong", "Xiaojun", ""], ["Gu", "Yan", ""], ["Sun", "Yihan", ""], ["Zhang", "Yunming", ""]]}, {"id": "2105.06176", "submitter": "Manasi Tiwari", "authors": "Manasi Tiwari, Sathish Vadhiyar", "title": "Efficient executions of Pipelined Conjugate Gradient Method on\n  Heterogeneous Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Preconditioned Conjugate Gradient (PCG) method is widely used for solving\nlinear systems of equations with sparse matrices. A recent version of PCG,\nPipelined PCG, eliminates the dependencies in the computations of the PCG\nalgorithm so that the non-dependent computations can be overlapped with\ncommunication. In this paper, we propose three methods for efficient execution\nof the Pipelined PCG algorithm on GPU accelerated heterogeneous architectures.\n  The first two methods achieve task-parallelism using asynchronous executions\nof different tasks on CPU cores and GPU. The third method achieves data\nparallelism by decomposing the workload between CPU and GPU based on a\nperformance model. The performance model takes into account the relative\nperformance of CPU cores and GPU using some initial executions and performs 2D\ndata decomposition. We also implement optimization strategies like kernel\nfusion for GPU and merging vector operations for CPU. Our methods give up to 8x\nspeedup and on average 3x speedup over PCG CPU implementation of Paralution and\nPETSc libraries. They also give up to 5x speedup and on average 1.45x speedup\nover PCG GPU implementation of Paralution and PETSc libraries. The third method\nalso provides an efficient solution for solving problems that cannot be fit\ninto the GPU memory and gives up to 2.5x speedup for such problems.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 10:07:25 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Tiwari", "Manasi", ""], ["Vadhiyar", "Sathish", ""]]}, {"id": "2105.06256", "submitter": "Kang Wei", "authors": "Chuan Ma, Jun Li, Ming Ding, Kang Wei, Wen Chen and H. Vincent Poor", "title": "Federated Learning with Unreliable Clients: Performance Analysis and\n  Mechanism Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Owing to the low communication costs and privacy-promoting capabilities,\nFederated Learning (FL) has become a promising tool for training effective\nmachine learning models among distributed clients. However, with the\ndistributed architecture, low quality models could be uploaded to the\naggregator server by unreliable clients, leading to a degradation or even a\ncollapse of training. In this paper, we model these unreliable behaviors of\nclients and propose a defensive mechanism to mitigate such a security risk.\nSpecifically, we first investigate the impact on the models caused by\nunreliable clients by deriving a convergence upper bound on the loss function\nbased on the gradient descent updates. Our theoretical bounds reveal that with\na fixed amount of total computational resources, there exists an optimal number\nof local training iterations in terms of convergence performance. We further\ndesign a novel defensive mechanism, named deep neural network based secure\naggregation (DeepSA). Our experimental results validate our theoretical\nanalysis. In addition, the effectiveness of DeepSA is verified by comparing\nwith other state-of-the-art defensive mechanisms.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 08:02:27 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Ma", "Chuan", ""], ["Li", "Jun", ""], ["Ding", "Ming", ""], ["Wei", "Kang", ""], ["Chen", "Wen", ""], ["Poor", "H. Vincent", ""]]}, {"id": "2105.06322", "submitter": "Maurice Herlihy", "authors": "Yingjie Xue and Maurice Herlihy", "title": "Hedging Against Sore Loser Attacks in Cross-Chain Transactions", "comments": "To apper in PODC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A *sore loser attack* in cross-blockchain commerce rises when one party\ndecides to halt participation partway through, leaving other parties' assets\nlocked up for a long duration. Although vulnerability to sore loser attacks\ncannot be entirely eliminated, it can be reduced to an arbitrarily low level.\nThis paper proposes new distributed protocols for hedging a range of\ncross-chain transactions in a synchronous communication model, such as\ntwo-party swaps, $n$-party swaps, brokered transactions, and auctions.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 14:22:04 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 17:21:23 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Xue", "Yingjie", ""], ["Herlihy", "Maurice", ""]]}, {"id": "2105.06413", "submitter": "Sarthak Pati", "authors": "G Anthony Reina, Alexey Gruzdev, Patrick Foley, Olga Perepelkina,\n  Mansi Sharma, Igor Davidyuk, Ilya Trushkin, Maksim Radionov, Aleksandr\n  Mokrov, Dmitry Agapov, Jason Martin, Brandon Edwards, Micah J. Sheller,\n  Sarthak Pati, Prakash Narayana Moorthy, Shih-han Wang, Prashant Shah,\n  Spyridon Bakas", "title": "OpenFL: An open-source framework for Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) is a computational paradigm that enables\norganizations to collaborate on machine learning (ML) projects without sharing\nsensitive data, such as, patient records, financial data, or classified\nsecrets. Open Federated Learning (OpenFL https://github.com/intel/openfl) is an\nopen-source framework for training ML algorithms using the data-private\ncollaborative learning paradigm of FL. OpenFL works with training pipelines\nbuilt with both TensorFlow and PyTorch, and can be easily extended to other ML\nand deep learning frameworks. Here, we summarize the motivation and development\ncharacteristics of OpenFL, with the intention of facilitating its application\nto existing ML model training in a production environment. Finally, we describe\nthe first use of the OpenFL framework to train consensus ML models in a\nconsortium of international healthcare organizations, as well as how it\nfacilitates the first computational competition on FL.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 16:40:19 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Reina", "G Anthony", ""], ["Gruzdev", "Alexey", ""], ["Foley", "Patrick", ""], ["Perepelkina", "Olga", ""], ["Sharma", "Mansi", ""], ["Davidyuk", "Igor", ""], ["Trushkin", "Ilya", ""], ["Radionov", "Maksim", ""], ["Mokrov", "Aleksandr", ""], ["Agapov", "Dmitry", ""], ["Martin", "Jason", ""], ["Edwards", "Brandon", ""], ["Sheller", "Micah J.", ""], ["Pati", "Sarthak", ""], ["Moorthy", "Prakash Narayana", ""], ["Wang", "Shih-han", ""], ["Shah", "Prashant", ""], ["Bakas", "Spyridon", ""]]}, {"id": "2105.06524", "submitter": "Hongpeng Guo", "authors": "Hongpeng Guo, Shuochao Yao, Zhe Yang, Qian Zhou, Klara Nahrstedt", "title": "CrossRoI: Cross-camera Region of Interest Optimization for Efficient\n  Real Time Video Analytics at Scale", "comments": "accepted in 12th ACM Multimedia Systems Conference (MMsys 21')", "journal-ref": null, "doi": "10.1145/3458305.3463381", "report-no": null, "categories": "cs.DC cs.CV cs.MM cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Video cameras are pervasively deployed in city scale for public good or\ncommunity safety (i.e. traffic monitoring or suspected person tracking).\nHowever, analyzing large scale video feeds in real time is data intensive and\nposes severe challenges to network and computation systems today. We present\nCrossRoI, a resource-efficient system that enables real time video analytics at\nscale via harnessing the videos content associations and redundancy across a\nfleet of cameras. CrossRoI exploits the intrinsic physical correlations of\ncross-camera viewing fields to drastically reduce the communication and\ncomputation costs. CrossRoI removes the repentant appearances of same objects\nin multiple cameras without harming comprehensive coverage of the scene.\nCrossRoI operates in two phases - an offline phase to establish cross-camera\ncorrelations, and an efficient online phase for real time video inference.\nExperiments on real-world video feeds show that CrossRoI achieves 42% - 65%\nreduction for network overhead and 25% - 34% reduction for response delay in\nreal time video analytics applications with more than 99% query accuracy, when\ncompared to baseline methods. If integrated with SotA frame filtering systems,\nthe performance gains of CrossRoI reach 50% - 80% (network overhead) and 33% -\n61% (end-to-end delay).\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 19:29:14 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Guo", "Hongpeng", ""], ["Yao", "Shuochao", ""], ["Yang", "Zhe", ""], ["Zhou", "Qian", ""], ["Nahrstedt", "Klara", ""]]}, {"id": "2105.06571", "submitter": "Michael Salim", "authors": "Michael Salim, Thomas Uram, J. Taylor Childers, Venkat Vishwanath,\n  Michael E. Papka", "title": "Toward Real-time Analysis of Experimental Science Workloads on\n  Geographically Distributed Supercomputers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Massive upgrades to science infrastructure are driving data velocities\nupwards while stimulating adoption of increasingly data-intensive analytics.\nWhile next-generation exascale supercomputers promise strong support for\nI/O-intensive workflows, HPC remains largely untapped by live experiments,\nbecause data transfers and disparate batch-queueing policies are prohibitive\nwhen faced with scarce instrument time. To bridge this divide, we introduce\nBalsam: a distributed orchestration platform enabling workflows at the edge to\nsecurely and efficiently trigger analytics tasks across a user-managed\nfederation of HPC execution sites. We describe the architecture of the Balsam\nservice, which provides a workflow management API, and distributed sites that\nprovision resources and schedule scalable, fault-tolerant execution. We\ndemonstrate Balsam in efficiently scaling real-time analytics from two DOE\nlight sources simultaneously onto three supercomputers (Theta, Summit, and\nCori), while maintaining low overheads for on-demand computing, and providing a\nPython library for seamless integration with existing ecosystems of data\nanalysis tools.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 22:24:20 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 19:09:09 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Salim", "Michael", ""], ["Uram", "Thomas", ""], ["Childers", "J. Taylor", ""], ["Vishwanath", "Venkat", ""], ["Papka", "Michael E.", ""]]}, {"id": "2105.06614", "submitter": "Jennifer Welch", "authors": "Hagit Attiya, Constantin Enea, Jennifer Welch", "title": "Impossibility of Strongly-Linearizable Message-Passing Objects via\n  Simulation by Single-Writer Registers", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key way to construct complex distributed systems is through modular\ncomposition of linearizable concurrent objects. A prominent example is shared\nregisters, which have crash-tolerant implementations on top of message-passing\nsystems, allowing the advantages of shared memory to carry over to\nmessage-passing. Yet linearizable registers do not always behave properly when\nused inside randomized programs. A strengthening of linearizability, called\nstrong linearizability, has been shown to preserve probabilistic behavior, as\nwell as other hypersafety properties. In order to exploit composition and\nabstraction in message-passing systems, it is crucial to know whether there\nexist strongly-linearizable implementations of registers in message-passing.\nThis paper answers the question in the negative: there are no\nstrongly-linearizable fault-tolerant message-passing implementations of\nmulti-writer registers, max-registers, snapshots or counters. This result is\nproved by reduction from the corresponding result by Helmi et al. The reduction\nis a novel extension of the BG simulation that connects shared-memory and\nmessage-passing, supports long-lived objects, and preserves strong\nlinearizability. The main technical challenge arises from the discrepancy\nbetween the potentially minuscule fraction of failures to be tolerated in the\nsimulated message-passing algorithm and the large fraction of failures that can\nafflict the simulating shared-memory system. The reduction is general and can\nbe viewed as the reverse of the ABD simulation of shared memory in\nmessage-passing.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 02:07:52 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Attiya", "Hagit", ""], ["Enea", "Constantin", ""], ["Welch", "Jennifer", ""]]}, {"id": "2105.06676", "submitter": "Rathish Das", "authors": "Zafar Ahmad, Rezaul Chowdhury, Rathish Das, Pramod Ganapathi, Aaron\n  Gregory, Yimin Zhu", "title": "Fast Stencil Computations using Fast Fourier Transforms", "comments": "This paper will appear in the proceedings of SPAA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stencil computations are widely used to simulate the change of state of\nphysical systems across a multidimensional grid over multiple timesteps. The\nstate-of-the-art techniques in this area fall into three groups: cache-aware\ntiled looping algorithms, cache-oblivious divide-and-conquer trapezoidal\nalgorithms, and Krylov subspace methods.\n  In this paper, we present two efficient parallel algorithms for performing\nlinear stencil computations. Current direct solvers in this domain are\ncomputationally inefficient, and Krylov methods require manual labor and\nmathematical training. We solve these problems for linear stencils by using DFT\npreconditioning on a Krylov method to achieve a direct solver which is both\nfast and general. Indeed, while all currently available algorithms for solving\ngeneral linear stencils perform $\\Theta(NT)$ work, where $N$ is the size of the\nspatial grid and $T$ is the number of timesteps, our algorithms perform $o(NT)$\nwork.\n  To the best of our knowledge, we give the first algorithms that use fast\nFourier transforms to compute final grid data by evolving the initial data for\nmany timesteps at once. Our algorithms handle both periodic and aperiodic\nboundary conditions, and achieve polynomially better performance bounds (i.e.,\ncomputational complexity and parallel runtime) than all other existing\nsolutions.\n  Initial experimental results show that implementations of our algorithms that\nevolve grids of roughly $10^7$ cells for around $10^5$ timesteps run orders of\nmagnitude faster than state-of-the-art implementations for periodic stencil\nproblems, and 1.3$\\times$ to 8.5$\\times$ faster for aperiodic stencil problems.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 07:24:36 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Ahmad", "Zafar", ""], ["Chowdhury", "Rezaul", ""], ["Das", "Rathish", ""], ["Ganapathi", "Pramod", ""], ["Gregory", "Aaron", ""], ["Zhu", "Yimin", ""]]}, {"id": "2105.06697", "submitter": "Jiaqi Zhang", "authors": "Jiaqi Zhang, Keyou You, Lihua Xie", "title": "Innovation Compression for Communication-efficient Distributed\n  Optimization with Linear Convergence", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG cs.SY eess.SP eess.SY", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Information compression is essential to reduce communication cost in\ndistributed optimization over peer-to-peer networks. This paper proposes a\ncommunication-efficient linearly convergent distributed (COLD) algorithm to\nsolve strongly convex optimization problems. By compressing innovation vectors,\nwhich are the differences between decision vectors and their estimates, COLD is\nable to achieve linear convergence for a class of $\\delta$-contracted\ncompressors. We explicitly quantify how the compression affects the convergence\nrate and show that COLD matches the same rate of its uncompressed version. To\naccommodate a wider class of compressors that includes the binary quantizer, we\nfurther design a novel dynamical scaling mechanism and obtain the linearly\nconvergent Dyna-COLD. Importantly, our results strictly improve existing\nresults for the quantized consensus problem. Numerical experiments demonstrate\nthe advantages of both algorithms under different compressors.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 08:15:18 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Zhang", "Jiaqi", ""], ["You", "Keyou", ""], ["Xie", "Lihua", ""]]}, {"id": "2105.06712", "submitter": "Daniel Anderson", "authors": "Daniel Anderson, Guy E. Blelloch, Anubhav Baweja, Umut A. Acar", "title": "Efficient Parallel Self-Adjusting Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-adjusting computation is an approach for automatically producing dynamic\nalgorithms from static ones. The approach works by tracking control and data\ndependencies, and propagating changes through the dependencies when making an\nupdate. Extensively studied in the sequential setting, some results on parallel\nself-adjusting computation exist, but are either only applicable to limited\nclasses of computations, such as map-reduce, or are ad-hoc systems with no\ntheoretical analysis of their performance.\n  In this paper, we present the first system for parallel self-adjusting\ncomputation that applies to a wide class of nested parallel algorithms and\nprovides theoretical bounds on the work and span of the resulting dynamic\nalgorithms. As with bounds in the sequential setting, our bounds relate a\n\"distance\" measure between computations on different inputs to the cost of\npropagating an update. However, here we also consider parallelism in the\npropagation cost. The main innovation in the paper is in using Series-Parallel\ntrees (SP trees) to track sequential and parallel control dependencies to allow\npropagation of changes to be applied safely in parallel. We show both\ntheoretically and through experiments that our system allows algorithms to\nproduce updated results over large datasets significantly faster than\nfrom-scratch execution. We demonstrate our system with several example\napplications, including algorithms for dynamic sequences and dynamic trees. In\nall cases studied, we show that parallel self-adjusting computation can provide\na significant benefit in both work savings and parallel time.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 08:48:57 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Anderson", "Daniel", ""], ["Blelloch", "Guy E.", ""], ["Baweja", "Anubhav", ""], ["Acar", "Umut A.", ""]]}, {"id": "2105.06737", "submitter": "Gal Sela", "authors": "Gal Sela, Maurice Herlihy, Erez Petrank", "title": "Linearizability: A Typo", "comments": null, "journal-ref": "Proceedings of the 2021 ACM Symposium on Principles of Distributed\n  Computing (PODC '21), July 26-30, 2021, Virtual Event, Italy. ACM, New York,\n  NY, USA, p. 561-564", "doi": "10.1145/3465084.3467944", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linearizability is the de facto consistency condition for concurrent objects,\nwidely used in theory and practice. Loosely speaking, linearizability\nclassifies concurrent executions as correct if operations on shared objects\nappear to take effect instantaneously during the operation execution time. This\npaper calls attention to a somewhat-neglected aspect of linearizability:\nrestrictions on how pending invocations are handled, an issue that has become\nincreasingly important for software running on systems with non-volatile main\nmemory. Interestingly, the original published definition of linearizability\nincludes a typo (a symbol is missing a prime) that concerns exactly this issue.\nIn this paper we point out the typo and provide an amendment to make the\ndefinition complete. We believe that pointing this typo out rigorously and\nproposing a fix is important and timely.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 09:56:00 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 17:33:44 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Sela", "Gal", ""], ["Herlihy", "Maurice", ""], ["Petrank", "Erez", ""]]}, {"id": "2105.06961", "submitter": "David Dice", "authors": "Dave Dice and Alex Kogan", "title": "Ready When You Are: Efficient Condition Variables via Delegated\n  Condition Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Multi-thread applications commonly utilize condition variables for\ncommunication between threads. Condition variables allow threads to block and\nwait until a certain condition holds, and also enable threads to wake up their\nblocked peers notifying them about a change to the state of shared data. Quite\noften such notifications are delivered to all threads, while only a small\nnumber of specific threads is interested in it. This results in so-called\nfutile wakeups, where threads receiving the notification wake up and resume\ntheir execution only to realize that the condition they are waiting for does\nnot hold and they need to wait again. Those wakeups cause numerous context\nswitches, increase lock contention and cache pressure, translating into lots of\nwasted computing cycles and energy.\n  In this work, we propose to delegate conditions on which threads are waiting\nto the thread sending notifications. This enables the latter to evaluate the\nconditions and send the notification(s) only to the relevant thread(s),\npractically eliminating futile wakeups altogether. Our initial evaluation of\nthis idea shows promising results, achieving 3-4x throughput improvement over\nlegacy condition variables.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 17:02:47 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Dice", "Dave", ""], ["Kogan", "Alex", ""]]}, {"id": "2105.07028", "submitter": "Michael Crusoe", "authors": "Michael R. Crusoe, Sanne Abeln, Alexandru Iosup, Peter Amstutz, John\n  Chilton, Neboj\\v{s}a Tijani\\'c, Herv\\'e M\\'enager, Stian Soiland-Reyes,\n  Carole Goble (for the CWL Community)", "title": "Methods Included: Standardizing Computational Reuse and Portability with\n  the Common Workflow Language", "comments": "8 pages, 3 figures. For the LaTex source code of this paper, see\n  https://github.com/mr-c/cwl_methods_included", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A widely used standard for portable multilingual data analysis pipelines\nwould enable considerable benefits to scholarly publication reuse,\nresearch/industry collaboration, regulatory cost control, and to the\nenvironment. Published research that used multiple computer languages for their\nanalysis pipelines would include a complete and reusable description of that\nanalysis that is runnable on a diverse set of computing environments.\nResearchers would be able to easier collaborate and reuse these pipelines,\nadding or exchanging components regardless of programming language used;\ncollaborations with and within the industry would be easier; approval of new\nmedical interventions that rely on such pipelines would be faster. Time will be\nsaved and environmental impact would also be reduced, as these descriptions\ncontain enough information for advanced optimization without user intervention.\nWorkflows are widely used in data analysis pipelines, enabling innovation and\ndecision-making for the modern society. In many domains the analysis components\nare numerous and written in multiple different computer languages by third\nparties. However, lacking a standard for reusable and portable multilingual\nworkflows, then reusing published multilingual workflows, collaborating on open\nproblems, and optimizing their execution would be severely hampered. Moreover,\nonly a standard for multilingual data analysis pipelines that was widely used\nwould enable considerable benefits to research-industry collaboration,\nregulatory cost control, and to preserving the environment. Prior to the start\nof the CWL project, there was no standard for describing multilingual analysis\npipelines in a portable and reusable manner. Even today / currently, although\nthere exist hundreds of single-vendor and other single-source systems that run\nworkflows, none is a general, community-driven, and consensus-built standard.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 18:44:48 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Crusoe", "Michael R.", "", "for the CWL Community"], ["Abeln", "Sanne", "", "for the CWL Community"], ["Iosup", "Alexandru", "", "for the CWL Community"], ["Amstutz", "Peter", "", "for the CWL Community"], ["Chilton", "John", "", "for the CWL Community"], ["Tijani\u0107", "Neboj\u0161a", "", "for the CWL Community"], ["M\u00e9nager", "Herv\u00e9", "", "for the CWL Community"], ["Soiland-Reyes", "Stian", "", "for the CWL Community"], ["Goble", "Carole", "", "for the CWL Community"]]}, {"id": "2105.07123", "submitter": "Costas Busch", "authors": "Costas Busch and Dariusz R. Kowalski", "title": "Byzantine-Resilient Population Protocols", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Population protocols model information spreading in networks where pairwise\nnode exchanges are determined by an external random scheduler. Most of the\npopulation protocols in the literature assume that the participating $n$ nodes\nare honest. Such assumption may not be, however, accurate for large-scale\nsystems of small devices. Hence, in this work, we study a variant of population\nprotocols, where up to $f$ nodes can be Byzantine. We examine the majority\n(binary) consensus problem against different levels of adversary strengths,\nranging from the Full adversary that has complete knowledge of all the node\nstates to the Weak adversary that has only knowledge about which exchanges take\nplace. We also take into account Dynamic vs Static node corruption by the\nadversary. We give lower bounds that require any algorithm solving the majority\nconsensus to have initial difference $d = \\Omega(f + 1)$ for the tally between\nthe two proposed values, which holds for both the Full Static and Weak Dynamic\nadversaries. We then present an algorithm that solves the majority consensus\nproblem and tolerates $f \\leq n / c$ Byzantine nodes, for some constant $c>0$,\nwith $d = \\Omega(f + \\sqrt{n \\log n})$ and $O(\\log^3 n)$ parallel time steps,\nusing $O(\\log n)$ states per node. We also give an alternative algorithm with\n$d = \\Omega(\\min\\{f \\log^2 n + 1,n\\})$. Moreover, we combine both algorithms\ninto one using random coins. The only other known previous work on\nByzantine-resilient population protocols tolerates up to $f = o(\\sqrt n)$\nfaulty nodes and works against a static adversary; hence, our protocols\nsignificantly improve the tolerance by an $\\omega(\\sqrt n)$ factor and all of\nthem work correctly against a stronger dynamic adversary.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 03:37:46 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Busch", "Costas", ""], ["Kowalski", "Dariusz R.", ""]]}, {"id": "2105.07298", "submitter": "Enzo Rucci", "authors": "Manuel Costanzo and Enzo Rucci and Ulises Costi and Franco Chichizola\n  and Marcelo Naiouf", "title": "Comparison of HPC Architectures for Computing All-Pairs Shortest Paths.\n  Intel Xeon Phi KNL vs NVIDIA Pascal", "comments": "Computer Science - CACIC 2020. CACIC 2020. Communications in Computer\n  and Information Science, vol 1409. Springer, Cham", "journal-ref": null, "doi": "10.1007/978-3-030-75836-3_3", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Today, one of the main challenges for high-performance computing systems is\nto improve their performance by keeping energy consumption at acceptable\nlevels. In this context, a consolidated strategy consists of using accelerators\nsuch as GPUs or many-core Intel Xeon Phi processors. In this work, devices of\nthe NVIDIA Pascal and Intel Xeon Phi Knights Landing architectures are\ndescribed and compared. Selecting the Floyd-Warshall algorithm as a\nrepresentative case of graph and memory-bound applications, optimized\nimplementations were developed to analyze and compare performance and energy\nefficiency on both devices. As it was expected, Xeon Phi showed superior when\nconsidering double-precision data. However, contrary to what was considered in\nour preliminary analysis, it was found that the performance and energy\nefficiency of both devices were comparable using single-precision datatype.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 22:01:37 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Costanzo", "Manuel", ""], ["Rucci", "Enzo", ""], ["Costi", "Ulises", ""], ["Chichizola", "Franco", ""], ["Naiouf", "Marcelo", ""]]}, {"id": "2105.07320", "submitter": "Vipul Gupta", "authors": "Vipul Gupta, Avishek Ghosh, Michal Derezinski, Rajiv Khanna, Kannan\n  Ramchandran, Michael Mahoney", "title": "LocalNewton: Reducing Communication Bottleneck for Distributed Learning", "comments": "To be published in Uncertainty in Artificial Intelligence (UAI) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To address the communication bottleneck problem in distributed optimization\nwithin a master-worker framework, we propose LocalNewton, a distributed\nsecond-order algorithm with local averaging. In LocalNewton, the worker\nmachines update their model in every iteration by finding a suitable\nsecond-order descent direction using only the data and model stored in their\nown local memory. We let the workers run multiple such iterations locally and\ncommunicate the models to the master node only once every few (say L)\niterations. LocalNewton is highly practical since it requires only one\nhyperparameter, the number L of local iterations. We use novel matrix\nconcentration-based techniques to obtain theoretical guarantees for\nLocalNewton, and we validate them with detailed empirical evaluation. To\nenhance practicability, we devise an adaptive scheme to choose L, and we show\nthat this reduces the number of local iterations in worker machines between two\nmodel synchronizations as the training proceeds, successively refining the\nmodel quality at the master. Via extensive experiments using several real-world\ndatasets with AWS Lambda workers and an AWS EC2 master, we show that\nLocalNewton requires fewer than 60% of the communication rounds (between master\nand workers) and less than 40% of the end-to-end running time, compared to\nstate-of-the-art algorithms, to reach the same training~loss.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 00:15:08 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Gupta", "Vipul", ""], ["Ghosh", "Avishek", ""], ["Derezinski", "Michal", ""], ["Khanna", "Rajiv", ""], ["Ramchandran", "Kannan", ""], ["Mahoney", "Michael", ""]]}, {"id": "2105.07497", "submitter": "David Dice", "authors": "Dave Dice and Alex Kogan", "title": "Intra-process Caching and Reuse of Threads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Creating and destroying threads on modern Linux systems incurs high latency,\nabsent concurrency, and fails to scale as we increase concurrency. To address\nthis concern we introduce a process-local cache of idle threads. Specifically,\ninstead of destroying a thread when it terminates, we cache and then recycle\nthat thread in the context of subsequent thread creation requests. This\napproach shows significant promise in various applications and benchmarks that\ncreate and destroy threads rapidly and illustrates the need for and potential\nbenefits of improved concurrency infrastructure. With caching, the cost of\ncreating a new thread drops by almost an order of magnitude. As our experiments\ndemonstrate, this results in significant performance improvements for multiple\napplications that aggressively create and destroy numerous threads.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 19:13:22 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Dice", "Dave", ""], ["Kogan", "Alex", ""]]}, {"id": "2105.07526", "submitter": "Yuping Fan", "authors": "Yuping Fan and Zhiling Lan", "title": "DRAS-CQSim: A Reinforcement Learning based Framework for HPC Cluster\n  Scheduling", "comments": null, "journal-ref": "Software Impacts 2021", "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For decades, system administrators have been striving to design and tune\ncluster scheduling policies to improve the performance of high performance\ncomputing (HPC) systems. However, the increasingly complex HPC systems combined\nwith highly diverse workloads make such manual process challenging,\ntime-consuming, and error-prone. We present a reinforcement learning based HPC\nscheduling framework named DRAS-CQSim to automatically learn optimal scheduling\npolicy. DRAS-CQSim encapsulates simulation environments, agents, hyperparameter\ntuning options, and different reinforcement learning algorithms, which allows\nthe system administrators to quickly obtain customized scheduling policies.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 21:56:31 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Fan", "Yuping", ""], ["Lan", "Zhiling", ""]]}, {"id": "2105.07603", "submitter": "Weiming Zhuang", "authors": "Weiming Zhuang, Xin Gan, Yonggang Wen, Shuai Zhang", "title": "EasyFL: A Low-code Federated Learning Platform For Dummies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Academia and industry have developed several platforms to support the popular\nprivacy-preserving distributed learning method -- Federated Learning (FL).\nHowever, these platforms are complex to use and require a deep understanding of\nFL, which imposes high barriers to entry for beginners, limits the productivity\nof researchers, and compromises deployment efficiency. In this paper, we\npropose the first low-code FL platform, EasyFL, to enable users with various\nlevels of expertise to experiment and prototype FL applications with little\ncoding. We achieve this goal while ensuring great flexibility and extensibility\nfor customization by unifying simple API design, modular design, and granular\ntraining flow abstraction. With only a few lines of code, EasyFL empowers them\nwith many out-of-the-box functionalities to accelerate experimentation and\ndeployment. These practical functionalities are heterogeneity simulation,\ncomprehensive tracking, distributed training optimization, and seamless\ndeployment. They are proposed based on challenges identified in the proposed FL\nlife cycle. Compared with other platforms, EasyFL not only requires just three\nlines of code (at least 10x lesser) to build a vanilla FL application but also\nincurs lower training overhead. Besides, our evaluations demonstrate that\nEasyFL expedites distributed training by 1.5x. It also improves the efficiency\nof deployment. We believe that EasyFL will increase the productivity of\nresearchers and democratize FL to wider audiences.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 04:15:55 GMT"}, {"version": "v2", "created": "Sat, 3 Jul 2021 13:18:23 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Zhuang", "Weiming", ""], ["Gan", "Xin", ""], ["Wen", "Yonggang", ""], ["Zhang", "Shuai", ""]]}, {"id": "2105.07606", "submitter": "Weiming Zhuang", "authors": "Weiming Zhuang, Xin Gan, Yonggang Wen, Xuesen Zhang, Shuai Zhang,\n  Shuai Yi", "title": "Towards Unsupervised Domain Adaptation for Deep Face Recognition under\n  Privacy Constraints via Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation has been widely adopted to generalize models\nfor unlabeled data in a target domain, given labeled data in a source domain,\nwhose data distributions differ from the target domain. However, existing works\nare inapplicable to face recognition under privacy constraints because they\nrequire sharing sensitive face images between two domains. To address this\nproblem, we propose a novel unsupervised federated face recognition approach\n(FedFR). FedFR improves the performance in the target domain by iteratively\naggregating knowledge from the source domain through federated learning. It\nprotects data privacy by transferring models instead of raw data between\ndomains. Besides, we propose a new domain constraint loss (DCL) to regularize\nsource domain training. DCL suppresses the data volume dominance of the source\ndomain. We also enhance a hierarchical clustering algorithm to predict pseudo\nlabels for the unlabeled target domain accurately. To this end, FedFR forms an\nend-to-end training pipeline: (1) pre-train in the source domain; (2) predict\npseudo labels by clustering in the target domain; (3) conduct\ndomain-constrained federated learning across two domains. Extensive experiments\nand analysis on two newly constructed benchmarks demonstrate the effectiveness\nof FedFR. It outperforms the baseline and classic methods in the target domain\nby over 4% on the more realistic benchmark. We believe that FedFR will shed\nlight on applying federated learning to more computer vision tasks under\nprivacy constraints.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 04:24:25 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Zhuang", "Weiming", ""], ["Gan", "Xin", ""], ["Wen", "Yonggang", ""], ["Zhang", "Xuesen", ""], ["Zhang", "Shuai", ""], ["Yi", "Shuai", ""]]}, {"id": "2105.07739", "submitter": "Akash Kumar", "authors": "Behnaz Ranjbar, Ali Hosseinghorban, Mohammad Salehi, Alireza Ejlali\n  and Akash Kumar", "title": "Toward the Design of Fault-Tolerance- and Peak- Power-Aware Multi-Core\n  Mixed-Criticality Systems", "comments": "This is an extended version of the paper accepted to be published in\n  IEEE TCAD 2021 which includes Appendices A-E", "journal-ref": null, "doi": "10.1109/TCAD.2021.3082495", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mixed-Criticality (MC) systems have recently been devised to address the\nrequirements of real-time systems in industrial applications, where the system\nruns tasks with different criticality levels on a single platform. In some\nworkloads, a high-critically task might overrun and overload the system, or a\nfault can occur during the execution. However, these systems must be\nfault-tolerant and guarantee the correct execution of all high-criticality\ntasks by their deadlines to avoid catastrophic consequences, in any situation.\nFurthermore, in these MC systems, the peak power consumption of the system may\nincrease, especially in an overload situation and exceed the processor Thermal\nDesign Power (TDP) constraint. This may cause generating heat beyond the\ncooling capacity, resulting the system stop to avoid excessive heat and halting\nthe processor. In this paper, we propose a technique for dependent\ndual-criticality tasks in fault-tolerant multi-core MC systems to manage peak\npower consumption and temperature. The technique develops a tree of possible\ntask mapping and scheduling at design-time to cover all possible scenarios and\nreduce the low-criticality task drop rate in the high-criticality mode. At\nrun-time, the system exploits the tree to select a proper schedule according to\nfault occurrences and criticality mode changes. Experimental results show that\nthe average task schedulability is 74.14% on average for the proposed method,\nwhile the peak power consumption and maximum temperature are improved by 16.65%\nand 14.9 C on average, respectively, compared to a recent work. In addition,\nfor a real-life application, our method reduces the peak power and maximum\ntemperature by up to 20.06% and 5 C, respectively, compared to a\nstate-of-the-art approach.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 11:20:14 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 12:34:23 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Ranjbar", "Behnaz", ""], ["Hosseinghorban", "Ali", ""], ["Salehi", "Mohammad", ""], ["Ejlali", "Alireza", ""], ["Kumar", "Akash", ""]]}, {"id": "2105.07782", "submitter": "B\\'erenger Bramas", "authors": "B\\'erenger Bramas", "title": "A fast vectorized sorting implementation based on the ARM scalable\n  vector extension (SVE)", "comments": "https://gitlab.inria.fr/bramas/arm-sve-sort", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The way developers implement their algorithms and how these implementations\nbehave on modern CPUs are governed by the design and organization of these. The\nvectorization units (SIMD) are among the few CPUs' parts that can and must be\nexplicitly controlled. In the HPC community, the x86 CPUs and their\nvectorization instruction sets were de-facto the standard for decades. Each new\nrelease of an instruction set was usually a doubling of the vector length\ncoupled with new operations. Each generation was pushing for adapting and\nimproving previous implementations. The release of the ARM scalable vector\nextension (SVE) changed things radically for several reasons. First, we expect\nARM processors to equip many supercomputers in the next years. Second, SVE's\ninterface is different in several aspects from the x86 extensions as it\nprovides different instructions, uses a predicate to control most operations,\nand has a vector size that is only known at execution time. Therefore, using\nSVE opens new challenges on how to adapt algorithms including the ones that are\nalready well-optimized on x86. In this paper, we port a hybrid sort based on\nthe well-known Quicksort and Bitonic-sort algorithms. We use a Bitonic sort to\nprocess small partitions/arrays and a vectorized partitioning implementation to\ndivide the partitions. We explain how we use the predicates and how we manage\nthe non-static vector size. We explain how we efficiently implement the sorting\nkernels. Our approach only needs an array of O(log N) for the recursive calls\nin the partitioning phase, both in the sequential and in the parallel case. We\ntest the performance of our approach on a modern ARMv8.2 and assess the\ndifferent layers of our implementation by sorting/partitioning integers, double\nfloating-point numbers, and key/value pairs of integers. Our approach is faster\nthan the GNU C++ sort algorithm by a speedup factor of 4 on average.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 12:48:54 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Bramas", "B\u00e9renger", ""]]}, {"id": "2105.07806", "submitter": "Jiawei Jiang", "authors": "Jiawei Jiang, Shaoduo Gan, Yue Liu, Fanlin Wang, Gustavo Alonso, Ana\n  Klimovic, Ankit Singla, Wentao Wu, Ce Zhang", "title": "Towards Demystifying Serverless Machine Learning Training", "comments": null, "journal-ref": null, "doi": "10.1145/3448016.3459240", "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The appeal of serverless (FaaS) has triggered a growing interest on how to\nuse it in data-intensive applications such as ETL, query processing, or machine\nlearning (ML). Several systems exist for training large-scale ML models on top\nof serverless infrastructures (e.g., AWS Lambda) but with inconclusive results\nin terms of their performance and relative advantage over \"serverful\"\ninfrastructures (IaaS). In this paper we present a systematic, comparative\nstudy of distributed ML training over FaaS and IaaS. We present a design space\ncovering design choices such as optimization algorithms and synchronization\nprotocols, and implement a platform, LambdaML, that enables a fair comparison\nbetween FaaS and IaaS. We present experimental results using LambdaML, and\nfurther develop an analytic model to capture cost/performance tradeoffs that\nmust be considered when opting for a serverless infrastructure. Our results\nindicate that ML training pays off in serverless only for models with efficient\n(i.e., reduced) communication and that quickly converge. In general, FaaS can\nbe much faster but it is never significantly cheaper than IaaS.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 13:19:23 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Jiang", "Jiawei", ""], ["Gan", "Shaoduo", ""], ["Liu", "Yue", ""], ["Wang", "Fanlin", ""], ["Alonso", "Gustavo", ""], ["Klimovic", "Ana", ""], ["Singla", "Ankit", ""], ["Wu", "Wentao", ""], ["Zhang", "Ce", ""]]}, {"id": "2105.07829", "submitter": "Yuchen Zhong", "authors": "Yuchen Zhong, Cong Xie, Shuai Zheng, Haibin Lin", "title": "Compressed Communication for Distributed Training: Adaptive Methods and\n  System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communication overhead severely hinders the scalability of distributed\nmachine learning systems. Recently, there has been a growing interest in using\ngradient compression to reduce the communication overhead of the distributed\ntraining. However, there is little understanding of applying gradient\ncompression to adaptive gradient methods. Moreover, its performance benefits\nare often limited by the non-negligible compression overhead. In this paper, we\nfirst introduce a novel adaptive gradient method with gradient compression. We\nshow that the proposed method has a convergence rate of\n$\\mathcal{O}(1/\\sqrt{T})$ for non-convex problems. In addition, we develop a\nscalable system called BytePS-Compress for two-way compression, where the\ngradients are compressed in both directions between workers and parameter\nservers. BytePS-Compress pipelines the compression and decompression on CPUs\nand achieves a high degree of parallelism. Empirical evaluations show that we\nimprove the training time of ResNet50, VGG16, and BERT-base by 5.0%, 58.1%,\n23.3%, respectively, without any accuracy loss with 25 Gb/s networking.\nFurthermore, for training the BERT models, we achieve a compression rate of\n333x compared to the mixed-precision training.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 13:41:47 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Zhong", "Yuchen", ""], ["Xie", "Cong", ""], ["Zheng", "Shuai", ""], ["Lin", "Haibin", ""]]}, {"id": "2105.07902", "submitter": "David \\'Alvarez Robert", "authors": "David \\'Alvarez, Kevin Sala, Marcos Maro\\~nas, Aleix Roca, Vicen\\c{c}\n  Beltran", "title": "Advanced Synchronization Techniques for Task-based Runtime Systems", "comments": "14 pages, 11 figures. Published in the 26th ACM SIGPLAN Symposium on\n  Principles and Practice of Parallel Programming (PPoPP'21)", "journal-ref": "Proceedings of the 26th ACM SIGPLAN Symposium on Principles and\n  Practice of Parallel Programming (2021) 334-347", "doi": "10.1145/3437801.3441601", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Task-based programming models like OmpSs-2 and OpenMP provide a flexible\ndata-flow execution model to exploit dynamic, irregular and nested parallelism.\nProviding an efficient implementation that scales well with small granularity\ntasks remains a challenge, and bottlenecks can manifest in several runtime\ncomponents. In this paper, we analyze the limiting factors in the scalability\nof a task-based runtime system and propose individual solutions for each of the\nchallenges, including a wait-free dependency system and a novel scalable\nscheduler design based on delegation. We evaluate how the optimizations impact\nthe overall performance of the runtime, both individually and in combination.\nWe also compare the resulting runtime against state of the art OpenMP\nimplementations, showing equivalent or better performance, especially for\nfine-grained tasks.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 14:32:54 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["\u00c1lvarez", "David", ""], ["Sala", "Kevin", ""], ["Maro\u00f1as", "Marcos", ""], ["Roca", "Aleix", ""], ["Beltran", "Vicen\u00e7", ""]]}, {"id": "2105.07936", "submitter": "Narges Mehran", "authors": "Narges Mehran, Dragi Kimovski, Radu Prodan", "title": "A Two-Sided Matching Model for Data Stream Processing in the Cloud-Fog\n  Continuum", "comments": "7 figures", "journal-ref": null, "doi": "10.1109/CCGrid51090.2021.00061", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Latency-sensitive and bandwidth-intensive stream processing applications are\ndominant traffic generators over the Internet network. A stream consists of a\ncontinuous sequence of data elements, which require processing in nearly\nreal-time. To improve communication latency and reduce the network congestion,\nFog computing complements the Cloud services by moving the computation towards\nthe edge of the network. Unfortunately, the heterogeneity of the new Cloud-Fog\ncontinuum raises important challenges related to deploying and executing data\nstream applications. We explore in this work a two-sided stable matching model\ncalled Cloud-Fog to data stream application matching (CODA) for deploying a\ndistributed application represented as a workflow of stream processing\nmicroservices on heterogeneous Cloud-Fog computing resources. In CODA, the\napplication microservices rank the continuum resources based on their\nmicroservice stream processing time, while resources rank the stream processing\nmicroservices based on their residual bandwidth. A stable many-to-one matching\nalgorithm assigns microservices to resources based on their mutual preferences,\naiming to optimize the complete stream processing time on the application side,\nand the total streaming traffic on the resource side. We evaluate the CODA\nalgorithm using simulated and real-world Cloud-Fog scenarios. We achieved 11 to\n45% lower stream processing time and 1.3 to 20% lower streaming traffic\ncompared to related state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 15:22:57 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Mehran", "Narges", ""], ["Kimovski", "Dragi", ""], ["Prodan", "Radu", ""]]}, {"id": "2105.08023", "submitter": "Kun Yuan", "authors": "Kun Yuan and Sulaiman A. Alghunaim", "title": "Removing Data Heterogeneity Influence Enhances Network Topology\n  Dependence of Decentralized SGD", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider decentralized stochastic optimization problems where a network of\nagents each owns a local cost function cooperate to find a minimizer of the\nglobal-averaged cost. A widely studied decentralized algorithm for this problem\nis D-SGD in which each node applies a stochastic gradient descent step, then\naverages its estimate with its neighbors. D-SGD is attractive due to its\nefficient single-iteration communication and can achieve linear speedup in\nconvergence (in terms of the network size). However, D-SGD is very sensitive to\nthe network topology. For smooth objective functions, the transient stage\n(which measures how fast the algorithm can reach the linear speedup stage) of\nD-SGD is on the order of $O(n/(1-\\beta)^2)$ and $O(n^3/(1-\\beta)^4)$ for\nstrongly convex and generally convex cost functions, respectively, where\n$1-\\beta \\in (0,1)$ is a topology-dependent quantity that approaches $0$ for a\nlarge and sparse network. Hence, D-SGD suffers from slow convergence for large\nand sparse networks.\n  In this work, we study the non-asymptotic convergence property of the\nD$^2$/Exact-diffusion algorithm. By eliminating the influence of data\nheterogeneity between nodes, D$^2$/Exact-diffusion is shown to have an enhanced\ntransient stage that are on the order of $O(n/(1-\\beta))$ and\n$O(n^3/(1-\\beta)^2)$ for strongly convex and generally convex cost functions,\nrespectively. Moreover, we provide a lower bound of the transient stage of\nD-SGD under homogeneous data distributions, which coincides with the transient\nstage of D$^2$/Exact-diffusion in the strongly-convex setting. These results\nshow that removing the influence of data heterogeneity can ameliorate the\nnetwork topology dependence of D-SGD. Compared with existing decentralized\nalgorithms bounds, D$^2$/Exact-diffusion is least sensitive to network\ntopology.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 17:16:52 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Yuan", "Kun", ""], ["Alghunaim", "Sulaiman A.", ""]]}, {"id": "2105.08098", "submitter": "Nikita Koval", "authors": "Alexander Fedorov, Nikita Koval, Dan Alistarh", "title": "A Scalable Concurrent Algorithm for Dynamic Connectivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dynamic Connectivity is a fundamental algorithmic graph problem, motivated by\na wide range of applications to social and communication networks and used as a\nbuilding block in various other algorithms, such as the bi-connectivity and the\ndynamic minimal spanning tree problems. In brief, we wish to maintain the\nconnected components of the graph under dynamic edge insertions and deletions.\nIn the sequential case, the problem has been well-studied from both theoretical\nand practical perspectives. However, much less is known about efficient\nconcurrent solutions to this problem. This is the gap we address in this paper.\n  We start from one of the classic data structures used to solve this problem,\nthe Euler Tour Tree. Our first contribution is a non-blocking single-writer\nimplementation of it. We leverage this data structure to obtain the first truly\nconcurrent generalization of dynamic connectivity, which preserves the time\ncomplexity of its sequential counterpart, but is also scalable in practice. To\nachieve this, we rely on three main techniques. The first is to ensure that\nconnectivity queries, which usually dominate real-world workloads, are\nnon-blocking. The second non-trivial technique expands the above idea by making\nall queries that do not change the connectivity structure non-blocking. The\nthird ingredient is applying fine-grained locking for updating the connected\ncomponents, which allows operations on disjoint components to occur in\nparallel.\n  We evaluate the resulting algorithm on various workloads, executing on both\nreal and synthetic graphs. The results show the efficiency of each of the\nproposed optimizations; the most efficient variant improves the performance of\na coarse-grained based implementation on realistic scenarios up to 6x on\naverage and up to 30x when connectivity queries dominate.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 18:11:49 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Fedorov", "Alexander", ""], ["Koval", "Nikita", ""], ["Alistarh", "Dan", ""]]}, {"id": "2105.08239", "submitter": "Yangjie Qi", "authors": "Yangjie Qi, Shuo Zhang and Tarek M. Taha", "title": "TRIM: A Design Space Exploration Model for Deep Neural Networks\n  Inference and Training Accelerators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is increasing demand for specialized hardware for training deep neural\nnetworks, both in edge/IoT environments and in high-performance computing\nsystems. The design space of such hardware is very large due to the wide range\nof processing architectures, deep neural network configurations, and dataflow\noptions. This makes developing deep neural network processors quite complex,\nespecially for training. We present TRIM, an infrastructure to help hardware\narchitects explore the design space of deep neural network accelerators for\nboth inference and training in the early design stages. The model evaluates at\nthe whole network level, considering both inter-layer and intra-layer\nactivities. Given applications, essential hardware specifications, and a design\ngoal, TRIM can quickly explore different hardware design options, select the\noptimal dataflow and guide new hardware architecture design. We validated TRIM\nwith FPGA-based implementation of deep neural network accelerators and\nASIC-based architectures. We also show how to use TRIM to explore the design\nspace through several case studies. TRIM is a powerful tool to help architects\nevaluate different hardware choices to develop efficient inference and training\narchitecture design.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 02:46:26 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 02:38:13 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Qi", "Yangjie", ""], ["Zhang", "Shuo", ""], ["Taha", "Tarek M.", ""]]}, {"id": "2105.08275", "submitter": "Yuanming Li", "authors": "Yuanming Li, Huaizheng Zhang, Shanshan Jiang, Fan Yang, Yonggang Wen\n  and Yong Luo", "title": "ModelPS: An Interactive and Collaborative Platform for Editing\n  Pre-trained Models at Scale", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AI engineering has emerged as a crucial discipline to democratize deep neural\nnetwork (DNN) models among software developers with a diverse background. In\nparticular, altering these DNN models in the deployment stage posits a\ntremendous challenge. In this research, we propose and develop a low-code\nsolution, ModelPS (an acronym for \"Model Photoshop\"), to enable and empower\ncollaborative DNN model editing and intelligent model serving. The ModelPS\nsolution embodies two transformative features: 1) a user-friendly web interface\nfor a developer team to share and edit DNN models pictorially, in a low-code\nfashion, and 2) a model genie engine in the backend to aid developers in\ncustomizing model editing configurations for given deployment requirements or\nconstraints. Our case studies with a wide range of deep learning (DL) models\nshow that the system can tremendously reduce both development and communication\noverheads with improved productivity. The code has been released as an\nopen-source package at GitHub.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 04:51:56 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 10:13:16 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Li", "Yuanming", ""], ["Zhang", "Huaizheng", ""], ["Jiang", "Shanshan", ""], ["Yang", "Fan", ""], ["Wen", "Yonggang", ""], ["Luo", "Yong", ""]]}, {"id": "2105.08668", "submitter": "JunKyu Lee", "authors": "JunKyu Lee, Blesson Varghese, Roger Woods, Hans Vandierendonck", "title": "TOD: Transprecise Object Detection to Maximise Real-Time Accuracy on the\n  Edge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time video analytics on the edge is challenging as the computationally\nconstrained resources typically cannot analyse video streams at full fidelity\nand frame rate, which results in loss of accuracy. This paper proposes a\nTransprecise Object Detector (TOD) which maximises the real-time object\ndetection accuracy on an edge device by selecting an appropriate Deep Neural\nNetwork (DNN) on the fly with negligible computational overhead. TOD makes two\nkey contributions over the state of the art: (1) TOD leverages characteristics\nof the video stream such as object size and speed of movement to identify\nnetworks with high prediction accuracy for the current frames; (2) it selects\nthe best-performing network based on projected accuracy and computational\ndemand using an effective and low-overhead decision mechanism. Experimental\nevaluation on a Jetson Nano demonstrates that TOD improves the average object\ndetection precision by 34.7 % over the YOLOv4-tiny-288 model on average over\nthe MOT17Det dataset. In the MOT17-05 test dataset, TOD utilises only 45.1 % of\nGPU resource and 62.7 % of the GPU board power without losing accuracy,\ncompared to YOLOv4-416 model. We expect that TOD will maximise the application\nof edge devices to real-time object detection, since TOD maximises real-time\nobject detection accuracy given edge devices according to dynamic input\nfeatures without increasing inference latency in practice.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 16:52:46 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Lee", "JunKyu", ""], ["Varghese", "Blesson", ""], ["Woods", "Roger", ""], ["Vandierendonck", "Hans", ""]]}, {"id": "2105.08671", "submitter": "Neel Kanwal", "authors": "Jiahui Geng, Neel Kanwal, Martin Gilje Jaatun, Chunming Rong", "title": "DID-eFed: Facilitating Federated Learning as a Service with\n  Decentralized Identities", "comments": "Paper accepted in EASE2021", "journal-ref": null, "doi": "10.1145/3463274.3463352", "report-no": null, "categories": "cs.CR cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have entered the era of big data, and it is considered to be the \"fuel\"\nfor the flourishing of artificial intelligence applications. The enactment of\nthe EU General Data Protection Regulation (GDPR) raises concerns about\nindividuals' privacy in big data. Federated learning (FL) emerges as a\nfunctional solution that can help build high-performance models shared among\nmultiple parties while still complying with user privacy and data\nconfidentiality requirements. Although FL has been intensively studied and used\nin real applications, there is still limited research related to its prospects\nand applications as a FLaaS (Federated Learning as a Service) to interested 3rd\nparties. In this paper, we present a FLaaS system: DID-eFed, where FL is\nfacilitated by decentralized identities (DID) and a smart contract. DID enables\na more flexible and credible decentralized access management in our system,\nwhile the smart contract offers a frictionless and less error-prone process. We\ndescribe particularly the scenario where our DID-eFed enables the FLaaS among\nhospitals and research institutions.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 16:55:34 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 07:44:07 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Geng", "Jiahui", ""], ["Kanwal", "Neel", ""], ["Jaatun", "Martin Gilje", ""], ["Rong", "Chunming", ""]]}, {"id": "2105.08706", "submitter": "Gal Sela", "authors": "Gal Sela, Erez Petrank", "title": "Durable Queues: The Second Amendment", "comments": "Code: https://github.com/galysela/DurableQueues", "journal-ref": "Proceedings of the 33rd ACM Symposium on Parallelism in Algorithms\n  and Architectures (SPAA '21), July 6-8, 2021, Virtual Event, USA. ACM, New\n  York, NY, USA, 385-397", "doi": "10.1145/3409964.3461791", "report-no": null, "categories": "cs.DC cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider durable data structures for non-volatile main memory, such as the\nnew Intel Optane memory architecture. Substantial recent work has concentrated\non making concurrent data structures durable with low overhead, by adding a\nminimal number of blocking persist operations (i.e., flushes and fences). In\nthis work we show that focusing on minimizing the number of persist\ninstructions is important, but not enough. We show that access to flushed\ncontent is of high cost due to cache invalidation in current architectures.\nGiven this finding, we present a design of the queue data structure that\nproperly takes care of minimizing blocking persist operations as well as\nminimizing access to flushed content. The proposed design outperforms\nstate-of-the-art durable queues.\n  We start by providing a durable version of the Michael Scott queue (MSQ). We\namend MSQ by adding a minimal number of persist instructions, fewer than in\navailable durable queues, and meeting the theoretical lower bound on the number\nof blocking persist operations. We then proceed with a second amendment to this\ndesign, that eliminates accesses to flushed data. Evaluation shows that the\nsecond amendment yields substantial performance improvement, outperforming the\nstate of the art and demonstrating the importance of reduced accesses to\nflushed content. The presented queues are durably linearizable and lock-free.\nFinally, we discuss the theoretical optimal number of accesses to flushed\ncontent.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 17:45:37 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 15:42:37 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Sela", "Gal", ""], ["Petrank", "Erez", ""]]}, {"id": "2105.08764", "submitter": "Weijian Zheng", "authors": "Weijian Zheng, Dali Wang, Fengguang Song", "title": "OpenGraphGym-MG: Using Reinforcement Learning to Solve Large Graph\n  Optimization Problems on MultiGPU Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large scale graph optimization problems arise in many fields. This paper\npresents an extensible, high performance framework (named OpenGraphGym-MG) that\nuses deep reinforcement learning and graph embedding to solve large graph\noptimization problems with multiple GPUs. The paper uses a common RL algorithm\n(deep Q-learning) and a representative graph embedding (structure2vec) to\ndemonstrate the extensibility of the framework and, most importantly, to\nillustrate the novel optimization techniques, such as spatial parallelism,\ngraph-level and node-level batched processing, distributed sparse graph\nstorage, efficient parallel RL training and inference algorithms, repeated\ngradient descent iterations, and adaptive multiple-node selections. This study\nperforms a comprehensive performance analysis on parallel efficiency and memory\ncost that proves the parallel RL training and inference algorithms are\nefficient and highly scalable on a number of GPUs. This study also conducts a\nrange of large graph experiments, with both generated graphs (over 30 million\nedges) and real-world graphs, using a single compute node (with six GPUs) of\nthe Summit supercomputer. Good scalability in both RL training and inference is\nachieved: as the number of GPUs increases from one to six, the time of a single\nstep of RL training and a single step of RL inference on large graphs with more\nthan 30 million edges, is reduced from 316.4s to 54.5s, and 23.8s to 3.4s,\nrespectively. The research results on a single node lay out a solid foundation\nfor the future work to address graph optimization problems with a large number\nof GPUs across multiple nodes in the Summit.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 18:24:42 GMT"}, {"version": "v2", "created": "Thu, 24 Jun 2021 02:01:48 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Zheng", "Weijian", ""], ["Wang", "Dali", ""], ["Song", "Fengguang", ""]]}, {"id": "2105.08820", "submitter": "Udit Gupta", "authors": "Udit Gupta, Samuel Hsia, Jeff Zhang, Mark Wilkening, Javin Pombra,\n  Hsien-Hsin S. Lee, Gu-Yeon Wei, Carole-Jean Wu, David Brooks", "title": "RecPipe: Co-designing Models and Hardware to Jointly Optimize\n  Recommendation Quality and Performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning recommendation systems must provide high quality, personalized\ncontent under strict tail-latency targets and high system loads. This paper\npresents RecPipe, a system to jointly optimize recommendation quality and\ninference performance. Central to RecPipe is decomposing recommendation models\ninto multi-stage pipelines to maintain quality while reducing compute\ncomplexity and exposing distinct parallelism opportunities. RecPipe implements\nan inference scheduler to map multi-stage recommendation engines onto\ncommodity, heterogeneous platforms (e.g., CPUs, GPUs).While the hardware-aware\nscheduling improves ranking efficiency, the commodity platforms suffer from\nmany limitations requiring specialized hardware. Thus, we design RecPipeAccel\n(RPAccel), a custom accelerator that jointly optimizes quality, tail-latency,\nand system throughput. RPAc-cel is designed specifically to exploit the\ndistinct design space opened via RecPipe. In particular, RPAccel processes\nqueries in sub-batches to pipeline recommendation stages, implements dual\nstatic and dynamic embedding caches, a set of top-k filtering units, and a\nreconfigurable systolic array. Com-pared to prior-art and at iso-quality, we\ndemonstrate that RPAccel improves latency and throughput by 3x and 6x.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 20:44:04 GMT"}, {"version": "v2", "created": "Sat, 22 May 2021 17:41:29 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Gupta", "Udit", ""], ["Hsia", "Samuel", ""], ["Zhang", "Jeff", ""], ["Wilkening", "Mark", ""], ["Pombra", "Javin", ""], ["Lee", "Hsien-Hsin S.", ""], ["Wei", "Gu-Yeon", ""], ["Wu", "Carole-Jean", ""], ["Brooks", "David", ""]]}, {"id": "2105.08917", "submitter": "Shreyas Pai", "authors": "Shreyas Pai and Gopal Pandurangan and Sriram V. Pemmaraju and Peter\n  Robinson", "title": "Can We Break Symmetry with o(m) Communication?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the communication cost (or message complexity) of fundamental\ndistributed symmetry breaking problems, namely, coloring and MIS. While\nsignificant progress has been made in understanding and improving the running\ntime of such problems, much less is known about the message complexity of these\nproblems. In fact, all known algorithms need at least $\\Omega(m)$ communication\nfor these problems, where $m$ is the number of edges in the graph. We address\nthe following question in this paper: can we solve problems such as coloring\nand MIS using sublinear, i.e., $o(m)$ communication, and if so under what\nconditions? [See full abstract in pdf]\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 04:34:32 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Pai", "Shreyas", ""], ["Pandurangan", "Gopal", ""], ["Pemmaraju", "Sriram V.", ""], ["Robinson", "Peter", ""]]}, {"id": "2105.08925", "submitter": "Di Chai", "authors": "Di Chai, Leye Wang, Lianzhi Fu, Junxue Zhang, Kai Chen, Qiang Yang", "title": "Federated Singular Vector Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the promulgation of data protection laws (e.g., GDPR in 2018), privacy\npreservation has become a general agreement in applications where cross-domain\nsensitive data are utilized. Out of many privacy-preserving techniques,\nfederated learning (FL) has received much attention as a bridge for secure data\nconnection and cooperation. Although FL's research works have surged, some\nclassical data modeling methods are not well accommodated in FL. In this paper,\nwe propose the first masking-based federated singular vector decomposition\nmethod, called FedSVD. FedSVD protects the raw data through a singular value\ninvariance mask, which can be further removed from the SVD results. Compared\nwith prior privacy-preserving SVD solutions, FedSVD has lossless results, high\nconfidentiality, and excellent scalability. We provide privacy proof showing\nthat FedSVD has guaranteed data confidentiality. Empirical experiments on\nreal-life datasets and synthetic data have verified the effectiveness of our\nmethod. The reconstruction error of FedSVD is around 0.000001% of the raw data,\nvalidating the lossless property of FedSVD. The scalability of FedSVD is nearly\nthe same as the standalone SVD algorithm. Hence, FedSVD can bring privacy\nprotection almost without sacrificing any computation time or communication\noverhead.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 04:51:12 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Chai", "Di", ""], ["Wang", "Leye", ""], ["Fu", "Lianzhi", ""], ["Zhang", "Junxue", ""], ["Chen", "Kai", ""], ["Yang", "Qiang", ""]]}, {"id": "2105.08937", "submitter": "Gang Li", "authors": "Gang Li, Zejian Liu, Fanrong Li, Jian Cheng", "title": "Block Convolution: Towards Memory-Efficient Inference of Large-Scale\n  CNNs on FPGA", "comments": "Accepted to IEEE Transactions on Computer-Aided Design of Integrated\n  Circuits and Systems (TCAD), 2021. This is an extended version of the\n  conference paper published on DATE'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep convolutional neural networks have achieved remarkable progress in\nrecent years. However, the large volume of intermediate results generated\nduring inference poses a significant challenge to the accelerator design for\nresource-constraint FPGA. Due to the limited on-chip storage, partial results\nof intermediate layers are frequently transferred back and forth between\non-chip memory and off-chip DRAM, leading to a non-negligible increase in\nlatency and energy consumption. In this paper, we propose block convolution, a\nhardware-friendly, simple, yet efficient convolution operation that can\ncompletely avoid the off-chip transfer of intermediate feature maps at\nrun-time. The fundamental idea of block convolution is to eliminate the\ndependency of feature map tiles in the spatial dimension when spatial tiling is\nused, which is realized by splitting a feature map into independent blocks so\nthat convolution can be performed separately on individual blocks. We conduct\nextensive experiments to demonstrate the efficacy of the proposed block\nconvolution on both the algorithm side and the hardware side. Specifically, we\nevaluate block convolution on 1) VGG-16, ResNet-18, ResNet-50, and MobileNet-V1\nfor ImageNet classification task; 2) SSD, FPN for COCO object detection task,\nand 3) VDSR for Set5 single image super-resolution task. Experimental results\ndemonstrate that comparable or higher accuracy can be achieved with block\nconvolution. We also showcase two CNN accelerators via algorithm/hardware\nco-design based on block convolution on memory-limited FPGAs, and evaluation\nshows that both accelerators substantially outperform the baseline without\noff-chip transfer of intermediate feature maps.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 06:03:59 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Li", "Gang", ""], ["Liu", "Zejian", ""], ["Li", "Fanrong", ""], ["Cheng", "Jian", ""]]}, {"id": "2105.08969", "submitter": "Wei Shao Dr", "authors": "Wei Shao, Arian Prabowo, Sichen Zhao, Piotr Koniusz, Flora D. Salim", "title": "Predicting Flight Delay with Spatio-Temporal Trajectory Convolutional\n  Network and Airport Situational Awareness Map", "comments": "single column", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To model and forecast flight delays accurately, it is crucial to harness\nvarious vehicle trajectory and contextual sensor data on airport tarmac areas.\nThese heterogeneous sensor data, if modelled correctly, can be used to generate\na situational awareness map. Existing techniques apply traditional supervised\nlearning methods onto historical data, contextual features and route\ninformation among different airports to predict flight delay are inaccurate and\nonly predict arrival delay but not departure delay, which is essential to\nairlines. In this paper, we propose a vision-based solution to achieve a high\nforecasting accuracy, applicable to the airport. Our solution leverages a\nsnapshot of the airport situational awareness map, which contains various\ntrajectories of aircraft and contextual features such as weather and airline\nschedules. We propose an end-to-end deep learning architecture, TrajCNN, which\ncaptures both the spatial and temporal information from the situational\nawareness map. Additionally, we reveal that the situational awareness map of\nthe airport has a vital impact on estimating flight departure delay. Our\nproposed framework obtained a good result (around 18 minutes error) for\npredicting flight departure delay at Los Angeles International Airport.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 07:38:57 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Shao", "Wei", ""], ["Prabowo", "Arian", ""], ["Zhao", "Sichen", ""], ["Koniusz", "Piotr", ""], ["Salim", "Flora D.", ""]]}, {"id": "2105.09058", "submitter": "George Chernishev", "authors": "Alexander Slesarev, Evgeniy Klyuchikov, Kirill Smirnov, George\n  Chernishev", "title": "Revisiting Data Compression in Column-Stores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data compression is widely used in contemporary column-oriented DBMSes to\nlower space usage and to speed up query processing. Pioneering systems have\nintroduced compression to tackle the disk bandwidth bottleneck by trading CPU\nprocessing power for it. The main issue of this is a trade-off between the\ncompression ratio and the decompression CPU cost. Existing results state that\nlight-weight compression with small decompression costs outperforms\nheavy-weight compression schemes in column-stores. However, since the time\nthese results were obtained, CPU, RAM, and disk performance have advanced\nconsiderably. Moreover, novel compression algorithms have emerged.\n  In this paper, we revisit the problem of compression in disk-based\ncolumn-stores. More precisely, we study the I/O-RAM compression scheme which\nimplies that there are two types of pages of different size: disk pages\n(compressed) and in-memory pages (uncompressed). In this scheme, the buffer\nmanager is responsible for decompressing pages as soon as they arrive from\ndisk. This scheme is rather popular as it is easy to implement: several modern\ncolumn and row-stores use it.\n  We pose and address the following research questions: 1) Are heavy-weight\ncompression schemes still inappropriate for disk-based column-stores?, 2) Are\nnew light-weight compression algorithms better than the old ones?, 3) Is there\na need for SIMD-employing decompression algorithms in case of a disk-based\nsystem? We study these questions experimentally using a columnar query engine\nand Star Schema Benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 10:53:41 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Slesarev", "Alexander", ""], ["Klyuchikov", "Evgeniy", ""], ["Smirnov", "Kirill", ""], ["Chernishev", "George", ""]]}, {"id": "2105.09080", "submitter": "Yiming Chen", "authors": "Yiming Chen, Kun Yuan, Yingya Zhang, Pan Pan, Yinghui Xu, Wotao Yin", "title": "Accelerating Gossip SGD with Periodic Global Averaging", "comments": "Accepted to ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communication overhead hinders the scalability of large-scale distributed\ntraining. Gossip SGD, where each node averages only with its neighbors, is more\ncommunication-efficient than the prevalent parallel SGD. However, its\nconvergence rate is reversely proportional to quantity $1-\\beta$ which measures\nthe network connectivity. On large and sparse networks where $1-\\beta \\to 0$,\nGossip SGD requires more iterations to converge, which offsets against its\ncommunication benefit. This paper introduces Gossip-PGA, which adds Periodic\nGlobal Averaging into Gossip SGD. Its transient stage, i.e., the iterations\nrequired to reach asymptotic linear speedup stage, improves from\n$\\Omega(\\beta^4 n^3/(1-\\beta)^4)$ to $\\Omega(\\beta^4 n^3 H^4)$ for non-convex\nproblems. The influence of network topology in Gossip-PGA can be controlled by\nthe averaging period $H$. Its transient-stage complexity is also superior to\nLocal SGD which has order $\\Omega(n^3 H^4)$. Empirical results of large-scale\ntraining on image classification (ResNet50) and language modeling (BERT)\nvalidate our theoretical findings.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 11:59:25 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Chen", "Yiming", ""], ["Yuan", "Kun", ""], ["Zhang", "Yingya", ""], ["Pan", "Pan", ""], ["Xu", "Yinghui", ""], ["Yin", "Wotao", ""]]}, {"id": "2105.09187", "submitter": "Adri\\'an Castell\\'o", "authors": "Adri\\'an Castell\\'o, Sergio Barrachina, Manuel F. Dolz, Enrique S.\n  Quintana-Ort\\'i, Pau San Juan", "title": "High performance and energy efficient inference for deep learning on ARM\n  processors", "comments": "13 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.PF", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We evolve PyDTNN, a framework for distributed parallel training of Deep\nNeural Networks (DNNs), into an efficient inference tool for convolutional\nneural networks. Our optimization process on multicore ARM processors involves\nseveral high-level transformations of the original framework, such as the\ndevelopment and integration of Cython routines to exploit thread-level\nparallelism; the design and development of micro-kernels for the matrix\nmultiplication, vectorized with ARMs NEON intrinsics, that can accommodate\nlayer fusion; and the appropriate selection of several cache configuration\nparameters tailored to the memory hierarchy of the target ARM processors. Our\nexperiments evaluate both inference throughput (measured in processed images/s)\nand inference latency (i.e., time-to-response) as well as energy consumption\nper image when varying the level of thread parallelism and the processor power\nmodes. The experiments with the new inference engine are reported for the\nResNet50 v1.5 model on the ImageNet dataset from the MLPerf suite using the ARM\nv8.2 cores in the NVIDIA Jetson AGX Xavier board. These results show superior\nperformance compared with the well-spread TFLite from Google and slightly\ninferior results when compared with ArmNN, the native library from ARM for DNN\ninference.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 15:05:13 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Castell\u00f3", "Adri\u00e1n", ""], ["Barrachina", "Sergio", ""], ["Dolz", "Manuel F.", ""], ["Quintana-Ort\u00ed", "Enrique S.", ""], ["Juan", "Pau San", ""]]}, {"id": "2105.09282", "submitter": "Ganapati Bhat", "authors": "Aryan Deshwal, Syrine Belakaria, Ganapati Bhat, Janardhan Rao Doppa,\n  Partha Pratim Pande", "title": "Learning Pareto-Frontier Resource Management Policies for Heterogeneous\n  SoCs: An Information-Theoretic Approach", "comments": "To be published in proceedings DAC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile system-on-chips (SoCs) are growing in their complexity and\nheterogeneity (e.g., Arm's Big-Little architecture) to meet the needs of\nemerging applications, including games and artificial intelligence. This makes\nit very challenging to optimally manage the resources (e.g., controlling the\nnumber and frequency of different types of cores) at runtime to meet the\ndesired trade-offs among multiple objectives such as performance and energy.\nThis paper proposes a novel information-theoretic framework referred to as\nPaRMIS to create Pareto-optimal resource management policies for given target\napplications and design objectives. PaRMIS specifies parametric policies to\nmanage resources and learns statistical models from candidate policy evaluation\ndata in the form of target design objective values. The key idea is to select a\ncandidate policy for evaluation in each iteration guided by statistical models\nthat maximize the information gain about the true Pareto front. Experiments on\na commercial heterogeneous SoC show that PaRMIS achieves better Pareto fronts\nand is easily usable to optimize complex objectives (e.g., performance per\nWatt) when compared to prior methods.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 05:14:52 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Deshwal", "Aryan", ""], ["Belakaria", "Syrine", ""], ["Bhat", "Ganapati", ""], ["Doppa", "Janardhan Rao", ""], ["Pande", "Partha Pratim", ""]]}, {"id": "2105.09389", "submitter": "Shay Vargaftik", "authors": "Guy Goren, Shay Vargaftik, Yoram Moses", "title": "Stochastic Coordination in Heterogeneous Load Balancing Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current-day data centers and high-volume cloud services employ a broad set of\nheterogeneous servers. In such settings, client requests typically arrive at\nmultiple entry points, and dispatching them to servers is an urgent distributed\nsystems problem. This paper presents an efficient solution to the load\nbalancing problem in such systems that improves on and overcomes problems of\nprevious solutions. The load balancing problem is formulated as a stochastic\noptimization problem, and an efficient algorithmic solution is obtained based\non a subtle mathematical analysis of the problem. Finally, extensive evaluation\nof the solution on simulated data shows that it outperforms previous solutions.\nMoreover, the resulting dispatching policy can be computed very efficiently,\nmaking the solution practically viable.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 20:38:37 GMT"}, {"version": "v2", "created": "Sun, 25 Jul 2021 14:04:09 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Goren", "Guy", ""], ["Vargaftik", "Shay", ""], ["Moses", "Yoram", ""]]}, {"id": "2105.09508", "submitter": "Wentao Cai", "authors": "Wentao Cai, Haosen Wen, Vladimir Maksimovski, Mingzhe Du, Rafaello\n  Sanna, Shreif Abdallah, Michael L. Scott", "title": "Fast Nonblocking Persistence for Concurrent Data Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a fully lock-free variant of the recent Montage system for\npersistent data structures. Our variant, nbMontage, adds persistence to almost\nany nonblocking concurrent structure without introducing significant overhead\nor blocking of any kind. Like its predecessor, nbMontage is buffered durably\nlinearizable: it guarantees that the state recovered in the wake of a crash\nwill represent a consistent prefix of pre-crash execution. Unlike its\npredecessor, nbMontage ensures wait-free progress of the persistence frontier,\nthereby bounding the number of recent updates that may be lost on a crash, and\nallowing a thread to force an update of the frontier (i.e., to perform a sync\noperation) without the risk of blocking. As an extra benefit, the helping\nmechanism employed by our wait-free sync significantly reduces its latency.\n  Performance results for nonblocking queues, skip lists, trees, and hash\ntables rival custom data structures in the literature -- dramatically faster\nthan achieved with prior general-purpose systems, and generally within 50% of\nequivalent non-persistent structures placed in DRAM.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 04:24:58 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Cai", "Wentao", ""], ["Wen", "Haosen", ""], ["Maksimovski", "Vladimir", ""], ["Du", "Mingzhe", ""], ["Sanna", "Rafaello", ""], ["Abdallah", "Shreif", ""], ["Scott", "Michael L.", ""]]}, {"id": "2105.09578", "submitter": "Hongshi Tan", "authors": "Hongshi Tan, Xinyu Chen, Yao Chen, Bingsheng He, Weng-Fai Wong", "title": "ThundeRiNG: Generating Multiple Independent Random Number Sequences on\n  FPGAs", "comments": "Accepted to ICS 2021", "journal-ref": null, "doi": "10.1145/3447818.3461664", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose ThundeRiNG, a resource-efficient and\nhigh-throughput system for generating multiple independent sequences of random\nnumbers (MISRN) on FPGAs. Generating MISRN can be a time-consuming step in many\napplications such as numeric computation and approximate computing. Despite\nthat decades of studies on generating a single sequence of random numbers on\nFPGAs have achieved very high throughput and high quality of randomness,\nexisting MISRN approaches either suffer from heavy resource consumption or fail\nto achieve statistical independence among sequences. In contrast, ThundeRiNG\nresolves the dependence by using a resource-efficient decorrelator among\nmultiple sequences, guaranteeing a high statistical quality of randomness.\nMoreover, ThundeRiNG develops a novel state sharing among a massive number of\npseudo-random number generator instances on FPGAs. The experimental results\nshow that ThundeRiNG successfully passes the widely used statistical test,\nTestU01, only consumes a constant number of DSPs (less than 1\\% of the FPGA\nresource capacity) for generating any number of sequences, and achieves a\nthroughput of 655 billion random numbers per second. Compared to the\nstate-of-the-art GPU library, ThundeRiNG demonstrates a $10.62\\times$ speedup\non MISRN and delivers up to $9.15\\times$ performance and $26.63\\times$ power\nefficiency improvement on two applications ($\\pi$ estimation and Monte Carlo\noption pricing). This work is open-sourced on Github at\nhttps://github.com/Xtra-Computing/ThundeRiNG\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 08:09:57 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Tan", "Hongshi", ""], ["Chen", "Xinyu", ""], ["Chen", "Yao", ""], ["He", "Bingsheng", ""], ["Wong", "Weng-Fai", ""]]}, {"id": "2105.09642", "submitter": "Mohak Chadha", "authors": "Mohak Chadha, Michael Gerndt", "title": "Modelling DVFS and UFS for Region-Based Energy Aware Tuning of HPC\n  Applications", "comments": "IPDPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy efficiency and energy conservation are one of the most crucial\nconstraints for meeting the 20MW power envelope desired for exascale systems.\nTowards this, most of the research in this area has been focused on the\nutilization of user-controllable hardware switches such as per-core dynamic\nvoltage frequency scaling (DVFS) and software controlled clock modulation at\nthe application level. In this paper, we present a tuning plugin for the\nPeriscope Tuning Framework which integrates fine-grained autotuning at the\nregion level with DVFS and uncore frequency scaling (UFS). The tuning is based\non a feed-forward neural network which is formulated using Performance\nMonitoring Counters (PMC) supported by x86 systems and trained using\nstandardized benchmarks. Experiments on five standardized hybrid benchmarks\nshow an energy improvement of 16.1% on average when the applications are tuned\naccording to our methodology as compared to 7.8% for static tuning.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 10:18:37 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Chadha", "Mohak", ""], ["Gerndt", "Michael", ""]]}, {"id": "2105.09667", "submitter": "Sebastien Tixeuil", "authors": "Adam Heriban (NPA), S\\'ebastien Tixeuil (NPA, LINCS)", "title": "Unreliable Sensors for Reliable Efficient Robots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vast majority of existing Distributed Computing literature about mobile\nrobotic swarms considers computability issues: characterizing the set of system\nhypotheses that enables problem solvability. By contrast, the focus of this\nwork is to investigate complexity issues: obtaining quantitative results about\na given problem that admits solutions. Our quantitative measurements rely on a\nnewly developed simulation framework to benchmark pen and paper designs. First,\nwe consider the maximum traveled distance when gathering robots at a given\nlocation, not known beforehand (both in the two robots and in the n robots\nsettings) in the classical OBLOT model, for the FSYNC, SSYNC, and ASYNC\nschedulers. This particular metric appears relevant as it correlates closely to\nwhat would be real world fuel consumption. Then, we introduce the possibility\nof errors in the vision of robots, and assess the behavior of known rendezvous\n(aka two robots gathering) and leader election protocols when sensors are\nunreliable. We also introduce two new algorithms, one for fuel efficient\nconvergence, and one for leader election, that operate reliably despite\nunreliable sensors.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 10:55:22 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Heriban", "Adam", "", "NPA"], ["Tixeuil", "S\u00e9bastien", "", "NPA, LINCS"]]}, {"id": "2105.09756", "submitter": "Shimon Bitton", "authors": "Shimon Bitton, Yuval Emek, Taisuke Izumi, and Shay Kutten", "title": "Fully Adaptive Self-Stabilizing Transformer for LCL Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The first generic self-stabilizing transformer for local problems in a\nconstrained bandwidth model is introduced. This transformer can be applied to a\nwide class of locally checkable labeling (LCL) problems, converting a given\nfault free synchronous algorithm that satisfies certain conditions into a\nself-stabilizing synchronous algorithm for the same problem. The resulting\nself-stabilizing algorithms are anonymous, size-uniform, and \\emph{fully\nadaptive} in the sense that their time complexity is bounded as a function of\nthe number $k$ of nodes that suffered faults (possibly at different times)\nsince the last legal configuration. Specifically, for graphs whose degrees are\nup-bounded by $\\Delta$, the algorithms produced by the transformer stabilize in\ntime proportional to $\\log (k + \\Delta)$ in expectation, independently of the\nnumber of nodes in the graph (in some cases, the dependency on $\\Delta$ can\nalso be omitted). As such, the transformer is applicable also for infinite\ngraphs (with degree bound $\\Delta$). Another appealing feature of the\ntransformer is its small message size overhead. The transformer is applied to\nknown algorithms (or simple variants thereof) for some classic LCL problems,\nproducing the first anonymous size-uniform self-stabilizing algorithms for\nthese problems that are provably fully adaptive. From a technical point of\nview, the transformer's key design feature is a novel probabilistic tool that\nallows different nodes to act in synchrony even though their clocks may have\nbeen adversarially manipulated.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 14:02:28 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Bitton", "Shimon", ""], ["Emek", "Yuval", ""], ["Izumi", "Taisuke", ""], ["Kutten", "Shay", ""]]}, {"id": "2105.09837", "submitter": "Junxiang Wang", "authors": "Junxiang Wang, Hongyi Li, Zheng Chai, Yongchao Wang, Yue Cheng and\n  Liang Zhao", "title": "Towards Quantized Model Parallelism for Graph-Augmented MLPs Based on\n  Gradient-Free ADMM framework", "comments": "Junxiang Wang and Hongyi Li contribute equally to this work, and\n  Yongchao Wang and Liang Zhao are corresponding authors. This work is under\n  progress. arXiv admin note: substantial text overlap with arXiv:2009.02868", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Graph Augmented Multi-layer Perceptron (GA-MLP) model is an attractive\nalternative to Graph Neural Networks (GNNs). This is because it is resistant to\nthe over-smoothing problem, and deeper GA-MLP models yield better performance.\nGA-MLP models are traditionally optimized by the Stochastic Gradient Descent\n(SGD). However, SGD suffers from the layer dependency problem, which prevents\nthe gradients of different layers of GA-MLP models from being calculated in\nparallel. In this paper, we propose a parallel deep learning Alternating\nDirection Method of Multipliers (pdADMM) framework to achieve model\nparallelism: parameters in each layer of GA-MLP models can be updated in\nparallel. The extended pdADMM-Q algorithm reduces communication cost by\nutilizing the quantization technique. Theoretical convergence to a critical\npoint of the pdADMM algorithm and the pdADMM-Q algorithm is provided with a\nsublinear convergence rate $o(1/k)$. Extensive experiments in six benchmark\ndatasets demonstrate that the pdADMM can lead to high speedup, and outperforms\nall the existing state-of-the-art comparison methods.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 15:37:42 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Wang", "Junxiang", ""], ["Li", "Hongyi", ""], ["Chai", "Zheng", ""], ["Wang", "Yongchao", ""], ["Cheng", "Yue", ""], ["Zhao", "Liang", ""]]}, {"id": "2105.09894", "submitter": "Jayjeet Chakraborty", "authors": "Jayjeet Chakraborty, Ivo Jimenez, Sebastiaan Alvarez Rodriguez,\n  Alexandru Uta, Jeff LeFevre, Carlos Maltzahn", "title": "Towards an Arrow-native Storage System", "comments": "7 pages, 6 figures, workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the ever-increasing dataset sizes, several file formats like Parquet,\nORC, and Avro have been developed to store data efficiently and to save network\nand interconnect bandwidth at the price of additional CPU utilization. However,\nwith the advent of networks supporting 25-100 Gb/s and storage devices\ndelivering 1, 000, 000 reqs/sec the CPU has become the bottleneck, trying to\nkeep up feeding data in and out of these fast devices. The result is that data\naccess libraries executed on single clients are often CPU-bound and cannot\nutilize the scale-out benefits of distributed storage systems. One attractive\nsolution to this problem is to offload data-reducing processing and filtering\ntasks to the storage layer. However, modifying legacy storage systems to\nsupport compute offloading is often tedious and requires extensive\nunderstanding of the internals. Previous approaches re-implemented\nfunctionality of data processing frameworks and access library for a particular\nstorage system, a duplication of effort that might have to be repeated for\ndifferent storage systems. In this paper, we introduce a new design paradigm\nthat allows extending programmable object storage systems to embed existing,\nwidely used data processing frameworks and access libraries into the storage\nlayer with minimal modifications. In this approach data processing frameworks\nand access libraries can evolve independently from storage systems while\nleveraging the scale-out and availability properties of distributed storage\nsystems. We present one example implementation of our design paradigm using\nCeph, Apache Arrow, and Parquet. We provide a brief performance evaluation of\nour implementation and discuss key results.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 16:59:39 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 04:03:43 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Chakraborty", "Jayjeet", ""], ["Jimenez", "Ivo", ""], ["Rodriguez", "Sebastiaan Alvarez", ""], ["Uta", "Alexandru", ""], ["LeFevre", "Jeff", ""], ["Maltzahn", "Carlos", ""]]}, {"id": "2105.09926", "submitter": "Frederik Mallmann-Trenn", "authors": "Nan Kang, Frederik Mallmann-Trenn and Nicol\\'as Rivera", "title": "Diversity, Fairness, and Sustainability in Population Protocols", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the years, population protocols with the goal of reaching consensus have\nbeen studied in great depth. However, many systems in the real-world do not\nresult in all agents eventually reaching consensus, but rather in the opposite:\nthey converge to a state of rich diversity. Consider for example task\nallocation in ants. If eventually all ants perform the same task, then the\ncolony will perish (lack of food, no brood care, etc.). Then, it is vital for\nthe survival of the colony to have a diverse set of tasks and enough ants\nworking on each task. What complicates matters is that ants need to switch\ntasks periodically to adjust the needs of the colony; e.g., when too many\nforagers fell victim to other ant colonies. Moreover, all tasks are equally\nimportant and maybe they need to keep certain proportions in the distribution\nof the task. How can ants keep a healthy and balanced allocation of tasks?\n  To answer this question, we propose a simple population protocol for $n$\nagents on a complete graph and an arbitrary initial distribution of $k$ colours\n(tasks). We assume that each colour $i$ has an associated weight (importance)\n$w_i \\geq 1$. By denoting $w$ as the sum of the weights of different colours,\nwe show that the protocol converges in $O(w^2 n \\log n)$ rounds to a\nconfiguration where the number of agents supporting each colour $i$ is\nconcentrated on the fair share $w_in/w$ and will stay concentrated for a large\nnumber of rounds, w.h.p.\n  Our protocol has many interesting properties: agents do not need to know\nother colours and weights in the system, and our protocol requires very little\nmemory per agent. Furthermore, the protocol guarantees fairness meaning that\nover a long period each agent has each colour roughly a number of times\nproportional to the weight of the colour. Finally, our protocol also fulfils\nsustainability meaning that no colour ever vanishes.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 17:39:56 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 09:41:22 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Kang", "Nan", ""], ["Mallmann-Trenn", "Frederik", ""], ["Rivera", "Nicol\u00e1s", ""]]}, {"id": "2105.10056", "submitter": "Zhuangdi Zhu", "authors": "Zhuangdi Zhu, Junyuan Hong, Jiayu Zhou", "title": "Data-Free Knowledge Distillation for Heterogeneous Federated Learning", "comments": null, "journal-ref": "Proceedings of the 38th International Conference on Machine\n  Learning, PMLR 139, 2021", "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Federated Learning (FL) is a decentralized machine-learning paradigm, in\nwhich a global server iteratively averages the model parameters of local users\nwithout accessing their data. User heterogeneity has imposed significant\nchallenges to FL, which can incur drifted global models that are slow to\nconverge. Knowledge Distillation has recently emerged to tackle this issue, by\nrefining the server model using aggregated knowledge from heterogeneous users,\nother than directly averaging their model parameters. This approach, however,\ndepends on a proxy dataset, making it impractical unless such a prerequisite is\nsatisfied. Moreover, the ensemble knowledge is not fully utilized to guide\nlocal model learning, which may in turn affect the quality of the aggregated\nmodel. Inspired by the prior art, we propose a data-free knowledge\ndistillation} approach to address heterogeneous FL, where the server learns a\nlightweight generator to ensemble user information in a data-free manner, which\nis then broadcasted to users, regulating local training using the learned\nknowledge as an inductive bias. Empirical studies powered by theoretical\nimplications show that, our approach facilitates FL with better generalization\nperformance using fewer communication rounds, compared with the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 22:30:45 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 19:31:35 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Zhu", "Zhuangdi", ""], ["Hong", "Junyuan", ""], ["Zhou", "Jiayu", ""]]}, {"id": "2105.10312", "submitter": "Houssam-Eddine Zahaf", "authors": "Houssam-Eddine Zahaf, Ignacio Sanudo Olmedo, Jayati Singh, Nicola\n  Capodieci, Sebastien Faucou", "title": "Contention-Aware GPU Partitioning and Task-to-Partition Allocation for\n  Real-Time Workloads", "comments": "26 pages, 10 figures, RTNS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to satisfy timing constraints, modern real-time applications require\nmassively parallel accelerators such as General Purpose Graphic Processing\nUnits (GPGPUs). Generation after generation, the number of computing clusters\nmade available in novel GPU architectures is steadily increasing, hence,\ninvestigating suitable scheduling approaches is now mandatory. Such scheduling\napproaches are related to mapping different and concurrent compute kernels\nwithin the GPU computing clusters, hence grouping GPU computing clusters into\nschedulable partitions. In this paper we propose novel techniques to define GPU\npartitions; this allows us to define suitable task-to-partition allocation\nmechanisms in which tasks are GPU compute kernels featuring different timing\nrequirements. Such mechanisms will take into account the interference that GPU\nkernels experience when running in overlapping time windows. Hence, an\neffective and simple way to quantify the magnitude of such interference is also\npresented. We demonstrate the efficiency of the proposed approaches against the\nclassical techniques that considered the GPU as a single, non-partitionable\nresource.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 12:28:55 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Zahaf", "Houssam-Eddine", ""], ["Olmedo", "Ignacio Sanudo", ""], ["Singh", "Jayati", ""], ["Capodieci", "Nicola", ""], ["Faucou", "Sebastien", ""]]}, {"id": "2105.10326", "submitter": "Bashir Mohammed", "authors": "Divneet Kaur, Bashir Mohammed, Mariam Kiran", "title": "NetGraf: A Collaborative Network Monitoring Stack for Network\n  Experimental Testbeds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network performance monitoring collects heterogeneous data suchas network\nflow data to give an overview of network performance,and other metrics,\nnecessary for diagnosing and optimizing servicequality. However, due to\ndisparate and heterogeneity, to obtainmetrics and visualize entire data from\nseveral devices, engineershave to log into multiple dashboards.In this paper we\npresent NetGraf, a complete end-to-end networkmonitoring stack, that uses\nopen-source network monitoring toolsand collects, aggregates, and visualizes\nnetwork measurements on asingle easy-to-use real-time Grafana dashboard. We\ndevelop a novelNetGraf architecture and can deploy it on any network testbed\nsuchas Chameleon Cloud by single easy-to-use script for a full view ofnetwork\nperformance in one dashboard.This paper contributes to the theme of automating\nopen-sourcenetwork monitoring tools software setups and their usability\nforresearchers looking to deploy an end-to-end monitoring stack ontheir own\ntestbeds.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 20:38:55 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Kaur", "Divneet", ""], ["Mohammed", "Bashir", ""], ["Kiran", "Mariam", ""]]}, {"id": "2105.10332", "submitter": "Kyle Niemeyer", "authors": "Anthony S. Walker and Kyle E. Niemeyer", "title": "The Two-Dimensional Swept Rule Applied on Heterogeneous Architectures", "comments": "18 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The partial differential equations describing compressible fluid flows can be\nnotoriously difficult to resolve on a pragmatic scale and often require the use\nof high performance computing systems and/or accelerators. However, these\nsystems face scaling issues such as latency, the fixed cost of communicating\ninformation between devices in the system. The swept rule is a technique\ndesigned to minimize these costs by obtaining a solution to unsteady equations\nat as many possible spatial locations and times prior to communicating. In this\nstudy, we implemented and tested the swept rule for solving two-dimensional\nproblems on heterogeneous computing systems across two distinct systems. Our\nsolver showed a speedup range of 0.22-2.71 for the heat diffusion equation and\n0.52-1.46 for the compressible Euler equations. We can conclude from this study\nthat the swept rule offers both potential for speedups and slowdowns and that\ncare should be taken when designing such a solver to maximize benefits. These\nresults can help make decisions to maximize these benefits and inform designs.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 20:06:09 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Walker", "Anthony S.", ""], ["Niemeyer", "Kyle E.", ""]]}, {"id": "2105.10355", "submitter": "Julien Gedeon", "authors": "Julien Gedeon, Martin Wagner, Karolis Skaisgiris, Florian Brandherm,\n  Max M\\\"uhlh\\\"auser", "title": "Chameleons on Cloudlets: Elastic Edge Computing Through Microservice\n  Variants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Common deployment models for Edge Computing are based on (composable)\nmicroservices that are offloaded to cloudlets. Runtime adaptations-in response\nto varying load, QoS fulfillment, mobility, etc.-are typically based on\ncoarse-grained and costly management operations such as resource re-allocation\nor migration. The services themselves, however, remain non-adaptive, worsening\nthe already limited elasticity of Edge Computing compared to Cloud Computing.\nEdge computing applications often have stringent requirements on the execution\ntime but are flexible regarding the quality of a computation. The potential\nbenefits of exploiting this trade-off remain untapped. This paper introduces\nthe concept of adaptable microservices that provide alternative variants of\nspecific functionalities. We define so-called service variants that differ\nw.r.t. the internal functioning of the service, manifested in different\nalgorithms, parameters, and auxiliary data they use. Such variants allow\nfine-grained trade-offs between the QoS (e.g., a maximum tolerable execution\ntime) and the quality of the computation. We integrate adaptable microservices\ninto an Edge Computing framework, show the practical impact of service\nvariants, and present a strategy for switching variants at runtime.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 19:10:41 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Gedeon", "Julien", ""], ["Wagner", "Martin", ""], ["Skaisgiris", "Karolis", ""], ["Brandherm", "Florian", ""], ["M\u00fchlh\u00e4user", "Max", ""]]}, {"id": "2105.10384", "submitter": "Leonid Sokolinsky", "authors": "Leonid B. Sokolinsky and Irina M. Sokolinskaya", "title": "FRaGenLP: A Generator of Random Linear Programming Problems for Cluster\n  Computing Systems", "comments": "Submitted to \"Communications in Computer and Information Science\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The article presents and evaluates a scalable FRaGenLP algorithm for\ngenerating random linear programming problems of large dimension $n$ on cluster\ncomputing systems. To ensure the consistency of the problem and the boundedness\nof the feasible region, the constraint system includes $2n+1$ standard\ninequalities, called support inequalities. New random inequalities are\ngenerated and added to the system in a manner that ensures the consistency of\nthe constraints. Furthermore, the algorithm uses two likeness metrics to\nprevent the addition of a new random inequality that is similar to one already\npresent in the constraint system. The algorithm also rejects random\ninequalities that cannot affect the solution of the linear programming problem\nbounded by the support inequalities. The parallel implementation of the\nFRaGenLP algorithm is performed in C++ through the parallel BSF-skeleton, which\nencapsulates all aspects related to the MPI-based parallelization of the\nprogram. We provide the results of large-scale computational experiments on a\ncluster computing system to study the scalability of the FRaGenLP algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 14:55:24 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Sokolinsky", "Leonid B.", ""], ["Sokolinskaya", "Irina M.", ""]]}, {"id": "2105.10397", "submitter": "R\\'emi Dulong", "authors": "R\\'emi Dulong, Rafael Pires, Andreia Correia, Valerio Schiavoni, Pedro\n  Ramalhete, Pascal Felber, Ga\\\"el Thomas", "title": "NVCache: A Plug-and-Play NVMM-based I/O Booster for Legacy Systems", "comments": "13 pages, 7 figures, to be published in the 51th IEEE/IFIP\n  International Conference on Dependable Systems and Networks (DSN 21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.OS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces NVCache, an approach that uses a non-volatile main\nmemory (NVMM) as a write cache to improve the write performance of legacy\napplications. We compare NVCache against file systems tailored for NVMM\n(Ext4-DAX and NOVA) and with I/O-heavy applications (SQLite, RocksDB). Our\nevaluation shows that NVCache reaches the performance level of the existing\nstate-of-the-art systems for NVMM, but without their limitations: NVCache does\nnot limit the size of the stored data to the size of the NVMM, and works\ntransparently with unmodified legacy applications, providing additional\npersistence guarantees even when their source code is not available.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 14:08:10 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Dulong", "R\u00e9mi", ""], ["Pires", "Rafael", ""], ["Correia", "Andreia", ""], ["Schiavoni", "Valerio", ""], ["Ramalhete", "Pedro", ""], ["Felber", "Pascal", ""], ["Thomas", "Ga\u00ebl", ""]]}, {"id": "2105.10399", "submitter": "Joshua Ellul", "authors": "Joshua Ellul and Gordon J. Pace", "title": "Towards External Calls for Blockchain and Distributed Ledger Technology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is widely accepted that blockchain systems cannot execute calls to\nexternal systems or services due to each node having to reach a deterministic\nstate. However, in this paper we show that this belief is preconceived by\ndemonstrating a method that enables blockchain and distributed ledger\ntechnologies to perform calls to external systems initiated from the\nblockchain/DLT itself.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 13:39:29 GMT"}, {"version": "v2", "created": "Mon, 24 May 2021 20:09:03 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Ellul", "Joshua", ""], ["Pace", "Gordon J.", ""]]}, {"id": "2105.10464", "submitter": "David Cerezo S\\'anchez", "authors": "David Cerezo S\\'anchez", "title": "Pravuil: Global Consensus for a United World", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC econ.GN q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pravuil is a robust, secure, and scalable consensus protocol for a\npermissionless blockchain suitable for deployment in an adversarial environment\nsuch as the Internet. Pravuil circumvents previous shortcomings of other\nblockchains:\n  - Bitcoin's limited adoption problem: as transaction demand grows, payment\nconfirmation times grow much lower than other PoW blockchains\n  - higher transaction security at a lower cost\n  - more decentralisation than other permissionless blockchains\n  - impossibility of full decentralisation and the blockchain scalability\ntrilemma: decentralisation, scalability, and security can be achieved\nsimultaneously\n  - Sybil-resistance for free implementing the social optimum\n  - Pravuil goes beyond the economic limits of Bitcoin or other PoW/PoS\nblockchains, leading to a more valuable and stable crypto-currency\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 17:02:14 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["S\u00e1nchez", "David Cerezo", ""]]}, {"id": "2105.10486", "submitter": "Matthew Andres Moreno", "authors": "Matthew Andres Moreno, Santiago Rodriguez Papa, Charles Ofria", "title": "Conduit: A C++ Library for Best-effort High Performance Computing", "comments": null, "journal-ref": null, "doi": "10.1145/3449726.3463205", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing software to effectively take advantage of growth in parallel and\ndistributed processing capacity poses significant challenges. Traditional\nprogramming techniques allow a user to assume that execution, message passing,\nand memory are always kept synchronized. However, maintaining this consistency\nbecomes increasingly costly at scale. One proposed strategy is \"best-effort\ncomputing\", which relaxes synchronization and hardware reliability\nrequirements, accepting nondeterminism in exchange for efficiency. Although\nmany programming languages and frameworks aim to facilitate software\ndevelopment for high performance applications, existing tools do not directly\nprovide a prepackaged best-effort interface. The Conduit C++ Library aims to\nprovide such an interface for convenient implementation of software that uses\nbest-effort inter-thread and inter-process communication. Here, we describe the\nmotivation, objectives, design, and implementation of the library. Benchmarks\non a communication-intensive graph coloring problem and a compute-intensive\ndigital evolution simulation show that Conduit's best-effort model can improve\nscaling efficiency and solution quality, particularly in a distributed,\nmulti-node context.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 17:49:12 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Moreno", "Matthew Andres", ""], ["Papa", "Santiago Rodriguez", ""], ["Ofria", "Charles", ""]]}, {"id": "2105.10566", "submitter": "Kartik Nayak", "authors": "Naama Ben-David and Kartik Nayak", "title": "Classifying Trusted Hardware via Unidirectional Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that Byzantine fault tolerant (BFT) consensus cannot be\nsolved in the classic asynchronous message passing model when one-third or more\nof the processes may be faulty. Since many modern applications require higher\nfault tolerance, this bound has been circumvented by introducing\nnon-equivocation mechanisms that prevent Byzantine processes from sending\nconflicting messages to other processes. The use of trusted hardware is a way\nto implement non-equivocation.\n  Several different trusted hardware modules have been considered in the\nliterature. In this paper, we study whether all trusted hardware modules are\nequivalent in the power that they provide to a system. We show that while they\ndo all prevent equivocation, we can partition trusted hardware modules into two\ndifferent power classes; those that employ shared memory primitives, and those\nthat do not. We separate these classes using a new notion we call\nunidirectionality, which describes a useful guarantee on the ability of\nprocesses to prevent network partitions. We show that shared-memory based\nhardware primitives provide unidirectionality, while others do not.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 20:46:06 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Ben-David", "Naama", ""], ["Nayak", "Kartik", ""]]}, {"id": "2105.10605", "submitter": "Jayson Boubin", "authors": "Jayson Boubin, Codi Burley, Peida Han, Bowen Li, Barry Porter,\n  Christopher Stewart", "title": "Programming and Deployment of Autonomous Swarms using Multi-Agent\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Autonomous systems (AS) carry out complex missions by continuously observing\nthe state of their surroundings and taking actions toward a goal. Swarms of AS\nworking together can complete missions faster and more effectively than single\nAS alone. To build swarms today, developers handcraft their own software for\nstoring, aggregating, and learning from observations. We present the Fleet\nComputer, a platform for developing and managing swarms. The Fleet Computer\nprovides a programming paradigm that simplifies multi-agent reinforcement\nlearning (MARL) -- an emerging class of algorithms that coordinate swarms of\nagents. Using just two programmer-provided functions Map() and Eval(), the\nFleet Computer compiles and deploys swarms and continuously updates the\nreinforcement learning models that govern actions. To conserve compute\nresources, the Fleet Computer gives priority scheduling to models that\ncontribute to effective actions, drawing a novel link between online learning\nand resource management. We developed swarms for unmanned aerial vehicles (UAV)\nin agriculture and for video analytics on urban traffic. Compared to individual\nAS, our swarms achieved speedup of 4.4X using 4 UAV and 62X using 130 video\ncameras. Compared to a competing approach for building swarms that is widely\nused in practice, our swarms were 3X more effective, using 3.9X less energy.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 23:22:43 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Boubin", "Jayson", ""], ["Burley", "Codi", ""], ["Han", "Peida", ""], ["Li", "Bowen", ""], ["Porter", "Barry", ""], ["Stewart", "Christopher", ""]]}, {"id": "2105.10680", "submitter": "Mark Asch", "authors": "Mark Asch, Fran\\c{c}ois Bodin, Micah Beck, Terry Moore, Michela\n  Taufer, Martin Swany and Jean-Pierre Vilotte", "title": "Cybercosm: New Foundations for a Converged Science Data Ecosystem", "comments": "Updated author list", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Scientific communities naturally tend to organize around data ecosystems\ncreated by the combination of their observational devices, their data\nrepositories, and the workflows essential to carry their research from\nobservation to discovery. However, these legacy data ecosystems are now\nbreaking down under the pressure of the exponential growth in the volume and\nvelocity of these workflows, which are further complicated by the need to\nintegrate the highly data intensive methods of the Artificial Intelligence\nrevolution. Enabling ground breaking science that makes full use of this new,\ndata saturated research environment will require distributed systems that\nsupport dramatically improved resource sharing, workflow portability and\ncomposability, and data ecosystem convergence.\n  The Cybercosm vision presented in this white paper describes a radically\ndifferent approach to the architecture of distributed systems for\ndata-intensive science and its application workflows. As opposed to traditional\nmodels that restrict interoperability by hiving off storage, networking, and\ncomputing resources in separate technology silos, Cybercosm defines a minimally\nsufficient hypervisor as a spanning layer for its data plane that virtualizes\nand converges the local resources of the system's nodes in a fully\ninteroperable manner. By building on a common, universal interface into which\nthe problems that infect today's data-intensive workflows can be decomposed and\nattacked, Cybercosm aims to support scalable, portable and composable workflows\nthat span and merge the distributed data ecosystems that characterize leading\nedge research communities today.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 10:24:01 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 14:59:14 GMT"}, {"version": "v3", "created": "Tue, 29 Jun 2021 06:20:34 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Asch", "Mark", ""], ["Bodin", "Fran\u00e7ois", ""], ["Beck", "Micah", ""], ["Moore", "Terry", ""], ["Taufer", "Michela", ""], ["Swany", "Martin", ""], ["Vilotte", "Jean-Pierre", ""]]}, {"id": "2105.10699", "submitter": "Yulin Shao", "authors": "Yulin Shao and Soung Chang Liew and Deniz Gunduz", "title": "Denoising Noisy Neural Networks: A Bayesian Approach with Compensation", "comments": "Keywords: Noisy neural network, Bayesian estimation, analog device,\n  federated edge learning, over-the-air computation, analog storage", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC cs.IT eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noisy neural networks (NoisyNNs) refer to the inference and training of NNs\nin the presence of noise. Noise is inherent in most communication and storage\nsystems; hence, NoisyNNs emerge in many new applications, including federated\nedge learning, where wireless devices collaboratively train a NN over a noisy\nwireless channel, or when NNs are implemented/stored in an analog storage\nmedium. This paper studies a fundamental problem of NoisyNNs: how to estimate\nthe uncontaminated NN weights from their noisy observations or manifestations.\nWhereas all prior works relied on the maximum likelihood (ML) estimation to\nmaximize the likelihood function of the estimated NN weights, this paper\ndemonstrates that the ML estimator is in general suboptimal. To overcome the\nsuboptimality of the conventional ML estimator, we put forth an\n$\\text{MMSE}_{pb}$ estimator to minimize a compensated mean squared error (MSE)\nwith a population compensator and a bias compensator. Our approach works well\nfor NoisyNNs arising in both 1) noisy inference, where noise is introduced only\nin the inference phase on the already-trained NN weights; and 2) noisy\ntraining, where noise is introduced over the course of training. Extensive\nexperiments on the CIFAR-10 and SST-2 datasets with different NN architectures\nverify the significant performance gains of the $\\text{MMSE}_{pb}$ estimator\nover the ML estimator when used to denoise the NoisyNN. For noisy inference,\nthe average gains are up to $156\\%$ for a noisy ResNet34 model and $14.7\\%$ for\na noisy BERT model; for noisy training, the average gains are up to $18.1$ dB\nfor a noisy ResNet18 model.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 11:51:20 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Shao", "Yulin", ""], ["Liew", "Soung Chang", ""], ["Gunduz", "Deniz", ""]]}, {"id": "2105.10726", "submitter": "B\\'erenger Bramas", "authors": "Garip Kusoglu, Berenger Bramas, Stephane Genaud", "title": "Automatic task-based parallelization of C++ applications by\n  source-to-source transformations", "comments": "Published at Compas 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Currently, multi/many-core CPUs are considered standard in most types of\ncomputers including, mobile phones, PCs or supercomputers. However, the\nparallelization of applications as well as refactoring/design of applications\nfor efficient hardware usage remains restricted to experts who have advanced\ntechnical knowledge and who can invest time tuning their software. In this\ncontext, the compilation community has proposed different methods for automatic\nparallelization, but their focus is traditionally on loops and nested loops\nwith the support of polyhedral techniques. In this study, we propose a new\napproach to transform sequential C++ source code into a task-based parallel one\nby inserting annotations. We explain the different mechanisms we used to create\ntasks at each function/method call, and how we can limit the number of tasks.\nOur method can be implemented on top of the OpenMP 4.0 standard. It is\ncompiler-independent and can rely on external well-optimized OpenMP libraries.\nFinally, we provide preliminary performance results that illustrate the\npotential of our method.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 13:27:09 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Kusoglu", "Garip", ""], ["Bramas", "Berenger", ""], ["Genaud", "Stephane", ""]]}, {"id": "2105.10798", "submitter": "Alexander Brandt", "authors": "Alexander Brandt and Marc Moreno Maza", "title": "On the Complexity and Parallel Implementation of Hensel's Lemma and\n  Weierstrass Preparation", "comments": "21 pages, 3 figures, submitted to Computer Algebra in Scientific\n  Computing CASC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SC cs.DC cs.MS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Hensel's lemma, combined with repeated applications of Weierstrass\npreparation theorem, allows for the factorization of polynomials with\nmultivariate power series coefficients. We present a complexity analysis for\nthis method and leverage those results to guide the load-balancing of a\nparallel implementation to concurrently update all factors. In particular, the\nfactorization creates a pipeline where the terms of degree k of the first\nfactor are computed simultaneously with the terms of degree k-1 of the second\nfactor, etc. An implementation challenge is the inherent irregularity of\ncomputational work between factors, as our complexity analysis reveals.\nAdditional resource utilization and load-balancing is achieved through the\nparallelization of Weierstrass preparation. Experimental results show the\nefficacy of this mixed parallel scheme, achieving up to 9x parallel speedup on\n12 cores.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 19:26:52 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 18:50:05 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Brandt", "Alexander", ""], ["Maza", "Marc Moreno", ""]]}, {"id": "2105.11013", "submitter": "Emna Baccour", "authors": "Mohammed Jouhari, Abdulla Al-Ali, Emna Baccour, Amr Mohamed, Aiman\n  Erbad, Mohsen Guizani, Mounir Hamdi", "title": "Distributed CNN Inference on Resource-Constrained UAVs for Surveillance\n  Systems: Design and Optimization", "comments": "Accepted in IEEE Internet of Things Journal", "journal-ref": null, "doi": "10.1109/JIOT.2021.3079164", "report-no": null, "categories": "cs.DC cs.LG cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unmanned Aerial Vehicles (UAVs) have attracted great interest in the last few\nyears owing to their ability to cover large areas and access difficult and\nhazardous target zones, which is not the case of traditional systems relying on\ndirect observations obtained from fixed cameras and sensors. Furthermore,\nthanks to the advancements in computer vision and machine learning, UAVs are\nbeing adopted for a broad range of solutions and applications. However, Deep\nNeural Networks (DNNs) are progressing toward deeper and complex models that\nprevent them from being executed on-board. In this paper, we propose a DNN\ndistribution methodology within UAVs to enable data classification in\nresource-constrained devices and avoid extra delays introduced by the\nserver-based solutions due to data communication over air-to-ground links. The\nproposed method is formulated as an optimization problem that aims to minimize\nthe latency between data collection and decision-making while considering the\nmobility model and the resource constraints of the UAVs as part of the\nair-to-air communication. We also introduce the mobility prediction to adapt\nour system to the dynamics of UAVs and the network variation. The simulation\nconducted to evaluate the performance and benchmark the proposed methods,\nnamely Optimal UAV-based Layer Distribution (OULD) and OULD with Mobility\nPrediction (OULD-MP), were run in an HPC cluster. The obtained results show\nthat our optimization solution outperforms the existing and heuristic-based\napproaches.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 20:19:43 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Jouhari", "Mohammed", ""], ["Al-Ali", "Abdulla", ""], ["Baccour", "Emna", ""], ["Mohamed", "Amr", ""], ["Erbad", "Aiman", ""], ["Guizani", "Mohsen", ""], ["Hamdi", "Mounir", ""]]}, {"id": "2105.11028", "submitter": "Milad Khademinori", "authors": "Milad Khademi Nori, Sangseok Yun, and Il-Min Kim", "title": "Fast Federated Learning by Balancing Communication Trade-Offs", "comments": "14 pages, 24 figures, accepted for publication in IEEE Transactions\n  on Communications", "journal-ref": null, "doi": "10.1109/TCOMM.2021.3083316", "report-no": null, "categories": "cs.LG cs.AI cs.DC cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Federated Learning (FL) has recently received a lot of attention for\nlarge-scale privacy-preserving machine learning. However, high communication\noverheads due to frequent gradient transmissions decelerate FL. To mitigate the\ncommunication overheads, two main techniques have been studied: (i) local\nupdate of weights characterizing the trade-off between communication and\ncomputation and (ii) gradient compression characterizing the trade-off between\ncommunication and precision. To the best of our knowledge, studying and\nbalancing those two trade-offs jointly and dynamically while considering their\nimpacts on convergence has remained unresolved even though it promises\nsignificantly faster FL. In this paper, we first formulate our problem to\nminimize learning error with respect to two variables: local update\ncoefficients and sparsity budgets of gradient compression who characterize\ntrade-offs between communication and computation/precision, respectively. We\nthen derive an upper bound of the learning error in a given wall-clock time\nconsidering the interdependency between the two variables. Based on this\ntheoretical analysis, we propose an enhanced FL scheme, namely Fast FL (FFL),\nthat jointly and dynamically adjusts the two variables to minimize the learning\nerror. We demonstrate that FFL consistently achieves higher accuracies faster\nthan similar schemes existing in the literature.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 21:55:14 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Nori", "Milad Khademi", ""], ["Yun", "Sangseok", ""], ["Kim", "Il-Min", ""]]}, {"id": "2105.11033", "submitter": "Zahra Najafabadi", "authors": "Zahra Najafabadi Samani, Nishant Saurabh, Radu Prodan", "title": "Multilayer Resource-aware Partitioning for Fog Application Placement", "comments": "13 figures", "journal-ref": null, "doi": "10.1109/ICFEC51620.2021.00010", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fog computing emerged as a crucial platform for the deployment of IoT\napplications. The complexity of such applications requires methods that handle\nthe resource diversity and network structure of Fog devices while maximizing\nthe service placement and reducing resource wastage. Prior studies in this\ndomain primarily focused on optimizing specific application requirements and\nfail to address the network topology combined with the different types of\nresources encountered in Fog devices. To overcome these problems, we propose a\nmultilayer resource-aware partitioning method to minimize the resource wastage\nand maximize the service placement and deadline satisfaction rates in a Fog\ninfrastructure with high multi-user application placement requests. Our method\nrepresents the heterogeneous Fog resources as a multilayered network graph and\npartitions them based on network topology and resource features. Afterward, it\nidentifies the appropriate device partitions for placing an application\naccording to its requirements, which need to overlap in the same network\ntopology partition. Simulation results show that our multilayer resource-aware\npartitioning method is able to place twice as many services, satisfy deadlines\nfor three times as many application requests, and reduce the resource wastage\nby up to 15-32 times compared to two availability-aware and resource-aware\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 22:40:37 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Samani", "Zahra Najafabadi", ""], ["Saurabh", "Nishant", ""], ["Prodan", "Radu", ""]]}, {"id": "2105.11064", "submitter": "Saeed Taheri", "authors": "Saeed Taheri, Ganesh Gopalakrishnan", "title": "Automated Dynamic Concurrency Analysis for Go", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The concurrency features of the Go language have proven versatile in the\ndevelopment of a number of concurrency systems. However, correctness methods to\naddress challenges in Go concurrency debugging have not received much\nattention. In this work, we present an automatic dynamic tracing mechanism that\nefficiently captures and helps analyze the whole-program concurrency model.\nUsing an enhancement to the built-in tracer package of Go and a framework that\ncollects dynamic traces from application execution, we enable thorough\npost-mortem analysis for concurrency debugging. Preliminary results about the\neffectiveness and scalability (up to more than 2K goroutines) of our proposed\ndynamic tracing for concurrent debugging are presented. We discuss the future\ndirection for exploiting dynamic tracing towards accelerating concurrent bug\nexposure.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 02:12:05 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Taheri", "Saeed", ""], ["Gopalakrishnan", "Ganesh", ""]]}, {"id": "2105.11085", "submitter": "Haijin Wang", "authors": "Haijin Wang, Caomingzhe Si, Junhua Zhao, Guolong Liu, Fushuan Wen", "title": "Fed-NILM: A Federated Learning-based Non-Intrusive Load Monitoring\n  Method for Privacy-Protection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-intrusive load monitoring (NILM) is essential for understanding\ncustomer's power consumption patterns and may find wide applications like\ncarbon emission reduction and energy conservation. The training of NILM models\nrequires massive load data containing different types of appliances. However,\ninadequate load data and the risk of power consumer privacy breaches may be\nencountered by local data owners during the NILM model training. To prevent\nsuch potential risks, a novel NILM method named Fed-NILM which is based on\nFederated Learning (FL) is proposed in this paper. In Fed-NILM, local model\nparameters instead of local load data are shared among multiple data owners.\nThe global model is obtained by weighted averaging the parameters. Experiments\nbased on two measured load datasets are conducted to explore the generalization\nability of Fed-NILM. Besides, a comparison of Fed-NILM with locally-trained\nNILMs and the centrally-trained NILM is conducted. The experimental results\nshow that Fed-NILM has superior performance in scalability and convergence.\nFed-NILM outperforms locally-trained NILMs operated by local data owners and\napproximates the centrally-trained NILM which is trained on the entire load\ndataset without privacy protection. The proposed Fed-NILM significantly\nimproves the co-modeling capabilities of local data owners while protecting\npower consumers' privacy.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 04:12:10 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 13:04:03 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Wang", "Haijin", ""], ["Si", "Caomingzhe", ""], ["Zhao", "Junhua", ""], ["Liu", "Guolong", ""], ["Wen", "Fushuan", ""]]}, {"id": "2105.11099", "submitter": "Huanding Zhang", "authors": "Huanding Zhang, Tao Shen, Fei Wu, Mingyang Yin, Hongxia Yang, Chao Wu", "title": "Federated Graph Learning -- A Position Paper", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph neural networks (GNN) have been successful in many fields, and derived\nvarious researches and applications in real industries. However, in some\nprivacy sensitive scenarios (like finance, healthcare), training a GNN model\ncentrally faces challenges due to the distributed data silos. Federated\nlearning (FL) is a an emerging technique that can collaboratively train a\nshared model while keeping the data decentralized, which is a rational solution\nfor distributed GNN training. We term it as federated graph learning (FGL).\nAlthough FGL has received increasing attention recently, the definition and\nchallenges of FGL is still up in the air. In this position paper, we present a\ncategorization to clarify it. Considering how graph data are distributed among\nclients, we propose four types of FGL: inter-graph FL, intra-graph FL and\ngraph-structured FL, where intra-graph is further divided into horizontal and\nvertical FGL. For each type of FGL, we make a detailed discussion about the\nformulation and applications, and propose some potential challenges.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 05:39:24 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Zhang", "Huanding", ""], ["Shen", "Tao", ""], ["Wu", "Fei", ""], ["Yin", "Mingyang", ""], ["Yang", "Hongxia", ""], ["Wu", "Chao", ""]]}, {"id": "2105.11118", "submitter": "John Thorpe", "authors": "John Thorpe, Yifan Qiao, Jonathan Eyolfson, Shen Teng, Guanzhou Hu,\n  Zhihao Jia, Jinliang Wei, Keval Vora, Ravi Netravali, Miryung Kim, Guoqing\n  Harry Xu", "title": "Dorylus: Affordable, Scalable, and Accurate GNN Training with\n  Distributed CPU Servers and Serverless Threads", "comments": "Paper accepted in OSDI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph neural network (GNN) enables deep learning on structured graph data.\nThere are two major GNN training obstacles: 1) it relies on high-end servers\nwith many GPUs which are expensive to purchase and maintain, and 2) limited\nmemory on GPUs cannot scale to today's billion-edge graphs. This paper presents\nDorylus: a distributed system for training GNNs. Uniquely, Dorylus can take\nadvantage of serverless computing to increase scalability at a low cost.\n  The key insight guiding our design is computation separation. Computation\nseparation makes it possible to construct a deep, bounded-asynchronous pipeline\nwhere graph and tensor parallel tasks can fully overlap, effectively hiding the\nnetwork latency incurred by Lambdas. With the help of thousands of Lambda\nthreads, Dorylus scales GNN training to billion-edge graphs. Currently, for\nlarge graphs, CPU servers offer the best performance-per-dollar over GPU\nservers. Just using Lambdas on top of CPU servers offers up to 2.75x more\nperformance-per-dollar than training only with CPU servers. Concretely, Dorylus\nis 1.22x faster and 4.83x cheaper than GPU servers for massive sparse graphs.\nDorylus is up to 3.8x faster and 10.7x cheaper compared to existing\nsampling-based systems.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 06:49:08 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 01:14:05 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Thorpe", "John", ""], ["Qiao", "Yifan", ""], ["Eyolfson", "Jonathan", ""], ["Teng", "Shen", ""], ["Hu", "Guanzhou", ""], ["Jia", "Zhihao", ""], ["Wei", "Jinliang", ""], ["Vora", "Keval", ""], ["Netravali", "Ravi", ""], ["Kim", "Miryung", ""], ["Xu", "Guoqing Harry", ""]]}, {"id": "2105.11220", "submitter": "Kissami Imad", "authors": "Imad Kissami, Christophe Cerin, Fayssal Benkhaldoun, Gilles Scarella", "title": "Towards Parallel CFD computation for the ADAPT framework", "comments": null, "journal-ref": "In: Carretero J., Garcia-Blas J., Ko R., Mueller P., Nakano K.\n  (eds) Algorithms and Architectures for Parallel Processing. ICA3PP 2016.\n  Lecture Notes in Computer Science, vol 10048. Springer, Cham", "doi": "10.1007/978-3-319-49583-5_28", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In order to run Computational Fluid Dynamics (CFD) codes on large scale\ninfrastructures, parallel computing has to be used because of the computational\nintensive nature of the problems. In this paper we investigate the ADAPT\nplatform where we couple flow Partial Differential Equations and a Poisson\nequation. This leads to a linear system which we solve using direct methods.\nThe implementation deals with the MUMPS parallel multi-frontal direct solver\nand mesh partitioning methods using METIS to improve the performance of the\nframework. We also investigate, in this paper, how the mesh partitioning\nmethods are able to optimize the mesh cell distribution for the ADAPT solver.\nThe experience gained in this paper facilitates the move to the 3D version of\nADAPT and the move to a Service Oriented view of ADAPT as future work.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 11:57:58 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Kissami", "Imad", ""], ["Cerin", "Christophe", ""], ["Benkhaldoun", "Fayssal", ""], ["Scarella", "Gilles", ""]]}, {"id": "2105.11229", "submitter": "Ao Wang", "authors": "Ao Wang, Shuai Chang, Huangshi Tian, Hongqi Wang, Haoran Yang, Huiba\n  Li, Rui Du, Yue Cheng", "title": "FaaSNet: Scalable and Fast Provisioning of Custom Serverless Container\n  Runtimes at Alibaba Cloud Function Compute", "comments": "This is the preprint version of a paper published in USENIX ATC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Serverless computing, or Function-as-a-Service (FaaS), enables a new way of\nbuilding and scaling applications by allowing users to deploy fine-grained\nfunctions while providing fully-managed resource provisioning and auto-scaling.\nCustom FaaS container support is gaining traction as it enables better control\nover OSes, versioning, and tooling for modernizing FaaS applications. However,\nproviding rapid container provisioning introduces non-trivial challenges for\nFaaS providers, since container provisioning is costly, and real-world FaaS\nworkloads exhibit highly dynamic patterns. In this paper, we design FaaSNet, a\nhighly-scalable middleware system for accelerating FaaS container provisioning.\nFaaSNet is driven by the workload and infrastructure requirements of the FaaS\nplatform at one of the world's largest cloud providers, Alibaba Cloud Function\nCompute. FaaSNet enables scalable container provisioning via a lightweight,\nadaptive function tree (FT) structure. FaaSNet uses an I/O efficient, on-demand\nfetching mechanism to further reduce provisioning costs at scale. We implement\nand integrate FaaSNet in Alibaba Cloud Function Compute. Evaluation results\nshow that FaaSNet: (1) finishes provisioning 2500 function containers on 1000\nvirtual machines in 8.3 seconds, (2) scales 13.4x and 16.3x faster than Alibaba\nCloud's current FaaS platform and a state-of-the-art P2P container registry\n(Kraken), respectively, and (3) sustains a bursty workload using 75.2% less\ntime than an optimized baseline.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 12:08:24 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 07:42:58 GMT"}, {"version": "v3", "created": "Thu, 15 Jul 2021 02:57:32 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Wang", "Ao", ""], ["Chang", "Shuai", ""], ["Tian", "Huangshi", ""], ["Wang", "Hongqi", ""], ["Yang", "Haoran", ""], ["Li", "Huiba", ""], ["Du", "Rui", ""], ["Cheng", "Yue", ""]]}, {"id": "2105.11236", "submitter": "Kissami Imad", "authors": "Imad Kissami, Souhail Maazioui, Fayssal Benkhaldoun", "title": "Parallel adaptive procedure for CFD simulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present paper describes a parallel adaptive procedure in CFD solvers. The\ncode dynamically refines and coarses the discretization for accurately\nresolution of the different features regarding the electron density. Our\npurpose is to examine the performance of A new parallel adaptive procedure\nintroduced on the ADAPT platform, which resolve of a relatively complicated\nsystem coupling the flow partial differential equations to the Poissons\nequation. The implementation deals with the MUMPS parallel multi-frontal direct\nsolver and mesh partitioning methods using METIS to improve the performance of\nthe framework. The standard MPI is used to establish communication between\nprocessors. Performance analysis of the parallel adaptive procedure shows the\nefficiency and the potential of the method for the transport equation coupled\nwith Poissons equation and for the propagation equations of ionization waves.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 12:20:14 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Kissami", "Imad", ""], ["Maazioui", "Souhail", ""], ["Benkhaldoun", "Fayssal", ""]]}, {"id": "2105.11303", "submitter": "Kissami Imad", "authors": "Fayssal Benkhaldoun, Christophe C\\'erin, Imad Kissami, Walid Saad", "title": "Challenges of Translating HPC codes to Workflows for Heterogeneous and\n  Dynamic Environments", "comments": null, "journal-ref": null, "doi": "10.1109/HPCS.2017.130", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we would like to share our experience for transforming a\nparallel code for a Computational Fluid Dynamics (CFD) problem into a parallel\nversion for the RedisDG workflow engine. This system is able to capture\nheterogeneous and highly dynamic environments, thanks to opportunistic\nscheduling strategies. We show how to move to the field of \"HPC as a Service\"\nin order to use heterogeneous platforms. We mainly explain, through the CFD use\ncase, how to transform the parallel code and we exhibit challenges to 'unfold'\nthe task graph dynamically in order to improve the overall performance (in a\nbroad sense) of the workflow engine. We discuss in particular of the impact on\nthe workflow engine of such dynamic feature. This paper states that new models\nfor High Performance Computing are possible, under the condition we revisit our\nmind in the direction of the potential of new paradigms such as cloud, edge\ncomputing.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 14:36:38 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Benkhaldoun", "Fayssal", ""], ["C\u00e9rin", "Christophe", ""], ["Kissami", "Imad", ""], ["Saad", "Walid", ""]]}, {"id": "2105.11367", "submitter": "Fan Lai", "authors": "Fan Lai, Yinwei Dai, Xiangfeng Zhu, Mosharaf Chowdhury", "title": "FedScale: Benchmarking Model and System Performance of Federated\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present FedScale, a diverse set of challenging and realistic benchmark\ndatasets to facilitate scalable, comprehensive, and reproducible federated\nlearning (FL) research. FedScale datasets are large-scale, encompassing a\ndiverse range of important FL tasks, such as image classification, object\ndetection, language modeling, speech recognition, and reinforcement learning.\nFor each dataset, we provide a unified evaluation protocol using realistic data\nsplits and evaluation metrics. To meet the pressing need for reproducing\nrealistic FL at scale, we have also built an efficient evaluation platform to\nsimplify and standardize the process of FL experimental setup and model\nevaluation. Our evaluation platform provides flexible APIs to implement new FL\nalgorithms and includes new execution backends with minimal developer efforts.\nFinally, we perform indepth benchmark experiments on these datasets. Our\nexperiments suggest fruitful opportunities in heterogeneity-aware\nco-optimizations of the system and statistical efficiency under realistic FL\ncharacteristics. FedScale is open-source with permissive licenses and actively\nmaintained,1 and we welcome feedback and contributions from the community.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 15:55:27 GMT"}, {"version": "v2", "created": "Sun, 13 Jun 2021 03:58:13 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Lai", "Fan", ""], ["Dai", "Yinwei", ""], ["Zhu", "Xiangfeng", ""], ["Chowdhury", "Mosharaf", ""]]}, {"id": "2105.11491", "submitter": "Jou-Ming Chang", "authors": "Xiao-Yan Li, Wanling Lin, Jou-Ming Chang, Xiaohua Jia", "title": "Transmission Failure Analysis of Multi-Protection Routing in Data Center\n  Networks with Heterogeneous Edge-Core Servers", "comments": "22 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently proposed RCube network is a cube-based server-centric data\ncenter network (DCN), including two types of heterogeneous servers, called core\nand edge servers. Remarkably, it takes the latter as backup servers to deal\nwith server failures and thus achieve high availability. This paper first\npoints out that RCube is suitable as a candidate topology of DCNs for edge\ncomputing. Three transmission types are among core and edge servers based on\nthe demand for applications' computation and instant response. We then employ\nprotection routing to analyze the transmission failure of RCube DCNs. Unlike\ntraditional protection routing, which only tolerates a single link or node\nfailure, we use the multi-protection routing scheme to improve fault-tolerance\ncapability. To configure a protection routing in a network, according to\nTapolcai's suggestion, we need to construct two completely independent spanning\ntrees (CISTs). A logic graph of RCube, denoted by $L$-$RCube(n,m,k)$, is a\nnetwork with a recursive structure. Each basic building element consists of $n$\ncore servers and $m$ edge servers, where the order $k$ is the number of\nrecursions applied in the structure. In this paper, we provide algorithms to\nconstruct $\\min\\{n,\\lfloor(n+m)/2\\rfloor\\}$ CISTs in $L$-$RCube(n,m,k)$ for\n$n+m\\geqslant 4$ and $n>1$. From a combination of the multiple CISTs, we can\nconfigure the desired multi-protection routing. In our simulation, we configure\nup to 10 protection routings for RCube DCNs. As far as we know, in past\nresearch, there were at most three protection routings developed in other\nnetwork structures. Finally, we summarize some crucial analysis viewpoints\nabout the transmission efficiency of DCNs with heterogeneous edge-core servers\nfrom the simulation results.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 18:34:01 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Li", "Xiao-Yan", ""], ["Lin", "Wanling", ""], ["Chang", "Jou-Ming", ""], ["Jia", "Xiaohua", ""]]}, {"id": "2105.11592", "submitter": "Anupama Mampage", "authors": "Anupama Mampage, Shanika Karunasekera and Rajkumar Buyya", "title": "A Holistic View on Resource Management in Serverless Computing\n  Environments: Taxonomy and Future Directions", "comments": "32 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Serverless computing has emerged as an attractive deployment option for cloud\napplications in recent times. The unique features of this computing model\ninclude, rapid auto-scaling, strong isolation, fine-grained billing options and\naccess to a massive service ecosystem which autonomously handles resource\nmanagement decisions. This model is increasingly being explored for deployments\nin geographically distributed edge and fog computing networks as well, due to\nthese characteristics. Effective management of computing resources has always\ngained a lot of attention among researchers. The need to automate the entire\nprocess of resource provisioning, allocation, scheduling, monitoring and\nscaling, has resulted in the need for specialized focus on resource management\nunder the serverless model. In this article, we identify the major aspects\ncovering the broader concept of resource management in serverless environments\nand propose a taxonomy of elements which influence these aspects, encompassing\ncharacteristics of system design, workload attributes and stakeholder\nexpectations. We take a holistic view on serverless environments deployed\nacross edge, fog and cloud computing networks. We also analyse existing works\ndiscussing aspects of serverless resource management using this taxonomy. This\narticle further identifies gaps in literature and highlights future research\ndirections for improving capabilities of this computing model.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 00:55:03 GMT"}, {"version": "v2", "created": "Sun, 30 May 2021 06:00:50 GMT"}, {"version": "v3", "created": "Tue, 1 Jun 2021 01:33:17 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Mampage", "Anupama", ""], ["Karunasekera", "Shanika", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "2105.11713", "submitter": "Ran Gelles", "authors": "Pierre Fraigniaud and Ran Gelles and Zvi Lotker", "title": "The Topology of Randomized Symmetry-Breaking Distributed Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Studying distributed computing through the lens of algebraic topology has\nbeen the source of many significant breakthroughs during the last two decades,\nespecially in the design of lower bounds or impossibility results for\ndeterministic algorithms. This paper aims at studying randomized synchronous\ndistributed computing through the lens of algebraic topology. We do so by\nstudying the wide class of (input-free) symmetry-breaking tasks, e.g., leader\nelection, in synchronous fault-free anonymous systems. We show that it is\npossible to redefine solvability of a task \"locally\", i.e., for each simplex of\nthe protocol complex individually, without requiring any global consistency.\nHowever, this approach has a drawback: it eliminates the topological aspect of\nthe computation, since a single facet has a trivial topological structure. To\novercome this issue, we introduce a \"projection\" $\\pi$ of both protocol and\noutput complexes, where every simplex $\\sigma$ is mapped to a complex\n$\\pi(\\sigma)$; the later has a rich structure that replaces the structure we\nlost by considering one single facet at a time.\n  To show the significance and applicability of our topological approach, we\nderive necessary and sufficient conditions for solving leader election in\nsynchronous fault-free anonymous shared-memory and message-passing models. In\nboth models, we consider scenarios in which there might be correlations between\nthe random values provided to the nodes. In particular, different parties might\nhave access to the same randomness source so their randomness is not\nindependent but equal. Interestingly, we find that solvability of leader\nelection relates to the number of parties that possess correlated randomness,\neither directly or via their greatest common divisor, depending on the specific\ncommunication model.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 07:38:36 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Fraigniaud", "Pierre", ""], ["Gelles", "Ran", ""], ["Lotker", "Zvi", ""]]}, {"id": "2105.11730", "submitter": "Jinyang Liu", "authors": "Jinyang Liu, Sheng Di, Kai Zhao, Sian Jin, Dingwen Tao, Xin Liang,\n  Zizhong Chen, Franck Cappello", "title": "Exploring Autoencoder-based Error-bounded Compression for Scientific\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Error-bounded lossy compression is becoming an indispensable technique for\nthe success of today's scientific projects with vast volumes of data produced\nduring the simulations or instrument data acquisitions. Not only can it\nsignificantly reduce data size, but it also can control the compression errors\nbased on user-specified error bounds. Autoencoder (AE) models have been widely\nused in image compression, but few AE-based compression approaches support\nerror-bounding features, which are highly required by scientific applications.\nTo address this issue, we explore using convolutional autoencoders to improve\nerror-bounded lossy compression for scientific data, with the following three\nkey contributions. (1) We provide an in-depth investigation of the\ncharacteristics of various autoencoder models and develop an error-bounded\nautoencoder-based framework in terms of the SZ model. (2) We optimize the\ncompression quality for main stages in our designed AE-based error-bounded\ncompression framework, fine-tuning the block sizes and latent sizes and also\noptimizing the compression efficiency of latent vectors. (3) We evaluate our\nproposed solution using five real-world scientific datasets and comparing them\nwith six other related works. Experiments show that our solution exhibits a\nvery competitive compression quality from among all the compressors in our\ntests. In absolute terms, it can obtain a much better compression quality (100%\n~ 800% improvement in compression ratio with the same data distortion) compared\nwith SZ2.1 and ZFP in cases with a high compression ratio.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 07:53:32 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 22:15:29 GMT"}, {"version": "v3", "created": "Tue, 20 Jul 2021 03:07:52 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Liu", "Jinyang", ""], ["Di", "Sheng", ""], ["Zhao", "Kai", ""], ["Jin", "Sian", ""], ["Tao", "Dingwen", ""], ["Liang", "Xin", ""], ["Chen", "Zizhong", ""], ["Cappello", "Franck", ""]]}, {"id": "2105.11788", "submitter": "Jan Martens", "authors": "Jan Martens, Jan Friso Groote, Lars van den Haak, Pieter Hijma and\n  Anton Wijs", "title": "A linear parallel algorithm to compute bisimulation and relational\n  coarsest partitions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most efficient way to calculate strong bisimilarity is by calculation the\nrelational coarsest partition on a transition system. We provide the first\nlinear time algorithm to calculate strong bisimulation using parallel random\naccess machines (PRAMs). More precisely, with $n$ states, $m$ transitions and\n$|\\mathit{Act}|\\leq m$ action labels, we provide an algorithm on $max(n,m)$\nprocessors that calculates strong bisimulation in time $O(n+|\\mathit{Act}|)$\nand space $O(n+m)$. The best-known PRAM algorithm has time complexity $O(n\\log\nn)$ on a smaller number of processors making it less suitable for massive\nparallel devices such as GPUs. An implementation on a GPU shows that the linear\ntime-bound is achievable on contemporary hardware.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 09:32:50 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Martens", "Jan", ""], ["Groote", "Jan Friso", ""], ["Haak", "Lars van den", ""], ["Hijma", "Pieter", ""], ["Wijs", "Anton", ""]]}, {"id": "2105.11821", "submitter": "Thomas Orton", "authors": "Thomas Orton", "title": "Payment Does Not Imply Consensus (For Distributed Payment Systems)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decentralized payment systems such as Bitcoin have become massively popular\nin the last few years, yet there is still much to be done in understanding\ntheir formal properties. The vast majority of decentralized payment systems\nwork by achieving consensus on the state of the network; a natural question to\ntherefore ask is whether this consensus is necessary. In this paper, we\nformally define a model of payment systems, and present two main results. In\nTheorem 1, we show that even though there exists a single step black box\nreduction from Payment Systems to Byzantine Broadcast, there does not exist any\nblack box reduction in the other direction which is significantly better than a\ntrivial reduction. In Theorem 2, we show how to construct Payment Systems which\nonly require a very small number of messages to be sent per transaction. In\nparticular, global consensus about which transactions have occurred is not\nnecessary for payments in this model. We then show a relation between the\nconstruction in Theorem 2 and the Lightning Network, relating the formal model\nconstructions we have given to a practical algorithm proposed by the\ncryptocurrency community.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 10:44:31 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Orton", "Thomas", ""]]}, {"id": "2105.11827", "submitter": "Alberto Sonnino", "authors": "George Danezis, Eleftherios Kokoris Kogias, Alberto Sonnino, Alexander\n  Spiegelman", "title": "Narwhal and Tusk: A DAG-based Mempool and Efficient BFT Consensus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose separating the task of transaction dissemination from transaction\nordering, to enable high-performance Byzantine fault-tolerant consensus in a\npermissioned setting. To this end, we design and evaluate a mempool protocol,\nNarwhal, specializing in high-throughput reliable dissemination and storage of\ncausal histories of transactions. Narwhal tolerates an asynchronous network and\nmaintains its performance despite failures. We demonstrate that composing\n\\Narwhal with a partially synchronous consensus protocol (HotStuff) yields\nsignificantly better throughput even in the presence of faults. However, loss\nof liveness during view-changes can result in high latency. To achieve overall\ngood performance when faults occur we propose Tusk, a zero-message overhead\nasynchronous consensus protocol embedded within Narwhal. We demonstrate its\nhigh performance under a variety of configurations and faults. Further, Narwhal\nis designed to easily scale-out using multiple workers at each validator, and\nwe demonstrate that there is no foreseeable limit to the throughput we can\nachieve for consensus, with a few seconds latency. As a summary of results, on\na Wide Area Network (WAN), Hotstuff over Narwhal achieves 170,000 tx/sec with a\n2.5-sec latency instead of 1,800 tx/sec with 1-sec latency of Hotstuff.\nAdditional workers increase throughput linearly to 600,000 tx/sec without any\nlatency increase. Tusk achieves 140,000 tx/sec with 4 seconds latency or 20x\nbetter than the state-of-the-art asynchronous protocol. Under faults, both\nNarwhal based protocols maintain high throughput, but the HotStuff variant\nsuffers from slightly higher latency.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 10:53:41 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 12:10:22 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Danezis", "George", ""], ["Kogias", "Eleftherios Kokoris", ""], ["Sonnino", "Alberto", ""], ["Spiegelman", "Alexander", ""]]}, {"id": "2105.11895", "submitter": "Hongbin Zhuang", "authors": "Hongbin Zhuang, Wenzhong Guo, Xiaoyan Li, Ximeng Liu and Cheng-Kuan\n  Lin", "title": "The Component Diagnosability of General Networks", "comments": "20 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The processor failures in a multiprocessor system have a negative impact on\nits distributed computing efficiency. Because of the rapid expansion of\nmultiprocessor systems, the importance of fault diagnosis is becoming\nincreasingly prominent. The $h$-component diagnosability of $G$, denoted by\n$ct_{h}(G)$, is the maximum number of nodes of the faulty set $F$ that is\ncorrectly identified in a system, and the number of components in $G-F$ is at\nleast $h$. In this paper, we determine the $(h+1)$-component diagnosability of\ngeneral networks under the PMC model and MM$^{*}$ model. As applications, the\ncomponent diagnosability is explored for some well-known networks, including\ncomplete cubic networks, hierarchical cubic networks, generalized exchanged\nhypercubes, dual-cube-like networks, hierarchical hypercubes, Cayley graphs\ngenerated by transposition trees (except star graphs), and DQcube as well.\nFurthermore, we provide some comparison results between the component\ndiagnosability and other fault diagnosabilities.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 13:06:54 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Zhuang", "Hongbin", ""], ["Guo", "Wenzhong", ""], ["Li", "Xiaoyan", ""], ["Liu", "Ximeng", ""], ["Lin", "Cheng-Kuan", ""]]}, {"id": "2105.11932", "submitter": "Ilya Kokorin", "authors": "Vitaly Aksenov, Ohad Ben-Baruch, Danny Hendler, Ilya Kokorin, Matan\n  Rusanovsky", "title": "Execution of NVRAM Programs with Persistent Stack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Non-Volatile Random Access Memory (NVRAM) is a novel type of hardware that\ncombines the benefits of traditional persistent memory (persistency of data\nover hardware failures) and DRAM (fast random access). In this work, we\ndescribe an algorithm that can be used to execute NVRAM programs and recover\nthe system after a hardware failure while taking the architecture of real-world\nNVRAM systems into account. Moreover, the algorithm can be used to execute\nNVRAM-destined programs on commodity persistent hardware, such as hard drives.\nThat allows us to test NVRAM algorithms using only cheap hardware, without\nhaving access to the NVRAM. We report the usage of our algorithm to implement\nand test NVRAM CAS algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 13:34:44 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Aksenov", "Vitaly", ""], ["Ben-Baruch", "Ohad", ""], ["Hendler", "Danny", ""], ["Kokorin", "Ilya", ""], ["Rusanovsky", "Matan", ""]]}, {"id": "2105.11955", "submitter": "Mark Christopher Ballandies", "authors": "Mark C. Ballandies and Marcus M. Dapp and Benjamin A. Degenhart and\n  Dirk Helbing", "title": "Finance 4.0: Design principles for a value-sensitive cryptoecnomic\n  system to address sustainability", "comments": null, "journal-ref": "ECIS 2021 Research Papers. 62 (2021)", "doi": null, "report-no": null, "categories": "cs.CY cs.DC cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cryptoeconomic systems derive their power but can not be controlled by the\nunderlying software systems and the rules they enshrine. This adds a level of\ncomplexity to the software design process. At the same time, such systems, when\ndesigned with human values in mind, offer new approaches to tackle\nsustainability challenges, that are plagued by commons dilemmas and negative\nexternal effects caused by a one-dimensional monetary system. This paper\nproposes a design science research methodology with value-sensitive design\nmethods to derive design principles for a value-sensitive socio-ecological\ncryptoeconomic system that incentivizes actions toward sustainability via\nmulti-dimensional token incentives. These design principles are implemented in\na software that is validated in user studies that demonstrate its relevance,\nusability and impact. Our findings provide new insights on designing\ncryptoeconomic systems. Moreover, the identified design principles for a\nvalue-sensitive socio-ecological financial system indicate opportunities for\nnew research directions and business innovations.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 14:09:50 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Ballandies", "Mark C.", ""], ["Dapp", "Marcus M.", ""], ["Degenhart", "Benjamin A.", ""], ["Helbing", "Dirk", ""]]}, {"id": "2105.12026", "submitter": "Philipp-Jan Honysz", "authors": "Philipp-Jan Honysz and Alexander Schulze-Struchtrup and Sebastian\n  Buschj\\\"ager and Katharina Morik", "title": "Providing Meaningful Data Summarizations Using Exemplar-based Clustering\n  in Industry 4.0", "comments": "arXiv admin note: substantial text overlap with arXiv:2101.08763", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data summarizations are a valuable tool to derive knowledge from large data\nstreams and have proven their usefulness in a great number of applications.\nSummaries can be found by optimizing submodular functions. These functions map\nsubsets of data to real values, which indicate their \"representativeness\" and\nwhich should be maximized to find a diverse summary of the underlying data. In\nthis paper, we studied Exemplar-based clustering as a submodular function and\nprovide a GPU algorithm to cope with its high computational complexity. We\nshow, that our GPU implementation provides speedups of up to 72x using\nsingle-precision and up to 452x using half-precision computation compared to\nconventional CPU algorithms. We also show, that the GPU algorithm not only\nprovides remarkable runtime benefits with workstation-grade GPUs but also with\nlow-power embedded computation units for which speedups of up to 35x are\npossible. Furthermore, we apply our algorithm to real-world data from injection\nmolding manufacturing processes and discuss how found summaries help with\nsteering this specific process to cut costs and reduce the manufacturing of bad\nparts. Beyond pure speedup considerations, we show, that our approach can\nprovide summaries within reasonable time frames for this kind of industrial,\nreal-world data.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 15:55:14 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 21:46:04 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Honysz", "Philipp-Jan", ""], ["Schulze-Struchtrup", "Alexander", ""], ["Buschj\u00e4ger", "Sebastian", ""], ["Morik", "Katharina", ""]]}, {"id": "2105.12083", "submitter": "Andrzej Lingas", "authors": "Leszek Gasieniec, Jesper Jansson, Christos Levcopoulos, and Andrzej\n  Lingas", "title": "Efficient Assignment of Identities in Anonymous Populations", "comments": "28 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the fundamental problem of assigning distinct labels to agents in\nthe probabilistic model of population protocols. Our protocols operate under\nthe assumption that the size $n$ of the population is embedded in the\ntransition function. They are silent, i.e., eventually each agent reaches its\nfinal state and remains in it forever, as well as are safe, i.e., they can\nproduce a valid agent labeling in a finite number of interactions, and\nguarantee that at any step of the protocol no two agents have the same label.\nWe first present a fast, silent and safe labeling protocol for which the\nrequired number of interactions is asymptotically optimal, i.e., $O(n \\log\nn/\\epsilon)$ w.h.p. It uses $(2+\\epsilon)n+O(n^c)$ states, for any $c<1,$ and\nthe label range $1,\\dots,(1+\\epsilon)n.$ Furthermore, we consider the so-called\npool labeling protocols that include our fast protocol. We show that the\nexpected number of interactions required by any pool protocol is $\\ge\n\\frac{n^2}{r+1}$, when the labels range is $1,\\dots, n+r<2n.$ Next, we provide\na silent and safe protocol which uses only $n+5\\sqrt n +O(n^c)$ states, for any\n$c<1,$ and draws labels from the range $1,\\dots,n.$ . The expected number of\ninteractions required by the protocol is $O(n^3).$ On the other hand, we show\nthat any safe protocol, as well as any silent protocol which provides a valid\nlabeling with probability $>1-\\frac 1n$, uses $\\ge n+\\sqrt n-1$ states. Hence,\nour protocol is almost state-optimal. We also present a generalization of the\nprotocol to include a trade-off between the number of states and the expected\nnumber of interactions. Furthermore, we show that for any safe labeling\nprotocol utilizing $n+t<2n$ states the expected number of interactions required\nto achieve a valid labeling is $\\ge \\frac{n^2}{t+1}$.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 17:05:32 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 07:12:59 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Gasieniec", "Leszek", ""], ["Jansson", "Jesper", ""], ["Levcopoulos", "Christos", ""], ["Lingas", "Andrzej", ""]]}, {"id": "2105.12262", "submitter": "Yuichi Sudo", "authors": "Gregory Schwartzman and Yuichi Sudo", "title": "Smoothed Analysis of Population Protocols", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we initiate the study of \\emph{smoothed analysis} of population\nprotocols. We consider a population protocol model where an adaptive adversary\ndictates the interactions between agents, but with probability $p$ every such\ninteraction may change into an interaction between two agents chosen uniformly\nat random. That is, $p$-fraction of the interactions are random, while\n$(1-p)$-fraction are adversarial. The aim of our model is to bridge the gap\nbetween a uniformly random scheduler (which is too idealistic) and an\nadversarial scheduler (which is too strict).\n  We focus on the fundamental problem of leader election in population\nprotocols. We show that, for a population of size $n$, the leader election\nproblem can be solved in $O(p^{-2}n \\log^3 n)$ steps with high probability,\nusing $O((\\log^2 n) \\cdot (\\log (n/p)))$ states per agent, for \\emph{all}\nvalues of $p\\leq 1$. Although our result does not match the best known running\ntime of $O(n \\log n)$ for the uniformly random scheduler ($p=1$), we are able\nto present a \\emph{smooth transition} between a running time of $O(n \\cdot\n\\mathrm{polylog} n)$ for $p=1$ and an infinite running time for the adversarial\nscheduler ($p=0$), where the problem cannot be solved. The key technical\ncontribution of our work is a novel \\emph{phase clock} algorithm for our model.\nThis is a key primitive for much-studied fundamental population protocol\nalgorithms (leader election, majority), and we believe it is of independent\ninterest.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 23:55:18 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Schwartzman", "Gregory", ""], ["Sudo", "Yuichi", ""]]}, {"id": "2105.12301", "submitter": "Keichi Takahashi", "authors": "Keichi Takahashi (1), Wassapon Watanakeesuntorn (1), Kohei Ichikawa\n  (1), Joseph Park (2), Ryousei Takano (3), Jason Haga (3), George Sugihara\n  (4), Gerald M. Pao (5) ((1) Nara Institute of Science and Technology, (2)\n  U.S. Department of the Interior, (3) National Institute of Advanced\n  Industrial Science and Technology, (4) University of California San Diego,\n  (5) Salk Institute for Biological Studies)", "title": "kEDM: A Performance-portable Implementation of Empirical Dynamic\n  Modeling using Kokkos", "comments": "8 pages, 9 figures, accepted at Practice & Experience in Advanced\n  Research Computing (PEARC'21), corresponding authors: Keichi Takahashi,\n  Gerald M. Pao", "journal-ref": null, "doi": "10.1145/3437359.3465571", "report-no": null, "categories": "cs.DC cs.MS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical Dynamic Modeling (EDM) is a state-of-the-art non-linear time-series\nanalysis framework. Despite its wide applicability, EDM was not scalable to\nlarge datasets due to its expensive computational cost. To overcome this\nobstacle, researchers have attempted and succeeded in accelerating EDM from\nboth algorithmic and implementational aspects. In previous work, we developed a\nmassively parallel implementation of EDM targeting HPC systems (mpEDM).\nHowever, mpEDM maintains different backends for different architectures. This\ndesign becomes a burden in the increasingly diversifying HPC systems, when\nporting to new hardware. In this paper, we design and develop a\nperformance-portable implementation of EDM based on the Kokkos performance\nportability framework (kEDM), which runs on both CPUs and GPUs while based on a\nsingle codebase. Furthermore, we optimize individual kernels specifically for\nEDM computation, and use real-world datasets to demonstrate up to $5.5\\times$\nspeedup compared to mpEDM in convergent cross mapping computation.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 02:21:55 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Takahashi", "Keichi", ""], ["Watanakeesuntorn", "Wassapon", ""], ["Ichikawa", "Kohei", ""], ["Park", "Joseph", ""], ["Takano", "Ryousei", ""], ["Haga", "Jason", ""], ["Sugihara", "George", ""], ["Pao", "Gerald M.", ""]]}, {"id": "2105.12597", "submitter": "Wenjie Li", "authors": "Wenjie Li, Mohamad Assaad", "title": "Distributed Zeroth-Order Stochastic Optimization in Time-varying\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider a distributed convex optimization problem in a network which is\ntime-varying and not always strongly connected. The local cost function of each\nnode is affected by some stochastic process. All nodes of the network\ncollaborate to minimize the average of their local cost functions. The major\nchallenge of our work is that the gradient of cost functions is supposed to be\nunavailable and has to be estimated only based on the numerical observation of\ncost functions. Such problem is known as zeroth-order stochastic convex\noptimization (ZOSCO). In this paper we take a first step towards the\ndistributed optimization problem with a ZOSCO setting. The proposed algorithm\ncontains two basic steps at each iteration: i) each unit updates a local\nvariable according to a random perturbation based single point gradient\nestimator of its own local cost function; ii) each unit exchange its local\nvariable with its direct neighbors and then perform a weighted average. In the\nsituation where the cost function is smooth and strongly convex, our attainable\noptimization error is $O(T^{-1/2})$ after $T$ iterations. This result is\ninteresting as $O(T^{-1/2})$ is the optimal convergence rate in the ZOSCO\nproblem. We have also investigate the optimization error with the general\nLipschitz convex function, the result is $O(T^{-1/4})$.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 14:54:07 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Li", "Wenjie", ""], ["Assaad", "Mohamad", ""]]}, {"id": "2105.12663", "submitter": "Maciej Besta", "authors": "Maciej Besta, Marcel Schneider, Salvatore Di Girolamo, Ankit Singla,\n  Torsten Hoefler", "title": "Towards Million-Server Network Simulations on Just a Laptop", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing size of data center and HPC networks pose unprecedented\nrequirements on the scalability of simulation infrastructure. The ability to\nsimulate such large-scale interconnects on a simple PC would facilitate\nresearch efforts. Unfortunately, as we first show in this work, existing\nshared-memory packet-level simulators do not scale to the sizes of the largest\nnetworks considered today. We then illustrate a feasibility analysis and a set\nof enhancements that enable a simple packet-level htsim simulator to scale to\nthe unprecedented simulation sizes on a single PC. Our code is available online\nand can be used to design novel schemes in the coming era of omnipresent data\ncenters and HPC clusters.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 16:21:33 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Besta", "Maciej", ""], ["Schneider", "Marcel", ""], ["Di Girolamo", "Salvatore", ""], ["Singla", "Ankit", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2105.12706", "submitter": "Alex Weaver", "authors": "Seth Gilbert, Calvin Newport, Nitin Vaidya, and Alex Weaver", "title": "Contention Resolution with Predictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we consider contention resolution algorithms that are\naugmented with predictions about the network. We begin by studying the natural\nsetup in which the algorithm is provided a distribution defined over the\npossible network sizes that predicts the likelihood of each size occurring. The\ngoal is to leverage the predictive power of this distribution to improve on\nworst-case time complexity bounds. Using a novel connection between contention\nresolution and information theory, we prove lower bounds on the expected time\ncomplexity with respect to the Shannon entropy of the corresponding network\nsize random variable, for both the collision detection and no collision\ndetection assumptions. We then analyze upper bounds for these settings,\nassuming now that the distribution provided as input might differ from the\nactual distribution generating network sizes. We express their performance with\nrespect to both entropy and the statistical divergence between the two\ndistributions -- allowing us to quantify the cost of poor predictions. Finally,\nwe turn our attention to the related perfect advice setting, parameterized with\na length $b\\geq 0$, in which all active processes in a given execution are\nprovided the best possible $b$ bits of information about their network. We\nprovide tight bounds on the speed-up possible with respect to $b$ for\ndeterministic and randomized algorithms, with and without collision detection.\nThese bounds provide a fundamental limit on the maximum power that can be\nprovided by any predictive model with a bounded output size.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 17:39:11 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Gilbert", "Seth", ""], ["Newport", "Calvin", ""], ["Vaidya", "Nitin", ""], ["Weaver", "Alex", ""]]}, {"id": "2105.12739", "submitter": "Holger Schulz", "authors": "Holger Schulz and Gonzalo Brito Gadeschi and Oleksandr Rudyy and\n  Tobias Weinzierl", "title": "Task inefficiency patterns for a wave equation solver", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The orchestration of complex algorithms demands high levels of automation to\nuse modern hardware efficiently. Task-based programming with OpenMP 5.0 is a\nprominent candidate to accomplish this goal. We study OpenMP 5.0's tasking in\nthe context of a wave equation solver (ExaHyPE) using three different\narchitectures and runtimes. We describe several task-scheduling flaws present\nin currently available runtimes, demonstrate how they impact performance and\nshow how to work around them. Finally, we propose extensions to the OpenMP\nstandard.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 18:00:01 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 12:40:58 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Schulz", "Holger", ""], ["Gadeschi", "Gonzalo Brito", ""], ["Rudyy", "Oleksandr", ""], ["Weinzierl", "Tobias", ""]]}, {"id": "2105.12764", "submitter": "Jieyang Chen", "authors": "Jieyang Chen, Lipeng Wan, Xin Liang, Ben Whitney, Qing Liu, Qian Gong,\n  David Pugmire, Nicholas Thompson, Jong Youl Choi, Matthew Wolf, Todd Munson,\n  Ian Foster, Scott Klasky", "title": "Scalable Multigrid-based Hierarchical Scientific Data Refactoring on\n  GPUs", "comments": "arXiv admin note: text overlap with arXiv:2007.04457", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid growth in scientific data and a widening gap between computational\nspeed and I/O bandwidth makes it increasingly infeasible to store and share all\ndata produced by scientific simulations. Instead, we need methods for reducing\ndata volumes: ideally, methods that can scale data volumes adaptively so as to\nenable negotiation of performance and fidelity tradeoffs in different\nsituations. Multigrid-based hierarchical data representations hold promise as a\nsolution to this problem, allowing for flexible conversion between different\nfidelities so that, for example, data can be created at high fidelity and then\ntransferred or stored at lower fidelity via logically simple and mathematically\nsound operations. However, the effective use of such representations has been\nhindered until now by the relatively high costs of creating, accessing,\nreducing, and otherwise operating on such representations. We describe here\nhighly optimized data refactoring kernels for GPU accelerators that enable\nefficient creation and manipulation of data in multigrid-based hierarchical\nforms. We demonstrate that our optimized design can achieve up to 264 TB/s\naggregated data refactoring throughput -- 92% of theoretical peak -- on 1024\nnodes of the Summit supercomputer. We showcase our optimized design by applying\nit to a large-scale scientific visualization workflow and the MGARD lossy\ncompression software.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 18:03:41 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Chen", "Jieyang", ""], ["Wan", "Lipeng", ""], ["Liang", "Xin", ""], ["Whitney", "Ben", ""], ["Liu", "Qing", ""], ["Gong", "Qian", ""], ["Pugmire", "David", ""], ["Thompson", "Nicholas", ""], ["Choi", "Jong Youl", ""], ["Wolf", "Matthew", ""], ["Munson", "Todd", ""], ["Foster", "Ian", ""], ["Klasky", "Scott", ""]]}, {"id": "2105.12769", "submitter": "Alexander Jung", "authors": "Yasmin SarcheshmehPour, Yu Tian, Linli Zhang, Alexander Jung", "title": "Networked Federated Multi-Task Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many important application domains generate distributed collections of\nheterogeneous local datasets. These local datasets are often related via an\nintrinsic network structure that arises from domain-specific notions of\nsimilarity between local datasets. Different notions of similarity are induced\nby spatiotemporal proximity, statistical dependencies, or functional relations.\nWe use this network structure to adaptively pool similar local datasets into\nnearly homogenous training sets for learning tailored models. Our main\nconceptual contribution is to formulate networked federated learning using the\nconcept of generalized total variation (GTV) minimization as a regularizer.\nThis formulation is highly flexible and can be combined with almost any\nparametric model including Lasso or deep neural networks. We unify and\nconsiderably extend some well-known approaches to federated multi-task\nlearning. Our main algorithmic contribution is a novel federated learning\nalgorithm that is well suited for distributed computing environments such as\nedge computing over wireless networks. This algorithm is robust against model\nmisspecification and numerical errors arising from limited computational\nresources including processing time or wireless channel bandwidth. As our main\ntechnical contribution, we offer precise conditions on the local models as well\non their network structure such that our algorithm learns nearly optimal local\nmodels. Our analysis reveals an interesting interplay between the\n(information-) geometry of local models and the (cluster-) geometry of their\nnetwork.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 18:07:19 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["SarcheshmehPour", "Yasmin", ""], ["Tian", "Yu", ""], ["Zhang", "Linli", ""], ["Jung", "Alexander", ""]]}, {"id": "2105.12839", "submitter": "Geraldo Francisco De Oliveira Junior", "authors": "Nastaran Hajinazar, Geraldo F. Oliveira, Sven Gregorio, Jo\\~ao\n  Ferreira, Nika Mansouri Ghiasi, Minesh Patel, Mohammed Alser, Saugata Ghose,\n  Juan G\\'omez Luna, and Onur Mutlu", "title": "SIMDRAM: An End-to-End Framework for Bit-Serial SIMD Computing in DRAM", "comments": "This is an extended version of the paper that appeared at ASPLOS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Processing-using-DRAM has been proposed for a limited set of basic operations\n(i.e., logic operations, addition). However, in order to enable full adoption\nof processing-using-DRAM, it is necessary to provide support for more complex\noperations. In this paper, we propose SIMDRAM, a flexible general-purpose\nprocessing-using-DRAM framework that (1) enables the efficient implementation\nof complex operations, and (2) provides a flexible mechanism to support the\nimplementation of arbitrary user-defined operations. The SIMDRAM framework\ncomprises three key steps. The first step builds an efficient MAJ/NOT\nrepresentation of a given desired operation. The second step allocates DRAM\nrows that are reserved for computation to the operation's input and output\noperands, and generates the required sequence of DRAM commands to perform the\nMAJ/NOT implementation of the desired operation in DRAM. The third step uses\nthe SIMDRAM control unit located inside the memory controller to manage the\ncomputation of the operation from start to end, by executing the DRAM commands\ngenerated in the second step of the framework. We design the hardware and ISA\nsupport for SIMDRAM framework to (1) address key system integration challenges,\nand (2) allow programmers to employ new SIMDRAM operations without hardware\nchanges.\n  We evaluate SIMDRAM for reliability, area overhead, throughput, and energy\nefficiency using a wide range of operations and seven real-world applications\nto demonstrate SIMDRAM's generality. Using 16 DRAM banks, SIMDRAM provides (1)\n88x and 5.8x the throughput, and 257x and 31x the energy efficiency, of a CPU\nand a high-end GPU, respectively, over 16 operations; (2) 21x and 2.1x the\nperformance of the CPU and GPU, over seven real-world applications. SIMDRAM\nincurs an area overhead of only 0.2% in a high-end CPU.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 21:06:55 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 13:48:51 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Hajinazar", "Nastaran", ""], ["Oliveira", "Geraldo F.", ""], ["Gregorio", "Sven", ""], ["Ferreira", "Jo\u00e3o", ""], ["Ghiasi", "Nika Mansouri", ""], ["Patel", "Minesh", ""], ["Alser", "Mohammed", ""], ["Ghose", "Saugata", ""], ["Luna", "Juan G\u00f3mez", ""], ["Mutlu", "Onur", ""]]}, {"id": "2105.12880", "submitter": "Eli Dart", "authors": "Eli Dart, William Allcock, Wahid Bhimji, Tim Boerner, Ravinderjeet\n  Cheema, Andrew Cherry, Brent Draney, Salman Habib, Damian Hazen, Jason Hill,\n  Matt Kollross, Suzanne Parete-Koon, Daniel Pelfrey, Adrian Pope, Jeff Porter,\n  David Wheeler", "title": "The Petascale DTN Project: High Performance Data Transfer for HPC\n  Facilities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The movement of large-scale (tens of Terabytes and larger) data sets between\nhigh performance computing (HPC) facilities is an important and increasingly\ncritical capability. A growing number of scientific collaborations rely on HPC\nfacilities for tasks which either require large-scale data sets as input or\nproduce large-scale data sets as output. In order to enable the transfer of\nthese data sets as needed by the scientific community, HPC facilities must\ndesign and deploy the appropriate data transfer capabilities to allow users to\ndo data placement at scale.\n  This paper describes the Petascale DTN Project, an effort undertaken by four\nHPC facilities, which succeeded in achieving routine data transfer rates of\nover 1PB/week between the facilities. We describe the design and configuration\nof the Data Transfer Node (DTN) clusters used for large-scale data transfers at\nthese facilities, the software tools used, and the performance tuning that\nenabled this capability.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 23:55:23 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Dart", "Eli", ""], ["Allcock", "William", ""], ["Bhimji", "Wahid", ""], ["Boerner", "Tim", ""], ["Cheema", "Ravinderjeet", ""], ["Cherry", "Andrew", ""], ["Draney", "Brent", ""], ["Habib", "Salman", ""], ["Hazen", "Damian", ""], ["Hill", "Jason", ""], ["Kollross", "Matt", ""], ["Parete-Koon", "Suzanne", ""], ["Pelfrey", "Daniel", ""], ["Pope", "Adrian", ""], ["Porter", "Jeff", ""], ["Wheeler", "David", ""]]}, {"id": "2105.12912", "submitter": "Dingwen Tao", "authors": "Jiannan Tian, Sheng Di, Xiaodong Yu, Cody Rivera, Kai Zhao, Sian Jin,\n  Yunhe Feng, Xin Liang, Dingwen Tao, Franck Cappello", "title": "cuSZ(x): Optimizing Error-Bounded Lossy Compression for Scientific Data\n  on GPUs", "comments": "12 pages, 3 figures, 8 table, submitted to IEEE Cluster'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Error-bounded lossy compression is a critical technique for significantly\nreducing scientific data volumes. With ever-emerging heterogeneous HPC\narchitecture, GPU-accelerated error-bounded compressors (such as cuSZ and\ncuZFP) have been developed. However, they suffer from either low performance or\nlow compression ratios. To this end, we propose cuSZ(x) to target both high\ncompression ratio and throughput. We identify that data sparsity and data\nsmoothness are key factors for high compression throughput. Our key\ncontributions in this work are fourfold: (1) We propose an efficient\ncompression workflow to adaptively perform run-length encoding and/or\nvariable-length encoding. (2) We derive Lorenzo reconstruction in decompression\nas multidimensional partial-sum computation and propose a fine-grained Lorenzo\nreconstruction algorithm for GPU architectures. (3) We carefully optimize each\nof cuSZ's kernels by leveraging state-of-the-art CUDA parallel primitives. (4)\nWe evaluate cuSZ(x) using seven real-world HPC application datasets on V100 and\nA100 GPUs. Experiments show cuSZ(x) improves the compression performance and\nratios by up to 18.4$\\times$ and 5.3$\\times$, respectively, over cuSZ on the\ntested datasets.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 02:17:04 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Tian", "Jiannan", ""], ["Di", "Sheng", ""], ["Yu", "Xiaodong", ""], ["Rivera", "Cody", ""], ["Zhao", "Kai", ""], ["Jin", "Sian", ""], ["Feng", "Yunhe", ""], ["Liang", "Xin", ""], ["Tao", "Dingwen", ""], ["Cappello", "Franck", ""]]}, {"id": "2105.12929", "submitter": "Dingwen Tao", "authors": "Bo Fang, Daoce Wang, Sian Jin, Quincey Koziol, Zhao Zhang, Qiang Guan,\n  Suren Byna, Sriram Krishnamoorthy, Dingwen Tao", "title": "Characterizing Impacts of Storage Faults on HPC Applications: A\n  Methodology and Insights", "comments": "12 pages, 9 figures, 4 tables, submitted to IEEE Cluster'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, the increasing complexity in scientific simulations and\nemerging demands for training heavy artificial intelligence models require\nmassive and fast data accesses, which urges high-performance computing (HPC)\nplatforms to equip with more advanced storage infrastructures such as\nsolid-state disks (SSDs). While SSDs offer high-performance I/O, the\nreliability challenges faced by the HPC applications under the SSD-related\nfailures remains unclear, in particular for failures resulting in data\ncorruptions. The goal of this paper is to understand the impact of SSD-related\nfaults on the behaviors of complex HPC applications. To this end, we propose\nFFIS, a FUSE-based fault injection framework that systematically introduces\nstorage faults into the application layer to model the errors originated from\nSSDs. FFIS is able to plant different I/O related faults into the data returned\nfrom underlying file systems, which enables the investigation on the error\nresilience characteristics of the scientific file format. We demonstrate the\nuse of FFIS with three representative real HPC applications, showing how each\napplication reacts to the data corruptions, and provide insights on the error\nresilience of the widely adopted HDF5 file format for the HPC applications.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 03:45:25 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Fang", "Bo", ""], ["Wang", "Daoce", ""], ["Jin", "Sian", ""], ["Koziol", "Quincey", ""], ["Zhang", "Zhao", ""], ["Guan", "Qiang", ""], ["Byna", "Suren", ""], ["Krishnamoorthy", "Sriram", ""], ["Tao", "Dingwen", ""]]}, {"id": "2105.13042", "submitter": "Giuseppe Prencipe", "authors": "David Kirkpatrick and Irina Kostitsyna and Alfredo Navarra and\n  Giuseppe Prencipe and Nicola Santoro", "title": "Separating Bounded and Unbounded Asynchrony for Autonomous Robots: Point\n  Convergence with Limited Visibility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Among fundamental problems in the context of distributed computing by\nautonomous mobile entities, one of the most representative and well studied is\n{\\sc Point Convergence}: given an arbitrary initial configuration of identical\nentities, disposed in the Euclidean plane, move in such a way that, for all\n$\\eps>0$, a configuration in which the separation between all entities is at\nmost $\\eps$ is eventually reached and maintained.\n  The problem has been previously studied in a variety of settings, including\nfull visibility, exact measurements (like distances and angles), and\nsynchronous activation of entities. Our study concerns the minimal assumptions\nunder which entities, moving asynchronously with limited and unknown visibility\nrange and subject to limited imprecision in measurements, can be guaranteed to\nconverge in this way.\n  We present an algorithm that solves {\\sc Point Convergence}, for entities in\nthe plane, in such a setting, provided the degree of asynchrony is bounded:\nwhile any one entity is active, any other entity can be activated at most $k$\ntimes, for some arbitrarily large but fixed $k$. This provides a strong\npositive answer to a decade old open question posed by Katreniak.\n  We also prove that in a comparable setting that permits unbounded asynchrony,\n{\\sc Point Convergence} in the plane is impossible, contingent on the natural\nassumption that algorithms maintain the (visible) connectivity among entities\npresent in the initial configuration. This variant, that we call {\\sc Cohesive\nConvergence}, serves to distinguish the power of bounded and unbounded\nasynchrony in the control of autonomous mobile entities, settling at the same\ntime a long-standing question whether in the Euclidean plane synchronously\nscheduled entities are more powerful than asynchronously scheduled entities.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 10:20:59 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Kirkpatrick", "David", ""], ["Kostitsyna", "Irina", ""], ["Navarra", "Alfredo", ""], ["Prencipe", "Giuseppe", ""], ["Santoro", "Nicola", ""]]}, {"id": "2105.13116", "submitter": "Alex Shamis", "authors": "Alex Shamis, Peter Pietzuch, Miguel Castro, Edward Ashton, Amaury\n  Chamayou, Sylvan Clebsch, Antoine Delignat-Lavaud, Cedric Fournet, Matthew\n  Kerner, Julien Maffre, Manuel Costa and Mark Russinovich", "title": "PAC: Practical Accountability for CCF", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Permissioned ledger systems execute transactions on a set of replicas\ngoverned by members of a consortium. They use Byzantine fault tolerance\nprotocols to distribute trust among the replicas, and thus can ensure\nlinearizability if fewer than 1/3 of the replicas misbehave. With more\nmisbehaving replicas, current systems provide no guarantees, and all replicas\nand members share the blame.\n  We describe PAC, a permissioned ledger system that \\emph{assigns blame to\nmisbehaving replicas} while supporting \\emph{governance transactions} to change\nthe consortium membership and the set of replicas. PAC signs and stores\nprotocol messages in the ledger and provides clients with signed,\nuniversally-verifiable \\emph{receipts} as evidence that a transaction executed\nat a certain ledger position. If clients obtain a sequence of receipts that\nviolate linearizability, anyone can \\emph{audit} the ledger and the sequence of\nreceipts to assign blame to at least 1/3 of the replicas, even if all replicas\nand members misbehave. Auditing assigns blame by finding contradictory\nstatements signed by the same replica. Since the set of replicas changes, PAC\ndetermines the valid signing keys at any point in the ledger using a shorter\nsub-ledger of governance transactions. PAC provides a strong disincentive to\nmisbehavior at low cost: it can execute more than 48,000~transactions per\nsecond, and clients receive receipts in two network round trips.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 13:19:58 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Shamis", "Alex", ""], ["Pietzuch", "Peter", ""], ["Castro", "Miguel", ""], ["Ashton", "Edward", ""], ["Chamayou", "Amaury", ""], ["Clebsch", "Sylvan", ""], ["Delignat-Lavaud", "Antoine", ""], ["Fournet", "Cedric", ""], ["Kerner", "Matthew", ""], ["Maffre", "Julien", ""], ["Costa", "Manuel", ""], ["Russinovich", "Mark", ""]]}, {"id": "2105.13120", "submitter": "Fuzhao Xue", "authors": "Shenggui Li, Fuzhao Xue, Yongbin Li, Yang You", "title": "Sequence Parallelism: Making 4D Parallelism Possible", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within Transformer, self-attention is the key module to learn powerful\ncontext-aware representations. However, self-attention suffers from quadratic\nmemory requirements with respect to the sequence length, which limits us to\nprocess longer sequence on GPU. In this work, we propose sequence parallelism,\na memory efficient parallelism method to help us break input sequence length\nlimitation and train with longer sequence on GPUs. Compared with existing\nparallelism, our approach no longer requires a single device to hold the whole\nsequence. Specifically, we split the input sequence into multiple chunks and\nfeed each chunk into its corresponding device (i.e. GPU). To compute the\nattention output, we communicate attention embeddings among GPUs. Inspired by\nring all-reduce, we integrated ring-style communication with self-attention\ncalculation and proposed Ring Self-Attention (RSA). Our implementation is fully\nbased on PyTorch. Without extra compiler or library changes, our approach is\ncompatible with data parallelism and pipeline parallelism. Experiments show\nthat sequence parallelism performs well when scaling with batch size and\nsequence length. Compared with tensor parallelism, our approach achieved\n$13.7\\times$ and $3.0\\times$ maximum batch size and sequence length\nrespectively when scaling up to 64 NVIDIA P100 GPUs. We plan to integrate our\nsequence parallelism with data, pipeline and tensor parallelism to further\ntrain large-scale models with 4D parallelism in our future work.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 13:40:58 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Li", "Shenggui", ""], ["Xue", "Fuzhao", ""], ["Li", "Yongbin", ""], ["You", "Yang", ""]]}, {"id": "2105.13185", "submitter": "Aymen Alsaadi", "authors": "Aymen Alsaadi, Andre Merzky, Kyle Chard, Shantenu Jha, Matteo Turilli", "title": "RADICAL-Pilot and Parsl: Executing Heterogeneous Workflows on HPC\n  Platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Executing scientific workflows with heterogeneous tasks on HPC platforms\nposes several challenges which will be further exacerbated by the upcoming\nexascale platforms. At that scale, bespoke solutions will not enable effective\nand efficient workflow executions. In preparation, we need to look at ways to\nmanage engineering effort and capability duplication across software systems by\nintegrating independently developed, production-grade software solutions. In\nthis paper, we integrate RADICAL-Pilot (RP) and Parsl and develop an MPI\nexecutor to enable the execution of workflows with heterogeneous (non)MPI\nPython functions at scale. We characterize the strong and weak scaling of the\nintegrated RP-Parsl system when executing two use cases from polar science, and\nof the function executor on both SDSC Comet and TACC Frontera. We gain\nengineering insight about how to analyze and integrate workflow and runtime\nsystems, minimizing changes in their code bases and overall development effort.\nOur experiments show that the overheads of the integrated system are invariant\nof resource and workflow scale, and measure the impact of diverse MPI\noverheads. Together, those results define a blueprint towards an ecosystem\npopulated by specialized, efficient, effective and independently-maintained\nsoftware systems to face the upcoming scaling challenges.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 14:39:43 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Alsaadi", "Aymen", ""], ["Merzky", "Andre", ""], ["Chard", "Kyle", ""], ["Jha", "Shantenu", ""], ["Turilli", "Matteo", ""]]}, {"id": "2105.13194", "submitter": "Ami Paz", "authors": "Seth Gilbert, Uri Meir, Ami Paz, Gregory Schwartzman", "title": "On the Complexity of Load Balancing in Dynamic Networks", "comments": "To be presented in SPAA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the load balancing problem, each node in a network is assigned a load, and\nthe goal is to equally distribute the loads among the nodes, by preforming\nlocal load exchanges. While load balancing was extensively studied in static\nnetworks, only recently a load balancing algorithm for dynamic networks with a\nbounded convergence time was presented. In this paper, we further study the\ntime complexity of load balancing in the context of dynamic networks.\n  First, we show that randomness is not necessary, and present a deterministic\nalgorithm which slightly improves the running time of the previous algorithm,\nat the price of not being matching-based. Then, we consider integral loads,\ni.e., loads that cannot be split indefinitely, and prove that no matching-based\nalgorithm can have a bounded convergence time for this case.\n  To circumvent both this impossibility result, and a known one for the\nnon-integral case, we apply the method of smoothed analysis, where random\nperturbations are made over the worst-case choices of network topologies. We\nshow both impossibility results do not hold under this kind of analysis,\nsuggesting that load-balancing in real world systems might be faster than the\nlower bounds suggest.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 14:42:56 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Gilbert", "Seth", ""], ["Meir", "Uri", ""], ["Paz", "Ami", ""], ["Schwartzman", "Gregory", ""]]}, {"id": "2105.13279", "submitter": "Gianluca Palermo", "authors": "Emanuele Vitali and Anton Lokhmotov and Gianluca Palermo", "title": "Dynamic Network selection for the Object Detection task: why it matters\n  and what we (didn't) achieve", "comments": "Paper accepted at SAMOS21 - International Conference on Embedded\n  Computer Systems: Architectures, Modeling and Simulation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we want to show the potential benefit of a dynamic auto-tuning\napproach for the inference process in the Deep Neural Network (DNN) context,\ntackling the object detection challenge. We benchmarked different neural\nnetworks to find the optimal detector for the well-known COCO 17 database, and\nwe demonstrate that even if we only consider the quality of the prediction\nthere is not a single optimal network. This is even more evident if we also\nconsider the time to solution as a metric to evaluate, and then select, the\nmost suitable network. This opens to the possibility for an adaptive\nmethodology to switch among different object detection networks according to\nrun-time requirements (e.g. maximum quality subject to a time-to-solution\nconstraint).\n  Moreover, we demonstrated by developing an ad hoc oracle, that an additional\nproactive methodology could provide even greater benefits, allowing us to\nselect the best network among the available ones given some characteristics of\nthe processed image. To exploit this method, we need to identify some image\nfeatures that can be used to steer the decision on the most promising network.\nDespite the optimization opportunity that has been identified, we were not able\nto identify a predictor function that validates this attempt neither adopting\nclassical image features nor by using a DNN classifier.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 16:25:18 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Vitali", "Emanuele", ""], ["Lokhmotov", "Anton", ""], ["Palermo", "Gianluca", ""]]}, {"id": "2105.13336", "submitter": "Kaixin Zhang", "authors": "Kaixin Zhang, Hongzhi Wang, Tongxin Li, Han Hu, Jiye Qiu, Songling Zou", "title": "TENSILE: A Tensor granularity dynamic GPU memory scheduler method\n  towards multiple dynamic workloads system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.DB cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep learning has been an area of intense researching. However, as\na kind of computing intensive task, deep learning highly relies on the the\nscale of the GPU memory, which is usually expensive and scarce. Although there\nare some extensive works have been proposed for dynamic GPU memory management,\nthey are hard to be applied to systems with multitasking dynamic workloads,\nsuch as in-database machine learning system.\n  In this paper, we demonstrated TENSILE, a method of managing GPU memory in\ntensor granularity to reduce the GPU memory peak, with taking the multitasking\ndynamic workloads into consideration. As far as we know, TENSILE is the first\nmethod which is designed to manage multiple workloads' GPU memory using. We\nimplement TENSILE on our own deep learning framework, and evaluated its\nperformance. The experiment results shows that our method can achieve less time\noverhead than prior works with more GPU memory saved.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 17:46:16 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 03:31:38 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Zhang", "Kaixin", ""], ["Wang", "Hongzhi", ""], ["Li", "Tongxin", ""], ["Hu", "Han", ""], ["Qiu", "Jiye", ""], ["Zou", "Songling", ""]]}, {"id": "2105.13395", "submitter": "Camille Coti", "authors": "Camille Coti and Allen D Malony", "title": "Measuring OpenSHMEM Communication Routines with SKaMPI-OpenSHMEM User's\n  manual", "comments": "This paper is a technical report that comes with our benchmarking\n  software. It implements distributed algorithms for the measurement of\n  distributed operations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This document presents the OpenSHMEM extension for the Special Karlsruhe MPI\nbenchmark and the measurement algorithms used to measure the routines.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 18:50:08 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Coti", "Camille", ""], ["Malony", "Allen D", ""]]}, {"id": "2105.13424", "submitter": "Christina Delimitrou", "authors": "Yanqi Zhang, Weizhe Hua, Zhuangzhuang Zhou, Edward Suh, Christina\n  Delimitrou", "title": "Sinan: Data-Driven, QoS-Aware Cluster Management for Microservices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cloud applications are increasingly shifting from large monolithic services,\nto large numbers of loosely-coupled, specialized microservices. Despite their\nadvantages in terms of facilitating development, deployment, modularity, and\nisolation, microservices complicate resource management, as dependencies\nbetween them introduce backpressure effects and cascading QoS violations.\n  We present Sinan, a data-driven cluster manager for interactive cloud\nmicroservices that is online and QoS-aware. Sinan leverages a set of scalable\nand validated machine learning models to determine the performance impact of\ndependencies between microservices, and allocate appropriate resources per tier\nin a way that preserves the end-to-end tail latency target. We evaluate Sinan\nboth on dedicated local clusters and large-scale deployments on Google Compute\nEngine (GCE) across representative end-to-end applications built with\nmicroservices, such as social networks and hotel reservation sites. We show\nthat Sinan always meets QoS, while also maintaining cluster utilization high,\nin contrast to prior work which leads to unpredictable performance or\nsacrifices resource efficiency. Furthermore, the techniques in Sinan are\nexplainable, meaning that cloud operators can yield insights from the ML models\non how to better deploy and design their applications to reduce unpredictable\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 19:57:51 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Zhang", "Yanqi", ""], ["Hua", "Weizhe", ""], ["Zhou", "Zhuangzhuang", ""], ["Suh", "Edward", ""], ["Delimitrou", "Christina", ""]]}, {"id": "2105.13480", "submitter": "Rui Li", "authors": "Rui Li, Yufan Xu, Aravind Sukumaran-Rajam, Atanas Rountev and P\n  Sadayappan", "title": "Efficient distributed algorithms for Convolutional Neural Networks", "comments": "Proceedings of the 33rd ACM Symposium on Parallelism in Algorithms\n  and Architectures (SPAA '21), July 6--8, 2021, Virtual Event, USA", "journal-ref": "Proceedings of the 33rd ACM Symposium on Parallelism in Algorithms\n  and Architectures (SPAA '21), July 6--8, 2021, Virtual Event, USA", "doi": "10.1145/3409964.3461828", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Several efficient distributed algorithms have been developed for\nmatrix-matrix multiplication: the 3D algorithm, the 2D SUMMA algorithm, and the\n2.5D algorithm. Each of these algorithms was independently conceived and they\ntrade-off memory needed per node and the inter-node data communication volume.\n  The convolutional neural network (CNN) computation may be viewed as a\ngeneralization of matrix-multiplication combined with neighborhood stencil\ncomputations. We develop communication-efficient distributed-memory algorithms\nfor CNNs that are analogous to the 2D/2.5D/3D algorithms for matrix-matrix\nmultiplication.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 22:25:38 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 00:34:48 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Li", "Rui", ""], ["Xu", "Yufan", ""], ["Sukumaran-Rajam", "Aravind", ""], ["Rountev", "Atanas", ""], ["Sadayappan", "P", ""]]}, {"id": "2105.13487", "submitter": "Andrea Flamini", "authors": "Andrea Flamini, Riccardo Longo, Alessio Meneghetti", "title": "Multidimensional Byzantine Agreement in a Synchronous Setting", "comments": "15 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we will present the Multidimensional Byzantine Agreement (MBA)\nProtocol, a leaderless Byzantine agreement protocol defined for complete and\nsynchronous networks that allows a network of nodes to reach consensus on a\nvector of relevant information regarding a set of observed events.\n  The consensus process is carried out in parallel on each component, and the\noutput is a vector whose components are either values with wide agreement in\nthe network (even if no individual node agrees on every value) or a special\nvalue $\\bot$ that signals irreconcilable disagreement. The MBA Protocol is\nprobabilistic and its execution halts with probability 1, and the number of\nsteps necessary to halt follows a Bernoulli-like distribution.\n  The design combines a Multidimensional Graded Consensus and a\nMultidimensional Binary Byzantine Agreement, the generalization to the\nmultidimensional case of two protocols by Micali and Feldman.\n  We prove the correctness and security of the protocol assuming a synchronous\nnetwork where less than a third of the nodes are malicious.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 22:51:10 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 14:59:26 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Flamini", "Andrea", ""], ["Longo", "Riccardo", ""], ["Meneghetti", "Alessio", ""]]}, {"id": "2105.13489", "submitter": "Enzo Rucci", "authors": "Manuel Costanzo and Enzo Rucci and Carlos Garc\\'ia Sanchez and Marcelo\n  Naiouf", "title": "Early Experiences Migrating CUDA codes to oneAPI", "comments": "Accepted for publication in 9th Conference on Cloud Computing\n  Conference, Big Data & Emerging Topics (JCC-BD&ET 2021,\n  https://jcc.info.unlp.edu.ar/en/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The heterogeneous computing paradigm represents a real programming challenge\ndue to the proliferation of devices with different hardware characteristics.\nRecently Intel introduced oneAPI, a new programming environment that allows\ncode developed in DPC++ to be run on different devices such as CPUs, GPUs,\nFPGAs, among others. This paper presents our first experiences in porting two\nCUDA applications to DPC++ using the oneAPI dpct tool. From the experimental\nwork, it was possible to verify that dpct does not achieve 100% of the\nmigration task; however, it performs most of the work, reporting the programmer\nof possible pending adaptations. Additionally, it was possible to verify the\nfunctional portability of the DPC++ code obtained, having successfully executed\nit on different CPU and GPU architectures.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 22:54:15 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Costanzo", "Manuel", ""], ["Rucci", "Enzo", ""], ["Sanchez", "Carlos Garc\u00eda", ""], ["Naiouf", "Marcelo", ""]]}, {"id": "2105.13574", "submitter": "Youry Khmelevsky", "authors": "Youry Khmelevsky and Gaetan J.D.R. Hains", "title": "Parallel Programming Applied Research Projects for Teaching Parallel\n  Programming to Beginner Students", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we discuss the educational value of a few mid-size and one\nlarge applied research projects at the Computer Science Department of Okanagan\nCollege (OC) and at the Universities of Paris East Creteil (LACL) and Orleans\n(LIFO) in France. We found, that some freshmen students are very active and\neager to be involved in applied research projects starting from the second\nsemester. They are actively participating in programming competitions and want\nto be involved in applied research projects to compete with sophomore and older\nstudents. Our observation is based on five NSERC Engage College and Applied\nResearch and Development (ARD) grants, and several small applied projects.\nStudent involvement in applied research is a key motivation and success factor\nin our activities, but we are also involved in transferring some results of\napplied research, namely programming techniques, into the parallel programming\ncourses for beginners at the senior- and first-year MSc levels. We illustrate\nthis feedback process with programming notions for beginners, practical tools\nto acquire them and the overall success/failure of students as experienced for\nmore than 10 years in our French University courses.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 03:50:57 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 01:05:38 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Khmelevsky", "Youry", ""], ["Hains", "Gaetan J. D. R.", ""]]}, {"id": "2105.13601", "submitter": "Christian Meurisch", "authors": "Christian Meurisch", "title": "The Trusted Edge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge computing promises to reshape the centralized nature of today's\ncloud-based applications by bringing computing resources, at least in part,\ncloser to the user. Reasons include the increasing need for real-time\n(short-delay, reliably-connected) computing and resource-demanding artificial\nintelligence (AI) algorithms that overstrain mobile devices' batteries or\ncompute power but are too bandwidth-demanding to be offloaded to a distant\ncloud. However, companies may need to run their protected business logic on\n(untrusted) third-party edge devices, which can lead to serious issues due to\nweaker security measures than in cloud environments. This article makes the\ncase for trusted edge computing (TEC), which focuses on developing concepts and\nmethods for protecting application providers' business logic (and thus their\nintellectual property) specifically tailored to open edge infrastructures. This\narticle further discusses open challenges in TEC to be addressed in the future.\nOtherwise, edge computing risks being a non-starter for most businesses due to\nthe inadequate and neglected protection of intellectual property.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 06:06:31 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Meurisch", "Christian", ""]]}, {"id": "2105.13618", "submitter": "Jia Yan", "authors": "Jia Yan, Suzhi Bi, Ying-Jun Angela Zhang", "title": "Optimal Model Placement and Online Model Splitting for Device-Edge\n  Co-Inference", "comments": "This paper has been submitted to IEEE Transactions on Wireless\n  Communications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Device-edge co-inference opens up new possibilities for resource-constrained\nwireless devices (WDs) to execute deep neural network (DNN)-based applications\nwith heavy computation workloads. In particular, the WD executes the first few\nlayers of the DNN and sends the intermediate features to the edge server that\nprocesses the remaining layers of the DNN. By adapting the model splitting\ndecision, there exists a tradeoff between local computation cost and\ncommunication overhead. In practice, the DNN model is re-trained and updated\nperiodically at the edge server. Once the DNN parameters are regenerated, part\nof the updated model must be placed at the WD to facilitate on-device\ninference. In this paper, we study the joint optimization of the model\nplacement and online model splitting decisions to minimize the energy-and-time\ncost of device-edge co-inference in presence of wireless channel fading. The\nproblem is challenging because the model placement and model splitting\ndecisions are strongly coupled, while involving two different time scales. We\nfirst tackle online model splitting by formulating an optimal stopping problem,\nwhere the finite horizon of the problem is determined by the model placement\ndecision. In addition to deriving the optimal model splitting rule based on\nbackward induction, we further investigate a simple one-stage look-ahead rule,\nfor which we are able to obtain analytical expressions of the model splitting\ndecision. The analysis is useful for us to efficiently optimize the model\nplacement decision in a larger time scale. In particular, we obtain a\nclosed-form model placement solution for the fully-connected multilayer\nperceptron with equal neurons. Simulation results validate the superior\nperformance of the joint optimal model placement and splitting with various DNN\nstructures.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 06:55:04 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Yan", "Jia", ""], ["Bi", "Suzhi", ""], ["Zhang", "Ying-Jun Angela", ""]]}, {"id": "2105.13732", "submitter": "\\'Alvaro Garc\\'ia-P\\'erez", "authors": "Silvia Bonomi and Antonella Del Pozzo and \\'Alvaro Garc\\'ia-P\\'erez\n  and Sara Tucci-Piergiovanni", "title": "SoK: Achieving State Machine Replication in Blockchains based on\n  Repeated Consensus", "comments": "10 pages, 2 figures, 5 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper revisits the ubiquitous problem of achieving state machine\nreplication in blockchains based on repeated consensus, like Tendermint. To\nachieve state machine replication in blockchains built on top of consensus, one\nneeds to guarantee fairness of user transactions. A huge body of work has been\ncarried out on the relation between state machine replication and consensus in\nthe past years, in a variety of system models and with respect to varied\nproblem specifications. We systematize this work by proposing novel and\nrigorous abstractions for state machine replication and repeated consensus in a\nsystem model that accounts for realistic blockchains in which blocks may\ncontain several transactions issued by one or more users, and where validity\nand order of transactions within a block is determined by an external\napplication-dependent function that can capture various approaches for\norder-fairness in the literature. Based on these abstractions, we propose a\nreduction from state machine replication to repeated consensus, such that user\nfairness is achieved using the consensus module as a black box. This approach\nallows to achieve fairness as an add-on on top of preexisting consensus modules\nin blockchains based on repeated consensus.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 10:57:15 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Bonomi", "Silvia", ""], ["Del Pozzo", "Antonella", ""], ["Garc\u00eda-P\u00e9rez", "\u00c1lvaro", ""], ["Tucci-Piergiovanni", "Sara", ""]]}, {"id": "2105.13797", "submitter": "Guangye Chen", "authors": "Guangye Chen, Luis Chac\\'on, Truong B. Nguyen", "title": "An unsupervised machine-learning checkpoint-restart algorithm using\n  Gaussian mixtures for particle-in-cell simulations", "comments": "Extended abstract for Supercheck21. arXiv admin note: substantial\n  text overlap with arXiv:2007.12273", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an unsupervised machine-learning checkpoint-restart (CR) lossy\nalgorithm for particle-in-cell (PIC) algorithms using Gaussian mixtures (GM).\nThe algorithm features a particle compression stage and a particle\nreconstruction stage, where a continuum particle distribution function is\nconstructed and resampled, respectively. To guarantee fidelity of the CR\nprocess, we ensure the exact preservation of charge, momentum, and energy for\nboth compression and reconstruction stages, everywhere on the mesh. We also\nensure the preservation of Gauss' law after particle reconstruction. As a\nresult, the GM CR algorithm is shown to provide a clean, conservative restart\ncapability while potentially affording orders of magnitude savings in\ninput/output requirements. We demonstrate the algorithm using a recently\ndeveloped exactly energy- and charge-conserving PIC algorithm on physical\nproblems of interest, with compression factors $\\gtrsim75$ with no appreciable\nimpact on the quality of the restarted dynamics.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 01:38:02 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Chen", "Guangye", ""], ["Chac\u00f3n", "Luis", ""], ["Nguyen", "Truong B.", ""]]}, {"id": "2105.13845", "submitter": "Tanvir Ahamed", "authors": "Tanvir Ahamed, Bo Zou", "title": "Multi-Tier Adaptive Memory Programming and Cluster- and Job-based\n  Relocation for Distributed On-demand Crowdshipping", "comments": "46 pages, 9 figures, 4 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With rapid e-commerce growth, on-demand urban delivery is having a high time\nespecially for food, grocery, and retail, often requiring delivery in a very\nshort amount of time after an order is placed. This imposes significant\nfinancial and operational challenges for traditional vehicle-based delivery\nmethods. Crowdshipping, which employs ordinary people with a low pay rate and\nlimited time availability, has emerged as an attractive alternative. This paper\nproposes a multi-tier adaptive memory programming (M-TAMP) to tackle on-demand\nassignment of requests to crowdsourcees with spatially distributed request\norigins and destination and crowdsourcee starting points. M-TAMP starts with\nmultiple initial solutions constructed based on different plausible\ncontemplations in assigning requests to crowdsourcees, and organizes solution\nsearch through waves, phases, and steps, imitating both ocean waves and human\nmemory functioning while seeking the best solution. The assignment is further\nenforced by proactively relocating idle crowdsourcees, for which a\ncomputationally efficient cluster- and job-based strategy is devised. Numerical\nexperiments demonstrate the superiority of MTAMP over a number of existing\nmethods, and that relocation can greatly improve the efficiency of\ncrowdsourcee-request assignment.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 18:33:27 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Ahamed", "Tanvir", ""], ["Zou", "Bo", ""]]}, {"id": "2105.13855", "submitter": "Menglu Yu", "authors": "Menglu Yu, Chuan Wu, Bo Ji, Jia Liu", "title": "A Sum-of-Ratios Multi-Dimensional-Knapsack Decomposition for DNN\n  Resource Scheduling", "comments": null, "journal-ref": "in Proc. IEEE INFOCOM, Virtual Event, May 2021", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, to sustain the resource-intensive computational needs for\ntraining deep neural networks (DNNs), it is widely accepted that exploiting the\nparallelism in large-scale computing clusters is critical for the efficient\ndeployments of DNN training jobs. However, existing resource schedulers for\ntraditional computing clusters are not well suited for DNN training, which\nresults in unsatisfactory job completion time performance. The limitations of\nthese resource scheduling schemes motivate us to propose a new computing\ncluster resource scheduling framework that is able to leverage the special\nlayered structure of DNN jobs and significantly improve their job completion\ntimes. Our contributions in this paper are three-fold: i) We develop a new\nresource scheduling analytical model by considering DNN's layered structure,\nwhich enables us to analytically formulate the resource scheduling optimization\nproblem for DNN training in computing clusters; ii) Based on the proposed\nperformance analytical model, we then develop an efficient resource scheduling\nalgorithm based on the widely adopted parameter-server architecture using a\nsum-of-ratios multi-dimensional-knapsack decomposition (SMD) method to offer\nstrong performance guarantee; iii) We conduct extensive numerical experiments\nto demonstrate the effectiveness of the proposed schedule algorithm and its\nsuperior performance over the state of the art.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 14:08:38 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Yu", "Menglu", ""], ["Wu", "Chuan", ""], ["Ji", "Bo", ""], ["Liu", "Jia", ""]]}, {"id": "2105.13866", "submitter": "Yaroslav Golubev", "authors": "Vladislav Tankov, Yaroslav Golubev, Timofey Bryksin", "title": "Kotless: a Serverless Framework for Kotlin", "comments": "4 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent trends in Web development demonstrate an increased interest in\nserverless applications, i.e. applications that utilize computational resources\nprovided by cloud services on demand instead of requiring traditional server\nmanagement. This approach enables better resource management while being\nscalable, reliable, and cost-effective. However, it comes with a number of\norganizational and technical difficulties which stem from the interaction\nbetween the application and the cloud infrastructure, for example, having to\nset up a recurring task of reuploading updated files. In this paper, we present\nKotless - a Kotlin Serverless Framework. Kotless is a cloud-agnostic toolkit\nthat solves these problems by interweaving the deployed application into the\ncloud infrastructure and automatically generating the necessary deployment\ncode. This relieves developers from having to spend their time integrating and\nmanaging their applications instead of developing them. Kotless has proven its\ncapabilities and has been used to develop several serverless applications\nalready in production. Its source code is available at\nhttps://github.com/JetBrains/kotless, a tool demo can be found at\nhttps://www.youtube.com/watch?v=IMSakPNl3TY\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 10:01:52 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Tankov", "Vladislav", ""], ["Golubev", "Yaroslav", ""], ["Bryksin", "Timofey", ""]]}, {"id": "2105.13894", "submitter": "Paulo Silva Feitosa", "authors": "Paulo Silva, Thiago Emmanuel Pereira", "title": "Performance Evaluation of Snapshot Methods to Warm the Serverless Cold\n  Start", "comments": "in Portuguese", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.OS cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The serverless computing model strengthens the cloud computing tendency to\nabstract resource management. Serverless platforms are responsible for\ndeploying and scaling the developer's applications. Serverless also\nincorporated the pay-as-you-go billing model, which only considers the time\nspent processing client requests. Such a decision created a natural incentive\nfor improving the platform's efficient resource usage. This search for\nefficiency can lead to the cold start problem, which represents a delay to\nexecute serverless applications. Among the solutions proposed to deal with the\ncold start, those based on the snapshot method stand out. Despite the rich\nexploration of the technique, there is a lack of research that evaluates the\nsolution's trade-offs. In this direction, this work compares two solutions to\nmitigate the cold start: Prebaking and SEUSS. We analyzed the solution's\nperformance with functions of different levels of complexity: NoOp, a function\nthat renders Markdown to HTML, and a function that loads 41 MB of dependencies.\nPreliminary results indicated that Prebaking showed a 33% and 25% superior\nperformance to startup the NoOp and Markdown functions, respectively. Further\nanalysis also revealed that Prebaking's warmup mechanism reduced the Markdown\nfirst request processing time by 69%.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 14:57:49 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Silva", "Paulo", ""], ["Pereira", "Thiago Emmanuel", ""]]}, {"id": "2105.13906", "submitter": "Saif Ur Rehaman", "authors": "Saif Ur Rehman, Muhammad Rashid Razzaq, Muhammad Hadi Hussian", "title": "Training of SSD(Single Shot Detector) for Facial Detection using Nvidia\n  Jetson Nano", "comments": "7 Pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.DC eess.IV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this project, we have used the computer vision algorithm SSD (Single Shot\ndetector) computer vision algorithm and trained this algorithm from the dataset\nwhich consists of 139 Pictures. Images were labeled using Intel CVAT (Computer\nVision Annotation Tool)\n  We trained this model for facial detection. We have deployed our trained\nmodel and software in the Nvidia Jetson Nano Developer kit. Model code is\nwritten in Pytorch's deep learning framework. The programming language used is\nPython.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 15:16:24 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Rehman", "Saif Ur", ""], ["Razzaq", "Muhammad Rashid", ""], ["Hussian", "Muhammad Hadi", ""]]}, {"id": "2105.13967", "submitter": "Zhengchun Liu", "authors": "Zhengchun Liu, Ahsan Ali, Peter Kenesei, Antonino Miceli, Hemant\n  Sharma, Nicholas Schwarz, Dennis Trujillo, Hyunseung Yoo, Ryan Coffee, Ryan\n  Herbst, Jana Thayer, Chun Hong Yoon, Ian Foster", "title": "Bridge Data Center AI Systems with Edge Computing for Actionable\n  Information Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extremely high data rates at modern synchrotron and X-ray free-electron\nlasers (XFELs) light source beamlines motivate the use of machine learning\nmethods for data reduction, feature detection, and other purposes. Regardless\nof the application, the basic concept is the same: data collected in early\nstages of an experiment, data from past similar experiments, and/or data\nsimulated for the upcoming experiment are used to train machine learning models\nthat, in effect, learn specific characteristics of those data; these models are\nthen used to process subsequent data more efficiently than would\ngeneral-purpose models that lack knowledge of the specific dataset or data\nclass. Thus, a key challenge is to be able to train models with sufficient\nrapidity that they can be deployed and used within useful timescales. We\ndescribe here how specialized data center AI systems can be used for this\npurpose.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 16:47:01 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Liu", "Zhengchun", ""], ["Ali", "Ahsan", ""], ["Kenesei", "Peter", ""], ["Miceli", "Antonino", ""], ["Sharma", "Hemant", ""], ["Schwarz", "Nicholas", ""], ["Trujillo", "Dennis", ""], ["Yoo", "Hyunseung", ""], ["Coffee", "Ryan", ""], ["Herbst", "Ryan", ""], ["Thayer", "Jana", ""], ["Yoon", "Chun Hong", ""], ["Foster", "Ian", ""]]}, {"id": "2105.13980", "submitter": "Rustam Latypov", "authors": "Rustam Latypov and Jara Uitto", "title": "Deterministic 3-Coloring of Trees in the Sublinear MPC model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present deterministic $O(\\log^2 \\log n)$ time sublinear Massively Parallel\nComputation (MPC) algorithms for 3-coloring, maximal independent set and\nmaximal matching in trees with $n$ nodes. In accordance with the sublinear MPC\nregime, our algorithms run on machines that have memory as little as\n$O(n^\\delta)$ for any arbitrary constant $0<\\delta<1$. Furthermore, our\nalgorithms use only $O(n)$ global memory. Our main result is the 3-coloring\nalgorithm, which contrasts the probabilistic 4-coloring algorithm of Ghaffari,\nGrunau and Jin [DISC'20]. The maximal independent set and maximal matching\nalgorithms follow in $O(1)$ time after obtaining the coloring. The key\ningredient of our 3-coloring algorithm is an $O(\\log^2 \\log n)$ time MPC\nimplementation of a variant of the rake-and-compress tree decomposition used by\nChang and Pettie [FOCS'17], which is closely related to the $H$-partition by\nBarenboim and Elkin [PODC'08]. When restricting to trees of constant maximum\ndegree, we bring the runtime down to $O(\\log \\log n)$.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 17:03:28 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Latypov", "Rustam", ""], ["Uitto", "Jara", ""]]}, {"id": "2105.14004", "submitter": "Zhiyong Sun", "authors": "Zhiyong Sun, Anders Rantzer, Zhongkui Li, Anders Robertsson", "title": "Distributed adaptive stabilization", "comments": "16 Pages and 7 figures", "journal-ref": "Automatica: Volume 129, 109616 (1-13), July 2021", "doi": "10.1016/j.automatica.2021.109616", "report-no": null, "categories": "eess.SY cs.DC cs.MA cs.SY math.OC nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider distributed adaptive stabilization for uncertain\nmultivariable linear systems with a time-varying diagonal matrix gain. We show\nthat uncertain multivariable linear systems are stabilizable by diagonal matrix\nhigh gains if the system matrix is an H-matrix with positive diagonal entries.\nBased on matrix measure and stability theory for diagonally dominant systems,\nwe consider two classes of uncertain linear systems, and derive a threshold\ncondition to ensure their exponential stability by a monotonically increasing\ndiagonal gain matrix. When each individual gain function in the matrix gain is\nupdated by state-dependent functions using only local state information, the\nboundedness and convergence of both system states and adaptive matrix gains are\nguaranteed. We apply the adaptive distributed stabilization approach to\nadaptive synchronization control for large-scale complex networks consisting of\nnonlinear node dynamics and time-varying coupling weights. A unified framework\nfor adaptive synchronization is proposed that includes several general design\napproaches for adaptive coupling weights to guarantee network synchronization.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 17:28:29 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Sun", "Zhiyong", ""], ["Rantzer", "Anders", ""], ["Li", "Zhongkui", ""], ["Robertsson", "Anders", ""]]}, {"id": "2105.14088", "submitter": "Liang Luo", "authors": "Liang Luo, Jacob Nelson, Arvind Krishnamurthy, Luis Ceze", "title": "Cloud Collectives: Towards Cloud-aware Collectives forML Workloads with\n  Rank Reordering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  ML workloads are becoming increasingly popular in the cloud. Good cloud\ntraining performance is contingent on efficient parameter exchange among VMs.\nWe find that Collectives, the widely used distributed communication algorithms,\ncannot perform optimally out of the box due to the hierarchical topology of\ndatacenter networks and multi-tenancy nature of the cloudenvironment.In this\npaper, we present Cloud Collectives , a prototype that accelerates collectives\nby reordering theranks of participating VMs such that the communication pattern\ndictated by the selected collectives operation best exploits the locality in\nthe network.Collectives is non-intrusive, requires no code changes nor rebuild\nof an existing application, and runs without support from cloud providers. Our\npreliminary application of Cloud Collectives on allreduce operations in public\nclouds results in a speedup of up to 3.7x in multiple microbenchmarks and 1.3x\nin real-world workloads of distributed training of deep neural networks and\ngradient boosted decision trees using state-of-the-art frameworks.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 20:14:38 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Luo", "Liang", ""], ["Nelson", "Jacob", ""], ["Krishnamurthy", "Arvind", ""], ["Ceze", "Luis", ""]]}, {"id": "2105.14156", "submitter": "Kaustubh Shivdikar", "authors": "Kaustubh Shivdikar", "title": "SMASH: Sparse Matrix Atomic Scratchpad Hashing", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.17515.87840", "report-no": null, "categories": "cs.DC cs.AR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sparse matrices, more specifically SpGEMM kernels, are commonly found in a\nwide range of applications, spanning graph-based path-finding to machine\nlearning algorithms (e.g., neural networks). A particular challenge in\nimplementing SpGEMM kernels has been the pressure placed on DRAM memory. One\napproach to tackle this problem is to use an inner product method for the\nSpGEMM kernel implementation. While the inner product produces fewer\nintermediate results, it can end up saturating the memory bandwidth, given the\nhigh number of redundant fetches of the input matrix elements. Using an outer\nproduct-based SpGEMM kernel can reduce redundant fetches, but at the cost of\nincreased overhead due to extra computation and memory accesses for\nproducing/managing partial products.\n  In this thesis, we introduce a novel SpGEMM kernel implementation based on\nthe row-wise product approach. We leverage atomic instructions to merge\nintermediate partial products as they are generated. The use of atomic\ninstructions eliminates the need to create partial product matrices.\n  To evaluate our row-wise product approach, we map an optimized SpGEMM kernel\nto a custom accelerator designed to accelerate graph-based applications. The\ntargeted accelerator is an experimental system named PIUMA, being developed by\nIntel. PIUMA provides several attractive features, including fast context\nswitching, user-configurable caches, globally addressable memory, non-coherent\ncaches, and asynchronous pipelines. We tailor our SpGEMM kernel to exploit many\nof the features of the PIUMA fabric.\n  This thesis compares our SpGEMM implementation against prior solutions, all\nmapped to the PIUMA framework. We briefly describe some of the PIUMA\narchitecture features and then delve into the details of our optimized SpGEMM\nkernel. Our SpGEMM kernel can achieve 9.4x speedup as compared to competing\napproaches.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 00:22:50 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Shivdikar", "Kaustubh", ""]]}, {"id": "2105.14157", "submitter": "Tevfik Kosar", "authors": "Bing Zhang and Tevfik Kosar", "title": "SMURF: Efficient and Scalable Metadata Access for Distributed\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In parallel with big data processing and analysis dominating the usage of\ndistributed and cloud infrastructures, the demand for distributed metadata\naccess and transfer has increased. In many application domains, the volume of\ndata generated exceeds petabytes, while the corresponding metadata amounts to\nterabytes or even more. This paper proposes a novel solution for efficient and\nscalable metadata access for distributed applications across wide-area\nnetworks, dubbed SMURF. Our solution combines novel pipelining and concurrent\ntransfer mechanisms with reliability, provides distributed continuum caching\nand prefetching strategies to sidestep fetching latency, and achieves scalable\nand high-performance metadata fetch/prefetch services in the cloud. We also\nstudy the phenomenon of semantic locality in real trace logs, which is not well\nutilized in metadata access prediction. We implement a novel prefetch predictor\nbased on this observation and compare it with three existing state-of-the-art\nprefetch schemes on Yahoo! Hadoop audit traces. By effectively caching and\nprefetching metadata based on the access patterns, our continuum caching and\nprefetching mechanism significantly improves local cache hit rate and reduces\nthe average fetching latency. We replayed approximately 20 Million metadata\naccess operations from real audit traces, in which our system achieved 90%\naccuracy during prefetch prediction and reduced the average fetch latency by\n50% compared to the state-of-the-art mechanisms.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 00:27:27 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zhang", "Bing", ""], ["Kosar", "Tevfik", ""]]}, {"id": "2105.14216", "submitter": "Jiahao Xie", "authors": "Jiahao Xie, Chao Zhang, Yunsong Zhang, Zebang Shen, Hui Qian", "title": "A Federated Learning Framework for Nonconvex-PL Minimax Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a general class of nonconvex-PL minimax problems in the\ncross-device federated learning setting. Although nonconvex-PL minimax problems\nhave received a lot of interest in recent years, existing algorithms do not\napply to the cross-device federated learning setting which is substantially\ndifferent from conventional distributed settings and poses new challenges. To\nbridge this gap, we propose an algorithmic framework named FedSGDA. FedSGDA\nperforms multiple local update steps on a subset of active clients in each\nround and leverages global gradient estimates to correct the bias in local\nupdate directions. By incorporating FedSGDA with two representative global\ngradient estimators, we obtain two specific algorithms. We establish\nconvergence rates of the proposed algorithms by using novel potential\nfunctions. Experimental results on synthetic and real data corroborate our\ntheory and demonstrate the effectiveness of our algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 05:18:02 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Xie", "Jiahao", ""], ["Zhang", "Chao", ""], ["Zhang", "Yunsong", ""], ["Shen", "Zebang", ""], ["Qian", "Hui", ""]]}, {"id": "2105.14295", "submitter": "Muhui Jiang", "authors": "Muhui Jiang, Lin Ma, Yajin Zhou, Qiang Liu, Cen Zhang, Zhi Wang, Xiapu\n  Luo, Lei Wu, Kui Ren", "title": "ECMO: Peripheral Transplantation to Rehost Embedded Linux Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic analysis based on the full-system emulator QEMU is widely used for\nvarious purposes. However, it is challenging to run firmware images of embedded\ndevices in QEMU, especially theprocess to boot the Linux kernel (we call this\nprocess rehosting the Linux kernel in this paper.) That's because embedded\ndevices usually use different system-on-chips (SoCs) from multiple vendors\nandonly a limited number of SoCs are currently supported in QEMU.\n  In this work, we propose a technique calledperipheral transplantation. The\nmain idea is to transplant the device drivers of designated peripherals into\nthe Linux kernel binary. By doing so, it can replace the peripherals in the\nkernel that are currently unsupported in QEMU with supported ones, thus making\nthe Linux kernel rehostable. After that, various applications can be built\nupon.\n  We implemented this technique inside a prototype system called ECMO and\napplied it to 824 firmware images, which consist of 17 kernel versions, 37\ndevice models, and 24 vendors. The resultshows that ECMO can successfully\ntransplant peripherals for all the 824 Linux kernels. Among them, 719 kernels\ncan be successfully rehosted, i.e., launching a user-space shell (87.3% success\nrate). The failed cases are mainly because the root file system format (ramfs)\nis not supported by the kernel. We further build three applications, i.e.,\nkernel crash analysis, rootkit forensic analysis, and kernel fuzzing, based on\nthe rehosted kernels to demonstrate the usage scenarios of ECMO.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 13:14:24 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Jiang", "Muhui", ""], ["Ma", "Lin", ""], ["Zhou", "Yajin", ""], ["Liu", "Qiang", ""], ["Zhang", "Cen", ""], ["Wang", "Zhi", ""], ["Luo", "Xiapu", ""], ["Wu", "Lei", ""], ["Ren", "Kui", ""]]}, {"id": "2105.14352", "submitter": "Rafael Ferreira da Silva", "authors": "Tain\\~a Coleman, Henri Casanova, Lo\\\"ic Pottier, Manav Kaushik, Ewa\n  Deelman, Rafael Ferreira da Silva", "title": "WfCommons: A Framework for Enabling Scientific Workflow Research and\n  Development", "comments": "arXiv admin note: substantial text overlap with arXiv:2009.00250", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scientific workflows are a cornerstone of modern scientific computing. They\nare used to describe complex computational applications that require efficient\nand robust management of large volumes of data, which are typically\nstored/processed on heterogeneous, distributed resources. The workflow research\nand development community has employed a number of methods for the quantitative\nevaluation of existing and novel workflow algorithms and systems. In\nparticular, a common approach is to simulate workflow executions. In previous\nworks, we have presented a collection of tools that have been adopted by the\ncommunity for conducting workflow research. Despite their popularity, they\nsuffer from several shortcomings that prevent easy adoption, maintenance, and\nconsistency with the evolving structures and computational requirements of\nproduction workflows. In this work, we present WfCommons, a framework that\nprovides a collection of tools for analyzing workflow executions, for producing\ngenerators of synthetic workflows, and for simulating workflow executions. We\ndemonstrate the realism of the generated synthetic workflows by comparing their\nsimulated executions to real workflow executions. We also contrast these\nresults with results obtained when using the previously available collection of\ntools. We find that the workflow generators that are automatically constructed\nby our framework not only generate representative same-scale workflows (i.e.,\nwith structures and task characteristics distributions that resemble those\nobserved in real-world workflows), but also do so at scales larger than that of\navailable real-world workflows. Finally, we conduct a case study to demonstrate\nthe usefulness of our framework for estimating the energy consumption of\nlarge-scale workflow executions.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 18:31:14 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Coleman", "Tain\u00e3", ""], ["Casanova", "Henri", ""], ["Pottier", "Lo\u00efc", ""], ["Kaushik", "Manav", ""], ["Deelman", "Ewa", ""], ["da Silva", "Rafael Ferreira", ""]]}, {"id": "2105.14416", "submitter": "Qiongxiu Li", "authors": "Qiongxiu Li, Richard Heusdens and Mads Gr{\\ae}sb{\\o}ll Christensen", "title": "Communication efficient privacy-preserving distributed optimization\n  using adaptive differential quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC eess.SP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Privacy issues and communication cost are both major concerns in distributed\noptimization. There is often a trade-off between them because the encryption\nmethods required for privacy-preservation often incur expensive communication\nbandwidth. To address this issue, we, in this paper, propose a\nquantization-based approach to achieve both communication efficient and\nprivacy-preserving solutions in the context of distributed optimization. By\ndeploying an adaptive differential quantization scheme, we allow each node in\nthe network to achieve its optimum solution with a low communication cost while\nkeeping its private data unrevealed. Additionally, the proposed approach is\ngeneral and can be applied in various distributed optimization methods, such as\nthe primal-dual method of multipliers (PDMM) and the alternating direction\nmethod of multipliers (ADMM). Moveover, we consider two widely used adversary\nmodels: passive and eavesdropping. Finally, we investigate the properties of\nthe proposed approach using different applications and demonstrate its superior\nperformance in terms of several parameters including accuracy, privacy, and\ncommunication cost.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 02:43:41 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Li", "Qiongxiu", ""], ["Heusdens", "Richard", ""], ["Christensen", "Mads Gr\u00e6sb\u00f8ll", ""]]}, {"id": "2105.14450", "submitter": "Yang You", "authors": "Zhengda Bian and Qifan Xu and Boxiang Wang and Yang You", "title": "Maximizing Parallelism in Distributed Training for Huge Neural Networks", "comments": "Technical Report of NUS HPC-AI Lab (https://ai.comp.nus.edu.sg). The\n  leading two authors have equal contributions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent Natural Language Processing techniques have been refreshing the\nstate-of-the-art performance at an incredible speed. Training huge language\nmodels is therefore an imperative demand in both industry and academy. However,\nhuge language models impose challenges to both hardware and software. Graphical\nprocessing units (GPUs) are iterated frequently to meet the exploding demand,\nand a variety of ASICs like TPUs are spawned. However, there is still a tension\nbetween the fast growth of the extremely huge models and the fact that Moore's\nlaw is approaching the end. To this end, many model parallelism techniques are\nproposed to distribute the model parameters to multiple devices, so as to\nalleviate the tension on both memory and computation. Our work is the first to\nintroduce a 3-dimensional model parallelism for expediting huge language\nmodels. By reaching a perfect load balance, our approach presents smaller\nmemory and communication cost than existing state-of-the-art 1-D and 2-D model\nparallelism. Our experiments on 64 TACC's V100 GPUs show that our 3-D\nparallelism outperforms the 1-D and 2-D parallelism with 2.32x and 1.57x\nspeedup, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 07:41:08 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Bian", "Zhengda", ""], ["Xu", "Qifan", ""], ["Wang", "Boxiang", ""], ["You", "Yang", ""]]}, {"id": "2105.14500", "submitter": "Boxiang Wang", "authors": "Boxiang Wang, Qifan Xu, Zhengda Bian, Yang You", "title": "2.5-dimensional distributed model training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data parallelism does a good job in speeding up the training. However, when\nit comes to the case when the memory of a single device can not host a whole\nmodel, data parallelism would not have the chance to do anything. Another\noption is to split the model by operator, or horizontally. Megatron-LM\nintroduced a 1-Dimensional distributed method to use GPUs to speed up the\ntraining process. Optimus is a 2D solution for distributed tensor parallelism.\nHowever, these methods have a high communication overhead and a low scaling\nefficiency on large-scale computing clusters. To solve this problem, we\ninvestigate the 2.5-Dimensional distributed tensor parallelism.Introduced by\nSolomonik et al., 2.5-Dimensional Matrix Multiplication developed an effective\nmethod to perform multiple Cannon's algorithm at the same time to increase the\nefficiency. With many restrictions of Cannon's Algorithm and a huge amount of\nshift operation, we need to invent a new method of 2.5-dimensional matrix\nmultiplication to enhance the performance. Absorbing the essence from both\nSUMMA and 2.5-Dimensional Matrix Multiplication, we introduced SUMMA2.5-LM for\nlanguage models to overcome the abundance of unnecessary transmission loss\nresult from the increasing size of language model parallelism. Compared to\nprevious 1D and 2D model parallelization of language models, our SUMMA2.5-LM\nmanaged to reduce the transmission cost on each layer, which could get a 1.45X\nefficiency according to our weak scaling result between 2.5-D [4,4,4]\narrangement and 2-D [8,8,1] arrangement.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 11:06:49 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Wang", "Boxiang", ""], ["Xu", "Qifan", ""], ["Bian", "Zhengda", ""], ["You", "Yang", ""]]}, {"id": "2105.14607", "submitter": "Adnan Akhunzada", "authors": "Adnan Akhunzada (Senior Member, IEEE), Sherali Zeadally (Senior\n  Member, IEEE), Saif ul Islam", "title": "Power and Performance Efficient SDN-Enabled Fog Architecture", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Software Defined Networks (SDNs) have dramatically simplified network\nmanagement. However, enabling pure SDNs to respond in real-time while handling\nmassive amounts of data still remains a challenging task. In contrast, fog\ncomputing has strong potential to serve large surges of data in real-time. SDN\ncontrol plane enables innovation, and greatly simplifies network operations and\nmanagement thereby providing a promising solution to implement energy and\nperformance aware SDN-enabled fog computing. Besides, power efficiency and\nperformance evaluation in SDN-enabled fog computing is an area that has not yet\nbeen fully explored by the research community. We present a novel SDN-enabled\nfog architecture to improve power efficacy and performance by leveraging\ncooperative and non-cooperative policy-based computing. Preliminary results\nfrom extensive simulation demonstrate an improvement in the power utilization\nas well as the overall performance (i.e., processing time, response time).\nFinally, we discuss several open research issues that need further\ninvestigation in the future.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 19:28:52 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Akhunzada", "Adnan", "", "Senior Member, IEEE"], ["Zeadally", "Sherali", "", "Senior\n  Member, IEEE"], ["Islam", "Saif ul", ""]]}, {"id": "2105.14613", "submitter": "Priyaa Thavasimani Dr", "authors": "Priyaa Thavasimani, Anna Scaife", "title": "Square Kilometre Array : Processing Voluminous MeerKAT Data on IRIS", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Processing astronomical data often comes with huge challenges with regards to\ndata management as well as data processing. MeerKAT telescope is one of the\nprecursor telescopes of the World's largest observatory - Square Kilometre\nArray. So far, MeerKAT data was processed using the South African computing\nfacility i.e. IDIA, and exploited to make ground-breaking discoveries. However,\nto process MeerKAT data on UK's IRIS computing facility requires new\nimplementation of the MeerKAT pipeline. This paper focuses on how to transfer\nMeerKAT data from the South African site to UK's IRIS systems for processing.\nWe discuss about our RapifXfer Data transfer framework for transferring the\nMeerKAT data from South Africa to the UK, and the MeerKAT job processing\nframework pertaining to the UK's IRIS resources.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 20:01:48 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Thavasimani", "Priyaa", ""], ["Scaife", "Anna", ""]]}, {"id": "2105.14618", "submitter": "Lun Wang", "authors": "Lun Wang, Qi Pang, Shuai Wang and Dawn Song", "title": "FED-$\\chi^2$: Privacy Preserving Federated Correlation Test", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose the first secure federated $\\chi^2$-test protocol\nFed-$\\chi^2$. To minimize both the privacy leakage and the communication cost,\nwe recast $\\chi^2$-test to the second moment estimation problem and thus can\ntake advantage of stable projection to encode the local information in a short\nvector. As such encodings can be aggregated with only summation, secure\naggregation can be naturally applied to hide the individual updates. We\nformally prove the security guarantee of Fed-$\\chi^2$ that the joint\ndistribution is hidden in a subspace with exponential possible distributions.\nOur evaluation results show that Fed-$\\chi^2$ achieves negligible accuracy\ndrops with small client-side computation overhead. In several real-world case\nstudies, the performance of Fed-$\\chi^2$ is comparable to the centralized\n$\\chi^2$-test.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 20:29:59 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Wang", "Lun", ""], ["Pang", "Qi", ""], ["Wang", "Shuai", ""], ["Song", "Dawn", ""]]}, {"id": "2105.14675", "submitter": "Huanle Zhang", "authors": "Huanle Zhang, Jeonghoon Kim", "title": "Towards a Federated Learning Framework for Heterogeneous Devices of\n  Internet of Things", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning (FL) has received a significant amount of attention in the\nindustry and research community due to its capability of keeping data on local\ndevices. To aggregate the gradients of local models to train the global model,\nexisting works require that the global model and the local models are the same.\nHowever, Internet of Things (IoT) devices are inherently diverse regarding\ncomputation speed and onboard memory. In this paper, we propose an FL framework\ntargeting the heterogeneity of IoT devices. Specifically, local models are\ncompressed from the global model, and the gradients of the compressed local\nmodels are used to update the global model. We conduct preliminary experiments\nto illustrate that our framework can facilitate the design of IoT-aware FL.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 02:08:36 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zhang", "Huanle", ""], ["Kim", "Jeonghoon", ""]]}, {"id": "2105.14708", "submitter": "Kang Wei", "authors": "Xiumei Deng, Jun Li, Chuan Ma, Kang Wei, Long Shi, Ming Ding, Wen\n  Chen, and H. Vincent Poor", "title": "On Dynamic Resource Allocation for Blockchain Assisted Federated\n  Learning over Wireless Channels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain assisted federated learning (BFL) has been intensively studied as\na promising technology to process data at the network edge in a distributed\nmanner. In this paper, we focus on BFL over wireless environments with varying\nchannels and energy harvesting at clients. We are interested in proposing\ndynamic resource allocation (i.e., transmit power, computation frequency for\nmodel training and block mining for each client) and client scheduling (DRACS)\nto maximize the long-term time average (LTA) training data size with an LTA\nenergy consumption constraint. Specifically, we first define the Lyapunov drift\nby converting the LTA energy consumption to a queue stability constraint. Then,\nwe construct a Lyapunov drift-plus-penalty ratio function to decouple the\noriginal stochastic problem into multiple deterministic optimizations along the\ntime line. Our construction is capable of dealing with uneven durations of\ncommunication rounds. To make the one-shot deterministic optimization problem\nof combinatorial fractional form tractable, we next convert the fractional\nproblem into a subtractive-form one by Dinkelbach method, which leads to the\nasymptotically optimal solution in an iterative way. In addition, the\nclosed-form of the optimal resource allocation and client scheduling is\nobtained in each iteration with a low complexity. Furthermore, we conduct the\nperformance analysis for the proposed algorithm, and discover that the LTA\ntraining data size and energy consumption obey an [$\\mathcal{O}(1/V)$,\n$\\mathcal{O}(\\sqrt{V})$] trade-off. Our experimental results show that the\nproposed algorithm can provide both higher learning accuracy and faster\nconvergence with limited time and energy consumption based on the MNIST and\nFashion-MNIST datasets.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 05:01:01 GMT"}, {"version": "v2", "created": "Sun, 20 Jun 2021 12:53:41 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Deng", "Xiumei", ""], ["Li", "Jun", ""], ["Ma", "Chuan", ""], ["Wei", "Kang", ""], ["Shi", "Long", ""], ["Ding", "Ming", ""], ["Chen", "Wen", ""], ["Poor", "H. Vincent", ""]]}, {"id": "2105.14772", "submitter": "Anis Elgabli", "authors": "Anis Elgabli, Chaouki Ben Issaid, Amrit S. Bedi, Mehdi Bennis, Vaneet\n  Aggarwal", "title": "Energy-Efficient and Federated Meta-Learning via Projected Stochastic\n  Gradient Ascent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose an energy-efficient federated meta-learning\nframework. The objective is to enable learning a meta-model that can be\nfine-tuned to a new task with a few number of samples in a distributed setting\nand at low computation and communication energy consumption. We assume that\neach task is owned by a separate agent, so a limited number of tasks is used to\ntrain a meta-model. Assuming each task was trained offline on the agent's local\ndata, we propose a lightweight algorithm that starts from the local models of\nall agents, and in a backward manner using projected stochastic gradient ascent\n(P-SGA) finds a meta-model. The proposed method avoids complex computations\nsuch as computing hessian, double looping, and matrix inversion, while\nachieving high performance at significantly less energy consumption compared to\nthe state-of-the-art methods such as MAML and iMAML on conducted experiments\nfor sinusoid regression and image classification tasks.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 08:15:44 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Elgabli", "Anis", ""], ["Issaid", "Chaouki Ben", ""], ["Bedi", "Amrit S.", ""], ["Bennis", "Mehdi", ""], ["Aggarwal", "Vaneet", ""]]}, {"id": "2105.14845", "submitter": "Muhammad Bilal", "authors": "Muhammad Bilal and Marco Canini and Rodrigo Fonseca and Rodrigo\n  Rodrigues", "title": "With Great Freedom Comes Great Opportunity: Rethinking Resource\n  Allocation for Serverless Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current serverless offerings give users a limited degree of flexibility for\nconfiguring the resources allocated to their function invocations by either\ncoupling memory and CPU resources together or providing no knobs at all. These\nconfiguration choices simplify resource allocation decisions on behalf of\nusers, but at the same time, create deployments that are resource inefficient.\n  In this paper, we take a principled approach to the problem of resource\nallocation for serverless functions, allowing this choice to be made in an\nautomatic way that leads to the best combination of performance and cost. In\nparticular, we systematically explore the opportunities that come with\ndecoupling memory and CPU resource allocations and also enabling the use of\ndifferent VM types. We find a rich trade-off space between performance and\ncost. The provider can use this in a number of ways: from exposing all these\nparameters to the user, to eliciting preferences for performance and cost from\nusers, or by simply offering the same performance with lower cost. This\nflexibility can also enable the provider to optimize its resource utilization\nand enable a cost-effective service with predictable performance.\n  Our results show that, by decoupling memory and CPU allocation, there is\npotential to have up to 40% lower execution cost than the preset coupled\nconfigurations that are the norm in current serverless offerings. Similarly,\nmaking the correct choice of VM instance type can provide up to 50% better\nexecution time. Furthermore, we demonstrate that providers can utilize\ndifferent instance types for the same functions to maximize resource\nutilization while providing performance within 10-20% of the best resource\nconfiguration for each respective function.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 09:59:43 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Bilal", "Muhammad", ""], ["Canini", "Marco", ""], ["Fonseca", "Rodrigo", ""], ["Rodrigues", "Rodrigo", ""]]}, {"id": "2105.15023", "submitter": "Zolt\\'an Zvara", "authors": "Zolt\\'an Zvara, P\\'eter G.N. Szab\\'o, Bal\\'azs Barnab\\'as L\\'or\\'ant\n  and Andr\\'as A. Bencz\\'ur", "title": "System-aware dynamic partitioning for batch and streaming workloads", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  When processing data streams with highly skewed and nonstationary key\ndistributions, we often observe overloaded partitions when the hash\npartitioning fails to balance data correctly. To avoid slow tasks that delay\nthe completion of the whole stage of computation, it is necessary to apply\nadaptive, on-the-fly partitioning that continuously recomputes an optimal\npartitioner, given the observed key distribution. While such solutions exist\nfor batch processing of static data sets and stateless stream processing, the\ntask is difficult for long-running stateful streaming jobs where key\ndistribution changes over time. Careful checkpointing and operator state\nmigration is necessary to change the partitioning while the operation is\nrunning.\n  Our key result is a lightweight on-the-fly Dynamic Repartitioning (DR) module\nfor distributed data processing systems (DDPS), including Apache Spark and\nFlink, which improves the performance with negligible overhead. DR can\nadaptively repartition data during execution using our Key Isolator Partitioner\n(KIP). In our experiments with real workloads and power-law distributions, we\nreach a speedup of 1.5-6 for a variety of Spark and Flink jobs.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 15:04:18 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zvara", "Zolt\u00e1n", ""], ["Szab\u00f3", "P\u00e9ter G. N.", ""], ["L\u00f3r\u00e1nt", "Bal\u00e1zs Barnab\u00e1s", ""], ["Bencz\u00far", "Andr\u00e1s A.", ""]]}, {"id": "2105.15105", "submitter": "Davide Callegaro", "authors": "Davide Callegaro and Marco Levorato and Francesco Restuccia", "title": "SeReMAS: Self-Resilient Mobile Autonomous Systems Through Predictive\n  Edge Computing", "comments": "Corrected typo in title", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge computing enables Mobile Autonomous Systems (MASs) to execute continuous\nstreams of heavy-duty mission-critical processing tasks, such as real-time\nobstacle detection and navigation. However, in practical applications, erratic\npatterns in channel quality, network load, and edge server load can interrupt\nthe task flow execution, which necessarily leads to severe disruption of the\nsystem's key operations. Existing work has mostly tackled the problem with\nreactive approaches, which cannot guarantee task-level reliability. Conversely,\nin this paper we focus on learning-based predictive edge computing to achieve\nself-resilient task offloading. By conducting a preliminary experimental\nevaluation, we show that there is no dominant feature that can predict the\nedge-MAS system reliability, which calls for an ensemble and selection of\nweaker features. To tackle the complexity of the problem, we propose SeReMAS, a\ndata-driven optimization framework. We first mathematically formulate a\nRedundant Task Offloading Problem (RTOP), where a MAS may connect to multiple\nedge servers for redundancy, and needs to select which server(s) to transmit\nits computing tasks in order to maximize the probability of task execution\nwhile minimizing channel and edge resource utilization. We then create a\npredictor based on Deep Reinforcement Learning (DRL), which produces the\noptimum task assignment based on application-, network- and telemetry-based\nfeatures. We prototype SeReMAS on a testbed composed by a drone, mounting a\nPixHawk flight controller, a Jetson Nano board, and three 802.11n WiFi\ninterfaces. We extensively evaluate SeReMAS by considering an application where\none drone offloads high-resolution images for real-time analysis to three edge\nservers on the ground. Experimental results show that SeReMAS improves task\nexecution probability by $17\\%$ with respect to existing reactive-based\napproaches.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 05:15:44 GMT"}, {"version": "v2", "created": "Sun, 11 Jul 2021 16:59:05 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Callegaro", "Davide", ""], ["Levorato", "Marco", ""], ["Restuccia", "Francesco", ""]]}, {"id": "2105.15147", "submitter": "Mohammad Sina Kiarostami", "authors": "Mohammad Sina Kiarostami", "title": "Comparing Two Different Approaches in Big Data and Business Analysis for\n  Churn Prediction with the Focus on How Apache Spark Employed", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Due to the significant importance of Big Data analysis, especially in\nbusiness-related topics such as improving services, finding potential\ncustomers, and selecting practical approaches to manage income and expenses,\nmany companies attempt to collaborate with scientists to find how, why, and\nwhat they should analysis. In this work, we would like to compare and discuss\ntwo different approaches that employed in business analysis topic in Big Data\nwith more consideration on how they utilized Spark. Both studies have\ninvestigated Churn Prediction as their case study for their proposed approaches\nsince it is an essential topic in business analysis for companies to recognize\na customer intends to leave or stop using their services. Here, we focus on\nApache Spark since it has provided several solutions to handle a massive amount\nof data in recent years efficiently. This feature in Spark makes it one of the\nmost robust candidate tools to upfront with a Big Data problem, particularly\ntime and resource are concerns.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 12:19:30 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Kiarostami", "Mohammad Sina", ""]]}]