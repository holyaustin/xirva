[{"id": "1209.0410", "submitter": "George Teodoro", "authors": "George Teodoro, Eduardo Valle, Nathan Mariano, Ricardo Torres, Wagner\n  Meira Jr, Joel H. Saltz", "title": "Approximate Similarity Search for Online Multimedia Services on\n  Distributed CPU-GPU Platforms", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.DB cs.DC", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Similarity search in high-dimentional spaces is a pivotal operation found a\nvariety of database applications. Recently, there has been an increase interest\nin similarity search for online content-based multimedia services. Those\nservices, however, introduce new challenges with respect to the very large\nvolumes of data that have to be indexed/searched, and the need to minimize\nresponse times observed by the end-users. Additionally, those users dynamically\ninteract with the systems creating fluctuating query request rates, requiring\nthe search algorithm to adapt in order to better utilize the underline hardware\nto reduce response times. In order to address these challenges, we introduce\nhypercurves, a flexible framework for answering approximate k-nearest neighbor\n(kNN) queries for very large multimedia databases, aiming at online\ncontent-based multimedia services. Hypercurves executes on hybrid CPU--GPU\nenvironments, and is able to employ those devices cooperatively to support\nmassive query request rates. In order to keep the response times optimal as the\nrequest rates vary, it employs a novel dynamic scheduler to partition the work\nbetween CPU and GPU. Hypercurves was throughly evaluated using a large database\nof multimedia descriptors. Its cooperative CPU--GPU execution achieved\nperformance improvements of up to 30x when compared to the single CPU-core\nversion. The dynamic work partition mechanism reduces the observed query\nresponse times in about 50% when compared to the best static CPU--GPU task\npartition configuration. In addition, Hypercurves achieves superlinear\nscalability in distributed (multi-node) executions, while keeping a high\nguarantee of equivalence with its sequential version --- thanks to the proof of\nprobabilistic equivalence, which supported its aggressive parallelization\ndesign.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2012 17:12:59 GMT"}], "update_date": "2012-09-04", "authors_parsed": [["Teodoro", "George", ""], ["Valle", "Eduardo", ""], ["Mariano", "Nathan", ""], ["Torres", "Ricardo", ""], ["Meira", "Wagner", "Jr"], ["Saltz", "Joel H.", ""]]}, {"id": "1209.0851", "submitter": "Ehsan Saboori Mr.", "authors": "Ehsan Saboori, Shahriar Mohammadi, Shafigh Parsazad", "title": "A new scheduling algorithm for server farms load balancing", "comments": "4 Pages", "journal-ref": "2010 2nd International Conference on Industrial and Information\n  Systems (IIS), vol.1, no., pp.417-420, 2010", "doi": "10.1109/INDUSIS.2010.5565821", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a new scheduling algorithm to distribute jobs in server\nfarm systems. The proposed algorithm overcomes the starvation caused by SRPT\n(Shortest Remaining Processing Time). This algorithm is used in process\nscheduling in operating system approach. The algorithm was developed to be used\nin dispatcher scheduling. This algorithm is non-preemptive discipline, similar\nto SRPT, in which the priority of each job depends on its estimated run time,\nand also the amount of time it has spent on waiting. Tasks in the servers are\nserved in order of priority to optimize the system response time. The\nexperiments show that the mean round around time is reduced in the server farm\nsystem.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2012 02:53:30 GMT"}], "update_date": "2012-09-06", "authors_parsed": [["Saboori", "Ehsan", ""], ["Mohammadi", "Shahriar", ""], ["Parsazad", "Shafigh", ""]]}, {"id": "1209.0948", "submitter": "Ian Sommerville Prof", "authors": "Ian Sommerville", "title": "Teaching cloud computing: a software engineering perspective", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This short papers discusses the issues of teaching cloud computing from a\nsoftware engineering rather than a business perspective. It discusses what\ntopics might be covered in a senior course on cloud software engineering.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2012 12:31:34 GMT"}], "update_date": "2012-09-06", "authors_parsed": [["Sommerville", "Ian", ""]]}, {"id": "1209.0960", "submitter": "Markus Blatt", "authors": "Markus Blatt, Olaf Ippisch, Peter Bastian", "title": "A Massively Parallel Algebraic Multigrid Preconditioner based on\n  Aggregation for Elliptic Problems with Heterogeneous Coefficients", "comments": "22 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a massively parallel algebraic multigrid method based on\nnon-smoothed aggregation. It is especially suited for solving heterogeneous\nelliptic problems as it uses a greedy heuristic algorithm for the aggregation\nthat detects changes in the coefficients and prevents aggregation across them.\nUsing decoupled aggregation on each process with data agglomeration onto fewer\nprocesses on the coarse level, it weakly scales well in terms of both total\ntime to solution and time per iteration to nearly 300,000 cores. Because of\nsimple piecewise constant interpolation between the levels, its memory\nconsumption is low and allows solving problems with more than 100,000,000,000\ndegrees of freedom.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2012 13:07:36 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2013 15:12:57 GMT"}], "update_date": "2013-10-01", "authors_parsed": [["Blatt", "Markus", ""], ["Ippisch", "Olaf", ""], ["Bastian", "Peter", ""]]}, {"id": "1209.1076", "submitter": "Konstantinos Tsianos", "authors": "Konstantinos I. Tsianos and Sean Lawlor and Michael G. Rabbat", "title": "Communication/Computation Tradeoffs in Consensus-Based Distributed\n  Optimization", "comments": "10 Pages, 3 Figures, Appearing at NIPS 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the scalability of consensus-based distributed optimization\nalgorithms by considering two questions: How many processors should we use for\na given problem, and how often should they communicate when communication is\nnot free? Central to our analysis is a problem-specific value $r$ which\nquantifies the communication/computation tradeoff. We show that organizing the\ncommunication among nodes as a $k$-regular expander graph (Reingold, Vadhan,\nand Wigderson, 2002) yields speedups, while when all pairs of nodes communicate\n(as in a complete graph), there is an optimal number of processors that depends\non $r$. Surprisingly, a speedup can be obtained, in terms of the time to reach\na fixed level of accuracy, by communicating less and less frequently as the\ncomputation progresses. Experiments on a real cluster solving metric learning\nand non-smooth convex minimization tasks demonstrate strong agreement between\ntheory and practice.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2012 18:45:21 GMT"}], "update_date": "2012-09-06", "authors_parsed": [["Tsianos", "Konstantinos I.", ""], ["Lawlor", "Sean", ""], ["Rabbat", "Michael G.", ""]]}, {"id": "1209.1358", "submitter": "Sebastien Tixeuil", "authors": "Alexandre Maurer (LIP6, LINCS), S\\'ebastien Tixeuil (LIP6, LINCS, IUF)", "title": "On Byzantine Broadcast in Loosely Connected Networks", "comments": "14", "journal-ref": null, "doi": "10.1007/978-3-642-33651-5_18", "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of reliably broadcasting information in a multihop\nasynchronous network that is subject to Byzantine failures. Most existing\napproaches give conditions for perfect reliable broadcast (all correct nodes\ndeliver the authentic message and nothing else), but they require a highly\nconnected network. An approach giving only probabilistic guarantees (correct\nnodes deliver the authentic message with high probability) was recently\nproposed for loosely connected networks, such as grids and tori. Yet, the\nproposed solution requires a specific initialization (that includes global\nknowledge) of each node, which may be difficult or impossible to guarantee in\nself-organizing networks - for instance, a wireless sensor network, especially\nif they are prone to Byzantine failures. In this paper, we propose a new\nprotocol offering guarantees for loosely connected networks that does not\nrequire such global knowledge dependent initialization. In more details, we\ngive a methodology to determine whether a set of nodes will always deliver the\nauthentic message, in any execution. Then, we give conditions for perfect\nreliable broadcast in a torus network. Finally, we provide experimental\nevaluation for our solution, and determine the number of randomly distributed\nByzantine failures than can be tolerated, for a given correct broadcast\nprobability.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2012 14:55:32 GMT"}], "update_date": "2012-11-15", "authors_parsed": [["Maurer", "Alexandre", "", "LIP6, LINCS"], ["Tixeuil", "S\u00e9bastien", "", "LIP6, LINCS, IUF"]]}, {"id": "1209.1425", "submitter": "Reynold Xin", "authors": "Reynold S. Xin", "title": "The End of an Architectural Era for Analytical Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Traditional enterprise warehouse solutions center around an analytical\ndatabase system that is monolithic and inflexible: data needs to be extracted,\ntransformed, and loaded into the rigid relational form before analysis. It\ntakes years of sophisticated planning to provision and deploy a warehouse;\nadding new hardware resources to an existing warehouse is an equally lengthy\nand daunting task.\n  Additionally, modern data analysis employs statistical methods that go well\nbeyond the typical roll-up and drill-down capabilities provided by warehouse\nsystems. Although it is possible to implement such methods using a combination\nof SQL and UDFs, query engines in relational databases are ill-suited for\nthese.\n  The Hadoop ecosystem introduces a suite of tools for data analytics that\novercome some of the problems of traditional solutions. These systems, however,\nforgo years of warehouse research. Memory is significantly underutilized in\nHadoop clusters, and execution engine is naive compared with its relational\ncounterparts.\n  It is time to rethink the design of data warehouse systems and take the best\nfrom both worlds. The new generation of warehouse systems should be modular,\nhigh performance, fault-tolerant, easy to provision, and designed to support\nboth SQL query processing and machine learning applications.\n  This paper references the Shark system developed at Berkeley as an initial\nattempt.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2012 23:15:46 GMT"}], "update_date": "2012-09-10", "authors_parsed": [["Xin", "Reynold S.", ""]]}, {"id": "1209.1734", "submitter": "Vishnuvardhan Mannava M.E", "authors": "Vishnuvardhan Mannava and T. Ramesh", "title": "Load Distribution Composite Design Pattern for Genetic Algorithm-Based\n  Autonomic Computing Systems", "comments": "International Journal on Soft Computing (IJSC), 15 pages, 11 figures", "journal-ref": "Vishnuvardhan, Mannava., & Ramesh, T. (2012). Load Distribution\n  Composite Design Pattern for Genetic Algorithm-Based Autonomic Computing\n  Systems. International Journal on Soft Computing (IJSC), 3(3), 85-99", "doi": "10.5121/ijsc", "report-no": null, "categories": "cs.SE cs.DC cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Current autonomic computing systems are ad hoc solutions that are designed\nand implemented from the scratch. When designing software, in most cases two or\nmore patterns are to be composed to solve a bigger problem. A composite design\npatterns shows a synergy that makes the composition more than just the sum of\nits parts which leads to ready-made software architectures. As far as we know,\nthere are no studies on composition of design patterns for autonomic computing\ndomain. In this paper we propose pattern-oriented software architecture for\nself-optimization in autonomic computing system using design patterns\ncomposition and multi objective evolutionary algorithms that software designers\nand/or programmers can exploit to drive their work. Main objective of the\nsystem is to reduce the load in the server by distributing the population to\nclients. We used Case Based Reasoning, Database Access, and Master Slave design\npatterns. We evaluate the effectiveness of our architecture with and without\ndesign patterns compositions. The use of composite design patterns in the\narchitecture and quantitative measurements are presented. A simple UML class\ndiagram is used to describe the architecture.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2012 17:39:46 GMT"}], "update_date": "2012-09-11", "authors_parsed": [["Mannava", "Vishnuvardhan", ""], ["Ramesh", "T.", ""]]}, {"id": "1209.1877", "submitter": "Slava V. Kitaev", "authors": "Vyacheslav V. Kitaeff, Chen Wu, Andreas Wicenec, Andrew D. Cannon and\n  Kevin Vinsen", "title": "SkuareView: Client-Server Framework for Accessing Extremely Large Radio\n  Astronomy Image Data", "comments": "8 pages, 1 figure, Astro-HPC '12 Proceedings of the 2012 workshop on\n  High-Performance Computing for Astronomy", "journal-ref": null, "doi": "10.1145/2286976.2286984", "report-no": null, "categories": "astro-ph.IM cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The new wide-field radio telescopes, such as: ASKAP, MWA, and SKA; will\nproduce spectral-imaging data-cubes (SIDC) of unprecedented volume. This\nrequires new approaches to managing and servicing the data to the end-user. We\npresent a new integrated framework based on the JPEG2000/ISO/IEC 15444 standard\nto address the challenges of working with extremely large SIDC. We also present\nthe developed j2k software, that converts and encodes FITS image cubes into\nJPEG2000 images, paving the way to implementing the pre- sented framework.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2012 04:20:02 GMT"}], "update_date": "2012-09-11", "authors_parsed": [["Kitaeff", "Vyacheslav V.", ""], ["Wu", "Chen", ""], ["Wicenec", "Andreas", ""], ["Cannon", "Andrew D.", ""], ["Vinsen", "Kevin", ""]]}, {"id": "1209.1885", "submitter": "Simon Kramer", "authors": "Simon Kramer and Joshua Sack", "title": "Parametric Constructive Kripke-Semantics for Standard Multi-Agent Belief\n  and Knowledge (Knowledge As Unbiased Belief)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose parametric constructive Kripke-semantics for multi-agent\nKD45-belief and S5-knowledge in terms of elementary set-theoretic constructions\nof two basic functional building blocks, namely bias (or viewpoint) and\nvisibility, functioning also as the parameters of the doxastic and epistemic\naccessibility relation. The doxastic accessibility relates two possible worlds\nwhenever the application of the composition of bias with visibility to the\nfirst world is equal to the application of visibility to the second world. The\nepistemic accessibility is the transitive closure of the union of our doxastic\naccessibility and its converse. Therefrom, accessibility relations for common\nand distributed belief and knowledge can be constructed in a standard way. As a\nresult, we obtain a general definition of knowledge in terms of belief that\nenables us to view S5-knowledge as accurate (unbiased and thus true)\nKD45-belief, negation-complete belief and knowledge as exact KD45-belief and\nS5-knowledge, respectively, and perfect S5-knowledge as precise (exact and\naccurate) KD45-belief, and all this generically for arbitrary functions of bias\nand visibility. Our results can be seen as a semantic complement to previous\nfoundational results by Halpern et al. about the (un)definability and\n(non-)reducibility of knowledge in terms of and to belief, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2012 06:48:19 GMT"}], "update_date": "2012-09-11", "authors_parsed": [["Kramer", "Simon", ""], ["Sack", "Joshua", ""]]}, {"id": "1209.2058", "submitter": "Taylor T Johnson", "authors": "Taylor T. Johnson and Sayan Mitra", "title": "Safe and Stabilizing Distributed Multi-Path Cellular Flows", "comments": "An earlier version of this paper appeared in the 30th IEEE\n  International Conference on Distributed Computing Systems (ICDCS 2010)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.DC cs.MA cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of distributed traffic control in the partitioned plane,\nwhere the movement of all entities (robots, vehicles, etc.) within each\npartition (cell) is coupled. Establishing liveness in such systems is\nchallenging, but such analysis will be necessary to apply such distributed\ntraffic control algorithms in applications like coordinating robot swarms and\nthe intelligent highway system. We present a formal model of a distributed\ntraffic control protocol that guarantees minimum separation between entities,\neven as some cells fail. Once new failures cease occurring, in the case of a\nsingle target, the protocol is guaranteed to self-stabilize and the entities\nwith feasible paths to the target cell make progress towards it. For multiple\ntargets, failures may cause deadlocks in the system, so we identify a class of\nnon-deadlocking failures where all entities are able to make progress to their\nrespective targets. The algorithm relies on two general principles: temporary\nblocking for maintenance of safety and local geographical routing for\nguaranteeing progress. Our assertional proofs may serve as a template for the\nanalysis of other distributed traffic control protocols. We present simulation\nresults that provide estimates of throughput as a function of entity velocity,\nsafety separation, single-target path complexity, failure-recovery rates, and\nmulti-target path complexity.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2012 16:45:01 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2012 15:38:32 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Johnson", "Taylor T.", ""], ["Mitra", "Sayan", ""]]}, {"id": "1209.2191", "submitter": "Jimmy Lin", "authors": "Jimmy Lin", "title": "MapReduce is Good Enough? If All You Have is a Hammer, Throw Away\n  Everything That's Not a Nail!", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hadoop is currently the large-scale data analysis \"hammer\" of choice, but\nthere exist classes of algorithms that aren't \"nails\", in the sense that they\nare not particularly amenable to the MapReduce programming model. To address\nthis, researchers have proposed MapReduce extensions or alternative programming\nmodels in which these algorithms can be elegantly expressed. This essay\nespouses a very different position: that MapReduce is \"good enough\", and that\ninstead of trying to invent screwdrivers, we should simply get rid of\neverything that's not a nail. To be more specific, much discussion in the\nliterature surrounds the fact that iterative algorithms are a poor fit for\nMapReduce: the simple solution is to find alternative non-iterative algorithms\nthat solve the same problem. This essay captures my personal experiences as an\nacademic researcher as well as a software engineer in a \"real-world\" production\nanalytics environment. From this combined perspective I reflect on the current\nstate and future of \"big data\" research.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2012 01:04:38 GMT"}], "update_date": "2012-09-12", "authors_parsed": [["Lin", "Jimmy", ""]]}, {"id": "1209.2614", "submitter": "Venkataramana Kanaparti", "authors": "K. Venkataramana, M. Padmavathamma", "title": "A threshold secure data sharing scheme for federated clouds", "comments": "8 pages, 3 Figures, International Journal of Research in Computer\n  Science 2012. arXiv admin note: text overlap with arXiv:1003.3920 by other\n  authors", "journal-ref": null, "doi": "10.7815/ijorcs.25.2012.044", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing allows users to view computing in a new direction, as it uses\nthe existing technologies to provide better IT services at low-cost. To offer\nhigh QOS to customers according SLA, cloud services broker or cloud service\nprovider uses individual cloud providers that work collaboratively to form a\nfederation of clouds. It is required in applications like Real-time online\ninteractive applications, weather research and forecasting etc., in which the\ndata and applications are complex and distributed. In these applications secret\ndata should be shared, so secure data sharing mechanism is required in\nFederated clouds to reduce the risk of data intrusion, the loss of service\navailability and to ensure data integrity. So In this paper we have proposed\nzero knowledge data sharing scheme where Trusted Cloud Authority (TCA) will\ncontrol federated clouds for data sharing where the secret to be exchanged for\ncomputation is encrypted and retrieved by individual cloud at the end. Our\nscheme is based on the difficulty of solving the Discrete Logarithm problem\n(DLOG) in a finite abelian group of large prime order which is NP-Hard. So our\nproposed scheme provides data integrity in transit, data availability when one\nof host providers are not available during the computation.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2012 13:44:48 GMT"}], "update_date": "2012-09-13", "authors_parsed": [["Venkataramana", "K.", ""], ["Padmavathamma", "M.", ""]]}, {"id": "1209.3050", "submitter": "Samuel King Opoku", "authors": "Samuel King Opoku (Kumasi Polytechnic)", "title": "Parallel Sorting System for Objects", "comments": "8 pages, 13 figures", "journal-ref": "Cyber Journals: Multidisciplinary Journals in Science and\n  Technology, Journal of Selected Areas in Software Engineering (JSSE), Vol. 2,\n  No. 12, pages 1-8, 2011", "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional sorting algorithms make use of such data structures as array,\nfile and list which define access methods of the items to be sorted. Such\ntraditional methods as exchange sort, divide and conquer sort, selection sort\nand insertion sort require supervisory control program. The supervisory control\nprogram has access to the items and is responsible for arranging them in the\nproper order. This paper presents a different sorting algorithm that does not\nrequire supervisory control program. The objects sort themselves and they are\nable to terminate when sorting is completed. The algorithm also employs\nparallel processing mechanisms to increase its efficiency and effectiveness.\nThe paper makes a review of the traditional sorting methods, identifying their\npros and cons and proposes a different design based on conceptual combination\nof these algorithms. Algorithms designed were implemented and tested in Java\ndesktop application\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2012 22:01:07 GMT"}], "update_date": "2012-09-17", "authors_parsed": [["Opoku", "Samuel King", "", "Kumasi Polytechnic"]]}, {"id": "1209.3314", "submitter": "George Teodoro", "authors": "George Teodoro, Tony Pan, Tahsin Kurc, Jun Kong, Lee Cooper, Joel\n  Saltz", "title": "Efficient Irregular Wavefront Propagation Algorithms on Hybrid CPU-GPU\n  Machines", "comments": "37 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of efficient execution of a computation\npattern, referred to here as the irregular wavefront propagation pattern\n(IWPP), on hybrid systems with multiple CPUs and GPUs. The IWPP is common in\nseveral image processing operations. In the IWPP, data elements in the\nwavefront propagate waves to their neighboring elements on a grid if a\npropagation condition is satisfied. Elements receiving the propagated waves\nbecome part of the wavefront. This pattern results in irregular data accesses\nand computations. We develop and evaluate strategies for efficient computation\nand propagation of wavefronts using a multi-level queue structure. This queue\nstructure improves the utilization of fast memories in a GPU and reduces\nsynchronization overheads. We also develop a tile-based parallelization\nstrategy to support execution on multiple CPUs and GPUs. We evaluate our\napproaches on a state-of-the-art GPU accelerated machine (equipped with 3 GPUs\nand 2 multicore CPUs) using the IWPP implementations of two widely used image\nprocessing operations: morphological reconstruction and euclidean distance\ntransform. Our results show significant performance improvements on GPUs. The\nuse of multiple CPUs and GPUs cooperatively attains speedups of 50x and 85x\nwith respect to single core CPU executions for morphological reconstruction and\neuclidean distance transform, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2012 20:17:23 GMT"}], "update_date": "2012-09-18", "authors_parsed": [["Teodoro", "George", ""], ["Pan", "Tony", ""], ["Kurc", "Tahsin", ""], ["Kong", "Jun", ""], ["Cooper", "Lee", ""], ["Saltz", "Joel", ""]]}, {"id": "1209.3332", "submitter": "George Teodoro", "authors": "George Teodoro, Tony Pan, Tahsin M. Kurc, Jun Kong, Lee A. D. Cooper,\n  Joel H. Saltz", "title": "High-throughput Execution of Hierarchical Analysis Pipelines on Hybrid\n  Cluster Platforms", "comments": "12 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose, implement, and experimentally evaluate a runtime middleware to\nsupport high-throughput execution on hybrid cluster machines of large-scale\nanalysis applications. A hybrid cluster machine consists of computation nodes\nwhich have multiple CPUs and general purpose graphics processing units (GPUs).\nOur work targets scientific analysis applications in which datasets are\nprocessed in application-specific data chunks, and the processing of a data\nchunk is expressed as a hierarchical pipeline of operations. The proposed\nmiddleware system combines a bag-of-tasks style execution with coarse-grain\ndataflow execution. Data chunks and associated data processing pipelines are\nscheduled across cluster nodes using a demand driven approach, while within a\nnode operations in a given pipeline instance are scheduled across CPUs and\nGPUs. The runtime system implements several optimizations, including\nperformance aware task scheduling, architecture aware process placement, data\nlocality conscious task assignment, and data prefetching and asynchronous data\ncopy, to maximize utilization of the aggregate computing power of CPUs and GPUs\nand minimize data copy overheads. The application and performance benefits of\nthe runtime middleware are demonstrated using an image analysis application,\nwhich is employed in a brain cancer study, on a state-of-the-art hybrid cluster\nin which each node has two 6-core CPUs and three GPUs. Our results show that\nimplementing and scheduling application data processing as a set of fine-grain\noperations provide more opportunities for runtime optimizations and attain\nbetter performance than a coarser-grain, monolithic implementation. The\nproposed runtime system can achieve high-throughput processing of large\ndatasets - we were able to process an image dataset consisting of 36,848\n4Kx4K-pixel image tiles at about 150 tiles/second rate on 100 nodes.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2012 21:56:51 GMT"}], "update_date": "2012-09-18", "authors_parsed": [["Teodoro", "George", ""], ["Pan", "Tony", ""], ["Kurc", "Tahsin M.", ""], ["Kong", "Jun", ""], ["Cooper", "Lee A. D.", ""], ["Saltz", "Joel H.", ""]]}, {"id": "1209.3356", "submitter": "Rajkumar Buyya", "authors": "Rajkumar Buyya, Rodrigo N. Calheiros, and Xiaorong Li", "title": "Autonomic Cloud Computing: Open Challenges and Architectural Elements", "comments": "8 pages, 6 figures, conference keynote paper", "journal-ref": "Proceedings of the Third International Conference of Emerging\n  Applications of Information Technology (EAIT 2012, IEEE Press, USA), Kolkata,\n  India, November 29-December 01, 2012", "doi": "10.1109/EAIT.2012.6407847", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As Clouds are complex, large-scale, and heterogeneous distributed systems,\nmanagement of their resources is a challenging task. They need automated and\nintegrated intelligent strategies for provisioning of resources to offer\nservices that are secure, reliable, and cost-efficient. Hence, effective\nmanagement of services becomes fundamental in software platforms that\nconstitute the fabric of computing Clouds. In this direction, this paper\nidentifies open issues in autonomic resource provisioning and presents\ninnovative management techniques for supporting SaaS applications hosted on\nClouds. We present a conceptual architecture and early results evidencing the\nbenefits of autonomic management of Clouds.\n", "versions": [{"version": "v1", "created": "Sat, 15 Sep 2012 04:40:46 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Buyya", "Rajkumar", ""], ["Calheiros", "Rodrigo N.", ""], ["Li", "Xiaorong", ""]]}, {"id": "1209.3487", "submitter": "Tom Kelsey", "authors": "Lars Kotthoff, Tom Kelsey and Martin McCaffery", "title": "A framework for large-scale distributed AI search across disconnected\n  heterogeneous infrastructures", "comments": "18 pages plus references. arXiv admin note: substantial text overlap\n  with arXiv:1008.4328", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for a large-scale distributed eScience Artificial\nIntelligence search. Our approach is generic and can be used for many different\nproblems. Unlike many other approaches, we do not require dedicated machines,\nhomogeneous infrastructure or the ability to communicate between nodes. We give\nspecial consideration to the robustness of the framework, minimising the loss\nof effort even after total loss of infrastructure, and allowing easy\nverification of every step of the distribution process. In contrast to most\neScience applications, the input data and specification of the problem is very\nsmall, being easily given in a paragraph of text. The unique challenges our\nframework tackles are related to the combinatorial explosion of the space that\ncontains the possible solutions and the robustness of long-running\ncomputations. Not only is the time required to finish the computations unknown,\nbut also the resource requirements may change during the course of the\ncomputation. We demonstrate the applicability of our framework by using it to\nsolve a challenging and hitherto open problem in computational mathematics. The\nresults demonstrate that our approach easily scales to computations of a size\nthat would have been impossible to tackle in practice just a decade ago.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2012 14:12:49 GMT"}], "update_date": "2012-09-18", "authors_parsed": [["Kotthoff", "Lars", ""], ["Kelsey", "Tom", ""], ["McCaffery", "Martin", ""]]}, {"id": "1209.3721", "submitter": "Constandinos Mavromoustakis X.", "authors": "Marios C. Charalambous, Constandinos X. Mavromoustakis and Muneer Bani\n  Yassein", "title": "A Resource Intensive Traffic-Aware Scheme for Cluster-based Energy\n  Conservation in Wireless Devices", "comments": "6 pages, 8 figures, To appear in the proceedings of IEEE 14th\n  International Conference on High Performance Computing and Communications\n  (HPCC-2012) of the Third International Workshop on Wireless Networks and\n  Multimedia (WNM-2012), 25-27 June 2012, Liverpool, UK", "journal-ref": null, "doi": "10.1109/HPCC.2012.125", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wireless traffic that is destined for a certain device in a network, can be\nexploited in order to minimize the availability and delay trade-offs, and\nmitigate the Energy consumption. The Energy Conservation (EC) mechanism can be\nnode-centric by considering the traversed nodal traffic in order to prolong the\nnetwork lifetime. This work describes a quantitative traffic-based approach\nwhere a clustered Sleep-Proxy mechanism takes place in order to enable each\nnode to sleep according to the time duration of the active traffic that each\nnode expects and experiences. Sleep-proxies within the clusters are created\naccording to pairwise active-time comparison, where each node expects during\nthe active periods, a requested traffic. For resource availability and recovery\npurposes, the caching mechanism takes place in case where the node for which\nthe traffic is destined is not available. The proposed scheme uses Role-based\nnodes which are assigned to manipulate the traffic in a cluster, through the\ntime-oriented backward difference traffic evaluation scheme. Simulation study\nis carried out for the proposed backward estimation scheme and the\neffectiveness of the end-to-end EC mechanism taking into account a number of\nmetrics and measures for the effects while incrementing the sleep time duration\nunder the proposed framework. Comparative simulation results show that the\nproposed scheme could be applied to infrastructure-less systems, providing\nenergy-efficient resource exchange with significant minimization in the power\nconsumption of each device.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2012 16:59:51 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Charalambous", "Marios C.", ""], ["Mavromoustakis", "Constandinos X.", ""], ["Yassein", "Muneer Bani", ""]]}, {"id": "1209.3904", "submitter": "Chryssis Georgiou", "authors": "Chrysovalandis Agathangelou, Chryssis Georgiou and Marios Mavronicolas", "title": "A Distributed Algorithm for Gathering Many Fat Mobile Robots in the\n  Plane", "comments": "39 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we consider the problem of gathering autonomous robots in the\nplane. In particular, we consider non-transparent unit-disc robots (i.e., fat)\nin an asynchronous setting. Vision is the only mean of coordination. Using a\nstate-machine representation we formulate the gathering problem and develop a\ndistributed algorithm that solves the problem for any number of robots.\n  The main idea behind our algorithm is for the robots to reach a configuration\nin which all the following hold: (a) The robots' centers form a convex hull in\nwhich all robots are on the convex, (b) Each robot can see all other robots,\nand (c) The configuration is connected, that is, every robot touches another\nrobot and all robots together form a connected formation. We show that starting\nfrom any initial configuration, the robots, making only local decisions and\ncoordinate by vision, eventually reach such a configuration and terminate,\nyielding a solution to the gathering problem.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2012 10:36:45 GMT"}], "update_date": "2012-09-19", "authors_parsed": [["Agathangelou", "Chrysovalandis", ""], ["Georgiou", "Chryssis", ""], ["Mavronicolas", "Marios", ""]]}, {"id": "1209.3913", "submitter": "M\\'arton Trencs\\'eni", "authors": "M\\'arton Trencs\\'eni, Attila Gazs\\'o", "title": "Keyspace: A Consistently Replicated, Highly-Available Key-Value Store", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the design and architecture of Keyspace, a distributed\nkey-value store offering strong consistency, fault-tolerance and high\navailability. The source code is available under the open-source AGPL license\nfor Linux, Windows and BSD-like platforms. As of 2012, Keyspace is no longer\nundergoing active development.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2012 11:35:57 GMT"}], "update_date": "2012-09-19", "authors_parsed": [["Trencs\u00e9ni", "M\u00e1rton", ""], ["Gazs\u00f3", "Attila", ""]]}, {"id": "1209.3977", "submitter": "Bernat Gaston", "authors": "Bernat Gast\\'on, Jaume Pujol, and Merc\\`e Villanueva", "title": "Quasi-cyclic Flexible Regenerating Codes", "comments": "17 pages, 1 figure, submitted to IEEE Transactions on Information\n  Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a distributed storage environment, where the data is placed in nodes\nconnected through a network, it is likely that one of these nodes fails. It is\nknown that the use of erasure coding improves the fault tolerance and minimizes\nthe redundancy added in distributed storage environments. The use of\nregenerating codes not only make the most of the erasure coding improvements,\nbut also minimizes the amount of data needed to regenerate a failed node.\n  In this paper, a new family of regenerating codes based on quasi-cyclic codes\nis presented. Quasi-cyclic flexible minimum storage regenerating (QCFMSR) codes\nare constructed and their existence is proved. Quasi-cyclic flexible\nregenerating codes with minimum bandwidth constructed from a base QCFMSR code\nare also provided. These codes not only achieve optimal MBR parameters in terms\nof stored data and repair bandwidth, but also for an specific choice of the\nparameters involved, they can be decreased under the optimal MBR point.\n  Quasi-cyclic flexible regenerating codes are very interesting because of\ntheir simplicity and low complexity. They allow exact repair-by-transfer in the\nminimum bandwidth case and an exact pseudo repair-by-transfer in the MSR case,\nwhere operations are needed only when a new node enters into the system\nreplacing a lost one.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2012 14:26:14 GMT"}, {"version": "v2", "created": "Thu, 16 May 2013 08:58:58 GMT"}], "update_date": "2013-05-17", "authors_parsed": [["Gast\u00f3n", "Bernat", ""], ["Pujol", "Jaume", ""], ["Villanueva", "Merc\u00e8", ""]]}, {"id": "1209.4187", "submitter": "M\\'arton Trencs\\'eni", "authors": "M\\'arton Trencs\\'eni, Attila Gazs\\'o, Holger Reinhardt", "title": "PaxosLease: Diskless Paxos for Leases", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes PaxosLease, a distributed algorithm for lease\nnegotiation. PaxosLease is based on Paxos, but does not require disk writes or\nclock synchrony. PaxosLease is used for master lease negotation in the\nopen-source Keyspace and ScalienDB replicated key-value stores.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2012 08:58:52 GMT"}], "update_date": "2012-09-20", "authors_parsed": [["Trencs\u00e9ni", "M\u00e1rton", ""], ["Gazs\u00f3", "Attila", ""], ["Reinhardt", "Holger", ""]]}, {"id": "1209.4257", "submitter": "Dang Hoan Tran", "authors": "Dang-Hoan Tran", "title": "Communication-Efficient and Exact Clustering Distributed Streaming Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A widely used approach to clustering a single data stream is the two-phased\napproach in which the online phase creates and maintains micro-clusters while\nthe off-line phase generates the macro-clustering from the micro-clusters. We\nuse this approach to propose a distributed framework for clustering streaming\ndata. Our proposed framework consists of fundamen- tal processes: one\ncoordinator-site process and many remote-site processes. Remote-site processes\ncan directly communicate with the coordinator-process but cannot communicate\nthe other remote site processes. Every remote-site process generates and\nmaintains micro- clusters that represent cluster information summary, from its\nlocal data stream. Remote sites send the local micro-clusterings to the\ncoordinator by the serialization technique, or the coordinator invokes the\nremote methods in order to get the local micro-clusterings from the remote\nsites. After the coordinator receives all the local micro-clusterings from the\nremote sites, it generates the global clustering by the macro-clustering\nmethod. Our theoretical and empirical results show that, the global clustering\ngenerated by our distributed framework is similar to the clustering generated\nby the underlying centralized algorithm on the same data set. By using the\nlocal micro-clustering approach, our framework achieves high scalability, and\ncommunication-efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2012 14:31:09 GMT"}], "update_date": "2012-09-20", "authors_parsed": [["Tran", "Dang-Hoan", ""]]}, {"id": "1209.4297", "submitter": "Benjamin Ong", "authors": "Benjamin Ong, Andrew Melfi and Andrew Christlieb", "title": "Parallel Semi-Implicit Time Integrators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we further develop a family of parallel time integrators known\nas Revisionist Integral Deferred Correction methods (RIDC) to allow for the\nsemi-implicit solution of time dependent PDEs. Additionally, we show that our\nsemi-implicit RIDC algorithm can harness the computational potential of\nmultiple general purpose graphical processing units (GPUs) in a single node by\nutilizing existing CUBLAS libraries for matrix linear algebra routines in our\nimplementation. In the numerical experiments, we show that our implementation\ncomputes a fourth order solution using four GPUs and four CPUs in approximately\nthe same wall clock time as a first order solution computed using a single GPU\nand a single CPU.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2012 16:18:33 GMT"}], "update_date": "2012-09-20", "authors_parsed": [["Ong", "Benjamin", ""], ["Melfi", "Andrew", ""], ["Christlieb", "Andrew", ""]]}, {"id": "1209.4408", "submitter": "Longfei Ma", "authors": "Longfei Ma, Xue Chen, Zhouxiang Meng", "title": "A performance Analysis of the Game of Life based on parallel algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, Conway's Game of Life using OpenMP parallel processing to\nsimulate several different parallel methods, experimental performance results\nand compare to find the optimal solution of the parallelization of the Game of\nLife. Finally pointed out the importance of the design of parallel algorithms\nin solving the parallel problem.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2012 02:12:03 GMT"}], "update_date": "2012-09-21", "authors_parsed": [["Ma", "Longfei", ""], ["Chen", "Xue", ""], ["Meng", "Zhouxiang", ""]]}, {"id": "1209.4485", "submitter": "Ivan Zuzak", "authors": "Ivan Zuzak (1) and Ivan Benc (2) ((1) University of Zagreb, Faculty of\n  Electrical Engineering and Computing, Croatia, (2) Croatian Telecom, Croatia)", "title": "Performance Evaluation of Hierarchical Publish-Subscribe Monitoring\n  Architecture for Service-Oriented Applications", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Contemporary high-performance service-oriented applications demand a\nperformance efficient run-time monitoring. In this paper, we analyze a\nhierarchical publish-subscribe architecture for monitoring service-oriented\napplications. The analyzed architecture is based on a tree topology and\npublish-subscribe communication model for aggregation of distributed monitoring\ndata. In order to satisfy interoperability and platform independence of\nservice-orientation, monitoring reports are represented as XML documents. Since\nXML formatting introduces a significant processing and network load, we analyze\nthe performance of monitoring architecture with respect to the number of\nmonitored nodes, the load of system machines, and the overall latency of the\nmonitoring system.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2012 10:15:52 GMT"}], "update_date": "2012-09-21", "authors_parsed": [["Zuzak", "Ivan", ""], ["Benc", "Ivan", ""]]}, {"id": "1209.4506", "submitter": "Duy Truong", "authors": "Truong Vinh Truong Duy and Taisuke Ozaki", "title": "A three-dimensional domain decomposition method for large-scale DFT\n  electronic structure calculations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.mtrl-sci cs.CE cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With tens of petaflops supercomputers already in operation and exaflops\nmachines expected to appear within the next 10 years, efficient parallel\ncomputational methods are required to take advantage of such extreme-scale\nmachines. In this paper, we present a three-dimensional domain decomposition\nscheme for enabling large-scale electronic calculations based on density\nfunctional theory (DFT) on massively parallel computers. It is composed of two\nmethods: (i) atom decomposition method and (ii) grid decomposition method. In\nthe former, we develop a modified recursive bisection method based on inertia\ntensor moment to reorder the atoms along a principal axis so that atoms that\nare close in real space are also close on the axis to ensure data locality. The\natoms are then divided into sub-domains depending on their projections onto the\nprincipal axis in a balanced way among the processes. In the latter, we define\nfour data structures for the partitioning of grids that are carefully\nconstructed to make data locality consistent with that of the clustered atoms\nfor minimizing data communications between the processes. We also propose a\ndecomposition method for solving the Poisson equation using three-dimensional\nFFT in Hartree potential calculation, which is shown to be better than a\npreviously proposed parallelization method based on a two-dimensional\ndecomposition in terms of communication efficiency. For evaluation, we perform\nbenchmark calculations with our open-source DFT code, OpenMX, paying particular\nattention to the O(N) Krylov subspace method. The results show that our scheme\nexhibits good strong and weak scaling properties, with the parallel efficiency\nat 131,072 cores being 67.7% compared to the baseline of 16,384 cores with\n131,072 diamond atoms on the K computer.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2012 12:22:04 GMT"}], "update_date": "2012-11-13", "authors_parsed": [["Duy", "Truong Vinh Truong", ""], ["Ozaki", "Taisuke", ""]]}, {"id": "1209.4560", "submitter": "Patrick Prosser", "authors": "Ciaran McCreesh and Patrick Prosser", "title": "Distributing an Exact Algorithm for Maximum Clique: maximising the\n  costup", "comments": "13 pages, 2 Algorithms, 2 figures, 2 tables", "journal-ref": "Algorithms 2012, 5(4), 545-587", "doi": "10.3390/a5040545", "report-no": "TR-2012-334", "categories": "cs.DS cs.DC cs.DM cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We take an existing implementation of an algorithm for the maximum clique\nproblem and modify it so that we can distribute it over an ad-hoc cluster of\nmachines. Our goal was to achieve a significant speedup in performance with\nminimal development effort, i.e. a maximum costup. We present a simple\nmodification to a state-of-the-art exact algorithm for maximum clique that\nallows us to distribute it across many machines. An empirical study over large\nhard benchmarks shows that speedups of an order of magnitude are routine for 25\nor more machines.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2012 15:18:54 GMT"}], "update_date": "2013-03-22", "authors_parsed": [["McCreesh", "Ciaran", ""], ["Prosser", "Patrick", ""]]}, {"id": "1209.4605", "submitter": "Marcin Kik", "authors": "Marcin Kik, Maciej G\\c{e}bala, Miros{\\l}aw Kuty{\\l}owski", "title": "One-side Energy costs of the RBO receiver", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.DM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $n = 2^k$ be the length of the broadcast cycle of the RBO broadcast\nscheduling protocol (see [arXiv:1108.5095] and [arXiv:1201.3318]). Let $lb$ and\n$ub$ be the variables of the RBO receiver as defined in [ arXiv:1201.3318 ]. We\nshow that the number of changes of $lb$ (the \"left-side energy\") is not greater\nthan $k + 1$. We also show that the number of changes of $rb$ (the \"right-side\nenergy\") is not greater than $k + 2$. Thus the \"extra energy\" (defined in\n[arXiv:1201.3318]) is bounded by $2 k + 3$. This updates the previous bound\nfrom [arXiv:1201.3318], which was $4 k + 2$.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2012 18:46:46 GMT"}], "update_date": "2012-09-21", "authors_parsed": [["Kik", "Marcin", ""], ["G\u0229bala", "Maciej", ""], ["Kuty\u0142owski", "Miros\u0142aw", ""]]}, {"id": "1209.4620", "submitter": "Lewis Tseng", "authors": "Lewis Tseng and Nitin Vaidya and Vartika Bhandari", "title": "Broadcast Using Certified Propagation Algorithm in Presence of Byzantine\n  Faults", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the correctness of the Certified Propagation Algorithm (CPA) [6,\n1, 8, 5] in solving broadcast with locally bounded Byzantine faults. CPA allows\nthe nodes to use only local information regarding the network topology. We\nprovide a tight necessary and sufficient condition on the network topology for\nthe correctness of CPA. To the best of our knowledge, this work is the first to\nsolve the open problem in [8]. We also present some simple extensions of this\nresult\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2012 19:15:50 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2013 22:07:06 GMT"}], "update_date": "2013-10-25", "authors_parsed": [["Tseng", "Lewis", ""], ["Vaidya", "Nitin", ""], ["Bhandari", "Vartika", ""]]}, {"id": "1209.4751", "submitter": "Priyanka Chatterjee", "authors": "Priyanka Chatterjee, Nikhil Agarwal", "title": "Energy Aware, Scalable, K-Hop Based Cluster Formation In MANET", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of Mobile Ad-hoc Network remains attractive due to the desire to\nachieve better performance and scalability. MANETs are distributed systems\nconsisting of mobile hosts that are connected by multi-hop wireless links. Such\nsystems are self organized and facilitate communication in the network without\nany centralized administration. MANETs exhibit battery power constraint and\nsuffer scalability issues therefore cluster formation is expensive. This is due\nto the large number of messages passed during the process of cluster formation.\nClustering has evolved as an imperative research domain that enhances system\nperformance such as throughput and delay in Mobile Ad hoc Networks (MANETs) in\nthe presence of both mobility and a large number of mobile terminals.In this\nthesis, we present a clustering scheme that minimizes message overhead and\ncongestion for cluster formation and maintenance. The algorithm is devised to\nbe independent of the MANET Routing algorithm. Depending upon the context, the\nclustering algorithm may be implemented in the routing or in higher layers. The\ndynamic formation of clusters helps reduce data packet overhead, node\ncomplexity and power consumption, The simulation has been performed in ns-2.\nThe simulation shows that the number of clusters formed is in proportion with\nthe number of nodes in MANET.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2012 08:49:57 GMT"}], "update_date": "2012-09-24", "authors_parsed": [["Chatterjee", "Priyanka", ""], ["Agarwal", "Nikhil", ""]]}, {"id": "1209.4935", "submitter": "Melvyn Wright", "authors": "Melvyn Wright", "title": "Adaptive Real Time Imaging Synthesis Telescopes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The digital revolution is transforming astronomy from a data-starved to a\ndata-submerged science. Instruments such as the Atacama Large Millimeter Array\n(ALMA), the Large Synoptic Survey Telescope (LSST), and the Square Kilometer\nArray (SKA) will measure their accumulated data in petabytes. The capacity to\nproduce enormous volumes of data must be matched with the computing power to\nprocess that data and produce meaningful results. In addition to handling huge\ndata rates, we need adaptive calibration and beamforming to handle atmospheric\nfluctuations and radio frequency interference, and to provide a user\nenvironment which makes the full power of large telescope arrays accessible to\nboth expert and non-expert users. Delayed calibration and analysis limit the\nscience which can be done. To make the best use of both telescope and human\nresources we must reduce the burden of data reduction.\n  Our instrumentation comprises of a flexible correlator, beam former and\nimager with digital signal processing closely coupled with a computing cluster.\nThis instrumentation will be highly accessible to scientists, engineers, and\nstudents for research and development of real-time processing algorithms, and\nwill tap into the pool of talented and innovative students and visiting\nscientists from engineering, computing, and astronomy backgrounds.\n  Adaptive real-time imaging will transform radio astronomy by providing\nreal-time feedback to observers. Calibration of the data is made in close to\nreal time using a model of the sky brightness distribution. The derived\ncalibration parameters are fed back into the imagers and beam formers. The\nregions imaged are used to update and improve the a-priori model, which becomes\nthe final calibrated image by the time the observations are complete.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2012 23:41:42 GMT"}], "update_date": "2012-09-25", "authors_parsed": [["Wright", "Melvyn", ""]]}, {"id": "1209.5025", "submitter": "Mohammed Amin Abdullah Dr", "authors": "Mohammed Amin Abdullah and Moez Draief", "title": "Global Majority Consensus by Local Majority Polling on Graphs of a Given\n  Degree Sequence", "comments": "Discrete Applied Mathematics 180:1-10 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose in a graph $G$ vertices can be either red or blue. Let $k$ be odd. At\neach time step, each vertex $v$ in $G$ polls $k$ random neighbours and takes\nthe majority colour. If it doesn't have $k$ neighbours, it simply polls all of\nthem, or all less one if the degree of $v$ is even. We study this protocol on\ngraphs of a given degree sequence, in the following setting: initially each\nvertex of $G$ is red independently with probability $\\alpha < \\frac{1}{2}$, and\nis otherwise blue. We show that if $\\alpha$ is sufficiently biased, then with\nhigh probability consensus is reached on the initial global majority within\n$O(\\log_k \\log_k n)$ steps if $5 \\leq k \\leq d$, and $O(\\log_d \\log_d n)$ steps\nif $k > d$. Here, $d\\geq 5$ is the effective minimum degree, the smallest\ninteger which occurs $\\Theta(n)$ times in the degree sequence. We further show\nthat on such graphs, any local protocol in which a vertex does not change\ncolour if all its neighbours have that same colour, takes time at least\n$\\Omega(\\log_d \\log_d n)$, with high probability. Additionally, we demonstrate\nhow the technique for the above sparse graphs can be applied in a\nstraightforward manner to get bounds for the Erd\\H{o}s-R\\'enyi random graphs in\nthe connected regime.\n", "versions": [{"version": "v1", "created": "Sat, 22 Sep 2012 22:32:57 GMT"}, {"version": "v2", "created": "Wed, 27 Feb 2013 22:00:31 GMT"}, {"version": "v3", "created": "Tue, 30 Jul 2019 23:17:52 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Abdullah", "Mohammed Amin", ""], ["Draief", "Moez", ""]]}, {"id": "1209.5158", "submitter": "Shubhabrata Roy", "authors": "Shubhabrata Roy (LIP), Thomas Begin (LIP), Patrick Loiseau (EURECOM),\n  Paulo Goncalves (LIP)", "title": "Un mod\\`ele de trafic adapt\\'e \\`a la volatilit\\'e de charge d'un\n  service de vid\\'eo \\`a la demande: Identification, validation et application\n  \\`a la gestion dynamique de ressources", "comments": "arXiv admin note: substantial text overlap with arXiv:1209.4806", "journal-ref": null, "doi": null, "report-no": "RR-8072", "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic resource management has become an active area of research in the\nCloud Computing paradigm. Cost of resources varies significantly depending on\nconfiguration for using them. Hence efficient management of resources is of\nprime interest to both Cloud Providers and Cloud Users. In this report we\nsuggest a probabilistic resource provisioning approach that can be exploited as\nthe input of a dynamic resource management scheme. Using a Video on Demand use\ncase to justify our claims, we propose an analytical model inspired from\nstandard models developed for epidemiology spreading, to represent sudden and\nintense workload variations. As an essential step we also derive a heuristic\nidentification procedure to calibrate all the model parameters and evaluate the\nperformance of our estimator on synthetic time series. We show how good can our\nmodel fit to real workload traces with respect to the stationary case in terms\nof steady-state probability and autocorrelation structure. We find that the\nresulting model verifies a Large Deviation Principle that statistically\ncharacterizes extreme rare events, such as the ones produced by \"buzz effects\"\nthat may cause workload overflow in the VoD context. This analysis provides\nvaluable insight on expectable abnormal behaviors of systems. We exploit the\ninformation obtained using the Large Deviation Principle for the proposed Video\non Demand use-case for defining policies (Service Level Agreements). We believe\nthese policies for elastic resource provisioning and usage may be of some\ninterest to all stakeholders in the emerging context of cloud networking.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2012 06:29:17 GMT"}, {"version": "v2", "created": "Tue, 2 Oct 2012 07:45:33 GMT"}], "update_date": "2012-10-03", "authors_parsed": [["Roy", "Shubhabrata", "", "LIP"], ["Begin", "Thomas", "", "LIP"], ["Loiseau", "Patrick", "", "EURECOM"], ["Goncalves", "Paulo", "", "LIP"]]}, {"id": "1209.5319", "submitter": "Nourah Al-Angari", "authors": "Nourah Al-Angari, Abdullatif ALAbdullatif", "title": "Multiprocessor Scheduling Using Parallel Genetic Algorithm", "comments": "5 pages, 5 figures, published in (IJCSI, Volume 9, Issue 4, July\n  2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tasks scheduling is the most challenging problem in the parallel computing.\nHence, the inappropriate scheduling will reduce or even abort the utilization\nof the true potential of the parallelization. Genetic algorithm (GA) has been\nsuccessfully applied to solve the scheduling problem. The fitness evaluation is\nthe most time consuming GA operation for the CPU time, which affect the GA\nperformance. The proposed synchronous master-slave algorithm outperforms the\nsequential algorithm in case of complex and high number of generations problem.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2012 16:12:20 GMT"}], "update_date": "2012-09-25", "authors_parsed": [["Al-Angari", "Nourah", ""], ["ALAbdullatif", "Abdullatif", ""]]}, {"id": "1209.5360", "submitter": "Michael Mitzenmacher", "authors": "Michael Mitzenmacher", "title": "Balanced Allocations and Double Hashing", "comments": "Further updated, small improvements/typos fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Double hashing has recently found more common usage in schemes that use\nmultiple hash functions. In double hashing, for an item $x$, one generates two\nhash values $f(x)$ and $g(x)$, and then uses combinations $(f(x) +k g(x)) \\bmod\nn$ for $k=0,1,2,...$ to generate multiple hash values from the initial two. We\nfirst perform an empirical study showing that, surprisingly, the performance\ndifference between double hashing and fully random hashing appears negligible\nin the standard balanced allocation paradigm, where each item is placed in the\nleast loaded of $d$ choices, as well as several related variants. We then\nprovide theoretical results that explain the behavior of double hashing in this\ncontext.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2012 18:37:40 GMT"}, {"version": "v2", "created": "Fri, 15 Feb 2013 00:47:22 GMT"}, {"version": "v3", "created": "Thu, 11 Jul 2013 18:45:05 GMT"}, {"version": "v4", "created": "Wed, 29 Jan 2014 18:36:15 GMT"}], "update_date": "2014-01-30", "authors_parsed": [["Mitzenmacher", "Michael", ""]]}, {"id": "1209.5430", "submitter": "Spyros Sioutas SS", "authors": "Spyros Sioutas, Alexandros Panaretos, Ioannis Karydis, Dimitrios\n  Tsoumakos, Giannis Tzimas and Dimitrios Tsolis", "title": "SART: Speeding up Query Processing in Sensor Networks with an Autonomous\n  Range Tree Structure", "comments": "11 pages, 23 figures, 5 algorithms or operations", "journal-ref": "ACM Applied Computing Review (ACR), Vol. 12, No.3, 2012, pp.60-74", "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of constructing efficient P2P overlays for sensornets\nproviding \"Energy-Level Application and Services\". The method presented in\n\\cite{SOPXM09} presents a novel P2P overlay for Energy Level discovery in a\nsensornet. However, this solution is not dynamic, since requires periodical\nrestructuring. In particular, it is not able to support neither join of\nsensor\\_nodes with energy level out of the ranges supported by the existing p2p\noverlay nor leave of \\emph{empty} overlay\\_peers to which no sensor\\_nodes are\ncurrently associated. On this purpose and based on the efficient P2P method\npresented in \\cite{SPSTMT10}, we design a dynamic P2P overlay for Energy Level\ndiscovery in a sensornet, the so-called SART (Sensors' Autonomous Range Tree).\nThe adaptation of the P2P index presented in \\cite{SPSTMT10} guarantees the\nbest-known dynamic query performance of the above operation. We experimentally\nverify this performance, via the D-P2P-Sim simulator (D-P2P-Sim is publicly\navailable at http://code.google.com/p/d-p2p-sim/).\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2012 21:24:36 GMT"}], "update_date": "2012-09-26", "authors_parsed": [["Sioutas", "Spyros", ""], ["Panaretos", "Alexandros", ""], ["Karydis", "Ioannis", ""], ["Tsoumakos", "Dimitrios", ""], ["Tzimas", "Giannis", ""], ["Tsolis", "Dimitrios", ""]]}, {"id": "1209.5912", "submitter": "Philippe Ciblat", "authors": "Franck Iutzeler, Philippe Ciblat, and Walid Hachem", "title": "Analysis of Sum-Weight-like algorithms for averaging in Wireless Sensor\n  Networks", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2013.2256904", "report-no": null, "categories": "cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed estimation of the average value over a Wireless Sensor Network\nhas recently received a lot of attention. Most papers consider single variable\nsensors and communications with feedback (e.g. peer-to-peer communications).\nHowever, in order to use efficiently the broadcast nature of the wireless\nchannel, communications without feedback are advocated. To ensure the\nconvergence in this feedback-free case, the recently-introduced Sum-Weight-like\nalgorithms which rely on two variables at each sensor are a promising solution.\nIn this paper, the convergence towards the consensus over the average of the\ninitial values is analyzed in depth. Furthermore, it is shown that the squared\nerror decreases exponentially with the time. In addition, a powerful algorithm\nrelying on the Sum-Weight structure and taking into account the broadcast\nnature of the channel is proposed.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2012 12:22:32 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Iutzeler", "Franck", ""], ["Ciblat", "Philippe", ""], ["Hachem", "Walid", ""]]}, {"id": "1209.6158", "submitter": "Carola Doerr", "authors": "Benjamin Doerr, Carola Doerr, Shay Moran, Shlomo Moran", "title": "Simple and Optimal Randomized Fault-Tolerant Rumor Spreading", "comments": "This is the author-generated version of a paper which is to appear in\n  Distributed Computing, Springer, DOI: 10.1007/s00446-014-0238-z It is\n  available online from\n  http://link.springer.com/article/10.1007/s00446-014-0238-z This version\n  contains some new results (Section 6)", "journal-ref": "Distributed Computing, Springer, 2015", "doi": "10.1007/s00446-014-0238-z", "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the classic problem of spreading a piece of information in a group\nof $n$ fully connected processors. By suitably adding a small dose of\nrandomness to the protocol of Gasienic and Pelc (1996), we derive for the first\ntime protocols that (i) use a linear number of messages, (ii) are correct even\nwhen an arbitrary number of adversarially chosen processors does not\nparticipate in the process, and (iii) with high probability have the\nasymptotically optimal runtime of $O(\\log n)$ when at least an arbitrarily\nsmall constant fraction of the processors are working. In addition, our\nprotocols do not require that the system is synchronized nor that all\nprocessors are simultaneously woken up at time zero, they are fully based on\npush-operations, and they do not need an a priori estimate on the number of\nfailed nodes.\n  Our protocols thus overcome the typical disadvantages of the two known\napproaches, algorithms based on random gossip (typically needing a large number\nof messages due to their unorganized nature) and algorithms based on fair\nworkload splitting (which are either not {time-efficient} or require intricate\npreprocessing steps plus synchronization).\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2012 08:25:23 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2012 16:30:53 GMT"}, {"version": "v3", "created": "Wed, 13 Feb 2013 12:25:42 GMT"}, {"version": "v4", "created": "Tue, 17 Sep 2013 10:03:56 GMT"}, {"version": "v5", "created": "Mon, 5 Jan 2015 09:38:43 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Doerr", "Benjamin", ""], ["Doerr", "Carola", ""], ["Moran", "Shay", ""], ["Moran", "Shlomo", ""]]}, {"id": "1209.6308", "submitter": "Sutanay Choudhury", "authors": "George Chin Jr., Andres Marquez, Sutanay Choudhury, John Feo", "title": "Scalable Triadic Analysis of Large-Scale Graphs: Multi-Core vs. Multi-\n  Processor vs. Multi-Threaded Shared Memory Architectures", "comments": null, "journal-ref": "24th International Symposium on Computer Architecture and High\n  Performance Computing (SBAC-PAD), 2012", "doi": null, "report-no": null, "categories": "cs.DC cs.SI", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Triadic analysis encompasses a useful set of graph mining methods that are\ncentered on the concept of a triad, which is a subgraph of three nodes. Such\nmethods are often applied in the social sciences as well as many other diverse\nfields. Triadic methods commonly operate on a triad census that counts the\nnumber of triads of every possible edge configuration in a graph. Like other\ngraph algorithms, triadic census algorithms do not scale well when graphs reach\ntens of millions to billions of nodes. To enable the triadic analysis of\nlarge-scale graphs, we developed and optimized a triad census algorithm to\nefficiently execute on shared memory architectures. We then conducted\nperformance evaluations of the parallel triad census algorithm on three\nspecific systems: Cray XMT, HP Superdome, and AMD multi-core NUMA machine.\nThese three systems have shared memory architectures but with markedly\ndifferent hardware capabilities to manage parallelism.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2012 18:00:16 GMT"}], "update_date": "2012-09-28", "authors_parsed": [["Chin", "George", "Jr."], ["Marquez", "Andres", ""], ["Choudhury", "Sutanay", ""], ["Feo", "John", ""]]}, {"id": "1209.6470", "submitter": "Reena Philips Mrs", "authors": "K. S. Rashmi, V. Suma, M. Vaidehi", "title": "Enhanced Load Balancing Approach to Avoid Deadlocks in Cloud", "comments": "5 Pages, 4 Figures, 5 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state-of-art of the technology focuses on data processing to deal with\nmassive amount of data. Cloud computing is an emerging technology, which\nenables one to accomplish the aforementioned objective, leading towards\nimproved business performance. It comprises of users requesting for the\nservices of diverse applications from various distributed virtual servers. The\ncloud should provide resources on demand to its clients with high availability,\nscalability and with reduced cost. Load balancing is one of the essential\nfactors to enhance the working performance of the cloud service provider.\nSince, cloud has inherited characteristic of distributed computing and\nvirtualization there is a possibility of occurrence of deadlock. Hence, in this\npaper, a load balancing algorithm has been proposed to avoid deadlocks among\nthe Virtual Machines (VMs) while processing the requests received from the\nusers by VM migration. Further, this paper also provides the anticipated\nresults with the implementation of the proposed algorithm. The deadlock\navoidance enhances the number of jobs to be serviced by cloud service provider\nand thereby improving working performance and the business of the cloud service\nprovider.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2012 10:13:18 GMT"}], "update_date": "2012-10-01", "authors_parsed": [["Rashmi", "K. S.", ""], ["Suma", "V.", ""], ["Vaidehi", "M.", ""]]}, {"id": "1209.6476", "submitter": "Reena Philips Mrs", "authors": "K. S. Rashmi, V. Suma, M. Vaidehi", "title": "Factors Influencing Job Rejections in Cloud Environment", "comments": "6 Pages, 5 Figures, 8 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The IT organizations invests heavy capital by consuming large scale\ninfrastructure and advanced operating platforms. The advances in technology has\nresulted in emergence of cloud computing, which is promising technology to\nachieve the aforementioned objective. At the peak hours, the jobs arriving to\nthe cloud system are normally high demanding efficient execution and dispatch.\nAn observation that has been carried out in this paper by capturing a job\narriving pattern from a monitoring system explains that most of the jobs get\nrejected because of lack of efficient technology. The job rejections can be\ncontrolled by certain factors such as job scheduling and load balancing.\nTherefore, in this paper the efficiency of Round Robin (RR) scheduling strategy\nused for job scheduling and Shortest Job First Scheduling (SJFS) technique used\nfor load balancing in reducing the job rejections are analyzed. Further, a\nproposal for an effective load balancing approach to avoid deadlocks has been\ndiscussed.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2012 10:39:04 GMT"}], "update_date": "2012-10-01", "authors_parsed": [["Rashmi", "K. S.", ""], ["Suma", "V.", ""], ["Vaidehi", "M.", ""]]}, {"id": "1209.6577", "submitter": "Rena Bakhshi", "authors": "Suhail Yousaf, Rena Bakhshi, Maarten van Steen, Spyros Voulgaris, and\n  John L. Kelley", "title": "Exploring Design Tradeoffs Of A Distributed Algorithm For Cosmic Ray\n  Event Detection", "comments": "submitted to JINST", "journal-ref": null, "doi": "10.1088/1748-0221/8/03/P03011", "report-no": null, "categories": "physics.ins-det astro-ph.IM cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many sensor networks, including large particle detector arrays measuring\nhigh-energy cosmic-ray air showers, traditionally rely on centralised trigger\nalgorithms to find spatial and temporal coincidences of individual nodes. Such\nschemes suffer from scalability problems, especially if the nodes communicate\nwirelessly or have bandwidth limitations. However, nodes which instead\ncommunicate with each other can, in principle, use a distributed algorithm to\nfind coincident events themselves without communication with a central node. We\npresent such an algorithm and consider various design tradeoffs involved, in\nthe context of a potential trigger for the Auger Engineering Radio Array\n(AERA).\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2012 17:44:24 GMT"}, {"version": "v2", "created": "Mon, 4 Feb 2013 21:58:16 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Yousaf", "Suhail", ""], ["Bakhshi", "Rena", ""], ["van Steen", "Maarten", ""], ["Voulgaris", "Spyros", ""], ["Kelley", "John L.", ""]]}, {"id": "1209.6580", "submitter": "Jo\\~ao Eugenio Marynowski", "authors": "Jo\\~ao Eugenio Marynowski, Michel Albonico, Eduardo Cunha de Almeida,\n  Gerson Suny\\'e", "title": "Testing MapReduce-Based Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MapReduce (MR) is the most popular solution to build applications for\nlarge-scale data processing. These applications are often deployed on large\nclusters of commodity machines, where failures happen constantly due to bugs,\nhardware problems, and outages. Testing MR-based systems is hard, since it is\nneeded a great effort of test harness to execute distributed test cases upon\nfailures. In this paper, we present a novel testing solution to tackle this\nissue called HadoopTest. This solution is based on a scalable harness approach,\nwhere distributed tester components are hung around each map and reduce worker\n(i.e., node). Testers are allowed to stimulate each worker to inject failures\non them, monitor their behavior, and validate testing results. HadoopTest was\nused to test two applications bundled into Hadoop, the Apache open source\nMapReduce implementation. Our initial implementation demonstrates promising\nresults, with HadoopTest coordinating test cases across distributed MapReduce\nworkers, and finding bugs.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2012 17:47:31 GMT"}, {"version": "v2", "created": "Fri, 8 Feb 2013 02:35:01 GMT"}], "update_date": "2013-02-11", "authors_parsed": [["Marynowski", "Jo\u00e3o Eugenio", ""], ["Albonico", "Michel", ""], ["de Almeida", "Eduardo Cunha", ""], ["Suny\u00e9", "Gerson", ""]]}]