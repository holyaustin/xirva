[{"id": "1606.00031", "submitter": "Junyao Guo", "authors": "Junyao Guo, Gabriela Hug, and Ozan Tonguz", "title": "Impact of Power System Partitioning on the Efficiency of Distributed\n  Multi-Step Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown that multi-step optimization based on Model\nPredictive Control (MPC) can effectively coordinate the increasing number of\ndistributed renewable energy and storage resources in the power system.\nHowever, the computation complexity of MPC is usually high which limits its use\nin practical implementation. To improve the efficiency of MPC, in this paper,\nwe apply a distributed optimization method to MPC. The approach consists of a\npartitioning technique based on spectral clustering that determines the best\nsystem partition and an improved Optimality Condition Decomposition method that\nsolves the optimization problem in a distributed manner. Results of simulations\nconducted on the IEEE 14-bus and 118-bus systems show that the distributed MPC\nproblem can be solved significantly faster by using a good partition of the\nsystem and this partition is applicable to multiple time steps without frequent\nchanges.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 20:21:52 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Guo", "Junyao", ""], ["Hug", "Gabriela", ""], ["Tonguz", "Ozan", ""]]}, {"id": "1606.00094", "submitter": "Matthew Moskewicz", "authors": "Matthew Moskewicz and Forrest Iandola and Kurt Keutzer", "title": "Boda-RTC: Productive Generation of Portable, Efficient Code for\n  Convolutional Neural Networks on Mobile Computing Platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The popularity of neural networks (NNs) spans academia, industry, and popular\nculture. In particular, convolutional neural networks (CNNs) have been applied\nto many image based machine learning tasks and have yielded strong results. The\navailability of hardware/software systems for efficient training and deployment\nof large and/or deep CNN models has been, and continues to be, an important\nconsideration for the field. Early systems for NN computation focused on\nleveraging existing dense linear algebra techniques and libraries. Current\napproaches use low-level machine specific programming and/or closed-source,\npurpose-built vendor libraries. In this work, we present an open source system\nthat, compared to existing approaches, achieves competitive computational speed\nwhile achieving higher portability. We achieve this by targeting the\nvendor-neutral OpenCL platform using a code-generation approach. We argue that\nour approach allows for both: (1) the rapid development of new computational\nkernels for existing hardware targets, and (2) the rapid tuning of existing\ncomputational kernels for new hardware targets. Results are presented for a\ncase study of targeting the Qualcomm Snapdragon 820 mobile computing platform\nfor CNN deployment.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 02:17:26 GMT"}, {"version": "v2", "created": "Tue, 13 Sep 2016 16:20:09 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Moskewicz", "Matthew", ""], ["Iandola", "Forrest", ""], ["Keutzer", "Kurt", ""]]}, {"id": "1606.00195", "submitter": "Ioannis Marcoullis", "authors": "Shlomi Dolev, Chryssis Georgiou, Ioannis Marcoullis, Elad M. Schiller", "title": "Self-stabilizing Reconfiguration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current reconfiguration techniques are based on starting the system in a\nconsistent configuration, in which all participating entities are in their\ninitial state. Starting from that state, the system must preserve consistency\nas long as a predefined churn rate of processors joins and leaves is not\nviolated, and unbounded storage is available. Many working systems cannot\ncontrol this churn rate and do not have access to unbounded storage. System\ndesigners that neglect the outcome of violating the above assumptions may doom\nthe system to exhibit illegal behaviors. We present the first automatically\nrecovering reconfiguration scheme that recovers from transient faults, such as\ntemporal violations of the above assumptions. Our self-stabilizing solutions\nregain safety automatically by assuming temporal access to reliable failure\ndetectors. Once safety is re-established, the failure detector reliability is\nno longer needed. Still, liveness is conditioned by the failure detector's\nunreliable signals. We show that our self-stabilizing reconfiguration\ntechniques can serve as the basis for the implementation of several dynamic\nservices over message passing systems. Examples include self-stabilizing\nreconfigurable virtual synchrony, which, in turn, can be used for implementing\na self-stabilizing reconfigurable state-machine replication and\nself-stabilizing reconfigurable emulation of shared memory.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 09:44:57 GMT"}, {"version": "v2", "created": "Tue, 6 Dec 2016 09:18:57 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Dolev", "Shlomi", ""], ["Georgiou", "Chryssis", ""], ["Marcoullis", "Ioannis", ""], ["Schiller", "Elad M.", ""]]}, {"id": "1606.00215", "submitter": "Sascha Hunold", "authors": "Sascha Hunold, Alexandra Carpen-Amarie, Felix Donatus L\\\"ubbe, Jesper\n  Larsson Tr\\\"aff", "title": "PGMPI: Automatically Verifying Self-Consistent MPI Performance\n  Guidelines", "comments": "14 pages, 4 figures, 13 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Message Passing Interface (MPI) is the most commonly used application\nprogramming interface for process communication on current large-scale parallel\nsystems. Due to the scale and complexity of modern parallel architectures, it\nis becoming increasingly difficult to optimize MPI libraries, as many factors\ncan influence the communication performance. To assist MPI developers and\nusers, we propose an automatic way to check whether MPI libraries respect\nself-consistent performance guidelines for collective communication operations.\nWe introduce the PGMPI framework to detect violations of performance guidelines\nthrough benchmarking. Our experimental results show that PGMPI can pinpoint\nundesired and often unexpected performance degradations of collective MPI\noperations. We demonstrate how to overcome performance issues of several\nlibraries by adapting the algorithmic implementations of their respective\ncollective MPI calls.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 10:52:52 GMT"}, {"version": "v2", "created": "Fri, 2 Sep 2016 14:09:48 GMT"}], "update_date": "2016-09-05", "authors_parsed": [["Hunold", "Sascha", ""], ["Carpen-Amarie", "Alexandra", ""], ["L\u00fcbbe", "Felix Donatus", ""], ["Tr\u00e4ff", "Jesper Larsson", ""]]}, {"id": "1606.00310", "submitter": "Jeffrey Kelling", "authors": "Jeffrey Kelling, G\\'eza \\'Odor, Sibylle Gemming", "title": "Bit-Vectorized GPU Implementation of a Stochastic Cellular Automaton\n  Model for Surface Growth", "comments": "INES 2016, Budapest http://www.ines-conf.org/ines-conf/2016index.html", "journal-ref": null, "doi": "10.1109/INES.2016.7555127", "report-no": null, "categories": "cs.DC cond-mat.mtrl-sci cond-mat.stat-mech physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic surface growth models aid in studying properties of universality\nclasses like the Kardar--Paris--Zhang class. High precision results obtained\nfrom large scale computational studies can be transferred to many physical\nsystems. Many properties, such as roughening and some two-time functions can be\nstudied using stochastic cellular automaton (SCA) variants of stochastic\nmodels. Here we present a highly efficient SCA implementation of a surface\ngrowth model capable of simulating billions of lattice sites on a single GPU.\nWe also provide insight into cases requiring arbitrary random probabilities\nwhich are not accessible through bit-vectorization.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 14:40:16 GMT"}], "update_date": "2017-02-10", "authors_parsed": [["Kelling", "Jeffrey", ""], ["\u00d3dor", "G\u00e9za", ""], ["Gemming", "Sibylle", ""]]}, {"id": "1606.00350", "submitter": "Kibaek Kim", "authors": "Kibaek Kim, Fan Yang, Victor M. Zavala, Andrew A. Chien", "title": "Data Centers as Dispatchable Loads to Harness Stranded Power", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze how both traditional data center integration and dispatchable load\nintegration affect power grid efficiency. We use detailed network models,\nparallel optimization solvers, and thousands of renewable generation scenarios\nto perform our analysis. Our analysis reveals that significant spillage and\nstranded power will be observed in power grids as wind power levels are\nincreased. A counter-intuitive finding is that collocating data centers with\ninflexible loads next to wind farms has limited impacts on renewable portfolio\nstandard (RPS) goals because it provides limited system-level flexibility and\ncan in fact increase stranded power and fossil-fueled generation. In contrast,\noptimally placing data centers that are dispatchable (with flexible loads)\nprovides system-wide flexibility, reduces stranded power, and improves\nefficiency. In short, optimally placed dispatchable computing loads can enable\nbetter scaling to high RPS. We show that these dispatchable computing loads are\npowered to 60~80\\% of their requested capacity, indicating that there are\nsignificant economic incentives provided by stranded power.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 16:37:07 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Kim", "Kibaek", ""], ["Yang", "Fan", ""], ["Zavala", "Victor M.", ""], ["Chien", "Andrew A.", ""]]}, {"id": "1606.00396", "submitter": "Svetozar Miucin", "authors": "Svetozar Miucin, Conor Brady, Alexandra Fedorova", "title": "DINAMITE: A modern approach to memory performance profiling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diagnosing and fixing performance problems on multicore machines with deep\nmemory hierarchies is extremely challenging. Certain problems are best\naddressed when we can analyze the entire trace of program execution, e.g.,\nevery memory access. Unfortunately such detailed execution logs are very large\nand cannot be analyzed by direct inspection. We present DINAMITE: a toolkit for\nDynamic INstrumentation and Analysis for MassIve Trace Exploration. DINAMITE is\na collection of tools for end-to-end performance analysis: from the LLVM\ncompiler pass that instruments the program to plug-and-play tools that use a\nmodern data analytics engine Spark Streaming for trace introspection. Using\nDINAMITE we found opportunities to improve data layout in several applications\nthat resulted in 15-20% performance improvements and found a shared-variable\nbottleneck in a popular key-value store, whose elimination improved performance\nby 20x.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 18:52:53 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Miucin", "Svetozar", ""], ["Brady", "Conor", ""], ["Fedorova", "Alexandra", ""]]}, {"id": "1606.00511", "submitter": "Xi He", "authors": "Xi He and Dheevatsa Mudigere and Mikhail Smelyanskiy and Martin\n  Tak\\'a\\v{c}", "title": "Distributed Hessian-Free Optimization for Deep Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep neural network is a high dimensional and a highly non-convex\noptimization problem. Stochastic gradient descent (SGD) algorithm and it's\nvariations are the current state-of-the-art solvers for this task. However, due\nto non-covexity nature of the problem, it was observed that SGD slows down near\nsaddle point. Recent empirical work claim that by detecting and escaping saddle\npoint efficiently, it's more likely to improve training performance. With this\nobjective, we revisit Hessian-free optimization method for deep networks. We\nalso develop its distributed variant and demonstrate superior scaling potential\nto SGD, which allows more efficiently utilizing larger computing resources thus\nenabling large models and faster time to obtain desired solution. Furthermore,\nunlike truncated Newton method (Marten's HF) that ignores negative curvature\ninformation by using na\\\"ive conjugate gradient method and Gauss-Newton Hessian\napproximation information - we propose a novel algorithm to explore negative\ncurvature direction by solving the sub-problem with stabilized bi-conjugate\nmethod involving possible indefinite stochastic Hessian information. We show\nthat these techniques accelerate the training process for both the standard\nMNIST dataset and also the TIMIT speech recognition problem, demonstrating\nrobust performance with upto an order of magnitude larger batch sizes. This\nincreased scaling potential is illustrated with near linear speed-up on upto 16\nCPU nodes for a simple 4-layer network.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 00:39:03 GMT"}, {"version": "v2", "created": "Sun, 15 Jan 2017 13:51:26 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["He", "Xi", ""], ["Mudigere", "Dheevatsa", ""], ["Smelyanskiy", "Mikhail", ""], ["Tak\u00e1\u010d", "Martin", ""]]}, {"id": "1606.00519", "submitter": "Evangelia Sitaridi", "authors": "Evangelia Sitaridi, Rene Mueller, Tim Kaldewey, Guy Lohman, Kenneth\n  Ross", "title": "Massively-Parallel Lossless Data Decompression", "comments": "A shorter version of the paper to appear in ICPP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's exponentially increasing data volumes and the high cost of storage\nmake compression essential for the Big Data industry. Although research has\nconcentrated on efficient compression, fast decompression is critical for\nanalytics queries that repeatedly read compressed data. While decompression can\nbe parallelized somewhat by assigning each data block to a different process,\nbreak-through speed-ups require exploiting the massive parallelism of modern\nmulti-core processors and GPUs for data decompression within a block. We\npropose two new techniques to increase the degree of parallelism during\ndecompression. The first technique exploits the massive parallelism of GPU and\nSIMD architectures. The second sacrifices some compression efficiency to\neliminate data dependencies that limit parallelism during decompression. We\nevaluate these techniques on the decompressor of the DEFLATE scheme, called\nInflate, which is based on LZ77 compression and Huffman encoding. We achieve a\n2X speed-up in a head-to-head comparison with several multi-core CPU-based\nlibraries, while achieving a 17% energy saving with comparable compression\nratios.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 02:07:24 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Sitaridi", "Evangelia", ""], ["Mueller", "Rene", ""], ["Kaldewey", "Tim", ""], ["Lohman", "Guy", ""], ["Ross", "Kenneth", ""]]}, {"id": "1606.00541", "submitter": "Hui Liu Mr", "authors": "Zhangxin Chen, Hui Liu, Bo Yang", "title": "Parallel Triangular Solvers on GPU", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate GPU based parallel triangular solvers\nsystematically. The parallel triangular solvers are fundamental to incomplete\nLU factorization family preconditioners and algebraic multigrid solvers. We\ndevelop a new matrix format suitable for GPU devices. Parallel lower triangular\nsolvers and upper triangular solvers are developed for this new data structure.\nWith these solvers, ILU preconditioners and domain decomposition\npreconditioners are developed. Numerical results show that we can speed\ntriangular solvers around seven times faster.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 05:54:09 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Chen", "Zhangxin", ""], ["Liu", "Hui", ""], ["Yang", "Bo", ""]]}, {"id": "1606.00545", "submitter": "Hui Liu Mr", "authors": "Bo Yang, Hui Liu, Zhangxin Chen", "title": "Development of Krylov and AMG linear solvers for large-scale sparse\n  matrices on GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research introduce our work on developing Krylov subspace and AMG\nsolvers on NVIDIA GPUs. As SpMV is a crucial part for these iterative methods,\nSpMV algorithms for single GPU and multiple GPUs are implemented. A HEC matrix\nformat and a communication mechanism are established. And also, a set of\nspecific algorithms for solving preconditioned systems in parallel environments\nare designed, including ILU(k), RAS and parallel triangular solvers. Based on\nthese work, several Krylov solvers and AMG solvers are developed. According to\nnumerical experiments, favorable acceleration performance is acquired from our\nKrylov solver and AMG solver under various parameter conditions.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 06:01:05 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Yang", "Bo", ""], ["Liu", "Hui", ""], ["Chen", "Zhangxin", ""]]}, {"id": "1606.00548", "submitter": "Hui Liu Mr", "authors": "Hui Liu and Kun Wang and Zhangxin Chen", "title": "Large-scale Reservoir Simulations on IBM Blue Gene/Q", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents our work on simulation of large-scale reservoir models on\nIBM Blue Gene/Q and studying the scalability of our parallel reservoir\nsimulators. An in-house black oil simulator has been implemented. It uses MPI\nfor communication and is capable of simulating reservoir models with hundreds\nof millions of grid cells. Benchmarks show that our parallel simulator are\nthousands of times faster than sequential simulators that designed for\nworkstations and personal computers, and the simulator has excellent\nscalability.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 06:06:28 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Liu", "Hui", ""], ["Wang", "Kun", ""], ["Chen", "Zhangxin", ""]]}, {"id": "1606.00556", "submitter": "Hui Liu Mr", "authors": "Hui Liu, Lihua Shen, Kun Wang, Bo Yang, Zhangxin Chen", "title": "Numerical Simulation of Multi-phase Flow in Porous Media on Parallel\n  Computers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A parallel reservoir simulator has been developed, which is designed for\nlarge-scale black oil simulations. It handles three phases, including water,\noil and gas, and three components, including water, oil and gas. This simulator\ncan calculate traditional reservoir models and naturally fractured models.\nVarious well operations are supported, such as water flooding, gas flooding and\npolymer flooding. The operation constraints can be fixed bottom-hole pressure,\na fixed fluid rate, and combinations of them. The simulator is based on our\nin-house platform, which provides grids, cell-centred data, linear solvers,\npreconditioners and well modeling. The simulator and the platform use MPI for\ncommunications among computation nodes. Our simulator is capable of simulating\ngiant reservoir models with hundreds of millions of grid cells. Numerical\nsimulations show that our simulator matches with commercial simulators and it\nhas excellent scalability.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 06:51:10 GMT"}, {"version": "v2", "created": "Fri, 7 Oct 2016 18:26:44 GMT"}], "update_date": "2016-10-10", "authors_parsed": [["Liu", "Hui", ""], ["Shen", "Lihua", ""], ["Wang", "Kun", ""], ["Yang", "Bo", ""], ["Chen", "Zhangxin", ""]]}, {"id": "1606.00575", "submitter": "Shizhao Sun", "authors": "Shizhao Sun, Wei Chen, Jiang Bian, Xiaoguang Liu, Tie-Yan Liu", "title": "Ensemble-Compression: A New Method for Parallel Training of Deep Neural\n  Networks", "comments": "ECML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallelization framework has become a necessity to speed up the training of\ndeep neural networks (DNN) recently. Such framework typically employs the Model\nAverage approach, denoted as MA-DNN, in which parallel workers conduct\nrespective training based on their own local data while the parameters of local\nmodels are periodically communicated and averaged to obtain a global model\nwhich serves as the new start of local models. However, since DNN is a highly\nnon-convex model, averaging parameters cannot ensure that such global model can\nperform better than those local models. To tackle this problem, we introduce a\nnew parallel training framework called Ensemble-Compression, denoted as EC-DNN.\nIn this framework, we propose to aggregate the local models by ensemble, i.e.,\naveraging the outputs of local models instead of the parameters. As most of\nprevalent loss functions are convex to the output of DNN, the performance of\nensemble-based global model is guaranteed to be at least as good as the average\nperformance of local models. However, a big challenge lies in the explosion of\nmodel size since each round of ensemble can give rise to multiple times size\nincrement. Thus, we carry out model compression after each ensemble,\nspecialized by a distillation based method in this paper, to reduce the size of\nthe global model to be the same as the local ones. Our experimental results\ndemonstrate the prominent advantage of EC-DNN over MA-DNN in terms of both\naccuracy and speedup.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 08:10:10 GMT"}, {"version": "v2", "created": "Tue, 18 Jul 2017 08:50:05 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Sun", "Shizhao", ""], ["Chen", "Wei", ""], ["Bian", "Jiang", ""], ["Liu", "Xiaoguang", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1606.00637", "submitter": "Bahram Alinia", "authors": "Bahram Alinia, Mohammad H. Hajiesmaili, Ahmad Khonsari, and Noel\n  Crespi", "title": "Maximum-Quality Tree Construction for Deadline-Constrained Aggregation\n  in WSNs", "comments": "31 pages. arXiv admin note: substantial text overlap with\n  arXiv:1405.0597", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In deadline-constrained wireless sensor networks (WSNs), quality of\naggregation (QoA) is determined by the number of participating nodes in the\ndata aggregation process. The previous studies have attempted to propose\noptimal scheduling algorithms to obtain the maximum QoA assuming a fixed\nunderlying aggregation tree. However, there exists no prior work to address the\nissue of constructing optimal aggregation tree in deadline-constraints WSNs.\nThe structure of underlying aggregation tree is important since our analysis\ndemonstrates that the ratio between the maximum achievable QoAs of different\ntrees could be as large as O(2^D), where D is the deadline. This paper casts a\ncombinatorial optimization problem to address optimal tree construction for\ndeadline-constrained data aggregation in WSNs. While the problem is proved to\nbe NP-hard, we employ the recently proposed Markov approximation framework and\ndevise two distributed algorithms with different computation overheads to find\nclose-to-optimal solutions with bounded approximation gap. To further improve\nthe convergence of the proposed Markov-based algorithms, we devise another\ninitial tree construction algorithm with low computational complexity. Our\nextensive experiments for a set randomly-generated scenarios demonstrate that\nthe proposed algorithms outperforms the existing alternative methods by\nobtaining better quality of aggregations.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 11:50:42 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Alinia", "Bahram", ""], ["Hajiesmaili", "Mohammad H.", ""], ["Khonsari", "Ahmad", ""], ["Crespi", "Noel", ""]]}, {"id": "1606.01387", "submitter": "Saksham Chand", "authors": "Saksham Chand, Yanhong A. Liu, Scott D. Stoller", "title": "Formal Verification of Multi-Paxos for Distributed Consensus", "comments": null, "journal-ref": "FM 2016: Proceedings of the 21st International Symposium on Formal\n  Methods. LNCS 9995. Pages 119-136. Springer,", "doi": "10.1007/978-3-319-48989-6_8", "report-no": null, "categories": "cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Paxos is an important algorithm for a set of distributed processes to agree\non a single value or a sequence of values, for which it is called Basic Paxos\nor Multi-Paxos, respectively. Consensus is critical when distributed services\nare replicated for fault-tolerance, because non-faulty replicas must agree on\nthe state of the system or the sequence of operations that have been performed.\nUnfortunately, consensus algorithms including Multi-Paxos in particular are\nwell-known to be difficult to understand, and their accurate specifications and\ncorrectness proofs remain challenging, despite extensive studies ever since\nLamport introduced Paxos.\n  This article describes formal specification and verification of Lamport's\nMulti-Paxos algorithm for distributed consensus. The specification is written\nin TLA+, Lamport's Temporal Logic of Actions. The proof is written and\nautomatically checked using TLAPS, the TLA+ Proof System. The proof is for the\nsafety property of the algorithm. Building on Lamport, Merz, and Doligez's\nspecification and proof for Basic Paxos, we aim to facilitate the understanding\nof Multi-Paxos and its proof by minimizing the difference from those for Basic\nPaxos, and to demonstrate a general way of proving other variants of Paxos and\nother sophisticated distributed algorithms. We also discuss our general\nstrategies and results for proving complex invariants using invariance lemmas\nand increments, for proving properties about sets and tuples to help the proof\ncheck succeed in significantly reduced time, and for overall proof improvement\nleading to considerably reduced proof size.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jun 2016 15:48:20 GMT"}, {"version": "v2", "created": "Tue, 7 Jun 2016 02:25:41 GMT"}, {"version": "v3", "created": "Wed, 2 Jan 2019 19:08:49 GMT"}, {"version": "v4", "created": "Mon, 11 Nov 2019 13:28:19 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Chand", "Saksham", ""], ["Liu", "Yanhong A.", ""], ["Stoller", "Scott D.", ""]]}, {"id": "1606.01536", "submitter": "Baosen Zhang", "authors": "Yuanyuan Shi and Bolun Xu and Baosen Zhang and Di Wang", "title": "Leveraging energy storage to optimize data center electricity cost in\n  emerging power markets", "comments": "to appear in Proceedings of ACM E-Energy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy storage in data centers has mainly been used as devices to backup\ngenerators during power outages. Recently, there has been a growing interest in\nusing energy storage devices to actively shape power consumption in data\ncenters to reduce their skyrocketing electricity bills. In this paper, we\nconsider using energy storage in data centers for two applications in a joint\nfashion: reducing peak demand charges and enabling data centers to participate\nin regulation markets. We develop an optimization framework that captures the\ncost of electricity, degradation of energy storage devices, as well as the\nbenefit from regulation markets. Under this frame- work, using real data\nMicrosoft data center traces and PJM regulation signals, we show the\nelectricity bill of a data center can be reduced by up to 20%. Furthermore, we\ndemonstrate that the saving from joint optimization can be even larger than the\nsum of individually optimizing each component. We quantify the particular\naspects of data center load profiles that lead to this superlinear gain.\nCompared to prior works that consider using energy storage devices for each\nsingle application alone, our results suggest that energy storage in data\ncenters can have much larger impacts than previously thought possible.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jun 2016 17:31:46 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Shi", "Yuanyuan", ""], ["Xu", "Bolun", ""], ["Zhang", "Baosen", ""], ["Wang", "Di", ""]]}, {"id": "1606.01588", "submitter": "Mahmoud Ismail", "authors": "Salman Niazi (1), Mahmoud Ismail (1), Steffen Grohsschmiedt (2),\n  Mikael Ronstr\\\"om (3), Seif Haridi (1), Jim Dowling (1) ((1) KTH - Royal\n  Institute of Technology, (2) Spotify AB, (3) Oracle)", "title": "HopsFS: Scaling Hierarchical File System Metadata Using NewSQL Databases", "comments": null, "journal-ref": "The 15th USENIX Conference on File and Storage Technologies (FAST\n  17) (2017) 89-104", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent improvements in both the performance and scalability of\nshared-nothing, transactional, in-memory NewSQL databases have reopened the\nresearch question of whether distributed metadata for hierarchical file systems\ncan be managed using commodity databases. In this paper, we introduce HopsFS, a\nnext generation distribution of the Hadoop Distributed File System (HDFS) that\nreplaces HDFS' single node in-memory metadata service, with a distributed\nmetadata service built on a NewSQL database. By removing the metadata\nbottleneck, HopsFS enables an order of magnitude larger and higher throughput\nclusters compared to HDFS. Metadata capacity has been increased to at least 37\ntimes HDFS' capacity, and in experiments based on a workload trace from\nSpotify, we show that HopsFS supports 16 to 37 times the throughput of Apache\nHDFS. HopsFS also has lower latency for many concurrent clients, and no\ndowntime during failover. Finally, as metadata is now stored in a commodity\ndatabase, it can be safely extended and easily exported to external systems for\nonline analysis and free-text search.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 00:10:35 GMT"}, {"version": "v2", "created": "Wed, 22 Feb 2017 14:18:02 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Niazi", "Salman", ""], ["Ismail", "Mahmoud", ""], ["Grohsschmiedt", "Steffen", ""], ["Ronstr\u00f6m", "Mikael", ""], ["Haridi", "Seif", ""], ["Dowling", "Jim", ""]]}, {"id": "1606.01833", "submitter": "Michael Mitzenmacher", "authors": "Michael Mitzenmacher", "title": "Analyzing Distributed Join-Idle-Queue: A Fluid Limit Approach", "comments": "11 pages, draft paper, likely to be at Allerton 2016, possibly\n  improved before then", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of load balancing, Lu et al. introduced the distributed\nJoin-Idle-Queue algorithm, where a group of dispatchers distribute jobs to a\ncluster of parallel servers. Each dispatcher maintains a queue of idle servers;\nwhen a job arrives to a dispatcher, it sends it to a server on its queue, or to\na random server if the queue is empty. In turn, when a server has no jobs, it\nrequests to be placed on the idle queue of a randomly chosen dispatcher.\n  Although this algorithm was shown to be quite effective, the original\nasymptotic analysis makes simplifying assumptions that become increasingly\ninaccurate as the system load increases. Further, the analysis does not\nnaturally generalize to interesting variations, such as having a server request\nto be placed on the idle queue of a dispatcher before it has completed all\njobs, which can be beneficial under high loads.\n  We provide a new asymptotic analysis of Join-Idle-Queue systems based on mean\nfield fluid limit methods, deriving families of differential equations that\ndescribe these systems. Our analysis avoids previous simplifying assumptions,\nis empirically more accurate, and generalizes naturally to the variation\ndescribed above, as well as other simple variations. Our theoretical and\nempirical analyses shed further light on the performance of Join-Idle-Queue,\nincluding potential performance pitfalls under high load.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 17:15:50 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Mitzenmacher", "Michael", ""]]}, {"id": "1606.01966", "submitter": "Omid Mashayekhi", "authors": "Omid Mashayekhi, Chinmayee Shah, Hang Qu, Andrew Lim, Philip Levis", "title": "Distributed Graphical Simulation in the Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical simulations are a cornerstone of modern media and films. But\nexisting software packages are designed to run on HPC nodes, and perform poorly\nin the computing cloud. These simulations have complex data access patterns\nover complex data structures, and mutate data arbitrarily, and so are a poor\nfit for existing cloud computing systems. We describe a software architecture\nfor running graphical simulations in the cloud that decouples control logic,\ncomputations and data exchanges. This allows a central controller to balance\nload by redistributing computations, and recover from failures. Evaluations\nshow that the architecture can run existing, state-of-the-art simulations in\nthe presence of stragglers and failures, thereby enabling this large class of\napplications to use the computing cloud for the first time.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 22:35:31 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Mashayekhi", "Omid", ""], ["Shah", "Chinmayee", ""], ["Qu", "Hang", ""], ["Lim", "Andrew", ""], ["Levis", "Philip", ""]]}, {"id": "1606.01972", "submitter": "Omid Mashayekhi", "authors": "Omid Mashayekhi, Hang Qu, Chinmayee Shah, Philip Levis", "title": "Scalable, Fast Cloud Computing with Execution Templates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale cloud data analytics applications are often CPU bound. Most of\nthese cycles are wasted: benchmarks written in C++ run 10-51 times faster than\nframeworks such as Naiad and Spark. However, calling faster implementations\nfrom those frameworks only sees moderate (3-5x) speedups because their control\nplanes cannot schedule work fast enough.\n  This paper presents execution templates, a control plane abstraction for\nCPU-bound cloud applications, such as machine learning. Execution templates\nleverage highly repetitive control flow to cache scheduling decisions as {\\it\ntemplates}. Rather than reschedule hundreds of thousands of tasks on every loop\nexecution, nodes instantiate these templates. A controller's template specifies\nthe execution across all worker nodes, which it partitions into per-worker\ntemplates. To ensure that templates execute correctly, controllers dynamically\npatch templates to match program control flow. We have implemented execution\ntemplates in Nimbus, a C++ cloud computing framework. Running in Nimbus,\nanalytics benchmarks can run 16-43 times faster than in Naiad and Spark.\nNimbus's control plane can scale out to run these faster benchmarks on up to\n100 nodes (800 cores).\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 23:10:06 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Mashayekhi", "Omid", ""], ["Qu", "Hang", ""], ["Shah", "Chinmayee", ""], ["Levis", "Philip", ""]]}, {"id": "1606.02007", "submitter": "Amir Vahid Dastjerdi", "authors": "Harshit Gupta, Amir Vahid Dastjerdi, Soumya K. Ghosh, and Rajkumar\n  Buyya", "title": "iFogSim: A Toolkit for Modeling and Simulation of Resource Management\n  Techniques in Internet of Things, Edge and Fog Computing Environments", "comments": "Cloud Computing and Distributed Systems Laboratory, The University of\n  Melbourne, June 6, 2016", "journal-ref": null, "doi": null, "report-no": "CLOUDS-TR-2016-2", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet of Things (IoT) aims to bring every object (e.g. smart cameras,\nwearable, environmental sensors, home appliances, and vehicles) online, hence\ngenerating massive amounts of data that can overwhelm storage systems and data\nanalytics applications. Cloud computing offers services at the infrastructure\nlevel that can scale to IoT storage and processing requirements. However, there\nare applications such as health monitoring and emergency response that require\nlow latency, and delay caused by transferring data to the cloud and then back\nto the application can seriously impact their performances. To overcome this\nlimitation, Fog computing paradigm has been proposed, where cloud services are\nextended to the edge of the network to decrease the latency and network\ncongestion. To realize the full potential of Fog and IoT paradigms for\nreal-time analytics, several challenges need to be addressed. The first and\nmost critical problem is designing resource management techniques that\ndetermine which modules of analytics applications are pushed to each edge\ndevice to minimize the latency and maximize the throughput. To this end, we\nneed a evaluation platform that enables the quantification of performance of\nresource management policies on an IoT or Fog computing infrastructure in a\nrepeatable manner. In this paper we propose a simulator, called iFogSim, to\nmodel IoT and Fog environments and measure the impact of resource management\ntechniques in terms of latency, network congestion, energy consumption, and\ncost. We describe two case studies to demonstrate modeling of an IoT\nenvironment and comparison of resource management policies. Moreover,\nscalability of the simulation toolkit in terms of RAM consumption and execution\ntime is verified under different circumstances.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 02:54:45 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Gupta", "Harshit", ""], ["Dastjerdi", "Amir Vahid", ""], ["Ghosh", "Soumya K.", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "1606.02023", "submitter": "EPTCS", "authors": "Brijesh Dongol (Brunel University London), Lindsay Groves (Victoria\n  University of Wellington)", "title": "Towards linking correctness conditions for concurrent objects and\n  contextual trace refinement", "comments": "In Proceedings Refine'15, arXiv:1606.01344", "journal-ref": "EPTCS 209, 2016, pp. 107-111", "doi": "10.4204/EPTCS.209.8", "report-no": null, "categories": "cs.LO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correctness conditions for concurrent objects describe how atomicity of an\nabstract sequential object may be decomposed. Many different concurrent objects\nand proof methods for them have been developed. However, arguments about\ncorrectness are conducted with respect to an object in isolation. This is in\ncontrast to real-world practice, where concurrent objects are often implemented\nas part of a programming language library (e.g., java.util.concurrent) and are\ninstantiated within a client program. A natural question to ask, then is: How\ndoes a correctness condition for a concurrent object ensure correctness of a\nclient program that uses the concurrent object? This paper presents the main\nissues that surround this question and provides some answers by linking\ndifferent correctness conditions with a form of trace refinement.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 04:09:53 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Dongol", "Brijesh", "", "Brunel University London"], ["Groves", "Lindsay", "", "Victoria\n  University of Wellington"]]}, {"id": "1606.02421", "submitter": "Igor Colin", "authors": "Igor Colin, Aur\\'elien Bellet, Joseph Salmon, St\\'ephan\n  Cl\\'emen\\c{c}on", "title": "Gossip Dual Averaging for Decentralized Optimization of Pairwise\n  Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.DC cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In decentralized networks (of sensors, connected objects, etc.), there is an\nimportant need for efficient algorithms to optimize a global cost function, for\ninstance to learn a global model from the local data collected by each\ncomputing unit. In this paper, we address the problem of decentralized\nminimization of pairwise functions of the data points, where these points are\ndistributed over the nodes of a graph defining the communication topology of\nthe network. This general problem finds applications in ranking, distance\nmetric learning and graph inference, among others. We propose new gossip\nalgorithms based on dual averaging which aims at solving such problems both in\nsynchronous and asynchronous settings. The proposed framework is flexible\nenough to deal with constrained and regularized variants of the optimization\nproblem. Our theoretical analysis reveals that the proposed algorithms preserve\nthe convergence rate of centralized dual averaging up to an additive bias term.\nWe present numerical simulations on Area Under the ROC Curve (AUC) maximization\nand metric learning problems which illustrate the practical interest of our\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 07:01:47 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Colin", "Igor", ""], ["Bellet", "Aur\u00e9lien", ""], ["Salmon", "Joseph", ""], ["Cl\u00e9men\u00e7on", "St\u00e9phan", ""]]}, {"id": "1606.02635", "submitter": "Eric Kerrigan", "authors": "Mason Thammawichai and Eric C. Kerrigan", "title": "Feedback Scheduling for Energy-Efficient Real-Time Homogeneous\n  Multiprocessor Systems", "comments": null, "journal-ref": "Proc. 55th IEEE Conference on Decision and Control, 2016", "doi": "10.1109/CDC.2016.7798501", "report-no": null, "categories": "cs.OS cs.DC cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time scheduling algorithms proposed in the literature are often based on\nworst-case estimates of task parameters. The performance of an open-loop scheme\ncan be degraded significantly if there are uncertainties in task parameters,\nsuch as the execution times of the tasks. Therefore, to cope with such a\nsituation, a closed-loop scheme, where feedback is exploited to adjust the\nsystem parameters, can be applied. We propose an optimal control framework that\ntakes advantage of feeding back information of finished tasks to solve a\nreal-time multiprocessor scheduling problem with uncertainty in task execution\ntimes, with the objective of minimizing the total energy consumption.\nSpecifically, we propose a linear programming based algorithm to solve a\nworkload partitioning problem and adopt McNaughton's wrap around algorithm to\nfind the task execution order. The simulation results illustrate that our\nfeedback scheduling algorithm can save energy by as much as 40% compared to an\nopen-loop method for two processor models, i.e. a PowerPC 405LP and an XScale\nprocessor.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 16:52:03 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Thammawichai", "Mason", ""], ["Kerrigan", "Eric C.", ""]]}, {"id": "1606.02686", "submitter": "J\\'anos V\\'egh", "authors": "J\\'anos V\\'egh and P\\'eter Moln\\'ar and J\\'ozsef V\\'as\\'arhelyi", "title": "A figure of merit for describing the performance of scaling of\n  parallelization", "comments": "14 pages, 6 figures. Contains qualifying merits for scaling in case\n  of compiling methods, HW architectures and algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the spread of multi- and many-core processors more and more typical task\nis to re-implement some source code written originally for a single processor\nto run on more than one cores. Since it is a serious investment, it is\nimportant to decide how much efforts pays off, and whether the resulting\nimplementation has as good performability as it could be. The Amdahl's law\nprovides some theoretical upper limits for the performance gain reachable\nthrough parallelizing the code, but it needs the detailed architectural\nknowledge of the program code, does not consider the housekeeping activity\nneeded for parallelization and cannot tell how the actual stage of\nparallelization implementation performs. The present paper suggests a\nquantitative measure for that goal. This figure of merit is derived\nexperimentally, from measured running time, and number of threads/cores. It can\nbe used to quantify the used parallelization technology, the connection between\nthe computing units, the acceleration technology under the given conditions,\ncommunication method within SoC, or the performance of the software\nteam/compiler.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 18:55:10 GMT"}, {"version": "v2", "created": "Tue, 12 Jul 2016 06:40:18 GMT"}, {"version": "v3", "created": "Fri, 22 Jul 2016 06:58:52 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["V\u00e9gh", "J\u00e1nos", ""], ["Moln\u00e1r", "P\u00e9ter", ""], ["V\u00e1s\u00e1rhelyi", "J\u00f3zsef", ""]]}, {"id": "1606.02738", "submitter": "Matthieu Schaller", "authors": "Matthieu Schaller (1), Pedro Gonnet (2,3), Aidan B. G. Chalk (2),\n  Peter W. Draper (1) ((1) ICC, Durham University, (2) ECS, Durham University,\n  (3) Google Switzerland GmbH)", "title": "SWIFT: Using task-based parallelism, fully asynchronous communication,\n  and graph partition-based domain decomposition for strong scaling on more\n  than 100,000 cores", "comments": "9 pages, 7 figures. Code, scripts and examples available at\n  http://icc.dur.ac.uk/swift/", "journal-ref": "Proceedings of the Platform for Advanced Scientific Computing\n  Conference, 2016, 2, 2:1-2:10, Lausanne, Switzerland", "doi": "10.1145/2929908.2929916", "report-no": null, "categories": "cs.DC astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new open-source cosmological code, called SWIFT, designed to\nsolve the equations of hydrodynamics using a particle-based approach (Smooth\nParticle Hydrodynamics) on hybrid shared/distributed-memory architectures.\nSWIFT was designed from the bottom up to provide excellent strong scaling on\nboth commodity clusters (Tier-2 systems) and Top100-supercomputers (Tier-0\nsystems), without relying on architecture-specific features or specialized\naccelerator hardware. This performance is due to three main computational\napproaches: (1) Task-based parallelism for shared-memory parallelism, which\nprovides fine-grained load balancing and thus strong scaling on large numbers\nof cores. (2) Graph-based domain decomposition, which uses the task graph to\ndecompose the simulation domain such that the work, as opposed to just the\ndata, as is the case with most partitioning schemes, is equally distributed\nacross all nodes. (3) Fully dynamic and asynchronous communication, in which\ncommunication is modelled as just another task in the task-based scheme,\nsending data whenever it is ready and deferring on tasks that rely on data from\nother nodes until it arrives. In order to use these approaches, the code had to\nbe re-written from scratch, and the algorithms therein adapted to the\ntask-based paradigm. As a result, we can show upwards of 60% parallel\nefficiency for moderate-sized problems when increasing the number of cores\n512-fold, on both x86-based and Power8-based architectures.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 20:22:15 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Schaller", "Matthieu", ""], ["Gonnet", "Pedro", ""], ["Chalk", "Aidan B. G.", ""], ["Draper", "Peter W.", ""]]}, {"id": "1606.02862", "submitter": "Erik Zenker", "authors": "Erik Zenker, Ren\\'e Widera, Axel Huebl, Guido Juckeland, Andreas\n  Kn\\\"upfer, Wolfgang E. Nagel, Michael Bussmann", "title": "Performance-Portable Many-Core Plasma Simulations: Porting PIConGPU to\n  OpenPower and Beyond", "comments": "9 pages, 3 figures, accepted on IWOPH 2016", "journal-ref": "Lecture Notes in Computer Science, 9945, pp 293-301, 2016", "doi": "10.1007/978-3-319-46079-6_21", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the appearance of the heterogeneous platform OpenPower,many-core\naccelerator devices have been coupled with Power host processors for the first\ntime. Towards utilizing their full potential, it is worth investigating\nperformance portable algorithms that allow to choose the best-fitting hardware\nfor each domain-specific compute task. Suiting even the high level of\nparallelism on modern GPGPUs, our presented approach relies heavily on abstract\nmeta-programming techniques, which are essential to focus on fine-grained\ntuning rather than code porting. With this in mind, the CUDA-based open-source\nplasma simulation code PIConGPU is currently being abstracted to support the\nheterogeneous OpenPower platform using our fast porting interface cupla, which\nwraps the abstract parallel C++11 kernel acceleration library Alpaka. We\ndemonstrate how PIConGPU can benefit from the tunable kernel execution\nstrategies of the Alpaka library, achieving portability and performance with\nsingle-source kernels on conventional CPUs, Power8 CPUs and NVIDIA GPUs.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 08:24:07 GMT"}, {"version": "v2", "created": "Sun, 12 Jun 2016 12:50:07 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Zenker", "Erik", ""], ["Widera", "Ren\u00e9", ""], ["Huebl", "Axel", ""], ["Juckeland", "Guido", ""], ["Kn\u00fcpfer", "Andreas", ""], ["Nagel", "Wolfgang E.", ""], ["Bussmann", "Michael", ""]]}, {"id": "1606.02942", "submitter": "Leandro Indrusiak", "authors": "Leandro Soares Indrusiak and Alan Burns and Borislav Nikolic", "title": "Analysis of buffering effects on hard real-time priority-preemptive\n  wormhole networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are several approaches to analyse the worst-case response times of\nsporadic packets transmitted over priority-preemptive wormhole networks. In\nthis paper, we provide an overview of the different approaches, discuss their\nstrengths and weaknesses, and propose an approach that captures all effects\nconsidered by previous approaches while providing tight yet safe upper bounds\nfor packet response times. We specifically address the problems created by\nbuffering and backpressure in wormhole networks, which amplifies the problem of\nindirect interference in a way that has not been considered by the early\nanalysis approaches. Didactic examples and large-scale experiments with\nsynthetically generated packet flow sets provide evidence of the strength of\nthe proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 13:04:16 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Indrusiak", "Leandro Soares", ""], ["Burns", "Alan", ""], ["Nikolic", "Borislav", ""]]}, {"id": "1606.03248", "submitter": "ShenChen Ruan", "authors": "Shenchen Ruan, Haixia Wang and Dongsheng Wang", "title": "MAC: a novel systematically multilevel cache replacement policy for PCM\n  memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid development of multi-core system and increase of data-intensive\napplication in recent years call for larger main memory. Traditional DRAM\nmemory can increase its capacity by reducing the feature size of storage cell.\nNow further scaling of DRAM faces great challenge, and the frequent refresh\noperations of DRAM can bring a lot of energy consumption. As an emerging\ntechnology, Phase Change Memory (PCM) is promising to be used as main memory.\nIt draws wide attention due to the advantages of low power consumption, high\ndensity and nonvolatility, while it incurs finite endurance and relatively long\nwrite latency. To handle the problem of write, optimizing the cache replacement\npolicy to protect dirty cache block is an efficient way. In this paper, we\nconstruct a systematically multilevel structure, and based on it propose a\nnovel cache replacement policy called MAC. MAC can effectively reduce write\ntraffic to PCM memory with low hardware overhead. We conduct simulation\nexperiments on GEM5 to evaluate the performances of MAC and other related\nworks. The results show that MAC performs best in reducing the amount of writes\n(averagely 25.12%) without increasing the program execution time.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 09:47:14 GMT"}], "update_date": "2016-06-13", "authors_parsed": [["Ruan", "Shenchen", ""], ["Wang", "Haixia", ""], ["Wang", "Dongsheng", ""]]}, {"id": "1606.03335", "submitter": "Roman Bartusiak", "authors": "Roman Bartusiak, {\\L}ukasz Augustyniak, Tomasz Kajdanowicz,\n  Przemys{\\l}aw Kazienko, Maciej Piasecki", "title": "WordNet2Vec: Corpora Agnostic Word Vectorization Method", "comments": "29 pages, 16 figures, submitted to journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A complex nature of big data resources demands new methods for structuring\nespecially for textual content. WordNet is a good knowledge source for\ncomprehensive abstraction of natural language as its good implementations exist\nfor many languages. Since WordNet embeds natural language in the form of a\ncomplex network, a transformation mechanism WordNet2Vec is proposed in the\npaper. It creates vectors for each word from WordNet. These vectors encapsulate\ngeneral position - role of a given word towards all other words in the natural\nlanguage. Any list or set of such vectors contains knowledge about the context\nof its component within the whole language. Such word representation can be\neasily applied to many analytic tasks like classification or clustering. The\nusefulness of the WordNet2Vec method was demonstrated in sentiment analysis,\ni.e. classification with transfer learning for the real Amazon opinion textual\ndataset.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 14:12:47 GMT"}], "update_date": "2016-06-13", "authors_parsed": [["Bartusiak", "Roman", ""], ["Augustyniak", "\u0141ukasz", ""], ["Kajdanowicz", "Tomasz", ""], ["Kazienko", "Przemys\u0142aw", ""], ["Piasecki", "Maciej", ""]]}, {"id": "1606.03418", "submitter": "Lili Su", "authors": "Lili Su, Nitin H. Vaidya", "title": "Asynchronous Distributed Hypothesis Testing in the Presence of Crash\n  Failures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of distributed hypothesis testing in\nmulti-agent networks, where agents repeatedly collect local observations about\nan unknown state of the world, and try to collaboratively detect the true state\nthrough information exchange. We focus on the impact of failures and asynchrony\n(two fundamental factors in distributed systems) on the performance of\nconsensus-based non-Bayesian learning. In particular, we consider the scenario\nwhere the networked agents may suffer crash faults, and messages delay can be\narbitrarily long but finite. We identify the minimal global detectability of\nthe network for non-Bayesian rule to succeed. In addition, we obtain a\ngeneralization of a celebrated result by Wolfowitz and Hajnal to submatrices,\nwhich might be of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2016 21:12:53 GMT"}], "update_date": "2016-06-13", "authors_parsed": [["Su", "Lili", ""], ["Vaidya", "Nitin H.", ""]]}, {"id": "1606.03742", "submitter": "Vincent T. Lee", "authors": "Vincent T. Lee, Amrita Mazumdar, Carlo C. del Mundo, Armin Alaghi,\n  Luis Ceze, Mark Oskin", "title": "Application-Driven Near-Data Processing for Similarity Search", "comments": "15 pages, 8 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity search is a key to a variety of applications including\ncontent-based search for images and video, recommendation systems, data\ndeduplication, natural language processing, computer vision, databases,\ncomputational biology, and computer graphics. At its core, similarity search\nmanifests as k-nearest neighbors (kNN), a computationally simple primitive\nconsisting of highly parallel distance calculations and a global top-k sort.\nHowever, kNN is poorly supported by today's architectures because of its high\nmemory bandwidth requirements.\n  This paper proposes an application-driven near-data processing accelerator\nfor similarity search: the Similarity Search Associative Memory (SSAM). By\ninstantiating compute units close to memory, SSAM benefits from the higher\nmemory bandwidth and density exposed by emerging memory technologies. We\nevaluate the SSAM design down to layout on top of the Micron hybrid memory cube\n(HMC), and show that SSAM can achieve up to two orders of magnitude\narea-normalized throughput and energy efficiency improvement over multicore\nCPUs; we also show SSAM is faster and more energy efficient than competing GPUs\nand FPGAs. Finally, we show that SSAM is also useful for other data intensive\ntasks like kNN index construction, and can be generalized to semantically\nfunction as a high capacity content addressable memory.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jun 2016 17:08:43 GMT"}, {"version": "v2", "created": "Mon, 10 Jul 2017 16:56:51 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Lee", "Vincent T.", ""], ["Mazumdar", "Amrita", ""], ["del Mundo", "Carlo C.", ""], ["Alaghi", "Armin", ""], ["Ceze", "Luis", ""], ["Oskin", "Mark", ""]]}, {"id": "1606.03928", "submitter": "Konrad Siek", "authors": "Konrad Siek and Pawe{\\l} T. Wojciechowski", "title": "Atomic RMI 2: Highly Parallel Pessimistic Distributed Transactional\n  Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed Transactional Memory (DTM) is an emerging approach to distributed\nsynchronization based on the application of the transaction abstraction to\ndistributed computation. DTM comes in several system models, but the control\nflow model (CF) is particularly powerful, since it allows transactions to\ndelegate computation to remote nodes as well as access shared data. However,\nthere are no existing CF DTM systems that perform on par with state-of-the-art\nsystems operating in other models. Hence, we introduce a CF DTM synchronization\nalgorithm, OptSVA-CF. It supports fine-grained pessimistic concurrency control,\nso it avoids aborts, and thus avoids problems with irrevocable operations.\nFurthermore, it uses early release and asynchrony to parallelize concurrent\ntransactions to a high degree, while retaining strong safety properties. We\nimplement it as Atomic RMI 2, in effect producing a CF DTM system that, as our\nevaluation shows, can outperform a state-of-the-art non-CF DTM such as HyFlow2.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 12:58:24 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Siek", "Konrad", ""], ["Wojciechowski", "Pawe\u0142 T.", ""]]}, {"id": "1606.03966", "submitter": "Siddhartha Sen", "authors": "Alekh Agarwal, Sarah Bird, Markus Cozowicz, Luong Hoang, John\n  Langford, Stephen Lee, Jiaji Li, Dan Melamed, Gal Oshri, Oswaldo Ribas,\n  Siddhartha Sen, Alex Slivkins", "title": "Making Contextual Decisions with Low Technical Debt", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications and systems are constantly faced with decisions that require\npicking from a set of actions based on contextual information.\nReinforcement-based learning algorithms such as contextual bandits can be very\neffective in these settings, but applying them in practice is fraught with\ntechnical debt, and no general system exists that supports them completely. We\naddress this and create the first general system for contextual learning,\ncalled the Decision Service.\n  Existing systems often suffer from technical debt that arises from issues\nlike incorrect data collection and weak debuggability, issues we systematically\naddress through our ML methodology and system abstractions. The Decision\nService enables all aspects of contextual bandit learning using four system\nabstractions which connect together in a loop: explore (the decision space),\nlog, learn, and deploy. Notably, our new explore and log abstractions ensure\nthe system produces correct, unbiased data, which our learner uses for online\nlearning and to enable real-time safeguards, all in a fully reproducible\nmanner.\n  The Decision Service has a simple user interface and works with a variety of\napplications: we present two live production deployments for content\nrecommendation that achieved click-through improvements of 25-30%, another with\n18% revenue lift in the landing page, and ongoing applications in tech support\nand machine failure handling. The service makes real-time decisions and learns\ncontinuously and scalably, while significantly lowering technical debt.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 14:17:00 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 14:41:15 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Agarwal", "Alekh", ""], ["Bird", "Sarah", ""], ["Cozowicz", "Markus", ""], ["Hoang", "Luong", ""], ["Langford", "John", ""], ["Lee", "Stephen", ""], ["Li", "Jiaji", ""], ["Melamed", "Dan", ""], ["Oshri", "Gal", ""], ["Ribas", "Oswaldo", ""], ["Sen", "Siddhartha", ""], ["Slivkins", "Alex", ""]]}, {"id": "1606.04074", "submitter": "Pedro Lopez-Garcia", "authors": "Kerstin Eder and John P. Gallagher and Pedro Lopez-Garcia and Henk\n  Muller and Zorana Bankovic and Kyriakos Georgiou and Remy Haemmerle and\n  Manuel V. Hermenegildo and Bishoksan Kafle and Steve Kerrison and Maja\n  Kirkeby and Maximiliano Klemen and Xueliang Li and Umer Liqat and Jeremy\n  Morse and Morten Rhiger and Mads Rosendahl", "title": "ENTRA: Whole-Systems Energy Transparency", "comments": "Revised preprint submitted to MICPRO on 27 May 2016, 23 pages, 3\n  figures", "journal-ref": null, "doi": "10.1016/j.micpro.2016.07.003", "report-no": null, "categories": "cs.AR cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Promoting energy efficiency to a first class system design goal is an\nimportant research challenge. Although more energy-efficient hardware can be\ndesigned, it is software that controls the hardware; for a given system the\npotential for energy savings is likely to be much greater at the higher levels\nof abstraction in the system stack. Thus the greatest savings are expected from\nenergy-aware software development, which is the vision of the EU ENTRA project.\nThis article presents the concept of energy transparency as a foundation for\nenergy-aware software development. We show how energy modelling of hardware is\ncombined with static analysis to allow the programmer to understand the energy\nconsumption of a program without executing it, thus enabling exploration of the\ndesign space taking energy into consideration. The paper concludes by\nsummarising the current and future challenges identified in the ENTRA project.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 19:16:52 GMT"}, {"version": "v2", "created": "Sat, 18 Jun 2016 05:56:38 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Eder", "Kerstin", ""], ["Gallagher", "John P.", ""], ["Lopez-Garcia", "Pedro", ""], ["Muller", "Henk", ""], ["Bankovic", "Zorana", ""], ["Georgiou", "Kyriakos", ""], ["Haemmerle", "Remy", ""], ["Hermenegildo", "Manuel V.", ""], ["Kafle", "Bishoksan", ""], ["Kerrison", "Steve", ""], ["Kirkeby", "Maja", ""], ["Klemen", "Maximiliano", ""], ["Li", "Xueliang", ""], ["Liqat", "Umer", ""], ["Morse", "Jeremy", ""], ["Rhiger", "Morten", ""], ["Rosendahl", "Mads", ""]]}, {"id": "1606.04209", "submitter": "Xuan Yang", "authors": "Xuan Yang, Jing Pu, Blaine Burton Rister, Nikhil Bhagdikar, Stephen\n  Richardson, Shahar Kvatinsky, Jonathan Ragan-Kelley, Ardavan Pedram and Mark\n  Horowitz", "title": "A Systematic Approach to Blocking Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) are the state of the art solution for\nmany computer vision problems, and many researchers have explored optimized\nimplementations. Most implementations heuristically block the computation to\ndeal with the large data sizes and high data reuse of CNNs. This paper explores\nhow to block CNN computations for memory locality by creating an analytical\nmodel for CNN-like loop nests. Using this model we automatically derive\noptimized blockings for common networks that improve the energy efficiency of\ncustom hardware implementations by up to an order of magnitude. Compared to\ntraditional CNN CPU implementations based on highly-tuned, hand-optimized BLAS\nlibraries,our x86 programs implementing the optimal blocking reduce the number\nof memory accesses by up to 90%.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 06:22:30 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Yang", "Xuan", ""], ["Pu", "Jing", ""], ["Rister", "Blaine Burton", ""], ["Bhagdikar", "Nikhil", ""], ["Richardson", "Stephen", ""], ["Kvatinsky", "Shahar", ""], ["Ragan-Kelley", "Jonathan", ""], ["Pedram", "Ardavan", ""], ["Horowitz", "Mark", ""]]}, {"id": "1606.04282", "submitter": "Polyvios Pratikakis", "authors": "Spyros Lyberis, Polyvios Pratikakis, Iakovos Mavroidis, Dimitrios S.\n  Nikolopoulos", "title": "Myrmics: Scalable, Dependency-aware Task Scheduling on Heterogeneous\n  Manycores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Task-based programming models have become very popular, as they offer an\nattractive solution to parallelize serial application code with task and data\nannotations. They usually depend on a runtime system that schedules the tasks\nto multiple cores in parallel while resolving any data hazards. However,\nexisting runtime system implementations are not ready to scale well on emerging\nmanycore processors, as they often rely on centralized structures and/or locks\non shared structures in a cache-coherent memory. We propose design choices,\npolicies and mechanisms to enhance runtime system scalability for single-chip\nprocessors with hundreds of cores. Based on these concepts, we create and\nevaluate Myrmics, a runtime system for a dependency-aware, task-based\nprogramming model on a heterogeneous hardware prototype platform that emulates\na single-chip processor of 8 latency-optimized and 512 throughput-optimized\nCPUs. We find that Myrmics scales successfully to hundreds of cores. Compared\nto MPI versions of the same benchmarks with hand-tuned message passing, Myrmics\nachieves similar scalability with a 10-30% performance overhead, but with less\nprogramming effort. We analyze the scalability of the runtime system in detail\nand identify the key factors that contribute to it.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 09:46:40 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Lyberis", "Spyros", ""], ["Pratikakis", "Polyvios", ""], ["Mavroidis", "Iakovos", ""], ["Nikolopoulos", "Dimitrios S.", ""]]}, {"id": "1606.04288", "submitter": "Polyvios Pratikakis", "authors": "Alexandros Labrineas, Polyvios Pratikakis, Dimitrios S. Nikolopoulos,\n  Angelos Bilas", "title": "BDDT-SCC: A Task-parallel Runtime for Non Cache-Coherent Multicores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents BDDT-SCC, a task-parallel runtime system for non\ncache-coherent multicore processors, implemented for the Intel Single-Chip\nCloud Computer. The BDDT-SCC runtime includes a dynamic dependence analysis and\nautomatic synchronization, and executes OpenMP-Ss tasks on a non cache-coherent\narchitecture. We design a runtime that uses fast on-chip inter-core\ncommunication with small messages. At the same time, we use non coherent shared\nmemory to avoid large core-to-core data transfers that would incur a high\nvolume of unnecessary copying. We evaluate BDDT-SCC on a set of representative\nbenchmarks, in terms of task granularity, locality, and communication. We find\nthat memory locality and allocation plays a very important role in performance,\nas the architecture of the SCC memory controllers can create strong contention\neffects. We suggest patterns that improve memory locality and thus the\nperformance of applications, and measure their impact.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 10:09:42 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Labrineas", "Alexandros", ""], ["Pratikakis", "Polyvios", ""], ["Nikolopoulos", "Dimitrios S.", ""], ["Bilas", "Angelos", ""]]}, {"id": "1606.04296", "submitter": "Foivos Zakkak", "authors": "Foivos S. Zakkak and Polyvios Pratikakis", "title": "DiSquawk: 512 cores, 512 memories, 1 JVM", "comments": null, "journal-ref": null, "doi": null, "report-no": "FORTH-ICS/TR-470, June 2016", "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trying to cope with the constantly growing number of cores per processor,\nhardware architects are experimenting with modular non-cache-coherent\narchitectures. Such architectures delegate the memory coherency to the\nsoftware. On the contrary, high productivity languages, like Java, are designed\nto abstract away the hardware details and allow developers to focus on the\nimplementation of their algorithm. Such programming languages rely on a process\nvirtual machine to perform the necessary operations to implement the\ncorresponding memory model. Arguing about the correctness of such\nimplementations is not trivial though.\n  In this work we present our implementation of the Java Memory Model in a Java\nVirtual Machine targeting a 512-core non-cache-coherent memory architecture. We\nshortly discuss design decisions and present early evaluation results, which\ndemonstrate that our implementation scales with the number of cores. We model\nour implementation as the operational semantics of a Java Core Calculus that we\nextend with synchronization actions, and prove its adherence to the Java Memory\nModel.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 10:43:18 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Zakkak", "Foivos S.", ""], ["Pratikakis", "Polyvios", ""]]}, {"id": "1606.04386", "submitter": "Bj\\\"orn Brandenburg", "authors": "Jian-Jia Chen and Bj\\\"orn B. Brandenburg", "title": "A Note on the Period Enforcer Algorithm for Self-Suspending Tasks", "comments": null, "journal-ref": null, "doi": "10.4230/LITES-v004-i001-a001", "report-no": null, "categories": "cs.DC cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The period enforcer algorithm for self-suspending real-time tasks is a\ntechnique for suppressing the \"back-to-back\" scheduling penalty associated with\ndeferred execution. Originally proposed in 1991, the algorithm has attracted\nrenewed interest in recent years. This note revisits the algorithm in the light\nof recent developments in the analysis of self-suspending tasks, carefully\nre-examines and explains its underlying assumptions and limitations, and points\nout three observations that have not been made in the literature to date: (i)\nperiod enforcement is not strictly superior (compared to the base case without\nenforcement) as it can cause deadline misses in self-suspending task sets that\nare schedulable without enforcement; (ii) to match the assumptions underlying\nthe analysis of the period enforcer, a schedulability analysis of\nself-suspending tasks subject to period enforcement requires a task set\ntransformation for which no solution is known in the general case, and which is\nsubject to exponential time complexity (with current techniques) in the limited\ncase of a single self-suspending task; and (iii) the period enforcer algorithm\nis incompatible with all existing analyses of suspension-based locking\nprotocols, and can in fact cause ever-increasing suspension times until a\ndeadline is missed.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 14:22:25 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Chen", "Jian-Jia", ""], ["Brandenburg", "Bj\u00f6rn B.", ""]]}, {"id": "1606.04427", "submitter": "Tim Dykes", "authors": "Timothy Dykes, Claudio Gheller, Marzia Rivi, Mel Krokos", "title": "Splotch: porting and optimizing for the Xeon Phi", "comments": "Version 1, 11 pages, 14 figures. Accepted for publication in\n  International Journal of High Performance Computing Applications (IJHPCA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing size and complexity of data produced by large scale\nnumerical simulations, it is of primary importance for scientists to be able to\nexploit all available hardware in heterogenous High Performance Computing\nenvironments for increased throughput and efficiency. We focus on the porting\nand optimization of Splotch, a scalable visualization algorithm, to utilize the\nXeon Phi, Intel's coprocessor based upon the new Many Integrated Core\narchitecture. We discuss steps taken to offload data to the coprocessor and\nalgorithmic modifications to aid faster processing on the many-core\narchitecture and make use of the uniquely wide vector capabilities of the\ndevice, with accompanying performance results using multiple Xeon Phi. Finally\nperformance is compared against results achieved with the GPU implementation of\nSplotch.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 15:36:58 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Dykes", "Timothy", ""], ["Gheller", "Claudio", ""], ["Rivi", "Marzia", ""], ["Krokos", "Mel", ""]]}, {"id": "1606.04434", "submitter": "Laurent Feuilloley", "authors": "Laurent Feuilloley (1), Pierre Fraigniaud (1) ((1) IRIF, GANG)", "title": "Survey of Distributed Decision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We survey the recent distributed computing literature on checking whether a\ngiven distributed system configuration satisfies a given boolean predicate,\ni.e., whether the configuration is legal or illegal w.r.t. that predicate. We\nconsider classical distributed computing environments, including mostly\nsynchronous fault-free network computing (LOCAL and CONGEST models), but also\nasynchronous crash-prone shared-memory computing (WAIT-FREE model), and mobile\ncomputing (FSYNC model).\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 16:00:15 GMT"}, {"version": "v2", "created": "Wed, 22 Jun 2016 16:33:20 GMT"}, {"version": "v3", "created": "Wed, 20 Sep 2017 11:25:15 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Feuilloley", "Laurent", "", "IRIF, GANG"], ["Fraigniaud", "Pierre", "", "IRIF, GANG"]]}, {"id": "1606.04456", "submitter": "Alina S\\^irbu", "authors": "Alina S\\^irbu and Ozalp Babaoglu", "title": "Towards Operator-less Data Centers Through Data-Driven, Predictive,\n  Proactive Autonomics", "comments": null, "journal-ref": "Cluster Computing, Volume 19, Issue 2, pp 865-878, 2016", "doi": "10.1007/s10586-016-0564-y", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continued reliance on human operators for managing data centers is a major\nimpediment for them from ever reaching extreme dimensions. Large computer\nsystems in general, and data centers in particular, will ultimately be managed\nusing predictive computational and executable models obtained through\ndata-science tools, and at that point, the intervention of humans will be\nlimited to setting high-level goals and policies rather than performing\nlow-level operations. Data-driven autonomics, where management and control are\nbased on holistic predictive models that are built and updated using live data,\nopens one possible path towards limiting the role of operators in data centers.\nIn this paper, we present a data-science study of a public Google dataset\ncollected in a 12K-node cluster with the goal of building and evaluating\npredictive models for node failures. Our results support the practicality of a\ndata-driven approach by showing the effectiveness of predictive models based on\ndata found in typical data center logs. We use BigQuery, the big data SQL\nplatform from the Google Cloud suite, to process massive amounts of data and\ngenerate a rich feature set characterizing node state over time. We describe\nhow an ensemble classifier can be built out of many Random Forest classifiers\neach trained on these features, to predict if nodes will fail in a future\n24-hour window. Our evaluation reveals that if we limit false positive rates to\n5%, we can achieve true positive rates between 27% and 88% with precision\nvarying between 50% and 72%.This level of performance allows us to recover\nlarge fraction of jobs' executions (by redirecting them to other nodes when a\nfailure of the present node is predicted) that would otherwise have been wasted\ndue to failures. [...]\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 16:55:01 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["S\u00eerbu", "Alina", ""], ["Babaoglu", "Ozalp", ""]]}, {"id": "1606.04473", "submitter": "Blesson Varghese", "authors": "Javier Prades, Blesson Varghese, Carlos Reano and Federico Silla", "title": "Multi-Tenant Virtual GPUs for Optimising Performance of a Financial Risk\n  Application", "comments": "Accepted to the Journal of Parallel and Distributed Computing (JPDC),\n  10 June 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphics Processing Units (GPUs) are becoming popular accelerators in modern\nHigh-Performance Computing (HPC) clusters. Installing GPUs on each node of the\ncluster is not efficient resulting in high costs and power consumption as well\nas underutilisation of the accelerator. The research reported in this paper is\nmotivated towards the use of few physical GPUs by providing cluster nodes\naccess to remote GPUs on-demand for a financial risk application. We\nhypothesise that sharing GPUs between several nodes, referred to as\nmulti-tenancy, reduces the execution time and energy consumed by an\napplication. Two data transfer modes between the CPU and the GPUs, namely\nconcurrent and sequential, are explored. The key result from the experiments is\nthat multi-tenancy with few physical GPUs using sequential data transfers\nlowers the execution time and the energy consumed, thereby improving the\noverall performance of the application.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 17:47:09 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Prades", "Javier", ""], ["Varghese", "Blesson", ""], ["Reano", "Carlos", ""], ["Silla", "Federico", ""]]}, {"id": "1606.04487", "submitter": "Stefan Hadjis", "authors": "Stefan Hadjis, Ce Zhang, Ioannis Mitliagkas, Dan Iter, Christopher\n  R\\'e", "title": "Omnivore: An Optimizer for Multi-device Deep Learning on CPUs and GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the factors affecting training time in multi-device deep learning\nsystems. Given a specification of a convolutional neural network, our goal is\nto minimize the time to train this model on a cluster of commodity CPUs and\nGPUs. We first focus on the single-node setting and show that by using standard\nbatching and data-parallel techniques, throughput can be improved by at least\n5.5x over state-of-the-art systems on CPUs. This ensures an end-to-end training\nspeed directly proportional to the throughput of a device regardless of its\nunderlying hardware, allowing each node in the cluster to be treated as a black\nbox. Our second contribution is a theoretical and empirical study of the\ntradeoffs affecting end-to-end training time in a multiple-device setting. We\nidentify the degree of asynchronous parallelization as a key factor affecting\nboth hardware and statistical efficiency. We see that asynchrony can be viewed\nas introducing a momentum term. Our results imply that tuning momentum is\ncritical in asynchronous parallel configurations, and suggest that published\nresults that have not been fully tuned might report suboptimal performance for\nsome configurations. For our third contribution, we use our novel understanding\nof the interaction between system and optimization dynamics to provide an\nefficient hyperparameter optimizer. Our optimizer involves a predictive model\nfor the total time to convergence and selects an allocation of resources to\nminimize that time. We demonstrate that the most popular distributed deep\nlearning systems fall within our tradeoff space, but do not optimize within the\nspace. By doing this optimization, our prototype runs 1.9x to 12x faster than\nthe fastest state-of-the-art systems.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 18:21:04 GMT"}, {"version": "v2", "created": "Mon, 18 Jul 2016 00:05:49 GMT"}, {"version": "v3", "created": "Fri, 26 Aug 2016 13:04:00 GMT"}, {"version": "v4", "created": "Wed, 19 Oct 2016 04:26:03 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Hadjis", "Stefan", ""], ["Zhang", "Ce", ""], ["Mitliagkas", "Ioannis", ""], ["Iter", "Dan", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1606.04669", "submitter": "Massimo Cafaro", "authors": "Massimo Cafaro, Marco Pulimeno, Italo Epicoco and Giovanni Aloisio", "title": "Parallel Space Saving on Multi and Many-Core Processors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an array $\\mathcal{A}$ of $n$ elements and a value $2 \\leq k \\leq n$, a\nfrequent item or $k$-majority element is an element occurring in $\\mathcal{A}$\nmore than $n/k$ times. The $k$-majority problem requires finding all of the\n$k$-majority elements. In this paper we deal with parallel shared-memory\nalgorithms for frequent items; we present a shared-memory version of the Space\nSaving algorithm and we study its behavior with regard to accuracy and\nperformance on many and multi-core processors, including the Intel Phi\naccelerator. We also investigate a hybrid MPI/OpenMP version against a pure MPI\nbased version. Through extensive experimental results we prove that the\nMPI/OpenMP parallel version of the algorithm significantly enhances the\nperformance of the earlier pure MPI version of the same algorithm. Results also\nprove that for this algorithm the Intel Phi accelerator does not introduce any\nimprovement with respect to the Xeon octa-core processor.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 08:16:11 GMT"}, {"version": "v2", "created": "Wed, 11 Jan 2017 16:49:13 GMT"}], "update_date": "2017-01-12", "authors_parsed": [["Cafaro", "Massimo", ""], ["Pulimeno", "Marco", ""], ["Epicoco", "Italo", ""], ["Aloisio", "Giovanni", ""]]}, {"id": "1606.04746", "submitter": "Vincenzo Gulisano", "authors": "Vincenzo Gulisano, Yiannis Nikolakopoulos, Daniel Cederman, Marina\n  Papatriantafilou and Philippas Tsigas", "title": "Efficient data streaming multiway aggregation through concurrent\n  algorithmic designs and new abstract data types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data streaming relies on continuous queries to process unbounded streams of\ndata in a real-time fashion. It is commonly demanding in computation capacity,\ngiven that the relevant applications involve very large volumes of data. Data\nstructures act as articulation points and maintain the state of data streaming\noperators, potentially supporting high parallelism and balancing the work\nbetween them. Prompted by this fact, in this work we study and analyze\nparallelization needs of these articulation points, focusing on the problem of\nstreaming multiway aggregation, where large data volumes are received from\nmultiple input streams. The analysis of the parallelization needs, as well as\nof the use and limitations of existing aggregate designs and their data\nstructures, leads us to identify needs for proper shared objects that can\nachieve low-latency and high throughput multiway aggregation. We present the\nrequirements of such objects as abstract data types and we provide efficient\nlock-free linearizable algorithmic implementations of them, along with new\nmultiway aggregate algorithmic designs that leverage them, supporting both\ndeterministic order-sensitive and order-insensitive aggregate functions.\nFurthermore, we point out future directions that open through these\ncontributions. The paper includes an extensive experimental study, based on a\nvariety of aggregation continuous queries on two large datasets extracted from\nSoundCloud, a music social network, and from a Smart Grid network. In all the\nexperiments, the proposed data structures and the enhanced aggregate operators\nimproved the processing performance significantly, up to one order of\nmagnitude, in terms of both throughput and latency, over the commonly-used\ntechniques based on queues.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 13:01:38 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Gulisano", "Vincenzo", ""], ["Nikolakopoulos", "Yiannis", ""], ["Cederman", "Daniel", ""], ["Papatriantafilou", "Marina", ""], ["Tsigas", "Philippas", ""]]}, {"id": "1606.04830", "submitter": "Alexandr Kosenkov", "authors": "Alex Kosenkov, Matthias Troyer", "title": "Bind: a Partitioned Global Workflow Parallel Programming Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High Performance Computing is notorious for its long and expensive software\ndevelopment cycle. To address this challenge, we present Bind: a \"partitioned\nglobal workflow\" parallel programming model for C++ applications that enables\nquick prototyping and agile development cycles for high performance computing\nsoftware targeting heterogeneous distributed many-core architectures. We\npresent applications of Bind to Linear Algebra and MapReduce algorithms\nalongside with performance results.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 16:00:59 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Kosenkov", "Alex", ""], ["Troyer", "Matthias", ""]]}, {"id": "1606.05134", "submitter": "Sabri Pllana", "authors": "Suejb Memeti and Sabri Pllana", "title": "Combinatorial Optimization of Work Distribution on Heterogeneous Systems", "comments": "ICPP Workshops 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an approach that uses combinatorial optimization and machine\nlearning to share the work between the host and device of heterogeneous\ncomputing systems such that the overall application execution time is\nminimized. We propose to use combinatorial optimization to search for the\noptimal system configuration in the given parameter space (such as, the number\nof threads, thread affinity, work distribution for the host and device). For\neach system configuration that is suggested by combinatorial optimization, we\nuse machine learning for evaluation of the system performance. We evaluate our\napproach experimentally using a heterogeneous platform that comprises two\n12-core Intel Xeon E5 CPUs and an Intel Xeon Phi 7120P co-processor with 61\ncores. Using our approach we are able to find a near-optimal system\nconfiguration by performing only about 5% of all possible experiments.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 10:47:58 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Memeti", "Suejb", ""], ["Pllana", "Sabri", ""]]}, {"id": "1606.05168", "submitter": "Severin Sadjina", "authors": "Severin Sadjina, Eilif Pedersen", "title": "Energy Conservation and Coupling Error Reduction in Non-Iterative\n  Co-Simulations", "comments": "8 pages, 6 figures, 9 tables", "journal-ref": "Engineering with Computers (2019)", "doi": "10.1007/s00366-019-00783-4", "report-no": null, "categories": "cs.SY cs.CE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When simulators are energetically coupled in a co-simulation, residual\nenergies alter the total energy of the full coupled system. This distorts the\nsystem dynamics, lowers the quality of the results, and can lead to\ninstability. By using power bonds to realize simulator coupling, the\nEnergy-Conservation-based Co-Simulation method (ECCO) [Sadjina et al. 2016]\nexploits these concepts to define non-iterative global error estimation and\nadaptive step size control relying on coupling variable data alone. Following\nsimilar argumentation, the Nearly Energy Preserving Coupling Element (NEPCE)\n[Benedikt et al. 2013] uses corrections to the simulator inputs to\napproximately ensure energy conservation. Here, we discuss a modification to\nNEPCE for when direct feed-through is present in one of the coupled simulators.\nWe further demonstrate how accuracy and efficiency in non-iterative\nco-simulations are substantially enhanced when combining NEPCE with ECCO's\nadaptive step size controller. A quarter car model with linear and nonlinear\ndamping characteristics serves as a co-simulation benchmark, and we observe\nreductions of the coupling errors of up to 98% utilizing the concepts discussed\nhere.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 12:55:42 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Sadjina", "Severin", ""], ["Pedersen", "Eilif", ""]]}, {"id": "1606.05293", "submitter": "Marco Aldinucci", "authors": "Claudia Misale and Maurizio Drocco and Marco Aldinucci and Guy\n  Tremblay", "title": "A Comparison of Big Data Frameworks on a Layered Dataflow Model", "comments": "19 pages, 6 figures, 2 tables, In Proc. of the 9th Intl Symposium on\n  High-Level Parallel Programming and Applications (HLPP), July 4-5 2016,\n  Muenster, Germany", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the world of Big Data analytics, there is a series of tools aiming at\nsimplifying programming applications to be executed on clusters. Although each\ntool claims to provide better programming, data and execution models, for which\nonly informal (and often confusing) semantics is generally provided, all share\na common underlying model, namely, the Dataflow model. The Dataflow model we\npropose shows how various tools share the same expressiveness at different\nlevels of abstraction. The contribution of this work is twofold: first, we show\nthat the proposed model is (at least) as general as existing batch and\nstreaming frameworks (e.g., Spark, Flink, Storm), thus making it easier to\nunderstand high-level data-processing applications written in such frameworks.\nSecond, we provide a layered model that can represent tools and applications\nfollowing the Dataflow paradigm and we show how the analyzed tools fit in each\nlevel.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 17:52:17 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Misale", "Claudia", ""], ["Drocco", "Maurizio", ""], ["Aldinucci", "Marco", ""], ["Tremblay", "Guy", ""]]}, {"id": "1606.05385", "submitter": "Theo Steininger", "authors": "T. Steininger, M. Greiner, F. Beaujean, T. En{\\ss}lin", "title": "D2O - a distributed data object for parallel high-performance computing\n  in Python", "comments": null, "journal-ref": null, "doi": "10.1186/s40537-016-0052-5", "report-no": null, "categories": "cs.MS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce D2O, a Python module for cluster-distributed multi-dimensional\nnumerical arrays. It acts as a layer of abstraction between the algorithm code\nand the data-distribution logic. The main goal is to achieve usability without\nlosing numerical performance and scalability. D2O's global interface is similar\nto the one of a numpy.ndarray, whereas the cluster node's local data is\ndirectly accessible for use in customized high-performance modules. D2O is\nwritten in pure Python which makes it portable and easy to use and modify.\nExpensive operations are carried out by dedicated external libraries like numpy\nand mpi4py. The performance of D2O is on a par with numpy for serial\napplications and scales well when moving to an MPI cluster. D2O is open-source\nsoftware available under the GNU General Public License v3 (GPL-3) at\nhttps://gitlab.mpcdf.mpg.de/ift/D2O\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 23:19:58 GMT"}, {"version": "v2", "created": "Sat, 13 Aug 2016 08:28:59 GMT"}], "update_date": "2016-11-02", "authors_parsed": [["Steininger", "T.", ""], ["Greiner", "M.", ""], ["Beaujean", "F.", ""], ["En\u00dflin", "T.", ""]]}, {"id": "1606.05403", "submitter": "EPTCS", "authors": "Dominic Orchard (University of Cambridge), Nobuko Yoshida (Imperial\n  College London)", "title": "Proceedings of the Ninth workshop on Programming Language Approaches to\n  Concurrency- and Communication-cEntric Software", "comments": null, "journal-ref": "EPTCS 211, 2016", "doi": "10.4204/EPTCS.211", "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PLACES 2016 (full title: Programming Language Approaches to Concurrency- and\nCommunication-Centric Software) is the ninth edition of the PLACES workshop\nseries. After the first PLACES, which was affiliated to DisCoTec in 2008, the\nworkshop has been part of ETAPS every year since 2009 and is now an established\npart of the ETAPS satellite events. PLACES 2016 was held on 8th April in\nEindhoven, The Netherlands. The workshop series was started in order to promote\nthe application of novel programming language ideas to the increasingly\nimportant problem of developing software for systems in which concurrency and\ncommunication are intrinsic aspects. This includes software for both multi-core\nsystems and large-scale distributed and/or service-oriented systems. The scope\nof PLACES includes new programming language features, whole new programming\nlanguage designs, new type systems, new semantic approaches, new program\nanalysis techniques, and new implementation mechanisms. This volume consists of\nthe papers accepted for presentation at the workshop.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 01:54:40 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Orchard", "Dominic", "", "University of Cambridge"], ["Yoshida", "Nobuko", "", "Imperial\n  College London"]]}, {"id": "1606.05473", "submitter": "Rajarshi Ray", "authors": "Amit Gurung, Arup Deka, Ezio Bartocci, Sergiy Bogomolov, Radu Grosu\n  and Rajarshi Ray", "title": "Parallel Reachability Analysis for Hybrid Systems", "comments": "16 Pages, LNCS style", "journal-ref": null, "doi": "10.1109/MEMCOD.2016.7797741", "report-no": null, "categories": "cs.DC cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two parallel state-space exploration algorithms for hybrid systems\nwith the goal of enhancing performance on multi-core shared memory systems. The\nfirst is an adaption of the parallel breadth first search in the SPIN model\nchecker. We show that the adapted algorithm does not provide the desired load\nbalancing for many hybrid systems benchmarks. The second is a task parallel\nalgorithm based on cheaply precomputing cost of post (continuous and discrete)\noperations for effective load balancing. We illustrate the task parallel\nalgorithm and the cost precomputation of post operators on a\nsupport-function-based algorithm for state-space exploration. The performance\ncomparison of the two algorithms displays a better CPU\nutilization/load-balancing of the second over the first, except for certain\ncases. The algorithms are implemented in the model checker XSpeed and our\nexperiments show a maximum speed-up of $900\\times$ on a navigation benchmark\nwith respect to SpaceEx LGG scenario, comparing on the basis of equal number of\npost operations evaluated.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 10:54:45 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Gurung", "Amit", ""], ["Deka", "Arup", ""], ["Bartocci", "Ezio", ""], ["Bogomolov", "Sergiy", ""], ["Grosu", "Radu", ""], ["Ray", "Rajarshi", ""]]}, {"id": "1606.05633", "submitter": "Burak Y{\\i}ld{\\i}z", "authors": "Burak Y{\\i}ld{\\i}z, Tolga B\\\"uy\\\"uktan{\\i}r and Fatih Emekci", "title": "Equi-depth Histogram Construction for Big Data with Quality Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The amount of data generated and stored in cloud systems has been increasing\nexponentially. The examples of data include user generated data, machine\ngenerated data as well as data crawled from the Internet. There have been\nseveral frameworks with proven efficiency to store and process the petabyte\nscale data such as Apache Hadoop, HDFS and several NoSQL frameworks. These\nsystems have been widely used in industry and thus are subject to several\nresearch. The proposed data processing techniques should be compatible with the\nabove frameworks in order to be practical. One of the key data operations is\nderiving equi-depth histograms as they are crucial in understanding the\nstatistical properties of the underlying data with many applications including\nquery optimization. In this paper, we focus on approximate equi-depth histogram\nconstruction for big data and propose a novel merge based histogram\nconstruction method with a histogram processing framework which constructs an\nequi-depth histogram for a given time interval. The proposed method constructs\napproximate equi-depth histograms by merging exact equi-depth histograms of\npartitioned data by guaranteeing a maximum error bound on the number of items\nin a bucket (bucket size) as well as any range on the histogram.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 19:45:03 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Y\u0131ld\u0131z", "Burak", ""], ["B\u00fcy\u00fcktan\u0131r", "Tolga", ""], ["Emekci", "Fatih", ""]]}, {"id": "1606.05688", "submitter": "Aleksandar Zlateski", "authors": "Aleksandar Zlateski, Kisuk Lee and H. Sebastian Seung", "title": "ZNNi - Maximizing the Inference Throughput of 3D Convolutional Networks\n  on Multi-Core CPUs and GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sliding window convolutional networks (ConvNets) have become a popular\napproach to computer vision problems such as image segmentation, and object\ndetection and localization. Here we consider the problem of inference, the\napplication of a previously trained ConvNet, with emphasis on 3D images. Our\ngoal is to maximize throughput, defined as average number of output voxels\ncomputed per unit time. Other things being equal, processing a larger image\ntends to increase throughput, because fractionally less computation is wasted\non the borders of the image. It follows that an apparently slower algorithm may\nend up having higher throughput if it can process a larger image within the\nconstraint of the available RAM. We introduce novel CPU and GPU primitives for\nconvolutional and pooling layers, which are designed to minimize memory\noverhead. The primitives include convolution based on highly efficient pruned\nFFTs. Our theoretical analyses and empirical tests reveal a number of\ninteresting findings. For some ConvNet architectures, cuDNN is outperformed by\nour FFT-based GPU primitives, and these in turn can be outperformed by our CPU\nprimitives. The CPU manages to achieve higher throughput because of its fast\naccess to more RAM. A novel primitive in which the GPU accesses host RAM can\nsignificantly increase GPU throughput. Finally, a CPU-GPU algorithm achieves\nthe greatest throughput of all, 10x or more than other publicly available\nimplementations of sliding window 3D ConvNets. All of our code has been made\navailable as open source project.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 22:16:39 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Zlateski", "Aleksandar", ""], ["Lee", "Kisuk", ""], ["Seung", "H. Sebastian", ""]]}, {"id": "1606.05696", "submitter": "Yang Shi", "authors": "Yang Shi, U. N. Niranjan, Animashree Anandkumar, Cris Cecka", "title": "Tensor Contractions with Extended BLAS Kernels on CPU and GPU", "comments": null, "journal-ref": null, "doi": "10.1109/HiPC.2016.031", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor contractions constitute a key computational ingredient of numerical\nmulti-linear algebra. However, as the order and dimension of tensors grow, the\ntime and space complexities of tensor-based computations grow quickly. Existing\napproaches for tensor contractions typically involves explicit copy and\ntranspose operations. In this paper, we propose and evaluate a new BLAS-like\nprimitive STRIDEDBATCHEDGEMM that is capable of performing a wide range of\ntensor contractions on CPU and GPU efficiently. Through systematic\nbenchmarking, we demonstrate the advantages of our approach over conventional\napproaches. Concretely, we implement the Tucker decomposition and show that\nusing our kernels yields 100x speedup as compared to the implementation using\nexisting state-of-the-art libraries.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 22:39:19 GMT"}, {"version": "v2", "created": "Sun, 2 Oct 2016 20:14:04 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Shi", "Yang", ""], ["Niranjan", "U. N.", ""], ["Anandkumar", "Animashree", ""], ["Cecka", "Cris", ""]]}, {"id": "1606.05777", "submitter": "Amir Rastegarnia", "authors": "Azam Khalili, Reza G. Rahmati, Amir Rastegarnia, Wael M. Bazzi", "title": "A Distributed Algorithm for Training Augmented Complex Adaptive IIR\n  Filters", "comments": "Draft version, 11 Pages, 4 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of decentralized (distributed) adaptive\nlearning, where the aim of the network is to train the coefficients of a widely\nlinear autoregressive moving average (ARMA) model by measurements collected by\nthe nodes. Such a problem arises in many sensor network-based applications such\nas target tracking, fast rerouting, data reduction and data aggregation. We\nassume that each node of the network uses the augmented complex adaptive\ninfinite impulse response (ACAIIR) filter as the learning rule, and nodes\ninteract with each other under an incremental mode of cooperation. Since the\nproposed algorithm (incremental augmented complex IIR (IACA-IIR) algorithm)\nrelies on the augmented complex statistics, it can be used to model both types\nof complex-valued signals (proper and improper signals). To evaluate the\nperformance of the proposed algorithm, we use both synthetic and real-world\ncomplex signals in our simulations. The results exhibit superior performance of\nthe proposed algorithm over the non-cooperative ACAIIR algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jun 2016 16:37:13 GMT"}, {"version": "v2", "created": "Wed, 6 Jul 2016 13:52:34 GMT"}, {"version": "v3", "created": "Tue, 12 Jul 2016 11:34:55 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Khalili", "Azam", ""], ["Rahmati", "Reza G.", ""], ["Rastegarnia", "Amir", ""], ["Bazzi", "Wael M.", ""]]}, {"id": "1606.05790", "submitter": "Jeremy Kepner", "authors": "Jeremy Kepner, Peter Aaltonen, David Bader, Ayd{\\i}n Buluc, Franz\n  Franchetti, John Gilbert, Dylan Hutchison, Manoj Kumar, Andrew Lumsdaine,\n  Henning Meyerhenke, Scott McMillan, Jose Moreira, John D. Owens, Carl Yang,\n  Marcin Zalewski, Timothy Mattson", "title": "Mathematical Foundations of the GraphBLAS", "comments": "9 pages; 11 figures; accepted to IEEE High Performance Extreme\n  Computing (HPEC) conference 2016", "journal-ref": null, "doi": "10.1109/HPEC.2016.7761646", "report-no": null, "categories": "cs.MS astro-ph.IM cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The GraphBLAS standard (GraphBlas.org) is being developed to bring the\npotential of matrix based graph algorithms to the broadest possible audience.\nMathematically the Graph- BLAS defines a core set of matrix-based graph\noperations that can be used to implement a wide class of graph algorithms in a\nwide range of programming environments. This paper provides an introduction to\nthe mathematics of the GraphBLAS. Graphs represent connections between vertices\nwith edges. Matrices can represent a wide range of graphs using adjacency\nmatrices or incidence matrices. Adjacency matrices are often easier to analyze\nwhile incidence matrices are often better for representing data. Fortunately,\nthe two are easily connected by matrix mul- tiplication. A key feature of\nmatrix mathematics is that a very small number of matrix operations can be used\nto manipulate a very wide range of graphs. This composability of small number\nof operations is the foundation of the GraphBLAS. A standard such as the\nGraphBLAS can only be effective if it has low performance overhead. Performance\nmeasurements of prototype GraphBLAS implementations indicate that the overhead\nis low.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jun 2016 18:46:20 GMT"}, {"version": "v2", "created": "Thu, 14 Jul 2016 02:52:48 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Kepner", "Jeremy", ""], ["Aaltonen", "Peter", ""], ["Bader", "David", ""], ["Buluc", "Ayd\u0131n", ""], ["Franchetti", "Franz", ""], ["Gilbert", "John", ""], ["Hutchison", "Dylan", ""], ["Kumar", "Manoj", ""], ["Lumsdaine", "Andrew", ""], ["Meyerhenke", "Henning", ""], ["McMillan", "Scott", ""], ["Moreira", "Jose", ""], ["Owens", "John D.", ""], ["Yang", "Carl", ""], ["Zalewski", "Marcin", ""], ["Mattson", "Timothy", ""]]}, {"id": "1606.05794", "submitter": "Jeremy Kepner", "authors": "Mike Jones, Bill Arcand, Bill Bergeron, David Bestor, Chansup Byun,\n  Lauren Milechin, Vijay Gadepally, Matt Hubbell, Jeremy Kepner, Pete\n  Michaleas, Julie Mullen, Andy Prout, Tony Rosa, Siddharth Samsi, Charles Yee,\n  Albert Reuther", "title": "Scalability of VM Provisioning Systems", "comments": "5 pages; 6 figures; accepted to the IEEE High Performance Extreme\n  Computing (HPEC) conference 2016", "journal-ref": null, "doi": "10.1109/HPEC.2016.7761629", "report-no": null, "categories": "cs.DC cs.CY cs.OS cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual machines and virtualized hardware have been around for over half a\ncentury. The commoditization of the x86 platform and its rapidly growing\nhardware capabilities have led to recent exponential growth in the use of\nvirtualization both in the enterprise and high performance computing (HPC). The\nstartup time of a virtualized environment is a key performance metric for high\nperformance computing in which the runtime of any individual task is typically\nmuch shorter than the lifetime of a virtualized service in an enterprise\ncontext. In this paper, a methodology for accurately measuring the startup\nperformance on an HPC system is described. The startup performance overhead of\nthree of the most mature, widely deployed cloud management frameworks\n(OpenStack, OpenNebula, and Eucalyptus) is measured to determine their\nsuitability for workloads typically seen in an HPC environment. A 10x\nperformance difference is observed between the fastest (Eucalyptus) and the\nslowest (OpenNebula) framework. This time difference is primarily due to delays\nin waiting on networking in the cloud-init portion of the startup. The\nmethodology and measurements presented should facilitate the optimization of\nstartup across a variety of virtualization environments.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jun 2016 18:55:42 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Jones", "Mike", ""], ["Arcand", "Bill", ""], ["Bergeron", "Bill", ""], ["Bestor", "David", ""], ["Byun", "Chansup", ""], ["Milechin", "Lauren", ""], ["Gadepally", "Vijay", ""], ["Hubbell", "Matt", ""], ["Kepner", "Jeremy", ""], ["Michaleas", "Pete", ""], ["Mullen", "Julie", ""], ["Prout", "Andy", ""], ["Rosa", "Tony", ""], ["Samsi", "Siddharth", ""], ["Yee", "Charles", ""], ["Reuther", "Albert", ""]]}, {"id": "1606.05797", "submitter": "Jeremy Kepner", "authors": "Jeremy Kepner, Vijay Gadepally, Dylan Hutchison, Hayden Jananthan,\n  Timothy Mattson, Siddharth Samsi, Albert Reuther", "title": "Associative Array Model of SQL, NoSQL, and NewSQL Databases", "comments": "9 pages; 6 figures; accepted to IEEE High Performance Extreme\n  Computing (HPEC) conference 2016", "journal-ref": null, "doi": "10.1109/HPEC.2016.7761647", "report-no": null, "categories": "cs.DB cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of SQL, NoSQL, and NewSQL databases is a reflection of their\nability to provide significant functionality and performance benefits for\nspecific domains, such as financial transactions, internet search, and data\nanalysis. The BigDAWG polystore seeks to provide a mechanism to allow\napplications to transparently achieve the benefits of diverse databases while\ninsulating applications from the details of these databases. Associative arrays\nprovide a common approach to the mathematics found in different databases: sets\n(SQL), graphs (NoSQL), and matrices (NewSQL). This work presents the SQL\nrelational model in terms of associative arrays and identifies the key\nmathematical properties that are preserved within SQL. These properties include\nassociativity, commutativity, distributivity, identities, annihilators, and\ninverses. Performance measurements on distributivity and associativity show the\nimpact these properties can have on associative array operations. These results\ndemonstrate that associative arrays could provide a mathematical model for\npolystores to optimize the exchange of data and execution queries.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jun 2016 19:29:17 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Kepner", "Jeremy", ""], ["Gadepally", "Vijay", ""], ["Hutchison", "Dylan", ""], ["Jananthan", "Hayden", ""], ["Mattson", "Timothy", ""], ["Samsi", "Siddharth", ""], ["Reuther", "Albert", ""]]}, {"id": "1606.05836", "submitter": "Junjie Wu", "authors": "Junjie Wu and Yong Liu and Baida Zhang and Xianmin Jin and Yang Wang\n  and Huiquan Wang and Xuejun Yang", "title": "A Benchmark Test of Boson Sampling on Tianhe-2 Supercomputer", "comments": "7 pages, 5 figures", "journal-ref": "National Science Review, 5 (2018) 715-720", "doi": "10.1093/nsr/nwy079", "report-no": null, "categories": "quant-ph cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boson sampling, thought to be intractable classically, can be solved by a\nquantum machine composed of merely generation, linear evolution and detection\nof single photons. Such an analog quantum computer for this specific problem\nprovides a shortcut to boost the absolute computing power of quantum computers\nto beat classical ones. However, the capacity bound of classical computers for\nsimulating boson sampling has not yet been identified. Here we simulate boson\nsampling on the Tianhe-2 supercomputer which occupied the first place in the\nworld ranking six times from 2013 to 2016. We computed the permanent of the\nlargest matrix using up to 312,000 CPU cores of Tianhe-2, and inferred from the\ncurrent most efficient permanent-computing algorithms that an upper bound on\nthe performance of Tianhe-2 is one 50-photon sample per ~100 min. In addition,\nwe found a precision issue with one of two permanent-computing algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jun 2016 07:01:18 GMT"}, {"version": "v2", "created": "Tue, 21 Aug 2018 01:28:13 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Wu", "Junjie", ""], ["Liu", "Yong", ""], ["Zhang", "Baida", ""], ["Jin", "Xianmin", ""], ["Wang", "Yang", ""], ["Wang", "Huiquan", ""], ["Yang", "Xuejun", ""]]}, {"id": "1606.05917", "submitter": "Jason Teutsch", "authors": "Sanjay Jain, Prateek Saxena, Frank Stephan, and Jason Teutsch", "title": "How to verify computation with a rational network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present paper introduces a practical protocol for provably secure,\noutsourced computation. Our protocol minimizes overhead for verification by\nrequiring solutions to withstand an interactive game between a prover and\nchallenger. For optimization problems, the best or nearly best of all submitted\nsolutions is expected to be accepted by this approach. Financial incentives and\ndeposits are used in order to overcome the problem of fake participants.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jun 2016 22:11:28 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Jain", "Sanjay", ""], ["Saxena", "Prateek", ""], ["Stephan", "Frank", ""], ["Teutsch", "Jason", ""]]}, {"id": "1606.05936", "submitter": "EPTCS", "authors": "Ilaria Castellani (INRIA Sophia Antipolis, France), Mariangiola\n  Dezani-Ciancaglini (Universit\\`a di Torino, Italia), Ugo de'Liguoro\n  (Universit\\`a di Torino, Italia)", "title": "Secure Multiparty Sessions with Topics", "comments": "In Proceedings PLACES 2016, arXiv:1606.05403", "journal-ref": "EPTCS 211, 2016, pp. 1-12", "doi": "10.4204/EPTCS.211.1", "report-no": null, "categories": "cs.LO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiparty session calculi have been recently equipped with security\nrequirements, in order to guarantee properties such as access control and leak\nfreedom. However, the proposed security requirements seem to be overly\nrestrictive in some cases. In particular, a party is not allowed to communicate\nany kind of public information after receiving a secret information. This does\nnot seem justified in case the two pieces of information are totally unrelated.\nThe aim of the present paper is to overcome this restriction, by designing a\ntype discipline for a simple multiparty session calculus, which classifies\nmessages according to their topics and allows unrestricted sequencing of\nmessages on independent topics.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 01:08:38 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Castellani", "Ilaria", "", "INRIA Sophia Antipolis, France"], ["Dezani-Ciancaglini", "Mariangiola", "", "Universit\u00e0 di Torino, Italia"], ["de'Liguoro", "Ugo", "", "Universit\u00e0 di Torino, Italia"]]}, {"id": "1606.05937", "submitter": "EPTCS", "authors": "Tiago Cogumbreiro (Rice University), Jun Shirako (Rice University),\n  Vivek Sarkar (Rice University)", "title": "Formalization of Phase Ordering", "comments": "In Proceedings PLACES 2016, arXiv:1606.05403", "journal-ref": "EPTCS 211, 2016, pp. 13-24", "doi": "10.4204/EPTCS.211.2", "report-no": null, "categories": "cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phasers pose an interesting synchronization mechanism that generalizes many\ncollective synchronization patterns seen in parallel programming languages,\nincluding barriers, clocks, and point-to-point synchronization using latches or\nsemaphores. This work characterizes scheduling constraints on phaser\noperations, by relating the execution state of two tasks that operate on the\nsame phaser. We propose a formalization of Habanero phasers,\nMay-Happen-In-Parallel, and Happens-Before relations for phaser operations, and\nshow that these relations conform with the semantics. Our formalization and\nproofs are fully mechanized using the Coq proof assistant, and are available\nonline.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 01:08:47 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Cogumbreiro", "Tiago", "", "Rice University"], ["Shirako", "Jun", "", "Rice University"], ["Sarkar", "Vivek", "", "Rice University"]]}, {"id": "1606.05938", "submitter": "EPTCS", "authors": "Mario Coppo (Universit\\`a di Torino, Italia), Mariangiola\n  Dezani-Ciancaglini (Universit\\`a di Torino, Italia), Betti Venneri\n  (Universit\\`a di Firenze, Italia)", "title": "Parallel Monitors for Self-adaptive Sessions", "comments": "In Proceedings PLACES 2016, arXiv:1606.05403", "journal-ref": "EPTCS 211, 2016, pp. 25-36", "doi": "10.4204/EPTCS.211.3", "report-no": null, "categories": "cs.LO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a data-driven model of self-adaptivity for multiparty\nsessions. System choreography is prescribed by a global type. Participants are\nincarnated by processes associated with monitors, which control their\nbehaviour. Each participant can access and modify a set of global data, which\nare able to trigger adaptations in the presence of critical changes of values.\n  The use of the parallel composition for building global types, monitors and\nprocesses enables a significant degree of flexibility: an adaptation step can\ndynamically reconfigure a set of participants only, without altering the\nremaining participants, even if the two groups communicate.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 01:08:57 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Coppo", "Mario", "", "Universit\u00e0 di Torino, Italia"], ["Dezani-Ciancaglini", "Mariangiola", "", "Universit\u00e0 di Torino, Italia"], ["Venneri", "Betti", "", "Universit\u00e0 di Firenze, Italia"]]}, {"id": "1606.05943", "submitter": "EPTCS", "authors": "Roly Perera (University of Glasgow), Julien Lange (Imperial College\n  London), Simon J. Gay (University of Glasgow)", "title": "Multiparty Compatibility for Concurrent Objects", "comments": "In Proceedings PLACES 2016, arXiv:1606.05403", "journal-ref": "EPTCS 211, 2016, pp. 73-82", "doi": "10.4204/EPTCS.211.8", "report-no": null, "categories": "cs.PL cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objects and actors are communicating state machines, offering and consuming\ndifferent services at different points in their lifecycle. Two complementary\nchallenges arise when programming such systems. When objects interact, their\nstate machines must be \"compatible\", so that services are requested only when\nthey are available. Dually, when objects refine other objects, their state\nmachines must be \"compliant\", so that services are honoured whenever they are\npromised.\n  In this paper we show how the idea of multiparty compatibility from the\nsession types literature can be applied to both of these problems. We present\nan untyped language in which concurrent objects are checked automatically for\ncompatibility and compliance. For simple objects, checking can be exhaustive\nand has the feel of a type system. More complex objects can be partially\nvalidated via test cases, leading to a methodology closer to continuous\ntesting. Our proof-of-concept implementation is limited in some important\nrespects, but demonstrates the potential value of the approach and the\nrelationship to existing software development practices.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 01:09:44 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Perera", "Roly", "", "University of Glasgow"], ["Lange", "Julien", "", "Imperial College\n  London"], ["Gay", "Simon J.", "", "University of Glasgow"]]}, {"id": "1606.05944", "submitter": "EPTCS", "authors": "Sanjiva Prasad (Indian Institute of Technology Delhi, New Delhi,\n  India)", "title": "Program Execution on Reconfigurable Multicore Architectures", "comments": "In Proceedings PLACES 2016, arXiv:1606.05403", "journal-ref": "EPTCS 211, 2016, pp. 83-91", "doi": "10.4204/EPTCS.211.9", "report-no": null, "categories": "cs.PL cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on the two observations that diverse applications perform better on\ndifferent multicore architectures, and that different phases of an application\nmay have vastly different resource requirements, Pal et al. proposed a novel\nreconfigurable hardware approach for executing multithreaded programs. Instead\nof mapping a concurrent program to a fixed architecture, the architecture\nadaptively reconfigures itself to meet the application's concurrency and\ncommunication requirements, yielding significant improvements in performance.\nBased on our earlier abstract operational framework for multicore execution\nwith hierarchical memory structures, we describe execution of multithreaded\nprograms on reconfigurable architectures that support a variety of clustered\nconfigurations. Such reconfiguration may not preserve the semantics of programs\ndue to the possible introduction of race conditions arising from concurrent\naccesses to shared memory by threads running on the different cores. We present\nan intuitive partial ordering notion on the cluster configurations, and show\nthat the semantics of multithreaded programs is always preserved for\nreconfigurations \"upward\" in that ordering, whereas semantics preservation for\narbitrary reconfigurations can be guaranteed for well-synchronised programs. We\nfurther show that a simple approximate notion of efficiency of execution on the\ndifferent configurations can be obtained using the notion of amortised\nbisimulations, and extend it to dynamic reconfiguration.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 01:09:54 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Prasad", "Sanjiva", "", "Indian Institute of Technology Delhi, New Delhi,\n  India"]]}, {"id": "1606.05962", "submitter": "Sandeep Kulkarni Sandeep Kulkarni", "authors": "Nitin H. Vaidya, Sandeep S. Kulkarni", "title": "Efficient Timestamps for Capturing Causality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider an asynchronous system consisting of processes that communicate via\nmessage-passing. The processes communicate over a potentially {\\em incomplete}\ncommunication network consisting of reliable bidirectional communication\nchannels. Thus, not every pair of processes is necessarily able to communicate\nwith each other directly. % For instance, when the communication network is a\n{\\em star} graph, there is a {\\em central} process % that can communicate with\nall the remaining processes (which are called {\\em radial} processes), % but\nthe radial processes cannot communicate with each other directly.\n  The goal of the algorithms discussed in this paper is to assign timestamps to\nthe events at all the processes such that (a) distinct events are assigned\ndistinct timestamps, and (b) the happened-before relationship between the\nevents can be inferred from the timestamps. We consider three types of\nalgorithms for assigning timestamps to events: (i) Online algorithms that must\n(greedily) assign a timestamp to each event when the event occurs. (ii) Offline\nalgorithms that assign timestamps to event after a finite execution is\ncomplete. (iii) Inline algorithms that assign a timestamp to each event when it\noccurs, but may modify some elements of a timestamp again at a later time.\n  For specific classes of graphs, particularly {\\em star} graphs and graphs\nwith connectivity $\\geq 1$, the paper presents bounds on the length of vector\ntimestamps assigned by an {\\em online} algorithm. The paper then presents an\n{\\em inline} algorithm, which typically assigns substantially smaller\ntimestamps than the optimal-length {\\em online} vector timestamps. In\nparticular, the inline algorithm assigns timestamp in the form of a tuple\ncontaining $2c+2$ integer elements, where $c$ is the size of the vertex cover\nfor the underlying communication graph.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 02:53:01 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Vaidya", "Nitin H.", ""], ["Kulkarni", "Sandeep S.", ""]]}, {"id": "1606.05963", "submitter": "Yong Xiang", "authors": "Yong Xiang, Hu Li, Sen Wang, Wei Xu", "title": "Debugging OpenStack Problems Using a State Graph Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is hard to operate and debug systems like OpenStack that integrate many\nindependently developed modules with multiple levels of abstractions. A major\nchallenge is to navigate through the complex dependencies and relationships of\nthe states in different modules or subsystems, to ensure the correctness and\nconsistency of these states. We present a system that captures the runtime\nstates and events from the entire OpenStack-Ceph stack, and automatically\norganizes these data into a graph that we call system operation state graph\n(SOSG).With SOSG we can use intuitive graph traversal techniques to solve\nproblems like reasoning about the state of a virtual machine. Also, using\ngraph-based anomaly detection, we can automatically discover hidden problems in\nOpenStack. We have a scalable implementation of SOSG, and evaluate the approach\non a 125-node production OpenStack cluster, finding a number of interesting\nproblems.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 02:58:24 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Xiang", "Yong", ""], ["Li", "Hu", ""], ["Wang", "Sen", ""], ["Xu", "Wei", ""]]}, {"id": "1606.06025", "submitter": "Xuhao Chen", "authors": "Xuhao Chen, Pingfan Li, Jianbin Fang, Tao Tang, Zhiying Wang, Canqun\n  Yang", "title": "Efficient and High-quality Sparse Graph Coloring on the GPU", "comments": "arXiv admin note: text overlap with arXiv:1205.3809 by other authors", "journal-ref": "Concurrency and Computation: Practice and Experience, Volume 29,\n  Issue 10, 2017", "doi": "10.1002/cpe.4064", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph coloring has been broadly used to discover concurrency in parallel\ncomputing. To speedup graph coloring for large-scale datasets, parallel\nalgorithms have been proposed to leverage modern GPUs. Existing GPU\nimplementations either have limited performance or yield unsatisfactory\ncoloring quality (too many colors assigned). We present a work-efficient\nparallel graph coloring implementation on GPUs with good coloring quality. Our\napproach employs the speculative greedy scheme which inherently yields better\nquality than the method of finding maximal independent set. In order to achieve\nhigh performance on GPUs, we refine the algorithm to leverage efficient\noperators and alleviate conflicts. We also incorporate common optimization\ntechniques to further improve performance. Our method is evaluated with both\nsynthetic and real-world sparse graphs on the NVIDIA GPU. Experimental results\nshow that our proposed implementation achieves averaged 4.1x (up to 8.9x)\nspeedup over the serial implementation. It also outperforms the existing GPU\nimplementation from the NVIDIA CUSPARSE library (2.2x average speedup), while\nyielding much better coloring quality than CUSPARSE.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 09:23:25 GMT"}, {"version": "v2", "created": "Fri, 17 Jan 2020 22:27:48 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Chen", "Xuhao", ""], ["Li", "Pingfan", ""], ["Fang", "Jianbin", ""], ["Tang", "Tao", ""], ["Wang", "Zhiying", ""], ["Yang", "Canqun", ""]]}, {"id": "1606.06133", "submitter": "Nico Reissmann", "authors": "Nico Reissmann, Jan Christian Meyer, Magnus Jahre", "title": "A Study of Energy and Locality Effects using Space-filling Curves", "comments": "Proceedings of the 2014 IEEE International Parallel & Distributed\n  Processing Symposium Workshops (IPDPSW)", "journal-ref": null, "doi": "10.1109/IPDPSW.2014.93", "report-no": null, "categories": "cs.DC cs.GL cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cost of energy is becoming an increasingly important driver for the\noperating cost of HPC systems, adding yet another facet to the challenge of\nproducing efficient code. In this paper, we investigate the energy implications\nof trading computation for locality using Hilbert and Morton space-filling\ncurves with dense matrix-matrix multiplication. The advantage of these curves\nis that they exhibit an inherent tiling effect without requiring specific\narchitecture tuning. By accessing the matrices in the order determined by the\nspace-filling curves, we can trade computation for locality. The index\ncomputation overhead of the Morton curve is found to be balanced against its\nlocality and energy efficiency, while the overhead of the Hilbert curve\noutweighs its improvements on our test system.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 14:18:25 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Reissmann", "Nico", ""], ["Meyer", "Jan Christian", ""], ["Jahre", "Magnus", ""]]}, {"id": "1606.06183", "submitter": "Hamidreza Jahanjou", "authors": "Hamidreza Jahanjou, Erez Kantor, Rajmohan Rajaraman", "title": "Asymptotically Optimal Approximation Algorithms for Coflow Scheduling", "comments": "Fixed minor typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern datacenter applications involve large-scale computations composed\nof multiple data flows that need to be completed over a shared set of\ndistributed resources. Such a computation completes when all of its flows\ncomplete. A useful abstraction for modeling such scenarios is a {\\em coflow},\nwhich is a collection of flows (e.g., tasks, packets, data transmissions) that\nall share the same performance goal.\n  In this paper, we present the first approximation algorithms for scheduling\ncoflows over general network topologies with the objective of minimizing total\nweighted completion time. We consider two different models for coflows based on\nthe nature of individual flows: circuits, and packets. We design\nconstant-factor polynomial-time approximation algorithms for scheduling\npacket-based coflows with or without given flow paths, and circuit-based\ncoflows with given flow paths. Furthermore, we give an $O(\\log n/\\log \\log\nn)$-approximation polynomial time algorithm for scheduling circuit-based\ncoflows where flow paths are not given (here $n$ is the number of network\nedges).\n  We obtain our results by developing a general framework for coflow schedules,\nbased on interval-indexed linear programs, which may extend to other coflow\nmodels and objective functions and may also yield improved approximation bounds\nfor specific network scenarios. We also present an experimental evaluation of\nour approach for circuit-based coflows that show a performance improvement of\nat least 22% on average over competing heuristics.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 15:51:31 GMT"}, {"version": "v2", "created": "Wed, 22 Jun 2016 16:36:38 GMT"}, {"version": "v3", "created": "Tue, 23 May 2017 23:06:01 GMT"}, {"version": "v4", "created": "Tue, 1 Aug 2017 02:51:24 GMT"}, {"version": "v5", "created": "Fri, 9 Mar 2018 01:17:29 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Jahanjou", "Hamidreza", ""], ["Kantor", "Erez", ""], ["Rajaraman", "Rajmohan", ""]]}, {"id": "1606.06204", "submitter": "Richard Barnes", "authors": "Richard Barnes", "title": "Parallel Priority-Flood Depression Filling For Trillion Cell Digital\n  Elevation Models On Desktops Or Clusters", "comments": "21 pages, 4 tables, 8 figures", "journal-ref": "Computers and Geosciences, Volume 96, November 2016, pp. 56-68", "doi": "10.1016/j.cageo.2016.07.001", "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms for extracting hydrologic features and properties from digital\nelevation models (DEMs) are challenged by large datasets, which often cannot\nfit within a computer's RAM. Depression filling is an important preconditioning\nstep to many of these algorithms. Here, I present a new, linearly-scaling\nalgorithm which parallelizes the Priority-Flood depression-filling algorithm by\nsubdividing a DEM into tiles. Using a single-producer, multi-consumer design,\nthe new algorithm works equally well on one core, multiple cores, or multiple\nmachines and can take advantage of large memories or cope with small ones.\nUnlike previous algorithms, the new algorithm guarantees a fixed number of\nmemory access and communication events per subdivision of the DEM. In\ncomparison testing, this results in the new algorithm running generally faster\nwhile using fewer resources than previous algorithms. For moderately sized\ntiles, the algorithm exhibits ~60% strong and weak scaling efficiencies up to\n48 cores, and linear time scaling across datasets ranging over three orders of\nmagnitude. The largest dataset on which I run the algorithm has 2 trillion\n(2*10^12) cells. With 48 cores, processing required 4.8 hours wall-time (9.3\ncompute-days). This test is three orders of magnitude larger than any\npreviously performed in the literature. Complete, well-commented source code\nand correctness tests are available for download from a repository.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 16:52:12 GMT"}, {"version": "v2", "created": "Mon, 15 Aug 2016 22:35:43 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Barnes", "Richard", ""]]}, {"id": "1606.06234", "submitter": "Chao Wang", "authors": "Maohua Zhu, Liu Liu, Chao Wang, Yuan Xie", "title": "CNNLab: a Novel Parallel Framework for Neural Networks using GPU and\n  FPGA-a Practical Study with Trade-off Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing and implementing efficient, provably correct parallel neural\nnetwork processing is challenging. Existing high-level parallel abstractions\nlike MapReduce are insufficiently expressive while low-level tools like MPI and\nPthreads leave ML experts repeatedly solving the same design challenges.\nHowever, the diversity and large-scale data size have posed a significant\nchallenge to construct a flexible and high-performance implementation of deep\nlearning neural networks. To improve the performance and maintain the\nscalability, we present CNNLab, a novel deep learning framework using GPU and\nFPGA-based accelerators. CNNLab provides a uniform programming model to users\nso that the hardware implementation and the scheduling are invisible to the\nprogrammers. At runtime, CNNLab leverages the trade-offs between GPU and FPGA\nbefore offloading the tasks to the accelerators. Experimental results on the\nstate-of-the-art Nvidia K40 GPU and Altera DE5 FPGA board demonstrate that the\nCNNLab can provide a universal framework with efficient support for diverse\napplications without increasing the burden of the programmers. Moreover, we\nanalyze the detailed quantitative performance, throughput, power, energy, and\nperformance density for both approaches. Experimental results leverage the\ntrade-offs between GPU and FPGA and provide useful practical experiences for\nthe deep learning research community.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 18:22:09 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Zhu", "Maohua", ""], ["Liu", "Liu", ""], ["Wang", "Chao", ""], ["Xie", "Yuan", ""]]}, {"id": "1606.06530", "submitter": "Ralph Holz", "authors": "Luke Anderson, Ralph Holz, Alexander Ponomarev, Paul Rimba, Ingo Weber", "title": "New kids on the block: an analysis of modern blockchains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Half a decade after Bitcoin became the first widely used cryptocurrency,\nblockchains are receiving considerable interest from industry and the research\ncommunity. Modern blockchains feature services such as name registration and\nsmart contracts. Some employ new forms of consensus, such as proof-of-stake\ninstead of proof-of-work. However, these blockchains are so far relatively\npoorly investigated, despite the fact that they move considerable assets. In\nthis paper, we explore three representative, modern blockchains---Ethereum,\nNamecoin, and Peercoin. Our focus is on the features that set them apart from\nthe pure currency use case of Bitcoin. We investigate the blockchains' activity\nin terms of transactions and usage patterns, identifying some curiosities in\nthe process. For Ethereum, we are mostly interested in the smart contract\nfunctionality it offers. We also carry out a brief analysis of issues that are\nintroduced by negligent design of smart contracts. In the case of Namecoin, our\nfocus is how the name registration is used and has developed over time. For\nPeercoin, we are interested in the use of proof-of-stake, as this consensus\nalgorithm is poorly understood yet used to move considerable value. Finally, we\nrelate the above to the fundamental characteristics of the underlying\npeer-to-peer networks. We present a crawler for Ethereum and give statistics on\nthe network size. For Peercoin and Namecoin, we identify the relatively small\nsize of the networks and the weak bootstrapping process.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 12:11:44 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Anderson", "Luke", ""], ["Holz", "Ralph", ""], ["Ponomarev", "Alexander", ""], ["Rimba", "Paul", ""], ["Weber", "Ingo", ""]]}, {"id": "1606.06543", "submitter": "Pooyan Jamshidi", "authors": "Pooyan Jamshidi, Giuliano Casale", "title": "An Uncertainty-Aware Approach to Optimal Configuration of Stream\n  Processing Systems", "comments": "MASCOTS 2016, code is available at\n  https://github.com/dice-project/DICE-Configuration-BO4CO", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding optimal configurations for Stream Processing Systems (SPS) is a\nchallenging problem due to the large number of parameters that can influence\ntheir performance and the lack of analytical models to anticipate the effect of\na change. To tackle this issue, we consider tuning methods where an\nexperimenter is given a limited budget of experiments and needs to carefully\nallocate this budget to find optimal configurations. We propose in this setting\nBayesian Optimization for Configuration Optimization (BO4CO), an auto-tuning\nalgorithm that leverages Gaussian Processes (GPs) to iteratively capture\nposterior distributions of the configuration spaces and sequentially drive the\nexperimentation. Validation based on Apache Storm demonstrates that our\napproach locates optimal configurations within a limited experimental budget,\nwith an improvement of SPS performance typically of at least an order of\nmagnitude compared to existing configuration algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 12:39:46 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Jamshidi", "Pooyan", ""], ["Casale", "Giuliano", ""]]}, {"id": "1606.06570", "submitter": "Stephan Friedrichs", "authors": "Stephan Friedrichs and Matthias F\\\"ugger and Christoph Lenzen", "title": "Metastability-Containing Circuits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In digital circuits, metastability can cause deteriorated signals that\nneither are logical 0 or logical 1, breaking the abstraction of Boolean logic.\nUnfortunately, any way of reading a signal from an unsynchronized clock domain\nor performing an analog-to-digital conversion incurs the risk of a metastable\nupset; no digital circuit can deterministically avoid, resolve, or detect\nmetastability (Marino, 1981). Synchronizers, the only traditional\ncountermeasure, exponentially decrease the odds of maintained metastability\nover time. Trading synchronization delay for an increased probability to\nresolve metastability to logical 0 or 1, they do not guarantee success.\n  We propose a fundamentally different approach: It is possible to contain\nmetastability by fine-grained logical masking so that it cannot infect the\nentire circuit. This technique guarantees a limited degree of metastability\nin---and uncertainty about---the output.\n  At the heart of our approach lies a time- and value-discrete model for\nmetastability in synchronous clocked digital circuits. Metastability is\npropagated in a worst-case fashion, allowing to derive deterministic\nguarantees, without and unlike synchronizers. The proposed model permits\npositive results and passes the test of reproducing Marino's impossibility\nresults. We fully classify which functions can be computed by circuits with\nstandard registers. Regarding masking registers, we show that they become\ncomputationally strictly more powerful with each clock cycle, resulting in a\nnon-trivial hierarchy of computable functions.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 13:46:15 GMT"}, {"version": "v2", "created": "Sun, 26 Jun 2016 05:16:44 GMT"}, {"version": "v3", "created": "Wed, 14 Sep 2016 15:32:15 GMT"}, {"version": "v4", "created": "Tue, 8 Nov 2016 12:32:23 GMT"}, {"version": "v5", "created": "Thu, 27 Apr 2017 17:42:34 GMT"}, {"version": "v6", "created": "Mon, 29 May 2017 06:00:37 GMT"}, {"version": "v7", "created": "Tue, 30 May 2017 14:20:19 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Friedrichs", "Stephan", ""], ["F\u00fcgger", "Matthias", ""], ["Lenzen", "Christoph", ""]]}, {"id": "1606.06593", "submitter": "Haitham Bou Ammar PhD", "authors": "Rasul Tutunov, Haitham Bou Ammar, Ali Jadbabaie", "title": "A Distributed Newton Method for Large Scale Consensus Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a distributed Newton method for consensus\noptimization. Our approach outperforms state-of-the-art methods, including\nADMM. The key idea is to exploit the sparsity of the dual Hessian and recast\nthe computation of the Newton step as one of efficiently solving symmetric\ndiagonally dominant linear equations. We validate our algorithm both\ntheoretically and empirically. On the theory side, we demonstrate that our\nalgorithm exhibits superlinear convergence within a neighborhood of optimality.\nEmpirically, we show the superiority of this new method on a variety of machine\nlearning problems. The proposed approach is scalable to very large problems and\nhas a low communication overhead.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 14:26:01 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Tutunov", "Rasul", ""], ["Ammar", "Haitham Bou", ""], ["Jadbabaie", "Ali", ""]]}, {"id": "1606.06629", "submitter": "Camille Coti", "authors": "Olivier Bodini and Camille Coti and Julien David", "title": "Parallel Galton Watson Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a parallel version of Galton-Watson processes for the\nrandom generation of tree-shaped structures. Random trees are useful in many\nsituations (testing, binary search, simulation of physics phenomena,...) as\nattests more than 49000 citations on Google scholar. Using standard analytic\ncombinatorics, we first give a theoretical, average-case study of the random\nprocess in order to evaluate how parallelism can be extracted from this\nprocess, and we deduce a parallel generation algorithm. Then we present how it\ncan be implemented in a task-based parallel paradigm for shared memory (here,\nIntel Cilk). This implementation faces several challenges, among which\nefficient, thread-safe random bit generation, memory management and algorithmic\nmodifications for small-grain parallelism. Finally, we evaluate the performance\nof our implementation and the impact of different choices and parameters. We\nobtain a significant efficiency improvement for the generation of big trees. We\nalso conduct empirical and theoretical studies of the average behaviour of our\nalgorithm.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 15:54:59 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Bodini", "Olivier", ""], ["Coti", "Camille", ""], ["David", "Julien", ""]]}, {"id": "1606.06961", "submitter": "Pasquale Salza", "authors": "Pasquale Salza and Filomena Ferrucci", "title": "An Approach for Parallel Genetic Algorithms in the Cloud using Software\n  Containers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genetic Algorithms (GAs) are a powerful technique to address hard\noptimisation problems. However, scalability issues might prevent them from\nbeing applied to real-world problems. Exploiting parallel GAs in the cloud\nmight be an affordable approach to get time efficient solutions that benefit of\nthe appealing features of the cloud, such as scalability, reliability,\nfault-tolerance and cost-effectiveness. Nevertheless, distributed computation\nis very prone to cause considerable overhead for communication and making GAs\ndistributed in an on-demand fashion is not trivial. Aiming to keep under\ncontrol the communication overhead and support GAs developers in the\nconstruction and deployment of parallel GAs in the cloud, in this paper we\npropose an approach to distribute GAs using the global parallelisation model,\nexploiting software containers and their cloud orchestration. We also devised a\nconceptual workflow covering each cloud GAs distribution phase, from resources\nallocation to actual deployment and execution, in a DevOps fashion.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 14:32:34 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Salza", "Pasquale", ""], ["Ferrucci", "Filomena", ""]]}, {"id": "1606.07085", "submitter": "Dylan Hutchison", "authors": "Dylan Hutchison, Jeremy Kepner, Vijay Gadepally, Bill Howe", "title": "From NoSQL Accumulo to NewSQL Graphulo: Design and Utility of Graph\n  Algorithms inside a BigTable Database", "comments": "9 pages, to appear in 2016 IEEE High Performance Extreme Computing\n  Conference (HPEC)", "journal-ref": null, "doi": "10.1109/HPEC.2016.7761577", "report-no": null, "categories": "cs.DB cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Google BigTable's scale-out design for distributed key-value storage inspired\na generation of NoSQL databases. Recently the NewSQL paradigm emerged in\nresponse to analytic workloads that demand distributed computation local to\ndata storage. Many such analytics take the form of graph algorithms, a trend\nthat motivated the GraphBLAS initiative to standardize a set of matrix math\nkernels for building graph algorithms. In this article we show how it is\npossible to implement the GraphBLAS kernels in a BigTable database by\npresenting the design of Graphulo, a library for executing graph algorithms\ninside the Apache Accumulo database. We detail the Graphulo implementation of\ntwo graph algorithms and conduct experiments comparing their performance to two\nmain-memory matrix math systems. Our results shed insight into the conditions\nthat determine when executing a graph algorithm is faster inside a database\nversus an external system---in short, that memory requirements and relative I/O\nare critical factors.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 20:08:47 GMT"}, {"version": "v2", "created": "Thu, 11 Aug 2016 04:09:48 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Hutchison", "Dylan", ""], ["Kepner", "Jeremy", ""], ["Gadepally", "Vijay", ""], ["Howe", "Bill", ""]]}, {"id": "1606.07111", "submitter": "Johanne Cohen", "authors": "Johanne Cohen, Daniel Cordeiro, Loubna Echabbi", "title": "Detecting service provider alliances on the choreography enactment\n  pricing game", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the choreography enactment pricing game, a cooperative\ngame-theoretic model for the study of scheduling of jobs using competitor\nservice providers. A choreography (a peer-to-peer service composition model)\nneeds a set of services to fulfill its jobs requirements. Users must choose,\nfor each requirement, which service providers will be used to enact the\nchoreography at lowest cost. Due to the lack of centralization, vendors can\nform alliances to control the market. We show a novel algorithm capable of\ndetecting alliances among service providers, based on our study of the\nbargaining set of this game.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 21:14:45 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Cohen", "Johanne", ""], ["Cordeiro", "Daniel", ""], ["Echabbi", "Loubna", ""]]}, {"id": "1606.07226", "submitter": "Lingkang Wang", "authors": "Lingkang Wang, Tong Ye, Tony T. Lee, and Weisheng Hu", "title": "Parallel Scheduling Algorithm based on Complex Coloring for Input-Queued\n  Switches", "comments": "16 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the application of a new algebraic method of edge\ncoloring, called complex coloring, to the scheduling problems of input queued\nswitches. The proposed distributed parallel scheduling algorithm possesses two\nimportant features: optimality and rearrangeability. Optimality ensures that\nthe algorithm always returns a proper coloring with the minimum number of\nrequired colors, and rearrangeability allows partially re-coloring the existing\nconnection patterns if the underlying graph only changes slightly. The running\ntime of the proposed scheduling algorithm is on the order of $O(\\log^2 N)$ per\nframe, and the amortized time complexity, the time to compute a matching per\ntimeslot, is only $O(\\log N)$. The scheduling algorithm is highly robust in the\nface of traffic fluctuations. Since the higher the variable density, the higher\nthe efficiency of the variable elimination process, complex coloring provides a\nnatural adaptive solution to non-uniform input traffic patterns. The proposed\nscheduling algorithm for packet switching can achieve nearly 100% throughput.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 08:43:07 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Wang", "Lingkang", ""], ["Ye", "Tong", ""], ["Lee", "Tony T.", ""], ["Hu", "Weisheng", ""]]}, {"id": "1606.07310", "submitter": "Gabriele D'Angelo", "authors": "Gabriele D'Angelo, Stefano Ferretti, Moreno Marzolla, Lorenzo Armaroli", "title": "Fault-Tolerant Adaptive Parallel and Distributed Simulation", "comments": "Proceedings of the IEEE/ACM International Symposium on Distributed\n  Simulation and Real Time Applications (DS-RT 2016)", "journal-ref": null, "doi": "10.1109/DS-RT.2016.11", "report-no": null, "categories": "cs.DC cs.MA cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete Event Simulation is a widely used technique that is used to model\nand analyze complex systems in many fields of science and engineering. The\nincreasingly large size of simulation models poses a serious computational\nchallenge, since the time needed to run a simulation can be prohibitively\nlarge. For this reason, Parallel and Distributes Simulation techniques have\nbeen proposed to take advantage of multiple execution units which are found in\nmulticore processors, cluster of workstations or HPC systems. The current\ngeneration of HPC systems includes hundreds of thousands of computing nodes and\na vast amount of ancillary components. Despite improvements in manufacturing\nprocesses, failures of some components are frequent, and the situation will get\nworse as larger systems are built. In this paper we describe FT-GAIA, a\nsoftware-based fault-tolerant extension of the GAIA/ART\\`IS parallel simulation\nmiddleware. FT-GAIA transparently replicates simulation entities and\ndistributes them on multiple execution nodes. This allows the simulation to\ntolerate crash-failures of computing nodes; furthermore, FT-GAIA offers some\nprotection against byzantine failures since synchronization messages are\nreplicated as well, so that the receiving entity can identify and discard\ncorrupted messages. We provide an experimental evaluation of FT-GAIA on a\nrunning prototype. Results show that a high degree of fault tolerance can be\nachieved, at the cost of a moderate increase in the computational load of the\nexecution units.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 13:35:50 GMT"}, {"version": "v2", "created": "Thu, 29 Dec 2016 13:08:35 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["D'Angelo", "Gabriele", ""], ["Ferretti", "Stefano", ""], ["Marzolla", "Moreno", ""], ["Armaroli", "Lorenzo", ""]]}, {"id": "1606.07336", "submitter": "Sanjay Sahay", "authors": "Aruna Govada and Sanjay K. Sahay", "title": "Covariance estimation for vertically partitioned data in a distributed\n  environment", "comments": "Proceedings, 17th IEEE/ACIS International Conference on Software\n  Engineering, Artificial Intelligence, Networking and Parallel/Distributed\n  Computing (SNPD), 2016", "journal-ref": "Springer, Studies in Computational Intelligence, Vol. 653, pp\n  151-164, 2016", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The major sources of abundant data are constantly expanding with the\navailable data collection methodologies in various applications - medical,\ninsurance, scientific, bio-informatics and business. These data sets may be\ndistributed geographically, rich in size and as well as dimensions also. To\nanalyze these data sets to find out the hidden patterns, it is required to\ndown- load the data to a centralized site which is a challenging task in terms\nof the limited bandwidth available and computationally also expensive. The\ncovariance matrix is one of the methods to estimate the relation between any\ntwo dimensions. In this paper, we propose a communication efficient algorithm\nto estimate the covariance matrix in a distributed manner. The global\ncovariance matrix is computed by merging the local covariance matrices using a\ndistributed approach. The results show that it is exactly same as centralized\nmethod with good speed-up in terms of computation. The reason for speed-up is\nbecause of the parallel construction of local covariances and distributing the\ncross-covariances among the nodes so that the load is balanced. The results are\nanalyzed by considering Mfeat data set on the various partitions which address\nthe scalability also.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 14:52:26 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Govada", "Aruna", ""], ["Sahay", "Sanjay K.", ""]]}, {"id": "1606.07345", "submitter": "Sanjay Sahay", "authors": "Aruna Govada and Sanjay K. Sahay", "title": "A Communication Efficient and Scalable Distributed Data Mining for the\n  Astronomical Data", "comments": "Accepted in Astronomy and Computing, 2016, 20 Pages, 19 Figures", "journal-ref": "Elsevier, Astronomy and Computing, Vol. 16, pp. 166 - 174, 2016", "doi": "10.1016/j.ascom.2016.06.002", "report-no": null, "categories": "astro-ph.IM cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2020, ~60PB of archived data will be accessible to the astronomers. But to\nanalyze such a paramount data will be a challenging task. This is basically due\nto the computational model used to download the data from complex\ngeographically distributed archives to a central site and then analyzing it in\nthe local systems. Because the data has to be downloaded to the central site,\nthe network BW limitation will be a hindrance for the scientific discoveries.\nAlso analyzing this PB-scale on local machines in a centralized manner is\nchallenging. In this virtual observatory is a step towards this problem,\nhowever, it does not provide the data mining model. Adding the distributed data\nmining layer to the VO can be the solution in which the knowledge can be\ndownloaded by the astronomers instead the raw data and thereafter astronomers\ncan either reconstruct the data back from the downloaded knowledge or use the\nknowledge directly for further analysis.Therefore, in this paper, we present\nDistributed Load Balancing Principal Component Analysis for optimally\ndistributing the computation among the available nodes to minimize the\ntransmission cost and downloading cost for the end user. The experimental\nanalysis is done with Fundamental Plane(FP) data, Gadotti data and complex\nMfeat data. In terms of transmission cost, our approach performs better than\nQi. et al. and Yue.et al. The analysis shows that with the complex Mfeat data\n~90% downloading cost can be reduced for the end user with the negligible loss\nin accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 15:32:52 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Govada", "Aruna", ""], ["Sahay", "Sanjay K.", ""]]}, {"id": "1606.07490", "submitter": "Mark Moir", "authors": "Maurice Herlihy and Mark Moir", "title": "Enhancing Accountability and Trust in Distributed Ledgers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Permisionless decentralized ledgers (\"blockchains\") such as the one\nunderlying the cryptocurrency Bitcoin allow anonymous participants to maintain\nthe ledger, while avoiding control or \"censorship\" by any single entity. In\ncontrast, permissioned decentralized ledgers exploit real-world trust and\naccountability, allowing only explicitly authorized parties to maintain the\nledger. Permissioned ledgers support more flexible governance and a wider\nchoice of consensus mechanisms. Both kinds of decentralized ledgers may be\nsusceptible to manipulation by participants who favor some transactions over\nothers. The real-world accountability underlying permissioned ledgers provides\nan opportunity to impose fairness constraints that can be enforced by\npenalizing violators after-the- fact. To date, however, this opportunity has\nnot been fully exploited, unnecessarily leaving participants latitude to\nmanipulate outcomes undetectably. This paper draws attention to this issue, and\nproposes design principles to make such manipulation more difficult, as well as\nspecific mechanisms to make it easier to detect when violations occur.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 21:47:10 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Herlihy", "Maurice", ""], ["Moir", "Mark", ""]]}, {"id": "1606.07502", "submitter": "Biljana Risteska Stojkoska Dr", "authors": "Biljana Stojkoska, Ilinka Ivanoska and Danco Davcev", "title": "Wireless Sensor Networks Localization Methods: Multidimensional Scaling\n  vs. Semidefinite Programming Approach", "comments": "12 pages", "journal-ref": "ICT Innovations 2009, Ohrid, Macedonia, pp.145-155, Print ISBN\n  978-3-642-10780-1, Online ISBN 978-3-642-10781-8", "doi": "10.1007/978-3-642-10781-8_16", "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent development of technology, wireless sensor networks are\nbecoming an important part of many applications such as health and medical\napplications, military applications, agriculture monitoring, home and office\napplications, environmental monitoring, etc. Knowing the location of a sensor\nis important, but GPS receivers and ophisticated sensors are too expensive and\nrequire processing power. Therefore, the localization wireless sensor network\nproblem is a growing field of interest. The aim of this paper is to give a\ncomparison of wireless sensor network localization methods, and therefore,\nmultidimensional scaling and semidefinite programming are chosen for this\nresearch. Multidimensional scaling is a simple mathematical technique\nwidely-discussed that solves the wireless sensor networks localization problem.\nIn contrast, semidefinite programming is a relatively new field of optimization\nwith a growing use, although being more complex. In this paper, using extensive\nsimulations, a detailed overview of these two approaches is given, regarding\ndifferent network topologies, various network parameters and performance\nissues. The performances of both techniques are highly satisfactory and\nestimation errors are minimal\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 22:46:52 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Stojkoska", "Biljana", ""], ["Ivanoska", "Ilinka", ""], ["Davcev", "Danco", ""]]}, {"id": "1606.07506", "submitter": "Biljana Risteska Stojkoska Dr", "authors": "Biljana Stojkoska, Danco Davcev and Andrea Kulakov", "title": "Cluster-based MDS Algorithm for Nodes Localization in Wireless Sensor\n  Networks with Irregular Topologies", "comments": "6 pages. arXiv admin note: text overlap with arXiv:1606.07389", "journal-ref": "Proceedings of the 5th international conference on Soft computing\n  as transdisciplinary science and technology, ISBN: 978-1-60558-046-3, ACM,\n  2008. pp. 384-389", "doi": "10.1145/1456223.1456302", "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nodes localization in Wireless Sensor Networks (WSN) has arisen as a very\nchallenging problem in the research community. Most of the applications for WSN\nare not useful without a priori known nodes positions. One solution to the\nproblem is by adding GPS receivers to each node. Since this is an expensive\napproach and inapplicable for indoor environments, we need to find an\nalternative intelligent mechanism for determining nodes location. In this\npaper, we propose our cluster-based approach of multidimensional scaling (MDS)\ntechnique. Our initial experiments show that our algorithm outperforms\nMDS-MAP[8], particularly for irregular topologies in terms of accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 23:03:22 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Stojkoska", "Biljana", ""], ["Davcev", "Danco", ""], ["Kulakov", "Andrea", ""]]}, {"id": "1606.07516", "submitter": "EPTCS", "authors": "Krzysztof R. Apt (Centrum Wiskunde Informatica), Davide Grossi\n  (University of Liverpool), Wiebe van der Hoek (University of Liverpool)", "title": "Epistemic Protocols for Distributed Gossiping", "comments": "In Proceedings TARK 2015, arXiv:1606.07295", "journal-ref": "EPTCS 215, 2016, pp. 51-66", "doi": "10.4204/EPTCS.215.5", "report-no": null, "categories": "cs.AI cs.DC cs.LO cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gossip protocols aim at arriving, by means of point-to-point or group\ncommunications, at a situation in which all the agents know each other's\nsecrets. We consider distributed gossip protocols which are expressed by means\nof epistemic logic. We provide an operational semantics of such protocols and\nset up an appropriate framework to argue about their correctness. Then we\nanalyze specific protocols for complete graphs and for directed rings.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 00:30:33 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Apt", "Krzysztof R.", "", "Centrum Wiskunde Informatica"], ["Grossi", "Davide", "", "University of Liverpool"], ["van der Hoek", "Wiebe", "", "University of Liverpool"]]}, {"id": "1606.07525", "submitter": "EPTCS", "authors": "Yoram Moses", "title": "Relating Knowledge and Coordinated Action: The Knowledge of\n  Preconditions Principle", "comments": "In Proceedings TARK 2015, arXiv:1606.07295", "journal-ref": "EPTCS 215, 2016, pp. 231-245", "doi": "10.4204/EPTCS.215.17", "report-no": null, "categories": "cs.MA cs.AI cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Knowledge of Preconditions principle (KoP) is proposed as a widely\napplicable connection between knowledge and action in multi-agent systems.\nRoughly speaking, it asserts that if some condition is a necessary condition\nfor performing a given action A, then knowing that this condition holds is also\na necessary condition for performing A. Since the specifications of tasks often\ninvolve necessary conditions for actions, the KoP principle shows that such\nspecifications induce knowledge preconditions for the actions. Distributed\nprotocols or multi-agent plans that satisfy the specifications must ensure that\nthis knowledge be attained, and that it is detected by the agents as a\ncondition for action. The knowledge of preconditions principle is formalised in\nthe runs and systems framework, and is proven to hold in a wide class of\nsettings. Well-known connections between knowledge and coordinated action are\nextended and shown to derive directly from the KoP principle: a \"common\nknowledge of preconditions\" principle is established showing that common\nknowledge is a necessary condition for performing simultaneous actions, and a\n\"nested knowledge of preconditions\" principle is proven, showing that\ncoordinating actions to be performed in linear temporal order requires a\ncorresponding form of nested knowledge.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 00:32:41 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Moses", "Yoram", ""]]}, {"id": "1606.07621", "submitter": "Anshu Shukla", "authors": "Anshu Shukla, Yogesh Simmhan", "title": "Benchmarking Distributed Stream Processing Platforms for IoT\n  Applications", "comments": "To Appear in VLDB-TPCTC (2016) , 20 pages,Benchmark,IoT,smart cities\n  ,velocity", "journal-ref": "Proceedings of the Technology Conference on Performance Evaluation\n  and Benchmarking (TPCTC), New Delhi, India, 2016, Co-located with VLDB, 2016.\n  Lecture Notes in Computer Science, vol 10080. Springer", "doi": "10.1007/978-3-319-54334-5_7", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet of Things (IoT) is a technology paradigm where millions of sensors\nmonitor, and help inform or manage, physical, envi- ronmental and human systems\nin real-time. The inherent closed-loop re- sponsiveness and decision making of\nIoT applications makes them ideal candidates for using low latency and scalable\nstream processing plat- forms. Distributed Stream Processing Systems (DSPS) are\nbecoming es- sential components of any IoT stack, but the efficacy and\nperformance of contemporary DSPS have not been rigorously studied for IoT data\nstreams and applications. Here, we develop a benchmark suite and per- formance\nmetrics to evaluate DSPS for streaming IoT applications. The benchmark includes\n13 common IoT tasks classified across various func- tional categories and\nforming micro-benchmarks, and two IoT applica- tions for statistical\nsummarization and predictive analytics that leverage various dataflow\ncompositional features of DSPS. These are coupled with stream workloads sourced\nfrom real IoT observations from smart cities. We validate the IoT benchmark for\nthe popular Apache Storm DSPS, and present empirical results.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 09:49:42 GMT"}, {"version": "v2", "created": "Tue, 26 Jul 2016 13:06:33 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Shukla", "Anshu", ""], ["Simmhan", "Yogesh", ""]]}, {"id": "1606.07676", "submitter": "Jesper Larsson Tr\\\"aff", "authors": "Jesper Larsson Tr\\\"aff, Alexandra Carpen-Amarie, Sascha Hunold,\n  Antoine Rougier", "title": "Message-Combining Algorithms for Isomorphic, Sparse Collective\n  Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Isomorphic (sparse) collective communication is a form of collective\ncommunication in which all involved processes communicate in small, identically\nstructured neighborhoods of other processes. Isomorphic neighborhoods are\ndefined via an embedding of the processes in a regularly structured topology,\ne.g., $d$-dimensional torus, which may correspond to the physical communication\nnetwork of the underlying system. Isomorphic collective communication is useful\nfor implementing stencil and other regular, sparse distributed computations,\nwhere the assumption that all processes behave (almost) symmetrically is\njustified.\n  In this paper, we show how efficient message-combining communication\nschedules for isomorphic, sparse collective communication can easily and\nefficiently be computed by purely local computations. We give schemes for\n\\emph{isomorphic \\alltoall} and \\emph{\\allgather} communication that reduce the\nnumber of communication rounds and thereby the communication latency from $s$\nto at most $Nd$, for neighborhoods consisting of $s$ processes with the (small)\nfactor $N$ depending on the structure of the neighborhood and the capabilities\nof the communication system. Using these schedules, we give \\emph{zero-copy\nimplementations} of the isomorphic collectives using MPI and its derived\ndatatypes to eliminate explicit, process-local copy operations. By benchmarking\nthe collective communication algorithms against straightforward implementations\nand against the corresponding MPI neighborhood collectives, we document\nsignificant latency improvements of our implementations for block sizes of up\nto a few kilobytes. We discuss further optimizations for computing even better\nschedules, some of which have been implemented and benchmarked.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 13:13:03 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Tr\u00e4ff", "Jesper Larsson", ""], ["Carpen-Amarie", "Alexandra", ""], ["Hunold", "Sascha", ""], ["Rougier", "Antoine", ""]]}, {"id": "1606.07785", "submitter": "Rahul Mishra", "authors": "Rahul Mishra", "title": "Incentive Schemes for Mobile Peer-to-Peer Systems and Free Riding\n  Problem: A Survey", "comments": "29 pages, 10 figures, scholarly report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GT cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile peer-to-peer networks are quite prevalent and popular now days due to\nadvent of business scenarios where all the services are going mobile like\nwhether it's to find good restaurants, healthy diet books making friends,\njob-hunting, real state info or cab-sharing etc. As the mobile users are\nincreasing day by day, peer-to-peer networks getting bigger and complex. In\ncontrast to client server system in peer-to-peer network resource sharing is\ndone on the basis of mutual consent and agreed policies with no central\nauthority and controlling entity. Incentive schemes for P2P networks are\ndevised to encourage the participation and to adhere the policies agreed. P2P\nservices based only on altruistic behaviour of users are facing serious\nchallenges like Free riding or The tragedy of commons. Free riders are the\nusers who consume the bandwidth of the system (perform downloading) but don't\nshow altruistic behaviour (deny uploading) and act as a parasite for the P2P\nnetwork. To counter the free riding issue many Incentive schemes are suggested\nby the researchers. In this paper we will survey the different incentive\nschemes, their architectures keeping eye on how they handle the challenges of\nmodern P2P network.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 10:44:08 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Mishra", "Rahul", ""]]}, {"id": "1606.07822", "submitter": "Jeroen Vuurens", "authors": "Jeroen B.P. Vuurens, Carsten Eickhoff, Arjen P. de Vries", "title": "Efficient Parallel Learning of Word2Vec", "comments": "ICML 2016 Machine Learning workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since its introduction, Word2Vec and its variants are widely used to learn\nsemantics-preserving representations of words or entities in an embedding\nspace, which can be used to produce state-of-art results for various Natural\nLanguage Processing tasks. Existing implementations aim to learn efficiently by\nrunning multiple threads in parallel while operating on a single model in\nshared memory, ignoring incidental memory update collisions. We show that these\ncollisions can degrade the efficiency of parallel learning, and propose a\nstraightforward caching strategy that improves the efficiency by a factor of 4.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 20:05:53 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Vuurens", "Jeroen B. P.", ""], ["Eickhoff", "Carsten", ""], ["de Vries", "Arjen P.", ""]]}, {"id": "1606.07876", "submitter": "Michele Amoretti", "authors": "Michele Amoretti and Francesco Zanichelli", "title": "P2P-PL: A Pattern Language to Design Efficient and Robust Peer-to-Peer\n  Systems", "comments": "43 pages, 27 figures", "journal-ref": null, "doi": "10.1007/s12083-017-0551-y", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To design peer-to-peer (P2P) software systems is a challenging task, because\nof their highly decentralized nature, which may cause unexpected emergent\nglobal behaviors. The last fifteen years have seen many P2P applications to\ncome out and win favor with millions of users. From success histories of\napplications like BitTorrent, Skype, MyP2P we have learnt a number of useful\ndesign patterns. Thus, in this article we present a P2P pattern language\n(shortly, P2P-PL) which encompasses all the aspects that a fully effective and\nefficient P2P software system should provide, namely consistency of stored\ndata, redundancy, load balancing, coping with asymmetric bandwidth,\ndecentralized security. The patterns of the proposed P2P-PL are described in\ndetail, and a composition strategy for designing robust, effective and\nefficient P2P software systems is proposed.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jun 2016 07:14:47 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Amoretti", "Michele", ""], ["Zanichelli", "Francesco", ""]]}, {"id": "1606.08150", "submitter": "Da Li", "authors": "Hancheng Wu, Da Li, Michela Becchi", "title": "Compiler-Assisted Workload Consolidation For Efficient Dynamic\n  Parallelism on GPU", "comments": "10 pages, 10 figures, to be published in IEEE International Parallel\n  and Distributed Processing Symposium (IPDPS) 2016", "journal-ref": null, "doi": "10.1109/IPDPS.2016.98", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GPUs have been widely used to accelerate computations exhibiting simple\npatterns of parallelism - such as flat or two-level parallelism - and a degree\nof parallelism that can be statically determined based on the size of the input\ndataset. However, the effective use of GPUs for algorithms exhibiting complex\npatterns of parallelism, possibly known only at runtime, is still an open\nproblem. Recently, Nvidia has introduced Dynamic Parallelism (DP) in its GPUs.\nBy making it possible to launch kernels directly from GPU threads, this feature\nenables nested parallelism at runtime. However, the effective use of DP must\nstill be understood: a naive use of this feature may suffer from significant\nruntime overhead and lead to GPU underutilization, resulting in poor\nperformance. In this work, we target this problem. First, we demonstrate how a\nnaive use of DP can result in poor performance. Second, we propose three\nworkload consolidation schemes to improve performance and hardware utilization\nof DP-based codes, and we implement these code transformations in a\ndirective-based compiler. Finally, we evaluate our framework on two categories\nof applications: algorithms including irregular loops and algorithms exhibiting\nparallel recursion. Our experiments show that our approach significantly\nreduces runtime overhead and improves GPU utilization, leading to speedup\nfactors from 90x to 3300x over basic DP-based solutions and speedups from 2x to\n6x over flat implementations.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 07:55:23 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Wu", "Hancheng", ""], ["Li", "Da", ""], ["Becchi", "Michela", ""]]}, {"id": "1606.08156", "submitter": "Georgios Chasparis", "authors": "Georgios C. Chasparis and Michael Rossbory", "title": "Efficient Dynamic Pinning of Parallelized Applications by Distributed\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a resource allocation framework specifically tailored\nfor addressing the problem of dynamic placement (or pinning) of parallelized\napplications to processing units. Under the proposed setup each thread of the\nparallelized application constitutes an independent decision maker (or agent),\nwhich (based on its own prior performance measurements and its own prior\nCPU-affinities) decides on which processing unit to run next. Decisions are\nupdated recursively for each thread by a resource manager/scheduler which runs\nin parallel to the application's threads and periodically records their\nperformances and assigns to them new CPU affinities. For updating the\nCPU-affinities, the scheduler uses a distributed reinforcement-learning\nalgorithm, each branch of which is responsible for assigning a new placement\nstrategy to each thread. According to this algorithm, prior allocations are\ngoing to be reinforced in the future proportionally to their prior performance.\nThe proposed resource allocation framework is flexible enough to address\nalternative optimization criteria, such as maximum average processing speed and\nminimum speed variance among threads. We demonstrate analytically that\nconvergence to locally-optimal placements is achieved asymptotically. Finally,\nwe validate these results through experiments in Linux platforms.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 08:14:11 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Chasparis", "Georgios C.", ""], ["Rossbory", "Michael", ""]]}, {"id": "1606.08277", "submitter": "Manos Koutsoubelias", "authors": "Manos Koutsoubelias and Spyros Lalis", "title": "Coordinated Broadcast-based Request-Reply and Group Management for\n  Tightly-Coupled Wireless Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the domain of cyber-physical systems continues to grow, an increasing\nnumber of tightly-coupled distributed applications will be implemented on top\nof wireless networking technologies. Some of these applications, including\ncollaborative robotic teams, work in a coordinated fashion, whereby a\ndistinguished node takes control decisions and sends commands to other nodes,\nwhich in turn perform the requested action/operation and send back a\nreply/acknowledgment. The implementation of such interactions via reliable\npoint-to-point flows may lead to a significant performance degradation due to\ncollisions, especially when the system operates close to the capacity of the\ncommunication channel. We propose a coordinated protocol which exploits the\nbroadcast nature of the wireless medium in order to support this\napplication-level interaction with a minimal number of message transmissions\nand predictable latency. The protocol also comes with group management\nfunctionality, allowing new processes to join and existing processes to leave\nthe group in a controlled way. We evaluate a prototype implementation over\nWiFi, using a simulated setup as well as a physical testbed. Our results show\nthat the proposed protocol can achieve significantly better performance\ncompared to point-to- point approaches, and remains fully predictable and\ndependable even when operating close to the wireless channel capacity.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 13:51:00 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Koutsoubelias", "Manos", ""], ["Lalis", "Spyros", ""]]}, {"id": "1606.08766", "submitter": "Michael Axtmann", "authors": "Michael Axtmann, Peter Sanders", "title": "Robust Massively Parallel Sorting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate distributed memory parallel sorting algorithms that scale to\nthe largest available machines and are robust with respect to input size and\ndistribution of the input elements. The main outcome is that four sorting\nalgorithms cover the entire range of possible input sizes. For three algorithms\nwe devise new low overhead mechanisms to make them robust with respect to\nduplicate keys and skewed input distributions. One of these, designed for\nmedium sized inputs, is a new variant of quicksort with fast high-quality pivot\nselection.\n  At the same time asymptotic analysis provides performance guarantees and\nguides the selection and configuration of the algorithms. We validate these\nhypotheses using extensive experiments on 7 algorithms, 10 input distributions,\nup to 262144 cores, and varying input sizes over 9 orders of magnitude. For\ndifficult input distributions, our algorithms are the only ones that work at\nall. For all but the largest input sizes, we are the first to perform\nexperiments on such large machines at all and our algorithms significantly\noutperform the ones one would conventionally have considered.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 15:57:27 GMT"}, {"version": "v2", "created": "Tue, 23 Aug 2016 14:10:26 GMT"}, {"version": "v3", "created": "Wed, 2 Nov 2016 14:39:44 GMT"}, {"version": "v4", "created": "Thu, 16 Jan 2020 09:44:54 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Axtmann", "Michael", ""], ["Sanders", "Peter", ""]]}, {"id": "1606.08881", "submitter": "Crist\\'obal A. Navarro", "authors": "Crist\\'obal A. Navarro, Benjam\\'in Bustos, Nancy Hitschfeld", "title": "Potential benefits of a block-space GPU approach for discrete\n  tetrahedral domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of data-parallel domain re-organization and thread-mapping\ntechniques are relevant topics as they can increase the efficiency of GPU\ncomputations when working on spatial discrete domains with non-box-shaped\ngeometry. In this work we study the potential benefits of applying a succint\ndata re-organization of a tetrahedral data-parallel domain of size\n$\\mathcal{O}(n^3)$ combined with an efficient block-space GPU map of the form\n$g:\\mathbb{N} \\rightarrow \\mathbb{N}^3$. Results from the analysis suggest that\nin theory the combination of these two optimizations produce significant\nperformance improvement as block-based data re-organization allows a coalesced\none-to-one correspondence at local thread-space while $g(\\lambda)$ produces an\nefficient block-space spatial correspondence between groups of data and groups\nof threads, reducing the number of unnecessary threads from $O(n^3)$ to\n$O(n^2\\rho^3)$ where $\\rho$ is the linear block-size and typically $\\rho^3 \\ll\nn$. From the analysis, we obtained that a block based succint data\nre-organization can provide up to $2\\times$ improved performance over a linear\ndata organization while the map can be up to $6\\times$ more efficient than a\nbounding box approach. The results from this work can serve as a useful guide\nfor a more efficient GPU computation on tetrahedral domains found in spin\nlattice, finite element and special n-body problems, among others.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 20:47:01 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Navarro", "Crist\u00f3bal A.", ""], ["Bustos", "Benjam\u00edn", ""], ["Hitschfeld", "Nancy", ""]]}, {"id": "1606.08883", "submitter": "Lili Su", "authors": "Lili Su, Nitin H. Vaidya", "title": "Defending Non-Bayesian Learning against Adversarial Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of non-Bayesian learning over multi-agent\nnetworks, where agents repeatedly collect partially informative observations\nabout an unknown state of the world, and try to collaboratively learn the true\nstate. We focus on the impact of the adversarial agents on the performance of\nconsensus-based non-Bayesian learning, where non-faulty agents combine local\nlearning updates with consensus primitives. In particular, we consider the\nscenario where an unknown subset of agents suffer Byzantine faults -- agents\nsuffering Byzantine faults behave arbitrarily. Two different learning rules are\nproposed.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 20:50:08 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Su", "Lili", ""], ["Vaidya", "Nitin H.", ""]]}, {"id": "1606.08904", "submitter": "Lili Su", "authors": "Lili Su", "title": "On the Convergence Rate of Average Consensus and Distributed\n  Optimization over Unreliable Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problems of reaching average consensus and solving\nconsensus-based optimization over unreliable communication networks wherein\npackets may be dropped accidentally during transmission. Existing work either\nassumes that the link failures affect the communication on both directions or\nthat the message senders {\\em know exactly}, in each iteration, how many of\ntheir outgoing links are functioning properly. In this paper, we consider\ndirected links, and we {\\em do not} require each node know its current outgoing\ndegree.\n  First, we propose and characterize the convergence rate of reaching average\nconsensus. Then we apply our robust consensus update to the classical\ndistributed dual averaging method wherein the consensus update is used as the\ninformation aggregation primitive. We show that the local iterates converge to\na common optimum of the global objective at rate $O(\\frac{1}{\\sqrt{t}})$, where\n$t$ is the number of iterations, matching the failure-free performance of the\ndistributed dual averaging method.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 22:24:35 GMT"}, {"version": "v2", "created": "Wed, 2 May 2018 18:17:24 GMT"}, {"version": "v3", "created": "Fri, 7 Dec 2018 22:45:00 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Su", "Lili", ""]]}, {"id": "1606.08905", "submitter": "Disa Mhembere", "authors": "Disa Mhembere, Da Zheng, Carey E. Priebe, Joshua T. Vogelstein and\n  Randal Burns", "title": "knor: A NUMA-Optimized In-Memory, Distributed and Semi-External-Memory\n  k-means Library", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  k-means is one of the most influential and utilized machine learning\nalgorithms. Its computation limits the performance and scalability of many\nstatistical analysis and machine learning tasks. We rethink and optimize\nk-means in terms of modern NUMA architectures to develop a novel\nparallelization scheme that delays and minimizes synchronization barriers. The\n\\textit{k-means NUMA Optimized Routine} (\\textsf{knor}) library has (i)\nin-memory (\\textsf{knori}), (ii) distributed memory (\\textsf{knord}), and (iii)\nsemi-external memory (\\textsf{knors}) modules that radically improve the\nperformance of k-means for varying memory and hardware budgets. \\textsf{knori}\nboosts performance for single machine datasets by an order of magnitude or\nmore. \\textsf{knors} improves the scalability of k-means on a memory budget\nusing SSDs. \\textsf{knors} scales to billions of points on a single machine,\nusing a fraction of the resources that distributed in-memory systems require.\n\\textsf{knord} retains \\textsf{knori}'s performance characteristics, while\nscaling in-memory through distributed computation in the cloud. \\textsf{knor}\nmodifies Elkan's triangle inequality pruning algorithm such that we utilize it\non billion-point datasets without the significant memory overhead of the\noriginal algorithm. We demonstrate \\textsf{knor} outperforms distributed\ncommercial products like H$_2$O, Turi (formerly Dato, GraphLab) and Spark's\nMLlib by more than an order of magnitude for datasets of $10^7$ to $10^9$\npoints.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 22:25:58 GMT"}, {"version": "v2", "created": "Tue, 17 Jan 2017 18:34:30 GMT"}, {"version": "v3", "created": "Thu, 9 Mar 2017 08:29:49 GMT"}, {"version": "v4", "created": "Thu, 16 Mar 2017 20:47:39 GMT"}, {"version": "v5", "created": "Mon, 1 May 2017 07:13:39 GMT"}, {"version": "v6", "created": "Sat, 24 Jun 2017 05:53:16 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Mhembere", "Disa", ""], ["Zheng", "Da", ""], ["Priebe", "Carey E.", ""], ["Vogelstein", "Joshua T.", ""], ["Burns", "Randal", ""]]}, {"id": "1606.08939", "submitter": "Shreyas Sundaram", "authors": "Shreyas Sundaram and Bahman Gharesifard", "title": "Distributed Optimization Under Adversarial Nodes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the vulnerabilities of consensus-based distributed\noptimization protocols to nodes that deviate from the prescribed update rule\n(e.g., due to failures or adversarial attacks). We first characterize certain\nfundamental limitations on the performance of any distributed optimization\nalgorithm in the presence of adversaries. We then propose a resilient\ndistributed optimization algorithm that guarantees that the non-adversarial\nnodes converge to the convex hull of the minimizers of their local functions\nunder certain conditions on the graph topology, regardless of the actions of a\ncertain number of adversarial nodes. In particular, we provide sufficient\nconditions on the graph topology to tolerate a bounded number of adversaries in\nthe neighborhood of every non-adversarial node, and necessary and sufficient\nconditions to tolerate a globally bounded number of adversaries. For situations\nwhere there are up to F adversaries in the neighborhood of every node, we use\nthe concept of maximal F-local sets of graphs to provide lower bounds on the\ndistance-to-optimality of achievable solutions under any algorithm. We show\nthat finding the size of such sets is NP-hard.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 02:40:21 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Sundaram", "Shreyas", ""], ["Gharesifard", "Bahman", ""]]}, {"id": "1606.09402", "submitter": "Wenjian Yu Prof.", "authors": "Wenjian Yu, Yu Gu, and Yaohang Li", "title": "Efficient Randomized Algorithms for the Fixed-Precision Low-Rank Matrix\n  Approximation", "comments": "21 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized algorithms for low-rank matrix approximation are investigated,\nwith the emphasis on the fixed-precision problem and computational efficiency\nfor handling large matrices. The algorithms are based on the so-called QB\nfactorization, where Q is an orthonormal matrix. Firstly, a mechanism for\ncalculating the approximation error in Frobenius norm is proposed, which\nenables efficient adaptive rank determination for large and/or sparse matrix.\nIt can be combined with any QB-form factorization algorithm in which B's rows\nare incrementally generated. Based on the blocked randQB algorithm by P.-G.\nMartinsson and S. Voronin, this results in an algorithm called randQB EI. Then,\nwe further revise the algorithm to obtain a pass-efficient algorithm, randQB\nFP, which is mathematically equivalent to the existing randQB algorithms and\nalso suitable for the fixed-precision problem. Especially, randQB FP can serve\nas a single-pass algorithm for calculating leading singular values, under\ncertain condition. With large and/or sparse test matrices, we have empirically\nvalidated the merits of the proposed techniques, which exhibit remarkable\nspeedup and memory saving over the blocked randQB algorithm. We have also\ndemonstrated that the single-pass algorithm derived by randQB FP is much more\naccurate than an existing single-pass algorithm. And with data from a scenic\nimage and an information retrieval application, we have shown the advantages of\nthe proposed algorithms over the adaptive range finder algorithm for solving\nthe fixed-precision problem.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 09:14:53 GMT"}, {"version": "v2", "created": "Thu, 7 Jul 2016 08:06:04 GMT"}, {"version": "v3", "created": "Sat, 10 Feb 2018 06:44:17 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Yu", "Wenjian", ""], ["Gu", "Yu", ""], ["Li", "Yaohang", ""]]}]