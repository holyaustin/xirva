[{"id": "1510.00116", "submitter": "Smruti Ranjan Sarangi", "authors": "Seep Goel, Pooja Aggarwal, Smruti R. Sarangi", "title": "A Wait-Free Stack", "comments": "21 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe a novel algorithm to create a con- current\nwait-free stack. To the best of our knowledge, this is the first wait-free\nalgorithm for a general purpose stack. In the past, researchers have proposed\nrestricted wait-free implementations of stacks, lock-free implementations, and\nefficient universal constructions that can support wait-free stacks. The crux\nof our wait-free implementation is a fast pop operation that does not modify\nthe stack top; instead, it walks down the stack till it finds a node that is\nunmarked. It marks it but does not delete it. Subsequently, it is lazily\ndeleted by a cleanup operation. This operation keeps the size of the stack in\ncheck by not allowing the size of the stack to increase beyond a factor of W as\ncompared to the actual size. All our operations are wait-free and linearizable.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2015 06:14:42 GMT"}], "update_date": "2015-10-02", "authors_parsed": [["Goel", "Seep", ""], ["Aggarwal", "Pooja", ""], ["Sarangi", "Smruti R.", ""]]}, {"id": "1510.00132", "submitter": "MIkhail Hushchyn", "authors": "Mikhail Hushchyn, Philippe Charpentier, Andrey Ustyuzhanin", "title": "Disk storage management for LHCb based on Data Popularity estimator", "comments": null, "journal-ref": null, "doi": "10.1088/1742-6596/664/4/042026", "report-no": null, "categories": "cs.DC cs.LG physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an algorithm providing recommendations for optimizing the\nLHCb data storage. The LHCb data storage system is a hybrid system. All\ndatasets are kept as archives on magnetic tapes. The most popular datasets are\nkept on disks. The algorithm takes the dataset usage history and metadata\n(size, type, configuration etc.) to generate a recommendation report. This\narticle presents how we use machine learning algorithms to predict future data\npopularity. Using these predictions it is possible to estimate which datasets\nshould be removed from disk. We use regression algorithms and time series\nanalysis to find the optimal number of replicas for datasets that are kept on\ndisk. Based on the data popularity and the number of replicas optimization, the\nalgorithm minimizes a loss function to find the optimal data distribution. The\nloss function represents all requirements for data distribution in the data\nstorage system. We demonstrate how our algorithm helps to save disk space and\nto reduce waiting times for jobs using this data.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2015 07:40:37 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Hushchyn", "Mikhail", ""], ["Charpentier", "Philippe", ""], ["Ustyuzhanin", "Andrey", ""]]}, {"id": "1510.00220", "submitter": "Smruti Ranjan Sarangi", "authors": "Sandeep Chandran, Eldhose Peter, Preeti Ranjan Panda, Smruti R.\n  Sarangi", "title": "Fundamental Results for a Generic Implementation of Barriers using\n  Optical Interconnects", "comments": "3 pages, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report, we report some fundamental results and bounds on the number\nof messages and storage required to implement barriers using futuristic on-chip\noptical and RF networks. We prove that it is necessary to maintain a count to\nat least N (number of threads) in memory, broadcast the barrier id at least\nonce, and if we elect a co-ordinator, we can reduce the number of messages by a\nfactor of O(N ).\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2015 13:24:06 GMT"}], "update_date": "2015-10-02", "authors_parsed": [["Chandran", "Sandeep", ""], ["Peter", "Eldhose", ""], ["Panda", "Preeti Ranjan", ""], ["Sarangi", "Smruti R.", ""]]}, {"id": "1510.00627", "submitter": "Setareh Maghsudi", "authors": "Setareh Maghsudi and Ekram Hossain", "title": "Multi-armed Bandits with Application to 5G Small Cells", "comments": "10 pages", "journal-ref": null, "doi": "10.1109/MWC.2016.7498076", "report-no": null, "categories": "cs.LG cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the pervasive demand for mobile services, next generation wireless\nnetworks are expected to be able to deliver high date rates while wireless\nresources become more and more scarce. This requires the next generation\nwireless networks to move towards new networking paradigms that are able to\nefficiently support resource-demanding applications such as personalized mobile\nservices. Examples of such paradigms foreseen for the emerging fifth generation\n(5G) cellular networks include very densely deployed small cells and\ndevice-to-device communications. For 5G networks, it will be imperative to\nsearch for spectrum and energy-efficient solutions to the resource allocation\nproblems that i) are amenable to distributed implementation, ii) are capable of\ndealing with uncertainty and lack of information, and iii) can cope with users'\nselfishness. The core objective of this article is to investigate and to\nestablish the potential of multi-armed bandit (MAB) framework to address this\nchallenge. In particular, we provide a brief tutorial on bandit problems,\nincluding different variations and solution approaches. Furthermore, we discuss\nrecent applications as well as future research directions. In addition, we\nprovide a detailed example of using an MAB model for energy-efficient small\ncell planning in 5G networks.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2015 15:49:59 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Maghsudi", "Setareh", ""], ["Hossain", "Ekram", ""]]}, {"id": "1510.00664", "submitter": "Mark Scanlon", "authors": "Hessel Schut, Mark Scanlon, Jason Farina and Nhien-An Le-Khac", "title": "Towards the Forensic Identification and Investigation of Cloud Hosted\n  Servers through Noninvasive Wiretaps", "comments": "Proceedings of 10th International Conference on Availability,\n  Reliability and Security (ARES 2015)", "journal-ref": null, "doi": "10.1109/ARES.2015.77", "report-no": null, "categories": "cs.DC cs.CR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When conducting modern cybercrime investigations, evidence has often to be\ngathered from computer systems located at cloud-based data centres of hosting\nproviders. In cases where the investigation cannot rely on the cooperation of\nthe hosting provider, or where documentation is not available, investigators\ncan often find the identification of which distinct server among many is of\ninterest difficult and extremely time consuming. To address the problem of\nidentifying these servers, in this paper a new approach to rapidly and reliably\nidentify these cloud hosting computer systems is presented. In the outlined\napproach, a handheld device composed of an embedded computer combined with a\nmethod of undetectable interception of Ethernet based communications is\npresented. This device is tested and evaluated, and a discussion is provided on\nits usefulness in identifying of server of interest to an investigation.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2015 17:58:54 GMT"}], "update_date": "2015-10-05", "authors_parsed": [["Schut", "Hessel", ""], ["Scanlon", "Mark", ""], ["Farina", "Jason", ""], ["Le-Khac", "Nhien-An", ""]]}, {"id": "1510.00817", "submitter": "Qi Li", "authors": "Qi Li", "title": "Distributed Parameter Map-Reduce", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes how to convert a machine learning problem into a series\nof map-reduce tasks. We study logistic regression algorithm. In logistic\nregression algorithm, it is assumed that samples are independent and each\nsample is assigned a probability. Parameters are obtained by maxmizing the\nproduct of all sample probabilities. Rapid expansion of training samples brings\nchallenges to machine learning method. Training samples are so many that they\ncan be only stored in distributed file system and driven by map-reduce style\nprograms. The main step of logistic regression is inference. According to\nmap-reduce spirit, each sample makes inference through a separate map\nprocedure. But the premise of inference is that the map procedure holds\nparameters for all features in the sample. In this paper, we propose\nDistributed Parameter Map-Reduce, in which not only samples, but also\nparameters are distributed in nodes of distributed filesystem. Through a series\nof map-reduce tasks, we assign each sample parameters for its features, make\ninference for the sample and update paramters of the model. The above processes\nare excuted looply until convergence. We test the proposed algorithm in actual\nhadoop production environment. Experiments show that the acceleration of the\nalgorithm is in linear relationship with the number of cluster nodes.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2015 13:12:28 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Li", "Qi", ""]]}, {"id": "1510.00844", "submitter": "Aydin Buluc", "authors": "Ariful Azad, Grey Ballard, Aydin Buluc, James Demmel, Laura Grigori,\n  Oded Schwartz, Sivan Toledo, Samuel Williams", "title": "Exploiting Multiple Levels of Parallelism in Sparse Matrix-Matrix\n  Multiplication", "comments": null, "journal-ref": "SIAM Journal of Scientific Computing, Volume 38, Number 6, pp.\n  C624-C651, 2016", "doi": "10.1137/15M104253X", "report-no": null, "categories": "cs.DC cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse matrix-matrix multiplication (or SpGEMM) is a key primitive for many\nhigh-performance graph algorithms as well as for some linear solvers, such as\nalgebraic multigrid. The scaling of existing parallel implementations of SpGEMM\nis heavily bound by communication. Even though 3D (or 2.5D) algorithms have\nbeen proposed and theoretically analyzed in the flat MPI model on Erdos-Renyi\nmatrices, those algorithms had not been implemented in practice and their\ncomplexities had not been analyzed for the general case. In this work, we\npresent the first ever implementation of the 3D SpGEMM formulation that also\nexploits multiple (intra-node and inter-node) levels of parallelism, achieving\nsignificant speedups over the state-of-the-art publicly available codes at all\nlevels of concurrencies. We extensively evaluate our implementation and\nidentify bottlenecks that should be subject to further research.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2015 16:32:43 GMT"}, {"version": "v2", "created": "Mon, 8 Aug 2016 17:59:17 GMT"}, {"version": "v3", "created": "Wed, 16 Nov 2016 23:19:52 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Azad", "Ariful", ""], ["Ballard", "Grey", ""], ["Buluc", "Aydin", ""], ["Demmel", "James", ""], ["Grigori", "Laura", ""], ["Schwartz", "Oded", ""], ["Toledo", "Sivan", ""], ["Williams", "Samuel", ""]]}, {"id": "1510.01155", "submitter": "Janis Keuper", "authors": "Janis Keuper and Franz-Josef Pfreundt", "title": "Balancing the Communication Load of Asynchronously Parallelized Machine\n  Learning Algorithms", "comments": "arXiv admin note: substantial text overlap with arXiv:1505.04956", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Gradient Descent (SGD) is the standard numerical method used to\nsolve the core optimization problem for the vast majority of machine learning\n(ML) algorithms. In the context of large scale learning, as utilized by many\nBig Data applications, efficient parallelization of SGD is in the focus of\nactive research. Recently, we were able to show that the asynchronous\ncommunication paradigm can be applied to achieve a fast and scalable\nparallelization of SGD. Asynchronous Stochastic Gradient Descent (ASGD)\noutperforms other, mostly MapReduce based, parallel algorithms solving large\nscale machine learning problems. In this paper, we investigate the impact of\nasynchronous communication frequency and message size on the performance of\nASGD applied to large scale ML on HTC cluster and cloud environments. We\nintroduce a novel algorithm for the automatic balancing of the asynchronous\ncommunication load, which allows to adapt ASGD to changing network bandwidths\nand latencies.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2015 13:57:21 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Keuper", "Janis", ""], ["Pfreundt", "Franz-Josef", ""]]}, {"id": "1510.01989", "submitter": "Malcolm Atkinson", "authors": "Malcolm Atkinson, Michele Carpen\\'e, Emanuele Casarotti, Steffen\n  Claus, Rosa Filgueira, Anton Frank, Michelle Galea, Tom Garth, Andr\\'e\n  Gem\\\"und, Heiner Igel, Iraklis Klampanos, Amrey Krause, Lion Krischer, Siew\n  Hoon Leong, Federica Magnoni, Jonas Matser, Alberto Michelini, Andreas\n  Rietbrock, Horst Schwichtenberg, Alessandro Spinuso and Jean-Pierre Vilotte", "title": "VERCE delivers a productive e-Science environment for seismology\n  research", "comments": "14 pages, 3 figures. Pre-publication version of paper accepted and\n  published at the IEEE eScience 2015 conference in Munich with substantial\n  additions, particularly in the analysis of issues", "journal-ref": null, "doi": "10.1109/eScience.2015.38", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The VERCE project has pioneered an e-Infrastructure to support researchers\nusing established simulation codes on high-performance computers in conjunction\nwith multiple sources of observational data. This is accessed and organised via\nthe VERCE science gateway that makes it convenient for seismologists to use\nthese resources from any location via the Internet. Their data handling is made\nflexible and scalable by two Python libraries, ObsPy and dispel4py and by data\nservices delivered by ORFEUS and EUDAT. Provenance driven tools enable rapid\nexploration of results and of the relationships between data, which accelerates\nunderstanding and method improvement. These powerful facilities are integrated\nand draw on many other e-Infrastructures. This paper presents the motivation\nfor building such systems, it reviews how solid-Earth scientists can make\nsignificant research progress using them and explains the architecture and\nmechanisms that make their construction and operation achievable. We conclude\nwith a summary of the achievements to date and identify the crucial steps\nneeded to extend the capabilities for seismologists, for solid-Earth scientists\nand for similar disciplines.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2015 15:37:48 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Atkinson", "Malcolm", ""], ["Carpen\u00e9", "Michele", ""], ["Casarotti", "Emanuele", ""], ["Claus", "Steffen", ""], ["Filgueira", "Rosa", ""], ["Frank", "Anton", ""], ["Galea", "Michelle", ""], ["Garth", "Tom", ""], ["Gem\u00fcnd", "Andr\u00e9", ""], ["Igel", "Heiner", ""], ["Klampanos", "Iraklis", ""], ["Krause", "Amrey", ""], ["Krischer", "Lion", ""], ["Leong", "Siew Hoon", ""], ["Magnoni", "Federica", ""], ["Matser", "Jonas", ""], ["Michelini", "Alberto", ""], ["Rietbrock", "Andreas", ""], ["Schwichtenberg", "Horst", ""], ["Spinuso", "Alessandro", ""], ["Vilotte", "Jean-Pierre", ""]]}, {"id": "1510.02065", "submitter": "Alexandre Goncalves", "authors": "Alexandre Domingues Gon\\c{c}alves and Artur Alves Pessoa and L\\'ucia\n  Maria de Assump\\c{c}\\~ao Drummond and Cristiana Bentes and Ricardo Farias", "title": "Solving the Quadratic Assignment Problem on heterogeneous environment\n  (CPUs and GPUs) with the application of Level 2 Reformulation and\n  Linearization Technique", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Quadratic Assignment Problem, QAP, is a classic combinatorial\noptimization problem, classified as NP-hard and widely studied. This problem\nconsists in assigning N facilities to N locations obeying the relation of 1 to\n1, aiming to minimize costs of the displacement between the facilities. The\napplication of Reformulation and Linearization Technique, RLT, to the QAP leads\nto a tight linear relaxation but large and difficult to solve. Previous works\nbased on level 3 RLT needed about 700GB of working memory to process one large\ninstances (N = 30 facilities). We present a modified version of the algorithm\nproposed by Adams et al. which executes on heterogeneous systems (CPUs and\nGPUs), based on level 2 RLT. For some instances, our algorithm is up to 140\ntimes faster and occupy 97% less memory than the level 3 RLT version. The\nproposed algorithm was able to solve by first time two instances: tai35b and\ntai40b.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2015 19:05:47 GMT"}], "update_date": "2015-10-08", "authors_parsed": [["Gon\u00e7alves", "Alexandre Domingues", ""], ["Pessoa", "Artur Alves", ""], ["Drummond", "L\u00facia Maria de Assump\u00e7\u00e3o", ""], ["Bentes", "Cristiana", ""], ["Farias", "Ricardo", ""]]}, {"id": "1510.02086", "submitter": "Yann Busnel", "authors": "Peter Okech, Nicholas Mc Guire, William Okelo-Odongo", "title": "Inherent Diversity in Replicated Architectures", "comments": "in Yann Busnel. 11th European Dependable Computing Conference (EDCC\n  2015), Sep 2015, Paris, France. 2015, Proceedings of Student Forum - EDCC\n  2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we report our ongoing investigations of the inherent\nnon-determinism in contemporary execution environments that can potentially\nlead to divergence in state of a multi-channel hardware/software system. Our\napproach involved setting up of experiments to study execution path variability\nof a simple program by tracing its execution at the kernel level. In the first\nof the two experiments, we analyzed the execution path by repeated execution of\nthe program. In the second, we executed in parallel two instances of the same\nprogram, each pinned to a separate processor core. Our results show that for a\nprogram executing in a contemporary hardware/software platform , there is\nsufcient path non-determinism in kernel space that can potentially lead to\ndiversity in replicated architectures. We believe the execution non-determinism\ncan impact the activation of residual systematic faults in software. If this is\ntrue, then the inherent diversity can be used together with architectural means\nto protect safety related systems against residual systematic faults in the\noperating systems.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2015 14:57:45 GMT"}], "update_date": "2015-10-09", "authors_parsed": [["Okech", "Peter", ""], ["Guire", "Nicholas Mc", ""], ["Okelo-Odongo", "William", ""]]}, {"id": "1510.02135", "submitter": "Jerome Soumagne", "authors": "Jerome Soumagne, Philip H. Carns, Dries Kimpe, Quincey Koziol, Robert\n  B. Ross", "title": "A Remote Procedure Call Approach for Extreme-scale Services", "comments": "CSESSP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When working at exascale, the various constraints imposed by the extreme\nscale of the system bring new challenges for application users and\nsoftware/middleware developers. In that context, and to provide best\nperformance, resiliency and energy efficiency, software may be provided as a\nservice oriented approach, adjusting resource utilization to best meet facility\nand user requirements. Remote procedure call (RPC) is a technique that\noriginally followed a client/server model and allowed local calls to be\ntransparently executed on remote resources. RPC consists of serializing the\nlocal function parameters into a memory buffer and sending that buffer to a\nremote target that in turn deserializes the parameters and executes the\ncorresponding function call, returning the result back to the caller. Building\nreusable services requires the definition of a communication model to remotely\naccess these services and for this purpose, RPC can serve as a foundation for\naccessing them. We introduce the necessary building blocks to enable this\necosystem to software and middleware developers with an RPC framework called\nMercury.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2015 21:18:23 GMT"}], "update_date": "2015-10-09", "authors_parsed": [["Soumagne", "Jerome", ""], ["Carns", "Philip H.", ""], ["Kimpe", "Dries", ""], ["Koziol", "Quincey", ""], ["Ross", "Robert B.", ""]]}, {"id": "1510.02163", "submitter": "Vahid Noormofidi", "authors": "Vahid Noormofidi, Susan R. Atlas, Huaiyu Duan", "title": "Performance Analysis of an Astrophysical Simulation Code on the Intel\n  Xeon Phi Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": "CARC-2015-174", "categories": "cs.DC cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have developed the astrophysical simulation code XFLAT to study neutrino\noscillations in supernovae. XFLAT is designed to utilize multiple levels of\nparallelism through MPI, OpenMP, and SIMD instructions (vectorization). It can\nrun on both CPU and Xeon Phi co-processors based on the Intel Many Integrated\nCore Architecture (MIC). We analyze the performance of XFLAT on configurations\nwith CPU only, Xeon Phi only and both CPU and Xeon Phi. We also investigate the\nimpact of I/O and the multi-node performance of XFLAT on the Xeon Phi-equipped\nStampede supercomputer at the Texas Advanced Computing Center (TACC).\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2015 23:02:00 GMT"}], "update_date": "2015-10-12", "authors_parsed": [["Noormofidi", "Vahid", ""], ["Atlas", "Susan R.", ""], ["Duan", "Huaiyu", ""]]}, {"id": "1510.02181", "submitter": "Alejandro Erickson", "authors": "Alejandro Erickson and and Iain A. Stewart and Javier Navaridas and\n  Abbas E. Kiasari", "title": "The Stellar Transformation: From Interconnection Networks to Datacenter\n  Networks", "comments": "Submitted to a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The first dual-port server-centric datacenter network, FiConn, was introduced\nin 2009 and there are several others now in existence; however, the pool of\ntopologies to choose from remains small. We propose a new generic construction,\nthe stellar transformation, that dramatically increases the size of this pool\nby facilitating the transformation of well-studied topologies from\ninterconnection networks, along with their networking properties and routing\nalgorithms, into viable dual-port server-centric datacenter network topologies.\nWe demonstrate that under our transformation, numerous interconnection networks\nyield datacenter network topologies with potentially good, and easily\ncomputable, baseline properties. We instantiate our construction so as to apply\nit to generalized hypercubes and obtain the datacenter networks GQ*. Our\nconstruction automatically yields routing algorithms for GQ* and we empirically\ncompare GQ* (and its routing algorithms) with the established datacenter\nnetworks FiConn and DPillar (and their routing algorithms); this comparison is\nwith respect to network throughput, latency, load balancing, fault-tolerance,\nand cost to build, and is with regard to all-to-all, many all-to-all,\nbutterfly, and random traffic patterns. We find that GQ* outperforms both\nFiConn and DPillar (sometimes significantly so) and that there is substantial\nscope for our stellar transformation to yield new dual-port server-centric\ndatacenter networks that are a considerable improvement on existing ones.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2015 02:09:29 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2015 01:16:36 GMT"}, {"version": "v3", "created": "Wed, 17 Feb 2016 01:01:34 GMT"}, {"version": "v4", "created": "Mon, 27 Jun 2016 21:41:20 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Erickson", "Alejandro", ""], ["Stewart", "and Iain A.", ""], ["Navaridas", "Javier", ""], ["Kiasari", "Abbas E.", ""]]}, {"id": "1510.02211", "submitter": "Gal Amram", "authors": "Gal Amram", "title": "The F-snapshot Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aguilera, Gafni and Lamport introduced the signaling problem in [5]. In this\nproblem, two processes numbered 0 and 1 can call two procedures: update and\nFscan. A parameter of the problem is a two- variable function $F(x_0,x_1)$.\nEach process $p_i$ can assign values to variable $x_i$ by calling update(v)\nwith some data value v, and compute the value: $F(x_0,x_1)$ by executing an\nFscan procedure. The problem is interesting when the domain of $F$ is infinite\nand the range of $F$ is finite. In this case, some \"access restrictions\" are\nimposed that limit the size of the registers that the Fscan procedure can\naccess. Aguilera et al. provided a non-blocking solution and asked whether a\nwait-free solution exists. A positive answer can be found in [7].\n  The natural generalization of the two-process signaling problem to an\narbitrary number of processes turns out to yield an interesting generalization\nof the fundamental snapshot problem, which we call the F-snapshot problem. In\nthis problem $n$ processes can write values to an $n$-segment array (each\nprocess to its own segment), and can read and obtain the value of an n-variable\nfunction $F$ on the array of segments. In case that the range of $F$ is finite,\nit is required that only bounded registers are accessed when the processes\napply the function $F$ to the array, although the data values written to the\nsegments may be taken from an infinite set. We provide here an affirmative\nanswer to the question of Aguilera et al. for an arbitrary number of processes.\nOur solution employs only single-writer atomic registers, and its time\ncomplexity is $O(n \\log n)$, which is also the time complexity of the fastest\nsnapshot algorithm that uses only single-writer registers.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2015 07:27:37 GMT"}], "update_date": "2015-10-09", "authors_parsed": [["Amram", "Gal", ""]]}, {"id": "1510.02215", "submitter": "Ethan R. Elenberg", "authors": "Ethan R. Elenberg, Karthikeyan Shanmugam, Michael Borokhovich,\n  Alexandros G. Dimakis", "title": "Distributed Estimation of Graph 4-Profiles", "comments": "To appear in part at WWW'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel distributed algorithm for counting all four-node induced\nsubgraphs in a big graph. These counts, called the $4$-profile, describe a\ngraph's connectivity properties and have found several uses ranging from\nbioinformatics to spam detection. We also study the more complicated problem of\nestimating the local $4$-profiles centered at each vertex of the graph. The\nlocal $4$-profile embeds every vertex in an $11$-dimensional space that\ncharacterizes the local geometry of its neighborhood: vertices that connect\ndifferent clusters will have different local $4$-profiles compared to those\nthat are only part of one dense cluster.\n  Our algorithm is a local, distributed message-passing scheme on the graph and\ncomputes all the local $4$-profiles in parallel. We rely on two novel\ntheoretical contributions: we show that local $4$-profiles can be calculated\nusing compressed two-hop information and also establish novel concentration\nresults that show that graphs can be substantially sparsified and still retain\ngood approximation quality for the global $4$-profile.\n  We empirically evaluate our algorithm using a distributed GraphLab\nimplementation that we scaled up to $640$ cores. We show that our algorithm can\ncompute global and local $4$-profiles of graphs with millions of edges in a few\nminutes, significantly improving upon the previous state of the art.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2015 07:49:23 GMT"}, {"version": "v2", "created": "Mon, 4 Apr 2016 08:49:33 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Elenberg", "Ethan R.", ""], ["Shanmugam", "Karthikeyan", ""], ["Borokhovich", "Michael", ""], ["Dimakis", "Alexandros G.", ""]]}, {"id": "1510.02229", "submitter": "Ilaria Castellani", "authors": "Massimo Bartoletti, Ilaria Castellani, Pierre-Malo Deni\\'elou,\n  Mariangiola Dezani-Ciancaglini, Silvia Ghilezan, Jovanka Pantovic, Jorge A.\n  P\\'erez, Peter Thiemann, Bernardo Toninho, Hugo Torres Vieira", "title": "Combining behavioural types with security analysis", "comments": null, "journal-ref": "Journal of Logical and Algebraic Methods in Programming, Elsevier,\n  2015, pp.18", "doi": "10.1016/j.jlamp.2015.09.003", "report-no": null, "categories": "cs.PL cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's software systems are highly distributed and interconnected, and they\nincreasingly rely on communication to achieve their goals; due to their\nsocietal importance, security and trustworthiness are crucial aspects for the\ncorrectness of these systems. Behavioural types, which extend data types by\ndescribing also the structured behaviour of programs, are a widely studied\napproach to the enforcement of correctness properties in communicating systems.\nThis paper offers a unified overview of proposals based on behavioural types\nwhich are aimed at the analysis of security properties.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2015 08:23:35 GMT"}], "update_date": "2015-10-09", "authors_parsed": [["Bartoletti", "Massimo", ""], ["Castellani", "Ilaria", ""], ["Deni\u00e9lou", "Pierre-Malo", ""], ["Dezani-Ciancaglini", "Mariangiola", ""], ["Ghilezan", "Silvia", ""], ["Pantovic", "Jovanka", ""], ["P\u00e9rez", "Jorge A.", ""], ["Thiemann", "Peter", ""], ["Toninho", "Bernardo", ""], ["Vieira", "Hugo Torres", ""]]}, {"id": "1510.02237", "submitter": "Daniel Ruprecht", "authors": "Daniel Ruprecht, Rolf Krause", "title": "Explicit Parallel-in-time Integration of a Linear Acoustic-Advection\n  System", "comments": null, "journal-ref": "Computers & Fluids 59, pp. 72-83, 2012", "doi": "10.1016/j.compfluid.2012.02.015", "report-no": null, "categories": "cs.CE cs.DC math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The applicability of the Parareal parallel-in-time integration scheme for the\nsolution of a linear, two-dimensional hyperbolic acoustic-advection system,\nwhich is often used as a test case for integration schemes for numerical\nweather prediction (NWP), is addressed. Parallel-in-time schemes are a possible\nway to increase, on the algorithmic level, the amount of parallelism, a\nrequirement arising from the rapidly growing number of CPUs in high performance\ncomputer systems. A recently introduced modification of the \"parallel implicit\ntime-integration algorithm\" could successfully solve hyperbolic problems\narising in structural dynamics. It has later been cast into the framework of\nParareal. The present paper adapts this modified Parareal and employs it for\nthe solution of a hyperbolic flow problem, where the initial value problem\nsolved in parallel arises from the spatial discretization of a partial\ndifferential equation by a finite difference method. It is demonstrated that\nthe modified Parareal is stable and can produce reasonably accurate solutions\nwhile allowing for a noticeable reduction of the time-to-solution. The\nimplementation relies on integration schemes already widely used in NWP (RK-3,\npartially split forward Euler, forward-backward). It is demonstrated that using\nan explicit partially split scheme for the coarse integrator allows to avoid\nthe use of an implicit scheme while still achieving speedup.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2015 08:47:17 GMT"}], "update_date": "2015-10-09", "authors_parsed": [["Ruprecht", "Daniel", ""], ["Krause", "Rolf", ""]]}, {"id": "1510.02259", "submitter": "Dongsoo Har", "authors": "Hyun-Gyu Ryu, Sang-Keum Lee, and Dongsoo Har", "title": "Data Transmission with Reduced Delay for Distributed Acoustic Sensors", "comments": "Accepted to IJDSN, final preprinted version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a channel access control scheme fit to dense acoustic\nsensor nodes in a sensor network. In the considered scenario, multiple acoustic\nsensor nodes within communication range of a cluster head are grouped into\nclusters. Acoustic sensor nodes in a cluster detect acoustic signals and\nconvert them into electric signals (packets). Detection by acoustic sensors can\nbe executed periodically or randomly and random detection by acoustic sensors\nis event driven. As a result, each acoustic sensor generates their packets\n(50bytes each) periodically or randomly over short time intervals\n(400ms~4seconds) and transmits directly to a cluster head (coordinator node).\nOur approach proposes to use a slotted carrier sense multiple access. All\nacoustic sensor nodes in a cluster are allocated to time slots and the number\nof allocated sensor nodes to each time slot is uniform. All sensor nodes\nallocated to a time slot listen for packet transmission from the beginning of\nthe time slot for a duration proportional to their priority. The first node\nthat detect the channel to be free for its whole window is allowed to transmit.\nThe order of packet transmissions with the acoustic sensor nodes in the time\nslot is autonomously adjusted according to the history of packet transmissions\nin the time slot. In simulations, performances of the proposed scheme are\ndemonstrated by the comparisons with other low rate wireless channel access\nschemes.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2015 09:50:05 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2015 09:24:57 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Ryu", "Hyun-Gyu", ""], ["Lee", "Sang-Keum", ""], ["Har", "Dongsoo", ""]]}, {"id": "1510.02391", "submitter": "Yann Busnel", "authors": "Shahedeh Khani, Cristina Gacek, Peter Popov", "title": "Security-aware selection of Web Services for Reliable Composition", "comments": "Yann Busnel. 11th European Dependable Computing Conference (EDCC\n  2015), Sep 2015, Paris, France. 2015, Proceedings of Student Forum - EDCC\n  2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.IT cs.SE math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dependability is an important characteristic that a trustworthy computer\nsystem should have. It is a measure of Availability, Reliability,\nMaintainability, Safety and Security. The focus of our research is on security\nof web services. Web services enable the composition of independent services\nwith complementary functionalities to produce value-added services, which\nallows organizations to implement their core business only and outsource other\nservice components over the Internet, either pre-selected or on-the-fly. The\nselected third party web services may have security vulnerabilities. Vulnerable\nweb services are of limited practical use. We propose to use an\nintrusion-tolerant composite web service for each functionality that should be\nfulfilled by a third party web service. The third party services employed in\nthis approach should be selected based on their security vulnerabilities in\naddition to their performance. The security vulnerabilities of the third party\nservices are assessed using a penetration testing tool. In this paper we\npresent our preliminary research work.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2015 14:54:09 GMT"}], "update_date": "2015-10-09", "authors_parsed": [["Khani", "Shahedeh", ""], ["Gacek", "Cristina", ""], ["Popov", "Peter", ""]]}, {"id": "1510.02614", "submitter": "Dongsoo Har", "authors": "Muhammad Usman, Dongsoo Har, and Insoo Koo", "title": "Energy-Efficient Infrastructure Sensor Network for Ad Hoc Cognitive\n  Radio Network", "comments": "submitted to IEEE sensors journal", "journal-ref": null, "doi": "10.1109/JSEN.2016.2516018", "report-no": null, "categories": "cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an energy-efficient network architecture that consists of ad hoc\n(mobile) cognitive radios (CRs) and infrastructure wireless sensor nodes. The\nsensor nodes within communications range of each CR are grouped into a cluster\nand the clusters of CRs are regularly updated according to the random mobility\nof the CRs. We reduce the energy consumption and the end-to-end delay of the\nsensor network by dividing each cluster into disjoint subsets with overlapped\nsensing coverage of primary user (PU) activity. Respective subset of a CR\nprovides target detection and false alarm probabilities. Substantial energy\nefficiency is achieved by activating only one subset of the cluster, while\nputting the rest of the subsets in the cluster into sleep mode.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2015 10:11:52 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Usman", "Muhammad", ""], ["Har", "Dongsoo", ""], ["Koo", "Insoo", ""]]}, {"id": "1510.02689", "submitter": "Alejandro Erickson", "authors": "Xi Wang and Alejandro Erickson and Jianxi Fan and Xiaohua Jia", "title": "Hamiltonian Properties of DCell Networks", "comments": "To appear", "journal-ref": "Wang, Xi and Erickson, Alejandro and Fan, Jianxi and Jia, Xiaohua,\n  Hamiltonian Properties of DCell Networks. The Computer Journal. 2015", "doi": "10.1093/comjnl/bxv019", "report-no": null, "categories": "cs.DC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DCell has been proposed for data centers as a server centric interconnection\nnetwork structure. DCell can support millions of servers with high network\ncapacity by only using commodity switches. With one exception, we prove that a\n$k$ level DCell built with $n$ port switches is Hamiltonian-connected for $k\n\\geq 0$ and $n \\geq 2$. Our proof extends to all generalized DCell connection\nrules for $n\\ge 3$. Then, we propose an $O(t_k)$ algorithm for finding a\nHamiltonian path in $DCell_{k}$, where $t_k$ is the number of servers in\n$DCell_{k}$. What's more, we prove that $DCell_{k}$ is $(n+k-4)$-fault\nHamiltonian-connected and $(n+k-3)$-fault Hamiltonian. In addition, we show\nthat a partial DCell is Hamiltonian connected if it conforms to a few practical\nrestrictions.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2015 14:42:53 GMT"}], "update_date": "2015-10-12", "authors_parsed": [["Wang", "Xi", ""], ["Erickson", "Alejandro", ""], ["Fan", "Jianxi", ""], ["Jia", "Xiaohua", ""]]}, {"id": "1510.02709", "submitter": "Kairan Sun", "authors": "Kairan Sun, Xu Wei, Gengtao Jia, Risheng Wang, Ruizhi Li", "title": "Large-scale Artificial Neural Network: MapReduce-based Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Faced with continuously increasing scale of data, original back-propagation\nneural network based machine learning algorithm presents two non-trivial\nchallenges: huge amount of data makes it difficult to maintain both efficiency\nand accuracy; redundant data aggravates the system workload. This project is\nmainly focused on the solution to the issues above, combining deep learning\nalgorithm with cloud computing platform to deal with large-scale data. A\nMapReduce-based handwriting character recognizer will be designed in this\nproject to verify the efficiency improvement this mechanism will achieve on\ntraining and practical large-scale data. Careful discussion and experiment will\nbe developed to illustrate how deep learning algorithm works to train\nhandwritten digits data, how MapReduce is implemented on deep learning neural\nnetwork, and why this combination accelerates computation. Besides performance,\nthe scalability and robustness will be mentioned in this report as well. Our\nsystem comes with two demonstration software that visually illustrates our\nhandwritten digit recognition/encoding application.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2015 15:45:44 GMT"}], "update_date": "2015-10-12", "authors_parsed": [["Sun", "Kairan", ""], ["Wei", "Xu", ""], ["Jia", "Gengtao", ""], ["Wang", "Risheng", ""], ["Li", "Ruizhi", ""]]}, {"id": "1510.02975", "submitter": "Guillermo Gallego", "authors": "Daniel Berj\\'on, Guillermo Gallego, Carlos Cuevas, Francisco Mor\\'an\n  and Narciso Garc\\'ia", "title": "Optimal Piecewise Linear Function Approximation for GPU-based\n  Applications", "comments": "12 pages, 12 figures, post-print, IEEE Transactions on Cybernetics,\n  Oct. 2015", "journal-ref": "IEEE Transactions on Cybernetics, vol. 46, no. 11, pp. 2584-2595,\n  Nov. 2016", "doi": "10.1109/TCYB.2015.2482365", "report-no": null, "categories": "math.OC cs.CV cs.DC cs.NA cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many computer vision and human-computer interaction applications developed in\nrecent years need evaluating complex and continuous mathematical functions as\nan essential step toward proper operation. However, rigorous evaluation of this\nkind of functions often implies a very high computational cost, unacceptable in\nreal-time applications. To alleviate this problem, functions are commonly\napproximated by simpler piecewise-polynomial representations. Following this\nidea, we propose a novel, efficient, and practical technique to evaluate\ncomplex and continuous functions using a nearly optimal design of two types of\npiecewise linear approximations in the case of a large budget of evaluation\nsubintervals. To this end, we develop a thorough error analysis that yields\nasymptotically tight bounds to accurately quantify the approximation\nperformance of both representations. It provides an improvement upon previous\nerror estimates and allows the user to control the trade-off between the\napproximation error and the number of evaluation subintervals. To guarantee\nreal-time operation, the method is suitable for, but not limited to, an\nefficient implementation in modern Graphics Processing Units (GPUs), where it\noutperforms previous alternative approaches by exploiting the fixed-function\ninterpolation routines present in their texture units. The proposed technique\nis a perfect match for any application requiring the evaluation of continuous\nfunctions, we have measured in detail its quality and efficiency on several\nfunctions, and, in particular, the Gaussian function because it is extensively\nused in many areas of computer vision and cybernetics, and it is expensive to\nevaluate.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2015 20:49:17 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Berj\u00f3n", "Daniel", ""], ["Gallego", "Guillermo", ""], ["Cuevas", "Carlos", ""], ["Mor\u00e1n", "Francisco", ""], ["Garc\u00eda", "Narciso", ""]]}, {"id": "1510.03145", "submitter": "Ravikant Dindokar", "authors": "Ravikant Dindokar and Yogesh Simmhan", "title": "Elastic Resource Allocation for Distributed Graph Processing Platforms", "comments": null, "journal-ref": "Proceedings of the 16th IEEE/ACM International Symposium on\n  Cluster, Cloud and Grid Computing (CCGrid), Cartagena, Colombia, 2016", "doi": "10.1109/CCGrid.2016.97", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed graph platforms like Pregel have used vertex- centric programming\nmodels to process the growing corpus of graph datasets using commodity\nclusters. The irregular structure of graphs cause load imbalances across\nmachines operating on graph partitions, and this is exacerbated for\nnon-stationary graph algorithms such as traversals, where not all parts of the\ngraph are active at the same time. As a result, such graph platforms, even as\nthey scale, do not make efficient use of distributed resources. Clouds offer\nelastic virtual machines that can be leveraged to improve the resource\nutilization for such platforms and hence reduce the monetary cost for their\nexecution. In this paper, we propose strategies for elastic placement of graph\npartitions on Cloud VMs for subgraphcentric programming model to reduce the\ncost of execution compared to a static placement, even as we minimize the\nincrease in makespan. These strategies are innovative in modeling the graph\nalgorithms behavior. We validate our strategies for several graphs, using\nruntime tra- ces for their distributed execution of a Breadth First Search\n(BFS) algorithms on our subgraph-centric GoFFish graph platform. Our strategies\nare able to reduce the cost of exe- cution by up to 42%, compared to a static\nplacement, while achieving a makespan that is within 29% of the optimal\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2015 05:41:05 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Dindokar", "Ravikant", ""], ["Simmhan", "Yogesh", ""]]}, {"id": "1510.03173", "submitter": "Shafi'i Muhammad Abdulhamid Mr", "authors": "Shafii Muhammad Abdulhamid, Muhammad Shafie Abd Latiff and Ismaila\n  Idris", "title": "Tasks Scheduling Technique Using League Championship Algorithm for\n  Makespan Minimization in IaaS Cloud", "comments": "5 pages, 4 figures", "journal-ref": "ARPN Journal of Engineering and Applied Sciences 9 (12), 2528 -\n  2533 (2014)", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Makespan minimization in tasks scheduling of infrastructure as a service\n(IaaS) cloud is an NP-hard problem. A number of techniques had been used in the\npast to optimize the makespan time of scheduled tasks in IaaS cloud, which is\npropotional to the execution cost billed to customers. In this paper, we\nproposed a League Championship Algorithm (LCA) based makespan time minimization\nscheduling technique in IaaS cloud. The LCA is a sports-inspired population\nbased algorithmic framework for global optimization over a continuous search\nspace. Three other existing algorithms that is, First Come First Served (FCFS),\nLast Job First (LJF) and Best Effort First (BEF) were used to evaluate the\nperformance of the proposed algorithm. All algorithms under consideration\nassumed to be non-preemptive. The results obtained shows that, the LCA\nscheduling technique perform moderately better than the other algorithms in\nminimizing the makespan time of scheduled tasks in IaaS cloud.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2015 08:00:45 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Abdulhamid", "Shafii Muhammad", ""], ["Latiff", "Muhammad Shafie Abd", ""], ["Idris", "Ismaila", ""]]}, {"id": "1510.03354", "submitter": "Julian Araoz PhD", "authors": "Juli\\'an Ar\\'aoz and Cristina Zoltan", "title": "Parallel Triangles Counting Using Pipelining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The generalized method to have a parallel solution to a computational\nproblem, is to find a way to use Divide & Conquer paradigm in order to have\nprocessors acting on its own data and therefore all can be scheduled in\nparallel. MapReduce is an example of this approach: Input data is transformed\nby the mappers, in order to feed the reducers that can run in parallel. In\ngeneral this schema gives efficient problem solutions, but it stops being true\nwhen the replication factor grows. We present another program schema that is\nuseful for describing problem solutions, that can exploit dynamic pipeline\nparallelism without having to deal with replica- tion factors. We present the\nschema in an example: counting triangles in graphs, in particular when the\ngraph do not fit in memory. We describe the solution in NiMo, a graphical\nprogramming language that implements the implicitly functional parallel data ow\nmodel of computation. The solution obtained using NiMo, is architecture\nagnostic and can be deployed in any parallel/distributed architecture adapting\ndynamically the processor usage to input characteristics.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2015 16:21:21 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Ar\u00e1oz", "Juli\u00e1n", ""], ["Zoltan", "Cristina", ""]]}, {"id": "1510.03560", "submitter": "Julien Duchateau", "authors": "Julien Duchateau, Fran\\c{c}ois Rousselle, Nicolas Maquignon, Gilles\n  Roussel, Christophe Renaud", "title": "A progressive mesh method for physical simulations using lattice\n  Boltzmann method on single-node multi-gpu architectures", "comments": "15 pages, International Journal of Distributed and Parallel Systems\n  (IJDPS) Vol.6, No.5, September 2015", "journal-ref": null, "doi": "10.5121/ijdps.2015.6501", "report-no": null, "categories": "cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new progressive mesh algorithm is introduced in order to\nperform fast physical simulations by the use of a lattice Boltzmann method\n(LBM) on a single-node multi-GPU architecture. This algorithm is able to mesh\nautomatically the simulation domain according to the propagation of fluids.\nThis method can also be useful in order to perform various types of simulations\non complex geometries. The use of this algorithm combined with the massive\nparallelism of GPUs allows to obtain very good performance in comparison with\nthe static mesh method used in literature. Several simulations are shown in\norder to evaluate the algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2015 07:32:24 GMT"}], "update_date": "2015-10-14", "authors_parsed": [["Duchateau", "Julien", ""], ["Rousselle", "Fran\u00e7ois", ""], ["Maquignon", "Nicolas", ""], ["Roussel", "Gilles", ""], ["Renaud", "Christophe", ""]]}, {"id": "1510.03676", "submitter": "Peter Elmer", "authors": "David Abdurachmanov, Peter Elmer, Giulio Eulisse, Robert Knight", "title": "Future Computing Platforms for Science in a Power Constrained Era", "comments": "Submitted to proceedings of the 21st International Conference on\n  Computing in High Energy and Nuclear Physics (CHEP2015), Okinawa, Japan", "journal-ref": null, "doi": "10.1088/1742-6596/664/9/092007", "report-no": null, "categories": "cs.DC hep-ex", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power consumption will be a key constraint on the future growth of\nDistributed High Throughput Computing (DHTC) as used by High Energy Physics\n(HEP). This makes performance-per-watt a crucial metric for selecting\ncost-efficient computing solutions. For this paper, we have done a wide survey\nof current and emerging architectures becoming available on the market\nincluding x86-64 variants, ARMv7 32-bit, ARMv8 64-bit, Many-Core and GPU\nsolutions, as well as newer System-on-Chip (SoC) solutions. We compare\nperformance and energy efficiency using an evolving set of standardized\nHEP-related benchmarks and power measurement techniques we have been\ndeveloping. We evaluate the potential for use of such computing solutions in\nthe context of DHTC systems, such as the Worldwide LHC Computing Grid (WLCG).\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2015 13:50:24 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Abdurachmanov", "David", ""], ["Elmer", "Peter", ""], ["Eulisse", "Giulio", ""], ["Knight", "Robert", ""]]}, {"id": "1510.03913", "submitter": "Ubiratam de Paula Junior", "authors": "Ubiratam de Paula and Daniel de Oliveira and Yuri Frota and Valmir C.\n  Barbosa and L\\'ucia Drummond", "title": "Detecting and Handling Flash-Crowd Events on Cloud Environments", "comments": "Submitted to the ACM Transactions on the Web (TWEB)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing is a highly scalable computing paradigm where resources are\ndelivered to users on demand via Internet. There are several areas that can\nbenefit from cloud computing and one in special is gaining much attention: the\nflash-crowd handling. Flash-crowd events happen when servers are unable to\nhandle the volume of requests for a specific content (or a set of contents)\nthat actually reach it, thus causing some requests to be denied. For the\nhandling of flash-crowd events in Web applications, clouds can offer elastic\ncomputing and storage capacity during these events in order to process all\nrequests. However, it is important that flash-crowd events are quickly detected\nand the amount of resources to be instantiated during flash crowds is correctly\nestimated. In this paper, a new mechanism for detection of flash crowds based\non concepts of entropy and total correlation is proposed. Moreover, the\nFlash-Crowd Handling Problem (FCHP) is precisely defined and formulated as an\ninteger programming problem. A new algorithm for solving it, named FCHP-ILS, is\nalso proposed. With FCHP-ILS the Web provider is able to replicate contents in\nthe available resources and define the types and amount of resources to\ninstantiate in the cloud during a flash-crowd event. Finally we present a case\nstudy, based on a synthetic dataset representing flash-crowd events in small\nscenarios aiming at comparing the proposed approach with de facto standard\nAmazon's Auto Scaling mechanism.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2015 22:10:06 GMT"}], "update_date": "2015-10-15", "authors_parsed": [["de Paula", "Ubiratam", ""], ["de Oliveira", "Daniel", ""], ["Frota", "Yuri", ""], ["Barbosa", "Valmir C.", ""], ["Drummond", "L\u00facia", ""]]}, {"id": "1510.04160", "submitter": "Anshu Shukla", "authors": "Yogesh Simmhan, Anshu Shukla and Arun Verma", "title": "Benchmarking Fast Data Platforms for the Aadhaar Biometric Database", "comments": "9 pages,Preprint:To appear in Seventh Workshop on Big Data\n  Benchmarking (WBDB) proceedings (LNCS )", "journal-ref": "Proceedings of the Workshop on Big Data Benchmarks, WBDB, 2015.\n  Lecture Notes in Computer Science, vol 10044", "doi": "10.1007/978-3-319-49748-8_2", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aadhaar is the world's largest biometric database with a billion records,\nbeing compiled as an identity platform to deliver social services to residents\nof India.Aadhaar processes streams of biometric data as residents are enrolled\nand updated.Besides 1 million enrolments and updates per day,up to 100 million\ndaily biometric authentications are expected during delivery of various public\nservices.These form critical Big Data applications,with large volumes and high\nvelocity of data.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2015 15:39:01 GMT"}, {"version": "v2", "created": "Sat, 12 Mar 2016 04:06:11 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Simmhan", "Yogesh", ""], ["Shukla", "Anshu", ""], ["Verma", "Arun", ""]]}, {"id": "1510.04163", "submitter": "Willie Neiswanger", "authors": "Willie Neiswanger, Chong Wang, Eric Xing", "title": "Embarrassingly Parallel Variational Inference in Nonconjugate Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.DC cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a parallel variational inference (VI) procedure for use in\ndata-distributed settings, where each machine only has access to a subset of\ndata and runs VI independently, without communicating with other machines. This\ntype of \"embarrassingly parallel\" procedure has recently been developed for\nMCMC inference algorithms; however, in many cases it is not possible to\ndirectly extend this procedure to VI methods without requiring certain\nrestrictive exponential family conditions on the form of the model.\nFurthermore, most existing (nonparallel) VI methods are restricted to use on\nconditionally conjugate models, which limits their applicability. To combat\nthese issues, we make use of the recently proposed nonparametric VI to\nfacilitate an embarrassingly parallel VI procedure that can be applied to a\nwider scope of models, including to nonconjugate models. We derive our\nembarrassingly parallel VI algorithm, analyze our method theoretically, and\ndemonstrate our method empirically on a few nonconjugate models.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2015 15:48:19 GMT"}], "update_date": "2015-10-15", "authors_parsed": [["Neiswanger", "Willie", ""], ["Wang", "Chong", ""], ["Xing", "Eric", ""]]}, {"id": "1510.04233", "submitter": "Marco Serafini", "authors": "Carlos H. C. Teixeira, Alexandre J. Fonseca, Marco Serafini, Georgos\n  Siganos, Mohammed J. Zaki, Ashraf Aboulnaga", "title": "Arabesque: A System for Distributed Graph Mining - Extended version", "comments": "A short version of this report appeared in the Proceedings of the\n  25th ACM Symp. on Operating Systems Principles (SOSP), 2015", "journal-ref": null, "doi": null, "report-no": "QCRI-TR-2015-005", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed data processing platforms such as MapReduce and Pregel have\nsubstantially simplified the design and deployment of certain classes of\ndistributed graph analytics algorithms. However, these platforms do not\nrepresent a good match for distributed graph mining problems, as for example\nfinding frequent subgraphs in a graph. Given an input graph, these problems\nrequire exploring a very large number of subgraphs and finding patterns that\nmatch some \"interestingness\" criteria desired by the user. These algorithms are\nvery important for areas such as social net- works, semantic web, and\nbioinformatics. In this paper, we present Arabesque, the first distributed data\nprocessing platform for implementing graph mining algorithms. Arabesque\nautomates the process of exploring a very large number of subgraphs. It defines\na high-level filter-process computational model that simplifies the development\nof scalable graph mining algorithms: Arabesque explores subgraphs and passes\nthem to the application, which must simply compute outputs and decide whether\nthe subgraph should be further extended. We use Arabesque's API to produce\ndistributed solutions to three fundamental graph mining problems: frequent\nsubgraph mining, counting motifs, and finding cliques. Our implementations\nrequire a handful of lines of code, scale to trillions of subgraphs, and\nrepresent in some cases the first available distributed solutions.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2015 18:37:30 GMT"}], "update_date": "2015-10-15", "authors_parsed": [["Teixeira", "Carlos H. C.", ""], ["Fonseca", "Alexandre J.", ""], ["Serafini", "Marco", ""], ["Siganos", "Georgos", ""], ["Zaki", "Mohammed J.", ""], ["Aboulnaga", "Ashraf", ""]]}, {"id": "1510.04240", "submitter": "Dongsoo Har", "authors": "Dongsoo Har", "title": "Dementia assistive system as a dense network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As elderly population increases, portion of dementia patients becomes larger.\nThus social cost of caring dementia patients has been a major concern to many\nnations. This article introduces a dementia assistive system operated by\nvarious sensors and devices installed in body area and activity area of\npatients. Since this system is served based on a network which includes a\nnumber of nodes, it requires techniques to reduce the network performance\ndegradation caused by densely composed sensors and devices. This article\nintroduces existing protocols for communications of sensors and devices at both\nlow rate and high rate transmission.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2015 19:05:18 GMT"}], "update_date": "2015-10-15", "authors_parsed": [["Har", "Dongsoo", ""]]}, {"id": "1510.04317", "submitter": "Hung Nghiep Tran", "authors": "Hung Nghiep Tran, Atsuhiro Takasu", "title": "Partitioning Algorithms for Improving Efficiency of Topic Modeling\n  Parallelization", "comments": null, "journal-ref": "IEEE Pacific Rim Conference on Communications, Computers and\n  Signal Processing (PACRIM 2015)", "doi": "10.1109/PACRIM.2015.7334854", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic modeling is a very powerful technique in data analysis and data mining\nbut it is generally slow. Many parallelization approaches have been proposed to\nspeed up the learning process. However, they are usually not very efficient\nbecause of the many kinds of overhead, especially the load-balancing problem.\nWe address this problem by proposing three partitioning algorithms, which\neither run more quickly or achieve better load balance than current\npartitioning algorithms. These algorithms can easily be extended to improve\nparallelization efficiency on other topic models similar to LDA, e.g., Bag of\nTimestamps, which is an extension of LDA with time information. We evaluate\nthese algorithms on two popular datasets, NIPS and NYTimes. We also build a\ndataset containing over 1,000,000 scientific publications in the computer\nscience domain from 1951 to 2010 to experiment with Bag of Timestamps\nparallelization, which we design to demonstrate the proposed algorithms'\nextensibility. The results strongly confirm the advantages of these algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2015 21:10:50 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Tran", "Hung Nghiep", ""], ["Takasu", "Atsuhiro", ""]]}, {"id": "1510.04340", "submitter": "Xiang Sun", "authors": "Xiang Sun and Nirwan Ansari", "title": "PRIMAL: PRofIt Maximization Avatar pLacement for Mobile Edge Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a cloudlet network architecture to bring the computing resources\nfrom the centralized cloud to the edge. Thus, each User Equipment (UE) can\ncommunicate with its Avatar, a software clone located in a cloudlet, with lower\nend-to-end (E2E) delay. However, UEs are moving over time, and so the low E2E\ndelay may not be maintained if UEs' Avatars stay in their original cloudlets.\nThus, live Avatar migration (i.e., migrating a UE's Avatar to a suitable\ncloudlet based on the UE's location) is enabled to maintain low E2E delay\nbetween each UE and its Avatar. On the other hand, the migration itself incurs\nextra overheads in terms of resources of the Avatar, which compromise the\nperformance of applications running in the Avatar. By considering the gain\n(i.e., the E2E delay reduction) and the cost (i.e., the migration overheads) of\nthe live Avatar migration, we propose a PRofIt Maximization Avatar pLacement\n(PRIMAL) strategy for the cloudlet network in order to optimize the tradeoff\nbetween the migration gain and the migration cost by selectively migrating the\nAvatars to their optimal locations. Simulation results demonstrate that as\ncompared to the other two strategies (i.e., Follow Me Avatar and Static),\nPRIMAL maximizes the profit in terms of maintaining the low average E2E delay\nbetween UEs and their Avatars and minimizing the migration cost simultaneously.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2015 22:44:59 GMT"}], "update_date": "2015-10-16", "authors_parsed": [["Sun", "Xiang", ""], ["Ansari", "Nirwan", ""]]}, {"id": "1510.04347", "submitter": "Alan Davoust", "authors": "Alan Davoust and Babak Esfandiari", "title": "Processing Regular Path Queries on Arbitrarily Distributed Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regular Path Queries (RPQs) are a type of graph query where answers are pairs\nof nodes connected by a sequence of edges matching a regular expression. We\nstudy the techniques to process such queries on a distributed graph of data.\nWhile many techniques assume the location of each data element (node or edge)\nis known, when the components of the distributed system are autonomous, the\ndata will be arbitrarily distributed. As the different query processing\nstrategies are equivalently costly in the worst case, we isolate\nquery-dependent cost factors and present a method to choose between strategies,\nusing new query cost estimation techniques. We evaluate our techniques using\nmeaningful queries on biomedical data.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2015 23:31:41 GMT"}], "update_date": "2015-10-16", "authors_parsed": [["Davoust", "Alan", ""], ["Esfandiari", "Babak", ""]]}, {"id": "1510.04686", "submitter": "Daniel Lemus", "authors": "Robert Andrawis, Jose David Bermeo, James Charles, Jianbin Fang, Jim\n  Fonseca, Yu He, Gerhard Klimeck, Zhengping Jiang, Tillmann Kubis, Daniel\n  Mejia, Daniel Lemus, Michael Povolotskyi, Santiago Alonso Perez Rubiano,\n  Prasad Sarangapani, Lang Zeng", "title": "NEMO5: Achieving High-end Internode Communication for Performance\n  Projection Beyond Moore's Law", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic performance predictions of modern nanotransistors require\nnonequilibrium Green's functions including incoherent scattering on phonons as\nwell as inclusion of random alloy disorder and surface roughness effects. The\nsolution of all these effects is numerically extremely expensive and has to be\ndone on the world's largest supercomputers due to the large memory requirement\nand the high performance demands on the communication network between the\ncompute nodes. In this work, it is shown that NEMO5 covers all required\nphysical effects and their combination. Furthermore, it is also shown that\nNEMO5's implementation of the algorithm scales very well up to about 178176CPUs\nwith a sustained performance of about 857 TFLOPS. Therefore, NEMO5 is ready to\nsimulate future nanotransistors.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 19:56:55 GMT"}], "update_date": "2015-10-16", "authors_parsed": [["Andrawis", "Robert", ""], ["Bermeo", "Jose David", ""], ["Charles", "James", ""], ["Fang", "Jianbin", ""], ["Fonseca", "Jim", ""], ["He", "Yu", ""], ["Klimeck", "Gerhard", ""], ["Jiang", "Zhengping", ""], ["Kubis", "Tillmann", ""], ["Mejia", "Daniel", ""], ["Lemus", "Daniel", ""], ["Povolotskyi", "Michael", ""], ["Rubiano", "Santiago Alonso Perez", ""], ["Sarangapani", "Prasad", ""], ["Zeng", "Lang", ""]]}, {"id": "1510.04731", "submitter": "Gauri Joshi", "authors": "Gauri Joshi and Emina Soljanin and Gregory Wornell", "title": "Efficient Replication of Queued Tasks for Latency Reduction in Cloud\n  Systems", "comments": "presented at Allerton 2015. arXiv admin note: substantial text\n  overlap with arXiv:1508.03599", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In cloud computing systems, assigning a job to multiple servers and waiting\nfor the earliest copy to finish is an effective method to combat the\nvariability in response time of individual servers. Although adding redundant\nreplicas always reduces service time, the total computing time spent per job\nmay be higher, thus increasing waiting time in queue. The total time spent per\njob is also proportional to the cost of computing resources. We analyze how\ndifferent redundancy strategies, for eg. number of replicas, and the time when\nthey are issued and canceled, affect the latency and computing cost. We get the\ninsight that the log-concavity of the service time distribution is a key factor\nin determining whether adding redundancy reduces latency and cost. If the\nservice distribution is log-convex, then adding maximum redundancy reduces both\nlatency and cost. And if it is log-concave, then having fewer replicas and\ncanceling the redundant requests early is more effective.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 22:29:42 GMT"}], "update_date": "2015-10-21", "authors_parsed": [["Joshi", "Gauri", ""], ["Soljanin", "Emina", ""], ["Wornell", "Gregory", ""]]}, {"id": "1510.04868", "submitter": "Alexander Thomasian", "authors": "Alexander Thomasian and Jun Xu", "title": "Data Allocation in a Heterogeneous Disk Array - HDA with Multiple RAID\n  Levels for Database Applications", "comments": "IEEE 2-column format", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the allocation of Virtual Arrays (VAs) in a Heterogeneous Disk\nArray (HDA). Each VA holds groups of related objects and datasets such as\nfiles, relational tables, which has similar performance and availability\ncharacteristics. We evaluate single-pass data allocation methods for HDA using\na synthetic stream of allocation requests, where each VA is characterized by\nits RAID level, disk loads and space requirements. The goal is to maximize the\nnumber of allocated VAs and maintain high disk bandwidth and capacity\nutilization, while balancing disk loads. Although only RAID1 (basic mirroring)\nand RAID5 (rotated parity arrays) are considered in the experimental study, we\ndevelop the analysis required to estimate disk loads for other RAID levels.\nSince VA loads vary significantly over time, the VA allocation is carried out\nat the peak load period, while ensuring that disk bandwidth is not exceeded at\nother high load periods. Experimental results with a synthetic stream of\nallocation requests show that allocation methods minimizing the maximum disk\nbandwidth and capacity utilization or their variance across all disks yield the\nmaximum number of allocated VAs. HDA saves disk bandwidth, since a single RAID\nlevel accommodating the most stringent availability requirements for a small\nsubset of objects would incur an unnecessarily high overhead for updating check\nblocks or data replicas for all objects. The number of allocated VAs can be\nincreased by adopting the clustered RAID5 paradigm, which exploits the tradeoff\nbetween redundancy and bandwidth utilization. Since rebuild can be carried out\nat the level of individual VAs, prioritizing rebuild of VAs with higher access\nrates can improve overall performance.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2015 12:58:15 GMT"}], "update_date": "2015-10-19", "authors_parsed": [["Thomasian", "Alexander", ""], ["Xu", "Jun", ""]]}, {"id": "1510.04914", "submitter": "Charlie Vanaret", "authors": "Charlie Vanaret and Jean-Baptiste Gotteland and Nicolas Durand and\n  Jean-Marc Alliot", "title": "Hybridization of Interval CP and Evolutionary Algorithms for Optimizing\n  Difficult Problems", "comments": "21st International Conference on Principles and Practice of\n  Constraint Programming (CP 2015), 2015", "journal-ref": null, "doi": "10.1007/978-3-319-23219-5_32", "report-no": null, "categories": "cs.AI cs.DC cs.MS math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The only rigorous approaches for achieving a numerical proof of optimality in\nglobal optimization are interval-based methods that interleave branching of the\nsearch-space and pruning of the subdomains that cannot contain an optimal\nsolution. State-of-the-art solvers generally integrate local optimization\nalgorithms to compute a good upper bound of the global minimum over each\nsubspace. In this document, we propose a cooperative framework in which\ninterval methods cooperate with evolutionary algorithms. The latter are\nstochastic algorithms in which a population of candidate solutions iteratively\nevolves in the search-space to reach satisfactory solutions.\n  Within our cooperative solver Charibde, the evolutionary algorithm and the\ninterval-based algorithm run in parallel and exchange bounds, solutions and\nsearch-space in an advanced manner via message passing. A comparison of\nCharibde with state-of-the-art interval-based solvers (GlobSol, IBBA, Ibex) and\nNLP solvers (Couenne, BARON) on a benchmark of difficult COCONUT problems shows\nthat Charibde is highly competitive against non-rigorous solvers and converges\nfaster than rigorous solvers by an order of magnitude.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2015 15:18:42 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Vanaret", "Charlie", ""], ["Gotteland", "Jean-Baptiste", ""], ["Durand", "Nicolas", ""], ["Alliot", "Jean-Marc", ""]]}, {"id": "1510.04995", "submitter": "Tareq Malas", "authors": "Tareq Malas, Georg Hager, Hatem Ltaief and David Keyes", "title": "Multi-dimensional intra-tile parallelization for memory-starved stencil\n  computations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimizing the performance of stencil algorithms has been the subject of\nintense research over the last two decades. Since many stencil schemes have low\narithmetic intensity, most optimizations focus on increasing the temporal data\naccess locality, thus reducing the data traffic through the main memory\ninterface with the ultimate goal of decoupling from this bottleneck. There are,\nhowever, only few approaches that explicitly leverage the shared cache feature\nof modern multicore chips. If every thread works on its private, separate cache\nblock, the available cache space can become too small, and sufficient temporal\nlocality may not be achieved.\n  We propose a flexible multi-dimensional intra-tile parallelization method for\nstencil algorithms on multicore CPUs with a shared outer-level cache. This\nmethod leads to a significant reduction in the required cache space without\nadverse effects from hardware prefetching or TLB shortage. Our \\emph{Girih}\nframework includes an auto-tuner to select optimal parameter configurations on\nthe target hardware. We conduct performance experiments on two contemporary\nIntel processors and compare with the state-of-the-art stencil frameworks PLUTO\nand Pochoir, using four corner-case stencil schemes and a wide range of problem\nsizes. \\emph{Girih} shows substantial performance advantages and best\narithmetic intensity at almost all problem sizes, especially on low-intensity\nstencils with variable coefficients. We study in detail the performance\nbehavior at varying grid size using phenomenological performance modeling. Our\nanalysis of energy consumption reveals that our method can save energy by\nreduced DRAM bandwidth usage even at marginal performance gain. It is thus well\nsuited for future architectures that will be strongly challenged by the cost of\ndata movement, be it in terms of performance or energy consumption.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2015 19:43:00 GMT"}], "update_date": "2015-10-19", "authors_parsed": [["Malas", "Tareq", ""], ["Hager", "Georg", ""], ["Ltaief", "Hatem", ""], ["Keyes", "David", ""]]}, {"id": "1510.05041", "submitter": "Linnan Wang", "authors": "Linnan Wang, Wei Wu, Jianxiong Xiao, Yi Yang", "title": "BLASX: A High Performance Level-3 BLAS Library for Heterogeneous\n  Multi-GPU Computing", "comments": "under review for IPDPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Basic Linear Algebra Subprograms (BLAS) are a set of low level linear algebra\nkernels widely adopted by applications involved with the deep learning and\nscientific computing. The massive and economic computing power brought forth by\nthe emerging GPU architectures drives interest in implementation of\ncompute-intensive level 3 BLAS on multi-GPU systems. In this paper, we\ninvestigate existing multi-GPU level 3 BLAS and present that 1) issues, such as\nthe improper load balancing, inefficient communication, insufficient GPU stream\nlevel concurrency and data caching, impede current implementations from fully\nharnessing heterogeneous computing resources; 2) and the inter-GPU\nPeer-to-Peer(P2P) communication remains unexplored. We then present BLASX: a\nhighly optimized multi-GPU level-3 BLAS. We adopt the concepts of\nalgorithms-by-tiles treating a matrix tile as the basic data unit and\noperations on tiles as the basic task. Tasks are guided with a dynamic\nasynchronous runtime, which is cache and locality aware. The communication cost\nunder BLASX becomes trivial as it perfectly overlaps communication and\ncomputation across multiple streams during asynchronous task progression. It\nalso takes the current tile cache scheme one step further by proposing an\ninnovative 2-level hierarchical tile cache, taking advantage of inter-GPU P2P\ncommunication. As a result, linear speedup is observable with BLASX under\nmulti-GPU configurations; and the extensive benchmarks demonstrate that BLASX\nconsistently outperforms the related leading industrial and academic projects\nsuch as cuBLAS-XT, SuperMatrix, MAGMA and PaRSEC.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2015 22:34:24 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Wang", "Linnan", ""], ["Wu", "Wei", ""], ["Xiao", "Jianxiong", ""], ["Yang", "Yi", ""]]}, {"id": "1510.05107", "submitter": "Julien Langou", "authors": "Willy Quach and Julien Langou", "title": "A Makespan Lower Bound for the Scheduling of the Tiled Cholesky\n  Factorization based on ALAP scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the advent of multicore architectures and massive parallelism, the\ntiled Cholesky factorization algorithm has recently received plenty of\nattention and is often referenced by practitioners as a case study. It is also\nimplemented in mainstream dense linear algebra libraries. However, we note that\ntheoretical study of the parallelism of this algorithm is currently lacking. In\nthis paper, we present new theoretical results about the tiled Cholesky\nfactorization in the context of a parallel homogeneous model without\ncommunication costs. We use standard flop-based weights for the tasks. For a\n$t$-by-$t$ matrix, we know that the critical path of the tiled Cholesky\nalgorithm is $9t-10$ and that the weight of all tasks is $t^3$. In this\ncontext, we prove that no schedule with less than $0.185 t^2$ processing units\ncan finish in a time less than the critical path. In perspective, a naive bound\ngives $0.11 t^2.$ We then give a schedule which needs less than $0.25\nt^2+0.16t+3$ processing units to complete in the time of the critical path. In\nperspective, a naive schedule gives $0.50 t^2.$ In addition, given a fixed\nnumber of processing units, $p$, we give a lower bound on the execution time as\nfollows: $$\\max( \\frac{t^{3}}{p}, \\frac{t^{3}}{p} - 3\\frac{t^2}{p} + 6\\sqrt{2p}\n- 7 , 9t-10).$$ The interest of the latter formula lies in the middle term. Our\nresults stem from the observation that the tiled Cholesky factorization is much\nbetter behaved when we schedule it with an ALAP (As Late As Possible) heuristic\nthan an ASAP (As Soon As Possible) heuristic. We also provide scheduling\nheuristics which match closely the lower bound on execution time. We believe\nthat our theoretical results will help practical scheduling studies. Indeed,\nour results enable to better characterize the quality of a practical schedule\nwith respect to an optimal schedule.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2015 09:18:39 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Quach", "Willy", ""], ["Langou", "Julien", ""]]}, {"id": "1510.05182", "submitter": "Fereydoun Farrahi Moghaddam", "authors": "Fereydoun Farrahi Moghaddam and Mohamed Cheriet", "title": "Sustainability-Aware Cloud Computing Using Virtual Carbon Tax", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a solution for sustainable cloud system is proposed and then\nimplemented on a real testbed. The solution composes of optimization of a\nprofit model and introduction of virtual carbon tax to limit environmental\nfootprint of the cloud. The proposed multi-criteria optimizer of the cloud\nsystem suggests new optimum CPU frequencies for CPU-cores when the local grid\nenergy mix or the cloud workload changes. The cloud system is implemented on a\nblade system, and proper middlewares are developed to interact with the blades.\nThe experimental results show that it is possible to significantly decrease the\ntargeted environmental footprint of the system and keep it profitable.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2015 23:33:20 GMT"}, {"version": "v2", "created": "Wed, 1 Nov 2017 12:55:42 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Moghaddam", "Fereydoun Farrahi", ""], ["Cheriet", "Mohamed", ""]]}, {"id": "1510.05218", "submitter": "Tareq Malas", "authors": "Tareq M. Malas, Julian Hornich, Georg Hager, Hatem Ltaief, Christoph\n  Pflaum and David E. Keyes", "title": "Optimization of an electromagnetics code with multicore wavefront\n  diamond blocking and multi-dimensional intra-tile parallelization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding and optimizing the properties of solar cells is becoming a key\nissue in the search for alternatives to nuclear and fossil energy sources. A\ntheoretical analysis via numerical simulations involves solving Maxwell's\nEquations in discretized form and typically requires substantial computing\neffort. We start from a hybrid-parallel (MPI+OpenMP) production code that\nimplements the Time Harmonic Inverse Iteration Method (THIIM) with\nFinite-Difference Frequency Domain (FDFD) discretization. Although this\nalgorithm has the characteristics of a strongly bandwidth-bound stencil update\nscheme, it is significantly different from the popular stencil types that have\nbeen exhaustively studied in the high performance computing literature to date.\nWe apply a recently developed stencil optimization technique, multicore\nwavefront diamond tiling with multi-dimensional cache block sharing, and\ndescribe in detail the peculiarities that need to be considered due to the\nspecial stencil structure. Concurrency in updating the components of the\nelectric and magnetic fields provides an additional level of parallelism. The\ndependence of the cache size requirement of the optimized code on the blocking\nparameters is modeled accurately, and an auto-tuner searches for optimal\nconfigurations in the remaining parameter space. We were able to completely\ndecouple the execution from the memory bandwidth bottleneck, accelerating the\nimplementation by a factor of three to four compared to an optimal\nimplementation with pure spatial blocking on an 18-core Intel Haswell CPU.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2015 10:13:47 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Malas", "Tareq M.", ""], ["Hornich", "Julian", ""], ["Hager", "Georg", ""], ["Ltaief", "Hatem", ""], ["Pflaum", "Christoph", ""], ["Keyes", "David E.", ""]]}, {"id": "1510.05454", "submitter": "Daniel Jung", "authors": "Sebastian Abshoff, Andreas Cord-Landwehr, Matthias Fischer, Daniel\n  Jung, Friedhelm Meyer auf der Heide", "title": "Gathering a Closed Chain of Robots on a Grid", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following variant of the two dimensional gathering problem\nfor swarms of robots: Given a swarm of $n$ indistinguishable, point shaped\nrobots on a two dimensional grid. Initially, the robots form a closed chain on\nthe grid and must keep this connectivity during the whole process of their\ngathering. Connectivity means, that neighboring robots of the chain need to be\npositioned at the same or neighboring points of the grid. In our model,\ngathering means to keep shortening the chain until the robots are located\ninside a $2\\times 2$ subgrid. Our model is completely local (no global control,\nno global coordinates, no compass, no global communication or vision, \\ldots).\nEach robot can only see its next constant number of left and right neighbors on\nthe chain. This fixed constant is called the \\emph{viewing path length}. All\nits operations and detections are restricted to this constant number of robots.\nOther robots, even if located at neighboring or the same grid point cannot be\ndetected. Only based on the relative positions of its detectable chain\nneighbors, a robot can decide to obtain a certain state. Based on this state\nand their local knowledge, the robots do local modifications to the chain by\nmoving to neighboring grid points without breaking the chain. These\nmodifications are performed without the knowledge whether they lead to a global\nprogress or not. We assume the fully synchronous $\\mathcal{FSYNC}$ model. For\nthis problem, we present a gathering algorithm which needs linear time. This\nresult generalizes the result from \\cite{hopper}, where an open chain with\nspecified distinguishable (and fixed) endpoints is considered.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2015 13:07:39 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Abshoff", "Sebastian", ""], ["Cord-Landwehr", "Andreas", ""], ["Fischer", "Matthias", ""], ["Jung", "Daniel", ""], ["der Heide", "Friedhelm Meyer auf", ""]]}, {"id": "1510.05546", "submitter": "Bei  Wang", "authors": "Bei Wang and Stephane Ethier and William Tang and Khaled Ibrahim and\n  Kamesh Madduri and Samuel Williams and Leonid Oliker", "title": "Modern Gyrokinetic Particle-In-Cell Simulation of Fusion Plasmas on Top\n  Supercomputers", "comments": "submitted to International Journal of High Performance Computing\n  Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC physics.comp-ph physics.plasm-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gyrokinetic Toroidal Code at Princeton (GTC-P) is a highly scalable and\nportable particle-in-cell (PIC) code. It solves the 5D Vlasov-Poisson equation\nfeaturing efficient utilization of modern parallel computer architectures at\nthe petascale and beyond. Motivated by the goal of developing a modern code\ncapable of dealing with the physics challenge of increasing problem size with\nsufficient resolution, new thread-level optimizations have been introduced as\nwell as a key additional domain decomposition. GTC-P's multiple levels of\nparallelism, including inter-node 2D domain decomposition and particle\ndecomposition, as well as intra-node shared memory partition and vectorization\nhave enabled pushing the scalability of the PIC method to extreme computational\nscales. In this paper, we describe the methods developed to build a highly\nparallelized PIC code across a broad range of supercomputer designs. This\nparticularly includes implementations on heterogeneous systems using NVIDIA GPU\naccelerators and Intel Xeon Phi (MIC) co-processors and performance comparisons\nwith state-of-the-art homogeneous HPC systems such as Blue Gene/Q. New\ndiscovery science capabilities in the magnetic fusion energy application domain\nare enabled, including investigations of Ion-Temperature-Gradient (ITG) driven\nturbulence simulations with unprecedented spatial resolution and long temporal\nduration. Performance studies with realistic fusion experimental parameters are\ncarried out on multiple supercomputing systems spanning a wide range of cache\ncapacities, cache-sharing configurations, memory bandwidth, interconnects and\nnetwork topologies. These performance comparisons using a realistic\ndiscovery-science-capable domain application code provide valuable insights on\noptimization techniques across one of the broadest sets of current high-end\ncomputing platforms worldwide.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2015 15:55:00 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Wang", "Bei", ""], ["Ethier", "Stephane", ""], ["Tang", "William", ""], ["Ibrahim", "Khaled", ""], ["Madduri", "Kamesh", ""], ["Williams", "Samuel", ""], ["Oliker", "Leonid", ""]]}, {"id": "1510.05714", "submitter": "Muhammad Anis Uddin Nasir", "authors": "Muhammad Anis Uddin Nasir, Gianmarco De Francisci Morales, Nicolas\n  Kourtellis, Marco Serafini", "title": "When Two Choices Are not Enough: Balancing at Scale in Distributed\n  Stream Processing", "comments": "12 pages, 14 Figures, this paper is accepted and will be published at\n  ICDE 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Carefully balancing load in distributed stream processing systems has a\nfundamental impact on execution latency and throughput. Load balancing is\nchallenging because real-world workloads are skewed: some tuples in the stream\nare associated to keys which are significantly more frequent than others. Skew\nis remarkably more problematic in large deployments: more workers implies fewer\nkeys per worker, so it becomes harder to \"average out\" the cost of hot keys\nwith cold keys.\n  We propose a novel load balancing technique that uses a heaving hitter\nalgorithm to efficiently identify the hottest keys in the stream. These hot\nkeys are assigned to $d \\geq 2$ choices to ensure a balanced load, where $d$ is\ntuned automatically to minimize the memory and computation cost of operator\nreplication. The technique works online and does not require the use of routing\ntables. Our extensive evaluation shows that our technique can balance\nreal-world workloads on large deployments, and improve throughput and latency\nby $\\mathbf{150\\%}$ and $\\mathbf{60\\%}$ respectively over the previous\nstate-of-the-art when deployed on Apache Storm.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2015 23:05:09 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2015 08:15:41 GMT"}, {"version": "v3", "created": "Wed, 27 Jan 2016 15:29:29 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Nasir", "Muhammad Anis Uddin", ""], ["Morales", "Gianmarco De Francisci", ""], ["Kourtellis", "Nicolas", ""], ["Serafini", "Marco", ""]]}, {"id": "1510.06113", "submitter": "Lex Fridman", "authors": "Lex Fridman, Daniel E Brown, William Angell, Irman Abdi\\'c, Bryan\n  Reimer, Hae Young Noh", "title": "Automated Synchronization of Driving Data Using Vibration and Steering\n  Events", "comments": "Accepted for Publication in Elsevier Pattern Recognition Letters", "journal-ref": null, "doi": "10.1016/j.patrec.2016.02.011", "report-no": null, "categories": "cs.RO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for automated synchronization of vehicle sensors useful\nfor the study of multi-modal driver behavior and for the design of advanced\ndriver assistance systems. Multi-sensor decision fusion relies on synchronized\ndata streams in (1) the offline supervised learning context and (2) the online\nprediction context. In practice, such data streams are often out of sync due to\nthe absence of a real-time clock, use of multiple recording devices, or\nimproper thread scheduling and data buffer management. Cross-correlation of\naccelerometer, telemetry, audio, and dense optical flow from three video\nsensors is used to achieve an average synchronization error of 13 milliseconds.\nThe insight underlying the effectiveness of the proposed approach is that the\ndescribed sensors capture overlapping aspects of vehicle vibrations and vehicle\nsteering allowing the cross-correlation function to serve as a way to compute\nthe delay shift in each sensor. Furthermore, we show the decrease in\nsynchronization error as a function of the duration of the data stream.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2015 02:27:10 GMT"}, {"version": "v2", "created": "Tue, 1 Mar 2016 18:01:34 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Fridman", "Lex", ""], ["Brown", "Daniel E", ""], ["Angell", "William", ""], ["Abdi\u0107", "Irman", ""], ["Reimer", "Bryan", ""], ["Noh", "Hae Young", ""]]}, {"id": "1510.06358", "submitter": "Maximilian Imgrund", "authors": "Maximilian Imgrund, Alexander Arth", "title": "Rambrain - a library for virtually extending physical memory", "comments": "17 pages, 8 coloured figures, 3 listings, 1 table; Submitted to\n  SoftwareX", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Rambrain, a user space library that manages memory consumption\nof your code. Using Rambrain you can overcommit memory over the size of\nphysical memory present in the system. Rambrain takes care of temporarily\nswapping out data to disk and can handle multiples of the physical memory size\npresent. Rambrain is thread-safe, OpenMP and MPI compatible and supports\nAsynchronous IO. The library was designed to require minimal changes to\nexisting programs and to be easy to use.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2015 20:19:08 GMT"}, {"version": "v2", "created": "Wed, 7 Jun 2017 16:23:32 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Imgrund", "Maximilian", ""], ["Arth", "Alexander", ""]]}, {"id": "1510.06486", "submitter": "Rajkumar Buyya", "authors": "Rajkumar Buyya, Kotagiri Ramamohanarao, Chris Leckie, Rodrigo N.\n  Calheiros, Amir Vahid Dastjerdi, and Steve Versteeg", "title": "Big Data Analytics-Enhanced Cloud Computing: Challenges, Architectural\n  Elements, and Future Directions", "comments": "10 pages, 2 figures, conference paper in Proceedings of the 21st IEEE\n  International Conference on Parallel and Distributed Systems (ICPADS 2015,\n  IEEE Press, USA), Melbourne, Australia, December 14-17, 2015", "journal-ref": null, "doi": "10.1109/ICPADS.2015.18", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of cloud computing has made dynamic provisioning of elastic\ncapacity to applications on-demand. Cloud data centers contain thousands of\nphysical servers hosting orders of magnitude more virtual machines that can be\nallocated on demand to users in a pay-as-you-go model. However, not all systems\nare able to scale up by just adding more virtual machines. Therefore, it is\nessential, even for scalable systems, to project workloads in advance rather\nthan using a purely reactive approach. Given the scale of modern cloud\ninfrastructures generating real time monitoring information, along with all the\ninformation generated by operating systems and applications, this data poses\nthe issues of volume, velocity, and variety that are addressed by Big Data\napproaches. In this paper, we investigate how utilization of Big Data analytics\nhelps in enhancing the operation of cloud computing environments. We discuss\ndiverse applications of Big Data analytics in clouds, open issues for enhancing\ncloud operations via Big Data analytics, and architecture for anomaly detection\nand prevention in clouds along with future research directions.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2015 04:07:55 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Buyya", "Rajkumar", ""], ["Ramamohanarao", "Kotagiri", ""], ["Leckie", "Chris", ""], ["Calheiros", "Rodrigo N.", ""], ["Dastjerdi", "Amir Vahid", ""], ["Versteeg", "Steve", ""]]}, {"id": "1510.06549", "submitter": "Aaron Li", "authors": "Aaron Q Li", "title": "Multi-GPU Distributed Parallel Bayesian Differential Topic Modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an explosion of data, documents, and other content, and people\nrequire tools to analyze and interpret these, tools to turn the content into\ninformation and knowledge. Topic modeling have been developed to solve these\nproblems. Topic models such as LDA [Blei et. al. 2003] allow salient patterns\nin data to be extracted automatically. When analyzing texts, these patterns are\ncalled topics. Among numerous extensions of LDA, few of them can reliably\nanalyze multiple groups of documents and extract topic similarities. Recently,\nthe introduction of differential topic modeling (SPDP) [Chen et. al. 2012]\nperforms uniformly better than many topic models in a discriminative setting.\n  There is also a need to improve the sampling speed for topic models. While\nsome effort has been made for distributed algorithms, there is no work\ncurrently done using graphical processing units (GPU). Note the GPU framework\nhas already become the most cost-efficient platform for many problems.\n  In this thesis, I propose and implement a scalable multi-GPU distributed\nparallel framework which approximates SPDP. Through experiments, I have shown\nmy algorithms have a gain in speed of about 50 times while being almost as\naccurate, with only one single cheap laptop GPU. Furthermore, I have shown the\nspeed improvement is sublinearly scalable when multiple GPUs are used, while\nfairly maintaining the accuracy. Therefore on a medium-sized GPU cluster, the\nspeed improvement could potentially reach a factor of a thousand.\n  Note SPDP is just a representative of other extensions of LDA. Although my\nalgorithm is implemented to work with SPDP, it is designed to be a general\nenough to work with other topic models. The speed-up on smaller collections\n(i.e., 1000s of documents), means that these more complex LDA extensions could\nnow be done in real-time, thus opening up a new way of using these LDA models\nin industry.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2015 09:40:54 GMT"}], "update_date": "2015-10-23", "authors_parsed": [["Li", "Aaron Q", ""]]}, {"id": "1510.06585", "submitter": "Herv\\'e Paulino", "authors": "F\\'abio Soldado, Fernando Alexandre, Herv\\'e Paulino", "title": "Execution of Compound Multi-Kernel OpenCL Computations in\n  Multi-CPU/Multi-GPU Environments", "comments": "in Concurrency Computat.: Pract. Exper., 2015", "journal-ref": null, "doi": "10.1002/cpe.3612", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current computational systems are heterogeneous by nature, featuring a\ncombination of CPUs and GPUs. As the latter are becoming an established\nplatform for high-performance computing, the focus is shifting towards the\nseamless programming of these hybrid systems as a whole. The distinct nature of\nthe architectural and execution models in place raises several challenges, as\nthe best hardware configuration is behaviour and workload dependent. In this\npaper, we address the execution of compound, multi-kernel, OpenCL computations\nin multi-CPU/multi-GPU environments. We address how these computations may be\nefficiently scheduled onto the target hardware, and how the system may adapt\nitself to changes in the workload to process and to fluctuations in the CPU's\nload. An experimental evaluation attests the performance gains obtained by the\nconjoined use of the CPU and GPU devices, when compared to GPU-only executions,\nand also by the use of data-locality optimizations in CPU environments.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2015 11:55:04 GMT"}], "update_date": "2015-10-23", "authors_parsed": [["Soldado", "F\u00e1bio", ""], ["Alexandre", "Fernando", ""], ["Paulino", "Herv\u00e9", ""]]}, {"id": "1510.06689", "submitter": "Grey Ballard", "authors": "Woody Austin and Grey Ballard and Tamara G. Kolda", "title": "Parallel Tensor Compression for Large-Scale Scientific Data", "comments": null, "journal-ref": "IPDPS'16: Proceedings of the 30th IEEE International Parallel and\n  Distributed Processing Symposium, pp. 912-922, May 2016", "doi": "10.1109/IPDPS.2016.67", "report-no": null, "categories": "cs.NA cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As parallel computing trends towards the exascale, scientific data produced\nby high-fidelity simulations are growing increasingly massive. For instance, a\nsimulation on a three-dimensional spatial grid with 512 points per dimension\nthat tracks 64 variables per grid point for 128 time steps yields 8~TB of data,\nassuming double precision. By viewing the data as a dense five-way tensor, we\ncan compute a Tucker decomposition to find inherent low-dimensional multilinear\nstructure, achieving compression ratios of up to 5000 on real-world data sets\nwith negligible loss in accuracy. So that we can operate on such massive data,\nwe present the first-ever distributed-memory parallel implementation for the\nTucker decomposition, whose key computations correspond to parallel linear\nalgebra operations, albeit with nonstandard data layouts. Our approach\nspecifies a data distribution for tensors that avoids any tensor data\nredistribution, either locally or in parallel. We provide accompanying analysis\nof the computation and communication costs of the algorithms. To demonstrate\nthe compression and accuracy of the method, we apply our approach to real-world\ndata sets from combustion science simulations. We also provide detailed\nperformance results, including parallel performance in both weak and strong\nscaling experiments.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2015 17:06:12 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2016 20:55:59 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Austin", "Woody", ""], ["Ballard", "Grey", ""], ["Kolda", "Tamara G.", ""]]}, {"id": "1510.06706", "submitter": "Aleksandar Zlateski", "authors": "Aleksandar Zlateski, Kisuk Lee and H. Sebastian Seung", "title": "ZNN - A Fast and Scalable Algorithm for Training 3D Convolutional\n  Networks on Multi-Core and Many-Core Shared Memory Machines", "comments": null, "journal-ref": null, "doi": "10.1109/IPDPS.2016.119", "report-no": null, "categories": "cs.NE cs.CV cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional networks (ConvNets) have become a popular approach to computer\nvision. It is important to accelerate ConvNet training, which is\ncomputationally costly. We propose a novel parallel algorithm based on\ndecomposition into a set of tasks, most of which are convolutions or FFTs.\nApplying Brent's theorem to the task dependency graph implies that linear\nspeedup with the number of processors is attainable within the PRAM model of\nparallel computation, for wide network architectures. To attain such\nperformance on real shared-memory machines, our algorithm computes convolutions\nconverging on the same node of the network with temporal locality to reduce\ncache misses, and sums the convergent convolution outputs via an almost\nwait-free concurrent method to reduce time spent in critical sections. We\nimplement the algorithm with a publicly available software package called ZNN.\nBenchmarking with multi-core CPUs shows that ZNN can attain speedup roughly\nequal to the number of physical cores. We also show that ZNN can attain over\n90x speedup on a many-core CPU (Xeon Phi Knights Corner). These speedups are\nachieved for network architectures with widths that are in common use. The task\nparallelism of the ZNN algorithm is suited to CPUs, while the SIMD parallelism\nof previous algorithms is compatible with GPUs. Through examples, we show that\nZNN can be either faster or slower than certain GPU implementations depending\non specifics of the network architecture, kernel sizes, and density and size of\nthe output patch. ZNN may be less costly to develop and maintain, due to the\nrelative ease of general-purpose CPU programming.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2015 18:14:42 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Zlateski", "Aleksandar", ""], ["Lee", "Kisuk", ""], ["Seung", "H. Sebastian", ""]]}, {"id": "1510.06882", "submitter": "Damien Imbs", "authors": "Damien Imbs and Michel Raynal", "title": "Simple and Efficient Reliable Broadcast in the Presence of Byzantine\n  Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a surprisingly simple and efficient reliable broadcast\nalgorithm for asynchronous message-passing systems made up of $n$ processes,\namong which up to $t<n/3$ may behave arbitrarily (Byzantine processes). This\nalgorithm requires two communication steps and $n^2$ messages. (The best\nalgorithm known so far requires three communication steps and $2n^2$ messages.)\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2015 10:29:19 GMT"}], "update_date": "2015-10-26", "authors_parsed": [["Imbs", "Damien", ""], ["Raynal", "Michel", ""]]}, {"id": "1510.06937", "submitter": "Roger Schaer", "authors": "Dimitrios Markonis, Roger Schaer, Ivan Eggel, Henning M\\\"uller, Adrien\n  Depeursinge", "title": "Using MapReduce for Large-scale Medical Image Analysis", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growth of the amount of medical image data produced on a daily basis in\nmodern hospitals forces the adaptation of traditional medical image analysis\nand indexing approaches towards scalable solutions. The number of images and\ntheir dimensionality increased dramatically during the past 20 years. We\npropose solutions for large-scale medical image analysis based on parallel\ncomputing and algorithm optimization. The MapReduce framework is used to speed\nup and make possible three large-scale medical image processing use-cases: (i)\nparameter optimization for lung texture segmentation using support vector\nmachines, (ii) content-based medical image indexing, and (iii)\nthree-dimensional directional wavelet analysis for solid texture\nclassification. A cluster of heterogeneous computing nodes was set up in our\ninstitution using Hadoop allowing for a maximum of 42 concurrent map tasks. The\nmajority of the machines used are desktop computers that are also used for\nregular office work. The cluster showed to be minimally invasive and stable.\nThe runtimes of each of the three use-case have been significantly reduced when\ncompared to a sequential execution. Hadoop provides an easy-to-employ framework\nfor data analysis tasks that scales well for many tasks but requires\noptimization for specific tasks.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2015 14:16:09 GMT"}], "update_date": "2015-10-26", "authors_parsed": [["Markonis", "Dimitrios", ""], ["Schaer", "Roger", ""], ["Eggel", "Ivan", ""], ["M\u00fcller", "Henning", ""], ["Depeursinge", "Adrien", ""]]}, {"id": "1510.06967", "submitter": "Anshu S Anand", "authors": "Anshu S Anand, R K Shyamasundar, Sathya Peri", "title": "Opacity Proof for CaPR+ Algorithm", "comments": "arXiv admin note: text overlap with arXiv:1307.8256", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe an enhanced Automatic Check- pointing and Partial\nRollback algorithm(CaP R + ) to realize Software Transactional Memory(STM) that\nis based on con- tinuous conflict detection, lazy versioning with automatic\ncheckpointing, and partial rollback. Further, we provide a proof of correctness\nof CaP R+ algorithm, in particular, Opacity, a STM correctness criterion, that\nprecisely captures the intuitive correctness guarantees required of\ntransactional memories. The algorithm provides a natural way to realize a\nhybrid system of pure aborts and partial rollbacks. We have also implemented\nthe algorithm, and shown its effectiveness with reference to the Red-black tree\nmicro-benchmark and STAMP benchmarks. The results obtained demonstrate the\neffectiveness of the Partial Rollback mechanism over pure abort mechanisms,\nparticularly in applications consisting of large transaction lengths.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2015 15:25:05 GMT"}], "update_date": "2015-10-26", "authors_parsed": [["Anand", "Anshu S", ""], ["Shyamasundar", "R K", ""], ["Peri", "Sathya", ""]]}, {"id": "1510.07035", "submitter": "Aaron Li", "authors": "Joseph W Robinson, Aaron Q Li", "title": "Fast Latent Variable Models for Inference and Visualization on Mobile\n  Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this project we outline Vedalia, a high performance distributed network\nfor performing inference on latent variable models in the context of Amazon\nreview visualization. We introduce a new model, RLDA, which extends Latent\nDirichlet Allocation (LDA) [Blei et al., 2003] for the review space by\nincorporating auxiliary data available in online reviews to improve modeling\nwhile simultaneously remaining compatible with pre-existing fast sampling\ntechniques such as [Yao et al., 2009; Li et al., 2014a] to achieve high\nperformance. The network is designed such that computation is efficiently\noffloaded to the client devices using the Chital system [Robinson & Li, 2015],\nimproving response times and reducing server costs. The resulting system is\nable to rapidly compute a large number of specialized latent variable models\nwhile requiring minimal server resources.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2015 05:26:09 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Robinson", "Joseph W", ""], ["Li", "Aaron Q", ""]]}, {"id": "1510.07148", "submitter": "Abhinav Singh", "authors": "Abhinav Singh, Awadhesh Kumar Singh", "title": "Mobility and Energy Conscious Clustering Protocol for Wireless Networks", "comments": "10 pages, International Congress on Information and Communication\n  Technology(ICICT-2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a distributed clustering protocol for mobile\nwireless sensor networks. A large majority of research in clustering and\nrouting algorithms for WSNs assume a static network and hence are rendered\ninefficient in cases of highly mobile sensor networks, which is an aspect\naddressed here. MECP is an energy efficient, mobility aware protocol and\nutilizes information about movement of sensor nodes and residual energy as\nattributes in network formation. It also provides a mechanism for fault\ntolerance to decrease packet data loss in case of cluster head failures.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2015 13:50:14 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Singh", "Abhinav", ""], ["Singh", "Awadhesh Kumar", ""]]}, {"id": "1510.07250", "submitter": "Chathura Sarathchandra Magurawalage", "authors": "Chathura Sarathchandra Magurawalage, Kun Yang, Kezhi Wang", "title": "Aqua Computing: Coupling Computing and Communications", "comments": "A shorter version of this paper will be submitted to an IEEE magazine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CY cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The authors introduce a new vision for providing computing services for\nconnected devices. It is based on the key concept that future computing\nresources will be coupled with communication resources, for enhancing user\nexperience of the connected users, and also for optimising resources in the\nproviders' infrastructures. Such coupling is achieved by Joint/Cooperative\nresource allocation algorithms, by integrating computing and communication\nservices and by integrating hardware in networks. Such type of computing, by\nwhich computing services are not delivered independently but dependent of\nnetworking services, is named Aqua Computing. The authors see Aqua Computing as\na novel approach for delivering computing resources to end devices, where\ncomputing power of the devices are enhanced automatically once they are\nconnected to an Aqua Computing enabled network. The process of resource\ncoupling is named computation dissolving. Then, an Aqua Computing architecture\nis proposed for mobile edge networks, in which computing and wireless\nnetworking resources are allocated jointly or cooperatively by a Mobile Cloud\nController, for the benefit of the end-users and/or for the benefit of the\nservice providers. Finally, a working prototype of the system is shown and the\ngathered results show the performance of the Aqua Computing prototype.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2015 14:27:22 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Magurawalage", "Chathura Sarathchandra", ""], ["Yang", "Kun", ""], ["Wang", "Kezhi", ""]]}, {"id": "1510.07254", "submitter": "Jian-Jia Chen", "authors": "Jian-Jia Chen", "title": "Federated Scheduling Admits No Constant Speedup Factors for\n  Constrained-Deadline DAG Task Systems", "comments": "in Real-Time Systems Journal 2016", "journal-ref": null, "doi": "10.1007/s11241-016-9255-2", "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the federated scheduling approaches in multiprocessor systems, a task\neither 1) is restricted to execute sequentially on a single processor or 2) has\nexclusive access to the assigned processors. There have been several positive\nresults to conduct good federated scheduling policies, which have constant\nspeedup factors with respect to any optimal federated scheduling algorithm.\nThis paper answers an open question: \"For constrained-deadline task systems\nwith directed acyclic graph (DAG) dependency structures, do federated\nscheduling policies have a constant speedup factor with respect to any optimal\nscheduling algorithm?\" The answer is \"No!\" This paper presents an example,\nwhich demonstrates that any federated scheduling algorithm has a speedup factor\nof at least $\\Omega(\\min\\{M, N\\})$ with respect to any optimal scheduling\nalgorithm, where $N$ is the number of tasks and $M$ is the number of\nprocessors.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2015 14:46:12 GMT"}, {"version": "v2", "created": "Wed, 22 Jun 2016 19:56:30 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Chen", "Jian-Jia", ""]]}, {"id": "1510.07357", "submitter": "Bogdan Chlebus", "authors": "Bogdan S. Chlebus and Dariusz R. Kowalski and Shailesh Vaya", "title": "Distributed Bare-Bones Communication in Wireless Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider wireless networks operating under the SINR model of interference.\nNodes have limited individual knowledge and capabilities: they do not know\ntheir positions in a coordinate system in the plane, further they do not know\ntheir neighborhoods, nor do they know the size of the network $n$, and finally\nthey cannot sense collisions resulting from simultaneous transmissions by at\nleast two neighbors. Each node is equipped with a unique integer name, where\n$N$ as an upper bound on the a range of names. We refer as a backbone to a\nsubnetwork induced by a diameter-preserving dominating set of nodes. Let\n$\\Delta$ denote a maximum number of nodes that can successfully receive a\nmessage transmitted by a node when no other nodes transmit concurrently. We\nstudy distributed algorithms for communication problems in three settings. In\nthe single-node-start case, when one node starts an execution and other nodes\nare awoken by receiving messages from already awoken nodes, we present a\nrandomized broadcast algorithm that wakes up all nodes in $O(n \\log^2 N)$\nrounds with high probability. For the synchronized-start case, when all nodes\nstart an execution simultaneously, we give a randomized algorithm computing a\nbackbone in $O(\\Delta\\log^{7} N)$ rounds with high probability. In the\npartly-coordinated-start case, when a number of nodes start an execution\ntogether and other nodes are awoken by receiving messages from the already\nawoken nodes, we develop an algorithm that creates a backbone in time\n$O(n\\log^2 N +\\Delta\\log^{7} N)$ with high probability.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2015 02:56:40 GMT"}, {"version": "v2", "created": "Sat, 28 Jul 2018 17:34:10 GMT"}, {"version": "v3", "created": "Sun, 13 Jun 2021 17:57:49 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Chlebus", "Bogdan S.", ""], ["Kowalski", "Dariusz R.", ""], ["Vaya", "Shailesh", ""]]}, {"id": "1510.07623", "submitter": "Muhammad Anis Uddin Nasir", "authors": "Muhammad Anis Uddin Nasir, Gianmarco De Francisci Morales, David\n  Garcia-Soriano, Nicolas Kourtellis, and Marco Serafini", "title": "Partial Key Grouping: Load-Balanced Partitioning of Distributed Streams", "comments": "14 pages. arXiv admin note: substantial text overlap with\n  arXiv:1504.00788", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of load balancing in distributed stream processing\nengines, which is exacerbated in the presence of skew. We introduce Partial Key\nGrouping (PKG), a new stream partitioning scheme that adapts the classical\n\"power of two choices\" to a distributed streaming setting by leveraging two\nnovel techniques: key splitting and local load estimation. In so doing, it\nachieves better load balancing than key grouping while being more scalable than\nshuffle grouping.\n  We test PKG on several large datasets, both real-world and synthetic.\nCompared to standard hashing, PKG reduces the load imbalance by up to several\norders of magnitude, and often achieves nearly-perfect load balance. This\nresult translates into an improvement of up to 175% in throughput and up to 45%\nin latency when deployed on a real Storm cluster. PKG has been integrated in\nApache Storm v0.10.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2015 15:35:14 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Nasir", "Muhammad Anis Uddin", ""], ["Morales", "Gianmarco De Francisci", ""], ["Garcia-Soriano", "David", ""], ["Kourtellis", "Nicolas", ""], ["Serafini", "Marco", ""]]}, {"id": "1510.07768", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu, Aditya Bhaskara, Silvio Lattanzi, Vahab Mirrokni,\n  Lorenzo Orecchia", "title": "Expanders via Local Edge Flips", "comments": "To appear in the proceedings of the 27th ACM-SIAM Symposium on\n  Discrete Algorithms (SODA) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.NI math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing distributed and scalable algorithms to improve network connectivity\nis a central topic in peer-to-peer networks. In this paper we focus on the\nfollowing well-known problem: given an $n$-node $d$-regular network for\n$d=\\Omega(\\log n)$, we want to design a decentralized, local algorithm that\ntransforms the graph into one that has good connectivity properties (low\ndiameter, expansion, etc.) without affecting the sparsity of the graph. To this\nend, Mahlmann and Schindelhauer introduced the random \"flip\" transformation,\nwhere in each time step, a random pair of vertices that have an edge decide to\n`swap a neighbor'. They conjectured that performing $O(n d)$ such flips at\nrandom would convert any connected $d$-regular graph into a $d$-regular\nexpander graph, with high probability. However, the best known upper bound for\nthe number of steps is roughly $O(n^{17} d^{23})$, obtained via a delicate\nMarkov chain comparison argument.\n  Our main result is to prove that a natural instantiation of the random flip\nproduces an expander in at most $O(n^2 d^2 \\sqrt{\\log n})$ steps, with high\nprobability. Our argument uses a potential-function analysis based on the\nmatrix exponential, together with the recent beautiful results on the\nhigher-order Cheeger inequality of graphs. We also show that our technique can\nbe used to analyze another well-studied random process known as the `random\nswitch', and show that it produces an expander in $O(n d)$ steps with high\nprobability.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2015 04:13:17 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Bhaskara", "Aditya", ""], ["Lattanzi", "Silvio", ""], ["Mirrokni", "Vahab", ""], ["Orecchia", "Lorenzo", ""]]}, {"id": "1510.07787", "submitter": "Kazuki Yoshizoe", "authors": "Kazuki Yoshizoe and Aika Terada and Koji Tsuda", "title": "Redesigning pattern mining algorithms for supercomputers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Upcoming many core processors are expected to employ a distributed memory\narchitecture similar to currently available supercomputers, but parallel\npattern mining algorithms amenable to the architecture are not comprehensively\nstudied. We present a novel closed pattern mining algorithm with a\nwell-engineered communication protocol, and generalize it to find statistically\nsignificant patterns from personal genome data. For distributing communication\nevenly, it employs global load balancing with multiple stacks distributed on a\nset of cores organized as a hypercube with random edges. Our algorithm achieved\nup to 1175-fold speedup by using 1200 cores for solving a problem with 11,914\nitems and 697 transactions, while the naive approach of separating the search\nspace failed completely.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2015 06:59:14 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Yoshizoe", "Kazuki", ""], ["Terada", "Aika", ""], ["Tsuda", "Koji", ""]]}, {"id": "1510.07799", "submitter": "Daniel Rohe", "authors": "Daniel Rohe", "title": "Hierarchical Parallelisation of Functional Renormalisation Group\n  Calculations -- hp-fRG", "comments": null, "journal-ref": null, "doi": "10.1016/j.cpc.2016.05.024", "report-no": null, "categories": "physics.comp-ph cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The functional renormalisation group (fRG) has evolved into a versatile tool\nin condensed matter theory for studying important aspects of correlated\nelectron systems. Practical applications of the method often involve a high\nnumerical effort, motivating the question in how far High Performance Computing\n(HPC) can leverage the approach. In this work we report on a multi-level\nparallelisation of the underlying computational machinery and show that this\ncan speed up the code by several orders of magnitude. This in turn can extend\nthe applicability of the method to otherwise inaccessible cases. We exploit\nthree levels of parallelisation: Distributed computing by means of Message\nPassing (MPI), shared-memory computing using OpenMP, and vectorisation by means\nof SIMD units (single-instruction-multiple-data). Results are provided for two\ndistinct High Performance Computing (HPC) platforms, namely the IBM-based\nBlueGene/Q system JUQUEEN and an Intel Sandy-Bridge-based development cluster.\nWe discuss how certain issues and obstacles were overcome in the course of\nadapting the code. Most importantly, we conclude that this vast improvement can\nactually be accomplished by introducing only moderate changes to the code, such\nthat this strategy may serve as a guideline for other researcher to likewise\nimprove the efficiency of their codes.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2015 07:50:56 GMT"}, {"version": "v2", "created": "Wed, 25 May 2016 10:01:10 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Rohe", "Daniel", ""]]}, {"id": "1510.07986", "submitter": "Huaming Wu", "authors": "Huaming Wu, Daniel Seidenst\\\"ucker, Yi Sun, Carlos Mart\\'in Nieto,\n  William Knottenbelt, and Katinka Wolter", "title": "A Novel Offloading Partitioning Algorithm in Mobile Cloud Computing", "comments": "This paper has been withdrawn by the author", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper has been withdrawn by the author\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2015 17:01:32 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2015 18:04:55 GMT"}, {"version": "v3", "created": "Thu, 12 Nov 2015 20:42:09 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Wu", "Huaming", ""], ["Seidenst\u00fccker", "Daniel", ""], ["Sun", "Yi", ""], ["Nieto", "Carlos Mart\u00edn", ""], ["Knottenbelt", "William", ""], ["Wolter", "Katinka", ""]]}, {"id": "1510.08334", "submitter": "Robert Speck", "authors": "Robert Speck, Daniel Ruprecht", "title": "Toward fault-tolerant parallel-in-time integration with PFASST", "comments": null, "journal-ref": "Parallel Computing 62, pp. 20 - 37, 2017", "doi": "10.1016/j.parco.2016.12.001", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and analyze different strategies for the parallel-in-time\nintegration method PFASST to recover from hard faults and subsequent data loss.\nSince PFASST stores solutions at multiple time steps on different processors,\ninformation from adjacent steps can be used to recover after a processor has\nfailed. PFASST's multi-level hierarchy allows to use the coarse level for\ncorrecting the reconstructed solution, which can help to minimize overhead. A\ntheoretical model is devised linking overhead to the number of additional\nPFASST iterations required for convergence after a fault. The potential\nefficiency of different strategies is assessed in terms of required additional\niterations for examples of diffusive and advective type.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2015 15:00:42 GMT"}, {"version": "v2", "created": "Tue, 31 May 2016 07:42:12 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Speck", "Robert", ""], ["Ruprecht", "Daniel", ""]]}, {"id": "1510.08345", "submitter": "Michael B Hynes", "authors": "Michael B Hynes and Hans De Sterck", "title": "A polynomial expansion line search for large-scale unconstrained\n  minimization of smooth L2-regularized loss functions, with implementation in\n  Apache Spark", "comments": "9 pages, 8 figures, 2 tables. Preprint appearing in SIAM Conf on Data\n  Mining, Miami, FL, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In large-scale unconstrained optimization algorithms such as limited memory\nBFGS (LBFGS), a common subproblem is a line search minimizing the loss function\nalong a descent direction. Commonly used line searches iteratively find an\napproximate solution for which the Wolfe conditions are satisfied, typically\nrequiring multiple function and gradient evaluations per line search, which is\nexpensive in parallel due to communication requirements. In this paper we\npropose a new line search approach for cases where the loss function is\nanalytic, as in least squares regression, logistic regression, or low rank\nmatrix factorization. We approximate the loss function by a truncated Taylor\npolynomial, whose coefficients may be computed efficiently in parallel with\nless communication than evaluating the gradient, after which this polynomial\nmay be minimized with high accuracy in a neighbourhood of the expansion point.\nOur Polynomial Expansion Line Search (PELS) was implemented in the Apache Spark\nframework and used to accelerate the training of a logistic regression model on\nbinary classification datasets from the LIBSVM repository with LBFGS and the\nNonlinear Conjugate Gradient (NCG) method. In large-scale numerical experiments\nin parallel on a 16-node cluster with 256 cores using the URL, KDDA, and KDDB\ndatasets, the PELS approach produced significant convergence improvements\ncompared to the use of classical Wolfe line searches. For example, to reach the\nfinal training label prediction accuracies, LBFGS using PELS had speedup\nfactors of 1.8--2 over LBFGS using a Wolfe line search, measured by both the\nnumber of iterations and the time required, due to the better accuracy of step\nsizes computed in the line search. PELS has the potential to significantly\naccelerate large-scale regression and factorization computations, and is\napplicable to continuous optimization problems with smooth loss functions.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2015 15:27:26 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2016 07:01:03 GMT"}], "update_date": "2016-01-27", "authors_parsed": [["Hynes", "Michael B", ""], ["De Sterck", "Hans", ""]]}, {"id": "1510.08473", "submitter": "Christopher Meiklejohn", "authors": "Christopher S. Meiklejohn", "title": "A Certain Tendency Of The Database Community", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We posit that striving for distributed systems that provide \"single system\nimage\" semantics is fundamentally flawed and at odds with how systems operate\nin the physical world. We realize the database as an optimization of this\nsystem: a required, essential optimization in practice that facilitates central\ndata placement and ease of access to participants in a system. We motivate a\nnew model of computation that is designed to address the problems of\ncomputation over \"eventually consistent\" information in a large-scale\ndistributed system.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2015 20:20:44 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2015 02:11:16 GMT"}, {"version": "v3", "created": "Wed, 8 Mar 2017 13:39:26 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Meiklejohn", "Christopher S.", ""]]}, {"id": "1510.08545", "submitter": "Salman Habib", "authors": "Salman Habib, Robert Roser, Tom LeCompte, Zach Marshall, Anders\n  Borgland, Brett Viren, Peter Nugent, Makoto Asai, Lothar Bauerdick, Hal\n  Finkel, Steve Gottlieb, Stefan Hoeche, Paul Sheldon, Jean-Luc Vay, Peter\n  Elmer, Michael Kirby, Simon Patton, Maxim Potekhin, Brian Yanny, Paolo\n  Calafiura, Eli Dart, Oliver Gutsche, Taku Izubuchi, Adam Lyon, Don Petravick", "title": "High Energy Physics Forum for Computational Excellence: Working Group\n  Reports (I. Applications Software II. Software Libraries and Tools III.\n  Systems)", "comments": "72 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.CE cs.DC hep-ex", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing plays an essential role in all aspects of high energy physics. As\ncomputational technology evolves rapidly in new directions, and data throughput\nand volume continue to follow a steep trend-line, it is important for the HEP\ncommunity to develop an effective response to a series of expected challenges.\nIn order to help shape the desired response, the HEP Forum for Computational\nExcellence (HEP-FCE) initiated a roadmap planning activity with two key\noverlapping drivers -- 1) software effectiveness, and 2) infrastructure and\nexpertise advancement. The HEP-FCE formed three working groups, 1) Applications\nSoftware, 2) Software Libraries and Tools, and 3) Systems (including systems\nsoftware), to provide an overview of the current status of HEP computing and to\npresent findings and opportunities for the desired HEP computational roadmap.\nThe final versions of the reports are combined in this document, and are\npresented along with introductory material.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2015 02:15:05 GMT"}], "update_date": "2015-10-30", "authors_parsed": [["Habib", "Salman", ""], ["Roser", "Robert", ""], ["LeCompte", "Tom", ""], ["Marshall", "Zach", ""], ["Borgland", "Anders", ""], ["Viren", "Brett", ""], ["Nugent", "Peter", ""], ["Asai", "Makoto", ""], ["Bauerdick", "Lothar", ""], ["Finkel", "Hal", ""], ["Gottlieb", "Steve", ""], ["Hoeche", "Stefan", ""], ["Sheldon", "Paul", ""], ["Vay", "Jean-Luc", ""], ["Elmer", "Peter", ""], ["Kirby", "Michael", ""], ["Patton", "Simon", ""], ["Potekhin", "Maxim", ""], ["Yanny", "Brian", ""], ["Calafiura", "Paolo", ""], ["Dart", "Eli", ""], ["Gutsche", "Oliver", ""], ["Izubuchi", "Taku", ""], ["Lyon", "Adam", ""], ["Petravick", "Don", ""]]}, {"id": "1510.08628", "submitter": "Jianfei Chen", "authors": "Jianfei Chen, Kaiwei Li, Jun Zhu, Wenguang Chen", "title": "WarpLDA: a Cache Efficient O(1) Algorithm for Latent Dirichlet\n  Allocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing efficient and scalable algorithms for Latent Dirichlet Allocation\n(LDA) is of wide interest for many applications. Previous work has developed an\nO(1) Metropolis-Hastings sampling method for each token. However, the\nperformance is far from being optimal due to random accesses to the parameter\nmatrices and frequent cache misses.\n  In this paper, we first carefully analyze the memory access efficiency of\nexisting algorithms for LDA by the scope of random access, which is the size of\nthe memory region in which random accesses fall, within a short period of time.\nWe then develop WarpLDA, an LDA sampler which achieves both the best O(1) time\ncomplexity per token and the best O(K) scope of random access. Our empirical\nresults in a wide range of testing conditions demonstrate that WarpLDA is\nconsistently 5-15x faster than the state-of-the-art Metropolis-Hastings based\nLightLDA, and is comparable or faster than the sparsity aware F+LDA. With\nWarpLDA, users can learn up to one million topics from hundreds of millions of\ndocuments in a few hours, at an unprecedentedly throughput of 11G tokens per\nsecond.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2015 10:33:20 GMT"}, {"version": "v2", "created": "Wed, 2 Mar 2016 06:29:30 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Chen", "Jianfei", ""], ["Li", "Kaiwei", ""], ["Zhu", "Jun", ""], ["Chen", "Wenguang", ""]]}, {"id": "1510.08789", "submitter": "Travis Johnston", "authors": "Travis Johnston, Boyu Zhang, Adam Liwo, Silvia Crivelli, Michela\n  Taufer", "title": "In-Situ Data Analysis of Protein Folding Trajectories", "comments": "40 pages, 15 figures, this paper is presently in the format request\n  of the journal to which it was submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.DC q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The transition from petascale to exascale computers is characterized by\nsubstantial changes in the computer architectures and technologies. The\nresearch community relying on computational simulations is being forced to\nrevisit the algorithms for data generation and analysis due to various\nconcerns, such as higher degrees of concurrency, deeper memory hierarchies,\nsubstantial I/O and communication constraints. Simulations today typically save\nall data to analyze later. Simulations at the exascale will require us to\nanalyze data as it is generated and save only what is really needed for\nanalysis, which must be performed predominately in-situ, i.e., executed\nsufficiently fast locally, limiting memory and disk usage, and avoiding the\nneed to move large data across nodes.\n  In this paper, we present a distributed method that enables in-situ data\nanalysis for large protein folding trajectory datasets. Traditional trajectory\nanalysis methods currently follow a centralized approach that moves the\ntrajectory datasets to a centralized node and processes the data only after\nsimulations have been completed. Our method, on the other hand, captures\nconformational information in-situ using local data only while reducing the\nstorage space needed for the part of the trajectory under consideration. This\nmethod processes the input trajectory data in one pass, breaks from the\ncentralized approach of traditional analysis, avoids the movement of trajectory\ndata, and still builds the global knowledge on the formation of individual\n$\\alpha$-helices or $\\beta$-strands as trajectory frames are generated.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2015 17:34:57 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2015 15:41:25 GMT"}], "update_date": "2015-11-02", "authors_parsed": [["Johnston", "Travis", ""], ["Zhang", "Boyu", ""], ["Liwo", "Adam", ""], ["Crivelli", "Silvia", ""], ["Taufer", "Michela", ""]]}, {"id": "1510.08940", "submitter": "Emanuele Carlini", "authors": "Emanuele Carlini", "title": "Combining Peer-to-Peer and Cloud Computing for Large Scale On-line Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This thesis investigates the combination of Peer-to-Peer (P2P) and Cloud\nComputing to support Massively Multiplayer On- line Games (MMOGs). MMOGs are\nlarge-scale distributed applications where a large number of users concurrently\nshare a real-time virtual environment. Commercial MMOG infrastructures are\nsized to support peak loads, incurring in high economical cost. Cloud Computing\nrepresents an attractive solution, as it lifts MMOG operators from the burden\nof buying and maintaining hardware, while offering the illusion of infinite\nmachines. However, it requires balancing the tradeoff between resource\nprovisioning and operational costs. P2P- based solutions present several\nadvantages, including the inherent scalability, self-repairing, and natural\nload distribution capabilities. They require additional mechanisms to suit the\nrequirements of a MMOG, such as backup solutions to cope with peer\nunreliability and heterogeneity. We propose mechanisms that integrate P2P and\nCloud Computing combining their advantages. Our techniques allow operators to\nselect the ideal tradeoff between performance and economical costs. Using\nrealistic workloads, we show that hybrid infrastructures can reduce the\neconomical effort of the operator, while offering a level of service comparable\nwith centralized architectures.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2015 23:37:34 GMT"}], "update_date": "2015-11-02", "authors_parsed": [["Carlini", "Emanuele", ""]]}, {"id": "1510.08982", "submitter": "Kooktae Lee", "authors": "Kooktae Lee, Raktim Bhattacharya", "title": "Asynchronous Parallel Computing Algorithm implemented in 1D Heat\n  Equation with CUDA", "comments": "arXiv admin note: text overlap with arXiv:1503.03952", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, we present the stability as well as performance analysis of\nasynchronous parallel computing algorithm implemented in 1D heat equation with\nCUDA. The primary objective of this note lies in dissemination of asynchronous\nparallel computing algorithm by providing CUDA code for fast and easy\nimplementation. We show that the simulations carried out on nVIDIA GPU device\nwith asynchronous scheme outperforms synchronous parallel computing algorithm.\nIn addition, we also discuss some drawbacks of asynchronous parallel computing\nalgorithms.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2015 06:34:18 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2015 03:05:24 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Lee", "Kooktae", ""], ["Bhattacharya", "Raktim", ""]]}, {"id": "1510.09117", "submitter": "Samuel Skipsey", "authors": "Samuel Cadellin Skipsey, Paulin Todev, David Britton, David Crooks and\n  Gareth Roy", "title": "Extending DIRAC File Management with Erasure-Coding for efficient\n  storage", "comments": "21st International Conference on Computing for High Energy and\n  Nuclear Physics (CHEP2015)", "journal-ref": null, "doi": "10.1088/1742-6596/664/4/042051", "report-no": null, "categories": "cs.DC hep-ex", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The state of the art in Grid style data management is to achieve increased\nresilience of data via multiple complete replicas of data files across multiple\nstorage endpoints. While this is effective, it is not the most space-efficient\napproach to resilience, especially when the reliability of individual storage\nendpoints is sufficiently high that only a few will be inactive at any point in\ntime. We report on work performed as part of GridPP\\cite{GridPP}, extending the\nDirac File Catalogue and file management interface to allow the placement of\nerasure-coded files: each file distributed as N identically-sized chunks of\ndata striped across a vector of storage endpoints, encoded such that any M\nchunks can be lost and the original file can be reconstructed. The tools\ndeveloped are transparent to the user, and, as well as allowing up and\ndownloading of data to Grid storage, also provide the possibility of\nparallelising access across all of the distributed chunks at once, improving\ndata transfer and IO performance. We expect this approach to be of most\ninterest to smaller VOs, who have tighter bounds on the storage available to\nthem, but larger (WLCG) VOs may be interested as their total data increases\nduring Run 2. We provide an analysis of the costs and benefits of the approach,\nalong with future development and implementation plans in this area. In\ngeneral, overheads for multiple file transfers provide the largest issue for\ncompetitiveness of this approach at present.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2015 15:20:12 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Skipsey", "Samuel Cadellin", ""], ["Todev", "Paulin", ""], ["Britton", "David", ""], ["Crooks", "David", ""], ["Roy", "Gareth", ""]]}, {"id": "1510.09119", "submitter": "Damien Imbs", "authors": "Damien Imbs, Michel Raynal, Julien Stainer", "title": "From Byzantine Failures to Crash Failures in Message-Passing Systems: a\n  BG Simulation-based approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The BG-simulation is a powerful reduction algorithm designed for asynchronous\nread/write crash-prone systems. It allows a set of $(t+1)$ asynchronous\nsequential processes to wait-free simulate (i.e., despite the crash of up to\n$t$ of them) an arbitrary number $n$ of processes under the assumption that at\nmost $t$ of them may crash. The BG simulation shows that, in read/write\nsystems, the crucial parameter is not the number $n$ of processes, but the\nupper bound $t$ on the number of process crashes.\n  The paper extends the concept of BG simulation to asynchronous\nmessage-passing systems prone to Byzantine failures. Byzantine failures are the\nmost general type of failure: a faulty process can exhibit any arbitrary\nbehavior. Because of this, they are also the most difficult to analyze and to\nhandle algorithmically. The main contribution of the paper is a signature-free\nreduction of Byzantine failures to crash failures. Assuming $t<\\min(n',n/3)$,\nthe paper presents an algorithm that simulates a system of $n'$ processes where\nup to $t$ may crash, on top of a basic system of $n$ processes where up to $t$\nmay be Byzantine. While topological techniques have been used to relate the\ncomputability of Byzantine failure-prone systems to that of crash failure-prone\nones, this simulation is the first, to our knowledge, that establishes this\nrelation directly, in an algorithmic way.\n  In addition to extending the basic BG simulation to message-passing systems\nand failures more severe than process crashes, being modular and direct, this\nsimulation provides us with a deeper insight in the nature and understanding of\ncrash and Byzantine failures in the context of asynchronous message-passing\nsystems. Moreover, it also allows crash-tolerant algorithms, designed for\nasynchronous read/write systems, to be executed on top of asynchronous\nmessage-passing systems prone to Byzantine failures.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2015 15:23:43 GMT"}, {"version": "v2", "created": "Thu, 14 Jul 2016 11:41:53 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Imbs", "Damien", ""], ["Raynal", "Michel", ""], ["Stainer", "Julien", ""]]}, {"id": "1510.09185", "submitter": "Huaming Wu", "authors": "Huaming Wu and Katinka Wolter", "title": "Analysis of the Energy-Performance Tradeoff for Delayed Mobile\n  Offloading", "comments": "This paper has been withdrawn by the author", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper has been withdrawn by the author\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2015 18:32:52 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2015 20:43:19 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Wu", "Huaming", ""], ["Wolter", "Katinka", ""]]}]