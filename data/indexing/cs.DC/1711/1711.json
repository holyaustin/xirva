[{"id": "1711.00100", "submitter": "Gang Chen", "authors": "Gang Chen and Nan Guan and Di Liu and Qingqiang He and Kai Huang and\n  Todor Stefanov and Wang Yi", "title": "Utilization-Based Scheduling of Flexible Mixed-Criticality Real-Time\n  Tasks", "comments": "This paper has been submitted to IEEE Transaction on Computers (TC)\n  on Sept-09th-2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixed-criticality models are an emerging paradigm for the design of real-time\nsystems because of their significantly improved resource efficiency. However,\nformal mixed-criticality models have traditionally been characterized by two\nimpractical assumptions: once \\textit{any} high-criticality task overruns,\n\\textit{all} low-criticality tasks are suspended and \\textit{all other}\nhigh-criticality tasks are assumed to exhibit high-criticality behaviors at the\nsame time. In this paper, we propose a more realistic mixed-criticality model,\ncalled the flexible mixed-criticality (FMC) model, in which these two issues\nare addressed in a combined manner. In this new model, only the overrun task\nitself is assumed to exhibit high-criticality behavior, while other\nhigh-criticality tasks remain in the same mode as before. The guaranteed\nservice levels of low-criticality tasks are gracefully degraded with the\noverruns of high-criticality tasks. We derive a utilization-based technique to\nanalyze the schedulability of this new mixed-criticality model under EDF-VD\nscheduling. During runtime, the proposed test condition serves an important\ncriterion for dynamic service level tuning, by means of which the maximum\navailable execution budget for low-criticality tasks can be directly determined\nwith minimal overhead while guaranteeing mixed-criticality schedulability.\nExperiments demonstrate the effectiveness of the FMC scheme compared with\nstate-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 18:01:38 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Chen", "Gang", ""], ["Guan", "Nan", ""], ["Liu", "Di", ""], ["He", "Qingqiang", ""], ["Huang", "Kai", ""], ["Stefanov", "Todor", ""], ["Yi", "Wang", ""]]}, {"id": "1711.00231", "submitter": "Sathish Vadhiyar", "authors": "Ananya Raval, Rupesh Nasre, Vivek Kumar, Vasudevan R, Sathish\n  Vadhiyar, Keshav Pingali", "title": "Dynamic Load Balancing Strategies for Graph Applications on GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acceleration of graph applications on GPUs has found large interest due to\nthe ubiquitous use of graph processing in various domains. The inherent\n\\textit{irregularity} in graph applications leads to several challenges for\nparallelization. A key challenge, which we address in this paper, is that of\nload-imbalance. If the work-assignment to threads uses node-based graph\npartitioning, it can result in skewed task-distribution, leading to poor\nload-balance. In contrast, if the work-assignment uses edge-based graph\npartitioning, the load-balancing is better, but the memory requirement is\nrelatively higher. This makes it unsuitable for large graphs. In this work, we\npropose three techniques for improved load-balancing of graph applications on\nGPUs. Each technique brings in unique advantages, and a user may have to employ\na specific technique based on the requirement. Using Breadth First Search and\nSingle Source Shortest Paths as our processing kernels, we illustrate the\neffectiveness of each of the proposed techniques in comparison to the existing\nnode-based and edge-based mechanisms.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 07:24:24 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Raval", "Ananya", ""], ["Nasre", "Rupesh", ""], ["Kumar", "Vivek", ""], ["R", "Vasudevan", ""], ["Vadhiyar", "Sathish", ""], ["Pingali", "Keshav", ""]]}, {"id": "1711.00244", "submitter": "Anamitra R. Choudhury", "authors": "Dharma Teja Vooturi, Saurabh Goyal, Anamitra R. Choudhury, Yogish\n  Sabharwal, Ashish Verma", "title": "Efficient Inferencing of Compressed Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large number of weights in deep neural networks makes the models difficult to\nbe deployed in low memory environments such as, mobile phones, IOT edge devices\nas well as \"inferencing as a service\" environments on cloud. Prior work has\nconsidered reduction in the size of the models, through compression techniques\nlike pruning, quantization, Huffman encoding etc. However, efficient\ninferencing using the compressed models has received little attention,\nspecially with the Huffman encoding in place. In this paper, we propose\nefficient parallel algorithms for inferencing of single image and batches,\nunder various memory constraints. Our experimental results show that our\napproach of using variable batch size for inferencing achieves 15-25\\%\nperformance improvement in the inference throughput for AlexNet, while\nmaintaining memory and latency constraints.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 08:16:40 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Vooturi", "Dharma Teja", ""], ["Goyal", "Saurabh", ""], ["Choudhury", "Anamitra R.", ""], ["Sabharwal", "Yogish", ""], ["Verma", "Ashish", ""]]}, {"id": "1711.00270", "submitter": "Sathish Vadhiyar", "authors": "K. Raghavendra, Sathish S Vadhiyar", "title": "Determination of Checkpointing Intervals for Malleable Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selecting optimal intervals of checkpointing an application is important for\nminimizing the run time of the application in the presence of system failures.\nMost of the existing efforts on checkpointing interval selection were developed\nfor sequential applications while few efforts deal with parallel applications\nwhere the applications are executed on the same number of processors for the\nentire duration of execution. Some checkpointing systems support parallel\napplications where the number of processors on which the applications execute\ncan be changed during the execution. We refer to these kinds of parallel\napplications as {\\em malleable} applications. In this paper, we develop a\nperformance model for malleable parallel applications that estimates the amount\nof useful work performed in unit time (UWT) by a malleable application in the\npresence of failures as a function of checkpointing interval. We use this\nperformance model function with different intervals and select the interval\nthat maximizes the UWT value. By conducting a large number of simulations with\nthe traces obtained on real supercomputing systems, we show that the\ncheckpointing intervals determined by our model can lead to high efficiency of\napplications in the presence of failures.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 10:19:22 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Raghavendra", "K.", ""], ["Vadhiyar", "Sathish S", ""]]}, {"id": "1711.00289", "submitter": "Sathish Vadhiyar", "authors": "Srinivasan Ramesh, Sathish Vadhiyar, Ravi Nanjundiah, PN\n  Vinayachandran", "title": "Deep and Shallow convections in Atmosphere Models on Intel Xeon Phi\n  Coprocessor Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep and shallow convection calculations occupy significant times in\natmosphere models. These calculations also present significant load imbalances\ndue to varying cloud covers over different regions of the grid. In this work,\nwe accelerate these calculations on Intel{\\textregistered} Xeon\nPhi{\\texttrademark} Coprocessor Systems. By employing dynamic scheduling in\nOpenMP, we demonstrate large reductions in load imbalance and about 10%\nincrease in speedups. By careful categorization of data as private,\nfirstprivate and shared, we minimize data copying overheads for the\ncoprocessors. We identify regions of false sharing among threads and eliminate\nthem by loop rearrangements. We also employ proportional partitioning of\nindependent column computations across both the CPU and coprocessor cores based\non the performance ratio of the computations on the heterogeneous resources.\nThese techniques along with various vectorization strategies resulted in about\n30% improvement in convection calculations.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 11:16:11 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Ramesh", "Srinivasan", ""], ["Vadhiyar", "Sathish", ""], ["Nanjundiah", "Ravi", ""], ["Vinayachandran", "PN", ""]]}, {"id": "1711.00375", "submitter": "Oliver Gutsche", "authors": "Oliver Gutsche (2), Luca Canali (1), Illia Cremer (4), Matteo\n  Cremonesi (2), Peter Elmer (5), Ian Fisk (3), Maria Girone (1), Bo Jayatilaka\n  (2), Jim Kowalkowski (2), Viktor Khristenko (1), Evangelos Motesnitsalis (1),\n  Jim Pivarski (5), Saba Sehrish (2), Kacper Surdy (1), Alexey Svyatkovskiy (5)\n  ((1) European Organization for Nuclear Research CERN, Geneva, Switzerland,\n  (2) Fermi National Accelerator Laboratory, Batavia, IL, USA, (3) Flatiron\n  Institute of the Sions Foundation, New York, NY, USA, (4) Intel Corp., (5)\n  Princeton University, Princeton, NJ, USA)", "title": "CMS Analysis and Data Reduction with Apache Spark", "comments": "Proceedings for 18th International Workshop on Advanced Computing and\n  Analysis Techniques in Physics Research (ACAT 2017). arXiv admin note: text\n  overlap with arXiv:1703.04171", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experimental Particle Physics has been at the forefront of analyzing the\nworld's largest datasets for decades. The HEP community was among the first to\ndevelop suitable software and computing tools for this task. In recent times,\nnew toolkits and systems for distributed data processing, collectively called\n\"Big Data\" technologies have emerged from industry and open source projects to\nsupport the analysis of Petabyte and Exabyte datasets in industry. While the\nprinciples of data analysis in HEP have not changed (filtering and transforming\nexperiment-specific data formats), these new technologies use different\napproaches and tools, promising a fresh look at analysis of very large datasets\nthat could potentially reduce the time-to-physics with increased interactivity.\nMoreover these new tools are typically actively developed by large communities,\noften profiting of industry resources, and under open source licensing. These\nfactors result in a boost for adoption and maturity of the tools and for the\ncommunities supporting them, at the same time helping in reducing the cost of\nownership for the end-users. In this talk, we are presenting studies of using\nApache Spark for end user data analysis. We are studying the HEP analysis\nworkflow separated into two thrusts: the reduction of centrally produced\nexperiment datasets and the end-analysis up to the publication plot. Studying\nthe first thrust, CMS is working together with CERN openlab and Intel on the\nCMS Big Data Reduction Facility. The goal is to reduce 1 PB of official CMS\ndata to 1 TB of ntuple output for analysis. We are presenting the progress of\nthis 2-year project with first results of scaling up Spark-based HEP analysis.\nStudying the second thrust, we are presenting studies on using Apache Spark for\na CMS Dark Matter physics search, comparing Spark's feasibility, usability and\nperformance to the ROOT-based analysis.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 16:25:40 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Gutsche", "Oliver", ""], ["Canali", "Luca", ""], ["Cremer", "Illia", ""], ["Cremonesi", "Matteo", ""], ["Elmer", "Peter", ""], ["Fisk", "Ian", ""], ["Girone", "Maria", ""], ["Jayatilaka", "Bo", ""], ["Kowalkowski", "Jim", ""], ["Khristenko", "Viktor", ""], ["Motesnitsalis", "Evangelos", ""], ["Pivarski", "Jim", ""], ["Sehrish", "Saba", ""], ["Surdy", "Kacper", ""], ["Svyatkovskiy", "Alexey", ""]]}, {"id": "1711.00398", "submitter": "Anna Minaeva", "authors": "Anna Minaeva, Benny Akesson, Zdenek Hanzalek, Dakshina Dasari", "title": "Time-Triggered Co-Scheduling of Computation and Communication with\n  Jitter Requirements", "comments": "IEEE Transactions on Computers (2017)", "journal-ref": null, "doi": "10.1109/TC.2017.2722443", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The complexity of embedded application design is increasing with growing user\ndemands. In particular, automotive embedded systems are highly complex in\nnature, and their functionality is realized by a set of periodic tasks. These\ntasks may have hard real-time requirements and communicate over an\ninterconnect. The problem is to efficiently co-schedule task execution on cores\nand message transmission on the interconnect so that timing constraints are\nsatisfied. Contemporary works typically deal with zero-jitter scheduling, which\nresults in lower resource utilization, but has lower memory requirements. This\narticle focuses on jitter-constrained scheduling that puts constraints on the\ntasks jitter, increasing schedulability over zero- jitter scheduling. The\ncontributions of this article are: 1) Integer Linear Programming and\nSatisfiability Modulo Theory model exploiting problem-specific information to\nreduce the formulations complexity to schedule small applications. 2) A\nheuristic approach, employing three levels of scheduling scaling to real-world\nuse-cases with 10000 tasks and messages. 3) An experimental evaluation of the\nproposed approaches on a case-study and on synthetic data sets showing the\nefficiency of both zero-jitter and jitter- constrained scheduling. It shows\nthat up to 28% higher resource utilization can be achieved by having up to 10\ntimes longer computation time with relaxed jitter requirements.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 14:49:27 GMT"}, {"version": "v2", "created": "Mon, 27 Nov 2017 09:46:40 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Minaeva", "Anna", ""], ["Akesson", "Benny", ""], ["Hanzalek", "Zdenek", ""], ["Dasari", "Dakshina", ""]]}, {"id": "1711.00489", "submitter": "Samuel L. Smith", "authors": "Samuel L. Smith, Pieter-Jan Kindermans, Chris Ying and Quoc V. Le", "title": "Don't Decay the Learning Rate, Increase the Batch Size", "comments": "11 pages, 8 figures. Published as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common practice to decay the learning rate. Here we show one can\nusually obtain the same learning curve on both training and test sets by\ninstead increasing the batch size during training. This procedure is successful\nfor stochastic gradient descent (SGD), SGD with momentum, Nesterov momentum,\nand Adam. It reaches equivalent test accuracies after the same number of\ntraining epochs, but with fewer parameter updates, leading to greater\nparallelism and shorter training times. We can further reduce the number of\nparameter updates by increasing the learning rate $\\epsilon$ and scaling the\nbatch size $B \\propto \\epsilon$. Finally, one can increase the momentum\ncoefficient $m$ and scale $B \\propto 1/(1-m)$, although this tends to slightly\nreduce the test accuracy. Crucially, our techniques allow us to repurpose\nexisting training schedules for large batch training with no hyper-parameter\ntuning. We train ResNet-50 on ImageNet to $76.1\\%$ validation accuracy in under\n30 minutes.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 18:04:31 GMT"}, {"version": "v2", "created": "Sat, 24 Feb 2018 00:16:12 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Smith", "Samuel L.", ""], ["Kindermans", "Pieter-Jan", ""], ["Ying", "Chris", ""], ["Le", "Quoc V.", ""]]}, {"id": "1711.00618", "submitter": "Aurojit Panda", "authors": "Michael Alan Chang and Aurojit Panda and Yuan-Cheng Tsai and Hantao\n  Wang and Scott Shenker", "title": "ThrottleBot - Performance without Insight", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale applications are increasingly built by composing sets of\nmicroservices. In this model the functionality for a single application might\nbe split across 100s or 1000s of microservices. Resource provisioning for these\napplications is complex, requiring administrators to understand both the\nfunctioning of each microservice, and dependencies between microservices in an\napplication. In this paper we present ThrottleBot, a system that automates the\nprocess of determining what resource when allocated to which microservice is\nlikely to have the greatest impact on application performance. We demonstrate\nthe efficacy of our approach by applying ThrottleBot to both synthetic and real\nworld applications. We believe that ThrottleBot when combined with existing\nmicroservice orchestrators, e.g., Kubernetes, enables push-button deployment of\nweb scale applications.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 05:54:31 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Chang", "Michael Alan", ""], ["Panda", "Aurojit", ""], ["Tsai", "Yuan-Cheng", ""], ["Wang", "Hantao", ""], ["Shenker", "Scott", ""]]}, {"id": "1711.00705", "submitter": "Dheeraj Sreedhar", "authors": "Sameer Kumar, Dheeraj Sreedhar, Vaibhav Saxena, Yogish Sabharwal,\n  Ashish Verma", "title": "Efficient Training of Convolutional Neural Nets on Large Distributed\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have achieved im- pressive accuracy in many\napplication domains including im- age classification. Training of DNNs is an\nextremely compute- intensive process and is solved using variants of the\nstochastic gradient descent (SGD) algorithm. A lot of recent research has\nfocussed on improving the performance of DNN training. In this paper, we\npresent optimization techniques to improve the performance of the data parallel\nsynchronous SGD algorithm using the Torch framework: (i) we maintain data\nin-memory to avoid file I/O overheads, (ii) we present a multi-color based MPI\nAllreduce algorithm to minimize communication overheads, and (iii) we propose\noptimizations to the Torch data parallel table framework that handles\nmulti-threading. We evaluate the performance of our optimizations on a Power 8\nMinsky cluster with 32 nodes and 128 NVidia Pascal P100 GPUs. With our\noptimizations, we are able to train 90 epochs of the ResNet-50 model on the\nImagenet-1k dataset using 256 GPUs in just 48 minutes. This significantly\nimproves on the previously best known performance of training 90 epochs of the\nResNet-50 model on the same dataset using 256 GPUs in 65 minutes. To the best\nof our knowledge, this is the best known training performance demonstrated for\nthe Imagenet- 1k dataset.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 12:30:20 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Kumar", "Sameer", ""], ["Sreedhar", "Dheeraj", ""], ["Saxena", "Vaibhav", ""], ["Sabharwal", "Yogish", ""], ["Verma", "Ashish", ""]]}, {"id": "1711.00903", "submitter": "Katarzyna Swirydowicz", "authors": "Kasia \\'Swirydowicz, Noel Chalmers, Ali Karakus and Timothy Warburton", "title": "Acceleration of tensor-product operations for high-order finite element\n  methods", "comments": "31 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC cs.NA cs.PF math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is devoted to GPU kernel optimization and performance analysis of\nthree tensor-product operators arising in finite element methods. We provide a\nmathematical background to these operations and implementation details.\nAchieving close-to-the-peak performance for these operators requires extensive\noptimization because of the operators' properties: low arithmetic intensity,\ntiered structure, and the need to store intermediate results inside the kernel.\nWe give a guided overview of optimization strategies and we present a\nperformance model that allows us to compare the efficacy of these optimizations\nagainst an empirically calibrated roofline.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 19:45:33 GMT"}, {"version": "v2", "created": "Mon, 13 Nov 2017 15:18:19 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["\u015awirydowicz", "Kasia", ""], ["Chalmers", "Noel", ""], ["Karakus", "Ali", ""], ["Warburton", "Timothy", ""]]}, {"id": "1711.01008", "submitter": "Mohsen Amini Salehi", "authors": "Xiangbo Li, Mohsen Amini Salehi, Magdy Bayoumi, Nian-Feng Tzeng,\n  Rajkumar Buyya", "title": "Cost-Efficient and Robust On-Demand Video Transcoding Using\n  Heterogeneous Cloud Services", "comments": "IEEE Transactions on Parallel and Distributed Systems", "journal-ref": null, "doi": "10.1109/TPDS.2017.2766069", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video streams usually have to be transcoded to match the characteristics of\nviewers' devices. Streaming providers have to store numerous transcoded\nversions of a given video to serve various display devices. Given the fact that\nviewers' access pattern to video streams follows a long tail distribution, for\nthe video streams with low access rate, we propose to transcode them in an\non-demand manner using cloud computing services. The challenge in utilizing\ncloud services for on-demand video transcoding is to maintain a robust QoS for\nviewers and cost-efficiency for streaming service providers. To address this\nchallenge, we present the Cloud-based Video Streaming Services (CVS2)\narchitecture. It includes a QoS-aware scheduling that maps transcoding tasks to\nthe VMs by considering the affinity of the transcoding tasks with the allocated\nheterogeneous VMs. To maintain robustness in the presence of varying streaming\nrequests, the architecture includes a cost-efficient VM Provisioner. This\ncomponent provides a self- configurable cluster of heterogeneous VMs. The\ncluster is reconfigured dynamically to maintain the maximum affinity with the\narriving workload. Results obtained under diverse workload conditions\ndemonstrate that CVS2 architecture can maintain a robust QoS for viewers while\nreducing the incurred cost of the streaming service provider up to 85%\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 03:22:47 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Li", "Xiangbo", ""], ["Salehi", "Mohsen Amini", ""], ["Bayoumi", "Magdy", ""], ["Tzeng", "Nian-Feng", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "1711.01034", "submitter": "Minghui Qiu", "authors": "Xu Hu, Jun Huang, Minghui Qiu, Cen Chen and Wei Chu", "title": "PS-DBSCAN: An Efficient Parallel DBSCAN Algorithm Based on Platform Of\n  AI (PAI)", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present PS-DBSCAN, a communication efficient parallel DBSCAN algorithm\nthat combines the disjoint-set data structure and Parameter Server framework in\nPlatform of AI (PAI). Since data points within the same cluster may be\ndistributed over different workers which result in several disjoint-sets,\nmerging them incurs large communication costs. In our algorithm, we employ a\nfast global union approach to union the disjoint-sets to alleviate the\ncommunication burden. Experiments over the datasets of different scales\ndemonstrate that PS-DBSCAN outperforms the PDSDBSCAN with 2-10 times speedup on\ncommunication efficiency.\n  We have released our PS-DBSCAN in an algorithm platform called Platform of AI\n(PAI - https://pai.base.shuju.aliyun.com/) in Alibaba Cloud. We have also\ndemonstrated how to use the method in PAI.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 06:36:20 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Hu", "Xu", ""], ["Huang", "Jun", ""], ["Qiu", "Minghui", ""], ["Chen", "Cen", ""], ["Chu", "Wei", ""]]}, {"id": "1711.01110", "submitter": "Yufan Zheng", "authors": "Yufan Zheng", "title": "A Rudimentary Model for Low-Latency Anonymous Communication Systems", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a rudimentary model for low-latency anonymous\ncommunication systems. Specifically, we study distributed OR algorithm as an\nabstract of the system. Based on our model, we give several satisfactory lower\nbounds of anonymity leakage of a deterministic OR algorithm. Some of them\nreveal a trade-off between anonymity and communication complexity. For the\nrandomized OR algorithm, we only give a relatively trivial but possibly tight\nlower bound when leaving out communication complexity. And we find the\nrelationship between our model and some open case in the study of secret\nsharing scheme, if considering communication complexity.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 11:25:09 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Zheng", "Yufan", ""]]}, {"id": "1711.01229", "submitter": "Jim Pivarski", "authors": "Jim Pivarski, David Lange, Thanat Jatuphattharachat", "title": "Toward real-time data query systems in HEP", "comments": "6 pages, 2 figures, proceedings for ACAT 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploratory data analysis tools must respond quickly to a user's questions,\nso that the answer to one question (e.g. a visualized histogram or fit) can\ninfluence the next. In some SQL-based query systems used in industry, even very\nlarge (petabyte) datasets can be summarized on a human timescale (seconds),\nemploying techniques such as columnar data representation, caching, indexing,\nand code generation/JIT-compilation. This article describes progress toward\nrealizing such a system for High Energy Physics (HEP), focusing on the\nintermediate problems of optimizing data access and calculations for \"query\nsized\" payloads, such as a single histogram or group of histograms, rather than\nlarge reconstruction or data-skimming jobs. These techniques include direct\nextraction of ROOT TBranches into Numpy arrays and compilation of Python\nanalysis functions (rather than SQL) to be executed very quickly. We will also\ndiscuss the problem of caching and actively delivering jobs to worker nodes\nthat have the necessary input data preloaded in cache. All of these pieces of\nthe larger solution are available as standalone GitHub repositories, and could\nbe used in current analyses.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 16:36:32 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 18:51:04 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Pivarski", "Jim", ""], ["Lange", "David", ""], ["Jatuphattharachat", "Thanat", ""]]}, {"id": "1711.01262", "submitter": "He Sun", "authors": "He Sun and Luca Zanetti", "title": "Distributed Graph Clustering and Sparsification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph clustering is a fundamental computational problem with a number of\napplications in algorithm design, machine learning, data mining, and analysis\nof social networks. Over the past decades, researchers have proposed a number\nof algorithmic design methods for graph clustering. Most of these methods,\nhowever, are based on complicated spectral techniques or convex optimisation,\nand cannot be directly applied for clustering many networks that occur in\npractice, whose information is often collected on different sites. Designing a\nsimple and distributed clustering algorithm is of great interest, and has wide\napplications for processing big datasets.\n  In this paper we present a simple and distributed algorithm for graph\nclustering: for a wide class of graphs that are characterised by a strong\ncluster-structure, our algorithm finishes in a poly-logarithmic number of\nrounds, and recovers a partition of the graph close to optimal. One of the main\ncomponents behind our algorithm is a sampling scheme that, given a dense graph\nas input, produces a sparse subgraph that provably preserves the\ncluster-structure of the input. Compared with previous sparsification\nalgorithms that require Laplacian solvers or involve combinatorial\nconstructions, this component is easy to implement in a distributed way and\nruns fast in practice.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 17:52:28 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Sun", "He", ""], ["Zanetti", "Luca", ""]]}, {"id": "1711.01336", "submitter": "Yoji Yamato", "authors": "Yoji Yamato, Naoto Hoshikawa, Hirofumi Noguchi, Tatsyua Demizu and\n  Misao Kataoka", "title": "A Study of Optimizing Heterogeneous Resources for Open IoT", "comments": "6 pages, in japanese 2 figures", "journal-ref": "IEICE Technical Report, SC2017-26, Nov. 2017. (c) 2017 IEICE", "doi": null, "report-no": "IEICE Technical Report, SC2017-26, Nov. 2017", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, IoT technologies have been progressed, and many sensors and\nactuators are connected to networks. Previously, IoT services were developed by\nvertical integration style. But now Open IoT concept has attracted attentions\nwhich achieves various IoT services by integrating horizontal separated devices\nand services. For Open IoT era, we have proposed the Tacit Computing technology\nto discover the devices with necessary data for users on demand and use them\ndynamically. We also implemented elemental technologies of Tacit Computing. In\nthis paper, we propose three layers optimizations to reduce operation cost and\nimprove performance of Tacit computing service, in order to make as a\ncontinuous service of discovered devices by Tacit Computing. In optimization\nprocess, appropriate function allocations are calculated for device, network\nand cloud layer before full-scale operation.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 21:19:20 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Yamato", "Yoji", ""], ["Hoshikawa", "Naoto", ""], ["Noguchi", "Hirofumi", ""], ["Demizu", "Tatsyua", ""], ["Kataoka", "Misao", ""]]}, {"id": "1711.01361", "submitter": "Yi-Jun Chang", "authors": "Yi-Jun Chang, Wenzheng Li, Seth Pettie", "title": "An Optimal Distributed $(\\Delta+1)$-Coloring Algorithm?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vertex coloring is one of the classic symmetry breaking problems studied in\ndistributed computing. In this paper we present a new algorithm for\n$(\\Delta+1)$-list coloring in the randomized ${\\sf LOCAL}$ model running in\n$O(\\mathsf{Det}_{\\scriptscriptstyle d}(\\text{poly} \\log n))$ time, where\n$\\mathsf{Det}_{\\scriptscriptstyle d}(n')$ is the deterministic complexity of\n$(\\text{deg}+1)$-list coloring on $n'$-vertex graphs. (In this problem, each\n$v$ has a palette of size $\\text{deg}(v)+1$.) This improves upon a previous\nrandomized algorithm of Harris, Schneider, and Su [STOC'16, JACM'18] with\ncomplexity $O(\\sqrt{\\log \\Delta} + \\log\\log n +\n\\mathsf{Det}_{\\scriptscriptstyle d}(\\text{poly} \\log n))$, and, for some range\nof $\\Delta$, is much faster than the best known deterministic algorithm of\nFraigniaud, Heinrich, and Kosowski [FOCS'16] and Barenboim, Elkin, and\nGoldenberg [PODC'18], with complexity $O(\\sqrt{\\Delta\\log \\Delta}\\log^\\ast\n\\Delta + \\log^* n)$.\n  Our algorithm \"appears to be\" optimal, in view of the\n$\\Omega(\\mathsf{Det}(\\text{poly} \\log n))$ randomized lower bound due to Chang,\nKopelowitz, and Pettie [FOCS'16], where $\\mathsf{Det}$ is the deterministic\ncomplexity of $(\\Delta+1)$-list coloring. At present, the best upper bounds on\n$\\mathsf{Det}_{\\scriptscriptstyle d}(n')$ and $\\mathsf{Det}(n')$ are both\n$2^{O(\\sqrt{\\log n'})}$ and use a black box application of network\ndecompositions (Panconesi and Srinivasan [Journal of Algorithms'96]). It is\nquite possible that the true complexities of both problems are the same,\nasymptotically, which would imply the randomized optimality of our\n$(\\Delta+1)$-list coloring algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 23:46:52 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2018 17:58:09 GMT"}, {"version": "v3", "created": "Mon, 11 Mar 2019 19:10:04 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Chang", "Yi-Jun", ""], ["Li", "Wenzheng", ""], ["Pettie", "Seth", ""]]}, {"id": "1711.01364", "submitter": "Sebastian Forster", "authors": "Sebastian Forster, Danupon Nanongkai", "title": "A Faster Distributed Single-Source Shortest Paths Algorithm", "comments": "Presented at the the 59th Annual IEEE Symposium on Foundations of\n  Computer Science (FOCS 2018)", "journal-ref": null, "doi": "10.1109/FOCS.2018.00071", "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We devise new algorithms for the single-source shortest paths (SSSP) problem\nwith non-negative edge weights in the CONGEST model of distributed computing.\nWhile close-to-optimal solutions, in terms of the number of rounds spent by the\nalgorithm, have recently been developed for computing SSSP approximately, the\nfastest known exact algorithms are still far away from matching the lower bound\nof $ \\tilde \\Omega (\\sqrt{n} + D) $ rounds by Peleg and Rubinovich [SIAM\nJournal on Computing 2000], where $ n $ is the number of nodes in the network\nand $ D $ is its diameter. The state of the art is Elkin's randomized algorithm\n[STOC 2017] that performs $ \\tilde O(n^{2/3} D^{1/3} + n^{5/6}) $ rounds. We\nsignificantly improve upon this upper bound with our two new randomized\nalgorithms for polynomially bounded integer edge weights, the first performing\n$ \\tilde O (\\sqrt{n D}) $ rounds and the second performing $ \\tilde O (\\sqrt{n}\nD^{1/4} + n^{3/5} + D) $ rounds. Our bounds also compare favorably to the\nindependent result by Ghaffari and Li [STOC 2018]. As side results, we obtain a\n$ (1 + \\epsilon) $-approximation $ \\tilde O ((\\sqrt{n} D^{1/4} + D) / \\epsilon)\n$-round algorithm for directed SSSP and a new work/depth trade-off for exact\nSSSP on directed graphs in the PRAM model.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 23:57:49 GMT"}, {"version": "v2", "created": "Mon, 9 Apr 2018 07:48:09 GMT"}, {"version": "v3", "created": "Tue, 14 Aug 2018 08:58:32 GMT"}, {"version": "v4", "created": "Wed, 31 Jul 2019 08:04:29 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Forster", "Sebastian", ""], ["Nanongkai", "Danupon", ""]]}, {"id": "1711.01410", "submitter": "Jonas \\v{S}ukys", "authors": "Jonas \\v{S}ukys and Mira Kattwinkel", "title": "SPUX: Scalable Particle Markov Chain Monte Carlo for uncertainty\n  quantification in stochastic ecological models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.CE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calibration of individual based models (IBMs), successful in modeling complex\necological dynamical systems, is often performed only ad-hoc. Bayesian\ninference can be used for both parameter estimation and uncertainty\nquantification, but its successful application to realistic scenarios has been\nhindered by the complex stochastic nature of IBMs. Computationally expensive\ntechniques such as Particle Filter (PF) provide marginal likelihood estimates,\nwhere multiple model simulations (particles) are required to get a sample from\nthe state distribution conditional on the observed data. Particle ensembles are\nre-sampled at each data observation time, requiring particle destruction and\nreplication, which lead to an increase in algorithmic complexity. We present\nSPUX, a Python implementation of parallel Particle Markov Chain Monte Carlo\n(PMCMC) algorithm, which mitigates high computational costs by distributing\nparticles over multiple computational units. Adaptive load re-balancing\ntechniques are used to mitigate computational work imbalances introduced by\nre-sampling. Framework performance is investigated and significant speed-ups\nare observed for a simple predator-prey IBM model.\n", "versions": [{"version": "v1", "created": "Sat, 4 Nov 2017 07:34:31 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["\u0160ukys", "Jonas", ""], ["Kattwinkel", "Mira", ""]]}, {"id": "1711.01454", "submitter": "Zahra Ghodsi", "authors": "Zahra Ghodsi, Siddharth Garg, Ramesh Karri", "title": "Optimal Checkpointing for Secure Intermittently-Powered IoT Devices", "comments": "ICCAD 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy harvesting is a promising solution to power Internet of Things (IoT)\ndevices. Due to the intermittent nature of these energy sources, one cannot\nguarantee forward progress of program execution. Prior work has advocated for\ncheckpointing the intermediate state to off-chip non-volatile memory (NVM).\nEncrypting checkpoints addresses the security concern, but significantly\nincreases the checkpointing overheads. In this paper, we propose a new online\ncheckpointing policy that judiciously determines when to checkpoint so as to\nminimize application time to completion while guaranteeing security. Compared\nto state-of-the-art checkpointing schemes that do not account for the overheads\nof encrypted checkpoints we improve execution time up to 1.4x.\n", "versions": [{"version": "v1", "created": "Sat, 4 Nov 2017 15:51:32 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Ghodsi", "Zahra", ""], ["Garg", "Siddharth", ""], ["Karri", "Ramesh", ""]]}, {"id": "1711.01519", "submitter": "Zahra Khatami", "authors": "Zahra Khatami and Lukas Troska and Hartmut Kaiser and J. Ramanujam and\n  Adrian Serio", "title": "HPX Smart Executors", "comments": "In Proceedings of ESPM2'17: Third International Workshop on Extreme\n  Scale Programming Models and Middleware, Denver, CO, USA, November\n  12-17,,2017 (ESPM2'17), 8 pages", "journal-ref": null, "doi": "10.1145/3152041.3152084", "report-no": null, "categories": "cs.DC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of many parallel applications depends on loop-level\nparallelism. However, manually parallelizing all loops may result in degrading\nparallel performance, as some of them cannot scale desirably to a large number\nof threads. In addition, the overheads of manually tuning loop parameters might\nprevent an application from reaching its maximum parallel performance. We\nillustrate how machine learning techniques can be applied to address these\nchallenges. In this research, we develop a framework that is able to\nautomatically capture the static and dynamic information of a loop. Moreover,\nwe advocate a novel method by introducing HPX smart executors for determining\nthe execution policy, chunk size, and prefetching distance of an HPX loop to\nachieve higher possible performance by feeding static information captured\nduring compilation and runtime-based dynamic information to our learning model.\nOur evaluated execution results show that using these smart executors can speed\nup the HPX execution process by around 12%-35% for the Matrix Multiplication,\nStream and $2D$ Stencil benchmarks compared to setting their HPX loop's\nexecution policy/parameters manually or using HPX auto-parallelization\ntechniques.\n", "versions": [{"version": "v1", "created": "Sun, 5 Nov 2017 02:11:07 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Khatami", "Zahra", ""], ["Troska", "Lukas", ""], ["Kaiser", "Hartmut", ""], ["Ramanujam", "J.", ""], ["Serio", "Adrian", ""]]}, {"id": "1711.01623", "submitter": "Seri Khoury", "authors": "Amir Abboud, Keren Censor-Hillel, Seri Khoury, and Christoph Lenzen", "title": "Fooling Views: A New Lower Bound Technique for Distributed Computations\n  under Congestion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel lower bound technique for distributed graph algorithms\nunder bandwidth limitations.\n  We define the notion of \\emph{fooling views} and exemplify its strength by\nproving two new lower bounds for triangle membership in the CONGEST(B) model:\n  (i) Any $1$-round algorithm requires $B\\geq c\\Delta \\log n$ for a constant\n$c>0$.\n  (ii) If $B=1$, even in constant-degree graphs any algorithm must take\n$\\Omega(\\log^* n)$ rounds.\n  The implication of the former is the first proven separation between the\nLOCAL and the CONGEST models for deterministic triangle membership.\n  The latter result is the first non-trivial lower bound on the number of\nrounds required, even for \\emph{triangle detection}, under limited bandwidth.\n  All previous known techniques are provably incapable of giving these bounds.\n  We hope that our approach may pave the way for proving lower bounds for\nadditional problems in various settings of distributed computing for which\nprevious techniques do not suffice.\n", "versions": [{"version": "v1", "created": "Sun, 5 Nov 2017 17:45:37 GMT"}, {"version": "v2", "created": "Sun, 3 Dec 2017 18:37:48 GMT"}, {"version": "v3", "created": "Wed, 13 Dec 2017 10:22:45 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Abboud", "Amir", ""], ["Censor-Hillel", "Keren", ""], ["Khoury", "Seri", ""], ["Lenzen", "Christoph", ""]]}, {"id": "1711.01654", "submitter": "Mohamed Zahran", "authors": "Chris Quackenbush and Mohamed Zahran", "title": "Beyond Profiling: Scaling Profiling Data Usage to Multiple Applications", "comments": "18 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Profiling techniques are used extensively at different parts of the computing\nstack to achieve many goals. One major goal is to make a piece of software\nexecute more efficiently on a specific hardware platform, where efficiency\nspans criteria such as power, performance, resource requirements, etc.\nResearchers, both in academia and industry, have introduced many techniques to\ngather, and make use of, profiling data. However, one thing remains unchanged:\nmaking application A run more efficiently on machine 1. In this paper, we\nextend this criteria by asking: can profiling information of application A on\nmachine 1 be used to make application B run more efficiently on machine 1? If\nso, then this means as machine 1 continues to execute more applications, it\nbecomes better and more efficient. We present a generalized method for using\nprofiling information gathered from the execution of programs from a limited\ncorpus of applications to improve the performance of software from outside our\ncorpus. As a proof of concept, we apply our technique to the specific problem\nof selecting the most efficient last-level-cache with which to execute an\napplication. We were able to turn off an average of 19% of last-level-cache\nblocks for selected programs from PARSEC benchmark suite and only saw an\naverage 2.8% increase in the rate of last-level cache misses.\n", "versions": [{"version": "v1", "created": "Sun, 5 Nov 2017 20:02:39 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Quackenbush", "Chris", ""], ["Zahran", "Mohamed", ""]]}, {"id": "1711.01702", "submitter": "Junyao Guo", "authors": "Junyao Guo, Gabriela Hug, Ozan Tonguz", "title": "Impact of Communication Delay on Asynchronous Distributed Optimal Power\n  Flow Using ADMM", "comments": "SmartGridComm 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed optimization has attracted lots of attention in the operation of\npower systems in recent years, where a large area is decomposed into smaller\ncontrol regions each solving a local optimization problem with periodic\ninformation exchange with neighboring regions. However, most distributed\noptimization methods are iterative and require synchronization of all regions\nat each iteration, which is hard to achieve without a centralized coordinator\nand might lead to under-utilization of computation resources due to the\nheterogeneity of the regions. To address such limitations of synchronous\nschemes, this paper investigates the applicability of asynchronous distributed\noptimization methods to power system optimization. Particularly, we focus on\nsolving the AC Optimal Power Flow problem and propose an algorithmic framework\nbased on the Alternating Direction Method of Multipliers (ADMM) method that\nallows the regions to perform local updates with information received from a\nsubset of but not all neighbors. Through experimental studies, we demonstrate\nthat the convergence performance of the proposed asynchronous scheme is\ndependent on the communication delay of passing messages among the regions.\nUnder mild communication delays, the proposed scheme can achieve comparable or\neven faster convergence compared with its synchronous counterpart, which can be\nused as a good alternative to centralized or synchronous distributed\noptimization approaches.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 03:08:22 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Guo", "Junyao", ""], ["Hug", "Gabriela", ""], ["Tonguz", "Ozan", ""]]}, {"id": "1711.01725", "submitter": "Dimitris Sakavalas", "authors": "Aris Pagourtzis, Giorgos Panagiotakos, Dimitris Sakavalas", "title": "Joining Local Knowledge to Communicate Reliably (Extended Abstract)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental primitive in distributed computing is Reliable Message\nTransmission (RMT), which refers to the task of correctly sending a message\nfrom a party (or player) to another, in a network where some intermediate\nrelays might be controlled by an adversary. We address the problem under the\nrealistic assumption that the topological knowledge of players is restricted to\na certain subgraph and specifically study the role of local information\nexchange in the feasibility of RMT. We employ the General Adversary model of\nHirt and Maurer and the recently introduced Partial Knowledge Model which\nsubsume all known models for the adversary and local knowledge respectively.\nTight feasibility conditions, naturally involving the network topology, the\nadversary and the local knowledge of players, are presented.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 04:31:16 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Pagourtzis", "Aris", ""], ["Panagiotakos", "Giorgos", ""], ["Sakavalas", "Dimitris", ""]]}, {"id": "1711.01763", "submitter": "Zijie Zheng", "authors": "Zijie Zheng, Lingyang Song, Zhu Han, Geoffrey Ye Li, H. Vincent Poor", "title": "Game Theoretic Approaches to Massive Data Processing in Wireless\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wireless communication networks are becoming highly virtualized with\ntwo-layer hierarchies, in which controllers at the upper layer with tasks to\nachieve can ask a large number of agents at the lower layer to help realize\ncomputation, storage, and transmission functions. Through offloading data\nprocessing to the agents, the controllers can accomplish otherwise prohibitive\nbig data processing. Incentive mechanisms are needed for the agents to perform\nthe controllers' tasks in order to satisfy the corresponding objectives of\ncontrollers and agents. In this article, a hierarchical game framework with\nfast convergence and scalability is proposed to meet the demand for real-time\nprocessing for such situations. Possible future research directions in this\nemerging area are also discussed.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 07:56:31 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Zheng", "Zijie", ""], ["Song", "Lingyang", ""], ["Han", "Zhu", ""], ["Li", "Geoffrey Ye", ""], ["Poor", "H. Vincent", ""]]}, {"id": "1711.01871", "submitter": "Janne H. Korhonen", "authors": "Alkida Balliu and Juho Hirvonen and Janne H. Korhonen and Tuomo\n  Lempi\\\"ainen and Dennis Olivetti and Jukka Suomela", "title": "New Classes of Distributed Time Complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of recent papers -- e.g. Brandt et al. (STOC 2016), Chang et al.\n(FOCS 2016), Ghaffari & Su (SODA 2017), Brandt et al. (PODC 2017), and Chang &\nPettie (FOCS 2017) -- have advanced our understanding of one of the most\nfundamental questions in theory of distributed computing: what are the possible\ntime complexity classes of LCL problems in the LOCAL model? In essence, we have\na graph problem $\\Pi$ in which a solution can be verified by checking all\nradius-$O(1)$ neighbourhoods, and the question is what is the smallest $T$ such\nthat a solution can be computed so that each node chooses its own output based\non its radius-$T$ neighbourhood. Here $T$ is the distributed time complexity of\n$\\Pi$.\n  The time complexity classes for deterministic algorithms in bounded-degree\ngraphs that are known to exist by prior work are $\\Theta(1)$, $\\Theta(\\log^*\nn)$, $\\Theta(\\log n)$, $\\Theta(n^{1/k})$, and $\\Theta(n)$. It is also known\nthat there are two gaps: one between $\\omega(1)$ and $o(\\log \\log^* n)$, and\nanother between $\\omega(\\log^* n)$ and $o(\\log n)$. It has been conjectured\nthat many more gaps exist, and that the overall time hierarchy is relatively\nsimple -- indeed, this is known to be the case in restricted graph families\nsuch as cycles and grids.\n  We show that the picture is much more diverse than previously expected. We\npresent a general technique for engineering LCL problems with numerous\ndifferent deterministic time complexities, including $\\Theta(\\log^{\\alpha}n)$\nfor any $\\alpha\\ge1$, $2^{\\Theta(\\log^{\\alpha}n)}$ for any $\\alpha\\le 1$, and\n$\\Theta(n^{\\alpha})$ for any $\\alpha <1/2$ in the high end of the complexity\nspectrum, and $\\Theta(\\log^{\\alpha}\\log^* n)$ for any $\\alpha\\ge 1$,\n$\\smash{2^{\\Theta(\\log^{\\alpha}\\log^* n)}}$ for any $\\alpha\\le 1$, and\n$\\Theta((\\log^* n)^{\\alpha})$ for any $\\alpha \\le 1$ in the low end; here\n$\\alpha$ is a positive rational number.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 13:05:30 GMT"}, {"version": "v2", "created": "Thu, 5 Apr 2018 12:21:26 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Balliu", "Alkida", ""], ["Hirvonen", "Juho", ""], ["Korhonen", "Janne H.", ""], ["Lempi\u00e4inen", "Tuomo", ""], ["Olivetti", "Dennis", ""], ["Suomela", "Jukka", ""]]}, {"id": "1711.01897", "submitter": "Timo Betcke", "authors": "Kerstin Vater, Timo Betcke, Boris Dilba", "title": "Simple and efficient GPU parallelization of existing H-Matrix\n  accelerated BEM code", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we demonstrate how GPU-accelerated BEM routines can be used in\na simple black-box fashion to accelerate fast boundary element formulations\nbased on Hierarchical Matrices (H-Matrices) with ACA (Adaptive Cross\nApproximation). In particular, we focus on the expensive evaluation of the\ndiscrete weak form of boundary operators associated with the Laplace and the\nHelmholtz equation in three space dimensions. The method is based on offloading\nthe CPU assembly of elements during the ACA assembly onto a GPU device and to\nuse threading strategies across ACA blocks to create sufficient workload for\nthe GPU. The proposed GPU strategy is designed such that it can be implemented\nin existing code with minimal changes to the surrounding application structure.\nThis is in particular interesting for existing legacy code that is not from the\nground-up designed with GPU computing in mind. Our benchmark study gives\nrealistic impressions of the benefits of GPU-accelerated BEM simulations by\nusing state-of-the-art multi-threaded computations on modern high-performance\nCPUs as a reference, rather than drawing synthetic comparisons with\nsingle-threaded codes. Speed-up plots illustrate that performance gains up to a\nfactor of 5.5 could be realized with GPU computing under these conditions. This\nrefers to a boundary element model with about 4 million unknowns, whose\nH-Matrix weak form associated with a real-valued (Laplace) boundary operator is\nset up in only 100 minutes harnessing the two GPUs instead of 9 hours when\nusing the 20 CPU cores at disposal only. The benchmark study is followed by a\nparticularly demanding real-life application, where we compute the scattered\nhigh-frequency sound field of a submarine to demonstrate the increase in\noverall application performance from moving to a GPU-based ACA assembly.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 14:02:02 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Vater", "Kerstin", ""], ["Betcke", "Timo", ""], ["Dilba", "Boris", ""]]}, {"id": "1711.01912", "submitter": "Ruben Mayer", "authors": "Ruben Mayer and Christian Mayer and Larissa Laich", "title": "The TensorFlow Partitioning and Scheduling Problem: It's the Critical\n  Path!", "comments": "6 pages. To be published in Proceedings of DIDL '17: Workshop on\n  Distributed Infrastructures for Deep Learning, hosted by ACM Middleware 2017\n  Conference. https://doi.org/10.1145/3154842.3154843", "journal-ref": null, "doi": "10.1145/3154842.3154843", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art data flow systems such as TensorFlow impose iterative\ncalculations on large graphs that need to be partitioned on heterogeneous\ndevices such as CPUs, GPUs, and TPUs. However, partitioning can not be viewed\nin isolation. Each device has to select the next graph vertex to be executed,\ni.e., perform local scheduling decisions. Both problems, partitioning and\nscheduling, are NP-complete by themselves but have to be solved in combination\nin order to minimize overall execution time of an iteration. In this paper, we\npropose several heuristic strategies to solve the partitioning and scheduling\nproblem in TensorFlow. We simulate the performance of the proposed strategies\nin heterogeneous environments with communication-intensive workloads that are\ncommon to TensorFlow. Our findings indicate that the best partitioning and\nscheduling heuristics are those that focus on minimizing the execution time of\nthe critical path in the graph. Those strategies provide a speed-up of up to 4\ntimes in comparison to strategies that are agnostic to the critical path, such\nas hash-based partitioning and FIFO scheduling.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 14:44:05 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Mayer", "Ruben", ""], ["Mayer", "Christian", ""], ["Laich", "Larissa", ""]]}, {"id": "1711.01919", "submitter": "Mahdieh Poostchi", "authors": "Mahdieh Poostchi, Kannappan Palaniappan, Da Li, Michela Becchi, Filiz\n  Bunyak and Guna Seetharaman", "title": "Fast Integral Histogram Computations on GPU for Real-Time Video\n  Analytics", "comments": "GPU Implementation of Integral Histogram", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many Multimedia content analytics frameworks feature likelihood maps\nrepresented as histograms play a critical role in the overall algorithm.\nIntegral histograms provide an efficient computational framework for extracting\nmulti-scale histogram-based regional descriptors in constant time which are\nconsidered as the principle building blocks of many video content analytics\nframeworks. We evaluate four different mappings of the integral histogram\ncomputation onto Graphics Processing Units (GPUs) using different kernel\noptimization strategies. Our kernels perform cumulative sums on row and column\nhistograms in a cross-weave or wavefront scan order, use different data\norganization and scheduling methods that is shown to critically affect\nutilization of GPU resources (cores and shared memory). Tiling the 3-D array\ninto smaller regular data blocks significantly speeds up the efficiency of the\ncomputation compared to a strip-based organization. The tiled integral\nhistogram using a diagonal wavefront scan has the best performance of about\n300.4 frames/sec for 640 x 480 images and 32 bins with a speedup factor of\nabout 120 using GTX Titan X graphics card compared to a single threaded\nsequential CPU implementation. Double-buffering has been exploited to overlap\ncomputation and communication across sequence of images. Mapping integral\nhistogram bins computations onto multiple GPUs enables us to process 32 giga\nbytes integral histogram data (of 64MB Image and 128 bins) with a frame rate of\n0.73 Hz and speedup factor of 153X over single-threaded CPU implementation and\nthe speedup of 45X over 16-threaded CPU implementation.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 14:54:41 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Poostchi", "Mahdieh", ""], ["Palaniappan", "Kannappan", ""], ["Li", "Da", ""], ["Becchi", "Michela", ""], ["Bunyak", "Filiz", ""], ["Seetharaman", "Guna", ""]]}, {"id": "1711.01977", "submitter": "Syed Eqbal Alam", "authors": "Syed Eqbal Alam, Robert Shorten, Fabian Wirth and Jia Yuan Yu", "title": "Distributed Multi-resource Allocation with Little Communication Overhead", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a distributed algorithm to solve a special distributed\nmulti-resource allocation problem with no direct inter-agent communication. We\ndo so by extending a recently introduced additive-increase\nmultiplicative-decrease (AIMD) algorithm, which only uses very little\ncommunication between the system and agents. Namely, a control unit broadcasts\na one-bit signal to agents whenever one of the allocated resources exceeds\ncapacity. Agents then respond to this signal in a probabilistic manner. In the\nproposed algorithm, each agent is unaware of the resource allocation of other\nagents. We also propose a version of the AIMD algorithm for multiple binary\nresources (e.g., parking spaces). Binary resources are indivisible unit-demand\nresources, and each agent either allocated one unit of the resource or none. In\nempirical results, we observe that in both cases, the average allocations\nconverge over time to optimal allocations.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 16:02:41 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Alam", "Syed Eqbal", ""], ["Shorten", "Robert", ""], ["Wirth", "Fabian", ""], ["Yu", "Jia Yuan", ""]]}, {"id": "1711.01981", "submitter": "Isabel Campos Dr.", "authors": "INDIGO-DataCloud Collaboration: Davide Salomoni, Isabel Campos,\n  Luciano Gaido, Jesus Marco de Lucas, Peter Solagna, Jorge Gomes, Ludek\n  Matyska, Patrick Fuhrman, Marcus Hardt, Giacinto Donvito, Lukasz Dutka,\n  Marcin Plociennik, Roberto Barbera, Ignacio Blanquer, Andrea Ceccanti, Mario\n  David, Cristina Duma, Alvaro L\\'opez-Garc\\'ia, Germ\\'an Molt\\'o, Pablo Orviz,\n  Zdenek Sustr, Matthew Viljoen, Fernando Aguilar, Luis Alves, Marica\n  Antonacci, Lucio Angelo Antonelli, Stefano Bagnasco, Alexandre M.J.J. Bonvin,\n  Riccardo Bruno, Eva Cetinic, Yin Chen, Fabrizio Chiarello, Alessandro Costa,\n  Stefano Dal Pra, Davor Davidovic, Alvise Dorigo, Benjamin Ertl, Federica\n  Fanzago, Marco Fargetta, Sandro Fiore, Stefano Gallozzi, Zeynep Kurkcuoglu,\n  Lara Lloret, Joao Martins, Alessandra Nuzzo, Paola Nassisi, Cosimo Palazzo,\n  Joao Pina, Eva Sciacca, Matteo Segatta, Massimo Sgaravatto, Daniele Spiga,\n  Sonia Taneja, Marco Antonio Tangaro, Michal Urbaniak, Sara Vallero, Marco\n  Verlato, Bas Wegh, Valentina Zaccolo, Federico Zambelli, Lisa Zangrando,\n  Stefano Zani and Tomasz Zok", "title": "INDIGO-DataCloud:A data and computing platform to facilitate seamless\n  access to e-infrastructures", "comments": "39 pages, 15 figures.Version accepted in Journal of Grid Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the achievements of the H2020 project INDIGO-DataCloud.\nThe project has provided e-infrastructures with tools, applications and cloud\nframework enhancements to manage the demanding requirements of scientific\ncommunities, either locally or through enhanced interfaces. The middleware\ndeveloped allows to federate hybrid resources, to easily write, port and run\nscientific applications to the cloud. In particular, we have extended existing\nPaaS (Platform as a Service) solutions, allowing public and private\ne-infrastructures, including those provided by EGI, EUDAT, and Helix Nebula, to\nintegrate their existing services and make them available through AAI services\ncompliant with GEANT interfederation policies, thus guaranteeing transparency\nand trust in the provisioning of such services. Our middleware facilitates the\nexecution of applications using containers on Cloud and Grid based\ninfrastructures, as well as on HPC clusters. Our developments are freely\ndownloadable as open source components, and are already being integrated into\nmany scientific applications.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 16:06:49 GMT"}, {"version": "v2", "created": "Thu, 9 Nov 2017 14:39:04 GMT"}, {"version": "v3", "created": "Fri, 10 Nov 2017 14:06:56 GMT"}, {"version": "v4", "created": "Sat, 25 Nov 2017 17:09:24 GMT"}, {"version": "v5", "created": "Wed, 25 Jul 2018 08:58:50 GMT"}, {"version": "v6", "created": "Thu, 26 Jul 2018 09:31:53 GMT"}, {"version": "v7", "created": "Tue, 5 Feb 2019 18:00:33 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["DataCloud Collaboration", "", ""], ["Salomoni", "Davide", ""], ["Campos", "Isabel", ""], ["Gaido", "Luciano", ""], ["de Lucas", "Jesus Marco", ""], ["Solagna", "Peter", ""], ["Gomes", "Jorge", ""], ["Matyska", "Ludek", ""], ["Fuhrman", "Patrick", ""], ["Hardt", "Marcus", ""], ["Donvito", "Giacinto", ""], ["Dutka", "Lukasz", ""], ["Plociennik", "Marcin", ""], ["Barbera", "Roberto", ""], ["Blanquer", "Ignacio", ""], ["Ceccanti", "Andrea", ""], ["David", "Mario", ""], ["Duma", "Cristina", ""], ["L\u00f3pez-Garc\u00eda", "Alvaro", ""], ["Molt\u00f3", "Germ\u00e1n", ""], ["Orviz", "Pablo", ""], ["Sustr", "Zdenek", ""], ["Viljoen", "Matthew", ""], ["Aguilar", "Fernando", ""], ["Alves", "Luis", ""], ["Antonacci", "Marica", ""], ["Antonelli", "Lucio Angelo", ""], ["Bagnasco", "Stefano", ""], ["Bonvin", "Alexandre M. J. J.", ""], ["Bruno", "Riccardo", ""], ["Cetinic", "Eva", ""], ["Chen", "Yin", ""], ["Chiarello", "Fabrizio", ""], ["Costa", "Alessandro", ""], ["Pra", "Stefano Dal", ""], ["Davidovic", "Davor", ""], ["Dorigo", "Alvise", ""], ["Ertl", "Benjamin", ""], ["Fanzago", "Federica", ""], ["Fargetta", "Marco", ""], ["Fiore", "Sandro", ""], ["Gallozzi", "Stefano", ""], ["Kurkcuoglu", "Zeynep", ""], ["Lloret", "Lara", ""], ["Martins", "Joao", ""], ["Nuzzo", "Alessandra", ""], ["Nassisi", "Paola", ""], ["Palazzo", "Cosimo", ""], ["Pina", "Joao", ""], ["Sciacca", "Eva", ""], ["Segatta", "Matteo", ""], ["Sgaravatto", "Massimo", ""], ["Spiga", "Daniele", ""], ["Taneja", "Sonia", ""], ["Tangaro", "Marco Antonio", ""], ["Urbaniak", "Michal", ""], ["Vallero", "Sara", ""], ["Verlato", "Marco", ""], ["Wegh", "Bas", ""], ["Zaccolo", "Valentina", ""], ["Zambelli", "Federico", ""], ["Zangrando", "Lisa", ""], ["Zani", "Stefano", ""], ["Zok", "Tomasz", ""]]}, {"id": "1711.02014", "submitter": "Lewis Tseng", "authors": "Lewis Tseng and Takamasa Higuchi and Onur Altintas", "title": "When Cars Meet Distributed Computing: Data Storage as an Example", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As cars are ubiquitous they could play a major role in a next generation\ncommunication and computation framework. In the last years, the development of\nvehicle-to-vehicle communication and vehicle-to-infrastructure communication\ntook huge steps forward and therefore gives us the tools to build \"mobile\ncomputing service\" on cars equipped with computation capabilities. Recently,\nseveral groups of researchers independently proposed the design of \"vehicular\nclouds\" that materializes the concept. In this paper, we introduce a new\nparadigm of the vehicular clouds, followed by a case study of data storage on\ntop of the proposed cloud. Finally, we present several challenges and\nopportunities in the intersection of vehicular clouds and distributed\ncomputing.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 16:57:03 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Tseng", "Lewis", ""], ["Higuchi", "Takamasa", ""], ["Altintas", "Onur", ""]]}, {"id": "1711.02087", "submitter": "Mansaf Alam Dr", "authors": "Samiya Khan, Kashish Ara Shakil, Mansaf Alam", "title": "Workflow-Based Big Data Analytics in The Cloud Environment Present\n  Research Status and Future Prospects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Workflow is a common term used to describe a systematic breakdown of tasks\nthat need to be performed to solve a problem. This concept has found best use\nin scientific and business applications for streamlining and improving the\nperformance of the underlying processes targeted towards achieving an outcome.\nThe growing complexity of big data analytical problems has invited the use of\nscientific workflows for performing complex tasks for specific domain\napplications. This research investigates the efficacy of workflow-based big\ndata analytics in the cloud environment, giving insights on the research\nalready performed in the area and possible future research directions in the\nfield.\n", "versions": [{"version": "v1", "created": "Sat, 4 Nov 2017 13:18:13 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Khan", "Samiya", ""], ["Shakil", "Kashish Ara", ""], ["Alam", "Mansaf", ""]]}, {"id": "1711.02194", "submitter": "David Harris", "authors": "Mohsen Ghaffari and David G. Harris and Fabian Kuhn", "title": "On Derandomizing Local Distributed Algorithms", "comments": "37 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The gap between the known randomized and deterministic local distributed\nalgorithms underlies arguably the most fundamental and central open question in\ndistributed graph algorithms. In this paper, we develop a generic and clean\nrecipe for derandomizing LOCAL algorithms. We also exhibit how this simple\nrecipe leads to significant improvements on a number of problem. Two main\nresults are:\n  - An improved distributed hypergraph maximal matching algorithm, improving on\nFischer, Ghaffari, and Kuhn [FOCS'17], and giving improved algorithms for\nedge-coloring, maximum matching approximation, and low out-degree edge\norientation. The first gives an improved algorithm for Open Problem 11.4 of the\nbook of Barenboim and Elkin, and the last gives the first positive resolution\nof their Open Problem 11.10.\n  - An improved distributed algorithm for the Lov\\'{a}sz Local Lemma, which\ngets closer to a conjecture of Chang and Pettie [FOCS'17], and moreover leads\nto improved distributed algorithms for problems such as defective coloring and\n$k$-SAT.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 22:14:06 GMT"}, {"version": "v2", "created": "Fri, 6 Apr 2018 23:13:23 GMT"}, {"version": "v3", "created": "Tue, 31 Jul 2018 15:08:21 GMT"}, {"version": "v4", "created": "Tue, 17 Sep 2019 20:09:04 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Ghaffari", "Mohsen", ""], ["Harris", "David G.", ""], ["Kuhn", "Fabian", ""]]}, {"id": "1711.02368", "submitter": "Masato Asahara", "authors": "Masato Asahara and Ryohei Fujimaki", "title": "Distributed Bayesian Piecewise Sparse Linear Models", "comments": "Short version of this paper will be published in IEEE BigData 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The importance of interpretability of machine learning models has been\nincreasing due to emerging enterprise predictive analytics, threat of data\nprivacy, accountability of artificial intelligence in society, and so on.\nPiecewise linear models have been actively studied to achieve both accuracy and\ninterpretability. They often produce competitive accuracy against\nstate-of-the-art non-linear methods. In addition, their representations (i.e.,\nrule-based segmentation plus sparse linear formula) are often preferred by\ndomain experts. A disadvantage of such models, however, is high computational\ncost for simultaneous determinations of the number of \"pieces\" and cardinality\nof each linear predictor, which has restricted their applicability to\nmiddle-scale data sets. This paper proposes a distributed factorized asymptotic\nBayesian (FAB) inference of learning piece-wise sparse linear models on\ndistributed memory architectures. The distributed FAB inference solves the\nsimultaneous model selection issue without communicating $O(N)$ data where N is\nthe number of training samples and achieves linear scale-out against the number\nof CPU cores. Experimental results demonstrate that the distributed FAB\ninference achieves high prediction accuracy and performance scalability with\nboth synthetic and benchmark data.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 10:05:31 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Asahara", "Masato", ""], ["Fujimaki", "Ryohei", ""]]}, {"id": "1711.02455", "submitter": "Leqi Zhu", "authors": "Faith Ellen, Rati Gelashvili, Leqi Zhu", "title": "Revisionist Simulations: A New Approach to Proving Space Lower Bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining the space complexity of $x$-obstruction-free $k$-set agreement\nfor $x\\leq k$ is an open problem. In $x$-obstruction-free protocols, processes\nare required to return in executions where at most $x$ processes take steps.\nThe best known upper bound on the number of registers needed to solve this\nproblem among $n>k$ processes is $n-k+x$ registers. No general lower bound\nbetter than $2$ was known.\n  We prove that any $x$-obstruction-free protocol solving $k$-set agreement\namong $n>k$ processes uses at least $\\lfloor(n-x)/(k+1-x)\\rfloor+1$ registers.\nOur main tool is a simulation that serves as a reduction from the impossibility\nof deterministic wait-free $k$-set agreement: if a protocol uses fewer\nregisters, then it is possible for $k+1$ processes to simulate the protocol and\ndeterministically solve $k$-set agreement in a wait-free manner, which is\nimpossible. A critical component of the simulation is the ability of simulating\nprocesses to revise the past of simulated processes. We introduce a new\naugmented snapshot object, which facilitates this.\n  We also prove that any space lower bound on the number of registers used by\nobstruction-free protocols applies to protocols that satisfy nondeterministic\nsolo termination. Hence, our lower bound of $\\lfloor(n-1)/k\\rfloor+1$ for the\nobstruction-free ($x=1$) case also holds for randomized wait-free free\nprotocols. In particular, this gives a tight lower bound of exactly $n$\nregisters for solving obstruction-free and randomized wait-free consensus.\n  Finally, our new techniques can be applied to get a space lower of $\\lfloor\nn/2\\rfloor+1$ for $\\epsilon$-approximate agreement, for sufficiently small\n$\\epsilon$. It requires participating processes to return values within\n$\\epsilon$ of each other. The best known upper bounds are\n$\\lceil\\log(1/\\epsilon)\\rceil$ and $n$, while no general lower bounds were\nknown.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 13:34:52 GMT"}, {"version": "v2", "created": "Thu, 23 Nov 2017 21:42:50 GMT"}, {"version": "v3", "created": "Sat, 6 Jan 2018 04:01:03 GMT"}, {"version": "v4", "created": "Mon, 26 Feb 2018 11:00:07 GMT"}, {"version": "v5", "created": "Wed, 10 Oct 2018 17:18:27 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Ellen", "Faith", ""], ["Gelashvili", "Rati", ""], ["Zhu", "Leqi", ""]]}, {"id": "1711.02659", "submitter": "Zhe Zhang", "authors": "Brian Bockelman, Zhe Zhang and Jim Pivarski", "title": "Optimizing ROOT IO For Analysis", "comments": "18th International Workshop on Advanced Computing and Analysis\n  Techniques in Physics Research (ACAT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ROOT I/O (RIO) subsystem is foundational to most HEP experiments - it\nprovides a file format, a set of APIs/semantics, and a reference implementation\nin C++. It is often found at the base of an experiment's framework and is used\nto serialize the experiment's data; in the case of an LHC experiment, this may\nbe hundreds of petabytes of files! Individual physicists will further use RIO\nto perform their end-stage analysis, reading from intermediate files they\ngenerate from experiment data.\n  RIO is thus incredibly flexible: it must serve as a file format for archival\n(optimized for space) and for working data (optimized for read speed). To date,\nmost of the technical work has focused on improving the former use case. We\npresent work designed to help improve RIO for analysis. We analyze the\nreal-world impact of LZ4 to decrease decompression times (and the corresponding\ncost in disk space). We introduce new APIs that read RIO data in bulk, removing\nthe per-event overhead of a C++ function call. We compare the performance with\nthe existing RIO APIs for simple structure data and show how this can be\ncomplimentary with efforts to improve the parallelism of the RIO stack.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 18:50:13 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Bockelman", "Brian", ""], ["Zhang", "Zhe", ""], ["Pivarski", "Jim", ""]]}, {"id": "1711.02976", "submitter": "Bo Zhang", "authors": "W. Guan, X. Cheng, J. Huang, G. Huber, W. Li, J. A. McCammon, B. Zhang", "title": "RPYFMM: Parallel Adaptive Fast Multipole Method for\n  Rotne-Prager-Yamakawa Tensor in Biomolecular Hydrodynamics Simulations", "comments": null, "journal-ref": null, "doi": "10.1016/j.cpc.2018.02.005", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RPYFMM is a software package for the efficient evaluation of the potential\nfield governed by the Rotne-Prager-Yamakawa (RPY) tensor interactions in\nbiomolecular hydrodynamics simulations. In our algorithm, the RPY tensor is\ndecomposed as a linear combination of four Laplace interactions, each of which\nis evaluated using the adaptive fast multipole method (FMM) [1] where the\nexponential expansions are applied to diagonalize the multipole-to-local\ntranslation operators. RPYFMM offers a unified execution on both shared and\ndistributed memory computers by leveraging the DASHMM library [2, 3].\nPreliminary numerical results show that the interactions for a molecular system\nof 15 million particles (beads) can be computed within one second on a Cray\nXC30 cluster using 12, 288 cores, while achieving approximately 54%\nstrong-scaling efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 14:44:12 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Guan", "W.", ""], ["Cheng", "X.", ""], ["Huang", "J.", ""], ["Huber", "G.", ""], ["Li", "W.", ""], ["McCammon", "J. A.", ""], ["Zhang", "B.", ""]]}, {"id": "1711.03076", "submitter": "Sepehr Assadi", "authors": "Sepehr Assadi, MohammadHossein Bateni, Aaron Bernstein, Vahab\n  Mirrokni, Cliff Stein", "title": "Coresets Meet EDCS: Algorithms for Matching and Vertex Cover on Massive\n  Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As massive graphs become more prevalent, there is a rapidly growing need for\nscalable algorithms that solve classical graph problems, such as maximum\nmatching and minimum vertex cover, on large datasets. For massive inputs,\nseveral different computational models have been introduced, including the\nstreaming model, the distributed communication model, and the massively\nparallel computation (MPC) model that is a common abstraction of\nMapReduce-style computation. In each model, algorithms are analyzed in terms of\nresources such as space used or rounds of communication needed, in addition to\nthe more traditional approximation ratio.\n  In this paper, we give a single unified approach that yields better\napproximation algorithms for matching and vertex cover in all these models. The\nhighlights include:\n  * The first one pass, significantly-better-than-2-approximation for matching\nin random arrival streams that uses subquadratic space, namely a\n$(1.5+\\epsilon)$-approximation streaming algorithm that uses $O(n^{1.5})$ space\nfor constant $\\epsilon > 0$.\n  * The first 2-round, better-than-2-approximation for matching in the MPC\nmodel that uses subquadratic space per machine, namely a\n$(1.5+\\epsilon)$-approximation algorithm with $O(\\sqrt{mn} + n)$ memory per\nmachine for constant $\\epsilon > 0$.\n  By building on our unified approach, we further develop parallel algorithms\nin the MPC model that give a $(1 + \\epsilon)$-approximation to matching and an\n$O(1)$-approximation to vertex cover in only $O(\\log\\log{n})$ MPC rounds and\n$O(n/poly\\log{(n)})$ memory per machine. These results settle multiple open\nquestions posed in the recent paper of Czumaj~et.al. [STOC 2018].\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 18:13:20 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 16:08:52 GMT"}, {"version": "v3", "created": "Thu, 27 Dec 2018 21:28:24 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Assadi", "Sepehr", ""], ["Bateni", "MohammadHossein", ""], ["Bernstein", "Aaron", ""], ["Mirrokni", "Vahab", ""], ["Stein", "Cliff", ""]]}, {"id": "1711.03178", "submitter": "Long Gong", "authors": "Long Gong and Jun (Jim) Xu", "title": "R(QPS-Serena) and R(QPS-Serenade): Two Novel Augmenting-Path Based\n  Algorithms for Computing Approximate Maximum Weight Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this addendum, we show that the switching algorithm QPS-SERENA can be\nconverted R(QPS-SERENA), an algorithm for computing approximate Maximum Weight\nMatching (MWM). Empirically, R(QPS-SERENA) computes $(1-\\epsilon)$-MWM within\nlinear time (with respect to the number of edges $N^2$) for any fixed\n$\\epsilon\\in (0,1)$, for complete bipartite graphs with {\\it i.i.d.} uniform\nedge weight distributions. This efficacy matches that of the state-of-art\nsolution, although we so far cannot prove any theoretical guarantees on the\ntime complexities needed to attain a certain approximation ratio. Then, we have\nsimilarly converted QPS-SERENADE to R(QPS-SERENADE), which empirically should\noutput $(1-\\epsilon)$-MWM within only $O(N \\log N)$ time for the same type of\ncomplete bipartite graphs as described above.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 21:43:12 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Gong", "Long", "", "Jim"], ["Jun", "", "", "Jim"], ["Xu", "", ""]]}, {"id": "1711.03204", "submitter": "Rajsimman Ravichandiran", "authors": "Hamzeh Khazaei, Rajsimman Ravichandiran, Byungchul Park, Hadi\n  Bannazadeh, Ali Tizghadam, Alberto Leon-Garcia", "title": "Elascale: Autoscaling and Monitoring as a Service", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Auto-scalability has become an evident feature for cloud software systems\nincluding but not limited to big data and IoT applications. Cloud application\nproviders now are in full control over their applications' microservices and\nmacroservices; virtual machines and containers can be provisioned or\ndeprovisioned on demand at runtime. Elascale strives to adjust both micro/macro\nresources with respect to workload and changes in the internal state of the\nwhole application stack. Elascale leverages Elasticsearch stack for collection,\nanalysis and storage of performance metrics. Elascale then uses its default\nscaling engine to elastically adapt the managed application. Extendibility is\nguaranteed through provider, schema, plug-in and policy elements in the\nElascale by which flexible scalability algorithms, including both reactive and\nproactive techniques, can be designed and implemented for various technologies,\ninfrastructures and software stacks. In this paper, we present the architecture\nand initial implementation of Elascale; an instance will be leveraged to add\nauto-scalability to a generic IoT application. Due to zero dependency to the\ntarget software system, Elascale can be leveraged to provide auto-scalability\nand monitoring as-a-service for any type of cloud software system.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 23:16:38 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Khazaei", "Hamzeh", ""], ["Ravichandiran", "Rajsimman", ""], ["Park", "Byungchul", ""], ["Bannazadeh", "Hadi", ""], ["Tizghadam", "Ali", ""], ["Leon-Garcia", "Alberto", ""]]}, {"id": "1711.03229", "submitter": "Wanling Gao", "authors": "Wanling Gao, Lei Wang, Jianfeng Zhan, Chunjie Luo, Daoyi Zheng, Zhen\n  Jia, Biwei Xie, Chen Zheng, Qiang Yang, Haibin Wang", "title": "A Dwarf-based Scalable Big Data Benchmarking Methodology", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different from the traditional benchmarking methodology that creates a new\nbenchmark or proxy for every possible workload, this paper presents a scalable\nbig data benchmarking methodology. Among a wide variety of big data analytics\nworkloads, we identify eight big data dwarfs, each of which captures the common\nrequirements of each class of unit of computation while being reasonably\ndivorced from individual implementations. We implement the eight dwarfs on\ndifferent software stacks, e.g., OpenMP, MPI, Hadoop as the dwarf components.\nFor the purpose of architecture simulation, we construct and tune big data\nproxy benchmarks using the directed acyclic graph (DAG)-like combinations of\nthe dwarf components with different weights to mimic the benchmarks in\nBigDataBench. Our proxy benchmarks preserve the micro-architecture, memory, and\nI/O characteristics, and they shorten the simulation time by 100s times while\nmaintain the average micro-architectural data accuracy above 90 percentage on\nboth X86 64 and ARMv8 processors. We will open-source the big data dwarf\ncomponents and proxy benchmarks soon.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 01:55:44 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Gao", "Wanling", ""], ["Wang", "Lei", ""], ["Zhan", "Jianfeng", ""], ["Luo", "Chunjie", ""], ["Zheng", "Daoyi", ""], ["Jia", "Zhen", ""], ["Xie", "Biwei", ""], ["Zheng", "Chen", ""], ["Yang", "Qiang", ""], ["Wang", "Haibin", ""]]}, {"id": "1711.03244", "submitter": "Qianqian Fang", "authors": "Leiming Yu, Fanny Nina-Paravecino, David Kaeli, and Qianqian Fang", "title": "Scalable and massively parallel Monte Carlo photon transport simulations\n  for heterogeneous computing platforms", "comments": "Accepted for Publication in Journal of Biomedical Optics Letters on\n  Jan 4, 2018, to appear in Volume 23, Issue 2", "journal-ref": "J. Biomed. Opt. 23(1), 010504 (2018)", "doi": "10.1117/1.JBO.23.1.010504", "report-no": null, "categories": "cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a highly scalable Monte Carlo (MC) three-dimensional photon\ntransport simulation platform designed for heterogeneous computing systems.\nThrough the development of a massively parallel MC algorithm using the Open\nComputing Language (OpenCL) framework, this research extends our existing\ngraphics processing unit (GPU)-accelerated MC technique to a highly scalable\nvendor-independent heterogeneous computing environment, achieving significantly\nimproved performance and software portability. A number of parallel computing\ntechniques are investigated to achieve portable performance over a wide range\nof computing hardware. Furthermore, multiple thread-level and device-level\nload-balancing strat- egies are developed to obtain efficient simulations using\nmultiple central processing units (CPUs) and GPUs.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 03:39:33 GMT"}, {"version": "v2", "created": "Thu, 25 Jan 2018 05:30:36 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Yu", "Leiming", ""], ["Nina-Paravecino", "Fanny", ""], ["Kaeli", "David", ""], ["Fang", "Qianqian", ""]]}, {"id": "1711.03334", "submitter": "\\'Alvaro L\\'opez Garc\\'ia", "authors": "Miguel Caballer, Sahdev Zala, \\'Alvaro L\\'opez Garc\\'ia, Germ\\'an\n  Molt\\'o, Pablo Orviz Fern\\'andez, Mathieu Velten", "title": "Orchestrating Complex Application Architectures in Heterogeneous Clouds", "comments": null, "journal-ref": "J Grid Computing (2017)", "doi": "10.1007/s10723-017-9418-y", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Private cloud infrastructures are now widely deployed and adopted across\ntechnology industries and research institutions. Although cloud computing has\nemerged as a reality, it is now known that a single cloud provider cannot fully\nsatisfy complex user requirements. This has resulted in a growing interest in\ndeveloping hybrid cloud solutions that bind together distinct and heterogeneous\ncloud infrastructures. In this paper we describe the orchestration approach for\nheterogeneous clouds that has been implemented and used within the\nINDIGO-DataCloud project. This orchestration model uses existing open-source\nsoftware like OpenStack and leverages the OASIS Topology and Specification for\nCloud Applications (TOSCA) open standard as the modeling language. Our approach\nuses virtual machines and Docker containers in an homogeneous and transparent\nway providing consistent application deployment for the users. This approach is\nillustrated by means of two different use cases in different scientific\ncommunities, implemented using the INDIGO-DataCloud solutions.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 11:30:40 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Caballer", "Miguel", ""], ["Zala", "Sahdev", ""], ["Garc\u00eda", "\u00c1lvaro L\u00f3pez", ""], ["Molt\u00f3", "Germ\u00e1n", ""], ["Fern\u00e1ndez", "Pablo Orviz", ""], ["Velten", "Mathieu", ""]]}, {"id": "1711.03359", "submitter": "Michal Dory", "authors": "Keren Censor-Hillel, Michal Dory", "title": "Fast Distributed Approximation for TAP and 2-Edge-Connectivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The tree augmentation problem (TAP) is a fundamental network design problem,\nin which the input is a graph $G$ and a spanning tree $T$ for it, and the goal\nis to augment $T$ with a minimum set of edges $Aug$ from $G$, such that $T \\cup\nAug$ is 2-edge-connected.\n  TAP has been widely studied in the sequential setting. The best known\napproximation ratio of 2 for the weighted case dates back to the work of\nFrederickson and J\\'{a}J\\'{a}, SICOMP 1981. Recently, a 3/2-approximation was\ngiven for unweighted TAP by Kortsarz and Nutov, TALG 2016. Recent breakthroughs\ngive an approximation of 1.458 for unweighted TAP [Grandoni et al., STOC 2018],\nand approximations better than 2 for bounded weights [Adjiashvili, SODA 2017;\nFiorini et al., SODA 2018].\n  In this paper, we provide the first fast distributed approximations for TAP.\nWe present a distributed $2$-approximation for weighted TAP which completes in\n$O(h)$ rounds, where $h$ is the height of $T$. When $h$ is large, we show a\nmuch faster 4-approximation algorithm for the unweighted case, completing in\n$O(D+\\sqrt{n}\\log^*{n})$ rounds, where $n$ is the number of vertices and $D$ is\nthe diameter of $G$.\n  Immediate consequences of our results are an $O(D)$-round 2-approximation\nalgorithm for the minimum size 2-edge-connected spanning subgraph, which\nsignificantly improves upon the running time of previous approximation\nalgorithms, and an $O(h_{MST}+\\sqrt{n}\\log^{*}{n})$-round 3-approximation\nalgorithm for the weighted case, where $h_{MST}$ is the height of the MST of\nthe graph. Additional applications are algorithms for verifying\n2-edge-connectivity and for augmenting the connectivity of any connected\nspanning subgraph to 2.\n  Finally, we complement our study with proving lower bounds for distributed\napproximations of TAP.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 12:58:12 GMT"}, {"version": "v2", "created": "Fri, 10 May 2019 09:31:50 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Censor-Hillel", "Keren", ""], ["Dory", "Michal", ""]]}, {"id": "1711.03386", "submitter": "Pengfei Xu", "authors": "Pengfei Xu, Shaohuai Shi, Xiaowen Chu", "title": "Performance Evaluation of Deep Learning Tools in Docker Containers", "comments": "Conference: BIgCom2017, 9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the success of deep learning techniques in a broad range of application\ndomains, many deep learning software frameworks have been developed and are\nbeing updated frequently to adapt to new hardware features and software\nlibraries, which bring a big challenge for end users and system administrators.\nTo address this problem, container techniques are widely used to simplify the\ndeployment and management of deep learning software. However, it remains\nunknown whether container techniques bring any performance penalty to deep\nlearning applications. The purpose of this work is to systematically evaluate\nthe impact of docker container on the performance of deep learning\napplications. We first benchmark the performance of system components (IO, CPU\nand GPU) in a docker container and the host system and compare the results to\nsee if there's any difference. According to our results, we find that\ncomputational intensive jobs, either running on CPU or GPU, have small overhead\nindicating docker containers can be applied to deep learning programs. Then we\nevaluate the performance of some popular deep learning tools deployed in a\ndocker container and the host system. It turns out that the docker container\nwill not cause noticeable drawbacks while running those deep learning tools. So\nencapsulating deep learning tool in a container is a feasible solution.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 14:28:12 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Xu", "Pengfei", ""], ["Shi", "Shaohuai", ""], ["Chu", "Xiaowen", ""]]}, {"id": "1711.03573", "submitter": "Wahid Bhimji", "authors": "Wahid Bhimji, Steven Andrew Farrell, Thorsten Kurth, Michela Paganini,\n  Prabhat, Evan Racah", "title": "Deep Neural Networks for Physics Analysis on low-level whole-detector\n  data at the LHC", "comments": "Presented at ACAT 2017 Conference, Submitted to J. Phys. Conf. Ser", "journal-ref": null, "doi": null, "report-no": null, "categories": "hep-ex cs.DC cs.LG physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been considerable recent activity applying deep convolutional\nneural nets (CNNs) to data from particle physics experiments. Current\napproaches on ATLAS/CMS have largely focussed on a subset of the calorimeter,\nand for identifying objects or particular particle types. We explore approaches\nthat use the entire calorimeter, combined with track information, for directly\nconducting physics analyses: i.e. classifying events as known-physics\nbackground or new-physics signals.\n  We use an existing RPV-Supersymmetry analysis as a case study and explore\nCNNs on multi-channel, high-resolution sparse images: applied on GPU and\nmulti-node CPU architectures (including Knights Landing (KNL) Xeon Phi nodes)\non the Cori supercomputer at NERSC.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 20:02:59 GMT"}, {"version": "v2", "created": "Wed, 29 Nov 2017 18:03:18 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Bhimji", "Wahid", ""], ["Farrell", "Steven Andrew", ""], ["Kurth", "Thorsten", ""], ["Paganini", "Michela", ""], ["Prabhat", "", ""], ["Racah", "Evan", ""]]}, {"id": "1711.03888", "submitter": "Dingwen Tao", "authors": "Dingwen Tao, Sheng Di, Zizhong Chen, Franck Cappello", "title": "In-Depth Exploration of Single-Snapshot Lossy Compression Techniques for\n  N-Body Simulations", "comments": "Accepted by IEEE BigData 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In situ lossy compression allowing user-controlled data loss can\nsignificantly reduce the I/O burden. For large-scale N-body simulations where\nonly one snapshot can be compressed at a time, the lossy compression ratio is\nvery limited because of the fairly low spatial coherence of the particle data.\nIn this work, we assess the state-of-the-art single-snapshot lossy compression\ntechniques of two common N-body simulation models: cosmology and molecular\ndynamics. We design a series of novel optimization techniques based on the two\nrepresentative real-world N-body simulation codes. For molecular dynamics\nsimulation, we propose three compression modes (i.e., best speed, best\ntradeoff, best compression mode) that can refine the tradeoff between the\ncompression rate (a.k.a., speed/throughput) and ratio. For cosmology\nsimulation, we identify that our improved SZ is the best lossy compressor with\nrespect to both compression ratio and rate. Its compression ratio is higher\nthan the second-best compressor by 11% with comparable compression rate.\nExperiments with up to 1024 cores on the Blues supercomputer at Argonne show\nthat our proposed lossy compression method can reduce I/O time by 80% compared\nwith writing data directly to a parallel file system and outperforms the\nsecond-best solution by 60%. Moreover, our proposed lossy compression methods\nhave the best rate-distortion with reasonable compression errors on the tested\nN-body simulation data compared with state-of-the-art compressors.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 15:42:18 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Tao", "Dingwen", ""], ["Di", "Sheng", ""], ["Chen", "Zizhong", ""], ["Cappello", "Franck", ""]]}, {"id": "1711.03906", "submitter": "Amr Alanwar", "authors": "Amr Alanwar, Henrique Ferraz, Kevin Hsieh, Rohit Thazhath, Paul\n  Martin, Joao Hespanha, Mani Srivastava", "title": "D-SLATS: Distributed Simultaneous Localization and Time Synchronization", "comments": null, "journal-ref": null, "doi": "10.1145/3084041.3084049", "report-no": null, "categories": "cs.LG cs.DC cs.NI cs.RO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Through the last decade, we have witnessed a surge of Internet of Things\n(IoT) devices, and with that a greater need to choreograph their actions across\nboth time and space. Although these two problems, namely time synchronization\nand localization, share many aspects in common, they are traditionally treated\nseparately or combined on centralized approaches that results in an ineffcient\nuse of resources, or in solutions that are not scalable in terms of the number\nof IoT devices. Therefore, we propose D-SLATS, a framework comprised of three\ndifferent and independent algorithms to jointly solve time synchronization and\nlocalization problems in a distributed fashion. The First two algorithms are\nbased mainly on the distributed Extended Kalman Filter (EKF) whereas the third\none uses optimization techniques. No fusion center is required, and the devices\nonly communicate with their neighbors. The proposed methods are evaluated on\ncustom Ultra-Wideband communication Testbed and a quadrotor, representing a\nnetwork of both static and mobile nodes. Our algorithms achieve up to three\nmicroseconds time synchronization accuracy and 30 cm localization error.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 16:26:29 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Alanwar", "Amr", ""], ["Ferraz", "Henrique", ""], ["Hsieh", "Kevin", ""], ["Thazhath", "Rohit", ""], ["Martin", "Paul", ""], ["Hespanha", "Joao", ""], ["Srivastava", "Mani", ""]]}, {"id": "1711.04149", "submitter": "Dominik Pajak", "authors": "Marek Klonowski, Dominik Paj\\k{a}k", "title": "Broadcast in radio networks: time vs. energy tradeoffs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In wireless networks, consisting of battery-powered devices, energy is a\ncostly resource and most of it is spent on transmitting and receiving messages.\nBroadcast is a problem where a message needs to be transmitted from one node to\nall other nodes of the network. We study algorithms that can work under limited\nenergy measured as the maximum number of transmissions by a single station. The\ngoal of the paper is to study tradeoffs between time and energy complexity of\nbroadcast problem in multi-hop radio networks. We consider a model where the\ntopology of the network is unknown and if two neighbors of a station are\ntransmitting in the same discrete time slot, then the signals collide and the\nreceiver cannot distinguish the collided signals from silence.\n  We observe that existing, time efficient, algorithms are not optimized with\nrespect to energy expenditure. We then propose and analyse two new randomized\nenergy-efficient algorithms. Our first algorithm works in time\n$O((D+\\varphi)\\cdot n^{1/\\varphi}\\cdot \\varphi)$ with high probability and uses\n$O(\\varphi)$ energy per station for any $\\varphi \\leq \\log n/(2\\log\\log n)$ for\nany graph with $n$ nodes and diameter $D$. Our second algorithm works in time\n$O((D+\\log n)\\log n)$ with high probability and uses $O(\\log n/\\log\\log n)$\nenergy.\n  We prove that our algorithms are almost time-optimal for given energy limits\nfor graphs with constant diameters by constructing lower bound on time of\n$\\Omega(n^{1/\\varphi} \\cdot \\varphi)$. The lower bound shows also that any\nalgorithm working in polylogaritmic time in $n$ for all graphs needs energy\n$\\Omega(\\log n/\\log\\log n)$.\n", "versions": [{"version": "v1", "created": "Sat, 11 Nov 2017 15:17:40 GMT"}, {"version": "v2", "created": "Sun, 13 May 2018 18:50:54 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Klonowski", "Marek", ""], ["Paj\u0105k", "Dominik", ""]]}, {"id": "1711.04189", "submitter": "Valmir C. Barbosa", "authors": "Ricardo M. Oliveira, Flavio B. Gonzaga, Valmir C. Barbosa, Geraldo B.\n  Xex\\'eo", "title": "A distributed system for SearchOnMath based on the Microsoft BizSpark\n  program", "comments": null, "journal-ref": "Proceedings of the 33rd Brazilian Symposium on Databases, 289-294,\n  2018", "doi": null, "report-no": null, "categories": "cs.IR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mathematical information retrieval is a relatively new area, so the first\nsearch tools capable of retrieving mathematical formulas began to appear only a\nfew years ago. The proposals made public so far mostly implement searches on\ninternal university databases, small sets of scientific papers, or Wikipedia in\nEnglish. As such, only modest computing power is required. In this context,\nSearchOnMath has emerged as a pioneering tool in that it indexes several\ndifferent databases and is compatible with several mathematical representation\nlanguages. Given the significantly greater number of formulas it handles, a\ndistributed system becomes necessary to support it. The present study is based\non the Microsoft BizSpark program and has aimed, for 38 different\ndistributed-system scenarios, to pinpoint the one affording the best response\ntimes when searching the SearchOnMath databases for a collection of 120\nformulas.\n", "versions": [{"version": "v1", "created": "Sat, 11 Nov 2017 20:12:42 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Oliveira", "Ricardo M.", ""], ["Gonzaga", "Flavio B.", ""], ["Barbosa", "Valmir C.", ""], ["Xex\u00e9o", "Geraldo B.", ""]]}, {"id": "1711.04325", "submitter": "Takuya Akiba", "authors": "Takuya Akiba, Shuji Suzuki, Keisuke Fukuda", "title": "Extremely Large Minibatch SGD: Training ResNet-50 on ImageNet in 15\n  Minutes", "comments": "NIPS'17 Workshop: Deep Learning at Supercomputer Scale", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate that training ResNet-50 on ImageNet for 90 epochs can be\nachieved in 15 minutes with 1024 Tesla P100 GPUs. This was made possible by\nusing a large minibatch size of 32k. To maintain accuracy with this large\nminibatch size, we employed several techniques such as RMSprop warm-up, batch\nnormalization without moving averages, and a slow-start learning rate schedule.\nThis paper also describes the details of the hardware and software of the\nsystem used to achieve the above performance.\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 17:36:46 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Akiba", "Takuya", ""], ["Suzuki", "Shuji", ""], ["Fukuda", "Keisuke", ""]]}, {"id": "1711.04471", "submitter": "Wim Vanderbauwhede", "authors": "Wim Vanderbauwhede, Gavin Davidson", "title": "Domain-Specific Acceleration and Auto-Parallelization of Legacy\n  Scientific Code in FORTRAN 77 using Source-to-Source Compilation", "comments": "12 pages, 5 figures, submitted to \"Computers and Fluids\" as full\n  paper from ParCFD conference entry", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massively parallel accelerators such as GPGPUs, manycores and FPGAs represent\na powerful and affordable tool for scientists who look to speed up simulations\nof complex systems. However, porting code to such devices requires a detailed\nunderstanding of heterogeneous programming tools and effective strategies for\nparallelization. In this paper we present a source to source compilation\napproach with whole-program analysis to automatically transform single-threaded\nFORTRAN 77 legacy code into OpenCL-accelerated programs with parallelized\nkernels.\n  The main contributions of our work are: (1) whole-source refactoring to allow\nany subroutine in the code to be offloaded to an accelerator. (2) Minimization\nof the data transfer between the host and the accelerator by eliminating\nredundant transfers. (3) Pragmatic auto-parallelization of the code to be\noffloaded to the accelerator by identification of parallelizable maps and\nreductions.\n  We have validated the code transformation performance of the compiler on the\nNIST FORTRAN 78 test suite and several real-world codes: the Large Eddy\nSimulator for Urban Flows, a high-resolution turbulent flow model; the shallow\nwater component of the ocean model Gmodel; the Linear Baroclinic Model, an\natmospheric climate model and Flexpart-WRF, a particle dispersion simulator.\n  The automatic parallelization component has been tested on as 2-D Shallow\nWater model (2DSW) and on the Large Eddy Simulator for Urban Flows (UFLES) and\nproduces a complete OpenCL-enabled code base. The fully OpenCL-accelerated\nversions of the 2DSW and the UFLES are resp. 9x and 20x faster on GPU than the\noriginal code on CPU, in both cases this is the same performance as manually\nported code.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 08:56:43 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Vanderbauwhede", "Wim", ""], ["Davidson", "Gavin", ""]]}, {"id": "1711.04489", "submitter": "Yang Yang", "authors": "Yang Yang, Marius Pesavento", "title": "A Parallel Best-Response Algorithm with Exact Line Search for Nonconvex\n  Sparsity-Regularized Rank Minimization", "comments": "Submitted to IEEE ICASSP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a convergent parallel best-response algorithm with\nthe exact line search for the nondifferentiable nonconvex sparsity-regularized\nrank minimization problem. On the one hand, it exhibits a faster convergence\nthan subgradient algorithms and block coordinate descent algorithms. On the\nother hand, its convergence to a stationary point is guaranteed, while ADMM\nalgorithms only converge for convex problems. Furthermore, the exact line\nsearch procedure in the proposed algorithm is performed efficiently in\nclosed-form to avoid the meticulous choice of stepsizes, which is however a\ncommon bottleneck in subgradient algorithms and successive convex approximation\nalgorithms. Finally, the proposed algorithm is numerically tested.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 09:28:43 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Yang", "Yang", ""], ["Pesavento", "Marius", ""]]}, {"id": "1711.04556", "submitter": "Libor Bukata", "authors": "Libor Bukata, Premysl Sucha, Zdenek Hanzalek", "title": "Solving the Resource Constrained Project Scheduling Problem Using the\n  Parallel Tabu Search Designed for the CUDA Platform", "comments": "Published in Journal of Parallel and Distributed Computing", "journal-ref": "Journal of Parallel and Distributed Computing, 77 (2015), 58-68", "doi": "10.1016/j.jpdc.2014.11.005", "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the paper, a parallel Tabu Search algorithm for the Resource Constrained\nProject Scheduling Problem is proposed. To deal with this NP-hard combinatorial\nproblem many optimizations have been performed. For example, a resource\nevaluation algorithm is selected by a heuristic and an effective Tabu List was\ndesigned. In addition to that, a capacity-indexed resource evaluation algorithm\nwas proposed and the GPU (Graphics Processing Unit) version uses a homogeneous\nmodel to reduce the required communication bandwidth. According to the\nexperiments, the GPU version outperforms the optimized parallel CPU version\nwith respect to the computational time and the quality of solutions. In\ncomparison with other existing heuristics, the proposed solution often gives\nbetter quality solutions.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 12:52:49 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Bukata", "Libor", ""], ["Sucha", "Premysl", ""], ["Hanzalek", "Zdenek", ""]]}, {"id": "1711.04628", "submitter": "Mansaf Alam Dr", "authors": "Samiya Khan and Mansaf Alam", "title": "On Designing a Generic Framework for Cloud-based Big Data Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data analytics has gathered immense research attention lately because of\nits ability to harness useful information from heaps of data. Cloud computing\nhas been adjudged as one of the best infrastructural solutions for\nimplementation of big data analytics. This research paper proposes a five-layer\nmodel for cloud-based big data analytics that uses dew computing and edge\ncomputing concepts. Besides this, the paper also presents an approach for\ncreation of custom big data stack by selecting technologies on the basis of\nidentified data and computing models for the application\n", "versions": [{"version": "v1", "created": "Sat, 4 Nov 2017 13:14:16 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Khan", "Samiya", ""], ["Alam", "Mansaf", ""]]}, {"id": "1711.04709", "submitter": "Ad\\'an S\\'anchez De Pedro Crespo", "authors": "Ad\\'an S\\'anchez de Pedro Crespo and Luis Iv\\'an Cuende Garc\\'ia", "title": "Stampery Blockchain Timestamping Architecture (BTA) - Version 6", "comments": "21 pages, 16 figures", "journal-ref": null, "doi": "10.13140/RG.2.2.17223.80805", "report-no": null, "categories": "cs.CR cs.DC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A method for timestamping, anchoring and certification of a virtually\nunlimited amount of data in one or more blockchains, focusing on scalability\nand cost-effectiveness while ensuring existence, integrity and ownership by\nusing cryptographic proofs that are independently verifiable by anyone in the\nworld without disclosure of the original data and without the intervention of\nthe certifying party.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 17:17:03 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Crespo", "Ad\u00e1n S\u00e1nchez de Pedro", ""], ["Garc\u00eda", "Luis Iv\u00e1n Cuende", ""]]}, {"id": "1711.04728", "submitter": "Moshe Sulamy", "authors": "Yehuda Afek, Shaked Rafaeli, Moshe Sulamy", "title": "Cheating by Duplication: Equilibrium Requires Global Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The question of what global information must distributed rational agents\na-priori know about the network in order for equilibrium to be possible is\nresearched here. Until now, distributed algorithms with rational agents have\nassumed that $n$, the size of the network, is a-priori known to the\nparticipants. We investigate the above question, considering different\ndistributed computing problems and showing how much each agent must a-priori\nknow about $n$ in order for distributed algorithms to be equilibria. The main\ntool considered throughout the paper is the advantage an agent may gain by\nduplication- pretending to be more than one agent.\n  We start by proving that when no bound on $n$ is given equilibrium for\nColoring and Knowledge Sharing is impossible. %We prove that when agents have\nno a-priori knowledge on $n$, or even a known bound, equilibrium for both\nKnowledge Sharing and Coloring is impossible. We provide new algorithms for\nboth problems when $n$ \\emph{is} a-priori known to all agents, thus showing\nthat there are algorithms in which the only way for an agent to gain an\nadvantage is duplication. We further show that for each distributed problem\nthere is an a-priori known range, an upper and a lower bound on $n$, such that\nif the actual $n$ is guaranteed to lay in that range, equilibrium is possible.\nBy providing equilibria for a specific range, and impossibility results for any\nlarger range, we prove the tight range necessary for equilibrium in: Leader\nElection, Knowledge Sharing, Coloring, Partition and Orientation.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 17:55:45 GMT"}, {"version": "v2", "created": "Sun, 8 Apr 2018 08:04:55 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Afek", "Yehuda", ""], ["Rafaeli", "Shaked", ""], ["Sulamy", "Moshe", ""]]}, {"id": "1711.04883", "submitter": "Peter Boyle", "authors": "Peter Boyle, Michael Chuvelev, Guido Cossu, Christopher Kelly,\n  Christoph Lehner, Lawrence Meadows", "title": "Accelerating HPC codes on Intel(R) Omni-Path Architecture networks: From\n  particle physics to Machine Learning", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI hep-lat", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss practical methods to ensure near wirespeed performance from\nclusters with either one or two Intel(R) Omni-Path host fabric interfaces (HFI)\nper node, and Intel(R) Xeon Phi(TM) 72xx (Knight's Landing) processors, and\nusing the Linux operating system.\n  The study evaluates the performance improvements achievable and the required\nprogramming approaches in two distinct example problems: firstly in Cartesian\ncommunicator halo exchange problems, appropriate for structured grid PDE\nsolvers that arise in quantum chromodynamics simulations of particle physics,\nand secondly in gradient reduction appropriate to synchronous stochastic\ngradient descent for machine learning. As an example, we accelerate a published\nBaidu Research reduction code and obtain a factor of ten speedup over the\noriginal code using the techniques discussed in this paper. This displays how a\nfactor of ten speedup in strongly scaled distributed machine learning could be\nachieved when synchronous stochastic gradient descent is massively parallelised\nwith a fixed mini-batch size.\n  We find a significant improvement in performance robustness when memory is\nobtained using carefully allocated 2MB \"huge\" virtual memory pages, implying\nthat either non-standard allocation routines should be used for communication\nbuffers. These can be accessed via a LD\\_PRELOAD override in the manner\nsuggested by libhugetlbfs. We make use of a the Intel(R) MPI 2019 library\n\"Technology Preview\" and underlying software to enable thread concurrency\nthroughout the communication software stake via multiple PSM2 endpoints per\nprocess and use of multiple independent MPI communicators. When using a single\nMPI process per node, we find that this greatly accelerates delivered bandwidth\nin many core Intel(R) Xeon Phi processors.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 22:51:30 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Boyle", "Peter", ""], ["Chuvelev", "Michael", ""], ["Cossu", "Guido", ""], ["Kelly", "Christopher", ""], ["Lehner", "Christoph", ""], ["Meadows", "Lawrence", ""]]}, {"id": "1711.04969", "submitter": "Can Karakus", "authors": "Can Karakus, Yifan Sun, Suhas Diggavi, Wotao Yin", "title": "Straggler Mitigation in Distributed Optimization Through Data Encoding", "comments": "appeared at NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Slow running or straggler tasks can significantly reduce computation speed in\ndistributed computation. Recently, coding-theory-inspired approaches have been\napplied to mitigate the effect of straggling, through embedding redundancy in\ncertain linear computational steps of the optimization algorithm, thus\ncompleting the computation without waiting for the stragglers. In this paper,\nwe propose an alternate approach where we embed the redundancy directly in the\ndata itself, and allow the computation to proceed completely oblivious to\nencoding. We propose several encoding schemes, and demonstrate that popular\nbatch algorithms, such as gradient descent and L-BFGS, applied in a\ncoding-oblivious manner, deterministically achieve sample path linear\nconvergence to an approximate solution of the original problem, using an\narbitrarily varying subset of the nodes at each iteration. Moreover, this\napproximation can be controlled by the amount of redundancy and the number of\nnodes used in each iteration. We provide experimental results demonstrating the\nadvantage of the approach over uncoded and data replication strategies.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 06:29:41 GMT"}, {"version": "v2", "created": "Mon, 22 Jan 2018 23:28:11 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Karakus", "Can", ""], ["Sun", "Yifan", ""], ["Diggavi", "Suhas", ""], ["Yin", "Wotao", ""]]}, {"id": "1711.05136", "submitter": "Guillaume Bellec", "authors": "Guillaume Bellec, David Kappel, Wolfgang Maass and Robert Legenstein", "title": "Deep Rewiring: Training very sparse deep networks", "comments": "Accepted for publication at ICLR 2018. 10 pages (12 with references,\n  24 with appendix), 4 Figures in the main text. Reviews are available at:\n  https://openreview.net/forum?id=BJ_wN01C- . This recent version contains\n  minor corrections in the appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuromorphic hardware tends to pose limits on the connectivity of deep\nnetworks that one can run on them. But also generic hardware and software\nimplementations of deep learning run more efficiently for sparse networks.\nSeveral methods exist for pruning connections of a neural network after it was\ntrained without connectivity constraints. We present an algorithm, DEEP R, that\nenables us to train directly a sparsely connected neural network. DEEP R\nautomatically rewires the network during supervised training so that\nconnections are there where they are most needed for the task, while its total\nnumber is all the time strictly bounded. We demonstrate that DEEP R can be used\nto train very sparse feedforward and recurrent neural networks on standard\nbenchmark tasks with just a minor loss in performance. DEEP R is based on a\nrigorous theoretical foundation that views rewiring as stochastic sampling of\nnetwork configurations from a posterior.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 15:02:47 GMT"}, {"version": "v2", "created": "Fri, 15 Dec 2017 18:33:53 GMT"}, {"version": "v3", "created": "Fri, 2 Feb 2018 15:57:44 GMT"}, {"version": "v4", "created": "Mon, 5 Feb 2018 11:01:41 GMT"}, {"version": "v5", "created": "Tue, 7 Aug 2018 18:12:10 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Bellec", "Guillaume", ""], ["Kappel", "David", ""], ["Maass", "Wolfgang", ""], ["Legenstein", "Robert", ""]]}, {"id": "1711.05244", "submitter": "Ravi  Tandon", "authors": "Maryam Abdul-Wahid, Firas Almoualem, Deepak Kumar, Ravi Tandon", "title": "Private Information Retrieval from Storage Constrained Databases --\n  Coded Caching meets PIR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Private information retrieval (PIR) allows a user to retrieve a desired\nmessage out of $K$ possible messages from $N$ databases without revealing the\nidentity of the desired message. Majority of existing works on PIR assume the\npresence of replicated databases, each storing all the $K$ messages. In this\nwork, we consider the problem of PIR from storage constrained databases. Each\ndatabase has a storage capacity of $\\mu KL$ bits, where $K$ is the number of\nmessages, $L$ is the size of each message in bits, and $\\mu \\in [1/N, 1]$ is\nthe normalized storage. In the storage constrained PIR problem, there are two\nkey design questions: a) how to store content across each database under\nstorage constraints; and b) construction of schemes that allow efficient PIR\nthrough storage constrained databases. The main contribution of this work is a\ngeneral achievable scheme for PIR from storage constrained databases for any\nvalue of storage. In particular, for any $(N,K)$, with normalized storage $\\mu=\nt/N$, where the parameter $t$ can take integer values $t \\in \\{1, 2, \\ldots,\nN\\}$, we show that our proposed PIR scheme achieves a download cost of\n$\\left(1+ \\frac{1}{t}+ \\frac{1}{t^{2}}+ \\cdots + \\frac{1}{t^{K-1}}\\right)$. The\nextreme case when $\\mu=1$ (i.e., $t=N$) corresponds to the setting of\nreplicated databases with full storage. For this extremal setting, our scheme\nrecovers the information-theoretically optimal download cost characterized by\nSun and Jafar as $\\left(1+ \\frac{1}{N}+ \\cdots + \\frac{1}{N^{K-1}}\\right)$. For\nthe other extreme, when $\\mu= 1/N$ (i.e., $t=1$), the proposed scheme achieves\na download cost of $K$. The interesting aspect of the result is that for\nintermediate values of storage, i.e., $1/N < \\mu <1$, the proposed scheme can\nstrictly outperform memory-sharing between extreme values of storage.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 18:39:00 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Abdul-Wahid", "Maryam", ""], ["Almoualem", "Firas", ""], ["Kumar", "Deepak", ""], ["Tandon", "Ravi", ""]]}, {"id": "1711.05429", "submitter": "Alok Singh", "authors": "Alok Singh, Mai Nguyen, Shweta Purawat, Daniel Crawl, Ilkay Altintas", "title": "Modular Resource Centric Learning for Workflow Performance Prediction", "comments": "This paper was presented at: 6th Workshop on Big Data Analytics:\n  Challenges, and Opportunities (BDAC) at the 27th IEEE/ACM International\n  Conference for High Performance Computing, Networking, Storage, and Analysis\n  (SC 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Workflows provide an expressive programming model for fine-grained control of\nlarge-scale applications in distributed computing environments. Accurate\nestimates of complex workflow execution metrics on large-scale machines have\nseveral key advantages. The performance of scheduling algorithms that rely on\nestimates of execution metrics degrades when the accuracy of predicted\nexecution metrics decreases. This in-progress paper presents a technique being\ndeveloped to improve the accuracy of predicted performance metrics of\nlarge-scale workflows on distributed platforms. The central idea of this work\nis to train resource-centric machine learning agents to capture complex\nrelationships between a set of program instructions and their performance\nmetrics when executed on a specific resource. This resource-centric view of a\nworkflow exploits the fact that predicting execution times of sub-modules of a\nworkflow requires monitoring and modeling of a few dynamic and static features.\nWe transform the input workflow that is essentially a directed acyclic graph of\nactions into a Physical Resource Execution Plan (PREP). This transformation\nenables us to model an arbitrarily complex workflow as a set of simpler\nprograms running on physical nodes. We delegate a machine learning model to\ncapture performance metrics for each resource type when it executes different\nprogram instructions under varying degrees of resource contention. Our\nalgorithm takes the prediction metrics from each resource agent and composes\nthe overall workflow performance metrics by utilizing the structure of the\ncorresponding Physical Resource Execution Plan.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 06:58:47 GMT"}, {"version": "v2", "created": "Thu, 16 Nov 2017 01:57:07 GMT"}, {"version": "v3", "created": "Tue, 17 Apr 2018 06:26:23 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Singh", "Alok", ""], ["Nguyen", "Mai", ""], ["Purawat", "Shweta", ""], ["Crawl", "Daniel", ""], ["Altintas", "Ilkay", ""]]}, {"id": "1711.05469", "submitter": "Yannic Maus", "authors": "Mohsen Ghaffari, Fabian Kuhn, Yannic Maus, Jara Uitto", "title": "Deterministic Distributed Edge-Coloring with Fewer Colors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deterministic distributed algorithm, in the LOCAL model, that\ncomputes a $(1+o(1))\\Delta$-edge-coloring in polylogarithmic-time, so long as\nthe maximum degree $\\Delta=\\tilde{\\Omega}(\\log n)$. For smaller $\\Delta$, we\ngive a polylogarithmic-time $3\\Delta/2$-edge-coloring. These are the first\ndeterministic algorithms to go below the natural barrier of $2\\Delta-1$ colors,\nand they improve significantly on the recent polylogarithmic-time\n$(2\\Delta-1)(1+o(1))$-edge-coloring of Ghaffari and Su [SODA'17] and the\n$(2\\Delta-1)$-edge-coloring of Fischer, Ghaffari, and Kuhn [FOCS'17],\npositively answering the main open question of the latter. The key technical\ningredient of our algorithm is a simple and novel gradual packing of\njudiciously chosen near-maximum matchings, each of which becomes one of the\ncolor classes.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 09:22:14 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Ghaffari", "Mohsen", ""], ["Kuhn", "Fabian", ""], ["Maus", "Yannic", ""], ["Uitto", "Jara", ""]]}, {"id": "1711.05518", "submitter": "Dawand Sulaiman", "authors": "Dawand Sulaiman and Adam Barker", "title": "MAMoC: Multisite Adaptive Offloading Framework for Mobile Cloud\n  Applications", "comments": "8 pages, CloudCom 2017 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents MAMoC, a framework which brings together a diverse range\nof infrastructure types including mobile devices, cloudlets, and remote cloud\nresources under one unified API. MAMoC allows mobile applications to leverage\nthe power of multiple offloading destinations. MAMoC's intelligent offloading\ndecision engine adapts to the contextual changes in this heterogeneous\nenvironment, in order to reduce the overall runtime for both single-site and\nmulti-site offloading scenarios. MAMoC is evaluated through a set of offloading\nexperiments, which evaluate the performance of our offloading decision engine.\nThe results show that offloading computation using our framework can reduce the\noverall task completion time for both single-site and multi-site offloading\nscenarios.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 12:24:04 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Sulaiman", "Dawand", ""], ["Barker", "Adam", ""]]}, {"id": "1711.05573", "submitter": "Jia Zou", "authors": "Jia Zou, R. Matthew Barnett, Tania Lorido-Botran, Shangyu Luo, Carlos\n  Monroy, Sourav Sikdar, Kia Teymourian, Binhang Yuan, Chris Jermaine", "title": "PlinyCompute: A Platform for High-Performance, Distributed,\n  Data-Intensive Tool Development", "comments": "48 pages, including references and Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes PlinyCompute, a system for development of\nhigh-performance, data-intensive, distributed computing tools and libraries. In\nthe large, PlinyCompute presents the programmer with a very high-level,\ndeclarative interface, relying on automatic, relational-database style\noptimization to figure out how to stage distributed computations. However, in\nthe small, PlinyCompute presents the capable systems programmer with a\npersistent object data model and API (the \"PC object model\") and associated\nmemory management system that has been designed from the ground-up for high\nperformance, distributed, data-intensive computing. This contrasts with most\nother Big Data systems, which are constructed on top of the Java Virtual\nMachine (JVM), and hence must at least partially cede performance-critical\nconcerns such as memory management (including layout and de/allocation) and\nvirtual method/function dispatch to the JVM. This hybrid approach---declarative\nin the large, trusting the programmer's ability to utilize PC object model\nefficiently in the small---results in a system that is ideal for the\ndevelopment of reusable, data-intensive tools and libraries. Through extensive\nbenchmarking, we show that implementing complex objects manipulation and\nnon-trivial, library-style computations on top of PlinyCompute can result in a\nspeedup of 2x to more than 50x or more compared to equivalent implementations\non Spark.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 14:01:06 GMT"}, {"version": "v2", "created": "Thu, 16 Nov 2017 02:30:18 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Zou", "Jia", ""], ["Barnett", "R. Matthew", ""], ["Lorido-Botran", "Tania", ""], ["Luo", "Shangyu", ""], ["Monroy", "Carlos", ""], ["Sikdar", "Sourav", ""], ["Teymourian", "Kia", ""], ["Yuan", "Binhang", ""], ["Jermaine", "Chris", ""]]}, {"id": "1711.05734", "submitter": "Francesco Conti", "authors": "Francesco Conti, Lukas Cavigelli, Gianna Paulin, Igor Susmelj, Luca\n  Benini", "title": "Chipmunk: A Systolically Scalable 0.9 mm${}^2$, 3.08 Gop/s/mW @ 1.2 mW\n  Accelerator for Near-Sensor Recurrent Neural Network Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.NE cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) are state-of-the-art in voice\nawareness/understanding and speech recognition. On-device computation of RNNs\non low-power mobile and wearable devices would be key to applications such as\nzero-latency voice-based human-machine interfaces. Here we present Chipmunk, a\nsmall (<1 mm${}^2$) hardware accelerator for Long-Short Term Memory RNNs in UMC\n65 nm technology capable to operate at a measured peak efficiency up to 3.08\nGop/s/mW at 1.24 mW peak power. To implement big RNN models without incurring\nin huge memory transfer overhead, multiple Chipmunk engines can cooperate to\nform a single systolic array. In this way, the Chipmunk architecture in a 75\ntiles configuration can achieve real-time phoneme extraction on a demanding RNN\ntopology proposed by Graves et al., consuming less than 13 mW of average power.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 10:15:44 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 21:43:55 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Conti", "Francesco", ""], ["Cavigelli", "Lukas", ""], ["Paulin", "Gianna", ""], ["Susmelj", "Igor", ""], ["Benini", "Luca", ""]]}, {"id": "1711.05932", "submitter": "Andreas Weichslgartner", "authors": "Andreas Weichslgartner, Stefan Wildermann, Deepak Gangadharan, Michael\n  Gla{\\ss}, J\\\"urgen Teich", "title": "A Design-Time/Run-Time Application Mapping Methodology for Predictable\n  Execution Time in MPSoCs", "comments": null, "journal-ref": null, "doi": "10.1145/3274665", "report-no": null, "categories": "cs.DC cs.MA cs.NI cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Executing multiple applications on a single MPSoC brings the major challenge\nof satisfying multiple quality requirements regarding real-time, energy, etc.\nHybrid application mapping denotes the combination of design-time analysis with\nrun-time application mapping. In this article, we present such a methodology,\nwhich comprises a design space exploration coupled with a formal performance\nanalysis. This results in several resource reservation configurations,\noptimized for multiple objectives, with verified real-time guarantees for each\nindividual application. The Pareto-optimal configurations are handed over to\nrun-time management which searches for a suitable mapping according to this\ninformation. To provide any real-time guarantees, the performance analysis\nneeds to be composable and the influence of the applications on each other has\nto be bounded. We achieve this either by spatial or a novel temporal isolation\nfor tasks and by exploiting composable NoCs. With the proposed temporal\nisolation, tasks of different applications can be mapped to the same resource\nwhile with spatial isolation, one computing resource can be exclusively used by\nonly one application. The experiments reveal that the success rate in finding\nfeasible application mappings can be increased by the proposed temporal\nisolation by up to 30% and energy consumption can be reduced compared to\nspatial isolation.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 05:31:42 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Weichslgartner", "Andreas", ""], ["Wildermann", "Stefan", ""], ["Gangadharan", "Deepak", ""], ["Gla\u00df", "Michael", ""], ["Teich", "J\u00fcrgen", ""]]}, {"id": "1711.05938", "submitter": "Yang Zhang", "authors": "Zehui Xiong, Yang Zhang, Dusit Niyato, Ping Wang and Zhu Han", "title": "When Mobile Blockchain Meets Edge Computing", "comments": "Accepted by IEEE Communications Magazine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain, as the backbone technology of the current popular Bitcoin digital\ncurrency, has become a promising decentralized data management framework.\nAlthough blockchain has been widely adopted in many applications, e.g.,\nfinance, healthcare, and logistics, its application in mobile services is still\nlimited. This is due to the fact that blockchain users need to solve preset\nproof-of-work puzzles to add new data, i.e., a block, to the blockchain.\nSolving the proof-of-work, however, consumes substantial resources in terms of\nCPU time and energy, which is not suitable for resource-limited mobile devices.\nTo facilitate blockchain applications in future mobile Internet of Things\nsystems, multiple access mobile edge computing appears to be an auspicious\nsolution to solve the proof-of-work puzzles for mobile users. We first\nintroduce a novel concept of edge computing for mobile blockchain. Then, we\nintroduce an economic approach for edge computing resource management.\nMoreover, a prototype of mobile edge computing enabled blockchain systems is\npresented with experimental results to justify the proposed concept.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 05:53:57 GMT"}, {"version": "v2", "created": "Wed, 11 Apr 2018 23:14:28 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Xiong", "Zehui", ""], ["Zhang", "Yang", ""], ["Niyato", "Dusit", ""], ["Wang", "Ping", ""], ["Han", "Zhu", ""]]}, {"id": "1711.05979", "submitter": "Shaohuai Shi", "authors": "Shaohuai Shi, Qiang Wang and Xiaowen Chu", "title": "Performance Modeling and Evaluation of Distributed Deep Learning\n  Frameworks on GPUs", "comments": "Published at DataCom'2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning frameworks have been widely deployed on GPU servers for deep\nlearning applications in both academia and industry. In training deep neural\nnetworks (DNNs), there are many standard processes or algorithms, such as\nconvolution and stochastic gradient descent (SGD), but the running performance\nof different frameworks might be different even running the same deep model on\nthe same GPU hardware. In this study, we evaluate the running performance of\nfour state-of-the-art distributed deep learning frameworks (i.e., Caffe-MPI,\nCNTK, MXNet, and TensorFlow) over single-GPU, multi-GPU, and multi-node\nenvironments. We first build performance models of standard processes in\ntraining DNNs with SGD, and then we benchmark the running performance of these\nframeworks with three popular convolutional neural networks (i.e., AlexNet,\nGoogleNet and ResNet-50), after that, we analyze what factors that result in\nthe performance gap among these four frameworks. Through both analytical and\nexperimental analysis, we identify bottlenecks and overheads which could be\nfurther optimized. The main contribution is that the proposed performance\nmodels and the analysis provide further optimization directions in both\nalgorithmic design and system configuration.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 08:20:13 GMT"}, {"version": "v2", "created": "Fri, 8 Dec 2017 06:47:47 GMT"}, {"version": "v3", "created": "Mon, 20 Aug 2018 06:18:07 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Shi", "Shaohuai", ""], ["Wang", "Qiang", ""], ["Chu", "Xiaowen", ""]]}, {"id": "1711.06127", "submitter": "R\\\"udiger G\\\"obl", "authors": "R\\\"udiger G\\\"obl, Nassir Navab, Christoph Hennersperger", "title": "SUPRA: Open Source Software Defined Ultrasound Processing for Real-Time\n  Applications", "comments": "This is a pre-print of an article published in the International\n  Journal of Computer Assisted Radiology and Surgery. The final authenticated\n  version is available online at: https://doi.org/10.1007/s11548-018-1750-6", "journal-ref": "G\\\"obl, R., Navab, N. & Hennersperger, C. , \"SUPRA: Open Source\n  Software Defined Ultrasound Processing for Real-Time Applications\" Int J CARS\n  (2018)", "doi": "10.1007/s11548-018-1750-6", "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in ultrasound imaging is limited in reproducibility by two factors:\nFirst, many existing ultrasound pipelines are protected by intellectual\nproperty, rendering exchange of code difficult. Second, most pipelines are\nimplemented in special hardware, resulting in limited flexibility of\nimplemented processing steps on such platforms.\n  Methods: With SUPRA we propose an open-source pipeline for fully Software\nDefined Ultrasound Processing for Real-time Applications to alleviate these\nproblems. Covering all steps from beamforming to output of B-mode images, SUPRA\ncan help improve the reproducibility of results and make modifications to the\nimage acquisition mode accessible to the research community. We evaluate the\npipeline qualitatively, quantitatively, and regarding its run-time.\n  Results: The pipeline shows image quality comparable to a clinical system and\nbacked by point-spread function measurements a comparable resolution. Including\nall processing stages of a usual ultrasound pipeline, the run-time analysis\nshows that it can be executed in 2D and 3D on consumer GPUs in real-time.\n  Conclusions: Our software ultrasound pipeline opens up the research in image\nacquisition. Given access to ultrasound data from early stages (raw channel\ndata, radiofrequency data) it simplifies the development in imaging.\nFurthermore, it tackles the reproducibility of research results, as code can be\nshared easily and even be executed without dedicated ultrasound hardware.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 15:12:35 GMT"}, {"version": "v2", "created": "Fri, 17 Nov 2017 18:04:11 GMT"}, {"version": "v3", "created": "Thu, 10 May 2018 15:45:57 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["G\u00f6bl", "R\u00fcdiger", ""], ["Navab", "Nassir", ""], ["Hennersperger", "Christoph", ""]]}, {"id": "1711.06315", "submitter": "Sanchari Sen", "authors": "Sanchari Sen, Shubham Jain, Swagath Venkataramani, Anand Raghunathan", "title": "SparCE: Sparsity aware General Purpose Core Extensions to Accelerate\n  Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have emerged as the method of choice for solving\na wide range of machine learning tasks. The enormous computational demands\nposed by DNNs have most commonly been addressed through the design of custom\naccelerators. However, these accelerators are prohibitive in many design\nscenarios (e.g., wearable devices and IoT sensors), due to stringent area/cost\nconstraints. Accelerating DNNs on these low-power systems, comprising of mainly\nthe general-purpose processor (GPP) cores, requires new approaches. We improve\nthe performance of DNNs on GPPs by exploiting a key attribute of DNNs, i.e.,\nsparsity. We propose Sparsity aware Core Extensions (SparCE)- a set of\nmicro-architectural and ISA extensions that leverage sparsity and are minimally\nintrusive and low-overhead. We dynamically detect zero operands and skip a set\nof future instructions that use it. Our design ensures that the instructions to\nbe skipped are prevented from even being fetched, as squashing instructions\ncomes with a penalty. SparCE consists of 2 key micro-architectural\nenhancements- a Sparsity Register File (SpRF) that tracks zero registers and a\nSparsity aware Skip Address (SASA) table that indicates instructions to be\nskipped. When an instruction is fetched, SparCE dynamically pre-identifies\nwhether the following instruction(s) can be skipped and appropriately modifies\nthe program counter, thereby skipping the redundant instructions and improving\nperformance. We model SparCE using the gem5 architectural simulator, and\nevaluate our approach on 6 image-recognition DNNs in the context of both\ntraining and inference using the Caffe framework. On a scalar microprocessor,\nSparCE achieves 19%-31% reduction in application-level. We also evaluate SparCE\non a 4-way SIMD ARMv8 processor using the OpenBLAS library, and demonstrate\nthat SparCE achieves 8%-15% reduction in the application-level execution time.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 01:20:19 GMT"}, {"version": "v2", "created": "Wed, 29 Nov 2017 16:42:03 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Sen", "Sanchari", ""], ["Jain", "Shubham", ""], ["Venkataramani", "Swagath", ""], ["Raghunathan", "Anand", ""]]}, {"id": "1711.06433", "submitter": "Clement Mommessin", "authors": "Marcos Amaris and Giorgio Lucarelli and Cl\\'ement Mommessin and Denis\n  Trystram", "title": "Generic algorithms for scheduling applications on heterogeneous\n  multi-core platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of executing an application represented by a precedence\ntask graph on a parallel machine composed of standard computing cores and\naccelerators. Contrary to most existing approaches, we distinguish the\nallocation and the scheduling phases and we mainly focus on the allocation part\nof the problem: choose the most appropriate type of computing unit for each\ntask. We address both off-line and on-line settings and design generic\nscheduling approaches. In the first case, we establish strong lower bounds on\nthe worst-case performance of a known approach based on Linear Programming for\nsolving the allocation problem. Then, we refine the scheduling phase and we\nreplace the greedy List Scheduling policy used in this approach by a better\nordering of the tasks. Although this modification leads to the same\napproximability guarantees, it performs much better in practice. We also extend\nthis algorithm to more types of computing units, achieving an approximation\nratio which depends on the number of different types. In the on-line case, we\nassume that the tasks arrive in any, not known in advance, order which respects\nthe precedence relations and the scheduler has to take irrevocable decisions\nabout their allocation and execution. In this setting, we propose the first\non-line scheduling algorithm which takes into account precedences. Our\nalgorithm is based on adequate rules for selecting the type of processor where\nto allocate the tasks and it achieves a constant factor approximation guarantee\nif the ratio of the number of CPUs over the number of GPUs is bounded. Finally,\nall the previous algorithms for hybrid architectures have been experimented on\na large number of simulations built on actual libraries. These simulations\nassess the good practical behavior of the algorithms with respect to the\nstate-of-the-art solutions, whenever these exist, or baseline algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 07:09:29 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Amaris", "Marcos", ""], ["Lucarelli", "Giorgio", ""], ["Mommessin", "Cl\u00e9ment", ""], ["Trystram", "Denis", ""]]}, {"id": "1711.06771", "submitter": "Zachary Charles", "authors": "Zachary Charles, Dimitris Papailiopoulos, Jordan Ellenberg", "title": "Approximate Gradient Coding via Sparse Random Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.IT cs.LG math.IT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed algorithms are often beset by the straggler effect, where the\nslowest compute nodes in the system dictate the overall running time.\nCoding-theoretic techniques have been recently proposed to mitigate stragglers\nvia algorithmic redundancy. Prior work in coded computation and gradient coding\nhas mainly focused on exact recovery of the desired output. However, slightly\ninexact solutions can be acceptable in applications that are robust to noise,\nsuch as model training via gradient-based algorithms. In this work, we present\ncomputationally simple gradient codes based on sparse graphs that guarantee\nfast and approximately accurate distributed computation. We demonstrate that\nsacrificing a small amount of accuracy can significantly increase algorithmic\nrobustness to stragglers.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 23:19:30 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Charles", "Zachary", ""], ["Papailiopoulos", "Dimitris", ""], ["Ellenberg", "Jordan", ""]]}, {"id": "1711.06920", "submitter": "Orr Fischer", "authors": "Orr Fischer, Tzlil Gonen, Rotem Oshman", "title": "Superlinear Lower Bounds for Distributed Subgraph Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the distributed subgraph-freeness problem, we are given a graph $H$, and\nasked to determine whether the network graph contains $H$ as a subgraph or not.\nSubgraph-freeness is an extremely local problem: if the network had no\nbandwidth constraints, we could detect any subgraph $H$ in $|H|$ rounds, by\nhaving each node of the network learn its entire $|H|$-neighborhood. However,\nwhen bandwidth is limited, the problem becomes harder.\n  Upper and lower bounds in the presence of congestion have been established\nfor several classes of subgraphs, including cycles, trees, and more complicated\nsubgraphs. All bounds shown so far have been linear or sublinear. We show that\nthe subgraph-freeness problem is not, in general, solvable in linear time: for\nany $k \\geq 2$, there exists a subgraph $H_k$ such that $H_k$-freeness requires\n$\\Omega( n^{2-1/k} / (Bk) )$ rounds to solve. Here $B$ is the bandwidth of each\ncommunication link. The lower bound holds even for diameter-3 subgraphs and\ndiameter-3 network graphs. In particular, taking $k = \\Theta(\\log n)$, we\nobtain a lower bound of $\\Omega(n^2 / (B \\log n))$.\n", "versions": [{"version": "v1", "created": "Sat, 18 Nov 2017 19:14:28 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Fischer", "Orr", ""], ["Gonen", "Tzlil", ""], ["Oshman", "Rotem", ""]]}, {"id": "1711.06964", "submitter": "Amitabha Roy", "authors": "Amitabha Roy, Subramanya R. Dulloor", "title": "Cyclone: High Availability for Persistent Key Value Stores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Persistent key value stores are an important component of many distributed\ndata serving solutions with innovations targeted at taking advantage of growing\nflash speeds. Unfortunately their performance is hampered by the need to\nmaintain and replicate a write ahead log to guarantee availability in the face\nof machine and storage failures. Cyclone is a replicated log plug-in for key\nvalue stores that systematically addresses various sources of this bottleneck.\nIt uses a small amount of non-volatile memory directly addressable by the CPU -\nsuch as in the form of NVDIMMs or Intel 3DXPoint - to remove block oriented IO\ndevices such as SSDs from the critical path for appending to the log. This\nenables it to address network overheads using an implementation of the RAFT\nconsensus protocol that is designed around a userspace network stack to relieve\nthe CPU of the burden of data copies. Finally, it provides a way to efficiently\nmap the commutativity in key-value store APIs to the parallelism available in\ncommodity NICs. Cyclone is able to replicate millions of small updates per\nsecond using only commodity 10 gigabit ethernet adapters. As a practical\napplication, we use it to improve the performance (and availability) of\nRocksDB, a popular persistent key value store by an order of magnitude when\ncompared to its own write ahead log without replication.\n", "versions": [{"version": "v1", "created": "Sun, 19 Nov 2017 04:07:34 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Roy", "Amitabha", ""], ["Dulloor", "Subramanya R.", ""]]}, {"id": "1711.07033", "submitter": "Kian Hsiang Low", "authors": "Trong Nghia Hoang, Quang Minh Hoang, Ruofei Ouyang, Kian Hsiang Low", "title": "Decentralized High-Dimensional Bayesian Optimization with Factor Graphs", "comments": "32nd AAAI Conference on Artificial Intelligence (AAAI 2018), Extended\n  version with proofs, 13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel decentralized high-dimensional Bayesian\noptimization (DEC-HBO) algorithm that, in contrast to existing HBO algorithms,\ncan exploit the interdependent effects of various input components on the\noutput of the unknown objective function f for boosting the BO performance and\nstill preserve scalability in the number of input dimensions without requiring\nprior knowledge or the existence of a low (effective) dimension of the input\nspace. To realize this, we propose a sparse yet rich factor graph\nrepresentation of f to be exploited for designing an acquisition function that\ncan be similarly represented by a sparse factor graph and hence be efficiently\noptimized in a decentralized manner using distributed message passing. Despite\nrichly characterizing the interdependent effects of the input components on the\noutput of f with a factor graph, DEC-HBO can still guarantee no-regret\nperformance asymptotically. Empirical evaluation on synthetic and real-world\nexperiments (e.g., sparse Gaussian process model with 1811 hyperparameters)\nshows that DEC-HBO outperforms the state-of-the-art HBO algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 19 Nov 2017 15:45:53 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 10:08:08 GMT"}, {"version": "v3", "created": "Wed, 24 Jan 2018 18:56:10 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Hoang", "Trong Nghia", ""], ["Hoang", "Quang Minh", ""], ["Ouyang", "Ruofei", ""], ["Low", "Kian Hsiang", ""]]}, {"id": "1711.07135", "submitter": "Susumu Nishimura", "authors": "Susumu Nishimura", "title": "Schlegel Diagram and Optimizable Immediate Snapshot Protocol", "comments": "To appear in OPODIS 2017 - The 21st International Conference on\n  Principles of Distributed Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the topological study of distributed systems, the immediate snapshot is\nthe fundamental computation block for the topological characterization of\nwait-free solvable tasks. However, in reality, the immediate snapshot is not\navailable as a native built-in operation on shared memory distributed systems.\nBorowsky and Gafni have proposed a wait-free multi-round protocol that\nimplements the immediate snapshot using more primitive operations, namely the\natomic reads and writes.\n  In this paper, up to an appropriate reformulation on the original protocol by\nBorowsky and Gafni, we establish a tight link between each round of the\nprotocol and a topological operation of subdivision using Schlegel diagram. Due\nto the fact shown by Kozlov that the standard chromatic subdivision is obtained\nby iterated subdivision using Schlegel diagram, the reformulated version is\nproven to compute the immediate snapshot in a topologically smoother way. We\nalso show that the reformulated protocol is amenable to optimization: Since\neach round restricts the possible candidates of output to an iteratively\nsmaller region of finer subdivision, each process executing the protocol can\ndecide at an earlier round, beyond which the same final output is reached no\nmatter how the remaining rounds are executed. This reduces the number of read\nand write operations involved in the overall execution of the protocol,\nrelieving the bottleneck of access to shared memory.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 04:16:20 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Nishimura", "Susumu", ""]]}, {"id": "1711.07221", "submitter": "Manish Kesarwani", "authors": "Manish Kesarwani, Bhaskar Mukhoty, Vijay Arya, Sameep Mehta", "title": "Model Extraction Warning in MLaaS Paradigm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud vendors are increasingly offering machine learning services as part of\ntheir platform and services portfolios. These services enable the deployment of\nmachine learning models on the cloud that are offered on a pay-per-query basis\nto application developers and end users. However recent work has shown that the\nhosted models are susceptible to extraction attacks. Adversaries may launch\nqueries to steal the model and compromise future query payments or privacy of\nthe training data. In this work, we present a cloud-based extraction monitor\nthat can quantify the extraction status of models by observing the query and\nresponse streams of both individual and colluding adversarial users. We present\na novel technique that uses information gain to measure the model learning rate\nby users with increasing number of queries. Additionally, we present an\nalternate technique that maintains intelligent query summaries to measure the\nlearning rate relative to the coverage of the input feature space in the\npresence of collusion. Both these approaches have low computational overhead\nand can easily be offered as services to model owners to warn them of possible\nextraction attacks from adversaries. We present performance results for these\napproaches for decision tree models deployed on BigML MLaaS platform, using\nopen source datasets and different adversarial attack strategies.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 09:33:45 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Kesarwani", "Manish", ""], ["Mukhoty", "Bhaskar", ""], ["Arya", "Vijay", ""], ["Mehta", "Sameep", ""]]}, {"id": "1711.07227", "submitter": "Kubilay Atasu", "authors": "Kubilay Atasu, Thomas Parnell, Celestine D\\\"unner, Manolis Sifalakis,\n  Haralampos Pozidis, Vasileios Vasileiadis, Michail Vlachos, Cesar Berrospi,\n  Abdel Labbi", "title": "Linear-Complexity Relaxed Word Mover's Distance with GPU Acceleration", "comments": "To appear in the 2017 IEEE International Conference on Big Data (Big\n  Data 2017) http://cci.drexel.edu/bigdata/bigdata2017/ December 11-14, 2017,\n  Boston, MA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The amount of unstructured text-based data is growing every day. Querying,\nclustering, and classifying this big data requires similarity computations\nacross large sets of documents. Whereas low-complexity similarity metrics are\navailable, attention has been shifting towards more complex methods that\nachieve a higher accuracy. In particular, the Word Mover's Distance (WMD)\nmethod proposed by Kusner et al. is a promising new approach, but its time\ncomplexity grows cubically with the number of unique words in the documents.\nThe Relaxed Word Mover's Distance (RWMD) method, again proposed by Kusner et\nal., reduces the time complexity from qubic to quadratic and results in a\nlimited loss in accuracy compared with WMD. Our work contributes a\nlow-complexity implementation of the RWMD that reduces the average time\ncomplexity to linear when operating on large sets of documents. Our\nlinear-complexity RWMD implementation, henceforth referred to as LC-RWMD, maps\nwell onto GPUs and can be efficiently distributed across a cluster of GPUs. Our\nexperiments on real-life datasets demonstrate 1) a performance improvement of\ntwo orders of magnitude with respect to our GPU-based distributed\nimplementation of the quadratic RWMD, and 2) a performance improvement of three\nto four orders of magnitude with respect to our distributed WMD implementation\nthat uses GPU-based RWMD for pruning.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 09:45:02 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Atasu", "Kubilay", ""], ["Parnell", "Thomas", ""], ["D\u00fcnner", "Celestine", ""], ["Sifalakis", "Manolis", ""], ["Pozidis", "Haralampos", ""], ["Vasileiadis", "Vasileios", ""], ["Vlachos", "Michail", ""], ["Berrospi", "Cesar", ""], ["Labbi", "Abdel", ""]]}, {"id": "1711.07295", "submitter": "Edans Flavius De Oliveira Sandes", "authors": "Edans F. O. Sandes, George Teodoro and Alba C. M. A. Melo", "title": "Bitmap Filter: Speeding up Exact Set Similarity Joins with Bitwise\n  Operations", "comments": "13 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Exact Set Similarity Join problem aims to find all similar sets between\ntwo collections of sets, with respect to a threshold and a similarity function\nsuch as overlap, Jaccard, dice or cosine. The naive approach verifies all pairs\nof sets and it is often considered impractical due the high number of\ncombinations. So, Exact Set Similarity Join algorithms are usually based on the\nFilter-Verification Framework, that applies a series of filters to reduce the\nnumber of verified pairs. This paper presents a new filtering technique called\nBitmap Filter, which is able to accelerate state-of-the-art algorithms for the\nexact Set Similarity Join problem. The Bitmap Filter uses hash functions to\ncreate bitmaps of fixed b bits, representing characteristics of the sets. Then,\nit applies bitwise operations (such as xor and population count) on the bitmaps\nin order to infer a similarity upper bound for each pair of sets. If the upper\nbound is below a given similarity threshold, the pair of sets is pruned. The\nBitmap Filter benefits from the fact that bitwise operations are efficiently\nimplemented by many modern general-purpose processors and it was easily applied\nto four state-of-the-art algorithms implemented in CPU: AllPairs, PPJoin,\nAdaptJoin and GroupJoin. Furthermore, we propose a Graphic Processor Unit (GPU)\nalgorithm based on the naive approach but using the Bitmap Filter to speedup\nthe computation. The experiments considered 9 collections containing from 100\nthousands up to 10 million sets and the joins were made using Jaccard\nthresholds from 0.50 to 0.95. The Bitmap Filter was able to improve 90% of the\nexperiments in CPU, with speedups of up to 4.50x and 1.43x on average. Using\nthe GPU algorithm, the experiments were able to speedup the original CPU\nalgorithms by up to 577x using an Nvidia Geforce GTX 980 Ti.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 13:06:10 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Sandes", "Edans F. O.", ""], ["Teodoro", "George", ""], ["Melo", "Alba C. M. A.", ""]]}, {"id": "1711.07325", "submitter": "Wiktor Daszczuk", "authors": "W{\\l}odzimierz Choroma\\'nski, Wiktor Daszczuk, Jaros{\\l}aw Dyduch,\n  Mariusz Maciejewski, Pawe{\\l} Brach, Waldemar Grabski", "title": "PRT (Personal Rapid Transit) network simulation", "comments": "17 pages, 6 figures", "journal-ref": "Proceedings of the 13th World Conference on Transportation\n  Research, Rio de Janeiro, Brasil, 7-10 July 2013, Joao Victor (ed.), 2014,\n  Federal University of Rio de Janeiro, ISBN 978-85-285-0232-9", "doi": null, "report-no": null, "categories": "cs.DC cs.CE cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transportation problems of large urban conurbations inspire search for new\ntransportation systems, that meet high environmental standards, are relatively\ncheap and user friendly. The latter element also includes the needs of disabled\nand elderly people. This article concerns a new transportation system PRT -\nPersonal Rapid Transit. In this article the attention is focused on the\nanalysis of the efficiency of the PRT transport network. The simulator of\nvehicle movement in PRT network as well as algorithms for traffic management\nand control will be presented. The proposal of its physical implementation will\nbe also included.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 19:09:48 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Choroma\u0144ski", "W\u0142odzimierz", ""], ["Daszczuk", "Wiktor", ""], ["Dyduch", "Jaros\u0142aw", ""], ["Maciejewski", "Mariusz", ""], ["Brach", "Pawe\u0142", ""], ["Grabski", "Waldemar", ""]]}, {"id": "1711.07370", "submitter": "Jaroslaw Zola", "authors": "Steven Y. Ko, Lauren Sassoubre, Jaroslaw Zola", "title": "Applications and Challenges of Real-time Mobile DNA Analysis", "comments": null, "journal-ref": null, "doi": "10.1145/3177102.3177114", "report-no": null, "categories": "cs.CE cs.DC q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The DNA sequencing is the process of identifying the exact order of\nnucleotides within a given DNA molecule. The new portable and relatively\ninexpensive DNA sequencers, such as Oxford Nanopore MinION, have the potential\nto move DNA sequencing outside of laboratory, leading to faster and more\naccessible DNA-based diagnostics. However, portable DNA sequencing and analysis\nare challenging for mobile systems, owing to high data throughputs and\ncomputationally intensive processing performed in environments with unreliable\nconnectivity and power.\n  In this paper, we provide an analysis of the challenges that mobile systems\nand mobile computing must address to maximize the potential of portable DNA\nsequencing, and in situ DNA analysis. We explain the DNA sequencing process and\nhighlight the main differences between traditional and portable DNA sequencing\nin the context of the actual and envisioned applications. We look at the\nidentified challenges from the perspective of both algorithms and systems\ndesign, showing the need for careful co-design.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 15:17:31 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Ko", "Steven Y.", ""], ["Sassoubre", "Lauren", ""], ["Zola", "Jaroslaw", ""]]}, {"id": "1711.07423", "submitter": "Ahad N. Zehmakan", "authors": "Bernd G\\\"artner and Ahad N. Zehmakan", "title": "Majority Model on Random Regular Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a graph $G=(V,E)$ and an initial random coloring where each vertex\n$v \\in V$ is blue with probability $P_b$ and red otherwise, independently from\nall other vertices. In each round, all vertices simultaneously switch their\ncolor to the most frequent color in their neighborhood and in case of a tie, a\nvertex keeps its current color. The main goal of the present paper is to\nanalyze the behavior of this basic and natural process on the random\n$d$-regular graph $\\mathbb{G}_{n,d}$. It is shown that for all $\\epsilon>0$,\n$P_b \\le 1/2-\\epsilon$ results in final complete occupancy by red in\n$\\mathcal{O}(\\log_d\\log n)$ rounds with high probability, provided that $d\\geq\nc/\\epsilon^2$ for a suitable constant $c$. Furthermore, we show that with high\nprobability, $\\mathbb{G}_{n,d}$ is immune; i.e., the smallest dynamic monopoly\nis of linear size. A dynamic monopoly is a subset of vertices that can take\nover in the sense that a commonly chosen initial color eventually spreads\nthroughout the whole graph, irrespective of the colors of other vertices. This\nanswers an open question of Peleg.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 20:50:35 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["G\u00e4rtner", "Bernd", ""], ["Zehmakan", "Ahad N.", ""]]}, {"id": "1711.07440", "submitter": "Weijia Chen", "authors": "Weijia Chen, Yuedong Xu, Xiaofeng Wu", "title": "Deep Reinforcement Learning for Multi-Resource Multi-Machine Job\n  Scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimizing job scheduling time is a fundamental issue in data center networks\nthat has been extensively studied in recent years. The incoming jobs require\ndifferent CPU and memory units, and span different number of time slots. The\ntraditional solution is to design efficient heuristic algorithms with\nperformance guarantee under certain assumptions. In this paper, we improve a\nrecently proposed job scheduling algorithm using deep reinforcement learning\nand extend it to multiple server clusters. Our study reveals that deep\nreinforcement learning method has the potential to outperform traditional\nresource allocation algorithms in a variety of complicated environments.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 17:50:54 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Chen", "Weijia", ""], ["Xu", "Yuedong", ""], ["Wu", "Xiaofeng", ""]]}, {"id": "1711.07639", "submitter": "Rui Ren", "authors": "Rui Ren, Jiechao Cheng, Xiwen He, Lei Wang, Chunjie Luo, Jianfeng Zhan", "title": "HybridTune: Spatio-temporal Data and Model Driven Performance Diagnosis\n  for Big Data Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With tremendous growing interests in Big Data systems, analyzing and\nfacilitating their performance improvement become increasingly important.\nAlthough there have much research efforts for improving Big Data systems\nperformance, efficiently analysing and diagnosing performance bottlenecks over\nthese massively distributed systems remain a major challenge. In this paper, we\npropose a spatio-temporal correlation analysis approach based on stage\ncharacteristic and distribution characteristic of Big Data applications, which\ncan associate the multi-level performance data fine-grained. On the basis of\ncorrelation data, we define some priori rules, select features and vectorize\nthe corresponding datasets for different performance bottlenecks, such as,\nworkload imbalance, data skew, abnormal node and outlier metrics. And then, we\nutilize the data and model driven algorithms for bottlenecks detection and\ndiagnosis. In addition, we design and develop a lightweight, extensible tool\nHybridTune, and validate the diagnosis effectiveness of our tool with\nBigDataBench on several benchmark experiments in which the outperform\nstate-of-the-art methods. Our experiments show that the accuracy of\nabnormal/outlier detection we obtained reaches about 80%. At last, we report\nseveral Spark and Hadoop use cases, which are demonstrated how HybridTune\nsupports users to carry out the performance analysis and diagnosis efficiently\non the Spark and Hadoop applications, and our experiences demonstrate\nHybridTune can help users find the performance bottlenecks and provide\noptimization recommendations.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 06:10:09 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Ren", "Rui", ""], ["Cheng", "Jiechao", ""], ["He", "Xiwen", ""], ["Wang", "Lei", ""], ["Luo", "Chunjie", ""], ["Zhan", "Jianfeng", ""]]}, {"id": "1711.07733", "submitter": "Gon\\c{c}alo Cabrita", "authors": "Gon\\c{c}alo Cabrita, Nuno Pregui\\c{c}a", "title": "Non-uniform Replication", "comments": "Preprint, accepted for publication", "journal-ref": null, "doi": "10.4230/LIPIcs.OPODIS.2017.24", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Replication is a key technique in the design of efficient and reliable\ndistributed systems. As information grows, it becomes difficult or even\nimpossible to store all information at every replica. A common approach to deal\nwith this problem is to rely on partial replication, where each replica\nmaintains only a part of the total system information. As a consequence, a\nremote replica might need to be contacted for computing the reply to some given\nquery, which leads to high latency costs particularly in geo-replicated\nsettings. In this work, we introduce the concept of non-uniform replication,\nwhere each replica stores only part of the information, but where all replicas\nstore enough information to answer every query. We apply this concept to\neventual consistency and conflict-free replicated data types. We show that this\nmodel can address useful problems and present two data types that solve such\nproblems. Our evaluation shows that non-uniform replication is more efficient\nthan traditional replication, using less storage space and network bandwidth.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 11:49:17 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Cabrita", "Gon\u00e7alo", ""], ["Pregui\u00e7a", "Nuno", ""]]}, {"id": "1711.08060", "submitter": "\\'Alvaro L\\'opez Garc\\'ia", "authors": "\\'Alvaro L\\'opez Garc\\'ia, Enol Fern\\'andez del Castillo", "title": "Efficient image deployment in cloud environments", "comments": null, "journal-ref": "Journal of Network and Computer Applications, Volume 63, 2016,\n  Pages 140-149", "doi": "10.1016/j.jnca.2015.10.015", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The biggest overhead for the instantiation of a virtual machine in a cloud\ninfrastructure is the time spent in transferring the image of the virtual\nmachine into the physical node that executes it. This overhead becomes larger\nfor requests composed of several virtual machines to be started concurrently,\nand the illusion of flexibility and elasticity usually associated with the\ncloud computing model may vanish. This poses a problem for both the resource\nproviders and the software developers, since tackling those overheads is not a\ntrivial issue.\n  In this work we implement and evaluate several improvements for virtual\nmachine image distribution problem in a cloud infrastructure and propose a\nmethod based on BitTorrent and local caching of the virtual machine images that\nreduces the transfer time when large requests are made\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 21:51:40 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Garc\u00eda", "\u00c1lvaro L\u00f3pez", ""], ["del Castillo", "Enol Fern\u00e1ndez", ""]]}, {"id": "1711.08076", "submitter": "Marijn Heule", "authors": "Marijn J.H. Heule", "title": "Schur Number Five", "comments": "accepted by AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the solution of a century-old problem known as Schur Number Five:\nWhat is the largest (natural) number $n$ such that there exists a five-coloring\nof the positive numbers up to $n$ without a monochromatic solution of the\nequation $a + b = c$? We obtained the solution, $n = 160$, by encoding the\nproblem into propositional logic and applying massively parallel satisfiability\nsolving techniques on the resulting formula. We constructed and validated a\nproof of the solution to increase trust in the correctness of the\nmulti-CPU-year computations. The proof is two petabytes in size and was\ncertified using a formally verified proof checker, demonstrating that any\nresult by satisfiability solvers---no matter how large---can now be validated\nusing highly trustworthy systems.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 22:54:59 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Heule", "Marijn J. H.", ""]]}, {"id": "1711.08297", "submitter": "Giorgio Audrito", "authors": "Mirko Viroli, Giorgio Audrito, Jacob Beal, Ferruccio Damiani, Danilo\n  Pianini", "title": "Engineering Resilient Collective Adaptive Systems by Self-Stabilisation", "comments": "To appear on ACM Transactions on Modeling and Computer Simulation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collective adaptive systems are an emerging class of networked computational\nsystems, particularly suited in application domains such as smart cities,\ncomplex sensor networks, and the Internet of Things. These systems tend to\nfeature large scale, heterogeneity of communication model (including\nopportunistic peer-to-peer wireless interaction), and require inherent\nself-adaptiveness properties to address unforeseen changes in operating\nconditions. In this context, it is extremely difficult (if not seemingly\nintractable) to engineer reusable pieces of distributed behaviour so as to make\nthem provably correct and smoothly composable.\n  Building on the field calculus, a computational model (and associated\ntoolchain) capturing the notion of aggregate network-level computation, we\naddress this problem with an engineering methodology coupling formal theory and\ncomputer simulation. On the one hand, functional properties are addressed by\nidentifying the largest-to-date field calculus fragment generating\nself-stabilising behaviour, guaranteed to eventually attain a correct and\nstable final state despite any transient perturbation in state or topology, and\nincluding highly reusable building blocks for information spreading,\naggregation, and time evolution. On the other hand, dynamical properties are\naddressed by simulation, empirically evaluating the different performances that\ncan be obtained by switching between implementations of building blocks with\nprovably equivalent functional properties. Overall, our methodology sheds light\non how to identify core building blocks of collective behaviour, and how to\nselect implementations that improve system performance while leaving overall\nsystem function and resiliency properties unchanged.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 14:38:50 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Viroli", "Mirko", ""], ["Audrito", "Giorgio", ""], ["Beal", "Jacob", ""], ["Damiani", "Ferruccio", ""], ["Pianini", "Danilo", ""]]}, {"id": "1711.08452", "submitter": "Mohamed Attia", "authors": "Mohamed A. Attia, Ravi Tandon", "title": "Combating Computational Heterogeneity in Large-Scale Distributed\n  Computing via Work Exchange", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Owing to data-intensive large-scale applications, distributed computation\nsystems have gained significant recent interest, due to their ability of\nrunning such tasks over a large number of commodity nodes in a time efficient\nmanner. One of the major bottlenecks that adversely impacts the time efficiency\nis the computational heterogeneity of distributed nodes, often limiting the\ntask completion time due to the slowest worker.\n  In this paper, we first present a lower bound on the expected computation\ntime based on the work-conservation principle. We then present our approach of\nwork exchange to combat the latency problem, in which faster workers can be\nreassigned additional leftover computations that were originally assigned to\nslower workers. We present two variations of the work exchange approach: a)\nwhen the computational heterogeneity knowledge is known a priori; and b) when\nheterogeneity is unknown and is estimated in an online manner to assign tasks\nto distributed workers. As a baseline, we also present and analyze the use of\nan optimized Maximum Distance Separable (MDS) coded distributed computation\nscheme over heterogeneous nodes. Simulation results also compare the proposed\napproach of work exchange, the baseline MDS coded scheme and the lower bound\nobtained via work-conservation principle. We show that the work exchange scheme\nachieves time for computation which is very close to the lower bound with\nlimited coordination and communication overhead even when the knowledge about\nheterogeneity levels is not available.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 18:55:01 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Attia", "Mohamed A.", ""], ["Tandon", "Ravi", ""]]}, {"id": "1711.08731", "submitter": "Jesper Larsson Tr\\\"aff", "authors": "Jesper Larsson Tr\\\"aff", "title": "On Optimal Trees for Irregular Gather and Scatter Collectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of finding communication trees with the lowest\npossible completion time for rooted, irregular gather and scatter collective\ncommunication operations in fully connected, $k$-ported communication networks\nunder a linear-time transmission cost model. Consecutively numbered processors\nspecify data blocks of possibly different sizes to be collected at or\ndistributed from some (given) root processor where they are stored in processor\norder. Data blocks can be combined into larger segments consisting of blocks\nfrom or to different processors, but individual blocks cannot be split. We\ndistinguish between ordered and non-ordered communication trees depending on\nwhether segments of blocks are maintained in processor order. We show that\nlowest completion time, ordered communication trees under one-ported\ncommunication can be found in polynomial time by giving simple, but costly\ndynamic programming algorithms. In contrast, we show that it is an NP-complete\nproblem to construct cost-optimal, non-ordered communication trees. We have\nimplemented the dynamic programming algorithms for homogeneous networks to\nevaluate the quality of different types of communication trees, in particular\nto analyze a recent, distributed, problem-adaptive tree construction algorithm.\nModel experiments show that this algorithm is close to the optimum for a\nselection of block size distributions. A concrete implementation for specially\nstructured problems shows that optimal, non-binomial trees can possibly have\neven further practical advantage.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 15:07:08 GMT"}, {"version": "v2", "created": "Tue, 12 Dec 2017 13:45:56 GMT"}, {"version": "v3", "created": "Wed, 13 Dec 2017 07:36:19 GMT"}, {"version": "v4", "created": "Tue, 27 Nov 2018 14:27:58 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Tr\u00e4ff", "Jesper Larsson", ""]]}, {"id": "1711.08973", "submitter": "Adam Barker", "authors": "Long Thai, Blesson Varghese and Adam Barker", "title": "A Survey and Taxonomy of Resource Optimisation for Executing Bag-of-Task\n  Applications on Public Clouds", "comments": "Accepted to Future Generation Computer Systems, 23 November 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing has been widely adopted due to the flexibility in resource\nprovisioning and on-demand pricing models. Entire clusters of Virtual Machines\n(VMs) can be dynamically provisioned to meet the computational demands of\nusers. However, from a user's perspective, it is still challenging to utilise\ncloud resources efficiently. This is because an overwhelmingly wide variety of\nresource types with different prices and significant performance variations are\navailable.\n  This paper presents a survey and taxonomy of existing research in optimising\nthe execution of Bag-of-Task applications on cloud resources. A BoT application\nconsists of multiple independent tasks, each of which can be executed by a VM\nin any order; these applications are widely used by both the scientific\ncommunities and commercial organisations. The objectives of this survey are as\nfollows: (i) to provide the reader with a concise understanding of existing\nresearch on optimising the execution of BoT applications on the cloud, (ii) to\ndefine a taxonomy that categorises current frameworks to compare and contrast\nthem, and (iii) to present current trends and future research directions in the\narea.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 14:20:05 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Thai", "Long", ""], ["Varghese", "Blesson", ""], ["Barker", "Adam", ""]]}, {"id": "1711.08974", "submitter": "Elaheh Sadredini", "authors": "Elaheh Sadredini, Mohammad Hashem Haghbayan, Mahmood Fathy,\n  Zainalabedin Navabi", "title": "Test Generation and Scheduling for a Hybrid BIST Considering Test Time\n  and Power Constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach for test generation and test scheduling\nfor multi-clock domain SoCs. A concurrent hybrid BIST architecture is proposed\nfor testing cores. Furthermore, a heuristic for selecting cores to be tested\nconcurrently and order of applying test patterns is proposed. Experimental\nresults show that the proposed heuristics give us an optimized method for multi\nclock domain SoC testing in comparison with the previous works.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 04:24:23 GMT"}, {"version": "v2", "created": "Tue, 16 Jan 2018 14:31:02 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Sadredini", "Elaheh", ""], ["Haghbayan", "Mohammad Hashem", ""], ["Fathy", "Mahmood", ""], ["Navabi", "Zainalabedin", ""]]}, {"id": "1711.08975", "submitter": "Elaheh Sadredini", "authors": "Elaheh Sadredini, Reza Rahimi, Paniz Foroutan, Mahmood Fathy,\n  Zainalabedin Navabi", "title": "An Improved Scheme for Pre-computed Patterns in Core-based SoC\n  Architecture", "comments": null, "journal-ref": "Design & Test Symposium (EWDTS), IEEE, Armenia, 2016", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By advances in technology, integrated circuits have come to include more\nfunctionality and more complexity in a single chip. Although methods of testing\nhave improved, but the increase in complexity of circuits, keeps testing a\nchallenging problem. Two important challenges in testing of digital circuits\nare test time and accessing the circuit under test (CUT) for testing. These\nchallenges become even more important in complex system on chip (SoC) zone.\nThis paper presents an improved scheme for generating precomputed test patterns\nin core based systems on chip. This approach reduces the number of pre computed\ntest patterns and as the result, test application time (TAT) will be decreased.\nExperimental results on ISCAS89 benchmark circuits show improvement in the\nnumber of test clock cycles.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 04:10:46 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Sadredini", "Elaheh", ""], ["Rahimi", "Reza", ""], ["Foroutan", "Paniz", ""], ["Fathy", "Mahmood", ""], ["Navabi", "Zainalabedin", ""]]}, {"id": "1711.08993", "submitter": "Laurens Versluis", "authors": "Laurens Versluis, Mihai Neac\\c{s}u, Alexandru Iosup", "title": "Technical Report: A Trace-Based Performance Study of Autoscaling\n  Workloads of Workflows in Datacenters", "comments": "Technical Report for the CCGrid 2018 submission \"A Trace-Based\n  Performance Study of Autoscaling Workloads of Workflows in Datacenters\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To improve customer experience, datacenter operators offer support for\nsimplifying application and resource management. For example, running workloads\nof workflows on behalf of customers is desirable, but requires increasingly\nmore sophisticated autoscaling policies, that is, policies that dynamically\nprovision resources for the customer. Although selecting and tuning autoscaling\npolicies is a challenging task for datacenter operators, so far relatively few\nstudies investigate the performance of autoscaling for workloads of workflows.\nComplementing previous knowledge, in this work we propose the first\ncomprehensive performance study in the field. Using trace-based simulation, we\ncompare state-of-the-art autoscaling policies across multiple application\ndomains, workload arrival patterns (e.g., burstiness), and system utilization\nlevels. We further investigate the interplay between autoscaling and regular\nallocation policies, and the complexity cost of autoscaling. Our quantitative\nstudy focuses not only on traditional performance metrics and on\nstate-of-the-art elasticity metrics, but also on time- and memory-related\nautoscaling-complexity metrics. Our main results give strong and quantitative\nevidence about previously unreported operational behavior, for example, that\nautoscaling policies perform differently across application domains and by how\nmuch they differ.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 14:46:08 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Versluis", "Laurens", ""], ["Neac\u015fu", "Mihai", ""], ["Iosup", "Alexandru", ""]]}, {"id": "1711.09123", "submitter": "Satish N. Srirama", "authors": "Rajkumar Buyya, Satish Narayana Srirama, Giuliano Casale, Rodrigo\n  Calheiros, Yogesh Simmhan, Blesson Varghese, Erol Gelenbe, Bahman Javadi,\n  Luis Miguel Vaquero, Marco A. S. Netto, Adel Nadjaran Toosi, Maria Alejandra\n  Rodriguez, Ignacio M. Llorente, Sabrina De Capitani di Vimercati, Pierangela\n  Samarati, Dejan Milojicic, Carlos Varela, Rami Bahsoon, Marcos Dias de\n  Assuncao, Omer Rana, Wanlei Zhou, Hai Jin, Wolfgang Gentzsch, Albert Y.\n  Zomaya, Haiying Shen", "title": "A Manifesto for Future Generation Cloud Computing: Research Directions\n  for the Next Decade", "comments": "51 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Cloud computing paradigm has revolutionised the computer science horizon\nduring the past decade and has enabled the emergence of computing as the fifth\nutility. It has captured significant attention of academia, industries, and\ngovernment bodies. Now, it has emerged as the backbone of modern economy by\noffering subscription-based services anytime, anywhere following a\npay-as-you-go model. This has instigated (1) shorter establishment times for\nstart-ups, (2) creation of scalable global enterprise applications, (3) better\ncost-to-value associativity for scientific and high performance computing\napplications, and (4) different invocation/execution models for pervasive and\nubiquitous applications. The recent technological developments and paradigms\nsuch as serverless computing, software-defined networking, Internet of Things,\nand processing at network edge are creating new opportunities for Cloud\ncomputing. However, they are also posing several new challenges and creating\nthe need for new approaches and research strategies, as well as the\nre-evaluation of the models that were developed to address issues such as\nscalability, elasticity, reliability, security, sustainability, and application\nmodels. The proposed manifesto addresses them by identifying the major open\nchallenges in Cloud computing, emerging trends, and impact areas. It then\noffers research directions for the next decade, thus helping in the realisation\nof Future Generation Cloud Computing.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 20:00:30 GMT"}, {"version": "v2", "created": "Fri, 24 Aug 2018 09:17:17 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Buyya", "Rajkumar", ""], ["Srirama", "Satish Narayana", ""], ["Casale", "Giuliano", ""], ["Calheiros", "Rodrigo", ""], ["Simmhan", "Yogesh", ""], ["Varghese", "Blesson", ""], ["Gelenbe", "Erol", ""], ["Javadi", "Bahman", ""], ["Vaquero", "Luis Miguel", ""], ["Netto", "Marco A. S.", ""], ["Toosi", "Adel Nadjaran", ""], ["Rodriguez", "Maria Alejandra", ""], ["Llorente", "Ignacio M.", ""], ["di Vimercati", "Sabrina De Capitani", ""], ["Samarati", "Pierangela", ""], ["Milojicic", "Dejan", ""], ["Varela", "Carlos", ""], ["Bahsoon", "Rami", ""], ["de Assuncao", "Marcos Dias", ""], ["Rana", "Omer", ""], ["Zhou", "Wanlei", ""], ["Jin", "Hai", ""], ["Gentzsch", "Wolfgang", ""], ["Zomaya", "Albert Y.", ""], ["Shen", "Haiying", ""]]}, {"id": "1711.09138", "submitter": "Sheriffo Ceesay", "authors": "Sheriffo Ceesay, Adam Barker, Blesson Varghese", "title": "Plug and Play Bench: Simplifying Big Data Benchmarking Using Containers", "comments": "8 pages, Published as a workshop paper in 2017 IEEE International\n  Conference on Big Data in Boston Dec 11 - 14", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent boom of big data, coupled with the challenges of its processing\nand storage gave rise to the development of distributed data processing and\nstorage paradigms like MapReduce, Spark, and NoSQL databases. With the advent\nof cloud computing, processing and storing such massive datasets on clusters of\nmachines is now feasible with ease. However, there are limited tools and\napproaches, which users can rely on to gauge and comprehend the performance of\ntheir big data applications deployed locally on clusters, or in the cloud.\nResearchers have started exploring this area by providing benchmarking suites\nsuitable for big data applications. However, many of these tools are\nfragmented, complex to deploy and manage, and do not provide transparency with\nrespect to the monetary cost of benchmarking an application.\n  In this paper, we present Plug And Play Bench, an infrastructure aware\nabstraction built to integrate and simplify the deployment of big data\nbenchmarking tools on clusters of machines. PAPB automates the tedious process\nof installing, configuring and executing common big data benchmark workloads by\ncontainerising the tools and settings based on the underlying cluster\ndeployment framework. Our proof of concept implementation utilises HiBench as\nthe benchmark suite, HDP as the cluster deployment framework and Azure as the\ncloud platform. The paper further illustrates the inclusion of cost metrics\nbased on the underlying Microsoft Azure cloud platform.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 21:02:42 GMT"}, {"version": "v2", "created": "Wed, 29 Nov 2017 12:04:28 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Ceesay", "Sheriffo", ""], ["Barker", "Adam", ""], ["Varghese", "Blesson", ""]]}, {"id": "1711.09157", "submitter": "Yoji Yamato", "authors": "Yoji Yamato, Naoto Hoshikawa, Hirofumi Noguchi, Tatsuya Demizu and\n  Misao Kataoka", "title": "A Study to Optimize Heterogeneous Resources for Open IoT", "comments": "3 pages, 1 figure, 2017 Fifth International Symposium on Computing\n  and Networking (CANDAR2017), Nov. 2017", "journal-ref": "2017 Fifth International Symposium on Computing and Networking\n  (CANDAR2017), pp.609-611, Nov. 2017. (c) 2017 CANDAR2017", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, IoT technologies have been progressed, and many sensors and\nactuators are connected to networks. Previously, IoT services were developed by\nvertical integration style. But now Open IoT concept has attracted attentions\nwhich achieves various IoT services by integrating horizontal separated devices\nand services. For Open IoT era, we have proposed the Tacit Computing technology\nto discover the devices with necessary data for users on demand and use them\ndynamically. We also implemented elemental technologies of Tacit Computing. In\nthis paper, we propose three layers optimizations to reduce operation cost and\nimprove performance of Tacit computing service, in order to make as a\ncontinuous service of discovered devices by Tacit Computing. In optimization\nprocess, appropriate function allocation or offloading specific functions are\ncalculated on device, network and cloud layer before full-scale operation.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 22:26:01 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Yamato", "Yoji", ""], ["Hoshikawa", "Naoto", ""], ["Noguchi", "Hirofumi", ""], ["Demizu", "Tatsuya", ""], ["Kataoka", "Misao", ""]]}, {"id": "1711.09192", "submitter": "Mohammad Hosseini", "authors": "Mohammad Hosseini, Richard Berlin, Lui Sha, Axel Terfloth, Houbing\n  Song", "title": "Communication and Synchronization of Distributed Medical Models: Design,\n  Development, and Performance Analysis", "comments": "12 pages, IEEE Journal of Translational Engineering in Health and\n  Medicine, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based development is a widely-used method to describe complex systems\nthat enables the rapid prototyping. Advances in the science of distributed\nsystems has led to the development of large scale statechart models which are\ndistributed among multiple locations. Taking medicine for example, models of\nbest-practice guidelines during rural ambulance transport are distributed\nacross hospital settings from a rural hospital, to an ambulance, to a central\ntertiary hospital. Unfortunately, these medical models require continuous and\nreal-time communication across individual medical models in physically\ndistributed treatment locations which provides vital assistance to the\nclinicians and physicians. This makes it necessary to offer methods for\nmodel-driven communication and synchronization in a distributed environment. In\nthis paper, we describe ModelSink, a middleware to address the problem of\ncommunication and synchronization of heterogeneous distributed models. Being\nmotivated by the synchronization requirements during emergency ambulance\ntransport, we use medical best-practice models as a case study to illustrate\nthe notion of distributed models. Through ModelSink, we achieve an efficient\ncommunication architecture, open-loop-safe protocol, and queuing and mapping\nmechanisms compliant with the semantics of statechart-based model-driven\ndevelopment. We evaluated the performance of ModelSink on distributed sets of\nmedical models that we have developed to assess how ModelSink performs in\nvarious loads. Our work is intended to assist clinicians, EMT, and medical\nstaff to prevent unintended deviations from medical best practices, and\novercome connectivity and coordination challenges that exist in a distributed\nhospital network. Our practice suggests that there are in fact additional\npotential domains beyond medicine where our middleware can provide needed\nutility.\n", "versions": [{"version": "v1", "created": "Sat, 25 Nov 2017 05:13:27 GMT"}, {"version": "v2", "created": "Mon, 22 Jan 2018 06:33:54 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Hosseini", "Mohammad", ""], ["Berlin", "Richard", ""], ["Sha", "Lui", ""], ["Terfloth", "Axel", ""], ["Song", "Houbing", ""]]}, {"id": "1711.09221", "submitter": "Ivano Notarnicola", "authors": "Ivano Notarnicola, Giuseppe Notarstefano", "title": "Constraint Coupled Distributed Optimization: a Relaxation and Duality\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider a general, challenging distributed optimization\nset-up arising in several important network control applications. Agents of a\nnetwork want to minimize the sum of local cost functions, each one depending on\na local variable, subject to local and coupling constraints, with the latter\ninvolving all the decision variables. We propose a novel fully distributed\nalgorithm based on a relaxation of the primal problem and an elegant\nexploration of duality theory. Despite its complex derivation, based on several\nduality steps, the distributed algorithm has a very simple and intuitive\nstructure. That is, each node finds a primal-dual optimal solution pair of a\nlocal, relaxed version of the original problem, and then updates suitable\nauxiliary local variables. We prove that agents asymptotically compute their\nportion of an optimal (feasible) solution of the original problem. This primal\nrecovery property is obtained without any averaging mechanism typically used in\ndual decomposition methods. To corroborate the theoretical results, we show how\nthe methodology applies to an instance of a Distributed Model Predictive\nControl scheme in a microgrid control scenario.\n", "versions": [{"version": "v1", "created": "Sat, 25 Nov 2017 09:15:43 GMT"}, {"version": "v2", "created": "Thu, 14 Jun 2018 08:17:57 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Notarnicola", "Ivano", ""], ["Notarstefano", "Giuseppe", ""]]}, {"id": "1711.09258", "submitter": "Hsin-Hao Su", "authors": "Bernhard Haeupler, Jeet Mohapatra, Hsin-Hao Su", "title": "Optimal Gossip Algorithms for Exact and Approximate Quantile\n  Computations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper gives drastically faster gossip algorithms to compute exact and\napproximate quantiles.\n  Gossip algorithms, which allow each node to contact a uniformly random other\nnode in each round, have been intensely studied and been adopted in many\napplications due to their fast convergence and their robustness to failures.\nKempe et al. [FOCS'03] gave gossip algorithms to compute important aggregate\nstatistics if every node is given a value. In particular, they gave a beautiful\n$O(\\log n + \\log \\frac{1}{\\epsilon})$ round algorithm to $\\epsilon$-approximate\nthe sum of all values and an $O(\\log^2 n)$ round algorithm to compute the exact\n$\\phi$-quantile, i.e., the the $\\lceil \\phi n \\rceil$ smallest value.\n  We give an quadratically faster and in fact optimal gossip algorithm for the\nexact $\\phi$-quantile problem which runs in $O(\\log n)$ rounds. We furthermore\nshow that one can achieve an exponential speedup if one allows for an\n$\\epsilon$-approximation. We give an $O(\\log \\log n + \\log \\frac{1}{\\epsilon})$\nround gossip algorithm which computes a value of rank between $\\phi n$ and\n$(\\phi+\\epsilon)n$ at every node.% for any $0 \\leq \\phi \\leq 1$ and $0 <\n\\epsilon < 1$. Our algorithms are extremely simple and very robust - they can\nbe operated with the same running times even if every transmission fails with\na, potentially different, constant probability. We also give a matching\n$\\Omega(\\log \\log n + \\log \\frac{1}{\\epsilon})$ lower bound which shows that\nour algorithm is optimal for all values of $\\epsilon$.\n", "versions": [{"version": "v1", "created": "Sat, 25 Nov 2017 16:30:42 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Haeupler", "Bernhard", ""], ["Mohapatra", "Jeet", ""], ["Su", "Hsin-Hao", ""]]}, {"id": "1711.09590", "submitter": "Anna Minaeva", "authors": "Anna Minaeva, Premysl Sucha, Benny Akesson, Zdenek Hanzalek", "title": "Scalable and Efficient Configuration of Time-Division Multiplexed\n  Resources", "comments": null, "journal-ref": "Journal of Systems and Software 113 (2016): 44-58", "doi": "10.1016/j.jss.2015.11.019", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consumer-electronics systems are becoming increasingly complex as the number\nof integrated applications is growing. Some of these applications have\nreal-time requirements, while other non-real-time applications only require\ngood average performance. For cost-efficient design, contemporary platforms\nfeature an increasing number of cores that share resources, such as memories\nand interconnects. However, resource sharing causes contention that must be\nresolved by a resource arbiter, such as Time-Division Multiplexing. A key\nchallenge is to configure this arbiter to satisfy the bandwidth and latency\nrequirements of the real-time applications, while maximizing the slack capacity\nto improve performance of their non-real-time counterparts. As this\nconfiguration problem is NP-hard, a sophisticated automated configuration\nmethod is required to avoid negatively impacting design time.\n  The main contributions of this article are: (1) an optimal approach that\ntakes an existing integer linear programming (ILP) model addressing the problem\nand wraps it in a branch-and-price framework to improve scalability. (2) A\nfaster heuristic algorithm that typically provides near-optimal solutions. (3)\nAn experimental evaluation that quantitatively compares the branch-and-price\napproach to the previously formulated ILP model and the proposed heuristic. (4)\nA case study of an HD video and graphics processing system that demonstrates\nthe practical applicability of the approach.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 09:15:55 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Minaeva", "Anna", ""], ["Sucha", "Premysl", ""], ["Akesson", "Benny", ""], ["Hanzalek", "Zdenek", ""]]}, {"id": "1711.09713", "submitter": "Tristan Glatard", "authors": "Tristan Glatard, Gregory Kiar, Tristan Aumentado-Armstrong, Natacha\n  Beck, Pierre Bellec, R\\'emi Bernard, Axel Bonnet, Sorina Camarasu-Pop,\n  Fr\\'ed\\'eric Cervenansky, Samir Das, Rafael Ferreira da Silva, Guillaume\n  Flandin, Pascal Girard, Krzysztof J. Gorgolewski, Charles R.G. Guttmann,\n  Val\\'erie Hayot-Sasson, Pierre-Olivier Quirion, Pierre Rioux, Marc-Eienne\n  Rousseau and Alan C. Evans", "title": "Boutiques: a flexible framework for automated application integration in\n  computing platforms", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present Boutiques, a system to automatically publish, integrate and\nexecute applications across computational platforms. Boutiques applications are\ninstalled through software containers described in a rich and flexible JSON\nlanguage. A set of core tools facilitate the construction, validation, import,\nexecution, and publishing of applications. Boutiques is currently supported by\nseveral distinct virtual research platforms, and it has been used to describe\ndozens of applications in the neuroinformatics domain. We expect Boutiques to\nimprove the quality of application integration in computational platforms, to\nreduce redundancy of effort, to contribute to computational reproducibility,\nand to foster Open Science.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 03:46:16 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Glatard", "Tristan", ""], ["Kiar", "Gregory", ""], ["Aumentado-Armstrong", "Tristan", ""], ["Beck", "Natacha", ""], ["Bellec", "Pierre", ""], ["Bernard", "R\u00e9mi", ""], ["Bonnet", "Axel", ""], ["Camarasu-Pop", "Sorina", ""], ["Cervenansky", "Fr\u00e9d\u00e9ric", ""], ["Das", "Samir", ""], ["da Silva", "Rafael Ferreira", ""], ["Flandin", "Guillaume", ""], ["Girard", "Pascal", ""], ["Gorgolewski", "Krzysztof J.", ""], ["Guttmann", "Charles R. G.", ""], ["Hayot-Sasson", "Val\u00e9rie", ""], ["Quirion", "Pierre-Olivier", ""], ["Rioux", "Pierre", ""], ["Rousseau", "Marc-Eienne", ""], ["Evans", "Alan C.", ""]]}, {"id": "1711.09745", "submitter": "Hung Cao", "authors": "Hung Cao, Monica Wachowicz, Chiara Renso, Emanuele Carlini", "title": "An edge-fog-cloud platform for anticipatory learning process designed\n  for Internet of Mobile Things", "comments": "Keywords: Internet of Mobile Things, data streams, edge-fog-cloud\n  platform, anticipatory learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel architecture for data analytics targeting an\nanticipatory learning process in the context of the Internet of Mobile Things.\nThe architecture is geo-distributed and composed by edge, fog, and cloud\nresources that operate collectively to support such an anticipatory learning\nprocess. We designed the architecture to manage large volumes of data streams\ncoming from the IoMT devices, analyze in successive phases climbing up in the\nhierarchy of resources from edge, fog and cloud. We discuss the characteristics\nof the analytical tasks at each layer. We notice that the amount of data being\ntransported in the network decreases going from the edge, to the fog and\nfinally to the cloud, while the complexity of the computation increases. Such\ndesign allows to support different kind of analytical needs, from real-time to\nhistorical according to the type of resource being utilized. We have\nimplemented the proposed architecture as a proof-of-concept using the transit\ndata feeds from the area of Greater Moncton, Canada.\n", "versions": [{"version": "v1", "created": "Sun, 19 Nov 2017 21:20:16 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 22:40:08 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Cao", "Hung", ""], ["Wachowicz", "Monica", ""], ["Renso", "Chiara", ""], ["Carlini", "Emanuele", ""]]}, {"id": "1711.09756", "submitter": "Ad\\'an S\\'anchez de Pedro Crespo", "authors": "Ad\\'an S\\'anchez de Pedro and Daniele Levi and Luis Iv\\'an Cuende", "title": "Witnet: A Decentralized Oracle Network Protocol", "comments": "Version 0.1 - 58 pages, 18 figures - Reviewed and edited by D. Levi\n  and L.I. Cuende", "journal-ref": null, "doi": "10.13140/RG.2.2.28152.34560", "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Witnet is a decentralized oracle network (DON) that connects smart contracts\nto the outer world. Generally speaking, it allows any piece of software to\nretrieve the contents published at any web address at a certain point in time,\nwith complete and verifiable proof of its integrity and without blindly\ntrusting any third party. Witnet runs on a blockchain with a native protocol\ntoken (called Wit), which miners-called witnesses-earn by retrieving, attesting\nand delivering web contents for clients. On the other hand, clients spend Wit\nto pay witnesses for their Retrieve-Attest-Deliver (RAD) work. Witnesses also\ncompete to mine blocks with considerable rewards, but Witnet mining power is\nproportional to their previous performance in terms of honesty and\ntrustworthiness-this is, their reputation as witnesses. This creates a powerful\nincentive for witnesses to do their work honestly, protect their reputation and\nnot to deceive the network. The Witnet protocol is designed to assign the RAD\ntasks to witnesses in a way that mitigates most attack vectors to the greatest\nextent. At the same time, it includes a novel 'sharding' feature that (1)\nguarantees the efficiency and scalability of the network, (2) keeps the price\nof RAD tasks within reasonable bounds and (3) gives clients the freedom to\nadjust certainty and price by letting them choose how many witnesses will work\non their RAD tasks. When coupled with a Decentralized Storage Network (DSN),\nWitnet also gives us the possibility to build the Digital Knowledge Ark: a\ndecentralized, immutable, censorship-resistant and eternal archive of\nhumanity's most relevant digital data. A truth vault aimed to ensure that\nknowledge will remain democratic and verifiable forever and to prevent history\nfrom being written by the victors.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 15:23:42 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["de Pedro", "Ad\u00e1n S\u00e1nchez", ""], ["Levi", "Daniele", ""], ["Cuende", "Luis Iv\u00e1n", ""]]}, {"id": "1711.09791", "submitter": "Ashkan Tousimojarad Mr", "authors": "Ashkan Tousimojarad, Wim Vanderbauwhede, W Paul Cockshott", "title": "2D Image Convolution using Three Parallel Programming Models on the Xeon\n  Phi", "comments": "This work has been included in the doctoral thesis of Ashkan\n  Tousimojarad, \"GPRM: a high performance programming framework for manycore\n  processors\", University of Glasgow, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image convolution is widely used for sharpening, blurring and edge detection.\nIn this paper, we review two common algorithms for convolving a 2D image by a\nseparable kernel (filter). After optimising the naive codes using loop\nunrolling and SIMD vectorisation, we choose the algorithm with better\nperformance as the baseline for parallelisation. We then compare the parallel\nperformance of the optimised code using OpenMP, OpenCL and GPRM implementations\non the Intel Xeon Phi. We also measure the effects of optimisation techniques\nand demonstrate how they affect both sequential and parallel performance. Apart\nfrom comparing the code complexity as well as the performance of the chosen\nparallel programming models, we investigate the impact of a parallelisation\ntechnique, task agglomeration in GPRM.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 15:59:53 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Tousimojarad", "Ashkan", ""], ["Vanderbauwhede", "Wim", ""], ["Cockshott", "W Paul", ""]]}, {"id": "1711.09964", "submitter": "Vaneet Aggarwal", "authors": "Vaneet Aggarwal and Tian Lan and Suresh Subramaniam and Maotong Xu", "title": "On the Approximability of Related Machine Scheduling under Arbitrary\n  Precedence", "comments": "Accepted to IEEE Transactions on Network and Service Management, Apr\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed computing systems often need to consider the scheduling problem\ninvolving a collection of highly dependent data-processing tasks that must work\nin concert to achieve mission-critical objectives. This paper considers the\nunrelated machine scheduling problem for minimizing weighted sum completion\ntime under arbitrary precedence constraints and on heterogeneous machines with\ndifferent processing speeds. The problem is known to be strongly NP-hard even\nin the single machine setting. By making use of Queyranne's constraint set and\nconstructing a novel Linear Programming relaxation for the scheduling problem\nunder arbitrary precedence constraints, our results in this paper advance the\nstate of the art. We develop a $2(1+(m-1)/D)$-approximation algorithm (and\n$2(1+(m-1)/D)+1$-approximation) for the scheduling problem with zero release\ntime (and arbitrary release time), where $m$ is the number of servers and $D$\nis the task-skewness product. The algorithm can be efficiently computed in\npolynomial time using the Ellipsoid method and achieves nearly optimal\nperformance in practice as $D>O(m)$ when the number of tasks per job to\nschedule is sufficiently larger than the number of machines available. Our\nimplementation and evaluation using a heterogeneous testbed and real-world\nbenchmarks confirms significant improvement in weighted sum completion time for\ndependent computing tasks.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 20:14:29 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 17:44:37 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Aggarwal", "Vaneet", ""], ["Lan", "Tian", ""], ["Subramaniam", "Suresh", ""], ["Xu", "Maotong", ""]]}, {"id": "1711.10102", "submitter": "Zhi Cao", "authors": "Zhi Cao, Honggang Zhang, Benyuan Liu and Bo Sheng", "title": "A Game-theoretic Framework for Revenue Sharing in Edge-Cloud Computing\n  System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a game-theoretic framework to ex- plore revenue sharing in an\nEdge-Cloud computing system, in which computing service providers at the edge\nof the Internet (edge providers) and computing service providers at the cloud\n(cloud providers) co-exist and collectively provide computing resources to\nclients (e.g., end users or applications) at the edge. Different from\ntraditional cloud computing, the providers in an Edge-Cloud system are\nindependent and self-interested. To achieve high system-level efficiency, the\nmanager of the system adopts a task distribution mechanism to maximize the\ntotal revenue received from clients and also adopts a revenue sharing mechanism\nto split the received revenue among computing servers (and hence service\nproviders). Under those system-level mechanisms, service providers attempt to\ngame with the system in order to maximize their own utilities, by strategically\nallocating their resources (e.g., computing servers).\n  Our framework models the competition among the providers in an Edge-Cloud\nsystem as a non-cooperative game. Our simulations and experiments on an\nemulation system have shown the existence of Nash equilibrium in such a game.\nWe find that revenue sharing mechanisms have a significant impact on the\nsystem-level efficiency at Nash equilibria, and surprisingly the revenue\nsharing mechanism based directly on actual contributions can result in\nsignificantly worse system efficiency than Shapley value sharing mechanism and\nOrtmann proportional sharing mechanism. Our framework provides an effective\neconomics approach to understanding and designing efficient Edge-Cloud\ncomputing systems.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 03:29:24 GMT"}, {"version": "v2", "created": "Fri, 5 Oct 2018 03:12:57 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["Cao", "Zhi", ""], ["Zhang", "Honggang", ""], ["Liu", "Benyuan", ""], ["Sheng", "Bo", ""]]}, {"id": "1711.10123", "submitter": "Jaehee Jang", "authors": "Jaehee Jang and Byungook Na and Sungroh Yoon", "title": "Homomorphic Parameter Compression for Distributed Deep Learning Training", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed training of deep neural networks has received significant\nresearch interest, and its major approaches include implementations on multiple\nGPUs and clusters. Parallelization can dramatically improve the efficiency of\ntraining deep and complicated models with large-scale data. A fundamental\nbarrier against the speedup of DNN training, however, is the trade-off between\ncomputation and communication time. In other words, increasing the number of\nworker nodes decreases the time consumed in computation while simultaneously\nincreasing communication overhead under constrained network bandwidth,\nespecially in commodity hardware environments. To alleviate this trade-off, we\nsuggest the idea of homomorphic parameter compression, which compresses\nparameters with the least expense and trains the DNN with the compressed\nrepresentation. Although the specific method is yet to be discovered, we\ndemonstrate that there is a high probability that the homomorphism can reduce\nthe communication overhead, thanks to little compression and decompression\ntimes. We also provide theoretical speedup of homomorphic compression.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 04:47:59 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Jang", "Jaehee", ""], ["Na", "Byungook", ""], ["Yoon", "Sungroh", ""]]}, {"id": "1711.10155", "submitter": "Gregory Schwartzman", "authors": "Ken-ichi Kawarabayashi, Gregory Schwartzman", "title": "Adapting Local Sequential Algorithms to the Distributed Setting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a well known fact that sequential algorithms which exhibit a strong\n\"local\" nature can be adapted to the distributed setting given a legal graph\ncoloring. The running time of the distributed algorithm will then be at least\nthe number of colors. Surprisingly, this well known idea was never formally\nstated as a unified framework. In this paper we aim to define a robust family\nof local sequential algorithms which can be easily adapted to the distributed\nsetting. We then develop new tools to further enhance these algorithms,\nachieving state of the art results for fundamental problems.\n  We define a simple class of greedy-like algorithms which we call\n\\emph{orderless-local} algorithms. We show that given a legal $c$-coloring of\nthe graph, every algorithm in this family can be converted into a distributed\nalgorithm running in $O(c)$ communication rounds in the CONGEST model. We show\nthat this family is indeed robust as both the method of conditional\nexpectations and the unconstrained submodular maximization algorithm of\nBuchbinder \\etal \\cite{BuchbinderFNS15} can be expressed as orderless-local\nalgorithms for \\emph{local utility functions} --- Utility functions which have\na strong local nature to them.\n  We use the above algorithms as a base for new distributed approximation\nalgorithms for the weighted variants of some fundamental problems: Max $k$-Cut,\nMax-DiCut, Max 2-SAT and correlation clustering. We develop algorithms which\nhave the same approximation guarantees as their sequential counterparts, up to\na constant additive $\\epsilon$ factor, while achieving an $O(\\log^* n)$ running\ntime for deterministic algorithms and $O(\\epsilon^{-1})$ running time for\nrandomized ones. This improves exponentially upon the currently best known\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 07:18:21 GMT"}, {"version": "v2", "created": "Mon, 19 Feb 2018 02:40:10 GMT"}, {"version": "v3", "created": "Sat, 12 May 2018 08:16:46 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Kawarabayashi", "Ken-ichi", ""], ["Schwartzman", "Gregory", ""]]}, {"id": "1711.10221", "submitter": "Sarod Yatawatta", "authors": "Sarod Yatawatta, Faruk Diblen, Hanno Spreeuw, L.V.E. Koopmans", "title": "Data Multiplexing in Radio Interferometric Calibration", "comments": "MNRAS Accepted 2017 November 28. Received 2017 November 28; in\n  original form 2017 July 06", "journal-ref": null, "doi": "10.1093/mnras/stx3130", "report-no": null, "categories": "astro-ph.IM cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New and upcoming radio interferometers will produce unprecedented amounts of\ndata that demand extremely powerful computers for processing. This is a\nlimiting factor due to the large computational power and energy costs involved.\nSuch limitations restrict several key data processing steps in radio\ninterferometry. One such step is calibration where systematic errors in the\ndata are determined and corrected. Accurate calibration is an essential\ncomponent in reaching many scientific goals in radio astronomy and the use of\nconsensus optimization that exploits the continuity of systematic errors across\nfrequency significantly improves calibration accuracy. In order to reach full\nconsensus, data at all frequencies need to be calibrated simultaneously. In the\nSKA regime, this can become intractable if the available compute agents do not\nhave the resources to process data from all frequency channels simultaneously.\nIn this paper, we propose a multiplexing scheme that is based on the\nalternating direction method of multipliers (ADMM) with cyclic updates. With\nthis scheme, it is possible to simultaneously calibrate the full dataset using\nfar fewer compute agents than the number of frequencies at which data are\navailable. We give simulation results to show the feasibility of the proposed\nmultiplexing scheme in simultaneously calibrating a full dataset when a limited\nnumber of compute agents are available.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 10:54:34 GMT"}, {"version": "v2", "created": "Wed, 29 Nov 2017 11:09:37 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Yatawatta", "Sarod", ""], ["Diblen", "Faruk", ""], ["Spreeuw", "Hanno", ""], ["Koopmans", "L. V. E.", ""]]}, {"id": "1711.10464", "submitter": "Yasser El-Sonbaty", "authors": "Ibrahim Abdelkader, Yasser El-Sonbaty and Mohamed El-Habrouk", "title": "Openmv: A Python powered, extensible machine vision camera", "comments": null, "journal-ref": "International Conferences Computer Graphics, Visualization,\n  Computer Vision and Image Processing 2017", "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in semiconductor manufacturing processes and large scale integration\nkeep pushing demanding applications further away from centralized processing,\nand closer to the edges of the network (i.e. Edge Computing). It has become\npossible to perform complex in-network image processing using low-power\nembedded smart cameras, enabling a multitude of new collaborative image\nprocessing applications. This paper introduces OpenMV, a new low-power smart\ncamera that lends itself naturally to wireless sensor networks and machine\nvision applications. The uniqueness of this platform lies in running an\nembedded Python3 interpreter, allowing its peripherals and machine vision\nlibrary to be scripted in Python. In addition, its hardware is extensible via\nmodules that augment the platform with new capabilities, such as thermal\nimaging and networking modules.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 08:52:12 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Abdelkader", "Ibrahim", ""], ["El-Sonbaty", "Yasser", ""], ["El-Habrouk", "Mohamed", ""]]}, {"id": "1711.10556", "submitter": "Ziyang Chen", "authors": "Ziyang Chen, Tamanna Shikh-Bahaei, Mohammad Shikh-Bahaei", "title": "Edge Computing and Dynamic Vision Sensing for Low Delay Access to Visual\n  Medical Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new method is proposed to decrease the transmission delay of visual and\nnon-visual medical records by using edge computing and Dynamic Vision Sensing\n(DVS) technologies. The simulation results show that the proposed scheme can\ndecrease the transmission delay by 89.15% to 93.23%. The maximum number of\npatients who can be served by edge devices is analysed.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 16:16:11 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Chen", "Ziyang", ""], ["Shikh-Bahaei", "Tamanna", ""], ["Shikh-Bahaei", "Mohammad", ""]]}, {"id": "1711.10685", "submitter": "Rajni Aron", "authors": "Deepak kumar Aggarwal, Rajni Aron", "title": "IoT based Platform as a Service for Provisioning of Concurrent\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The modern era has seen a speedy growth in the Internet of Things (IoT). As\nper statistics of 2020, twenty billion devices will be connected to the\nInternet. This massive increase in Internet connected devices will lead to a\nlot of efforts to execute critical concurrent applications such fire detection,\nhealth care based system, disaster management, high energy physics,\nautomobiles, and medical imaging efficiently. To fasten the emergence of novel\napplications, this vast infrastructure requires \"Platform as a Service(PaaS)\"\nmodel to leverage IoT things. As a single global standard for all device types\nand IoT-based application domain is impracticable, we propose an IoT-based\nCloud to leverage PaaS model in this paper. This model can host the concurrent\napplication for Wireless Sensor Network (WSN). The proposed model offers the\ncommunication interface among processes by uniquely allocating network\ninterface to a particular container.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 05:23:54 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Aggarwal", "Deepak kumar", ""], ["Aron", "Rajni", ""]]}, {"id": "1711.10783", "submitter": "Tiancheng Li", "authors": "Tiancheng Li, Juan M Corchado and Shudong Sun", "title": "Partial Consensus and Conservative Fusion of Gaussian Mixtures for\n  Distributed PHD Fusion", "comments": "7 pages", "journal-ref": "IEEE Transactions on Aerospace and Electronic Systems, Volume: 55,\n  Issue: 5, Oct. 2019, Pages: 2150 - 2163", "doi": "10.1109/TAES.2018.2882960", "report-no": null, "categories": "cs.SY cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel consensus notion, called \"partial consensus\", for\ndistributed GM-PHD (Gaussian mixture probability hypothesis density) fusion\nbased on a peer-to-peer (P2P) sensor network, in which only highly-weighted\nposterior Gaussian components (GCs) are disseminated in the P2P communication\nfor fusion while the insignificant GCs are not involved. The partial consensus\ndoes not only enjoy high efficiency in both network communication and local\nfusion computation, but also significantly reduces the affect of potential\nfalse data (clutter) to the filter, leading to increased signal-to-noise ratio\nat local sensors. Two \"conservative\" mixture reduction schemes are advocated\nfor fusing the shared GCs in a fully distributed manner. One is given by\npairwise averaging GCs between sensors based on Hungarian assignment and the\nother is merging close GCs based a new GM merging scheme. The proposed\napproaches have a close connection to the conservative fusion approaches known\nas covariance union and arithmetic mean density. In parallel, average consensus\nis sought on the cardinality distribution (namely the GM weight sum) among\nsensors. Simulations for tracking either a single target or multiple targets\nthat simultaneously appear are presented based on a sensor network where each\nsensor operates a GM-PHD filter, in order to compare our approaches with the\nbenchmark generalized covariance intersection approach. The results demonstrate\nthat the partial, arithmetic average, consensus outperforms the complete,\ngeometric average, consensus.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 11:31:00 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Li", "Tiancheng", ""], ["Corchado", "Juan M", ""], ["Sun", "Shudong", ""]]}, {"id": "1711.10838", "submitter": "Dhafer Ben Arbia", "authors": "Dhafer Ben Arbia (1), Muhammad Mahtab Alam, Rabah Attia (1), Elyes\n  Hamida (2) ((1) SERCOM, (2) IRT SystemX)", "title": "Behavior of Wireless Body-to-Body Networks Routing Strategies for Public\n  Protection and Disaster Relief", "comments": "WiMob, Oct 2015, Abu Dhabi, United Arab Emirates", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Critical and public safety operations require real-time data transfer from\nthe incident area(s) to the distant operations command center going through the\nevacuation and medical support areas. Any delay in communication may cause\nsignificant loss. In some cases, it is anticipated that the existing\ncommunication infrastructures can be damaged or out-of-service. It is thus\nrequired to deploy tactical ad-hoc networks to cover the operation zones.\nRouting data over the deployed network is a significant challenge with\nconsideration to the operations conditions. In this paper we evaluate the\nperformance of mutli-hop routing protocols while using different wireless\ntechnologies in an urban critical and emergency scenario. Using a realistic\nmobility model, Mobile Ad hoc, geographic based and data-centric routing\nprotocols are evaluated with different communication technologies (i.e. WiFi\nIEEE 802.11; WSN IEEE 802.15.4; WBAN IEEE 802.15.6). It is concluded that, WiFi\nIEEE 802.11 is the best wireless technology with consideration to the packet\nreception rate and the energy consumption. Whereas, in terms of delay, WBAN\nIEEE 802.15.6 is the most efficient. With regards to the routing protocols,\nassuming that the location information is available, geographical based routing\nprotocol with WiFi IEEE 802.11 performed much better compared to the others\nrouting protocols. In case where the location information is unavailable,\ngradient based routing protocol with WBAN IEEE 802.15.6 seems the best\ncombination.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 13:21:46 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Arbia", "Dhafer Ben", "", "SERCOM"], ["Alam", "Muhammad Mahtab", "", "SERCOM"], ["Attia", "Rabah", "", "SERCOM"], ["Hamida", "Elyes", "", "IRT SystemX"]]}, {"id": "1711.10845", "submitter": "Dhafer Ben Arbia", "authors": "Dhafer Ben Arbia (1), Muhammad Mahtab Alam (2), Rabah Attia (1), Elyes\n  Hamida (2) ((1) SERCOM, (2) TT\\\"U, (3) IRT SystemX)", "title": "Data Dissemination Strategies for Emerging Wireless Body-to-Body\n  Networks based Internet of Humans", "comments": "WiMob, Oct 2015, Abu Dhabi, United Arab Emirates", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent advent of Internet of Humans (IoH), wireless body-to-body\nnetworks (WBBNs) are emerging as the fundamental part of this new paradigm. In\nparticular with reference to newly emerging applications, the research trends\non data routing and dissemination strategies have gained a great interest in\nWBBN. In this paper, we present the performance evaluation of the clustered and\ndistributed data dissemination approaches in tactical WBBN. We used a realistic\nradio-link and biomechanical mobility model for on-body motions, and group\nmobility model for WBBN to effectively realize rescue and emergency management\napplication scenario. In this regard, we are using the newly proposed IEEE\n802.15.6 standard targeted for body area networks. Extensive (IEEE 802.15.6\nstandard compliance) network level, packet oriented simulations are conducted\nin WSNet simulator. During the simulations, various payloads, frequencies\n(narrow-band) and modulation techniques are exploited. We based our performance\nevaluation on relevant metrics according to the operational requirements for\ntactical networks such as packet reception ratio, latency, energy consumption\nand hop count. The results showed a trade-offs between clustered-based and\ndistributed-based dissemination approaches. With regards to packet delay,\ndistributed approach provided the best performance. However, in terms of\naverage packet reception ratio (PRR), clustered-based approach achieves up to\n97% reception and remained the best strategy. Whereas, the results of the hop\ncount and energy consumption are almost comparable in both schemes.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 13:41:47 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Arbia", "Dhafer Ben", "", "SERCOM"], ["Alam", "Muhammad Mahtab", "", "TT\u00dc"], ["Attia", "Rabah", "", "SERCOM"], ["Hamida", "Elyes", "", "TT\u00dc"]]}, {"id": "1711.11005", "submitter": "Sufian Hameed", "authors": "Bilal Karim Mughal, Sufian Hameed, Ghulam Muhammad Shaikh", "title": "A Centralized Reputation Management Scheme for Isolating Malicious\n  Controller(s) in Distributed Software-Defined Networks", "comments": "6 pages, 4 figures", "journal-ref": "International Journal of Advanced Computer Science and\n  Applications(ijacsa), 7(12), 2016", "doi": "10.14569/IJACSA.2016.071248", "report-no": null, "categories": "cs.NI cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software-Defined Networks have seen an increasing in their deployment because\nthey offer better network manageability compared to traditional networks.\nDespite their immense success and popularity, various security issues in SDN\nremain open problems for research. Particularly, the problem of securing the\ncontrollers in distributed environment is still short of any solutions. This\npaper proposes a scheme to identify any rogue/malicious controller(s) in a\ndistributed environment. Our scheme is based on trust and reputation system\nwhich is centrally managed. As such, our scheme identifies any controllers\nacting maliciously by comparing the state of installed flows/policies with\npolicies that should be installed. Controllers rate each other on this basis\nand report the results to a central entity, which reports it to the network\nadministrator.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 18:30:11 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Mughal", "Bilal Karim", ""], ["Hameed", "Sufian", ""], ["Shaikh", "Ghulam Muhammad", ""]]}, {"id": "1711.11208", "submitter": "EPTCS", "authors": "Tobias Prehn (TU Berlin, Germany), Stephan Mennicke (TU Braunschweig,\n  Germany)", "title": "Keep it Fair: Equivalences", "comments": "In Proceedings ICE 2017, arXiv:1711.10708", "journal-ref": "EPTCS 261, 2017, pp. 5-16", "doi": "10.4204/EPTCS.261.4", "report-no": null, "categories": "cs.LO cs.DC cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For models of concurrent and distributed systems, it is important and also\nchallenging to establish correctness in terms of safety and/or liveness\nproperties. Theories of distributed systems consider equivalences fundamental,\nsince they (1) preserve desirable correctness characteristics and (2) often\nallow for component substitution making compositional reasoning feasible.\nModeling distributed systems often requires abstraction utilizing\nnondeterminism which induces unintended behaviors in terms of infinite\nexecutions with one nondeterministic choice being recurrently resolved, each\ntime neglecting a single alternative. These situations are considered\nunrealistic or highly improbable. Fairness assumptions are commonly used to\nfilter system behaviors, thereby distinguishing between realistic and\nunrealistic executions. This allows for key arguments in correctness proofs of\ndistributed systems, which would not be possible otherwise. Our contribution is\nan equivalence spectrum in which fairness assumptions are preserved. The\nidentified equivalences allow for (compositional) reasoning about correctness\nincorporating fairness assumptions.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 03:45:30 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Prehn", "Tobias", "", "TU Berlin, Germany"], ["Mennicke", "Stephan", "", "TU Braunschweig,\n  Germany"]]}]