[{"id": "1306.0077", "submitter": "Steven William Cheng", "authors": "Steven Cheng, Lisa Higham, Jalal Kawash", "title": "Partition Consistency: A Case Study in Modeling Systems with Weak Memory\n  Consistency and Proving Correctness of their Implementations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiprocess systems, including grid systems, multiprocessors and multicore\ncomputers, incorporate a variety of specialized hardware and software\nmechanisms, which speed computation, but result in complex memory behavior. As\na consequence, the possible outcomes of a concurrent program can be unexpected.\nA memory consistency model is a description of the behaviour of such a system.\nAbstract memory consistency models aim to capture the concrete implementations\nand architectures. Therefore, formal specification of the implementation or\narchitecture is necessary, and proofs of correspondence between the abstract\nand the concrete models are required.\n  This paper provides a case study of this process. We specify a new model,\npartition consistency, that generalizes many existing consistency models. A\nconcrete message-passing network model is also specified. Implementations of\npartition consistency on this network model are then presented and proved\ncorrect. A middle level of abstraction is utilized to facilitate the proofs.\nAll three levels of abstraction are specified using the same framework. The\npaper aims to illustrate a general methodology and techniques for specifying\nmemory consistency models and proving the correctness of their implementations.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2013 04:35:02 GMT"}], "update_date": "2013-06-04", "authors_parsed": [["Cheng", "Steven", ""], ["Higham", "Lisa", ""], ["Kawash", "Jalal", ""]]}, {"id": "1306.0260", "submitter": "Choon Yik Tang", "authors": "Jie Lu, Choon Yik Tang", "title": "A Distributed Algorithm for Solving Positive Definite Linear Equations\n  over Networks with Membership Dynamics", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of solving a symmetric positive definite\nsystem of linear equations over a network of agents with arbitrary asynchronous\ninteractions and membership dynamics. The latter implies that each agent is\nallowed to join and leave the network at any time, for infinitely many times,\nand lose all its memory upon leaving. We develop Subset Equalizing (SE), a\ndistributed asynchronous algorithm for solving such a problem. To design and\nanalyze SE, we introduce a novel time-varying Lyapunov-like function, defined\non a state space with changing dimension, and a generalized concept of network\nconnectivity, capable of handling such interactions and membership dynamics.\nBased on them, we establish the boundedness, asymptotic convergence, and\nexponential convergence of SE, along with a bound on its convergence rate.\nFinally, through extensive simulation, we show that SE is effective in a\nvolatile agent network and that a special case of SE, termed Groupwise\nEqualizing, is significantly more bandwidth/energy efficient than two existing\nalgorithms in multi-hop wireless networks.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jun 2013 22:59:02 GMT"}, {"version": "v2", "created": "Sun, 12 Jun 2016 19:45:07 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Lu", "Jie", ""], ["Tang", "Choon Yik", ""]]}, {"id": "1306.0326", "submitter": "Tomasz Kajdanowicz", "authors": "Tomasz Kajdanowicz, Przemyslaw Kazienko, Wojciech Indyk", "title": "Parallel Processing of Large Graphs", "comments": "Preprint submitted to Future Generation Computer Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  More and more large data collections are gathered worldwide in various IT\nsystems. Many of them possess the networked nature and need to be processed and\nanalysed as graph structures. Due to their size they require very often usage\nof parallel paradigm for efficient computation. Three parallel techniques have\nbeen compared in the paper: MapReduce, its map-side join extension and Bulk\nSynchronous Parallel (BSP). They are implemented for two different graph\nproblems: calculation of single source shortest paths (SSSP) and collective\nclassification of graph nodes by means of relational influence propagation\n(RIP). The methods and algorithms are applied to several network datasets\ndiffering in size and structural profile, originating from three domains:\ntelecommunication, multimedia and microblog. The results revealed that\niterative graph processing with the BSP implementation always and\nsignificantly, even up to 10 times outperforms MapReduce, especially for\nalgorithms with many iterations and sparse communication. Also MapReduce\nextension based on map-side join usually noticeably presents better efficiency,\nalthough not as much as BSP. Nevertheless, MapReduce still remains the good\nalternative for enormous networks, whose data structures do not fit in local\nmemories.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2013 08:44:32 GMT"}], "update_date": "2013-06-04", "authors_parsed": [["Kajdanowicz", "Tomasz", ""], ["Kazienko", "Przemyslaw", ""], ["Indyk", "Wojciech", ""]]}, {"id": "1306.0441", "submitter": "Islam Elgedawy", "authors": "Islam Elgedawy", "title": "DCaaS: Data Consistency as a Service for Managing Data Uncertainty on\n  the Clouds", "comments": "10 pages, 6 figures, 2 tables", "journal-ref": "International Journal of Advanced Computer Science and\n  Applications(IJACSA), paper 10, pages (59-68), Vol. 4, No.5, 2013", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensuring data correctness over partitioned distributed database systems is a\nclassical problem. Classical solutions proposed to solve this problem are\nmainly adopting locking or blocking techniques. These techniques are not\nsuitable for cloud environments as they produce terrible response times; due to\nthe long latency and faultiness of wide area network connections among cloud\ndatacenters. One way to improve performance is to restrict access of\nusers-bases to specific datacenters and avoid data sharing between datacenters.\nHowever, conflicts might appear when data is replicated between datacenters;\nnevertheless change propagation timeliness is not guaranteed. Such problems\ncreated data uncertainty on cloud environments. Managing data uncertainty is\none of the main obstacles for supporting global distributed transactions on the\nclouds. To overcome this problem, this paper proposes an quota-based approach\nfor managing data uncertainty on the clouds that guarantees global data\ncorrectness without global locking or blocking. To decouple service developers\nfrom the hassles of managing data uncertainty, we propose to use a new platform\nservice (i.e. Data Consistency as a Service (DCaaS)) to encapsulate the\nproposed approach. DCaaS service also ensures SaaS services cloud portability,\nas it works as a cloud adapter between SaaS service instances. Experiments show\nthat proposed approach realized by the DCaaS service provides much better\nresponse time when compared with classical locking and blocking techniques.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2013 14:46:08 GMT"}], "update_date": "2013-06-04", "authors_parsed": [["Elgedawy", "Islam", ""]]}, {"id": "1306.0448", "submitter": "Islam Elgedawy", "authors": "W. El-Haweet, Islam Elgedawy, Ibrahim Abd El-Salam", "title": "Adaptive Fixed Priority End-To-End Imprecise Scheduling In Distributed\n  Real Time Systems", "comments": "8 pages, 6 tables, conference, In Proceedings of SPECTS 2001", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In end-to-end distributed real time systems, a task may be executed\nsequentially on different processors. The end-toend task response time must not\nexceed the end-to-end task deadline to consider the task a schedulable task. In\ntransient over load periods, deadlines may be missed or processors may\nsaturate. The imprecise computation technique is a way to overcome the\nmentioned problems by trading off precision and timeliness. We developed an\nimprecise integrated framework for scheduling fixed priority end-to-end tasks\nin distributed real time systems by extending an existing integrated framework\nfor the same problem. We devised a new priority assignment scheme called global\nmandatory relevance scheme to meet the concept of imprecise computation. We\ndevised an algorithm for processor utilization adjustment, this algorithm\ndecreases the processor load when the processor utilization is greater than\none. Also we extended the schedulability analysis algorithms presented in the\nold framework to allow adaptive priority assignment and to meet imprecise\ncomputation concept. Simulation results showed that our new framework is more\ndependable and predictable than the existing framework over transient overload\nperiods.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2013 15:07:27 GMT"}], "update_date": "2013-06-04", "authors_parsed": [["El-Haweet", "W.", ""], ["Elgedawy", "Islam", ""], ["El-Salam", "Ibrahim Abd", ""]]}, {"id": "1306.0573", "submitter": "Navtej Singh", "authors": "Navtej Singh, Lisa-Marie Browne, Ray Butler", "title": "Parallel Astronomical Data Processing with Python: Recipes for multicore\n  machines", "comments": "15 pages, 7 figures, 1 table, \"for associated test code, see\n  http://astro.nuigalway.ie/staff/navtejs\", Accepted for publication in\n  Astronomy and Computing", "journal-ref": null, "doi": "10.1016/j.ascom.2013.04.002", "report-no": null, "categories": "astro-ph.IM cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High performance computing has been used in various fields of astrophysical\nresearch. But most of it is implemented on massively parallel systems\n(supercomputers) or graphical processing unit clusters. With the advent of\nmulticore processors in the last decade, many serial software codes have been\nre-implemented in parallel mode to utilize the full potential of these\nprocessors. In this paper, we propose parallel processing recipes for multicore\nmachines for astronomical data processing. The target audience are astronomers\nwho are using Python as their preferred scripting language and who may be using\nPyRAF/IRAF for data processing. Three problems of varied complexity were\nbenchmarked on three different types of multicore processors to demonstrate the\nbenefits, in terms of execution time, of parallelizing data processing tasks.\nThe native multiprocessing module available in Python makes it a relatively\ntrivial task to implement the parallel code. We have also compared the three\nmultiprocessing approaches - Pool/Map, Process/Queue, and Parallel Python. Our\ntest codes are freely available and can be downloaded from our website.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2013 20:00:04 GMT"}], "update_date": "2013-07-30", "authors_parsed": [["Singh", "Navtej", ""], ["Browne", "Lisa-Marie", ""], ["Butler", "Ray", ""]]}, {"id": "1306.0604", "submitter": "Yingyu Liang", "authors": "Maria Florina Balcan, Steven Ehrlich, Yingyu Liang", "title": "Distributed k-Means and k-Median Clustering on General Topologies", "comments": "Corrected Theorem 4 in the appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides new algorithms for distributed clustering for two popular\ncenter-based objectives, k-median and k-means. These algorithms have provable\nguarantees and improve communication complexity over existing approaches.\nFollowing a classic approach in clustering by \\cite{har2004coresets}, we reduce\nthe problem of finding a clustering with low cost to the problem of finding a\ncoreset of small size. We provide a distributed method for constructing a\nglobal coreset which improves over the previous methods by reducing the\ncommunication complexity, and which works over general communication\ntopologies. Experimental results on large scale data sets show that this\napproach outperforms other coreset-based distributed clustering algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2013 21:49:19 GMT"}, {"version": "v2", "created": "Sun, 30 Jun 2013 02:26:33 GMT"}, {"version": "v3", "created": "Wed, 30 Oct 2013 19:20:30 GMT"}, {"version": "v4", "created": "Sat, 25 Jan 2020 23:23:11 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Balcan", "Maria Florina", ""], ["Ehrlich", "Steven", ""], ["Liang", "Yingyu", ""]]}, {"id": "1306.0846", "submitter": "Gary McGilvary Mr", "authors": "Gary A. McGilvary, Adam Barker, Ashley Lloyd and Malcolm Atkinson", "title": "V-BOINC: The Virtualization of BOINC", "comments": "9 pages, Proceedings of the 13th IEEE/ACM International Symposium on\n  Cluster, Cloud and Grid Computing (CCGrid 2013)", "journal-ref": null, "doi": "10.1109/CCGrid.2013.14", "report-no": null, "categories": "cs.DC cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Berkeley Open Infrastructure for Network Computing (BOINC) is an open\nsource client-server middleware system created to allow projects with large\ncomputational requirements, usually set in the scientific domain, to utilize a\ntechnically unlimited number of volunteer machines distributed over large\nphysical distances. However various problems exist deploying applications over\nthese heterogeneous machines using BOINC: applications must be ported to each\nmachine architecture type, the project server must be trusted to supply\nauthentic applications, applications that do not regularly checkpoint may lose\nexecution progress upon volunteer machine termination and applications that\nhave dependencies may find it difficult to run under BOINC.\n  To solve such problems we introduce virtual BOINC, or V-BOINC, where virtual\nmachines are used to run computations on volunteer machines. Application\ndevelopers can then compile their applications on a single architecture,\ncheckpointing issues are solved through virtualization API's and many security\nconcerns are addressed via the virtual machine's sandbox environment. In this\npaper we focus on outlining a unique approach on how virtualization can be\nintroduced into BOINC and demonstrate that V-BOINC offers acceptable\ncomputational performance when compared to regular BOINC. Finally we show that\napplications with dependencies can easily run under V-BOINC in turn increasing\nthe computational potential volunteer computing offers to the general public\nand project developers.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2013 16:27:30 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["McGilvary", "Gary A.", ""], ["Barker", "Adam", ""], ["Lloyd", "Ashley", ""], ["Atkinson", "Malcolm", ""]]}, {"id": "1306.1158", "submitter": "Harish Chintakunta", "authors": "Harish Chintakunta, Hamid Krim", "title": "Distributed computation of homology using harmonics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AT cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a distributed algorithm to compute the first homology of a\nsimplicial complex. Such algorithms are very useful in topological analysis of\nsensor networks, such as its coverage properties. We employ spanning trees to\ncompute a basis for algebraic 1-cycles, and then use harmonics to efficiently\nidentify the contractible and homologous cycles. The computational complexity\nof the algorithm is $O(|P|^\\omega)$, where $|P|$ is much smaller than the\nnumber of edges, and $\\omega$ is the complexity order of matrix multiplication.\nFor geometric graphs, we show using simulations that $|P|$ is very close to the\nfirst Betti number.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2013 15:54:19 GMT"}], "update_date": "2013-06-06", "authors_parsed": [["Chintakunta", "Harish", ""], ["Krim", "Hamid", ""]]}, {"id": "1306.1303", "submitter": "Putti Srinivasarao", "authors": "Putti Srinivasrao, V. P. C. Rao, A. Govardhan, Ambika Prasad Mohanty", "title": "Scalable Distributed Job Processing with Dynamic Load Balancing", "comments": "12 pages", "journal-ref": "International Journal of Distributed and Parallel Systems (IJDPS)\n  Vol.4, No.3, May 2013", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present here a cost effective framework for a robust scalable and\ndistributed job processing system that adapts to the dynamic computing needs\neasily with efficient load balancing for heterogeneous systems. The design is\nsuch that each of the components are self contained and do not depend on each\nother. Yet, they are still interconnected through an enterprise message bus so\nas to ensure safe, secure and reliable communication based on transactional\nfeatures to avoid duplication as well as data loss. The load balancing,\nfault-tolerance and failover recovery are built into the system through a\nmechanism of health check facility and a queue based load balancing. The system\nhas a centralized repository with central monitors to keep track of the\nprogress of various job executions as well as status of processors in\nreal-time. The basic requirement of assigning a priority and processing as per\npriority is built into the framework. The most important aspect of the\nframework is that it avoids the need for job migration by computing the target\nprocessors based on the current load and the various cost factors. The\nframework will have the capability to scale horizontally as well as vertically\nto achieve the required performance, thus effectively minimizing the total cost\nof ownership.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2013 06:32:54 GMT"}], "update_date": "2013-06-07", "authors_parsed": [["Srinivasrao", "Putti", ""], ["Rao", "V. P. C.", ""], ["Govardhan", "A.", ""], ["Mohanty", "Ambika Prasad", ""]]}, {"id": "1306.1373", "submitter": "Kgotlaetsile Mathews Modieginyane MSc", "authors": "Kgotlaetsile Mathews Modieginyane, Zenzo Polite Ncube and Naison\n  Gasela", "title": "CUDA Based Performance Evaluation of the Computational Efficiency of the\n  DCT Image Compression Technique on Both the CPU and GPU", "comments": "15 Pages, 11 Figures, 4 Tables, Advanced Computing: An International\n  Journal (ACIJ), Three Author Pictures (with little Bio for each) at last page", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in computing such as the massively parallel GPUs (Graphical\nProcessing Units),coupled with the need to store and deliver large quantities\nof digital data especially images, has brought a number of challenges for\nComputer Scientists, the research community and other stakeholders. These\nchallenges, such as prohibitively large costs to manipulate the digital data\namongst others, have been the focus of the research community in recent years\nand has led to the investigation of image compression techniques that can\nachieve excellent results. One such technique is the Discrete Cosine Transform,\nwhich helps separate an image into parts of differing frequencies and has the\nadvantage of excellent energy-compaction.\n  This paper investigates the use of the Compute Unified Device Architecture\n(CUDA) programming model to implement the DCT based Cordic based Loeffler\nalgorithm for efficient image compression. The computational efficiency is\nanalyzed and evaluated under both the CPU and GPU. The PSNR (Peak Signal to\nNoise Ratio) is used to evaluate image reconstruction quality in this paper.\nThe results are presented and discussed.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2013 11:11:16 GMT"}], "update_date": "2013-06-07", "authors_parsed": [["Modieginyane", "Kgotlaetsile Mathews", ""], ["Ncube", "Zenzo Polite", ""], ["Gasela", "Naison", ""]]}, {"id": "1306.1394", "submitter": "Adam Barker", "authors": "Jonathan Stuart Ward and Adam Barker", "title": "A Cloud Computing Survey: Developments and Future Trends in\n  Infrastructure as a Service Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing is a recent paradigm based around the notion of delivery of\nresources via a service model over the Internet. Despite being a new paradigm\nof computation, cloud computing owes its origins to a number of previous\nparadigms. The term cloud computing is well defined and no longer merits\nrigorous taxonomies to furnish a definition. Instead this survey paper\nconsiders the past, present and future of cloud computing. As an evolution of\nprevious paradigms, we consider the predecessors to cloud computing and what\nsignificance they still hold to cloud services. Additionally we examine the\ntechnologies which comprise cloud computing and how the challenges and future\ndevelopments of these technologies will influence the field. Finally we examine\nthe challenges that limit the growth, application and development of cloud\ncomputing and suggest directions required to overcome these challenges in order\nto further the success of cloud computing.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2013 12:41:57 GMT"}], "update_date": "2013-06-07", "authors_parsed": [["Ward", "Jonathan Stuart", ""], ["Barker", "Adam", ""]]}, {"id": "1306.1467", "submitter": "Munther Abualkibash", "authors": "Munther Abualkibash, Ahmed ElSayed, Ausif Mahmood", "title": "Highly Scalable, Parallel and Distributed AdaBoost Algorithm using Light\n  Weight Threads and Web Services on a Network of Multi-Core Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AdaBoost is an important algorithm in machine learning and is being widely\nused in object detection. AdaBoost works by iteratively selecting the best\namongst weak classifiers, and then combines several weak classifiers to obtain\na strong classifier. Even though AdaBoost has proven to be very effective, its\nlearning execution time can be quite large depending upon the application e.g.,\nin face detection, the learning time can be several days. Due to its increasing\nuse in computer vision applications, the learning time needs to be drastically\nreduced so that an adaptive near real time object detection system can be\nincorporated. In this paper, we develop a hybrid parallel and distributed\nAdaBoost algorithm that exploits the multiple cores in a CPU via light weight\nthreads, and also uses multiple machines via a web service software\narchitecture to achieve high scalability. We present a novel hierarchical web\nservices based distributed architecture and achieve nearly linear speedup up to\nthe number of processors available to us. In comparison with the previously\npublished work, which used a single level master-slave parallel and distributed\nimplementation [1] and only achieved a speedup of 2.66 on four nodes, we\nachieve a speedup of 95.1 on 31 workstations each having a quad-core processor,\nresulting in a learning time of only 4.8 seconds per feature.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2013 16:38:26 GMT"}], "update_date": "2013-06-07", "authors_parsed": [["Abualkibash", "Munther", ""], ["ElSayed", "Ahmed", ""], ["Mahmood", "Ausif", ""]]}, {"id": "1306.1491", "submitter": "Kian Hsiang Low", "authors": "Jie Chen, Kian Hsiang Low, Colin Keng-Yan Tan", "title": "Gaussian Process-Based Decentralized Data Fusion and Active Sensing for\n  Mobility-on-Demand System", "comments": "Robotics: Science and Systems (RSS 2013), Extended version with\n  proofs, 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.DC cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobility-on-demand (MoD) systems have recently emerged as a promising\nparadigm of one-way vehicle sharing for sustainable personal urban mobility in\ndensely populated cities. In this paper, we enhance the capability of a MoD\nsystem by deploying robotic shared vehicles that can autonomously cruise the\nstreets to be hailed by users. A key challenge to managing the MoD system\neffectively is that of real-time, fine-grained mobility demand sensing and\nprediction. This paper presents a novel decentralized data fusion and active\nsensing algorithm for real-time, fine-grained mobility demand sensing and\nprediction with a fleet of autonomous robotic vehicles in a MoD system. Our\nGaussian process (GP)-based decentralized data fusion algorithm can achieve a\nfine balance between predictive power and time efficiency. We theoretically\nguarantee its predictive performance to be equivalent to that of a\nsophisticated centralized sparse approximation for the GP model: The\ncomputation of such a sparse approximate GP model can thus be distributed among\nthe MoD vehicles, hence achieving efficient and scalable demand prediction.\nThough our decentralized active sensing strategy is devised to gather the most\ninformative demand data for demand prediction, it can achieve a dual effect of\nfleet rebalancing to service the mobility demands. Empirical evaluation on\nreal-world mobility demand data shows that our proposed algorithm can achieve a\nbetter balance between predictive accuracy and time efficiency than\nstate-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jun 2013 14:05:49 GMT"}], "update_date": "2013-06-07", "authors_parsed": [["Chen", "Jie", ""], ["Low", "Kian Hsiang", ""], ["Tan", "Colin Keng-Yan", ""]]}, {"id": "1306.1618", "submitter": "Kevin Vinsen", "authors": "Kevin Vinsen and David Thilker", "title": "A BOINC based, citizen-science project for pixel Spectral Energy\n  Distribution fitting of resolved galaxies in multi-wavelength surveys", "comments": "14 pages, 16 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM astro-ph.GA cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present our experience from the first year of theSkyNet\nPan-STARRS1 Optical Galaxy Survey (POGS) project. This citizen-scientist driven\nresearch project uses the Berkeley Open Infrastructure for Network Computing\n(BOINC) middleware and thousands of Internet-connected computers to measure the\nresolved galactic structural properties of ~100,000 low redshift galaxies. We\nare combining the spectral coverage of GALEX, Pan-STARRS1, SDSS, and WISE to\ngenerate a value-added, multi-wavelength UV-optical-NIR galaxy atlas for the\nnearby Universe. Specifically, we are measuring physical parameters (such as\nlocal stellar mass, star formation rate, and first-order star formation\nhistory) on a resolved pixel-by-pixel basis using spectral energy distribution\n(SED) fitting techniques in a distributed computing mode.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2013 05:36:28 GMT"}, {"version": "v2", "created": "Fri, 20 Sep 2013 02:15:50 GMT"}, {"version": "v3", "created": "Mon, 23 Sep 2013 07:28:25 GMT"}, {"version": "v4", "created": "Mon, 30 Sep 2013 03:36:01 GMT"}, {"version": "v5", "created": "Thu, 3 Oct 2013 01:33:03 GMT"}], "update_date": "2013-10-04", "authors_parsed": [["Vinsen", "Kevin", ""], ["Thilker", "David", ""]]}, {"id": "1306.1639", "submitter": "Tarandeep Kaur", "authors": "Inderveer Chana and Tarandeep Kaur", "title": "Delivering IT as A Utility- A Systematic Review", "comments": "No. of Pages- 20 No. of Figures- 3 No. of Tables- 11", "journal-ref": "International Journal in Foundations of Computer Science &\n  Technology(IJFCST), Volume 3, No.3, May, 2013", "doi": "10.5121/ijfcst.2013.3302", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Utility Computing has facilitated the creation of new markets that has made\nit possible to realize the long held dream of delivering IT as a Utility. Even\nthough utility computing is in its nascent stage today, the proponents of\nutility computing envisage that it will become a commodity business in the\nupcoming time and utility service providers will meet all the IT requests of\nthe companies. This paper takes a cross-sectional view at the emergence of\nutility computing along with different requirements needed to realize utility\nmodel. It also surveys the current trends in utility computing highlighting\ndiverse architecture models aligned towards delivering IT as a utility.\nDifferent resource management systems for proficient allocation of resources\nhave been listed together with various resource scheduling and pricing\nstrategies used by them. Further, a review of generic key perspectives closely\nrelated to the concept of delivering IT as a Utility has been taken citing the\ncontenders for the future enhancements in this technology in the form of Grid\nand Cloud Computing.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2013 07:37:31 GMT"}], "update_date": "2013-06-10", "authors_parsed": [["Chana", "Inderveer", ""], ["Kaur", "Tarandeep", ""]]}, {"id": "1306.1692", "submitter": "Sebastian Kniesburges", "authors": "Sebastian Kniesburges, Andreas Koutsopoulos and Christian Scheideler", "title": "A DeterministicWorst-Case Message Complexity Optimal Solution for\n  Resource Discovery", "comments": "Full Version of Sirocco 2013 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of resource discovery in distributed systems. In\nparticular we give an algorithm, such that each node in a network discovers the\naddress of any other node in the network. We model the knowledge of the nodes\nas a virtual overlay network given by a directed graph such that complete\nknowledge of all nodes corresponds to a complete graph in the overlay network.\nAlthough there are several solutions for resource discovery, our solution is\nthe first that achieves worst-case optimal work for each node, i.e. the number\nof addresses (O(n)) or bits (O(n log n)) a node receives or sends coincides\nwith the lower bound, while ensuring only a linear runtime (O(n)) on the number\nof rounds.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2013 11:23:40 GMT"}], "update_date": "2013-06-10", "authors_parsed": [["Kniesburges", "Sebastian", ""], ["Koutsopoulos", "Andreas", ""], ["Scheideler", "Christian", ""]]}, {"id": "1306.1723", "submitter": "Nur Rakhmawati", "authors": "Nur Aini Rakhmawati and J\\\"urgen Umbrich and Marcel Karnstedt and Ali\n  Hasnain and Michael Hausenblas", "title": "Querying over Federated SPARQL Endpoints ---A State of the Art Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The increasing amount of Linked Data and its inherent distributed nature have\nattracted significant attention throughout the research community and amongst\npractitioners to search data, in the past years. Inspired by research results\nfrom traditional distributed databases, different approaches for managing\nfederation over SPARQL Endpoints have been introduced. SPARQL is the\nstandardised query language for RDF, the default data model used in Linked Data\ndeployments and SPARQL Endpoints are a popular access mechanism provided by\nmany Linked Open Data (LOD) repositories. In this paper, we initially give an\noverview of the federation framework infrastructure and then proceed with a\ncomparison of existing SPARQL federation frameworks. Finally, we highlight\nshortcomings in existing frameworks, which we hope helps spawning new research\ndirections.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2013 13:42:19 GMT"}], "update_date": "2013-06-10", "authors_parsed": [["Rakhmawati", "Nur Aini", ""], ["Umbrich", "J\u00fcrgen", ""], ["Karnstedt", "Marcel", ""], ["Hasnain", "Ali", ""], ["Hausenblas", "Michael", ""]]}, {"id": "1306.1769", "submitter": "Antonio Fern\\'andez Anta", "authors": "Antonio Fern\\'andez Anta and Chryssis Georgiou and Dariusz R. Kowalski\n  and Joerg Widmer and Elli Zavou", "title": "Measuring the Impact of Adversarial Errors on Packet Scheduling\n  Strategies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we explore the problem of achieving efficient packet\ntransmission over unreliable links with worst case occurrence of errors. In\nsuch a setup, even an omniscient offline scheduling strategy cannot achieve\nstability of the packet queue, nor is it able to use up all the available\nbandwidth. Hence, an important first step is to identify an appropriate metric\nfor measuring the efficiency of scheduling strategies in such a setting. To\nthis end, we propose a relative throughput metric which corresponds to the long\nterm competitive ratio of the algorithm with respect to the optimal. We then\nexplore the impact of the error detection mechanism and feedback delay on our\nmeasure. We compare instantaneous error feedback with deferred error feedback,\nthat requires a faulty packet to be fully received in order to detect the\nerror. We propose algorithms for worst-case adversarial and stochastic packet\narrival models, and formally analyze their performance. The relative throughput\nachieved by these algorithms is shown to be close to optimal by deriving lower\nbounds on the relative throughput of the algorithms and almost matching upper\nbounds for any algorithm in the considered settings. Our collection of results\ndemonstrate the potential of using instantaneous feedback to improve the\nperformance of communication systems in adverse environments.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2013 16:31:54 GMT"}], "update_date": "2013-06-10", "authors_parsed": [["Anta", "Antonio Fern\u00e1ndez", ""], ["Georgiou", "Chryssis", ""], ["Kowalski", "Dariusz R.", ""], ["Widmer", "Joerg", ""], ["Zavou", "Elli", ""]]}, {"id": "1306.1861", "submitter": "Elli Zavou", "authors": "Antonio Fern\\'andez Anta and Chryssis Georgiou and Dariusz R. Kowalski\n  and Elli Zavou", "title": "Online Parallel Scheduling of Non-uniform Tasks: Trading Failures for\n  Energy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a system in which tasks of different execution times arrive\ncontinuously and have to be executed by a set of processors that are prone to\ncrashes and restarts. In this paper we model and study the impact of\nparallelism and failures on the competitiveness of such an online system. In a\nfault-free environment, a simple Longest-in-System scheduling policy, enhanced\nby a redundancy-avoidance mechanism, guarantees optimality in a long-term\nexecution. In the presence of failures though, scheduling becomes a much more\nchallenging task. In particular, no parallel deterministic algorithm can be\ncompetitive against an offline optimal solution, even with one single processor\nand tasks of only two different execution times. We find that when additional\nenergy is provided to the system in the form of processor speedup, the\nsituation changes. Specifically, we identify thresholds on the speedup under\nwhich such competitiveness cannot be achieved by any deterministic algorithm,\nand above which competitive algorithms exist. Finally, we propose algorithms\nthat achieve small bounded competitive ratios when the speedup is over the\nthreshold.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jun 2013 00:34:57 GMT"}], "update_date": "2013-06-11", "authors_parsed": [["Anta", "Antonio Fern\u00e1ndez", ""], ["Georgiou", "Chryssis", ""], ["Kowalski", "Dariusz R.", ""], ["Zavou", "Elli", ""]]}, {"id": "1306.1888", "submitter": "Elarbi Badidi", "authors": "Elarbi Badidi", "title": "A Framework for Software-as-a-Service Selection and Provisioning", "comments": "12 pages", "journal-ref": "International Journal of Computer Networks & Communications\n  (IJCNC) Vol.5, No.3, pp. 189-200, May 2013", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As cloud computing is increasingly transforming the information technology\nlandscape, organizations and businesses are exhibiting strong interest in\nSoftware-as-a-Service (SaaS) offerings that can help them increase business\nagility and reduce their operational costs. They increasingly demand services\nthat can meet their functional and non-functional requirements. Given the\nplethora and the variety of SaaS offerings, we propose, in this paper, a\nframework for SaaS provisioning, which relies on brokered Service Level\nagreements (SLAs), between service consumers and SaaS providers. The Cloud\nService Broker (CSB) helps service consumers find the right SaaS providers that\ncan fulfil their functional and non-functional requirements. The proposed\nselection algorithm ranks potential SaaS providers by matching their offerings\nagainst the requirements of the service consumer using an aggregate utility\nfunction. Furthermore, the CSB is in charge of conducting SLA negotiation with\nselected SaaS providers, on behalf of service consumers, and performing SLA\ncompliance monitoring.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jun 2013 07:20:49 GMT"}], "update_date": "2013-06-11", "authors_parsed": [["Badidi", "Elarbi", ""]]}, {"id": "1306.1928", "submitter": "Joel Crichlow", "authors": "Joel M. Crichlow, Stephen J. Hartley and Michael Hosein", "title": "A Light-Weight Distributed System for the processing of Replicated\n  Counter-like Objects", "comments": "15 pages, 5 figures, 8 tables", "journal-ref": "International Journal of Distributed and Parallel Systems (IJDPS)\n  Vol.4, No.3, May 2013, 1 -15", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to increase availability in a distributed system some or all of the\ndata items are replicated and stored at separate sites. This is an issue of key\nconcern especially since there is such a proliferation of wireless technologies\nand mobile users. However, the concurrent processing of transactions at\nseparate sites can generate inconsistencies in the stored information. We have\nbuilt a distributed service that manages updates to widely deployed\ncounter-like replicas. There are many heavy-weight distributed systems\ntargeting large information critical applications. Our system is intentionally,\nrelatively lightweight and useful for the somewhat reduced information critical\napplications. The service is built on our distributed concurrency control\nscheme which combines optimism and pessimism in the processing of transactions.\nThe service allows a transaction to be processed immediately (optimistically)\nat any individual replica as long as the transaction satisfies a cost bound.\nAll transactions are also processed in a concurrent pessimistic manner to\nensure mutual consistency.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jun 2013 15:05:18 GMT"}], "update_date": "2013-06-11", "authors_parsed": [["Crichlow", "Joel M.", ""], ["Hartley", "Stephen J.", ""], ["Hosein", "Michael", ""]]}, {"id": "1306.2160", "submitter": "Patrizio Dazzi Ph.D.", "authors": "Ranieri Baraglia and Patrizio Dazzi and Matteo Mordacchini and Laura\n  Ricci", "title": "ATLAAS-P2P: a two layer network solution for easing the resource\n  discovery process in unstructured networks", "comments": "2 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ATLAAS-P2P is a two-layered P2P architecture for developing systems providing\nresource aggregation and approximated discovery in P2P networks. Such systems\nallow users to search the desired resources by specifying their requirements in\na flexible and easy way. From the point of view of resource providers, this\nsystem makes available an effective solution supporting providers in being\nreached by resource requests.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2013 10:37:56 GMT"}], "update_date": "2013-06-11", "authors_parsed": [["Baraglia", "Ranieri", ""], ["Dazzi", "Patrizio", ""], ["Mordacchini", "Matteo", ""], ["Ricci", "Laura", ""]]}, {"id": "1306.2258", "submitter": "Konstantin Berlin", "authors": "Konstantin Berlin, Nail A. Gumerov, Ramani Duraiswami, David Fushman", "title": "Performance of a GPU-based Direct Summation Algorithm for Computation of\n  Small Angle Scattering Profile", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Small Angle Scattering (SAS) of X-rays or neutrons is an experimental\ntechnique that provides valuable structural information for biological\nmacromolecules under physiological conditions and with no limitation on the\nmolecular size. In order to refine molecular structure against experimental SAS\ndata, ab initio prediction of the scattering profile must be recomputed\nhundreds of thousands of times, which involves the computation of the sinc\nkernel over all pairs of atoms in a molecule. The quadratic computational\ncomplexity of predicting the SAS profile limits the size of the molecules and\nand has been a major impediment for integration of SAS data into structure\nrefinement protocols. In order to significantly speed up prediction of the SAS\nprofile we present a general purpose graphical processing unit (GPU) algorithm,\nwritten in OpenCL, for the summation of the sinc kernel (Debye summation) over\nall pairs of atoms. This program is an order of magnitude faster than a\nparallel CPU algorithm, and faster than an FMM-like approximation method for\ncertain input domains. We show that our algorithm is currently the fastest\nmethod for performing SAS computation for small and medium size molecules\n(around 50000 atoms or less). This algorithm is critical for quick and accurate\nSAS profile computation of elongated structures, such as DNA, RNA, and sparsely\nspaced pseudo-atom molecules.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2013 17:44:17 GMT"}], "update_date": "2013-06-11", "authors_parsed": [["Berlin", "Konstantin", ""], ["Gumerov", "Nail A.", ""], ["Duraiswami", "Ramani", ""], ["Fushman", "David", ""]]}, {"id": "1306.2267", "submitter": "Patrizio Dazzi", "authors": "Patrizio Dazzi", "title": "Let's Annotate to Let Our Code Run in Parallel", "comments": "4 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an approach that exploits Java annotations to provide\nmeta information needed to automatically transform plain Java programs into\nparallel code that can be run on multicore workstation. Programmers just need\nto decorate the methods that will eventually be executed in parallel with\nstandard Java annotations. Annotations are automatically processed at\nlaunch-time and parallel byte code is derived. Once in execution the program\nautomatically retrieves the information about the executing platform and\nevaluates the information specified inside the annotations to transform the\nbyte-code into a semantically equivalent multithreaded version, depending on\nthe target architecture features. The results returned by the annotated\nmethods, when invoked, are futures with a wait-by-necessity semantics.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2013 13:07:30 GMT"}], "update_date": "2013-06-11", "authors_parsed": [["Dazzi", "Patrizio", ""]]}, {"id": "1306.2453", "submitter": "Dibakar Saha", "authors": "Dibakar Saha, Nabanita Das", "title": "A Fast Fault Tolerant Partitioning Algorithm for Wireless Sensor\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, given a random uniform distribution of sensor nodes on a 2-D\nplane, a fast self-organized distributed algorithm is proposed to find the\nmaximum number of partitions of the nodes such that each partition is connected\nand covers the area to be monitored. Each connected partition remains active in\na round robin fashion to cover the query region individually. In case of a node\nfailure, the proposed distributed fault recovery algorithm reconstructs the\naffected partition locally utilizing the available free nodes. Simulation\nstudies show significant improvement in performance compared to the earlier\nworks in terms of computation time, the diameter of each partition, message\noverhead and network lifetime.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2013 08:41:27 GMT"}], "update_date": "2013-06-12", "authors_parsed": [["Saha", "Dibakar", ""], ["Das", "Nabanita", ""]]}, {"id": "1306.2552", "submitter": "Francesco Silvestri", "authors": "Andrea Pietracaprina and Geppino Pucci and Francesco Silvestri and\n  Fabio Vandin", "title": "Space-Efficient Parallel Algorithms for Combinatorial Search Problems", "comments": "Extended version of the paper in the Proc. of 38th International\n  Symposium on Mathematical Foundations of Computer Science (MFCS)", "journal-ref": null, "doi": "10.4230/LIPIcs.STACS.2014.627", "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present space-efficient parallel strategies for two fundamental\ncombinatorial search problems, namely, backtrack search and branch-and-bound,\nboth involving the visit of an $n$-node tree of height $h$ under the assumption\nthat a node can be accessed only through its father or its children. For both\nproblems we propose efficient algorithms that run on a $p$-processor\ndistributed-memory machine. For backtrack search, we give a deterministic\nalgorithm running in $O(n/p+h\\log p)$ time, and a Las Vegas algorithm requiring\noptimal $O(n/p+h)$ time, with high probability. Building on the backtrack\nsearch algorithm, we also derive a Las Vegas algorithm for branch-and-bound\nwhich runs in $O((n/p+h\\log p \\log n)h\\log^2 n)$ time, with high probability. A\nremarkable feature of our algorithms is the use of only constant space per\nprocessor, which constitutes a significant improvement upon previous algorithms\nwhose space requirements per processor depend on the (possibly huge) tree to be\nexplored.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2013 15:29:17 GMT"}, {"version": "v2", "created": "Wed, 26 Mar 2014 13:17:39 GMT"}], "update_date": "2014-03-27", "authors_parsed": [["Pietracaprina", "Andrea", ""], ["Pucci", "Geppino", ""], ["Silvestri", "Francesco", ""], ["Vandin", "Fabio", ""]]}, {"id": "1306.2743", "submitter": "Raphael kena Poss", "authors": "Raphael Poss and Merijn Verstraaten and Frank Penczek and Clemens\n  Grelck and Raimund Kirner and Alex Shafarenko", "title": "S+Net: extending functional coordination with extra-functional semantics", "comments": "34 pages, 11 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical report introduces S+Net, a compositional coordination language\nfor streaming networks with extra-functional semantics. Compositionality\nsimplifies the specification of complex parallel and distributed applications;\nextra-functional semantics allow the application designer to reason about and\ncontrol resource usage, performance and fault handling. The key feature of\nS+Net is that functional and extra-functional semantics are defined\northogonally from each other. S+Net can be seen as a simultaneous\nsimplification and extension of the existing coordination language S-Net, that\ngives control of extra-functional behavior to the S-Net programmer. S+Net can\nalso be seen as a transitional research step between S-Net and AstraKahn,\nanother coordination language currently being designed at the University of\nHertfordshire. In contrast with AstraKahn which constitutes a re-design from\nthe ground up, S+Net preserves the basic operational semantics of S-Net and\nthus provides an incremental introduction of extra-functional control in an\nexisting language.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2013 08:25:51 GMT"}], "update_date": "2013-06-13", "authors_parsed": [["Poss", "Raphael", ""], ["Verstraaten", "Merijn", ""], ["Penczek", "Frank", ""], ["Grelck", "Clemens", ""], ["Kirner", "Raimund", ""], ["Shafarenko", "Alex", ""]]}, {"id": "1306.3295", "submitter": "Jeff M Phillips", "authors": "Mary Hall, Robert M. Kirby, Feifei Li, Miriah Meyer, Valerio Pascucci,\n  Jeff M. Phillips, Rob Ricci, Jacobus Van der Merwe, Suresh Venkatasubramanian", "title": "Rethinking Abstractions for Big Data: Why, Where, How, and What", "comments": "8 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": "UUCS-13-002", "categories": "cs.GL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data refers to large and complex data sets that, under existing\napproaches, exceed the capacity and capability of current compute platforms,\nsystems software, analytical tools and human understanding. Numerous lessons on\nthe scalability of big data can already be found in asymptotic analysis of\nalgorithms and from the high-performance computing (HPC) and applications\ncommunities. However, scale is only one aspect of current big data trends;\nfundamentally, current and emerging problems in big data are a result of\nunprecedented complexity--in the structure of the data and how to analyze it,\nin dealing with unreliability and redundancy, in addressing the human factors\nof comprehending complex data sets, in formulating meaningful analyses, and in\nmanaging the dense, power-hungry data centers that house big data.\n  The computer science solution to complexity is finding the right\nabstractions, those that hide as much triviality as possible while revealing\nthe essence of the problem that is being addressed. The \"big data challenge\"\nhas disrupted computer science by stressing to the very limits the familiar\nabstractions which define the relevant subfields in data analysis, data\nmanagement and the underlying parallel systems. As a result, not enough of\nthese challenges are revealed by isolating abstractions in a traditional\nsoftware stack or standard algorithmic and analytical techniques, and attempts\nto address complexity either oversimplify or require low-level management of\ndetails. The authors believe that the abstractions for big data need to be\nrethought, and this reorganization needs to evolve and be sustained through\ncontinued cross-disciplinary collaboration.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2013 05:11:34 GMT"}], "update_date": "2013-06-17", "authors_parsed": [["Hall", "Mary", ""], ["Kirby", "Robert M.", ""], ["Li", "Feifei", ""], ["Meyer", "Miriah", ""], ["Pascucci", "Valerio", ""], ["Phillips", "Jeff M.", ""], ["Ricci", "Rob", ""], ["Van der Merwe", "Jacobus", ""], ["Venkatasubramanian", "Suresh", ""]]}, {"id": "1306.3534", "submitter": "Ashish Vulimiri", "authors": "Ashish Vulimiri, P. Brighten Godfrey, Sri Varsha Gorge, Zitian Liu and\n  Scott Shenker", "title": "A cost-benefit analysis of low latency via added utilization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several recently proposed techniques achieve latency reduction by trading it\noff for some amount of additional bandwidth usage. But how would one quantify\nwhether the tradeoff is actually beneficial in a given system? We develop an\neconomic cost vs. benefit analysis for answering this question. We use the\nanalysis to derive a benchmark for wide-area client-server applications, and\ndemonstrate how it can be applied to reason about a particular latency saving\ntechnique --- redundant DNS requests.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2013 00:31:56 GMT"}, {"version": "v2", "created": "Mon, 10 Nov 2014 16:56:41 GMT"}, {"version": "v3", "created": "Fri, 14 Nov 2014 12:38:20 GMT"}, {"version": "v4", "created": "Thu, 4 Dec 2014 20:53:48 GMT"}], "update_date": "2014-12-05", "authors_parsed": [["Vulimiri", "Ashish", ""], ["Godfrey", "P. Brighten", ""], ["Gorge", "Sri Varsha", ""], ["Liu", "Zitian", ""], ["Shenker", "Scott", ""]]}, {"id": "1306.3543", "submitter": "Joshua Vogelstein", "authors": "Randal Burns, William Gray Roncal, Dean Kleissas, Kunal Lillaney,\n  Priya Manavalan, Eric Perlman, Daniel R. Berger, Davi D. Bock, Kwanghun\n  Chung, Logan Grosenick, Narayanan Kasthuri, Nicholas C. Weiler, Karl\n  Deisseroth, Michael Kazhdan, Jeff Lichtman, R. Clay Reid, Stephen J. Smith,\n  Alexander S. Szalay, Joshua T. Vogelstein, R. Jacob Vogelstein", "title": "The Open Connectome Project Data Cluster: Scalable Analysis and Vision\n  for High-Throughput Neuroscience", "comments": "11 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a scalable database cluster for the spatial analysis and\nannotation of high-throughput brain imaging data, initially for 3-d electron\nmicroscopy image stacks, but for time-series and multi-channel data as well.\nThe system was designed primarily for workloads that build connectomes---neural\nconnectivity maps of the brain---using the parallel execution of computer\nvision algorithms on high-performance compute clusters. These services and\nopen-science data sets are publicly available at http://openconnecto.me.\n  The system design inherits much from NoSQL scale-out and data-intensive\ncomputing architectures. We distribute data to cluster nodes by partitioning a\nspatial index. We direct I/O to different systems---reads to parallel disk\narrays and writes to solid-state storage---to avoid I/O interference and\nmaximize throughput. All programming interfaces are RESTful Web services, which\nare simple and stateless, improving scalability and usability. We include a\nperformance evaluation of the production system, highlighting the effectiveness\nof spatial data organization.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2013 03:19:51 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2013 15:58:27 GMT"}], "update_date": "2013-06-19", "authors_parsed": [["Burns", "Randal", ""], ["Roncal", "William Gray", ""], ["Kleissas", "Dean", ""], ["Lillaney", "Kunal", ""], ["Manavalan", "Priya", ""], ["Perlman", "Eric", ""], ["Berger", "Daniel R.", ""], ["Bock", "Davi D.", ""], ["Chung", "Kwanghun", ""], ["Grosenick", "Logan", ""], ["Kasthuri", "Narayanan", ""], ["Weiler", "Nicholas C.", ""], ["Deisseroth", "Karl", ""], ["Kazhdan", "Michael", ""], ["Lichtman", "Jeff", ""], ["Reid", "R. Clay", ""], ["Smith", "Stephen J.", ""], ["Szalay", "Alexander S.", ""], ["Vogelstein", "Joshua T.", ""], ["Vogelstein", "R. Jacob", ""]]}, {"id": "1306.3624", "submitter": "Rajiv Ranjan Dr.", "authors": "Shilin Lu, Rajiv Ranjan, Peter Strazdins", "title": "Reporting an Experience on Design and Implementation of e-Health Systems\n  on Azure Cloud", "comments": "Submitted to third IEEE International Conference on Cloud and Green\n  Computing (CGC 2013)", "journal-ref": null, "doi": "10.1007/978-3-642-41428-2_12", "report-no": null, "categories": "cs.CY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic Health (e-Health) technology has brought the world with\nsignificant transformation from traditional paper-based medical practice to\nInformation and Communication Technologies (ICT)-based systems for automatic\nmanagement (storage, processing, and archiving) of information. Traditionally\ne-Health systems have been designed to operate within stovepipes on dedicated\nnetworks, physical computers, and locally managed software platforms that make\nit susceptible to many serious limitations including: 1) lack of on-demand\nscalability during critical situations; 2) high administrative overheads and\ncosts; and 3) in-efficient resource utilization and energy consumption due to\nlack of automation. In this paper, we present an approach to migrate the ICT\nsystems in the e-Health sector from traditional in-house Client/Server (C/S)\narchitecture to the virtualised cloud computing environment. To this end, we\ndeveloped two cloud-based e-Health applications (Medical Practice Management\nSystem and Telemedicine Practice System) for demonstrating how cloud services\ncan be leveraged for developing and deploying such applications. The Windows\nAzure cloud computing platform is selected as an example public cloud platform\nfor our study. We conducted several performance evaluation experiments to\nunderstand the Quality Service (QoS) tradeoffs of our applications under\nvariable workload on Azure.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jun 2013 04:07:57 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Lu", "Shilin", ""], ["Ranjan", "Rajiv", ""], ["Strazdins", "Peter", ""]]}, {"id": "1306.3906", "submitter": "Masoud Saeida Ardekani", "authors": "Masoud Saeida Ardekani (LIP6, INRIA Rocquencourt), Pierre Sutra\n  (IIUN), Nuno Pregui\\c{c}a (CITI), Marc Shapiro (LIP6, INRIA Rocquencourt)", "title": "Non-Monotonic Snapshot Isolation", "comments": null, "journal-ref": "N&deg; RR-7805 (2013)", "doi": null, "report-no": "RR-7805", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many distributed applications require transactions. However, transactional\nprotocols that require strong synchronization are costly in large scale\nenvironments. Two properties help with scalability of a transactional system:\ngenuine partial replication (GPR), which leverages the intrinsic parallelism of\na workload, and snapshot isolation (SI), which decreases the need for\nsynchronization. We show that, under standard assumptions (data store accesses\nare not known in advance, and transactions may access arbitrary objects in the\ndata store), it is impossible to have both SI and GPR. To circumvent this\nimpossibility, we propose a weaker consistency criterion, called Non-monotonic\nSnapshot Isolation (NMSI). NMSI retains the most important properties of SI,\ni.e., read-only transactions always commit, and two write-conflicting updates\ndo not both commit. We present a GPR protocol that ensures NMSI, and has lower\nmessage cost (i.e., it contacts fewer replicas and/or commits faster) than\nprevious approaches.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2013 15:45:13 GMT"}], "update_date": "2013-06-18", "authors_parsed": [["Ardekani", "Masoud Saeida", "", "LIP6, INRIA Rocquencourt"], ["Sutra", "Pierre", "", "IIUN"], ["Pregui\u00e7a", "Nuno", "", "CITI"], ["Shapiro", "Marc", "", "LIP6, INRIA Rocquencourt"]]}, {"id": "1306.4161", "submitter": "Alexey Lastovetsky", "authors": "Jean-Noel Quintin, Khalid Hasanov, Alexey Lastovetsky", "title": "Hierarchical Parallel Matrix Multiplication on Large-Scale Distributed\n  Memory Platforms", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix multiplication is a very important computation kernel both in its own\nright as a building block of many scientific applications and as a popular\nrepresentative for other scientific applications. Cannon algorithm which dates\nback to 1969 was the first efficient algorithm for parallel matrix\nmultiplication providing theoretically optimal communication cost. However this\nalgorithm requires a square number of processors. In the mid 1990s, the SUMMA\nalgorithm was introduced. SUMMA overcomes the shortcomings of Cannon algorithm\nas it can be used on a non-square number of processors as well. Since then the\nnumber of processors in HPC platforms has increased by two orders of magnitude\nmaking the contribution of communication in the overall execution time more\nsignificant. Therefore, the state of the art parallel matrix multiplication\nalgorithms should be revisited to reduce the communication cost further. This\npaper introduces a new parallel matrix multiplication algorithm, Hierarchical\nSUMMA (HSUMMA), which is a redesign of SUMMA. Our algorithm reduces the\ncommunication cost of SUMMA by introducing a two-level virtual hierarchy into\nthe two-dimensional arrangement of processors. Experiments on an IBM BlueGene-P\ndemonstrate the reduction of communication cost up to 2.08 times on 2048 cores\nand up to 5.89 times on 16384 cores.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2013 12:17:36 GMT"}], "update_date": "2013-06-19", "authors_parsed": [["Quintin", "Jean-Noel", ""], ["Hasanov", "Khalid", ""], ["Lastovetsky", "Alexey", ""]]}, {"id": "1306.4242", "submitter": "Xavier Urbain", "authors": "C\\'edric Auger, Zohir Bouzid (LIP6), Pierre Courtieu (CEDRIC),\n  S\\'ebastien Tixeuil (LIP6, LINCS, IUF), Xavier Urbain (CEDRIC, LRI)", "title": "Certified Impossibility Results for Byzantine-Tolerant Mobile Robots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework to build formal developments for robot networks using\nthe COQ proof assistant, to state and to prove formally various properties. We\nfocus in this paper on impossibility proofs, as it is natural to take advantage\nof the COQ higher order calculus to reason about algorithms as abstract\nobjects. We present in particular formal proofs of two impossibility results\nforconvergence of oblivious mobile robots if respectively more than one half\nand more than one third of the robots exhibit Byzantine failures, starting from\nthe original theorems by Bouzid et al.. Thanks to our formalization, the\ncorresponding COQ developments are quite compact. To our knowledge, these are\nthe first certified (in the sense of formally proved) impossibility results for\nrobot networks.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2013 15:26:11 GMT"}], "update_date": "2013-06-19", "authors_parsed": [["Auger", "C\u00e9dric", "", "LIP6"], ["Bouzid", "Zohir", "", "LIP6"], ["Courtieu", "Pierre", "", "CEDRIC"], ["Tixeuil", "S\u00e9bastien", "", "LIP6, LINCS, IUF"], ["Urbain", "Xavier", "", "CEDRIC, LRI"]]}, {"id": "1306.4521", "submitter": "Deepak Ajwani", "authors": "Deepak Ajwani and Nodari Sitchinava", "title": "Empirical Evaluation of the Parallel Distribution Sweeping Framework on\n  Multicore Architectures", "comments": "Longer version of ESA'13 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we perform an empirical evaluation of the Parallel External\nMemory (PEM) model in the context of geometric problems. In particular, we\nimplement the parallel distribution sweeping framework of Ajwani, Sitchinava\nand Zeh to solve batched 1-dimensional stabbing max problem. While modern\nprocessors consist of sophisticated memory systems (multiple levels of caches,\nset associativity, TLB, prefetching), we empirically show that algorithms\ndesigned in simple models, that focus on minimizing the I/O transfers between\nshared memory and single level cache, can lead to efficient software on current\nmulticore architectures. Our implementation exhibits significantly fewer\naccesses to slow DRAM and, therefore, outperforms traditional approaches based\non plane sweep and two-way divide and conquer.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2013 12:41:37 GMT"}], "update_date": "2013-06-20", "authors_parsed": [["Ajwani", "Deepak", ""], ["Sitchinava", "Nodari", ""]]}, {"id": "1306.4956", "submitter": "Saeid Abolfazli", "authors": "Saeid Abolfazli, Zohreh Sanaei, Ejaz Ahmed, Abdullah Gani, Rajkumar\n  Buyya", "title": "Cloud-Based Augmentation for Mobile Devices: Motivation, Taxonomies, and\n  Open Challenges", "comments": "Accepted for Publication in IEEE Communications Surveys & Tutorials", "journal-ref": "IEEE Communications Surveys & Tutorials (2013) 1-32", "doi": "10.1109/SURV.2013.070813.00285", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Cloud-based Mobile Augmentation (CMA) approaches have gained\nremarkable ground from academia and industry. CMA is the state-of-the-art\nmobile augmentation model that employs resource-rich clouds to increase,\nenhance, and optimize computing capabilities of mobile devices aiming at\nexecution of resource-intensive mobile applications. Augmented mobile devices\nenvision to perform extensive computations and to store big data beyond their\nintrinsic capabilities with least footprint and vulnerability. Researchers\nutilize varied cloud-based computing resources (e.g., distant clouds and nearby\nmobile nodes) to meet various computing requirements of mobile users. However,\nemploying cloud-based computing resources is not a straightforward panacea.\nComprehending critical factors that impact on augmentation process and optimum\nselection of cloud-based resource types are some challenges that hinder CMA\nadaptability. This paper comprehensively surveys the mobile augmentation domain\nand presents taxonomy of CMA approaches. The objectives of this study is to\nhighlight the effects of remote resources on the quality and reliability of\naugmentation processes and discuss the challenges and opportunities of\nemploying varied cloud-based resources in augmenting mobile devices. We present\naugmentation definition, motivation, and taxonomy of augmentation types,\nincluding traditional and cloud-based. We critically analyze the\nstate-of-the-art CMA approaches and classify them into four groups of distant\nfixed, proximate fixed, proximate mobile, and hybrid to present a taxonomy.\nVital decision making and performance limitation factors that influence on the\nadoption of CMA approaches are introduced and an exemplary decision making\nflowchart for future CMA approaches are presented. Impacts of CMA approaches on\nmobile computing is discussed and open challenges are presented as the future\nresearch directions.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2013 18:41:00 GMT"}], "update_date": "2013-08-20", "authors_parsed": [["Abolfazli", "Saeid", ""], ["Sanaei", "Zohreh", ""], ["Ahmed", "Ejaz", ""], ["Gani", "Abdullah", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "1306.5076", "submitter": "Volker Weichert", "authors": "Nodari Sitchinava, Volker Weichert", "title": "Bank Conflict Free Comparison-based Sorting On GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a framework for designing algorithms in shared\nmemory of GPUs without incurring memory bank conflicts. Using our framework we\ndevelop the first comparison-based shared memory sorting algorithm that incurs\nno bank conflicts. It can be used as a subroutine for GPU sorting algorithms to\nreplace current use of sorting networks in shared memory. Using our bank\nconflict free shared memory sorting subroutine as a black box, we design\nBCFMergesort, an algorithm for merging sorted streams of data that are larger\nthan shared memory. Our algorithm performs all accesses to global memory in\ncoalesced manner and incurs no bank conflicts during the merge.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2013 08:57:39 GMT"}, {"version": "v2", "created": "Tue, 11 Oct 2016 05:32:00 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Sitchinava", "Nodari", ""], ["Weichert", "Volker", ""]]}, {"id": "1306.5390", "submitter": "Tejaswi Agarwal", "authors": "Tejaswi Agarwal, Saurabh Jha and B. Rajesh Kanna", "title": "P-HGRMS: A Parallel Hypergraph Based Root Mean Square Algorithm for\n  Image Denoising", "comments": "2 pages, 2 figures. Published as poster at the 22nd ACM International\n  Symposium on High Performance Parallel and Distributed Systems, HPDC 2013,\n  New York, USA. Won the Best Poster Award at HPDC 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a parallel Salt and Pepper (SP) noise removal algorithm\nin a grey level digital image based on the Hypergraph Based Root Mean Square\n(HGRMS) approach. HGRMS is generic algorithm for identifying noisy pixels in\nany digital image using a two level hierarchical serial approach. However, for\nSP noise removal, we reduce this algorithm to a parallel model by introducing a\ncardinality matrix and an iteration factor, k, which helps us reduce the\ndependencies in the existing approach. We also observe that the performance of\nthe serial implementation is better on smaller images, but once the threshold\nis achieved in terms of image resolution, its computational complexity\nincreases drastically. We test P-HGRMS using standard images from the Berkeley\nSegmentation dataset on NVIDIAs Compute Unified Device Architecture (CUDA) for\nnoise identification and attenuation. We also compare the noise removal\nefficiency of the proposed algorithm using Peak Signal to Noise Ratio (PSNR) to\nthe existing approach. P-HGRMS maintains the noise removal efficiency and\noutperforms its sequential counterpart by 6 to 18 times (6x - 18x) in\ncomputational efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jun 2013 09:36:08 GMT"}, {"version": "v2", "created": "Sat, 29 Jun 2013 01:32:41 GMT"}], "update_date": "2013-07-02", "authors_parsed": [["Agarwal", "Tejaswi", ""], ["Jha", "Saurabh", ""], ["Kanna", "B. Rajesh", ""]]}, {"id": "1306.5530", "submitter": "Yong Wang", "authors": "Yong Wang", "title": "A Formal Model of QoS-Aware Web Service Orchestration Engine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  QoS-aware applications can satisfy not only the functional requirements of\nthe customers, but also the QoS requirements. QoS-aware Web Service\norchestration translates the QoS requirements of the customers into those of\nits component Web Services. In a system viewpoint, we discuss issues on\nQoS-aware Web Service orchestration and design a typical QoS-aware Web Service\norchestration engine called QoS-WSOE. More importantly, we establish a formal\nmodel of QoS-WSOE based on actor systems theory. Within the formal model, we\nuse a three-layered pyramidal structure to capture the requirements of the\ncustomers with a concept named QoS-Aware WSO Service, characteristics of\nQoS-WSOE with a concept named QoS-Aware WSO System, and structures and\nbehaviors of QoS-WSOE with a concept named QoS-Aware WSO Behavior. Conclusions\nshowing that a system with QoS-Aware WSO Behavior is a QoS-Aware WSO System and\nfurther can provide QoS-Aware WSO Service are drawn.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2013 07:40:03 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2013 01:58:43 GMT"}], "update_date": "2013-12-04", "authors_parsed": [["Wang", "Yong", ""]]}, {"id": "1306.5535", "submitter": "Yong Wang", "authors": "Yong Wang", "title": "A Survey on Formal Methods for Web Service Composition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web Service Composition creates new composite Web Services from existing Web\nServices which embodies the added values of Web Service technology and is a key\ntechnology to solve cross-organizational business process integrations. We do a\nsurvey on formal methods for Web Service Composition in the following way.\nThrough analyses of Web Service Composition, we establish a reference model\ncalled RM-WSComposition to capture elements of Web Service Composition. Based\non the RM-WSComposition, issues on formalization for Web Service Composition\nare pointed out and state-of-the-art on formal methods for Web Service\nComposition is introduced. Finally, we point out the trends on this topic. For\nconvenience, we use an example called BuyingBooks to illustrate the concepts\nand mechanisms in Web Service Composition.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2013 08:05:44 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2013 01:41:33 GMT"}], "update_date": "2013-12-04", "authors_parsed": [["Wang", "Yong", ""]]}, {"id": "1306.5586", "submitter": "Robert Primmer", "authors": "Robert Primmer, Scott Nyman, Wayzen Lin", "title": "Creating a Relational Distributed Object Store", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In and of itself, data storage has apparent business utility. But when we can\nconvert data to information, the utility of stored data increases dramatically.\nIt is the layering of relation atop the data mass that is the engine for such\nconversion. Frank relation amongst discrete objects sporadically ingested is\nrare, making the process of synthesizing such relation all the more\nchallenging, but the challenge must be met if we are ever to see an equivalent\nbusiness value for unstructured data as we already have with structured data.\nThis paper describes a novel construct, referred to as a relational distributed\nobject store (RDOS), that seeks to solve the twin problems of how to\npersistently and reliably store petabytes of unstructured data while\nsimultaneously creating and persisting relations amongst billions of objects.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2013 12:00:10 GMT"}], "update_date": "2013-06-25", "authors_parsed": [["Primmer", "Robert", ""], ["Nyman", "Scott", ""], ["Lin", "Wayzen", ""]]}, {"id": "1306.5782", "submitter": "Patrizio Dazzi Ph.D.", "authors": "Patrizio Dazzi", "title": "A Tool for Programming Embarrassingly Task Parallel Applications on CoW\n  and NoW", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embarrassingly parallel problems can be split in parts that are characterized\nby a really low (or sometime absent) exchange of information during their\ncomputation in parallel. As a consequence they can be effectively computed in\nparallel exploiting commodity hardware, hence without particularly\nsophisticated interconnection networks. Basically, this means Clusters,\nNetworks of Workstations and Desktops as well as Computational Clouds. Despite\nthe simplicity of this computational model, it can be exploited to compute a\nquite large range of problems. This paper describes JJPF, a tool for developing\ntask parallel applications based on Java and Jini that showed to be an\neffective and efficient solution in environment like Clusters and Networks of\nWorkstations and Desktops.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2013 21:05:44 GMT"}], "update_date": "2013-06-26", "authors_parsed": [["Dazzi", "Patrizio", ""]]}, {"id": "1306.5858", "submitter": "Raz Nissim", "authors": "Raz Nissim and Ronen Brafman", "title": "Distributed Heuristic Forward Search for Multi-Agent Systems", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a number of distributed forward search algorithms for\nsolving multi-agent planning problems. We introduce a distributed formulation\nof non-optimal forward search, as well as an optimal version, MAD-A*. Our\nalgorithms exploit the structure of multi-agent problems to not only distribute\nthe work efficiently among different agents, but also to remove symmetries and\nreduce the overall workload. The algorithms ensure that private information is\nnot shared among agents, yet computation is still efficient -- outperforming\ncurrent state-of-the-art distributed planners, and in some cases even\ncentralized search -- despite the fact that each agent has access only to\npartial information.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2013 06:58:31 GMT"}], "update_date": "2013-06-26", "authors_parsed": [["Nissim", "Raz", ""], ["Brafman", "Ronen", ""]]}, {"id": "1306.6023", "submitter": "Matteo Dell'Amico Ph.D.", "authors": "Matteo Dell'Amico", "title": "A Simulator for Data-Intensive Job Scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": "EURECOM RR-13-282", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the fact that size-based schedulers can give excellent results in\nterms of both average response times and fairness, data-intensive computing\nexecution engines generally do not employ size-based schedulers, mainly because\nof the fact that job size is not known a priori.\n  In this work, we perform a simulation-based analysis of the performance of\nsize-based schedulers when they are employed with the workload of typical\ndata-intensive schedules and with approximated size estimations. We show\nresults that are very promising: even when size estimation is very imprecise,\nresponse times of size-based schedulers can be definitely smaller than those of\nsimple scheduling techniques such as processor sharing or FIFO.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2013 16:34:05 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2013 14:25:23 GMT"}], "update_date": "2013-08-22", "authors_parsed": [["Dell'Amico", "Matteo", ""]]}, {"id": "1306.6116", "submitter": "Sivaraman Dasarathan", "authors": "Sivaraman Dasarathan and Cihan Tepedelenlioglu", "title": "Distributed Estimation and Detection with Bounded Transmissions over\n  Gaussian Multiple Access Channels", "comments": "24 Pages, 7 Figures, Will be submitted to an IEEE journal", "journal-ref": null, "doi": "10.1109/TSP.2014.2327573", "report-no": null, "categories": "cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A distributed inference scheme which uses bounded transmission functions over\na Gaussian multiple access channel is considered. When the sensor measurements\nare decreasingly reliable as a function of the sensor index, the conditions on\nthe transmission functions under which consistent estimation and reliable\ndetection are possible is characterized. For the distributed estimation\nproblem, an estimation scheme that uses bounded transmission functions is\nproved to be strongly consistent provided that the variance of the noise\nsamples are bounded and that the transmission function is one-to-one. The\nproposed estimation scheme is compared with the amplify-and-forward technique\nand its robustness to impulsive sensing noise distributions is highlighted. In\ncontrast to amplify-and-forward schemes, it is also shown that bounded\ntransmissions suffer from inconsistent estimates if the sensing noise variance\ngoes to infinity. For the distributed detection problem, similar results are\nobtained by studying the deflection coefficient. Simulations corroborate our\nanalytical results.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2013 01:58:09 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Dasarathan", "Sivaraman", ""], ["Tepedelenlioglu", "Cihan", ""]]}, {"id": "1306.6192", "submitter": "Lukasz Swierczewski", "authors": "Lukasz Swierczewski", "title": "Akceleracja obliczen algebry liniowej z wykorzystaniem masywnie\n  rownoleglych, wielordzeniowych procesorow GPU", "comments": "10 pages in polish", "journal-ref": "Prace Naukowe Studentow 2; Wyzwania XXI Wieku, Przyroda, Technika,\n  Czlowiek; Materialy z III Ogolnopolskiej Sesji Kol Naukowych, 2012", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents the aspect of use of modern graphics accelerators\nsupporting CUDA technology for high-performance computing in the field of\nlinear algebra. Fully programmable graphic cards have been available for\nseveral years for both ordinary users and research units. They provide the\ncapability of performing virtually any computing with high performance, which\nis often beyond the reach of conventional CPUs. GPU architecture, also in case\nof classical problems of linear algebra which is the basis for many\ncalculations, can bring many benefits to the developer. Performance increase,\nobserved during matrix multiplication on nVidia Tesla C2050, was more than\nthousandfold compared to ordinary CPU, resulting in drastic reduction of\nlatency for some of the results, thus the cost of obtaining them.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2013 10:14:37 GMT"}], "update_date": "2013-06-27", "authors_parsed": [["Swierczewski", "Lukasz", ""]]}, {"id": "1306.6397", "submitter": "Ankit Mundra", "authors": "Ankit Mundra, Nitin Rakesh, Vipin Tyagi", "title": "Query Centric CPS (QCPS) Approach for Multiple Heterogeneous Systems", "comments": "5 pages, 5 Figures. In International Journal of Computer Science And\n  Technology 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modern scenario we need to have mechanisms which can provide better\ninteraction with physical world by an efficient and more effective\ncommunication and computation approach for multiple heterogeneous sensor\nnetworks. Previous work provides efficient communication approach between\nsensor nodes and a query centric approach for multiple collaborative\nheterogeneous sensor networks. Even there is energy issues involved in wireless\nsensor network operation. In this paper we have proposed Query centric Cyber\nPhysical System (QCPS)model to implement query centric user request using Cyber\nPhysical System (CPS). CPS takes both communication and computation in parallel\nto provide better interaction with physical world. This feature of CPS reduces\nsystem cost and makes it more energy efficient. This paper provides an\nefficient query processing approach for multiple heterogeneous sensor networks\nusing cyber physical system.This approach results in reduction of communication\nand computation cost as sensor network communicates using centroid of\nrespective grids which reduces cost of communication while involvement of CPS\nreduces the computation cost.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2013 03:13:09 GMT"}], "update_date": "2013-06-28", "authors_parsed": [["Mundra", "Ankit", ""], ["Rakesh", "Nitin", ""], ["Tyagi", "Vipin", ""]]}, {"id": "1306.6410", "submitter": "Chi Zhou", "authors": "Amelie Chi Zhou, Bingsheng He, Cheng Liu", "title": "Monetary Cost Optimizations for Hosting Workflow-as-a-Service in IaaS\n  Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": "Technical Report 2013-12", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, we have witnessed workflows from science and other data-intensive\napplications emerging on Infrastructure-asa-Service (IaaS) clouds, and many\nworkflow service providers offering workflow as a service (WaaS). The major\nconcern of WaaS providers is to minimize the monetary cost of executing\nworkflows in the IaaS cloud. While there have been previous studies on this\nconcern, most of them assume static task execution time and static pricing\nscheme, and have the QoS notion of satisfying a deterministic deadline.\nHowever, cloud environment is dynamic, with performance dynamics caused by the\ninterference from concurrent executions and price dynamics like spot prices\noffered by Amazon EC2. Therefore, we argue that WaaS providers should have the\nnotion of offering probabilistic performance guarantees for individual\nworkflows on IaaS clouds. We develop a probabilistic scheduling framework\ncalled Dyna to minimize the monetary cost while offering probabilistic deadline\nguarantees. The framework includes an A*-based instance configuration method\nfor performance dynamics, and a hybrid instance configuration refinement for\nutilizing spot instances. Experimental results with three real-world scientific\nworkflow applications on Amazon EC2 demonstrate (1) the accuracy of our\nframework on satisfying the probabilistic deadline guarantees required by the\nusers; (2) the effectiveness of our framework on reducing monetary cost in\ncomparison with the existing approaches.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2013 05:50:26 GMT"}, {"version": "v2", "created": "Wed, 30 Apr 2014 03:51:14 GMT"}], "update_date": "2014-05-01", "authors_parsed": [["Zhou", "Amelie Chi", ""], ["He", "Bingsheng", ""], ["Liu", "Cheng", ""]]}, {"id": "1306.6597", "submitter": "Sukhpal  Singh", "authors": "Sukhpal Singh, Rishideep Singh", "title": "Earthquake Disaster based Efficient Resource Utilization Technique in\n  IaaS Cloud", "comments": "6 Pages including 4 figures, Published by IJARCET", "journal-ref": "International Journal of Advanced Research in Computer Engineering\n  & Technology (IJARCET) Volume 2, Issue 6, 2013, 1933-1938", "doi": null, "report-no": "IJARCET - 6831", "categories": "cs.DC", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Cloud Computing is an emerging area. The main aim of the initial\nsearch-and-rescue period after strong earthquakes is to reduce the whole number\nof mortalities. One main trouble rising in this period is to and the greatest\nassignment of available resources to functioning zones. For this issue a\ndynamic optimization model is presented. The model uses thorough descriptions\nof the operational zones and of the available resources to determine the\nresource performance and efficiency for different workloads related to the\nresponse. A suitable solution method for the model is offered as well. In this\npaper, Earthquake Disaster Based Resource Scheduling (EDBRS) Framework has been\nproposed. The allocation of resources to cloud workloads based on urgency\n(emergency during Earthquake Disaster). Based on this criterion, the resource\nscheduling algorithm has been proposed. The performance of the proposed\nalgorithm has been assessed with the existing common scheduling algorithms\nthrough the CloudSim. The experimental results show that the proposed algorithm\noutperforms the existing algorithms by reducing execution cost and time of\ncloud consumer workloads submitted to the cloud.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2013 18:23:06 GMT"}, {"version": "v2", "created": "Fri, 17 Jan 2014 17:30:05 GMT"}, {"version": "v3", "created": "Mon, 27 Jan 2014 12:02:43 GMT"}], "update_date": "2014-01-28", "authors_parsed": [["Singh", "Sukhpal", ""], ["Singh", "Rishideep", ""]]}]