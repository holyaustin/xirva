[{"id": "1307.0048", "submitter": "Kun  Yang", "authors": "Kun Yang", "title": "Simple one-pass algorithm for penalized linear regression with\n  cross-validation on MapReduce", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a one-pass algorithm on MapReduce for penalized\nlinear regression\n  \\[f_\\lambda(\\alpha, \\beta) = \\|Y - \\alpha\\mathbf{1} - X\\beta\\|_2^2 +\np_{\\lambda}(\\beta)\\] where $\\alpha$ is the intercept which can be omitted\ndepending on application; $\\beta$ is the coefficients and $p_{\\lambda}$ is the\npenalized function with penalizing parameter $\\lambda$. $f_\\lambda(\\alpha,\n\\beta)$ includes interesting classes such as Lasso, Ridge regression and\nElastic-net. Compared to latest iterative distributed algorithms requiring\nmultiple MapReduce jobs, our algorithm achieves huge performance improvement;\nmoreover, our algorithm is exact compared to the approximate algorithms such as\nparallel stochastic gradient decent. Moreover, what our algorithm distinguishes\nwith others is that it trains the model with cross validation to choose optimal\n$\\lambda$ instead of user specified one.\n  Key words: penalized linear regression, lasso, elastic-net, ridge, MapReduce\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2013 23:32:11 GMT"}, {"version": "v2", "created": "Wed, 13 Apr 2016 05:52:11 GMT"}, {"version": "v3", "created": "Thu, 14 Apr 2016 01:55:55 GMT"}], "update_date": "2016-04-15", "authors_parsed": [["Yang", "Kun", ""]]}, {"id": "1307.0049", "submitter": "Minko Dudev", "authors": "Minko Dudev, Sebastian Gerling, Philip Peter", "title": "SAHER: Secure and Efficient Routing in Sensor Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an increasing amount of research is being done on various applications of\nsensor networks in adversarial environments, ensuring secure routing becomes of\ncritical importance for the success of such deployments. The problem of\ndesigning a secure routing protocol for ad hoc networks has been already\naddressed, yet, there exists no complete solution that meets the specific\nrequirements of sensor networks, where nodes are extremely constrained in terms\nof both power and computational resources. Thus, we propose a new protocol that\nis not built solely around security but also has efficiency and simplicity\namong its main goals. We propose the Secure Ad Hoc Efficient Routing protocol\n(SAHER) which employs a two-tier architecture based on node clustering. Also,\nwe combine mechanisms like localscale geographic routing, per-node reputation\ntables, credit based alternate route enforcement and cumulative authentication.\nUsing these techniques we examine ways to efficiently defend against the two\nmost common network layer attacks: selective packet dropping and message\nflooding. Further, we consider join/leave operations which have not yet been\nstudied in sufficient depth for sensor networks from a security standpoint. We\nprovide a description of the protocol along with comprehensive experimental\nevaluation under different node distributions, different proportions of\nnon-malicious vs. malicious nodes and different types of activity that\nmalicious nodes could exhibit.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2013 23:37:31 GMT"}], "update_date": "2013-07-02", "authors_parsed": [["Dudev", "Minko", ""], ["Gerling", "Sebastian", ""], ["Peter", "Philip", ""]]}, {"id": "1307.0100", "submitter": "Konstantin Solnushkin S", "authors": "Konstantin S. Solnushkin and Yuichi Tsujita", "title": "Marrying Many-core Accelerators and InfiniBand for a New Commodity\n  Processor", "comments": "5 pages, 4 figures. The work was presented at one of the workshops at\n  the International Conference on Computational Science (ICCS 2013) in\n  Barcelona, Spain. However, due to prolonged deadlines, papers from this\n  workshop didn't get into conference proceedings; we rectify this by\n  submitting the paper to arXiv.org", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the last 15 years, the supercomputing industry has been using\nmass-produced, off-the-shelf components to build cluster computers. Such\ncomponents are not perfect for HPC purposes, but are cheap due to effect of\nscale in their production. The coming exa-scale era changes the landscape:\nexa-scale computers will contain components in quantities large enough to\njustify their custom development and production.\n  We propose a new heterogeneous processor, equipped with a network controller\nand designed specifically for HPC. We then show how it can be used for\nenterprise computing market, guaranteeing its widespread adoption and therefore\nlow production costs.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2013 13:07:10 GMT"}], "update_date": "2013-07-02", "authors_parsed": [["Solnushkin", "Konstantin S.", ""], ["Tsujita", "Yuichi", ""]]}, {"id": "1307.0433", "submitter": "Laura Tosoratto", "authors": "Roberto Ammendola, Andrea Biagioni, Ottorino Frezza, Francesca Lo\n  Cicero, Alessandro Lonardo, Pier Stanislao Paolucci, Davide Rossetti,\n  Francesco Simula, Laura Tosoratto and Piero Vicini", "title": "'Mutual Watch-dog Networking': Distributed Awareness of Faults and\n  Critical Events in Petascale/Exascale systems", "comments": "Technical Report, Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many tile systems require techniques to be applied to increase components\nresilience and control the FIT (Failures In Time) rate. When scaling to peta-\nexa-scale systems the FIT rate may become unacceptable due to component\nnumerosity, requiring more systemic countermeasures. Thus, the ability to be\nfault aware, i.e. to detect and collect information about fault and critical\nevents, is a necessary feature that large scale distributed architectures must\nprovide in order to apply systemic fault tolerance techniques. In this context,\nthe LO|FA|MO approach is a way to obtain systemic fault awareness, by\nimplementing a mutual watchdog mechanism and guaranteeing fault detection in a\nno-single-point-of-failure fashion. This document contains specification and\nimplementation details about this approach, in the shape of a technical report.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2013 16:41:40 GMT"}, {"version": "v2", "created": "Tue, 2 Jul 2013 10:15:54 GMT"}], "update_date": "2013-07-03", "authors_parsed": [["Ammendola", "Roberto", ""], ["Biagioni", "Andrea", ""], ["Frezza", "Ottorino", ""], ["Cicero", "Francesca Lo", ""], ["Lonardo", "Alessandro", ""], ["Paolucci", "Pier Stanislao", ""], ["Rossetti", "Davide", ""], ["Simula", "Francesco", ""], ["Tosoratto", "Laura", ""], ["Vicini", "Piero", ""]]}, {"id": "1307.0473", "submitter": "Maxim Raginsky", "authors": "Maxim Raginsky and Angelia Nedi\\'c", "title": "Online discrete optimization in social networks in the presence of\n  Knightian uncertainty", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a model of collective real-time decision-making (or learning) in a\nsocial network operating in an uncertain environment, for which no a priori\nprobabilistic model is available. Instead, the environment's impact on the\nagents in the network is seen through a sequence of cost functions, revealed to\nthe agents in a causal manner only after all the relevant actions are taken.\nThere are two kinds of costs: individual costs incurred by each agent and\nlocal-interaction costs incurred by each agent and its neighbors in the social\nnetwork. Moreover, agents have inertia: each agent has a default mixed strategy\nthat stays fixed regardless of the state of the environment, and must expend\neffort to deviate from this strategy in order to respond to cost signals coming\nfrom the environment. We construct a decentralized strategy, wherein each agent\nselects its action based only on the costs directly affecting it and on the\ndecisions made by its neighbors in the network. In this setting, we quantify\nsocial learning in terms of regret, which is given by the difference between\nthe realized network performance over a given time horizon and the best\nperformance that could have been achieved in hindsight by a fictitious\ncentralized entity with full knowledge of the environment's evolution. We show\nthat our strategy achieves the regret that scales polylogarithmically with the\ntime horizon and polynomially with the number of agents and the maximum number\nof neighbors of any agent in the social network.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2013 18:46:06 GMT"}, {"version": "v2", "created": "Thu, 29 Jan 2015 04:20:48 GMT"}], "update_date": "2015-01-30", "authors_parsed": [["Raginsky", "Maxim", ""], ["Nedi\u0107", "Angelia", ""]]}, {"id": "1307.1051", "submitter": "Lewis Tseng", "authors": "Lewis Tseng and Nitin Vaidya", "title": "Byzantine Convex Consensus: Preliminary Version", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of the past work on asynchronous approximate Byzantine consensus has\nassumed scalar inputs at the nodes [3, 7]. Recent work has yielded approximate\nByzantine consensus algorithms for the case when the input at each node is a\nd-dimensional vector, and the nodes must reach consensus on a vector in the\nconvex hull of the input vectors at the fault-free nodes [8, 12]. The\nd-dimensional vectors can be equivalently viewed as points in the d-dimensional\nEuclidean space. Thus, the algorithms in [8, 12] require the fault-free nodes\nto decide on a point in the d-dimensional space.\n  In this paper, we generalize the problem to allow the decision to be a convex\npolytope in the d-dimensional space, such that the decided polytope is within\nthe convex hull of the input vectors at the fault-free nodes. We name this\nproblem as Byzantine convex consensus (BCC), and present an asynchronous\napproximate BCC algorithm with optimal fault tolerance. Ideally, the goal here\nis to agree on a convex polytope that is as large as possible. While we do not\nclaim that our algorithm satisfies this goal, we show a bound on the output\nconvex polytope chosen by our algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2013 15:49:13 GMT"}], "update_date": "2013-07-04", "authors_parsed": [["Tseng", "Lewis", ""], ["Vaidya", "Nitin", ""]]}, {"id": "1307.1270", "submitter": "Francesco Simula Francesco Simula", "authors": "Roberto Ammendola, Andrea Biagioni, Ottorino Frezza, Werner Geurts,\n  Gert Goossens, Francesca Lo Cicero, Alessandro Lonardo, Pier Stanislao\n  Paolucci, Davide Rossetti, Francesco Simula, Laura Tosoratto, Piero Vicini", "title": "A heterogeneous many-core platform for experiments on scalable custom\n  interconnects and management of fault and critical events, applied to\n  many-process applications: Vol. II, 2012 technical report", "comments": "119 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the second of a planned collection of four yearly volumes describing\nthe deployment of a heterogeneous many-core platform for experiments on\nscalable custom interconnects and management of fault and critical events,\napplied to many-process applications. This volume covers several topics, among\nwhich: 1- a system for awareness of faults and critical events (named LO|FA|MO)\non experimental heterogeneous many-core hardware platforms; 2- the integration\nand test of the experimental hardware heterogeneous many-core platform QUoNG,\nbased on the APEnet+ custom interconnect; 3- the design of a\nSoftware-Programmable Distributed Network Processor architecture (DNP) using\nASIP technology; 4- the initial stages of design of a new DNP generation onto a\n28nm FPGA. These developments were performed in the framework of the EURETILE\nEuropean Project under the Grant Agreement no. 247846.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2013 10:55:43 GMT"}], "update_date": "2013-07-05", "authors_parsed": [["Ammendola", "Roberto", ""], ["Biagioni", "Andrea", ""], ["Frezza", "Ottorino", ""], ["Geurts", "Werner", ""], ["Goossens", "Gert", ""], ["Cicero", "Francesca Lo", ""], ["Lonardo", "Alessandro", ""], ["Paolucci", "Pier Stanislao", ""], ["Rossetti", "Davide", ""], ["Simula", "Francesco", ""], ["Tosoratto", "Laura", ""], ["Vicini", "Piero", ""]]}, {"id": "1307.1332", "submitter": "Lewis Tseng", "authors": "Lewis Tseng and Nitin Vaidya", "title": "Byzantine Convex Consensus: An Optimal Algorithm", "comments": "arXiv admin note: substantial text overlap with arXiv:1307.1051", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of the past work on asynchronous approximate Byzantine consensus has\nassumed scalar inputs at the nodes [4, 8]. Recent work has yielded approximate\nByzantine consensus algorithms for the case when the input at each node is a\nd-dimensional vector, and the nodes must reach consensus on a vector in the\nconvex hull of the input vectors at the fault-free nodes [9, 13]. The\nd-dimensional vectors can be equivalently viewed as points in the d-dimensional\nEuclidean space. Thus, the algorithms in [9, 13] require the fault-free nodes\nto decide on a point in the d-dimensional space.\n  In our recent work [arXiv:/1307.1051], we proposed a generalization of the\nconsensus problem, namely Byzantine convex consensus (BCC), which allows the\ndecision to be a convex polytope in the d-dimensional space, such that the\ndecided polytope is within the convex hull of the input vectors at the\nfault-free nodes. We also presented an asynchronous approximate BCC algorithm.\n  In this paper, we propose a new BCC algorithm with optimal fault-tolerance\nthat also agrees on a convex polytope that is as large as possible under\nadversarial conditions. Our prior work [arXiv:/1307.1051] does not guarantee\nthe optimality of the output polytope.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2013 14:01:57 GMT"}, {"version": "v2", "created": "Tue, 9 Jul 2013 15:10:32 GMT"}], "update_date": "2013-07-10", "authors_parsed": [["Tseng", "Lewis", ""], ["Vaidya", "Nitin", ""]]}, {"id": "1307.1448", "submitter": "Paolo Di Lorenzo", "authors": "Sergio Barbarossa, Stefania Sardellitti, Paolo Di Lorenzo", "title": "Distributed Detection and Estimation in Wireless Sensor Networks", "comments": "92 pages, 24 figures. To appear in E-Reference Signal Processing, R.\n  Chellapa and S. Theodoridis, Eds., Elsevier, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we consider the problems of distributed detection and\nestimation in wireless sensor networks. In the first part, we provide a general\nframework aimed to show how an efficient design of a sensor network requires a\njoint organization of in-network processing and communication. Then, we recall\nthe basic features of consensus algorithm, which is a basic tool to reach\nglobally optimal decisions through a distributed approach. The main part of the\npaper starts addressing the distributed estimation problem. We show first an\nentirely decentralized approach, where observations and estimations are\nperformed without the intervention of a fusion center. Then, we consider the\ncase where the estimation is performed at a fusion center, showing how to\nallocate quantization bits and transmit powers in the links between the nodes\nand the fusion center, in order to accommodate the requirement on the maximum\nestimation variance, under a constraint on the global transmit power. We extend\nthe approach to the detection problem. Also in this case, we consider the\ndistributed approach, where every node can achieve a globally optimal decision,\nand the case where the decision is taken at a central node. In the latter case,\nwe show how to allocate coding bits and transmit power in order to maximize the\ndetection probability, under constraints on the false alarm rate and the global\ntransmit power. Then, we generalize consensus algorithms illustrating a\ndistributed procedure that converges to the projection of the observation\nvector onto a signal subspace. We then address the issue of energy consumption\nin sensor networks, thus showing how to optimize the network topology in order\nto minimize the energy necessary to achieve a global consensus. Finally, we\naddress the problem of matching the topology of the network to the graph\ndescribing the statistical dependencies among the observed variables.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2013 18:51:36 GMT"}, {"version": "v2", "created": "Fri, 5 Jul 2013 14:35:38 GMT"}], "update_date": "2013-07-08", "authors_parsed": [["Barbarossa", "Sergio", ""], ["Sardellitti", "Stefania", ""], ["Di Lorenzo", "Paolo", ""]]}, {"id": "1307.1517", "submitter": "Nandan Mirajkar Mr", "authors": "Nandan Mirajkar, Sandeep Bhujbal, Aaradhana Deshmukh", "title": "Perform wordcount Map-Reduce Job in Single Node Apache Hadoop cluster\n  and compress data using Lempel-Ziv-Oberhumer (LZO) algorithm", "comments": "10 pages, 17 figures, Journal", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 10,\n  Issue 1, No 2, January 2013 ISSN (Print): 1694-0784 | ISSN (Online):\n  1694-0814 www.IJCSI.org", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications like Yahoo, Facebook, Twitter have huge data which has to be\nstored and retrieved as per client access. This huge data storage requires huge\ndatabase leading to increase in physical storage and becomes complex for\nanalysis required in business growth. This storage capacity can be reduced and\ndistributed processing of huge data can be done using Apache Hadoop which uses\nMap-reduce algorithm and combines the repeating data so that entire data is\nstored in reduced format. The paper describes performing a wordcount Map-Reduce\nJob in Single Node Apache Hadoop cluster and compress data using\nLempel-Ziv-Oberhumer (LZO) algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2013 04:10:34 GMT"}], "update_date": "2013-07-08", "authors_parsed": [["Mirajkar", "Nandan", ""], ["Bhujbal", "Sandeep", ""], ["Deshmukh", "Aaradhana", ""]]}, {"id": "1307.1650", "submitter": "Miguel Mosteiro", "authors": "Antonio Fernandez Anta, Chryssis Georgiou, and Miguel A. Mosteiro", "title": "Algorithmic Mechanisms for Reliable Internet-based Computing under\n  Collusion", "comments": "23 pages. A preliminary version of this work appears in the\n  Proceedings of NCA 2008", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, using a game-theoretic approach, cost-sensitive mechanisms that\nlead to reliable Internet-based computing are designed. In particular, we\nconsider Internet-based master-worker computations, where a master processor\nassigns, across the Internet, a computational task to a set of potentially\nuntrusted worker processors and collects their responses. Workers may collude\nin order to increase their benefit. Several game-theoretic models that capture\nthe nature of the problem are analyzed, and algorithmic mechanisms that, for\neach given set of cost and system parameters, achieve high reliability are\ndesigned. Additionally, two specific realistic system scenarios are studied.\nThese scenarios are a system of volunteer computing like SETI, and a company\nthat buys computing cycles from Internet computers and sells them to its\ncustomers in the form of a task- computation service. Notably, under certain\nconditions, non redundant allocation yields the best trade-off between cost and\nreliability.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2013 16:10:23 GMT"}], "update_date": "2013-07-08", "authors_parsed": [["Anta", "Antonio Fernandez", ""], ["Georgiou", "Chryssis", ""], ["Mosteiro", "Miguel A.", ""]]}, {"id": "1307.1743", "submitter": "Srikumar Venugopal", "authors": "Richard Gow, Srikumar Venugopal and Pradeep Ray", "title": "\"The tail wags the dog\": A study of anomaly detection in commercial\n  application performance", "comments": "10 pages; Longer version of the short paper accepted for MASCOTS 2013", "journal-ref": null, "doi": null, "report-no": "UNSW-CSE-TR-201317", "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The IT industry needs systems management models that leverage available\napplication information to detect quality of service, scalability and health of\nservice. Ideally this technique would be common for varying application types\nwith different n-tier architectures under normal production conditions of\nvarying load, user session traffic, transaction type, transaction mix, and\nhosting environment.\n  This paper shows that a whole of service measurement paradigm utilizing a\nblack box M/M/1 queuing model and auto regression curve fitting of the\nassociated CDF are an accurate model to characterize system performance\nsignatures. This modeling method is also used to detect application slow down\nevents. The technique was shown to work for a diverse range of workloads\nranging from 76 Tx/ 5min to 19,025 Tx/ 5min. The method did not rely on\ncustomizations specific to the n-tier architecture of the systems being\nanalyzed and so the performance anomaly detection technique was shown to be\nplatform and configuration agnostic.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2013 02:02:57 GMT"}], "update_date": "2013-07-09", "authors_parsed": [["Gow", "Richard", ""], ["Venugopal", "Srikumar", ""], ["Ray", "Pradeep", ""]]}, {"id": "1307.1805", "submitter": "Michele Scquizzato", "authors": "Michele Scquizzato and Francesco Silvestri", "title": "Communication Lower Bounds for Distributed-Memory Computations", "comments": "Minor edits", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give lower bounds on the communication complexity required to solve\nseveral computational problems in a distributed-memory parallel machine, namely\nstandard matrix multiplication, stencil computations, comparison sorting, and\nthe Fast Fourier Transform. We revisit the assumptions under which preceding\nresults were derived and provide new lower bounds which use much weaker and\nappropriate hypotheses. Our bounds rely on a mild assumption on work\ndistribution, and strengthen previous results which require either the\ncomputation to be balanced among the processors, or specific initial\ndistributions of the input data, or an upper bound on the size of processors'\nlocal memories.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2013 18:40:47 GMT"}, {"version": "v2", "created": "Fri, 20 Sep 2013 21:51:59 GMT"}], "update_date": "2013-09-24", "authors_parsed": [["Scquizzato", "Michele", ""], ["Silvestri", "Francesco", ""]]}, {"id": "1307.1955", "submitter": "Jiong He", "authors": "Jiong He, Mian Lu, Bingsheng He", "title": "Revisiting Co-Processing for Hash Joins on the Coupled CPU-GPU\n  Architecture", "comments": "14 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query co-processing on graphics processors (GPUs) has become an effective\nmeans to improve the performance of main memory databases. However, the\nrelatively low bandwidth and high latency of the PCI-e bus are usually\nbottleneck issues for co-processing. Recently, coupled CPU-GPU architectures\nhave received a lot of attention, e.g. AMD APUs with the CPU and the GPU\nintegrated into a single chip. That opens up new opportunities for optimizing\nquery co-processing. In this paper, we experimentally revisit hash joins, one\nof the most important join algorithms for main memory databases, on a coupled\nCPU-GPU architecture. Particularly, we study the fine-grained co-processing\nmechanisms on hash joins with and without partitioning. The co-processing\noutlines an interesting design space. We extend existing cost models to\nautomatically guide decisions on the design space. Our experimental results on\na recent AMD APU show that (1) the coupled architecture enables fine-grained\nco-processing and cache reuses, which are inefficient on discrete CPU-GPU\narchitectures; (2) the cost model can automatically guide the design and tuning\nknobs in the design space; (3) fine-grained co-processing achieves up to 53%,\n35% and 28% performance improvement over CPU-only, GPU-only and conventional\nCPU-GPU co-processing, respectively. We believe that the insights and\nimplications from this study are initial yet important for further research on\nquery co-processing on coupled CPU-GPU architectures.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2013 06:23:12 GMT"}], "update_date": "2016-11-26", "authors_parsed": [["He", "Jiong", ""], ["Lu", "Mian", ""], ["He", "Bingsheng", ""]]}, {"id": "1307.2036", "submitter": "Eike Hermann M\\\"uller", "authors": "Eike H. Mueller and Robert Scheichl", "title": "Massively parallel solvers for elliptic PDEs in Numerical Weather- and\n  Climate Prediction", "comments": "24 pages, 7 figures, 7 tables", "journal-ref": null, "doi": "10.1002/qj.2327", "report-no": null, "categories": "cs.DC math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The demand for substantial increases in the spatial resolution of global\nweather- and climate- prediction models makes it necessary to use numerically\nefficient and highly scalable algorithms to solve the equations of large scale\natmospheric fluid dynamics. For stability and efficiency reasons several of the\noperational forecasting centres, in particular the Met Office and the ECMWF in\nthe UK, use semi-implicit semi-Lagrangian time stepping in the dynamical core\nof the model. The additional burden with this approach is that a three\ndimensional elliptic partial differential equation (PDE) for the pressure\ncorrection has to be solved at every model time step and this often constitutes\na significant proportion of the time spent in the dynamical core. To run within\ntight operational time scales the solver has to be parallelised and there seems\nto be a (perceived) misconception that elliptic solvers do not scale to large\nprocessor counts and hence implicit time stepping can not be used in very high\nresolution global models. After reviewing several methods for solving the\nelliptic PDE for the pressure correction and their application in atmospheric\nmodels we demonstrate the performance and very good scalability of Krylov\nsubspace solvers and multigrid algorithms for a representative model equation\nwith more than $10^{10}$ unknowns on 65536 cores on HECToR, the UK's national\nsupercomputer. For this we tested and optimised solvers from two existing\nnumerical libraries (DUNE and hypre) and implemented both a Conjugate Gradient\nsolver and a geometric multigrid algorithm based on a tensor-product approach\nwhich exploits the strong vertical anisotropy of the discretised equation. We\nstudy both weak and strong scalability and compare the absolute solution times\nfor all methods; in contrast to one-level methods the multigrid solver is\nrobust with respect to parameter variations.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2013 11:07:39 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Mueller", "Eike H.", ""], ["Scheichl", "Robert", ""]]}, {"id": "1307.2062", "submitter": "Catuscia Palamidessi", "authors": "Catuscia Palamidessi", "title": "Comparing the Expressive Power of the Synchronous and the Asynchronous\n  pi-calculi", "comments": null, "journal-ref": "Mathematical Structures in Computer Science, 13(5) , 685-719, 2003", "doi": "10.1017/S0960129503004043", "report-no": null, "categories": "cs.LO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Asynchronous pi-calculus, proposed by Honda and Tokoro (1991) and,\nindependently, by Boudol (1992), is a subset of the pi-calculus (Milner, 1992)\nwhich contains no explicit operators for choice and output-prefixing. The\ncommunication mechanism of this calculus, however, is powerful enough to\nsimulate output-prefixing, as shown by Honda and Tokoro (1991) and by Boudol\n(1992), and input-guarded choice, as shown by Nestmann and Pierce (2000). A\nnatural question arises, then, whether or not it is as expressive as the full\npi-calculus. We show that this is not the case. More precisely, we show that\nthere does not exist any uniform, fully distributed translation from the\npi-calculus into the asynchronous pi-calculus, up to any \"reasonable\" notion of\nequivalence. This result is based on the incapability of the asynchronous\npi-calculus to break certain symmetries possibly present in the initial\ncommunication graph. By similar arguments, we prove a separation result between\nthe pi-calculus and CCS, and between the pi-calculus and the pi-calculus with\ninternal mobility, a subset of the pi-calculus proposed by Sangiorgi where the\noutput actions can only transmit private names.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2013 12:31:50 GMT"}], "update_date": "2013-07-09", "authors_parsed": [["Palamidessi", "Catuscia", ""]]}, {"id": "1307.2354", "submitter": "Kazutaka Motoyama", "authors": "Kazutaka Motoyama, Yoshikazu Tanaka, Kento Aida, Eisaku Sakane,\n  Kenichi Miura", "title": "Effective System for Simulating Dust Continuum Observations on\n  Distributed Computing Resources", "comments": "7 pages, 9 figures, accepted for publication in Journal of Space\n  Science Informatics Japan", "journal-ref": "Journal of Space Science Informatics Japan, Volume 3, pp. 155-161,\n  (2014)", "doi": null, "report-no": "JAXA-RR-13-010", "categories": "astro-ph.IM cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an effective system for simulating dust continuum observations by\nradiative transfer simulations. By using workflow management system RENKEI-WFT,\nwe utilized distributed computing resources and automated a sequence of\ncomputational tasks required for radiative transfer modeling, namely, main\nradiative transfer simulations, pre-/post-processes, and data transfer between\ncomputing resources. Our system simultaneously executes a lot of radiative\ntransfer simulations with different input parameters on distributed computing\nresources. This capability of our system enables us to conduct effective\nresearch by radiative transfer simulation. As a demonstration of our system, we\nsimulated dust continuum observations of protoplanetary disk. We performed\nhydrodynamic simulation modeling photoevaporating protoplanetary disk\nirradiated by ultra violet radiation from nearby massive stars. Results of this\nhydrodynamic simulation were used as input data for radiative transfer\nsimulations. Expected spectral energy distributions and intensity maps were\nobtained by our system.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2013 07:32:05 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["Motoyama", "Kazutaka", ""], ["Tanaka", "Yoshikazu", ""], ["Aida", "Kento", ""], ["Sakane", "Eisaku", ""], ["Miura", "Kenichi", ""]]}, {"id": "1307.2380", "submitter": "Sena Seneviratne S R", "authors": "Sena Seneviratne, David C. Levy and Rajkumar Buyya", "title": "A Taxonomy of Performance Prediction Systems in the Parallel and\n  Distributed Computing Grids", "comments": "35 pages,4 figures,2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As Grids are loosely-coupled congregations of geographically distributed\nheterogeneous resources, the efficient utilization of the resources requires\nthe support of a sound Performance Prediction System (PPS). The performance\nprediction of grid resources is helpful for both Resource Management Systems\nand grid users to make optimized resource usage decisions. There have been many\nPPS projects that span over several grid resources in several dimensions. In\nthis paper the taxonomy for describing the PPS architecture is discussed. The\ntaxonomy is used to categorize and identify approaches which are followed in\nthe implementation of the existing PPSs for Grids. The taxonomy and the survey\nresults are used to identify approaches and issues that have not been fully\nexplored in research.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2013 09:34:25 GMT"}, {"version": "v2", "created": "Fri, 12 Jul 2013 08:52:42 GMT"}], "update_date": "2013-07-15", "authors_parsed": [["Seneviratne", "Sena", ""], ["Levy", "David C.", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "1307.2483", "submitter": "Lewis Tseng", "authors": "Nitin H. Vaidya", "title": "Iterative Byzantine Vector Consensus in Incomplete Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses Byzantine vector consensus (BVC), wherein the input at\neach process is a d-dimensional vector of reals, and each process is expected\nto decide on a decision vector that is in the convex hull of the input vectors\nat the fault-free processes [3, 8]. The input vector at each process may also\nbe viewed as a point in the d-dimensional Euclidean space R^d, where d > 0 is a\nfinite integer. Recent work [3, 8] has addressed Byzantine vector consensus in\nsystems that can be modeled by a complete graph. This paper considers Byzantine\nvector consensus in incomplete graphs. In particular, we address a particular\nclass of iterative algorithms in incomplete graphs, and prove a necessary\ncondition, and a sufficient condition, for the graphs to be able to solve the\nvector consensus problem iteratively. We present an iterative Byzantine vector\nconsensus algorithm, and prove it correct under the sufficient condition. The\nnecessary condition presented in this paper for vector consensus does not match\nwith the sufficient condition for d > 1; thus, a weaker condition may\npotentially suffice for Byzantine vector consensus.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2013 14:53:05 GMT"}], "update_date": "2016-11-26", "authors_parsed": [["Vaidya", "Nitin H.", ""]]}, {"id": "1307.2560", "submitter": "Tejaswi Agarwal", "authors": "Saurabh Jha, Tejaswi Agarwal and B. Rajesh Kanna", "title": "Exploiting Data Parallelism in the yConvex Hypergraph Algorithm for\n  Image Representation using GPGPUs", "comments": "1 page, 1 figure published in Proceedings of the 27th ACM\n  International Conference on Supercomputing, ICS 2013, Eugene, Oregon, USA", "journal-ref": "ACM 978-1-4503-2130-3/13/06 2013", "doi": null, "report-no": null, "categories": "cs.DC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To define and identify a region-of-interest (ROI) in a digital image, the\nshape descriptor of the ROI has to be described in terms of its boundary\ncharacteristics. To address the generic issues of contour tracking, the yConvex\nHypergraph (yCHG) model was proposed by Kanna et al [1]. In this work, we\npropose a parallel approach to implement the yCHG model by exploiting massively\nparallel cores of NVIDIA's Compute Unified Device Architecture (CUDA). We\nperform our experiments on the MODIS satellite image database by NASA, and\nbased on our analysis we observe that the performance of the serial\nimplementation is better on smaller images, but once the threshold is achieved\nin terms of image resolution, the parallel implementation outperforms its\nsequential counterpart by 2 to 10 times (2x-10x). We also conclude that an\nincrease in the number of hyperedges in the ROI of a given size does not impact\nthe performance of the overall algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jun 2013 22:31:49 GMT"}], "update_date": "2013-07-10", "authors_parsed": [["Jha", "Saurabh", ""], ["Agarwal", "Tejaswi", ""], ["Kanna", "B. Rajesh", ""]]}, {"id": "1307.2690", "submitter": "Robert Lychev", "authors": "Robert Lychev, Sharon Goldberg, Michael Schapira", "title": "BGP Security in Partial Deployment: Is the Juice Worth the Squeeze?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the rollout of secure route origin authentication with the RPKI slowly\ngains traction among network operators, there is a push to standardize secure\npath validation for BGP (i.e., S*BGP: S-BGP, soBGP, BGPSEC, etc.). Origin\nauthentication already does much to improve routing security. Moreover, the\ntransition to S*BGP is expected to be long and slow, with S*BGP coexisting in\n\"partial deployment\" alongside BGP for a long time. We therefore use\ntheoretical and experimental approach to study the security benefits provided\nby partially-deployed S*BGP, vis-a-vis those already provided by origin\nauthentication. Because routing policies have a profound impact on routing\nsecurity, we use a survey of 100 network operators to find the policies that\nare likely to be most popular during partial S*BGP deployment. We find that\nS*BGP provides only meagre benefits over origin authentication when these\npopular policies are used. We also study the security benefits of other routing\npolicies, provide prescriptive guidelines for partially-deployed S*BGP, and\nshow how interactions between S*BGP and BGP can introduce new vulnerabilities\ninto the routing system.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2013 06:35:22 GMT"}], "update_date": "2013-07-11", "authors_parsed": [["Lychev", "Robert", ""], ["Goldberg", "Sharon", ""], ["Schapira", "Michael", ""]]}, {"id": "1307.2783", "submitter": "Evgenia Christoforou", "authors": "Evgenia Christoforou and Antonio Fernandez Anta and Chryssis Georgiou\n  and Miguel A. Mosteiro and Angel Sanchez", "title": "Coping with Unreliable Workers in Internet-based Computing: An\n  Evaluation of Reputation Mechanisms", "comments": "28 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present reputation-based mechanisms for building reliable task computing\nsystems over the Internet. The most characteristic examples of such systems are\nthe volunteer computing and the crowdsourcing platforms. In both examples end\nusers are offering over the Internet their computing power or their human\nintelligence to solve tasks either voluntarily or under payment. While the main\nadvantage of these systems is the inexpensive computational power provided, the\nmain drawback is the untrustworthy nature of the end users. Generally, this\ntype of systems are modeled under the \"master-worker\" setting. A \"master\" has a\nset of tasks to compute and instead of computing them locally she sends these\ntasks to available \"workers\" that compute and report back the task results. We\ncategorize these workers in three generic types: altruistic, malicious and\nrational. Altruistic workers that always return the correct result, malicious\nworkers that always return an incorrect result, and rational workers that\ndecide to reply or not truthfully depending on what increases their benefit. We\ndesign a reinforcement learning mechanism to induce a correct behavior to\nrational workers, while the mechanism is complemented by four reputation\nschemes that cope with malice. The goal of the mechanism is to reach a state of\neventual correctness, that is, a stable state of the system in which the master\nalways obtains the correct task results. Analysis of the system gives provable\nguarantees under which truthful behavior can be ensured. Finally, we observe\nthe behavior of the mechanism through simulations that use realistic system\nparameters values. Simulations not only agree with the analysis but also reveal\ninteresting trade-offs between various metrics and parameters. Finally, the\nfour reputation schemes are assessed against the tolerance to cheaters.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2013 13:18:24 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2013 15:38:12 GMT"}, {"version": "v3", "created": "Wed, 2 Oct 2013 14:48:40 GMT"}, {"version": "v4", "created": "Mon, 19 Mar 2018 13:10:43 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Christoforou", "Evgenia", ""], ["Anta", "Antonio Fernandez", ""], ["Georgiou", "Chryssis", ""], ["Mosteiro", "Miguel A.", ""], ["Sanchez", "Angel", ""]]}, {"id": "1307.2915", "submitter": "Woo-Cheol Kim Dr.", "authors": "Woo-Cheol Kim, Changryong Baek, Dongwon Lee", "title": "Measuring the Optimality of Hadoop Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, much research has focused on how to optimize Hadoop jobs.\nTheir approaches are diverse, ranging from improving HDFS and Hadoop job\nscheduler to optimizing parameters in Hadoop configurations. Despite their\nsuccess in improving the performance of Hadoop jobs, however, very little is\nknown about the limit of their optimization performance. That is, how optimal\nis a given Hadoop optimization? When a Hadoop optimization method X improves\nthe performance of a job by Y %, how do we know if this improvement is as good\nas it can be? To answer this question, in this paper, we first examine the\nideal best case, the lower bound, of running time for Hadoop jobs and develop a\nmeasure to accurately estimate how optimal a given Hadoop optimization is with\nrespect to the lower bound. Then, we demonstrate how one may exploit the\nproposed measure to improve the optimization of Hadoop jobs.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2013 20:39:24 GMT"}], "update_date": "2013-07-12", "authors_parsed": [["Kim", "Woo-Cheol", ""], ["Baek", "Changryong", ""], ["Lee", "Dongwon", ""]]}, {"id": "1307.3080", "submitter": "Erez Kantor", "authors": "Erez Kantor and Shay Kutten", "title": "Optimal competitiveness for Symmetric Rectilinear Steiner Arborescence\n  and related problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present optimal competitive algorithms for two interrelated known problems\ninvolving Steiner Arborescence. One is the continuous problem of the Symmetric\nRectilinear Steiner Arborescence (SRSA), studied by Berman and Coulston.\n  A very related, but discrete problem (studied separately in the past) is the\nonline Multimedia Content Delivery (MCD) problem on line networks, presented\noriginally by Papadimitriu, Ramanathan, and Rangan. An efficient content\ndelivery was modeled as a low cost Steiner arborescence in a grid of\nnetwork*time they defined. We study here the version studied by Charikar,\nHalperin, and Motwani (who used the same problem definitions, but removed some\nconstraints on the inputs).\n  The bounds on the competitive ratios introduced separately in the above\npapers are similar for the two problems: O(log N) for the continuous problem\nand O(log n) for the network problem, where N was the number of terminals to\nserve, and n was the size of the network. The lower bounds were Omega(sqrt{log\nN}) and Omega(sqrt{log n}) correspondingly. Berman and Coulston conjectured\nthat both the upper bound and the lower bound could be improved.\n  We disprove this conjecture and close these quadratic gaps for both problems.\nWe first present an O(sqrt{log n}) deterministic competitive algorithm for MCD\non the line, matching the lower bound. We then translate this algorithm to\nbecome a competitive optimal algorithm O(sqrt{log N}) for SRSA. Finally, we\ntranslate the latter back to solve MCD problem, this time competitive optimally\neven in the case that the number of requests is small (that is, O(min{sqrt{log\nn},sqrt{log N}})). We also present a Omega(sqrt[3]{log n}) lower bound on the\ncompetitiveness of any randomized algorithm. Some of the techniques may be\nuseful in other contexts.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2013 12:02:18 GMT"}], "update_date": "2013-07-12", "authors_parsed": [["Kantor", "Erez", ""], ["Kutten", "Shay", ""]]}, {"id": "1307.3207", "submitter": "Paulo S\\'ergio Almeida", "authors": "Paulo S\\'ergio Almeida, Carlos Baquero", "title": "Scalable Eventually Consistent Counters over Unreliable Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counters are an important abstraction in distributed computing, and play a\ncentral role in large scale geo-replicated systems, counting events such as web\npage impressions or social network \"likes\". Classic distributed counters,\nstrongly consistent, cannot be made both available and partition-tolerant, due\nto the CAP Theorem, being unsuitable to large scale scenarios. This paper\ndefines Eventually Consistent Distributed Counters (ECDC) and presents an\nimplementation of the concept, Handoff Counters, that is scalable and works\nover unreliable networks. By giving up the sequencer aspect of classic\ndistributed counters, ECDC implementations can be made AP in the CAP design\nspace, while retaining the essence of counting. Handoff Counters are the first\nCRDT (Conflict-free Replicated Data Type) based mechanism that overcomes the\nidentity explosion problem in naive CRDTs, such as G-Counters (where state size\nis linear in the number of independent actors that ever incremented the\ncounter), by managing identities towards avoiding global propagation and\ngarbage collecting temporary entries. The approach used in Handoff Counters is\nnot restricted to counters, being more generally applicable to other data types\nwith associative and commutative operations.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2013 18:28:06 GMT"}], "update_date": "2013-07-12", "authors_parsed": [["Almeida", "Paulo S\u00e9rgio", ""], ["Baquero", "Carlos", ""]]}, {"id": "1307.3306", "submitter": "Ashkan Paya Mr.", "authors": "Ashkan Paya and Dan C. Marinescu", "title": "Energy-aware Application Scaling on a Cloud", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud elasticity - the ability to use as much resources as needed at any\ngiven time - and low cost - a user pays only for the resources it consumes -\nrepresent solid incentives for many organizations to migrate some of their\ncomputational activities to a public cloud. As the interest in cloud computing\ngrows, so does the size of the cloud computing centers and their energy\nfootprint. The realization that power consumption of cloud computing centers is\nsignificant and it is expected to increase substantially in the future\nmotivates our interest in scheduling and scaling algorithms which minimize\npower consumption. We propose energy-aware application scaling and resource\nmanagement algorithms. Though targeting primarily the Infrastructure as a\nService (IaaS), the system models and the algorithms we propose can be applied\nto the other cloud delivery models and to private clouds.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2013 02:43:46 GMT"}], "update_date": "2013-07-15", "authors_parsed": [["Paya", "Ashkan", ""], ["Marinescu", "Dan C.", ""]]}, {"id": "1307.3396", "submitter": "Karnavel Kuppusamy", "authors": "R. Swaminathan, K. Karnavel", "title": "Software as a Service - Common Service Bus (SAAS-CSB)", "comments": null, "journal-ref": null, "doi": "10.5121/cseij.2013.3301", "report-no": null, "categories": "cs.DC cs.SE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Software-as-a-Service (SaaS) is a form of cloud computing that relieves the\nuser from the concern of hardware, software installation and management. It is\nan emerging business model that delivers software applications to the users\nthrough Web-based technology. Software vendors have varying requirements and\nSaaS applications most typically support such requirements. The various\napplications used by unique customers in a single instance are known as\nMulti-Tenancy. There would be a delay in service when the user sends the data\nfrom multiple applications to multiple destinations and from multiple\napplications to single destination due to the use of single CSB. This problem\ncan be overcome by using multiple CSB concepts and hence multiple senders can\nefficiently send their data to multiple receivers at the same time. The\nmultiple clouds are monitored and managed by the SaaS-CSB portal. The idea of\nSaaS-CSB Portal is to provide a single pane of glass for the user to consume\nand govern any service from any cloud. Thus, SaaS-CSB application allows\ncompanies to save their IT cost and valuable time.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2013 10:25:28 GMT"}], "update_date": "2013-07-15", "authors_parsed": [["Swaminathan", "R.", ""], ["Karnavel", "K.", ""]]}, {"id": "1307.3544", "submitter": "Bhavya Kailkhura", "authors": "Bhavya Kailkhura, Yunghsiang S. Han, Swastik Brahma, Pramod K.\n  Varshney", "title": "Distributed Bayesian Detection with Byzantine Data", "comments": "32 pages, 4 figures, Submitted to IEEE Transactions on Signal\n  Processing", "journal-ref": null, "doi": "10.1109/TSP.2015.2450191", "report-no": null, "categories": "cs.IT cs.CR cs.DC cs.GT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of distributed Bayesian detection in\nthe presence of Byzantines in the network. It is assumed that a fraction of the\nnodes in the network are compromised and reprogrammed by an adversary to\ntransmit false information to the fusion center (FC) to degrade detection\nperformance. The problem of distributed detection is formulated as a binary\nhypothesis test at the FC based on 1-bit data sent by the sensors. The\nexpression for minimum attacking power required by the Byzantines to blind the\nFC is obtained. More specifically, we show that above a certain fraction of\nByzantine attackers in the network, the detection scheme becomes completely\nincapable of utilizing the sensor data for detection. We analyze the problem\nunder different attacking scenarios and derive results for different\nnon-asymptotic cases. It is found that existing asymptotics-based results do\nnot hold under several non-asymptotic scenarios. When the fraction of\nByzantines is not sufficient to blind the FC, we also provide closed form\nexpressions for the optimal attacking strategies for the Byzantines that most\ndegrade the detection performance.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2013 19:28:00 GMT"}, {"version": "v2", "created": "Wed, 3 Sep 2014 15:56:46 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Kailkhura", "Bhavya", ""], ["Han", "Yunghsiang S.", ""], ["Brahma", "Swastik", ""], ["Varshney", "Pramod K.", ""]]}, {"id": "1307.3550", "submitter": "Karnavel Kuppusamy", "authors": "K. Karnavel, L. shalini, M. Ramananthini", "title": "Refining Data Security in Infrastructure Networks Support of Multipath\n  Routing", "comments": "arXiv admin note: substantial text overlap with arXiv:1307.3402", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.NI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  An infrastructure network is a self-organizing network with help of Access\nPoint (AP) of wireless links connecting nodes to another. The nodes can\ncommunicate without an ad hoc. They form an uninformed topology (BSS/ESS),\nwhere the nodes play the role of routers and are free to move randomly.\nInfrastructure networks proved their efficiency being used in different fields\nbut they are highly vulnerable to security attacks and dealing with this is one\nof the main challenges of these networks at present. In recent times some\nclarification are proposed to provide authentication, confidentiality,\navailability, secure routing and intrusion avoidance in infrastructure\nnetworks. Implementing security in such dynamically changing networks is a hard\ntask. Infrastructure network characteristics should be taken into consideration\nto be clever to design efficient solutions. Here we spotlight on civilizing the\nflow transmission privacy in infrastructure networks based on multipath\nrouting. Certainly, we take benefit of the being of multiple paths between\nnodes in an infrastructure network to increase the confidentiality robustness\nof transmitted data with the help of Access Point. In our approach the original\nmessage to secure is split into shares through access point that are encrypted\nand combined then transmitted along different disjointed existing paths between\nsender and receiver. Even if an intruder achieve something to get one or more\ntransmitted distribute the likelihood that the unique message will be\nreconstituted is very squat.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2013 10:42:08 GMT"}], "update_date": "2013-07-16", "authors_parsed": [["Karnavel", "K.", ""], ["shalini", "L.", ""], ["Ramananthini", "M.", ""]]}, {"id": "1307.4127", "submitter": "Atta ur Rehman Khan", "authors": "Atta ur Rehman Khan, Shahzad Ali, Saad Mustafa, Mazliza Othman", "title": "Impact of mobility models on clustering based routing protocols in\n  mobile WSNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents comparison of different hierarchical (position and\nnon-position based) protocols with respect to different mobility models.\nPrevious work mainly focuses on static networks or at most a single mobility\nmodel. Using only one mobility model may not predict the behavior of routing\nprotocol accurately. Simulation results show that mobility has large impact on\nthe behavior of WSN routing protocols. Also, position based routing protocols\nperforms better in terms of packet delivery compared to non position based\nrouting protocols.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2013 22:56:25 GMT"}], "update_date": "2013-07-17", "authors_parsed": [["Khan", "Atta ur Rehman", ""], ["Ali", "Shahzad", ""], ["Mustafa", "Saad", ""], ["Othman", "Mazliza", ""]]}, {"id": "1307.4129", "submitter": "Atta ur Rehman Khan", "authors": "Atta ur Rehman Khana, Sardar M. Bilalb, Mazliza Othmana", "title": "A Performance Comparison of Network Simulators for Wireless Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network simulation is the most useful and common methodology used to evaluate\ndifferent network to-pologies without real world implementation. Network\nsimulators are widely used by the research community to evaluate new theories\nand hypotheses. There are a number of network simulators, for instance, ns-2,\nns-3, OMNET++, SWAN, OPNET, Jist, and GloMoSiM etc. Therefore, the selection of\na network simulator for evaluating research work is a crucial task for\nresearchers. The main focus of this paper is to compare the state-of-the-art,\nopen source network simulators based on the following parameters: CPU\nutilization, memory usage, computational time, and scalability by simulating a\nMANET routing protocol, to identify an optimal network simulator for the\nresearch community.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2013 23:09:06 GMT"}], "update_date": "2013-07-17", "authors_parsed": [["Khana", "Atta ur Rehman", ""], ["Bilalb", "Sardar M.", ""], ["Othmana", "Mazliza", ""]]}, {"id": "1307.4259", "submitter": "Robert Gmyr", "authors": "Shlomi Dolev, Robert Gmyr, Andrea W. Richa, Christian Scheideler", "title": "Ameba-inspired Self-organizing Particle Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle systems are physical systems of simple computational particles that\ncan bond to neighboring particles and use these bonds to move from one spot to\nanother (non-occupied) spot. These particle systems are supposed to be able to\nself-organize in order to adapt to a desired shape without any central control.\nSelf-organizing particle systems have many interesting applications like\ncoating objects for monitoring and repair purposes and the formation of\nnano-scale devices for surgery and molecular-scale electronic structures. While\nthere has been quite a lot of systems work in this area, especially in the\ncontext of modular self-reconfigurable robotic systems, only very little\ntheoretical work has been done in this area so far. We attempt to bridge this\ngap by proposing a model inspired by the behavior of ameba that allows rigorous\nalgorithmic research on self-organizing particle systems.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2013 12:41:05 GMT"}], "update_date": "2013-07-17", "authors_parsed": [["Dolev", "Shlomi", ""], ["Gmyr", "Robert", ""], ["Richa", "Andrea W.", ""], ["Scheideler", "Christian", ""]]}, {"id": "1307.4567", "submitter": "Michael Lange", "authors": "Michael Lange and Gerard Gorman and Michele Weiland and Lawrence\n  Mitchell and Xiaohu Guo and James Southern", "title": "Benchmarking mixed-mode PETSc performance on high-performance\n  architectures", "comments": "arXiv admin note: substantial text overlap with arXiv:1303.5275", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The trend towards highly parallel multi-processing is ubiquitous in all\nmodern computer architectures, ranging from handheld devices to large-scale HPC\nsystems; yet many applications are struggling to fully utilise the multiple\nlevels of parallelism exposed in modern high-performance platforms. In order to\nrealise the full potential of recent hardware advances, a mixed-mode between\nshared-memory programming techniques and inter-node message passing can be\nadopted which provides high-levels of parallelism with minimal overheads. For\nscientific applications this entails that not only the simulation code itself,\nbut the whole software stack needs to evolve. In this paper, we evaluate the\nmixed-mode performance of PETSc, a widely used scientific library for the\nscalable solution of partial differential equations. We describe the addition\nof OpenMP threaded functionality to the library, focusing on sparse\nmatrix-vector multiplication. We highlight key challenges in achieving good\nparallel performance, such as explicit communication overlap using task-based\nparallelism, and show how to further improve performance by explicitly load\nbalancing threads within MPI processes. Using a set of matrices extracted from\nFluidity, a CFD application code which uses the library as its linear solver\nengine, we then benchmark the parallel performance of mixed-mode PETSc across\nmultiple nodes on several modern HPC architectures. We evaluate the parallel\nscalability on Uniform Memory Access (UMA) systems, such as the Fujitsu\nPRIMEHPC FX10 and IBM BlueGene/Q, as well as a Non-Uniform Memory Access (NUMA)\nCray XE6 platform. A detailed comparison is performed which highlights the\ncharacteristics of each particular architecture, before demonstrating efficient\nstrong scalability of sparse matrix-vector multiplication with significant\nspeedups over the pure-MPI mode.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2013 10:36:35 GMT"}], "update_date": "2013-07-19", "authors_parsed": [["Lange", "Michael", ""], ["Gorman", "Gerard", ""], ["Weiland", "Michele", ""], ["Mitchell", "Lawrence", ""], ["Guo", "Xiaohu", ""], ["Southern", "James", ""]]}, {"id": "1307.4716", "submitter": "Mehdi Bahrami", "authors": "Mehdi Bahrami", "title": "Cloud Template, a Big Data Solution", "comments": "JSCSE Journal Publication", "journal-ref": "International Journal of Soft Computing and Software Engineering\n  [JSCSE], Vol. 3, No. 2, pp. 13-16, 2013", "doi": "10.7321/jscse.v3.n2.2", "report-no": null, "categories": "cs.DC cs.NI cs.SE cs.SI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Today cloud computing has become as a new concept for hosting and delivering\ndifferent services over the Internet for big data solutions. Cloud computing is\nattractive to different business owners of both small and enterprise as it\neliminates the requirement for users to plan ahead for provisioning, and allows\nenterprises to start from the small and increase resources only when there is a\nrise in service demand. Despite the fact that cloud computing offers huge\nopportunities to the IT industry, the development of cloud computing technology\nis currently has several issues. This study presents an idea for introducing\ncloud templates which will be used for analyzing, designing, developing and\nimplementing cloud computing systems. We will present a template based design\nfor cloud computing systems, highlighting its key concepts, architectural\nprinciples and state of the art implementation, as well as research challenges\nand future work requirements. The aim of this idea is to provide a better\nunderstanding of the design challenges of cloud computing and identify\nimportant research directions in this big data increasingly important area. We\nwill describe a series of studies by which we and other researchers have\nassessed the effectiveness of these techniques in practical situations.\nFinally, in this study we will show how this idea could be implemented in a\npractical and useful way in industry.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2013 18:17:25 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2013 00:15:39 GMT"}], "update_date": "2013-08-06", "authors_parsed": [["Bahrami", "Mehdi", ""]]}, {"id": "1307.4731", "submitter": "Hari Sundar", "authors": "Jesse Kelly and Omar Ghattas and Hari Sundar", "title": "A Nested Partitioning Scheme for Parallel Heterogeneous Clusters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern supercomputers are increasingly requiring the presence of accelerators\nand co-processors. However, it has not been easy to achieve good performance on\nsuch heterogeneous clusters. The key challenge has been to ensure good load\nbalance and that neither the CPU nor the accelerator is left idle. Traditional\napproaches have offloaded entire computations to the accelerator, resulting in\nan idle CPU, or have opted for task-level parallelism requiring large data\ntransfers between the CPU and the accelerator. True work-parallelism has been\nhard as the Accelerators cannot directly communicate with other CPUs (besides\nthe host) and Accelerators. In this work, we present a new nested partition\nscheme to overcome this problem. By partitioning the work assignment on a given\nnode asymmetrically into boundary and interior work, and assigning the interior\nto the accelerator, we are able to achieve excellent efficiency while ensure\nproper utilization of both the CPU and Accelerator resources. The problem used\nfor evaluating the new partition is an $hp$ discontinuous Galerkin spectral\nelement method for a coupled elastic--acoustic wave propagation problem.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2013 19:17:32 GMT"}], "update_date": "2013-07-18", "authors_parsed": [["Kelly", "Jesse", ""], ["Ghattas", "Omar", ""], ["Sundar", "Hari", ""]]}, {"id": "1307.4839", "submitter": "Olivier Delestre", "authors": "St\\'ephane Cordier (MAPMO), H\\'el\\`ene Coullon (LIFO), Olivier\n  Delestre (JAD), Christian Laguerre (MAPMO), Minh Hoang Le (MAPMO), Daniel\n  Pierre, Georges Sadaka (LAMFA, MAP5)", "title": "FullSWOF_Paral: Comparison of two parallelization strategies (MPI and\n  SKELGIS) on a software designed for hydrology applications", "comments": "27 pages", "journal-ref": "ESAIM: Proc. 43 (2013) 59-79", "doi": "10.1051/proc/201343004", "report-no": null, "categories": "math.NA cs.DC cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we perform a comparison of two approaches for the\nparallelization of an existing, free software, FullSWOF 2D (http://www.\nuniv-orleans.fr/mapmo/soft/FullSWOF/ that solves shallow water equations for\napplications in hydrology) based on a domain decomposition strategy. The first\napproach is based on the classical MPI library while the second approach uses\nParallel Algorithmic Skeletons and more precisely a library named SkelGIS\n(Skeletons for Geographical Information Systems). The first results presented\nin this article show that the two approaches are similar in terms of\nperformance and scalability. The two implementation strategies are however very\ndifferent and we discuss the advantages of each one.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2013 06:45:12 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["Cordier", "St\u00e9phane", "", "MAPMO"], ["Coullon", "H\u00e9l\u00e8ne", "", "LIFO"], ["Delestre", "Olivier", "", "JAD"], ["Laguerre", "Christian", "", "MAPMO"], ["Le", "Minh Hoang", "", "MAPMO"], ["Pierre", "Daniel", "", "LAMFA, MAP5"], ["Sadaka", "Georges", "", "LAMFA, MAP5"]]}, {"id": "1307.5435", "submitter": "Arash Mohammadi ARASH MOHAMMADI", "authors": "Arash Mohammadi, Amir Asif, Xionghu Zhong, A.B. Premkumar", "title": "Distributed Computation of the Conditional PCRLB for Quantized\n  Decentralized Particle Filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conditional posterior Cramer-Rao lower bound (PCRLB) is an effective\nsensor resource management criteria for large, geographically distributed\nsensor networks. Existing algorithms for distributed computation of the PCRLB\n(dPCRLB) are based on raw observations leading to significant communication\noverhead to the estimation mechanism. This letter derives distributed\ncomputational techniques for determining the conditional dPCRLB for quantized,\ndecentralized sensor networks (CQ/dPCRLB). Analytical expressions for the\nCQ/dPCRLB are derived, which are particularly useful for particle filter-based\nestimators. The CQ/dPCRLB is compared for accuracy with its centralized\ncounterpart through Monte-Carlo simulations.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2013 16:16:11 GMT"}], "update_date": "2013-07-23", "authors_parsed": [["Mohammadi", "Arash", ""], ["Asif", "Amir", ""], ["Zhong", "Xionghu", ""], ["Premkumar", "A. B.", ""]]}, {"id": "1307.5442", "submitter": "Hong Xu", "authors": "Hong Xu and Baochun Li", "title": "Reducing Electricity Demand Charge for Data Centers with Partial\n  Execution", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data centers consume a large amount of energy and incur substantial\nelectricity cost. In this paper, we study the familiar problem of reducing data\ncenter energy cost with two new perspectives. First, we find, through an\nempirical study of contracts from electric utilities powering Google data\ncenters, that demand charge per kW for the maximum power used is a major\ncomponent of the total cost. Second, many services such as Web search tolerate\npartial execution of the requests because the response quality is a concave\nfunction of processing time. Data from Microsoft Bing search engine confirms\nthis observation.\n  We propose a simple idea of using partial execution to reduce the peak power\ndemand and energy cost of data centers. We systematically study the problem of\nscheduling partial execution with stringent SLAs on response quality. For a\nsingle data center, we derive an optimal algorithm to solve the workload\nscheduling problem. In the case of multiple geo-distributed data centers, the\ndemand of each data center is controlled by the request routing algorithm,\nwhich makes the problem much more involved. We decouple the two aspects, and\ndevelop a distributed optimization algorithm to solve the large-scale request\nrouting problem. Trace-driven simulations show that partial execution reduces\ncost by $3\\%--10.5\\%$ for one data center, and by $15.5\\%$ for geo-distributed\ndata centers together with request routing.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2013 17:19:37 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2013 09:31:31 GMT"}], "update_date": "2013-12-18", "authors_parsed": [["Xu", "Hong", ""], ["Li", "Baochun", ""]]}, {"id": "1307.5619", "submitter": "Uri Abraham", "authors": "Uri Abraham, Gal Amram", "title": "On the Mailbox Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The Mailbox Problem was described and solved by Aguilera, Gafni, and Lamport\nin their 2010 DC paper with an algorithm that uses two flag registers that\ncarry 14 values each. An interesting problem that they ask is whether there is\na mailbox algorithm with smaller flag values. We give a positive answer by\ndescribing a mailbox algorithm with 6 and 4 values in the two flag registers.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2013 08:27:31 GMT"}], "update_date": "2013-07-23", "authors_parsed": [["Abraham", "Uri", ""], ["Amram", "Gal", ""]]}, {"id": "1307.5894", "submitter": "Md Mansurul Bhuiyan", "authors": "Mansurul A Bhuiyan and Mohammad Al Hasan", "title": "MIRAGE: An Iterative MapReduce based FrequentSubgraph Mining Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frequent subgraph mining (FSM) is an important task for exploratory data\nanalysis on graph data. Over the years, many algorithms have been proposed to\nsolve this task. These algorithms assume that the data structure of the mining\ntask is small enough to fit in the main memory of a computer. However, as the\nreal-world graph data grows, both in size and quantity, such an assumption does\nnot hold any longer. To overcome this, some graph database-centric methods have\nbeen proposed in recent years for solving FSM; however, a distributed solution\nusing MapReduce paradigm has not been explored extensively. Since, MapReduce is\nbecoming the de- facto paradigm for computation on massive data, an efficient\nFSM algorithm on this paradigm is of huge demand. In this work, we propose a\nfrequent subgraph mining algorithm called MIRAGE which uses an iterative\nMapReduce based framework. MIRAGE is complete as it returns all the frequent\nsubgraphs for a given user-defined support, and it is efficient as it applies\nall the optimizations that the latest FSM algorithms adopt. Our experiments\nwith real life and large synthetic datasets validate the effectiveness of\nMIRAGE for mining frequent subgraphs from large graph datasets. The source code\nof MIRAGE is available from www.cs.iupui.edu/alhasan/software/\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2013 21:26:00 GMT"}], "update_date": "2013-07-24", "authors_parsed": [["Bhuiyan", "Mansurul A", ""], ["Hasan", "Mohammad Al", ""]]}, {"id": "1307.6066", "submitter": "Xuelin Shi", "authors": "Xuelin Shi, Ke Xu, JiangChuan Liu, Yong Wang", "title": "Continuous Double Auction Mechanism and Bidding Strategies in Cloud\n  Computing Markets", "comments": "16 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing has been an emerging model which aims at allowing customers\nto utilize computing resources hosted by Cloud Service Providers (CSPs). More\nand more consumers rely on CSPs to supply computing and storage service on the\none hand, and CSPs try to attract consumers on favorable terms on the other. In\nsuch competitive cloud computing markets, pricing policies are critical to\nmarket efficiency. While CSPs often publish their prices and charge users\naccording to the amount of resources they consume, auction mechanism is rarely\napplied. In fact a feasible auction mechanism is the most effective method for\nallocation of resources, especially double auction is more efficient and\nflexible for it enables buyers and sellers to enter bids and offers\nsimultaneously. In this paper we bring up an electronic auction platform for\ncloud, and a cloud Continuous Double Auction (CDA) mechanism is formulated to\nmatch orders and facilitate trading based on the platform. Some evaluating\ncriteria are defined to analyze the efficiency of markets and strategies.\nFurthermore, the selection of bidding strategies for the auction plays a very\nimportant role for each player to maximize its own profit, so we developed a\nnovel bidding strategy for cloud CDA, BH-strategy, which is a two-stage game\nbidding strategy. At last we designed three simulation scenarios to compare the\nperformance of our strategy with other dominating bidding strategies and proved\nthat BH-strategy has better performance on surpluses, successful transactions\nand market efficiency. In addition, we discussed that our cloud CDA mechanism\nis feasible for cloud computing resource allocation.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2013 13:28:56 GMT"}], "update_date": "2013-07-24", "authors_parsed": [["Shi", "Xuelin", ""], ["Xu", "Ke", ""], ["Liu", "JiangChuan", ""], ["Wang", "Yong", ""]]}, {"id": "1307.6209", "submitter": "Georg Hager", "authors": "Moritz Kreutzer, Georg Hager, Gerhard Wellein, Holger Fehske, Alan R.\n  Bishop", "title": "A unified sparse matrix data format for efficient general sparse\n  matrix-vector multiply on modern processors with wide SIMD units", "comments": "23 pages, 7 figures, 6 listings", "journal-ref": "SIAM Journal on Scientific Computing 2014 36:5, C401-C423", "doi": "10.1137/130930352", "report-no": null, "categories": "cs.MS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse matrix-vector multiplication (spMVM) is the most time-consuming kernel\nin many numerical algorithms and has been studied extensively on all modern\nprocessor and accelerator architectures. However, the optimal sparse matrix\ndata storage format is highly hardware-specific, which could become an obstacle\nwhen using heterogeneous systems. Also, it is as yet unclear how the wide\nsingle instruction multiple data (SIMD) units in current multi- and many-core\nprocessors should be used most efficiently if there is no structure in the\nsparsity pattern of the matrix. We suggest SELL-C-sigma, a variant of Sliced\nELLPACK, as a SIMD-friendly data format which combines long-standing ideas from\nGeneral Purpose Graphics Processing Units (GPGPUs) and vector computer\nprogramming. We discuss the advantages of SELL-C-sigma compared to established\nformats like Compressed Row Storage (CRS) and ELLPACK and show its suitability\non a variety of hardware platforms (Intel Sandy Bridge, Intel Xeon Phi and\nNvidia Tesla K20) for a wide range of test matrices from different application\nareas. Using appropriate performance models we develop deep insight into the\ndata transfer properties of the SELL-C-sigma spMVM kernel. SELL-C-sigma comes\nwith two tuning parameters whose performance impact across the range of test\nmatrices is studied and for which reasonable choices are proposed. This leads\nto a hardware-independent (\"catch-all\") sparse matrix format, which achieves\nvery high efficiency for all test matrices across all hardware platforms.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2013 12:50:28 GMT"}, {"version": "v2", "created": "Wed, 5 Mar 2014 14:54:58 GMT"}], "update_date": "2014-10-21", "authors_parsed": [["Kreutzer", "Moritz", ""], ["Hager", "Georg", ""], ["Wellein", "Gerhard", ""], ["Fehske", "Holger", ""], ["Bishop", "Alan R.", ""]]}, {"id": "1307.6574", "submitter": "Abhirup  Chakraborty", "authors": "Abhirup Chakraborty, Ajit Singh", "title": "Parallelizing Windowed Stream Joins in a Shared-Nothing Cluster", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of large number of processing nodes in a parallel and\ndistributed computing environment enables sophisticated real time processing\nover high speed data streams, as required by many emerging applications.\nSliding window stream joins are among the most important operators in a stream\nprocessing system. In this paper, we consider the issue of parallelizing a\nsliding window stream join operator over a shared nothing cluster. We propose a\nframework, based on fixed or predefined communication pattern, to distribute\nthe join processing loads over the shared-nothing cluster. We consider various\noverheads while scaling over a large number of nodes, and propose solution\nmethodologies to cope with the issues. We implement the algorithm over a\ncluster using a message passing system, and present the experimental results\nshowing the effectiveness of the join processing algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2013 20:30:07 GMT"}], "update_date": "2013-07-26", "authors_parsed": [["Chakraborty", "Abhirup", ""], ["Singh", "Ajit", ""]]}, {"id": "1307.6590", "submitter": "Egor Derevenetc", "authors": "Georgel Calin, Egor Derevenetc, Rupak Majumdar, Roland Meyer", "title": "A Theory of Partitioned Global Address Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partitioned global address space (PGAS) is a parallel programming model for\nthe development of applications on clusters. It provides a global address space\npartitioned among the cluster nodes, and is supported in programming languages\nlike C, C++, and Fortran by means of APIs. In this paper we provide a formal\nmodel for the semantics of single instruction, multiple data programs using\nPGAS APIs. Our model reflects the main features of popular real-world APIs such\nas SHMEM, ARMCI, GASNet, GPI, and GASPI.\n  A key feature of PGAS is the support for one-sided communication: a node may\ndirectly read and write the memory located at a remote node, without explicit\nsynchronization with the processes running on the remote side. One-sided\ncommunication increases performance by decoupling process synchronization from\ndata transfer, but requires the programmer to reason about appropriate\nsynchronizations between reads and writes. As a second contribution, we propose\nand investigate robustness, a criterion for correct synchronization of PGAS\nprograms. Robustness corresponds to acyclicity of a suitable happens-before\nrelation defined on PGAS computations. The requirement is finer than the\nclassical data race freedom and rules out most false error reports.\n  Our main result is an algorithm for checking robustness of PGAS programs. The\nalgorithm makes use of two insights. Using combinatorial arguments we first\nshow that, if a PGAS program is not robust, then there are computations in a\ncertain normal form that violate happens-before acyclicity. Intuitively,\nnormal-form computations delay remote accesses in an ordered way. We then\ndevise an algorithm that checks for cyclic normal-form computations.\nEssentially, the algorithm is an emptiness check for a novel automaton model\nthat accepts normal-form computations in streaming fashion. Altogether, we\nprove the robustness problem is PSpace-complete.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2013 21:24:57 GMT"}], "update_date": "2013-07-26", "authors_parsed": [["Calin", "Georgel", ""], ["Derevenetc", "Egor", ""], ["Majumdar", "Rupak", ""], ["Meyer", "Roland", ""]]}, {"id": "1307.6622", "submitter": "Hai Zhang", "authors": "Xia Liu, Li Fan", "title": "Priority-aware Gray-box Placement of Virtual Machines in Cloud Platforms", "comments": "submitted to Advances in Computer Science and its Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual machine (VM) placement is very important for cloud platforms. While\ntechniques, such as live virtual machine migration, are very useful to balance\nthe load in the data centers, they are expensive operations. In this position\npaper, we propose to minimize the chance of the load hot spots in the data\ncenter by applying the workload patterns of the VMs in the virtual machine\nplacement algorithms - place VMs that require a lot of same type of resource\nacross different physical servers. In this way, the resource competition of VMs\non the same physical server is significantly mitigated. Meanwhile, we also\nconsider the priorities of applications and VMs in our virtual machine\nplacement algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2013 01:47:32 GMT"}], "update_date": "2013-07-26", "authors_parsed": [["Liu", "Xia", ""], ["Fan", "Li", ""]]}, {"id": "1307.6638", "submitter": "Chetan Jhurani", "authors": "Chetan Jhurani, Travis M. Austin, Michael A. Heroux, James M.\n  Willenbring", "title": "Supporting 64-bit global indices in Epetra and other Trilinos packages\n  -- Techniques used and lessons learned", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Trilinos Project is an effort to facilitate the design, development,\nintegration and ongoing support of mathematical software libraries within an\nobject-oriented framework. It is intended for large-scale, complex multiphysics\nengineering and scientific applications. Epetra is one of its basic packages.\nIt provides serial and parallel linear algebra capabilities. Before Trilinos\nversion 11.0, released in 2012, Epetra used the C++ int data-type for storing\nglobal and local indices for degrees of freedom (DOFs). Since int is typically\n32-bit, this limited the largest problem size to be smaller than approximately\ntwo billion DOFs. This was true even if a distributed memory machine could\nhandle larger problems. We have added optional support for C++ long long\ndata-type, which is at least 64-bit wide, for global indices. To save memory,\nmaintain the speed of memory-bound operations, and reduce further changes to\nthe code, the local indices are still 32-bit. We document the changes required\nto achieve this feature and how the new functionality can be used. We also\nreport on the lessons learned in modifying a mature and popular package from\nvarious perspectives -- design goals, backward compatibility, engineering\ndecisions, C++ language features, effects on existing users and other packages,\nand build integration.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2013 06:52:00 GMT"}], "update_date": "2013-07-26", "authors_parsed": [["Jhurani", "Chetan", ""], ["Austin", "Travis M.", ""], ["Heroux", "Michael A.", ""], ["Willenbring", "James M.", ""]]}, {"id": "1307.6649", "submitter": "Dr. Rajesh Kumar  Tiwari", "authors": "Kumar Gunjan, R. K. Tiwari, G. Sahoo", "title": "Towards Securing APIs in Cloud Computing", "comments": "International Journal of Computer Engineering and Applications, June\n  2013. arXiv admin note: text overlap with arXiv:0901.0131 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every organisation today wants to adopt cloud computing paradigm and leverage\nits various advantages. Today everyone is aware of its characteristics which\nhave made it so popular and how it can help the organisations focus on their\ncore activities leaving all IT services development and maintenance to the\ncloud service providers. Application Programming Interfaces (APIs) act as the\ninterface between the CSPs and the consumers. This paper proposes an improved\naccess control mechanism for securing the Cloud APIs.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2013 07:41:44 GMT"}], "update_date": "2013-07-26", "authors_parsed": [["Gunjan", "Kumar", ""], ["Tiwari", "R. K.", ""], ["Sahoo", "G.", ""]]}, {"id": "1307.6747", "submitter": "Sebastian Kniesburges", "authors": "Sebastian Kniesburges, Andreas Koutsopoulos and Christian Scheideler", "title": "CONE-DHT: A distributed self-stabilizing algorithm for a heterogeneous\n  storage system", "comments": "Full version of DISC 2013 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of managing a dynamic heterogeneous storage system in\na distributed way so that the amount of data assigned to a host in that system\nis related to its capacity. Two central problems have to be solved for this:\n(1) organizing the hosts in an overlay network with low degree and diameter so\nthat one can efficiently check the correct distribution of the data and route\nbetween any two hosts, and (2) distributing the data among the hosts so that\nthe distribution respects the capacities of the hosts and can easily be adapted\nas the set of hosts or their capacities change. We present distributed\nprotocols for these problems that are self-stabilizing and that do not need any\nglobal knowledge about the system such as the number of nodes or the overall\ncapacity of the system. Prior to this work no solution was known satisfying\nthese properties.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2013 14:01:26 GMT"}], "update_date": "2013-07-26", "authors_parsed": [["Kniesburges", "Sebastian", ""], ["Koutsopoulos", "Andreas", ""], ["Scheideler", "Christian", ""]]}, {"id": "1307.7037", "submitter": "Drazen Lucanin MSc", "authors": "Dra\\v{z}en Lu\\v{c}anin, Ivona Brandi\\'c", "title": "Take a break: cloud scheduling optimized for real-time electricity\n  pricing", "comments": "8 pages, to appear at CGC 2013\n  (http://socialcloud.aifb.uni-karlsruhe.de/confs/CGC2013/)", "journal-ref": null, "doi": "10.1109/CGC.2013.25", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Cloud computing revolutionised the industry with its elastic, on-demand\napproach to computational resources, but has lead to a tremendous impact on the\nenvironment. Data centers constitute 1.1-1.5% of total electricity usage in the\nworld. Taking a more informed view of the electrical grid by analysing\nreal-time electricity prices, we set the foundations of a grid-conscious cloud.\nWe propose a scheduling algorithm that predicts electricity price peaks and\nthrottles energy consumption by pausing virtual machines. We evaluate the\napproach on the OpenStack cloud manager through an empirical approach and show\nreductions in energy consumption and costs. Finally, we define green instances\nin which cloud providers can offer such services to their customers under\nbetter pricing options.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2013 13:54:20 GMT"}], "update_date": "2014-01-22", "authors_parsed": [["Lu\u010danin", "Dra\u017een", ""], ["Brandi\u0107", "Ivona", ""]]}, {"id": "1307.7451", "submitter": "Hong Xu", "authors": "Hong Xu, Baochun Li", "title": "RepFlow: Minimizing Flow Completion Times with Replicated Flows in Data\n  Centers", "comments": "To appear in IEEE INFOCOM 2014", "journal-ref": null, "doi": "10.1109/INFOCOM.2014.6848094", "report-no": null, "categories": "cs.NI cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Short TCP flows that are critical for many interactive applications in data\ncenters are plagued by large flows and head-of-line blocking in switches.\nHash-based load balancing schemes such as ECMP aggravate the matter and result\nin long-tailed flow completion times (FCT). Previous work on reducing FCT\nusually requires custom switch hardware and/or protocol changes. We propose\nRepFlow, a simple yet practically effective approach that replicates each short\nflow to reduce the completion times, without any change to switches or host\nkernels. With ECMP the original and replicated flows traverse distinct paths\nwith different congestion levels, thereby reducing the probability of having\nlong queueing delay. We develop a simple analytical model to demonstrate the\npotential improvement of RepFlow. Extensive NS-3 simulations and Mininet\nimplementation show that RepFlow provides 50%--70% speedup in both mean and\n99-th percentile FCT for all loads, and offers near-optimal FCT when used with\nDCTCP.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2013 03:10:01 GMT"}, {"version": "v2", "created": "Sun, 8 Dec 2013 12:11:22 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Xu", "Hong", ""], ["Li", "Baochun", ""]]}, {"id": "1307.7563", "submitter": "Preetha Theresa Joy", "authors": "Preetha Theresa Joy and K. Poulose Jacob", "title": "Cooperative Caching Framework for Mobile Cloud Computing", "comments": null, "journal-ref": "Global Journal of Computer Science and Technology, Volume 13 Issue\n  8 Version 1.0 Year 2013 Network, Web & Security Volume 13 Issue 8 Version 1.0\n  Year 2013", "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the advancement in mobile devices and wireless networks mobile cloud\ncomputing, which combines mobile computing and cloud computing has gained\nmomentum since 2009. The characteristics of mobile devices and wireless network\nmakes the implementation of mobile cloud computing more complicated than for\nfixed clouds. This section lists some of the major issues in Mobile Cloud\nComputing. One of the key issues in mobile cloud computing is the end to end\ndelay in servicing a request. Data caching is o ne of the techniques widely\nused in wired and wireless networks to improve data access efficiency. In this\npaper we explore the possibility of a cooperative caching approach to enhance\ndata access efficiency in mobile cloud computing. The proposed approach is\nbased on cloudlets, one of the architecture designed for mobile cloud\ncomputing.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2013 12:57:54 GMT"}], "update_date": "2013-07-30", "authors_parsed": [["Joy", "Preetha Theresa", ""], ["Jacob", "K. Poulose", ""]]}, {"id": "1307.7751", "submitter": "Guoming Tang", "authors": "Guoming Tang, Kui Wu, Jingsheng Lei, Zhongqin Bi and Jiuyang Tang", "title": "From Landscape to Portrait: A New Approach for Outlier Detection in Load\n  Curve Data", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In power systems, load curve data is one of the most important datasets that\nare collected and retained by utilities. The quality of load curve data,\nhowever, is hard to guarantee since the data is subject to communication\nlosses, meter malfunctions, and many other impacts. In this paper, a new\napproach to analyzing load curve data is presented. The method adopts a new\nview, termed \\textit{portrait}, on the load curve data by analyzing the\nperiodic patterns in the data and re-organizing the data for ease of analysis.\nFurthermore, we introduce algorithms to build the virtual portrait load curve\ndata, and demonstrate its application on load curve data cleansing. Compared to\nexisting regression-based methods, our method is much faster and more accurate\nfor both small-scale and large-scale real-world datasets.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2013 21:59:30 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2013 17:22:07 GMT"}, {"version": "v3", "created": "Mon, 7 Apr 2014 19:17:23 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Tang", "Guoming", ""], ["Wu", "Kui", ""], ["Lei", "Jingsheng", ""], ["Bi", "Zhongqin", ""], ["Tang", "Jiuyang", ""]]}, {"id": "1307.7867", "submitter": "Daniel Ruprecht", "authors": "Robert Speck, Daniel Ruprecht, Matthew Emmett, Matthias Bolten, Rolf\n  Krause", "title": "A space-time parallel solver for the three-dimensional heat equation", "comments": "10 pages", "journal-ref": "Advances in Parallel Computing 25, IOS Press, pages 263 - 272,\n  2014", "doi": "10.3233/978-1-61499-381-0-263", "report-no": null, "categories": "cs.NA cs.DC math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a combination of the time-parallel \"parallel full\napproximation scheme in space and time\" (PFASST) with a parallel multigrid\nmethod (PMG) in space, resulting in a mesh-based solver for the\nthree-dimensional heat equation with a uniquely high degree of efficient\nconcurrency. Parallel scaling tests are reported on the Cray XE6 machine \"Monte\nRosa\" on up to 16,384 cores and on the IBM Blue Gene/Q system \"JUQUEEN\" on up\nto 65,536 cores. The efficacy of the combined spatial- and temporal\nparallelization is shown by demonstrating that using PFASST in addition to PMG\nsignificantly extends the strong-scaling limit. Implications of using spatial\ncoarsening strategies in PFASST's multi-level hierarchy in large-scale parallel\nsimulations are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 08:21:13 GMT"}, {"version": "v2", "created": "Mon, 14 Jul 2014 14:19:53 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Speck", "Robert", ""], ["Ruprecht", "Daniel", ""], ["Emmett", "Matthew", ""], ["Bolten", "Matthias", ""], ["Krause", "Rolf", ""]]}, {"id": "1307.7976", "submitter": "Christoph Lenzen", "authors": "Danny Dolev and Christoph Lenzen", "title": "Node-Initiated Byzantine Consensus Without a Common Clock", "comments": "19 pages, no figures; under submission to SODA 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of the literature on consensus assumes that protocols are\njointly started at all nodes of the distributed system. We show how to remove\nthis problematic assumption in semi-synchronous systems, where messages delays\nand relative drifts of local clocks may vary arbitrarily within known bounds.\nOur framework is self-stabilizing and efficient both in terms of communication\nand time; more concretely, compared to a synchronous start in a synchronous\nmodel of a non-self-stabilizing protocol, we achieve a constant-factor increase\nin the time and communicated bits to complete an instance, plus an additive\ncommunication overhead of O(n log n) broadcasted bits per time unit and node.\nThe latter can be further reduced, at an additive increase in time complexity.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 13:41:29 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2013 13:07:31 GMT"}], "update_date": "2013-08-01", "authors_parsed": [["Dolev", "Danny", ""], ["Lenzen", "Christoph", ""]]}, {"id": "1307.8049", "submitter": "Stefanie Jegelka", "authors": "Xinghao Pan, Joseph E. Gonzalez, Stefanie Jegelka, Tamara Broderick,\n  Michael I. Jordan", "title": "Optimistic Concurrency Control for Distributed Unsupervised Learning", "comments": "25 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on distributed machine learning algorithms has focused primarily on\none of two extremes - algorithms that obey strict concurrency constraints or\nalgorithms that obey few or no such constraints. We consider an intermediate\nalternative in which algorithms optimistically assume that conflicts are\nunlikely and if conflicts do arise a conflict-resolution protocol is invoked.\nWe view this \"optimistic concurrency control\" paradigm as particularly\nappropriate for large-scale machine learning algorithms, particularly in the\nunsupervised setting. We demonstrate our approach in three problem areas:\nclustering, feature learning and online facility location. We evaluate our\nmethods via large-scale experiments in a cluster computing environment.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 17:07:58 GMT"}], "update_date": "2013-07-31", "authors_parsed": [["Pan", "Xinghao", ""], ["Gonzalez", "Joseph E.", ""], ["Jegelka", "Stefanie", ""], ["Broderick", "Tamara", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1307.8228", "submitter": "Dr. Rajesh Kumar  Tiwari", "authors": "Abu Salim, Rajesh Kumar Tiwari, Sachin Tripathi", "title": "Addressing Security Challenges in Cloud Computing", "comments": "13 pages. International Journal of Computer Engineering and\n  Applications,April June 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing is a new computing paradigm which allows sharing of resources\non remote server such as hardware, network, storage using internet and provides\nthe way through which application, computing power, computing infrastructure\ncan be delivered to the user as a service. Cloud computing unique attribute\npromise cost effective Information Technology Solution (IT Solution) to the\nuser. All computing needs are provided by the Cloud Service Provider (CSP) and\nthey can be increased or decreased dynamically as required by the user. As data\nand Application are located at the server and may be beyond geographical\nboundary, this leads a number of concern from the user prospective. The\nobjective of this paper is to explore the key issues of cloud computing which\nis delaying its adoption.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2013 05:53:00 GMT"}], "update_date": "2013-08-01", "authors_parsed": [["Salim", "Abu", ""], ["Tiwari", "Rajesh Kumar", ""], ["Tripathi", "Sachin", ""]]}, {"id": "1307.8256", "submitter": "Sathya Peri", "authors": "Priyanka Kumar and Sathya Peri", "title": "Multi-Version Conflict Notion", "comments": "arXiv admin note: substantial text overlap with arXiv:1211.6315,\n  arXiv:1305.6624", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the useful notion of Multi-Version Conflict notion.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2013 09:17:23 GMT"}], "update_date": "2013-08-01", "authors_parsed": [["Kumar", "Priyanka", ""], ["Peri", "Sathya", ""]]}, {"id": "1307.8276", "submitter": "Francesca Lo Cicero", "authors": "Roberto Ammendola, Massimo Bernaschi, Andrea Biagioni, Mauro Bisson,\n  Massimiliano Fatica, Ottorino Frezza, Francesca Lo Cicero, Alessandro\n  Lonardo, Enrico Mastrostefano, Pier Stanislao Paolucci, Davide Rossetti,\n  Francesco Simula, Laura Tosoratto and Piero Vicini", "title": "GPU peer-to-peer techniques applied to a cluster interconnect", "comments": "paper accepted to CASS 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern GPUs support special protocols to exchange data directly across the\nPCI Express bus. While these protocols could be used to reduce GPU data\ntransmission times, basically by avoiding staging to host memory, they require\nspecific hardware features which are not available on current generation\nnetwork adapters. In this paper we describe the architectural modifications\nrequired to implement peer-to-peer access to NVIDIA Fermi- and Kepler-class\nGPUs on an FPGA-based cluster interconnect. Besides, the current software\nimplementation, which integrates this feature by minimally extending the RDMA\nprogramming model, is discussed, as well as some issues raised while employing\nit in a higher level API like MPI. Finally, the current limits of the technique\nare studied by analyzing the performance improvements on low-level benchmarks\nand on two GPU-accelerated applications, showing when and how they seem to\nbenefit from the GPU peer-to-peer method.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2013 10:46:17 GMT"}], "update_date": "2013-08-01", "authors_parsed": [["Ammendola", "Roberto", ""], ["Bernaschi", "Massimo", ""], ["Biagioni", "Andrea", ""], ["Bisson", "Mauro", ""], ["Fatica", "Massimiliano", ""], ["Frezza", "Ottorino", ""], ["Cicero", "Francesca Lo", ""], ["Lonardo", "Alessandro", ""], ["Mastrostefano", "Enrico", ""], ["Paolucci", "Pier Stanislao", ""], ["Rossetti", "Davide", ""], ["Simula", "Francesco", ""], ["Tosoratto", "Laura", ""], ["Vicini", "Piero", ""]]}]