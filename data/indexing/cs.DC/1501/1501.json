[{"id": "1501.00039", "submitter": "James Cusick", "authors": "James J. Cusick, William Miller, Nicholas Laurita, Tasha Pitt", "title": "Design, Construction, and Use of a Single Board Computer Beowulf\n  Cluster: Application of the Small-Footprint, Low-Cost, InSignal 5420 Octa\n  Board", "comments": "9 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years development in the area of Single Board Computing has been\nadvancing rapidly. At Wolters Kluwer's Corporate Legal Services Division a\nprototyping effort was undertaken to establish the utility of such devices for\npractical and general computing needs. This paper presents the background of\nthis work, the design and construction of a 64 core 96 GHz cluster, and their\npossibility of yielding approximately 400 GFLOPs from a set of small footprint\nInSignal boards created for just over $2,300. Additionally this paper discusses\nthe software environment on the cluster, the use of a standard Beowulf library\nand its operation, as well as other software application uses including Elastic\nSearch and ownCloud. Finally, consideration will be given to the future use of\nsuch technologies in a business setting in order to introduce new Open Source\ntechnologies, reduce computing costs, and improve Time to Market.\n  Index Terms: Single Board Computing, Raspberry Pi, InSignal Exynos 5420,\nLinaro Ubuntu Linux, High Performance Computing, Beowulf clustering, Open\nSource, MySQL, MongoDB, ownCloud, Computing Architectures, Parallel Computing,\nCluster Computing\n", "versions": [{"version": "v1", "created": "Tue, 30 Dec 2014 22:47:17 GMT"}, {"version": "v2", "created": "Mon, 5 Jan 2015 23:25:08 GMT"}], "update_date": "2015-01-07", "authors_parsed": [["Cusick", "James J.", ""], ["Miller", "William", ""], ["Laurita", "Nicholas", ""], ["Pitt", "Tasha", ""]]}, {"id": "1501.00048", "submitter": "Dimitrios Nikolopoulos", "authors": "Giorgis Georgakoudis, Charles J. Gillan, Ahmed Sayed, Ivor Spence,\n  Richard Faloon and Dimitrios S. Nikolopoulos", "title": "Methods and Metrics for Fair Server Assessment under Real-Time Financial\n  Workloads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy efficiency has been a daunting challenge for datacenters. The\nfinancial industry operates some of the largest datacenters in the world. With\nincreasing energy costs and the financial services sector growth, emerging\nfinancial analytics workloads may incur extremely high operational costs, to\nmeet their latency targets. Microservers have recently emerged as an\nalternative to high-end servers, promising scalable performance and low energy\nconsumption in datacenters via scale-out. Unfortunately, stark differences in\narchitectural features, form factor and design considerations make a fair\ncomparison between servers and microservers exceptionally challenging. In this\npaper we present a rigorous methodology and new metrics for fair comparison of\nserver and microserver platforms. We deploy our methodology and metrics to\ncompare a microserver with ARM cores against two servers with x86 cores,\nrunning the same real-time financial analytics workload. We define\nworkload-specific but platform-independent performance metrics for platform\ncomparison, targeting both datacenter operators and end users. Our methodology\nestablishes that a server based the Xeon Phi processor delivers the highest\nperformance and energy-efficiency. However, by scaling out energy-efficient\nmicroservers, we achieve competitive or better energy-efficiency than a\npower-equivalent server with two Sandy Bridge sockets despite the microserver's\nslower cores. Using a new iso-QoS (iso-Quality of Service) metric, we find that\nthe ARM microserver scales enough to meet market throughput demand, i.e. a 100%\nQoS in terms of timely option pricing, with as little as 55% of the energy\nconsumed by the Sandy Bridge server.\n", "versions": [{"version": "v1", "created": "Tue, 30 Dec 2014 23:56:53 GMT"}], "update_date": "2015-01-05", "authors_parsed": [["Georgakoudis", "Giorgis", ""], ["Gillan", "Charles J.", ""], ["Sayed", "Ahmed", ""], ["Spence", "Ivor", ""], ["Faloon", "Richard", ""], ["Nikolopoulos", "Dimitrios S.", ""]]}, {"id": "1501.00067", "submitter": "Xiaoming Liu", "authors": "Xiaoming Liu, Yadong Zhou, Xiaohong Guan", "title": "A Feasible Graph Partition Framework for Random Walks Implemented by\n  Parallel Computing in Big Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DC physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph partition is a fundamental problem of parallel computing for big graph\ndata. Many graph partition algorithms have been proposed to solve the problem\nin various applications, such as matrix computations and PageRank, etc., but\nnone has pay attention to random walks. Random walks is a widely used method to\nexplore graph structure in lots of fields. The challenges of graph partition\nfor random walks include the large number of times of communication between\npartitions, lots of replications of the vertices, unbalanced partition, etc. In\nthis paper, we propose a feasible graph partition framework for random walks\nimplemented by parallel computing in big graph. The framework is based on two\noptimization functions to reduce the bandwidth, memory and storage cost in the\ncondition that the load balance is guaranteed. In this framework, several\ngreedy graph partition algorithms are proposed. We also propose five metrics\nfrom different perspectives to evaluate the performance of these algorithms. By\nrunning the algorithms on the big graph data set of real world, the\nexperimental results show that these algorithms in the framework are capable of\nsolving the problem of graph partition for random walks for different needs,\ne.g. the best result is improved more than 70 times in reducing the times of\ncommunication.\n", "versions": [{"version": "v1", "created": "Wed, 31 Dec 2014 03:02:56 GMT"}], "update_date": "2015-01-05", "authors_parsed": [["Liu", "Xiaoming", ""], ["Zhou", "Yadong", ""], ["Guan", "Xiaohong", ""]]}, {"id": "1501.00182", "submitter": "Marcos Portnoi", "authors": "Marcos Portnoi, Jason Zurawsky, Martin Swany", "title": "An information services algorithm to heuristically summarize IP\n  addresses for a distributed, hierarchical directory service", "comments": "Grid Computing (GRID), 2010 11th IEEE/ACM International Conference\n  on, 25-28 Oct. 2010", "journal-ref": null, "doi": "10.1109/GRID.2010.5697950", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A distributed, hierarchical information service for computer networks might\nrely in several instances, located in different layers. A distributed directory\nservice, for example, might be comprised of upper level listings, and local\ndirectories. The upper level listings contain a compact version of the local\ndirectories. Clients desiring to access the information contained in local\ndirectories might first access the high-level listings, in order to locate the\nappropriate local instance. One of the keys for the competent operation of such\nservice is the ability of properly summarizing the information, which will be\nmaintained in the upper level directories. We analyze the case of the Lookup\nService in the Information Services plane of perfSONAR performance monitoring\ndistributed architecture, which implements IPv4 summarization in its functions.\nWe propose an empirical method, or heuristic, to achieve the summarizations,\nbased on the PATRICIA tree. We further apply the heuristic on a simulated\ndistributed test bed and contemplate the results.\n", "versions": [{"version": "v1", "created": "Wed, 31 Dec 2014 17:39:15 GMT"}, {"version": "v2", "created": "Wed, 7 Jan 2015 23:13:53 GMT"}], "update_date": "2015-01-09", "authors_parsed": [["Portnoi", "Marcos", ""], ["Zurawsky", "Jason", ""], ["Swany", "Martin", ""]]}, {"id": "1501.00513", "submitter": "Jehan-Francois Paris", "authors": "Jehan-Fran\\c{c}ois P\\^aris, Ahmed Amer, Darrell D. E. Long, Thomas J.\n  E. Schwarz", "title": "Self-Repairing Disk Arrays", "comments": "Part of ADAPT Workshop proceedings, 2015 (arXiv:1412.2347)", "journal-ref": null, "doi": null, "report-no": "ADAPT/2015/02", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the prices of magnetic storage continue to decrease, the cost of replacing\nfailed disks becomes increasingly dominated by the cost of the service call\nitself. We propose to eliminate these calls by building disk arrays that\ncontain enough spare disks to operate without any human intervention during\ntheir whole lifetime. To evaluate the feasibility of this approach, we have\nsimulated the behavior of two-dimensional disk arrays with n parity disks and\nn(n-1)/2 data disks under realistic failure and repair assumptions. Our\nconclusion is that having n(n+1)/2 spare disks is more than enough to achieve a\n99.999 percent probability of not losing data over four years. We observe that\nthe same objectives cannot be reached with RAID level 6 organizations and would\nrequire RAID stripes that could tolerate triple disk failures.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jan 2015 23:35:06 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["P\u00e2ris", "Jehan-Fran\u00e7ois", ""], ["Amer", "Ahmed", ""], ["Long", "Darrell D. E.", ""], ["Schwarz", "Thomas J. E.", ""]]}, {"id": "1501.00567", "submitter": "Lan Wang", "authors": "Lan Wang and Erol Gelenbe", "title": "Adaptive Dispatching of Tasks in the Cloud", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasingly wide application of Cloud Computing enables the\nconsolidation of tens of thousands of applications in shared infrastructures.\nThus, meeting the quality of service requirements of so many diverse\napplications in such shared resource environments has become a real challenge,\nespecially since the characteristics and workload of applications differ widely\nand may change over time. This paper presents an experimental system that can\nexploit a variety of online quality of service aware adaptive task allocation\nschemes, and three such schemes are designed and compared. These are a\nmeasurement driven algorithm that uses reinforcement learning, secondly a\n\"sensible\" allocation algorithm that assigns jobs to sub-systems that are\nobserved to provide a lower response time, and then an algorithm that splits\nthe job arrival stream into sub-streams at rates computed from the hosts'\nprocessing capabilities. All of these schemes are compared via measurements\namong themselves and with a simple round-robin scheduler, on two experimental\ntest-beds with homogeneous and heterogeneous hosts having different processing\ncapacities.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jan 2015 14:50:30 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Wang", "Lan", ""], ["Gelenbe", "Erol", ""]]}, {"id": "1501.00666", "submitter": "Evgeny Nikulchev", "authors": "Oleg Lukyanchikov, Evgeniy Pluzhnik, Simon Payain, Evgeny Nikulchev", "title": "Using Object-Relational Mapping to Create the Distributed Databases in a\n  Hybrid Cloud Infrastructure", "comments": null, "journal-ref": "International Journal of Advanced Computer Science and\n  Applications, 2014, 5(12):61-64", "doi": "10.14569/IJACSA.2014.051208", "report-no": null, "categories": "cs.DB cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the challenges currently problems in the use of cloud services is the\ntask of designing of specialized data management systems. This is especially\nimportant for hybrid systems in which the data are located in public and\nprivate clouds. Implementation monitoring functions querying, scheduling and\nprocessing software must be properly implemented and is an integral part of the\nsystem. To provide these functions is proposed to use an object-relational\nmapping (ORM). The article devoted to presenting the approach of designing\ndatabases for information systems hosted in a hybrid cloud infrastructure. It\nalso provides an example of the development of ORM library.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jan 2015 11:23:57 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Lukyanchikov", "Oleg", ""], ["Pluzhnik", "Evgeniy", ""], ["Payain", "Simon", ""], ["Nikulchev", "Evgeny", ""]]}, {"id": "1501.01070", "submitter": "Herald Kllapi", "authors": "Herald Kllapi, Panos Sakkos, Alex Delis, Dimitrios Gunopulos, Yannis\n  Ioannidis", "title": "Elastic Processing of Analytical Query Workloads on IaaS Clouds", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern applications require the evaluation of analytical queries on\nlarge amounts of data. Such queries entail joins and heavy aggregations that\noften include user-defined functions (UDFs). The most efficient way to process\nthese specific type of queries is using tree execution plans. In this work, we\ndevelop an engine for analytical query processing and a suite of specialized\ntechniques that collectively take advantage of the tree form of such plans. The\nengine executes these tree plans in an elastic IaaS cloud infrastructure and\ndynamically adapts by allocating and releasing pertinent resources based on the\nquery workload monitored over a sliding time window. The engine offers its\nservices for a fee according to service-level agreements (SLAs) associated with\nthe incoming queries; its management of cloud resources aims at maximizing the\nprofit after removing the costs of using these resources. We have fully\nimplemented our algorithms in the Exareme dataflow processing system. We\npresent an extensive evaluation that demonstrates that our approach is very\nefficient (exhibiting fast response times), elastic (successfully adjusting the\ncloud resources it uses as the engine continually adapts to query workload\nchanges), and profitable (approximating very well the maximum difference\nbetween SLA-based income and cloud-based expenses).\n", "versions": [{"version": "v1", "created": "Tue, 6 Jan 2015 03:38:45 GMT"}], "update_date": "2015-01-07", "authors_parsed": [["Kllapi", "Herald", ""], ["Sakkos", "Panos", ""], ["Delis", "Alex", ""], ["Gunopulos", "Dimitrios", ""], ["Ioannidis", "Yannis", ""]]}, {"id": "1501.01323", "submitter": "Mansaf  Alam", "authors": "Mansaf Alam and Kashish Ara Shakil", "title": "Recent Developments in Cloud Based Systems: State of Art", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing is the new buzzword in the head of the techies round the\nclock these days. The importance and the different applications of cloud\ncomputing are overwhelming and thus, it is a topic of huge significance. It\nprovides several astounding features like Multitenancy, on demand service, pay\nper use etc. This manuscript presents an exhaustive survey on cloud computing\ntechnology and potential research issues in cloud computing that needs to be\naddressed.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jan 2015 11:07:13 GMT"}], "update_date": "2015-01-08", "authors_parsed": [["Alam", "Mansaf", ""], ["Shakil", "Kashish Ara", ""]]}, {"id": "1501.01405", "submitter": "Jonathan Passerat-Palmbach", "authors": "Jonathan Passerat-Palmbach (ISIMA, LIMOS, UBP), Jonathan Caux (LIMOS,\n  UBP, IBC, ISIMA), Pridi Siregar (IBC), Claude Mazel (LIMOS, UBP, ISIMA),\n  David Hill (UBP, LIMOS, ISIMA)", "title": "Warp-Level Parallelism: Enabling Multiple Replications In Parallel on\n  GPU", "comments": "Best paper award. from European Simulation and Modelling, Oct 2011,\n  Guimaraes, Portugal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic simulations need multiple replications in order to build\nconfidence intervals for their results. Even if we do not need a large amount\nof replications, it is a good practice to speed-up the whole simulation time\nusing the Multiple Replications In Parallel (MRIP) approach. This approach\nusually supposes to have access to a parallel computer such as a symmetric\nmul-tiprocessing machine (with many cores), a computing cluster or a computing\ngrid. In this paper, we propose Warp-Level Parallelism (WLP), a GP-GPU-enabled\nsolution to compute MRIP on GP-GPUs (General-Purpose Graphics Processing\nUnits). These devices display a great amount of parallel computational power at\nlow cost, but are tuned to process efficiently the same operation on several\ndata, through different threads. Indeed, this paradigm is called Single\nInstruction, Multiple Threads (SIMT). Our approach proposes to rely on small\nthreads groups, called warps, to perform independent computations such as\nreplications. We have benchmarked WLP with three different models: it allows\nMRIP to be computed up to six times faster than with the SIMT computing\nparadigm.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jan 2015 09:15:48 GMT"}], "update_date": "2015-01-08", "authors_parsed": [["Passerat-Palmbach", "Jonathan", "", "ISIMA, LIMOS, UBP"], ["Caux", "Jonathan", "", "LIMOS,\n  UBP, IBC, ISIMA"], ["Siregar", "Pridi", "", "IBC"], ["Mazel", "Claude", "", "LIMOS, UBP, ISIMA"], ["Hill", "David", "", "UBP, LIMOS, ISIMA"]]}, {"id": "1501.01426", "submitter": "Mansaf Alam", "authors": "Mansaf Alam, Kashish Ara Shakil and Shuchi Sethi", "title": "Analysis and Clustering of Workload in Google Cluster Trace based on\n  Resource Usage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing has gained interest amongst commercial organizations,\nresearch communities, developers and other individuals during the past few\nyears.In order to move ahead with research in field of data management and\nprocessing of such data, we need benchmark datasets and freely available data\nwhich are publicly accessible. Google in May 2011 released a trace of a cluster\nof 11k machines referred as Google Cluster Trace.This trace contains cell\ninformation of about 29 days.This paper provides analysis of resource usage and\nrequirements in this trace and is an attempt to give an insight into such kind\nof production trace similar to the ones in cloud environment.The major\ncontributions of this paper include Statistical Profile of Jobs based on\nresource usage, clustering of Workload Patterns and Classification of jobs into\ndifferent types based on k-means clustering.Though there have been earlier\nworks for analysis of this trace, but our analysis provides several new\nfindings such as jobs in a production trace are trimodal and there occurs\nsymmetry in the tasks within a long job type\n", "versions": [{"version": "v1", "created": "Wed, 7 Jan 2015 10:15:05 GMT"}], "update_date": "2015-01-08", "authors_parsed": [["Alam", "Mansaf", ""], ["Shakil", "Kashish Ara", ""], ["Sethi", "Shuchi", ""]]}, {"id": "1501.01661", "submitter": "Yin Sun", "authors": "Yin Sun, Zizhan Zheng, C. Emre Koksal, Kyu-Han Kim, and Ness B. Shroff", "title": "Provably Delay Efficient Data Retrieving in Storage Clouds", "comments": "17 pages, 4 figures. This is the technical report for a conference\n  paper accepted by IEEE INFOCOM 2015", "journal-ref": null, "doi": "10.1109/INFOCOM.2015.7218426", "report-no": null, "categories": "cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One key requirement for storage clouds is to be able to retrieve data\nquickly. Recent system measurements have shown that the data retrieving delay\nin storage clouds is highly variable, which may result in a long latency tail.\nOne crucial idea to improve the delay performance is to retrieve multiple data\ncopies by using parallel downloading threads. However, how to optimally\nschedule these downloading threads to minimize the data retrieving delay\nremains to be an important open problem. In this paper, we develop\nlow-complexity thread scheduling policies for several important classes of data\ndownloading time distributions, and prove that these policies are either\ndelay-optimal or within a constant gap from the optimum delay performance.\nThese theoretical results hold for an arbitrary arrival process of read\nrequests that may contain finite or infinite read requests, and for\nheterogeneous MDS storage codes that can support diverse storage redundancy and\nreliability requirements for different data files. Our numerical results show\nthat the delay performance of the proposed policies is significantly better\nthan that of First-Come- First-Served (FCFS) policies considered in prior work.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jan 2015 21:57:08 GMT"}, {"version": "v2", "created": "Sat, 10 Jan 2015 19:39:50 GMT"}, {"version": "v3", "created": "Sat, 24 Jan 2015 10:02:30 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Sun", "Yin", ""], ["Zheng", "Zizhan", ""], ["Koksal", "C. Emre", ""], ["Kim", "Kyu-Han", ""], ["Shroff", "Ness B.", ""]]}, {"id": "1501.01678", "submitter": "Changwang Zhang", "authors": "Changwang Zhang, Shi Zhou, and Benjamin M. Chain", "title": "LeoTask: a fast, flexible and reliable framework for computational\n  research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LeoTask is a Java library for computation-intensive and time-consuming\nresearch tasks. It automatically executes tasks in parallel on multiple CPU\ncores on a computing facility. It uses a configuration file to enable automatic\nexploration of parameter space and flexible aggregation of results, and\ntherefore allows researchers to focus on programming the key logic of a\ncomputing task. It also supports reliable recovery from interruptions, dynamic\nand cloneable networks, and integration with the plotting software Gnuplot.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jan 2015 22:33:40 GMT"}], "update_date": "2015-01-09", "authors_parsed": [["Zhang", "Changwang", ""], ["Zhou", "Shi", ""], ["Chain", "Benjamin M.", ""]]}, {"id": "1501.02162", "submitter": "Ludovic Courtes", "authors": "Ludovic Court\\`es (INRIA Bordeaux - Sud-Ouest)", "title": "Design and Implementation of rowe, a Web-Friendly Communication Library", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The INDES project-team of Inria has been developing HOP, a multi-tier\nlanguage for Web programming. As part of the RAPP FP7 European project, the\nteam has set out to use HOP as the lingua franca of the robotics applications\ndeveloped within that project. Part of the challenge lies in the integration of\nexisting robotics code, written using ROS or custom libraries, with HOP-based\napplication.\n  This document reports on the implementation of rowe, a communication library\ndesigned the fill the gap between low-level robotics C components on one hand,\nand other C, C++, ROS, or HOP components on the other. The library aims to be a\nlightweight, high-performance, \"Web-friendly\" communication library. It\nimplements a socket-like interface that allows programs to exchange JSON\nobjects over WebSockets. We describe the rationale, design, and implementation\nof rowe.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jan 2015 14:53:00 GMT"}], "update_date": "2015-01-12", "authors_parsed": [["Court\u00e8s", "Ludovic", "", "INRIA Bordeaux - Sud-Ouest"]]}, {"id": "1501.02165", "submitter": "Matthieu Perrin", "authors": "Matthieu Perrin, Achour Mostefaoui, Claude Jard", "title": "Update Consistency for Wait-free Concurrent Objects", "comments": "appears in International Parallel and Distributed Processing\n  Symposium, May 2015, Hyderabad, India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In large scale systems such as the Internet, replicating data is an essential\nfeature in order to provide availability and fault-tolerance. Attiya and Welch\nproved that using strong consistency criteria such as atomicity is costly as\neach operation may need an execution time linear with the latency of the\ncommunication network. Weaker consistency criteria like causal consistency and\nPRAM consistency do not ensure convergence. The different replicas are not\nguaranteed to converge towards a unique state. Eventual consistency guarantees\nthat all replicas eventually converge when the participants stop updating.\nHowever, it fails to fully specify the semantics of the operations on shared\nobjects and requires additional non-intuitive and error-prone distributed\nspecification techniques. This paper introduces and formalizes a new\nconsistency criterion, called update consistency, that requires the state of a\nreplicated object to be consistent with a linearization of all the updates. In\nother words, whereas atomicity imposes a linearization of all of the\noperations, this criterion imposes this only on updates. Consequently some read\noperations may return out-dated values. Update consistency is stronger than\neventual consistency, so we can replace eventually consistent objects with\nupdate consistent ones in any program. Finally, we prove that update\nconsistency is universal, in the sense that any object can be implemented under\nthis criterion in a distributed system where any number of nodes may crash.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jan 2015 15:03:33 GMT"}], "update_date": "2015-01-12", "authors_parsed": [["Perrin", "Matthieu", ""], ["Mostefaoui", "Achour", ""], ["Jard", "Claude", ""]]}, {"id": "1501.02175", "submitter": "Matthieu Perrin", "authors": "Matthieu Perrin, Achour Most\\'efaoui, Claude Jard", "title": "Brief Announcement: Update Consistency in Partitionable Systems", "comments": "in DISC14 - 28th International Symposium on Distributed Computing,\n  Oct 2014, Austin, United States", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data replication is essential to ensure reliability, availability and\nfault-tolerance of massive distributed applications over large scale systems\nsuch as the Internet. However, these systems are prone to partitioning, which\nby Brewer's CAP theorem [1] makes it impossible to use a strong consistency\ncriterion like atomicity. Eventual consistency [2] guaranties that all replicas\neventually converge to a common state when the participants stop updating.\nHowever, it fails to fully specify shared objects and requires additional\nnon-intuitive and error-prone distributed specification techniques, that must\ntake into account all possible concurrent histories of updates to specify this\ncommon state [3]. This approach, that can lead to specifications as complicated\nas the implementations themselves, is limited by a more serious issue. The\nconcurrent specification of objects uses the notion of concurrent events. In\nmessage-passing systems, two events are concurrent if they are enforced by\ndifferent processes and each process enforced its event before it received the\nnotification message from the other process. In other words, the notion of\nconcurrency depends on the implementation of the object, not on its\nspecification. Consequently, the final user may not know if two events are\nconcurrent without explicitly tracking the messages exchanged by the processes.\nA specification should be independent of the system on which it is implemented.\nWe believe that an object should be totally specified by two facets: its\nabstract data type, that characterizes its sequential executions, and a\nconsistency criterion, that defines how it is supposed to behave in a\ndistributed environment. Not only sequential specification helps repeal the\nproblem of intention, it also allows to use the well studied and understood\nnotions of languages and automata. This makes possible to apply all the tools\ndeveloped for sequential systems, from their simple definition using structures\nand classes to the most advanced techniques like model checking and formal\nverification. Eventual consistency (EC) imposes no constraint on the convergent\nstate, that very few depends on the sequential specification. For example, an\nimplementation that ignores all the updates is eventually consistent, as all\nreplicas converge to the initial state. We propose a new consistency criterion,\nupdate consistency (UC), in which the convergent state must be obtained by a\ntotal ordering of the updates, that contains the sequential order of each\n", "versions": [{"version": "v1", "created": "Fri, 9 Jan 2015 15:31:40 GMT"}], "update_date": "2015-01-12", "authors_parsed": [["Perrin", "Matthieu", ""], ["Most\u00e9faoui", "Achour", ""], ["Jard", "Claude", ""]]}, {"id": "1501.02282", "submitter": "Miquel Moret\\'o Dr", "authors": "David Prat, Cristobal Ortega, Marc Casas, Miquel Moret\\'o, Mateo\n  Valero", "title": "Adaptive and application dependent runtime guided hardware prefetcher\n  reconfiguration on the IBM POWER7", "comments": "Part of ADAPT Workshop proceedings, 2015 (arXiv:1412.2347)", "journal-ref": null, "doi": null, "report-no": "ADAPT/2015/07", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hardware data prefetcher engines have been extensively used to reduce the\nimpact of memory latency. However, microprocessors' hardware prefetcher engines\ndo not include any automatic hardware control able to dynamically tune their\noperation. This lacking architectural feature causes systems to operate with\nprefetchers in a fixed configuration, which in many cases harms performance and\nenergy consumption.\n  In this paper, a piece of software that solves the discussed problem in the\ncontext of the IBM POWER7 microprocessor is presented. The proposed solution\ninvolves using the runtime software as a bridge that is able to characterize\nuser applications' workload and dynamically reconfigure the prefetcher engine.\nThe proposed mechanisms has been deployed over OmpSs, a state-of-the-art\ntask-based programming model. The paper shows significant performance\nimprovements over a representative set of microbenchmarks and High Performance\nComputing (HPC) applications.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jan 2015 22:05:22 GMT"}], "update_date": "2015-01-13", "authors_parsed": [["Prat", "David", ""], ["Ortega", "Cristobal", ""], ["Casas", "Marc", ""], ["Moret\u00f3", "Miquel", ""], ["Valero", "Mateo", ""]]}, {"id": "1501.02330", "submitter": "Huanle Xu Mr", "authors": "Huanle Xu and Wing Cheong Lau", "title": "Task-Cloning Algorithms in a MapReduce Cluster with Competitive\n  Performance Bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Job scheduling for a MapReduce cluster has been an active research topic in\nrecent years. However, measurement traces from real-world production\nenvironment show that the duration of tasks within a job vary widely. The\noverall elapsed time of a job, i.e. the so-called flowtime, is often dictated\nby one or few slowly-running tasks within a job, generally referred as the\n\"stragglers\". The cause of stragglers include tasks running on\npartially/intermittently failing machines or the existence of some localized\nresource bottleneck(s) within a MapReduce cluster. To tackle this online job\nscheduling challenge, we adopt the task cloning approach and design the\ncorresponding scheduling algorithms which aim at minimizing the weighted sum of\njob flowtimes in a MapReduce cluster based on the Shortest Remaining Processing\nTime scheduler (SRPT). To be more specific, we first design a 2-competitive\noffline algorithm when the variance of task-duration is negligible. We then\nextend this offline algorithm to yield the so-called SRPTMS+C algorithm for the\nonline case and show that SRPTMS+C is $(1+\\epsilon)-speed$\n$o(\\frac{1}{\\epsilon^2})-competitive$ in reducing the weighted sum of job\nflowtimes within a cluster. Both of the algorithms explicitly consider the\nprecedence constraints between the two phases within the MapReduce framework.\nWe also demonstrate via trace-driven simulations that SRPTMS+C can\nsignificantly reduce the weighted/unweighted sum of job flowtimes by cutting\ndown the elapsed time of small jobs substantially. In particular, SRPTMS+C\nbeats the Microsoft Mantri scheme by nearly 25% according to this metric.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jan 2015 10:55:41 GMT"}], "update_date": "2015-01-13", "authors_parsed": [["Xu", "Huanle", ""], ["Lau", "Wing Cheong", ""]]}, {"id": "1501.02398", "submitter": "Denis Shestakov", "authors": "Denis Shestakov, Diana Moise", "title": "Scalable high-dimensional indexing and searching with Hadoop", "comments": "This paper has been withdrawn by the authors. The manuscript has been\n  withdrawn as having no new material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While high-dimensional search-by-similarity techniques reached their maturity\nand in overall provide good performance, most of them are unable to cope with\nvery large multimedia collections. The 'big data' challenge however has to be\naddressed as multimedia collections have been explosively growing and will grow\neven faster than ever within the next few years. Luckily, computational\nprocessing power has become more available to researchers due to easier access\nto distributed grid infrastructures. In this paper, we show how\nhigh-dimensional indexing and searching methods can be used on scientific grid\nenvironments and present a scalable workflow for indexing and searching over 30\nbillion SIFT descriptors using a cluster running Hadoop. Besides its\nscalability, the proposed scheme not only provides good search quality, but\nalso achieves a stable throughput of around 210ms per image when searching a\n100M image collection. Our findings could help other researchers and\npractitioners to cope with huge multimedia collections.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jan 2015 22:05:45 GMT"}, {"version": "v2", "created": "Fri, 30 Jan 2015 00:56:17 GMT"}], "update_date": "2015-02-02", "authors_parsed": [["Shestakov", "Denis", ""], ["Moise", "Diana", ""]]}, {"id": "1501.02484", "submitter": "Jihun Hamm", "authors": "Jihun Hamm, Adam Champion, Guoxing Chen, Mikhail Belkin, Dong Xuan", "title": "Crowd-ML: A Privacy-Preserving Learning Framework for a Crowd of Smart\n  Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart devices with built-in sensors, computational capabilities, and network\nconnectivity have become increasingly pervasive. The crowds of smart devices\noffer opportunities to collectively sense and perform computing tasks in an\nunprecedented scale. This paper presents Crowd-ML, a privacy-preserving machine\nlearning framework for a crowd of smart devices, which can solve a wide range\nof learning problems for crowdsensing data with differential privacy\nguarantees. Crowd-ML endows a crowdsensing system with an ability to learn\nclassifiers or predictors online from crowdsensing data privately with minimal\ncomputational overheads on devices and servers, suitable for a practical and\nlarge-scale employment of the framework. We analyze the performance and the\nscalability of Crowd-ML, and implement the system with off-the-shelf\nsmartphones as a proof of concept. We demonstrate the advantages of Crowd-ML\nwith real and simulated experiments under various conditions.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jan 2015 18:57:28 GMT"}], "update_date": "2015-01-13", "authors_parsed": [["Hamm", "Jihun", ""], ["Champion", "Adam", ""], ["Chen", "Guoxing", ""], ["Belkin", "Mikhail", ""], ["Xuan", "Dong", ""]]}, {"id": "1501.02716", "submitter": "Peter Robinson", "authors": "Martin Biely and Peter Robinson and Ulrich Schmid and Manfred Schwarz\n  and Kyrill Winkler", "title": "Gracefully Degrading Consensus and $k$-Set Agreement in Directed Dynamic\n  Networks", "comments": "arXiv admin note: text overlap with arXiv:1204.0641", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study distributed agreement in synchronous directed dynamic networks,\nwhere an omniscient message adversary controls the availability of\ncommunication links. We prove that consensus is impossible under a message\nadversary that guarantees weak connectivity only, and introduce vertex-stable\nroot components (VSRCs) as a means for circumventing this impossibility: A\nVSRC(k, d) message adversary guarantees that, eventually, there is an interval\nof $d$ consecutive rounds where every communication graph contains at most $k$\nstrongly (dynamic) connected components consisting of the same processes, which\nhave at most outgoing links to the remaining processes. We present a consensus\nalgorithm that works correctly under a VSRC(1, 4H + 2) message adversary, where\n$H$ is the dynamic causal network diameter. On the other hand, we show that\nconsensus is impossible against a VSRC(1, H - 1) or a VSRC(2, $\\infty$) message\nadversary, revealing that there is not much hope to deal with stronger message\nadversaries.\n  However, we show that gracefully degrading consensus, which degrades to\ngeneral $k$-set agreement in case of unfavourable network conditions, is\nfeasible against stronger message adversaries: We provide a $k$-uniform $k$-set\nagreement algorithm, where the number of system-wide decision values $k$ is not\nencoded in the algorithm, but rather determined by the actual power of the\nmessage adversary in a run: Our algorithm guarantees at most $k$ decision\nvalues under a VSRC(n, d) + MAJINF(k) message adversary, which combines VSRC(n,\nd) (for some small $d$, ensuring termination) with some information flow\nguarantee MAJINF(k) between certain VSRCs (ensuring $k$-agreement). Our results\nprovide a significant step towards the exact solvability/impossibility border\nof general $k$-set agreement in directed dynamic networks.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jan 2015 16:54:52 GMT"}], "update_date": "2015-01-13", "authors_parsed": [["Biely", "Martin", ""], ["Robinson", "Peter", ""], ["Schmid", "Ulrich", ""], ["Schwarz", "Manfred", ""], ["Winkler", "Kyrill", ""]]}, {"id": "1501.02724", "submitter": "Balaji Subramaniam", "authors": "Balaji Subramaniam and Wu-chun Feng", "title": "Towards Energy-Proportional Computing Using Subsystem-Level Power\n  Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive data centers housing thousands of computing nodes have become\ncommonplace in enterprise computing, and the power consumption of such data\ncenters is growing at an unprecedented rate. Adding to the problem is the\ninability of the servers to exhibit energy proportionality, i.e., provide\nenergy-efficient execution under all levels of utilization, which diminishes\nthe overall energy efficiency of the data center. It is imperative that we\nrealize effective strategies to control the power consumption of the server and\nimprove the energy efficiency of data centers. With the advent of Intel Sandy\nBridge processors, we have the ability to specify a limit on power consumption\nduring runtime, which creates opportunities to design new power-management\ntechniques for enterprise workloads and make the systems that they run on more\nenergy-proportional.\n  In this paper, we investigate whether it is possible to achieve energy\nproportionality for enterprise-class server workloads, namely SPECpower_ssj2008\nand SPECweb2009 benchmarks, by using Intel's Running Average Power Limit (RAPL)\ninterfaces. First, we analyze the average power consumption of the full system\nas well as the subsystems and describe the energy proportionality of these\ncomponents. We then characterize the instantaneous power profile of these\nbenchmarks within different subsystems using the on-chip energy meters exposed\nvia the RAPL interfaces. Finally, we present the effects of power limiting on\nthe energy proportionality, performance, power and energy efficiency of\nenterprise-class server workloads. Our observations and results shed light on\nthe efficacy of the RAPL interfaces and provide guidance for designing\npower-management techniques for enterprise-class workloads.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jan 2015 17:18:17 GMT"}], "update_date": "2015-01-13", "authors_parsed": [["Subramaniam", "Balaji", ""], ["Feng", "Wu-chun", ""]]}, {"id": "1501.02729", "submitter": "Balaji Subramaniam", "authors": "Balaji Subramaniam and Wu-chun Feng", "title": "On the Energy Proportionality of Scale-Out Workloads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our increasing reliance on the cloud has led to the emergence of scale-out\nworkloads. These scale-out workloads are latency-sensitive as they are user\ndriven. In order to meet strict latency constraints, they require massive\ncomputing infrastructure, which consume significant amount of energy and\ncontribute to operational costs. This cost is further aggravated by the lack of\nenergy proportionality in servers. As Internet services become even more\nubiquitous, scale-out workloads will need increasingly larger cluster\ninstallations. As such, we desire an investigation into the energy\nproportionality and the mechanisms to improve the power consumption of\nscale-out workloads.\n  Therefore, in this paper, we study the energy proportionality and power\nconsumption of clusters in the context of scale-out workloads. Towards this\nend, we evaluate the potential of power and resource provisioning to improve\nthe energy proportionality for this class of workloads. Using data serving, web\nsearching and data caching as our representative workloads, we first analyze\nthe component-level power distribution on a cluster. Second, we characterize\nhow these workloads utilize the cluster. Third, we analyze the potential of\npower provisioning techniques (i.e., active low-power, turbo and idle low-power\nmodes) to improve the energy proportionality of scale-out workloads. We then\ndescribe the ability of active low-power modes to provide trade-offs in power\nand latency. Finally, we compare and contrast power provisioning and resource\nprovisioning techniques. Our study reveals various insights which will help\nimprove the energy proportionality and power consumption of scale-out\nworkloads.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jan 2015 17:32:17 GMT"}], "update_date": "2015-01-13", "authors_parsed": [["Subramaniam", "Balaji", ""], ["Feng", "Wu-chun", ""]]}, {"id": "1501.03064", "submitter": "David Castells-Rufas", "authors": "Francisco Corbera, Andr\\'es Rodr\\'iguez, Rafael Asenjo, Angeles\n  Navarro, Antonio Vilches, Maria Garzaran, Ismat Chaib Draa, Jamel Tayeb,\n  Smail Niar, Mikael Desertot, Daniel Gregorek, Robert Schmidt, Alberto\n  Garcia-Ortiz, Pedro Lopez-Garcia, R\\'emy Haemmerl\\'e, Maximiliano Klemen,\n  Umer Liqat, Manuel V. Hermenegildo, Radim Vav\\v{r}\\'ik, Albert Sa\\`a-Garriga,\n  David Castells-Rufas, Jordi Carrabina", "title": "Proceedings of the Workshop on High Performance Energy Efficient\n  Embedded Systems (HIP3ES) 2015", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Proceedings of the Workshop on High Performance Energy Efficient Embedded\nSystems (HIP3ES) 2015. Amsterdam, January 21st. Collocated with HIPEAC 2015\nConference.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jan 2015 16:29:18 GMT"}], "update_date": "2015-01-14", "authors_parsed": [["Corbera", "Francisco", ""], ["Rodr\u00edguez", "Andr\u00e9s", ""], ["Asenjo", "Rafael", ""], ["Navarro", "Angeles", ""], ["Vilches", "Antonio", ""], ["Garzaran", "Maria", ""], ["Draa", "Ismat Chaib", ""], ["Tayeb", "Jamel", ""], ["Niar", "Smail", ""], ["Desertot", "Mikael", ""], ["Gregorek", "Daniel", ""], ["Schmidt", "Robert", ""], ["Garcia-Ortiz", "Alberto", ""], ["Lopez-Garcia", "Pedro", ""], ["Haemmerl\u00e9", "R\u00e9my", ""], ["Klemen", "Maximiliano", ""], ["Liqat", "Umer", ""], ["Hermenegildo", "Manuel V.", ""], ["Vav\u0159\u00edk", "Radim", ""], ["Sa\u00e0-Garriga", "Albert", ""], ["Castells-Rufas", "David", ""], ["Carrabina", "Jordi", ""]]}, {"id": "1501.03105", "submitter": "David Gleich", "authors": "Yao Zhu and David F. Gleich", "title": "A Parallel Min-Cut Algorithm using Iteratively Reweighted Least Squares", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a parallel algorithm for the undirected $s,t$-mincut problem with\nfloating-point valued weights. Our overarching algorithm uses an iteratively\nreweighted least squares framework. This generates a sequence of Laplacian\nlinear systems, which we solve using parallel matrix algorithms. Our overall\nimplementation is up to 30-times faster than a serial solver when using 128\ncores.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jan 2015 18:48:27 GMT"}], "update_date": "2015-01-14", "authors_parsed": [["Zhu", "Yao", ""], ["Gleich", "David F.", ""]]}, {"id": "1501.03336", "submitter": "Rafael Asenjo", "authors": "Francisco Corbera, Andr\\'es Rodr\\'iguez, Rafael Asenjo, Angeles\n  Navarro, Antonio Vilches, Mar\\'ia J. Garzar\\'an", "title": "Reducing overheads of dynamic scheduling on heterogeneous chips", "comments": "Presented at HIP3ES, 2015 (arXiv: 1501.03064)", "journal-ref": null, "doi": null, "report-no": "HIP3ES/2015/01", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent processor development, we have witnessed the integration of GPU and\nCPUs into a single chip. The result of this integration is a reduction of the\ndata communication overheads. This enables an efficient collaboration of both\ndevices in the execution of parallel workloads.\n  In this work, we focus on the problem of efficiently scheduling chunks of\niterations of parallel loops among the computing devices on the chip (the GPU\nand the CPU cores) in the context of irregular applications. In particular, we\nanalyze the sources of overhead that the host thread experiments when a chunk\nof iterations is offloaded to the GPU while other threads are executing\nconcurrently other chunks on the CPU cores. We carefully study these overheads\non different processor architectures and operating systems using Barnes Hut as\na study case representative of irregular applications. We also propose a set of\noptimizations to mitigate the overheads that arise in presence of\noversubscription and take advantage of the different features of the\nheterogeneous architectures. Thanks to these optimizations we reduce\nEnergy-Delay Product (EDP) by 18% and 84% on Intel Ivy Bridge and Haswell\narchitectures, respectively, and by 57% on the Exynos big.LITTLE.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jan 2015 13:04:51 GMT"}], "update_date": "2016-08-07", "authors_parsed": [["Corbera", "Francisco", ""], ["Rodr\u00edguez", "Andr\u00e9s", ""], ["Asenjo", "Rafael", ""], ["Navarro", "Angeles", ""], ["Vilches", "Antonio", ""], ["Garzar\u00e1n", "Mar\u00eda J.", ""]]}, {"id": "1501.03481", "submitter": "Ahmed Sayed", "authors": "Giorgis Georgakoudis, Charles J. Gillan, Ahmed Sayed, Ivor Spence,\n  Richard Faloon, and Dimitrios S. Nikolopoulos", "title": "Iso-Quality of Service: Fairly Ranking Servers for Real-Time Data\n  Analytics", "comments": "12 pages, 5 figures, 8 tables, 6 equations. arXiv admin note: text\n  overlap with arXiv:1501.00048", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a mathematically rigorous Quality-of-Service (QoS) metric which\nrelates the achievable quality of service metric (QoS) for a real-time\nanalytics service to the server energy cost of offering the service. Using a\nnew iso-QoS evaluation methodology, we scale server resources to meet QoS\ntargets and directly rank the servers in terms of their energy-efficiency and\nby extension cost of ownership. Our metric and method are platform-independent\nand enable fair comparison of datacenter compute servers with significant\narchitectural diversity, including micro-servers. We deploy our metric and\nmethodology to compare three servers running financial option pricing workloads\non real-life market data. We find that server ranking is sensitive to data\ninputs and desired QoS level and that although scale-out micro-servers can be\nup to two times more energy-efficient than conventional heavyweight servers for\nthe same target QoS, they are still six times less energy efficient than\nhigh-performance computational accelerators.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jan 2015 20:42:07 GMT"}], "update_date": "2015-01-15", "authors_parsed": [["Georgakoudis", "Giorgis", ""], ["Gillan", "Charles J.", ""], ["Sayed", "Ahmed", ""], ["Spence", "Ivor", ""], ["Faloon", "Richard", ""], ["Nikolopoulos", "Dimitrios S.", ""]]}, {"id": "1501.03610", "submitter": "Tom Z. J. Fu", "authors": "Tom Z. J. Fu, Jianbing Ding, Richard T. B. Ma, Marianne Winslett, Yin\n  Yang, Zhenjie Zhang", "title": "DRS: Dynamic Resource Scheduling for Real-Time Analytics over Fast\n  Streams", "comments": "This is the our latest version with certain modification", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a data stream management system (DSMS), users register continuous queries,\nand receive result updates as data arrive and expire. We focus on applications\nwith real-time constraints, in which the user must receive each result update\nwithin a given period after the update occurs. To handle fast data, the DSMS is\ncommonly placed on top of a cloud infrastructure. Because stream properties\nsuch as arrival rates can fluctuate unpredictably, cloud resources must be\ndynamically provisioned and scheduled accordingly to ensure real-time response.\nIt is quite essential, for the existing systems or future developments, to\npossess the ability of scheduling resources dynamically according to the\ncurrent workload, in order to avoid wasting resources, or failing in delivering\ncorrect results on time. Motivated by this, we propose DRS, a novel dynamic\nresource scheduler for cloud-based DSMSs. DRS overcomes three fundamental\nchallenges: (a) how to model the relationship between the provisioned resources\nand query response time (b) where to best place resources; and (c) how to\nmeasure system load with minimal overhead. In particular, DRS includes an\naccurate performance model based on the theory of \\emph{Jackson open queueing\nnetworks} and is capable of handling \\emph{arbitrary} operator topologies,\npossibly with loops, splits and joins. Extensive experiments with real data\nconfirm that DRS achieves real-time response with close to optimal resource\nconsumption.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jan 2015 09:37:32 GMT"}, {"version": "v2", "created": "Sat, 31 Jan 2015 07:59:22 GMT"}, {"version": "v3", "created": "Thu, 23 Apr 2015 09:48:22 GMT"}], "update_date": "2015-04-24", "authors_parsed": [["Fu", "Tom Z. J.", ""], ["Ding", "Jianbing", ""], ["Ma", "Richard T. B.", ""], ["Winslett", "Marianne", ""], ["Yang", "Yin", ""], ["Zhang", "Zhenjie", ""]]}, {"id": "1501.03619", "submitter": "Tom Z. J. Fu", "authors": "Jianbing Ding, Tom Z. J. Fu, Richard T. B. Ma, Marianne Winslett, Yin\n  Yang, Zhenjie Zhang, Hongyang Chao", "title": "Optimal Operator State Migration for Elastic Data Stream Processing", "comments": "The latest version with a few modifications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  A cloud-based data stream management system (DSMS) handles fast data by\nutilizing the massively parallel processing capabilities of the underlying\nplatform. An important property of such a DSMS is elasticity, meaning that\nnodes can be dynamically added to or removed from an application to match the\nlatter's workload, which may fluctuate in an unpredictable manner. For an\napplication involving stateful operations such as aggregates, the addition /\nremoval of nodes necessitates the migration of operator states. Although the\nimportance of migration has been recognized in existing systems, two key\nproblems remain largely neglected, namely how to migrate and what to migrate,\ni.e., the migration mechanism that reduces synchronization overhead and result\ndelay during migration, and the selection of the optimal task assignment that\nminimizes migration costs. Consequently, migration in current systems typically\nincurs a high spike in result delay caused by expensive synchronization\nbarriers and suboptimal task assignments. Motivated by this, we present the\nfirst comprehensive study on efficient operator states migration, and propose\ndesigns and algorithms that enable live, progressive, and optimized migrations.\nExtensive experiments using real data justify our performance claims.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jan 2015 10:11:17 GMT"}, {"version": "v2", "created": "Fri, 16 Jan 2015 17:47:42 GMT"}, {"version": "v3", "created": "Sat, 31 Jan 2015 07:59:55 GMT"}, {"version": "v4", "created": "Wed, 4 Feb 2015 17:03:49 GMT"}, {"version": "v5", "created": "Mon, 27 Apr 2015 02:11:50 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Ding", "Jianbing", ""], ["Fu", "Tom Z. J.", ""], ["Ma", "Richard T. B.", ""], ["Winslett", "Marianne", ""], ["Yang", "Yin", ""], ["Zhang", "Zhenjie", ""], ["Chao", "Hongyang", ""]]}, {"id": "1501.04343", "submitter": "Xiaohu Wu", "authors": "Xiaohu Wu, and Patrick Loiseau", "title": "Algorithms for Scheduling Malleable Cloud Tasks", "comments": "The conference version of this manuscript appeared at the 53rd Annual\n  Allerton Conference on Communication, Control, and Computing, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the ubiquity of batch data processing in cloud computing, the related\nproblem of scheduling malleable batch tasks and its extensions have received\nsignificant attention recently. In this paper, we consider a fundamental model\nwhere a set of n tasks is to be processed on C identical machines and each task\nis specified by a value, a workload, a deadline and a parallelism bound. Within\nthe parallelism bound, the number of machines assigned to a task can vary over\ntime without affecting its workload. For this model, we obtain two core\nresults: a sufficient and necessary condition such that a set of tasks can be\nfinished by their deadlines on C machines, and an algorithm to produce such a\nschedule. These core results provide a conceptual tool and an optimal\nscheduling algorithm that enable proposing new algorithmic analysis and design\nand improving existing algorithms under various objectives.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jan 2015 20:18:15 GMT"}, {"version": "v2", "created": "Tue, 20 Jan 2015 17:41:51 GMT"}, {"version": "v3", "created": "Tue, 7 Jul 2015 03:47:37 GMT"}, {"version": "v4", "created": "Tue, 1 Sep 2015 04:50:48 GMT"}, {"version": "v5", "created": "Wed, 17 Aug 2016 12:13:13 GMT"}, {"version": "v6", "created": "Sun, 29 Jan 2017 16:06:51 GMT"}, {"version": "v7", "created": "Tue, 31 Jan 2017 23:54:14 GMT"}, {"version": "v8", "created": "Sun, 4 Feb 2018 19:58:30 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Wu", "Xiaohu", ""], ["Loiseau", "Patrick", ""]]}, {"id": "1501.04473", "submitter": "Mansaf Alam Dr", "authors": "Shuchi Sethi, Kashish Ara Shakil and Mansaf Alam", "title": "Seeking Black Lining In Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is focused on attacks on confidentiality that require time\nsynchronization. This manuscript proposes a detection framework for covert\nchannel perspective in cloud security. This problem is interpreted as a binary\nclassification problem and the algorithm proposed is based on certain features\nthat emerged after data analysis of Google cluster trace that forms base for\nanalyzing attack free data. This approach can be generalized to study the flow\nof other systems and fault detection. The detection framework proposed does not\nmake assumptions pertaining to data distribution as a whole making it suitable\nto meet cloud dynamism.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jan 2015 12:33:55 GMT"}], "update_date": "2015-01-20", "authors_parsed": [["Sethi", "Shuchi", ""], ["Shakil", "Kashish Ara", ""], ["Alam", "Mansaf", ""]]}, {"id": "1501.04504", "submitter": "Xiangyao Yu", "authors": "Xiangyao Yu, Srinivas Devadas", "title": "TARDIS: Timestamp based Coherence Algorithm for Distributed Shared\n  Memory", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new memory coherence protocol, Tardis, is proposed. Tardis uses timestamp\ncounters representing logical time as well as physical time to order memory\noperations and enforce sequential consistency in any type of shared memory\nsystem. Tardis is unique in that as compared to the widely-adopted directory\ncoherence protocol, and its variants, it completely avoids multicasting and\nonly requires O(log N) storage per cache block for an N-core system rather than\nO(N) sharer information. Tardis is simpler and easier to reason about, yet\nachieves similar performance to directory protocols on a wide range of\nbenchmarks run on 16, 64 and 256 cores.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jan 2015 14:38:00 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2015 22:04:50 GMT"}], "update_date": "2015-09-25", "authors_parsed": [["Yu", "Xiangyao", ""], ["Devadas", "Srinivas", ""]]}, {"id": "1501.04552", "submitter": "Benson Muite", "authors": "S. Aseeri and O. Batra\\v{s}ev and M. Icardi and B. Leu and A. Liu and\n  N. Li and B.K. Muite and E. M\\\"uller and B. Palen and M. Quell and H. Servat\n  and P. Sheth and R. Speck and M. Van Moer and J. Vienne", "title": "Solving the Klein-Gordon equation using Fourier spectral methods: A\n  benchmark test for computer performance", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cubic Klein-Gordon equation is a simple but non-trivial partial\ndifferential equation whose numerical solution has the main building blocks\nrequired for the solution of many other partial differential equations. In this\nstudy, the library 2DECOMP&FFT is used in a Fourier spectral scheme to solve\nthe Klein-Gordon equation and strong scaling of the code is examined on\nthirteen different machines for a problem size of 512^3. The results are useful\nin assessing likely performance of other parallel fast Fourier transform based\nprograms for solving partial differential equations. The problem is chosen to\nbe large enough to solve on a workstation, yet also of interest to solve\nquickly on a supercomputer, in particular for parametric studies. Unlike other\nhigh performance computing benchmarks, for this problem size, the time to\nsolution will not be improved by simply building a bigger supercomputer.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jan 2015 16:48:00 GMT"}], "update_date": "2015-01-20", "authors_parsed": [["Aseeri", "S.", ""], ["Batra\u0161ev", "O.", ""], ["Icardi", "M.", ""], ["Leu", "B.", ""], ["Liu", "A.", ""], ["Li", "N.", ""], ["Muite", "B. K.", ""], ["M\u00fcller", "E.", ""], ["Palen", "B.", ""], ["Quell", "M.", ""], ["Servat", "H.", ""], ["Sheth", "P.", ""], ["Speck", "R.", ""], ["Van Moer", "M.", ""], ["Vienne", "J.", ""]]}, {"id": "1501.04557", "submitter": "Antoni Portero", "authors": "Radim Vav\\v{r}\\'ik, Antoni Portero, \\v{S}t\\v{e}p\\'an Kucha\\v{r},\n  Martin Golasowski, Simone Libutti, Giuseppe Massari, William Fornaciari,\n  V\\'it Vondr\\'ak", "title": "Precision-Aware application execution for Energy-optimization in HPC\n  node system", "comments": "Presented at HIP3ES, 2015 (arXiv: 1501.03064)", "journal-ref": null, "doi": null, "report-no": "HIP3ES/2015/05", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power consumption is a critical consideration in high performance computing\nsystems and it is becoming the limiting factor to build and operate Petascale\nand Exascale systems. When studying the power consumption of existing systems\nrunning HPC workloads, we find that power, energy and performance are closely\nrelated which leads to the possibility to optimize energy consumption without\nsacrificing (much or at all) the performance. In this paper, we propose a HPC\nsystem running with a GNU/Linux OS and a Real Time Resource Manager (RTRM) that\nis aware and monitors the healthy of the platform. On the system, an\napplication for disaster management runs. The application can run with\ndifferent QoS depending on the situation. We defined two main situations.\nNormal execution, when there is no risk of a disaster, even though we still\nhave to run the system to look ahead in the near future if the situation\nchanges suddenly. In the second scenario, the possibilities for a disaster are\nvery high. Then the allocation of more resources for improving the precision\nand the human decision has to be taken into account. The paper shows that at\ndesign time, it is possible to describe different optimal points that are going\nto be used at runtime by the RTOS with the application. This environment helps\nto the system that must run 24/7 in saving energy with the trade-off of losing\nprecision. The paper shows a model execution which can improve the precision of\nresults by 65% in average by increasing the number of iterations from 1e3 to\n1e4. This also produces one order of magnitude longer execution time which\nleads to the need to use a multi-node solution. The optimal trade-off between\nprecision vs. execution time is computed by the RTOS with the time overhead\nless than 10% against a native execution.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jan 2015 16:56:10 GMT"}, {"version": "v2", "created": "Mon, 26 Jan 2015 13:34:07 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Vav\u0159\u00edk", "Radim", ""], ["Portero", "Antoni", ""], ["Kucha\u0159", "\u0160t\u011bp\u00e1n", ""], ["Golasowski", "Martin", ""], ["Libutti", "Simone", ""], ["Massari", "Giuseppe", ""], ["Fornaciari", "William", ""], ["Vondr\u00e1k", "V\u00edt", ""]]}, {"id": "1501.04784", "submitter": "Francisco Javier Ramirez Gil", "authors": "Francisco Javier Ram\\'irez-Gil, Marcos de Sales Guerra Tsuzuki and\n  Wilfredo Montealegre-Rubio", "title": "Global finite element matrix construction based on a CPU-GPU\n  implementation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CE cs.DC cs.PF math.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The finite element method (FEM) has several computational steps to\nnumerically solve a particular problem, to which many efforts have been\ndirected to accelerate the solution stage of the linear system of equations.\nHowever, the finite element matrix construction, which is also time-consuming\nfor unstructured meshes, has been less investigated. The generation of the\nglobal finite element matrix is performed in two steps, computing the local\nmatrices by numerical integration and assembling them into a global system,\nwhich has traditionally been done in serial computing. This work presents a\nfast technique to construct the global finite element matrix that arises by\nsolving the Poisson's equation in a three-dimensional domain. The proposed\nmethodology consists in computing the numerical integration, due to its\nintrinsic parallel opportunities, in the graphics processing unit (GPU) and\ncomputing the matrix assembly, due to its intrinsic serial operations, in the\ncentral processing unit (CPU). In the numerical integration, only the lower\ntriangular part of each local stiffness matrix is computed thanks to its\nsymmetry, which saves GPU memory and computing time. As a result of symmetry,\nthe global sparse matrix also contains non-zero elements only in its lower\ntriangular part, which reduces the assembly operations and memory usage. This\nmethodology allows generating the global sparse matrix from any unstructured\nfinite element mesh size on GPUs with little memory capacity, only limited by\nthe CPU memory.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jan 2015 12:41:28 GMT"}], "update_date": "2015-01-21", "authors_parsed": [["Ram\u00edrez-Gil", "Francisco Javier", ""], ["Tsuzuki", "Marcos de Sales Guerra", ""], ["Montealegre-Rubio", "Wilfredo", ""]]}, {"id": "1501.04822", "submitter": "Francesco Pasquale", "authors": "Luca Becchetti, Andrea Clementi, Emanuele Natale, Francesco Pasquale\n  and Gustavo Posta", "title": "Self-Stabilizing Repeated Balls-into-Bins", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the following synchronous process that we call \"repeated\nballs-into-bins\". The process is started by assigning $n$ balls to $n$ bins in\nan arbitrary way. In every subsequent round, from each non-empty bin one ball\nis chosen according to some fixed strategy (random, FIFO, etc), and re-assigned\nto one of the $n$ bins uniformly at random.\n  We define a configuration \"legitimate\" if its maximum load is\n$\\mathcal{O}(\\log n)$. We prove that, starting from any configuration, the\nprocess will converge to a legitimate configuration in linear time and then it\nwill only take on legitimate configurations over a period of length bounded by\nany polynomial in $n$, with high probability (w.h.p.). This implies that the\nprocess is self-stabilizing and that every ball traverses all bins in\n$\\mathcal{O}(n \\log^2 n)$ rounds, w.h.p.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jan 2015 14:33:59 GMT"}, {"version": "v2", "created": "Tue, 27 Jan 2015 02:05:34 GMT"}, {"version": "v3", "created": "Tue, 24 May 2016 12:53:47 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Becchetti", "Luca", ""], ["Clementi", "Andrea", ""], ["Natale", "Emanuele", ""], ["Pasquale", "Francesco", ""], ["Posta", "Gustavo", ""]]}, {"id": "1501.04854", "submitter": "Yanfeng Zhang", "authors": "Yanfeng Zhang, Shimin Chen, Qiang Wang, Ge Yu", "title": "i2MapReduce: Incremental MapReduce for Mining Evolving Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As new data and updates are constantly arriving, the results of data mining\napplications become stale and obsolete over time. Incremental processing is a\npromising approach to refreshing mining results. It utilizes previously saved\nstates to avoid the expense of re-computation from scratch.\n  In this paper, we propose i2MapReduce, a novel incremental processing\nextension to MapReduce, the most widely used framework for mining big data.\nCompared with the state-of-the-art work on Incoop, i2MapReduce (i) performs\nkey-value pair level incremental processing rather than task level\nre-computation, (ii) supports not only one-step computation but also more\nsophisticated iterative computation, which is widely used in data mining\napplications, and (iii) incorporates a set of novel techniques to reduce I/O\noverhead for accessing preserved fine-grain computation states. We evaluate\ni2MapReduce using a one-step algorithm and three iterative algorithms with\ndiverse computation characteristics. Experimental results on Amazon EC2 show\nsignificant performance improvements of i2MapReduce compared to both plain and\niterative MapReduce performing re-computation.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jan 2015 15:47:46 GMT"}], "update_date": "2015-01-21", "authors_parsed": [["Zhang", "Yanfeng", ""], ["Chen", "Shimin", ""], ["Wang", "Qiang", ""], ["Yu", "Ge", ""]]}, {"id": "1501.04877", "submitter": "Daniel Jung", "authors": "Sebastian Abshoff, Andreas Cord-Landwehr, Daniel Jung, and Friedhelm\n  Meyer auf der Heide", "title": "Towards Gathering Robots with Limited View in Linear Time: The Closed\n  Chain Case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the gathering problem, n autonomous robots have to meet on a single point.\nWe consider the gathering of a closed chain of point-shaped, anonymous robots\non a grid. The robots only have local knowledge about a constant number of\nneighboring robots along the chain in both directions. Actions are performed in\nthe fully synchronous time model FSYNC. Every robot has a limited memory that\nmay contain one timestamp of the global clock, also visible to its direct\nneighbors. In this synchronous time model, there is no limited view gathering\nalgorithm known to perform better than in quadratic runtime. The configurations\nthat show the quadratic lower bound are closed chains. In this paper, we\npresent the first sub-quadratic---in fact linear time---gathering algorithm for\nclosed chains on a grid.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jan 2015 16:48:15 GMT"}], "update_date": "2015-01-21", "authors_parsed": [["Abshoff", "Sebastian", ""], ["Cord-Landwehr", "Andreas", ""], ["Jung", "Daniel", ""], ["der Heide", "Friedhelm Meyer auf", ""]]}, {"id": "1501.05041", "submitter": "Andre Luckow", "authors": "Andre Luckow, Pradeep Mantha, Shantenu Jha", "title": "Pilot-Abstraction: A Valid Abstraction for Data-Intensive Applications\n  on HPC, Hadoop and Cloud Infrastructures?", "comments": "Submitted to HPDC 2015, 12 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  HPC environments have traditionally been designed to meet the compute demand\nof scientific applications and data has only been a second order concern. With\nscience moving toward data-driven discoveries relying more on correlations in\ndata to form scientific hypotheses, the limitations of HPC approaches become\napparent: Architectural paradigms such as the separation of storage and compute\nare not optimal for I/O intensive workloads (e.g. for data preparation,\ntransformation and SQL). While there are many powerful computational and\nanalytical libraries available on HPC (e.g. for scalable linear algebra), they\ngenerally lack the usability and variety of analytical libraries found in other\nenvironments (e.g. the Apache Hadoop ecosystem). Further, there is a lack of\nabstractions that unify access to increasingly heterogeneous infrastructure\n(HPC, Hadoop, clouds) and allow reasoning about performance trade-offs in this\ncomplex environment. At the same time, the Hadoop ecosystem is evolving rapidly\nand has established itself as de-facto standard for data-intensive workloads in\nindustry and is increasingly used to tackle scientific problems. In this paper,\nwe explore paths to interoperability between Hadoop and HPC, examine the\ndifferences and challenges, such as the different architectural paradigms and\nabstractions, and investigate ways to address them. We propose the extension of\nthe Pilot-Abstraction to Hadoop to serve as interoperability layer for\nallocating and managing resources across different infrastructures. Further,\nin-memory capabilities have been deployed to enhance the performance of\nlarge-scale data analytics (e.g. iterative algorithms) for which the ability to\nre-use data across iterations is critical. As memory naturally fits in with the\nPilot concept of retaining resources for a set of tasks, we propose the\nextension of the Pilot-Abstraction to in-memory resources.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jan 2015 02:55:02 GMT"}], "update_date": "2015-01-22", "authors_parsed": [["Luckow", "Andre", ""], ["Mantha", "Pradeep", ""], ["Jha", "Shantenu", ""]]}, {"id": "1501.05286", "submitter": "Luigi Mascolo", "authors": "Luigi Mascolo, Marco Quartulli, Pietro Guccione, Giovanni Nico, Igor\n  G. Olaizola", "title": "Distributed mining of large scale remote sensing image archives on\n  public computing infrastructures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Earth Observation (EO) mining aims at supporting efficient access and\nexploration of petabyte-scale space- and airborne remote sensing archives that\nare currently expanding at rates of terabytes per day. A significant challenge\nis performing the analysis required by envisaged applications --- like for\ninstance process mapping for environmental risk management --- in reasonable\ntime. In this work, we address the problem of content-based image retrieval via\nexample-based queries from EO data archives. In particular, we focus on the\nanalysis of polarimetric SAR data, for which target decomposition theorems have\nproved fundamental in discovering patterns in data and characterize the ground\nscattering properties. To this end, we propose an interactive region-oriented\ncontent-based image mining system in which 1) unsupervised ingestion processes\nare distributed onto virtual machines in elastic, on-demand computing\ninfrastructures 2) archive-scale content hierarchical indexing is implemented\nin terms of a \"big data\" analytics cluster-computing framework 3) query\nprocessing amounts to traversing the generated binary tree index, computing\ndistances that correspond to descriptor-based similarity measures between image\ngroups and a query image tile. We describe in depth both the strategies and the\nactual implementations for the ingestion and indexing components, and verify\nthe approach by experiments carried out on the NASA/JPL UAVSAR full\npolarimetric data archive. We report the results of the tests performed on\ncomputer clusters by using a public Infrastructure-as-a-Service and evaluating\nthe impact of cluster configuration on system performance. Results are\npromising for data mapping and information retrieval applications.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jan 2015 08:37:44 GMT"}], "update_date": "2015-01-22", "authors_parsed": [["Mascolo", "Luigi", ""], ["Quartulli", "Marco", ""], ["Guccione", "Pietro", ""], ["Nico", "Giovanni", ""], ["Olaizola", "Igor G.", ""]]}, {"id": "1501.05367", "submitter": "Andreas Wicenec", "authors": "Peter Quinn, Tim Axelrod, Ian Bird, Richard Dodson, Alex Szalay,\n  Andreas Wicenec", "title": "Delivering SKA Science", "comments": "27 pages, 14 figures, Conference: Advancing Astrophysics with the\n  Square Kilometre Array June 8-13, 2014 Giardini Naxos, Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The SKA will be capable of producing a stream of science data products that\nare Exa-scale in terms of their storage and processing requirements. This\nGoogle-scale enterprise is attracting considerable international interest and\nexcitement from within the industrial and academic communities. In this chapter\nwe examine the data flow, storage and processing requirements of a number of\nkey SKA survey science projects to be executed on the baseline SKA1\nconfiguration. Based on a set of conservative assumptions about trends for HPC\nand storage costs, and the data flow process within the SKA Observatory, it is\napparent that survey projects of the scale proposed will potentially drive\nconstruction and operations costs beyond the current anticipated SKA1 budget.\nThis implies a sharing of the resources and costs to deliver SKA science\nbetween the community and what is contained within the SKA Observatory. A\nsimilar situation was apparent to the designers of the LHC more than 10 years\nago. We propose that it is time for the SKA project and community to consider\nthe effort and process needed to design and implement a distributed SKA science\ndata system that leans on the lessons of other projects and looks to recent\ndevelopments in Cloud technologies to ensure an affordable, effective and\nglobal achievement of SKA science goals.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2015 01:51:42 GMT"}], "update_date": "2015-01-23", "authors_parsed": [["Quinn", "Peter", ""], ["Axelrod", "Tim", ""], ["Bird", "Ian", ""], ["Dodson", "Richard", ""], ["Szalay", "Alex", ""], ["Wicenec", "Andreas", ""]]}, {"id": "1501.05387", "submitter": "Yangzihao Wang", "authors": "Yangzihao Wang, Andrew Davidson, Yuechao Pan, Yuduo Wu, Andy Riffel,\n  and John D. Owens", "title": "Gunrock: A High-Performance Graph Processing Library on the GPU", "comments": "14 pages, accepted by PPoPP'16 (removed the text repetition in the\n  previous version v5)", "journal-ref": null, "doi": "10.1145/2851141.2851145", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For large-scale graph analytics on the GPU, the irregularity of data access\nand control flow, and the complexity of programming GPUs have been two\nsignificant challenges for developing a programmable high-performance graph\nlibrary. \"Gunrock\", our graph-processing system designed specifically for the\nGPU, uses a high-level, bulk-synchronous, data-centric abstraction focused on\noperations on a vertex or edge frontier. Gunrock achieves a balance between\nperformance and expressiveness by coupling high performance GPU computing\nprimitives and optimization strategies with a high-level programming model that\nallows programmers to quickly develop new graph primitives with small code size\nand minimal GPU programming knowledge. We evaluate Gunrock on five key graph\nprimitives and show that Gunrock has on average at least an order of magnitude\nspeedup over Boost and PowerGraph, comparable performance to the fastest GPU\nhardwired primitives, and better performance than any other GPU high-level\ngraph library.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2015 04:21:53 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2015 21:10:00 GMT"}, {"version": "v3", "created": "Fri, 25 Sep 2015 08:11:02 GMT"}, {"version": "v4", "created": "Wed, 14 Oct 2015 03:50:54 GMT"}, {"version": "v5", "created": "Sat, 23 Jan 2016 01:34:16 GMT"}, {"version": "v6", "created": "Mon, 22 Feb 2016 22:40:09 GMT"}], "update_date": "2016-02-24", "authors_parsed": [["Wang", "Yangzihao", ""], ["Davidson", "Andrew", ""], ["Pan", "Yuechao", ""], ["Wu", "Yuduo", ""], ["Riffel", "Andy", ""], ["Owens", "John D.", ""]]}, {"id": "1501.05414", "submitter": "Weidong Li", "authors": "Weidong Li, Xi Liu, Xuejie Zhang, and Xiaobo Cai", "title": "A Task-Type-Based Algorithm for the Energy-Aware Profit Maximizing\n  Scheduling Problem in Heterogeneous Computing Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we design an efficient algorithm for the energy-aware profit\nmaximizing scheduling problem, where the high performance computing system\nadministrator is to maximize the profit per unit time. The running time of the\nproposed algorithm is depending on the number of task types, while the running\ntime of the previous algorithm is depending on the number of tasks. Moreover,\nwe prove that the worst-case performance ratio is close to 2, which maybe the\nbest result. Simulation experiments show that the proposed algorithm is more\naccurate than the previous method.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2015 07:53:11 GMT"}], "update_date": "2015-01-23", "authors_parsed": [["Li", "Weidong", ""], ["Liu", "Xi", ""], ["Zhang", "Xuejie", ""], ["Cai", "Xiaobo", ""]]}, {"id": "1501.05789", "submitter": "Minxian  Xu", "authors": "Minxian Xu, Wenhong Tian, Xinyang Wang, Qin Xiong", "title": "FlexCloud: A Flexible and Extendible Simulator for Performance\n  Evaluation of Virtual Machine Allocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud Data centers aim to provide reliable, sustainable and scalable services\nfor all kinds of applications. Resource scheduling is one of keys to cloud\nservices. To model and evaluate different scheduling policies and algorithms,\nwe propose FlexCloud, a flexible and scalable simulator that enables users to\nsimulate the process of initializing cloud data centers, allocating virtual\nmachine requests and providing performance evaluation for various scheduling\nalgorithms. FlexCloud can be run on a single computer with JVM to simulate\nlarge scale cloud environments with focus on infrastructure as a service;\nadopts agile design patterns to assure the flexibility and extensibility;\nmodels virtual machine migrations which is lack in the existing tools; provides\nuser-friendly interfaces for customized configurations and replaying. Comparing\nto existing simulators, FlexCloud has combining features for supporting public\ncloud providers, load-balance and energy-efficiency scheduling. FlexCloud has\nadvantage in computing time and memory consumption to support large-scale\nsimulations. The detailed design of FlexCloud is introduced and performance\nevaluation is provided.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jan 2015 13:05:35 GMT"}], "update_date": "2015-01-26", "authors_parsed": [["Xu", "Minxian", ""], ["Tian", "Wenhong", ""], ["Wang", "Xinyang", ""], ["Xiong", "Qin", ""]]}, {"id": "1501.05983", "submitter": "Taoufik Rachad", "authors": "J. Boutahar, T. Rachad and S. El ghazi", "title": "A new efficient Matching method for web services substitution", "comments": "9 pages, 11 figures, 2 tables", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 11,\n  Issue 5, No 2,196-204, September 2014", "doi": null, "report-no": null, "categories": "cs.IR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The internet is considered as the most extensive market in the world. To keep\nits gradual reputation, it must confront real problems that result from its\ndistribution and from the diversity of the protocols used to insure\ncommunications. The Web service technology has diminished significantly the\neffects of distribution and heterogeneity, but there are several problems that\nweaken their performance (unavailability, load increase of use, high cost of\nCPU time...). Faced with this situation, we are forced to move in the direction\nof the substitution of web services. In this context, we propose an effective\ntechnique of substitution based on a new method of matching that allows\ndetecting and expressing the matching between the web services pairwise by\nconsidering that each of them is ontology. Also, our method performs a\ndiscovery of the most similar web service to that to be replaced by using an\nefficient method of similarity measurement.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jan 2015 23:25:17 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Boutahar", "J.", ""], ["Rachad", "T.", ""], ["ghazi", "S. El", ""]]}, {"id": "1501.06102", "submitter": "Terrence Adams", "authors": "Terrence Adams", "title": "Development of a Big Data Framework for Connectomic Research", "comments": "6 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper outlines research and development of a new Hadoop-based\narchitecture for distributed processing and analysis of electron microscopy of\nbrains. We show development of a new C++ library for implementation of 3D image\nanalysis techniques, and deployment in a distributed map/reduce framework. We\ndemonstrate our new framework on a subset of the Kasthuri11 dataset from the\nOpen Connectome Project.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jan 2015 01:42:09 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Adams", "Terrence", ""]]}, {"id": "1501.06238", "submitter": "Houwu Chen", "authors": "Houwu Chen, Jiwu Shu", "title": "Sky: Opinion Dynamics Based Consensus for P2P Network with Trust\n  Relationships", "comments": "This paper has been withdrawn by the author due to a crucial\n  theoretic defect", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional Byzantine consensus does not work in P2P network due to Sybil\nattack while the most prevalent Sybil-proof consensus at present still can't\nresist adversary with dominant compute power. This paper proposed opinion\ndynamics based consensus for P2P network with trust relationships, consisting\nof the sky framework and the sky model. With the sky framework, opinion\ndynamics can be applied in P2P network for consensus which is Sybil-proof\nthrough trust relationships and emerges from local interactions of each node\nwith its direct contacts without topology, global information or even sample of\nthe network involved. The sky model has better performance of convergence than\nexisting models including MR, voter and Sznajd, and its lower bound of fault\ntolerance performance is also analyzed and proved. Simulations show that our\napproach can tolerant failures by at least 13% random nodes or 2% top\ninfluential nodes while over 96% correct nodes still make correct decision\nwithin 70 seconds on the SNAP Wikipedia who-votes-on-whom network for initial\nconfiguration of convergence>0.5 with reasonable latencies. Comparing to\ncompute power based consensus, our approach can resist any faulty or malicious\nnodes by unfollowing them. To the best of our knowledge, it's the first work to\nbring opinion dynamics to P2P network for consensus.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 02:35:07 GMT"}, {"version": "v2", "created": "Tue, 27 Jan 2015 04:26:47 GMT"}, {"version": "v3", "created": "Thu, 12 Feb 2015 14:34:50 GMT"}, {"version": "v4", "created": "Fri, 15 May 2015 11:17:27 GMT"}, {"version": "v5", "created": "Tue, 9 Jun 2015 15:30:27 GMT"}, {"version": "v6", "created": "Thu, 27 Aug 2015 12:24:25 GMT"}, {"version": "v7", "created": "Mon, 28 Sep 2015 16:36:05 GMT"}, {"version": "v8", "created": "Sun, 4 Oct 2015 10:42:10 GMT"}, {"version": "v9", "created": "Sun, 26 Feb 2017 23:22:56 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Chen", "Houwu", ""], ["Shu", "Jiwu", ""]]}, {"id": "1501.06326", "submitter": "Blesson Varghese", "authors": "Blesson Varghese", "title": "The GPU vs Phi Debate: Risk Analytics Using Many-Core Computing", "comments": "A modified version of this article is accepted to the Computers and\n  Electrical Engineering Journal under the title - \"The Hardware Accelerator\n  Debate: A Financial Risk Case Study Using Many-Core Computing\"; Blesson\n  Varghese, \"The Hardware Accelerator Debate: A Financial Risk Case Study Using\n  Many-Core Computing,\" Computers and Electrical Engineering, 2015", "journal-ref": null, "doi": "10.1016/j.compeleceng.2015.01.012", "report-no": null, "categories": "cs.DC cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The risk of reinsurance portfolios covering globally occurring natural\ncatastrophes, such as earthquakes and hurricanes, is quantified by employing\nsimulations. These simulations are computationally intensive and require large\namounts of data to be processed. The use of many-core hardware accelerators,\nsuch as the Intel Xeon Phi and the NVIDIA Graphics Processing Unit (GPU), are\ndesirable for achieving high-performance risk analytics. In this paper, we set\nout to investigate how accelerators can be employed in risk analytics, focusing\non developing parallel algorithms for Aggregate Risk Analysis, a simulation\nwhich computes the Probable Maximum Loss of a portfolio taking both primary and\nsecondary uncertainties into account. The key result is that both hardware\naccelerators are useful in different contexts; without taking data transfer\ntimes into account the Phi had lowest execution times when used independently\nand the GPU along with a host in a hybrid platform yielded best performance.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 10:52:18 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Varghese", "Blesson", ""]]}, {"id": "1501.06364", "submitter": "Bernd R. Schlei", "authors": "B. R. Schlei", "title": "GPU Programming - Speeding Up the 3D Surface Generator VESTA", "comments": "1 page, 1 figure, submitted contribution to the GSI Scientific Report\n  2014", "journal-ref": null, "doi": "10.15120/GR-2015-1-FG-GENERAL-42", "report-no": "GSI SCIENTIFIC REPORT 2014", "categories": "cs.GR cs.CG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The novel \"Volume-Enclosing Surface exTraction Algorithm\" (VESTA) generates\ntriangular isosurfaces from computed tomography volumetric images and/or\nthree-dimensional (3D) simulation data. Here, we present various benchmarks for\nGPU-based code implementations of both VESTA and the current state-of-the-art\nMarching Cubes Algorithm (MCA). One major result of this study is that VESTA\nruns significantly faster than the MCA.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 12:38:21 GMT"}], "update_date": "2016-01-27", "authors_parsed": [["Schlei", "B. R.", ""]]}, {"id": "1501.06451", "submitter": "Cristina Mu\\~noz", "authors": "Cristina Mu\\~noz and Pierre Leone", "title": "Design of a Novel Network Architecture for Distributed Event-Based\n  Systems Using Directional Random Walks in an Ubiquitous Sensing Scenario", "comments": "10 pages. arXiv admin note: text overlap with arXiv:1408.3033", "journal-ref": "International Journal on Advances in Networks and Services, 2014\n  vol 7 nr 3&4", "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Ubiquitous sensing devices frequently disseminate data among them. The use of\na distributed event-based system that decouples publishers from subscribers\narises as an ideal candidate to implement the dissemination process. In this\npaper, we present a network architecture that merges the network and overlay\nlayers of typical structured event-based systems. Directional random walks are\nused for the construction of this merged layer. Our strategy avoids using a\nspecific network protocol that provides point-to-point communication. This\nimplies that the topology of the network is not maintained, so that nodes not\ninvolved in the system are able to save energy and computing resources. We\nevaluate the performance of the overlay layer using directional random walks\nand pure random walks for its construction. Our results show that directional\nrandom walks are more efficient because: (1) they use less nodes of the network\nfor the establishment of the active path of the overlay layer and (2) they have\na more reliable performance. Furthermore, as the number of nodes in the network\nincreases, so do the number of nodes in the active path of the overlay layer\nfor the same number of publishers and subscribers. Finally, we discard any\ncorrelation between the number of nodes that form the overlay layer and the\nmaximum Euclidean distance traversed by the walkers.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 15:40:28 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Mu\u00f1oz", "Cristina", ""], ["Leone", "Pierre", ""]]}, {"id": "1501.06479", "submitter": "Prasant Misra", "authors": "Sabarish Sridhar, Prasant Misra, Gurinder Singh Gill, Jay Warrior", "title": "CheepSync: A Time Synchronization Service for Resource Constrained\n  Bluetooth Low Energy Advertisers", "comments": null, "journal-ref": "IEEE Communications Magazine, vol. 54, no. 1, pp. 136-143, January\n  2016", "doi": "10.1109/MCOM.2016.7378439", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clock synchronization is highly desirable in distributed systems, including\nmany applications in the Internet of Things and Humans (IoTH). It improves the\nefficiency, modularity and scalability of the system, and optimizes use of\nevent triggers. For IoTH, Bluetooth Low Energy (BLE) - a subset of the recent\nBluetooth v4.0 stack - provides a low-power and loosely coupled mechanism for\nsensor data collection with ubiquitous units (e.g., smartphones and tablets)\ncarried by humans. This fundamental design paradigm of BLE is enabled by a\nrange of broadcast advertising modes. While its operational benefits are\nnumerous, the lack of a common time reference in the broadcast mode of BLE has\nbeen a fundamental limitation. This paper presents and describes CheepSync: a\ntime synchronization service for BLE advertisers, especially tailored for\napplications requiring high time precision on resource constrained BLE\nplatforms. Designed on top of the existing Bluetooth v4.0 standard, the\nCheepSync framework utilizes low-level timestamping and comprehensive error\ncompensation mechanisms for overcoming uncertainties in message transmission,\nclock drift and other system specific constraints. CheepSync was implemented on\ncustom designed nRF24Cheep beacon platforms (as broadcasters) and commercial\noff-the-shelf Android ported smartphones (as passive listeners). We demonstrate\nthe efficacy of CheepSync by numerous empirical evaluations in a variety of\nexperimental setups, and show that its average (single-hop) time\nsynchronization accuracy is in the 10us range.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 16:52:40 GMT"}, {"version": "v2", "created": "Fri, 8 Apr 2016 10:15:18 GMT"}], "update_date": "2016-04-11", "authors_parsed": [["Sridhar", "Sabarish", ""], ["Misra", "Prasant", ""], ["Gill", "Gurinder Singh", ""], ["Warrior", "Jay", ""]]}, {"id": "1501.06625", "submitter": "Jan Verschelde", "authors": "Jan Verschelde and Xiangcheng Yu", "title": "Accelerating Polynomial Homotopy Continuation on a Graphics Processing\n  Unit with Double Double and Quad Double Arithmetic", "comments": "Accepted for publication in the Proceedings of the 7th International\n  Workshop on Parallel Symbolic Computation (PASCO 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC cs.NA math.AG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerical continuation methods track a solution path defined by a homotopy.\nThe systems we consider are defined by polynomials in several variables with\ncomplex coefficients. For larger dimensions and degrees, the numerical\nconditioning worsens and hardware double precision becomes often insufficient\nto reach the end of the solution path. With double double and quad double\narithmetic, we can solve larger problems that we could not solve with hardware\ndouble arithmetic, but at a higher computational cost. This cost overhead can\nbe compensated by acceleration on a Graphics Processing Unit (GPU). We describe\nour implementation and report on computational results on benchmark polynomial\nsystems.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 23:54:20 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2015 15:55:11 GMT"}, {"version": "v3", "created": "Fri, 12 Jun 2015 15:08:55 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Verschelde", "Jan", ""], ["Yu", "Xiangcheng", ""]]}, {"id": "1501.06633", "submitter": "Andrew Lavin", "authors": "Andrew Lavin", "title": "maxDNN: An Efficient Convolution Kernel for Deep Learning with Maxwell\n  GPUs", "comments": "7 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes maxDNN, a computationally efficient convolution kernel\nfor deep learning with the NVIDIA Maxwell GPU. maxDNN reaches 96.3%\ncomputational efficiency on typical deep learning network architectures. The\ndesign combines ideas from cuda-convnet2 with the Maxas SGEMM assembly code. We\nonly address forward propagation (FPROP) operation of the network, but we\nbelieve that the same techniques used here will be effective for backward\npropagation (BPROP) as well.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jan 2015 01:19:12 GMT"}, {"version": "v2", "created": "Wed, 28 Jan 2015 01:16:33 GMT"}, {"version": "v3", "created": "Fri, 30 Jan 2015 23:50:49 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Lavin", "Andrew", ""]]}, {"id": "1501.06663", "submitter": "Bojian Xu", "authors": "Yun Tian and Bojian Xu", "title": "On Longest Repeat Queries Using GPU", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Repeat finding in strings has important applications in subfields such as\ncomputational biology. The challenge of finding the longest repeats covering\nparticular string positions was recently proposed and solved by \\.{I}leri et\nal., using a total of the optimal $O(n)$ time and space, where $n$ is the\nstring size. However, their solution can only find the \\emph{leftmost} longest\nrepeat for each of the $n$ string position. It is also not known how to\nparallelize their solution. In this paper, we propose a new solution for\nlongest repeat finding, which although is theoretically suboptimal in time but\nis conceptually simpler and works faster and uses less memory space in practice\nthan the optimal solution. Further, our solution can find \\emph{all} longest\nrepeats of every string position, while still maintaining a faster processing\nspeed and less memory space usage. Moreover, our solution is\n\\emph{parallelizable} in the shared memory architecture (SMA), enabling it to\ntake advantage of the modern multi-processor computing platforms such as the\ngeneral-purpose graphics processing units (GPU). We have implemented both the\nsequential and parallel versions of our solution. Experiments with both\nbiological and non-biological data show that our sequential and parallel\nsolutions are faster than the optimal solution by a factor of 2--3.5 and 6--14,\nrespectively, and use less memory space.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jan 2015 06:04:35 GMT"}], "update_date": "2015-01-28", "authors_parsed": [["Tian", "Yun", ""], ["Xu", "Bojian", ""]]}, {"id": "1501.07056", "submitter": "Mansaf Alam Dr", "authors": "Kashish Ara Shakil, Shuchi Sethi and Mansaf Alam", "title": "An Effective Framework for Managing University Data using a Cloud based\n  Environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Management of data in education sector particularly management of data for\nbig universities with several employees, departments and students is a very\nchallenging task. There are also problems such as lack of proper funds and\nmanpower for management of such data in universities. Education sector can\neasily and effectively take advantage of cloud computing skills for management\nof data. It can enhance the learning experience as a whole and can add entirely\nnew dimensions to the way in which education is imbibed. Several benefits of\nCloud computing such as monetary benefits, environmental benefits and remote\ndata access for management of data such as university database can be used in\neducation sector. Therefore, in this paper we have proposed an effective\nframework for managing university data using a cloud based environment. We have\nalso proposed cloud data management simulator: a new simulation framework which\ndemonstrates the applicability of cloud in the current education sector. The\nframework consists of a cloud developed for processing a universities database\nwhich consists of staff and students. It has the following features (i) support\nfor modeling cloud computing infrastructure, which includes data centers\ncontaining university database; (ii) a user friendly interface; (iii)\nflexibility to switch between the different types of users; and (iv)\nvirtualized access to cloud data.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jan 2015 10:37:33 GMT"}], "update_date": "2015-01-29", "authors_parsed": [["Shakil", "Kashish Ara", ""], ["Sethi", "Shuchi", ""], ["Alam", "Mansaf", ""]]}, {"id": "1501.07079", "submitter": "Fabio Licht", "authors": "Fabio Licht, Bruno Schulze, Luis E. Bona, and Antonio R. Mury", "title": "The Affinity Effects of Parallelized Libraries in Concurrent\n  Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of cloud computing grows as it appears to be an additional resource\nfor High-Performance Parallel and Distributed Computing (HPDC), especially with\nrespect to its use in support of scientific applications. Many studies have\nbeen devoted to determining the effect of the virtualization layer on the\nperformance, but most of the studies conducted so far lack insight into the\njoint effects between application type, virtualization layer and parallelized\nlibraries in applications. This work introduces the concept of affinity with\nregard to the combined effects of the virtualization layer, class of\napplication and parallelized libraries used in these applications. Affinity is\nhere defined as the degree of influence that one application has on other\napplications when running concurrently in virtual environments hosted on the\nsame real server. The results presented here show how parallel libraries used\nin application implementation have a significant influence and how the\ncombinations between these types of libraries and classes of applications could\nsignificantly influence the performance of the environment. In this context,\nthe concept of affinity is then used to evaluate these impacts to contribute to\nbetter stability and performance in the computational environment.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jan 2015 12:10:46 GMT"}], "update_date": "2015-01-29", "authors_parsed": [["Licht", "Fabio", ""], ["Schulze", "Bruno", ""], ["Bona", "Luis E.", ""], ["Mury", "Antonio R.", ""]]}, {"id": "1501.07350", "submitter": "Truong Vinh Truong Duy", "authors": "Truong Vinh Truong Duy and Taisuke Ozaki", "title": "Performance Tuning of a Parallel 3-D FFT Package OpenFFT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fast Fourier transform (FFT) is a primitive kernel in numerous fields of\nscience and engineering. OpenFFT is an open-source parallel package for 3-D\nFFTs, built on a communication-optimal domain decomposition method for\nachieving minimal volume of communication. In this paper, we analyze and tune\nthe performance of OpenFFT, paying a particular attention to tuning of\ncommunication that dominates the run time of large-scale calculations. We first\nanalyze its performance on different machines for an understanding of the\nbehaviors of the package and machines. Based on the performance analysis, we\ndevelop six communication methods for performing communication with the aim of\ncovering varied calculation scales on a variety of computational platforms.\nOpenFFT is then augmented with an auto-tuning of communication to select the\nbest method in run time depending on their performance. Numerical results\ndemonstrate that the optimized OpenFFT is able to deliver relatively good\nperformance in comparison with other state-of-the-art packages at different\ncomputational scales on a number of parallel machines.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jan 2015 06:12:52 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2015 06:29:34 GMT"}], "update_date": "2015-08-27", "authors_parsed": [["Duy", "Truong Vinh Truong", ""], ["Ozaki", "Taisuke", ""]]}, {"id": "1501.07379", "submitter": "Maciej Pacut", "authors": "Carlo Fuerst, Maciej Pacut, Stefan Schmid", "title": "Hardness of Virtual Network Embedding with Replica Selection", "comments": "2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient embedding virtual clusters in physical network is a challenging\nproblem. In this paper we consider a scenario where physical network has a\nstructure of a balanced tree. This assumption is justified by many real- world\nimplementations of datacenters. We consider an extension to virtual cluster\nembedding by introducing replication among data chunks. In many real-world\napplications, data is stored in distributed and redundant way. This assumption\nintroduces additional hardness in deciding what replica to process. By\nreduction from classical NP-complete problem of Boolean Satisfia- bility, we\nshow limits of optimality of embedding. Our result holds even in trees of edge\nheight bounded by three. Also, we show that limiting repli- cation factor to\ntwo replicas per chunk type does not make the problem simpler.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jan 2015 08:59:38 GMT"}], "update_date": "2015-01-30", "authors_parsed": [["Fuerst", "Carlo", ""], ["Pacut", "Maciej", ""], ["Schmid", "Stefan", ""]]}, {"id": "1501.07400", "submitter": "Huber Markus", "authors": "Markus Huber, Bj\\\"orn Gmeiner, Ulrich R\\\"ude, Barbara Wohlmuth", "title": "Resilience for Exascale Enabled Multigrid Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing number of components and further miniaturization the mean\ntime between faults in supercomputers will decrease. System level fault\ntolerance techniques are expensive and cost energy, since they are often based\non redundancy. Also classical check-point-restart techniques reach their limits\nwhen the time for storing the system state to backup memory becomes excessive.\nTherefore, algorithm-based fault tolerance mechanisms can become an attractive\nalternative. This article investigates the solution process for elliptic\npartial differential equations that are discretized by finite elements. Faults\nthat occur in the parallel geometric multigrid solver are studied in various\nmodel scenarios. In a standard domain partitioning approach, the impact of a\nfailure of a core or a node will affect one or several subdomains. Different\nstrategies are developed to compensate the effect of such a failure\nalgorithmically. The recovery is achieved by solving a local subproblem with\nDirichlet boundary conditions using local multigrid cycling algorithms.\nAdditionally, we propose a superman strategy where extra compute power is\nemployed to minimize the time of the recovery process.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jan 2015 10:21:24 GMT"}], "update_date": "2015-01-30", "authors_parsed": [["Huber", "Markus", ""], ["Gmeiner", "Bj\u00f6rn", ""], ["R\u00fcde", "Ulrich", ""], ["Wohlmuth", "Barbara", ""]]}, {"id": "1501.07701", "submitter": "Jonathan Passerat-Palmbach", "authors": "Jonathan Passerat-Palmbach (UBP, LIMOS), Claude Mazel (LIMOS, UBP),\n  Antoine Mahul, David Hill (UBP, LIMOS)", "title": "Reliable Initialization of GPU-enabled Parallel Stochastic Simulations\n  Using Mersenne Twister for Graphics Processors", "comments": null, "journal-ref": "European Simulation and Modelling 2010, Oct 2010, Essen, Belgium.\n  pp.187 - 195", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel stochastic simulations tend to exploit more and more computing power\nand they are now also developed for General Purpose Graphics Process Units\n(GP-GPUs). Conse-quently, they need reliable random sources to feed their\napplications. We propose a survey of the current Pseudo Random Numbers\nGenerators (PRNG) available on GPU. We give a particular focus to the recent\nMersenne Twister for Graphics Processors (MTGP) that has just been released.\nOur work provides empirically checked statuses designed to initialize a\nparticular configuration of this generator, in order to prevent any potential\nbias introduced by the parallelization of the PRNG.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jan 2015 08:49:18 GMT"}], "update_date": "2015-02-02", "authors_parsed": [["Passerat-Palmbach", "Jonathan", "", "UBP, LIMOS"], ["Mazel", "Claude", "", "LIMOS, UBP"], ["Mahul", "Antoine", "", "UBP, LIMOS"], ["Hill", "David", "", "UBP, LIMOS"]]}, {"id": "1501.07719", "submitter": "Simon Perkins", "authors": "Simon Perkins, Patrick Marais, Jonathan Zwart, Iniyan Natarajan, Cyril\n  Tasse and Oleg Smirnov", "title": "Montblanc: GPU accelerated Radio Interferometer Measurement Equations in\n  support of Bayesian Inference for Radio Observations", "comments": "Submitted to Astronomy and Computing\n  (http://www.journals.elsevier.com/astronomy-and-computing). The code is\n  available online at https://github.com/ska-sa/montblanc. 29 pages long, with\n  10 figures, 6 tables and 3 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC astro-ph.IM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Montblanc, a GPU implementation of the Radio interferometer\nmeasurement equation (RIME) in support of the Bayesian inference for radio\nobservations (BIRO) technique. BIRO uses Bayesian inference to select sky\nmodels that best match the visibilities observed by a radio interferometer. To\naccomplish this, BIRO evaluates the RIME multiple times, varying sky model\nparameters to produce multiple model visibilities. Chi-squared values computed\nfrom the model and observed visibilities are used as likelihood values to drive\nthe Bayesian sampling process and select the best sky model.\n  As most of the elements of the RIME and chi-squared calculation are\nindependent of one another, they are highly amenable to parallel computation.\nAdditionally, Montblanc caters for iterative RIME evaluation to produce\nmultiple chi-squared values. Modified model parameters are transferred to the\nGPU between each iteration.\n  We implemented Montblanc as a Python package based upon NVIDIA's CUDA\narchitecture. As such, it is easy to extend and implement different pipelines.\nAt present, Montblanc supports point and Gaussian morphologies, but is designed\nfor easy addition of new source profiles.\n  Montblanc's RIME implementation is performant: On an NVIDIA K40, it is\napproximately 250 times faster than MeqTrees on a dual hexacore Intel E5-2620v2\nCPU. Compared to the OSKAR simulator's GPU-implemented RIME components it is\n7.7 and 12 times faster on the same K40 for single and double-precision\nfloating point respectively. However, OSKAR's RIME implementation is more\ngeneral than Montblanc's BIRO-tailored RIME.\n  Theoretical analysis of Montblanc's dominant CUDA kernel suggests that it is\nmemory bound. In practice, profiling shows that is balanced between compute and\nmemory, as much of the data required by the problem is retained in L1 and L2\ncache.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jan 2015 10:04:27 GMT"}, {"version": "v2", "created": "Tue, 3 Feb 2015 17:12:35 GMT"}, {"version": "v3", "created": "Fri, 19 Jun 2015 11:52:12 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Perkins", "Simon", ""], ["Marais", "Patrick", ""], ["Zwart", "Jonathan", ""], ["Natarajan", "Iniyan", ""], ["Tasse", "Cyril", ""], ["Smirnov", "Oleg", ""]]}, {"id": "1501.07800", "submitter": "Elias Rudberg", "authors": "Emanuel H. Rubensson and Elias Rudberg", "title": "Locality-aware parallel block-sparse matrix-matrix multiplication using\n  the Chunks and Tasks programming model", "comments": "35 pages, 14 figures", "journal-ref": "Parallel Comput. 57 (2016) 87-106", "doi": "10.1016/j.parco.2016.06.005", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for parallel block-sparse matrix-matrix multiplication on\ndistributed memory clusters. By using a quadtree matrix representation, data\nlocality is exploited without prior information about the matrix sparsity\npattern. A distributed quadtree matrix representation is straightforward to\nimplement due to our recent development of the Chunks and Tasks programming\nmodel [Parallel Comput. 40, 328 (2014)]. The quadtree representation combined\nwith the Chunks and Tasks model leads to favorable weak and strong scaling of\nthe communication cost with the number of processes, as shown both\ntheoretically and in numerical experiments.\n  Matrices are represented by sparse quadtrees of chunk objects. The leaves in\nthe hierarchy are block-sparse submatrices. Sparsity is dynamically detected by\nthe matrix library and may occur at any level in the hierarchy and/or within\nthe submatrix leaves. In case graphics processing units (GPUs) are available,\nboth CPUs and GPUs are used for leaf-level multiplication work, thus making use\nof the full computing capacity of each node.\n  The performance is evaluated for matrices with different sparsity structures,\nincluding examples from electronic structure calculations. Compared to methods\nthat do not exploit data locality, our locality-aware approach reduces\ncommunication significantly, achieving essentially constant communication per\nnode in weak scaling tests.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jan 2015 15:15:22 GMT"}, {"version": "v2", "created": "Sat, 7 Mar 2015 10:19:33 GMT"}, {"version": "v3", "created": "Fri, 18 Sep 2015 14:16:35 GMT"}, {"version": "v4", "created": "Mon, 27 Jun 2016 13:41:09 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Rubensson", "Emanuel H.", ""], ["Rudberg", "Elias", ""]]}]