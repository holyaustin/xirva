[{"id": "1505.00043", "submitter": "Yun Tian", "authors": "Yun Tian, Bojian Xu, Yanqing Ji, Jesse Scholer", "title": "CloudTree: A Library to Extend Cloud Services for Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a library that enables on a cloud the creation and\nmanagement of tree data structures from a cloud client. As a proof of concept,\nwe implement a new cloud service CloudTree. With CloudTree, users are able to\norganize big data into tree data structures of their choice that are physically\nstored in a cloud. We use caching, prefetching, and aggregation techniques in\nthe design and implementation of CloudTree to enhance performance. We have\nimplemented the services of Binary Search Trees (BST) and Prefix Trees as\ncurrent members in CloudTree and have benchmarked their performance using the\nAmazon Cloud. The idea and techniques in the design and implementation of a BST\nand prefix tree is generic and thus can also be used for other types of trees\nsuch as B-tree, and other link-based data structures such as linked lists and\ngraphs. Preliminary experimental results show that CloudTree is useful and\nefficient for various big data applications.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 21:59:29 GMT"}], "update_date": "2015-05-04", "authors_parsed": [["Tian", "Yun", ""], ["Xu", "Bojian", ""], ["Ji", "Yanqing", ""], ["Scholer", "Jesse", ""]]}, {"id": "1505.00144", "submitter": "Pierre-Yves Chevalier", "authors": "Pierre-Yves Chevalier, Julien M. Hendrickx and Rapha\\\"el M. Jungers", "title": "Reachability of Consensus and Synchronizing Automata", "comments": "Update after review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DM cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of determining the existence of a sequence of\nmatrices driving a discrete-time consensus system to consensus. We transform\nthis problem into one of the existence of a product of the transition\n(stochastic) matrices that has a positive column. We then generalize some\nresults from automata theory to sets of stochastic matrices. We obtain as a\nmain result a polynomial-time algorithm to decide the existence of a sequence\nof matrices achieving consensus.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 10:29:14 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2015 12:00:21 GMT"}], "update_date": "2015-10-05", "authors_parsed": [["Chevalier", "Pierre-Yves", ""], ["Hendrickx", "Julien M.", ""], ["Jungers", "Rapha\u00ebl M.", ""]]}, {"id": "1505.00236", "submitter": "Chaitanya Krishna Kande", "authors": "Chaitanya Krishna Kande", "title": "Discussion of various models related to cloud performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the various models related to cloud computing. Knowing\nthe metrics related to infrastructure is very critical to enhance the\nperformance of cloud services. Various metrics related to clouds such as\npageview response time, admission control and enforcing elasticity to cloud\ninfrastructure are very crucial in analyzing the characteristics of the cloud\nto enhance the cloud performance.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 18:10:51 GMT"}], "update_date": "2015-05-04", "authors_parsed": [["Kande", "Chaitanya Krishna", ""]]}, {"id": "1505.00267", "submitter": "K. Alex Mills", "authors": "Yanyan Zeng, K. Alex Mills, Shreyas Gokhale, Neeraj Mittal, S.\n  Venkatesan, R. Chandrasekaran", "title": "Robust Neighbor Discovery in Multi-Hop Multi-Channel Heterogeneous\n  Wireless Networks", "comments": "54 pages, 5 figures, 1 table, 6 algorithms, accepted in Journal of\n  Parallel and Distributed Computing", "journal-ref": null, "doi": "10.1016/j.jpdc.2016.02.001", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important first step when deploying a wireless ad hoc network is neighbor\ndiscovery in which every node attempts to determine the set of nodes it can\ncommunicate with in one wireless hop. In the recent years, cognitive radio (CR)\ntechnology has gained attention as an attractive approach to alleviate spectrum\ncongestion. A cognitive radio transceiver can operate over a wide range of\nfrequencies, possibly scanning multiple frequency bands. A cognitive radio node\ncan opportunistically utilize unused wireless spectrum without interference\nfrom other wireless devices in its vicinity. Due to spatial variations in\nfrequency usage and hardware variations in radio transceivers, different nodes\nin the network may perceive different subsets of frequencies available to them\nfor communication. This heterogeneity in the available channel sets across the\nnetwork increases the complexity of solving the neighbor discovery problem in a\ncognitive radio network. In this work, we design and analyze several randomized\nalgorithms for neighbor discovery in such a (heterogeneous) network under a\nvariety of assumptions (e.g. maximum node degree known or unknown) for both\nsynchronous and asynchronous systems under minimal knowledge. We also show that\nour randomized algorithms are naturally suited to tolerate unreliable channels\nand adversarial attacks.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 20:00:47 GMT"}, {"version": "v2", "created": "Wed, 2 Mar 2016 18:07:59 GMT"}], "update_date": "2017-01-09", "authors_parsed": [["Zeng", "Yanyan", ""], ["Mills", "K. Alex", ""], ["Gokhale", "Shreyas", ""], ["Mittal", "Neeraj", ""], ["Venkatesan", "S.", ""], ["Chandrasekaran", "R.", ""]]}, {"id": "1505.00344", "submitter": "Robert Merrison-Hort", "authors": "Robert Merrison-Hort", "title": "Fireflies: New software for interactively exploring dynamical systems\n  using GPU computing", "comments": "31 pages, 8 figures, 4 supplementary videos", "journal-ref": null, "doi": "10.1142/S0218127415501813", "report-no": null, "categories": "cs.MS cs.DC math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In non-linear systems, where explicit analytic solutions usually can't be\nfound, visualisation is a powerful approach which can give insights into the\ndynamical behaviour of models; it is also crucial for teaching this area of\nmathematics. In this paper we present new software, Fireflies, which exploits\nthe power of graphical processing unit (GPU) computing to produce spectacular\ninteractive visualisations of arbitrary systems of ordinary differential\nequations. In contrast to typical phase portraits, Fireflies draws the current\nposition of trajectories (projected onto 2D or 3D space) as single points of\nlight, which move as the system is simulated. Due to the massively parallel\nnature of GPU hardware, Fireflies is able to simulate millions of trajectories\nin parallel (even on standard desktop computer hardware), producing \"swarms\" of\nparticles that move around the screen in real-time according to the equations\nof the system. Particles that move forwards in time reveal stable attractors\n(e.g. fixed points and limit cycles), while the option of integrating another\ngroup of trajectories backwards in time can reveal unstable objects\n(repellers). Fireflies allows the user to change the parameters of the system\nas it is running, in order to see the effect that they have on the dynamics and\nto observe bifurcations. We demonstrate the capabilities of the software with\nthree examples: a two-dimensional \"mean field\" model of neuronal activity, the\nclassical Lorenz system, and a 15-dimensional model of three interacting\nbiologically realistic neurons.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2015 13:57:16 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Merrison-Hort", "Robert", ""]]}, {"id": "1505.00577", "submitter": "Alexander Ngenzi", "authors": "Alexander Ngenzi, Selvarani R, Suchithra R. Nair", "title": "Dynamic resource management in Cloud datacenters for Server\n  consolidation", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud resource management has been a key factor for the cloud datacenters\ndevelopment. Many cloud datacenters have problems in understanding and\nimplementing the techniques to manage, allocate and migrate the resources in\ntheir premises. The consequences of improper resource management may result\ninto underutilized and wastage of resources which may also result into poor\nservice delivery in these datacenters. Resources like, CPU, memory, Hard disk\nand servers need to be well identified and managed. In this Paper, Dynamic\nResource Management Algorithm(DRMA) shall limit itself in the management of CPU\nand memory as the resources in cloud datacenters. The target is to save those\nresources which may be underutilized at a particular period of time. It can be\nachieved through Implementation of suitable algorithms. Here, Bin packing\nalgorithm can be used whereby the best fit algorithm is deployed to obtain\nresults and compared to select suitable algorithm for efficient use of\nresources.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 10:23:58 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Ngenzi", "Alexander", ""], ["R", "Selvarani", ""], ["Nair", "Suchithra R.", ""]]}, {"id": "1505.00599", "submitter": "Emmanuel Godard", "authors": "J\\'er\\'emie Chalopin, Emmanuel Godard and Antoine Naudin", "title": "Anonymous Graph Exploration with Binoculars", "comments": "Preliminary version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the exploration of networks by a mobile agent. It is long\nknown that, without global information about the graph, it is not possible to\nmake the agent halts after the exploration except if the graph is a tree. We\ntherefore endow the agent with binoculars, a sensing device that can show the\nlocal structure of the environment at a constant distance of the agent current\nlocation. We show that, with binoculars, it is possible to explore and halt in\na large class of non-tree networks. We give a complete characterization of the\nclass of networks that can be explored using binoculars using standard notions\nof discrete topology. Our characterization is constructive, we present an\nExploration algorithm that is universal; this algorithm explores any network\nexplorable with binoculars, and never halts in non-explorable networks.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 11:46:59 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Chalopin", "J\u00e9r\u00e9mie", ""], ["Godard", "Emmanuel", ""], ["Naudin", "Antoine", ""]]}, {"id": "1505.01120", "submitter": "Oren Segal", "authors": "Oren Segal, Philip Colangelo, Nasibeh Nasiri, Zhuo Qian, Martin\n  Margala", "title": "SparkCL: A Unified Programming Framework for Accelerators on\n  Heterogeneous Clusters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce SparkCL, an open source unified programming framework based on\nJava, OpenCL and the Apache Spark framework. The motivation behind this work is\nto bring unconventional compute cores such as FPGAs/GPUs/APUs/DSPs and future\ncore types into mainstream programming use. The framework allows equal\ntreatment of different computing devices under the Spark framework and\nintroduces the ability to offload computations to acceleration devices. The new\nframework is seamlessly integrated into the standard Spark framework via a\nJava-OpenCL device programming layer which is based on Aparapi and a Spark\nprogramming layer that includes new kernel function types and modified Spark\ntransformations and actions. The framework allows a single code base to target\nany type of compute core that supports OpenCL and easy integration of new core\ntypes into a Spark cluster.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 18:34:25 GMT"}], "update_date": "2015-05-06", "authors_parsed": [["Segal", "Oren", ""], ["Colangelo", "Philip", ""], ["Nasiri", "Nasibeh", ""], ["Qian", "Zhuo", ""], ["Margala", "Martin", ""]]}, {"id": "1505.01139", "submitter": "Hesham Mostafa", "authors": "Hesham Mostafa, Lorenz K. M\\\"uller, Giacomo Indiveri", "title": "An event-based architecture for solving constraint satisfaction problems", "comments": "First two authors contributed equally to this work", "journal-ref": "Nature Communications 6, Article number: 8941 (2015), pg. 1-10", "doi": "10.1038/ncomms9941", "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constraint satisfaction problems (CSPs) are typically solved using\nconventional von Neumann computing architectures. However, these architectures\ndo not reflect the distributed nature of many of these problems and are thus\nill-suited to solving them. In this paper we present a hybrid analog/digital\nhardware architecture specifically designed to solve such problems. We cast\nCSPs as networks of stereotyped multi-stable oscillatory elements that\ncommunicate using digital pulses, or events. The oscillatory elements are\nimplemented using analog non-stochastic circuits. The non-repeating phase\nrelations among the oscillatory elements drive the exploration of the solution\nspace. We show that this hardware architecture can yield state-of-the-art\nperformance on a number of CSPs under reasonable assumptions on the\nimplementation. We present measurements from a prototype electronic chip to\ndemonstrate that a physical implementation of the proposed architecture is\nrobust to practical non-idealities and to validate the theory proposed.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 13:23:48 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Mostafa", "Hesham", ""], ["M\u00fcller", "Lorenz K.", ""], ["Indiveri", "Giacomo", ""]]}, {"id": "1505.01259", "submitter": "Rodolfo Conde", "authors": "Rodolfo Conde and Sergio Rajsbaum", "title": "The solvability of consensus in iterated models extended with\n  safe-consensus", "comments": "49 pages, A preliminar version of the main results appeared in the\n  SIROCCO 2014 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The safe-consensus task was introduced by Afek, Gafni and Lieber (DISC'09) as\na weakening of the classic consensus. When there is concurrency, the consensus\noutput can be arbitrary, not even the input of any process. They showed that\nsafe-consensus is equivalent to consensus, in a wait-free system. We study the\nsolvability of consensus in three shared memory iterated models extended with\nthe power of safe-consensus black boxes. In the first model, for the $i$-th\niteration, processes write to the memory, invoke safe-consensus boxes and\nfinally they snapshot the memory. We show that in this model, any wait-free\nimplementation of consensus requires $\\binom{n}{2}$ safe-consensus black-boxes\nand this bound is tight. In a second iterated model, the processes write to\nmemory, then they snapshot it and finally they invoke safe-consensus boxes. We\nprove that in this model, consensus cannot be implemented. In the last iterated\nmodel, processes first invoke safe-consensus, then they write to memory and\nfinally they snapshot it. We show that this model is equivalent to the previous\nmodel and thus consensus cannot be implemented.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2015 06:31:01 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Conde", "Rodolfo", ""], ["Rajsbaum", "Sergio", ""]]}, {"id": "1505.01448", "submitter": "Thomas Leibovici", "authors": "Thomas Leibovici", "title": "Taking back control of HPC file systems with Robinhood Policy Engine", "comments": "International Workshop on the Lustre Ecosystem: Challenges and\n  Opportunities, March 2015, Annapolis MD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, the largest Lustre file systems store billions of entries. On such\nsystems, classic tools based on namespace scanning become unusable. Operations\nsuch as managing file lifetime, scheduling data copies, and generating overall\nfilesystem statistics become painful as they require collecting, sorting and\naggregating information for billions of records. Robinhood Policy Engine is an\nopen source software developed to address these challenges. It makes it\npossible to schedule automatic actions on huge numbers of filesystem entries.\nIt also gives a synthetic understanding of file systems contents by providing\noverall statistics about data ownership, age and size profiles. Even if it can\nbe used with any POSIX filesystem, Robinhood supports Lustre specific features\nlike OSTs, pools, HSM, ChangeLogs, and DNE. It implements specific support for\nthese features, and takes advantage of them to manage Lustre file systems\nefficiently.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2015 18:14:56 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Leibovici", "Thomas", ""]]}, {"id": "1505.01765", "submitter": "Weikuan Yu", "authors": "Teng Wang, Sarp Oral, Michael Pritchard, Kevin Vasko, Weikuan Yu", "title": "Development of a Burst Buffer System for Data-Intensive Applications", "comments": "International Workshop on the Lustre Ecosystem: Challenges and\n  Opportunities, March 2015, Annapolis MD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern parallel filesystems such as Lustre are designed to provide high,\nscalable I/O bandwidth in response to growing I/O requirements; however, the\nbursty I/O characteristics of many data-intensive scientific applications make\nit difficult for back-end parallel filesystems to efficiently handle I/O\nrequests. A burst buffer system, through which data can be temporarily buffered\nvia high-performance storage mediums, allows for gradual flushing of data to\nback-end filesystems. In this paper, we explore issues surrounding the\ndevelopment of a burst buffer system for data-intensive scientific\napplications. Our initial results demonstrate that utilizing a burst buffer\nsystem on top of the Lustre filesystem shows promise for dealing with the\nintense I/O traffic generated by application checkpointing.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 16:31:21 GMT"}], "update_date": "2015-05-08", "authors_parsed": [["Wang", "Teng", ""], ["Oral", "Sarp", ""], ["Pritchard", "Michael", ""], ["Vasko", "Kevin", ""], ["Yu", "Weikuan", ""]]}, {"id": "1505.01919", "submitter": "Puja Gupta", "authors": "Puja Gupta", "title": "Characterization of Performance Anomalies in Hadoop", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the huge variety of data and equally large-scale systems, there is not a\nunique execution setting for these systems which can guarantee the best\nperformance for each query. In this project, we tried so study the impact of\ndifferent execution settings on execution time of workloads by varying them one\nat a time. Using the data from these experiments, a decision tree was built\nwhere each internal node represents the execution parameter, each branch\nrepresents value chosen for the parameter and each leaf node represents a range\nfor execution time in minutes. The attribute in the decision tree to split the\ndataset on is selected based on the maximum information gain or lowest entropy.\nOnce the tree is trained with the training samples, this tree can be used to\nget approximate range for the expected execution time. When the actual\nexecution time differs from this expected value, a performance anomaly can be\ndetected. For a test dataset with 400 samples, 99% of samples had actual\nexecution time in the range predicted time by the decision tree. Also on\nanalyzing the constructed tree, an idea about what configuration can give\nbetter performance for a given workload can be obtained. Initial experiments\nsuggest that the impact an execution parameter can have on the target attribute\n(here execution time) can be related to the distance of that feature node from\nthe root of the constructed decision tree. From initial results the percent\nchange in the values of the target attribute for various value of the feature\nnode which is closer to the root is 6 times larger than when that same iii\nfeature node is away from the root node. This observation will depend on how\nwell the decision tree was trained and may not be true for every case.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2015 03:24:33 GMT"}, {"version": "v2", "created": "Wed, 13 May 2015 03:50:26 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Gupta", "Puja", ""]]}, {"id": "1505.01998", "submitter": "Artur Gramacki", "authors": "Witold Andrzejewski, Artur Gramacki, Jaros{\\l}aw Gramacki", "title": "Density Estimations for Approximate Query Processing on SIMD\n  Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate query processing (AQP) is an interesting alternative for exact\nquery processing. It is a tool for dealing with the huge data volumes where\nresponse time is more important than perfect accuracy (this is typically the\ncase during initial phase of data exploration). There are many techniques for\nAQP, one of them is based on probability density functions (PDF). PDFs are\ntypically calculated using nonparametric data-driven methods. One of the most\npopular nonparametric method is the kernel density estimator (KDE). However, a\nvery serious drawback of using KDEs is the large number of calculations\nrequired to compute them. The shape of final density function is very sensitive\nto an entity called bandwidth or smoothing parameter. Calculating it's optimal\nvalue is not a trivial task and in general is very time consuming. In this\npaper we investigate the possibility of utilizing two SIMD architectures: SSE\nCPU extensions and NVIDIA's CUDA architecture to accelerate finding of the\nbandwidth. Our experiments show orders of magnitude improvements over a simple\nsequential implementation of classical algorithms used for that task.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2015 11:16:49 GMT"}], "update_date": "2015-05-11", "authors_parsed": [["Andrzejewski", "Witold", ""], ["Gramacki", "Artur", ""], ["Gramacki", "Jaros\u0142aw", ""]]}, {"id": "1505.02322", "submitter": "Tuomo Lempi\\\"ainen", "authors": "Tuomo Lempi\\\"ainen", "title": "Ability to Count Is Worth $\\Theta(\\Delta)$ Rounds", "comments": "23 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hella et al. (PODC 2012, Distributed Computing 2015) identified seven\ndifferent models of distributed computing - one of which is the port-numbering\nmodel - and provided a complete classification of their computational power\nrelative to each other. However, one of their simulation results involves an\nadditive overhead of $2\\Delta-2$ communication rounds, and it was not clear, if\nthis is actually optimal. In this paper we give a positive answer: there is a\nmatching linear-in-$\\Delta$ lower bound. This closes the final gap in our\nunderstanding of the models, with respect to the number of communication\nrounds.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2015 22:03:32 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Lempi\u00e4inen", "Tuomo", ""]]}, {"id": "1505.02435", "submitter": "Gabriele D'Angelo", "authors": "Gabriele D'Angelo and Stefano Ferretti and Moreno Marzolla", "title": "Cloud for Gaming", "comments": "Encyclopedia of Computer Graphics and Games. Newton Lee (Editor).\n  Springer International Publishing, 2015, ISBN 978-3-319-08234-9", "journal-ref": null, "doi": "10.1007/978-3-319-08234-9_39-1", "report-no": null, "categories": "cs.DC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud for Gaming refers to the use of cloud computing technologies to build\nlarge-scale gaming infrastructures, with the goal of improving scalability and\nresponsiveness, improve the user's experience and enable new business models.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2015 21:04:48 GMT"}, {"version": "v2", "created": "Tue, 17 May 2016 13:23:08 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["D'Angelo", "Gabriele", ""], ["Ferretti", "Stefano", ""], ["Marzolla", "Moreno", ""]]}, {"id": "1505.02586", "submitter": "Georg Hager", "authors": "Johannes Hofmann, Dietmar Fey, Jan Eitzinger, Georg Hager, Gerhard\n  Wellein", "title": "Performance analysis of the Kahan-enhanced scalar product on current\n  multicore processors", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": "10.1007/978-3-319-32149-3_7", "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the performance characteristics of a numerically enhanced\nscalar product (dot) kernel loop that uses the Kahan algorithm to compensate\nfor numerical errors, and describe efficient SIMD-vectorized implementations on\nrecent Intel processors. Using low-level instruction analysis and the\nexecution-cache-memory (ECM) performance model we pinpoint the relevant\nperformance bottlenecks for single-core and thread-parallel execution, and\npredict performance and saturation behavior. We show that the Kahan-enhanced\nscalar product comes at almost no additional cost compared to the naive\n(non-Kahan) scalar product if appropriate low-level optimizations, notably SIMD\nvectorization and unrolling, are applied. We also investigate the impact of\narchitectural changes across four generations of Intel Xeon processors.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2015 12:34:54 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Hofmann", "Johannes", ""], ["Fey", "Dietmar", ""], ["Eitzinger", "Jan", ""], ["Hager", "Georg", ""], ["Wellein", "Gerhard", ""]]}, {"id": "1505.02656", "submitter": "Henri Doreau", "authors": "Henri Doreau", "title": "Distributed Lustre activity tracking", "comments": "International Workshop on the Lustre Ecosystem: Challenges and\n  Opportunities, March 2015, Annapolis MD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous administration tools and techniques require near real time vision of\nthe activity occurring on a distributed filesystem. The changelog facility\nprovided by Lustre to address this need suffers limitations in terms of\nscalability and flexibility. We have been working on reducing those limitations\nby enhancing Lustre itself and developing external tools such as Lustre\nChangeLog Aggregate and Publish (LCAP) proxy. Beyond the ability to distribute\nchangelog processing, this effort aims at opening new prospectives by making\nthe changelog stream simpler to leverage for various purposes.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2015 14:59:12 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Doreau", "Henri", ""]]}, {"id": "1505.02690", "submitter": "Petr  Kuznetsov", "authors": "Carole Delporte-Gallet, Hugues Fauconnier, Petr Kuznetsov, Eric\n  Ruppert", "title": "On the Space Complexity of Set Agreement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $k$-set agreement problem is a generalization of the classical consensus\nproblem in which processes are permitted to output up to $k$ different input\nvalues. In a system of $n$ processes, an $m$-obstruction-free solution to the\nproblem requires termination only in executions where the number of processes\ntaking steps is eventually bounded by $m$. This family of progress conditions\ngeneralizes wait-freedom ($m=n$) and obstruction-freedom ($m=1$). In this\npaper, we prove upper and lower bounds on the number of registers required to\nsolve $m$-obstruction-free $k$-set agreement, considering both one-shot and\nrepeated formulations. In particular, we show that repeated $k$ set agreement\ncan be solved using $n+2m-k$ registers and establish a nearly matching lower\nbound of $n+m-k$.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2015 16:19:35 GMT"}, {"version": "v2", "created": "Tue, 19 May 2015 08:35:05 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Delporte-Gallet", "Carole", ""], ["Fauconnier", "Hugues", ""], ["Kuznetsov", "Petr", ""], ["Ruppert", "Eric", ""]]}, {"id": "1505.02826", "submitter": "Bing Li", "authors": "Xiuli Hu, Pangbei Hong, Bing Li", "title": "Benefit of Multipath TCP on the Stability of Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multipath-TCP receives a lot of attention recently and can potentially\nimprove quality of service for both private and commercial users. It leverages\nthe multiple available paths and send packets through all the available paths.\nThe growing of Mutipath TCP has received a growing interest from both\nresearchers who publish a growing number of articles on the topic and the\nvendors since Apple has decided to use Multipath TCP on its smartphones and\ntablets to support the Siri voice recognition application. In this paper, we\nstudy the performance of Multipath TCP from its impact on the stability of the\nnetwork. In particular, we study three scenarios, Internet, which is the\nlargest networks and involves heterogeneous traffic, data center, which is\nsmaller but has different traffic patterns compared with Internet scale network\nand wireless network, whose energy consumption also needs to be considered. Our\nstudy shows that stability is affected but not seriouly for Internet and\nwireless network, but datacenter network stability is seriously affected due to\nits bursty traffic pattern.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2015 22:39:40 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Hu", "Xiuli", ""], ["Hong", "Pangbei", ""], ["Li", "Bing", ""]]}, {"id": "1505.02884", "submitter": "Po-Huei Liang", "authors": "Po-Huei Liang and Jiann-Min Yang", "title": "Evaluation of Two-Level Load Balancing Framework in Cloud Environment", "comments": "11 pages", "journal-ref": "International Journal of Computer Science & Information Technology\n  (IJCSIT) Vol 7, No 2, April 2015", "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With technological advancements and constant changes of Internet, cloud\ncomputing has been today's trend. With the lower cost and convenience of cloud\ncomputing services, users have increasingly put their Web resources and\ninformation in the cloud environment. The availability and reliability of the\nclient systems will become increasingly important. Today cloud applications\nslightest interruption, the impact will be significant for users. It is an\nimportant issue that how to ensure reliability and stability of the cloud\nsites. Load balancing would be one good solution. This paper presents a\nframework for global server load balancing of the Web sites in a cloud with\ntwo-level load balancing model. The proposed framework is intended for adapting\nan open-source load-balancing system and the framework allows the network\nservice provider to deploy a load balancer in different data centers\ndynamically while the customers need more load balancers for increasing the\navailability.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 06:20:57 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Liang", "Po-Huei", ""], ["Yang", "Jiann-Min", ""]]}, {"id": "1505.02951", "submitter": "Jo\\~ao Louren\\c{c}o", "authors": "Diogo G. Sousa and Ricardo J. Dias and Carla Ferreira and Jo\\~ao M.\n  Louren\\c{c}o", "title": "Preventing Atomicity Violations with Contracts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software developers are expected to protect concurrent accesses to shared\nregions of memory with some mutual exclusion primitive that ensures atomicity\nproperties to a sequence of program statements. This approach prevents data\nraces but may fail to provide all necessary correctness properties.The\ncomposition of correlated atomic operations without further synchronization may\ncause atomicity violations. Atomic violations may be avoided by grouping the\ncorrelated atomic regions in a single larger atomic scope. Concurrent programs\nare particularly prone to atomicity violations when they use services provided\nby third party packages or modules, since the programmer may fail to identify\nwhich services are correlated. In this paper we propose to use contracts for\nconcurrency, where the developer of a module writes a set of contract terms\nthat specify which methods are correlated and must be executed in the same\natomic scope. These contracts are then used to verify the correctness of the\nmain program with respect to the usage of the module(s). If a contract is well\ndefined and complete, and the main program respects it, then the program is\nsafe from atomicity violations with respect to that module. We also propose a\nstatic analysis based methodology to verify contracts for concurrency that we\napplied to some real-world software packages. The bug we found in Tomcat 6.0\nwas immediately acknowledged and corrected by its development team.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 10:47:29 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Sousa", "Diogo G.", ""], ["Dias", "Ricardo J.", ""], ["Ferreira", "Carla", ""], ["Louren\u00e7o", "Jo\u00e3o M.", ""]]}, {"id": "1505.03015", "submitter": "Pier Stanislao Paolucci", "authors": "Pier Stanislao Paolucci, Roberto Ammendola, Andrea Biagioni, Ottorino\n  Frezza, Francesca Lo Cicero, Alessandro Lonardo, Michele Martinelli, Elena\n  Pastorelli, Francesco Simula, Piero Vicini", "title": "Power, Energy and Speed of Embedded and Server Multi-Cores applied to\n  Distributed Simulation of Spiking Neural Networks: ARM in NVIDIA Tegra vs\n  Intel Xeon quad-cores", "comments": "4 pages, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This short note regards a comparison of instantaneous power, total energy\nconsumption, execution time and energetic cost per synaptic event of a spiking\nneural network simulator (DPSNN-STDP) distributed on MPI processes when\nexecuted either on an embedded platform (based on a dual socket quad-core ARM\nplatform) or a server platform (INTEL-based quad-core dual socket platform). We\nalso compare the measure with those reported by leading custom and semi-custom\ndesigns: TrueNorth and SpiNNaker. In summary, we observed that: 1- we spent 2.2\nmicro-Joule per simulated event on the \"embedded platform\", approx. 4.4 times\nlower than what was spent by the \"server platform\"; 2- the instantaneous power\nconsumption of the \"embedded platform\" was 14.4 times better than the \"server\"\none; 3- the server platform is a factor 3.3 faster. The \"embedded platform\" is\nmade of NVIDIA Jetson TK1 boards, interconnected by Ethernet, each mounting a\nTegra K1 chip including a quad-core ARM Cortex-A15 at 2.3GHz. The \"server\nplatform\" is based on dual-socket quad-core Intel Xeon CPUs (E5620 at 2.4GHz).\nThe measures were obtained with the DPSNN-STDP simulator (Distributed Simulator\nof Polychronous Spiking Neural Network with synaptic Spike Timing Dependent\nPlasticity) developed by INFN, that already proved its efficient scalability\nand execution speed-up on hundreds of similar \"server\" cores and MPI processes,\napplied to neural nets composed of several billions of synapses.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 14:14:15 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Paolucci", "Pier Stanislao", ""], ["Ammendola", "Roberto", ""], ["Biagioni", "Andrea", ""], ["Frezza", "Ottorino", ""], ["Cicero", "Francesca Lo", ""], ["Lonardo", "Alessandro", ""], ["Martinelli", "Michele", ""], ["Pastorelli", "Elena", ""], ["Simula", "Francesco", ""], ["Vicini", "Piero", ""]]}, {"id": "1505.03060", "submitter": "Silvia Crafa", "authors": "Silvia Crafa and Luca Tronchin", "title": "Actors vs Shared Memory: two models at work on Big Data application\n  frameworks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work aims at analyzing how two different concurrency models, namely the\nshared memory model and the actor model, can influence the development of\napplications that manage huge masses of data, distinctive of Big Data\napplications. The paper compares the two models by analyzing a couple of\nconcrete projects based on the MapReduce and Bulk Synchronous Parallel\nalgorithmic schemes. Both projects are doubly implemented on two concrete\nplatforms: Akka Cluster and Managed X10. The result is both a conceptual\ncomparison of models in the Big Data Analytics scenario, and an experimental\nanalysis based on concrete executions on a cluster platform.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 15:32:58 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Crafa", "Silvia", ""], ["Tronchin", "Luca", ""]]}, {"id": "1505.03469", "submitter": "Petr  Kuznetsov", "authors": "Swan Dubois, Rachid Guerraoui, Petr Kuznetsov, Franck Petit, Pierre\n  Sens", "title": "The Weakest Failure Detector for Eventual Consistency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In its classical form, a consistent replicated service requires all replicas\nto witness the same evolution of the service state. Assuming a message-passing\nenvironment with a majority of correct processes, the necessary and sufficient\ninformation about failures for implementing a general state machine replication\nscheme ensuring consistency is captured by the {\\Omega} failure detector. This\npaper shows that in such a message-passing environment, {\\Omega} is also the\nweakest failure detector to implement an eventually consistent replicated\nservice, where replicas are expected to agree on the evolution of the service\nstate only after some (a priori unknown) time. In fact, we show that {\\Omega}\nis the weakest to implement eventual consistency in any message-passing\nenvironment, i.e., under any assumption on when and where failures might occur.\nEnsuring (strong) consistency in any environment requires, in addition to\n{\\Omega}, the quorum failure detector {\\Sigma}. Our paper thus captures, for\nthe first time, an exact computational difference be- tween building a\nreplicated state machine that ensures consistency and one that only ensures\neventual consistency.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2015 17:38:19 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Dubois", "Swan", ""], ["Guerraoui", "Rachid", ""], ["Kuznetsov", "Petr", ""], ["Petit", "Franck", ""], ["Sens", "Pierre", ""]]}, {"id": "1505.03509", "submitter": "Giuseppe Antonio Di Luna", "authors": "Giuseppe Antonio Di Luna and Roberto Baldoni", "title": "Investigating the Cost of Anonymity on Dynamic Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the difficulty of counting nodes in a synchronous\ndynamic network where nodes share the same identifier, they communicate by\nusing a broadcast with unlimited bandwidth and, at each synchronous round,\nnetwork topology may change. To count in such setting, it has been shown that\nthe presence of a leader is necessary. We focus on a particularly interesting\nsubset of dynamic networks, namely \\textit{Persistent Distance} - ${\\cal\nG}($PD$)_{h}$, in which each node has a fixed distance from the leader across\nrounds and such distance is at most $h$. In these networks the dynamic diameter\n$D$ is at most $2h$. We prove the number of rounds for counting in ${\\cal\nG}($PD$)_{2}$ is at least logarithmic with respect to the network size $|V|$.\nThanks to this result, we show that counting on any dynamic anonymous network\nwith $D$ constant w.r.t. $|V|$ takes at least $D+ \\Omega(\\text{log}\\, |V| )$\nrounds where $\\Omega(\\text{log}\\, |V|)$ represents the additional cost to be\npayed for handling anonymity. At the best of our knowledge this is the fist non\ntrivial, i.e. different from $\\Omega(D)$, lower bounds on counting in anonymous\ninterval connected networks with broadcast and unlimited bandwith.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2015 19:51:58 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Di Luna", "Giuseppe Antonio", ""], ["Baldoni", "Roberto", ""]]}, {"id": "1505.03532", "submitter": "Lingfei Wu", "authors": "Lingfei Wu, Kesheng Wu, Alex Sim, Michael Churchill, Jong Y. Choi,\n  Andreas Stathopoulos, Cs Chang and Scott Klasky", "title": "Towards Real-Time Detection and Tracking of Spatio-Temporal Features:\n  Blob-Filaments in Fusion Plasma", "comments": "14 pages, 40 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE cs.DS physics.plasm-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel algorithm and implementation of real-time identification and tracking\nof blob-filaments in fusion reactor data is presented. Similar spatio-temporal\nfeatures are important in many other applications, for example, ignition\nkernels in combustion and tumor cells in a medical image. This work presents an\napproach for extracting these features by dividing the overall task into three\nsteps: local identification of feature cells, grouping feature cells into\nextended feature, and tracking movement of feature through overlapping in\nspace. Through our extensive work in parallelization, we demonstrate that this\napproach can effectively make use of a large number of compute nodes to detect\nand track blob-filaments in real time in fusion plasma. On a set of 30GB fusion\nsimulation data, we observed linear speedup on 1024 processes and completed\nblob detection in less than three milliseconds using Edison, a Cray XC30 system\nat NERSC.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2015 20:01:14 GMT"}, {"version": "v2", "created": "Sat, 18 Jun 2016 13:22:05 GMT"}, {"version": "v3", "created": "Sat, 2 Jul 2016 17:19:52 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Wu", "Lingfei", ""], ["Wu", "Kesheng", ""], ["Sim", "Alex", ""], ["Churchill", "Michael", ""], ["Choi", "Jong Y.", ""], ["Stathopoulos", "Andreas", ""], ["Chang", "Cs", ""], ["Klasky", "Scott", ""]]}, {"id": "1505.03759", "submitter": "Keren Zhou", "authors": "Keren Zhou, Guocheng Niu, Wuzhao Zhang, Xueqi Li, Wenqin Liu", "title": "Parse Concurrent Data Structures: BST as an Example", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing concurrent data structures should follow some basic rules. By\nseparating the algorithms into two phases, we present guidelines for scalable\ndata structures, with a analysis model based on the Amadal's law. To the best\nof our knowledge, we are the first to formalize a practical model for measuring\nconcurrent structures' speedup. We also build some edge-cutting BSTs following\nour principles, testing them under different workloads. The result provides\ncompelling evidence to back the our guidelines, and shows that our theory is\nuseful for reasoning the varied speedup.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2015 15:38:21 GMT"}, {"version": "v2", "created": "Sat, 30 May 2015 16:29:50 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Zhou", "Keren", ""], ["Niu", "Guocheng", ""], ["Zhang", "Wuzhao", ""], ["Li", "Xueqi", ""], ["Liu", "Wenqin", ""]]}, {"id": "1505.03799", "submitter": "Tsvetomira Radeva", "authors": "Mohsen Ghaffari, Cameron Musco, Tsvetomira Radeva, Nancy Lynch", "title": "Distributed House-Hunting in Ant Colonies", "comments": "To appear in PODC 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the study of the ant colony house-hunting problem from a\ndistributed computing perspective. When an ant colony's nest becomes unsuitable\ndue to size constraints or damage, the colony must relocate to a new nest. The\ntask of identifying and evaluating the quality of potential new nests is\ndistributed among all ants. The ants must additionally reach consensus on a\nfinal nest choice and the full colony must be transported to this single new\nnest. Our goal is to use tools and techniques from distributed computing theory\nin order to gain insight into the house-hunting process.\n  We develop a formal model for the house-hunting problem inspired by the\nbehavior of the Temnothorax genus of ants. We then show a \\Omega(log n) lower\nbound on the time for all n ants to agree on one of k candidate nests. We also\npresent two algorithms that solve the house-hunting problem in our model. The\nfirst algorithm solves the problem in optimal O(log n) time but exhibits some\nfeatures not characteristic of natural ant behavior. The second algorithm runs\nin O(k log n) time and uses an extremely simple and natural rule for each ant\nto decide on the new nest.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2015 17:09:54 GMT"}], "update_date": "2015-05-15", "authors_parsed": [["Ghaffari", "Mohsen", ""], ["Musco", "Cameron", ""], ["Radeva", "Tsvetomira", ""], ["Lynch", "Nancy", ""]]}, {"id": "1505.03819", "submitter": "George Teodoro", "authors": "George Teodoro, Tahsin Kurc, Guilherme Andrade, Jun Kong, Renato\n  Ferreira, Joel Saltz", "title": "Performance Analysis and Efficient Execution on Systems with multi-core\n  CPUs, GPUs and MICs", "comments": "22 pages, 12 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We carry out a comparative performance study of multi-core CPUs, GPUs and\nIntel Xeon Phi (Many Integrated Core - MIC) with a microscopy image analysis\napplication. We experimentally evaluate the performance of computing devices on\ncore operations of the application. We correlate the observed performance with\nthe characteristics of computing devices and data access patterns, computation\ncomplexities, and parallelization forms of the operations. The results show a\nsignificant variability in the performance of operations with respect to the\ndevice used. The performances of operations with regular data access are\ncomparable or sometimes better on a MIC than that on a GPU. GPUs are more\nefficient than MICs for operations that access data irregularly, because of the\nlower bandwidth of the MIC for random data accesses. We propose new\nperformance-aware scheduling strategies that consider variabilities in\noperation speedups. Our scheduling strategies significantly improve application\nperformance compared to classic strategies in hybrid configurations.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2015 18:03:21 GMT"}], "update_date": "2015-05-15", "authors_parsed": [["Teodoro", "George", ""], ["Kurc", "Tahsin", ""], ["Andrade", "Guilherme", ""], ["Kong", "Jun", ""], ["Ferreira", "Renato", ""], ["Saltz", "Joel", ""]]}, {"id": "1505.03851", "submitter": "Guy Steele", "authors": "Guy L. Steele Jr. (Oracle Labs) and Jean-Baptiste Tristan (Oracle\n  Labs)", "title": "Using Butterfly-Patterned Partial Sums to Optimize GPU Memory Accesses\n  for Drawing from Discrete Distributions", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a technique for drawing values from discrete distributions, such\nas sampling from the random variables of a mixture model, that avoids computing\na complete table of partial sums of the relative probabilities. A table of\nalternate (\"butterfly-patterned\") form is faster to compute, making better use\nof coalesced memory accesses. From this table, complete partial sums are\ncomputed on the fly during a binary search. Measurements using an NVIDIA Titan\nBlack GPU show that for a sufficiently large number of clusters or topics (K >\n200), this technique alone more than doubles the speed of a latent Dirichlet\nallocation (LDA) application already highly tuned for GPU execution.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2015 19:46:28 GMT"}], "update_date": "2015-05-15", "authors_parsed": [["Steele", "Guy L.", "Jr.", "Oracle Labs"], ["Tristan", "Jean-Baptiste", "", "Oracle\n  Labs"]]}, {"id": "1505.04086", "submitter": "Georgios Rokos", "authors": "Georgios Rokos and Gerard Gorman and Paul H J Kelly", "title": "A Fast and Scalable Graph Coloring Algorithm for Multi-core and\n  Many-core Architectures", "comments": "To appear in the proceedings of Euro Par 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Irregular computations on unstructured data are an important class of\nproblems for parallel programming. Graph coloring is often an important\npreprocessing step, e.g. as a way to perform dependency analysis for safe\nparallel execution. The total run time of a coloring algorithm adds to the\noverall parallel overhead of the application whereas the number of colors used\ndetermines the amount of exposed parallelism. A fast and scalable coloring\nalgorithm using as few colors as possible is vital for the overall parallel\nperformance and scalability of many irregular applications that depend upon\nruntime dependency analysis.\n  Catalyurek et al. have proposed a graph coloring algorithm which relies on\nspeculative, local assignment of colors. In this paper we present an improved\nversion which runs even more optimistically with less thread synchronization\nand reduced number of conflicts compared to Catalyurek et al.'s algorithm. We\nshow that the new technique scales better on multi-core and many-core systems\nand performs up to 1.5x faster than its predecessor on graphs with high-degree\nvertices, while keeping the number of colors at the same near-optimal levels.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 15:03:30 GMT"}, {"version": "v2", "created": "Mon, 18 May 2015 16:09:31 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Rokos", "Georgios", ""], ["Gorman", "Gerard", ""], ["Kelly", "Paul H J", ""]]}, {"id": "1505.04134", "submitter": "Georgios Rokos", "authors": "Georgios Rokos and Gerard J. Gorman and Paul H. J. Kelly", "title": "An Interrupt-Driven Work-Sharing For-Loop Scheduler", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a parallel for-loop scheduler which is based on\nwork-stealing principles but runs under a completely cooperative scheme. POSIX\nsignals are used by idle threads to interrupt left-behind workers, which in\nturn decide what portion of their workload can be given to the requester. We\ncall this scheme Interrupt-Driven Work-Sharing (IDWS). This article describes\nhow IDWS works, how it can be integrated into any POSIX-compliant OpenMP\nimplementation and how a user can manually replace OpenMP parallel for-loops\nwith IDWS in existing POSIX-compliant C++ applications. Additionally, we\nmeasure its performance using both a synthetic benchmark with varying\ndistributions of workload across the iteration space and a real-life\napplication on Sandy Bridge and Xeon Phi systems. Regardless the workload\ndistribution and the underlying hardware, IDWS is always the best or among the\nbest-performing strategies, providing a good all-around solution to the\nscheduling-choice dilemma.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 17:30:17 GMT"}, {"version": "v2", "created": "Mon, 18 May 2015 16:04:00 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Rokos", "Georgios", ""], ["Gorman", "Gerard J.", ""], ["Kelly", "Paul H. J.", ""]]}, {"id": "1505.04224", "submitter": "Hammurabi Mendes", "authors": "Hammurabi Mendes and Maurice Herlihy", "title": "Tight Bounds for Connectivity and Set Agreement in Byzantine Synchronous\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show that the protocol complex of a Byzantine synchronous\nsystem can remain $(k - 1)$-connected for up to $\\lceil t/k \\rceil$ rounds,\nwhere $t$ is the maximum number of Byzantine processes, and $t \\ge k \\ge 1$.\nThis topological property implies that $\\lceil t/k \\rceil + 1$ rounds are\nnecessary to solve $k$-set agreement in Byzantine synchronous systems, compared\nto $\\lfloor t/k \\rfloor + 1$ rounds in synchronous crash-failure systems. We\nalso show that our connectivity bound is tight as we indicate solutions to\nByzantine $k$-set agreement in exactly $\\lceil t/k \\rceil + 1$ synchronous\nrounds, at least when $n$ is suitably large compared to $t$. In conclusion, we\nsee how Byzantine failures can potentially require one extra round to solve\n$k$-set agreement, and, for $n$ suitably large compared to $t$, at most that.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2015 01:04:18 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2015 13:25:32 GMT"}, {"version": "v3", "created": "Wed, 8 Feb 2017 20:45:37 GMT"}], "update_date": "2017-02-10", "authors_parsed": [["Mendes", "Hammurabi", ""], ["Herlihy", "Maurice", ""]]}, {"id": "1505.04308", "submitter": "Avery Miller", "authors": "Christian Glacet, Avery Miller, Andrzej Pelc", "title": "Time vs. Information Tradeoffs for Leader Election in Anonymous Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The leader election task calls for all nodes of a network to agree on a\nsingle node. If the nodes of the network are anonymous, the task of leader\nelection is formulated as follows: every node $v$ of the network must output a\nsimple path, coded as a sequence of port numbers, such that all these paths end\nat a common node, the leader. In this paper, we study deterministic leader\nelection in anonymous trees.\n  Our aim is to establish tradeoffs between the allocated time $\\tau$ and the\namount of information that has to be given $\\textit{a priori}$ to the nodes to\nenable leader election in time $\\tau$ in all trees for which leader election in\nthis time is at all possible. Following the framework of $\\textit{algorithms\nwith advice}$, this information (a single binary string) is provided to all\nnodes at the start by an oracle knowing the entire tree. The length of this\nstring is called the $\\textit{size of advice}$. For an allocated time $\\tau$,\nwe give upper and lower bounds on the minimum size of advice sufficient to\nperform leader election in time $\\tau$.\n  We consider $n$-node trees of diameter $diam \\leq D$. While leader election\nin time $diam$ can be performed without any advice, for time $diam-1$ we give\ntight upper and lower bounds of $\\Theta (\\log D)$. For time $diam-2$ we give\ntight upper and lower bounds of $\\Theta (\\log D)$ for even values of $diam$,\nand tight upper and lower bounds of $\\Theta (\\log n)$ for odd values of $diam$.\nFor the time interval $[\\beta \\cdot diam, diam-3]$ for constant $\\beta >1/2$,\nwe prove an upper bound of $O(\\frac{n\\log n}{D})$ and a lower bound of\n$\\Omega(\\frac{n}{D})$, the latter being valid whenever $diam$ is odd or when\nthe time is at most $diam-4$. Finally, for time $\\alpha \\cdot diam$ for any\nconstant $\\alpha <1/2$ (except for the case of very small diameters), we give\ntight upper and lower bounds of $\\Theta (n)$.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2015 18:41:40 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2015 03:45:23 GMT"}], "update_date": "2015-06-29", "authors_parsed": [["Glacet", "Christian", ""], ["Miller", "Avery", ""], ["Pelc", "Andrzej", ""]]}, {"id": "1505.04417", "submitter": "Gordon Inggs", "authors": "Gordon Inggs, David B. Thomas and Wayne Luk", "title": "A Domain Specific Approach to High Performance Heterogeneous Computing", "comments": "14 pages, preprint draft, minor revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users of heterogeneous computing systems face two problems: firstly, in\nunderstanding the trade-off relationships between the observable\ncharacteristics of their applications, such as latency and quality of the\nresult, and secondly, how to exploit knowledge of these characteristics to\nallocate work to distributed computing platforms efficiently. A domain specific\napproach addresses both of these problems. By considering a subset of\noperations or functions, models of the observable characteristics or domain\nmetrics may be formulated in advance, and populated at run-time for task\ninstances. These metric models can then be used to express the allocation of\nwork as a constrained integer program, which can be solved using heuristics,\nmachine learning or Mixed Integer Linear Programming (MILP) frameworks. These\nclaims are illustrated using the example domain of derivatives pricing in\ncomputational finance, with the domain metrics of workload latency or makespan\nand pricing accuracy. For a large, varied workload of 128 Black-Scholes and\nHeston model-based option pricing tasks, running upon a diverse array of 16\nMulticore CPUs, GPUs and FPGAs platforms, predictions made by models of both\nthe makespan and accuracy are generally within 10% of the run-time performance.\nWhen these models are used as inputs to machine learning and MILP-based\nworkload allocation approaches, a latency improvement of up to 24 and 270 times\nover the heuristic approach is seen.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2015 17:24:10 GMT"}, {"version": "v2", "created": "Sat, 23 May 2015 14:27:48 GMT"}, {"version": "v3", "created": "Sun, 10 Jan 2016 21:40:44 GMT"}, {"version": "v4", "created": "Mon, 14 Mar 2016 07:46:31 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Inggs", "Gordon", ""], ["Thomas", "David B.", ""], ["Luk", "Wayne", ""]]}, {"id": "1505.04514", "submitter": "Stephan Holzer", "authors": "Magnus M. Halldorsson, Stephan Holzer, Nancy Lynch", "title": "A Local Broadcast Layer for the SINR Network Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first algorithm that implements an abstract MAC (absMAC) layer\nin the Signal-to-Interference-plus-Noise-Ratio (SINR) wireless network model.\nWe first prove that efficient SINR implementations are not possible for the\nstandard absMAC specification. We modify that specification to an \"approximate\"\nversion that better suits the SINR model. We give an efficient algorithm to\nimplement the modified specification, and use it to derive efficient algorithms\nfor higher-level problems of global broadcast and consensus.\n  In particular, we show that the absMAC progress property has no efficient\nimplementation in terms of the SINR strong connectivity graph $G_{1-\\epsilon}$,\nwhich contains edges between nodes of distance at most $(1-\\epsilon)$ times the\ntransmission range, where $\\epsilon>0$ is a small constant that can be chosen\nby the user. This progress property bounds the time until a node is guaranteed\nto receive some message when at least one of its neighbors is transmitting.\n  To overcome this limitation, we introduce the slightly weaker notion of\napproximate progress into the absMAC specification. We provide a fast\nimplementation of the modified specification, based on decomposing a known\nalgorithm into local and global parts. We analyze our algorithm in terms of\nlocal parameters such as node degrees, rather than global parameters such as\nthe overall number of nodes. A key contribution is our demonstration that such\na local analysis is possible even in the presence of global interference.\n  Our absMAC algorithm leads to several new, efficient algorithms for solving\nhigher-level problems in the SINR model. Namely, by combining our algorithm\nwith known high-level algorithms, we obtain an improved algorithm for global\nsingle-message broadcast in the SINR model, and the first efficient algorithm\nfor multi-message broadcast in that model.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 05:20:19 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Halldorsson", "Magnus M.", ""], ["Holzer", "Stephan", ""], ["Lynch", "Nancy", ""]]}, {"id": "1505.04542", "submitter": "Daisuke Ishii", "authors": "Daisuke Ishii, Kazuki Yoshizoe, Toyotaro Suzumura", "title": "Scalable Parallel Numerical Constraint Solver Using Global Load\n  Balancing", "comments": "To be presented at X10'15 Workshop", "journal-ref": null, "doi": "10.1145/2771774.2771776", "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a scalable parallel solver for numerical constraint satisfaction\nproblems (NCSPs). Our parallelization scheme consists of homogeneous worker\nsolvers, each of which runs on an available core and communicates with others\nvia the global load balancing (GLB) method. The parallel solver is implemented\nwith X10 that provides an implementation of GLB as a library. In experiments,\nseveral NCSPs from the literature were solved and attained up to 516-fold\nspeedup using 600 cores of the TSUBAME2.5 supercomputer.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 08:04:45 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Ishii", "Daisuke", ""], ["Yoshizoe", "Kazuki", ""], ["Suzumura", "Toyotaro", ""]]}, {"id": "1505.04546", "submitter": "Yukiko Yamauchi", "authors": "Yukiko Yamauchi, Taichi Uehara, Shuji Kijima, and Masafumi Yamashita", "title": "Plane Formation by Synchronous Mobile Robots in the Three Dimensional\n  Euclidean Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Creating a swarm of mobile computing entities frequently called robots,\nagents or sensor nodes, with self-organization ability is a contemporary\nchallenge in distributed computing. Motivated by this, we investigate the plane\nformation problem that requires a swarm of robots moving in the three\ndimensional Euclidean space to land on a common plane. The robots are fully\nsynchronous and endowed with visual perception. But they do not have\nidentifiers, nor access to the global coordinate system, nor any means of\nexplicit communication with each other. Though there are plenty of results on\nthe agreement problem for robots in the two dimensional plane, for example, the\npoint formation problem, the pattern formation problem, and so on, this is the\nfirst result for robots in the three dimensional space. This paper presents a\nnecessary and sufficient condition for fully-synchronous robots to solve the\nplane formation problem that does not depend on obliviousness i.e., the\navailability of local memory at robots. An implication of the result is\nsomewhat counter-intuitive: The robots cannot form a plane from most of the\nsemi-regular polyhedra, while they can form a plane from every regular\npolyhedron (except a regular icosahedron), whose symmetry is usually considered\nto be higher than any semi-regular polyhedrdon.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 08:15:12 GMT"}, {"version": "v2", "created": "Sun, 24 May 2015 07:49:26 GMT"}, {"version": "v3", "created": "Tue, 16 Feb 2016 10:20:45 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Yamauchi", "Yukiko", ""], ["Uehara", "Taichi", ""], ["Kijima", "Shuji", ""], ["Yamashita", "Masafumi", ""]]}, {"id": "1505.04628", "submitter": "Faisal Shahzad", "authors": "Faisal Shahzad, Moritz Kreutzer, Thomas Zeiser, Rui Machado, Andreas\n  Pieper, Georg Hager, Gerhard Wellein", "title": "Building a fault tolerant application using the GASPI communication\n  layer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is commonly agreed that highly parallel software on Exascale computers\nwill suffer from many more runtime failures due to the decreasing trend in the\nmean time to failures (MTTF). Therefore, it is not surprising that a lot of\nresearch is going on in the area of fault tolerance and fault mitigation.\nApplications should survive a failure and/or be able to recover with minimal\ncost. MPI is not yet very mature in handling failures, the User-Level Failure\nMitigation (ULFM) proposal being currently the most promising approach is still\nin its prototype phase. In our work we use GASPI, which is a relatively new\ncommunication library based on the PGAS model. It provides the missing features\nto allow the design of fault-tolerant applications. Instead of introducing\nalgorithm-based fault tolerance in its true sense, we demonstrate how we can\nbuild on (existing) clever checkpointing and extend applications to allow\nintegrate a low cost fault detection mechanism and, if necessary, recover the\napplication on the fly. The aspects of process management, the restoration of\ngroups and the recovery mechanism is presented in detail. We use a sparse\nmatrix vector multiplication based application to perform the analysis of the\noverhead introduced by such modifications. Our fault detection mechanism causes\nno overhead in failure-free cases, whereas in case of failure(s), the failure\ndetection and recovery cost is of reasonably acceptable order and shows good\nscalability.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 13:18:53 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Shahzad", "Faisal", ""], ["Kreutzer", "Moritz", ""], ["Zeiser", "Thomas", ""], ["Machado", "Rui", ""], ["Pieper", "Andreas", ""], ["Hager", "Georg", ""], ["Wellein", "Gerhard", ""]]}, {"id": "1505.04636", "submitter": "Alex Smola J", "authors": "Mu Li, Dave G. Andersen, Alexander J. Smola", "title": "Graph Partitioning via Parallel Submodular Approximation to Accelerate\n  Distributed Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed computing excels at processing large scale data, but the\ncommunication cost for synchronizing the shared parameters may slow down the\noverall performance. Fortunately, the interactions between parameter and data\nin many problems are sparse, which admits efficient partition in order to\nreduce the communication overhead.\n  In this paper, we formulate data placement as a graph partitioning problem.\nWe propose a distributed partitioning algorithm. We give both theoretical\nguarantees and a highly efficient implementation. We also provide a highly\nefficient implementation of the algorithm and demonstrate its promising results\non both text datasets and social networks. We show that the proposed algorithm\nleads to 1.6x speedup of a state-of-the-start distributed machine learning\nsystem by eliminating 90\\% of the network communication.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 13:43:46 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Li", "Mu", ""], ["Andersen", "Dave G.", ""], ["Smola", "Alexander J.", ""]]}, {"id": "1505.04694", "submitter": "Georgios Rokos", "authors": "Georgios Rokos and Gerard J. Gorman and Kristian Ejlebjerg Jensen and\n  Paul H. J. Kelly", "title": "Thread Parallelism for Highly Irregular Computation in Anisotropic Mesh\n  Adaptation", "comments": "To appear in the proceedings of EASC 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thread-level parallelism in irregular applications with mutable data\ndependencies presents challenges because the underlying data is extensively\nmodified during execution of the algorithm and a high degree of parallelism\nmust be realized while keeping the code race-free. In this article we describe\na methodology for exploiting thread parallelism for a class of graph-mutating\nworklist algorithms, which guarantees safe parallel execution via processing in\nrounds of independent sets and using a deferred update strategy to commit\nchanges in the underlying data structures. Scalability is assisted by atomic\nfetch-and-add operations to create worklists and work-stealing to balance the\nshared-memory workload. This work is motivated by mesh adaptation algorithms,\nfor which we show a parallel efficiency of 60% and 50% on Intel(R) Xeon(R)\nSandy Bridge and AMD Opteron(tm) Magny-Cours systems, respectively, using these\ntechniques.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 15:55:46 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Rokos", "Georgios", ""], ["Gorman", "Gerard J.", ""], ["Jensen", "Kristian Ejlebjerg", ""], ["Kelly", "Paul H. J.", ""]]}, {"id": "1505.04880", "submitter": "Nasser Ghadiri", "authors": "Amin Beiranvand and Nasser Ghadiri", "title": "ADQUEX: Adaptive Processing of Federated Queries over Linked Data based\n  on Tuple Routing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the distribution of linked data across the web, the methods that\nprocess federated queries through a distributed approach are more attractive to\nthe users and have gained more prosperity. In distributed processing of\nfederated queries, we need methods and procedures to execute the query in an\noptimal manner. Most of the existing methods perform the optimization task\nbased on some statistical information, whereas the query processor does not\nhave precise statistical information about their properties, since the data\nsources are autonomous. When precise statistics are not available, the\npossibility of wrong estimations will highly increase, and may lead to\ninefficient execution of the query at runtime. Another problem of the existing\nmethods is that in the optimization phase, they assume that runtime conditions\nof query execution are stable, while the environment in which federated queries\nare executed over linked data is dynamic and non-predictable. By considering\nthese two problems, there is a great potential for exploiting the federated\nquery processing techniques in an adaptive manner. In this paper, an adaptive\nmethod is proposed for processing federated queries over linked data, based on\nthe concept of routing the tuples. The proposed method, named ADQUEX, is able\nto execute the query effectively without any prior statistical information.\nThis method can change the query execution plan at runtime so that less\nintermediate results are produced. It can also adapt the execution plan to new\nsituation if unpredicted network latencies arise. Extensive evaluation of our\nmethod by running real queries over well-known linked datasets shows very good\nresults especially for complex queries.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 05:51:27 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Beiranvand", "Amin", ""], ["Ghadiri", "Nasser", ""]]}, {"id": "1505.04935", "submitter": "Alina S\\^irbu", "authors": "Alina S\\^irbu and Ozalp Babaoglu", "title": "Towards Data-Driven Autonomics in Data Centers", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continued reliance on human operators for managing data centers is a major\nimpediment for them from ever reaching extreme dimensions. Large computer\nsystems in general, and data centers in particular, will ultimately be managed\nusing predictive computational and executable models obtained through\ndata-science tools, and at that point, the intervention of humans will be\nlimited to setting high-level goals and policies rather than performing\nlow-level operations. Data-driven autonomics, where management and control are\nbased on holistic predictive models that are built and updated using generated\ndata, opens one possible path towards limiting the role of operators in data\ncenters. In this paper, we present a data-science study of a public Google\ndataset collected in a 12K-node cluster with the goal of building and\nevaluating a predictive model for node failures. We use BigQuery, the big data\nSQL platform from the Google Cloud suite, to process massive amounts of data\nand generate a rich feature set characterizing machine state over time. We\ndescribe how an ensemble classifier can be built out of many Random Forest\nclassifiers each trained on these features, to predict if machines will fail in\na future 24-hour window. Our evaluation reveals that if we limit false positive\nrates to 5%, we can achieve true positive rates between 27% and 88% with\nprecision varying between 50% and 72%. We discuss the practicality of including\nour predictive model as the central component of a data-driven autonomic\nmanager and operating it on-line with live data streams (rather than off-line\non data logs). All of the scripts used for BigQuery and classification analyses\nare publicly available from the authors' website.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 09:58:05 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2015 13:45:52 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["S\u00eerbu", "Alina", ""], ["Babaoglu", "Ozalp", ""]]}, {"id": "1505.04956", "submitter": "Janis Keuper", "authors": "Janis Keuper and Franz-Josef Pfreundt", "title": "Asynchronous Parallel Stochastic Gradient Descent - A Numeric Core for\n  Scalable Distributed Machine Learning Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The implementation of a vast majority of machine learning (ML) algorithms\nboils down to solving a numerical optimization problem. In this context,\nStochastic Gradient Descent (SGD) methods have long proven to provide good\nresults, both in terms of convergence and accuracy. Recently, several\nparallelization approaches have been proposed in order to scale SGD to solve\nvery large ML problems. At their core, most of these approaches are following a\nmap-reduce scheme. This paper presents a novel parallel updating algorithm for\nSGD, which utilizes the asynchronous single-sided communication paradigm.\nCompared to existing methods, Asynchronous Parallel Stochastic Gradient Descent\n(ASGD) provides faster (or at least equal) convergence, close to linear scaling\nand stable accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 11:20:46 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2015 11:16:34 GMT"}, {"version": "v3", "created": "Wed, 22 Jul 2015 07:15:56 GMT"}, {"version": "v4", "created": "Wed, 30 Sep 2015 10:08:20 GMT"}, {"version": "v5", "created": "Mon, 5 Oct 2015 12:23:11 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Keuper", "Janis", ""], ["Pfreundt", "Franz-Josef", ""]]}, {"id": "1505.05025", "submitter": "Sebastien Tixeuil", "authors": "Quentin Bramas (LINCS, UPMC, LIP6, NPA), Dianne Foreback, Mikhail\n  Nesterenko, S\\'ebastien Tixeuil (IUF, LINCS, UPMC, LIP6, NPA)", "title": "Packet Efficient Implementation of the Omega Failure Detector", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We assume that a message may be delivered by packets through multiple hops\nand investigate the feasibility and efficiency of an implementation of the\nOmega Failure Detector under such an assumption.To motivate the study, we prove\nthat the existence and sustainability of a leader is exponentially more\nprobable in a multi-hop Omega implementation than in a single-hop one.An\nimplementation is: \\emph{message efficient} if all but finitely many messages\nare sent by a single process; \\emph{packet efficient} if the number of packets\nused to transmit a message in all but finitely many messages is linear w.r.t\nthe number of processes, packets of different messages may potentially use\ndifferent channels, thus the number of used channels is not limited;\n\\emph{super packet efficient} if the number of channels used by packets to\ntransmit all but finitely many messages is linear.We present the following\nresults for deterministic algorithms. If reliability and timeliness of one\nmessage does not correlate with another, i.e., there are no channel reliability\nproperties, then a packet efficient implementation of Omega is impossible. If\neventuallytimely and fair-lossy channels are considered, we establish necessary\nand sufficient conditions for the existence of a message and packet efficient\nimplementation of Omega. We also prove that the eventuality of timeliness of\nchannels makes a super packet efficientimplementation of Omega impossible. On\nthe constructive side, we present and prove correct a deterministic packet\nefficient implementation of Omega that matches the necessary conditions we\nestablished.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 14:46:18 GMT"}, {"version": "v2", "created": "Sat, 30 May 2015 23:03:04 GMT"}, {"version": "v3", "created": "Fri, 12 Feb 2016 14:39:49 GMT"}], "update_date": "2016-02-15", "authors_parsed": [["Bramas", "Quentin", "", "LINCS, UPMC, LIP6, NPA"], ["Foreback", "Dianne", "", "IUF, LINCS, UPMC, LIP6, NPA"], ["Nesterenko", "Mikhail", "", "IUF, LINCS, UPMC, LIP6, NPA"], ["Tixeuil", "S\u00e9bastien", "", "IUF, LINCS, UPMC, LIP6, NPA"]]}, {"id": "1505.05033", "submitter": "Nimrod Aviram", "authors": "Nimrod Aviram, Yuval Shavitt", "title": "Optimizing Dijkstra for real-world performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using Dijkstra's algorithm to compute the shortest paths in a graph from a\nsingle source node to all other nodes is common practice in industry and\nacademia. Although the original description of the algorithm advises using a\nFibonacci Heap as its internal queue, it has been noted that in practice, a\nbinary (or $d$-ary) heap implementation is significantly faster. This paper\nintroduces an even faster queue design for the algorithm.\n  Our experimental results currently put our prototype implementation at about\ntwice as fast as the Boost implementation of the algorithm on both real-world\nand generated large graphs. Furthermore, this preliminary implementation was\nwritten in only a few weeks, by a single programmer. The fact that such an\nearly prototype compares favorably against Boost, a well-known open source\nlibrary developed by expert programmers, gives us reason to believe our design\nfor the queue is indeed better suited to the problem at hand, and the favorable\ntime measurements are not a product of any specific implementation technique we\nemployed.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 14:58:40 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Aviram", "Nimrod", ""], ["Shavitt", "Yuval", ""]]}, {"id": "1505.05072", "submitter": "Laurent Feuilloley", "authors": "Laurent Feuilloley", "title": "Brief Announcement : Average Complexity for the LOCAL Model", "comments": null, "journal-ref": null, "doi": "10.1145/2767386.2767446", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A standard model in network synchronised distributed computing is the LOCAL\nmodel. In this model, the processors work in rounds and, in the classic\nsetting, they know the number of vertices of the network, $n$. Using $n$, they\ncan compute the number of rounds after which they must all stop and output. It\nhas been shown recently that for many problems, one can basically remove the\nassumption about the knowledge of $n$, without increasing the asymptotic\nrunning time. In this case, it is assumed that different vertices can choose\ntheir final output at different rounds, but continue to transmit messages. In\nboth models, the measure of the running time is the number of rounds before the\nlast node outputs. In this brief announcement, the vertices do not have the\nknowledge of $n$, and we consider an alternative measure: the average, over the\nnodes, of the number of rounds before they output. We prove that the complexity\nof a problem can be exponentially smaller with the new measure, but that\nLinial's lower bound for colouring still holds.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 16:31:57 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Feuilloley", "Laurent", ""]]}, {"id": "1505.05571", "submitter": "Radford M. Neal", "authors": "Radford M. Neal", "title": "Fast exact summation using small and large superaccumulators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.DC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I present two new methods for exactly summing a set of floating-point\nnumbers, and then correctly rounding to the nearest floating-point number.\nHigher accuracy than simple summation (rounding after each addition) is\nimportant in many applications, such as finding the sample mean of data. Exact\nsummation also guarantees identical results with parallel and serial\nimplementations, since the exact sum is independent of order. The new methods\nuse variations on the concept of a \"superaccumulator\" - a large fixed-point\nnumber that can exactly represent the sum of any reasonable number of\nfloating-point values. One method uses a \"small\" superaccumulator with\nsixty-seven 64-bit chunks, each with 32-bit overlap with the next chunk,\nallowing carry propagation to be done infrequently. The small superaccumulator\nis used alone when summing a small number of terms. For big summations, a\n\"large\" superaccumulator is used as well. It consists of 4096 64-bit chunks,\none for every possible combination of exponent bits and sign bit, plus counts\nof when each chunk needs to be transferred to the small superaccumulator. To\nadd a term to the large superaccumulator, only a single chunk and its\nassociated count need to be updated, which takes very few instructions if\ncarefully implemented. On modern 64-bit processors, exactly summing a large\narray using this combination of large and small superaccumulators takes less\nthan twice the time of simple, inexact, ordered summation, with a serial\nimplementation. A parallel implementation using a small number of processor\ncores can be expected to perform exact summation of large arrays at a speed\nthat reaches the limit imposed by memory bandwidth. Some common methods that\nattempt to improve accuracy without being exact may therefore be pointless, at\nleast for large summations, since they are slower than computing the sum\nexactly.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 01:17:04 GMT"}], "update_date": "2015-05-22", "authors_parsed": [["Neal", "Radford M.", ""]]}, {"id": "1505.05613", "submitter": "Chris De Vries", "authors": "Christopher M. de Vries, Lance De Vine, Shlomo Geva, Richi Nayak", "title": "Parallel Streaming Signature EM-tree: A Clustering Algorithm for Web\n  Scale Applications", "comments": "11 pages, WWW 2015", "journal-ref": null, "doi": "10.1145/2736277.2741111", "report-no": null, "categories": "cs.IR cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of the web presents an unsolved problem of automatically\nanalyzing billions of pages of natural language. We introduce a scalable\nalgorithm that clusters hundreds of millions of web pages into hundreds of\nthousands of clusters. It does this on a single mid-range machine using\nefficient algorithms and compressed document representations. It is applied to\ntwo web-scale crawls covering tens of terabytes. ClueWeb09 and ClueWeb12\ncontain 500 and 733 million web pages and were clustered into 500,000 to\n700,000 clusters. To the best of our knowledge, such fine grained clustering\nhas not been previously demonstrated. Previous approaches clustered a sample\nthat limits the maximum number of discoverable clusters. The proposed EM-tree\nalgorithm uses the entire collection in clustering and produces several orders\nof magnitude more clusters than the existing algorithms. Fine grained\nclustering is necessary for meaningful clustering in massive collections where\nthe number of distinct topics grows linearly with collection size. These\nfine-grained clusters show an improved cluster quality when assessed with two\nnovel evaluations using ad hoc search relevance judgments and spam\nclassifications for external validation. These evaluations solve the problem of\nassessing the quality of clusters where categorical labeling is unavailable and\nunfeasible.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 06:22:04 GMT"}], "update_date": "2015-05-22", "authors_parsed": [["de Vries", "Christopher M.", ""], ["De Vine", "Lance", ""], ["Geva", "Shlomo", ""], ["Nayak", "Richi", ""]]}, {"id": "1505.05655", "submitter": "Poorna Dasgupta", "authors": "Poorna Banerjee and Amit Dave", "title": "GPGPU Based Parallelized Client-Server Framework for Providing High\n  Performance Computation Support", "comments": "6 pages, Published with International Journal of Computer Science &\n  Technology (IJCST)- Vol 4 Issue 1 Jan - March 2013", "journal-ref": "International Journal of Computer Science and Technology (IJCST)\n  V4(1):508-513, Jan - March 2013", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel data processing has become indispensable for processing applications\ninvolving huge data sets. This brings into focus the Graphics Processing Units\n(GPUs) which emphasize on many-core computing. With the advent of General\nPurpose GPUs (GPGPU), applications not directly associated with graphics\noperations can also harness the computation capabilities of GPUs. Hence, it\nwould be beneficial if the computing capabilities of a given GPGPU could be\ntask optimized and made available. This paper describes a client-server\nframework in which users can choose a processing task and submit large\ndata-sets for processing to a remote GPGPU and receive the results back, using\nwell defined interfaces. The framework provides extensibility in terms of the\nnumber and type of tasks that the client can choose or submit for processing at\nthe remote GPGPU server machine, with complete transparency to the underlying\nhardware and operating systems. Parallelization of user-submitted tasks on the\nGPGPU has been achieved using NVIDIA Compute Unified Device Architecture\n(CUDA).\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 09:31:39 GMT"}], "update_date": "2015-05-22", "authors_parsed": [["Banerjee", "Poorna", ""], ["Dave", "Amit", ""]]}, {"id": "1505.05697", "submitter": "Leonid Barenboim", "authors": "Leonid Barenboim, Michael Elkin, and Cyril Gavoille", "title": "A Fast Network-Decomposition Algorithm and its Applications to\n  Constant-Time Distributed Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A partition $(C_1,C_2,...,C_q)$ of $G = (V,E)$ into clusters of strong\n(respectively, weak) diameter $d$, such that the supergraph obtained by\ncontracting each $C_i$ is $\\ell$-colorable is called a strong (resp., weak)\n$(d, \\ell)$-network-decomposition. Network-decompositions were introduced in a\nseminal paper by Awerbuch, Goldberg, Luby and Plotkin in 1989. Awerbuch et al.\nshowed that strong $(exp\\{O(\\sqrt{ \\log n \\log \\log n})\\}, exp\\{O(\\sqrt{ \\log n\n\\log \\log n})\\})$-network-decompositions can be computed in distributed\ndeterministic time $exp\\{O(\\sqrt{ \\log n \\log \\log n})\\}$.\n  The result of Awerbuch et al. was improved by Panconesi and Srinivasan in\n1992: in the latter result $d = \\ell = exp\\{O(\\sqrt{\\log n})\\}$, and the\nrunning time is $exp\\{O(\\sqrt{\\log n})\\}$ as well. Much more recently Barenboim\n(2012) devised a distributed randomized constant-time algorithm for computing\nstrong network decompositions with $d = O(1)$. However, the parameter $\\ell$ in\nhis result is $O(n^{1/2 + \\epsilon})$.\n  In this paper we drastically improve the result of Barenboim and devise a\ndistributed randomized constant-time algorithm for computing strong $(O(1),\nO(n^{\\epsilon}))$-network-decompositions. As a corollary we derive a\nconstant-time randomized $O(n^{\\epsilon})$-approximation algorithm for the\ndistributed minimum coloring problem, improving the previously best-known\n$O(n^{1/2 + \\epsilon})$ approximation guarantee. We also derive other improved\ndistributed algorithms for a variety of problems.\n  Most notably, for the extremely well-studied distributed minimum dominating\nset problem currently there is no known deterministic polylogarithmic-time\nalgorithm. We devise a {deterministic} polylogarithmic-time approximation\nalgorithm for this problem, addressing an open problem of Lenzen and\nWattenhofer (2010).\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 12:22:07 GMT"}, {"version": "v2", "created": "Sat, 25 Mar 2017 19:17:35 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Barenboim", "Leonid", ""], ["Elkin", "Michael", ""], ["Gavoille", "Cyril", ""]]}, {"id": "1505.05977", "submitter": "Yiqing Hu", "authors": "Yiqing Hu, Yan Xiong, Wenchao Huang, Xiang-Yang Li, Yanan Zhang, Xufei\n  Mao, Panlong Yang, Caimei Wang", "title": "A Visible Light Based Indoor Positioning System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel indoor localization scheme that exploits\nubiquitous visible lights, which are necessarily and densely deployed in almost\nall indoor environments. We unveil two phenomena of lights available for\npositioning: 1) the light strength varies according to different light sources,\nwhich can be easily detected by light sensors embedded in COTS devices (e.g.,\nsmart-phone, smart-glass and smart-watch); 2) the light strength is stable in\ndifferent times of the day thus exploiting it can avoid frequent site-survey\nand database maintenance. Hence, a user could locate oneself by differentiating\nthe light source of received light strength (RLS). However, different from\nexisting positioning systems that exploit special LEDs, ubiquitous visible\nlights lack fingerprints that can uniquely identify the light source, which\nresults in an ambiguity problem that an RLS may correspond to multiple\npositions. Moreover, RLS is not only determined by device's position, but also\nseriously affected by its orientation, which causes great complexity in\nsite-survey. To address these challenges, we first propose and validate a\nrealistic light strength model that can attributes RLS to arbitrary positions\nwith heterogenous orientations. This model is further perfected by taking\naccount of the device diversity, influence of multiple light sources and\nshading of obstacles. Then we design a localizing scheme that harness user's\nmobility to generate spatial-related RLS to tackle the position-ambiguity\nproblem of a single RLS, which is robust against sunlight interference, shading\neffect of human-body and unpredictable behaviours (e.g., put the device in\npocket) of user. Experiment results show that our scheme achieves mean accuracy\n$1.93$m and $1.98$m in office ($720m^2$) and library scenario ($960m^2$)\nrespectively.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2015 08:07:38 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2015 02:32:12 GMT"}, {"version": "v3", "created": "Tue, 2 Jun 2015 01:51:30 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Hu", "Yiqing", ""], ["Xiong", "Yan", ""], ["Huang", "Wenchao", ""], ["Li", "Xiang-Yang", ""], ["Zhang", "Yanan", ""], ["Mao", "Xufei", ""], ["Yang", "Panlong", ""], ["Wang", "Caimei", ""]]}, {"id": "1505.06107", "submitter": "Peter Davies", "authors": "Artur Czumaj, Peter Davies", "title": "Communicating with Beeps", "comments": null, "journal-ref": "OPODIS 2015", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The \\emph{beep model} is a very weak communications model in which devices in\na network can communicate only via beeps and silence. As a result of its weak\nassumptions, it has broad applicability to many different implementations of\ncommunications networks. This comes at the cost of a restrictive environment\nfor algorithm design.\n  Despite being only recently introduced, the beep model has received\nconsiderable attention, in part due to its relationship with other\ncommunication models such as that of ad-hoc radio networks. However, there has\nbeen no definitive published result for several fundamental tasks in the model.\nWe aim to rectify this with our paper.\n  We present algorithms and lower bounds for a variety of fundamental global\ncommunications tasks in the model.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2015 14:45:02 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2015 21:09:27 GMT"}, {"version": "v3", "created": "Thu, 3 Sep 2015 18:26:36 GMT"}, {"version": "v4", "created": "Sat, 16 Mar 2019 14:48:14 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Czumaj", "Artur", ""], ["Davies", "Peter", ""]]}, {"id": "1505.06149", "submitter": "Peter Davies", "authors": "Artur Czumaj, Peter Davies", "title": "Leader Election in Multi-Hop Radio Networks", "comments": null, "journal-ref": "Theoretical Computer Science, 2019", "doi": "10.1016/j.tcs.2019.02.027", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a framework for leader election in multi-hop radio\nnetworks which yield randomized leader election algorithms taking\n$O(\\text{broadcasting time})$ in expectation, and another which yields\nalgorithms taking fixed $O(\\sqrt{\\log n})$-times broadcasting time. Both\nsucceed with high probability.\n  We show how to implement these frameworks in radio networks without collision\ndetection, and in networks with collision detection (in fact in the strictly\nweaker beep model). In doing so, we obtain the first optimal expected-time\nleader election algorithms in both settings, and also improve the worst-case\nrunning time in directed networks without collision detection by an $O(\\sqrt\n{\\log n})$ factor.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2015 17:04:26 GMT"}, {"version": "v2", "created": "Sat, 16 Mar 2019 14:47:39 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Czumaj", "Artur", ""], ["Davies", "Peter", ""]]}, {"id": "1505.06299", "submitter": "Matthieu Perrin", "authors": "Matthieu Perrin, Claude Jard, Achour Mostefaoui", "title": "Tracking Causal Dependencies in Web Services Orchestrations Defined in\n  ORC", "comments": "NETYS - 3rd International Conference on NETwork sYStems, May 2015,\n  Agadir, Morocco. 2015, Proceedings of the third international conference on\n  network systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article shows how the operational semantics of a language like ORC can\nbe instrumented so that the execution of a program produces information on the\ncausal dependencies between events. The concurrent semantics we obtain is based\non asymmetric labeled event structures. The approach is illustrated using a Web\nservice orchestration instance and the detection of race conditions.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2015 10:03:49 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Perrin", "Matthieu", ""], ["Jard", "Claude", ""], ["Mostefaoui", "Achour", ""]]}, {"id": "1505.06459", "submitter": "Xiangyao Yu", "authors": "Xiangyao Yu, Muralidaran Vijayaraghavan, Srinivas Devadas", "title": "A Proof of Correctness for the Tardis Cache Coherence Protocol", "comments": "16 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove the correctness of a recently-proposed cache coherence protocol,\nTardis, which is simple, yet scalable to high processor counts, because it only\nrequires O(logN) storage per cacheline for an N-processor system. We prove that\nTardis follows the sequential consistency model and is both deadlock- and\nlivelock-free. Our proof is based on simple and intuitive invariants of the\nsystem and thus applies to any system scale and many variants of Tardis.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2015 17:33:56 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Yu", "Xiangyao", ""], ["Vijayaraghavan", "Muralidaran", ""], ["Devadas", "Srinivas", ""]]}, {"id": "1505.06539", "submitter": "Gary Lawson Jr", "authors": "Gary Lawson, Masha Sosonkina, Yuzhong Shen", "title": "Towards Modeling Energy Consumption of Xeon Phi", "comments": "Energy, Intel Xeon Phi, Symmetric Execution, Proxy Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In the push for exascale computing, energy efficiency is of utmost concern.\nSystem architectures often adopt accelerators to hasten application execution\nat the cost of power. The Intel Xeon Phi co-processor is unique accelerator\nthat offers application designers high degrees of parallelism, energy-efficient\ncores, and various execution modes. To explore the vast number of available\nconfigurations, a model must be developed to predict execution time, power, and\nenergy for the CPU and Xeon Phi. An experimentation method has been developed\nwhich measures power for the CPU and Xeon Phi separately, as well as total\nsystem power. Execution time and performance are also captured for two\nexperiments conducted in this work. The experiments, frequency scaling and\nstrong scaling, will help validate the adopted model and assist in the\ndevelopment of a model which defines the host and Xeon Phi. The proxy\napplications investigated, representative of large-scale real-world\napplications, are Co-Design Molecular Dynamics (CoMD) and Livermore\nUnstructured Lagrangian Explicit Shock Hydrodynamics (LULESH). The frequency\nexperiment discussed in this work is used to determine the time on-chip and\noff-chip to measure the compute- or latencyboundedness of the application.\nEnergy savings were not obtained in symmetric mode for either application.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2015 04:39:48 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Lawson", "Gary", ""], ["Sosonkina", "Masha", ""], ["Shen", "Yuzhong", ""]]}, {"id": "1505.06542", "submitter": "Ruby Annette", "authors": "Ruby Annette, Aisha Banu", "title": "A Service Broker Model for Cloud based Render Farm Selection", "comments": null, "journal-ref": "International Journal of Computer Applications 96.24 (2014): 11-14", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing is gaining popularity in the 3D Animation industry for\nrendering the 3D images. Rendering is an inevitable task in creating the 3d\nanimated scenes. It is a process where the scene files to be animated is read\nand converted into 3D photorealistic images automatically. Since it is a\ncomputationally intensive task, this process consumes the majority of the time\ntaken for 3D images production. As the scene files could be processed in\nparallel, clusters of computers called render farms can be used to speed up the\nrendering process. The advantage of using Cloud based render farms is that it\nis scalable and can be availed on demand. One of the important challenges faced\nby the 3D studios is the comparison and selection of the cloud based render\nfarm service provider who could satisfy their functional and the non functional\nQuality of Service (QoS) requirements. In this paper we propose, a frame work\nfor Cloud Service Broker (CSB) responsible for the selection and provision of\nthe cloud based render farm. The Cloud Service Broker matches the functional\nand the non functional Quality of Service requirements (QoS) of the user with\nthe service offerings of the render farm service providers and helps the user\nin selecting the right service provider using an aggregate utility function.\nThe CSB also facilitates the process of Service Level Agreement (SLA)\nnegotiation and monitoring by the third party monitoring services.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2015 05:20:18 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Annette", "Ruby", ""], ["Banu", "Aisha", ""]]}, {"id": "1505.06543", "submitter": "Ruby Annette", "authors": "Ruby Annette, Aisha Banu, Subash Chandran", "title": "Rendering-as-a-Service: Taxonomy and Comparison", "comments": null, "journal-ref": "Procedia Computer Science 50 (2015): 276-281", "doi": null, "report-no": null, "categories": "cs.DC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The movies like the Avatar are a good example of the stunning visual effects\nthat the animation could bring into a movie.The 3D wire frame models are\nconverted to 3D photo realistic images using a process called the rendering.\nThis rendering process is offered as a service in the cloud, where the\nanimation files to be rendered are split into frames and rendered in the cloud\nresources and are popularly known as Rendering as a Service. As this is gaining\nhigh popularity among the animators community, this work intends to enable the\nanimators to Gain basic knowledge about Rendering as a Service. Understand the\nvariety in the service models through the taxonomy,Explore, compare and\nclassify the services quickly using the tree-structured taxonomy of services.\nIn this paper, the various characteristics of the services are organized in the\nform of a tree to enable quick classification and comparison of the services.\nTo enhance the understandability, three popular services have been classified\nand verified according to the proposed tree-structured taxonomy.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2015 05:28:00 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Annette", "Ruby", ""], ["Banu", "Aisha", ""], ["Chandran", "Subash", ""]]}, {"id": "1505.06561", "submitter": "Mingzhe Wang", "authors": "Mingzhe Wang, Bo Wang, Qiu He, Xiuxiu Liu, Kunshuai Zhu", "title": "Analysis of GPU Parallel Computing based on Matlab", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matlab is very widely used in scientific computing, but Matlab computational\nefficiency is lower than C language program. In order to improve the computing\nspeed, some toolbox can use GPU to accelerate the computation. This paper\ndescribes GPU working principle, our experiments and results analysis of\nparallel computing by using GPU based on Matlab. Experimental results show that\nfor parallel operations, GPU computing speed is faster than CPU, for the\nlogical instructions, GPU computing speed is slower than CPU.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2015 08:23:31 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Wang", "Mingzhe", ""], ["Wang", "Bo", ""], ["He", "Qiu", ""], ["Liu", "Xiuxiu", ""], ["Zhu", "Kunshuai", ""]]}, {"id": "1505.06588", "submitter": "Pierre Ganty", "authors": "Antoine Durand-Gasselin, Javier Esparza, Pierre Ganty, Rupak Majumdar", "title": "Model Checking Parameterized Asynchronous Shared-Memory Systems", "comments": "27 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.FL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We characterize the complexity of liveness verification for parameterized\nsystems consisting of a leader process and arbitrarily many anonymous and\nidentical contributor processes. Processes communicate through a shared,\nbounded-value register. While each operation on the register is atomic, there\nis no synchronization primitive to execute a sequence of operations atomically.\n  We analyze the case in which processes are modeled by finite-state machines\nor pushdown machines and the property is given by a B\\\"uchi automaton over the\nalphabet of read and write actions of the leader. We show that the problem is\ndecidable, and has a surprisingly low complexity: it is NP-complete when all\nprocesses are finite-state machines, and is PSPACE-hard and in NEXPTIME when\nthey are pushdown machines. This complexity is lower than for the\nnon-parameterized case: liveness verification of finitely many finite-state\nmachines is PSPACE-complete, and undecidable for two pushdown machines.\n  For finite-state machines, our proofs characterize infinite behaviors using\nexistential abstraction and semilinear constraints. For pushdown machines, we\nshow how contributor computations of high stack height can be simulated by\ncomputations of many contributors, each with low stack height. Together, our\nresults characterize the complexity of verification for parameterized systems\nunder the assumptions of anonymity and asynchrony.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2015 10:17:26 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Durand-Gasselin", "Antoine", ""], ["Esparza", "Javier", ""], ["Ganty", "Pierre", ""], ["Majumdar", "Rupak", ""]]}, {"id": "1505.06596", "submitter": "Masahiro Shibata", "authors": "Masahiro Shibata, Shinji Kawai, Fukuhito Ooshita, Hirotsugu Kakugawa,\n  Toshimitsu Masuzawa", "title": "Partial Gathering of Mobile Agents in Asynchronous Rings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the partial gathering problem of mobile agents in\nasynchronous unidirectional rings equipped with whiteboards on nodes. The\npartial gathering problem is a new generalization of the total gathering\nproblem. The partial gathering problem requires, for a given integer $g$, that\neach agent should move to a node and terminate so that at least $g$ agents\nshould meet at the same node. The requirement for the partial gathering problem\nis weaker than that for the (well-investigated) total gathering problem, and\nthus, we have interests in clarifying the difference on the move complexity\nbetween them. We propose three algorithms to solve the partial gathering\nproblem. The first algorithm is deterministic but requires unique ID of each\nagent. This algorithm achieves the partial gathering in $O(gn)$ total moves,\nwhere $n$ is the number of nodes. The second algorithm is randomized and\nrequires no unique ID of each agent (i.e., anonymous). This algorithm achieves\nthe partial gathering in expected $O(gn)$ total moves. The third algorithm is\ndeterministic and requires no unique ID of each agent. For this case, we show\nthat there exist initial configurations in which no algorithm can solve the\nproblem and agents can achieve the partial gathering in $O(kn)$ total moves for\nsolvable initial configurations, where $k$ is the number of agents. Note that\nthe total gathering problem requires $\\Omega (kn)$ total moves, while the\npartial gathering problem requires $\\Omega (gn)$ total moves in each model.\nHence, we show that the move complexity of the first and second algorithms is\nasymptotically optimal.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2015 11:29:29 GMT"}, {"version": "v2", "created": "Fri, 29 May 2015 01:00:38 GMT"}, {"version": "v3", "created": "Thu, 10 Sep 2015 05:39:14 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Shibata", "Masahiro", ""], ["Kawai", "Shinji", ""], ["Ooshita", "Fukuhito", ""], ["Kakugawa", "Hirotsugu", ""], ["Masuzawa", "Toshimitsu", ""]]}, {"id": "1505.06699", "submitter": "Hao Zhuang", "authors": "Hao Zhuang, Wenjian Yu, Shih-Hung Weng, Ilgweon Kang, Jeng-Hau Lin,\n  Xiang Zhang, Ryan Coutts, Chung-Kuan Cheng", "title": "Simulation Algorithms with Exponential Integration for Time-Domain\n  Analysis of Large-Scale Power Delivery Networks", "comments": "Accepted by IEEE Transactions on Computer Aided Design of Integrated\n  Circuits and Systems (TCAD)", "journal-ref": null, "doi": "10.1109/TCAD.2016.2523908", "report-no": null, "categories": "cs.CE cs.DC cs.NA math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design an algorithmic framework using matrix exponentials for time-domain\nsimulation of power delivery network (PDN). Our framework can reuse factorized\nmatrices to simulate the large-scale linear PDN system with variable stepsizes.\nIn contrast, current conventional PDN simulation solvers have to use fixed\nstep-size approach in order to reuse factorized matrices generated by the\nexpensive matrix decomposition. Based on the proposed exponential integration\nframework, we design a PDN solver R-MATEX with the flexible time-stepping\ncapability. The key operation of matrix exponential and vector product (MEVP)\nis computed by the rational Krylov subspace method.\n  To further improve the runtime, we also propose a distributed computing\nframework DR-MATEX. DR-MATEX reduces Krylov subspace generations caused by\nfrequent breakpoints from a large number of current sources during simulation.\nBy virtue of the superposition property of linear system and scaling invariance\nproperty of Krylov subspace, DR-MATEX can divide the whole simulation task into\nsubtasks based on the alignments of breakpoints among those sources. The\nsubtasks are processed in parallel at different computing nodes without any\ncommunication during the computation of transient simulation. The final result\nis obtained by summing up the partial results among all the computing nodes\nafter they finish the assigned subtasks. Therefore, our computation model\nbelongs to the category known as Embarrassingly Parallel model.\n  Experimental results show R-MATEX and DR-MATEX can achieve up to around 14.4X\nand 98.0X runtime speedups over traditional trapezoidal integration based\nsolver with fixed timestep approach.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2015 17:39:17 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2015 06:28:51 GMT"}, {"version": "v3", "created": "Tue, 2 Feb 2016 04:54:09 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Zhuang", "Hao", ""], ["Yu", "Wenjian", ""], ["Weng", "Shih-Hung", ""], ["Kang", "Ilgweon", ""], ["Lin", "Jeng-Hau", ""], ["Zhang", "Xiang", ""], ["Coutts", "Ryan", ""], ["Cheng", "Chung-Kuan", ""]]}, {"id": "1505.06807", "submitter": "Ameet Talwalkar", "authors": "Xiangrui Meng, Joseph Bradley, Burak Yavuz, Evan Sparks, Shivaram\n  Venkataraman, Davies Liu, Jeremy Freeman, DB Tsai, Manish Amde, Sean Owen,\n  Doris Xin, Reynold Xin, Michael J. Franklin, Reza Zadeh, Matei Zaharia, Ameet\n  Talwalkar", "title": "MLlib: Machine Learning in Apache Spark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.MS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Apache Spark is a popular open-source platform for large-scale data\nprocessing that is well-suited for iterative machine learning tasks. In this\npaper we present MLlib, Spark's open-source distributed machine learning\nlibrary. MLlib provides efficient functionality for a wide range of learning\nsettings and includes several underlying statistical, optimization, and linear\nalgebra primitives. Shipped with Spark, MLlib supports several languages and\nprovides a high-level API that leverages Spark's rich ecosystem to simplify the\ndevelopment of end-to-end machine learning pipelines. MLlib has experienced a\nrapid growth due to its vibrant open-source community of over 140 contributors,\nand includes extensive documentation to support further growth and to let users\nquickly get up to speed.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 05:12:23 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Meng", "Xiangrui", ""], ["Bradley", "Joseph", ""], ["Yavuz", "Burak", ""], ["Sparks", "Evan", ""], ["Venkataraman", "Shivaram", ""], ["Liu", "Davies", ""], ["Freeman", "Jeremy", ""], ["Tsai", "DB", ""], ["Amde", "Manish", ""], ["Owen", "Sean", ""], ["Xin", "Doris", ""], ["Xin", "Reynold", ""], ["Franklin", "Michael J.", ""], ["Zadeh", "Reza", ""], ["Zaharia", "Matei", ""], ["Talwalkar", "Ameet", ""]]}, {"id": "1505.06865", "submitter": "Maria Potop-Butucaru", "authors": "Silvia Bonomi (MIDLAB), Antonella Del Pozzo (MIDLAB, NPA), Maria\n  Potop-Butucaru (NPA)", "title": "Tight Mobile Byzantine Tolerant Atomic Storage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes the first implementation of an atomic storage tolerant to\nmobile Byzantine agents. Our implementation is designed for the round-based\nsynchronous model where the set of Byzantine nodes changes from round to round.\nIn this model we explore the feasibility of multi-writer multi-reader atomic\nregister prone to various mobile Byzantine behaviors. We prove upper and lower\nbounds for solving the atomic storage in all the explored models. Our results,\nsignificantly different from the static case, advocate for a deeper study of\nthe main building blocks of distributed computing while the system is prone to\nmobile Byzantine failures.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 09:15:31 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Bonomi", "Silvia", "", "MIDLAB"], ["Del Pozzo", "Antonella", "", "MIDLAB, NPA"], ["Potop-Butucaru", "Maria", "", "NPA"]]}, {"id": "1505.07130", "submitter": "Kemele M. Endris", "authors": "Kemele M. Endris, Sidra Faisal, Fabrizio Orlandi, S\\\"oren Auer, Simon\n  Scerri", "title": "Interest-based RDF Update Propagation", "comments": "16 pages, Keywords: Change Propagation, Dataset Dynamics, Linked\n  Data, Replication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many LOD datasets, such as DBpedia and LinkedGeoData, are voluminous and\nprocess large amounts of requests from diverse applications. Many data products\nand services rely on full or partial local LOD replications to ensure faster\nquerying and processing. While such replicas enhance the flexibility of\ninformation sharing and integration infrastructures, they also introduce data\nduplication with all the associated undesirable consequences. Given the\nevolving nature of the original and authoritative datasets, to ensure\nconsistent and up-to-date replicas frequent replacements are required at a\ngreat cost. In this paper, we introduce an approach for interest-based RDF\nupdate propagation, which propagates only interesting parts of updates from the\nsource to the target dataset. Effectively, this enables remote applications to\n`subscribe' to relevant datasets and consistently reflect the necessary changes\nlocally without the need to frequently replace the entire dataset (or a\nrelevant subset). Our approach is based on a formal definition for\ngraph-pattern-based interest expressions that is used to filter interesting\nparts of updates from the source. We implement the approach in the iRap\nframework and perform a comprehensive evaluation based on DBpedia Live updates,\nto confirm the validity and value of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 20:36:42 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["Endris", "Kemele M.", ""], ["Faisal", "Sidra", ""], ["Orlandi", "Fabrizio", ""], ["Auer", "S\u00f6ren", ""], ["Scerri", "Simon", ""]]}, {"id": "1505.07158", "submitter": "Juntao Chen", "authors": "Juntao Chen and Quanyan Zhu", "title": "Resilient and Decentralized Control of Multi-level Cooperative Mobile\n  Networks to Maintain Connectivity under Adversarial Environment", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.DC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network connectivity plays an important role in the information exchange\nbetween different agents in the multi-level networks. In this paper, we\nestablish a game-theoretic framework to capture the uncoordinated nature of the\ndecision-making at different layers of the multi-level networks. Specifically,\nwe design a decentralized algorithm that aims to maximize the algebraic\nconnectivity of the global network iteratively. In addition, we show that the\ndesigned algorithm converges to a Nash equilibrium asymptotically and yields an\nequilibrium network. To study the network resiliency, we introduce three\nadversarial attack models and characterize their worst-case impacts on the\nnetwork performance. Case studies based on a two-layer mobile robotic network\nare used to corroborate the effectiveness and resiliency of the proposed\nalgorithm and show the interdependency between different layers of the network\nduring the recovery processes.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2015 00:27:33 GMT"}, {"version": "v2", "created": "Wed, 16 Mar 2016 17:22:01 GMT"}], "update_date": "2016-03-17", "authors_parsed": [["Chen", "Juntao", ""], ["Zhu", "Quanyan", ""]]}, {"id": "1505.07168", "submitter": "Giovanni Viglietta", "authors": "Linda Pagli, Giuseppe Prencipe, Giovanni Viglietta", "title": "Getting Close Without Touching: Near-Gathering for Autonomous Mobile\n  Robots", "comments": "25 pages, 8 fiugres", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the Near-Gathering problem for a finite set of\ndimensionless, deterministic, asynchronous, anonymous, oblivious and autonomous\nmobile robots with limited visibility moving in the Euclidean plane in\nLook-Compute-Move (LCM) cycles. In this problem, the robots have to get close\nenough to each other, so that every robot can see all the others, without\ntouching (i.e., colliding with) any other robot. The importance of solving the\nNear-Gathering problem is that it makes it possible to overcome the restriction\nof having robots with limited visibility. Hence it allows to exploit all the\nstudies (the majority, actually) done on this topic in the unlimited visibility\nsetting. Indeed, after the robots get close enough to each other, they are able\nto see all the robots in the system, a scenario that is similar to the one\nwhere the robots have unlimited visibility.\n  We present the first (deterministic) algorithm for the Near-Gathering\nproblem, to the best of our knowledge, which allows a set of autonomous mobile\nrobots to nearly gather within finite time without ever colliding. Our\nalgorithm assumes some reasonable conditions on the input configuration (the\nNear-Gathering problem is easily seen to be unsolvable in general). Further,\nall the robots are assumed to have a compass (hence they agree on the \"North\"\ndirection), but they do not necessarily have the same handedness (hence they\nmay disagree on the clockwise direction).\n  We also show how the robots can detect termination, i.e., detect when the\nNear-Gathering problem has been solved. This is crucial when the robots have to\nperform a generic task after having nearly gathered. We show that termination\ndetection can be obtained even if the total number of robots is unknown to the\nrobots themselves (i.e., it is not a parameter of the algorithm), and robots\nhave no way to explicitly communicate.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2015 01:17:10 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["Pagli", "Linda", ""], ["Prencipe", "Giuseppe", ""], ["Viglietta", "Giovanni", ""]]}, {"id": "1505.07417", "submitter": "Nikita Kazeev", "authors": "Andrey Ustyuzhanin, Alexey Artemov, Nikita Kazeev, Artem Redkin", "title": "Event Index - an LHCb Event Search System", "comments": "Report for the proceedings of the CHEP-2015 conference", "journal-ref": "Journal of Physics: Conference Series, vol. 664, num 3, pages\n  032019, 2015", "doi": "10.1088/1742-6596/664/3/032019", "report-no": null, "categories": "hep-ex cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  During LHC Run 1, the LHCb experiment recorded around $10^{11}$ collision\nevents. This paper describes Event Index - an event search system. Its primary\nfunction is to quickly select subsets of events from a combination of\nconditions, such as the estimated decay channel or number of hits in a\nsubdetector. Event Index is essentially Apache Lucene optimized for read-only\nindexes distributed over independent shards on independent nodes.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2015 17:50:54 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2015 21:02:02 GMT"}], "update_date": "2016-01-14", "authors_parsed": [["Ustyuzhanin", "Andrey", ""], ["Artemov", "Alexey", ""], ["Kazeev", "Nikita", ""], ["Redkin", "Artem", ""]]}, {"id": "1505.07605", "submitter": "Fangjin Guo", "authors": "Hongyu Meng, Fangjin Guo", "title": "Simple sorting algorithm test based on CUDA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of computing technology, CUDA has become a very\nimportant tool. In computer programming, sorting algorithm is widely used.\nThere are many simple sorting algorithms such as enumeration sort, bubble sort\nand merge sort. In this paper, we test some simple sorting algorithm based on\nCUDA and draw some useful conclusions.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2015 09:08:23 GMT"}], "update_date": "2015-05-29", "authors_parsed": [["Meng", "Hongyu", ""], ["Guo", "Fangjin", ""]]}, {"id": "1505.07630", "submitter": "Alexey Lastovetsky", "authors": "Amani AlOnazi, David Keyes, Alexey Lastovetsky, Vladimir Rychkov", "title": "Design and Optimization of OpenFOAM-based CFD Applications for Hybrid\n  and Heterogeneous HPC Platforms", "comments": "Presented at ParCFD 2014, prepared for submission to Computer and\n  Fluids. 12 pages, 9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hardware-aware design and optimization is crucial in exploiting emerging\narchitectures for PDE-based computational fluid dynamics applications. In this\nwork, we study optimizations aimed at acceleration of OpenFOAM-based\napplications on emerging hybrid heterogeneous platforms. OpenFOAM uses MPI to\nprovide parallel multi-processor functionality, which scales well on\nhomogeneous systems but does not fully utilize the potential per-node\nperformance on hybrid heterogeneous platforms. In our study, we use two\nOpenFOAM applications, icoFoam and laplacianFoam, both based on Krylov\niterative methods. We propose a number of optimizations of the dominant kernel\nof the Krylov solver, aimed at acceleration of the overall execution of the\napplications on modern GPU-accelerated heterogeneous platforms. Experimental\nresults show that the proposed hybrid implementation significantly outperforms\nthe state-of-the-art implementation.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2015 10:24:35 GMT"}], "update_date": "2015-05-29", "authors_parsed": [["AlOnazi", "Amani", ""], ["Keyes", "David", ""], ["Lastovetsky", "Alexey", ""], ["Rychkov", "Vladimir", ""]]}, {"id": "1505.07635", "submitter": "Xiaojing An", "authors": "Xiaojing An, Haojun Zhao, Lulu Ding, Zhongrui Fan, Hanyue Wang", "title": "Optimized Password Recovery for Encrypted RAR on GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RAR uses classic symmetric encryption algorithm SHA-1 hashing and AES\nalgorithm for encryption, and the only method of password recovery is brute\nforce, which is very time-consuming. In this paper, we present an approach\nusing GPUs to speed up the password recovery process. However, because the\nmajor calculation and time-consuming part, SHA-1 hashing, is hard to be\nparallelized, so this paper adopts coarse granularity parallel. That is, one\nGPU thread is responsible for the validation of one password. We mainly use\nthree optimization methods to optimize this parallel version: asynchronous\nparallel between CPU and GPU, redundant calculations and conditional statements\nreduction, and the usage of registers optimization. Experiment result shows\nthat the final version reaches 43~57 times speedup on an AMD FirePro W8000 GPU,\ncompared to a well-optimized serial version on Intel Core i5 CPU.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2015 10:41:47 GMT"}], "update_date": "2015-05-29", "authors_parsed": [["An", "Xiaojing", ""], ["Zhao", "Haojun", ""], ["Ding", "Lulu", ""], ["Fan", "Zhongrui", ""], ["Wang", "Hanyue", ""]]}, {"id": "1505.07716", "submitter": "Johannes Doerfert", "authors": "Johannes Doerfert, Kevin Streit, Sebastian Hack and Zino Benaissa", "title": "Polly's Polyhedral Scheduling in the Presence of Reductions", "comments": "Presented at the IMPACT15 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The polyhedral model provides a powerful mathematical abstraction to enable\neffective optimization of loop nests with respect to a given optimization goal,\ne.g., exploiting parallelism. Unexploited reduction properties are a frequent\nreason for polyhedral optimizers to assume parallelism prohibiting dependences.\nTo our knowledge, no polyhedral loop optimizer available in any production\ncompiler provides support for reductions. In this paper, we show that\nleveraging the parallelism of reductions can lead to a significant performance\nincrease. We give a precise, dependence based, definition of reductions and\ndiscuss ways to extend polyhedral optimization to exploit the associativity and\ncommutativity of reduction computations. We have implemented a\nreduction-enabled scheduling approach in the Polly polyhedral optimizer and\nevaluate it on the standard Polybench 3.2 benchmark suite. We were able to\ndetect and model all 52 arithmetic reductions and achieve speedups up to\n2.21$\\times$ on a quad core machine by exploiting the multidimensional\nreduction in the BiCG benchmark.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2015 15:05:46 GMT"}], "update_date": "2015-05-29", "authors_parsed": [["Doerfert", "Johannes", ""], ["Streit", "Kevin", ""], ["Hack", "Sebastian", ""], ["Benaissa", "Zino", ""]]}, {"id": "1505.07734", "submitter": "Sascha Hunold", "authors": "Sascha Hunold and Alexandra Carpen-Amarie", "title": "MPI Benchmarking Revisited: Experimental Design and Reproducibility", "comments": "38 pages, 44 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Message Passing Interface (MPI) is the prevalent programming model used\non today's supercomputers. Therefore, MPI library developers are looking for\nthe best possible performance (shortest run-time) of individual MPI functions\nacross many different supercomputer architectures. Several MPI benchmark suites\nhave been developed to assess the performance of MPI implementations.\nUnfortunately, the outcome of these benchmarks is often neither reproducible\nnor statistically sound. To overcome these issues, we show which experimental\nfactors have an impact on the run-time of blocking collective MPI operations\nand how to control them. We address the problem of process and clock\nsynchronization in MPI benchmarks. Finally, we present a new experimental\nmethod that allows us to obtain reproducible and statistically sound MPI\nmeasurements.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2015 15:47:01 GMT"}, {"version": "v2", "created": "Fri, 29 May 2015 10:24:46 GMT"}, {"version": "v3", "created": "Fri, 18 Sep 2015 14:09:42 GMT"}, {"version": "v4", "created": "Fri, 4 Dec 2015 15:00:52 GMT"}, {"version": "v5", "created": "Fri, 27 May 2016 19:39:06 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Hunold", "Sascha", ""], ["Carpen-Amarie", "Alexandra", ""]]}, {"id": "1505.08023", "submitter": "Meng Wu", "authors": "Meng Wu, Can Yang, Taoran Xiang, Daning Cheng", "title": "The Research and Optimization of Parallel Finite Element Algorithm based\n  on MiniFE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite element method (FEM) is one of the most important numerical methods in\nmodern engineering design and analysis. Since traditional serial FEM is\ndifficult to solve large FE problems efficiently and accurately,\nhigh-performance parallel FEM has become one of the essential way to solve\npractical engineering problems. Based on MiniFE program, which is released by\nNational Energy Research Scientific Computing Center(NERSC), this work analyzes\nconcrete steps, key computing pattern and parallel mechanism of parallel FEM.\nAccording to experimental results, this work analyzes the proportion of\ncalculation amount of each module and concludes the main performance bottleneck\nof the program. Based on that, we optimize the MiniFE program on a server\nplatform. The optimization focuses on the bottleneck of the program - SpMV\nkernel, and uses an efficient storage format named BCRS. Moreover, an improving\nplan of hybrid MPI+OpenMP programming is provided. Experimental results show\nthat the optimized program performs better in both SpMV kernel and\nsynchronization. It can increase the performance of the program, on average, by\n8.31%. Keywords : finite element, parallel, MiniFE, SpMV, performance\noptimization\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2015 12:44:04 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["Wu", "Meng", ""], ["Yang", "Can", ""], ["Xiang", "Taoran", ""], ["Cheng", "Daning", ""]]}, {"id": "1505.08067", "submitter": "Mohamed Amine Bergach", "authors": "Mohamed Amine Bergach and Emilien Kofman and Robert de Simone and\n  Serge Tissot and Michel Syska", "title": "Efficient FFT mapping on GPU for radar processing application: modeling\n  and implementation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  General-purpose multiprocessors (as, in our case, Intel IvyBridge and Intel\nHaswell) increasingly add GPU computing power to the former multicore\narchitectures. When used for embedded applications (for us, Synthetic aperture\nradar) with intensive signal processing requirements, they must constantly\ncompute convolution algorithms, such as the famous Fast Fourier Transform. Due\nto its \"fractal\" nature (the typical butterfly shape, with larger FFTs defined\nas combination of smaller ones with auxiliary data array transpose functions),\none can hope to compute analytically the size of the largest FFT that can be\nperformed locally on an elementary GPU compute block. Then, the full\napplication must be organized around this given building block size. Now, due\nto phenomena involved in the data transfers between various memory levels\nacross CPUs and GPUs, the optimality of such a scheme is only loosely\npredictable (as communications tend to overcome in time the complexity of\ncomputations). Therefore a mix of (theoretical) analytic approach and\n(practical) runtime validation is here needed. As we shall illustrate, this\noccurs at both stage, first at the level of deciding on a given elementary FFT\nblock size, then at the full application level.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2015 14:45:03 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["Bergach", "Mohamed Amine", ""], ["Kofman", "Emilien", ""], ["de Simone", "Robert", ""], ["Tissot", "Serge", ""], ["Syska", "Michel", ""]]}, {"id": "1505.08097", "submitter": "Gary McGilvary Mr", "authors": "Gary Andrew McGilvary, Adam Barker, Malcolm Atkinson", "title": "Ad hoc Cloud Computing: From Concept to Realization", "comments": "Published in IEEE Cloud 2015 named \"Ad hoc Cloud Computing\"", "journal-ref": null, "doi": "10.1109/CLOUD.2015.153", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the first complete, integrated and end-to-end solution\nfor ad hoc cloud computing environments. Ad hoc clouds harvest resources from\nexisting sporadically available, non-exclusive (i.e. primarily used for some\nother purpose) and unreliable infrastructures. In this paper we discuss the\nproblems ad hoc cloud computing solves and outline our architecture which is\nbased on BOINC.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2015 15:54:50 GMT"}, {"version": "v2", "created": "Sun, 19 Jul 2015 16:02:20 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["McGilvary", "Gary Andrew", ""], ["Barker", "Adam", ""], ["Atkinson", "Malcolm", ""]]}]