[{"id": "1902.00016", "submitter": "Dimche Kostadinov", "authors": "Dimche Kostadinov, Behrooz Razdehi, Slava Voloshynovskiy", "title": "Network Parameter Learning Using Nonlinear Transforms, Local\n  Representation Goals and Local Propagation Constraints", "comments": "arXiv admin note: text overlap with arXiv:1805.07802", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a novel concept for learning of the parameters in\na neural network. Our idea is grounded on modeling a learning problem that\naddresses a trade-off between (i) satisfying local objectives at each node and\n(ii) achieving desired data propagation through the network under (iii) local\npropagation constraints. We consider two types of nonlinear transforms which\ndescribe the network representations. One of the nonlinear transforms serves as\nactivation function. The other one enables a locally adjusted, deviation\ncorrective components to be included in the update of the network weights in\norder to enable attaining target specific representations at the last network\nnode. Our learning principle not only provides insight into the understanding\nand the interpretation of the learning dynamics, but it offers theoretical\nguarantees over decoupled and parallel parameter estimation strategy that\nenables learning in synchronous and asynchronous mode. Numerical experiments\nvalidate the potential of our approach on image recognition task. The\npreliminary results show advantages in comparison to the state-of-the-art\nmethods, w.r.t. the learning time and the network size while having competitive\nrecognition accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 14:43:55 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Kostadinov", "Dimche", ""], ["Razdehi", "Behrooz", ""], ["Voloshynovskiy", "Slava", ""]]}, {"id": "1902.00052", "submitter": "Ahmad Alhilal", "authors": "Ahmad Alhilal, Salah Dowaji", "title": "Base Station Distance Adaptive LEACH", "comments": "11 pages, in Arabic, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For some applications, we need to deploy a network of sensors in working\nfield to sense the environment and send collected data to a base station for\nprocessing; these sensors depend on non-rechargeable batteries, so the routing\nprotocols for such network of sensors need to be efficient. LEACH is one of\nthese protocols which is a hierarchical routing protocol and helps in saving\nenergy in wireless sensor networks. Enhanced LEACH depends on a mathematical\nmodel to calculate the estimated average energy in each round. consequently,\nutilizing the node remaining energy to ensure rotating cluster head role over\nall the nodes. It also depends on a mathematical model to calculate base\nstation distance from work field whereas LEACH does not take into its account\nany consideration for remaining energy of node. In this paper, we enhance LEACH\n(work efficiency in homogeneous networks) to adapt with base-station distance,\nthus more energy saving for certain distances from base-station. The obtained\nsimulation results show that enhanced LEACH saves energy better than LEACH and\nincrease network stability and reliability when base-station is inside working\nfield and consume the same energy as LEACH when base-station is outside work\nfield.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 22:40:22 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Alhilal", "Ahmad", ""], ["Dowaji", "Salah", ""]]}, {"id": "1902.00147", "submitter": "Amir Erfan Eshratifar", "authors": "Amir Erfan Eshratifar, Amirhossein Esmaili, Massoud Pedram", "title": "Towards Collaborative Intelligence Friendly Architectures for Deep\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern mobile devices are equipped with high-performance hardware resources\nsuch as graphics processing units (GPUs), making the end-side intelligent\nservices more feasible. Even recently, specialized silicons as neural engines\nare being used for mobile devices. However, most mobile devices are still not\ncapable of performing real-time inference using very deep models. Computations\nassociated with deep models for today's intelligent applications are typically\nperformed solely on the cloud. This cloud-only approach requires significant\namounts of raw data to be uploaded to the cloud over the mobile wireless\nnetwork and imposes considerable computational and communication load on the\ncloud server. Recent studies have shown that the latency and energy consumption\nof deep neural networks in mobile applications can be notably reduced by\nsplitting the workload between the mobile device and the cloud. In this\napproach, referred to as collaborative intelligence, intermediate features\ncomputed on the mobile device are offloaded to the cloud instead of the raw\ninput data of the network, reducing the size of the data needed to be sent to\nthe cloud. In this paper, we design a new collaborative intelligence friendly\narchitecture by introducing a unit responsible for reducing the size of the\nfeature data needed to be offloaded to the cloud to a greater extent, where\nthis unit is placed after a selected layer of a deep model. Our proposed\nmethod, across different wireless networks, achieves on average 53x\nimprovements for end-to-end latency and 68x improvements for mobile energy\nconsumption compared to the status quo cloud-only approach for ResNet-50, while\nthe accuracy loss is less than 2%.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 01:37:57 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Eshratifar", "Amir Erfan", ""], ["Esmaili", "Amirhossein", ""], ["Pedram", "Massoud", ""]]}, {"id": "1902.00319", "submitter": "Gregor Ulm", "authors": "Gregor Ulm, Simon Smith, Adrian Nilsson, Emil Gustavsson, Mats\n  Jirstrand", "title": "OODIDA: On-board/Off-board Distributed Real-Time Data Analytics for\n  Connected Vehicles", "comments": "28 pages, 9 figures, 2 algorithms, 2 code listings, 1 table", "journal-ref": "Data Science and Engineering Vol. 6 (2021)", "doi": "10.1007/s41019-021-00152-6", "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fleet of connected vehicles easily produces many gigabytes of data per\nhour, making centralized (off-board) data processing impractical. In addition,\nthere is the issue of distributing tasks to on-board units in vehicles and\nprocessing them efficiently. Our solution to this problem is OODIDA\n(On-board/Off-board Distributed Data Analytics), which is a platform that\ntackles both task distribution to connected vehicles as well as concurrent\nexecution of tasks on arbitrary subsets of edge clients. Its message-passing\ninfrastructure has been implemented in Erlang/OTP, while the end points use a\nlanguage-independent JSON interface. Computations can be carried out in\narbitrary programming languages. The message-passing infrastructure of OODIDA\nis highly scalable, facilitating the execution of large numbers of concurrent\ntasks.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 13:31:33 GMT"}, {"version": "v2", "created": "Thu, 6 Feb 2020 15:06:20 GMT"}, {"version": "v3", "created": "Sun, 31 Jan 2021 18:36:57 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Ulm", "Gregor", ""], ["Smith", "Simon", ""], ["Nilsson", "Adrian", ""], ["Gustavsson", "Emil", ""], ["Jirstrand", "Mats", ""]]}, {"id": "1902.00340", "submitter": "Anastasiia Koloskova", "authors": "Anastasia Koloskova, Sebastian U. Stich, Martin Jaggi", "title": "Decentralized Stochastic Optimization and Gossip Algorithms with\n  Compressed Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider decentralized stochastic optimization with the objective function\n(e.g. data samples for machine learning task) being distributed over $n$\nmachines that can only communicate to their neighbors on a fixed communication\ngraph. To reduce the communication bottleneck, the nodes compress (e.g.\nquantize or sparsify) their model updates. We cover both unbiased and biased\ncompression operators with quality denoted by $\\omega \\leq 1$ ($\\omega=1$\nmeaning no compression). We (i) propose a novel gossip-based stochastic\ngradient descent algorithm, CHOCO-SGD, that converges at rate\n$\\mathcal{O}\\left(1/(nT) + 1/(T \\delta^2 \\omega)^2\\right)$ for strongly convex\nobjectives, where $T$ denotes the number of iterations and $\\delta$ the\neigengap of the connectivity matrix. Despite compression quality and network\nconnectivity affecting the higher order terms, the first term in the rate,\n$\\mathcal{O}(1/(nT))$, is the same as for the centralized baseline with exact\ncommunication. We (ii) present a novel gossip algorithm, CHOCO-GOSSIP, for the\naverage consensus problem that converges in time\n$\\mathcal{O}(1/(\\delta^2\\omega) \\log (1/\\epsilon))$ for accuracy $\\epsilon >\n0$. This is (up to our knowledge) the first gossip algorithm that supports\narbitrary compressed messages for $\\omega > 0$ and still exhibits linear\nconvergence. We (iii) show in experiments that both of our algorithms do\noutperform the respective state-of-the-art baselines and CHOCO-SGD can reduce\ncommunication by at least two orders of magnitudes.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 14:11:20 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Koloskova", "Anastasia", ""], ["Stich", "Sebastian U.", ""], ["Jaggi", "Martin", ""]]}, {"id": "1902.00465", "submitter": "David Budden", "authors": "Peter Buchlovsky, David Budden, Dominik Grewe, Chris Jones, John\n  Aslanides, Frederic Besse, Andy Brock, Aidan Clark, Sergio G\\'omez\n  Colmenarejo, Aedan Pope, Fabio Viola and Dan Belov", "title": "TF-Replicator: Distributed Machine Learning for Researchers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe TF-Replicator, a framework for distributed machine learning\ndesigned for DeepMind researchers and implemented as an abstraction over\nTensorFlow. TF-Replicator simplifies writing data-parallel and model-parallel\nresearch code. The same models can be effortlessly deployed to different\ncluster architectures (i.e. one or many machines containing CPUs, GPUs or TPU\naccelerators) using synchronous or asynchronous training regimes. To\ndemonstrate the generality and scalability of TF-Replicator, we implement and\nbenchmark three very different models: (1) A ResNet-50 for ImageNet\nclassification, (2) a SN-GAN for class-conditional ImageNet image generation,\nand (3) a D4PG reinforcement learning agent for continuous control. Our results\nshow strong scalability performance without demanding any distributed systems\nexpertise of the user. The TF-Replicator programming model will be open-sourced\nas part of TensorFlow 2.0 (see\nhttps://github.com/tensorflow/community/pull/25).\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 17:26:07 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Buchlovsky", "Peter", ""], ["Budden", "David", ""], ["Grewe", "Dominik", ""], ["Jones", "Chris", ""], ["Aslanides", "John", ""], ["Besse", "Frederic", ""], ["Brock", "Andy", ""], ["Clark", "Aidan", ""], ["Colmenarejo", "Sergio G\u00f3mez", ""], ["Pope", "Aedan", ""], ["Viola", "Fabio", ""], ["Belov", "Dan", ""]]}, {"id": "1902.00475", "submitter": "Artem Lutov", "authors": "Artem Lutov, Mourad Khayati and Philippe Cudr\\'e-Mauroux", "title": "Clubmark: a Parallel Isolation Framework for Benchmarking and Profiling\n  Clustering Algorithms on NUMA Architectures", "comments": "Application sources and executables:\n  https://github.com/eXascaleInfolab/clubmark", "journal-ref": "2018 IEEE International Conference on Data Mining Workshops\n  (ICDMW)", "doi": "10.1109/ICDMW.2018.00212", "report-no": null, "categories": "cs.DC cs.SY physics.data-an", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is a great diversity of clustering and community detection algorithms,\nwhich are key components of many data analysis and exploration systems. To the\nbest of our knowledge, however, there does not exist yet any uniform\nbenchmarking framework, which is publicly available and suitable for the\nparallel benchmarking of diverse clustering algorithms on a wide range of\nsynthetic and real-world datasets. In this paper, we introduce Clubmark, a new\nextensible framework that aims to fill this gap by providing a parallel\nisolation benchmarking platform for clustering algorithms and their evaluation\non NUMA servers. Clubmark allows for fine-grained control over various\nexecution variables (timeouts, memory consumption, CPU affinity and cache\npolicy) and supports the evaluation of a wide range of clustering algorithms\nincluding multi-level, hierarchical and overlapping clustering techniques on\nboth weighted and unweighted input networks with built-in evaluation of several\nextrinsic and intrinsic measures. Our framework is open-source and provides a\nconsistent and systematic way to execute, evaluate and profile clustering\ntechniques considering a number of aspects that are often missing in\nstate-of-the-art frameworks and benchmarking systems.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 17:49:26 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Lutov", "Artem", ""], ["Khayati", "Mourad", ""], ["Cudr\u00e9-Mauroux", "Philippe", ""]]}, {"id": "1902.00795", "submitter": "Jinoh Kim", "authors": "Jinhwan Choi, Yu Gu, Jinoh Kim", "title": "Learning-based Dynamic Cache Management in a Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Caches are an important component of modern computing systems given their\nsignificant impact on performance. In particular, caches play a key role in the\ncloud due to the nature of large-scale, data-intensive processing. One of the\nkey challenges for the cloud providers is how to share the caching capacity\namong tenants, under the circumstance that each often requires a different\ndegree of quality of service (QoS) with respect to data access performance. The\ninvariant is that the individual tenants' QoS requirements should be satisfied\nwhile the cache usage is optimized in a system-wide manner. In this paper, we\nintroduce a learning-based approach for dynamic cache management in a cloud,\nwhich is based on the estimation of data access pattern of a tenant and the\nprediction of cache performance for the access pattern in question. We consider\na variety of probability distributions to estimate the data access pattern, and\nexamine a set of learning-based regression techniques to predict the cache hit\nrate for the access pattern. The predicted cache hit rate is then used to make\na decision whether reallocating cache space is needed to meet the QoS\nrequirement for the tenant. Our experimental results with an extensive set of\nsynthetic traces and the YCSB benchmark show that the proposed method\nconsistently optimizes the cache space while satisfying the QoS requirement.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2019 21:08:18 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Choi", "Jinhwan", ""], ["Gu", "Yu", ""], ["Kim", "Jinoh", ""]]}, {"id": "1902.00808", "submitter": "Jayant A. Gupchup", "authors": "Jayant Gupchup, Douglas Carlson, R\\u{a}zvan Mus\\u{a}loiu-E., Alex\n  Szalay, Andreas Terzis", "title": "Phoenix: An Epidemic Approach to Time Reconstruction", "comments": null, "journal-ref": "EWSN 2010 Proceedings of the 7th European Conference on Wireless\n  Sensor Networks", "doi": "10.1007/978-3-642-11917-0_2", "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Harsh deployment environments and uncertain run-time conditions create\nnumerous challenges for postmortem time reconstruction methods. For example,\nmotes often reboot and thus lose their clock state, considering that the\nmajority of mote platforms lack a real-time clock. While existing time\nreconstruction methods for long-term data gathering networks rely on a\npersistent basestation for assigning global timestamps to measurements, the\nbasestation may be unavailable due to hardware and software faults. We present\nPhoenix, a novel offline algorithm for reconstructing global timestamps that is\nrobust to frequent mote reboots and does not require a persistent global time\nsource. This independence sets Phoenix apart from the majority of time\nreconstruction algorithms which assume that such a source is always available.\nMotes in Phoenix exchange their time-related state with their neighbors,\nestablishing a chain of transitive temporal relationships to one or more motes\nwith references to the global time. These relationships allow Phoenix to\nreconstruct the measurement timeline for each mote. Results from simulations\nand a deployment indicate that Phoenix can achieve timing accuracy up to 6 ppm\nfor 99% of the collected measurements. Phoenix is able to maintain this\nperformance for periods that last for months without a persistent global time\nsource. To achieve this level of performance for the targeted environmental\nmonitoring application, Phoenix requires an additional space overhead of 4% and\nan additional duty cycle of 0.2%.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2019 23:15:20 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Gupchup", "Jayant", ""], ["Carlson", "Douglas", ""], ["Mus\u0103loiu-E.", "R\u0103zvan", ""], ["Szalay", "Alex", ""], ["Terzis", "Andreas", ""]]}, {"id": "1902.00837", "submitter": "Xiaoheng Deng", "authors": "Yajun Liu, Congxu Zhu, Xiaoheng Deng, Peiyuan Guan, Zhiwen Wan, Jie\n  Luo, Enlu Liu and Honggang Zhang", "title": "UAV-aided urban target tracking system based on edge computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Target tracking is an important issue of social security. In order to track a\ntarget, traditionally a large amount of surveillance video data need to be\nuploaded into the cloud for processing and analysis, which put stremendous\nbandwidth pressure on communication links in access networks and core networks.\nAt the same time, the long delay in wide area network is very likely to cause a\ntracking system to lose its target. Often, unmanned aerial vehicle (UAV) has\nbeen adopted for target tracking due to its flexibility, but its limited flight\ntime due to battery constraint and the blocking by various obstacles in the\nfield pose two major challenges to its target tracking task, which also very\nlikely results in the loss of target. A novel target tracking model that\ncoordinates the tracking by UAV and ground nodes in an edge computing\nenvironment is proposed in this study. The model can effectively reduce the\ncommunication cost and the long delay of the traditional surveillance camera\nsystem that relies on cloud computing, and it can improve the probability of\nfinding a target again after an UAV loses the tracing of that target. It has\nbeen demonstrated that the proposed system achieved a significantly better\nperformance in terms of low latency, high reliability, and optimal quality of\nexperience (QoE).\n", "versions": [{"version": "v1", "created": "Sun, 3 Feb 2019 03:35:36 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Liu", "Yajun", ""], ["Zhu", "Congxu", ""], ["Deng", "Xiaoheng", ""], ["Guan", "Peiyuan", ""], ["Wan", "Zhiwen", ""], ["Luo", "Jie", ""], ["Liu", "Enlu", ""], ["Zhang", "Honggang", ""]]}, {"id": "1902.00846", "submitter": "Jeremy Kepner", "authors": "Jeremy Kepner, Vijay Gadepally, Lauren Milechin, Siddharth Samsi,\n  William Arcand, David Bestor, William Bergeron, Chansup Byun, Matthew\n  Hubbell, Micheal Houle, Micheal Jones, Anne Klein, Peter Michaleas, Julie\n  Mullen, Andrew Prout, Antonio Rosa, Charles Yee, Albert Reuther", "title": "A Billion Updates per Second Using 30,000 Hierarchical In-Memory D4M\n  Databases", "comments": "Northeast Database Data 2019 (MIT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing large scale networks requires high performance streaming updates of\ngraph representations of these data. Associative arrays are mathematical\nobjects combining properties of spreadsheets, databases, matrices, and graphs,\nand are well-suited for representing and analyzing streaming network data. The\nDynamic Distributed Dimensional Data Model (D4M) library implements associative\narrays in a variety of languages (Python, Julia, and Matlab/Octave) and\nprovides a lightweight in-memory database. Associative arrays are designed for\nblock updates. Streaming updates to a large associative array requires a\nhierarchical implementation to optimize the performance of the memory\nhierarchy. Running 34,000 instances of a hierarchical D4M associative arrays on\n1,100 server nodes on the MIT SuperCloud achieved a sustained update rate of\n1,900,000,000 updates per second. This capability allows the MIT SuperCloud to\nanalyze extremely large streaming network data sets.\n", "versions": [{"version": "v1", "created": "Sun, 3 Feb 2019 04:58:07 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Kepner", "Jeremy", ""], ["Gadepally", "Vijay", ""], ["Milechin", "Lauren", ""], ["Samsi", "Siddharth", ""], ["Arcand", "William", ""], ["Bestor", "David", ""], ["Bergeron", "William", ""], ["Byun", "Chansup", ""], ["Hubbell", "Matthew", ""], ["Houle", "Micheal", ""], ["Jones", "Micheal", ""], ["Klein", "Anne", ""], ["Michaleas", "Peter", ""], ["Mullen", "Julie", ""], ["Prout", "Andrew", ""], ["Rosa", "Antonio", ""], ["Yee", "Charles", ""], ["Reuther", "Albert", ""]]}, {"id": "1902.00878", "submitter": "Christos Patsonakis Mr", "authors": "Christos Patsonakis, Katerina Samari, Aggelos Kiayias and Mema\n  Roussopoulos", "title": "On the Practicality of Smart Contract PKI", "comments": "10 pages, 3 figures, 2 tables, DAPPCON 2019", "journal-ref": "2019 IEEE International Conference on Decentralized Applications\n  and Infrastructures (DAPPCON)", "doi": "10.1109/DAPPCON.2019.00022", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Public key infrastructures (PKIs) are one of the main building blocks for\nsecuring communications over the Internet. Currently, PKIs are under the\ncontrol of centralized authorities, which is problematic as evidenced by\nnumerous incidents where they have been compromised. The distributed, fault\ntolerant log of transactions provided by blockchains and more recently, smart\ncontract platforms, constitutes a powerful tool for the decentralization of\nPKIs. To verify the validity of identity records, blockchain-based identity\nsystems store on chain either all identity records, or, a small (or even\nconstant) sized amount of data to verify identity records stored off chain.\nHowever, as most of these systems have never been implemented, there is little\ninformation regarding the practical implications of each design's tradeoffs.\n  In this work, we first implement and evaluate the only provably secure, smart\ncontract based PKI of [1] on top of Ethereum. This construction incurs\nconstant-sized storage at the expense of computational complexity. To explore\nthis tradeoff, we propose and implement a second construction which, eliminates\nthe need for trusted setup, preserves the security properties of [1] and, as\nillustrated through our evaluation, is the only version with constant-sized\nstate that can be deployed on the live chain of Ethereum. Furthermore, we\ncompare these two systems with the simple approach of most prior works, e.g.,\nthe Ethereum Name Service, where all identity records are stored on the smart\ncontract's state, to illustrate several shortcomings of Ethereum and its cost\nmodel. We propose several modifications for fine tuning the model, which would\nbe useful to be considered for any smart contract platform like Ethereum so\nthat it reaches its full potential to support arbitrary distributed\napplications.\n", "versions": [{"version": "v1", "created": "Sun, 3 Feb 2019 10:59:41 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Patsonakis", "Christos", ""], ["Samari", "Katerina", ""], ["Kiayias", "Aggelos", ""], ["Roussopoulos", "Mema", ""]]}, {"id": "1902.00881", "submitter": "Christos Patsonakis Mr", "authors": "Christos Patsonakis and Mema Roussopoulos", "title": "An Alternative Paradigm for Developing and Pricing Storage on Smart\n  Contract Platforms", "comments": "6 pages, 2 figures, DAPPCON 2019", "journal-ref": "2019 IEEE International Conference on Decentralized Applications\n  and Infrastructures (DAPPCON)", "doi": "10.1109/DAPPCON.2019.00032", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart contract platforms facilitate the development of important and diverse\ndistributed applications in a simple manner. This simplicity stems from the\ninherent utility of employing the state of smart contracts to store, query and\nverify the validity of application data. In Ethereum, data storage incurs an\nunderpriced, non-recurring, predefined fee. Furthermore, as there is no\nincentive for freeing or minimizing the state of smart contracts, Ethereum is\nfaced with a tragedy of the commons problem with regards to its monotonically\nincreasing state. This issue, if left unchecked, may lead to centralization and\ndirectly impact Ethereum's security and longevity. In this work, we introduce\nan alternative paradigm for developing smart contracts in which their state is\nof constant size and facilitates the verification of application data that are\nstored to and queried from an external, potentially unreliable, storage\nnetwork. This approach is relevant for a wide range of applications, such as\nany key-value store. We evaluate our approach by adapting the most widely\ndeployed standard for fungible tokens, i.e., the ERC20 token standard. We show\nthat Ethereum's current cost model penalizes our approach, even though it\nminimizes the overhead to Ethereum's state and aligns well with Ethereum's\nfuture. We address Ethereum's monotonically increasing state in a two-fold\nmanner. First, we introduce recurring fees that are proportional to the state\nof smart contracts and adjustable by the miners that maintain the network.\nSecond, we propose a scheme where the cost of storage-related operations\nreflects the effort that miners have to expend to execute them. Lastly, we show\nthat under such a pricing scheme that encourages economy in the state consumed\nby smart contracts, our ERC20 token adaptation reduces the incurred transaction\nfees by up to an order of magnitude.\n", "versions": [{"version": "v1", "created": "Sun, 3 Feb 2019 11:18:27 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Patsonakis", "Christos", ""], ["Roussopoulos", "Mema", ""]]}, {"id": "1902.01000", "submitter": "Amir Erfan Eshratifar", "authors": "Amir Erfan Eshratifar, Amirhossein Esmaili, Massoud Pedram", "title": "BottleNet: A Deep Learning Architecture for Intelligent Mobile Cloud\n  Computing Services", "comments": "arXiv admin note: text overlap with arXiv:1902.00147", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown the latency and energy consumption of deep neural\nnetworks can be significantly improved by splitting the network between the\nmobile device and cloud. This paper introduces a new deep learning\narchitecture, called BottleNet, for reducing the feature size needed to be sent\nto the cloud. Furthermore, we propose a training method for compensating for\nthe potential accuracy loss due to the lossy compression of features before\ntransmitting them to the cloud. BottleNet achieves on average 30x improvement\nin end-to-end latency and 40x improvement in mobile energy consumption compared\nto the cloud-only approach with negligible accuracy loss.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 01:15:41 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Eshratifar", "Amir Erfan", ""], ["Esmaili", "Amirhossein", ""], ["Pedram", "Massoud", ""]]}, {"id": "1902.01046", "submitter": "Wolfgang Grieskamp", "authors": "Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex\n  Ingerman, Vladimir Ivanov, Chloe Kiddon, Jakub Kone\\v{c}n\\'y, Stefano\n  Mazzocchi, H. Brendan McMahan, Timon Van Overveldt, David Petrou, Daniel\n  Ramage, Jason Roselander", "title": "Towards Federated Learning at Scale: System Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning is a distributed machine learning approach which enables\nmodel training on a large corpus of decentralized data. We have built a\nscalable production system for Federated Learning in the domain of mobile\ndevices, based on TensorFlow. In this paper, we describe the resulting\nhigh-level design, sketch some of the challenges and their solutions, and touch\nupon the open problems and future directions.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 06:27:41 GMT"}, {"version": "v2", "created": "Fri, 22 Mar 2019 20:25:57 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Bonawitz", "Keith", ""], ["Eichner", "Hubert", ""], ["Grieskamp", "Wolfgang", ""], ["Huba", "Dzmitry", ""], ["Ingerman", "Alex", ""], ["Ivanov", "Vladimir", ""], ["Kiddon", "Chloe", ""], ["Kone\u010dn\u00fd", "Jakub", ""], ["Mazzocchi", "Stefano", ""], ["McMahan", "H. Brendan", ""], ["Van Overveldt", "Timon", ""], ["Petrou", "David", ""], ["Ramage", "Daniel", ""], ["Roselander", "Jason", ""]]}, {"id": "1902.01064", "submitter": "Qinyi Luo", "authors": "Qinyi Luo, Jinkun Lin, Youwei Zhuo and Xuehai Qian", "title": "Hop: Heterogeneity-Aware Decentralized Training", "comments": null, "journal-ref": null, "doi": "10.1145/3297858.3304009", "report-no": null, "categories": "cs.DC cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that decentralized algorithms can deliver superior\nperformance over centralized ones in the context of machine learning. The two\napproaches, with the main difference residing in their distinct communication\npatterns, are both susceptible to performance degradation in heterogeneous\nenvironments. Although vigorous efforts have been devoted to supporting\ncentralized algorithms against heterogeneity, little has been explored in\ndecentralized algorithms regarding this problem.\n  This paper proposes Hop, the first heterogeneity-aware decentralized training\nprotocol. Based on a unique characteristic of decentralized training that we\nhave identified, the iteration gap, we propose a queue-based synchronization\nmechanism that can efficiently implement backup workers and bounded staleness\nin the decentralized setting. To cope with deterministic slowdown, we propose\nskipping iterations so that the effect of slower workers is further mitigated.\nWe build a prototype implementation of Hop on TensorFlow. The experiment\nresults on CNN and SVM show significant speedup over standard decentralized\ntraining in heterogeneous settings.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 07:50:44 GMT"}, {"version": "v2", "created": "Thu, 7 Feb 2019 17:25:30 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Luo", "Qinyi", ""], ["Lin", "Jinkun", ""], ["Zhuo", "Youwei", ""], ["Qian", "Xuehai", ""]]}, {"id": "1902.01321", "submitter": "Xiaohu Wu", "authors": "Xiaohu Wu, Francesco De Pellegrini, Guanyu Gao and Giuliano Casale", "title": "A Framework for Allocating Server Time to Spot and On-demand Services in\n  Cloud Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing delivers value to users by facilitating their access to\ncomputing capacity in periods when their need arises. An approach is to provide\nboth on-demand and spot services on shared servers. The former allows users to\naccess servers on demand at a fixed price and users occupy different periods of\nservers. The latter allows users to bid for the remaining unoccupied periods\nvia dynamic pricing; however, without appropriate design, such periods may be\narbitrarily small since on-demand users arrive randomly. This is also the\ncurrent service model adopted by Amazon Elastic Cloud Compute. In this paper,\nwe provide the first integral framework for sharing the time of servers between\non-demand and spot services while optimally pricing spot instances. It\nguarantees that on-demand users can get served quickly while spot users can\nstably utilize servers for a properly long period once accepted, which is a key\nfeature to make both on-demand and spot services accessible. Simulation results\nshow that, by complementing the on-demand market with a spot market, a cloud\nprovider can improve revenue by up to 464.7%. The framework is designed under\nassumptions which are met in real environments. It is a new tool that cloud\noperators can use to quantify the advantage of a hybrid spot and on-demand\nservice, eventually making the case for operating such service model in their\nown infrastructures.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 17:18:22 GMT"}, {"version": "v2", "created": "Sun, 1 Sep 2019 15:38:27 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Wu", "Xiaohu", ""], ["De Pellegrini", "Francesco", ""], ["Gao", "Guanyu", ""], ["Casale", "Giuliano", ""]]}, {"id": "1902.01437", "submitter": "Junhao Li", "authors": "Junhao Li, Hang Zhang", "title": "Blaze: Simplified High Performance Cluster Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG cs.PF", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  MapReduce and its variants have significantly simplified and accelerated the\nprocess of developing parallel programs. However, most MapReduce\nimplementations focus on data-intensive tasks while many real-world tasks are\ncompute intensive and their data can fit distributedly into the memory. For\nthese tasks, the speed of MapReduce programs can be much slower than those\nhand-optimized ones. We present Blaze, a C++ library that makes it easy to\ndevelop high performance parallel programs for such compute intensive tasks. At\nthe core of Blaze is a highly-optimized in-memory MapReduce function, which has\nthree main improvements over conventional MapReduce implementations: eager\nreduction, fast serialization, and special treatment for a small fixed key\nrange. We also offer additional conveniences that make developing parallel\nprograms similar to developing serial programs. These improvements make Blaze\nan easy-to-use cluster computing library that approaches the speed of\nhand-optimized parallel code. We apply Blaze to some common data mining tasks,\nincluding word frequency count, PageRank, k-means, expectation maximization\n(Gaussian mixture model), and k-nearest neighbors. Blaze outperforms Apache\nSpark by more than 10 times on average for these tasks, and the speed of Blaze\nscales almost linearly with the number of nodes. In addition, Blaze uses only\nthe MapReduce function and 3 utility functions in its implementation while\nSpark uses almost 30 different parallel primitives in its official\nimplementation.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 19:28:15 GMT"}, {"version": "v2", "created": "Wed, 6 Feb 2019 02:59:19 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Li", "Junhao", ""], ["Zhang", "Hang", ""]]}, {"id": "1902.01457", "submitter": "Mohammad Javad Amiri", "authors": "Mohammad Javad Amiri, Divyakant Agrawal, Amr El Abbadi", "title": "ParBlockchain: Leveraging Transaction Parallelism in Permissioned\n  Blockchain Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many existing blockchains do not adequately address all the characteristics\nof distributed system applications and suffer from serious architectural\nlimitations resulting in performance and confidentiality issues. While recent\npermissioned blockchain systems, have tried to overcome these limitations,\ntheir focus has mainly been on workloads with no-contention, i.e., no\nconflicting transactions. In this paper, we introduce OXII, a new paradigm for\npermissioned blockchains to support distributed applications that execute\nconcurrently. OXII is designed for workloads with (different degrees of)\ncontention. We then present ParBlockchain, a permissioned blockchain designed\nspecifically in the OXII paradigm. The evaluation of ParBlockchain using a\nseries of benchmarks reveals that its performance in workloads with any degree\nof contention is better than the state of the art permissioned blockchain\nsystems.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 20:56:04 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Amiri", "Mohammad Javad", ""], ["Agrawal", "Divyakant", ""], ["Abbadi", "Amr El", ""]]}, {"id": "1902.01613", "submitter": "Hidehito Yabuuchi", "authors": "Hidehito Yabuuchi, Daisuke Taniwaki, Shingo Omura", "title": "Low-latency job scheduling with preemption for the development of deep\n  learning", "comments": "10 pages, 8 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One significant challenge in the job scheduling of computing clusters for the\ndevelopment of deep learning algorithms is the efficient scheduling of\ntrial-and-error (TE) job, the type of job in which the users seek to conduct\nsmall-scale experiments while monitoring their processes. Unfortunately, the\nexisting job schedulers to date do not feature well-balanced scheduling for the\nmixture of TE jobs and best-effort (BE) jobs, or they can handle the mixture in\nlimited situations at most. To fill in this niche, we propose an algorithm that\ncan significantly reduce the latency of TE jobs in versatile situations without\ngreatly elongating the slowdown of the BE jobs. Our algorithm efficiently\nschedules both TE and BE jobs by selectively preempting the BE jobs that can\nbe, when the time comes, resumed without much delay. In our simulation study\nwith synthetic and real workloads, we were able to reduce the 95th percentile\nof the slowdown rates for the TE jobs in the standard FIFO strategy by 96.6%,\nwhile compromising the median of the BE slowdown rates by only 18.0% and the\n95th percentile by only 23.9%.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 09:46:53 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Yabuuchi", "Hidehito", ""], ["Taniwaki", "Daisuke", ""], ["Omura", "Shingo", ""]]}, {"id": "1902.01668", "submitter": "Stefan Jaax", "authors": "Michael Blondin, Javier Esparza, Stefan Jaax", "title": "Expressive Power of Broadcast Consensus Protocols", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Population protocols are a formal model of computation by identical,\nanonymous mobile agents interacting in pairs. Their computational power is\nrather limited: Angluin et al. have shown that they can only compute the\npredicates over $\\mathbb{N}^k$ expressible in Presburger arithmetic. For this\nreason, several extensions of the model have been proposed, including the\naddition of devices called cover-time services, absence detectors, and clocks.\nAll these extensions increase the expressive power to the class of predicates\nover $\\mathbb{N}^k$ lying in the complexity class NL when the input is given in\nunary. However, these devices are difficult to implement, since they require\nthat an agent atomically receives messages from all other agents in a\npopulation of unknown size; moreover, the agent must know that they have all\nbeen received. Inspired by the work of the verification community on Emerson\nand Namjoshi's broadcast protocols, we show that NL-power is also achieved by\nextending population protocols with reliable broadcasts, a simpler, standard\ncommunication primitive.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 13:12:43 GMT"}, {"version": "v2", "created": "Wed, 3 Jul 2019 22:12:08 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Blondin", "Michael", ""], ["Esparza", "Javier", ""], ["Jaax", "Stefan", ""]]}, {"id": "1902.01686", "submitter": "Sergei Volodin", "authors": "El-Mahdi El-Mhamdi, Rachid Guerraoui, Andrei Kucharavy, Sergei Volodin", "title": "The Probabilistic Fault Tolerance of Neural Networks in the Continuous\n  Limit", "comments": "10 pages (without references), 2 figures, 2 tables, 1 algorithm, 26\n  pages of supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The loss of a few neurons in a brain rarely results in any visible loss of\nfunction. However, the insight into what \"few\" means in this context is\nunclear. How many random neuron failures will it take to lead to a visible loss\nof function? In this paper, we address the fundamental question of the impact\nof the crash of a random subset of neurons on the overall computation of a\nneural network and the error in the output it produces. We study fault\ntolerance of neural networks subject to small random neuron/weight crash\nfailures in a probabilistic setting. We give provable guarantees on the\nrobustness of the network to these crashes. Our main contribution is a bound on\nthe error in the output of a network under small random Bernoulli crashes\nproved by using a Taylor expansion in the continuous limit, where close-by\nneurons at a layer are similar. The failure mode we adopt in our model is\ncharacteristic of neuromorphic hardware, a promising technology to speed up\nartificial neural networks, as well as of biological networks. We show that our\ntheoretical bounds can be used to compare the fault tolerance of different\narchitectures and to design a regularizer improving the fault tolerance of a\ngiven architecture. We design an algorithm achieving fault tolerance using a\nreasonable number of neurons. In addition to the theoretical proof, we also\nprovide experimental validation of our results and suggest a connection to the\ngeneralization capacity problem.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 14:08:21 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 14:18:27 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["El-Mhamdi", "El-Mahdi", ""], ["Guerraoui", "Rachid", ""], ["Kucharavy", "Andrei", ""], ["Volodin", "Sergei", ""]]}, {"id": "1902.01856", "submitter": "Ehsan Kazemi Dr", "authors": "Ehsan Kazemi, Liqiang Wang", "title": "Asynchronous Delay-Aware Accelerated Proximal Coordinate Descent for\n  Nonconvex Nonsmooth Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonconvex and nonsmooth problems have recently attracted considerable\nattention in machine learning. However, developing efficient methods for the\nnonconvex and nonsmooth optimization problems with certain performance\nguarantee remains a challenge. Proximal coordinate descent (PCD) has been\nwidely used for solving optimization problems, but the knowledge of PCD methods\nin the nonconvex setting is very limited. On the other hand, the asynchronous\nproximal coordinate descent (APCD) recently have received much attention in\norder to solve large-scale problems. However, the accelerated variants of APCD\nalgorithms are rarely studied. In this paper, we extend APCD method to the\naccelerated algorithm (AAPCD) for nonsmooth and nonconvex problems that\nsatisfies the sufficient descent property, by comparing between the function\nvalues at proximal update and a linear extrapolated point using a delay-aware\nmomentum value. To the best of our knowledge, we are the first to provide\nstochastic and deterministic accelerated extension of APCD algorithms for\ngeneral nonconvex and nonsmooth problems ensuring that for both bounded delays\nand unbounded delays every limit point is a critical point. By leveraging\nKurdyka-Lojasiewicz property, we will show linear and sublinear convergence\nrates for the deterministic AAPCD with bounded delays. Numerical results\ndemonstrate the practical efficiency of our algorithm in speed.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 00:40:26 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Kazemi", "Ehsan", ""], ["Wang", "Liqiang", ""]]}, {"id": "1902.01894", "submitter": "Ang Li", "authors": "Ang Li, Ola Spyra, Sagi Perel, Valentin Dalibard, Max Jaderberg,\n  Chenjie Gu, David Budden, Tim Harley, Pramod Gupta", "title": "A Generalized Framework for Population Based Training", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Population Based Training (PBT) is a recent approach that jointly optimizes\nneural network weights and hyperparameters which periodically copies weights of\nthe best performers and mutates hyperparameters during training. Previous PBT\nimplementations have been synchronized glass-box systems. We propose a general,\nblack-box PBT framework that distributes many asynchronous \"trials\" (a small\nnumber of training steps with warm-starting) across a cluster, coordinated by\nthe PBT controller. The black-box design does not make assumptions on model\narchitectures, loss functions or training procedures. Our system supports\ndynamic hyperparameter schedules to optimize both differentiable and\nnon-differentiable metrics. We apply our system to train a state-of-the-art\nWaveNet generative model for human voice synthesis. We show that our PBT system\nachieves better accuracy, less sensitivity and faster convergence compared to\nexisting methods, given the same computational resource.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 20:11:17 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Li", "Ang", ""], ["Spyra", "Ola", ""], ["Perel", "Sagi", ""], ["Dalibard", "Valentin", ""], ["Jaderberg", "Max", ""], ["Gu", "Chenjie", ""], ["Budden", "David", ""], ["Harley", "Tim", ""], ["Gupta", "Pramod", ""]]}, {"id": "1902.01898", "submitter": "Fei Wu", "authors": "Fei Wu, Yang Cao, Thomas Robertazzi", "title": "Optimal Divisible Load Scheduling for Resource-Sharing Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scheduling is an important task allowing parallel systems to perform\nefficiently and reliably. For modern computation systems, divisible load is a\nspecial type of data which can be divided into arbitrary sizes and\nindependently processed in parallel. Such loads are commonly encountered in\napplications which are processing a great amount of similar data units. For a\nmulti-task processor, the processor's speed may be time-varying due to the\narrival and departure of other background jobs. This paper studies an optimal\ndivisible loads scheduling problem on a single level tree network, whose\nprocessing speeds and channel speeds are time-varying. Two recursive algorithms\nare provided to solve this problem when the arrival and departure times of the\nbackground jobs are known a priori and an iterative algorithm is provided to\nsolve the case where such times are not known. Numerical tests and evaluations\nare performed for these three algorithms under different numbers of background\njobs and processors.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 20:27:06 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Wu", "Fei", ""], ["Cao", "Yang", ""], ["Robertazzi", "Thomas", ""]]}, {"id": "1902.01899", "submitter": "Fei Wu", "authors": "Fei Wu, Yang Cao, Thomas Robertazzi", "title": "Reinforcement Learning for Optimal Load Distribution Sequencing in\n  Resource-Sharing System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Divisible Load Theory (DLT) is a powerful tool for modeling divisible load\nproblems in data-intensive systems. This paper studied an optimal divisible\nload distribution sequencing problem using a machine learning framework. The\nproblem is to decide the optimal sequence to distribute divisible load to\nprocessors in order to achieve minimum finishing time. The scheduling is\nperformed in a resource-sharing system where each physical processor is\nvirtualized to multiple virtual processors. A reinforcement learning method\ncalled Multi-armed bandit (MAB) is used for our problem. We first provide a\nnaive solution using the MAB algorithm and then several optimizations are\nperformed. Various numerical tests are conducted. Our algorithm shows an\nincreasing performance during the training progress and the global optimum will\nbe acheived when the sample size is large enough.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 20:34:34 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Wu", "Fei", ""], ["Cao", "Yang", ""], ["Robertazzi", "Thomas", ""]]}, {"id": "1902.01952", "submitter": "Yang Cao", "authors": "Yang Cao, Fei Wu and Thomas Robertazzi", "title": "Integrating Amdahl-like Laws and Divisible Load Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrating Amdahl's and Amdahl-like laws with Divisible Load Theory promises\nmathematical design methodology that will make possible the efficient design of\ntomorrow's systems.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 22:09:00 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Cao", "Yang", ""], ["Wu", "Fei", ""], ["Robertazzi", "Thomas", ""]]}, {"id": "1902.01981", "submitter": "Saurav Prakash", "authors": "Amirhossein Reisizadeh, Saurav Prakash, Ramtin Pedarsani, Amir Salman\n  Avestimehr", "title": "CodedReduce: A Fast and Robust Framework for Gradient Aggregation in\n  Distributed Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.IT cs.LG math.IT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the commonly used synchronous Gradient Descent paradigm for\nlarge-scale distributed learning, for which there has been a growing interest\nto develop efficient and robust gradient aggregation strategies that overcome\ntwo key system bottlenecks: communication bandwidth and stragglers' delays. In\nparticular, Ring-AllReduce (RAR) design has been proposed to avoid bandwidth\nbottleneck at any particular node by allowing each worker to only communicate\nwith its neighbors that are arranged in a logical ring. On the other hand,\nGradient Coding (GC) has been recently proposed to mitigate stragglers in a\nmaster-worker topology by allowing carefully designed redundant allocation of\nthe data set to the workers. We propose a joint communication topology design\nand data set allocation strategy, named CodedReduce (CR), that combines the\nbest of both RAR and GC. That is, it parallelizes the communications over a\ntree topology leading to efficient bandwidth utilization, and carefully designs\na redundant data set allocation and coding strategy at the nodes to make the\nproposed gradient aggregation scheme robust to stragglers. In particular, we\nquantify the communication parallelization gain and resiliency of the proposed\nCR scheme, and prove its optimality when the communication topology is a\nregular tree. Furthermore, we empirically evaluate the performance of our\nproposed CR design over Amazon EC2 and demonstrate that it achieves speedups of\nup to 27.2x and 7.0x, respectively over the benchmarks GC and RAR.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 00:05:21 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 06:06:19 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Reisizadeh", "Amirhossein", ""], ["Prakash", "Saurav", ""], ["Pedarsani", "Ramtin", ""], ["Avestimehr", "Amir Salman", ""]]}, {"id": "1902.01994", "submitter": "Yang Cao", "authors": "Yang Cao, Fei Wu and Thomas Robertazzi", "title": "Scheduling and Trade-off Analysis for Multi-Source Multi-Processor\n  Systems with Divisible Loads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main goal of parallel processing is to provide users with performance\nthat is much better than that of single processor systems. The execution of\njobs is scheduled, which requires certain resources in order to meet certain\ncriteria. Divisible load is a special but widely used type of data which can be\ndivided into arbitrary sizes and independently processed in parallel. It can be\nused in applications which are processing a great amount of similar data units.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 01:27:22 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Cao", "Yang", ""], ["Wu", "Fei", ""], ["Robertazzi", "Thomas", ""]]}, {"id": "1902.02104", "submitter": "Annalisa Massini", "authors": "Viviana Arrigoni, Annalisa Massini", "title": "Fast Strassen-based $A^t A$ Parallel Multiplication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix multiplication $A^t A$ appears as intermediate operation during the\nsolution of a wide set of problems. In this paper, we propose a new\ncache-oblivious algorithm for the $A^t A$ multiplication. Our algorithm,\nA$\\scriptstyle \\mathsf{T}$A, calls classical Strassen's algorithm as\nsub-routine, decreasing the computational cost %(expressed in number of\nperformed products) of the conventional $A^t A$ multiplication to\n$\\frac{2}{7}n^{\\log_2 7}$. It works for generic rectangular matrices and\nexploits the peculiar symmetry of the resulting product matrix for sparing\nmemory. We used the MPI paradigm to implement A$\\scriptstyle \\mathsf{T}$A in\nparallel, and we tested its performances on a small subset of nodes of the\nGalileo cluster. Experiments highlight good scalability and speed-up, also\nthanks to minimal number of exchanged messages in the designed communication\nsystem. Parallel overhead and inherently sequential time fraction are\nnegligible in the tested configurations.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 10:46:03 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Arrigoni", "Viviana", ""], ["Massini", "Annalisa", ""]]}, {"id": "1902.02156", "submitter": "Mouadh Ayachi", "authors": "Mouadh Ayachi", "title": "Etude de la Distribution de Calculs Creux sur une Grappe Multi-coeurs", "comments": "in French, Master's thesis, University Tunis El Manar (2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, high performance computing is becoming more and more important in\ndifferent fields research and industry, such as medical imaging and\ndiagnostics, mathematics as well as oil exploration. It refers to intensive\ncomputing in some applications where one needs to use a large number of\ncomputing resources (computing power, memory rate, storage space, etc.). Thus,\nit is necessary in this case to run these applications on architectures\nparallel making multiple computers work together and running over 10 operations\nat floating point per second (or a petaflops). 15 Parallel computation consists\nof executing one or more programs, simultaneously, by multiple processors. In\ngeneral, we have two ways to perform a parallel calculation. The first is to\ncut the program into several calculation tasks then, run all these parallel\nspots by different processors. The second requires partitioning the data, so\nthat each part of the data is assigned to a different processor. Then, all the\nprocessors run the instructions of the same program in parallel but by\noperating on different data . This last method, called parallelism of data, is\nthat retained in this memory. Digital analysis is one of the areas where the\nuse of platforms is essential parallel, in particular, in the case where\nnumerical methods treat hollow matrices. This last method, called parallelism\nof data, is that retained in this memory. Digital analysis is one of the areas\nwhere the use of platforms is essential parallel, in particular, in the case\nwhere numerical methods treat hollow matrices. A hollow matrix is a very large\nmatrix that contains a small proportion non-zero elements. These matrices can\ncome from different domains such as simulation structural mechanics, image /\nsignal processing, the study of the dynamics of fluids etc.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 17:51:20 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Ayachi", "Mouadh", ""]]}, {"id": "1902.02174", "submitter": "Ryosuke Abe", "authors": "Ryosuke Abe", "title": "Blockchain Storage Load Balancing Among DHT Clustered Nodes", "comments": "Master Thesis in Keio University Graduate School of Media and\n  Governance", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Bitcoin, to independently verify whether new transactions are correct or\nnot, a type of a node called \"Full Node\" has to hold the whole of historical\ntransactions. The transactions are stored in ledger called \"Blockchain. \"\nBlockchain is an append-only data structure. Thus, to operate Full Nodes, the\nrequired storage capacity would grow too large for resource-constrained\ndevices. Due to the limitation, the existing lightweight node scheme is that a\nnode relies on other Full Nodes. In this thesis, to reduce storage capacity\nwith keeping the independence of each node, we propose a storage load balancing\nscheme \"KARAKASA\" using Distributed Hash Table (DHT). In KARAKASA, nodes\ndistributedly keep the whole blockchain among DHT networked nodes. We evaluated\nKARAKASA from the view of storage capacity and independence. As a result, a\nnode in a cluster does not need to trust other nodes. We concluded that nodes\nin a DHT cluster can behave like Full Nodes without holding the whole\nblockchain.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 13:36:13 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Abe", "Ryosuke", ""]]}, {"id": "1902.02343", "submitter": "Anastasiia Butko", "authors": "Anastasiia Butko, Florent Bruguier, David Novo, Abdoulaye Gamati\\'e,\n  Gilles Sassatelli", "title": "Exploration of Performance and Energy Trade-offs for Heterogeneous\n  Multicore Architectures", "comments": "11 pages, 6 figure, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy-efficiency has become a major challenge in modern computer systems. To\naddress this challenge, candidate systems increasingly integrate heterogeneous\ncores in order to satisfy diverse computation requirements by selecting cores\nwith suitable features. In particular, single-ISA heterogeneous multicore\nprocessors such as ARM big.LITTLE have become very attractive since they offer\ngood opportunities in terms of performance and power consumption trade-off.\nWhile existing works already showed that this feature can improve system\nenergy-efficiency, further gains are possible by generalizing the principle to\nhigher levels of heterogeneity. The present paper aims to explore these gains\nby considering single-ISA heterogeneous multicore architectures including three\ndifferent types of cores. For this purpose, we use the Samsung Exynos Octa 5422\nchip as baseline architecture. Then, we model and evaluate Cortex A7, A9, and\nA15 cores using the gem5 simulation framework coupled to McPAT for power\nestimation. We demonstrate that varying the level of heterogeneity as well as\nthe different core ratio can lead to up to 2.3x gains in energy efficiency and\nup to 1.5x in performance. This study further provides insights on the impact\nof workload nature on performance/energy trade-off and draws recommendations\nconcerning suitable architecture configurations. This contributes in fine to\nguide future research towards dynamically reconfigurable HSAs in which some\ncores/clusters can be disabled momentarily so as to optimize certain metrics\nsuch as energy efficiency. This is of particular interest when dealing with\nquality-tunable algorithms in which accuracy can be then traded for compute\neffort, thereby enabling to use only those cores that provide the best\nenergy-efficiency for the chosen algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 18:59:57 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Butko", "Anastasiia", ""], ["Bruguier", "Florent", ""], ["Novo", "David", ""], ["Gamati\u00e9", "Abdoulaye", ""], ["Sassatelli", "Gilles", ""]]}, {"id": "1902.02411", "submitter": "Stanko Novakovic", "authors": "Stanko Novakovic, Yizhou Shan, Aasheesh Kolli, Michael Cui, Yiying\n  Zhang, Haggai Eran, Liran Liss, Michael Wei, Dan Tsafrir, Marcos Aguilera", "title": "Storm: a fast transactional dataplane for remote data structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RDMA is an exciting technology that enables a host to access the memory of a\nremote host without involving the remote CPU. Prior work shows how to use RDMA\nto improve the performance of distributed in-memory storage systems. However,\nRDMA is widely believed to have scalability issues, due to the amount of active\nprotocol state that needs to be cached in the limited NIC cache. These concerns\nled to several software-based proposals to enhance scalability by trading off\nperformance. In this work, we revisit these trade-offs in light of newer RDMA\nhardware and propose new guidelines for scaling RDMA. We show that using\none-sided remote memory primitives leads to higher performance compared to\nsend/receive and kernel-based systems in rack-scale environments. Based on\nthese insights, we design and implement Storm, a transactional dataplane using\none-sided read and write-based RPC primitives. We show that Storm outperforms\neRPC, FaRM, and LITE by 3.3x, 3.6x, and 17.1x, respectively, on an Infinband\nEDR cluster with Mellanox ConnectX-4 NICs.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 22:15:57 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Novakovic", "Stanko", ""], ["Shan", "Yizhou", ""], ["Kolli", "Aasheesh", ""], ["Cui", "Michael", ""], ["Zhang", "Yiying", ""], ["Eran", "Haggai", ""], ["Liss", "Liran", ""], ["Wei", "Michael", ""], ["Tsafrir", "Dan", ""], ["Aguilera", "Marcos", ""]]}, {"id": "1902.02469", "submitter": "Jincheng Du", "authors": "Mark Harvilla, Jincheng Du", "title": "Prospective Hybrid Consensus for Project PAI", "comments": "24 pages, 3 figures; A Japanese translation is available in previous\n  version V2 [arXiv:1902.02469v2]; A Korean translation is available in\n  previous version V3 [arXiv:1902.02469v3]; A Simplified Chinese translation is\n  available in previous version V4 [arXiv:1902.02469v4];", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PAI Coin's Proof-of-Work (PoW) consensus mechanism utilizes the double\nSHA-256 hashing protocol-- the same mechanism used by Bitcoin Core. This\ncompatibility with classic Bitcoin-style mining provides low barrier to entry\nfor PAI Coin mining, consequently rendering the PAI Coin network vulnerable to\nso-called 51% attacks, among others. To mitigate such risks, this paper\nproposes a hybrid Proof-of-Work, Proof-of-Stake (PoS) consensus mechanism and\nprovides a detailed technical analysis of how such a mechanism would counter\nsome of the PAI Coin network's inherent vulnerabilities, if successfully\nimplemented. A detailed technical outline of blockchain-based PoW & PoS\nconsensus, including their advantages and disadvantages, when used both\nindependently and in the context of the hybrid model, is provided. An economic\nanalysis of attacking a hybrid-powered PAI Coin network is presented, and a\nfinal recommendation for future development of PAI Coin consensus is made.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 04:18:45 GMT"}, {"version": "v2", "created": "Thu, 14 Feb 2019 22:35:08 GMT"}, {"version": "v3", "created": "Mon, 18 Feb 2019 19:56:16 GMT"}, {"version": "v4", "created": "Wed, 20 Feb 2019 17:51:42 GMT"}, {"version": "v5", "created": "Thu, 21 Feb 2019 01:42:34 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Harvilla", "Mark", ""], ["Du", "Jincheng", ""]]}, {"id": "1902.02470", "submitter": "Jincheng Du", "authors": "Jincheng Du, Dan Fang, Mark Harvilla", "title": "PAI Data, Summary of the Project PAI Data Protocol", "comments": "8 pages, 3 figures; A Japanese translation is available in previous\n  version V2 [arXiv:1902.02470v2 ]; A Korean translation is available in\n  previous version V3 [arXiv:1902.02470v3 ]; A Simplified Chinese translation\n  is available in previous version V4 [arXiv:1902.02470v4];", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Project PAI Data Protocol (\"PAI Data\") is a specification that extends\nthe Project PAI Blockchain Protocol to include a method of securing and\nprovisioning access to arbitrary data. In the context of PAI Coin Development\nProposal (PDP) 2, this paper defines two important transaction types that PAI\nData supports: Storage Transactions, which facilitate storage of data and proof\nof ownership, and Sharing Transactions, designed to enable granting and\nrevocation of data access to designated recipients. A comparative analysis of\nPAI Data against similar blockchain-based file storage systems is also\npresented.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 04:26:17 GMT"}, {"version": "v2", "created": "Sun, 24 Feb 2019 07:21:47 GMT"}, {"version": "v3", "created": "Tue, 26 Feb 2019 17:37:23 GMT"}, {"version": "v4", "created": "Wed, 27 Feb 2019 04:09:32 GMT"}, {"version": "v5", "created": "Thu, 28 Feb 2019 01:54:59 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Du", "Jincheng", ""], ["Fang", "Dan", ""], ["Harvilla", "Mark", ""]]}, {"id": "1902.02763", "submitter": "Alex Weaver", "authors": "Calvin Newport and Alex Weaver", "title": "Random Gossip Processes in Smartphone Peer-to-Peer Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study random gossip processes in communication models that\ndescribe the peer-to-peer networking functionality included in standard\nsmartphone operating systems. Random gossip processes spread information\nthrough the basic mechanism of randomly selecting neighbors for connections.\nThese processes are well-understood in standard peer-to-peer network models,\nbut little is known about their behavior in models that abstract the smartphone\npeer-to-peer setting. With this in mind, we begin by studying a simple random\ngossip process in the synchronous mobile telephone model (the most common\nabstraction used to study smartphone peer-to-peer systems). By introducing a\nnew analysis technique, we prove that this simple process is actually more\nefficient than the best-known gossip algorithm in the mobile telephone model,\nwhich required complicated coordination among the nodes in the network. We then\nintroduce a novel variation of the mobile telephone model that removes the\nsynchronized round assumption, shrinking the gap between theory and practice.\nWe prove that simple random gossip processes still converge in this setting and\nthat information spreading still improves along with graph connectivity. This\nnew model and the tools we introduce provide a solid foundation for the further\ntheoretical analysis of algorithms meant to be deployed on real smartphone\npeer-to-peer networks. More generally, our results in this paper imply that\nsimple random information spreading processes should be expected to perform\nwell in this emerging new peer-to-peer setting.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 18:42:34 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Newport", "Calvin", ""], ["Weaver", "Alex", ""]]}, {"id": "1902.02837", "submitter": "Pablo Abad", "authors": "Adrian Colaso, Pablo Prieto, Jose-Angel Herrero, Pablo Abad, Valentin\n  Puente, Jose-Angel Gregorio", "title": "Accuracy vs. Computational Cost Tradeoff in Distributed Computer System\n  Simulation", "comments": "10 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation is a fundamental research tool in the computer architecture field.\nThese kinds of tools enable the exploration and evaluation of architectural\nproposals capturing the most relevant aspects of the highly complex systems\nunder study. Many state-of-the-art simulation tools focus on single-system\nscenarios, but the scalability required by trending applications has shifted\ntowards distributed computing systems integrated via complex software stacks.\nWeb services with client-server architectures or distributed storage and\nprocessing of scale-out data analytics (Big Data) are among the main exponents.\nThe complete simulation of a distributed computer system is the appropriate\nmethodology to conduct accurate evaluations. Unfortunately, this methodology\ncould have a significant impact on the already large computational effort\nderived from detailed simulation. In this work, we conduct a set of experiments\nto evaluate this accuracy/cost tradeoff. We measure the error made if\nclient-server applications are evaluated in a single-node environment, as well\nas the overhead induced by the methodology and simulation tool employed for\nmulti-node simulations. We quantify this error for different micro-architecture\ncomponents, such as last-level cache and instruction/data TLB. Our findings\nshow that accuracy loss can lead to completely wrong conclusions about the\neffects of proposed hardware optimizations. Fortunately, our results also\ndemonstrate that the computational overhead of a multi-node simulation\nframework is affordable, suggesting multi-node simulation as the most\nappropriate methodology.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 08:14:32 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Colaso", "Adrian", ""], ["Prieto", "Pablo", ""], ["Herrero", "Jose-Angel", ""], ["Abad", "Pablo", ""], ["Puente", "Valentin", ""], ["Gregorio", "Jose-Angel", ""]]}, {"id": "1902.03039", "submitter": "Uthman Baroudi Dr", "authors": "Gamal Sallam and Uthman Baroudi", "title": "A Framework for Autonomous Robot Deployment with Perfect Demand\n  Satisfaction using Virtual Forces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, robots autonomous deployment is preferable and\nsometimes it is the only affordable solution. To address this issue, virtual\nforce (VF) is one of the prominent approaches to performing multirobot\ndeployment autonomously. However, most of the existing VF-based approaches\nconsider only a uniform deployment to maximize the covered area while ignoring\nthe criticality of specific locations during the deployment process. To\novercome these limitations, we present a framework for autonomously deploy\nrobots or vehicles using virtual force. The framework is composed of two\nstages. In the first stage, a two-hop Cooperative Virtual Force based Robots\nDeployment (Two-hop COVER) is employed where a cooperative relation between\nrobots and neighboring landmarks is established to satisfy mission\nrequirements. The second stage complements the first stage and ensures perfect\ndemand satisfaction by utilizing the Trace Fingerprint technique which\ncollected traces while each robot traversing the deployment area. Finally, a\nfairness-aware version of Two-hop COVER is presented to consider scenarios\nwhere the mission requirements are greater than the available resources (i.e.\nrobots). We evaluate our framework via extensive simulations. The results\ndemonstrate outstanding performance compared to contemporary approaches in\nterms of total travelled distance, total exchanged messages, total deployment\ntime, and Jain fairness index.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 12:03:16 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Sallam", "Gamal", ""], ["Baroudi", "Uthman", ""]]}, {"id": "1902.03154", "submitter": "Salvatore Di Girolamo", "authors": "Salvatore Di Girolamo, Pirmin Schmid, Thomas Schulthess, Torsten\n  Hoefler", "title": "SimFS: A Simulation Data Virtualizing File System Interface", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Nowadays simulations can produce petabytes of data to be stored in parallel\nfilesystems or large-scale databases. This data is accessed over the course of\ndecades often by thousands of analysts and scientists. However, storing these\nvolumes of data for long periods of time is not cost effective and, in some\ncases, practically impossible. We propose to transparently virtualize the\nsimulation data, relaxing the storage requirements by not storing the full\noutput and re-simulating the missing data on demand. We develop SimFS, a file\nsystem interface that exposes a virtualized view of the simulation output to\nthe analysis applications and manages the re-simulations. SimFS monitors the\naccess patterns of the analysis applications in order to (1) decide the data to\nkeep stored for faster accesses and (2) to employ prefetching strategies to\nreduce the access time of missing data. Virtualizing simulation data allows us\nto trade storage for computation: this paradigm becomes similar to traditional\non-disk analysis (all data is stored) or in situ (no data is stored) according\nwith the storage resources that are assigned to SimFS. Overall, by exploiting\nthe growing computing power and relaxing the storage capacity requirements,\nSimFS offers a viable path towards exa-scale simulations.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 11:05:34 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Di Girolamo", "Salvatore", ""], ["Schmid", "Pirmin", ""], ["Schulthess", "Thomas", ""], ["Hoefler", "Torsten", ""]]}, {"id": "1902.03159", "submitter": "Uthman Baroudi Dr", "authors": "Hesham Alfares, Abdulrahman Abu Elkhail, Uthman Baroudi", "title": "Iterative Clustering for Energy-Efficient Large-Scale Tracking Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new technique is presented to design energy-efficient large-scale tracking\nsystems based on mobile clustering. The new technique optimizes the formation\nof mobile clusters to minimize energy consumption in large-scale tracking\nsystems. This technique can be used in large public gatherings with high crowd\ndensity and continuous mobility. Utilizing both Bluetooth and Wi-Fi\ntechnologies in smart phones, the technique tracks the movement of individuals\nin a large crowd within a specific area, and monitors their current locations\nand health conditions. The new system has several advantages, including good\npositioning accuracy, low energy consumption, short transmission delay, and low\nsignal interference. Two types of interference are reduced: between Bluetooth\nand Wi-Fi signals, and between different Bluetooth signals. An integer linear\nprogramming model is developed to optimize the construction of clusters. In\naddition, a simulation model is constructed and used to test the new technique\nunder different conditions. The proposed clustering technique shows superior\nperformance according to several evaluation criteria.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 16:02:43 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Alfares", "Hesham", ""], ["Elkhail", "Abdulrahman Abu", ""], ["Baroudi", "Uthman", ""]]}, {"id": "1902.03162", "submitter": "Uthman Baroudi Dr", "authors": "Uthman Baroudi, Abdulrahman Abu Elkhail, Hesham Alfares", "title": "Optimum Bi-level Hierarchical Clustering for Wireless Mobile Tracking\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel technique is proposed to optimize energy efficiency for wireless\nnetworks based on hierarchical mobile clustering. The new bi-level clustering\ntechnique minimizes mutual interference and energy consumption in large-scale\ntracking systems used in large public gatherings such as festivals and sports\nevents. This technique tracks random movements of a large number people in a\nbounded area by using a combination of smart-phone Bluetooth and Wi-Fi\nconnections. It can be effectively used for monitoring health conditions of\ncrowd members and providing their locations and movement directions. An integer\nlinear programming (ILP) model of the problem is formulated to optimize the\nformation of clusters in a two-level hierarchical structure. In order to\nevaluate the proposed technique, it is compared to the optimum solutions\nobtained from the ILP model for both single-level and two-level clustering.\nMoreover, a Matlab/Simulink simulation model is developed and used to test the\ntechnique performance under realistic operating conditions. The results\ndemonstrate a very good performance of the proposed technique.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 16:08:38 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Baroudi", "Uthman", ""], ["Elkhail", "Abdulrahman Abu", ""], ["Alfares", "Hesham", ""]]}, {"id": "1902.03305", "submitter": "Hesam Nejati Sharif Aldin", "authors": "Hesam Nejati Sharif Aldin, Hossein Deldari, Mohammad Hossein Moattar,\n  Mostafa Razavi Ghods", "title": "Consistency models in distributed systems: A survey on definitions,\n  disciplines, challenges and applications", "comments": "52 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The replication mechanism resolves some challenges with big data such as data\ndurability, data access, and fault tolerance. Yet, replication itself gives\nbirth to another challenge known as the consistency in distributed systems.\nScalability and availability are the challenging criteria on which the\nreplication is based upon in distributed systems which themselves require the\nconsistency. Consistency in distributed computing systems has been employed in\nthree different applicable fields, such as system architecture, distributed\ndatabase, and distributed systems. Consistency models based on their\napplicability could be sorted from strong to weak. Our goal is to propose a\nnovel viewpoint to different consistency models utilized in the distributed\nsystems. This research proposes two different categories of consistency models.\nInitially, consistency models are categorized into three groups of\ndata-centric, client-centric and hybrid models. Each of which is then grouped\ninto three subcategories of traditional, extended, and novel consistency\nmodels. Consequently, the concepts and procedures are expressed in mathematical\nterms, which are introduced in order to present our models' behavior without\nimplementation. Moreover, we have surveyed different aspects of challenges with\nrespect to the consistency i.e., availability, scalability, security, fault\ntolerance, latency, violation, and staleness, out of which the two latter i.e.\nviolation and staleness, play the most pivotal roles in terms of consistency\nand trade-off balancing. Finally, the contribution extent of each of the\nconsistency models and the growing need for them in distributed systems are\ninvestigated.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 22:11:01 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Aldin", "Hesam Nejati Sharif", ""], ["Deldari", "Hossein", ""], ["Moattar", "Mohammad Hossein", ""], ["Ghods", "Mostafa Razavi", ""]]}, {"id": "1902.03317", "submitter": "Jiajia Li", "authors": "Jiajia Li and Yuchen Ma and Xiaolong Wu and Ang Li and Kevin Barker", "title": "PASTA: A Parallel Sparse Tensor Algorithm Benchmark Suite", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor methods have gained increasingly attention from various applications,\nincluding machine learning, quantum chemistry, healthcare analytics, social\nnetwork analysis, data mining, and signal processing, to name a few. Sparse\ntensors and their algorithms become critical to further improve the performance\nof these methods and enhance the interpretability of their output. This work\npresents a sparse tensor algorithm benchmark suite (PASTA) for single- and\nmulti-core CPUs. To the best of our knowledge, this is the first benchmark\nsuite for sparse tensor world. PASTA targets on: 1) helping application users\nto evaluate different computer systems using its representative computational\nworkloads; 2) providing insights to better utilize existed computer\narchitecture and systems and inspiration for the future design. This benchmark\nsuite is publicly released https://gitlab.com/tensorworld/pasta.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 22:51:55 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Li", "Jiajia", ""], ["Ma", "Yuchen", ""], ["Wu", "Xiaolong", ""], ["Li", "Ang", ""], ["Barker", "Kevin", ""]]}, {"id": "1902.03387", "submitter": "Hamzeh Khazaei", "authors": "Hamzeh Khazaei, Nima Mahmoudi, Cornel Barna and Marin Litoiu", "title": "Performance Modeling of Microservice Platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microservice architecture has transformed the way developers are building and\ndeploying applications in the nowadays cloud computing centers. This new\napproach provides increased scalability, flexibility, manageability, and\nperformance while reducing the complexity of the whole software development\nlife cycle. The increase in cloud resource utilization also benefits\nmicroservice providers. Various microservice platforms have emerged to\nfacilitate the DevOps of containerized services by enabling continuous\nintegration and delivery. Microservice platforms deploy application containers\non virtual or physical machines provided by public/private cloud\ninfrastructures in a seamless manner. In this paper, we study and evaluate the\nprovisioning performance of microservice platforms by incorporating the details\nof all layers (i.e., both micro and macro layers) in the modelling process. To\nthis end, we first build a microservice platform on top of Amazon EC2 cloud and\nthen leverage it to develop a comprehensive performance model to perform\nwhat-if analysis and capacity planning for microservice platforms at scale. In\nother words, the proposed performance model provides a systematic approach to\nmeasure the elasticity of the microservice platform by analyzing the\nprovisioning performance at both the microservice platform and the back-end\nmacroservice infrastructures.\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2019 07:48:34 GMT"}, {"version": "v2", "created": "Sat, 3 Oct 2020 19:36:14 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Khazaei", "Hamzeh", ""], ["Mahmoudi", "Nima", ""], ["Barna", "Cornel", ""], ["Litoiu", "Marin", ""]]}, {"id": "1902.03522", "submitter": "Dmitrii Avdiukhin", "authors": "Dmitrii Avdiukhin, Sergey Pupyrev, Grigory Yaroslavtsev", "title": "Multi-Dimensional Balanced Graph Partitioning via Projected Gradient\n  Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by performance optimization of large-scale graph processing systems\nthat distribute the graph across multiple machines, we consider the balanced\ngraph partitioning problem. Compared to the previous work, we study the\nmulti-dimensional variant when balance according to multiple weight functions\nis required. As we demonstrate by experimental evaluation, such\nmulti-dimensional balance is important for achieving performance improvements\nfor typical distributed graph processing workloads. We propose a new scalable\ntechnique for the multidimensional balanced graph partitioning problem. The\nmethod is based on applying randomized projected gradient descent to a\nnon-convex continuous relaxation of the objective. We show how to implement the\nnew algorithm efficiently in both theory and practice utilizing various\napproaches for projection. Experiments with large-scale social networks\ncontaining up to hundreds of billions of edges indicate that our algorithm has\nsuperior performance compared with the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sun, 10 Feb 2019 00:23:16 GMT"}, {"version": "v2", "created": "Sat, 16 Feb 2019 00:25:12 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Avdiukhin", "Dmitrii", ""], ["Pupyrev", "Sergey", ""], ["Yaroslavtsev", "Grigory", ""]]}, {"id": "1902.03533", "submitter": "Wenxi Zeng", "authors": "Shuai Zhang, Wenxi Zeng, I-Ling Yen, Farokh B. Bastani", "title": "Semantically Enhanced Time Series Databases in IoT-Edge-Cloud\n  Infrastructure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many IoT systems are data intensive and are for the purpose of monitoring for\nfault detection and diagnosis of critical systems. A large volume of data\nsteadily come out of a large number of sensors in the monitoring system. Thus,\nwe need to consider how to store and manage these data. Existing time series\ndatabases (TSDBs) can be used for monitoring data storage, but they do not have\ngood models for describing the data streams stored in the database. In this\npaper, we develop a semantic model for the specification of the monitoring data\nstreams (time series data) in terms of which sensor generated the data stream,\nwhich metric of which entity the sensor is monitoring, what is the relation of\nthe entity to other entities in the system, which measurement unit is used for\nthe data stream, etc. We have also developed a tool suite, SE-TSDB, that can\nrun on top of existing TSDBs to help establish semantic specifications for data\nstreams and enable semantic-based data retrievals. With our semantic model for\nmonitoring data and our SE-TSDB tool suite, users can retrieve non-existing\ndata streams that can be automatically derived from the semantics. Users can\nalso retrieve data streams without knowing where they are. Semantic based\nretrieval is especially important in a large-scale integrated IoT-Edge-Cloud\nsystem, because of its sheer quantity of data, its huge number of computing and\nIoT devices that may store the data, and the dynamics in data migration and\nevolution. With better data semantics, data streams can be more effectively\ntracked and flexibly retrieved to help with timely data analysis and control\ndecision making anywhere and anytime.\n", "versions": [{"version": "v1", "created": "Sun, 10 Feb 2019 04:09:08 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Zhang", "Shuai", ""], ["Zeng", "Wenxi", ""], ["Yen", "I-Ling", ""], ["Bastani", "Farokh B.", ""]]}, {"id": "1902.03656", "submitter": "Blesson Varghese", "authors": "Blesson Varghese and Philipp Leitner and Suprio Ray and Kyle Chard and\n  Adam Barker and Yehia Elkhatib and Herry Herry and Cheol-Ho Hong and Jeremy\n  Singer and Fung Po Tso and Eiko Yoneki and Mohamed-Faten Zhani", "title": "Cloud Futurology", "comments": "Accepted to IEEE Computer, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Cloud has become integral to most Internet-based applications and user\ngadgets. This article provides a brief history of the Cloud and presents a\nresearcher's view of the prospects for innovating at the infrastructure,\nmiddleware, and application and delivery levels of the already crowded Cloud\ncomputing stack.\n", "versions": [{"version": "v1", "created": "Sun, 10 Feb 2019 19:37:24 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Varghese", "Blesson", ""], ["Leitner", "Philipp", ""], ["Ray", "Suprio", ""], ["Chard", "Kyle", ""], ["Barker", "Adam", ""], ["Elkhatib", "Yehia", ""], ["Herry", "Herry", ""], ["Hong", "Cheol-Ho", ""], ["Singer", "Jeremy", ""], ["Tso", "Fung Po", ""], ["Yoneki", "Eiko", ""], ["Zhani", "Mohamed-Faten", ""]]}, {"id": "1902.03700", "submitter": "Peng Peng", "authors": "Peng Peng, Lei Zou, Runyu Guan", "title": "Accelerating Partial Evaluation in Distributed SPARQL Query Evaluation", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partial evaluation has recently been used for processing SPARQL queries over\na large resource description framework (RDF) graph in a distributed\nenvironment. However, the previous approach is inefficient when dealing with\ncomplex queries. In this study, we further improve the \"partial evaluation and\nassembly\" framework for answering SPARQL queries over a distributed RDF graph,\nwhile providing performance guarantees. Our key idea is to explore the\nintrinsic structural characteristics of partial matches to filter out\nirrelevant partial results, while providing performance guarantees on a network\ntrace (data shipment) or the computational cost (response time). We also\npropose an efficient assembly algorithm to utilize the characteristics of\npartial matches to merge them and form final results. To improve the efficiency\nof finding partial matches further, we propose an optimization that\ncommunicates variables' candidates among sites to avoid redundant computations.\nIn addition, although our approach is partitioning-tolerant, different\npartitioning strategies result in different performances, and we evaluate\ndifferent partitioning strategies for our approach. Experiments over both real\nand synthetic RDF datasets confirm the superiority of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 01:40:51 GMT"}, {"version": "v2", "created": "Thu, 14 Feb 2019 17:00:56 GMT"}, {"version": "v3", "created": "Fri, 15 Feb 2019 14:04:44 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Peng", "Peng", ""], ["Zou", "Lei", ""], ["Guan", "Runyu", ""]]}, {"id": "1902.03833", "submitter": "Ga\\\"el Beck", "authors": "Ga\\\"el Beck, Tarn Duong, Mustapha Lebbah, Hanane Azzag, Christophe\n  C\\'erin", "title": "A Distributed and Approximated Nearest Neighbors Algorithm for an\n  Efficient Large Scale Mean Shift Clustering", "comments": "Algorithms are available at\n  https://github.com/Clustering4Ever/Clustering4Ever", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we target the class of modal clustering methods where clusters\nare defined in terms of the local modes of the probability density function\nwhich generates the data. The most well-known modal clustering method is the\nk-means clustering. Mean Shift clustering is a generalization of the k-means\nclustering which computes arbitrarily shaped clusters as defined as the basins\nof attraction to the local modes created by the density gradient ascent paths.\nDespite its potential, the Mean Shift approach is a computationally expensive\nmethod for unsupervised learning. Thus, we introduce two contributions aiming\nto provide clustering algorithms with a linear time complexity, as opposed to\nthe quadratic time complexity for the exact Mean Shift clustering. Firstly we\npropose a scalable procedure to approximate the density gradient ascent.\nSecond, our proposed scalable cluster labeling technique is presented. Both\npropositions are based on Locality Sensitive Hashing (LSH) to approximate\nnearest neighbors. These two techniques may be used for moderate sized\ndatasets. Furthermore, we show that using our proposed approximations of the\ndensity gradient ascent as a pre-processing step in other clustering methods\ncan also improve dedicated classification metrics. For the latter, a\ndistributed implementation, written for the Spark/Scala ecosystem is proposed.\nFor all these considered clustering methods, we present experimental results\nillustrating their labeling accuracy and their potential to solve concrete\nproblems.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 12:00:06 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Beck", "Ga\u00ebl", ""], ["Duong", "Tarn", ""], ["Lebbah", "Mustapha", ""], ["Azzag", "Hanane", ""], ["C\u00e9rin", "Christophe", ""]]}, {"id": "1902.03868", "submitter": "Martin Westerkamp", "authors": "Martin Westerkamp", "title": "Verifiable Smart Contract Portability", "comments": "9 pages, 4 figures, to be published in Proceedings of IEEE\n  International Conference on Blockchain and Cryptocurrency, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of blockchain technologies, the idea of decentralized\napplications has gained traction. Smart contracts permit the implementation of\napplication logic to foster distributed systems that are capable of removing\nintermediaries. Hereby, lock in effects originating from isolated data storage\nand central authorities are mitigated. Yet, smart contracts deployed to a\nledger generate dependencies on the underlying blockchain. Over time,\nrequirements regarding contract execution may detach from the utilized chain\ndue to contradicting incentives and security or performance issues. To avoid a\nnovel form of lock in effect towards a host blockchain, we introduce a concept\nfor smart contract portability that permits any user to migrate contract logic\nand state between blockchains in a flexible and verifiable manner. As the\nEthereum Virtual Machine (EVM) is supported by a multitude of blockchain\nimplementations, it poses a common execution environment for smart contracts.\nWe provide a toolbox that facilitates smart contract portability between\nEVM-compatible blockchains without trust requirements in the entity executing\nthe migration process. To prove the concept's soundness, we transfer token\ncontracts based on the ERC20 standard as well as applications containing\ndependencies to other smart contracts. Our evaluation shows the validity of\nported applications including their current states.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 13:34:47 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Westerkamp", "Martin", ""]]}, {"id": "1902.03899", "submitter": "Alexander Spiegelman", "authors": "Guy Goren and Alexander Spiegelman", "title": "Mind the Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we revisit the mining strategies in proof of work based\ncryptocurrencies and propose two strategies, we call smart and smarter mining,\nthat in many cases strictly dominate honest mining. In contrast to other known\nattacks, like selfish mining, which induce zero-sum games among the miners, the\nstrategies proposed in this paper increase miners' profit by reducing their\nvariable costs (i.e., electricity). Moreover, the proposed strategies are\nviable for much smaller miners than previously known attacks, and surprisingly,\nan attack performed by one miner is profitable for all other miners as well.\n  While saving electricity power is very encouraging for the environment, it is\nless so for the coin's security. The smart/smarter strategies expose the coin\nto under 50\\% attacks and this vulnerability might only grow when new miners\njoin the coin as a response to the increase in profit margins induced by these\nstrategies.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 14:38:04 GMT"}, {"version": "v2", "created": "Tue, 12 Feb 2019 10:22:31 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Goren", "Guy", ""], ["Spiegelman", "Alexander", ""]]}, {"id": "1902.03912", "submitter": "Taeho Jung", "authors": "Changhao Chenli, Boyang Li, Yiyu Shi, Taeho Jung", "title": "Energy-recycling Blockchain with Proof-of-Deep-Learning", "comments": "5 pages", "journal-ref": null, "doi": "10.1109/BLOC.2019.8751419", "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An enormous amount of energy is wasted in Proofof-Work (PoW) mechanisms\nadopted by popular blockchain applications (e.g., PoW-based cryptocurrencies),\nbecause miners must conduct a large amount of computation. Owing to this, one\nserious rising concern is that the energy waste not only dilutes the value of\nthe blockchain but also hinders its further application. In this paper, we\npropose a novel blockchain design that fully recycles the energy required for\nfacilitating and maintaining it, which is re-invested to the computation of\ndeep learning. We realize this by proposing Proof-of-Deep-Learning (PoDL) such\nthat a valid proof for a new block can be generated if and only if a proper\ndeep learning model is produced. We present a proof-of-concept design of PoDL\nthat is compatible with the majority of the cryptocurrencies that are based on\nhash-based PoW mechanisms. Our benchmark and simulation results show that the\nproposed design is feasible for various popular cryptocurrencies such as\nBitcoin, Bitcoin Cash, and Litecoin.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 14:47:47 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Chenli", "Changhao", ""], ["Li", "Boyang", ""], ["Shi", "Yiyu", ""], ["Jung", "Taeho", ""]]}, {"id": "1902.04002", "submitter": "Philipp Woelfel", "authors": "George Giakkoupis and Philipp Woelfel", "title": "Efficient Randomized Test-And-Set Implementations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study randomized test-and-set (TAS) implementations from registers in the\nasynchronous shared memory model with n processes. We introduce the problem of\ngroup election, a natural variant of leader election, and propose a framework\nfor the implementation of TAS objects from group election objects. We then\npresent two group election algorithms, each yielding an efficient TAS\nimplementation. The first implementation has expected max-step complexity\n$O(\\log^\\ast k)$ in the location-oblivious adversary model, and the second has\nexpected max-step complexity $O(\\log\\log k)$ against any read/write-oblivious\nadversary, where $k\\leq n$ is the contention. These algorithms improve the\nprevious upper bound by Alistarh and Aspnes [2] of $O(\\log\\log n)$ expected\nmax-step complexity in the oblivious adversary model. We also propose a\nmodification to a TAS algorithm by Alistarh, Attiya, Gilbert, Giurgiu, and\nGuerraoui [5] for the strong adaptive adversary, which improves its space\ncomplexity from super-linear to linear, while maintaining its $O(\\log n)$\nexpected max-step complexity. We then describe how this algorithm can be\ncombined with any randomized TAS algorithm that has expected max-step\ncomplexity $T(n)$ in a weaker adversary model, so that the resulting algorithm\nhas $O(\\log n)$ expected max-step complexity against any strong adaptive\nadversary and $O(T(n))$ in the weaker adversary model. Finally, we prove that\nfor any randomized 2-process TAS algorithm, there exists a schedule determined\nby an oblivious adversary such that with probability at least $(1/4)^t$ one of\nthe processes needs at least t steps to finish its TAS operation. This\ncomplements a lower bound by Attiya and Censor-Hillel [7] on a similar problem\nfor $n\\geq 3$ processes.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 17:07:40 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Giakkoupis", "George", ""], ["Woelfel", "Philipp", ""]]}, {"id": "1902.04362", "submitter": "Li Lin", "authors": "Li Lin, Peng Li, Jinbo Xiong, Mingwei Lin", "title": "Distributed and Application-aware Task Scheduling in Edge-clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge computing is an emerging technology which places computing at the edge\nof the network to provide an ultra-low latency. Computation offloading, a\nparadigm that migrates computing from mobile devices to remote servers, can now\nuse the power of edge computing by offloading computation to cloudlets in\nedge-clouds. However, the task scheduling of computation offloading in\nedge-clouds faces a two-fold challenge. First, as cloudlets are geographically\ndistributed, it is difficult for each cloudlet to perform load balancing\nwithout centralized control. Second, as tasks of computation offloading have a\nwide variety of types, to guarantee the user quality of experience (QoE) in\nterms of task types is challenging. In this paper, we present Petrel, a\ndistributed and application-aware task scheduling framework for edge-clouds.\nPetrel implements a sample-based load balancing technology and further adopts\nadaptive scheduling policies according to task types. This application-aware\nscheduling not only provides QoE guarantee but also improves the overall\nscheduling performance. Trace-driven simulations show that Petrel achieves a\nsignificant improvement over existing scheduling strategies.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 12:47:21 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Lin", "Li", ""], ["Li", "Peng", ""], ["Xiong", "Jinbo", ""], ["Lin", "Mingwei", ""]]}, {"id": "1902.04363", "submitter": "Antoine Durand", "authors": "Antoine Durand, Elyes Ben-Hamida, David Leporini, G\\'erard Memmi", "title": "Asymptotic Performance Analysis of Blockchain Protocols", "comments": "16 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the light of the recent fame of Blockchain technologies, numerous\nproposals and projects aiming at better practical viability have emerged.\nHowever, formally assessing their particularities and benefits has proven to be\na difficult task. The aim of this work is to compare the fundamental\ndifferences of such protocols to understand how they lead to different\npractical performances. To reach this goal, we undertake a complexity analysis\nof a wide range of prominent distributed algorithms proposed for blockchain\nsystems, under the lens of Total Order Broadcast protocols. We sampled\nprotocols designed for very different settings and that use a broad range of\ntechniques, thus giving a good overview of the achievements of state-of-the-art\ntechniques. By analyzing latency and network usage, we are able to discuss each\nprotocol's characteristics and properties in a consistent manner. One corollary\nresult to our work is a more robust criteria to classify protocols as\npermissioned or permissionless.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 12:51:53 GMT"}, {"version": "v2", "created": "Tue, 11 Jun 2019 21:18:05 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Durand", "Antoine", ""], ["Ben-Hamida", "Elyes", ""], ["Leporini", "David", ""], ["Memmi", "G\u00e9rard", ""]]}, {"id": "1902.04413", "submitter": "Do Le Quoc", "authors": "Roland Kunkel and Do Le Quoc and Franz Gregor and Sergei Arnautov and\n  Pramod Bhatotia and Christof Fetzer", "title": "TensorSCONE: A Secure TensorFlow Framework using Intel SGX", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning has become a critical component of modern data-driven online\nservices. Typically, the training phase of machine learning techniques requires\nto process large-scale datasets which may contain private and sensitive\ninformation of customers. This imposes significant security risks since modern\nonline services rely on cloud computing to store and process the sensitive\ndata. In the untrusted computing infrastructure, security is becoming a\nparamount concern since the customers need to trust the thirdparty cloud\nprovider. Unfortunately, this trust has been violated multiple times in the\npast. To overcome the potential security risks in the cloud, we answer the\nfollowing research question: how to enable secure executions of machine\nlearning computations in the untrusted infrastructure? To achieve this goal, we\npropose a hardware-assisted approach based on Trusted Execution Environments\n(TEEs), specifically Intel SGX, to enable secure execution of the machine\nlearning computations over the private and sensitive datasets. More\nspecifically, we propose a generic and secure machine learning framework based\non Tensorflow, which enables secure execution of existing applications on the\ncommodity untrusted infrastructure. In particular, we have built our system\ncalled TensorSCONE from ground-up by integrating TensorFlow with SCONE, a\nshielded execution framework based on Intel SGX. The main challenge of this\nwork is to overcome the architectural limitations of Intel SGX in the context\nof building a secure TensorFlow system. Our evaluation shows that we achieve\nreasonable performance overheads while providing strong security properties\nwith low TCB.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 14:48:25 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Kunkel", "Roland", ""], ["Quoc", "Do Le", ""], ["Gregor", "Franz", ""], ["Arnautov", "Sergei", ""], ["Bhatotia", "Pramod", ""], ["Fetzer", "Christof", ""]]}, {"id": "1902.04446", "submitter": "Jaroslaw Zola", "authors": "Frank Schoeneman and Jaroslaw Zola", "title": "Solving All-Pairs Shortest-Paths Problem in Large Graphs Using Apache\n  Spark", "comments": null, "journal-ref": null, "doi": "10.1145/3337821.3337852", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms for computing All-Pairs Shortest-Paths (APSP) are critical\nbuilding blocks underlying many practical applications. The standard sequential\nalgorithms, such as Floyd-Warshall and Johnson, quickly become infeasible for\nlarge input graphs, necessitating parallel approaches. In this work, we provide\ndetailed analysis of parallel APSP performance on distributed memory clusters\nwith Apache Spark. The Spark model allows for a portable and easy to deploy\ndistributed implementation, and hence is attractive from the end-user point of\nview. We propose four different APSP implementations for large undirected\nweighted graphs, which differ in complexity and degree of reliance on\ntechniques outside of pure Spark API. We demonstrate that Spark is able to\nhandle APSP problems with over 200,000 vertices on a 1024-core cluster, and can\ncompete with a naive MPI-based solution. However, our best performing solver\nrequires auxiliary shared persistent storage, and is over two times slower than\noptimized MPI-based solver.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 15:38:01 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2019 12:19:53 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Schoeneman", "Frank", ""], ["Zola", "Jaroslaw", ""]]}, {"id": "1902.04471", "submitter": "Mahdi Miraz", "authors": "Mahdi H. Miraz and David C. Donald", "title": "Atomic Cross-chain Swaps: Development, Trajectory and Potential of\n  Non-monetary Digital Token Swap Facilities", "comments": null, "journal-ref": "Annals of Emerging Technologies in Computing (AETiC), Print ISSN:\n  2516-0281, Online ISSN: 2516-029X, pp. 42-50, Vol. 3, No. 1, 1st January\n  2019, Published by International Association of Educators and Researchers\n  (IAER)", "doi": "10.33166/AETiC.2019.01.005", "report-no": null, "categories": "cs.CR cs.DC cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Since the introduction of Bitcoin in 2008, many other cryptocurrencies have\nbeen introduced and gained popularity. Lack of interoperability and scalability\namongst these cryptocurrencies was and still is, acting as a significant\nimpediment to the general adoption of cryptocurrencies and coloured tokens.\nAtomic Swaps, a smart exchange protocol for cryptocurrencies, is designed to\nfacilitate a wallet-to-wallet transfer enabling direct trades amongst different\ncryptocurrencies. Since swaps between cryptocurrencies are still relatively\nunknown, this article will investigate the operation and market development\nthus far and query the advantages they offer and the future challenges they\nface. The paper contains detailed literature and technology reviews, followed\nby the main analysis and findings.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jan 2019 14:39:13 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Miraz", "Mahdi H.", ""], ["Donald", "David C.", ""]]}, {"id": "1902.04491", "submitter": "Kashif Sharif", "authors": "Liehuang Zhu, Md Monjurul Karim, Kashif Sharif, Fan Li, Xiaojiang Du,\n  Mohsen Guizani", "title": "SDN Controllers: Benchmarking & Performance Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software Defined Networks offer flexible and intelligent network operations\nby splitting a traditional network into a centralized control plane and a\nprogrammable data plane. The intelligent control plane is responsible for\nproviding flow paths to switches and optimizes network performance. The\ncontroller in the control plane is the fundamental element used for all\noperations of data plane management. Hence, the performance and capabilities of\nthe controller itself are extremely important. Furthermore, the tools used to\nbenchmark their performance must be accurate and effective in measuring\ndifferent evaluation parameters. There are dozens of controller proposals\navailable in existing literature. However, there is no quantitative comparative\nanalysis for them. In this article, we present a comprehensive qualitative\ncomparison of different SDN controllers, along with a quantitative analysis of\ntheir performance in different network scenarios. More specifically, we\ncategorize and classify 34 controllers based on their capabilities, and present\na qualitative comparison of their properties. We also discuss in-depth\ncapabilities of benchmarking tools used for SDN controllers, along with best\npractices for quantitative controller evaluation. This work uses three\nbenchmarking tools to compare nine controllers against multiple criteria.\nFinally, we discuss detailed research findings on the performance, benchmarking\ncriteria, and evaluation testbeds for SDN controllers.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 16:54:09 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Zhu", "Liehuang", ""], ["Karim", "Md Monjurul", ""], ["Sharif", "Kashif", ""], ["Li", "Fan", ""], ["Du", "Xiaojiang", ""], ["Guizani", "Mohsen", ""]]}, {"id": "1902.04610", "submitter": "Peifeng Yu", "authors": "Peifeng Yu and Mosharaf Chowdhury", "title": "Salus: Fine-Grained GPU Sharing Primitives for Deep Learning\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GPU computing is becoming increasingly more popular with the proliferation of\ndeep learning (DL) applications. However, unlike traditional resources such as\nCPU or the network, modern GPUs do not natively support fine-grained sharing\nprimitives. Consequently, implementing common policies such as time sharing and\npreemption are expensive. Worse, when a DL application cannot completely use a\nGPU's resources, the GPU cannot be efficiently shared between multiple\napplications, leading to GPU underutilization.\n  We present Salus to enable two GPU sharing primitives: fast job switching and\nmemory sharing, in order to achieve fine-grained GPU sharing among multiple DL\napplications. Salus implements an efficient, consolidated execution service\nthat exposes the GPU to different DL applications, and enforces fine-grained\nsharing by performing iteration scheduling and addressing associated memory\nmanagement issues. We show that these primitives can then be used to implement\nflexible sharing policies such as fairness, prioritization, and packing for\nvarious use cases. Our integration of Salus with TensorFlow and evaluation on\npopular DL jobs show that Salus can improve the average completion time of DL\ntraining jobs by $3.19\\times$, GPU utilization for hyper-parameter tuning by\n$2.38\\times$, and GPU utilization of DL inference applications by $42\\times$\nover not sharing the GPU and $7\\times$ over NVIDIA MPS with small overhead.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 19:56:55 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Yu", "Peifeng", ""], ["Chowdhury", "Mosharaf", ""]]}, {"id": "1902.04774", "submitter": "Guodong Shi", "authors": "Deming Yuan, Alexandre Proutiere, Guodong Shi", "title": "Distributed Online Linear Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study online linear regression problems in a distributed setting, where\nthe data is spread over a network. In each round, each network node proposes a\nlinear predictor, with the objective of fitting the \\emph{network-wide} data.\nIt then updates its predictor for the next round according to the received\nlocal feedback and information received from neighboring nodes. The predictions\nmade at a given node are assessed through the notion of regret, defined as the\ndifference between their cumulative network-wide square errors and those of the\nbest off-line network-wide linear predictor. Various scenarios are\ninvestigated, depending on the nature of the local feedback (full information\nor bandit feedback), on the set of available predictors (the decision set), and\nthe way data is generated (by an oblivious or adaptive adversary). We propose\nsimple and natural distributed regression algorithms, involving, at each node\nand in each round, a local gradient descent step and a communication and\naveraging step where nodes aim at aligning their predictors to those of their\nneighbors. We establish regret upper bounds typically in ${\\cal O}(T^{3/4})$\nwhen the decision set is unbounded and in ${\\cal O}(\\sqrt{T})$ in case of\nbounded decision set.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 07:37:03 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Yuan", "Deming", ""], ["Proutiere", "Alexandre", ""], ["Shi", "Guodong", ""]]}, {"id": "1902.04789", "submitter": "Klaus-Dieter Schewe", "authors": "Klaus-Dieter Schewe, Andreas Prinz, Egon B\\\"orger", "title": "Concurrent Computing with Shared Replicated Memory", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The behavioural theory of concurrent systems states that any concurrent\nsystem can be captured by a behaviourally equivalent concurrent Abstract State\nMachine (cASM). While the theory in general assumes shared locations, it\nremains valid, if different agents can only interact via messages, i.e. sharing\nis restricted to mailboxes. There may even be a strict separation between\nmemory managing agents and other agents that can only access the shared memory\nby sending query and update requests to the memory agents. This article is\ndedicated to an investigation of replicated data that is maintained by a memory\nmanagement subsystem, whereas the replication neither appears in the requests\nnor in the corresponding answers. We show how the behaviour of a concurrent\nsystem with such a memory management can be specified using concurrent\ncommunicating ASMs. We provide several refinements of a high-level ground model\naddressing different replication policies and internal messaging between data\ncentres. For all these refinements we analyse their effects on the runs such\nthat decisions concerning the degree of consistency can be consciously made.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 08:53:40 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Schewe", "Klaus-Dieter", ""], ["Prinz", "Andreas", ""], ["B\u00f6rger", "Egon", ""]]}, {"id": "1902.04805", "submitter": "Charles Gueunet", "authors": "Charles Gueunet (LIP6), P. Fortin (LLR), J Jomier, J Tierny", "title": "Task-based Augmented Contour Trees with Fibonacci Heaps", "comments": null, "journal-ref": "IEEE Transactions on Parallel and Distributed Systems, Institute\n  of Electrical and Electronics Engineers, In press", "doi": null, "report-no": null, "categories": "cs.DC cs.CG cs.DM cs.DS cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new algorithm for the fast, shared memory, multi-core\ncomputation of augmented contour trees on triangulations. In contrast to most\nexisting parallel algorithms our technique computes augmented trees, enabling\nthe full extent of contour tree based applications including data segmentation.\nOur approach completely revisits the traditional, sequential contour tree\nalgorithm to re-formulate all the steps of the computation as a set of\nindependent local tasks. This includes a new computation procedure based on\nFibonacci heaps for the join and split trees, two intermediate data structures\nused to compute the contour tree, whose constructions are efficiently carried\nout concurrently thanks to the dynamic scheduling of task parallelism. We also\nintroduce a new parallel algorithm for the combination of these two trees into\nthe output global contour tree. Overall, this results in superior time\nperformance in practice, both in sequential and in parallel thanks to the\nOpenMP task runtime. We report performance numbers that compare our approach to\nreference sequential and multi-threaded implementations for the computation of\naugmented merge and contour trees. These experiments demonstrate the run-time\nefficiency of our approach and its scalability on common workstations. We\ndemonstrate the utility of our approach in data segmentation applications.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 09:31:03 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Gueunet", "Charles", "", "LIP6"], ["Fortin", "P.", "", "LLR"], ["Jomier", "J", ""], ["Tierny", "J", ""]]}, {"id": "1902.04820", "submitter": "Nicolas Holliman Professor", "authors": "Nicolas S. Holliman, Manu Antony, James Charlton, Stephen Dowsland,\n  Philip James and Mark Turner", "title": "Petascale Cloud Supercomputing for Terapixel Visualization of a Digital\n  Twin", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Photo-realistic terapixel visualization is computationally\nintensive and to date there have been no such visualizations of urban digital\ntwins, the few terapixel visualizations that exist have looked towards space\nrather than earth. Objective: our aims are: creating a scalable cloud\nsupercomputer software architecture for visualization; a photo-realistic\nterapixel 3D visualization of urban IoT data supporting daily updates; a\nrigorous evaluation of cloud supercomputing for our application. Method: we\nmigrated the Blender Cycles path tracer to the public cloud within a new\nsoftware framework designed to scale to petaFLOP performance. Results: we\ndemonstrate we can compute a terapixel visualization in under one hour, the\nsystem scaling at 98% efficiency to use 1024 public cloud GPU nodes delivering\n14 petaFLOPS. The resulting terapixel image supports interactive browsing of\nthe city and its data at a wide range of sensing scales. Conclusion: The GPU\ncompute resource available in the cloud is greater than anything available on\nour national supercomputers providing access to globally competitive resources.\nThe direct financial cost of access, compared to procuring and running these\nsystems, was low. The indirect cost, in overcoming teething issues with cloud\nsoftware development, should reduce significantly over time.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 09:54:14 GMT"}, {"version": "v2", "created": "Tue, 12 Mar 2019 11:35:02 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Holliman", "Nicolas S.", ""], ["Antony", "Manu", ""], ["Charlton", "James", ""], ["Dowsland", "Stephen", ""], ["James", "Philip", ""], ["Turner", "Mark", ""]]}, {"id": "1902.04899", "submitter": "Louis Esperet", "authors": "\\'Etienne Bamas and Louis Esperet", "title": "Local approximation of the Maximum Cut in regular graphs", "comments": "19 pages, 6 figures - Full version of a paper accepted in the 45th\n  International Workshop on Graph-Theoretic Concepts in Computer Science (WG\n  2019)", "journal-ref": "Theoretical Computer Science 820 (2020), 45-59", "doi": "10.1016/j.tcs.2020.03.008", "report-no": null, "categories": "math.CO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is devoted to the distributed complexity of finding an\napproximation of the maximum cut in graphs. A classical algorithm consists in\nletting each vertex choose its side of the cut uniformly at random. This does\nnot require any communication and achieves an approximation ratio of at least\n$\\tfrac12$ in average. When the graph is $d$-regular and triangle-free, a\nslightly better approximation ratio can be achieved with a randomized algorithm\nrunning in a single round. Here, we investigate the round complexity of\ndeterministic distributed algorithms for MAXCUT in regular graphs. We first\nprove that if $G$ is $d$-regular, with $d$ even and fixed, no deterministic\nalgorithm running in a constant number of rounds can achieve a constant\napproximation ratio. We then give a simple one-round deterministic algorithm\nachieving an approximation ratio of $\\tfrac1{d}$ for $d$-regular graphs with\n$d$ odd. We show that this is best possible in several ways, and in particular\nno deterministic algorithm with approximation ratio $\\tfrac1{d}+\\epsilon$ (with\n$\\epsilon>0$) can run in a constant number of rounds. We also prove results of\na similar flavour for the MAXDICUT problem in regular oriented graphs, where we\nwant to maximize the number of arcs oriented from the left part to the right\npart of the cut.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 13:32:24 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2019 12:46:41 GMT"}, {"version": "v3", "created": "Mon, 20 Jan 2020 13:49:51 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Bamas", "\u00c9tienne", ""], ["Esperet", "Louis", ""]]}, {"id": "1902.04950", "submitter": "Kaustav Bose", "authors": "Kaustav Bose, Manash Kumar Kundu, Ranendu Adhikary, Buddhadeb Sau", "title": "Arbitrary Pattern Formation by Asynchronous Opaque Robots with Lights", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Arbitrary Pattern Formation problem asks for a distributed algorithm that\nmoves a set of autonomous mobile robots to form any arbitrary pattern given as\ninput. The robots are assumed to be autonomous, anonymous and identical. They\noperate in Look-Compute-Move cycles under an asynchronous scheduler. The robots\ndo not have access to any global coordinate system. The movement of the robots\nis assumed to be rigid, which means that each robot is able to reach its\ndesired destination without interruption. The existing literature that\ninvestigates this problem, considers robots with unobstructed visibility. This\nwork considers the problem in the more realistic obstructed visibility model,\nwhere the view of a robot can be obstructed by the presence of other robots.\nThe robots are assumed to be punctiform and equipped with visible lights that\ncan assume a constant number of predefined colors. We have studied the problem\nin two settings based on the level of consistency among the local coordinate\nsystems of the robots: two axis agreement (they agree on the direction and\norientation of both coordinate axes) and one axis agreement (they agree on the\ndirection and orientation of only one coordinate axis). In both settings, we\nhave provided a full characterization of initial configurations from where any\narbitrary pattern can be formed.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 15:28:35 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 15:25:26 GMT"}, {"version": "v3", "created": "Tue, 14 Apr 2020 11:22:57 GMT"}, {"version": "v4", "created": "Wed, 7 Oct 2020 17:56:21 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Bose", "Kaustav", ""], ["Kundu", "Manash Kumar", ""], ["Adhikary", "Ranendu", ""], ["Sau", "Buddhadeb", ""]]}, {"id": "1902.04969", "submitter": "Kaidong Wu", "authors": "Kaidong Wu", "title": "An Empirical Study of Blockchain-based Decentralized Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A decentralized application (dapp for short) refers to an application that is\nexecuted by multiple users over a decentralized network. In recent years, the\nnumber of dapp keeps fast growing, mainly due to the popularity of blockchain\ntechnology. Despite the increasing importance of dapps as a typical application\ntype that is assumed to promote the adoption of blockchain, little is known on\nwhat, how, and how well dapps are used in practice. In addition, the insightful\nknowledge of whether and how a traditional application can be transformed to a\ndapp is yet missing. To bridge the knowledge gap, this paper presents a\ncomprehensive empirical study on an extensive dataset of 734 dapps that are\ncollected from three popular open dapp marketplaces, i.e., ethereum, state of\nthe dapp, and DAppRadar. We analyze the popularity of dapps, and summarize the\npatterns of how smart contracts are organized in a dapp. Based on the findings,\nwe draw some implications to help dapp developers and users better understand\nand deploy dapps.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 15:53:15 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Wu", "Kaidong", ""]]}, {"id": "1902.04995", "submitter": "John Charlton", "authors": "John Charlton, Steve Maddock, Paul Richmond", "title": "Two-Dimensional Batch Linear Programming on the GPU", "comments": "22 pages, 11 figures, 1 listing", "journal-ref": "Journal of Parallel and Distributed Computing, Volume 126, April\n  2019, Pages 152-160", "doi": "10.1016/j.jpdc.2019.01.001", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a novel, high-performance, graphical processing\nunit-based algorithm for efficiently solving two-dimensional linear programs in\nbatches. The domain of two-dimensional linear programs is particularly useful\ndue to the prevalence of relevant geometric problems. Batch linear programming\nrefers to solving numerous different linear programs within one operation. By\nsolving many linear programs simultaneously and distributing workload evenly\nacross threads, graphical processing unit utilization can be maximized.\nSpeedups of over 22 times and 63 times are obtained against state-of-the-art\ngraphics processing unit and CPU linear program solvers, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 16:40:15 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Charlton", "John", ""], ["Maddock", "Steve", ""], ["Richmond", "Paul", ""]]}, {"id": "1902.05234", "submitter": "Canhui Wang", "authors": "Canhui Wang and Xiaowen Chu", "title": "GPU Accelerated AES Algorithm", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been widely accepted that Graphics Processing Units (GPU) is one of\npromising schemes for encryption acceleration, in particular, the support of\ncomplex mathematical calculations such as integer and logical operations makes\nthe implementation easier; however, complexes such as parallel granularity,\nmemory allocation still imposes a burden on real world implementations. In this\npaper, we propose a new approach for Advanced Encryption Standard\naccelerations, including both encryption and decryption. Specifically, we adapt\nthe Electronic Code Book mode for cryptographic transformation, look up table\nscheme for fast lookup, and a granularity of one state per thread for thread\nscheduling. Our experimental results offer researchers a good understanding on\nGPU architectures and software accelerations. In addition, both our source code\nand experimental results are freely available.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 06:14:03 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Wang", "Canhui", ""], ["Chu", "Xiaowen", ""]]}, {"id": "1902.05320", "submitter": "Canhui Wang", "authors": "Canhui Wang and Xiaowen Chu", "title": "GPU Accelerated Keccak (SHA3) Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hash functions like SHA-1 or MD5 are one of the most important cryptographic\nprimitives, especially in the field of information integrity. Considering the\nfact that increasing methods have been proposed to break these hash algorithms,\na competition for a new family of hash functions was held by the US National\nInstitute of Standards and Technology. Keccak was the winner and selected to be\nthe next generation of hash function standard, named SHA-3. We aim to implement\nand optimize Batch mode based Keccak algorithms on NVIDIA GPU platform. Our\nwork consider the case of processing multiple hash tasks at once and implement\nthe case on CPU and GPU respectively. Our experimental results show that GPU\nperformance is significantly higher than CPU is the case of processing large\nbatches of small hash tasks.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 12:03:41 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Wang", "Canhui", ""], ["Chu", "Xiaowen", ""]]}, {"id": "1902.05416", "submitter": "Laurens Versluis", "authors": "Alexandru Iosup, Laurens Versluis, Animesh Trivedi, Erwin van Eyk,\n  Lucian Toader, Vincent van Beek, Giulia Frascaria, Ahmed Musaafir, Sacheendra\n  Talluri", "title": "The AtLarge Vision on the Design of Distributed Systems and Ecosystems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-quality designs of distributed systems and services are essential for\nour digital economy and society. Threatening to slow down the stream of working\ndesigns, we identify the mounting pressure of scale and complexity of\n\\mbox{(eco-)systems}, of ill-defined and wicked problems, and of unclear\nprocesses, methods, and tools. We envision design itself as a core research\ntopic in distributed systems, to understand and improve the science and\npractice of distributed (eco-)system design. Toward this vision, we propose the\nAtLarge design framework, accompanied by a set of 8 core design principles. We\nalso propose 10 key challenges, which we hope the community can address in the\nfollowing 5 years. In our experience so far, the proposed framework and\nprinciples are practical, and lead to pragmatic and innovative designs for\nlarge-scale distributed systems.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 15:18:43 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Iosup", "Alexandru", ""], ["Versluis", "Laurens", ""], ["Trivedi", "Animesh", ""], ["van Eyk", "Erwin", ""], ["Toader", "Lucian", ""], ["van Beek", "Vincent", ""], ["Frascaria", "Giulia", ""], ["Musaafir", "Ahmed", ""], ["Talluri", "Sacheendra", ""]]}, {"id": "1902.05577", "submitter": "Aakash Khochare", "authors": "Aakash Khochare, Aravindhan K, Yogesh Simmhan", "title": "A Scalable Platform for Distributed Object Tracking across a Many-camera\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Advances in deep neural networks (DNN) and computer vision (CV) algorithms\nhave made it feasible to extract meaningful insights from large-scale\ndeployments of urban cameras. Tracking an object of interest across the camera\nnetwork in near real-time is a canonical problem. However, current tracking\nplatforms have two key limitations: 1) They are monolithic, proprietary and\nlack the ability to rapidly incorporate sophisticated tracking models; and 2)\nThey are less responsive to dynamism across wide-area computing resources that\ninclude edge, fog and cloud abstractions. We address these gaps using Anveshak,\na runtime platform for composing and coordinating distributed tracking\napplications. It provides a domain-specific dataflow programming model to\nintuitively compose a tracking application, supporting contemporary CV advances\nlike query fusion and re-identification, and enabling dynamic scoping of the\ncamera network's search space to avoid wasted computation. We also offer\ntunable batching and data-dropping strategies for dataflow blocks deployed on\ndistributed resources to respond to network and compute variability. These\nbalance the tracking accuracy, its real-time performance and the active\ncamera-set size. We illustrate the concise expressiveness of the programming\nmodel for $4$ tracking applications. Our detailed experiments for a network of\n1000 camera-feeds on modest resources exhibit the tunable scalability,\nperformance and quality trade-offs enabled by our dynamic tracking, batching\nand dropping strategies.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 19:43:10 GMT"}, {"version": "v2", "created": "Sun, 15 Mar 2020 13:25:50 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Khochare", "Aakash", ""], ["K", "Aravindhan", ""], ["Simmhan", "Yogesh", ""]]}, {"id": "1902.05635", "submitter": "Abhinav Mishra", "authors": "Abhinav Mishra", "title": "Distributed Processes and Scalability in Sub-networks of Large-Scale\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance of standard processes over large distributed networks typically\nscales with the size of the network. For example, in planar topologies where\nnodes communicate with their natural neighbors, the scaling factor is $O(n)$,\nwhere $n$ is the number of nodes. As the size of the network increases, this\nmakes global convergence over the entire network less practical. On the other\nhand, for several applications, such as load balancing or detection of local\nevents, global convergence may not be necessary or even relevant. We introduce\nsimple distributed iterative processes which limit the scope of the\ncomputational task to a smaller subnetwork of the entire network. This is\nachieved using one additional local parameter which controls the size of the\nsubnetwork. We establish termination and convergence rate of such processes in\ntheory, in experiment, in comparison to the well understood behavior of Markov\nprocesses, and for a variety of network topologies and initial conditions.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 22:46:15 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Mishra", "Abhinav", ""]]}, {"id": "1902.05746", "submitter": "Wei Liu", "authors": "Xuanhua Shi, Wei Liu, Ligang He, Hai Jin, Ming Li, Yong Chen", "title": "Optimizing the SSD Burst Buffer by Traffic Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, Burst buffer has been proposed to manage the SSD buffering of\nbursty write requests. Although burst buffer can improve I/O performance in\nmany cases, we find that it has some limitations such as requiring large SSD\ncapacity and harmonious overlapping between computation phase and data flushing\nphase. In this paper, we propose a scheme, called SSDUP+1. SSDUP+ aims to\nimprove the burst buffer by addressing the above limitations. First, in order\nto reduce the demand for the SSD capacity, we develop a novel method to detect\nand quantify the data randomness in the write traffic. Further, an adaptive\nalgorithm is proposed to classify the random writes dynamically. By doing so,\nmuch less SSD capacity is required to achieve the similar performance as other\nburst buffer schemes. Next, in order to overcome the difficulty of perfectly\noverlapping the computation phase and the flushing phase, we propose a pipeline\nmechanism for the SSD buffer, in which data buffering and flushing are\nperformed in pipeline. In addition, in order to improve the I/O throughput, we\nadopt a traffic-aware flushing strategy to reduce the I/O interference in HDD.\nFinally, in order to further improve the performance of buffering random writes\nin SSD, SSDUP+ transforms the random writes to sequential writes in SSD by\nstoring the data with a log structure. Further, SSDUP+ uses the AVL tree\nstructure to store the sequence information of the data. We have implemented a\nprototype of SSDUP+ based on OrangeFS and conducted extensive experiments. The\nexperimental results show that our proposed SSDUP+ can save an average of 50%\nSSD space, while delivering almost the same performance as other common burst\nbuffer schemes. In addition, SSDUP+ can save about 20% SSD space compared with\nthe previous version of this work, SSDUP, while achieving 20%-30% higher I/O\nthroughput than SSDUP.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 10:04:05 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Shi", "Xuanhua", ""], ["Liu", "Wei", ""], ["He", "Ligang", ""], ["Jin", "Hai", ""], ["Li", "Ming", ""], ["Chen", "Yong", ""]]}, {"id": "1902.05808", "submitter": "Louis-Claude Canon", "authors": "Louis-Claude Canon and Mohamad El Sayah and Pierre-Cyrille H\\'eam", "title": "A Comparison of Random Task Graph Generation Methods for Scheduling\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to generate instances with relevant properties and without bias remains\nan open problem of critical importance for a fair comparison of heuristics. In\nthe context of scheduling with precedence constraints, the instance consists of\na task graph that determines a partial order on task executions. To avoid\nselecting instances among a set populated mainly with trivial ones, we rely on\nproperties that quantify the characteristics specific to difficult instances.\nAmong numerous identified such properties, the mass measures how much a task\ngraph can be decomposed into smaller ones. This property, together with an\nin-depth analysis of existing random task graph generation methods, establishes\nthe sub-exponential generic time complexity of the studied problem. Empirical\nobservations on the impact of existing generation methods on scheduling\nheuristics concludes our study.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 13:43:31 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Canon", "Louis-Claude", ""], ["Sayah", "Mohamad El", ""], ["H\u00e9am", "Pierre-Cyrille", ""]]}, {"id": "1902.05873", "submitter": "Balaji Arun", "authors": "Balaji Arun, Sebastiano Peluso, and Binoy Ravindran", "title": "Spectrum: A Framework for Adapting Consensus Protocols", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There exists a plethora of consensus protocols in literature. The reason is\nthat there is no one-size-fits-all solution, since every protocol is unique and\nits performance is directly tied to the deployment settings and workload\nconfigurations. Some protocols are well suited for geographical scale\nenvironments, e.g., leaderless, while others provide high performance under\nworkloads with high contention, e.g., single leader-based. Thus, existing\nprotocols seldom adapt to changing workload conditions. To overcome this\nlimitation, we propose Spectrum, a consensus framework that is able to switch\nconsensus protocols at run-time, to enable a dynamic reaction to changes in the\nworkload characteristics and deployment scenarios. With this framework, we\nprovide transparent instantiation of various consensus protocols, and a\ncompletely asynchronous switching mechanism with zero downtime. We assess the\neffectiveness of Spectrum via an extensive experimental evaluation, which shows\nthat Spectrum is able to limit the increase of the user perceived latency when\nswitching among consensus protocols.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 16:20:30 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Arun", "Balaji", ""], ["Peluso", "Sebastiano", ""], ["Ravindran", "Binoy", ""]]}, {"id": "1902.05968", "submitter": "Seyed Esmaeil Mirvakili", "authors": "Seyed Esmaeil Mirvakili, MohammadAmin Fazli, Jafar Habibi", "title": "Reactive Liquid: Optimized Liquid Architecture for Elastic and Resilient\n  Distributed Data Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's most prominent IT companies are built on the extraction of insight\nfrom data, and data processing has become crucial in data-intensive businesses.\nNevertheless, the size of data which should be processed is growing\nsignificantly fast. The pace of the data growing has changed the nature of data\nprocessing. Today, data-intensive industries demand highly scalable and fault\ntolerant data processing architectures which can handle the massive amount of\ndata. In this paper, we presented a distributed architecture for elastic and\nresilient data processing based on the Liquid which is a nearline and offline\nbig data architecture. We used the Reactive Manifesto to design the\narchitecture highly reactive to workload changes and failures. We evaluate our\narchitecture by drawing some numerical comparisons between our architecture\nprototype and the Liquid prototype. The performed evaluation shows that our\narchitecture can be more scalable against workload and more resilient against\nfailures than the Liquid architecture is.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 19:11:55 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Mirvakili", "Seyed Esmaeil", ""], ["Fazli", "MohammadAmin", ""], ["Habibi", "Jafar", ""]]}, {"id": "1902.06040", "submitter": "Paolo Bientinesi", "authors": "William McDoniel (1), Paolo Bientinesi (1) ((1) RWTH Aachen\n  University)", "title": "A Timer-Augmented Cost Function for Load Balanced DSMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to a hard dependency between time steps, large-scale simulations of gas\nusing the Direct Simulation Monte Carlo (DSMC) method proceed at the pace of\nthe slowest processor. Scalability is therefore achievable only by ensuring\nthat the work done each time step is as evenly apportioned among the processors\nas possible. Furthermore, as the simulated system evolves, the load shifts, and\nthus this load-balancing typically needs to be performed multiple times over\nthe course of a simulation. Common methods generally use either crude\nperformance models or processor-level timers. We combine both to create a\ntimer-augmented cost function which both converges quickly and yields\nwell-balanced processor decompositions. When compared to a particle-based\nperformance model alone, our method achieves 2x speedup at steady-state on up\nto 1024 processors for a test case consisting of a Mach 9 argon jet impacting a\nsolid wall.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2019 04:43:34 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["McDoniel", "William", ""], ["Bientinesi", "Paolo", ""]]}, {"id": "1902.06156", "submitter": "Moran Baruch", "authors": "Moran Baruch, Gilad Baruch, Yoav Goldberg", "title": "A Little Is Enough: Circumventing Defenses For Distributed Learning", "comments": null, "journal-ref": "https://papers.nips.cc/paper/2019/hash/ec1c59141046cd1866bbbcdfb6ae31d4-Abstract.html", "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed learning is central for large-scale training of deep-learning\nmodels. However, they are exposed to a security threat in which Byzantine\nparticipants can interrupt or control the learning process. Previous attack\nmodels and their corresponding defenses assume that the rogue participants are\n(a) omniscient (know the data of all other participants), and (b) introduce\nlarge change to the parameters. We show that small but well-crafted changes are\nsufficient, leading to a novel non-omniscient attack on distributed learning\nthat go undetected by all existing defenses. We demonstrate our attack method\nworks not only for preventing convergence but also for repurposing of the model\nbehavior (backdooring). We show that 20% of corrupt workers are sufficient to\ndegrade a CIFAR10 model accuracy by 50%, as well as to introduce backdoors into\nMNIST and CIFAR10 models without hurting their accuracy\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2019 21:05:29 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Baruch", "Moran", ""], ["Baruch", "Gilad", ""], ["Goldberg", "Yoav", ""]]}, {"id": "1902.06332", "submitter": "Lin Chen", "authors": "Mingrui Zhang, Lin Chen, Aryan Mokhtari, Hamed Hassani, Amin Karbasi", "title": "Quantized Frank-Wolfe: Faster Optimization, Lower Communication, and\n  Projection Free", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we efficiently mitigate the overhead of gradient communications in\ndistributed optimization? This problem is at the heart of training scalable\nmachine learning models and has been mainly studied in the unconstrained\nsetting. In this paper, we propose Quantized-Frank-Wolfe (QFW), the first\nprojection-free and communication-efficient algorithm for solving constrained\noptimization problems at scale. We consider both convex and non-convex\nobjective functions, expressed as a finite-sum or more generally a stochastic\noptimization problem, and provide strong theoretical guarantees on the\nconvergence rate of QFW. This is accomplished by proposing novel quantization\nschemes that efficiently compress gradients while controlling the noise\nvariance introduced during this process. Finally, we empirically validate the\nefficiency of QFW in terms of communication and the quality of returned\nsolution against natural baselines.\n", "versions": [{"version": "v1", "created": "Sun, 17 Feb 2019 21:43:10 GMT"}, {"version": "v2", "created": "Wed, 6 Mar 2019 15:36:18 GMT"}, {"version": "v3", "created": "Thu, 30 May 2019 20:15:16 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Zhang", "Mingrui", ""], ["Chen", "Lin", ""], ["Mokhtari", "Aryan", ""], ["Hassani", "Hamed", ""], ["Karbasi", "Amin", ""]]}, {"id": "1902.06468", "submitter": "Minsoo Rhu", "authors": "Youngeun Kwon, Minsoo Rhu", "title": "Beyond the Memory Wall: A Case for Memory-centric HPC System for Deep\n  Learning", "comments": "Published as a conference paper at the 51st IEEE/ACM International\n  Symposium on Microarchitecture (MICRO-51), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the models and the datasets to train deep learning (DL) models scale,\nsystem architects are faced with new challenges, one of which is the memory\ncapacity bottleneck, where the limited physical memory inside the accelerator\ndevice constrains the algorithm that can be studied. We propose a\nmemory-centric deep learning system that can transparently expand the memory\ncapacity available to the accelerators while also providing fast inter-device\ncommunication for parallel training. Our proposal aggregates a pool of memory\nmodules locally within the device-side interconnect, which are decoupled from\nthe host interface and function as a vehicle for transparent memory capacity\nexpansion. Compared to conventional systems, our proposal achieves an average\n2.8x speedup on eight DL applications and increases the system-wide memory\ncapacity to tens of TBs.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 09:07:07 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Kwon", "Youngeun", ""], ["Rhu", "Minsoo", ""]]}, {"id": "1902.06493", "submitter": "{\\L}ukasz Lachowski", "authors": "{\\L}ukasz Lachowski", "title": "Complexity of the quorum intersection property of the Federated\n  Byzantine Agreement System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Federated Byzantine Agreement System is defined as a pair $(V, Q)$\ncomprising a set of nodes $V$ and a quorum function $Q: V \\mapsto 2^{2^{V}}\n\\setminus \\{\\emptyset\\}$ specifying for each node a set of subsets of nodes,\ncalled quorum slices. A subset of nodes is a quorum if and only if for each of\nits nodes it also contains at least one of its quorum slices. The Disjoint\nQuorums Problem answers the question whether a given instance of \\acrlong{fbas}\ncontains two quorums that have no nodes in common. We show that this problem is\n$\\mathsf{NP-complete}$. We also study the problem of finding a quorum of\nminimal size and show it is $\\mathsf{NP-hard}$. Further, we consider the\nproblem of checking whether a given subset of nodes contains a quorum for some\nselected node. We show this problem is $\\mathsf{P-complete}$ and describe a\nmethod that solves it in linear time with respect to number of nodes and the\ntotal size of all quorum slices. Moreover, we analyze the complexity of some of\nthese problems using the parametrized point of view.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 10:20:05 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Lachowski", "\u0141ukasz", ""]]}, {"id": "1902.06661", "submitter": "Basit Qureshi", "authors": "Basit Qureshi, Kamal Kawlaq, Anis Koubaa, Basel Sultan, Mohammad\n  Younis", "title": "A Commodity SBC-Edge Cluster for Smart Cities", "comments": "6 pages. Submitted to 2nd International conference on Computer\n  Applications & Information Security, ICCAIS'2019. 19-21 March, 2019, Saudi\n  Computer Society, Riyadh, Saudi Arabia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The commodity Single Board Computers (SBCs) are increasingly becoming\npowerful and can execute standard operating systems and mainstream workloads.\nIn the context of cloud-based smart city applications, SBCs can be utilized as\nEdge computing devices reducing the network communication. In this paper, we\ninvestigate the design and implementation of a SBC based edge cluster (SBC-EC)\nframework for a smart parking application. Since SBCs are resource constrained\ndevices, we devise a container-based framework for a lighter foot-print.\nKubernetes was used as an orchestration tool to orchestrate various containers\nin the framework. To validate our approach, we implemented a proof-of-concept\nof the SBC based Edge cluster for a smart parking application, as a possible\nIoT use-case. Our implementation shows that, the use of SBC devices at the edge\nof a cloud based smart parking application is a cost effective and low energy,\ngreen computing solution. The proposed framework can be extended to similar\ncloud-based applications in the context of a smart city.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 08:33:17 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Qureshi", "Basit", ""], ["Kawlaq", "Kamal", ""], ["Koubaa", "Anis", ""], ["Sultan", "Basel", ""], ["Younis", "Mohammad", ""]]}, {"id": "1902.06776", "submitter": "Heidi Howard", "authors": "Heidi Howard, Richard Mortier", "title": "A Generalised Solution to Distributed Consensus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed consensus, the ability to reach agreement in the face of failures\nand asynchrony, is a fundamental primitive for constructing reliable\ndistributed systems from unreliable components. The Paxos algorithm is\nsynonymous with distributed consensus, yet it performs poorly in practice and\nis famously difficult to understand. In this paper, we re-examine the\nfoundations of distributed consensus. We derive an abstract solution to\nconsensus, which utilises immutable state for intuitive reasoning about safety.\nWe prove that our abstract solution generalises over Paxos as well as the Fast\nPaxos and Flexible Paxos algorithms. The surprising result of this analysis is\na substantial weakening to the quorum requirements of these widely studied\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 19:53:56 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Howard", "Heidi", ""], ["Mortier", "Richard", ""]]}, {"id": "1902.06803", "submitter": "Jukka Suomela", "authors": "Alkida Balliu, Sebastian Brandt, Dennis Olivetti, Jukka Suomela", "title": "How much does randomness help with locally checkable problems?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locally checkable labeling problems (LCLs) are distributed graph problems in\nwhich a solution is globally feasible if it is locally feasible in all\nconstant-radius neighborhoods. Vertex colorings, maximal independent sets, and\nmaximal matchings are examples of LCLs.\n  On the one hand, it is known that some LCLs benefit exponentially from\nrandomness---for example, any deterministic distributed algorithm that finds a\nsinkless orientation requires $\\Theta(\\log n)$ rounds in the LOCAL model, while\nthe randomized complexity of the problem is $\\Theta(\\log \\log n)$ rounds. On\nthe other hand, there are also many LCLs in which randomness is useless.\n  Previously, it was not known if there are any LCLs that benefit from\nrandomness, but only subexponentially. We show that such problems exist: for\nexample, there is an LCL with deterministic complexity $\\Theta(\\log^2 n)$\nrounds and randomized complexity $\\Theta(\\log n \\log \\log n)$ rounds.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 21:22:04 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 12:25:28 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Balliu", "Alkida", ""], ["Brandt", "Sebastian", ""], ["Olivetti", "Dennis", ""], ["Suomela", "Jukka", ""]]}, {"id": "1902.06855", "submitter": "Peng Sun", "authors": "Peng Sun, Wansen Feng, Ruobing Han, Shengen Yan and Yonggang Wen", "title": "Optimizing Network Performance for Distributed DNN Training on GPU\n  Clusters: ImageNet/AlexNet Training in 1.5 Minutes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is important to scale out deep neural network (DNN) training for reducing\nmodel training time. The high communication overhead is one of the major\nperformance bottlenecks for distributed DNN training across multiple GPUs. Our\ninvestigations have shown that popular open-source DNN systems could only\nachieve 2.5 speedup ratio on 64 GPUs connected by 56 Gbps network. To address\nthis problem, we propose a communication backend named GradientFlow for\ndistributed DNN training, and employ a set of network optimization techniques.\nFirst, we integrate ring-based allreduce, mixed-precision training, and\ncomputation/communication overlap into GradientFlow. Second, we propose lazy\nallreduce to improve network throughput by fusing multiple communication\noperations into a single one, and design coarse-grained sparse communication to\nreduce network traffic by only transmitting important gradient chunks. When\ntraining ImageNet/AlexNet on 512 GPUs, our approach achieves 410.2 speedup\nratio and completes 95-epoch training in 1.5 minutes, which outperforms\nexisting approaches.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 01:18:56 GMT"}, {"version": "v2", "created": "Sat, 16 Mar 2019 13:39:48 GMT"}, {"version": "v3", "created": "Tue, 22 Oct 2019 08:52:08 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Sun", "Peng", ""], ["Feng", "Wansen", ""], ["Han", "Ruobing", ""], ["Yan", "Shengen", ""], ["Wen", "Yonggang", ""]]}, {"id": "1902.06891", "submitter": "Hammurabi Mendes", "authors": "Samuel Thomas, Ana Hayne, Jonad Pulaj, Hammurabi Mendes", "title": "Using Skip Graphs for Increased NUMA Locality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a data partitioning technique performed over skip graphs that\npromotes significant quantitative and qualitative improvements on NUMA locality\nin concurrent data structures, as well as reduced contention. We build on\nprevious techniques of thread-local indexing and laziness, and, at a high\nlevel, our design consists of a partitioned skip graph, well-integrated with\nthread-local sequential maps, operating without contention. As a\nproof-of-concept, we implemented map and relaxed priority queue ADTs using our\ntechnique. Maps were conceived using lazy and non-lazy approaches to insertions\nand removals, and our implementations are shown to be competitive with\nstate-of-the-art maps. We observe a 6x higher CAS locality, a 68.6% reduction\non the number of remote CAS operations, and a increase from 88.3% to 99% CAS\nsuccess rate when using a lazy skip graph as compared to a control skip list\n(subject to the same codebase, optimizations, and implementation practices).\nQualitatively speaking, remote memory accesses are not only reduced in number,\nbut the larger the NUMA distance between threads, the larger the reduction is.\nWe consider two alternative implementations of relaxed priority queues that\nfurther take advantage of our data partitioning over skip graphs: (a) using\n``spraying'', a well-known random-walk technique usually performed over skip\nlists, but now performed over skip graphs; and (b) a custom protocol that\ntraverses the skip graph deterministically, marking elements along this\ntraversal. We provide formal arguments indicating that the first approach is\nmore \\emph{relaxed}, that is, that the span of removed keys is larger, while\nthe second approach has smaller contention. Experimental results indicate that\nthe approach based on spraying performs better on skip graphs, yet both seem to\nscale appropriately.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 04:38:46 GMT"}, {"version": "v2", "created": "Fri, 17 May 2019 04:01:49 GMT"}, {"version": "v3", "created": "Sat, 25 May 2019 01:49:07 GMT"}, {"version": "v4", "created": "Tue, 17 Sep 2019 03:38:13 GMT"}, {"version": "v5", "created": "Tue, 18 Feb 2020 07:58:07 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Thomas", "Samuel", ""], ["Hayne", "Ana", ""], ["Pulaj", "Jonad", ""], ["Mendes", "Hammurabi", ""]]}, {"id": "1902.06893", "submitter": "Chen Yuan", "authors": "Chen Yuan, Yi Lu, Wei Feng, Guangyi Liu, Renchang Dai, Yachen Tang,\n  Zhiwei Wang", "title": "Graph Computing based Distributed Fast Decoupled Power Flow Analysis", "comments": "5 figures, 3 tables, 2019 IEEE Power and Energy Society General\n  Meeting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power flow analysis plays a fundamental and critical role in the energy\nmanagement system (EMS). It is required to well accommodate large and complex\npower system. To achieve a high performance and accurate power flow analysis, a\ngraph computing based distributed power flow analysis approach is proposed in\nthis paper. Firstly, a power system network is divided into multiple areas.\nSlack buses are selected for each area and, at each SCADA sampling period, the\ninter-area transmission line power flows are equivalently allocated as extra\nload injections to corresponding buses. Then, the system network is converted\ninto multiple independent areas. In this way, the power flow analysis could be\nconducted in parallel for each area and the solved system states could be\nguaranteed without compromise of accuracy. Besides, for each area, graph\ncomputing based fast decoupled power flow (FDPF) is employed to quickly analyze\nsystem states. IEEE 118-bus system and MP 10790-bus system are employed to\nverify the results accuracy and present the promising computation performance\nof the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 04:56:00 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Yuan", "Chen", ""], ["Lu", "Yi", ""], ["Feng", "Wei", ""], ["Liu", "Guangyi", ""], ["Dai", "Renchang", ""], ["Tang", "Yachen", ""], ["Wang", "Zhiwei", ""]]}, {"id": "1902.07009", "submitter": "Andres Arcia Moret", "authors": "John Moore and Andr\\'es Arcia-Moret and Poonam Yadav and Richard\n  Mortier and Anthony Brown and Derek McAuley and Andy Crabtree and Chris\n  Greenhalgh and Hamed Haddadi and Yousef Amar", "title": "Zest: REST over ZeroMQ", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce Zest (REST over ZeroMQ), a middleware technology\nin support of an Internet of Things (IoT). Our work is influenced by the\nConstrained Application Protocol (CoAP) but emphasises systems that can support\nfine-grained access control to both resources and audit information, and can\nprovide features such as asynchronous communication patterns between nodes. We\nachieve this by using a hybrid approach that combines a RESTful architecture\nwith a variant of a publisher/subscriber topology that has enhanced routing\nsupport. The primary motivation for Zest is to provide inter-component\ncommunications in the Databox, but it is applicable in other contexts where\ntight control needs to be maintained over permitted communication patterns.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 12:04:29 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Moore", "John", ""], ["Arcia-Moret", "Andr\u00e9s", ""], ["Yadav", "Poonam", ""], ["Mortier", "Richard", ""], ["Brown", "Anthony", ""], ["McAuley", "Derek", ""], ["Crabtree", "Andy", ""], ["Greenhalgh", "Chris", ""], ["Haddadi", "Hamed", ""], ["Amar", "Yousef", ""]]}, {"id": "1902.07022", "submitter": "Andres Arcia Moret", "authors": "Alessandro E. C. Redondi and Andr\\'es Arcia-Moret and Pietro Manzoni", "title": "Towards a Scaled IoT Pub/Sub Architecture for 5G Networks: the Case of\n  Multiaccess Edge Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vision of the Internet of Thing is becoming a reality and novel\ncommunications technologies such as the upcoming 5G network architecture are\ndesigned to support its full deployment. In this scenario, we discuss the\nbenefits that a publish/subscribe protocol such as MQTT or its recently\nproposed enhancement MQTT+ could bring into the picture. However, deploying\npub/sub brokers with advanced caching and aggregation functionalities in a\ndistributed fashion poses challenges in protocol design and management of\ncommunication resources. In this paper, we identify the main research\nchallenges and possible solutions to scale up a pub/sub architecture for\nupcoming IoT applications in 5G networks, and we present our perspective on\nsystems design, optimisation, and working implementations.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 12:34:37 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Redondi", "Alessandro E. C.", ""], ["Arcia-Moret", "Andr\u00e9s", ""], ["Manzoni", "Pietro", ""]]}, {"id": "1902.07055", "submitter": "Przemys{\\l}aw Uzna\\'nski", "authors": "Adrian Kosowski, Przemys{\\l}aw Uzna\\'nski, Laurent Viennot", "title": "Hardness of Exact Distance Queries in Sparse Graphs Through Hub Labeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A distance labeling scheme is an assignment of bit-labels to the vertices of\nan undirected, unweighted graph such that the distance between any pair of\nvertices can be decoded solely from their labels. An important class of\ndistance labeling schemes is that of hub labelings, where a node $v \\in G$\nstores its distance to the so-called hubs $S_v \\subseteq V$, chosen so that for\nany $u,v \\in V$ there is $w \\in S_u \\cap S_v$ belonging to some shortest $uv$\npath. Notice that for most existing graph classes, the best distance labelling\nconstructions existing use at some point a hub labeling scheme at least as a\nkey building block. Our interest lies in hub labelings of sparse graphs, i.e.,\nthose with $|E(G)| = O(n)$, for which we show a lowerbound of\n$\\frac{n}{2^{O(\\sqrt{\\log n})}}$ for the average size of the hubsets.\nAdditionally, we show a hub-labeling construction for sparse graphs of average\nsize $O(\\frac{n}{RS(n)^{c}})$ for some $0 < c < 1$, where $RS(n)$ is the\nso-called Ruzsa-Szemer{\\'e}di function, linked to structure of induced\nmatchings in dense graphs. This implies that further improving the lower bound\non hub labeling size to $\\frac{n}{2^{(\\log n)^{o(1)}}}$ would require a\nbreakthrough in the study of lower bounds on $RS(n)$, which have resisted\nsubstantial improvement in the last 70 years. For general distance labeling of\nsparse graphs, we show a lowerbound of $\\frac{1}{2^{O(\\sqrt{\\log n})}}\nSumIndex(n)$, where $SumIndex(n)$ is the communication complexity of the\nSum-Index problem over $Z_n$. Our results suggest that the best achievable\nhub-label size and distance-label size in sparse graphs may be\n$\\Theta(\\frac{n}{2^{(\\log n)^c}})$ for some $0<c < 1$.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 13:57:29 GMT"}, {"version": "v2", "created": "Thu, 23 May 2019 20:16:04 GMT"}, {"version": "v3", "created": "Fri, 21 Jun 2019 18:28:46 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Kosowski", "Adrian", ""], ["Uzna\u0144ski", "Przemys\u0142aw", ""], ["Viennot", "Laurent", ""]]}, {"id": "1902.07138", "submitter": "Hadrien Hendrikx", "authors": "Aur\\'elien Bellet, Rachid Guerraoui, Hadrien Hendrikx", "title": "Who started this rumor? Quantifying the natural differential privacy\n  guarantees of gossip protocols", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gossip protocols are widely used to disseminate information in massive\npeer-to-peer networks. These protocols are often claimed to guarantee privacy\nbecause of the uncertainty they introduce on the node that started the\ndissemination. But is that claim really true? Can the source of a gossip safely\nhide in the crowd? This paper examines, for the first time, gossip protocols\nthrough a rigorous mathematical framework based on differential privacy to\ndetermine the extent to which the source of a gossip can be traceable.\nConsidering the case of a complete graph in which a subset of the nodes are\ncurious, we study a family of gossip protocols parameterized by a ``muting''\nparameter $s$: nodes stop emitting after each communication with a fixed\nprobability $1-s$. We first prove that the standard push protocol,\ncorresponding to the case $s=1$, does not satisfy differential privacy for\nlarge graphs. In contrast, the protocol with $s=0$ achieves optimal privacy\nguarantees but at the cost of a drastic increase in the spreading time compared\nto standard push, revealing an interesting tension between privacy and\nspreading time. Yet, surprisingly, we show that some choices of the muting\nparameter $s$ lead to protocols that achieve an optimal order of magnitude in\nboth privacy and speed. We also confirm empirically that, with appropriate\nchoices of $s$, we indeed obtain protocols that are very robust against\nconcrete source location attacks while spreading the information almost as fast\nas the standard (and non-private) push protocol.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 16:52:51 GMT"}, {"version": "v2", "created": "Fri, 22 Feb 2019 12:49:50 GMT"}, {"version": "v3", "created": "Mon, 27 May 2019 09:01:07 GMT"}, {"version": "v4", "created": "Tue, 14 Jan 2020 13:59:51 GMT"}, {"version": "v5", "created": "Mon, 3 Aug 2020 15:01:20 GMT"}, {"version": "v6", "created": "Tue, 4 Aug 2020 13:01:37 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Bellet", "Aur\u00e9lien", ""], ["Guerraoui", "Rachid", ""], ["Hendrikx", "Hadrien", ""]]}, {"id": "1902.07254", "submitter": "Mark Day", "authors": "Mark Stuart Day", "title": "The Shutdown Problem: How Does a Blockchain System End?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define and examine the shutdown problem for blockchain systems: how to\ngracefully end the system's operation at the end of its useful life. A\nparticular focus is those blockchain systems that hold archival data of\nlong-lived interest. We outline what it means to achieve a successful shutdown,\nand compare those criteria to likely end-of-life conditions in a generic\nblockchain system. We conclude that the decentralized nature of blockchain\nsystems makes shutdown difficult, particularly if the system uses an unstable\nconsensus like the Nakamoto consensus of Bitcoin. Accordingly, we recommend\nagainst using blockchain with unstable consensus for any data whose value is\nlikely to persist beyond the life of the blockchain system. For any such\nsystems that are already in operation, we recommend considering a hard fork to\nimplement stable consensus. Such consideration needs to happen well in advance\nof the system's end of life.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 19:51:12 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Day", "Mark Stuart", ""]]}, {"id": "1902.07354", "submitter": "Abdolhamid Ghodselahi", "authors": "Abdolhamid Ghodselahi, Fabian Kuhn, Volker Turau", "title": "Concurrent Distributed Serving with Mobile Servers", "comments": "34 pages, conference version in ISAAC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new resource allocation problem in distributed\ncomputing called distributed serving with mobile servers (DSMS). In DSMS, there\nare $k$ identical mobile servers residing at the processors of a network. At\narbitrary points of time, any subset of processors can invoke one or more\nrequests. To serve a request, one of the servers must move to the processor\nthat invoked the request. Resource allocation is performed in a distributed\nmanner since only the processor that invoked the request initially knows about\nit. All processors cooperate by passing messages to achieve correct resource\nallocation. They do this with the goal to minimize the communication cost.\n  Routing servers in large-scale distributed systems requires a scalable\nlocation service. We introduce the distributed protocol GNN that solves the\nDSMS problem on overlay trees. We prove that GNN is starvation-free and\ncorrectly integrates locating the servers and synchronizing the concurrent\naccess to servers despite asynchrony, even when the requests are invoked over\ntime. Further, we analyze GNN for \"one-shot\" executions, i.e., all requests are\ninvoked simultaneously. We prove that when running GNN on top of a special\nfamily of tree topologies---known as hierarchically well-separated trees\n(HSTs)---we obtain a randomized distributed protocol with an expected\ncompetitive ratio of $O(\\log n)$ on general network topologies with $n$\nprocessors. From a technical point of view, our main result is that GNN\noptimally solves the DSMS problem on HSTs for one-shot executions, even if\ncommunication is asynchronous. Further, we present a lower bound of\n$\\Omega(\\max\\{k, \\log n/\\log\\log n\\})$ on the competitive ratio for DSMS. The\nlower bound even holds when communication is synchronous and requests are\ninvoked sequentially.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 00:10:02 GMT"}, {"version": "v2", "created": "Sat, 16 Nov 2019 22:59:53 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Ghodselahi", "Abdolhamid", ""], ["Kuhn", "Fabian", ""], ["Turau", "Volker", ""]]}, {"id": "1902.07463", "submitter": "Yu Xing", "authors": "Yu Xing, Shuang Liang, Lingzhi Sui, Xijie Jia, Jiantao Qiu, Xin Liu,\n  Yushun Wang, Yu Wang, and Yi Shan", "title": "DNNVM : End-to-End Compiler Leveraging Heterogeneous Optimizations on\n  FPGA-based CNN Accelerators", "comments": "18 pages, 9 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The convolutional neural network (CNN) has become a state-of-the-art method\nfor several artificial intelligence domains in recent years. The increasingly\ncomplex CNN models are both computation-bound and I/O-bound. FPGA-based\naccelerators driven by custom instruction set architecture (ISA) achieve a\nbalance between generality and efficiency, but there is much on them left to be\noptimized. We propose the full-stack compiler DNNVM, which is an integration of\noptimizers for graphs, loops and data layouts, and an assembler, a runtime\nsupporter and a validation environment. The DNNVM works in the context of deep\nlearning frameworks and transforms CNN models into the directed acyclic graph:\nXGraph. Based on XGraph, we transform the optimization challenges for both the\ndata layout and pipeline into graph-level problems. DNNVM enumerates all\npotentially profitable fusion opportunities by a heuristic subgraph isomorphism\nalgorithm to leverage pipeline and data layout optimizations, and searches for\nthe best choice of execution strategies of the whole computing graph. On the\nXilinx ZU2 @330 MHz and ZU9 @330 MHz, we achieve equivalently state-of-the-art\nperformance on our benchmarks by na\\\"ive implementations without optimizations,\nand the throughput is further improved up to 1.26x by leveraging heterogeneous\noptimizations in DNNVM. Finally, with ZU9 @330 MHz, we achieve state-of-the-art\nperformance for VGG and ResNet50. We achieve a throughput of 2.82 TOPs/s and an\nenergy efficiency of 123.7 GOPs/s/W for VGG. Additionally, we achieve 1.38\nTOPs/s for ResNet50 and 1.41 TOPs/s for GoogleNet.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 09:30:17 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 09:41:31 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Xing", "Yu", ""], ["Liang", "Shuang", ""], ["Sui", "Lingzhi", ""], ["Jia", "Xijie", ""], ["Qiu", "Jiantao", ""], ["Liu", "Xin", ""], ["Wang", "Yushun", ""], ["Wang", "Yu", ""], ["Shan", "Yi", ""]]}, {"id": "1902.07490", "submitter": "Sergey Rykovanov G.", "authors": "Igor Zacharov, Rinat Arslanov, Maxim Gunin, Daniil Stefonishin, Sergey\n  Pavlov, Oleg Panarin, Anton Maliutin, Sergey Rykovanov, Maxim Fedorov", "title": "'Zhores' -- Petaflops supercomputer for data-driven modeling, machine\n  learning and artificial intelligence installed in Skolkovo Institute of\n  Science and Technology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Petaflops supercomputer \"Zhores\" recently launched in the \"Center for\nComputational and Data-Intensive Science and Engineering\" (CDISE) of Skolkovo\nInstitute of Science and Technology (Skoltech) opens up new exciting\nopportunities for scientific discoveries in the institute especially in the\nareas of data-driven modeling, machine learning and artificial intelligence.\nThis supercomputer utilizes the latest generation of Intel and NVidia\nprocessors to provide resources for the most compute intensive tasks of the\nSkoltech scientists working in digital pharma, predictive analytics, photonics,\nmaterial science, image processing, plasma physics and many more. Currently it\nplaces 6th in the Russian and CIS TOP-50 (2018) supercomputer list. In this\narticle we summarize the cluster properties and discuss the measured\nperformance and usage modes of this scientific instrument in Skoltech.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 10:30:08 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Zacharov", "Igor", ""], ["Arslanov", "Rinat", ""], ["Gunin", "Maxim", ""], ["Stefonishin", "Daniil", ""], ["Pavlov", "Sergey", ""], ["Panarin", "Oleg", ""], ["Maliutin", "Anton", ""], ["Rykovanov", "Sergey", ""], ["Fedorov", "Maxim", ""]]}, {"id": "1902.07590", "submitter": "Zhang Yang", "authors": "Zhang Yang, Aiqing Zhang, Zeyao Mo", "title": "JArena: Partitioned Shared Memory for NUMA-awareness in Multi-threaded\n  Scientific Applications", "comments": "12 pages, 3 figures, submitted to Euro-Par 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distributed shared memory (DSM) architecture is widely used in today's\ncomputer design to mitigate the ever-widening processing-memory gap, and\ninevitably exhibits non-uniform memory access (NUMA) to shared-memory parallel\napplications. Failure to achieve full NUMA-awareness can significantly\ndowngrade application performance, especially on today's manycore platforms\nwith tens to hundreds of cores. Yet traditional approaches such as first-touch\nand memory policy fail short in either false page-sharing, fragmentation, or\nease-of-use. In this paper, we propose a partitioned shared memory approach\nwhich allows multi-threaded applications to achieve full NUMA-awareness with\nonly minor code changes and develop a companying NUMA-aware heap manager which\neliminates false page-sharing and minimizes fragmentation. Experiments on a\n256-core cc-NUMA computing node show that the proposed approach achieves true\nNUMA-awareness and improves the performance of typical multi-threaded\nscientific applications up to 4.3 folds with the increased use of cores.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 15:07:54 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Yang", "Zhang", ""], ["Zhang", "Aiqing", ""], ["Mo", "Zeyao", ""]]}, {"id": "1902.07747", "submitter": "Ziran Wang", "authors": "Ziran Wang and Kyuntae Han and BaekGyu Kim and Guoyuan Wu and Matthew\n  J. Barth", "title": "Lookup Table-Based Consensus Algorithm for Real-Time Longitudinal Motion\n  Control of Connected and Automated Vehicles", "comments": "2019 American Control Conference (ACC)Philadelphia, PA, USA, July\n  10-12, 2019978-1-5386-7928-9", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.AI cs.DC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Connected and automated vehicle (CAV) technology is one of the promising\nsolutions to addressing the safety, mobility and sustainability issues of our\ncurrent transportation systems. Specifically, the control algorithm plays an\nimportant role in a CAV system, since it executes the commands generated by\nformer steps, such as communication, perception, and planning. In this study,\nwe propose a consensus algorithm to control the longitudinal motion of CAVs in\nreal time. Different from previous studies in this field where control gains of\nthe consensus algorithm are pre-determined and fixed, we develop algorithms to\nbuild up a lookup table, searching for the ideal control gains with respect to\ndifferent initial conditions of CAVs in real time. Numerical simulation shows\nthat, the proposed lookup table-based consensus algorithm outperforms the\nauthors' previous work, as well as van Arem's linear feedback-based\nlongitudinal motion control algorithm in all four different scenarios with\nvarious initial conditions of CAVs, in terms of convergence time and maximum\njerk of the simulation run.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 19:39:28 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 19:09:07 GMT"}, {"version": "v3", "created": "Sat, 20 Jul 2019 19:19:08 GMT"}, {"version": "v4", "created": "Sat, 27 Jul 2019 23:50:46 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Wang", "Ziran", ""], ["Han", "Kyuntae", ""], ["Kim", "BaekGyu", ""], ["Wu", "Guoyuan", ""], ["Barth", "Matthew J.", ""]]}, {"id": "1902.07848", "submitter": "Chengjie Li", "authors": "Chengjie Li, Ruixuan Li, Haozhao Wang, Yuhua Li, Pan Zhou, Song Guo,\n  Keqin Li", "title": "Gradient Scheduling with Global Momentum for Non-IID Data Distributed\n  Asynchronous Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed asynchronous offline training has received widespread attention\nin recent years because of its high performance on large-scale data and complex\nmodels. As data are distributed from cloud-centric to edge nodes, a big\nchallenge for distributed machine learning systems is how to handle native and\nnatural non-independent and identically distributed (non-IID) data for\ntraining. Previous asynchronous training methods do not have a satisfying\nperformance on non-IID data because it would result in that the training\nprocess fluctuates greatly which leads to an abnormal convergence. We propose a\ngradient scheduling algorithm with partly averaged gradients and global\nmomentum (GSGM) for non-IID data distributed asynchronous training. Our key\nidea is to apply global momentum and local average to the biased gradient after\nscheduling, in order to make the training process steady. Experimental results\nshow that for non-IID data training under the same experimental conditions,\nGSGM on popular optimization algorithms can achieve a 20% increase in training\nstability with a slight improvement in accuracy on Fashion-Mnist and CIFAR-10\ndatasets. Meanwhile, when expanding distributed scale on CIFAR-100 dataset that\nresults in sparse data distribution, GSGM can perform a 37% improvement on\ntraining stability. Moreover, only GSGM can converge well when the number of\ncomputing nodes grows to 30, compared to the state-of-the-art distributed\nasynchronous algorithms. At the same time, GSGM is robust to different degrees\nof non-IID data.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 02:39:07 GMT"}, {"version": "v2", "created": "Fri, 29 Mar 2019 06:42:27 GMT"}, {"version": "v3", "created": "Tue, 23 Apr 2019 09:05:44 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Li", "Chengjie", ""], ["Li", "Ruixuan", ""], ["Wang", "Haozhao", ""], ["Li", "Yuhua", ""], ["Zhou", "Pan", ""], ["Guo", "Song", ""], ["Li", "Keqin", ""]]}, {"id": "1902.07895", "submitter": "Yackolley Amoussou-Guenou", "authors": "Yackolley Amoussou-Guenou (LIP6, LIST, NPA), Bruno Biais (HEC Paris,\n  TSM), Maria Potop-Butucaru (NPA, LINCS, LIP6), Sara Tucci-Piergiovanni (LIST,\n  DILS)", "title": "Rationals vs Byzantines in Consensus-based Blockchains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we analyze from the game theory point of view Byzantine Fault\nTolerant blockchains when processes exhibit rational or Byzantine behavior. Our\nwork is the first to model the Byzantine-consensus based blockchains as a\ncommittee coordination game. Our first contribution is to offer a\ngame-theoretical methodology to analyse equilibrium interactions between\nByzantine and rational committee members in Byzantine Fault Tolerant\nblockchains. Byzantine processes seek to inflict maximum damage to the system,\nwhile rational processes best-respond to maximise their expected net gains. Our\nsecond contribution is to derive conditions under which consensus properties\nare satisfied or not in equilibrium. When the majority threshold is lower than\nthe proportion of Byzantine processes, invalid blocks are accepted in\nequilibrium. When the majority threshold is large, equilibrium can involve\ncoordination failures , in which no block is ever accepted. However, when the\ncost of accepting invalid blocks is large, there exists an equilibrium in which\nblocks are accepted iff they are valid.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 07:44:50 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Amoussou-Guenou", "Yackolley", "", "LIP6, LIST, NPA"], ["Biais", "Bruno", "", "HEC Paris,\n  TSM"], ["Potop-Butucaru", "Maria", "", "NPA, LINCS, LIP6"], ["Tucci-Piergiovanni", "Sara", "", "LIST,\n  DILS"]]}, {"id": "1902.07901", "submitter": "Anastasios Gounaris", "authors": "Theodoros Toliopoulos, Anastasios Gounaris, Kostas Tsichlas, Apostolos\n  Papadopoulos, Sandra Sampaio", "title": "Continuous Outlier Mining of Streaming Data in Flink", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we focus on distance-based outliers in a metric space, where\nthe status of an entity as to whether it is an outlier is based on the number\nof other entities in its neighborhood. In recent years, several solutions have\ntackled the problem of distance-based outliers in data streams, where outliers\nmust be mined continuously as new elements become available. An interesting\nresearch problem is to combine the streaming environment with massively\nparallel systems to provide scalable streambased algorithms. However, none of\nthe previously proposed techniques refer to a massively parallel setting. Our\nproposal fills this gap and investigates the challenges in transferring\nstate-of-the-art techniques to Apache Flink, a modern platform for intensive\nstreaming analytics. We thoroughly present the technical challenges encountered\nand the alternatives that may be applied. We show speed-ups of up to 117 (resp.\n2076) times over a naive parallel (resp. non-parallel) solution in Flink, by\nusing just an ordinary four-core machine and a real-world dataset. When moving\nto a three-machine cluster, due to less contention, we manage to achieve both\nbetter scalability in terms of the window slide size and the data\ndimensionality, and even higher speed-ups, e.g., by a factor of 510. Overall,\nour results demonstrate that oulier mining can be achieved in an efficient and\nscalable manner. The resulting techniques have been made publicly available as\nopen-source software.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 07:51:51 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Toliopoulos", "Theodoros", ""], ["Gounaris", "Anastasios", ""], ["Tsichlas", "Kostas", ""], ["Papadopoulos", "Apostolos", ""], ["Sampaio", "Sandra", ""]]}, {"id": "1902.08007", "submitter": "Maximilien Gadouleau", "authors": "Florian Bridoux, Maximilien Gadouleau, Guillaume Theyssier", "title": "Expansive Automata Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.CO math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An Automata Network is a map ${f:Q^n\\rightarrow Q^n}$ where $Q$ is a finite\nalphabet. It can be viewed as a network of $n$ entities, each holding a state\nfrom $Q$, and evolving according to a deterministic synchronous update rule in\nsuch a way that each entity only depends on its neighbors in the network's\ngraph, called interaction graph. A major trend in automata network theory is to\nunderstand how the interaction graph affects dynamical properties of $f$. In\nthis work we introduce the following property called expansivity: the\nobservation of the sequence of states at any given node is sufficient to\ndetermine the initial configuration of the whole network. Our main result is a\ncharacterization of interaction graphs that allow expansivity. Moreover, we\nshow that this property is generic among linear automata networks over such\ngraphs with large enough alphabet. We show however that the situation is more\ncomplex when the alphabet is fixed independently of the size of the interaction\ngraph: no alphabet is sufficient to obtain expansivity on all admissible\ngraphs, and only non-linear solutions exist in some cases. Finally, among other\nresults, we consider a stronger version of expansivity where we ask to\ndetermine the initial configuration from any large enough observation of the\nsystem. We show that it can be achieved for any number of nodes and naturally\ngives rise to maximum distance separable codes.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 12:45:51 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Bridoux", "Florian", ""], ["Gadouleau", "Maximilien", ""], ["Theyssier", "Guillaume", ""]]}, {"id": "1902.08018", "submitter": "Mohamed Bamakhrama", "authors": "Mohamed A. Bamakhrama, Alejandro Arrizabalaga, Frank Overman,\n  Jean-Paul Smeets, Kornel van der Sommen, Remko van der Vossen, John\n  Wagensveld", "title": "GPU Acceleration of Real-Time Control Loops", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extreme Ultraviolet (EUV) photolithography is seen as the key enabler for\nincreasing transistor density in the next decade. In EUV lithography, 13.5 nm\nEUV light is illuminated through a reticle, holding a pattern to be printed,\nonto a silicon wafer. This process is performed about 100 times per wafer, at a\nrate of over a hundred wafers an hour. During this process, a certain\npercentage of the light energy is converted into heat in the wafer. In turn,\nthis heat causes the wafer to deform which increases the overlay error, and as\na result, reduces the manufacturing yield. To alleviate this, we propose a firm\nreal-time control system that uses a wafer heat feed-forward model to\ncompensate for the wafer deformation. The model calculates the expected wafer\ndeformation, and then, compensates for that by adjusting the light projection\nand/or the wafer movement. However, the model computational demands are very\nhigh. As a result, it needs to be executed on dedicated HW that can perform\ncomputations quickly. To this end, we deploy Graphics Processing Units (GPUs)\nto accelerate the calculations. In order to fit the computations within the\nrequired time budgets, we combine in a novel manner multiple techniques, such\nas compression and mixed-precision arithmetic, with recent advancements in GPUs\nto build a GPU-based real-time control system. A proof-of-concept\nimplementation using NVIDIA P100 GPUs is able to deliver decompression\nthroughput of 33 GB/s and a sustained 198 GFLOP/s per GPU for mixed-precision\ndense matrix-vector multiplication.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 13:07:13 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Bamakhrama", "Mohamed A.", ""], ["Arrizabalaga", "Alejandro", ""], ["Overman", "Frank", ""], ["Smeets", "Jean-Paul", ""], ["van der Sommen", "Kornel", ""], ["van der Vossen", "Remko", ""], ["Wagensveld", "John", ""]]}, {"id": "1902.08029", "submitter": "Hayato Ushijima-Mwesigwa", "authors": "Hayato Ushijima-Mwesigwa, Jeffrey D. Hyman, Aric Hagberg, Ilya Safro,\n  Satish Karra, Carl W. Gable, Matthew R. Sweeney, and Gowri Srinivasan", "title": "Multilevel Graph Partitioning for Three-Dimensional Discrete Fracture\n  Network Flow Simulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a topology-based method for mesh-partitioning in three-dimensional\ndiscrete fracture network (DFN) simulations that take advantage of the\nintrinsic multi-level nature of a DFN. DFN models are used to simulate flow and\ntransport through low-permeability fractured media in the subsurface by\nexplicitly representing fractures as discrete entities. The governing equations\nfor flow and transport are numerically integrated on computational meshes\ngenerated on the interconnected fracture networks. Modern high-fidelity DFN\nsimulations require high-performance computing on multiple processors where\nperformance and scalability depend partially on obtaining a high-quality\npartition of the mesh to balance work-loads and minimize communication across\nall processors. The discrete structure of a DFN naturally lends itself to\nvarious graph representations. We develop two applications of the multilevel\ngraph partitioning algorithm to partition the mesh of a DFN. In the first, we\nproject a partition of the graph based on the DFN topology onto the mesh of the\nDFN and in the second, this projection is used as the initial condition for\nfurther partitioning refinement of the mesh. We compare the performance of\nthese methods with standard multi-level graph partitioning using graph-based\nmetrics (cut, imbalance, partitioning time), computational-based metrics\n(FLOPS, iterations, solver time), and total run time. The DFN-based and the\nmesh-based partitioning methods are comparable in terms of the graph-based\nmetrics, but the time required to obtain the partition is several orders of\nmagnitude faster using the DFN-based partitions. In combination, these\npartitions are several orders of magnitude faster than the mesh-based\npartition. In turn, this hybrid method outperformed both of the other methods\nin terms of the total run time.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 22:12:11 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 22:57:24 GMT"}, {"version": "v3", "created": "Thu, 1 Apr 2021 21:18:44 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Ushijima-Mwesigwa", "Hayato", ""], ["Hyman", "Jeffrey D.", ""], ["Hagberg", "Aric", ""], ["Safro", "Ilya", ""], ["Karra", "Satish", ""], ["Gable", "Carl W.", ""], ["Sweeney", "Matthew R.", ""], ["Srinivasan", "Gowri", ""]]}, {"id": "1902.08040", "submitter": "Tahar Kechadi M", "authors": "I.K. Savvas, M. Tahar Kechadi", "title": "Dynamic task scheduling in computing cluster environments", "comments": "Third International Symposium on Parallel and Distributed\n  Computing/Third International Workshop on Algorithms, Models and Tools for\n  Parallel Computing on Heterogeneous Networks", "journal-ref": null, "doi": "10.1109/ISPDC.2004.21", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, a cluster-computing environment is employed as a computational\nplatform. In order to increase the efficiency of the system, a dynamic task\nscheduling algorithm is proposed, which balances the load among the nodes of\nthe cluster. The technique is dynamic, nonpreemptive, adaptive, and it uses a\nmixed centralised and decentralised policies. Based on the divide and conquer\nprinciple, the algorithm models the cluster as hyper-grids and then balances\nthe load among them. Recursively, the hyper-grids of dimension k are divided\ninto grids of dimensions k - 1, until the dimension is 1. Then, all the nodes\nof the cluster are almost equally loaded. The optimum dimension of the\nhyper-grid is chosen in order to achieve the best performance. The simulation\nresults show the effective use of the algorithm. In addition, we determined the\ncritical points (lower bounds) in which the algorithm can to be triggered.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 13:32:50 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Savvas", "I. K.", ""], ["Kechadi", "M. Tahar", ""]]}, {"id": "1902.08042", "submitter": "Will Rosenbaum", "authors": "Johannes Bund, Christoph Lenzen, Will Rosenbaum", "title": "Fault Tolerant Gradient Clock Synchronization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synchronizing clocks in distributed systems is well-understood, both in terms\nof fault-tolerance in fully connected systems and the dependence of local and\nglobal worst-case skews (i.e., maximum clock difference between neighbors and\narbitrary pairs of nodes, respectively) on the diameter of fault-free systems.\nHowever, so far nothing non-trivial is known about the local skew that can be\nachieved in topologies that are not fully connected even under a single\nByzantine fault. Put simply, in this work we show that the most powerful known\ntechniques for fault-tolerant and gradient clock synchronization are\ncompatible, in the sense that the best of both worlds can be achieved\nsimultaneously.\n  Concretely, we combine the Lynch-Welch algorithm [Welch1988] for\nsynchronizing a clique of $n$ nodes despite up to $f<n/3$ Byzantine faults with\nthe gradient clock synchronization (GCS) algorithm by Lenzen et al.\n[Lenzen2010] in order to render the latter resilient to faults. As this is not\npossible on general graphs, we augment an input graph $\\mathcal{G}$ by\nreplacing each node by $3f+1$ fully connected copies, which execute an instance\nof the Lynch-Welch algorithm. We then interpret these clusters as supernodes\nexecuting the GCS algorithm, where for each cluster its correct nodes'\nLynch-Welch clocks provide estimates of the logical clock of the supernode in\nthe GCS algorithm. By connecting clusters corresponding to neighbors in\n$\\mathcal{G}$ in a fully bipartite manner, supernodes can inform each other\nabout (estimates of) their logical clock values. This way, we achieve\nasymptotically optimal local skew, granted that no cluster contains more than\n$f$ faulty nodes, at factor $O(f)$ and $O(f^2)$ overheads in terms of nodes and\nedges, respectively. Note that tolerating $f$ faulty neighbors trivially\nrequires degree larger than $f$, so this is asymptotically optimal as well.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 13:35:08 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Bund", "Johannes", ""], ["Lenzen", "Christoph", ""], ["Rosenbaum", "Will", ""]]}, {"id": "1902.08069", "submitter": "Will Rosenbaum", "authors": "Avery Miller, Boaz Patt-Shamir, Will Rosenbaum", "title": "With Great Speed Come Small Buffers: Space-Bandwidth Tradeoffs for\n  Routing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Adversarial Queuing Theory (AQT) model, where packet arrivals\nare subject to a maximum average rate $0\\le\\rho\\le1$ and burstiness\n$\\sigma\\ge0$. In this model, we analyze the size of buffers required to avoid\noverflows in the basic case of a path. Our main results characterize the space\nrequired by the average rate and the number of distinct destinations: we show\nthat $O(k d^{1/k})$ space suffice, where $d$ is the number of distinct\ndestinations and $k=\\lfloor 1/\\rho \\rfloor$; and we show that $\\Omega(\\frac 1 k\nd^{1/k})$ space is necessary. For directed trees, we describe an algorithm\nwhose buffer space requirement is at most $1 + d' + \\sigma$ where $d'$ is the\nmaximum number of destinations on any root-leaf path.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 14:31:25 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Miller", "Avery", ""], ["Patt-Shamir", "Boaz", ""], ["Rosenbaum", "Will", ""]]}, {"id": "1902.08422", "submitter": "EPTCS", "authors": "Manfred Schmidt-Schau{\\ss} (Goethe-University Frankfurt am Main), Nils\n  Dallmeyer (Goethe-University Frankfurt am Main)", "title": "Optimizing Space of Parallel Processes", "comments": "In Proceedings WPTE 2018, arXiv:1902.07818", "journal-ref": "EPTCS 289, 2019, pp. 53-67", "doi": "10.4204/EPTCS.289.4", "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is a contribution to exploring and analyzing space-improvements in\nconcurrent programming languages, in particular in the functional\nprocess-calculus CHF. Space-improvements are defined as a generalization of the\ncorresponding notion in deterministic pure functional languages. The main part\nof the paper is the O(n*log n) algorithm SpOptN for offline space optimization\nof several parallel independent processes. Applications of this algorithm are:\n(i) affirmation of space improving transformations for particular classes of\nprogram transformations; (ii) support of an interpreter-based method for\nrefuting space-improvements; and (iii) as a stand-alone offline-optimizer for\nspace (or similar resources) of parallel processes.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2019 10:09:36 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Schmidt-Schau\u00df", "Manfred", "", "Goethe-University Frankfurt am Main"], ["Dallmeyer", "Nils", "", "Goethe-University Frankfurt am Main"]]}, {"id": "1902.08447", "submitter": "Andrea Borghesi", "authors": "Andrea Borghesi, Antonio Libri, Luca Benini, Andrea Bartolini", "title": "Online Anomaly Detection in HPC Systems", "comments": "Preprint of paper submitted and accepted AICAS2019 Conference (1st\n  IEEE International Conference on Artificial Intelligence Circuits and\n  Systems)", "journal-ref": "2019 IEEE International Conference on Artificial Intelligence\n  Circuits and Systems (AICAS), Hsinchu, Taiwan, 2019, pp. 229-233", "doi": "10.1109/AICAS.2019.8771527", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliability is a cumbersome problem in High Performance Computing Systems and\nData Centers evolution. During operation, several types of fault conditions or\nanomalies can arise, ranging from malfunctioning hardware to improper\nconfigurations or imperfect software. Currently, system administrator and final\nusers have to discover it manually. Clearly this approach does not scale to\nlarge scale supercomputers and facilities: automated methods to detect faults\nand unhealthy conditions is needed. Our method uses a type of neural network\ncalled autoncoder trained to learn the normal behavior of a real, in-production\nHPC system and it is deployed on the edge of each computing node. We obtain a\nvery good accuracy (values ranging between 90% and 95%) and we also demonstrate\nthat the approach can be deployed on the supercomputer nodes without negatively\naffecting the computing units performance.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2019 11:45:03 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Borghesi", "Andrea", ""], ["Libri", "Antonio", ""], ["Benini", "Luca", ""], ["Bartolini", "Andrea", ""]]}, {"id": "1902.08505", "submitter": "Nibesh Shrestha", "authors": "Nibesh Shrestha, Mohan Kumar, SiSi Duan", "title": "Revisiting hBFT: Speculative Byzantine Fault Tolerance with Minimum Cost", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FaB Paxos[5] sets a lower bound of 5f + 1 replicas for any two-step consensus\nprotocols tolerating f byzantine failures. Yet, hBFT[3] promises a two-step\nconsensus protocol with only 3f + 1 replicas. As a result, it violates safety\nproperty of a consensus protocol. In this note, we review the lower bound set\nby FaB Paxos and present a simple execution scenario that produces a safety\nviolation in hBFT. To demonstrate the scenario, we require a relatively simple\nsetup with only 4 replicas and one view-change.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2019 14:21:49 GMT"}, {"version": "v2", "created": "Sat, 27 Apr 2019 21:23:43 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Shrestha", "Nibesh", ""], ["Kumar", "Mohan", ""], ["Duan", "SiSi", ""]]}, {"id": "1902.08653", "submitter": "Kaipeng Li", "authors": "Kaipeng Li, Oscar Castaneda, Charles Jeon, Joseph R. Cavallaro,\n  Christoph Studer", "title": "Decentralized Coordinate-Descent Data Detection and Precoding for\n  Massive MU-MIMO", "comments": "To appear in a conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive multiuser (MU) multiple-input multiple-output (MIMO) promises\nsignificant improvements in spectral efficiency compared to small-scale MIMO.\nTypical massive MU-MIMO base-station (BS) designs rely on centralized linear\ndata detectors and precoders which entail excessively high complexity,\ninterconnect data rates, and chip input/output (I/O) bandwidth when executed on\na single computing fabric. To resolve these complexity and bandwidth\nbottlenecks, we propose new decentralized algorithms for data detection and\nprecoding that use coordinate descent. Our methods parallelize computations\nacross multiple computing fabrics, while minimizing interconnect and I/O\nbandwidth. The proposed decentralized algorithms achieve near-optimal\nerror-rate performance and multi-Gbps throughput at sub-1 ms latency when\nimplemented on a multi-GPU cluster with half-precision floating-point\narithmetic.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2019 19:47:48 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Li", "Kaipeng", ""], ["Castaneda", "Oscar", ""], ["Jeon", "Charles", ""], ["Cavallaro", "Joseph R.", ""], ["Studer", "Christoph", ""]]}, {"id": "1902.08730", "submitter": "Rong Zhu", "authors": "Rong Zhu, Kun Zhao, Hongxia Yang, Wei Lin, Chang Zhou, Baole Ai, Yong\n  Li, Jingren Zhou", "title": "AliGraph: A Comprehensive Graph Neural Network Platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An increasing number of machine learning tasks require dealing with large\ngraph datasets, which capture rich and complex relationship among potentially\nbillions of elements. Graph Neural Network (GNN) becomes an effective way to\naddress the graph learning problem by converting the graph data into a low\ndimensional space while keeping both the structural and property information to\nthe maximum extent and constructing a neural network for training and\nreferencing. However, it is challenging to provide an efficient graph storage\nand computation capabilities to facilitate GNN training and enable development\nof new GNN algorithms. In this paper, we present a comprehensive graph neural\nnetwork system, namely AliGraph, which consists of distributed graph storage,\noptimized sampling operators and runtime to efficiently support not only\nexisting popular GNNs but also a series of in-house developed ones for\ndifferent scenarios. The system is currently deployed at Alibaba to support a\nvariety of business scenarios, including product recommendation and\npersonalized search at Alibaba's E-Commerce platform. By conducting extensive\nexperiments on a real-world dataset with 492.90 million vertices, 6.82 billion\nedges and rich attributes, AliGraph performs an order of magnitude faster in\nterms of graph building (5 minutes vs hours reported from the state-of-the-art\nPowerGraph platform). At training, AliGraph runs 40%-50% faster with the novel\ncaching strategy and demonstrates around 12 times speed up with the improved\nruntime. In addition, our in-house developed GNN models all showcase their\nstatistically significant superiorities in terms of both effectiveness and\nefficiency (e.g., 4.12%-17.19% lift by F1 scores).\n", "versions": [{"version": "v1", "created": "Sat, 23 Feb 2019 03:45:31 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Zhu", "Rong", ""], ["Zhao", "Kun", ""], ["Yang", "Hongxia", ""], ["Lin", "Wei", ""], ["Zhou", "Chang", ""], ["Ai", "Baole", ""], ["Li", "Yong", ""], ["Zhou", "Jingren", ""]]}, {"id": "1902.08819", "submitter": "Gal Oren", "authors": "Leonid Barenboim, Gal Oren", "title": "Fast Distributed Backup Placement in Sparse and Dense Networks", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Backup Placement problem in networks in the\n$\\mathcal{CONGEST}$ distributed setting. Given a network graph $G = (V,E)$, the\ngoal of each vertex $v \\in V$ is selecting a neighbor, such that the maximum\nnumber of vertices in $V$ that select the same vertex is minimized. The backup\nplacement problem was introduced by Halldorsson, Kohler, Patt-Shamir, and\nRawitz, who obtained an $O(\\log n/ \\log \\log n)$ approximation with randomized\npolylogarithmic time. Their algorithm remained the state-of-the-art for general\ngraphs, as well as specific graph topologies. In this paper we obtain\nsignificantly improved algorithms for various graph topologies. Specifically,\nwe show that $O(1)$-approximation to optimal backup placement can be computed\ndeterministically in $O(1)$ rounds in graphs that model wireless networks,\ncertain social networks, claw-free graphs, and more generally, in any graph\nwith neighborhood independence bounded by a constant. At the other end, we\nconsider sparse graphs, such as trees, forests, planar graphs and graphs of\nconstant arboricity, and obtain a constant approximation to optimal backup\nplacement in $O(\\log n)$ deterministic rounds.\n  Clearly, our constant-time algorithms for graphs with constant neighborhood\nindependence are asymptotically optimal. Moreover, we show that our algorithms\nfor sparse graphs are not far from optimal as well, by proving several lower\nbounds. Specifically, optimal backup placement of unoriented trees requires\n$\\Omega(\\log n)$ time, and approximate backup placement with a polylogarithmic\napproximation factor requires $\\Omega(\\sqrt {\\log n / \\log \\log n})$ time. Our\nresults extend the knowledge regarding the question of \"what can be computed\nlocally?\", and reveal surprising gaps between complexities of distributed\nsymmetry breaking problems.\n", "versions": [{"version": "v1", "created": "Sat, 23 Feb 2019 17:45:17 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2019 19:28:40 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Barenboim", "Leonid", ""], ["Oren", "Gal", ""]]}, {"id": "1902.08942", "submitter": "Daniel S. Katz", "authors": "Daniel S. Katz, Patrick Aerts, Neil P. Chue Hong, Anshu Dubey, Sandra\n  Gesing, Henry J. Neeman, David E. Pearah", "title": "Sustaining Research Software: an SC18 Panel", "comments": "The 2018 International Conference for High Performance Computing,\n  Networking, Storage, and Analysis (SC18), Dallas, Texas, USA, November 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CY cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many science advances have been possible thanks to the use of research\nsoftware, which has become essential to advancing virtually every Science,\nTechnology, Engineering and Mathematics (STEM) discipline and many non-STEM\ndisciplines including social sciences and humanities. And while much of it is\nmade available under open source licenses, work is needed to develop, support,\nand sustain it, as underlying systems and software as well as user needs\nevolve.\n  In addition, the changing landscape of high-performance computing (HPC)\nplatforms, where performance and scaling advances are ever more reliant on\nsoftware and algorithm improvements as we hit hardware scaling barriers, is\ncausing renewed tension between sustainability of software and its performance.\nWe must do more to highlight the trade-off between performance and\nsustainability, and to emphasize the need for sustainability given the fact\nthat complex software stacks don't survive without frequent maintenance; made\nmore difficult as a generation of developers of established and heavily-used\nresearch software retire. Several HPC forums are doing this, and it has become\nan active area of funding as well.\n  In response, the authors organized and ran a panel at the SC18 conference.\nThe objectives of the panel were to highlight the importance of sustainability,\nto illuminate the tension between pure performance and sustainability, and to\nsteer SC community discussion toward understanding and addressing this issue\nand this tension. The outcome of the discussions, as presented in this paper,\ncan inform choices of advance compute and data infrastructures to positively\nimpact future research software and future research.\n", "versions": [{"version": "v1", "created": "Sun, 24 Feb 2019 13:29:35 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Katz", "Daniel S.", ""], ["Aerts", "Patrick", ""], ["Hong", "Neil P. Chue", ""], ["Dubey", "Anshu", ""], ["Gesing", "Sandra", ""], ["Neeman", "Henry J.", ""], ["Pearah", "David E.", ""]]}, {"id": "1902.08979", "submitter": "Suman Sourav", "authors": "John Augustine, Seth Gilbert, Fabian Kuhn, Peter Robinson, Suman\n  Sourav", "title": "Latency, Capacity, and Distributed MST", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the cost of distributed MST construction in the setting where each\nedge has a latency and a capacity, along with the weight. Edge latencies\ncapture the delay on the links of the communication network, while capacity\ncaptures their throughput (in this case, the rate at which messages can be\nsent). Depending on how the edge latencies relate to the edge weights, we\nprovide several tight bounds on the time and messages required to construct an\nMST.\n  When edge weights exactly correspond with the latencies, we show that,\nperhaps interestingly, the bottleneck parameter in determining the running time\nof an algorithm is the total weight $W$ of the MST (rather than the total\nnumber of nodes $n$, as in the standard CONGEST model). That is, we show a\ntight bound of $\\tilde{\\Theta}(D + \\sqrt{W/c})$ rounds, where $D$ refers to the\nlatency diameter of the graph, $W$ refers to the total weight of the\nconstructed MST and edges have capacity $c$. The proposed algorithm sends\n$\\tilde{O}(m+W)$ messages, where $m$, the total number of edges in the network\ngraph under consideration, is a known lower bound on message complexity for MST\nconstruction. We also show that $\\Omega(W)$ is a lower bound for fast MST\nconstructions.\n  When the edge latencies and the corresponding edge weights are unrelated, and\neither can take arbitrary values, we show that (unlike the sub-linear time\nalgorithms in the standard CONGEST model, on small diameter graphs), the best\ntime complexity that can be achieved is $\\tilde{\\Theta}(D+n/c)$. However, if we\nrestrict all edges to have equal latency $\\ell$ and capacity $c$ while having\npossibly different weights (weights could deviate arbitrarily from $\\ell$), we\ngive an algorithm that constructs an MST in $\\tilde{O}(D + \\sqrt{n\\ell/c})$\ntime. In each case, we provide nearly matching upper and lower bounds.\n", "versions": [{"version": "v1", "created": "Sun, 24 Feb 2019 16:59:37 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 17:24:29 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Augustine", "John", ""], ["Gilbert", "Seth", ""], ["Kuhn", "Fabian", ""], ["Robinson", "Peter", ""], ["Sourav", "Suman", ""]]}, {"id": "1902.09046", "submitter": "David Warne", "authors": "David J. Warne (1), Scott A. Sisson (2), Christopher Drovandi (1) ((1)\n  Queensland University of Technology, (2) University of New South Wales)", "title": "Vector operations for accelerating expensive Bayesian computations -- a\n  tutorial guide", "comments": null, "journal-ref": null, "doi": "10.1214/21-BA1265", "report-no": null, "categories": "stat.CO cs.DC cs.MS cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications in Bayesian statistics are extremely computationally\nintensive. However, they are often inherently parallel, making them prime\ntargets for modern massively parallel processors. Multi-core and distributed\ncomputing is widely applied in the Bayesian community, however, very little\nattention has been given to fine-grain parallelisation using single instruction\nmultiple data (SIMD) operations that are available on most modern commodity\nCPUs and is the basis of GPGPU computing. In this work, we practically\ndemonstrate, using standard programming libraries, the utility of the SIMD\napproach for several topical Bayesian applications. We show that SIMD can\nimprove the floating point arithmetic performance resulting in up to $6\\times$\nimprovement in serial algorithm performance. Importantly, these improvements\nare multiplicative to any gains achieved through multi-core processing. We\nillustrate the potential of SIMD for accelerating Bayesian computations and\nprovide the reader with techniques for exploiting modern massively parallel\nprocessing environments using standard tools.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 00:38:23 GMT"}, {"version": "v2", "created": "Fri, 12 Jul 2019 08:31:45 GMT"}, {"version": "v3", "created": "Mon, 14 Dec 2020 06:22:53 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Warne", "David J.", ""], ["Sisson", "Scott A.", ""], ["Drovandi", "Christopher", ""]]}, {"id": "1902.09327", "submitter": "Kristina Spirovska", "authors": "Kristina Spirovska, Diego Didona, Willy Zwaenepoel", "title": "PaRiS: Causally Consistent Transactions with Non-blocking Reads and\n  Partial Replication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geo-replicated data platforms are at the backbone of several large-scale\nonline services. Transactional Causal Consistency (TCC) is an attractive\nconsistency level for building such platforms. TCC avoids many anomalies of\neventual consistency, eschews the synchronization costs of strong consistency,\nand supports interactive read-write transactions. Partial replication is\nanother attractive design choice for building geo-replicated platforms, as it\nincreases the storage capacity and reduces update propagation costs. This paper\npresents PaRiS, the first TCC system that supports partial replication and\nimplements non-blocking parallel read operations, whose latency is paramount\nfor the performance of read-intensive applications. PaRiS relies on a novel\nprotocol to track dependencies, called Universal Stable Time (UST). By means of\na lightweight background gossip process, UST identifies a snapshot of the data\nthat has been installed by every DC in the system. Hence, transactions can\nconsistently read from such a snapshot on any server in any replication site\nwithout having to block. Moreover, PaRiS requires only one timestamp to track\ndependencies and define transactional snapshots, thereby achieving resource\nefficiency and scalability. We evaluate PaRiS on a large-scale AWS deployment\ncomposed of up to 10 replication sites. We show that PaRiS scales well with the\nnumber of DCs and partitions, while being able to handle larger data-sets than\nexisting solutions that assume full replication. We also demonstrate a\nperformance gain of non-blocking reads vs. a blocking alternative (up to 1.47x\nhigher throughput with 5.91x lower latency for read-dominated workloads and up\nto 1.46x higher throughput with 20.56x lower latency for write-heavy\nworkloads).\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 14:59:30 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Spirovska", "Kristina", ""], ["Didona", "Diego", ""], ["Zwaenepoel", "Willy", ""]]}, {"id": "1902.09377", "submitter": "Ran Ben Basat", "authors": "Ran Ben-Basat and Guy Even and Ken-ichi Kawarabayashi and Gregory\n  Schwartzman", "title": "Optimal Distributed Covering Algorithms", "comments": "This paper extends and improves arXiv:1808.05809 which we keep\n  separate as it is considerably simpler", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a time-optimal deterministic distributed algorithm for\napproximating a minimum weight vertex cover in hypergraphs of rank $f$. This\nproblem is equivalent to the Minimum Weight Set Cover problem in which the\nfrequency of every element is bounded by $f$. The approximation factor of our\nalgorithm is $(f+\\epsilon)$. Our algorithm runs in the CONGEST model and\nrequires $O(\\log\\Delta/ \\log\\log\\Delta)$ rounds, for constants\n$\\epsilon\\in(0,1]$ and $f\\in N^+$. This is the first distributed algorithm for\nthis problem whose running time does not depend on the vertex weights nor the\nnumber of vertices. For constant values of $f$ and $\\epsilon$, our algorithm\nimproves over the $(f+\\epsilon)$-approximation algorithm of KMW06 whose running\ntime is $O(\\log \\Delta + \\log W)$, where $W$ is the ratio between the largest\nand smallest vertex weights in the graph. Our algorithm also achieves an\n$f$-approximation for the problem in $O(f\\log n)$ rounds, improving over the\nclassical result of KVY94 that achieves a running time of $O(f\\log^2 n)$.\nFinally, for weighted vertex cover ($f=2$) our algorithm achieves a\n\\emph{deterministic} running time of $O(\\log n)$, matching the\n\\emph{randomized} previously best result of KY11. We also show that integer\ncovering-programs can be reduced to the Minimum Weight Set Cover problem in the\ndistributed setting. This allows us to achieve an $(f+\\epsilon)$-approximate\nintegral solution in $O(\\frac{\\log\\Delta}{\\log\\log\\Delta}+(f\\cdot\\log\nM)^{1.01}\\log\\epsilon^{-1}(\\log\\Delta)^{0.01})$ rounds, where $f$ bounds the\nnumber of variables in a constraint, $\\Delta$ bounds the number of constraints\na variable appears in, and $M=\\max \\{1, 1/a_{\\min}\\}$, where $a_{min}$ is the\nsmallest normalized constraint coefficient. This improves over the results of\nKMW06 for the integral case, which runs in $O(\\epsilon^{-4}\\cdot f^4\\cdot \\log\nf\\cdot\\log(M\\cdot\\Delta))$ rounds.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 15:45:13 GMT"}, {"version": "v2", "created": "Thu, 30 May 2019 12:37:30 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Ben-Basat", "Ran", ""], ["Even", "Guy", ""], ["Kawarabayashi", "Ken-ichi", ""], ["Schwartzman", "Gregory", ""]]}, {"id": "1902.09468", "submitter": "Tommaso Polonelli", "authors": "Tommaso Polonelli, Davide Brunelli, Achille Marzocchi, Luca Benini", "title": "Slotted ALOHA on LoRaWAN - Design, Analysis, and Deployment", "comments": "19 pages", "journal-ref": "Special Issue \"Low Energy Wireless Sensor Networks: Protocols,\n  Architectures and Solutions\" January 2019", "doi": "10.3390/s19040838", "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LoRaWAN is one of the most promising standards for long-range sensing\napplications. However, the high number of end devices expected in at-scale\ndeployment, combined with the absence of an effective synchronization scheme,\nchallenge the scalability of this standard. In this paper, we present an\napproach to increase network throughput through a Slotted-ALOHA overlay on\nLoRaWAN networks. To increase the single channel capacity, we propose to\nregulate the communication of LoRaWAN networks using a Slotted-ALOHA variant on\nthe top of the Pure-ALOHA approach used by the standard; thus, no modification\nin pre-existing libraries is necessary. Our method is based on an innovative\nsynchronization service that is suitable for low-cost wireless sensor nodes. We\nmodelled the LoRaWAN channel with extensive measurement on hardware platforms,\nand we quantified the impact of tuning parameters on physical and medium access\ncontrol layers, as well as the packet collision rate. Results show that\nSlotted-ALOHA supported by our synchronization service significantly improves\nthe performance of traditional LoRaWAN networks regarding packet loss rate and\nnetwork throughput.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 14:03:26 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Polonelli", "Tommaso", ""], ["Brunelli", "Davide", ""], ["Marzocchi", "Achille", ""], ["Benini", "Luca", ""]]}, {"id": "1902.09472", "submitter": "Gewu Bu", "authors": "Gewu Bu (NPA, LIP6, UPMC), \\\"Onder G\\\"urcan (DILS), Maria\n  Potop-Butucaru (LIP6, NPA, UPMC, LINCS, MIMOVE)", "title": "G-IOTA: Fair and confidence aware tangle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes strategies to improve the IOTA tangle in terms of\nresilience to splitting attacks. Our contribution is two fold. First, we define\nthe notion of confidence fairness for tips selection algorithms to guarantee\nthe first approval for all honest tips. Then, we analyze IOTA-tangle from the\npoint of view of confidence fairness and identify its drawbacks. Second, we\npropose a new selection mechanism, G-IOTA, that targets to protect tips left\nbehind. G-IOTA therefore has a good confidence fairness. G-IOTA lets honest\ntransactions increase their confidence efficiently. Furthermore, G-IOTA\nincludes an incentive mechanism for users who respect the algorithm and\npunishes conflicting transactions. Additionally, G-IOTA provides a mutual\nsupervision mechanism that reduces the benefits of speculative and lazy\nbehaviours.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 14:48:11 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Bu", "Gewu", "", "NPA, LIP6, UPMC"], ["G\u00fcrcan", "\u00d6nder", "", "DILS"], ["Potop-Butucaru", "Maria", "", "LIP6, NPA, UPMC, LINCS, MIMOVE"]]}, {"id": "1902.09493", "submitter": "Christian M. Fuchs", "authors": "Christian M. Fuchs, Nadia M. Murillo, Aske Plaat, Erik van der Kouwe,\n  Todor Stefanov", "title": "Dynamic Fault Tolerance Through Resource Pooling", "comments": null, "journal-ref": "2018 NASA/ESA Conference on Adaptive Hardware and Systems (AHS)", "doi": "10.1109/AHS.2018.8541457", "report-no": null, "categories": "cs.DC cs.OS cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Miniaturized satellites are currently not considered suitable for critical,\nhigh-priority, and complex multi-phased missions, due to their low reliability.\nAs hardware-side fault tolerance (FT) solutions designed for larger spacecraft\ncan not be adopted aboard very small satellites due to budget, energy, and size\nconstraints, we developed a hybrid FT-approach based upon only COTS components,\ncommodity processor cores, library IP, and standard software. This approach\nfacilitates fault detection, isolation, and recovery in software, and utilizes\nfault-coverage techniques across the embedded stack within an multiprocessor\nsystem-on-chip (MPSoC). This allows our FPGA-based proof-of-concept\nimplementation to deliver strong fault-coverage even for missions with a long\nduration, but also to adapt to varying performance requirements during the\nmission. The operator of a spacecraft utilizing this approach can define\nperformance profiles, which allow an on-board computer (OBC) to trade between\nprocessing capacity, fault coverage, and energy consumption using simple\nheuristics. The software-side FT approach developed also offers advantages if\ndeployed aboard larger spacecraft through spare resource pooling, enabling an\nOBC to more efficiently handle permanent faults. This FT approach in part\nmimics a critical biological systems's way of tolerating and adjusting to\nfailures, enabling graceful ageing of an MPSoC.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2019 00:39:43 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Fuchs", "Christian M.", ""], ["Murillo", "Nadia M.", ""], ["Plaat", "Aske", ""], ["van der Kouwe", "Erik", ""], ["Stefanov", "Todor", ""]]}, {"id": "1902.09520", "submitter": "Adam Bennet J", "authors": "Adam J Bennet, Shakib Daryanoosh", "title": "Energy efficient mining on a quantum-enabled blockchain using light", "comments": "25 pages, 5 figures", "journal-ref": null, "doi": "10.5195/ledger.2019.143", "report-no": null, "categories": "quant-ph cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We outline a quantum-enabled blockchain architecture based on a consortium of\nquantum servers. The network is hybridised, utilising digital systems for\nsharing and processing classical information combined with a fibre--optic\ninfrastructure and quantum devices for transmitting and processing quantum\ninformation. We deliver an energy efficient interactive mining protocol enacted\nbetween clients and servers which uses quantum information encoded in light and\nremoves the need for trust in network infrastructure. Instead, clients on the\nnetwork need only trust the transparent network code, and that their devices\nadhere to the rules of quantum physics. To demonstrate the energy efficiency of\nthe mining protocol, we elaborate upon the results of two previous experiments\n(one performed over 1km of optical fibre) as applied to this work. Finally, we\naddress some key vulnerabilities, explore open questions, and observe\nforward--compatibility with the quantum internet and quantum computing\ntechnologies.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 18:59:32 GMT"}, {"version": "v2", "created": "Wed, 27 Feb 2019 06:44:46 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Bennet", "Adam J", ""], ["Daryanoosh", "Shakib", ""]]}, {"id": "1902.09527", "submitter": "Disa Mhembere", "authors": "Disa Mhembere, Da Zheng, Carey E. Priebe, Joshua T. Vogelstein, Randal\n  Burns", "title": "clusterNOR: A NUMA-Optimized Clustering Framework", "comments": "arXiv admin note: Journal version of arXiv:1606.08905", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering algorithms are iterative and have complex data access patterns\nthat result in many small random memory accesses. The performance of parallel\nimplementations suffer from synchronous barriers for each iteration and skewed\nworkloads. We rethink the parallelization of clustering for modern non-uniform\nmemory architectures (NUMA) to maximizes independent, asynchronous computation.\nWe eliminate many barriers, reduce remote memory accesses, and maximize cache\nreuse. We implement the 'Clustering NUMA Optimized Routines' (clusterNOR)\nextensible parallel framework that provides algorithmic building blocks. The\nsystem is generic, we demonstrate nine modern clustering algorithms that have\nsimple implementations. clusterNOR includes (i) in-memory, (ii) semi-external\nmemory, and (iii) distributed memory execution, enabling computation for\nvarying memory and hardware budgets. For algorithms that rely on Euclidean\ndistance, clusterNOR defines an updated Elkan's triangle inequality pruning\nalgorithm that uses asymptotically less memory so that it works on\nbillion-point data sets. clusterNOR extends and expands the scope of the 'knor'\nlibrary for k-means clustering by generalizing underlying principles, providing\na uniform programming interface and expanding the scope to hierarchical and\nlinear algebraic classes of algorithms. The compound effect of our\noptimizations is an order of magnitude improvement in speed over other\nstate-of-the-art solutions, such as Spark's MLlib and Apple's Turi.\n", "versions": [{"version": "v1", "created": "Sun, 24 Feb 2019 06:28:21 GMT"}, {"version": "v2", "created": "Sat, 16 Mar 2019 02:39:43 GMT"}, {"version": "v3", "created": "Fri, 12 Apr 2019 06:44:57 GMT"}, {"version": "v4", "created": "Mon, 15 Apr 2019 05:19:55 GMT"}, {"version": "v5", "created": "Sat, 22 Feb 2020 01:33:24 GMT"}, {"version": "v6", "created": "Sun, 17 Jan 2021 22:50:55 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Mhembere", "Disa", ""], ["Zheng", "Da", ""], ["Priebe", "Carey E.", ""], ["Vogelstein", "Joshua T.", ""], ["Burns", "Randal", ""]]}, {"id": "1902.09587", "submitter": "Ryan Shah", "authors": "Ryan Shah, Shishir Nagaraja", "title": "A Unified Access Control Model for Calibration Traceability in\n  Safety-Critical IoT", "comments": "19 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Calibration plays an important role in ensuring device accuracy within\nsafety-critical IoT deployments. The process of calibration involves a number\nof parties which must collaborate to support calibration. Calibration checks\noften precede safety-critical operations such as preparing a robot for surgery,\nrequiring inter-party interaction to complete checks. At the same time, the\nparties involved in a calibration ecosystem may share an adversarial\nrelationship with a subset of other parties. For instance, a surgical robot\nmanufacturer may wish to hide the identities of third-parties from the operator\n(hospital), in order to maintain confidentiality of business relationships\naround its robot products. Thus, information flows that reveal\nwho-calibrates-for-whom need to be managed to ensure confidentiality.\nSimilarly, information about what-is-being-calibrated and\nhow-often-it-is-calibrated may compromise operational confidentiality. For\nexample, calibration-verification of connected medical devices may reveal the\ntiming of surgical procedures and compromise PII when combined with other meta\ninformation. We show that the challenge of managing information flows between\nthe parties involved in calibration cannot be met by any of the classical\naccess control models, as any one of them or a simple conjunction of a subset\nsuch as the lattice model fails to meet the desired access control\nrequirements. We demonstrate that a new unified access control model that\ncombines BIBA, BLP, and Chinese Walls holds rich promise. We study the case for\nunification, system properties, and develop an XACML-based authorisation\nframework which enforces the unified model. Upon evaluation against a baseline\nsimple conjunction of the three models individually, our unified model\noutperforms this, demonstrating it is capable of solving the novel access\ncontrol challenges thrown up by digital-calibration supply chains.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 19:45:03 GMT"}, {"version": "v2", "created": "Thu, 21 Mar 2019 11:54:23 GMT"}, {"version": "v3", "created": "Thu, 1 Oct 2020 11:18:07 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Shah", "Ryan", ""], ["Nagaraja", "Shishir", ""]]}, {"id": "1902.09604", "submitter": "Tiago Manuel Fern\\'andez-Caram\\'es", "authors": "Tiago M. Fern\\'andez-Caram\\'es, Paula Fraga-Lamas", "title": "A Review on the Application of Blockchain for the Next Generation of\n  Cybersecure Industry 4.0 Smart Factories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Industry 4.0 is a concept devised for improving the way modern factories\noperate through the use of some of the latest technologies, like the ones used\nfor creating Industrial Internet of Things (IIoT), robotics or Big Data\napplications. One of such technologies is blockchain, which is able to add\ntrust, security and decentralization to different industrial fields. This\narticle focuses on analyzing the benefits and challenges that arise when using\nblockchain and smart contracts to develop Industry 4.0 applications. In\naddition, this paper presents a thorough review on the most relevant\nblockchain-based applications for Industry 4.0 technologies. Thus, its aim is\nto provide a detailed guide for future Industry 4.0 developers that allows for\ndetermining how blockchain can enhance the next generation of cybersecure\nindustrial applications.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 20:45:22 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Fern\u00e1ndez-Caram\u00e9s", "Tiago M.", ""], ["Fraga-Lamas", "Paula", ""]]}, {"id": "1902.09636", "submitter": "Richard Mortier", "authors": "Masoud Koleini, Carlos Oviedo, Derek McAuley, Charalampos Rotsos, Anil\n  Madhavapeddy, Thomas Gazagnaire, Magnus Skejgstad, Richard Mortier", "title": "Fractal: Automated Application Scaling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To date, cloud applications have used datacenter resources through manual\nconfiguration and deployment of virtual machines and containers. Current trends\nsee increasing use of microservices, where larger applications are split into\nmany small containers, to be developed and deployed independently. However,\neven with the rise of the devops movement and orchestration facilities such as\nKubernetes, there is a tendency to separate development from deployment. We\npresent an exploration of a more extreme point on the devops spectrum: Fractal.\nDevelopers embed orchestration logic inside their application, fully automating\nthe processes of scaling up and down. Providing a set of extensions to and an\nAPI over the Jitsu platform, we outline the design of Fractal and describe the\nkey features of its implementation: how an application is self-replicated, how\nreplica lifecycles are managed, how failure recovery is handled, and how\nnetwork traffic is transparently distributed between replicas. We present\nevaluation of a self-scaling website, and demonstrate that Fractal is both\nuseful and feasible.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 22:00:48 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Koleini", "Masoud", ""], ["Oviedo", "Carlos", ""], ["McAuley", "Derek", ""], ["Rotsos", "Charalampos", ""], ["Madhavapeddy", "Anil", ""], ["Gazagnaire", "Thomas", ""], ["Skejgstad", "Magnus", ""], ["Mortier", "Richard", ""]]}, {"id": "1902.09645", "submitter": "Wojciech Krzemien", "authors": "Wojciech Krzemien, Federico Stagni, Christophe Haen, Zoltan Mathe,\n  Andrew McNab, and Milosz Zdybal", "title": "Addressing Scalability with Message Queues: Architecture and Use Cases\n  for DIRAC Interware", "comments": "5 pages, 3 figures", "journal-ref": "EPJ Web of Conferences 214, 03018 (2019)", "doi": "10.1051/epjconf/201921403018", "report-no": null, "categories": "cs.DC hep-ex", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Message Queue (MQ) architecture is an asynchronous communication scheme\nthat provides an attractive solution for certain scenarios in a distributed\ncomputing model. The introduction of MQ as an intermediate component in-between\nthe interacting processes allows to decouple the end-points making the system\nmore flexible and providing high scalability and redundancy. DIRAC is a\ngeneral-purpose interware software for distributed computing systems, which\noffers a common interface to a number of heterogeneous providers and guarantees\ntransparent and reliable usage of the resources. The DIRAC platform has been\nadapted by several scientific projects, including High Energy Physics\ncommunities like LHCb, the Linear Collider and Belle2. A Message Queue generic\ninterface has been incorporated into the DIRAC framework to help solving the\nscalability challenges that must be addressed during LHC Run3, starting in\n2021. It allows to use the MQ scheme for a message exchange among the DIRAC\ncomponents or to communicate with third-party services. Within this\ncontribution we describe the integration of MQ systems with DIRAC and several\nuse cases are shown. Message Queues are foreseen to be used in the pilot\nlogging system, and as a backbone of the DIRAC component logging system and\nmonitoring.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 22:39:04 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Krzemien", "Wojciech", ""], ["Stagni", "Federico", ""], ["Haen", "Christophe", ""], ["Mathe", "Zoltan", ""], ["McNab", "Andrew", ""], ["Zdybal", "Milosz", ""]]}, {"id": "1902.09848", "submitter": "Andr\\'e Martin", "authors": "Sergei Arnautov, Andrey Brito, Pascal Felber, Christof Fetzer, Franz\n  Gregor, Robert Krahn, Wojciech Ozga, Andr\\'e Martin, Valerio Schiavoni,\n  F\\'abio Silva, Marcus Tenorio and Nikolaus Th\\\"ummel", "title": "PubSub-SGX: Exploiting Trusted Execution Environments for\n  Privacy-Preserving Publish/Subscribe Systems", "comments": null, "journal-ref": null, "doi": "10.1109/SRDS.2018.00023", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents PUBSUB-SGX, a content-based publish-subscribe system that\nexploits trusted execution environments (TEEs), such as Intel SGX, to guarantee\nconfidentiality and integrity of data as well as anonymity and privacy of\npublishers and subscribers. We describe the technical details of our Python\nimplementation, as well as the required system support introduced to deploy our\nsystem in a container-based runtime. Our evaluation results show that our\napproach is sound, while at the same time highlighting the performance and\nscalability trade-offs. In particular, by supporting just-in-time compilation\ninside of TEEs, Python programs inside of TEEs are in general faster than when\nexecuted natively using standard CPython.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 10:30:53 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Arnautov", "Sergei", ""], ["Brito", "Andrey", ""], ["Felber", "Pascal", ""], ["Fetzer", "Christof", ""], ["Gregor", "Franz", ""], ["Krahn", "Robert", ""], ["Ozga", "Wojciech", ""], ["Martin", "Andr\u00e9", ""], ["Schiavoni", "Valerio", ""], ["Silva", "F\u00e1bio", ""], ["Tenorio", "Marcus", ""], ["Th\u00fcmmel", "Nikolaus", ""]]}, {"id": "1902.09857", "submitter": "Mario Lassnig", "authors": "Martin Barisits, Thomas Beermann, Frank Berghaus, Brian Bockelman,\n  Joaquin Bogado, David Cameron, Dimitrios Christidis, Diego Ciangottini,\n  Gancho Dimitrov, Markus Elsing, Vincent Garonne, Alessandro di Girolamo, Luc\n  Goossens, Wen Guan, Jaroslav Guenther, Tomas Javurek, Dietmar Kuhn, Mario\n  Lassnig, Fernando Lopez, Nicolo Magini, Angelos Molfetas, Armin Nairz, Farid\n  Ould-Saada, Stefan Prenner, Cedric Serfon, Graeme Stewart, Eric Vaandering,\n  Petya Vasileva, Ralph Vigne, Tobias Wegner", "title": "Rucio - Scientific data management", "comments": "21 pages, 11 figures", "journal-ref": "Barisits, M., Beermann, T., Berghaus, F. et al. Comput Softw Big\n  Sci (2019) 3: 11", "doi": "10.1007/s41781-019-0026-3", "report-no": "2510-2044", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rucio is an open-source software framework that provides scientific\ncollaborations with the functionality to organize, manage, and access their\ndata at scale. The data can be distributed across heterogeneous data centers at\nwidely distributed locations. Rucio was originally developed to meet the\nrequirements of the high-energy physics experiment ATLAS, and now is\ncontinuously extended to support the LHC experiments and other diverse\nscientific communities. In this article, we detail the fundamental concepts of\nRucio, describe the architecture along with implementation details, and give\noperational experience from production usage.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 10:55:01 GMT"}, {"version": "v2", "created": "Thu, 6 Jun 2019 09:43:50 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Barisits", "Martin", ""], ["Beermann", "Thomas", ""], ["Berghaus", "Frank", ""], ["Bockelman", "Brian", ""], ["Bogado", "Joaquin", ""], ["Cameron", "David", ""], ["Christidis", "Dimitrios", ""], ["Ciangottini", "Diego", ""], ["Dimitrov", "Gancho", ""], ["Elsing", "Markus", ""], ["Garonne", "Vincent", ""], ["di Girolamo", "Alessandro", ""], ["Goossens", "Luc", ""], ["Guan", "Wen", ""], ["Guenther", "Jaroslav", ""], ["Javurek", "Tomas", ""], ["Kuhn", "Dietmar", ""], ["Lassnig", "Mario", ""], ["Lopez", "Fernando", ""], ["Magini", "Nicolo", ""], ["Molfetas", "Angelos", ""], ["Nairz", "Armin", ""], ["Ould-Saada", "Farid", ""], ["Prenner", "Stefan", ""], ["Serfon", "Cedric", ""], ["Stewart", "Graeme", ""], ["Vaandering", "Eric", ""], ["Vasileva", "Petya", ""], ["Vigne", "Ralph", ""], ["Wegner", "Tobias", ""]]}, {"id": "1902.09931", "submitter": "Andrew Gloster", "authors": "Andrew Gloster, Lennon O'Naraigh", "title": "cuSten -- CUDA Finite Difference and Stencil Library", "comments": "Re-upload with corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present cuSten, a new library of functions to handle the\nimplementation of 2D and batched 1D finite-difference/stencil programs in CUDA.\ncuSten wraps data handling, kernel calls and streaming into four easy to use\nfunctions that speed up development of numerical codes on GPU platforms. The\npaper also presents an example of this library applied to solve the\nCahn-Hilliard equation utilizing an ADI method with periodic boundary\nconditions, this solver is also used to benchmark the cuSten library\nperformance against a serial implementation.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 13:49:25 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 09:11:11 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Gloster", "Andrew", ""], ["O'Naraigh", "Lennon", ""]]}, {"id": "1902.09958", "submitter": "Sebastian Brandt", "authors": "Sebastian Brandt", "title": "An Automatic Speedup Theorem for Distributed Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Brandt et al. [STOC'16] proved a lower bound for the distributed\nLov\\'asz Local Lemma, which has been conjectured to be tight for sufficiently\nrelaxed LLL criteria by Chang and Pettie [FOCS'17]. At the heart of their\nresult lies a speedup technique that, for graphs of girth at least $2t+2$,\ntransforms any $t$-round algorithm for one specific LLL problem into a\n$(t-1)$-round algorithm for the same problem. We substantially improve on this\ntechnique by showing that such a speedup exists for any locally checkable\nproblem $\\Pi$, with the difference that the problem $\\Pi_1$ the inferred\n$(t-1)$-round algorithm solves is not (necessarily) the same problem as $\\Pi$.\nOur speedup is automatic in the sense that there is a fixed procedure that\ntransforms a description for $\\Pi$ into a description for $\\Pi_1$ and\nreversible in the sense that any $(t-1)$-round algorithm for $\\Pi_1$ can be\ntransformed into a $t$-round algorithm for $\\Pi$. In particular, for any\nlocally checkable problem $\\Pi$ with exact deterministic time complexity $T(n,\n\\Delta) \\leq t$ on graphs with $n$ nodes, maximum node degree $\\Delta$, and\ngirth at least $2t+2$, there is a sequence of problems $\\Pi_1, \\Pi_2, \\dots$\nwith time complexities $T(n, \\Delta)-1, T(n, \\Delta)-2, \\dots$, that can be\ninferred from $\\Pi$.\n  As a first application of our generalized speedup, we solve a long-standing\nopen problem of Naor and Stockmeyer [STOC'93]: we show that weak $2$-coloring\nin odd-degree graphs cannot be solved in $o(\\log^* \\Delta)$ rounds, thereby\nproviding a matching lower bound to their upper bound.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 14:27:52 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Brandt", "Sebastian", ""]]}, {"id": "1902.09992", "submitter": "Ruben Martinez-Cantin", "authors": "Javier Garcia-Barcos, Ruben Martinez-Cantin", "title": "Fully Distributed Bayesian Optimization with Stochastic Policies", "comments": null, "journal-ref": "Proceedings of the International Joint Conference on Artificial\n  Intelligence (IJCAI-19), 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization has become a popular method for high-throughput\ncomputing, like the design of computer experiments or hyperparameter tuning of\nexpensive models, where sample efficiency is mandatory. In these applications,\ndistributed and scalable architectures are a necessity. However, Bayesian\noptimization is mostly sequential. Even parallel variants require certain\ncomputations between samples, limiting the parallelization bandwidth. Thompson\nsampling has been previously applied for distributed Bayesian optimization.\nBut, when compared with other acquisition functions in the sequential setting,\nThompson sampling is known to perform suboptimally. In this paper, we present a\nnew method for fully distributed Bayesian optimization, which can be combined\nwith any acquisition function. Our approach considers Bayesian optimization as\na partially observable Markov decision process. In this context, stochastic\npolicies, such as the Boltzmann policy, have some interesting properties which\ncan also be studied for Bayesian optimization. Furthermore, the Boltzmann\npolicy trivially allows a distributed Bayesian optimization implementation with\nhigh level of parallelism and scalability. We present results in several\nbenchmarks and applications that shows the performance of our method.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 15:13:17 GMT"}, {"version": "v2", "created": "Fri, 5 Jul 2019 17:52:55 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Garcia-Barcos", "Javier", ""], ["Martinez-Cantin", "Ruben", ""]]}, {"id": "1902.10010", "submitter": "Daniel Collins", "authors": "Christian Cachin, Daniel Collins, Tyler Crain, Vincent Gramoli", "title": "Anonymity Preserving Byzantine Vector Consensus", "comments": "In submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collecting anonymous opinions finds various applications ranging from simple\nwhistleblowing, releasing secretive information, to complex forms of voting,\nwhere participants rank candidates by order of preferences. Unfortunately, as\nfar as we know there is no efficient distributed solution to this problem.\nPreviously, participants had to trust third parties, run expensive\ncryptographic protocols or sacrifice anonymity. In this paper, we propose a\nresilient-optimal solution to this problem called AVCP, which tolerates up to a\nthird of Byzantine participants. AVCP combines traceable ring signatures to\ndetect double votes with a reduction from vector consensus to binary consensus\nto ensure all valid votes are taken into account. We prove our algorithm\ncorrect and show that it preserves anonymity with at most a linear\ncommunication overhead and constant message overhead when compared to a recent\nconsensus baseline. Finally, we demonstrate empirically that the protocol is\npractical by deploying it on 100 machines geo-distributed in 3 continents:\nAmerica, Asia and Europe. Anonymous decisions are reached within 10 seconds\nwith a conservative choice of traceable ring signatures.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 15:46:03 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 16:28:15 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Cachin", "Christian", ""], ["Collins", "Daniel", ""], ["Crain", "Tyler", ""], ["Gramoli", "Vincent", ""]]}, {"id": "1902.10041", "submitter": "Mikhail Raskin", "authors": "Mikhail Raskin", "title": "Population protocols with unreliable communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Population protocols are a model of distributed computation intended for the\nstudy of networks of independent computing agents with dynamic communication\nstructure. Each agent has a finite number of states, and communication\nopportunities occur nondeterministically, allowing the agents involved to\nchange their states based on each other's states.\n  In the present paper we study unreliable models based on population protocols\nand their variations from the point of view of expressive power. We model the\neffects of message loss. We show that for a general definition of unreliable\nprotocols with constant-storage agents such protocols can only compute\npredicates computable by immediate observation population protocols (sometimes\nalso called one-way protocols). Immediate observation population protocols are\ninherently tolerant of unreliable communication and keep their expressive power\nunder a wide range of fairness conditions. We also prove that a large class of\nmessage-based models that are generally more expressive than immediate\nobservation becomes strictly less expressive than immediate observation in the\nunreliable case.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 16:39:24 GMT"}, {"version": "v2", "created": "Wed, 6 Mar 2019 13:17:40 GMT"}, {"version": "v3", "created": "Mon, 3 Feb 2020 12:28:27 GMT"}, {"version": "v4", "created": "Mon, 17 Feb 2020 13:56:13 GMT"}, {"version": "v5", "created": "Wed, 6 May 2020 12:59:10 GMT"}, {"version": "v6", "created": "Thu, 11 Feb 2021 12:27:10 GMT"}, {"version": "v7", "created": "Tue, 6 Jul 2021 04:03:00 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Raskin", "Mikhail", ""]]}, {"id": "1902.10069", "submitter": "Volodimir Begy", "authors": "Volodimir Begy, Joeri Hermans, Martin Barisits, Mario Lassnig, Erich\n  Schikuta", "title": "Simulating Data Access Profiles of Computational Jobs in Data Grids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The data access patterns of applications running in computing grids are\nchanging due to the recent proliferation of high speed local and wide area\nnetworks. The data-intensive jobs are no longer strictly required to run at the\ncomputing sites, where the respective input data are located. Instead, jobs may\naccess the data employing arbitrary combinations of data-placement, stage-in\nand remote data access. These data access profiles exhibit partially\nnon-overlapping throughput bottlenecks. This fact can be exploited in order to\nminimize the time jobs spend waiting for input data. In this work we present a\nnovel grid computing simulator, which puts a heavy emphasis on the various data\naccess profiles. The fundamental assumptions underlying our simulator are\njustified by empirical experiments performed in the Worldwide LHC Computing\nGrid (WLCG) at CERN. We demonstrate how to calibrate the simulator parameters\nin accordance with the true system using posterior inference with\nlikelihood-free Markov Chain Monte Carlo. Thereafter, we validate the\nsimulator's output with respect to an authentic production workload from WLCG,\ndemonstrating its remarkable accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 17:29:44 GMT"}, {"version": "v2", "created": "Tue, 12 Mar 2019 20:47:58 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Begy", "Volodimir", ""], ["Hermans", "Joeri", ""], ["Barisits", "Martin", ""], ["Lassnig", "Mario", ""], ["Schikuta", "Erich", ""]]}, {"id": "1902.10130", "submitter": "Chuangyi Gui", "authors": "Chuangyi Gui, Long Zheng, Bingsheng He, Cheng Liu, Xinyu Chen, Xiaofei\n  Liao, Hai Jin", "title": "A Survey on Graph Processing Accelerators: Challenges and Opportunities", "comments": "This article has been accepted by Journal of Computer Science and\n  Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph is a well known data structure to represent the associated\nrelationships in a variety of applications, e.g., data science and machine\nlearning. Despite a wealth of existing efforts on developing graph processing\nsystems for improving the performance and/or energy efficiency on traditional\narchitectures, dedicated hardware solutions, also referred to as graph\nprocessing accelerators, are essential and emerging to provide the benefits\nsignificantly beyond those pure software solutions can offer. In this paper, we\nconduct a systematical survey regarding the design and implementation of graph\nprocessing accelerator. Specifically, we review the relevant techniques in\nthree core components toward a graph processing accelerator: preprocessing,\nparallel graph computation and runtime scheduling. We also examine the\nbenchmarks and results in existing studies for evaluating a graph processing\naccelerator. Interestingly, we find that there is not an absolute winner for\nall three aspects in graph acceleration due to the diverse characteristics of\ngraph processing and complexity of hardware configurations. We finially present\nto discuss several challenges in details, and to further explore the\nopportunities for the future research.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 01:13:46 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Gui", "Chuangyi", ""], ["Zheng", "Long", ""], ["He", "Bingsheng", ""], ["Liu", "Cheng", ""], ["Chen", "Xinyu", ""], ["Liao", "Xiaofei", ""], ["Jin", "Hai", ""]]}, {"id": "1902.10192", "submitter": "Wei Feng", "authors": "Wei Feng, Jingjin Wu, Chen Yuan, Guangyi Liu, Renchang Dai, Qingxin\n  Shi, Fangxing Li", "title": "A Graph Computation based Sequential Power Flow Calculation for\n  Large-Scale ACDC Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a graph computation based sequential power flow\ncalculation method for Line Commutated Converter (LCC) based large-scale AC/DC\nsystems to achieve a high computing performance. Based on the graph theory, the\ncomplex AC/DC system is first converted to a graph model and stored in a graph\ndatabase. Then, the hybrid system is divided into several isolated areas with\ngraph partition algorithm by decoupling AC and DC networks. Thus, the power\nflow analysis can be executed in parallel for each independent area with the\nnew selected slack buses. Furthermore, for each area, the node-based parallel\ncomputing (NPC) and hierarchical parallel computing (HPC) used in graph\ncomputation are employed to speed up fast decoupled power flow (FDPF).\nComprehensive case studies on the IEEE 300-bus, polished South Carolina\n12,000-bus system and a China 11,119-bus system are performed to demonstrate\nthe accuracy and efficiency of the proposed method\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 20:08:07 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Feng", "Wei", ""], ["Wu", "Jingjin", ""], ["Yuan", "Chen", ""], ["Liu", "Guangyi", ""], ["Dai", "Renchang", ""], ["Shi", "Qingxin", ""], ["Li", "Fangxing", ""]]}, {"id": "1902.10222", "submitter": "Rachmad Vidya Wicaksana Putra", "authors": "Rachmad Vidya Wicaksana Putra, Muhammad Abdullah Hanif, Muhammad\n  Shafique", "title": "ROMANet: Fine-Grained Reuse-Driven Off-Chip Memory Access Management and\n  Data Organization for Deep Neural Network Accelerators", "comments": "Submitted to the IEEE-TVLSI journal, 14 pages, 26 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enabling high energy efficiency is crucial for embedded implementations of\ndeep learning. Several studies have shown that the DRAM-based off-chip memory\naccesses are one of the most energy-consuming operations in deep neural network\n(DNN) accelerators, and thereby limit the designs from achieving efficiency\ngains at the full potential. DRAM access energy varies depending upon the\nnumber of accesses required as well as the energy consumed per-access.\nTherefore, searching for a solution towards the minimum DRAM access energy is\nan important optimization problem. Towards this, we propose the ROMANet\nmethodology that aims at reducing the number of memory accesses, by searching\nfor the appropriate data partitioning and scheduling for each layer of a\nnetwork using a design space exploration, based on the knowledge of the\navailable on-chip memory and the data reuse factors. Moreover, ROMANet also\ntargets decreasing the number of DRAM row buffer conflicts and misses, by\nexploiting the DRAM multi-bank burst feature to improve the energy-per-access.\nBesides providing the energy benefits, our proposed DRAM data mapping also\nresults in an increased effective DRAM throughput, which is useful for\nlatency-constraint scenarios. Our experimental results show that the ROMANet\nsaves DRAM access energy by 12% for the AlexNet, by 36% for the VGG-16, and by\n46% for the MobileNet, while also improving the DRAM throughput by 10%, as\ncompared to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 20:04:37 GMT"}, {"version": "v2", "created": "Sun, 2 Aug 2020 11:40:06 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Putra", "Rachmad Vidya Wicaksana", ""], ["Hanif", "Muhammad Abdullah", ""], ["Shafique", "Muhammad", ""]]}, {"id": "1902.10244", "submitter": "Guillaume Jourjon", "authors": "Parinya Ekparinya, Vincent Gramoli and Guillaume Jourjon", "title": "The Attack of the Clones Against Proof-of-Authority", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore vulnerabilities and countermeasures of the recently\nproposed blockchain consensus based on proof-of-authority. The proof-of-work\nblockchains, like Bitcoin and Ethereum, have been shown both theoretically and\nempirically vulnerable to double spending attacks. This is why Byzantine fault\ntolerant consensus algorithms have gained popularity in the blockchain context\nfor their ability to tolerate a limited number t of attackers among n\nparticipants. We formalize the recently proposed proof-of-authority consensus\nalgorithms that are Byzantine fault tolerant by describing the Aura and Clique\nprotocols present in the two mainstream implementations of Ethereum. We then\nintroduce the Cloning Attack and show how to apply it to double spend in each\nof these protocols with a single malicious node. Our results show that the\nCloning Attack against Aura is always successful while the same attack against\nClique is about twice as fast and succeeds in most cases.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 21:58:38 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2019 22:24:21 GMT"}, {"version": "v3", "created": "Wed, 25 Sep 2019 00:16:33 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Ekparinya", "Parinya", ""], ["Gramoli", "Vincent", ""], ["Jourjon", "Guillaume", ""]]}, {"id": "1902.10268", "submitter": "Roja Eini", "authors": "Roja Eini, Lauren Linkous, Nasibeh Zohrabi, Sherif Abdelwahed", "title": "A Testbed for a Smart Building: Design and Implementation", "comments": "5 pages, 11 figures, accepted in 4th International Science of Smart\n  City Operations and Platforms Engineering (SCOPE2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the design and implementation of a smart building\nprototype. The implementation utilizes Internet of Things (IoT) solutions to\ncollect, analyze, and manage data from building systems in a smart city\nenvironment. The developed smart building prototype is capable of real-time\ninteractions with the residents. The main objective is to adapt the building\nsettings to the residents needs and provide the maximum comfort level with\nminimum operational costs. For this purpose, building parameters are collected\nvia the sensors and transferred to a database in real-time, which can be\naccessed or visualized upon the users need. Environment properties such as\ntemperature, light, humidity, audio, video, surveillance, and access status are\nmanaged through a model-based controller. The developed testbed and control\nscheme are generic and modular. The prototype can also be utilized for testing\ncyber-physical systems features and challenges.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 23:33:16 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Eini", "Roja", ""], ["Linkous", "Lauren", ""], ["Zohrabi", "Nasibeh", ""], ["Abdelwahed", "Sherif", ""]]}, {"id": "1902.10336", "submitter": "Richeng Jin", "authors": "Richeng Jin, Xiaofan He and Huaiyu Dai", "title": "Distributed Byzantine Tolerant Stochastic Gradient Descent in the Era of\n  Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent advances in sensor technologies and smart devices enable the\ncollaborative collection of a sheer volume of data from multiple information\nsources. As a promising tool to efficiently extract useful information from\nsuch big data, machine learning has been pushed to the forefront and seen great\nsuccess in a wide range of relevant areas such as computer vision, health care,\nand financial market analysis. To accommodate the large volume of data, there\nis a surge of interest in the design of distributed machine learning, among\nwhich stochastic gradient descent (SGD) is one of the mostly adopted methods.\nNonetheless, distributed machine learning methods may be vulnerable to\nByzantine attack, in which the adversary can deliberately share falsified\ninformation to disrupt the intended machine learning procedures. Therefore, two\nasynchronous Byzantine tolerant SGD algorithms are proposed in this work, in\nwhich the honest collaborative workers are assumed to store the model\nparameters derived from their own local data and use them as the ground truth.\nThe proposed algorithms can deal with an arbitrary number of Byzantine\nattackers and are provably convergent. Simulation results based on a real-world\ndataset are presented to verify the theoretical results and demonstrate the\neffectiveness of the proposed algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 05:39:06 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2019 06:15:13 GMT"}, {"version": "v3", "created": "Wed, 6 Mar 2019 19:48:52 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Jin", "Richeng", ""], ["He", "Xiaofan", ""], ["Dai", "Huaiyu", ""]]}, {"id": "1902.10345", "submitter": "Tal Ben-Nun", "authors": "Tal Ben-Nun, Johannes de Fine Licht, Alexandros Nikolaos Ziogas, Timo\n  Schneider, Torsten Hoefler", "title": "Stateful Dataflow Multigraphs: A Data-Centric Model for Performance\n  Portability on Heterogeneous Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ubiquity of accelerators in high-performance computing has driven\nprogramming complexity beyond the skill-set of the average domain scientist. To\nmaintain performance portability in the future, it is imperative to decouple\narchitecture-specific programming paradigms from the underlying scientific\ncomputations. We present the Stateful DataFlow multiGraph (SDFG), a\ndata-centric intermediate representation that enables separating program\ndefinition from its optimization. By combining fine-grained data dependencies\nwith high-level control-flow, SDFGs are both expressive and amenable to program\ntransformations, such as tiling and double-buffering. These transformations are\napplied to the SDFG in an interactive process, using extensible pattern\nmatching, graph rewriting, and a graphical user interface. We demonstrate SDFGs\non CPUs, GPUs, and FPGAs over various motifs --- from fundamental computational\nkernels to graph analytics. We show that SDFGs deliver competitive performance,\nallowing domain scientists to develop applications naturally and port them to\napproach peak hardware performance without modifying the original scientific\ncode.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 06:12:16 GMT"}, {"version": "v2", "created": "Fri, 30 Aug 2019 17:55:12 GMT"}, {"version": "v3", "created": "Thu, 2 Jan 2020 21:43:32 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Ben-Nun", "Tal", ""], ["Licht", "Johannes de Fine", ""], ["Ziogas", "Alexandros Nikolaos", ""], ["Schneider", "Timo", ""], ["Hoefler", "Torsten", ""]]}, {"id": "1902.10369", "submitter": "Yael Hitron", "authors": "Yael Hitron, Merav Parter", "title": "Counting to Ten with Two Fingers: Compressed Counting with Spiking\n  Neurons", "comments": "Accepted to ESA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of measuring time with probabilistic threshold gates\nimplemented by bio-inspired spiking neurons. In the model of spiking neural\nnetworks, network evolves in discrete rounds, where in each round, neurons fire\nin pulses in response to a sufficiently high membrane potential. This potential\nis induced by spikes from neighboring neurons that fired in the previous round,\nwhich can have either an excitatory or inhibitory effect. We first consider a\ndeterministic implementation of a neural timer and show that $\\Theta(\\log t)$\n(deterministic) threshold gates are both sufficient and necessary. This raised\nthe question of whether randomness can be leveraged to reduce the number of\nneurons. We answer this question in the affirmative by considering neural\ntimers with spiking neurons where the neuron $y$ is required to fire for $t$\nconsecutive rounds with probability at least $1-\\delta$, and should stop firing\nafter at most $2t$ rounds with probability $1-\\delta$ for some input parameter\n$\\delta \\in (0,1)$. Our key result is a construction of a neural timer with\n$O(\\log\\log 1/\\delta)$ spiking neurons. Interestingly, this construction uses\nonly one spiking neuron, while the remaining neurons can be deterministic\nthreshold gates. We complement this construction with a matching lower bound of\n$\\Omega(\\min\\{\\log\\log 1/\\delta, \\log t\\})$ neurons. This provides the first\nseparation between deterministic and randomized constructions in the setting of\nspiking neural networks. Finally, we demonstrate the usefulness of compressed\ncounting networks for synchronizing neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 07:39:17 GMT"}, {"version": "v2", "created": "Sun, 3 Mar 2019 15:21:15 GMT"}, {"version": "v3", "created": "Mon, 26 Aug 2019 09:22:08 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Hitron", "Yael", ""], ["Parter", "Merav", ""]]}, {"id": "1902.10489", "submitter": "William Moses Jr.", "authors": "Anisur Rahaman Molla and William K. Moses Jr", "title": "Dispersion of Mobile Robots: The Power of Randomness", "comments": "20 pages, 1 table. Accepted at TAMC 2019: Theory & Applications of\n  Models of Computation. The final authenticated version is available online at\n  https://doi.org/10.1007/978-3-030-14812-6_30", "journal-ref": null, "doi": "10.1007/978-3-030-14812-6_30", "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider cooperation among insects, modeled as cooperation between mobile\nrobots on a graph. Within this setting, we consider the problem of mobile robot\ndispersion on graphs. The study of mobile robots on a graph is an interesting\nparadigm with many interesting problems and applications. The problem of\ndispersion in this context, introduced by Augustine and Moses Jr., asks that\n$n$ robots, initially placed arbitrarily on an $n$ node graph, work together to\nquickly reach a configuration with exactly one robot at each node. Previous\nwork on this problem has looked at the trade-off between the time to achieve\ndispersion and the amount of memory required by each robot. However, the\ntrade-off was analyzed for \\textit{deterministic algorithms} and the minimum\nmemory required to achieve dispersion was found to be $\\Omega(\\log n)$ bits at\neach robot. In this paper, we show that by harnessing the power of\n\\textit{randomness}, one can achieve dispersion with $O(\\log \\Delta)$ bits of\nmemory at each robot, where $\\Delta$ is the maximum degree of the graph.\nFurthermore, we show a matching lower bound of $\\Omega(\\log \\Delta)$ bits for\nany \\textit{randomized algorithm} to solve dispersion.\n  We further extend the problem to a general $k$-dispersion problem where $k>\nn$ robots need to disperse over $n$ nodes such that at most $\\lceil k/n \\rceil$\nrobots are at each node in the final configuration.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 12:46:14 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Molla", "Anisur Rahaman", ""], ["Moses", "William K.", "Jr"]]}, {"id": "1902.10807", "submitter": "Vojtech Mrazek", "authors": "Vojtech Mrazek and Muhammad Abdullah Hanif and Zdenek Vasicek and\n  Lukas Sekanina and Muhammad Shafique", "title": "autoAx: An Automatic Design Space Exploration and Circuit Building\n  Methodology utilizing Libraries of Approximate Components", "comments": "Accepted for publication at the Design Automation Conference 2019\n  (DAC'19), Las Vegas, Nevada, USA", "journal-ref": null, "doi": "10.1145/3316781.3317781", "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate computing is an emerging paradigm for developing highly\nenergy-efficient computing systems such as various accelerators. In the\nliterature, many libraries of elementary approximate circuits have already been\nproposed to simplify the design process of approximate accelerators. Because\nthese libraries contain from tens to thousands of approximate implementations\nfor a single arithmetic operation it is intractable to find an optimal\ncombination of approximate circuits in the library even for an application\nconsisting of a few operations. An open problem is \"how to effectively combine\ncircuits from these libraries to construct complex approximate accelerators\".\nThis paper proposes a novel methodology for searching, selecting and combining\nthe most suitable approximate circuits from a set of available libraries to\ngenerate an approximate accelerator for a given application. To enable fast\ndesign space generation and exploration, the methodology utilizes machine\nlearning techniques to create computational models estimating the overall\nquality of processing and hardware cost without performing full synthesis at\nthe accelerator level. Using the methodology, we construct hundreds of\napproximate accelerators (for a Sobel edge detector) showing different but\nrelevant tradeoffs between the quality of processing and hardware cost and\nidentify a corresponding Pareto-frontier. Furthermore, when searching for\napproximate implementations of a generic Gaussian filter consisting of 17\narithmetic operations, the proposed approach allows us to identify\napproximately $10^3$ highly important implementations from $10^{23}$ possible\nsolutions in a few hours, while the exhaustive search would take four months on\na high-end processor.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2019 08:35:03 GMT"}, {"version": "v2", "created": "Mon, 1 Apr 2019 09:11:44 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Mrazek", "Vojtech", ""], ["Hanif", "Muhammad Abdullah", ""], ["Vasicek", "Zdenek", ""], ["Sekanina", "Lukas", ""], ["Shafique", "Muhammad", ""]]}, {"id": "1902.10810", "submitter": "Shantenu Jha", "authors": "Geoffrey Fox, James A. Glazier, JCS Kadupitiya, Vikram Jadhao, Minje\n  Kim, Judy Qiu, James P. Sluka, Endre Somogyi, Madhav Marathe, Abhijin Adiga,\n  Jiangzhuo Chen, Oliver Beckstein, Shantenu Jha", "title": "Learning Everywhere: Pervasive Machine Learning for Effective\n  High-Performance Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The convergence of HPC and data-intensive methodologies provide a promising\napproach to major performance improvements. This paper provides a general\ndescription of the interaction between traditional HPC and ML approaches and\nmotivates the Learning Everywhere paradigm for HPC. We introduce the concept of\neffective performance that one can achieve by combining learning methodologies\nwith simulation-based approaches, and distinguish between traditional\nperformance as measured by benchmark scores. To support the promise of\nintegrating HPC and learning methods, this paper examines specific examples and\nopportunities across a series of domains. It concludes with a series of open\ncomputer science and cyberinfrastructure questions and challenges that the\nLearning Everywhere paradigm presents.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 22:26:03 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Fox", "Geoffrey", ""], ["Glazier", "James A.", ""], ["Kadupitiya", "JCS", ""], ["Jadhao", "Vikram", ""], ["Kim", "Minje", ""], ["Qiu", "Judy", ""], ["Sluka", "James P.", ""], ["Somogyi", "Endre", ""], ["Marathe", "Madhav", ""], ["Adiga", "Abhijin", ""], ["Chen", "Jiangzhuo", ""], ["Beckstein", "Oliver", ""], ["Jha", "Shantenu", ""]]}, {"id": "1902.10932", "submitter": "Minseok Choi", "authors": "Minseok Choi, Albert No, Mingyue Ji and Joongheon Kim", "title": "Markov Decision Policies for Dynamic Video Delivery in Wireless Caching\n  Networks", "comments": "28 pages, 11 figures, submission to IEEE TWC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a video delivery strategy for dynamic streaming services\nwhich maximizes time-average streaming quality under a playback delay\nconstraint in wireless caching networks. The network where popular videos\nencoded by scalable video coding are already stored in randomly distributed\ncaching nodes is considered under adaptive video streaming concepts, and\ndistance-based interference management is investigated in this paper. In this\nnetwork model, a streaming user makes delay-constrained decisions depending on\nstochastic network states: 1) caching node for video delivery, 2) video\nquality, and 3) the quantity of video chunks to receive. Since wireless link\nactivation for video delivery may introduce delays, different timescales for\nupdating caching node association, video quality adaptation, and chunk amounts\nare considered. After associating with a caching node for video delivery, the\nstreaming user chooses combinations of quality and chunk amounts in the small\ntimescale. The dynamic decision making process for video quality and chunk\namounts at each slot is modeled using Markov decision process, and the caching\nnode decision is made based on the framework of Lyapunov optimization. Our\nintensive simulations verify that the proposed video delivery algorithm works\nreliably and also can control the tradeoff between video quality and playback\nlatency.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 07:23:40 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Choi", "Minseok", ""], ["No", "Albert", ""], ["Ji", "Mingyue", ""], ["Kim", "Joongheon", ""]]}, {"id": "1902.10995", "submitter": "Arik Rinberg", "authors": "Arik Rinberg, Alexander Spiegelman, Edward Bortnikov, Eshcar Hillel,\n  Idit Keidar, Lee Rhodes, and Hadar Serviansky", "title": "Fast Concurrent Data Sketches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data sketches are approximate succinct summaries of long streams. They are\nwidely used for processing massive amounts of data and answering statistical\nqueries about it in real-time. Existing libraries producing sketches are very\nfast, but do not allow parallelism for creating sketches using multiple threads\nor querying them while they are being built. We present a generic approach to\nparallelising data sketches efficiently, while bounding the error that such\nparallelism introduces. Utilising relaxed semantics and the notion of strong\nlinearisability we prove our algorithm's correctness and analyse the error it\ninduces in two specific sketches. Our implementation achieves high scalability\nwhile keeping the error small.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 10:29:35 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2019 08:57:20 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Rinberg", "Arik", ""], ["Spiegelman", "Alexander", ""], ["Bortnikov", "Edward", ""], ["Hillel", "Eshcar", ""], ["Keidar", "Idit", ""], ["Rhodes", "Lee", ""], ["Serviansky", "Hadar", ""]]}, {"id": "1902.10999", "submitter": "Aditi Sharma", "authors": "Ravi Ranjan, Aditi Sharma", "title": "Evaluation of Frequent Itemset Mining Platforms using Apriori and\n  FP-Growth Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the overwhelming amount of complex and heterogeneous data pouring from\nany-where, any-time, and any-device, there is undeniably an era of Big Data.\nThe emergence of the Big Data as a disruptive technology for next generation of\nintelligent systems, has brought many issues of how to extract and make use of\nthe knowledge obtained from the data within short times, limited budget and\nunder high rates of data generation. Companies are recognizing that big data\ncan be used to make more accurate predictions, and can be used to enhance the\nbusiness with the help of appropriate association rule mining algorithm. To\nhelp these organizations, with which software and algorithm is more appropriate\nfor them depending on their dataset, we compared the most famous three\nMapReduce based software Hadoop, Spark, Flink on two widely used algorithms\nApriori and Fp-Growth on different scales of dataset.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 10:36:51 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Ranjan", "Ravi", ""], ["Sharma", "Aditi", ""]]}, {"id": "1902.11131", "submitter": "Md. Abu Bakr Siddique", "authors": "Shadman Sakib, Md. Abu Bakr Siddique", "title": "Unsupervised Segmentation Algorithms' Implementation in ITK for Tissue\n  Classification via Human Head MRI Scans", "comments": "4 Pages, 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Tissue classification is one of the significant tasks in the field of\nbiomedical image analysis. Magnetic Resonance Imaging (MRI) is of great\nimportance in tissue classification especially in the areas of brain tissue\nclassification which is able to recognize anatomical areas of interest such as\nsurgical planning, monitoring therapy, clinical drug trials, image\nregistration, stereotactic neurosurgery, radiotherapy etc. The task of this\npaper is to implement different unsupervised classification algorithms in ITK\nand perform tissue classification (white matter, gray matter, cerebrospinal\nfluid (CSF) and background of the human brain). For this purpose, 5 grayscale\nhead MRI scans are provided. In order of classifying brain tissues, three\nalgorithms are used. These are: Otsu thresholding, Bayesian classification and\nBayesian classification with Gaussian smoothing. The obtained classification\nresults are analyzed in the results and discussion section.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 12:48:43 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 11:12:16 GMT"}, {"version": "v3", "created": "Thu, 16 Jan 2020 02:40:10 GMT"}, {"version": "v4", "created": "Sat, 25 Jan 2020 12:11:55 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Sakib", "Shadman", ""], ["Siddique", "Md. Abu Bakr", ""]]}, {"id": "1902.11259", "submitter": "Dylan Foster", "authors": "Jayadev Acharya and Christopher De Sa and Dylan J. Foster and Karthik\n  Sridharan", "title": "Distributed Learning with Sublinear Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In distributed statistical learning, $N$ samples are split across $m$\nmachines and a learner wishes to use minimal communication to learn as well as\nif the examples were on a single machine. This model has received substantial\ninterest in machine learning due to its scalability and potential for parallel\nspeedup. However, in high-dimensional settings, where the number examples is\nsmaller than the number of features (\"dimension\"), the speedup afforded by\ndistributed learning may be overshadowed by the cost of communicating a single\nexample. This paper investigates the following question: When is it possible to\nlearn a $d$-dimensional model in the distributed setting with total\ncommunication sublinear in $d$?\n  Starting with a negative result, we show that for learning $\\ell_1$-bounded\nor sparse linear models, no algorithm can obtain optimal error until\ncommunication is linear in dimension. Our main result is that that by slightly\nrelaxing the standard boundedness assumptions for linear models, we can obtain\ndistributed algorithms that enjoy optimal error with communication logarithmic\nin dimension. This result is based on a family of algorithms that combine\nmirror descent with randomized sparsification/quantization of iterates, and\nextends to the general stochastic convex optimization model.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 18:05:12 GMT"}, {"version": "v2", "created": "Mon, 18 Mar 2019 00:23:03 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Acharya", "Jayadev", ""], ["De Sa", "Christopher", ""], ["Foster", "Dylan J.", ""], ["Sridharan", "Karthik", ""]]}]