[{"id": "1607.00174", "submitter": "Michele Amoretti", "authors": "Giacomo Brambilla, Michele Amoretti, Francesco Medioli, Francesco\n  Zanichelli", "title": "Blockchain-based Proof of Location", "comments": "13 pages, 9 figures", "journal-ref": "2018 IEEE International Conference on Software Quality,\n  Reliability and Security Companion (QRS-C)", "doi": "10.1109/QRS-C.2018.00038", "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Location-Based Services (LBSs) build upon geographic information to provide\nusers with location-dependent functionalities. In such a context, it is\nparticularly important that geographic locations claimed by users are\ntrustworthy. Centralized verification approaches proposed in the last few years\nare not satisfactory, as they entail a high risk to the privacy of users. In\nthis paper, we present and evaluate a novel decentralized,\ninfrastructure-independent proof-of-location scheme based on blockchain\ntechnology. Our scheme guarantees both location trustworthiness and user\nprivacy preservation.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 09:35:28 GMT"}, {"version": "v2", "created": "Mon, 31 Jul 2017 16:25:44 GMT"}, {"version": "v3", "created": "Mon, 24 Feb 2020 14:16:19 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Brambilla", "Giacomo", ""], ["Amoretti", "Michele", ""], ["Medioli", "Francesco", ""], ["Zanichelli", "Francesco", ""]]}, {"id": "1607.00178", "submitter": "Sascha Hunold", "authors": "Alexandra Carpen-Amarie and Sascha Hunold and Jesper Larsson Tr\\\"aff", "title": "MPI Derived Datatypes: Performance Expectations and Status Quo", "comments": "46 pages, 107 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine natural expectations on communication performance using MPI\nderived datatypes in comparison to the baseline, \"raw\" performance of\ncommunicating simple, non-contiguous data layouts. We show that common MPI\nlibraries sometimes violate these datatype performance expectations, and\ndiscuss reasons why this happens, but also show cases where MPI libraries\nperform well. Our findings are in many ways surprising and disappointing.\nFirst, the performance of derived datatypes is sometimes worse than the\nsemantically equivalent packing and unpacking using the corresponding MPI\nfunctionality. Second, the communication performance equivalence stated in the\nMPI standard between a single contiguous datatype and the repetition of its\nconstituent datatype does not hold universally. Third, the heuristics that are\ntypically employed by MPI libraries at type-commit time are insufficient to\nenforce natural performance guidelines, and better type normalization\nheuristics may have a significant performance impact. We show cases where all\nthe MPI type constructors are necessary to achieve the expected performance for\ncertain data layouts. We describe our benchmarking approach to verify the\ndatatype performance guidelines, and present extensive verification results for\ndifferent MPI libraries.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 09:42:49 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Carpen-Amarie", "Alexandra", ""], ["Hunold", "Sascha", ""], ["Tr\u00e4ff", "Jesper Larsson", ""]]}, {"id": "1607.00249", "submitter": "Ying Sun", "authors": "Ying Sun, Gesualdo Scutari, and Daniel Palomar", "title": "Distributed Nonconvex Multiagent Optimization Over Time-Varying Networks", "comments": "Copyright 2001 SS&C. Published in the Proceedings of the 50th annual\n  Asilomar conference on signals, systems, and computers, Nov. 6-9, 2016, CA,\n  USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study nonconvex distributed optimization in multiagent networks where the\ncommunications between nodes is modeled as a time-varying sequence of arbitrary\ndigraphs. We introduce a novel broadcast-based distributed algorithmic\nframework for the (constrained) minimization of the sum of a smooth (possibly\nnonconvex and nonseparable) function, i.e., the agents' sum-utility, plus a\nconvex (possibly nonsmooth and nonseparable) regularizer. The latter is usually\nemployed to enforce some structure in the solution, typically sparsity. The\nproposed method hinges on Successive Convex Approximation (SCA) techniques\ncoupled with i) a tracking mechanism instrumental to locally estimate the\ngradients of agents' cost functions; and ii) a novel broadcast protocol to\ndisseminate information and distribute the computation among the agents.\nAsymptotic convergence to stationary solutions is established. A key feature of\nthe proposed algorithm is that it neither requires the double-stochasticity of\nthe consensus matrices (but only column stochasticity) nor the knowledge of the\ngraph sequence to implement. To the best of our knowledge, the proposed\nframework is the first broadcast-based distributed algorithm for convex and\nnonconvex constrained optimization over arbitrary, time-varying digraphs.\nNumerical results show that our algorithm outperforms current schemes on both\nconvex and nonconvex problems.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 13:51:55 GMT"}, {"version": "v2", "created": "Fri, 5 Aug 2016 03:21:57 GMT"}, {"version": "v3", "created": "Wed, 14 Dec 2016 22:12:39 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Sun", "Ying", ""], ["Scutari", "Gesualdo", ""], ["Palomar", "Daniel", ""]]}, {"id": "1607.00291", "submitter": "Devin Matthews", "authors": "Devin A. Matthews", "title": "High-Performance Tensor Contraction without Transposition", "comments": "24 pages, 8 figures, uses pgfplots", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor computations--in particular tensor contraction (TC)--are important\nkernels in many scientific computing applications. Due to the fundamental\nsimilarity of TC to matrix multiplication (MM) and to the availability of\noptimized implementations such as the BLAS, tensor operations have\ntraditionally been implemented in terms of BLAS operations, incurring both a\nperformance and a storage overhead. Instead, we implement TC using the flexible\nBLIS framework, which allows for transposition (reshaping) of the tensor to be\nfused with internal partitioning and packing operations, requiring no explicit\ntransposition operations or additional workspace. This implementation, TBLIS,\nachieves performance approaching that of MM, and in some cases considerably\nhigher than that of traditional TC. Our implementation supports multithreading\nusing an approach identical to that used for MM in BLIS, with similar\nperformance characteristics. The complexity of managing tensor-to-matrix\ntransformations is also handled automatically in our approach, greatly\nsimplifying its use in scientific applications.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 15:37:59 GMT"}, {"version": "v2", "created": "Wed, 17 Aug 2016 21:16:54 GMT"}, {"version": "v3", "created": "Fri, 10 Feb 2017 20:49:01 GMT"}, {"version": "v4", "created": "Tue, 11 Jul 2017 15:03:52 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Matthews", "Devin A.", ""]]}, {"id": "1607.00298", "submitter": "Christoph Lenzen", "authors": "Christoph Lenzen and Roger Wattenhofer", "title": "CLEX: Yet Another Supercomputer Architecture?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the CLEX supercomputer topology and routing scheme. We prove that\nCLEX can utilize a constant fraction of the total bandwidth for point-to-point\ncommunication, at delays proportional to the sum of the number of intermediate\nhops and the maximum physical distance between any two nodes. Moreover, %\napplying an asymmetric bandwidth assignment to the links, all-to-all\ncommunication can be realized $(1+o(1))$-optimally both with regard to\nbandwidth and delays. This is achieved at node degrees of $n^{\\varepsilon}$,\nfor an arbitrary small constant $\\varepsilon\\in (0,1]$. In contrast, these\nresults are impossible in any network featuring constant or polylogarithmic\nnode degrees. Through simulation, we assess the benefits of an implementation\nof the proposed communication strategy. Our results indicate that, for a\nmillion processors, CLEX can increase bandwidth utilization and reduce average\nrouting path length by at least factors $10$ respectively $5$ in comparison to\na torus network. Furthermore, the CLEX communication scheme features several\nother properties, such as deadlock-freedom, inherent fault-tolerance, and\ncanonical partition into smaller subsystems.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 15:58:51 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Lenzen", "Christoph", ""], ["Wattenhofer", "Roger", ""]]}, {"id": "1607.00307", "submitter": "K\\'evin Atighehchi", "authors": "Kevin Atighehchi", "title": "Optimal Tree Hash Modes: the Case of Trees Having their Leaves at All\n  the Levels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent work shows how we can optimize a tree based mode of operation for a\nhash function where the sizes of input message blocks and digest are the same,\nsubject to the constraint that the involved tree structure has all its leaves\nat the same depth. In this work, we show that we can further optimize the\nrunning time of such a mode by using a tree having leaves at all its levels. We\nmake the assumption that the input message block has a size a multiple of that\nof the digest and denote by $d$ the ratio block size over digest size. The\nrunning time is evaluated in terms of number of operations performed by the\nhash function, i.e. the number of calls to its underlying function. It turns\nout that a digest can be computed in $\\lceil \\log_{d+1} (l/2) \\rceil+2$\nevaluations of the underlying function using $\\lceil l/2 \\rceil$ processors,\nwhere $l$ is the number of blocks of the message. Other results of interest are\ndiscussed, such as the optimization of the parallel running time for a tree of\nrestricted height.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 16:34:02 GMT"}, {"version": "v10", "created": "Tue, 27 Jun 2017 12:35:41 GMT"}, {"version": "v2", "created": "Mon, 4 Jul 2016 18:07:26 GMT"}, {"version": "v3", "created": "Wed, 20 Jul 2016 15:57:27 GMT"}, {"version": "v4", "created": "Sat, 23 Jul 2016 17:55:27 GMT"}, {"version": "v5", "created": "Thu, 4 Aug 2016 13:59:02 GMT"}, {"version": "v6", "created": "Wed, 21 Sep 2016 17:10:50 GMT"}, {"version": "v7", "created": "Fri, 30 Sep 2016 13:27:23 GMT"}, {"version": "v8", "created": "Sun, 16 Oct 2016 16:21:33 GMT"}, {"version": "v9", "created": "Sat, 22 Apr 2017 12:46:23 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Atighehchi", "Kevin", ""]]}, {"id": "1607.01039", "submitter": "Yi Lu", "authors": "Yi Lu", "title": "Practical Tera-scale Walsh-Hadamard Transform", "comments": "to appear in proceedings of Future Technologies Conference - FTC\n  2016, San Francisco, 6 - 7 Dec, IEEE", "journal-ref": null, "doi": "10.1109/FTC.2016.7821757", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the mid-second decade of new millennium, the development of IT has reached\nunprecedented new heights. As one derivative of Moore's law, the operating\nsystem evolves from the initial 16 bits, 32 bits, to the ultimate 64 bits. Most\nmodern computing platforms are in transition to the 64-bit versions. For\nupcoming decades, IT industry will inevitably favor software and systems, which\ncan efficiently utilize the new 64-bit hardware resources. In particular, with\nthe advent of massive data outputs regularly, memory-efficient software and\nsystems would be leading the future.\n  In this paper, we aim at studying practical Walsh-Hadamard Transform (WHT).\nWHT is popular in a variety of applications in image and video coding, speech\nprocessing, data compression, digital logic design, communications, just to\nname a few. The power and simplicity of WHT has stimulated research efforts and\ninterests in (noisy) sparse WHT within interdisciplinary areas including (but\nis not limited to) signal processing, cryptography. Loosely speaking, sparse\nWHT refers to the case that the number of nonzero Walsh coefficients is much\nsmaller than the dimension; the noisy version of sparse WHT refers to the case\nthat the number of large Walsh coefficients is much smaller than the dimension\nwhile there exists a large number of small nonzero Walsh coefficients. Clearly,\ngeneral Walsh-Hadamard Transform is a first solution to the noisy sparse WHT,\nwhich can obtain all Walsh coefficients larger than a given threshold and the\nindex positions. In this work, we study efficient implementations of very large\ndimensional general WHT. Our work is believed to shed light on noisy sparse\nWHT, which remains to be a big open challenge. Meanwhile, the main idea behind\nwill help to study parallel data-intensive computing, which has a broad range\nof applications.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 15:07:56 GMT"}, {"version": "v2", "created": "Sat, 24 Dec 2016 20:46:12 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Lu", "Yi", ""]]}, {"id": "1607.01084", "submitter": "Travis Humble", "authors": "Travis S. Humble, Alexander J. McCaskey, Jonathan Schrock, Hadayat\n  Seddiqi, Keith A. Britt and Neena Imam", "title": "Performance Models for Split-execution Computing Systems", "comments": "Presented at 18th Workshop on Advances in Parallel and Distributed\n  Computational Models [APDCM2016] on 23 May 2016; 10 pages", "journal-ref": "2016 IEEE International Parallel and Distributed Processing\n  Symposium Workshops, pp. 545-554 (2016)", "doi": "10.1109/IPDPSW.2016.113", "report-no": null, "categories": "cs.ET cs.DC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Split-execution computing leverages the capabilities of multiple\ncomputational models to solve problems, but splitting program execution across\ndifferent computational models incurs costs associated with the translation\nbetween domains. We analyze the performance of a split-execution computing\nsystem developed from conventional and quantum processing units (QPUs) by using\nbehavioral models that track resource usage. We focus on asymmetric processing\nmodels built using conventional CPUs and a family of special-purpose QPUs that\nemploy quantum computing principles. Our performance models account for the\ntranslation of a classical optimization problem into the physical\nrepresentation required by the quantum processor while also accounting for\nhardware limitations and conventional processor speed and memory. We conclude\nthat the bottleneck in this split-execution computing system lies at the\nquantum-classical interface and that the primary time cost is independent of\nquantum processor behavior.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 01:11:10 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Humble", "Travis S.", ""], ["McCaskey", "Alexander J.", ""], ["Schrock", "Jonathan", ""], ["Seddiqi", "Hadayat", ""], ["Britt", "Keith A.", ""], ["Imam", "Neena", ""]]}, {"id": "1607.01210", "submitter": "Danny Dolev", "authors": "Danny Dolev and Eli Gafni", "title": "Some Garbage In - Some Garbage Out: Asynchronous t-Byzantine as\n  Asynchronous Benign t-resilient system with fixed t-Trojan-Horse Inputs", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that asynchronous $t$ faults Byzantine system is equivalent to\nasynchronous $t$-resilient system, where unbeknownst to all, the private inputs\nof at most $t$ processors were altered and installed by a malicious oracle.\n  The immediate ramification is that dealing with asynchronous Byzantine\nsystems does not call for new topological methods, as was recently employed by\nvarious researchers: Asynchronous Byzantine is a standard asynchronous system\nwith an input caveat. It also shows that two recent independent investigations\nof vector $\\epsilon$-agreement in the Byzantine model, and then in the\nfail-stop model, one was superfluous - in these problems the change of $t$\ninputs allowed in the Byzantine has no effect compared to the fail-stop case.\n  This result was motivated by the aim of casting any asynchronous system as a\nsynchronous system where all processors are correct and it is the communication\nsubstrate in the form of message-adversary that misbehaves. Thus, in addition,\nwe get such a characterization for the asynchronous Byzantine system.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 12:02:02 GMT"}, {"version": "v2", "created": "Wed, 3 Aug 2016 06:12:37 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Dolev", "Danny", ""], ["Gafni", "Eli", ""]]}, {"id": "1607.01335", "submitter": "Aditya Devarakonda", "authors": "Alex Gittens, Aditya Devarakonda, Evan Racah, Michael Ringenburg, Lisa\n  Gerhardt, Jey Kottalam, Jialin Liu, Kristyn Maschhoff, Shane Canon, Jatin\n  Chhugani, Pramod Sharma, Jiyan Yang, James Demmel, Jim Harrell, Venkat\n  Krishnamurthy, Michael W. Mahoney, Prabhat", "title": "Matrix Factorization at Scale: a Comparison of Scientific Data Analytics\n  in Spark and C+MPI Using Three Case Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the trade-offs of performing linear algebra using Apache Spark,\ncompared to traditional C and MPI implementations on HPC platforms. Spark is\ndesigned for data analytics on cluster computing platforms with access to local\ndisks and is optimized for data-parallel tasks. We examine three widely-used\nand important matrix factorizations: NMF (for physical plausability), PCA (for\nits ubiquity) and CX (for data interpretability). We apply these methods to\nTB-sized problems in particle physics, climate modeling and bioimaging. The\ndata matrices are tall-and-skinny which enable the algorithms to map\nconveniently into Spark's data-parallel model. We perform scaling experiments\non up to 1600 Cray XC40 nodes, describe the sources of slowdowns, and provide\ntuning guidance to obtain high performance.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 17:19:53 GMT"}, {"version": "v2", "created": "Tue, 23 Aug 2016 21:14:50 GMT"}, {"version": "v3", "created": "Tue, 20 Sep 2016 20:10:02 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Gittens", "Alex", ""], ["Devarakonda", "Aditya", ""], ["Racah", "Evan", ""], ["Ringenburg", "Michael", ""], ["Gerhardt", "Lisa", ""], ["Kottalam", "Jey", ""], ["Liu", "Jialin", ""], ["Maschhoff", "Kristyn", ""], ["Canon", "Shane", ""], ["Chhugani", "Jatin", ""], ["Sharma", "Pramod", ""], ["Yang", "Jiyan", ""], ["Demmel", "James", ""], ["Harrell", "Jim", ""], ["Krishnamurthy", "Venkat", ""], ["Mahoney", "Michael W.", ""], ["Prabhat", "", ""]]}, {"id": "1607.01341", "submitter": "Silvio Micali", "authors": "Jing Chen, Silvio Micali", "title": "Algorand", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A public ledger is a tamperproof sequence of data that can be read and\naugmented by everyone. Public ledgers have innumerable and compelling uses.\nThey can secure, in plain sight, all kinds of transactions ---such as titles,\nsales, and payments--- in the exact order in which they occur. Public ledgers\nnot only curb corruption, but also enable very sophisticated applications\n---such as cryptocurrencies and smart contracts. They stand to revolutionize\nthe way a democratic society operates. As currently implemented, however, they\nscale poorly and cannot achieve their potential.\n  Algorand is a truly democratic and efficient way to implement a public\nledger. Unlike prior implementations based on proof of work, it requires a\nnegligible amount of computation, and generates a transaction history that will\nnot \"fork\" with overwhelmingly high probability.\n  Algorand is based on (a novel and super fast) message-passing Byzantine\nagreement.\n  For concreteness, we shall describe Algorand only as a money platform.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 17:35:20 GMT"}, {"version": "v2", "created": "Fri, 15 Jul 2016 19:07:36 GMT"}, {"version": "v3", "created": "Mon, 1 Aug 2016 19:29:24 GMT"}, {"version": "v4", "created": "Wed, 24 Aug 2016 12:24:27 GMT"}, {"version": "v5", "created": "Mon, 3 Oct 2016 19:06:07 GMT"}, {"version": "v6", "created": "Fri, 11 Nov 2016 19:43:17 GMT"}, {"version": "v7", "created": "Wed, 16 Nov 2016 21:56:20 GMT"}, {"version": "v8", "created": "Tue, 23 May 2017 16:25:10 GMT"}, {"version": "v9", "created": "Fri, 26 May 2017 13:16:11 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Chen", "Jing", ""], ["Micali", "Silvio", ""]]}, {"id": "1607.01404", "submitter": "Lingfei Wu", "authors": "Lingfei Wu, Eloy Romero, Andreas Stathopoulos", "title": "PRIMME_SVDS: A High-Performance Preconditioned SVD Solver for Accurate\n  Large-Scale Computations", "comments": "23 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing number of applications requiring the solution of large scale\nsingular value problems have rekindled interest in iterative methods for the\nSVD. Some promising recent ad- vances in large scale iterative methods are\nstill plagued by slow convergence and accuracy limitations for computing\nsmallest singular triplets. Furthermore, their current implementations in\nMATLAB cannot address the required large problems. Recently, we presented a\npreconditioned, two-stage method to effectively and accurately compute a small\nnumber of extreme singular triplets. In this research, we present a\nhigh-performance software, PRIMME SVDS, that implements our hybrid method based\non the state-of-the-art eigensolver package PRIMME for both largest and\nsmallest singular values. PRIMME SVDS fills a gap in production level software\nfor computing the partial SVD, especially with preconditioning. The numerical\nexperiments demonstrate its superior performance compared to other\nstate-of-the-art software and its good parallel performance under strong and\nweak scaling.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 20:15:56 GMT"}, {"version": "v2", "created": "Tue, 24 Jan 2017 18:27:56 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Wu", "Lingfei", ""], ["Romero", "Eloy", ""], ["Stathopoulos", "Andreas", ""]]}, {"id": "1607.01643", "submitter": "J\\'anos V\\'egh", "authors": "J\\'anos V\\'egh", "title": "A configurable accelerator for manycores: the Explicitly Many-Processor\n  Approach", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  A new approach to designing processor accelerators is presented. A new\ncomputing model and a special kind of accelerator with dynamic (end-user\nprogrammable) architecture is suggested. The new model considers a processor,\nin which a newly introduced supervisor layer coordinates the job of the cores.\nThe cores have the ability (based on the parallelization information provided\nby the compiler, and using the help of the supervisor) to outsource part of the\njob they received to some neighbouring core. The introduced changes essentially\nand advantageously modify the architecture and operation of the computing\nsystems. The computing throughput drastically increases, the efficiency of the\ntechnological implementation (computing performance per logic gates) increases,\nthe non-payload activity for using operating system services decreases, the\nreal-time behavior changes advantageously, and connecting accelerators to the\nprocessor greatly simplifies. Here only some details of the architecture and\noperation of the processor are discussed, the rest is described elsewhere.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 14:40:03 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["V\u00e9gh", "J\u00e1nos", ""]]}, {"id": "1607.01838", "submitter": "Chengcheng Wang", "authors": "Chengcheng Wang, Yonggang Zhang, Bicheng Ying, and Ali H. Sayed", "title": "Coordinate-Descent Diffusion Learning by Networked Agents", "comments": "Accepted for publication", "journal-ref": null, "doi": "10.1109/TSP.2017.2757903", "report-no": null, "categories": "cs.MA cs.DC cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work examines the mean-square error performance of diffusion stochastic\nalgorithms under a generalized coordinate-descent scheme. In this setting, the\nadaptation step by each agent is limited to a random subset of the coordinates\nof its stochastic gradient vector. The selection of coordinates varies randomly\nfrom iteration to iteration and from agent to agent across the network. Such\nschemes are useful in reducing computational complexity at each iteration in\npower-intensive large data applications. They are also useful in modeling\nsituations where some partial gradient information may be missing at random.\nInterestingly, the results show that the steady-state performance of the\nlearning strategy is not always degraded, while the convergence rate suffers\nsome degradation. The results provide yet another indication of the resilience\nand robustness of adaptive distributed strategies.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 23:18:04 GMT"}, {"version": "v2", "created": "Wed, 11 Oct 2017 01:38:44 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Wang", "Chengcheng", ""], ["Zhang", "Yonggang", ""], ["Ying", "Bicheng", ""], ["Sayed", "Ali H.", ""]]}, {"id": "1607.02133", "submitter": "Fan Yang", "authors": "Fan Yang and Andrew A. Chien", "title": "Extreme Scaling of Supercomputing with Stranded Power: Costs and\n  Capabilities", "comments": "12 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power consumption (supply, heat, cost) and associated carbon emissions\n(environmental impact) are increasingly critical challenges in scaling\nsupercomputing to Exascale and beyond. We proposes to exploit stranded power,\nrenewable energy that has no value to the power grid, for scaling\nsupercomputers, Zero-Carbon Cloud (ZCCloud), and showing that stranded power\ncan be employed effectively to expand computing [1]. We build on those results\nwith a new analysis of stranded power, characterizing temporal, geographic, and\ninterval properties. We simulate production supercomputing workloads and model\ndatacenter total-cost-of-ownership (TCO), assessing the costs and capabilities\nof stranded-power based supercomputing. Results show that the ZCCloud approach\nis cost-effective today in regions with high cost power. The ZCCloud approach\nreduces TCO by 21-45%, and improves cost-effectiveness up to 34%. We study many\nscenarios. With higher power price, cheaper computing hardware and higher\nsystem power density, benefits rise to 55%, 97% and 116% respectively. Finally,\nwe study future extreme-scale systems, showing that beyond terascale, projected\npower requirements in excess of 100MW make ZCCloud up to 45% lower cost, for a\nfixed budget, increase peak PFLOPS achievable by 80%.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 19:31:37 GMT"}], "update_date": "2016-07-08", "authors_parsed": [["Yang", "Fan", ""], ["Chien", "Andrew A.", ""]]}, {"id": "1607.02214", "submitter": "Zhihui Du", "authors": "Xiangyu Guo, Binbin Tang, Jian Tao, Zhaohui Huang, Zhihui Du", "title": "Large Scale GPU Accelerated PPMLR-MHD Simulations for Space Weather\n  Forecast", "comments": null, "journal-ref": "ccgrid 2016", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  PPMLR-MHD is a new magnetohydrodynamics (MHD) model used to simulate the\ninteractions of the solar wind with the magnetosphere, which has been proved to\nbe the key element of the space weather cause-and-effect chain process from the\nSun to Earth. Compared to existing MHD methods, PPMLR-MHD achieves the\nadvantage of high order spatial accuracy and low numerical dissipation.\nHowever, the accuracy comes at a cost. On one hand, this method requires more\nintensive computation. On the other hand, more boundary data is subject to be\ntransferred during the process of simulation.s In this work, we present a\nparallel hybrid solution of the PPMLR-MHD model implemented using the computing\ncapabilities of both CPUs and GPUs. We demonstrate that our optimized\nimplementation alleviates the data transfer overhead by using GPU Direct\ntechnology and can scale up to 151 processes and achieve significant\nperformance gains by distributing the workload among the CPUs and GPUs on Titan\nat Oak Ridge National Laboratory. The performance results show that our\nimplementation is fast enough to carry out highly accurate MHD simulations in\nreal time.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 02:25:52 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Guo", "Xiangyu", ""], ["Tang", "Binbin", ""], ["Tao", "Jian", ""], ["Huang", "Zhaohui", ""], ["Du", "Zhihui", ""]]}, {"id": "1607.02480", "submitter": "Subutai Ahmad", "authors": "Subutai Ahmad, Scott Purdy", "title": "Real-Time Anomaly Detection for Streaming Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC cs.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Much of the worlds data is streaming, time-series data, where anomalies give\nsignificant information in critical situations. Yet detecting anomalies in\nstreaming data is a difficult task, requiring detectors to process data in\nreal-time, and learn while simultaneously making predictions. We present a\nnovel anomaly detection technique based on an on-line sequence memory algorithm\ncalled Hierarchical Temporal Memory (HTM). We show results from a live\napplication that detects anomalies in financial metrics in real-time. We also\ntest the algorithm on NAB, a published benchmark for real-time anomaly\ndetection, where our algorithm achieves best-in-class results.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 18:20:32 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Ahmad", "Subutai", ""], ["Purdy", "Scott", ""]]}, {"id": "1607.02497", "submitter": "Christian Glusa", "authors": "Mark Ainsworth, Christian Glusa", "title": "Is the Multigrid Method Fault Tolerant? The Two-Grid Case", "comments": "27 pages and 6 figures", "journal-ref": null, "doi": "10.1137/16M1100691", "report-no": null, "categories": "math.NA cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The predicted reduced resiliency of next-generation high performance\ncomputers means that it will become necessary to take into account the effects\nof randomly occurring faults on numerical methods. Further, in the event of a\nhard fault occurring, a decision has to be made as to what remedial action\nshould be taken in order to resume the execution of the algorithm. The action\nthat is chosen can have a dramatic effect on the performance and\ncharacteristics of the scheme. Ideally, the resulting algorithm should be\nsubjected to the same kind of mathematical analysis that was applied to the\noriginal, deterministic variant.\n  The purpose of this work is to provide an analysis of the behaviour of the\nmultigrid algorithm in the presence of faults. Multigrid is arguably the method\nof choice for the solution of large-scale linear algebra problems arising from\ndiscretization of partial differential equations and it is of considerable\nimportance to anticipate its behaviour on an exascale machine. The analysis of\nresilience of algorithms is in its infancy and the current work is perhaps the\nfirst to provide a mathematical model for faults and analyse the behaviour of a\nstate-of-the-art algorithm under the model. It is shown that the Two Grid\nMethod fails to be resilient to faults. Attention is then turned to identifying\nthe minimal necessary remedial action required to restore the rate of\nconvergence to that enjoyed by the ideal fault-free method.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 19:39:54 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Ainsworth", "Mark", ""], ["Glusa", "Christian", ""]]}, {"id": "1607.02573", "submitter": "Marcella Bonazzoli", "authors": "P.-H. Tournier, I. Aliferis, M. Bonazzoli, M. de Buhan, M. Darbas, V.\n  Dolean, F. Hecht, P. Jolivet, I. El Kanfoud, C. Migliaccio, F. Nataf, C.\n  Pichot, S. Semenov", "title": "Microwave Tomographic Imaging of Cerebrovascular Accidents by Using\n  High-Performance Computing", "comments": null, "journal-ref": "Parallel Computing, 85:88-97, 2019", "doi": "10.1016/j.parco.2019.02.004", "report-no": null, "categories": "cs.CE cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The motivation of this work is the detection of cerebrovascular accidents by\nmicrowave tomographic imaging. This requires the solution of an inverse problem\nrelying on a minimization algorithm (for example, gradient-based), where\nsuccessive iterations consist in repeated solutions of a direct problem. The\nreconstruction algorithm is extremely computationally intensive and makes use\nof efficient parallel algorithms and high-performance computing. The\nfeasibility of this type of imaging is conditioned on one hand by an accurate\nreconstruction of the material properties of the propagation medium and on the\nother hand by a considerable reduction in simulation time. Fulfilling these two\nrequirements will enable a very rapid and accurate diagnosis. From the\nmathematical and numerical point of view, this means solving Maxwell's\nequations in time-harmonic regime by appropriate domain decomposition methods,\nwhich are naturally adapted to parallel architectures.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jul 2016 06:57:38 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2020 18:44:32 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Tournier", "P. -H.", ""], ["Aliferis", "I.", ""], ["Bonazzoli", "M.", ""], ["de Buhan", "M.", ""], ["Darbas", "M.", ""], ["Dolean", "V.", ""], ["Hecht", "F.", ""], ["Jolivet", "P.", ""], ["Kanfoud", "I. El", ""], ["Migliaccio", "C.", ""], ["Nataf", "F.", ""], ["Pichot", "C.", ""], ["Semenov", "S.", ""]]}, {"id": "1607.02646", "submitter": "Vasiliki Kalavri", "authors": "Vasiliki Kalavri, Vladimir Vlassov, Seif Haridi", "title": "High-Level Programming Abstractions for Distributed Graph Processing", "comments": null, "journal-ref": null, "doi": "10.1109/TKDE.2017.2762294", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient processing of large-scale graphs in distributed environments has\nbeen an increasingly popular topic of research in recent years. Inter-connected\ndata that can be modeled as graphs arise in application domains such as machine\nlearning, recommendation, web search, and social network analysis. Writing\ndistributed graph applications is inherently hard and requires programming\nmodels that can cover a diverse set of problem domains, including iterative\nrefinement algorithms, graph transformations, graph aggregations, pattern\nmatching, ego-network analysis, and graph traversals. Several high-level\nprogramming abstractions have been proposed and adopted by distributed graph\nprocessing systems and big data platforms. Even though significant work has\nbeen done to experimentally compare distributed graph processing frameworks, no\nqualitative study and comparison of graph programming abstractions has been\nconducted yet. In this survey, we review and analyze the most prevalent\nhigh-level programming models for distributed graph processing, in terms of\ntheir semantics and applicability. We identify the classes of graph\napplications that can be naturally expressed by each abstraction and we also\ngive examples of applications that are hard or impossible to express. We review\n34 distributed graph processing systems with respect to their programming\nabstractions, execution models, and communication mechanisms. Finally, we\ndiscuss trends and open research questions in the area of distributed graph\nprocessing.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jul 2016 18:54:46 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Kalavri", "Vasiliki", ""], ["Vlassov", "Vladimir", ""], ["Haridi", "Seif", ""]]}, {"id": "1607.02734", "submitter": "Rui Han", "authors": "Rui Han, Siguang Huang, Fei Tang, Fugui Chang, Jianfeng Zhan", "title": "AccuracyTrader: Accuracy-aware Approximate Processing for Low Tail\n  Latency and High Result Accuracy in Cloud Online Services", "comments": "10 pages, 8 figures, 2 tables", "journal-ref": "The 45th International Conference on Parallel Processing\n  (ICPP-2016)", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern latency-critical online services such as search engines often process\nrequests by consulting large input data spanning massive parallel components.\nHence the tail latency of these components determines the service latency. To\ntrade off result accuracy for tail latency reduction, existing techniques use\nthe components responding before a specified deadline to produce approximate\nresults. However, they may skip a large proportion of components when load gets\nheavier, thus incurring large accuracy losses. This paper presents\nAccuracyTrader that produces approximate results with small accuracy losses\nwhile maintaining low tail latency. AccuracyTrader aggregates information of\ninput data on each component to create a small synopsis, thus enabling all\ncomponents producing initial results quickly using their synopses.\nAccuracyTrader also uses synopses to identify the parts of input data most\nrelated to arbitrary requests' result accuracy, thus first using these parts to\nimprove the produced results in order to minimize accuracy losses. We evaluated\nAccuracyTrader using workloads in real services. The results show: (i)\nAccuracyTrader reduces tail latency by over 40 times with accuracy losses of\nless than 7% compared to existing exact processing techniques; (ii) when using\nthe same latency, AccuracyTrader reduces accuracy losses by over 13 times\ncomparing to existing approximate processing techniques.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jul 2016 11:38:18 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Han", "Rui", ""], ["Huang", "Siguang", ""], ["Tang", "Fei", ""], ["Chang", "Fugui", ""], ["Zhan", "Jianfeng", ""]]}, {"id": "1607.02904", "submitter": "Markus H\\\"ohnerbach", "authors": "Markus H\\\"ohnerbach, Ahmed E. Ismail, Paolo Bientinesi", "title": "The Vectorization of the Tersoff Multi-Body Potential: An Exercise in\n  Performance Portability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.DC cs.MS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Molecular dynamics simulations, an indispensable research tool in\ncomputational chemistry and materials science, consume a significant portion of\nthe supercomputing cycles around the world. We focus on multi-body potentials\nand aim at achieving performance portability. Compared with well-studied pair\npotentials, multibody potentials deliver increased simulation accuracy but are\ntoo complex for effective compiler optimization. Because of this, achieving\ncross-platform performance remains an open question. By abstracting from target\narchitecture and computing precision, we develop a vectorization scheme\napplicable to both CPUs and accelerators. We present results for the Tersoff\npotential within the molecular dynamics code LAMMPS on several architectures,\ndemonstrating efficiency gains not only for computational kernels, but also for\nlarge-scale simulations. On a cluster of Intel Xeon Phi's, our optimized solver\nis between 3 and 5 times faster than the pure MPI reference.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 11:23:04 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["H\u00f6hnerbach", "Markus", ""], ["Ismail", "Ahmed E.", ""], ["Bientinesi", "Paolo", ""]]}, {"id": "1607.02951", "submitter": "Arnaud Casteigts", "authors": "Arnaud Casteigts, Yves M\\'etivier, John Michael Robson, Akka Zemmari", "title": "Design Patterns in Beeping Algorithms: Examples, Emulation, and Analysis", "comments": "Final version (accepted for publication in Information and\n  Computation, Elsevier)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider networks of processes which interact with beeps. In the basic\nmodel defined by Cornejo and Kuhn (2010), processes can choose in each round\neither to beep or to listen. Those who beep are unable to detect simultaneous\nbeeps. Those who listen can only distinguish between silence and the presence\nof at least one beep. We refer to this model as $BL$ (beep or listen). Stronger\nmodels exist where the nodes can detect collision while they are beeping\n($B_{cd}L$), listening ($BL_{cd}$), or both ($B_{cd}L_{cd}$). Beeping models\nare weak in essence and even simple tasks are difficult or unfeasible within.\n  We present a set of generic building blocks (design patterns) which seem to\noccur frequently in the design of beeping algorithms. They include multi-slot\nphases: the fact of dividing the main loop into a number of specialised slots;\nexclusive beeps: having a single node beep at a time in a neighbourhood (within\none or two hops); adaptive probability: increasing or decreasing the\nprobability of beeping to produce more exclusive beeps; internal (resp.\nperipheral) collision detection: for detecting collision while beeping (resp.\nlistening). Based on these patterns, we provide algorithms for a number of\nbasic problems, including colouring, 2-hop colouring, degree computation, 2-hop\nMIS, and collision detection (in $BL$). The patterns make it possible to\nformulate these algorithms in a rather concise and elegant way. Their analyses\nare more technical; one of them improves significantly upon that of the best\nknown MIS algorithm by Jeavons et al. (2016). Finally, inspired by a technique\nfrom Afek et al. (2013), our last contribution is to show that any Las Vegas\nalgorithm relying on collision detection can be transposed into a Monte Carlo\nalgorithm without collision detection at the cost of a logarithmic slowdown,\nwhich we prove is optimal.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 13:56:45 GMT"}, {"version": "v2", "created": "Wed, 8 Mar 2017 09:25:08 GMT"}, {"version": "v3", "created": "Wed, 20 Jun 2018 14:29:55 GMT"}, {"version": "v4", "created": "Thu, 30 Aug 2018 13:02:11 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Casteigts", "Arnaud", ""], ["M\u00e9tivier", "Yves", ""], ["Robson", "John Michael", ""], ["Zemmari", "Akka", ""]]}, {"id": "1607.02982", "submitter": "Andrew Prout", "authors": "Andrew Prout, William Arcand, David Bestor, Bill Bergeron, Chansup\n  Byun, Vijay Gadepally, Matthew Hubbell, Michael Houle, Michael Jones, Peter\n  Michaleas, Lauren Milechin, Julie Mullen, Antonio Rosa, Siddharth Samsi,\n  Albert Reuther, Jeremy Kepner", "title": "Enhancing HPC Security with a User-Based Firewall", "comments": null, "journal-ref": null, "doi": "10.1109/HPEC.2016.7761641", "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  HPC systems traditionally allow their users unrestricted use of their\ninternal network. While this network is normally controlled enough to guarantee\nprivacy without the need for encryption, it does not provide a method to\nauthenticate peer connections. Protocols built upon this internal network must\nprovide their own authentication. Many methods have been employed to perform\nthis authentication. However, support for all of these methods requires the HPC\napplication developer to include support and the user to configure and enable\nthese services. The user-based firewall capability we have prototyped enables a\nset of rules governing connections across the HPC internal network to be put\ninto place using Linux netfilter. By using an operating system-level\ncapability, the system is not reliant on any developer or user actions to\nenable security. The rules we have chosen and implemented are crafted to not\nimpact the vast majority of users and be completely invisible to them.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 14:44:01 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Prout", "Andrew", ""], ["Arcand", "William", ""], ["Bestor", "David", ""], ["Bergeron", "Bill", ""], ["Byun", "Chansup", ""], ["Gadepally", "Vijay", ""], ["Hubbell", "Matthew", ""], ["Houle", "Michael", ""], ["Jones", "Michael", ""], ["Michaleas", "Peter", ""], ["Milechin", "Lauren", ""], ["Mullen", "Julie", ""], ["Rosa", "Antonio", ""], ["Samsi", "Siddharth", ""], ["Reuther", "Albert", ""], ["Kepner", "Jeremy", ""]]}, {"id": "1607.03092", "submitter": "Qingjiang Shi", "authors": "Qingjiang Shi, Haoran Sun, Songtao Lu, Mingyi Hong, Meisam Razaviyayn", "title": "Inexact Block Coordinate Descent Methods For Symmetric Nonnegative\n  Matrix Factorization", "comments": "Submitted to TSP", "journal-ref": null, "doi": "10.1109/TSP.2017.2731321", "report-no": null, "categories": "cs.NA cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symmetric nonnegative matrix factorization (SNMF) is equivalent to computing\na symmetric nonnegative low rank approximation of a data similarity matrix. It\ninherits the good data interpretability of the well-known nonnegative matrix\nfactorization technique and have better ability of clustering nonlinearly\nseparable data. In this paper, we focus on the algorithmic aspect of the SNMF\nproblem and propose simple inexact block coordinate decent methods to address\nthe problem, leading to both serial and parallel algorithms. The proposed\nalgorithms have guaranteed stationary convergence and can efficiently handle\nlarge-scale and/or sparse SNMF problems. Extensive simulations verify the\neffectiveness of the proposed algorithms compared to recent state-of-the-art\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 19:48:41 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Shi", "Qingjiang", ""], ["Sun", "Haoran", ""], ["Lu", "Songtao", ""], ["Hong", "Mingyi", ""], ["Razaviyayn", "Meisam", ""]]}, {"id": "1607.03239", "submitter": "Martin Henze", "authors": "Martin Henze, Ren\\'e Hummen, Roman Matzutt, Klaus Wehrle", "title": "The SensorCloud Protocol: Securely Outsourcing Sensor Data to the Cloud", "comments": "19 pages, 1 figure, published as technical report of the Department\n  of Computer Science of RWTH Aachen University", "journal-ref": null, "doi": null, "report-no": "AIB-2016-06", "categories": "cs.NI cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing deployment of sensor networks, ranging from home networks to\nindustrial automation, leads to a similarly growing demand for storing and\nprocessing the collected sensor data. To satisfy this demand, the most\npromising approach to date is the utilization of the dynamically scalable,\non-demand resources made available via the cloud computing paradigm. However,\nprevalent security and privacy concerns are a huge obstacle for the outsourcing\nof sensor data to the cloud. Hence, sensor data needs to be secured properly\nbefore it can be outsourced to the cloud. When securing the outsourcing of\nsensor data to the cloud, one important challenge lies in the representation of\nsensor data and the choice of security measures applied to it. In this paper,\nwe present the SensorCloud protocol, which enables the representation of sensor\ndata and actuator commands using JSON as well as the encoding of the object\nsecurity mechanisms applied to a given sensor data item. Notably, we solely\nutilize mechanisms that have been or currently are in the process of being\nstandardized at the IETF to aid the wide applicability of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 06:49:14 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Henze", "Martin", ""], ["Hummen", "Ren\u00e9", ""], ["Matzutt", "Roman", ""], ["Wehrle", "Klaus", ""]]}, {"id": "1607.03252", "submitter": "Ulrich Ruede", "authors": "Bj\\\"orn Gmeiner and Daniel Drzisga and Ulrich Ruede and Robert\n  Scheichl and Barbara Wohlmuth", "title": "Scheduling massively parallel multigrid for multilevel Monte Carlo\n  methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.DC cs.MS cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational complexity of naive, sampling-based uncertainty\nquantification for 3D partial differential equations is extremely high.\nMultilevel approaches, such as multilevel Monte Carlo (MLMC), can reduce the\ncomplexity significantly, but to exploit them fully in a parallel environment,\nsophisticated scheduling strategies are needed. Often fast algorithms that are\nexecuted in parallel are essential to compute fine level samples in 3D, whereas\nto compute individual coarse level samples only moderate numbers of processors\ncan be employed efficiently. We make use of multiple instances of a parallel\nmultigrid solver combined with advanced load balancing techniques. In\nparticular, we optimize the concurrent execution across the three layers of the\nMLMC method: parallelization across levels, across samples, and across the\nspatial grid. The overall efficiency and performance of these methods will be\nanalyzed. Here the scalability window of the multigrid solver is revealed as\nbeing essential, i.e., the property that the solution can be computed with a\nrange of process numbers while maintaining good parallel efficiency. We\nevaluate the new scheduling strategies in a series of numerical tests, and\nconclude the paper demonstrating large 3D scaling experiments.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 07:47:45 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Gmeiner", "Bj\u00f6rn", ""], ["Drzisga", "Daniel", ""], ["Ruede", "Ulrich", ""], ["Scheichl", "Robert", ""], ["Wohlmuth", "Barbara", ""]]}, {"id": "1607.03369", "submitter": "Sorrachai Yingchareonthawornchai", "authors": "Sorrachai Yingchareonthawornchai, Duong Nguyen, Vidhya Tekken Valapil,\n  Sandeep Kulkarni, and Murat Demirbas", "title": "Precision, Recall, and Sensitivity of Monitoring Partially Synchronous\n  Distributed Systems", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Runtime verification focuses on analyzing the execution of a given program by\na monitor to determine if it is likely to violate its specifications. There is\noften an impedance mismatch between the assumptions/model of the monitor and\nthat of the underlying program. This constitutes problems especially for\ndistributed systems, where the concept of current time and state are inherently\nuncertain. A monitor designed with asynchronous system model assumptions may\ncause false-positives for a program executing in a partially synchronous\nsystem: the monitor may flag a global predicate that does not actually occur in\nthe underlying system. A monitor designed with a partially synchronous system\nmodel assumption may cause false negatives as well as false positives for a\nprogram executing in an environment where the bounds on partial synchrony\ndiffer (albeit temporarily) from the monitor model assumptions.\n  In this paper we analyze the effects of the impedance mismatch between the\nmonitor and the underlying program for the detection of conjunctive predicates.\nWe find that there is a small interval where the monitor assumptions are\nhypersensitive to the underlying program environment. We provide analytical\nderivations for this interval, and also provide simulation support for\nexploring the sensitivity of predicate detection to the impedance mismatch\nbetween the monitor and the program under a partially synchronous system.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 01:12:09 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Yingchareonthawornchai", "Sorrachai", ""], ["Nguyen", "Duong", ""], ["Valapil", "Vidhya Tekken", ""], ["Kulkarni", "Sandeep", ""], ["Demirbas", "Murat", ""]]}, {"id": "1607.03450", "submitter": "Leandro Indrusiak", "authors": "Leandro Soares Indrusiak and James Harbin and Martha Johanna Sepulveda", "title": "Side-Channel Attack Resilience through Route Randomisation in Secure\n  Real-Time Networks-on-Chip", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Security can be seen as an optimisation objective in NoC resource management,\nand as such poses trade-offs against other objectives such as real-time\nschedulability. In this paper, we show how to increase NoC resilience against a\nconcrete type of security attack, named side-channel attack, which exploit the\ncorrelation between specific non-functional properties (such as packet\nlatencies and routes, in the case of NoCs) to infer the functional behaviour of\nsecure applications. For instance, the transmission of a packet over a given\nlink of the NoC may hint on a cache miss, which can be used by an attacker to\nguess specific parts of a secret cryptographic key, effectively weakening it.\n  We therefore propose packet route randomisation as a mechanism to increase\nNoC resilience against side-channel attacks, focusing specifically on the\npotential impact of such an approach upon hard real-time systems, where\nschedulability is a vital design requirement. Using an evolutionary\noptimisation approach, we show how to effectively apply route randomisation in\nsuch a way that it can increase NoC security while controlling its impact on\nhard real-time performance guarantees. Extensive experimental evidence based on\nanalytical and simulation models supports our findings.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 18:04:46 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Indrusiak", "Leandro Soares", ""], ["Harbin", "James", ""], ["Sepulveda", "Martha Johanna", ""]]}, {"id": "1607.03677", "submitter": "Pierre Fraigniaud", "authors": "Simon Collet and Pierre Fraigniaud and Paolo Penna", "title": "Local Distributed Algorithms for Selfish Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the classical framework of local distributed network computing, it is\ngenerally assumed that the entities executing distributed algorithms are\naltruistic. However, in various scenarios, the value of the output produced by\nan entity may have a tremendous impact on its future. This is for instance the\ncase of tasks such as computing maximal independent sets (MIS) in networks.\nIndeed, a node belonging to the MIS may be later asked more than to a node not\nin the MIS, e.g., because MIS in networks are often used as backbones to\ncollect, transfer, and broadcast information, which is costly. In this paper,\nwe revisit typical local distributed network computing tasks in the framework\nof algorithmic game theory. Specifically, we focus on the construction of\nsolutions for locally checkable labeling (LCL) tasks, which form a large class\nof distributed tasks, including MIS, coloring, maximal matching, etc., and\nwhich have been studied for more than 20 years in distributed computing.\n  Given an LCL task, the nodes are collectively aiming at computing a solution,\nbut, at the individual level, every node plays rationally and selfishly with\nthe objective of optimizing its own profit. Surprisingly, the classical\nframeworks for game theory are not fully appropriate for the purpose of our\nstudy. Moreover, we show that classical notions like Nash equilibria may yield\nalgorithms requiring an arbitrarily large number of rounds to converge.\nNevertheless, by extending and generalizing core results from game theory, we\nestablish the existence of a so-called trembling-hand perfect equilibria, a\nsubset of Nash equilibria that is well suited to LCL construction tasks. The\nmain outcome of the paper is therefore that, for essentially all distributed\ntasks whose solutions are locally checkable, there exist construction\nalgorithms which are robust to selfishness.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 11:05:38 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["Collet", "Simon", ""], ["Fraigniaud", "Pierre", ""], ["Penna", "Paolo", ""]]}, {"id": "1607.03786", "submitter": "Chunlei Zhang", "authors": "Chunlei Zhang and Yongqiang Wang", "title": "Distributed Event Localization via Alternating Direction Method of\n  Multipliers", "comments": "accepted to IEEE Transactions on Mobile Computing", "journal-ref": "IEEE Transactions on Mobile Computing 17.2 (2018): 348-361", "doi": null, "report-no": null, "categories": "cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of distributed event localization using\nnoisy range measurements with respect to sensors with known positions. Event\nlocalization is fundamental in many wireless sensor network applications such\nas homeland security, law enforcement, and environmental studies. However, most\nexisting distributed algorithms require the target event to be within the\nconvex hull of the deployed sensors. Based on the alternating direction method\nof multipliers (ADMM), we propose two scalable distributed algorithms named\nGS-ADMM and J-ADMM which do not require the target event to be within the\nconvex hull of the deployed sensors. More specifically, the two algorithms can\nbe implemented in a scenario in which the entire sensor network is divided into\nseveral clusters with cluster heads collecting measurements within each cluster\nand exchanging intermediate computation information to achieve localization\nconsistency (consensus) across all clusters. This scenario is important in many\napplications such as homeland security and law enforcement. Simulation results\nconfirm effectiveness of the proposed algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 15:22:03 GMT"}, {"version": "v2", "created": "Sun, 1 Jul 2018 17:46:54 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Zhang", "Chunlei", ""], ["Wang", "Yongqiang", ""]]}, {"id": "1607.03830", "submitter": "Jian Du", "authors": "Jian Du and Yik-Chung Wu", "title": "Distributed Clock Skew and Offset Estimation in Wireless Sensor\n  Networks: Asynchronous Algorithm and Convergence Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a fully distributed algorithm for joint clock skew\nand offset estimation in wireless sensor networks based on belief propagation.\nIn the proposed algorithm, each node can estimate its clock skew and offset in\na completely distributed and asynchronous way: some nodes may update their\nestimates more frequently than others using outdated message from neighboring\nnodes. In addition, the proposed algorithm is robust to random packet loss.\nSuch algorithm does not require any centralized information processing or\ncoordination, and is scalable with network size. The proposed algorithm\nrepresents a unified framework that encompasses both classes of synchronous and\nasynchronous algorithms for network-wide clock synchronization. It is shown\nanalytically that the proposed asynchronous algorithm converges to the optimal\nestimates with estimation mean-square-error at each node approaching the\ncentralized Cram\\'er-Rao bound under any network topology. Simulation results\nfurther show that {the convergence speed is faster than that corresponding to a\nsynchronous algorithm}.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jul 2016 15:38:22 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["Du", "Jian", ""], ["Wu", "Yik-Chung", ""]]}, {"id": "1607.03884", "submitter": "Kyle Niemeyer", "authors": "Nicholas J. Curtis, Kyle E. Niemeyer, and Chih-Jen Sung", "title": "An investigation of GPU-based stiff chemical kinetics integration\n  methods", "comments": "34 pages, 6 figures; pdfLaTeX", "journal-ref": "Combust. Flame 179 (2017) 312-324", "doi": "10.1016/j.combustflame.2017.02.005", "report-no": null, "categories": "physics.comp-ph cs.DC physics.chem-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fifth-order implicit Runge-Kutta method and two fourth-order exponential\nintegration methods equipped with Krylov subspace approximations were\nimplemented for the GPU and paired with the analytical chemical kinetic\nJacobian software pyJac. The performance of each algorithm was evaluated by\nintegrating thermochemical state data sampled from stochastic partially stirred\nreactor simulations and compared with the commonly used CPU-based implicit\nintegrator CVODE. We estimated that the implicit Runge-Kutta method running on\na single GPU is equivalent to CVODE running on 12-38 CPU cores for integration\nof a single global integration time step of 1e-6 s with hydrogen and methane\nmodels. In the stiffest case studied---the methane model with a global\nintegration time step of 1e-4 s---thread divergence and higher memory traffic\nsignificantly decreased GPU performance to the equivalent of CVODE running on\napproximately three CPU cores. The exponential integration algorithms performed\nmore slowly than the implicit integrators on both the CPU and GPU. Thread\ndivergence and memory traffic were identified as the main limiters of GPU\nintegrator performance, and techniques to mitigate these issues were discussed.\nUse of a finite-difference Jacobian on the GPU---in place of the analytical\nJacobian provided by pyJac---greatly decreased integrator performance due to\nthread divergence, resulting in maximum slowdowns of 7.11-240.96 times; in\ncomparison, the corresponding slowdowns on the CPU were just 1.39-2.61 times,\nunderscoring the importance of use of an analytical Jacobian for efficient GPU\nintegration. Finally, future research directions for working towards enabling\nrealistic chemistry in reactive-flow simulations via GPU\\slash SIMD accelerated\nstiff chemical kinetic integration were identified.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 19:47:57 GMT"}, {"version": "v2", "created": "Mon, 8 Aug 2016 19:47:54 GMT"}, {"version": "v3", "created": "Tue, 14 Feb 2017 17:37:42 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["Curtis", "Nicholas J.", ""], ["Niemeyer", "Kyle E.", ""], ["Sung", "Chih-Jen", ""]]}, {"id": "1607.04077", "submitter": "Kristina Kapanova G", "authors": "K.G.Kapanova, J.M.Sellier", "title": "Designing a High Performance Parallel Personal Cluster", "comments": "12 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Today, many scientific and engineering areas require high performance\ncomputing to perform computationally intensive experiments. For example, many\nadvances in transport phenomena, thermodynamics, material properties,\ncomputational chemistry and physics are possible only because of the\navailability of such large scale computing infrastructures. Yet many challenges\nare still open. The cost of energy consumption, cooling, competition for\nresources have been some of the reasons why the scientific and engineering\ncommunities are turning their interests to the possibility of implementing\nenergy-efficient servers utilizing low-power CPUs for computing-intensive\ntasks. In this paper we introduce a novel approach, which was recently\npresented at Linux Conference Europe 2015, based on the Beowulf concept and\nutilizing single board computers (SBC). We present a low-energy consumption\narchitecture capable to tackle heavily demanding scientific computational\nproblems. Additionally, our goal is to provide a low cost personal solution for\nscientists and engineers. In order to evaluate the performance of the proposed\narchitecture we ran several standard benchmarking tests. Furthermore, we assess\nthe reliability of the machine in real life situations by performing two\nbenchmark tools involving practical TCAD for physicist and engineers in the\nsemiconductor industry.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 10:41:52 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Kapanova", "K. G.", ""], ["Sellier", "J. M.", ""]]}, {"id": "1607.04109", "submitter": "Katina Kralevska", "authors": "Katina Kralevska, Danilo Gligoroski, Harald {\\O}verby", "title": "General Sub-packetized Access-Optimal Regenerating Codes", "comments": null, "journal-ref": "IEEE Communications Letters, Vol. 20, No. 7, July 2016", "doi": "10.1109/LCOMM.2016.2561287", "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel construction of $(n,k,d=n-1)$ access-optimal\nregenerating codes for an arbitrary sub-packetization level $\\alpha$ for exact\nrepair of any systematic node. We refer to these codes as general\nsub-packetized because we provide an algorithm for constructing codes for any\n$\\alpha$ less than or equal to $r^{\\lceil \\frac{k}{r} \\rceil}$ where\n$\\frac{k}{r}$ is not necessarily an integer. This leads to a flexible\nconstruction of codes for different code rates compared to existing approaches.\nWe derive the lower and the upper bound of the repair bandwidth. The repair\nbandwidth depends on the code parameters and $\\alpha$. The repair process of a\nfailed systematic node is linear and highly parallelized, which means that a\nset of $\\lceil \\frac{\\alpha}{r} \\rceil$ symbols is independently repaired first\nand used along with the accessed data from other nodes to recover the remaining\nsymbols.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 12:43:12 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Kralevska", "Katina", ""], ["Gligoroski", "Danilo", ""], ["\u00d8verby", "Harald", ""]]}, {"id": "1607.04137", "submitter": "Katina Kralevska", "authors": "Katina Kralevska, Danilo Gligoroski and Harald {\\O}verby", "title": "Balanced Locally Repairable Codes", "comments": "Accepted for presentation at International Symposium on Turbo Codes\n  and Iterative Information Processing 2016", "journal-ref": "Published in: 2016 9th International Symposium on Turbo Codes and\n  Iterative Information Processing (ISTC)", "doi": "10.1109/ISTC.2016.7593121", "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a family of balanced locally repairable codes (BLRCs) $[n, k,\nd]$ for arbitrary values of $n$, $k$ and $d$. Similar to other locally\nrepairable codes (LRCs), the presented codes are suitable for applications that\nrequire a low repair locality. The novelty that we introduce in our\nconstruction is that we relax the strict requirement the repair locality to be\na fixed small number $l$, and we allow the repair locality to be either $l$ or\n$l+1$. This gives us the flexibility to construct BLRCs for arbitrary values of\n$n$ and $k$ which partially solves the open problem of finding a general\nconstruction of LRCs. Additionally, the relaxed locality criteria gives us an\nopportunity to search for BLRCs that have a low repair locality even when\ndouble failures occur. We use metrics such as a storage overhead, an average\nrepair bandwidth, a Mean Time To Data Loss (MTTDL) and an update complexity to\ncompare the performance of BLRCs with existing LRCs.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 14:04:46 GMT"}, {"version": "v2", "created": "Wed, 10 Aug 2016 11:45:22 GMT"}, {"version": "v3", "created": "Mon, 3 Jul 2017 10:54:53 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Kralevska", "Katina", ""], ["Gligoroski", "Danilo", ""], ["\u00d8verby", "Harald", ""]]}, {"id": "1607.04334", "submitter": "Farshid Farhat", "authors": "Farshid Farhat, Diman Zad Tootaghaj, and Mohammad Arjomand", "title": "Towards Stochastically Optimizing Data Computing Flows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With rapid growth in the amount of unstructured data produced by\nmemory-intensive applications, large scale data analytics has recently\nattracted increasing interest. Processing, managing and analyzing this huge\namount of data poses several challenges in cloud and data center computing\ndomain. Especially, conventional frameworks for distributed data analytics are\nbased on the assumption of homogeneity and non-stochastic distribution of\ndifferent data-processing nodes. The paper argues the fundamental limiting\nfactors for scaling big data computation. It is shown that as the number of\nseries and parallel computing servers increase, the tail (mean and variance) of\nthe job execution time increase. We will first propose a model to predict the\nresponse time of highly distributed processing tasks and then propose a new\npractical computational algorithm to optimize the response time.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 22:02:53 GMT"}, {"version": "v2", "created": "Tue, 2 Aug 2016 22:53:33 GMT"}, {"version": "v3", "created": "Thu, 25 Aug 2016 20:18:24 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Farhat", "Farshid", ""], ["Tootaghaj", "Diman Zad", ""], ["Arjomand", "Mohammad", ""]]}, {"id": "1607.04549", "submitter": "Philipp Wagner", "authors": "Philipp Wagner, Thomas Wild, Andreas Herkersdorf", "title": "DiaSys: Improving SoC Insight Through On-Chip Diagnosis", "comments": null, "journal-ref": null, "doi": "10.1016/j.sysarc.2017.01.005", "report-no": null, "categories": "cs.DC cs.AR cs.SE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  To find the cause of a functional or non-functional defect (bug) in software\nrunning on a multi-processor System-on-Chip (MPSoC), developers need insight\ninto the chip. Tracing systems provide this insight non-intrusively, at the\ncost of high off-chip bandwidth requirements. This I/O bottleneck limits the\nobservability, a problem becoming more severe as more functionality is\nintegrated on-chip. In this paper, we present DiaSys, an MPSoC diagnosis system\nwith the potential to replace today's tracing systems. Its main idea is to\npartially execute the analysis of observation data on the chip; in consequence,\nmore information and less data is sent to the attached host PC. With DiaSys,\nthe data analysis is performed by the diagnosis application. Its input are\nevents, which are generated by observation hardware at interesting points in\nthe program execution (like a function call). Its outputs are events with\nhigher information density. The event transformation is modeled as dataflow\napplication. For execution, it is mapped in part to dedicated and distributed\non-chip components, and in part to the host PC; the off-chip boundary is\ntransparent to the developer of the diagnosis application. We implement DiaSys\nas extension to an existing SoC with four tiles and a mesh network running on\nan FPGA platform. Two usage examples confirm that DiaSys is flexible enough to\nreplace a tracing system, while significantly lowering the off-chip bandwidth\nrequirements. In our examples, the debugging of a race-condition bug, and the\ncreation of a lock contention profile, we see a reduction of trace bandwidth of\nmore than three orders of magnitude, compared to a full trace created by a\ncommon tracing system.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 15:16:03 GMT"}, {"version": "v2", "created": "Wed, 18 Jan 2017 14:38:51 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Wagner", "Philipp", ""], ["Wild", "Thomas", ""], ["Herkersdorf", "Andreas", ""]]}, {"id": "1607.04818", "submitter": "Loris Cannelli", "authors": "Loris Cannelli, Francisco Facchinei, Vyacheslav Kungurtsev, Gesualdo\n  Scutari", "title": "Asynchronous Parallel Algorithms for Nonconvex Optimization", "comments": "This is the first part of a two-paper work. The second part can be\n  found at: arXiv:1701.04900", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new asynchronous parallel block-descent algorithmic framework\nfor the minimization of the sum of a smooth nonconvex function and a nonsmooth\nconvex one, subject to both convex and nonconvex constraints. The proposed\nframework hinges on successive convex approximation techniques and a novel\nprobabilistic model that captures key elements of modern computational\narchitectures and asynchronous implementations in a more faithful way than\ncurrent state-of-the-art models. Other key features of the framework are: i) it\ncovers in a unified way several specific solution methods; ii) it accommodates\na variety of possible parallel computing architectures; and iii) it can deal\nwith nonconvex constraints. Almost sure convergence to stationary solutions is\nproved, and theoretical complexity results are provided, showing nearly ideal\nlinear speedup when the number of workers is not too large.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jul 2016 02:13:26 GMT"}, {"version": "v2", "created": "Tue, 24 Jan 2017 02:59:05 GMT"}, {"version": "v3", "created": "Thu, 29 Mar 2018 22:27:43 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Cannelli", "Loris", ""], ["Facchinei", "Francisco", ""], ["Kungurtsev", "Vyacheslav", ""], ["Scutari", "Gesualdo", ""]]}, {"id": "1607.04984", "submitter": "He Sun", "authors": "He Sun, Luca Zanetti", "title": "Distributed Graph Clustering by Load Balancing", "comments": "There is a gap in the proof of the paper, which makes the paper's\n  main statement invalid. We presented an improved algorithm with a completely\n  different proof in the paper with arXiv:1711.01262", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph clustering is a fundamental computational problem with a number of\napplications in algorithm design, machine learning, data mining, and analysis\nof social networks. Over the past decades, researchers have proposed a number\nof algorithmic design methods for graph clustering. However, most of these\nmethods are based on complicated spectral techniques or convex optimisation,\nand cannot be applied directly for clustering many networks that occur in\npractice, whose information is often collected on different sites. Designing a\nsimple and distributed clustering algorithm is of great interest, and has wide\napplications for processing big datasets. In this paper we present a simple and\ndistributed algorithm for graph clustering: for a wide class of graphs that are\ncharacterised by a strong cluster-structure, our algorithm finishes in a\npoly-logarithmic number of rounds, and recovers a partition of the graph close\nto an optimal partition. The main component of our algorithm is an application\nof the random matching model of load balancing, which is a fundamental protocol\nin distributed computing and has been extensively studied in the past 20 years.\nHence, our result highlights an intrinsic and interesting connection between\ngraph clustering and load balancing. At a technical level, we present a purely\nalgebraic result characterising the early behaviours of load balancing\nprocesses for graphs exhibiting a cluster-structure. We believe that this\nresult can be further applied to analyse other gossip processes, such as rumour\nspreading and averaging processes.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 09:30:49 GMT"}, {"version": "v2", "created": "Mon, 5 Jun 2017 20:36:55 GMT"}, {"version": "v3", "created": "Thu, 11 Apr 2019 13:47:40 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Sun", "He", ""], ["Zanetti", "Luca", ""]]}, {"id": "1607.05069", "submitter": "Gordon Inggs", "authors": "Gordon Inggs", "title": "Algorithmic Trading: A brief, computational finance case study on data\n  centre FPGAs", "comments": "13 pages, published note", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasingly FPGAs will be deployed at scale due to the need for increased\nneed for power efficient computation and improved high level synthesis tool\nflows, creating a new category of device: data centre FPGAs. A method for using\nthese FPGAs is to identify what proportion of a given workload would benefit\nfrom being implemented upon the available FPGAs while minimising communication\noff-chip. As part of the implementation of these tasks, care should be taken in\nidentifying the parallel execution mode, task or pipeline parallelism that\nshould be used. When considering a case study of computational finance tasks, a\nbenchmark workload of Heston and Black-Scholes-based options implemented using\nOpenCL and OpenSPL, the benefit of this method of using data centre FPGAs is\nillustrated. These devices deliver latency performance close to that of\nworkstation grade GPUs, while requiring considerably less energy, resulting in\n30% more floating point operations per Joule of energy consumed.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2016 22:35:09 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Inggs", "Gordon", ""]]}, {"id": "1607.05075", "submitter": "Gopi Krishna Suvanam", "authors": "Gopi Krishna Suvanam", "title": "Functional Augmented State Transfer (FAST) Architecture for\n  Computationally Intensive Network Applications", "comments": "submitted for publication in IEEE Transactions on Services Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a novel architecture that combines the simplicity of RESTful\narchitecture with the power of functional programming for delivering\nweb-services. Although, RESTful architecture has been quite useful in\nsimplifying the development of scalable systems, it is not suited for all types\nof network applications. Our architecture improves upon the RESTful\narchitecture to provide scalable framework for computationally intensive\nnetwork applications. The proposed architecture is ideal for applications that\ninvolve data management and data analysis/calculations on data. Data analytics\nand financial calculations are two areas where the architecture can be applied\nefficiently.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 10:20:09 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Suvanam", "Gopi Krishna", ""]]}, {"id": "1607.05165", "submitter": "Thim Strothmann", "authors": "Christian Scheideler and Alexander Setzer and Thim Strothmann", "title": "Towards a Universal Approach for Monotonic Searchability in\n  Self-Stabilizing Overlay Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For overlay networks, the ability to recover from a variety of problems like\nmembership changes or faults is a key element to preserve their functionality.\nIn recent years, various self-stabilizing overlay networks have been proposed\nthat have the advantage of being able to recover from any illegal state.\nHowever, the vast majority of these networks cannot give any guarantees on its\nfunctionality while the recovery process is going on. We are especially\ninterested in searchability, i.e., the functionality that search messages for a\nspecific identifier are answered successfully if a node with that identifier\nexists in the network. We investigate overlay networks that are not only\nself-stabilizing but that also ensure that monotonic searchability is\nmaintained while the recovery process is going on, as long as there are no\ncorrupted messages in the system. More precisely, once a search message from\nnode $u$ to another node $v$ is successfully delivered, all future search\nmessages from $u$ to $v$ succeed as well. Monotonic searchability was recently\nintroduced in OPODIS 2015, in which the authors provide a solution for a simple\nline topology.\n  We present the first universal approach to maintain monotonic searchability\nthat is applicable to a wide range of topologies. As the base for our approach,\nwe introduce a set of primitives for manipulating overlay networks that allows\nus to maintain searchability and show how existing protocols can be transformed\nto use theses primitives.\n  We complement this result with a generic search protocol that together with\nthe use of our primitives guarantees monotonic searchability.\n  As an additional feature, searching existing nodes with the generic search\nprotocol is as fast as searching a node with any other fixed routing protocol\nonce the topology has stabilized.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 16:25:37 GMT"}, {"version": "v2", "created": "Wed, 28 Sep 2016 17:58:20 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Scheideler", "Christian", ""], ["Setzer", "Alexander", ""], ["Strothmann", "Thim", ""]]}, {"id": "1607.05178", "submitter": "Xiaohu Wu", "authors": "Xiaohu Wu, Patrick Loiseau, and Esa Hyytia", "title": "Towards Designing Cost-Optimal Policies to Utilize IaaS Clouds under\n  Online Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many businesses possess a small infrastructure that they can use for their\ncomputing tasks, but also often buy extra computing resources from clouds.\nCloud vendors such as Amazon EC2 offer two types of purchase options: on-demand\nand spot instances. As tenants have limited budgets to satisfy their computing\nneeds, it is crucial for them to determine how to purchase different options\nand utilize them (in addition to possible self-owned instances) in a\ncost-effective manner while respecting their response-time targets. In this\npaper, we propose a framework to design policies to allocate self-owned,\non-demand and spot instances to arriving jobs. In particular, we propose a\nnear-optimal policy to determine the number of self-owned instance and an\noptimal policy to determine the number of on-demand instances to buy and the\nnumber of spot instances to bid for at each time unit. Our policies rely on a\nsmall number of parameters and we use an online learning technique to infer\ntheir optimal values. Through numerical simulations, we show the effectiveness\nof our proposed policies, in particular that they achieve a cost reduction of\nup to 64.51% when spot and on-demand instances are considered and of up to\n43.74% when self-owned instances are considered, compared to previously\nproposed or intuitive policies.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 16:51:50 GMT"}, {"version": "v2", "created": "Fri, 22 Jul 2016 20:14:35 GMT"}, {"version": "v3", "created": "Mon, 20 Feb 2017 14:59:02 GMT"}, {"version": "v4", "created": "Wed, 8 Mar 2017 18:17:53 GMT"}, {"version": "v5", "created": "Tue, 6 Jun 2017 08:01:07 GMT"}, {"version": "v6", "created": "Sat, 21 Jul 2018 18:43:14 GMT"}, {"version": "v7", "created": "Mon, 12 Aug 2019 10:04:40 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Wu", "Xiaohu", ""], ["Loiseau", "Patrick", ""], ["Hyytia", "Esa", ""]]}, {"id": "1607.05212", "submitter": "Yannic Maus", "authors": "Dan Hefetz, Fabian Kuhn, Yannic Maus, Angelika Steger", "title": "Polynomial Lower Bound for Distributed Graph Coloring in a Weak LOCAL\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show an $\\Omega\\big(\\Delta^{\\frac{1}{3}-\\frac{\\eta}{3}}\\big)$ lower bound\non the runtime of any deterministic distributed\n$\\mathcal{O}\\big(\\Delta^{1+\\eta}\\big)$-graph coloring algorithm in a weak\nvariant of the \\LOCAL\\ model.\n  In particular, given a network graph \\mbox{$G=(V,E)$}, in the weak \\LOCAL\\\nmodel nodes communicate in synchronous rounds and they can use unbounded local\ncomputation. We assume that the nodes have no identifiers, but that instead,\nthe computation starts with an initial valid vertex coloring. A node can\n\\textbf{broadcast} a \\textbf{single} message of \\textbf{unbounded} size to its\nneighbors and receives the \\textbf{set of messages} sent to it by its\nneighbors. That is, if two neighbors of a node $v\\in V$ send the same message\nto $v$, $v$ will receive this message only a single time; without any further\nknowledge, $v$ cannot know whether a received message was sent by only one or\nmore than one neighbor.\n  Neighborhood graphs have been essential in the proof of lower bounds for\ndistributed coloring algorithms, e.g., \\cite{linial92,Kuhn2006On}. Our proof\nanalyzes the recursive structure of the neighborhood graph of the respective\nmodel to devise an $\\Omega\\big(\\Delta^{\\frac{1}{3}-\\frac{\\eta}{3}}\\big)$ lower\nbound on the runtime for any deterministic distributed\n$\\mathcal{O}\\big(\\Delta^{1+\\eta}\\big)$-graph coloring algorithm.\n  Furthermore, we hope that the proof technique improves the understanding of\nneighborhood graphs in general and that it will help towards finding a lower\n(runtime) bound for distributed graph coloring in the standard \\LOCAL\\ model.\nOur proof technique works for one-round algorithms in the standard \\LOCAL\\\nmodel and provides a simpler and more intuitive proof for an existing\n$\\Omega(\\Delta^2)$ lower bound.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 17:30:53 GMT"}, {"version": "v2", "created": "Wed, 14 Sep 2016 11:22:48 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Hefetz", "Dan", ""], ["Kuhn", "Fabian", ""], ["Maus", "Yannic", ""], ["Steger", "Angelika", ""]]}, {"id": "1607.05344", "submitter": "Eduardo Alchieri", "authors": "Eduardo Alchieri, Alysson Bessani, Fabiola Greve, Joni Fraga", "title": "Efficient and Modular Consensus-Free Reconfiguration for Fault-Tolerant\n  Storage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quorum systems are useful tools for implementing consistent and available\nstorage in the presence of failures. These systems usually comprise a static\nset of servers that provide a fault-tolerant read/write register accessed by a\nset of clients. We consider a dynamic variant of these systems and propose\nFreeStore, a set of fault-tolerant protocols that emulates a register in\ndynamic asynchronous systems in which processes are able to join/leave the\nservers set during the execution. These protocols use a new abstraction called\nview generators, that captures the agreement requirements of reconfiguration\nand can be implemented in different system models with different properties.\nParticularly interesting, we present a reconfiguration protocol that is\nmodular, efficient, consensus-free and loosely coupled with read/write\nprotocols, improving the overall system performance.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 22:42:01 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Alchieri", "Eduardo", ""], ["Bessani", "Alysson", ""], ["Greve", "Fabiola", ""], ["Fraga", "Joni", ""]]}, {"id": "1607.05507", "submitter": "Keyou You Dr.", "authors": "Keyou You and Roberto Tempo and Pei Xie", "title": "Distributed Algorithms for Robust Convex Optimization via the Scenario\n  Approach", "comments": "15 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes distributed algorithms to solve robust convex\noptimization (RCO) when the constraints are affected by nonlinear uncertainty.\nWe adopt a scenario approach by randomly sampling the uncertainty set. To\nfacilitate the computational task, instead of using a single centralized\nprocessor to obtain a \"global solution\" of the scenario problem (SP), we resort\nto {\\it multiple interconnected processors} that are distributed among\ndifferent nodes of a network to simultaneously solve the SP. Then, we propose a\nprimal-dual sub-gradient algorithm and a random projection algorithm to\ndistributedly solve the SP over undirected and directed graphs, respectively.\nBoth algorithms are given in an explicit recursive form with simple iterations,\nwhich are especially suited for processors with limited computational\ncapability. We show that, if the underlying graph is strongly connected, each\nnode asymptotically computes a common optimal solution to the SP with a\nconvergence rate $O(1/(\\sum_{t=1}^k\\zeta^t))$ where $\\{\\zeta^t\\}$ is a sequence\nof appropriately decreasing stepsizes. That is, the RCO is effectively solved\nin a distributed way. The relations with the existing literature on robust\nconvex programs are thoroughly discussed and an example of robust system\nidentification is included to validate the effectiveness of our distributed\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 10:24:46 GMT"}, {"version": "v2", "created": "Sun, 14 Jan 2018 12:57:55 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["You", "Keyou", ""], ["Tempo", "Roberto", ""], ["Xie", "Pei", ""]]}, {"id": "1607.05537", "submitter": "Gaurav Khanna", "authors": "Glenn Volkema, Gaurav Khanna", "title": "Scientific Computing Using Consumer Video-Gaming Hardware Devices", "comments": "8 pages, 6 figures; includes benchmark results on common scientific\n  computing kernels", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.DC cs.ET cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commodity video-gaming hardware (consoles, graphics cards, tablets, etc.)\nperformance has been advancing at a rapid pace owing to strong consumer demand\nand stiff market competition. Gaming hardware devices are currently amongst the\nmost powerful and cost-effective computational technologies available in\nquantity. In this article, we evaluate a sample of current generation\nvideo-gaming hardware devices for scientific computing and compare their\nperformance with specialized supercomputing general purpose graphics processing\nunits (GPGPUs). We use the OpenCL SHOC benchmark suite, which is a measure of\nthe performance of compute hardware on various different scientific application\nkernels, and also a popular public distributed computing application,\nEinstein@Home in the field of gravitational physics for the purposes of this\nevaluation.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 12:14:36 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Volkema", "Glenn", ""], ["Khanna", "Gaurav", ""]]}, {"id": "1607.05539", "submitter": "Amir Rastegarnia", "authors": "Vahid Vahidpour, Amir Rastegarnia, Azam Khalili, Saeid Sanei", "title": "Partial Diffusion Recursive Least-Squares for Distributed Estimation\n  under Noisy Links Condition", "comments": "9 Pages, 5 figures, Draft Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SI cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partial diffusion-based recursive least squares (PDRLS) is an effective\nmethod for reducing computational load and power consumption in adaptive\nnetwork implementation. In this method, each node shares a part of its\nintermediate estimate vector with its neighbors at each iteration. PDRLS\nalgorithm reduces the internode communications relative to the full-diffusion\nRLS algorithm. This selection of estimate entries becomes more appealing when\nthe information fuse over noisy links. In this paper, we study the steady-state\nperformance of PDRLS algorithm in presence of noisy links and investigate its\nconvergence in both mean and mean-square senses. We also derive a theoretical\nexpression for its steady-state meansquare deviation (MSD). The simulation\nresults illustrate that the stability conditions for PDRLS under noisy links\nare not sufficient to guarantee its convergence. Strictly speaking, considering\nnonideal links condition adds a new complexity to the estimation problem for\nwhich the PDRLS algorithm becomes unstable and do not converge for any value of\nthe forgetting factor.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 12:19:26 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Vahidpour", "Vahid", ""], ["Rastegarnia", "Amir", ""], ["Khalili", "Azam", ""], ["Sanei", "Saeid", ""]]}, {"id": "1607.05596", "submitter": "Matthieu Perrin", "authors": "Matthieu Perrin, Matoula Petrolia, Achour Mostefaoui (GDD), Claude\n  Jard", "title": "On Composition and Implementation of Sequential Consistency (Extended\n  Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been proved that to implement a linearizable shared memory in\nsynchronous message-passing systems it is necessary to wait for a time\nproportional to the uncertainty in the latency of the network for both read and\nwrite operations, while waiting during read or during write operations is\nsufficient for sequential consistency. This paper extends this result to\ncrash-prone asynchronous systems. We propose a distributed algorithm that\nbuilds a sequentially consistent shared memory abstraction with snapshot on top\nof an asynchronous message-passing system where less than half of the processes\nmay crash. We prove that it is only necessary to wait when a read/snapshot is\nimmediately preceded by a write on the same process. We also show that\nsequential consistency is composable in some cases commonly encountered: 1)\nobjects that would be linearizable if they were implemented on top of a\nlinearizable memory become sequentially consistent when implemented on top of a\nsequential memory while remaining composable and 2) in round-based algorithms,\nwhere each object is only accessed within one round.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 14:22:50 GMT"}, {"version": "v2", "created": "Wed, 20 Jul 2016 14:38:50 GMT"}, {"version": "v3", "created": "Thu, 21 Jul 2016 09:45:49 GMT"}, {"version": "v4", "created": "Fri, 29 Jul 2016 07:45:52 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Perrin", "Matthieu", "", "GDD"], ["Petrolia", "Matoula", "", "GDD"], ["Mostefaoui", "Achour", "", "GDD"], ["Jard", "Claude", ""]]}, {"id": "1607.05597", "submitter": "Ami Paz", "authors": "Keren Censor-Hillel, Telikepalli Kavitha, Ami Paz, Amir Yehudayoff", "title": "Distributed Construction of Purely Additive Spanners", "comments": "An extended abstract of this work will be presented in DISC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the complexity of distributed construction of purely\nadditive spanners in the CONGEST model. We describe algorithms for building\nsuch spanners in several cases. Because of the need to simultaneously make\ndecisions at far apart locations, the algorithms use additional mechanisms\ncompared to their sequential counterparts.\n  We complement our algorithms with a lower bound on the number of rounds\nrequired for computing pairwise spanners. The standard reductions from\nset-disjointness and equality seem unsuitable for this task because no specific\nedge needs to be removed from the graph. Instead, to obtain our lower bound, we\ndefine a new communication complexity problem that reduces to computing a\nsparse spanner, and prove a lower bound on its communication complexity using\ninformation theory. This technique significantly extends the current toolbox\nused for obtaining lower bounds for the CONGEST model, and we believe it may\nfind additional applications.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 14:24:25 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Censor-Hillel", "Keren", ""], ["Kavitha", "Telikepalli", ""], ["Paz", "Ami", ""], ["Yehudayoff", "Amir", ""]]}, {"id": "1607.05635", "submitter": "Petr  Kuznetsov", "authors": "Carole Delporte-Gallet, Hugues Fauconnier, Eli Gafni, Petr Kuznetsov", "title": "Set-Consensus Collections are Decidable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A natural way to measure the power of a distributed-computing model is to\ncharacterize the set of tasks that can be solved in it. %the model. In general,\nhowever, the question of whether a given task can be solved in a given model is\nundecidable, even if we only consider the wait-free shared-memory model.\n  In this paper, we address this question for restricted classes of models and\ntasks. We show that the question of whether a collection $C$ of\n\\emph{$(\\ell,j)$-set consensus} objects, for various $\\ell$ (the number of\nprocesses that can invoke the object) and $j$ (the number of distinct outputs\nthe object returns), can be used by $n$ processes to solve wait-free $k$-set\nconsensus is decidable. Moreover, we provide a simple $O(n^2)$ decision\nalgorithm, based on a dynamic programming solution to the Knapsack optimization\nproblem.\n  We then present an \\emph{adaptive} wait-free set-consensus algorithm that,\nfor each set of participating processes, achieves the best level of agreement\nthat is possible to achieve using $C$. Overall, this gives us a complete\ncharacterization of a read-write model defined by a collection of set-consensus\nobjects through its \\emph{set-consensus power}.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 15:34:20 GMT"}, {"version": "v2", "created": "Tue, 22 Nov 2016 17:24:08 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Delporte-Gallet", "Carole", ""], ["Fauconnier", "Hugues", ""], ["Gafni", "Eli", ""], ["Kuznetsov", "Petr", ""]]}, {"id": "1607.05645", "submitter": "Rajmohan Rajaraman", "authors": "John Augustine and Chen Avin and Mehraneh Liaee and Gopal Pandurangan\n  and Rajmohan Rajaraman", "title": "Information Spreading in Dynamic Networks under Oblivious Adversaries", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of gossip in dynamic networks controlled by an adversary\nthat can modify the network arbitrarily from one round to another, provided\nthat the network is always connected. In the gossip problem, $n$ tokens are\narbitrarily distributed among the $n$ network nodes, and the goal is to\ndisseminate all the $n$ tokens to every node. Our focus is on token-forwarding\nalgorithms, which do not manipulate tokens in any way other than storing,\ncopying, and forwarding them. Gossip can be completed in linear time in any\nstatic network, but a basic open question for dynamic networks is the existence\nof a distributed protocol that can do significantly better than an easily\nachievable bound of $O(n^2)$ rounds.\n  In previous work, it has been shown that under adaptive adversaries, every\ntoken forwarding algorithm requires $\\Omega(n^2/\\log n)$ rounds. In this paper,\nwe study oblivious adversaries, which differ from adaptive adversaries in one\ncrucial aspect--- they are oblivious to random choices made by the protocol. We\npresent an $\\tilde{\\Omega}(n^{3/2})$ lower bound under an oblivious adversary\nfor RANDDIFF, a natural algorithm in which neighbors exchange a token chosen\nuniformly at random from the difference of their token sets. We also present an\n$\\tilde{\\Omega}(n^{4/3})$ bound under a stronger notion of oblivious adversary\nfor symmetric knowledge-based algorithms. On the positive side, we present a\ncentralized algorithm that completes gossip in $\\tilde{O}(n^{3/2})$ rounds. We\nalso show an $\\tilde{O}(n^{5/3})$ upper bound for RANDDIFF in a restricted\nclass of oblivious adversaries, which we call paths-respecting.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 15:59:50 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Augustine", "John", ""], ["Avin", "Chen", ""], ["Liaee", "Mehraneh", ""], ["Pandurangan", "Gopal", ""], ["Rajaraman", "Rajmohan", ""]]}, {"id": "1607.05922", "submitter": "Erich Schikuta", "authors": "Rene Felder, Erich Schikuta", "title": "A XML Based Datagrid Description Language", "comments": "accepted at 1st Workshop on Hardware/Software Support for Parallel\n  and Distributed Scientific and Engineering Computing (SPDSEC-02) in\n  conjunction with The 11th International Conference on Parallel Architectures\n  and Compilation Techniques (PACT-02)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present xDGDL, an approach towards a concise but comprehensive Datagrid\ndescription language. Our framework is based on the portable XML language and\nallows to store syntactical and semantical information together with arbitrary\nfiles. This information can be used to administer, locate, search and process\nthe stored data on the Grid. As an application of the xDGDL approach we present\nViPFS, a novel distributed file system targeting the Grid.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 11:33:46 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Felder", "Rene", ""], ["Schikuta", "Erich", ""]]}, {"id": "1607.06044", "submitter": "Vaneet Aggarwal", "authors": "Vaneet Aggarwal and Tian Lan", "title": "Tail Index for a Distributed Storage System with Pareto File Size\n  Distribution", "comments": "Theorem 1 proof was replaced with a proof that uses the result in\n  [21], thus simplifying the analysis and making the paper concise", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed storage systems often employ erasure codes to achieve high data\nreliability while attaining space efficiency. Such storage systems are known to\nbe susceptible to long tails in response time. It has been shown that in modern\nonline applications such as Bing, Facebook, and Amazon, the long tail of\nlatency is of particular concern, with $99.9$th percentile response times that\nare orders of magnitude worse than the mean. Taming tail latency is very\nchallenging in erasure-coded storage systems since quantify tail latency (i.e.,\n$x$th-percentile latency for arbitrary $x\\in[0,1]$) has been a long-standing\nopen problem. In this paper, we propose a mathematical model to quantify {\\em\ntail index} of service latency for arbitrary erasure-coded storage systems, by\ncharacterizing the asymptotic behavior of latency distribution tails. When file\nsize has a heavy tailed distribution, we find tail index, defined as the\nexponent at which latency tail probability diminishes to zero, in closed-form,\nand further show that a family of probabilistic scheduling algorithms are\n(asymptotically) optimal since they are able to achieve the exact tail index.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 17:51:16 GMT"}, {"version": "v2", "created": "Thu, 3 Aug 2017 00:37:45 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Aggarwal", "Vaneet", ""], ["Lan", "Tian", ""]]}, {"id": "1607.06063", "submitter": "Mohammad Reza Abbasifard", "authors": "Mohammad Reza Abbasifard and Omid Isfahani Alamdari", "title": "Fragment Allocation Configuration in Distributed Database Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": "MODB-201607DDB", "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In distributed database (DDB) management systems, fragment allocation is one\nof the most important components that can directly affect the performance of\nDDB. In this research work, we will show that declarative programming\nlanguages, e.g. logic programming languages, can be used to represent different\ndata fragment allocation techniques. Results indicate that, using declarative\nprogramming language significantly simplifies the representation of fragment\nallocation algorithm, thus opens door for any further developments and\noptimizations. The under consideration case study also show that our approach\ncan be extended to be used in different areas of distributed systems.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 19:10:48 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Abbasifard", "Mohammad Reza", ""], ["Alamdari", "Omid Isfahani", ""]]}, {"id": "1607.06139", "submitter": "Rati Gelashvili", "authors": "Faith Ellen, Rati Gelashvili, Nir Shavit, Leqi Zhu", "title": "A Complexity-Based Hierarchy for Multiprocessor Synchronization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many years, Herlihy's elegant computability based Consensus Hierarchy has\nbeen our best explanation of the relative power of various types of\nmultiprocessor synchronization objects when used in deterministic algorithms.\nHowever, key to this hierarchy is treating synchronization instructions as\ndistinct objects, an approach that is far from the real-world, where\nmultiprocessor programs apply synchronization instructions to collections of\narbitrary memory locations. We were surprised to realize that, when considering\ninstructions applied to memory locations, the computability based hierarchy\ncollapses. This leaves open the question of how to better capture the power of\nvarious synchronization instructions.\n  In this paper, we provide an approach to answering this question. We present\na hierarchy of synchronization instructions, classified by their space\ncomplexity in solving obstruction-free consensus. Our hierarchy provides a\nclassification of combinations of known instructions that seems to fit with our\nintuition of how useful some are in practice, while questioning the\neffectiveness of others. We prove an essentially tight characterization of the\npower of buffered read and write instructions.Interestingly, we show a similar\nresult for multi-location atomic assignments.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 22:25:07 GMT"}, {"version": "v2", "created": "Fri, 4 May 2018 03:27:17 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Ellen", "Faith", ""], ["Gelashvili", "Rati", ""], ["Shavit", "Nir", ""], ["Zhu", "Leqi", ""]]}, {"id": "1607.06156", "submitter": "Chirag Jain", "authors": "Chirag Jain, Patrick Flick, Tony Pan, Oded Green, Srinivas Aluru", "title": "An Adaptive Parallel Algorithm for Computing Connected Components", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient distributed memory parallel algorithm for computing\nconnected components in undirected graphs based on Shiloach-Vishkin's PRAM\napproach. We discuss multiple optimization techniques that reduce communication\nvolume as well as load-balance the algorithm. We also note that the efficiency\nof the parallel graph connectivity algorithm depends on the underlying graph\ntopology. Particularly for short diameter graph components, we observe that\nparallel Breadth First Search (BFS) method offers better performance. However,\nrunning parallel BFS is not efficient for computing large diameter components\nor large number of small components. To address this challenge, we employ a\nheuristic that allows the algorithm to quickly predict the type of the network\nby computing the degree distribution and follow the optimal hybrid route. Using\nlarge graphs with diverse topologies from domains including metagenomics, web\ncrawl, social graph and road networks, we show that our hybrid implementation\nis efficient and scalable for each of the graph types. Our approach achieves a\nruntime of 215 seconds using 32K cores of Cray XC30 for a metagenomic graph\nwith over 50 billion edges. When compared against the previous state-of-the-art\nmethod, we see performance improvements up to 24x.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 00:03:40 GMT"}, {"version": "v2", "created": "Tue, 6 Dec 2016 21:50:57 GMT"}, {"version": "v3", "created": "Mon, 13 Feb 2017 20:41:10 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Jain", "Chirag", ""], ["Flick", "Patrick", ""], ["Pan", "Tony", ""], ["Green", "Oded", ""], ["Aluru", "Srinivas", ""]]}, {"id": "1607.06258", "submitter": "Matthieu Perrin", "authors": "Matthieu Perrin, Matoula Petrolia, Achour Mostefaoui (GDD), Claude\n  Jard", "title": "On Composition and Implementation of Sequential Consistency", "comments": "in 30th International Symposium on Distributed Computing, Sep 2016,\n  Paris, France", "journal-ref": null, "doi": null, "report-no": "hal-01347069", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To implement a linearizable shared memory in synchronous message-passing\nsystems it is necessary to wait for a time linear to the uncertainty in the\nlatency of the network for both read and write operations. Waiting only for one\nof them suffices for sequential consistency. This paper extends this result to\ncrash-prone asynchronous systems, proposing a distributed algorithm building a\nsequentially consistent shared snapshot memory on top of an asynchronous\nmessage-passing system where less than half of the processes may crash. We\nprove that waiting is needed only when a process invokes a read/snapshot right\nafter a write. We also show that sequential consistency is composable in some\ncases commonly encountered: 1) objects that would be linearizable if they were\nimplemented on top of a linearizable memory become sequentially consistent when\nimplemented on top of a sequential memory while remaining composable and 2) in\nround-based algorithms, where each object is only accessed within one round.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 10:44:49 GMT"}, {"version": "v2", "created": "Wed, 27 Jul 2016 13:21:44 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Perrin", "Matthieu", "", "GDD"], ["Petrolia", "Matoula", "", "GDD"], ["Mostefaoui", "Achour", "", "GDD"], ["Jard", "Claude", ""]]}, {"id": "1607.06269", "submitter": "Minxian Xu", "authors": "Minxian Xu, Wenhong Tian, Rajkumar Buyya", "title": "A Survey on Load Balancing Algorithms for VM Placement in Cloud\n  Computing", "comments": "22 Pages, 4 Figures, 4 Tables, in press", "journal-ref": "Concurrency & Computation: Practice & Experience, 2017", "doi": "10.1002/cpe.4123", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of cloud computing based on virtualization technologies brings\nhuge opportunities to host virtual resource at low cost without the need of\nowning any infrastructure. Virtualization technologies enable users to acquire,\nconfigure and be charged on pay-per-use basis. However, Cloud data centers\nmostly comprise heterogeneous commodity servers hosting multiple virtual\nmachines (VMs) with potential various specifications and fluctuating resource\nusages, which may cause imbalanced resource utilization within servers that may\nlead to performance degradation and service level agreements (SLAs) violations.\nTo achieve efficient scheduling, these challenges should be addressed and\nsolved by using load balancing strategies, which have been proved to be NP-hard\nproblem. From multiple perspectives, this work identifies the challenges and\nanalyzes existing algorithms for allocating VMs to PMs in infrastructure\nClouds, especially focuses on load balancing. A detailed classification\ntargeting load balancing algorithms for VM placement in cloud data centers is\ninvestigated and the surveyed algorithms are classified according to the\nclassification. The goal of this paper is to provide a comprehensive and\ncomparative understanding of existing literature and aid researchers by\nproviding an insight for potential future enhancements.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 11:19:06 GMT"}, {"version": "v2", "created": "Wed, 27 Jul 2016 02:12:10 GMT"}, {"version": "v3", "created": "Wed, 8 Feb 2017 06:50:49 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Xu", "Minxian", ""], ["Tian", "Wenhong", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "1607.06303", "submitter": "Oded Schwartz", "authors": "Vadim Stotland, Oded Schwartz, and Sivan Toledo", "title": "High-Performance Algorithms for Computing the Sign Function of\n  Triangular Matrices", "comments": "18 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.DC math.NA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Algorithms and implementations for computing the sign function of a\ntriangular matrix are fundamental building blocks in algorithms for computing\nthe sign of arbitrary square real or complex matrices. We present novel\nrecursive and cache efficient algorithms that are based on Higham's stabilized\nspecialization of Parlett's substitution algorithm for computing the sign of a\ntriangular matrix. We show that the new recursive algorithms are asymptotically\noptimal in terms of the number of cache misses that they generate. One of the\nnovel algorithms that we present performs more arithmetic than the\nnon-recursive version, but this allows it to benefit from calling\nhighly-optimized matrix-multiplication routines; the other performs the same\nnumber of operations as the non-recursive version, but it uses custom\ncomputational kernels instead. We present implementations of both, as well as a\ncache-efficient implementation of a block version of Parlett's algorithm. Our\nexperiments show that the blocked and recursive versions are much faster than\nthe previous algorithms, and that the inertia strongly influences their\nrelative performance, as predicted by our analysis.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 12:58:36 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Stotland", "Vadim", ""], ["Schwartz", "Oded", ""], ["Toledo", "Sivan", ""]]}, {"id": "1607.06541", "submitter": "Jeremy Kepner", "authors": "William S. Song, Vitaliy Gleyzer, Alexei Lomakin, Jeremy Kepner", "title": "Novel Graph Processor Architecture, Prototype System, and Results", "comments": "7 pages, 8 figures, IEEE HPEC 2016", "journal-ref": null, "doi": "10.1109/HPEC.2016.7761635", "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph algorithms are increasingly used in applications that exploit large\ndatabases. However, conventional processor architectures are inadequate for\nhandling the throughput and memory requirements of graph computation. Lincoln\nLaboratory's graph-processor architecture represents a rethinking of parallel\narchitectures for graph problems. Our processor utilizes innovations that\ninclude a sparse matrix-based graph instruction set, a cacheless memory system,\naccelerator-based architecture, a systolic sorter, high-bandwidth\nmulti-dimensional toroidal communication network, and randomized\ncommunications. A field-programmable gate array (FPGA) prototype of the new\ngraph processor has been developed with significant performance enhancement\nover conventional processors in graph computational throughput.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 02:22:44 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Song", "William S.", ""], ["Gleyzer", "Vitaliy", ""], ["Lomakin", "Alexei", ""], ["Kepner", "Jeremy", ""]]}, {"id": "1607.06543", "submitter": "Jeremy Kepner", "authors": "Chansup Byun, Jeremy Kepner, William Arcand, David Bestor, Bill\n  Bergeron, Vijay Gadepally, Matthew Hubbell, Peter Michaleas, Julie Mullen,\n  Andrew Prout, Antonio Rosa, Charles Yee, Albert Reuther", "title": "LLMapReduce: Multi-Level Map-Reduce for High Performance Data Analysis", "comments": "8 pages; 19 figures; IEEE HPEC 2016", "journal-ref": null, "doi": "10.1109/HPEC.2016.7761618", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The map-reduce parallel programming model has become extremely popular in the\nbig data community. Many big data workloads can benefit from the enhanced\nperformance offered by supercomputers. LLMapReduce provides the familiar\nmap-reduce parallel programming model to big data users running on a\nsupercomputer. LLMapReduce dramatically simplifies map-reduce programming by\nproviding simple parallel programming capability in one line of code.\nLLMapReduce supports all programming languages and many schedulers. LLMapReduce\ncan work with any application without the need to modify the application.\nFurthermore, LLMapReduce can overcome scaling limits in the map-reduce parallel\nprogramming model via options that allow the user to switch to the more\nefficient single-program-multiple-data (SPMD) parallel programming model. These\nfeatures allow users to reduce the computational overhead by more than 10x\ncompared to standard map-reduce for certain applications. LLMapReduce is widely\nused by hundreds of users at MIT. Currently LLMapReduce works with several\nschedulers such as SLURM, Grid Engine and LSF.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 02:45:53 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Byun", "Chansup", ""], ["Kepner", "Jeremy", ""], ["Arcand", "William", ""], ["Bestor", "David", ""], ["Bergeron", "Bill", ""], ["Gadepally", "Vijay", ""], ["Hubbell", "Matthew", ""], ["Michaleas", "Peter", ""], ["Mullen", "Julie", ""], ["Prout", "Andrew", ""], ["Rosa", "Antonio", ""], ["Yee", "Charles", ""], ["Reuther", "Albert", ""]]}, {"id": "1607.06544", "submitter": "Albert Reuther", "authors": "Albert Reuther, Chansup Byun, William Arcand, David Bestor, Bill\n  Bergeron, Matthew Hubbell, Michael Jones, Peter Michaleas, Andrew Prout,\n  Antonio Rosa, Jeremy Kepner", "title": "Scheduler Technologies in Support of High Performance Data Analysis", "comments": "6 pages, 5 figures, IEEE High Performance Extreme Computing\n  Conference 2016", "journal-ref": null, "doi": "10.1109/HPEC.2016.7761604", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Job schedulers are a key component of scalable computing infrastructures.\nThey orchestrate all of the work executed on the computing infrastructure and\ndirectly impact the effectiveness of the system. Recently, job workloads have\ndiversified from long-running, synchronously-parallel simulations to include\nshort-duration, independently parallel high performance data analysis (HPDA)\njobs. Each of these job types requires different features and scheduler tuning\nto run efficiently. A number of schedulers have been developed to address both\njob workload and computing system heterogeneity. High performance computing\n(HPC) schedulers were designed to schedule large-scale scientific modeling and\nsimulations on supercomputers. Big Data schedulers were designed to schedule\ndata processing and analytic jobs on clusters. This paper compares and\ncontrasts the features of HPC and Big Data schedulers with a focus on\naccommodating both scientific computing and high performance data analytic\nworkloads. Job latency is critical for the efficient utilization of scalable\ncomputing infrastructures, and this paper presents the results of job launch\nbenchmarking of several current schedulers: Slurm, Son of Grid Engine, Mesos,\nand Yarn. We find that all of these schedulers have low utilization for\nshort-running jobs. Furthermore, employing multilevel scheduling significantly\nimproves the utilization across all schedulers.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 03:02:04 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Reuther", "Albert", ""], ["Byun", "Chansup", ""], ["Arcand", "William", ""], ["Bestor", "David", ""], ["Bergeron", "Bill", ""], ["Hubbell", "Matthew", ""], ["Jones", "Michael", ""], ["Michaleas", "Peter", ""], ["Prout", "Andrew", ""], ["Rosa", "Antonio", ""], ["Kepner", "Jeremy", ""]]}, {"id": "1607.06658", "submitter": "Ilke Zilci", "authors": "Beg\\\"um \\.Ilke Zilci, Mathias Slawik, Axel K\\\"upper", "title": "Cloud Service Matchmaking using Constraint Programming", "comments": null, "journal-ref": null, "doi": "10.1109/WETICE.2015.44", "report-no": null, "categories": "cs.DC cs.SE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Service requesters with limited technical knowledge should be able to compare\nservices based on their quality of service (QoS) requirements in cloud service\nmarketplaces. Existing service matching approaches focus on QoS requirements as\ndiscrete numeric values and intervals. The analysis of existing research on\nnon-functional properties reveals two improvement opportunities: list-typed QoS\nproperties as well as explicit handling of preferences for lower or higher\nproperty values. We develop a concept and constraint models for a service\nmatcher which contributes to existing approaches by addressing these issues\nusing constraint solvers. The prototype uses an API at the standardisation\nstage and discovers implementation challenges. This paper concludes that\nconstraint solvers provide a valuable tool to solve the service matching\nproblem with soft constraints and are capable of covering all QoS property\ntypes in our analysis. Our approach is to be further investigated in the\napplication context of cloud federations.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 12:53:37 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Zilci", "Beg\u00fcm \u0130lke", ""], ["Slawik", "Mathias", ""], ["K\u00fcpper", "Axel", ""]]}, {"id": "1607.06674", "submitter": "Ilke Zilci", "authors": "Beg\\\"um \\.Ilke Zilci, Mathias Slawik, Axel K\\\"upper", "title": "Cloud Service Matchmaking Approaches: A Systematic Literature Survey", "comments": null, "journal-ref": null, "doi": "10.1109/DEXA.2015.50", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Service matching concerns finding suitable services according to the service\nrequester's requirements, which is a complex task due to the increasing number\nand diversity of cloud services available. Service matching is discussed in web\nservices composition and user oriented service marketplaces contexts. The\nsuggested approaches have different problem definitions and have to be examined\ncloser in order to identify comparable results and to find out which approaches\nhave built on the former ones. One of the most important use cases is service\nrequesters with limited technical knowledge who need to compare services based\non their QoS requirements in cloud service marketplaces. Our survey examines\nthe service matching approaches in order to find out the relation between their\ncontext and their objectives. Moreover, it evaluates their applicability for\nthe cloud service marketplaces context.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 13:28:08 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Zilci", "Beg\u00fcm \u0130lke", ""], ["Slawik", "Mathias", ""], ["K\u00fcpper", "Axel", ""]]}, {"id": "1607.06688", "submitter": "Ilke Zilci", "authors": "Mathias Slawik, Beg\\\"um \\.Ilke Zilci, Yuri Demchenko, Jos\\'e Ignacio\n  Aznar Baranda, Robert Branchat, Charles Loomis, Oleg Lodygensky, Christophe\n  Blanchet", "title": "CYCLONE Unified Deployment and Management of Federated, Multi-Cloud\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Various Cloud layers have to work in concert in order to manage and deploy\ncomplex multi-cloud applications, executing sophisticated workflows for Cloud\nresource deployment, activation, adjustment, interaction, and monitoring. While\nthere are ample solutions for managing individual Cloud aspects (e.g. network\ncontrollers, deployment tools, and application security software), there are no\nwell-integrated suites for managing an entire multi cloud environment with\nmultiple providers and deployment models. This paper presents the CYCLONE\narchitecture that integrates a number of existing solutions to create an open,\nunified, holistic Cloud management platform for multi-cloud applications,\ntailored to the needs of research organizations and SMEs. It discusses major\nchallenges in providing a network and security infrastructure for the\nIntercloud and concludes with the demonstration how the architecture is\nimplemented in a real life bioinformatics use case.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 14:18:22 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Slawik", "Mathias", ""], ["Zilci", "Beg\u00fcm \u0130lke", ""], ["Demchenko", "Yuri", ""], ["Baranda", "Jos\u00e9 Ignacio Aznar", ""], ["Branchat", "Robert", ""], ["Loomis", "Charles", ""], ["Lodygensky", "Oleg", ""], ["Blanchet", "Christophe", ""]]}, {"id": "1607.06883", "submitter": "Peter Robinson", "authors": "Gopal Pandurangan, Peter Robinson, Michele Scquizzato", "title": "A Time- and Message-Optimal Distributed Algorithm for Minimum Spanning\n  Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a randomized Las Vegas distributed algorithm that\nconstructs a minimum spanning tree (MST) in weighted networks with optimal (up\nto polylogarithmic factors) time and message complexity. This algorithm runs in\n$\\tilde{O}(D + \\sqrt{n})$ time and exchanges $\\tilde{O}(m)$ messages (both with\nhigh probability), where $n$ is the number of nodes of the network, $D$ is the\ndiameter, and $m$ is the number of edges. This is the first distributed MST\nalgorithm that matches \\emph{simultaneously} the time lower bound of\n$\\tilde{\\Omega}(D + \\sqrt{n})$ [Elkin, SIAM J. Comput. 2006] and the message\nlower bound of $\\Omega(m)$ [Kutten et al., J.ACM 2015] (which both apply to\nrandomized algorithms).\n  The prior time and message lower bounds are derived using two completely\ndifferent graph constructions; the existing lower bound construction that shows\none lower bound {\\em does not} work for the other. To complement our algorithm,\nwe present a new lower bound graph construction for which any distributed MST\nalgorithm requires \\emph{both} $\\tilde{\\Omega}(D + \\sqrt{n})$ rounds and\n$\\Omega(m)$ messages.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jul 2016 03:22:38 GMT"}, {"version": "v2", "created": "Fri, 4 Nov 2016 17:49:23 GMT"}, {"version": "v3", "created": "Wed, 24 Jan 2018 04:08:38 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Pandurangan", "Gopal", ""], ["Robinson", "Peter", ""], ["Scquizzato", "Michele", ""]]}, {"id": "1607.07348", "submitter": "Anastasios Gounaris", "authors": "Panagiotis Petridis, Anastasios Gounaris and Jordi Torres", "title": "Spark Parameter Tuning via Trial-and-Error", "comments": "full version of paper accepted in the 2nd INNS Conference on Big Data\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spark has been established as an attractive platform for big data analysis,\nsince it manages to hide most of the complexities related to parallelism, fault\ntolerance and cluster setting from developers. However, this comes at the\nexpense of having over 150 configurable parameters, the impact of which cannot\nbe exhaustively examined due to the exponential amount of their combinations.\nThe default values allow developers to quickly deploy their applications but\nleave the question as to whether performance can be improved open. In this\nwork, we investigate the impact of the most important of the tunable Spark\nparameters on the application performance and guide developers on how to\nproceed to changes to the default values. We conduct a series of experiments\nwith known benchmarks on the MareNostrum petascale supercomputer to test the\nperformance sensitivity. More importantly, we offer a trial-and-error\nmethodology for tuning parameters in arbitrary applications based on evidence\nfrom a very small number of experimental runs. We test our methodology in three\ncase studies, where we manage to achieve speedups of more than 10 times.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2016 16:28:14 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Petridis", "Panagiotis", ""], ["Gounaris", "Anastasios", ""], ["Torres", "Jordi", ""]]}, {"id": "1607.07696", "submitter": "George Tsatsanifos", "authors": "George Tsatsanifos", "title": "Parallelized Proximity-Based Query Processing Methods for Road Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose a paradigm for processing in parallel graph joins\nin road networks. The methodology we present can be used for distance join\nprocessing among the elements of two disjoint sets R,S of nodes from the road\nnetwork, with R preceding S, and we are in search for the pairs of vertices\n(u,v), where u in R and v in S, such that dist(u,v) < {\\theta}. Another\nvariation of the problem would involve retrieving the k closest pairs (u,v) in\nthe road network with u in R and v in S, such that dist(u,v) <= dist(w,y),\nwhere w,y do not belong in the result.\n  We reckon that this is an extremely useful paradigm with many practical\napplications. A typical example of usage of our methods would be to find the\npairs of restaurants and bars (in that order) from which to select for a night\nout, that either fall within walking distance for example, or just the k\nclosest pairs, depending on the parameters. Another entirely different scenario\nwould involve finding the points of two distinct trajectories that are within a\ncertain distance predicate, or the k closest such points. For example, we would\nlike to transfer from one train to another a few tones of freight, and hence,\nwe want to minimize the distance we have to cover for moving the cargo from the\ncarrying train to the other. We reckon that this endeavor of ours covers\nexactly those needs for processing such queries efficiently.\n  Moreover, for the specific purposes of this paper, we also propose a novel\nheuristic graph partitioning scheme. It resembles a recursive bisection method,\nand is tailored to the requirements of the problem, targeting at establishing\nwell separated partitions, so as to allow computations to be performed\nsimultaneously and independently within each partition, unlike hitherto work\nthat aims at minimizing either the number of edges among different partitions,\nor the number of nodes thereof.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 13:51:27 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Tsatsanifos", "George", ""]]}, {"id": "1607.07761", "submitter": "Qiang Zhu Professor", "authors": "Qiang Zhu (Member, IEEE), Fang Ma, Guodong Guo (Senior Member, IEEE),\n  Dajin Wang, Weisheng Chen (Member, IEEE)", "title": "From Graph Isoperimetric Inequality to Network Connectivity -- A New\n  Approach", "comments": "17 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new, novel approach to obtaining a network's connectivity. More\nspecifically, we show that there exists a relationship between a network's\ngraph isoperimetric properties and its conditional connectivity. A network's\nconnectivity is the minimum number of nodes, whose removal will cause the\nnetwork disconnected. It is a basic and important measure for the network's\nreliability, hence its overall robustness. Several conditional connectivities\nhave been proposed in the past for the purpose of accurately reflecting various\nrealistic network situations, with extra connectivity being one such\nconditional connectivity. In this paper, we will use isoperimetric properties\nof the hypercube network to obtain its extra connectivity. The result of the\npaper for the first time establishes a relationship between the age-old\nisoperimetric problem and network connectivity.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 06:01:52 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Zhu", "Qiang", "", "Member, IEEE"], ["Ma", "Fang", "", "Senior Member, IEEE"], ["Guo", "Guodong", "", "Senior Member, IEEE"], ["Wang", "Dajin", "", "Member, IEEE"], ["Chen", "Weisheng", "", "Member, IEEE"]]}, {"id": "1607.07763", "submitter": "Eric Kerrigan", "authors": "Mason Thammawichai and Eric C. Kerrigan", "title": "Energy-Efficient Real-Time Scheduling for Two-Type Heterogeneous\n  Multiprocessors", "comments": null, "journal-ref": "Real-Time Systems, 2017", "doi": "10.1007/s11241-017-9291-6", "report-no": null, "categories": "cs.DC cs.OS cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose three novel mathematical optimization formulations that solve the\nsame two-type heterogeneous multiprocessor scheduling problem for a real-time\ntaskset with hard constraints. Our formulations are based on a global\nscheduling scheme and a fluid model. The first formulation is a mixed-integer\nnonlinear program, since the scheduling problem is intuitively considered as an\nassignment problem. However, by changing the scheduling problem to first\ndetermine a task workload partition and then to find the execution order of all\ntasks, the computation time can be significantly reduced. Specifically, the\nworkload partitioning problem can be formulated as a continuous nonlinear\nprogram for a system with continuous operating frequency, and as a continuous\nlinear program for a practical system with a discrete speed level set. The task\nordering problem can be solved by an algorithm with a complexity that is linear\nin the total number of tasks. The work is evaluated against existing global\nenergy/feasibility optimal workload allocation formulations. The results\nillustrate that our algorithms are both feasibility optimal and energy optimal\nfor both implicit and constrained deadline tasksets. Specifically, our\nalgorithm can achieve up to 40% energy saving for some simulated tasksets with\nconstrained deadlines. The benefit of our formulation compared with existing\nwork is that our algorithms can solve a more general class of scheduling\nproblems due to incorporating a scheduling dynamic model in the formulations\nand allowing for a time-varying speed profile. Moreover, our algorithms can be\napplied to both online and offline scheduling schemes.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 14:52:57 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Thammawichai", "Mason", ""], ["Kerrigan", "Eric C.", ""]]}, {"id": "1607.07841", "submitter": "Stijn Volckaert", "authors": "Stijn Volckaert, Bjorn De Sutter, Koen De Bosschere, Per Larsen", "title": "Multi-Variant Execution of Parallel Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-Variant Execution Environments (MVEEs) are a promising technique to\nprotect software against memory corruption attacks. They transparently execute\nmultiple, diversified variants (often referred to as replicae) of the software\nreceiving the same inputs. By enforcing and monitoring the lock-step execution\nof the replicae's system calls, and by deploying diversity techniques that\nprevent an attacker from simultaneously compromising multiple replicae, MVEEs\ncan block attacks before they succeed.\n  Existing MVEEs cannot handle non-trivial multi-threaded programs because\ntheir undeterministic behavior introduces benign system call inconsistencies in\nthe replicae, which trigger false positive detections and deadlocks in the\nMVEEs. This paper for the first time extends the generality of MVEEs to protect\nmulti-threaded software by means of secure and efficient synchronization\nreplication agents. On the PARSEC 2.1 parallel benchmarks running with four\nworker threads, our prototype MVEE incurs a run-time overhead of only 1.32x.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 19:02:15 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Volckaert", "Stijn", ""], ["De Sutter", "Bjorn", ""], ["De Bosschere", "Koen", ""], ["Larsen", "Per", ""]]}, {"id": "1607.07846", "submitter": "Marco Netto", "authors": "Artur Baruchi, Edson T. Midorikawa, Liria M. Sato, Marco A. S. Netto", "title": "Exploiting Workload Cycles for Orchestration of Virtual Machine Live\n  Migrations in Clouds", "comments": "30 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual machine live migration in cloud environments aims at reducing energy\ncosts and increasing resource utilization. However, its potential has not been\nfully explored because of simultaneous migrations that may cause user\napplication performance degradation and network congestion. Research efforts on\nlive migration orchestration policies still mostly rely on system level\nmetrics. This work introduces an Application-aware Live Migration Architecture\n(ALMA) that selects suitable moments for migrations using application\ncharacterization data. This characterization consists in recognizing resource\nusage cycles via Fast Fourier Transform. From our experiments, live migration\ntimes were reduced by up to 74% for benchmarks and by up to 67% for real\napplications, when compared to migration policies with no application workload\nanalysis. Network data transfer during the live migration was reduced by up to\n62%.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 19:10:53 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Baruchi", "Artur", ""], ["Midorikawa", "Edson T.", ""], ["Sato", "Liria M.", ""], ["Netto", "Marco A. S.", ""]]}, {"id": "1607.07995", "submitter": "Jiajun Cao", "authors": "Jiajun Cao, Kapil Arya, Rohan Garg, Shawn Matott, Dhabaleswar K.\n  Panda, Hari Subramoni, J\\'er\\^ome Vienne, Gene Cooperman", "title": "System-level Scalable Checkpoint-Restart for Petascale Computing", "comments": "18 pages, 5 figures, to be published in ICPADS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fault tolerance for the upcoming exascale generation has long been an area of\nactive research. One of the components of a fault tolerance strategy is\ncheckpointing. Petascale-level checkpointing is demonstrated through a new\nmechanism for virtualization of the InfiniBand UD (unreliable datagram) mode,\nand for updating the remote address on each UD-based send, due to lack of a\nfixed peer. Note that InfiniBand UD is required to support modern MPI\nimplementations. An extrapolation from the current results to future SSD-based\nstorage systems provides evidence that the current approach will remain\npractical in the exascale generation. This transparent checkpointing approach\nis evaluated using a framework of the DMTCP checkpointing package. Results are\nshown for HPCG (linear algebra), NAMD (molecular dynamics), and the NAS NPB\nbenchmarks. In tests up to 32,752 MPI processes on 32,752 CPU cores,\ncheckpointing of a computation with a 38 TB memory footprint in 11 minutes is\ndemonstrated. Runtime overhead is reduced to less than 1%. The approach is also\nevaluated across three widely used MPI implementations.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 07:46:13 GMT"}, {"version": "v2", "created": "Sat, 24 Sep 2016 01:49:03 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Cao", "Jiajun", ""], ["Arya", "Kapil", ""], ["Garg", "Rohan", ""], ["Matott", "Shawn", ""], ["Panda", "Dhabaleswar K.", ""], ["Subramoni", "Hari", ""], ["Vienne", "J\u00e9r\u00f4me", ""], ["Cooperman", "Gene", ""]]}, {"id": "1607.08220", "submitter": "Mostofa Patwary", "authors": "Md. Mostofa Ali Patwary, Nadathur Rajagopalan Satish, Narayanan\n  Sundaram, Jialin Liu, Peter Sadowski, Evan Racah, Suren Byna, Craig Tull,\n  Wahid Bhimji, Prabhat, Pradeep Dubey", "title": "PANDA: Extreme Scale Parallel K-Nearest Neighbor on Distributed\n  Architectures", "comments": "11 pages in PANDA: Extreme Scale Parallel K-Nearest Neighbor on\n  Distributed Architectures, Md. Mostofa Ali Patwary et.al., IEEE International\n  Parallel and Distributed Processing Symposium (IPDPS), 2016", "journal-ref": null, "doi": "10.1109/IPDPS.2016.57", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing $k$-Nearest Neighbors (KNN) is one of the core kernels used in many\nmachine learning, data mining and scientific computing applications. Although\nkd-tree based $O(\\log n)$ algorithms have been proposed for computing KNN, due\nto its inherent sequentiality, linear algorithms are being used in practice.\nThis limits the applicability of such methods to millions of data points, with\nlimited scalability for Big Data analytics challenges in the scientific domain.\nIn this paper, we present parallel and highly optimized kd-tree based KNN\nalgorithms (both construction and querying) suitable for distributed\narchitectures. Our algorithm includes novel approaches for pruning search space\nand improving load balancing and partitioning among nodes and threads. Using\nTB-sized datasets from three science applications: astrophysics, plasma\nphysics, and particle physics, we show that our implementation can construct\nkd-tree of 189 billion particles in 48 seconds on utilizing $\\sim$50,000 cores.\nWe also demonstrate computation of KNN of 19 billion queries in 12 seconds. We\ndemonstrate almost linear speedup both for shared and distributed memory\ncomputers. Our algorithms outperforms earlier implementations by more than\norder of magnitude; thereby radically improving the applicability of our\nimplementation to state-of-the-art Big Data analytics problems. In addition, we\nshowcase performance and scalability on the recently released Intel Xeon Phi\nprocessor showing that our algorithm scales well even on massively parallel\narchitectures.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 19:13:07 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Patwary", "Md. Mostofa Ali", ""], ["Satish", "Nadathur Rajagopalan", ""], ["Sundaram", "Narayanan", ""], ["Liu", "Jialin", ""], ["Sadowski", "Peter", ""], ["Racah", "Evan", ""], ["Byna", "Suren", ""], ["Tull", "Craig", ""], ["Bhimji", "Wahid", ""], ["Prabhat", "", ""], ["Dubey", "Pradeep", ""]]}, {"id": "1607.08322", "submitter": "Kenneth Shum", "authors": "Kenneth W. Shum and Junyu Chen", "title": "Cooperative Repair of Multiple Node Failures in Distributed Storage\n  Systems", "comments": "Accepted for publication in Int. J. Information and Coding Theory\n  (special issue on Information and Coding Theory for Data Storage). Part of\n  this paper was presented in \"Repairing multiple failures in the\n  Suh-Ramchandran regenerating codes\", IEEE Symp. on Information Theory,\n  Istanbul, July 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cooperative regenerating codes are designed for repairing multiple node\nfailures in distributed storage systems. In contrast to the original repair\nmodel of regenerating codes, which are for the repair of single node failure,\ndata exchange among the new nodes is enabled. It is known that further\nreduction in repair bandwidth is possible with cooperative repair. Currently in\nthe literature, we have an explicit construction of exact-repair cooperative\ncode achieving all parameters corresponding to the minimum-bandwidth point. We\ngive a slightly generalized and more flexible version of this cooperative\nregenerating code in this paper. For minimum-storage regeneration with\ncooperation, we present an explicit code construction which can jointly repair\nany number of systematic storage nodes.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 05:50:18 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Shum", "Kenneth W.", ""], ["Chen", "Junyu", ""]]}, {"id": "1607.08325", "submitter": "Gianmarco De Francisci Morales", "authors": "Nicolas Kourtellis and Gianmarco De Francisci Morales and Albert Bifet\n  and Arinto Murdopo", "title": "VHT: Vertical Hoeffding Tree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  IoT Big Data requires new machine learning methods able to scale to large\nsize of data arriving at high speed. Decision trees are popular machine\nlearning models since they are very effective, yet easy to interpret and\nvisualize. In the literature, we can find distributed algorithms for learning\ndecision trees, and also streaming algorithms, but not algorithms that combine\nboth features. In this paper we present the Vertical Hoeffding Tree (VHT), the\nfirst distributed streaming algorithm for learning decision trees. It features\na novel way of distributing decision trees via vertical parallelism. The\nalgorithm is implemented on top of Apache SAMOA, a platform for mining\ndistributed data streams, and thus able to run on real-world clusters. We run\nseveral experiments to study the accuracy and throughput performance of our new\nVHT algorithm, as well as its ability to scale while keeping its superior\nperformance with respect to non-distributed decision trees.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 06:15:24 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Kourtellis", "Nicolas", ""], ["Morales", "Gianmarco De Francisci", ""], ["Bifet", "Albert", ""], ["Murdopo", "Arinto", ""]]}, {"id": "1607.08344", "submitter": "Nicolas Gilberto Gutierrez Ortiz", "authors": "Nicolas Gutierrez and Manuela Wiesinger-Widi", "title": "AUGURY: A time-series based application for the analysis and forecasting\n  of system and network performance metrics", "comments": "8 pages, 9 figures, submitted to SYNASC2016", "journal-ref": null, "doi": "10.1109/SYNASC.2016.062", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents AUGURY, an application for the analysis of monitoring\ndata from computers, servers or cloud infrastructures. The analysis is based on\nthe extraction of patterns and trends from historical data, using elements of\ntime-series analysis. The purpose of AUGURY is to aid a server administrator by\nforecasting the behaviour and resource usage of specific applications and in\npresenting a status report in a concise manner. AUGURY provides tools for\nidentifying network traffic congestion and peak usage times, and for making\nmemory usage projections. The application data processing specialises in two\ntasks: the parametrisation of the memory usage of individual applications and\nthe extraction of the seasonal component from network traffic data. AUGURY uses\na different underlying assumption for each of these two tasks. With respect to\nthe memory usage, a limited number of single-valued parameters are assumed to\nbe sufficient to parameterize any application being hosted on the server.\nRegarding the network traffic data, long-term patterns, such as hourly or daily\nexist and are being induced by work-time schedules and automatised\nadministrative jobs. In this paper, the implementation of each of the two tasks\nis presented, tested using locally-generated data, and applied to data from\nweather forecasting applications hosted on a web server. This data is used to\ndemonstrate the insight that AUGURY can add to the monitoring of server and\ncloud infrastructures.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 08:02:29 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Gutierrez", "Nicolas", ""], ["Wiesinger-Widi", "Manuela", ""]]}, {"id": "1607.08352", "submitter": "Chris Dowden", "authors": "Chris Dowden", "title": "Reaching Majority Agreement in a Disconnected Network", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of reaching majority agreement in a disconnected\nnetwork. We obtain conditions under which such an agreement is certainly\npossible/impossible, and observe that these coincide in the ternary case.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 08:36:17 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Dowden", "Chris", ""]]}, {"id": "1607.08388", "submitter": "Jingwei Li", "authors": "Chuan Qin and Jingwei Li and Patrick P. C. Lee", "title": "The Design and Implementation of a Rekeying-aware Encrypted\n  Deduplication Storage System", "comments": null, "journal-ref": "ACM Transactions on Storage (2017)", "doi": "10.1145/3032966", "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rekeying refers to an operation of replacing an existing key with a new key\nfor encryption. It renews security protection, so as to protect against key\ncompromise and enable dynamic access control in cryptographic storage. However,\nit is non-trivial to realize efficient rekeying in encrypted deduplication\nstorage systems, which use deterministic content-derived encryption keys to\nallow deduplication on ciphertexts. We design and implement REED, a\nrekeying-aware encrypted deduplication storage system. REED builds on a\ndeterministic version of all-or-nothing transform (AONT), such that it enables\nsecure and lightweight rekeying, while preserving the deduplication capability.\nWe propose two REED encryption schemes that trade between performance and\nsecurity, and extend REED for dynamic access control. We implement a REED\nprototype with various performance optimization techniques and demonstrate how\nwe can exploit similarity to mitigate key generation overhead. Our trace-driven\ntestbed evaluation shows that our REED prototype maintains high performance and\nstorage efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 10:05:24 GMT"}, {"version": "v2", "created": "Mon, 19 Dec 2016 05:05:23 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Qin", "Chuan", ""], ["Li", "Jingwei", ""], ["Lee", "Patrick P. C.", ""]]}, {"id": "1607.08502", "submitter": "Christian Glusa", "authors": "Mark Ainsworth, Christian Glusa", "title": "Is the Multigrid Method Fault Tolerant? The Multilevel Case", "comments": "25 pages, 10 figures", "journal-ref": null, "doi": "10.1137/16M1100691", "report-no": null, "categories": "math.NA cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing at the exascale level is expected to be affected by a significantly\nhigher rate of faults, due to increased component counts as well as power\nconsiderations. Therefore, current day numerical algorithms need to be\nreexamined as to determine if they are fault resilient, and which critical\noperations need to be safeguarded in order to obtain performance that is close\nto the ideal fault-free method.\n  In a previous paper, a framework for the analysis of random stationary linear\niterations was presented and applied to the two grid method. The present work\nis concerned with the multigrid algorithm for the solution of linear systems of\nequations, which is widely used on high performance computing systems. It is\nshown that the Fault-Prone Multigrid Method is not resilient, unless the\nprolongation operation is protected. Strategies for fault detection and\nmitigation as well as protection of the prolongation operation are presented\nand tested, and a guideline for an optimal choice of parameters is devised.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 15:23:15 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Ainsworth", "Mark", ""], ["Glusa", "Christian", ""]]}, {"id": "1607.08508", "submitter": "Brandon Wagner", "authors": "Brandon Wagner and Arun Sood", "title": "Economics of Resilient Cloud Services", "comments": "To appear in 1st IEEE International Workshop on Cyber Resilience\n  Economics, August 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer systems today must meet and maintain service availability,\nperformance, and security requirements. Each of these demands requires\nredundancy and some form of isolation. When service requirements are\nimplemented separately, the system architecture cannot easily share common\ncomponents of redundancy and isolation. We will present these service traits\ncollectively as cyber resilience with a system called Self Cleansing Intrusion\nTolerance or SCIT. Further, we will demonstrate that SCIT provides an effective\nresilient cloud implementation making cost effective utilization of cloud\nexcess capacity and economies of scale. Lastly, we will introduce the notion of\nserverless applications utilizing AWS Lambda and how a stateless architecture\ncan drastically reduce operational costs by utilizing cloud function services.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 15:47:43 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Wagner", "Brandon", ""], ["Sood", "Arun", ""]]}, {"id": "1607.08578", "submitter": "Hyoseung Kim", "authors": "Hyoseung Kim", "title": "Towards Predictable Real-Time Performance on Multi-Core Platforms", "comments": "This is the Ph.D. dissertation of the author", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cyber-physical systems (CPS) integrate sensing, computing, communication and\nactuation capabilities to monitor and control operations in the physical\nenvironment. A key requirement of such systems is the need to provide\npredictable real-time performance: the timing correctness of the system should\nbe analyzable at design time with a quantitative metric and guaranteed at\nruntime with high assurance. This requirement of predictability is particularly\nimportant for safety-critical domains such as automobiles, aerospace, defense,\nmanufacturing and medical devices.\n  The work in this dissertation focuses on the challenges arising from the use\nof modern multi-core platforms in CPS. Even as of today, multi-core platforms\nare rarely used in safety-critical applications primarily due to the temporal\ninterference caused by contention on various resources shared among processor\ncores, such as caches, memory buses, and I/O devices. Such interference is hard\nto predict and can significantly increase task execution time, e.g., up to 12x\non commodity quad-core platforms. To address the problem of ensuring timing\npredictability on multi-core platforms, we develop novel analytical and systems\ntechniques in this dissertation. Our proposed techniques theoretically bound\ntemporal interference that tasks may suffer from when accessing shared\nresources. Our techniques also involve software primitives and algorithms for\nreal-time operating systems and hypervisors, which significantly reduce the\ndegree of the temporal interference. Specifically, we tackle the issues of\ncache and memory contention, locking and synchronization, interrupt handling,\nand access control for computational accelerators such as GPGPUs, all of which\nare crucial to achieving predictable real-time performance on a modern\nmulti-core platform.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 19:05:43 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Kim", "Hyoseung", ""]]}]