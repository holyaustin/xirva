[{"id": "0909.0764", "submitter": "Serguei Mokhov", "authors": "Aihua Wu, Joey Paquet, and Serguei A. Mokhov", "title": "Object-Oriented Intensional Programming: Intensional Classes Using Java\n  and Lucid", "comments": "27 pages, 8 listings, 2 tables, 5 figures", "journal-ref": null, "doi": "10.1109/SERA.2010.29", "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces Object-Oriented Intensional Programming (OO-IP), a\nnew hybrid language between Object-Oriented and Intensional Programming\nLanguages in the sense of the latest evolutions of Lucid. This new hybrid\nlanguage combines the essential characteristics of Lucid and Java, and\nintroduces the notion of object streams which makes it is possible that each\nelement in a Lucid stream to be an object with embedded intensional properties.\nInterestingly, this hybrid language also brings to Java objects the power to\nexplicitly express and manipulate the notion of context, creating the novel\nconcept of intensional object, i.e. objects whose evaluation is\ncontext-dependent, which are here demonstrated to be translatable into standard\nobjects. By this new approach, we extend the use and meaning of the notion of\nintensional objects and enrich the meaning of object streams in Lucid and\nsemantics of intensional objects in Java.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2009 21:13:44 GMT"}], "update_date": "2010-07-09", "authors_parsed": [["Wu", "Aihua", ""], ["Paquet", "Joey", ""], ["Mokhov", "Serguei A.", ""]]}, {"id": "0909.1146", "submitter": "Saurabh Kumar Garg", "authors": "Saurabh Kumar Garg, Chee Shin Yeo, Arun Anandasivam, Rajkumar Buyya", "title": "Energy-Efficient Scheduling of HPC Applications in Cloud Computing\n  Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of High Performance Computing (HPC) in commercial and consumer IT\napplications is becoming popular. They need the ability to gain rapid and\nscalable access to high-end computing capabilities. Cloud computing promises to\ndeliver such a computing infrastructure using data centers so that HPC users\ncan access applications and data from a Cloud anywhere in the world on demand\nand pay based on what they use. However, the growing demand drastically\nincreases the energy consumption of data centers, which has become a critical\nissue. High energy consumption not only translates to high energy cost, which\nwill reduce the profit margin of Cloud providers, but also high carbon\nemissions which is not environmentally sustainable. Hence, energy-efficient\nsolutions are required that can address the high increase in the energy\nconsumption from the perspective of not only Cloud provider but also from the\nenvironment. To address this issue we propose near-optimal scheduling policies\nthat exploits heterogeneity across multiple data centers for a Cloud provider.\nWe consider a number of energy efficiency factors such as energy cost, carbon\nemission rate, workload, and CPU power efficiency which changes across\ndifferent data center depending on their location, architectural design, and\nmanagement system. Our carbon/energy based scheduling policies are able to\nachieve on average up to 30% of energy savings in comparison to profit based\nscheduling policies leading to higher profit and less carbon emissions.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2009 06:13:40 GMT"}], "update_date": "2009-09-08", "authors_parsed": [["Garg", "Saurabh Kumar", ""], ["Yeo", "Chee Shin", ""], ["Anandasivam", "Arun", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "0909.1187", "submitter": "Marco Aldinucci", "authors": "Marco Aldinucci, Massimo Torquati, Massimiliano Meneghin", "title": "FastFlow: Efficient Parallel Streaming Applications on Multi-core", "comments": "23 pages + cover", "journal-ref": null, "doi": null, "report-no": "TR-09-12", "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shared memory multiprocessors come back to popularity thanks to rapid\nspreading of commodity multi-core architectures. As ever, shared memory\nprograms are fairly easy to write and quite hard to optimise; providing\nmulti-core programmers with optimising tools and programming frameworks is a\nnowadays challenge. Few efforts have been done to support effective streaming\napplications on these architectures. In this paper we introduce FastFlow, a\nlow-level programming framework based on lock-free queues explicitly designed\nto support high-level languages for streaming applications. We compare FastFlow\nwith state-of-the-art programming frameworks such as Cilk, OpenMP, and Intel\nTBB. We experimentally demonstrate that FastFlow is always more efficient than\nall of them in a set of micro-benchmarks and on a real world application; the\nspeedup edge of FastFlow over other solutions might be bold for fine grain\ntasks, as an example +35% on OpenMP, +226% on Cilk, +96% on TBB for the\nalignment of protein P01111 against UniProt DB using Smith-Waterman algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2009 09:33:57 GMT"}], "update_date": "2009-09-10", "authors_parsed": [["Aldinucci", "Marco", ""], ["Torquati", "Massimo", ""], ["Meneghin", "Massimiliano", ""]]}, {"id": "0909.1397", "submitter": "R Doomun", "authors": "Iraj Ataollahi, Mortza Analoui", "title": "Resource Matchmaking Algorithm using Dynamic Rough Set in Grid\n  Environment", "comments": "10 Pages IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS 2009, ISSN 1947 5500, Impact Factor 0.423,\n  http://sites.google.com/site/ijcsis/", "journal-ref": "International Journal of Computer Science and Information\n  Security, IJCSIS, Vol. 4, No. 1 & 2, August 2009, USA", "doi": null, "report-no": "ISSN 1947 5500", "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grid environment is a service oriented infrastructure in which many\nheterogeneous resources participate to provide the high performance\ncomputation. One of the bug issues in the grid environment is the vagueness and\nuncertainty between advertised resources and requested resources. Furthermore,\nin an environment such as grid dynamicity is considered as a crucial issue\nwhich must be dealt with. Classical rough set have been used to deal with the\nuncertainty and vagueness. But it can just be used on the static systems and\ncan not support dynamicity in a system. In this work we propose a solution,\ncalled Dynamic Rough Set Resource Discovery (DRSRD), for dealing with cases of\nvagueness and uncertainty problems based on Dynamic rough set theory which\nconsiders dynamic features in this environment. In this way, requested resource\nproperties have a weight as priority according to which resource matchmaking\nand ranking process is done. We also report the result of the solution obtained\nfrom the simulation in GridSim simulator. The comparison has been made between\nDRSRD, classical rough set theory based algorithm, and UDDI and OWL S combined\nalgorithm. DRSRD shows much better precision for the cases with vagueness and\nuncertainty in a dynamic system such as the grid rather than the classical\nrough set theory based algorithm, and UDDI and OWL S combined algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2009 05:57:20 GMT"}], "update_date": "2009-09-09", "authors_parsed": [["Ataollahi", "Iraj", ""], ["Analoui", "Mortza", ""]]}, {"id": "0909.1517", "submitter": "Marco Aldinucci", "authors": "Marco Aldinucci, Marco Danelutto, Peter Kilpatrick", "title": "Autonomic management of multiple non-functional concerns in behavioural\n  skeletons", "comments": "20 pages + cover page", "journal-ref": null, "doi": "10.1007/978-1-4419-6794-7_8", "report-no": "TR-09-10", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and address the problem of concurrent autonomic management of\ndifferent non-functional concerns in parallel applications build as a\nhierarchical composition of behavioural skeletons. We first define the problems\narising when multiple concerns are dealt with by independent managers, then we\npropose a methodology supporting coordinated management, and finally we discuss\nhow autonomic management of multiple concerns may be implemented in a typical\nuse case. The paper concludes with an outline of the challenges involved in\nrealizing the proposed methodology on distributed target architectures such as\nclusters and grids. Being based on the behavioural skeleton concept proposed in\nthe CoreGRID GCM, it is anticipated that the methodology will be readily\nintegrated into the current reference implementation of GCM based on Java\nProActive and running on top of major grid middleware systems.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2009 17:02:58 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Aldinucci", "Marco", ""], ["Danelutto", "Marco", ""], ["Kilpatrick", "Peter", ""]]}, {"id": "0909.1768", "submitter": "Alan Fekete", "authors": "David Lomet (Microsoft Research), Alan Fekete (University of Sydney),\n  Gerhard Weikum (Max Plank Institute), Mike Zwilling (Microsoft SQL Server)", "title": "Unbundling Transaction Services in the Cloud", "comments": "CIDR 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The traditional architecture for a DBMS engine has the recovery, concurrency\ncontrol and access method code tightly bound together in a storage engine for\nrecords. We propose a different approach, where the storage engine is factored\ninto two layers (each of which might have multiple heterogeneous instances). A\nTransactional Component (TC) works at a logical level only: it knows about\ntransactions and their \"logical\" concurrency control and undo/redo recovery,\nbut it does not know about page layout, B-trees etc. A Data Component (DC)\nknows about the physical storage structure. It supports a record oriented\ninterface that provides atomic operations, but it does not know about\ntransactions. Providing atomic record operations may itself involve DC-local\nconcurrency control and recovery, which can be implemented using system\ntransactions. The interaction of the mechanisms in TC and DC leads to\nmulti-level redo (unlike the repeat history paradigm for redo in integrated\nengines). This refactoring of the system architecture could allow easier\ndeployment of application-specific physical structures and may also be helpful\nto exploit multi-core hardware. Particularly promising is its potential to\nenable flexible transactions in cloud database deployments. We describe the\nnecessary principles for unbundled recovery, and discuss implementation issues.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2009 18:09:36 GMT"}], "update_date": "2009-09-15", "authors_parsed": [["Lomet", "David", "", "Microsoft Research"], ["Fekete", "Alan", "", "University of Sydney"], ["Weikum", "Gerhard", "", "Max Plank Institute"], ["Zwilling", "Mike", "", "Microsoft SQL Server"]]}, {"id": "0909.1775", "submitter": "Michael Armbrust", "authors": "Michael Armbrust (UC Berkeley), Armando Fox (UC Berkeley), David\n  Patterson (UC Berkeley), Nick Lanham (UC Berkeley), Beth Trushkowsky (UC\n  Berkeley), Jesse Trutna (UC Berkeley), Haruki Oh (UC Berkeley)", "title": "SCADS: Scale-Independent Storage for Social Computing Applications", "comments": "CIDR 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Collaborative web applications such as Facebook, Flickr and Yelp present new\nchallenges for storing and querying large amounts of data. As users and\ndevelopers are focused more on performance than single copy consistency or the\nability to perform ad-hoc queries, there exists an opportunity for a\nhighly-scalable system tailored specifically for relaxed consistency and\npre-computed queries. The Web 2.0 development model demands the ability to both\nrapidly deploy new features and automatically scale with the number of users.\nThere have been many successful distributed key-value stores, but so far none\nprovide as rich a query language as SQL. We propose a new architecture, SCADS,\nthat allows the developer to declaratively state application specific\nconsistency requirements, takes advantage of utility computing to provide cost\neffective scale-up and scale-down, and will use machine learning models to\nintrospectively anticipate performance problems and predict the resource\nrequirements of new queries before execution.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2009 18:10:04 GMT"}], "update_date": "2009-09-15", "authors_parsed": [["Armbrust", "Michael", "", "UC Berkeley"], ["Fox", "Armando", "", "UC Berkeley"], ["Patterson", "David", "", "UC Berkeley"], ["Lanham", "Nick", "", "UC Berkeley"], ["Trushkowsky", "Beth", "", "UC\n  Berkeley"], ["Trutna", "Jesse", "", "UC Berkeley"], ["Oh", "Haruki", "", "UC Berkeley"]]}, {"id": "0909.1788", "submitter": "Pat Helland", "authors": "Pat Helland (Microsoft), David Campbell (Microsoft)", "title": "Building on Quicksand", "comments": "CIDR 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Reliable systems have always been built out of unreliable components. Early\non, the reliable components were small such as mirrored disks or ECC (Error\nCorrecting Codes) in core memory. These systems were designed such that\nfailures of these small components were transparent to the application. Later,\nthe size of the unreliable components grew larger and semantic challenges crept\ninto the application when failures occurred.\n  As the granularity of the unreliable component grows, the latency to\ncommunicate with a backup becomes unpalatable. This leads to a more relaxed\nmodel for fault tolerance. The primary system will acknowledge the work request\nand its actions without waiting to ensure that the backup is notified of the\nwork. This improves the responsiveness of the system.\n  There are two implications of asynchronous state capture: 1) Everything\npromised by the primary is probabilistic. There is always a chance that an\nuntimely failure shortly after the promise results in a backup proceeding\nwithout knowledge of the commitment. Hence, nothing is guaranteed! 2)\nApplications must ensure eventual consistency. Since work may be stuck in the\nprimary after a failure and reappear later, the processing order for work\ncannot be guaranteed.\n  Platform designers are struggling to make this easier for their applications.\nEmerging patterns of eventual consistency and probabilistic execution may soon\nyield a way for applications to express requirements for a \"looser\" form of\nconsistency while providing availability in the face of ever larger failures.\n  This paper recounts portions of the evolution of these trends, attempts to\nshow the patterns that span these changes, and talks about future directions as\nwe continue to \"build on quicksand\".\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2009 18:10:57 GMT"}], "update_date": "2009-09-15", "authors_parsed": [["Helland", "Pat", "", "Microsoft"], ["Campbell", "David", "", "Microsoft"]]}, {"id": "0909.1830", "submitter": "Deniz Ustebay", "authors": "Deniz Ustebay, Boris Oreshkin, Mark Coates, Michael Rabbat", "title": "Greedy Gossip with Eavesdropping", "comments": "25 pages, 7 figures", "journal-ref": null, "doi": "10.1109/TSP.2010.2046593", "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents greedy gossip with eavesdropping (GGE), a novel\nrandomized gossip algorithm for distributed computation of the average\nconsensus problem. In gossip algorithms, nodes in the network randomly\ncommunicate with their neighbors and exchange information iteratively. The\nalgorithms are simple and decentralized, making them attractive for wireless\nnetwork applications. In general, gossip algorithms are robust to unreliable\nwireless conditions and time varying network topologies. In this paper we\nintroduce GGE and demonstrate that greedy updates lead to rapid convergence. We\ndo not require nodes to have any location information. Instead, greedy updates\nare made possible by exploiting the broadcast nature of wireless\ncommunications. During the operation of GGE, when a node decides to gossip,\ninstead of choosing one of its neighbors at random, it makes a greedy\nselection, choosing the node which has the value most different from its own.\nIn order to make this selection, nodes need to know their neighbors' values.\nTherefore, we assume that all transmissions are wireless broadcasts and nodes\nkeep track of their neighbors' values by eavesdropping on their communications.\nWe show that the convergence of GGE is guaranteed for connected network\ntopologies. We also study the rates of convergence and illustrate, through\ntheoretical bounds and numerical simulations, that GGE consistently outperforms\nrandomized gossip and performs comparably to geographic gossip on\nmoderate-sized random geometric graph topologies.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2009 21:04:32 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Ustebay", "Deniz", ""], ["Oreshkin", "Boris", ""], ["Coates", "Mark", ""], ["Rabbat", "Michael", ""]]}, {"id": "0909.2000", "submitter": "Peter Krusche", "authors": "Peter Krusche, Alexander Tiskin", "title": "Computing alignment plots efficiently", "comments": "Presented at ParCo 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dot plots are a standard method for local comparison of biological sequences.\nIn a dot plot, a substring to substring distance is computed for all pairs of\nfixed-size windows in the input strings. Commonly, the Hamming distance is used\nsince it can be computed in linear time. However, the Hamming distance is a\nrather crude measure of string similarity, and using an alignment-based edit\ndistance can greatly improve the sensitivity of the dot plot method. In this\npaper, we show how to compute alignment plots of the latter type efficiently.\nGiven two strings of length m and n and a window size w, this problem consists\nin computing the edit distance between all pairs of substrings of length w, one\nfrom each input string. The problem can be solved by repeated application of\nthe standard dynamic programming algorithm in time O(mnw^2). This paper gives\nan improved data-parallel algorithm, running in time $O(mnw/\\gamma/p)$ using\nvector operations that work on $\\gamma$ values in parallel and $p$ processors.\nWe show experimental results from an implementation of this algorithm, which\nuses Intel's MMX/SSE instructions for vector parallelism and MPI for\ncoarse-grained parallelism.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2009 17:40:15 GMT"}], "update_date": "2009-09-11", "authors_parsed": [["Krusche", "Peter", ""], ["Tiskin", "Alexander", ""]]}, {"id": "0909.2055", "submitter": "Erich Schikuta", "authors": "Erich Schikuta, Thomas Weishaeupl, Flavia Donno, Heinz Stockinger,\n  Elisabeth Vinek, Helmut Wanek, Christoph Witzany, Irfan Ul Haq", "title": "Business in the Grid", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From 2004 to 2007 the Business In the Grid (BIG) project took place and was\ndriven by the following goals: Firstly, make business aware of Grid technology\nand, secondly, try to explore new business models. We disseminated Grid\ncomputing by mainly concentrating on the central European market and\ninterviewed several companies in order to gain insights into the Grid\nacceptance in industrial environments. In this article we present the results\nof the project, elaborate on a critical discussion on business adaptations, and\ndescribe a novel dynamic authorization workflow for business processes in the\nGrid.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2009 21:48:34 GMT"}], "update_date": "2009-09-30", "authors_parsed": [["Schikuta", "Erich", ""], ["Weishaeupl", "Thomas", ""], ["Donno", "Flavia", ""], ["Stockinger", "Heinz", ""], ["Vinek", "Elisabeth", ""], ["Wanek", "Helmut", ""], ["Witzany", "Christoph", ""], ["Haq", "Irfan Ul", ""]]}, {"id": "0909.2297", "submitter": "Andias Wira-Alam T", "authors": "Andias Wira-Alam", "title": "Simulation of Resource Usage in Parallel Evolutionary Peptide\n  Optimization using JavaSpaces Technology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Peptide Optimization is a highly complex problem and it takes very long time\nof computation. This optimization process uses many software applications in a\ncluster running GNU/Linux Operating System that perform special tasks. The\napplication to organize the whole optimization process had been already\ndeveloped, namely SEPP (System for Evolutionary Pareto Optimization of\nPeptides/Polymers). A single peptide optimization takes a lot of computation\ntime to produce a certain number of individuals. However, it can be accelerated\nby increasing the degree of parallelism as well as the number of nodes\n(processors) in the cluster. In this master thesis, I build a model simulating\nthe interplay of the programs so that the usage of each resource (processor)\ncan be determined and also the approximated time needed for the overall\noptimization process. There are two Evolutionary Algorithms that could be used\nin the optimization, namely Generation-based and Steady-state Evolutionary\nAlgorithm. The results of each Evolutionary Algorithm are shown based on the\nsimulations. Moreover, the results are also compared by using different\nparameters (the degree of parallelism and the number of processors) in the\nsimulation to give an overview of the advantages and the disadvantages of the\nalgorithms in terms of computation time and resource usage. The model is built\nup using JavaSpaces Technology.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2009 12:36:22 GMT"}], "update_date": "2009-12-05", "authors_parsed": [["Wira-Alam", "Andias", ""]]}, {"id": "0909.2859", "submitter": "Petar Maymounkov", "authors": "Jonathan Kelner, Petar Maymounkov", "title": "Electric routing and concurrent flow cutting", "comments": "21 pages, 0 figures. To be published in Springer LNCS Book No. 5878,\n  Proceedings of The 20th International Symposium on Algorithms and Computation\n  (ISAAC'09)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate an oblivious routing scheme, amenable to distributed\ncomputation and resilient to graph changes, based on electrical flow. Our main\ntechnical contribution is a new rounding method which we use to obtain a bound\non the L1->L1 operator norm of the inverse graph Laplacian. We show how this\nnorm reflects both latency and congestion of electric routing.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2009 19:50:37 GMT"}], "update_date": "2009-09-16", "authors_parsed": [["Kelner", "Jonathan", ""], ["Maymounkov", "Petar", ""]]}, {"id": "0909.5064", "submitter": "Norbert B\\'atfai", "authors": "Norbert B\\'atfai", "title": "A Conceivable Origin of Machine Consciousness in the IDLE process", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this short paper, we would like to call professional community's attention\nto a daring idea that is surely unhelpful, but is exciting for programmers and\nanyway conflicts with the trend of energy consumption in computer systems.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2009 10:54:48 GMT"}], "update_date": "2009-09-29", "authors_parsed": [["B\u00e1tfai", "Norbert", ""]]}, {"id": "0909.5177", "submitter": "Godwin Shen", "authors": "Godwin Shen and Antonio Ortega", "title": "Transform-based Distributed Data Gathering", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2010.2047640", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A general class of unidirectional transforms is presented that can be\ncomputed in a distributed manner along an arbitrary routing tree. Additionally,\nwe provide a set of conditions under which these transforms are invertible.\nThese transforms can be computed as data is routed towards the collection (or\nsink) node in the tree and exploit data correlation between nodes in the tree.\nMoreover, when used in wireless sensor networks, these transforms can also\nleverage data received at nodes via broadcast wireless communications. Various\nconstructions of unidirectional transforms are also provided for use in data\ngathering in wireless sensor networks. New wavelet transforms are also proposed\nwhich provide significant improvements over existing unidirectional transforms.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2009 19:57:13 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2009 21:22:24 GMT"}, {"version": "v3", "created": "Wed, 27 Jan 2010 07:38:33 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Shen", "Godwin", ""], ["Ortega", "Antonio", ""]]}, {"id": "0909.5413", "submitter": "Rio Yokota Dr.", "authors": "Rio Yokota, L. A. Barba, Matthew G. Knepley", "title": "PetRBF--A parallel O(N) algorithm for radial basis function\n  interpolation", "comments": "Submitted to Computer Methods in Applied Mechanics and Engineering", "journal-ref": "Computer Methods in Applied Mechanics and Engineering, 199(25-28),\n  pp. 1793-1804, 2010", "doi": "10.1016/j.cma.2010.02.008", "report-no": null, "categories": "cs.MS cs.DC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have developed a parallel algorithm for radial basis function (RBF)\ninterpolation that exhibits O(N) complexity,requires O(N) storage, and scales\nexcellently up to a thousand processes. The algorithm uses a GMRES iterative\nsolver with a restricted additive Schwarz method (RASM) as a preconditioner and\na fast matrix-vector algorithm. Previous fast RBF methods, --,achieving at most\nO(NlogN) complexity,--, were developed using multiquadric and polyharmonic\nbasis functions. In contrast, the present method uses Gaussians with a small\nvariance (a common choice in particle methods for fluid simulation, our main\ntarget application). The fast decay of the Gaussian basis function allows rapid\nconvergence of the iterative solver even when the subdomains in the RASM are\nvery small. The present method was implemented in parallel using the PETSc\nlibrary (developer version). Numerical experiments demonstrate its capability\nin problems of RBF interpolation with more than 50 million data points, timing\nat 106 seconds (19 iterations for an error tolerance of 10^-15 on 1024\nprocessors of a Blue Gene/L (700 MHz PowerPC processors). The parallel code is\nfreely available in the open-source model.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2009 19:27:51 GMT"}], "update_date": "2011-09-21", "authors_parsed": [["Yokota", "Rio", ""], ["Barba", "L. A.", ""], ["Knepley", "Matthew G.", ""]]}, {"id": "0909.5649", "submitter": "Vitaly Osipov", "authors": "Nikolaj Leischner, Vitaly Osipov, Peter Sanders", "title": "GPU sample sort", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the design of a sample sort algorithm for manycore\nGPUs. Despite being one of the most efficient comparison-based sorting\nalgorithms for distributed memory architectures its performance on GPUs was\npreviously unknown. For uniformly distributed keys our sample sort is at least\n25% and on average 68% faster than the best comparison-based sorting algorithm,\nGPU Thrust merge sort, and on average more than 2 times faster than GPU\nquicksort. Moreover, for 64-bit integer keys it is at least 63% and on average\n2 times faster than the highly optimized GPU Thrust radix sort that directly\nmanipulates the binary representation of keys. Our implementation is robust to\ndifferent distributions and entropy levels of keys and scales almost linearly\nwith the input size. These results indicate that multi-way techniques in\ngeneral and sample sort in particular achieve substantially better performance\nthan two-way merge sort and quicksort.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2009 15:58:53 GMT"}], "update_date": "2009-10-01", "authors_parsed": [["Leischner", "Nikolaj", ""], ["Osipov", "Vitaly", ""], ["Sanders", "Peter", ""]]}]