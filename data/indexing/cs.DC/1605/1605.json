[{"id": "1605.00085", "submitter": "Ramkumar Lakshminarayanan", "authors": "Ramkumar Lakshminarayanan, Rajasekar Ramalingam", "title": "Usage of Cloud Computing Simulators and Future Systems For Computational\n  Research", "comments": "ETRT-ICT 2016 - College of Applied Sciences, Salalah, Sultanate of\n  Oman", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cloud Computing is an Internet based computing, whereby shared resources,\nsoftware and information, are provided to computers and devices on demand, like\nthe electricity grid. Currently, IaaS (Infrastructure as a Service), PaaS\n(Platform as a Service) and SaaS (Software as a Service) are used as a business\nmodel for Cloud Computing. Nowadays, the adoption and deployment of Cloud\nComputing is increasing in various domains, forcing researchers to conduct\nresearch in the area of Cloud Computing globally. Setting up the research\nenvironment is critical for the researchers in the developing countries to\nevaluate the research outputs. Currently, modeling, simulation technology and\naccess of resources from various university data centers has become a useful\nand powerful tool in cloud computing research. Several cloud simulators have\nbeen specifically developed by various universities to carry out Cloud\nComputing research, including CloudSim, SPECI, Green Cloud and Future Systems\n(the Indiana University machines India, Bravo, Delta, Echo and Foxtrot)\nsupports leading edge data science research and a broad range of\ncomputing-enabled education as well as integration of ideas from cloud and HPC\nsystems. In this paper, the features, suitability, adaptability and the\nlearning curve of the existing Cloud Computing simulators and Future Systems\nare reviewed and analyzed.\n", "versions": [{"version": "v1", "created": "Sat, 30 Apr 2016 09:30:14 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Lakshminarayanan", "Ramkumar", ""], ["Ramalingam", "Rajasekar", ""]]}, {"id": "1605.00305", "submitter": "Abbas Soltanian", "authors": "Ahmad F. B. Alam, Abbas Soltanian, Sami Yangui, Mohammad A.\n  Salahuddin, Roch Glitho, Halima Elbiaze", "title": "A Cloud Platform-as-a-Service for Multimedia Conferencing Service\n  Provisioning", "comments": "6 pages, 6 figures, IEEE ISCC 2016", "journal-ref": null, "doi": "10.1109/ISCC.2016.7543756", "report-no": null, "categories": "cs.NI cs.DC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimedia conferencing is the real-time exchange of multimedia content\nbetween multiple parties. It is the basis of a wide range of applications\n(e.g., multimedia multiplayer game). Cloud-based provisioning of the\nconferencing services on which these applications rely will bring benefits,\nsuch as easy service provisioning and elastic scalability. However, it remains\na big challenge. This paper proposes a PaaS for conferencing service\nprovisioning. The proposed PaaS is based on a business model from the state of\nthe art. It relies on conferencing IaaSs that, instead of VMs, offer\nconferencing substrates (e.g., dial-in signaling, video mixer and audio mixer).\nThe PaaS enables composition of new conferences from substrates on the fly.\nThis has been prototyped in this paper and, in order to evaluate it, a\nconferencing IaaS is also implemented. Performance measurements are also made.\n", "versions": [{"version": "v1", "created": "Sun, 1 May 2016 20:24:20 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Alam", "Ahmad F. B.", ""], ["Soltanian", "Abbas", ""], ["Yangui", "Sami", ""], ["Salahuddin", "Mohammad A.", ""], ["Glitho", "Roch", ""], ["Elbiaze", "Halima", ""]]}, {"id": "1605.00498", "submitter": "Pramod Bhatotia", "authors": "J\\\"org Thalheim, Pramod Bhatotia, Christof Fetzer", "title": "Inspector: A Data Provenance Library for Multithreaded Programs", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data provenance strives for explaining how the computation was performed by\nrecording a trace of the execution. The provenance trace is useful across a\nwide-range of workflows to improve the dependability, security, and efficiency\nof software systems. In this paper, we present Inspector, a POSIX-compliant\ndata provenance library for shared-memory multithreaded programs. The Inspector\nlibrary is completely transparent and easy to use: it can be used as a\nreplacement for the pthreads library by a simple exchange of libraries linked,\nwithout even recompiling the application code. To achieve this result, we\npresent a parallel provenance algorithm that records control, data, and\nschedule dependencies using a Concurrent Provenance Graph (CPG). We implemented\nour algorithm to operate at the compiled binary code level by leveraging a\ncombination of OS-specific mechanisms, and recently released Intel PT ISA\nextensions as part of the Broadwell micro-architecture. Our evaluation on a\nmulticore platform using applications from multithreaded benchmarks suites\n(PARSEC and Phoenix) shows reasonable provenance overheads for a majority of\napplications. Lastly, we briefly describe three case-studies where the generic\ninterface exported by Inspector is being used to improve the dependability,\nsecurity, and efficiency of systems. The Inspector library is publicly\navailable for further use in a wide range of other provenance workflows.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2016 14:16:36 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Thalheim", "J\u00f6rg", ""], ["Bhatotia", "Pramod", ""], ["Fetzer", "Christof", ""]]}, {"id": "1605.00677", "submitter": "Shantanu Sharma", "authors": "Philip Derbeko, Shlomi Dolev, Ehud Gudes, Shantanu Sharma", "title": "Security and Privacy Aspects in MapReduce on Clouds: A Survey", "comments": "Accepted in Elsevier Computer Science Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MapReduce is a programming system for distributed processing large-scale data\nin an efficient and fault tolerant manner on a private, public, or hybrid\ncloud. MapReduce is extensively used daily around the world as an efficient\ndistributed computation tool for a large class of problems, e.g., search,\nclustering, log analysis, different types of join operations, matrix\nmultiplication, pattern matching, and analysis of social networks. Security and\nprivacy of data and MapReduce computations are essential concerns when a\nMapReduce computation is executed in public or hybrid clouds. In order to\nexecute a MapReduce job in public and hybrid clouds, authentication of\nmappers-reducers, confidentiality of data-computations, integrity of\ndata-computations, and correctness-freshness of the outputs are required.\nSatisfying these requirements shield the operation from several types of\nattacks on data and MapReduce computations. In this paper, we investigate and\ndiscuss security and privacy challenges and requirements, considering a variety\nof adversarial capabilities, and characteristics in the scope of MapReduce. We\nalso provide a review of existing security and privacy protocols for MapReduce\nand discuss their overhead issues.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2016 20:31:52 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Derbeko", "Philip", ""], ["Dolev", "Shlomi", ""], ["Gudes", "Ehud", ""], ["Sharma", "Shantanu", ""]]}, {"id": "1605.00910", "submitter": "Poonam Yadav Dr", "authors": "Poonam Yadav and John Darlington", "title": "Design Guidelines for the User-Centred Collaborative Citizen Science\n  Platforms", "comments": "In Review process", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.DC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online Citizen Science platforms are good examples of socio-technical systems\nwhere technology-enabled interactions occur between scientists and the general\npublic (volunteers). Citizen Science platforms usually host multiple Citizen\nScience projects, and allow volunteers to choose the ones to participate in.\nRecent work in the area has demonstrated a positive feedback loop between\nparticipation and learning and creativity in Citizen Science projects, which is\none of the motivating factors both for scientists and the volunteers. This\nemphasises the importance of creating successful Citizen Science platforms,\nwhich support this feedback process, and enable enhanced learning and\ncreativity to occur through knowledge sharing and diverse participation. In\nthis paper, we discuss how scientists' and volunteers' motivation and\nparticipation influence the design of Citizen Science platforms. We present our\nsummary as guidelines for designing these platforms as user-inspired\nsocio-technical systems. We also present the case-studies on popular Citizen\nScience platforms, including our own CitizenGrid platform, developed as part of\nthe CCL EU project, as well as Zooniverse, World Community Grid, CrowdCrafting\nand EpiCollect+ to see how closely these platforms follow our proposed\nguidelines and how these may be further improved to incorporate the creativity\nenabled by the collective knowledge sharing.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 13:51:54 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Yadav", "Poonam", ""], ["Darlington", "John", ""]]}, {"id": "1605.00928", "submitter": "Muhammad Anis Uddin Nasir", "authors": "Muhammad Anis Uddin Nasir", "title": "Fault Tolerance for Stream Processing Engines", "comments": "The survey is not complete and require major updates", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed Stream Processing Engines (DSPEs) target applications related to\ncontinuous computation, online machine learning and real-time query processing.\nDSPEs operate on high volume of data by applying lightweight operations on\nreal-time and continuous streams. Such systems require clusters of hundreds of\nmachine for their deployment. Streaming applications come with various\nrequirements, i.e., low-latency, high throughput, scalability and high\navailability. In this survey, we study the fault tolerance problem for DSPEs.\nWe discuss fault tolerance techniques that are used in modern stream processing\nengines that are Storm, S4, Samza, SparkStreaming and MillWheel. Further, we\ngive insight on fault tolerance approaches that we categorize as active\nreplication, passive replication and upstream backup. Finally, we discuss\nimplications of the fault tolerance techniques for different streaming\napplication requirements.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 14:30:05 GMT"}, {"version": "v2", "created": "Wed, 19 Oct 2016 19:06:52 GMT"}, {"version": "v3", "created": "Tue, 5 May 2020 09:40:16 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Nasir", "Muhammad Anis Uddin", ""]]}, {"id": "1605.00930", "submitter": "Jeremias Moreira Gomes", "authors": "Jeremias Gomes, George Teodoro", "title": "Efficient Execution of Irregular Wavefront Propagation Pattern on Many\n  Integrated Core Architecture", "comments": "in Portuguese", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The efficient execution of image processing algorithms is an active area of\nBioinformatics. In image processing, one of the classes of algorithms or\ncomputing pattern that works with irregular data structures is the Irregular\nWavefront Propagation Pattern (IWPP). In this class, elements propagate\ninformation to neighbors in the form of wave propagation. This propagation\nresults in irregular access to data and expansions. Due to this irregularity,\ncurrent implementations of this class of algorithms requires atomic operations,\nwhich is very costly and also restrains implementations with Single\nInstruction, Multiple Data (SIMD) instructions in Many Integrated Core (MIC)\narchitectures, which are critical to attain high performance on this processor.\nThe objective of this study is to redesign the Irregular Wavefront Propagation\nPattern algorithm in order to enable the efficient execution on processors with\nMany Integrated Core architecture using SIMD instructions. In this work, using\nthe Intel (R) Xeon Phi (TM) coprocessor, we have implemented a vector version\nof IWPP with up to 5.63x gains on non-vectored version, a parallel version\nusing First In, First Out (FIFO) queue that attained speedup up to 55x as\ncompared to the single core version on the coprocessor, a version using\npriority queue whose performance was 1.62x better than the fastest version of\nGPU based implementation available in the literature, and a cooperative version\nbetween heterogeneous processors that allow to process images bigger than the\nIntel (R) Xeon Phi (TM) memory and also provides a way to utilize all the\navailable devices in the computation.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 14:37:34 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Gomes", "Jeremias", ""], ["Teodoro", "George", ""]]}, {"id": "1605.00971", "submitter": "Peter Dugan Dr", "authors": "Peter J. Dugan, Christopher W. Clark, Yann Andr\\'e LeCun, Sofie M. Van\n  Parijs", "title": "Phase 1: DCL System Research Using Advanced Approaches for Land-based or\n  Ship-based Real-Time Recognition and Localization of Marine Mammals - HPC\n  System Implementation", "comments": "Year 1 National Oceanic Partnership Program Report, sponsored ONR,\n  NFWF. N000141210585", "journal-ref": null, "doi": null, "report-no": "N000141210585", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to investigate advancing the state of the art of detection,\nclassification and localization (DCL) in the field of bioacoustics. The two\nprimary goals are to develop transferable technologies for detection and\nclassification in: (1) the area of advanced algorithms, such as deep learning\nand other methods; and (2) advanced systems, capable of real-time and archival\nand processing. This project will focus on long-term, continuous datasets to\nprovide automatic recognition, minimizing human time to annotate the signals.\nEffort will begin by focusing on several years of multi-channel acoustic data\ncollected in the Stellwagen Bank National Marine Sanctuary (SBNMS) between 2006\nand 2010. Our efforts will incorporate existing technologies in the\nbioacoustics signal processing community, advanced high performance computing\n(HPC) systems, and new approaches aimed at automatically detecting-classifying\nand measuring features for species-specific marine mammal sounds within passive\nacoustic data.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 16:35:35 GMT"}, {"version": "v2", "created": "Thu, 5 May 2016 18:27:35 GMT"}], "update_date": "2016-05-06", "authors_parsed": [["Dugan", "Peter J.", ""], ["Clark", "Christopher W.", ""], ["LeCun", "Yann Andr\u00e9", ""], ["Van Parijs", "Sofie M.", ""]]}, {"id": "1605.00982", "submitter": "Peter Dugan Dr", "authors": "Peter J. Dugan, Christopher W. Clark, Yann Andr\\'e LeCun, Sofie M. Van\n  Parijs", "title": "Phase 4: DCL System Using Deep Learning Approaches for Land-Based or\n  Ship-Based Real-Time Recognition and Localization of Marine Mammals -\n  Distributed Processing and Big Data Applications", "comments": "National Oceanic Partnership Program (NOPP) sponsored by ONR and NFWF", "journal-ref": null, "doi": null, "report-no": "N000141210585", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the animal bioacoustics community at large is collecting huge amounts\nof acoustic data at an unprecedented pace, processing these data is\nproblematic. Currently in bioacoustics, there is no effective way to achieve\nhigh performance computing using commericial off the shelf (COTS) or government\noff the shelf (GOTS) tools. Although several advances have been made in the\nopen source and commercial software community, these offerings either support\nspecific applications that do not integrate well with data formats in\nbioacoustics or they are too general. Furthermore, complex algorithms that use\ndeep learning strategies require special considerations, such as very large\nlibraiers of exemplars (whale sounds) readily available for algorithm training\nand testing. Detection-classification for passive acoustics is a data-mining\nstrategy and our goals are aligned with best practices that appeal to the\ngeneral data mining and machine learning communities where the problem of\nprocessing large data is common. Therefore, the objective of this work is to\nadvance the state-of-the art for data-mining large passive acoustic datasets as\nthey pertain to bioacoustics. With this basic deficiency recognized at the\nforefront, portions of the grant were dedicated to fostering deep-learning by\nway of international competitions (kaggle.com) meant to attract deep-learning\nsolutions. The focus of this early work was targeted to make significant\nprogress in addressing big data systems and advanced algorithms over the\nduration of the grant from 2012 to 2015. This early work provided simulataneous\nadvances in systems-algorithms research while supporting various collaborations\nand projects.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 16:54:07 GMT"}, {"version": "v2", "created": "Thu, 5 May 2016 18:35:16 GMT"}], "update_date": "2016-05-06", "authors_parsed": [["Dugan", "Peter J.", ""], ["Clark", "Christopher W.", ""], ["LeCun", "Yann Andr\u00e9", ""], ["Van Parijs", "Sofie M.", ""]]}, {"id": "1605.00983", "submitter": "Peter Dugan Dr", "authors": "Peter J. Dugan, Christopher W. Clark, Yann Andr\\'e LeCun, Sofie M. Van\n  Parijs", "title": "Phase 3: DCL System Using Deep Learning Approaches for Land-based or\n  Ship-based Real-Time Recognition and Localization of Marine Mammals -\n  Bioacoustic Applicaitons", "comments": "National Oceanic Partnership Program (NOPP) sponsored by ONR and NFWF", "journal-ref": null, "doi": null, "report-no": "N000141210585", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Goals of this research phase is to investigate advanced detection and\nclassification pardims useful for data-mining passive large passive acoustic\narchives. Technical objectives are to develop and refine a High Performance\nComputing, Acoustic Data Accelerator (HPC-ADA) along with MATLAB based software\nbased on time series acoustic signal Detection cLassification using Machine\nlearning Algorithms, called DeLMA. Data scientists and biologists integrate to\nuse the HPC-ADA and DeLMA technologies to explore data using newly developed\ntechniques aimed at inspection of data extracted at large spatial and temporal\nscales.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 16:54:46 GMT"}, {"version": "v2", "created": "Thu, 5 May 2016 18:29:19 GMT"}], "update_date": "2016-05-06", "authors_parsed": [["Dugan", "Peter J.", ""], ["Clark", "Christopher W.", ""], ["LeCun", "Yann Andr\u00e9", ""], ["Van Parijs", "Sofie M.", ""]]}, {"id": "1605.01301", "submitter": "Masoud Nosrati", "authors": "Masoud Nosrati, Abdolah Chalechale, and Ronak Karimi", "title": "Latency Optimization for Resource Allocation in Cloud Computing System", "comments": "12 pages, 5 figures, In proceeding of ICCSA 2015, published by\n  Springer LNCS", "journal-ref": null, "doi": "10.1007/978-3-319-21404-7_26", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies in different fields of science caused emergence of needs for\nhigh performance computing systems like Cloud. A critical issue in design and\nimplementation of such systems is resource allocation which is directly\naffected by internal and external factors like the number of nodes,\ngeographical distance and communication latencies. Many optimizations took\nplace in resource allocation methods in order to achieve better performance by\nconcentrating on computing, network and energy resources. Communication\nlatencies as a limitation of network resources have always been playing an\nimportant role in parallel processing (especially in fine-grained programs). In\nthis paper, we are going to have a survey on the resource allocation issue in\nCloud and then do an optimization on common resource allocation method based on\nthe latencies of communications. Due to it, we added a table to Resource Agent\n(entity that allocates resources to the applicants) to hold the history of\nprevious allocations. Then, a probability matrix was constructed for allocation\nof resources partially based on the history of latencies. Response time was\nconsidered as a metric for evaluation of proposed method. Results indicated the\nbetter response time, especially by increasing the number of tasks. Besides,\nthe proposed method is inherently capable for detecting the unavailable\nresources through measuring the communication latencies. It assists other\nissues in cloud systems like migration, resource replication and fault\ntolerance.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2016 14:49:43 GMT"}], "update_date": "2016-05-05", "authors_parsed": [["Nosrati", "Masoud", ""], ["Chalechale", "Abdolah", ""], ["Karimi", "Ronak", ""]]}, {"id": "1605.01361", "submitter": "Konrad Siek", "authors": "Pawe{\\l} T. Wojciechowski and Konrad Siek", "title": "The Optimal Pessimistic Transactional Memory Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transactional Memory (TM) is an approach aiming to simplify concurrent\nprogramming by automating synchronization while maintaining efficiency. TM\nusually employs the optimistic concurrency control approach, which relies on\ntransactions aborting and restarting if conflicts occur. However, an aborted\ntransaction can still leave some effects in the system that cannot be cleaned\nup, if irrevocable operations are present within its code. The pessimistic\napproach eliminates that problem, since it relies on deferring operations in\ncase of conflict rather than aborting, but hitherto pessimistic TMs suffered\nfrom low parallelism due to the need of serializing transactions. In this\npaper, we aim to introduce OptSVA, a pessimistic TM concurrency control\nalgorithm that ensures a high level of parallelism through a battery of\nfar-reaching optimizations including early release, asynchronous execution, and\nthe extensive use of buffering.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2016 17:57:18 GMT"}, {"version": "v2", "created": "Mon, 20 Nov 2017 17:38:57 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Wojciechowski", "Pawe\u0142 T.", ""], ["Siek", "Konrad", ""]]}, {"id": "1605.01584", "submitter": "Yongchao Liu", "authors": "Yongchao Liu, Tony Pan, Srinivas Aluru", "title": "Parallel Pairwise Correlation Computation On Intel Xeon Phi Clusters", "comments": "9 pages, 2 figures, 2 tables, accepted by the SBAC-PAD 2016\n  conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Co-expression network is a critical technique for the identification of\ninter-gene interactions, which usually relies on all-pairs correlation (or\nsimilar measure) computation between gene expression profiles across multiple\nsamples. Pearson's correlation coefficient (PCC) is one widely used technique\nfor gene co-expression network construction. However, all-pairs PCC computation\nis computationally demanding for large numbers of gene expression profiles,\nthus motivating our acceleration of its execution using high-performance\ncomputing. In this paper, we present LightPCC, the first parallel and\ndistributed all-pairs PCC computation on Intel Xeon Phi (Phi) clusters. It\nachieves high speed by exploring the SIMD-instruction-level and thread-level\nparallelism within Phis as well as accelerator-level parallelism among multiple\nPhis. To facilitate balanced workload distribution, we have proposed a general\nframework for symmetric all-pairs computation by building bijective functions\nbetween job identifier and coordinate space for the first time. We have\nevaluated LightPCC and compared it to two CPU-based counterparts: a sequential\nC++ implementation in ALGLIB and an implementation based on a parallel general\nmatrix-matrix multiplication routine in Intel Math Kernel Library (MKL) (all\nuse double precision), using a set of gene expression datasets. Performance\nevaluation revealed that with one 5110P Phi and 16 Phis, LightPCC runs up to\n$20.6\\times$ and $218.2\\times$ faster than ALGLIB, and up to $6.8\\times$ and\n$71.4\\times$ faster than single-threaded MKL, respectively. In addition,\nLightPCC demonstrated good parallel scalability in terms of number of Phis.\nSource code of LightPCC is publicly available at\nhttp://lightpcc.sourceforge.net.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2016 13:30:28 GMT"}, {"version": "v2", "created": "Fri, 10 Jun 2016 13:35:27 GMT"}, {"version": "v3", "created": "Tue, 27 Sep 2016 00:15:44 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Liu", "Yongchao", ""], ["Pan", "Tony", ""], ["Aluru", "Srinivas", ""]]}, {"id": "1605.01701", "submitter": "Aman Mangal", "authors": "Brian Lebiednik, Aman Mangal, Niharika Tiwari", "title": "A Survey and Evaluation of Data Center Network Topologies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data centers are becoming increasingly popular for their flexibility and\nprocessing capabilities in the modern computing environment. They are managed\nby a single entity (administrator) and allow dynamic resource provisioning,\nperformance optimization as well as efficient utilization of available\nresources. Each data center consists of massive compute, network and storage\nresources connected with physical wires. The large scale nature of data centers\nrequires careful planning of compute, storage, network nodes, interconnection\nas well as inter-communication for their effective and efficient operations. In\nthis paper, we present a comprehensive survey and taxonomy of network\ntopologies either used in commercial data centers, or proposed by researchers\nworking in this space. We also compare and evaluate some of those topologies\nusing mininet as well as gem5 simulator for different traffic patterns, based\non various metrics including throughput, latency and bisection bandwidth.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2016 19:30:52 GMT"}], "update_date": "2016-05-06", "authors_parsed": [["Lebiednik", "Brian", ""], ["Mangal", "Aman", ""], ["Tiwari", "Niharika", ""]]}, {"id": "1605.01748", "submitter": "Narayana Moorthy Prakash", "authors": "Kishori M. Konwar, N. Prakash, Erez Kantor, Nancy Lynch, Muriel Medard\n  and Alexander A. Schwarzmann", "title": "Storage-Optimized Data-Atomic Algorithms for Handling Erasures and\n  Errors in Distributed Storage Systems", "comments": "Accepted for Publication at IEEE IPDPS, 2016", "journal-ref": null, "doi": "10.1109/IPDPS.2016.55", "report-no": null, "categories": "cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Erasure codes are increasingly being studied in the context of implementing\natomic memory objects in large scale asynchronous distributed storage systems.\nWhen compared with the traditional replication based schemes, erasure codes\nhave the potential of significantly lowering storage and communication costs\nwhile simultaneously guaranteeing the desired resiliency levels. In this work,\nwe propose the Storage-Optimized Data-Atomic (SODA) algorithm for implementing\natomic memory objects in the multi-writer multi-reader setting. SODA uses\nMaximum Distance Separable (MDS) codes, and is specifically designed to\noptimize the total storage cost for a given fault-tolerance requirement. For\ntolerating $f$ server crashes in an $n$-server system, SODA uses an $[n, k]$\nMDS code with $k=n-f$, and incurs a total storage cost of $\\frac{n}{n-f}$. SODA\nis designed under the assumption of reliable point-to-point communication\nchannels. The communication cost of a write and a read operation are\nrespectively given by $O(f^2)$ and $\\frac{n}{n-f}(\\delta_w+1)$, where\n$\\delta_w$ denotes the number of writes that are concurrent with the particular\nread. In comparison with the recent CASGC algorithm, which also uses MDS codes,\nSODA offers lower storage cost while pays more on the communication cost.\n  We also present a modification of SODA, called SODA$_{\\text{err}}$, to handle\nthe case where some of the servers can return erroneous coded elements during a\nread operation. Specifically, in order to tolerate $f$ server failures and $e$\nerror-prone coded elements, the SODA$_{\\text{err}}$ algorithm uses an $[n, k]$\nMDS code such that $k=n-2e-f$. SODA$_{\\text{err}}$ also guarantees liveness and\natomicity, while maintaining an optimized total storage cost of\n$\\frac{n}{n-f-2e}$.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2016 20:23:53 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Konwar", "Kishori M.", ""], ["Prakash", "N.", ""], ["Kantor", "Erez", ""], ["Lynch", "Nancy", ""], ["Medard", "Muriel", ""], ["Schwarzmann", "Alexander A.", ""]]}, {"id": "1605.01802", "submitter": "Tapan Sharma", "authors": "Tapan Sharma, Dr. Vinod Shokeen, Dr. Sunil Mathur", "title": "Multiple K Means++ Clustering of Satellite Image Using Hadoop MapReduce\n  and Spark", "comments": "9 Pages, Distributed Computing, Satellite Images, Clustering,\n  Published in International Journal of Advanced Studies in Computer Science\n  and Engineering, IJASCSE volume 5 issue 4, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering of image is one of the important steps of mining satellite images.\nIn our experiment we have simultaneously run multiple K-means algorithms with\ndifferent initial centroids and values of k in the same iteration of MapReduce\njobs. For initialization of initial centroids we have implemented Scalable\nK-Means++ MapReduce (MR) job [1]. We have also run a validation algorithm of\nSimplified Silhouette Index [2] for multiple clustering outputs, again in the\nsame iteration of MR jobs. This paper explored the behavior of above mentioned\nclustering algorithms when run on big data platforms like MapReduce and Spark\njobs. Spark has been chosen as it is popular for fast processing particularly\nwhere iterations are involved.\n", "versions": [{"version": "v1", "created": "Fri, 6 May 2016 02:05:04 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Sharma", "Tapan", ""], ["Shokeen", "Dr. Vinod", ""], ["Mathur", "Dr. Sunil", ""]]}, {"id": "1605.01903", "submitter": "Arnaud Casteigts", "authors": "Arnaud Casteigts, Yves M\\'etivier, John Michael Robson, Akka Zemmari", "title": "Deterministic Leader Election Takes $\\Theta(D + \\log n)$ Bit Rounds", "comments": "Accepted for publication in Algorithmica", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leader election is, together with consensus, one of the most central problems\nin distributed computing. This paper presents a distributed algorithm, called\n\\STT, for electing deterministically a leader in an arbitrary network, assuming\nprocessors have unique identifiers of size $O(\\log n)$, where $n$ is the number\nof processors. It elects a leader in $O(D +\\log n)$ rounds, where $D$ is the\ndiameter of the network, with messages of size $O(1)$. Thus it has a bit round\ncomplexity of $O(D +\\log n)$. This substantially improves upon the best known\nalgorithm whose bit round complexity is $O(D\\log n)$. In fact, using the lower\nbound by Kutten et al. (2015) and a result of Dinitz and Solomon (2007), we\nshow that the bit round complexity of \\STT is optimal (up to a constant\nfactor), which is a significant step forward in understanding the interplay\nbetween time and message optimality for the election problem. Our algorithm\nrequires no knowledge on the graph such as $n$ or $D$, and the pipelining\ntechnique we introduce to break the $O(D\\log n)$ barrier is general.\n", "versions": [{"version": "v1", "created": "Fri, 6 May 2016 12:01:40 GMT"}, {"version": "v2", "created": "Mon, 11 Jul 2016 09:07:52 GMT"}, {"version": "v3", "created": "Wed, 8 Mar 2017 09:31:01 GMT"}, {"version": "v4", "created": "Fri, 7 Sep 2018 07:54:24 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Casteigts", "Arnaud", ""], ["M\u00e9tivier", "Yves", ""], ["Robson", "John Michael", ""], ["Zemmari", "Akka", ""]]}, {"id": "1605.01994", "submitter": "Saurabh Hukerikar", "authors": "Saurabh Hukerikar, Robert F. Lucas", "title": "Rolex: Resilience-Oriented Language Extensions for Extreme-Scale Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future exascale high-performance computing (HPC) systems will be constructed\nfrom VLSI devices that will be less reliable than those used today, and faults\nwill become the norm, not the exception. This will pose significant problems\nfor system designers and programmers, who for half-a-century have enjoyed an\nexecution model that assumed correct behavior by the underlying computing\nsystem. The mean time to failure (MTTF) of the system scales inversely to the\nnumber of components in the system and therefore faults and resultant system\nlevel failures will increase, as systems scale in terms of the number of\nprocessor cores and memory modules used. However every error detected need not\ncause catastrophic failure. Many HPC applications are inherently fault\nresilient. Yet it is the application programmers who have this knowledge but\nlack mechanisms to convey it to the system.\n  In this paper, we present new Resilience Oriented Language Extensions (Rolex)\nwhich facilitate the incorporation of fault resilience as an intrinsic property\nof the application code. We describe the syntax and semantics of the language\nextensions as well as the implementation of the supporting compiler\ninfrastructure and runtime system. Our experiments show that an approach that\nleverages the programmer's insight to reason about the context and significance\nof faults to the application outcome significantly improves the probability\nthat an application runs to a successful conclusion.\n", "versions": [{"version": "v1", "created": "Fri, 6 May 2016 16:23:09 GMT"}, {"version": "v2", "created": "Mon, 23 May 2016 04:16:21 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Hukerikar", "Saurabh", ""], ["Lucas", "Robert F.", ""]]}, {"id": "1605.02022", "submitter": "Janne H. Korhonen", "authors": "Janne H. Korhonen", "title": "Deterministic MST Sparsification in the Congested Clique", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a simple deterministic constant-round algorithm in the congested\nclique model for reducing the number of edges in a graph to $n^{1+\\varepsilon}$\nwhile preserving the minimum spanning forest, where $\\varepsilon > 0$ is any\nconstant. This implies that in the congested clique model, it is sufficient to\nimprove MST and other connectivity algorithms on graphs with slightly\nsuperlinear number of edges to obtain a general improvement. As a byproduct, we\nalso obtain a simple alternative proof showing that MST can be computed\ndeterministically in $O(\\log \\log n)$ rounds.\n", "versions": [{"version": "v1", "created": "Fri, 6 May 2016 18:22:34 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Korhonen", "Janne H.", ""]]}, {"id": "1605.02043", "submitter": "Lingda Li", "authors": "Lingda Li, Ari B. Hayes, Stephen A. Hackler, Eddy Z. Zhang, Mario\n  Szegedy, Shuaiwen Leon Song", "title": "A Graph-based Model for GPU Caching Problems", "comments": "Currently under submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling data sharing in GPU programs is a challenging task because of the\nmassive parallelism and complex data sharing patterns provided by GPU\narchitectures. Better GPU caching efficiency can be achieved through careful\ntask scheduling among different threads. Traditionally, in the field of\nparallel computing, graph partition models are used to model data communication\nand guide task scheduling. However, we discover that the previous methods are\neither inaccurate or expensive when applied to GPU programs. In this paper, we\npropose a novel task partition model that is accurate and gives rise to the\ndevelopment of fast and high quality task/data reorganization algorithms. We\ndemonstrate the effectiveness of the proposed model by rigorous theoretical\nanalysis of the algorithm bounds and extensive experimental analysis. The\nexperimental results show that it achieves significant performance improvement\nacross a representative set of GPU applications.\n", "versions": [{"version": "v1", "created": "Fri, 6 May 2016 19:12:06 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Li", "Lingda", ""], ["Hayes", "Ari B.", ""], ["Hackler", "Stephen A.", ""], ["Zhang", "Eddy Z.", ""], ["Szegedy", "Mario", ""], ["Song", "Shuaiwen Leon", ""]]}, {"id": "1605.02279", "submitter": "Danny Dolev", "authors": "Danny Dolev and Eli Gafni", "title": "Synchronous Hybrid Message-Adversary", "comments": "adding the bbl file that was missing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theory of distributed computing, lagging in its development behind\npractice, has been biased in its modelling by employing mechanisms within the\nmodel mimicking reality. Reality means, processors can fail. But theory is\nabout predicting consequences of reality, hence if we capture reality by\n\"artificial models,\" but those nevertheless make analysis simpler, we should\npursue the artificial models.\n  Recently the idea was advocated to analyze distributed systems and view\nprocessors as infallible. It is the message delivery substrate that causes\nproblems. This view not only can effectively emulate reality, but above all\nseems to allow to view any past models as \\emph{synchronous} models.\nSynchronous models are easier to analyze than asynchronous ones. Furthermore,\nit gives rise to models we haven't contemplated in the past. One such model,\npresented here, is the Hybrid Message-Adversary. We motivate this model through\nthe need to analyze Byzantine faults. The Hybrid model exhibits a phenomenon\nnot seen in the past.\n", "versions": [{"version": "v1", "created": "Sun, 8 May 2016 05:33:04 GMT"}, {"version": "v2", "created": "Tue, 5 Jul 2016 17:25:13 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Dolev", "Danny", ""], ["Gafni", "Eli", ""]]}, {"id": "1605.02432", "submitter": "Elarbi Badidi", "authors": "Elarbi Badidi", "title": "A broker-based framework for integrated SLA-aware SaaS Provisioning", "comments": "19 pages", "journal-ref": "International Journal on Cloud Computing: Services and\n  Architecture (IJCCSA) Vol. 6, No. 2, April 2016", "doi": null, "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the service landscape, the issues of service selection, negotiation of\nService Level Agreements (SLA), and SLA-compliance monitoring have typically\nbeen used in separate and disparate ways, which affect the quality of the\nservices that consumers obtain from their providers. In this work, we propose a\nbroker-based framework to deal with these concerns in an integrated manner for\nSoftware as a Service (SaaS) provisioning. The SaaS Broker selects a suitable\nSaaS provider on behalf of the service consumer by using a utility-driven\nselection algorithm that ranks the QoS offerings of potential SaaS providers.\nThen, it negotiates the SLA terms with that provider based on the quality\nrequirements of the service consumer. The monitoring infrastructure observes\nSLA-compliance during service delivery by using measurements obtained from\nthird-party monitoring services. We also define a utility-based bargaining\ndecision model that allows the service consumer to express her sensitivity for\neach of the negotiated quality attributes and to evaluate the SaaS provider\noffer in each round of negotiation. A use-case with few quality attributes and\ntheir respective utility functions illustrates the approach.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 06:51:27 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Badidi", "Elarbi", ""]]}, {"id": "1605.02474", "submitter": "Dongxiao Yu", "authors": "Magnus M. Halldorsson and Tigran Tonoyan and Yuexuan Wang and Dongxiao\n  Yu", "title": "Data Dissemination in Unified Dynamic Wireless Networks", "comments": "28 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give efficient algorithms for the fundamental problems of Broadcast and\nLocal Broadcast in dynamic wireless networks. We propose a general model of\ncommunication which captures and includes both fading models (like SINR) and\ngraph-based models (such as quasi unit disc graphs, bounded-independence\ngraphs, and protocol model). The only requirement is that the nodes can be\nembedded in a bounded growth quasi-metric, which is the weakest condition known\nto ensure distributed operability. Both the nodes and the links of the network\nare dynamic: nodes can come and go, while the signal strength on links can go\nup or down.\n  The results improve some of the known bounds even in the static setting,\nincluding an optimal algorithm for local broadcasting in the SINR model, which\nis additionally uniform (independent of network size). An essential component\nis a procedure for balancing contention, which has potentially wide\napplicability. The results illustrate the importance of carrier sensing, a\nstock feature of wireless nodes today, which we encapsulate in primitives to\nbetter explore its uses and usefulness.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 08:44:15 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Halldorsson", "Magnus M.", ""], ["Tonoyan", "Tigran", ""], ["Wang", "Yuexuan", ""], ["Yu", "Dongxiao", ""]]}, {"id": "1605.02620", "submitter": "Jeffrey Kelling", "authors": "Jeffrey Kelling, G\\'eza \\'Odor, Sibylle Gemming", "title": "Universality of (2+1)-dimensional restricted solid-on-solid models", "comments": null, "journal-ref": "Phys. Rev. E 94, 022107 (2016)", "doi": "10.1103/PhysRevE.94.022107", "report-no": null, "categories": "cond-mat.stat-mech cond-mat.mtrl-sci cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extensive dynamical simulations of Restricted Solid on Solid models in\n$D=2+1$ dimensions have been done using parallel multisurface algorithms\nimplemented on graphics cards. Numerical evidence is presented that these\nmodels exhibit KPZ surface growth scaling, irrespective of the step heights\n$N$. We show that by increasing $N$ the corrections to scaling increase, thus\nsmaller step-sized models describe better the asymptotic, long-wave-scaling\nbehavior.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 15:19:15 GMT"}, {"version": "v2", "created": "Fri, 29 Jul 2016 11:35:14 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Kelling", "Jeffrey", ""], ["\u00d3dor", "G\u00e9za", ""], ["Gemming", "Sibylle", ""]]}, {"id": "1605.02669", "submitter": "Rafa{\\l} Skinderowicz", "authors": "Rafa{\\l} Skinderowicz", "title": "The GPU-based Parallel Ant Colony System", "comments": null, "journal-ref": null, "doi": "10.1016/j.jpdc.2016.04.014", "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Ant Colony System (ACS) is, next to Ant Colony Optimization (ACO) and the\nMAX-MIN Ant System (MMAS), one of the most efficient metaheuristic algorithms\ninspired by the behavior of ants. In this article we present three novel\nparallel versions of the ACS for the graphics processing units (GPUs). To the\nbest of our knowledge, this is the first such work on the ACS which shares many\nkey elements of the ACO and the MMAS, but differences in the process of\nbuilding solutions and updating the pheromone trails make obtaining an\nefficient parallel version for the GPUs a difficult task. The proposed parallel\nversions of the ACS differ mainly in their implementations of the pheromone\nmemory. The first two use the standard pheromone matrix, and the third uses a\nnovel selective pheromone memory. Computational experiments conducted on\nseveral Travelling Salesman Problem (TSP) instances of sizes ranging from 198\nto 2392 cities showed that the parallel ACS on Nvidia Kepler GK104 GPU (1536\nCUDA cores) is able to obtain a speedup up to 24.29x vs the sequential ACS\nrunning on a single core of Intel Xeon E5-2670 CPU. The parallel ACS with the\nselective pheromone memory achieved speedups up to 16.85x, but in most cases\nthe obtained solutions were of significantly better quality than for the\nsequential ACS.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 17:41:37 GMT"}, {"version": "v2", "created": "Fri, 5 May 2017 10:43:58 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Skinderowicz", "Rafa\u0142", ""]]}, {"id": "1605.03047", "submitter": "Nasser Ghadiri", "authors": "Nasser Ghadiri, Meysam Ghaffari, Mohammad Amin Nikbakht", "title": "BigFCM: Fast, Precise and Scalable FCM on Hadoop", "comments": null, "journal-ref": null, "doi": "10.1016/j.future.2017.06.010", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering plays an important role in mining big data both as a modeling\ntechnique and a preprocessing step in many data mining process implementations.\nFuzzy clustering provides more flexibility than non-fuzzy methods by allowing\neach data record to belong to more than one cluster to some degree. However, a\nserious challenge in fuzzy clustering is the lack of scalability. Massive\ndatasets in emerging fields such as geosciences, biology and networking do\nrequire parallel and distributed computations with high performance to solve\nreal-world problems. Although some clustering methods are already improved to\nexecute on big data platforms, but their execution time is highly increased for\nlarge datasets. In this paper, a scalable Fuzzy C-Means (FCM) clustering named\nBigFCM is proposed and designed for the Hadoop distributed data platform. Based\non the map-reduce programming model, it exploits several mechanisms including\nan efficient caching design to achieve several orders of magnitude reduction in\nexecution time. Extensive evaluation over multi-gigabyte datasets shows that\nBigFCM is scalable while it preserves the quality of clustering.\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2016 15:04:01 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Ghadiri", "Nasser", ""], ["Ghaffari", "Meysam", ""], ["Nikbakht", "Mohammad Amin", ""]]}, {"id": "1605.03283", "submitter": "Mohammad Mamun Rashid Or", "authors": "Mohammad Mamun Or Rashid, M. Masud Rana and Jugal Krishna Das", "title": "Implementation of the open source virtualization technologies in cloud\n  computing", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Virtualization and Cloud Computing is a recent buzzword in the digital\nworld. Cloud computing provide IT as a service to the users on demand basis.\nThis service has greater flexibility, availability, reliability and scalability\nwith utility computing model. This new concept of computing has an immense\npotential in it to be used in the field of e-governance and in the overall IT\ndevelopment perspective in developing countries like Bangladesh.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 05:03:27 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Rashid", "Mohammad Mamun Or", ""], ["Rana", "M. Masud", ""], ["Das", "Jugal Krishna", ""]]}, {"id": "1605.03547", "submitter": "Moslem Noori", "authors": "Moslem Noori, Emina Soljanin, Masoud Ardakani", "title": "On Storage Allocation for Maximum Service Rate in Distributed Storage\n  Systems", "comments": "This paper has been accepted for presentation in IEEE ISIT 2016", "journal-ref": null, "doi": "10.1109/ISIT.2016.7541297", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Storage allocation affects important performance measures of distributed\nstorage systems. Most previous studies on the storage allocation consider its\neffect separately either on the success of the data recovery or on the service\nrate (time) where it is assumed that no access failure happens in the system.\nIn this paper, we go one step further and incorporate the access model and the\nsuccess of data recovery into the service rate analysis. In particular, we\nfocus on quasi-uniform storage allocation and provide a service rate analysis\nfor both fixed-size and probabilistic access models at the nodes. Using this\nanalysis, we then show that for the case of exponential waiting time\ndistribution at individuals storage nodes, minimal spreading allocation results\nin the highest system service rate for both access models. This means that for\na given storage budget, replication provides a better service rate than a coded\nstorage solution.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 18:57:21 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Noori", "Moslem", ""], ["Soljanin", "Emina", ""], ["Ardakani", "Masoud", ""]]}, {"id": "1605.03622", "submitter": "Sven Helmer", "authors": "Pekka Abrahamsson, Sven Helmer, Tosin Daniel Oyetoyan, Stefan\n  Brocanelli, Filippo Cardano, Daniele Gadler, Daniel Morandini, Alessandro\n  Piccoli, Saifur Salam, Julian Sanin, Alam Mahabub Shahrear, Angelo Ventura", "title": "Bringing the Cloud to Rural and Remote Areas - Cloudlet by Cloudlet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instead of relying on huge and expensive data centers for rolling out\ncloud-based services to rural and remote areas, we propose a hardware platform\nbased on small single-board computers. The role of these micro-data centers is\ntwofold. On the one hand, they act as intermediaries between cloud services and\nclients, improving availability in the case of network or power outages. On the\nother hand, they run community-based services on local infrastructure. We\nillustrate how to build such a system without incurring high costs, high power\nconsumption, or single points of failure. Additionally, we opt for a system\nthat is extendable and scalable as well as easy to deploy, relying on an open\ndesign.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 21:16:33 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Abrahamsson", "Pekka", ""], ["Helmer", "Sven", ""], ["Oyetoyan", "Tosin Daniel", ""], ["Brocanelli", "Stefan", ""], ["Cardano", "Filippo", ""], ["Gadler", "Daniele", ""], ["Morandini", "Daniel", ""], ["Piccoli", "Alessandro", ""], ["Salam", "Saifur", ""], ["Sanin", "Julian", ""], ["Shahrear", "Alam Mahabub", ""], ["Ventura", "Angelo", ""]]}, {"id": "1605.03719", "submitter": "Pierre Fraigniaud", "authors": "Pierre Fraigniaud and Ivan Rapaport and Ville Salo and Ioan Todinca", "title": "Distributed Testing of Excluded Subgraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study property testing in the context of distributed computing, under the\nclassical CONGEST model. It is known that testing whether a graph is\ntriangle-free can be done in a constant number of rounds, where the constant\ndepends on how far the input graph is from being triangle-free. We show that,\nfor every connected 4-node graph H, testing whether a graph is H-free can be\ndone in a constant number of rounds too. The constant also depends on how far\nthe input graph is from being H-free, and the dependence is identical to the\none in the case of testing triangles. Hence, in particular, testing whether a\ngraph is K_4-free, and testing whether a graph is C_4-free can be done in a\nconstant number of rounds (where K_k denotes the k-node clique, and C_k denotes\nthe k-node cycle). On the other hand, we show that testing K_k-freeness and\nC_k-freeness for k>4 appear to be much harder. Specifically, we investigate two\nnatural types of generic algorithms for testing H-freeness, called DFS tester\nand BFS tester. The latter captures the previously known algorithm to test the\npresence of triangles, while the former captures our generic algorithm to test\nthe presence of a 4-node graph pattern H. We prove that both DFS and BFS\ntesters fail to test K_k-freeness and C_k-freeness in a constant number of\nrounds for k>4.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 08:15:54 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Fraigniaud", "Pierre", ""], ["Rapaport", "Ivan", ""], ["Salo", "Ville", ""], ["Todinca", "Ioan", ""]]}, {"id": "1605.03892", "submitter": "Pierre Fraigniaud", "authors": "Alkida Balliu and Gianlorenzo D'Angelo and Pierre Fraigniaud and\n  Dennis Olivetti", "title": "Local Distributed Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the framework of distributed network computing, it is known that, for\nevery network predicate, each network configuration that satisfies this\npredicate can be proved using distributed certificates which can be verified\nlocally. However, this requires to leak information about the identities of the\nnodes in the certificates, which might not be applicable in a context in which\nprivacy is desirable. Unfortunately, it is known that if one insists on\ncertificates independent of the node identities, then not all network\npredicates can be proved using distributed certificates that can be verified\nlocally. In this paper, we prove that, for every network predicate, there is a\ndistributed protocol satisfying the following two properties: (1) for every\nnetwork configuration that is legal w.r.t. the predicate, and for any attempt\nby an adversary to prove the illegality of that configuration using distributed\ncertificates, there is a locally verifiable proof that the adversary is wrong,\nalso using distributed certificates; (2) for every network configuration that\nis illegal w.r.t. the predicate, there is a proof of that illegality, using\ndistributed certificates, such that no matter the way an adversary assigns its\nown set of distributed certificates in an attempt to prove the legality of the\nconfiguration, the actual illegality of the configuration will be locally\ndetected. In both cases, the certificates are independent of the identities of\nthe nodes. These results are achieved by investigating the so-called local\nhierarchy of complexity classes in which the certificates do not exploit the\nnode identities. Indeed, we give a characterization of such a hierarchy, which\nis of its own interest\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 16:51:54 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Balliu", "Alkida", ""], ["D'Angelo", "Gianlorenzo", ""], ["Fraigniaud", "Pierre", ""], ["Olivetti", "Dennis", ""]]}, {"id": "1605.04069", "submitter": "Abdullah Yousafzai", "authors": "Abdullah Yousafzai, Abdullah Gani, Rafidah Md Noor", "title": "Availability Aware Continuous Replica Placement Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Replica placement (RP) intended at producing a set of duplicated data items\nacross the nodes of a distributed system in order to optimize fault tolerance,\navailability, system performance load balancing. Typically, RP formulations\nemploy dynamic methods to change the replica placement in the system\npotentially upon user request profile. Continuous Replica Placement Problem\n(CRPP) is an extension of replica placement problem that takes into\nconsideration the current replication state of the distributed system along\nwith user request profile to define a new replication scheme, subject to\noptimization criteria and constraints. This paper proposes an alternative\ntechnique, named Availability Aware Continuous Replica Placement Problem\n(AACRPP).AACRPP can be defined as: Given an already defined replica placement\nscheme, a user request profile, and a node failure profile define a new\nreplication scheme, subject to optimization criteria and constraints. In this\neffort we use modified greedy heuristics from the CRPP and investigated the\nproposed mechanism using a trace driven java based simulation.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2016 07:25:04 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Yousafzai", "Abdullah", ""], ["Gani", "Abdullah", ""], ["Noor", "Rafidah Md", ""]]}, {"id": "1605.04111", "submitter": "Chhaya Trehan", "authors": "Chhaya Trehan, Hans Vandierendonck, Georgios Karakonstantis, Dimitrios\n  S. Nikolopoulos", "title": "Energy Optimization of Memory Intensive Parallel workloads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy consumption is an important concern in modern multicore processors.\nThe energy consumed during the execution of an application can be minimized by\ntuning the hardware state utilizing knobs such as frequency, voltage etc. The\nexisting theoretical work on energy mini- mization using Global DVFS (Dynamic\nVoltage and Frequency Scaling), despite being thorough, ignores the energy\nconsumed by the CPU on memory accesses and the dynamic energy consumed by the\nidle cores. This article presents an analytical model for the performance and\nthe overall energy consumed by the CPU chip on CPU instructions as well as the\nmemory accesses without ignoring the dynamic energy consumed by the idle cores.\nWe present an analytical framework around our energy-performance model to\npredict the operating frequencies for global DVFS that minimize the overall CPU\nenergy consumption within a performance budget. Finally, we suggest a\nscheduling criteria for energy aware scheduling of memory intensive parallel\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2016 10:28:34 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Trehan", "Chhaya", ""], ["Vandierendonck", "Hans", ""], ["Karakonstantis", "Georgios", ""], ["Nikolopoulos", "Dimitrios S.", ""]]}, {"id": "1605.04447", "submitter": "Sayyed Ali Mirsoleimani", "authors": "S. Ali Mirsoleimani, Aske Plaat, Jaap van den Herik and Jos Vermaseren", "title": "A New Method for Parallel Monte Carlo Tree Search", "comments": "5 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years there has been much interest in the Monte Carlo tree search\nalgorithm, a new, adaptive, randomized optimization algorithm. In fields as\ndiverse as Artificial Intelligence, Operations Research, and High Energy\nPhysics, research has established that Monte Carlo tree search can find good\nsolutions without domain dependent heuristics. However, practice shows that\nreaching high performance on large parallel machines is not so successful as\nexpected. This paper proposes a new method for parallel Monte Carlo tree search\nbased on the pipeline computation pattern.\n", "versions": [{"version": "v1", "created": "Sat, 14 May 2016 18:28:08 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Mirsoleimani", "S. Ali", ""], ["Plaat", "Aske", ""], ["Herik", "Jaap van den", ""], ["Vermaseren", "Jos", ""]]}, {"id": "1605.04580", "submitter": "Kiril Dichev", "authors": "Kiril Dichev and Dimitrios S. Nikolopoulos", "title": "TwinCG: Dual Thread Redundancy with Forward Recovery for Conjugate\n  Gradient Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even though iterative solvers like the Conjugate Gradients method (CG) have\nbeen studied for over fifty years, fault tolerance for such solvers has seen\nmuch attention in recent years. For iterative solvers, two major reliable\nstrategies of recovery exist: checkpoint-restart for backward recovery, or some\ntype of redundancy technique for forward recovery. Important redundancy\ntechniques like ABFT techniques for sparse matrix-vector products (SpMxV) have\nrecently been proposed, which increase the resilience of CG methods. These\ntechniques offer limited recovery options, and introduce a tolerable overhead.\nIn this work, we study a more powerful resilience concept, which is redundant\nmultithreading. It offers more generic and stronger recovery guarantees,\nincluding any soft faults in CG iterations (among others covering ABFT SpMxV),\nbut also requires more resources. We carefully study this redundancy/efficiency\nconflict. We propose a fault tolerant CG method, called TwinCG, which\nintroduces minimal wallclock time overhead, and significant advantages in\ndetection and correction strategies. Our method uses Dual Modular Redundancy\ninstead of the more expensive Triple Modular Redundancy; still, it retains the\nTMR advantages of fault correction. We describe, implement, and benchmark our\niterative solver, and compare it in terms of efficiency and fault tolerance\ncapabilities to state-of-the-art techniques. We find that before\nparallelization, TwinCG introduces around 5-6% runtime overhead compared to\nstandard CG, and after parallelization efficiently uses BLAS. In the presence\nof faults, it reliably performs forward recovery for a range of problems,\noutperforming SpMxV ABFT solutions.\n", "versions": [{"version": "v1", "created": "Sun, 15 May 2016 17:34:57 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Dichev", "Kiril", ""], ["Nikolopoulos", "Dimitrios S.", ""]]}, {"id": "1605.04614", "submitter": "Amund Tveit", "authors": "Amund Tveit, Torbj{\\o}rn Morland and Thomas Brox R{\\o}st", "title": "DeepLearningKit - an GPU Optimized Deep Learning Framework for Apple's\n  iOS, OS X and tvOS developed in Metal and Swift", "comments": "9 pages, 12 figures, open source documentation and code at\n  deeplearningkit.org and github.com/deeplearningkit", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present DeepLearningKit - an open source framework that\nsupports using pretrained deep learning models (convolutional neural networks)\nfor iOS, OS X and tvOS. DeepLearningKit is developed in Metal in order to\nutilize the GPU efficiently and Swift for integration with applications, e.g.\niOS-based mobile apps on iPhone/iPad, tvOS-based apps for the big screen, or OS\nX desktop applications. The goal is to support using deep learning models\ntrained with popular frameworks such as Caffe, Torch, TensorFlow, Theano,\nPylearn, Deeplearning4J and Mocha. Given the massive GPU resources and time\nrequired to train Deep Learning models we suggest an App Store like model to\ndistribute and download pretrained and reusable Deep Learning models.\n", "versions": [{"version": "v1", "created": "Sun, 15 May 2016 23:19:48 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Tveit", "Amund", ""], ["Morland", "Torbj\u00f8rn", ""], ["R\u00f8st", "Thomas Brox", ""]]}, {"id": "1605.04652", "submitter": "Anand Padmanabha Iyer", "authors": "Anand Padmanabha Iyer, Ion Stoica, Mosharaf Chowdhury, Li Erran Li", "title": "Fast and Accurate Performance Analysis of LTE Radio Access Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An increasing amount of analytics is performed on data that is procured in a\nreal-time fashion to make real-time decisions. Such tasks include simple\nreporting on streams to sophisticated model building. However, the practicality\nof such analyses are impeded in several domains because they are faced with a\nfundamental trade-off between data collection latency and analysis accuracy.\n  In this paper, we study this trade-off in the context of a specific domain,\nCellular Radio Access Networks (RAN). Our choice of this domain is influenced\nby its commonalities with several other domains that produce real-time data,\nour access to a large live dataset, and their real-time nature and\ndimensionality which makes it a natural fit for a popular analysis technique,\nmachine learning (ML). We find that the latency accuracy trade-off can be\nresolved using two broad, general techniques: intelligent data grouping and\ntask formulations that leverage domain characteristics. Based on this, we\npresent CellScope, a system that addresses this challenge by applying a domain\nspecific formulation and application of Multi-task Learning (MTL) to RAN\nperformance analysis. It achieves this goal using three techniques: feature\nengineering to transform raw data into effective features, a PCA inspired\nsimilarity metric to group data from geographically nearby base stations\nsharing performance commonalities, and a hybrid online-offline model for\nefficient model updates. Our evaluation of CellScope shows that its accuracy\nimprovements over direct application of ML range from 2.5x to 4.4x while\nreducing the model update overhead by up to 4.8x. We have also used CellScope\nto analyze a live LTE consisting of over 2 million subscribers for a period of\nover 10 months, where it uncovered several problems and insights, some of them\npreviously unknown.\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2016 05:31:01 GMT"}, {"version": "v2", "created": "Tue, 17 May 2016 20:00:59 GMT"}], "update_date": "2016-05-19", "authors_parsed": [["Iyer", "Anand Padmanabha", ""], ["Stoica", "Ion", ""], ["Chowdhury", "Mosharaf", ""], ["Li", "Li Erran", ""]]}, {"id": "1605.04682", "submitter": "Gerhard Rauchecker", "authors": "Gerhard Rauchecker and Guido Schryen", "title": "High-Performance Computing for Scheduling Decision Support: A Parallel\n  Depth-First Search Heuristic", "comments": "ISBN# 978-0-646-95337-3 Presented at the Australasian Conference on\n  Information Systems 2015 (arXiv:1605.01032)", "journal-ref": null, "doi": null, "report-no": "ACIS/2015/7", "categories": "cs.DC cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Many academic disciplines - including information systems, computer science,\nand operations management - face scheduling problems as important decision\nmaking tasks. Since many scheduling problems are NP-hard in the strong sense,\nthere is a need for developing solution heuristics. For scheduling problems\nwith setup times on unrelated parallel machines, there is limited research on\nsolution methods and to the best of our knowledge, parallel computer\narchitectures have not yet been taken advantage of. We address this gap by\nproposing and implementing a new solution heuristic and by testing different\nparallelization strategies. In our computational experiments, we show that our\nheuristic calculates near-optimal solutions even for large instances and that\ncomputing time can be reduced substantially by our parallelization approach.\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2016 09:11:08 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Rauchecker", "Gerhard", ""], ["Schryen", "Guido", ""]]}, {"id": "1605.04876", "submitter": "Gabriele D'Angelo", "authors": "Gabriele D'Angelo, Stefano Ferretti, Vittorio Ghini", "title": "Simulation of the Internet of Things", "comments": "Proceedings of the IEEE 2016 International Conference on High\n  Performance Computing and Simulation (HPCS 2016)", "journal-ref": null, "doi": "10.1109/HPCSim.2016.7568309", "report-no": null, "categories": "cs.NI cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents main concepts and issues concerned with the simulation of\nInternet of Things (IoT). The heterogeneity of possible scenarios, arising from\nthe massive deployment of an enormous amount of sensors and devices, imposes\nthe use of sophisticated modeling and simulation techniques. In fact, the\nsimulation of IoT introduces several issues from both quantitative and\nqualitative aspects. We discuss novel simulation techniques to enhance\nscalability and to permit the real-time execution of massively populated IoT\nenvironments (e.g., large-scale smart cities). In particular, we claim that\nagent-based, adaptive Parallel and Distributed Simulation (PADS) approaches are\nneeded, together with multi-level simulation, which provide means to perform\nhighly detailed simulations, on demand. We present a use case concerned with\nthe simulation of smart territories.\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2016 19:17:59 GMT"}, {"version": "v2", "created": "Mon, 26 Sep 2016 09:45:48 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["D'Angelo", "Gabriele", ""], ["Ferretti", "Stefano", ""], ["Ghini", "Vittorio", ""]]}, {"id": "1605.05109", "submitter": "Seri Khoury", "authors": "Amir Abboud, Keren Censor-Hillel and Seri Khoury", "title": "Near-Linear Lower Bounds for Distributed Distance Computations, Even in\n  Sparse Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new technique for constructing sparse graphs that allow us to\nprove near-linear lower bounds on the round complexity of computing distances\nin the CONGEST model. Specifically, we show an $\\widetilde{\\Omega}(n)$ lower\nbound for computing the diameter in sparse networks, which was previously known\nonly for dense networks [Frishknecht et al., SODA 2012]. In fact, we can even\nmodify our construction to obtain graphs with constant degree, using a simple\nbut powerful degree-reduction technique which we define.\n  Moreover, our technique allows us to show $\\widetilde{\\Omega}(n)$ lower\nbounds for computing $(\\frac{3}{2}-\\varepsilon)$-approximations of the diameter\nor the radius, and for computing a $(\\frac{5}{3}-\\varepsilon)$-approximation of\nall eccentricities. For radius, we are unaware of any previous lower bounds.\nFor diameter, these greatly improve upon previous lower bounds and are tight up\nto polylogarithmic factors [Frishknecht et al., SODA 2012], and for\neccentricities the improvement is both in the lower bound and in the\napproximation factor [Holzer and Wattenhofer, PODC 2012].\n  Interestingly, our technique also allows showing an almost-linear lower bound\nfor the verification of $(\\alpha,\\beta)$-spanners, for $\\alpha < \\beta+1$.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 11:02:16 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Abboud", "Amir", ""], ["Censor-Hillel", "Keren", ""], ["Khoury", "Seri", ""]]}, {"id": "1605.05219", "submitter": "Jonny Daenen", "authors": "Jonny Daenen, Frank Neven, Tony Tan, Stijn Vansummeren", "title": "Parallel Evaluation of Multi-Semi-Joins", "comments": "added Gumbo code reference, added Subset Sum reference, adjusted\n  alignment in Figure 1, adjusted Figure 5 (remove redundant units, larger\n  font), removed capitals in Table 2, boxes for environment ends, clarified\n  proof in appendix, reference cleanup (pages, capitalization), uncapitalized\n  \"REQUEST\" and \"ASSERT\" when used in text, small rewordings (no results\n  affected)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While services such as Amazon AWS make computing power abundantly available,\nadding more computing nodes can incur high costs in, for instance,\npay-as-you-go plans while not always significantly improving the net running\ntime (aka wall-clock time) of queries. In this work, we provide algorithms for\nparallel evaluation of SGF queries in MapReduce that optimize total time, while\nretaining low net time. Not only can SGF queries specify all semi-join\nreducers, but also more expressive queries involving disjunction and negation.\nSince SGF queries can be seen as Boolean combinations of (potentially nested)\nsemi-joins, we introduce a novel multi-semi-join (MSJ) MapReduce operator that\nenables the evaluation of a set of semi-joins in one job. We use this operator\nto obtain parallel query plans for SGF queries that outvalue sequential plans\nw.r.t. net time and provide additional optimizations aimed at minimizing total\ntime without severely affecting net time. Even though the latter optimizations\nare NP-hard, we present effective greedy algorithms. Our experiments, conducted\nusing our own implementation Gumbo on top of Hadoop, confirm the usefulness of\nparallel query plans, and the effectiveness and scalability of our\noptimizations, all with a significant improvement over Pig and Hive.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 15:53:27 GMT"}, {"version": "v2", "created": "Sun, 22 May 2016 11:22:44 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Daenen", "Jonny", ""], ["Neven", "Frank", ""], ["Tan", "Tony", ""], ["Vansummeren", "Stijn", ""]]}, {"id": "1605.05236", "submitter": "William Kuszmaul", "authors": "William Kuszmaul", "title": "Fast Concurrent Cuckoo Kick-Out Eviction Schemes for High-Density Tables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cuckoo hashing guarantees constant-time lookups regardless of table density,\nmaking it a viable candidate for high-density tables. Cuckoo hashing insertions\nperform poorly at high table densities, however. In this paper, we mitigate\nthis problem through the introduction of novel kick-out eviction algorithms.\nExperimentally, our algorithms reduce the number of bins viewed per insertion\nfor high-density tables by as much as a factor of ten.\n  We also introduce an optimistic concurrency scheme for transactional\nmulti-writer cuckoo hash tables (not using hardware transactional memory). For\ndelete-light workloads, one of our kick-out algorithms avoids all competition\nbetween insertions with high probability, and significantly reduces\ntransaction-abort frequency. This result is extended to arbitrary workloads\nusing a new synchronization mechanism called a claim flag.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 16:55:43 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Kuszmaul", "William", ""]]}, {"id": "1605.05436", "submitter": "Michael Goodrich", "authors": "Michael T. Goodrich, Ahmed Eldawy", "title": "Parallel Algorithms for Summing Floating-Point Numbers", "comments": "Conference version appears in SPAA 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of exactly summing n floating-point numbers is a fundamental\nproblem that has many applications in large-scale simulations and computational\ngeometry. Unfortunately, due to the round-off error in standard floating-point\noperations, this problem becomes very challenging. Moreover, all existing\nsolutions rely on sequential algorithms which cannot scale to the huge datasets\nthat need to be processed.\n  In this paper, we provide several efficient parallel algorithms for summing n\nfloating point numbers, so as to produce a faithfully rounded floating-point\nrepresentation of the sum. We present algorithms in PRAM, external-memory, and\nMapReduce models, and we also provide an experimental analysis of our MapReduce\nalgorithms, due to their simplicity and practical efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 04:20:41 GMT"}], "update_date": "2016-05-19", "authors_parsed": [["Goodrich", "Michael T.", ""], ["Eldawy", "Ahmed", ""]]}, {"id": "1605.05438", "submitter": "Vincent Gramoli", "authors": "Christopher Natoli and Vincent Gramoli", "title": "The Blockchain Anomaly", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most popular blockchain solutions, like Bitcoin, rely on proof-of-work,\nguaranteeing that the output of the consensus is agreed upon with high\nprobability. However, this probability depends on the delivery of messages and\nthat the computational power of the system is sufficiently scattered among\npools of nodes in the network so that no pool can mine more blocks faster than\nthe crowd. New approaches, like Ethereum, generalise the proof-of-work approach\nby letting individuals deploy their own private blockchain with high\ntransaction throughput. As companies are starting to deploy private chains, it\nhas become crucial to better understand the guarantees blockchains offer in\nsuch a small and controlled environment.\n  In this paper, we present the \\emph{Blockchain Anomaly}, an execution that we\nexperienced when building our private chain at NICTA/Data61. Even though this\nanomaly has never been acknowledged before, it may translate into dramatic\nconsequences for the user of blockchains. Named after the infamous Paxos\nanomaly, this anomaly makes dependent transactions, like \"Bob sends money to\nCarole after he received money from Alice\" impossible. This anomaly relies on\nthe fact that existing blockchains do not ensure consensus safety\ndeterministically: there is no way for Bob to make sure that Alice actually\nsent him coins without Bob using an external mechanism, like converting these\ncoins into a fiat currency that allows him to withdraw. We also explore smart\ncontracts as a potential alternative to transactions in order to freeze coins,\nand show implementations of smart contract that can suffer from the Blockchain\nanomaly and others that may cope with it.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 04:51:33 GMT"}], "update_date": "2016-05-19", "authors_parsed": [["Natoli", "Christopher", ""], ["Gramoli", "Vincent", ""]]}, {"id": "1605.05590", "submitter": "Matteo Ceccarello", "authors": "Matteo Ceccarello, Andrea Pietracaprina, Geppino Pucci, Eli Upfal", "title": "MapReduce and Streaming Algorithms for Diversity Maximization in Metric\n  Spaces of Bounded Doubling Dimension", "comments": "Extended version of\n  http://www.vldb.org/pvldb/vol10/p469-ceccarello.pdf, PVLDB Volume 10, No. 5,\n  January 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a dataset of points in a metric space and an integer $k$, a diversity\nmaximization problem requires determining a subset of $k$ points maximizing\nsome diversity objective measure, e.g., the minimum or the average distance\nbetween two points in the subset. Diversity maximization is computationally\nhard, hence only approximate solutions can be hoped for. Although its\napplications are mainly in massive data analysis, most of the past research on\ndiversity maximization focused on the sequential setting. In this work we\npresent space and pass/round-efficient diversity maximization algorithms for\nthe Streaming and MapReduce models and analyze their approximation guarantees\nfor the relevant class of metric spaces of bounded doubling dimension. Like\nother approaches in the literature, our algorithms rely on the determination of\nhigh-quality core-sets, i.e., (much) smaller subsets of the input which contain\ngood approximations to the optimal solution for the whole input. For a variety\nof diversity objective functions, our algorithms attain an\n$(\\alpha+\\epsilon)$-approximation ratio, for any constant $\\epsilon>0$, where\n$\\alpha$ is the best approximation ratio achieved by a polynomial-time,\nlinear-space sequential algorithm for the same diversity objective. This\nimproves substantially over the approximation ratios attainable in Streaming\nand MapReduce by state-of-the-art algorithms for general metric spaces. We\nprovide extensive experimental evidence of the effectiveness of our algorithms\non both real world and synthetic datasets, scaling up to over a billion points.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 14:11:31 GMT"}, {"version": "v2", "created": "Mon, 20 Jun 2016 12:55:52 GMT"}, {"version": "v3", "created": "Sun, 16 Oct 2016 13:04:51 GMT"}, {"version": "v4", "created": "Mon, 23 Jan 2017 16:10:19 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Ceccarello", "Matteo", ""], ["Pietracaprina", "Andrea", ""], ["Pucci", "Geppino", ""], ["Upfal", "Eli", ""]]}, {"id": "1605.05619", "submitter": "Huynh Tu Dang Mr", "authors": "Huynh Tu Dang, Pietro Bressana, Han Wang, Ki Suh Lee, Hakim\n  Weatherspoon, Marco Canini, Fernando Pedone, Robert Soul\\'e", "title": "Network Hardware-Accelerated Consensus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consensus protocols are the foundation for building many fault-tolerant\ndistributed systems and services. This paper posits that there are significant\nperformance benefits to be gained by offering consensus as a network service\n(CAANS). CAANS leverages recent advances in commodity networking hardware\ndesign and programmability to implement consensus protocol logic in network\ndevices. CAANS provides a complete Paxos protocol, is a drop-in replacement for\nsoftware-based implementations of Paxos, makes no restrictions on network\ntopologies, and is implemented in a higher-level, data-plane programming\nlanguage, allowing for portability across a range of target devices. At the\nsame time, CAANS significantly increases throughput and reduces latency for\nconsensus operations. Consensus logic executing in hardware can transmit\nconsensus messages at line speed, with latency only slightly higher than simply\nforwarding packets.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 15:35:04 GMT"}], "update_date": "2016-05-19", "authors_parsed": [["Dang", "Huynh Tu", ""], ["Bressana", "Pietro", ""], ["Wang", "Han", ""], ["Lee", "Ki Suh", ""], ["Weatherspoon", "Hakim", ""], ["Canini", "Marco", ""], ["Pedone", "Fernando", ""], ["Soul\u00e9", "Robert", ""]]}, {"id": "1605.05717", "submitter": "Narayana Moorthy Prakash", "authors": "Kishori M. Konwar, N. Prakash, Nancy Lynch and Muriel Medard", "title": "RADON: Repairable Atomic Data Object in Networks", "comments": "To be presented at OPODIS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Erasure codes offer an efficient way to decrease storage and communication\ncosts while implementing atomic memory service in asynchronous distributed\nstorage systems. In this paper, we provide erasure-code-based algorithms having\nthe additional ability to perform background repair of crashed nodes. A repair\noperation of a node in the crashed state is triggered externally, and is\ncarried out by the concerned node via message exchanges with other active nodes\nin the system. Upon completion of repair, the node re-enters active state, and\nresumes participation in ongoing and future read, write, and repair operations.\nTo guarantee liveness and atomicity simultaneously, existing works assume\neither the presence of nodes with stable storage, or presence of nodes that\nnever crash during the execution. We demand neither of these; instead we\nconsider a natural, yet practical network stability condition $N1$ that only\nrestricts the number of nodes in the crashed/repair state during broadcast of\nany message.\n  We present an erasure-code based algorithm $RADON_C$ that is always live, and\nguarantees atomicity as long as condition $N1$ holds. In situations when the\nnumber of concurrent writes is limited, $RADON_C$ has significantly improved\nstorage and communication cost over a replication-based algorithm $RADON_R$,\nwhich also works under $N1$. We further show how a slightly stronger network\nstability condition $N2$ can be used to construct algorithms that never violate\natomicity. The guarantee of atomicity comes at the expense of having an\nadditional phase during the read and write operations.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 19:39:14 GMT"}, {"version": "v2", "created": "Mon, 21 Nov 2016 16:22:35 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Konwar", "Kishori M.", ""], ["Prakash", "N.", ""], ["Lynch", "Nancy", ""], ["Medard", "Muriel", ""]]}, {"id": "1605.05773", "submitter": "Richard Barnes", "authors": "Richard Barnes, Clarence Lehman, David Mulla", "title": "Distributed Parallel D8 Up-Slope Area Calculation in Digital Elevation\n  Models", "comments": "6 pages, 2 figures, 8 algorithms", "journal-ref": "Proceedings of the 2011 International Conference on Parallel and\n  Distributed Processing Techniques and Applications. Las Vegas, NV. p. 833-838", "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a parallel algorithm for calculating the\neight-directional (D8) up-slope contributing area in digital elevation models\n(DEMs). In contrast with previous algorithms, which have potentially unbounded\ninter-node communications, the algorithm presented here realizes strict bounds\non the number of inter-node communications. Those bounds in turn allow D8\nattributes to be processed for arbitrarily large DEMs on hardware ranging from\naverage desktops to supercomputers. The algorithm can use the OpenMP and MPI\nparallel computing models, either in combination or separately. It partitions\nthe DEM between slave nodes, calculates an internal up-slope area by replacing\ninformation from other slaves with variables representing unknown quantities,\npasses the results on to a master node which combines all the slaves' data, and\npasses information back to each slave, which then computes its final result. In\nthis way each slave's DEM partition is treated as a simple unit in the DEM as a\nwhole and only two communications take place per node.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 22:13:03 GMT"}], "update_date": "2016-05-20", "authors_parsed": [["Barnes", "Richard", ""], ["Lehman", "Clarence", ""], ["Mulla", "David", ""]]}, {"id": "1605.05826", "submitter": "Matthias Boehm", "authors": "Matthias Boehm, Alexandre V. Evfimievski, Niketan Pansare, Berthold\n  Reinwald", "title": "Declarative Machine Learning - A Classification of Basic Properties and\n  Types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Declarative machine learning (ML) aims at the high-level specification of ML\ntasks or algorithms, and automatic generation of optimized execution plans from\nthese specifications. The fundamental goal is to simplify the usage and/or\ndevelopment of ML algorithms, which is especially important in the context of\nlarge-scale computations. However, ML systems at different abstraction levels\nhave emerged over time and accordingly there has been a controversy about the\nmeaning of this general definition of declarative ML. Specification\nalternatives range from ML algorithms expressed in domain-specific languages\n(DSLs) with optimization for performance, to ML task (learning problem)\nspecifications with optimization for performance and accuracy. We argue that\nthese different types of declarative ML complement each other as they address\ndifferent users (data scientists and end users). This paper makes an attempt to\ncreate a taxonomy for declarative ML, including a definition of essential basic\nproperties and types of declarative ML. Along the way, we provide insights into\nimplications of these properties. We also use this taxonomy to classify\nexisting systems. Finally, we draw conclusions on defining appropriate\nbenchmarks and specification languages for declarative ML.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 06:39:28 GMT"}], "update_date": "2016-05-20", "authors_parsed": [["Boehm", "Matthias", ""], ["Evfimievski", "Alexandre V.", ""], ["Pansare", "Niketan", ""], ["Reinwald", "Berthold", ""]]}, {"id": "1605.06093", "submitter": "Giovanni Viglietta", "authors": "Marcello Mamino and Giovanni Viglietta", "title": "Square Formation by Asynchronous Oblivious Robots", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental problem in Distributed Computing is the Pattern Formation\nproblem, where some independent mobile entities, called robots, have to\nrearrange themselves in such a way as to form a given figure from every\npossible (non-degenerate) initial configuration.\n  In the present paper, we consider robots that operate in the Euclidean plane\nand are dimensionless, anonymous, oblivious, silent, asynchronous, disoriented,\nnon-chiral, and non-rigid. For this very elementary type of robots, the\nfeasibility of the Pattern Formation problem has been settled, either in the\npositive or in the negative, for every possible pattern, except for one case:\nthe Square Formation problem by a team of four robots.\n  Here we solve this last case by giving a Square Formation algorithm and\nproving its correctness. Our contribution represents the concluding chapter in\na long thread of research. Our results imply that in the context of the Pattern\nFormation problem for mobile robots, features such as synchronicity, chirality,\nand rigidity are computationally irrelevant.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 19:40:36 GMT"}, {"version": "v2", "created": "Fri, 20 May 2016 08:18:07 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Mamino", "Marcello", ""], ["Viglietta", "Giovanni", ""]]}, {"id": "1605.06399", "submitter": "Thomas Falch", "authors": "Thomas L. Falch, Anne C. Elster", "title": "ImageCL: An Image Processing Language for Performance Portability on\n  Heterogeneous Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern computer systems typically conbine multicore CPUs with accelerators\nlike GPUs for inproved performance and energy efficiency. However, these sys-\ntems suffer from poor performance portability, code tuned for one device must\nbe retuned to achieve high performance on another. Image processing is increas-\ning in importance , with applications ranging from seismology and medicine to\nPhotoshop. Based on our experience with medical image processing, we propose\nImageCL, a high-level domain-specific language and source-to-source compiler,\ntargeting heterogeneous hardware. ImageCL resembles OpenCL, but abstracts away\nper- formance optimization details, allowing the programmer to focus on\nalgorithm development, rather than performance tuning. The latter is left to\nour source-to- source compiler and auto-tuner. From high-level ImageCL kernels,\nour source- to-source compiler can generate multiple OpenCL implementations\nwith different optimizations applied. We rely on auto-tuning rather than\nmachine models or ex- pert programmer knowledge to determine which\noptimizations to apply, making our tuning procedure highly robust. Furthermore,\nwe can generate high perform- ing implementations for different devices from a\nsingle source code, thereby im- proving performance portability. We evaluate\nour approach on three image processing benchmarks, on different GPU and CPU\ndevices, and are able to outperform other state of the art solutions in several\ncases, achieving speedups of up to 4.57x.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 15:19:23 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Falch", "Thomas L.", ""], ["Elster", "Anne C.", ""]]}, {"id": "1605.06486", "submitter": "Talal Riaz", "authors": "Sriram Pemmaraju and Talal Riaz", "title": "Using Read-$k$ Inequalities to Analyze a Distributed MIS Algorithm", "comments": "To appear in PODC 2016 as a brief announcement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Until recently, the fastest distributed MIS algorithm, even for simple\ngraphs, e.g., unoriented trees has been the simple randomized algorithm\ndiscovered the 80s. This algorithm (commonly called Luby's algorithm) computes\nan MIS in $O(\\log n)$ rounds (with high probability). This situation changed\nwhen Lenzen and Wattenhofer (PODC 2011) presented a randomized $O(\\sqrt{\\log\nn}\\cdot \\log\\log n)$-round MIS algorithm for unoriented trees. This algorithm\nwas improved by Barenboim et al. (FOCS 2012), resulting in an $O(\\sqrt{\\log n\n\\cdot \\log\\log n})$-round MIS algorithm.\n  The analyses of these tree MIS algorithms depends on \"near independence\" of\nprobabilistic events, a feature of the tree structure of the network. In their\npaper, Lenzen and Wattenhofer hope that their algorithm and analysis could be\nextended to graphs with bounded arboricity. We show how to do this. By using a\nnew tail inequality for read-k families of random variables due to Gavinsky et\nal. (Random Struct Algorithms, 2015), we show how to deal with dependencies\ninduced by the recent tree MIS algorithms when they are executed on bounded\narboricity graphs. Specifically, we analyze a version of the tree MIS algorithm\nof Barenboim et al. and show that it runs in $O(\\mbox{poly}(\\alpha) \\cdot\n\\sqrt{\\log n \\cdot \\log\\log n})$ rounds in the $\\mathcal{CONGEST}$ model for\ngraphs with arboricity $\\alpha$.\n  While the main thrust of this paper is the new probabilistic analysis via\nread-$k$ inequalities, for small values of $\\alpha$, this algorithm is faster\nthan the bounded arboricity MIS algorithm of Barenboim et al. We also note that\nrecently (SODA 2016), Gaffari presented a novel MIS algorithm for general\ngraphs that runs in $O(\\log \\Delta) + 2^{O(\\sqrt{\\log\\log n})}$ rounds; a\ncorollary of this algorithm is an $O(\\log \\alpha + \\sqrt{\\log n})$-round MIS\nalgorithm on arboricity-$\\alpha$ graphs.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 19:42:16 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Pemmaraju", "Sriram", ""], ["Riaz", "Talal", ""]]}, {"id": "1605.06619", "submitter": "Yitan Li", "authors": "Yitan Li, Linli Xu, Xiaowei Zhong, Qing Ling", "title": "Make Workers Work Harder: Decoupled Asynchronous Proximal Stochastic\n  Gradient Descent", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asynchronous parallel optimization algorithms for solving large-scale machine\nlearning problems have drawn significant attention from academia to industry\nrecently. This paper proposes a novel algorithm, decoupled asynchronous\nproximal stochastic gradient descent (DAP-SGD), to minimize an objective\nfunction that is the composite of the average of multiple empirical losses and\na regularization term. Unlike the traditional asynchronous proximal stochastic\ngradient descent (TAP-SGD) in which the master carries much of the computation\nload, the proposed algorithm off-loads the majority of computation tasks from\nthe master to workers, and leaves the master to conduct simple addition\noperations. This strategy yields an easy-to-parallelize algorithm, whose\nperformance is justified by theoretical convergence analyses. To be specific,\nDAP-SGD achieves an $O(\\log T/T)$ rate when the step-size is diminishing and an\nergodic $O(1/\\sqrt{T})$ rate when the step-size is constant, where $T$ is the\nnumber of total iterations.\n", "versions": [{"version": "v1", "created": "Sat, 21 May 2016 10:27:50 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Li", "Yitan", ""], ["Xu", "Linli", ""], ["Zhong", "Xiaowei", ""], ["Ling", "Qing", ""]]}, {"id": "1605.06678", "submitter": "Xuanzhe Liu", "authors": "Xuanzhe Liu, Yun Ma, Shuailiang Dong, Yunxin Liu, Tao Xie, Gang Huang,\n  Hong Mei", "title": "Mitigating Redundant Data Transfers for Mobile Web Applications via\n  App-Specific Cache Space", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Redundant transfer of resources is a critical issue for compromising the\nperformance of mobile Web applications (a.k.a., apps) in terms of data traffic,\nload time, and even energy consumption. Evidence shows that the current cache\nmechanisms are far from satisfactory. With lessons learned from how native apps\nmanage their resources, in this paper, we propose the ReWAP approach to\nfundamentally reducing redundant transfers by restructuring the resource\nloading of mobile Web apps. ReWAP is based on an efficient mechanism of\nresource packaging where stable resources are encapsulated and maintained into\na package, and such a package shall be loaded always from the local storage and\nupdated by explicitly refreshing. By retrieving and analyzing the update of\nresources, ReWAP maintains resource packages that can accurately identify which\nresources can be loaded from the local storage for a considerably long period.\nReWAP also provides a wrapper for mobile Web apps to enable loading and\nupdating resource packages in the local storage as well as loading resources\nfrom resource packages. ReWAP can be easily and seamlessly deployed into\nexisting mobile Web architectures with minimal modifications, and is\ntransparent to end-users. We evaluate ReWAP based on continuous 15-day access\ntraces of 50 mobile Web apps that suffer heavily from the problem of redundant\ntransfers. Compared to the original mobile Web apps with cache enabled, ReWAP\ncan significantly reduce the data traffic, with the median saving up to 51%. In\naddition, ReWAP can incur only very minor runtime overhead of the client-side\nbrowsers.\n", "versions": [{"version": "v1", "created": "Sat, 21 May 2016 17:35:04 GMT"}, {"version": "v2", "created": "Fri, 9 Dec 2016 15:09:13 GMT"}], "update_date": "2016-12-12", "authors_parsed": [["Liu", "Xuanzhe", ""], ["Ma", "Yun", ""], ["Dong", "Shuailiang", ""], ["Liu", "Yunxin", ""], ["Xie", "Tao", ""], ["Huang", "Gang", ""], ["Mei", "Hong", ""]]}, {"id": "1605.06692", "submitter": "Andrey Nikiforov", "authors": "Elena V. Djukova, Andrey G. Nikiforov, Petr A. Prokofyev", "title": "Parallelizing asymptotically optimal algorithms for large-scale\n  dualization problems", "comments": "17 pages, 3 figures (5 images)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dualization is a key discrete enumeration problem. It is not known whether or\nnot this problem is polynomial-time solvable. Asymptotically optimal\ndualization algorithms are the fastest among the known dualization algorithms,\nwhich is supported by new experiments with various data described in this\npaper. A theoretical justification of the efficiency of these algorithms on the\naverage was given by E.V. Djukova more than 30 years ago. In this paper, new\nresults on the construction of parallel algorithms for intractable enumeration\nproblems are presented. A new static parallelization scheme for asymptotically\noptimal dualization algorithms is developed and tested. The scheme is based on\nstatistical estimations of subtasks size.\n", "versions": [{"version": "v1", "created": "Sat, 21 May 2016 19:48:26 GMT"}, {"version": "v2", "created": "Sat, 28 May 2016 20:21:41 GMT"}, {"version": "v3", "created": "Tue, 31 May 2016 06:01:31 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Djukova", "Elena V.", ""], ["Nikiforov", "Andrey G.", ""], ["Prokofyev", "Petr A.", ""]]}, {"id": "1605.06814", "submitter": "Will Rosenbaum", "authors": "Rafail Ostrovsky, Mor Perry, Will Rosenbaum", "title": "Space-Time Tradeoffs for Distributed Verification", "comments": "Pre-proceedings version of paper presented at the 24th International\n  Colloquium on Structural Information and Communication Complexity (SIROCCO\n  2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Verifying that a network configuration satisfies a given boolean predicate is\na fundamental problem in distributed computing. Many variations of this problem\nhave been studied, for example, in the context of proof labeling schemes (PLS),\nlocally checkable proofs (LCP), and non-deterministic local decision (NLD). In\nall of these contexts, verification time is assumed to be constant. Korman,\nKutten and Masuzawa [PODC 2011] presented a proof-labeling scheme for MST, with\npoly-logarithmic verification time, and logarithmic memory at each vertex.\n  In this paper we introduce the notion of a $t$-PLS, which allows the\nverification procedure to run for super-constant time. Our work analyzes the\ntradeoffs of $t$-PLS between time, label size, message length, and computation\nspace. We construct a universal $t$-PLS and prove that it uses the same amount\nof total communication as a known one-round universal PLS, and $t$ factor\nsmaller labels. In addition, we provide a general technique to prove lower\nbounds for space-time tradeoffs of $t$-PLS. We use this technique to show an\noptimal tradeoff for testing that a network is acyclic (cycle free). Our\noptimal $t$-PLS for acyclicity uses label size and computation space $O((\\log\nn)/t)$. We further describe a recursive $O(\\log^* n)$ space verifier for\nacyclicity which does not assume previous knowledge of the run-time $t$.\n", "versions": [{"version": "v1", "created": "Sun, 22 May 2016 16:13:24 GMT"}, {"version": "v2", "created": "Mon, 21 Aug 2017 03:28:23 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Ostrovsky", "Rafail", ""], ["Perry", "Mor", ""], ["Rosenbaum", "Will", ""]]}, {"id": "1605.06844", "submitter": "Zhiying Wang", "authors": "Viveck R. Cadambe and Zhiying Wang and Nancy Lynch", "title": "Information-Theoretic Lower Bounds on the Storage Cost of Shared Memory\n  Emulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The focus of this paper is to understand storage costs of emulating an atomic\nshared memory over an asynchronous, distributed message passing system.\nPrevious literature has developed several shared memory emulation algorithms\nbased on replication and erasure coding techniques. In this paper, we present\ninformation-theoretic lower bounds on the storage costs incurred by shared\nmemory emulation algorithms. Our storage cost lower bounds are universally\napplicable, that is, we make no assumption on the structure of the algorithm or\nthe method of encoding the data.\n  We consider an arbitrary algorithm $A$ that implements an atomic multi-writer\nsingle-reader (MWSR) shared memory variable whose values come from a finite set\n$\\mathcal{V}$ over a system of $N$ servers connected by point-to-point\nasynchronous links. We require that in every fair execution of algorithm $A$\nwhere the number of server failures is smaller than a parameter $f$, every\noperation invoked at a non-failing client terminates. We define the storage\ncost of a server in algorithm $A$ as the logarithm (to base 2) of number of\nstates it can take on; the total-storage cost of algorithm $A$ is the sum of\nthe storage cost of all servers.\n  Our results are as follows. (i) We show that if algorithm $A$ does not use\nserver gossip, then the total storage cost is lower bounded by $2\n\\frac{N}{N-f+1}\\log_2|\\mathcal{V}|-o(\\log_2|\\mathcal{V}|)$. (ii) The total\nstorage cost is at least $2 \\frac{N}{N-f+2}\n\\log_{2}|\\mathcal{V}|-o(\\log_{2}|\\mathcal{V}|)$ even if the algorithm uses\nserver gossip. (iii) We consider algorithms where the write protocol sends\ninformation about the value in at most one phase. We show that the total\nstorage cost is at least $\\nu^* \\frac{N}{N-f+\\nu^*-1} \\log_2( |\\mathcal{V}|)-\no(\\log_2(|\\mathcal{V}|),$ where $\\nu^*$ is the minimum of $f+1$ and the number\nof active write operations of an execution.\n", "versions": [{"version": "v1", "created": "Sun, 22 May 2016 19:55:05 GMT"}, {"version": "v2", "created": "Tue, 24 May 2016 19:25:37 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Cadambe", "Viveck R.", ""], ["Wang", "Zhiying", ""], ["Lynch", "Nancy", ""]]}, {"id": "1605.06882", "submitter": "Stephan Holzer", "authors": "Benjamin Dissler, Stephan Holzer, Roger Wattenhofer", "title": "Distributed Local Multi-Aggregation and Centrality Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study local aggregation and graph analysis in distributed environments\nusing the message passing model. We provide a flexible framework, where each of\nthe nodes in a set $S$--which is a subset of all nodes in the network--can\nperform a large range of common aggregation functions in its $k$-neighborhood.\nWe study this problem in the CONGEST model, where in each synchronous round,\nevery node can transmit a different (but short) message to each of its\nneighbors. While the $k$-neighborhoods of nodes in $S$ might overlap and\naggregation could cause congestion in this model, we present an algorithm that\nneeds time $O(|S|+k)$ even when each of the nodes in $S$ performs a different\naggregation on its $k$-neighborhood. The framework is not restricted to\naggregation-trees such that it can be used for more advanced graph analysis. We\ndemonstrate this by providing efficient approximations of centrality measures\nand approximation of minimum routing cost trees.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 03:31:47 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Dissler", "Benjamin", ""], ["Holzer", "Stephan", ""], ["Wattenhofer", "Roger", ""]]}, {"id": "1605.06894", "submitter": "Chao Wang", "authors": "Chao Wang, Qi Yu, Lei Gong, Xi Li, Yuan Xie, Xuehai Zhou", "title": "DLAU: A Scalable Deep Learning Accelerator Unit on FPGA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the emerging field of machine learning, deep learning shows excellent\nability in solving complex learning problems. However, the size of the networks\nbecomes increasingly large scale due to the demands of the practical\napplications, which poses significant challenge to construct a high performance\nimplementations of deep learning neural networks. In order to improve the\nperformance as well to maintain the low power cost, in this paper we design\nDLAU, which is a scalable accelerator architecture for large-scale deep\nlearning networks using FPGA as the hardware prototype. The DLAU accelerator\nemploys three pipelined processing units to improve the throughput and utilizes\ntile techniques to explore locality for deep learning applications.\nExperimental results on the state-of-the-art Xilinx FPGA board demonstrate that\nthe DLAU accelerator is able to achieve up to 36.1x speedup comparing to the\nIntel Core2 processors, with the power consumption at 234mW.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 04:56:04 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Wang", "Chao", ""], ["Yu", "Qi", ""], ["Gong", "Lei", ""], ["Li", "Xi", ""], ["Xie", "Yuan", ""], ["Zhou", "Xuehai", ""]]}, {"id": "1605.06904", "submitter": "Jhoirene Clemente", "authors": "Jhoirene B. Clemente, Francis George C. Cabarle, Henry N. Adorna", "title": "PROJECTION Algorithm for Motif Finding on GPUs", "comments": "Workshop on Computation: Theory and Practice. Theory and Practice of\n  Computation, Volume 5 of the series Proceedings in Information and\n  Communications Technology pp 101-115 (2012)", "journal-ref": null, "doi": "10.1007/978-4-431-54106-6_9", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Motif finding is one of the NP-complete problems in Computational Biology.\nExisting nondeterministic algorithms for motif finding do not guarantee the\nglobal optimality of results and are sensitive to initial parameters. To\naddress this problem, the PROJECTION algorithm provides a good initial estimate\nthat can be further refined using local optimization algorithms such as EM,\nMEME or Gibbs. For large enough input (600-1000 bp per sequence) or for\nchallenging motif finding problems, the PROJECTION algorithm may run in an\ninordinate amount of time. In this paper we present a parallel implementation\nof the PROJECTION algorithm in Graphics Processing Units (GPUs) using CUDA. We\nalso list down several major issues we have encountered including performing\nspace optimizations because of the GPU's space limitations.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 06:40:18 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Clemente", "Jhoirene B.", ""], ["Cabarle", "Francis George C.", ""], ["Adorna", "Henry N.", ""]]}, {"id": "1605.06940", "submitter": "Barry Hurley", "authors": "Barry Hurley, Deepak Mehta, Barry O'Sullivan", "title": "Elastic Solver: Balancing Solution Time and Energy Consumption", "comments": "Keywords: Combinatorial Optimisation, Energy Minimisation, Parallel\n  Solving", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combinatorial decision problems arise in many different domains such as\nscheduling, routing, packing, bioinformatics, and many more. Despite recent\nadvances in developing scalable solvers, there are still many problems which\nare often very hard to solve. Typically the most advanced solvers include\nelements which are stochastic in nature. If a same instance is solved many\ntimes using different seeds then depending on the inherent characteristics of a\nproblem instance and the solver, one can observe a highly-variant distribution\nof times spanning multiple orders of magnitude. Therefore, to solve a problem\ninstance efficiently it is often useful to solve the same instance in parallel\nwith different seeds. With the proliferation of cloud computing, it is natural\nto think about an elastic solver which can scale up by launching searches in\nparallel on thousands of machines (or cores). However, this could result in\nconsuming a lot of energy. Moreover, not every instance would require thousands\nof machines. The challenge is to resolve the tradeoff between solution time and\nenergy consumption optimally for a given problem instance. We analyse the\nimpact of the number of machines (or cores) on not only solution time but also\non energy consumption. We highlight that although solution time always drops as\nthe number of machines increases, the relation between the number of machines\nand energy consumption is more complicated. In many cases, the optimal energy\nconsumption may be achieved by a middle ground, we analyse this relationship in\ndetail. The tradeoff between solution time and energy consumption is studied\nfurther, showing that the energy consumption of a solver can be reduced\ndrastically if we increase the solution time marginally. We also develop a\nprediction model, demonstrating that such insights can be exploited to achieve\nfaster solutions times in a more energy efficient manor.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 08:54:27 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Hurley", "Barry", ""], ["Mehta", "Deepak", ""], ["O'Sullivan", "Barry", ""]]}, {"id": "1605.07083", "submitter": "Michele Ciavotta Dr.", "authors": "Michele Ciavotta, Eugenio Gianniti, Danilo Ardagna", "title": "D-SPACE4Cloud: A Design Tool for Big Data Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last years have seen a steep rise in data generation worldwide, with the\ndevelopment and widespread adoption of several software projects targeting the\nBig Data paradigm. Many companies currently engage in Big Data analytics as\npart of their core business activities, nonetheless there are no tools and\ntechniques to support the design of the underlying hardware configuration\nbacking such systems. In particular, the focus in this report is set on Cloud\ndeployed clusters, which represent a cost-effective alternative to on premises\ninstallations. We propose a novel tool implementing a battery of optimization\nand prediction techniques integrated so as to efficiently assess several\nalternative resource configurations, in order to determine the minimum cost\ncluster deployment satisfying QoS constraints. Further, the experimental\ncampaign conducted on real systems shows the validity and relevance of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 16:37:54 GMT"}, {"version": "v2", "created": "Tue, 24 May 2016 17:07:58 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Ciavotta", "Michele", ""], ["Gianniti", "Eugenio", ""], ["Ardagna", "Danilo", ""]]}, {"id": "1605.07354", "submitter": "Yannai A. Gonczarowski", "authors": "Armando Casta\\~neda, Yannai A. Gonczarowski, Yoram Moses", "title": "Unbeatable Set Consensus via Topological and Combinatorial Reasoning", "comments": "arXiv admin note: substantial text overlap with arXiv:1311.6902", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The set consensus problem has played an important role in the study of\ndistributed systems for over two decades. Indeed, the search for lower bounds\nand impossibility results for this problem spawned the topological approach to\ndistributed computing, which has given rise to new techniques in the design and\nanalysis of protocols. The design of efficient solutions to set consensus has\nalso proven to be challenging. In the synchronous crash failure model, the\nliterature contains a sequence of solutions to set consensus, each improving\nupon the previous ones.\n  This paper presents an unbeatable protocol for nonuniform k-set consensus in\nthe synchronous crash failure model. This is an efficient protocol whose\ndecision times cannot be improved upon. Moreover, the description of our\nprotocol is extremely succinct. Proving unbeatability of this protocol is a\nnontrivial challenge. We provide two proofs for its unbeatability: one is a\nsubtle constructive combinatorial proof, and the other is a topological proof\nof a new style. These two proofs provide new insight into the connection\nbetween topological reasoning and combinatorial reasoning about protocols,\nwhich has long been a subject of interest. In particular, our topological proof\nreasons in a novel way about subcomplexes of the protocol complex, and sheds\nlight on an open question posed by Guerraoui and Pochon (2009). Finally, using\nthe machinery developed in the design of this unbeatable protocol, we propose a\nprotocol for uniform k-set consensus that beats all known solutions by a large\nmargin.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 09:56:21 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Casta\u00f1eda", "Armando", ""], ["Gonczarowski", "Yannai A.", ""], ["Moses", "Yoram", ""]]}, {"id": "1605.07422", "submitter": "Rolf Jagerman", "authors": "Rolf Jagerman, Carsten Eickhoff and Maarten de Rijke", "title": "Computing Web-scale Topic Models using an Asynchronous Parameter Server", "comments": "To appear in SIGIR 2017", "journal-ref": null, "doi": "10.1145/3077136.3084135", "report-no": null, "categories": "cs.DC cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models such as Latent Dirichlet Allocation (LDA) have been widely used\nin information retrieval for tasks ranging from smoothing and feedback methods\nto tools for exploratory search and discovery. However, classical methods for\ninferring topic models do not scale up to the massive size of today's publicly\navailable Web-scale data sets. The state-of-the-art approaches rely on custom\nstrategies, implementations and hardware to facilitate their asynchronous,\ncommunication-intensive workloads.\n  We present APS-LDA, which integrates state-of-the-art topic modeling with\ncluster computing frameworks such as Spark using a novel asynchronous parameter\nserver. Advantages of this integration include convenient usage of existing\ndata processing pipelines and eliminating the need for disk writes as data can\nbe kept in memory from start to finish. Our goal is not to outperform highly\ncustomized implementations, but to propose a general high-performance topic\nmodeling framework that can easily be used in today's data processing\npipelines. We compare APS-LDA to the existing Spark LDA implementations and\nshow that our system can, on a 480-core cluster, process up to 135 times more\ndata and 10 times more topics without sacrificing model quality.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 12:40:29 GMT"}, {"version": "v2", "created": "Fri, 17 Jun 2016 08:43:56 GMT"}, {"version": "v3", "created": "Sun, 18 Jun 2017 22:37:23 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Jagerman", "Rolf", ""], ["Eickhoff", "Carsten", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1605.07888", "submitter": "Borislav Nikolic", "authors": "Borislav Nikolic, Leandro Soares Indrusiak, Stefan M. Petters", "title": "A Tighter Real-Time Communication Analysis for Wormhole-Switched\n  Priority-Preemptive NoCs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulations and runtime measurements are some of the methods which can be\nused to evaluate whether a given NoC-based platform can accommodate application\nworkload and fulfil its timing requirements. Yet, these techniques are often\ntime-consuming, and hence can evaluate only a limited set of scenarios.\nTherefore, these approaches are not suitable for safety-critical and hard\nreal-time systems, where one of the fundamental requirements is to provide\nstrong guarantees that all timing requirements will always be met, even in the\nworst-case conditions. For such systems the analytic-based real-time analysis\nis the only viable approach.\n  In this paper the focus is on the real-time communication analysis for\nwormhole-switched priority-preemptive NoCs. First, we elaborate on the existing\nanalysis and identify one source of pessimism. Then, we propose an extension to\nthe analysis, which efficiently overcomes this limitation, and allows for a\nless pessimistic analysis. Finally, through a comprehensive experimental\nevaluation, we compare the newly proposed approach against the existing one,\nand also observe how the trends change with different traffic parameters.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 13:59:55 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Nikolic", "Borislav", ""], ["Indrusiak", "Leandro Soares", ""], ["Petters", "Stefan M.", ""]]}, {"id": "1605.08023", "submitter": "Shiqiang Wang", "authors": "Shiqiang Wang, Murtaza Zafer, Kin K. Leung", "title": "Online Placement of Multi-Component Applications in Edge Computing\n  Environments", "comments": "This is the author's version of the paper accepted for publication in\n  IEEE Access", "journal-ref": null, "doi": "10.1109/ACCESS.2017.2665971", "report-no": null, "categories": "cs.DC cs.DS cs.NI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile edge computing is a new cloud computing paradigm which makes use of\nsmall-sized edge-clouds to provide real-time services to users. These mobile\nedge-clouds (MECs) are located in close proximity to users, thus enabling users\nto seamlessly access applications running on MECs. Due to the co-existence of\nthe core (centralized) cloud, users, and one or multiple layers of MECs, an\nimportant problem is to decide where (on which computational entity) to place\ndifferent components of an application. This problem, known as the application\nor workload placement problem, is notoriously hard, and therefore, heuristic\nalgorithms without performance guarantees are generally employed in common\npractice, which may unknowingly suffer from poor performance as compared to the\noptimal solution. In this paper, we address the application placement problem\nand focus on developing algorithms with provable performance bounds. We model\nthe user application as an application graph and the physical computing system\nas a physical graph, with resource demands/availabilities annotated on these\ngraphs. We first consider the placement of a linear application graph and\npropose an algorithm for finding its optimal solution. Using this result, we\nthen generalize the formulation and obtain online approximation algorithms with\npolynomial-logarithmic (poly-log) competitive ratio for tree application graph\nplacement. We jointly consider node and link assignment, and incorporate\nmultiple types of computational resources at nodes.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 19:45:06 GMT"}, {"version": "v2", "created": "Thu, 9 Feb 2017 14:32:51 GMT"}], "update_date": "2017-02-10", "authors_parsed": [["Wang", "Shiqiang", ""], ["Zafer", "Murtaza", ""], ["Leung", "Kin K.", ""]]}, {"id": "1605.08222", "submitter": "Vi Tran", "authors": "Vi Ngoc-Nha Tran and Phuong Hoai Ha", "title": "ICE: A General and Validated Energy Complexity Model for Multithreaded\n  Algorithms", "comments": "23 pages, 8 figures, 33 references", "journal-ref": null, "doi": null, "report-no": "IFI-UiT Technical Report 2016-77", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Like time complexity models that have significantly contributed to the\nanalysis and development of fast algorithms, energy complexity models for\nparallel algorithms are desired as crucial means to develop energy efficient\nalgorithms for ubiquitous multicore platforms. Ideal energy complexity models\nshould be validated on real multicore platforms and applicable to a wide range\nof parallel algorithms. However, existing energy complexity models for parallel\nalgorithms are either theoretical without model validation or\nalgorithm-specific without ability to analyze energy complexity for a\nwide-range of parallel algorithms.\n  This paper presents a new general validated energy complexity model for\nparallel (multithreaded) algorithms. The new model abstracts away possible\nmulticore platforms by their static and dynamic energy of computational\noperations and data access, and derives the energy complexity of a given\nalgorithm from its work, span and I/O complexity. The new model is validated by\ndifferent sparse matrix vector multiplication (SpMV) algorithms and dense\nmatrix multiplication (matmul) algorithms running on high performance computing\n(HPC) platforms (e.g., Intel Xeon and Xeon Phi). The new energy complexity\nmodel is able to characterize and compare the energy consumption of SpMV and\nmatmul kernels according to three aspects: different algorithms, different\ninput matrix types and different platforms. The prediction of the new model\nregarding which algorithm consumes more energy with different inputs on\ndifferent platforms, is confirmed by the experimental results. In order to\nimprove the usability and accuracy of the new model for a wide range of\nplatforms, the platform parameters of ICE model are provided for eleven\nplatforms including HPC, accelerator and embedded platforms.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 10:50:16 GMT"}, {"version": "v2", "created": "Tue, 4 Oct 2016 10:02:17 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Tran", "Vi Ngoc-Nha", ""], ["Ha", "Phuong Hoai", ""]]}, {"id": "1605.08325", "submitter": "He Ma", "authors": "He Ma, Fei Mao, and Graham W. Taylor", "title": "Theano-MPI: a Theano-based Distributed Training Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a scalable and extendable training framework that can utilize GPUs\nacross nodes in a cluster and accelerate the training of deep learning models\nbased on data parallelism. Both synchronous and asynchronous training are\nimplemented in our framework, where parameter exchange among GPUs is based on\nCUDA-aware MPI. In this report, we analyze the convergence and capability of\nthe framework to reduce training time when scaling the synchronous training of\nAlexNet and GoogLeNet from 2 GPUs to 8 GPUs. In addition, we explore novel ways\nto reduce the communication overhead caused by exchanging parameters. Finally,\nwe release the framework as open-source for further research on distributed\ndeep learning\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 15:13:46 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Ma", "He", ""], ["Mao", "Fei", ""], ["Taylor", "Graham W.", ""]]}, {"id": "1605.08695", "submitter": "Derek Murray", "authors": "Mart\\'in Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis,\n  Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael\n  Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G.\n  Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin\n  Wicke, Yuan Yu and Xiaoqiang Zheng", "title": "TensorFlow: A system for large-scale machine learning", "comments": "18 pages, 9 figures; v2 has a spelling correction in the metadata", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  TensorFlow is a machine learning system that operates at large scale and in\nheterogeneous environments. TensorFlow uses dataflow graphs to represent\ncomputation, shared state, and the operations that mutate that state. It maps\nthe nodes of a dataflow graph across many machines in a cluster, and within a\nmachine across multiple computational devices, including multicore CPUs,\ngeneral-purpose GPUs, and custom designed ASICs known as Tensor Processing\nUnits (TPUs). This architecture gives flexibility to the application developer:\nwhereas in previous \"parameter server\" designs the management of shared state\nis built into the system, TensorFlow enables developers to experiment with\nnovel optimizations and training algorithms. TensorFlow supports a variety of\napplications, with particularly strong support for training and inference on\ndeep neural networks. Several Google services use TensorFlow in production, we\nhave released it as an open-source project, and it has become widely used for\nmachine learning research. In this paper, we describe the TensorFlow dataflow\nmodel in contrast to existing systems, and demonstrate the compelling\nperformance that TensorFlow achieves for several real-world applications.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2016 15:49:50 GMT"}, {"version": "v2", "created": "Tue, 31 May 2016 19:46:10 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Abadi", "Mart\u00edn", ""], ["Barham", "Paul", ""], ["Chen", "Jianmin", ""], ["Chen", "Zhifeng", ""], ["Davis", "Andy", ""], ["Dean", "Jeffrey", ""], ["Devin", "Matthieu", ""], ["Ghemawat", "Sanjay", ""], ["Irving", "Geoffrey", ""], ["Isard", "Michael", ""], ["Kudlur", "Manjunath", ""], ["Levenberg", "Josh", ""], ["Monga", "Rajat", ""], ["Moore", "Sherry", ""], ["Murray", "Derek G.", ""], ["Steiner", "Benoit", ""], ["Tucker", "Paul", ""], ["Vasudevan", "Vijay", ""], ["Warden", "Pete", ""], ["Wicke", "Martin", ""], ["Yu", "Yuan", ""], ["Zheng", "Xiaoqiang", ""]]}, {"id": "1605.09021", "submitter": "Maninder Pal Singh", "authors": "Maninder Pal Singh, Mohammad A. Hoque, Sasu Tarkoma", "title": "A survey of systems for massive stream analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The immense growth of data demands switching from traditional data processing\nsolutions to systems, which can process a continuous stream of real time data.\nVarious applications employ stream processing systems to provide solutions to\nemerging Big Data problems. Open-source solutions such as Storm, Spark\nStreaming, and S4 are the attempts to answer key stream processing questions.\nThe recent introduction of real time stream processing commercial solutions\nsuch as Amazon Kinesis, IBM Infosphere Stream reflect industry requirements.\nThe system and application related challenges to handle massive stream of real\ntime data analytics are an active field of research.\n  In this paper, we present a comparative analysis of the existing\nstate-of-the-art stream processing solutions. We also include various\napplication domains, which are transforming their business model to benefit\nfrom these large scale stream processing systems.\n", "versions": [{"version": "v1", "created": "Sun, 29 May 2016 16:09:08 GMT"}, {"version": "v2", "created": "Sat, 4 Jun 2016 10:54:26 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Singh", "Maninder Pal", ""], ["Hoque", "Mohammad A.", ""], ["Tarkoma", "Sasu", ""]]}, {"id": "1605.09114", "submitter": "Miguel \\'A. Carreira-Perpi\\~n\\'an", "authors": "Miguel \\'A. Carreira-Perpi\\~n\\'an and Mehdi Alizadeh", "title": "ParMAC: distributed optimisation of nested functions, with application\n  to learning binary autoencoders", "comments": "40 pages, 13 figures. The abstract appearing here is slightly shorter\n  than the one in the PDF file because of the arXiv's limitation of the\n  abstract field to 1920 characters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many powerful machine learning models are based on the composition of\nmultiple processing layers, such as deep nets, which gives rise to nonconvex\nobjective functions. A general, recent approach to optimise such \"nested\"\nfunctions is the method of auxiliary coordinates (MAC). MAC introduces an\nauxiliary coordinate for each data point in order to decouple the nested model\ninto independent submodels. This decomposes the optimisation into steps that\nalternate between training single layers and updating the coordinates. It has\nthe advantage that it reuses existing single-layer algorithms, introduces\nparallelism, and does not need to use chain-rule gradients, so it works with\nnondifferentiable layers. With large-scale problems, or when distributing the\ncomputation is necessary for faster training, the dataset may not fit in a\nsingle machine. It is then essential to limit the amount of communication\nbetween machines so it does not obliterate the benefit of parallelism. We\ndescribe a general way to achieve this, ParMAC. ParMAC works on a cluster of\nprocessing machines with a circular topology and alternates two steps until\nconvergence: one step trains the submodels in parallel using stochastic\nupdates, and the other trains the coordinates in parallel. Only submodel\nparameters, no data or coordinates, are ever communicated between machines.\nParMAC exhibits high parallelism, low communication overhead, and facilitates\ndata shuffling, load balancing, fault tolerance and streaming data processing.\nWe study the convergence of ParMAC and propose a theoretical model of its\nruntime and parallel speedup. We develop ParMAC to learn binary autoencoders\nfor fast, approximate image retrieval. We implement it in MPI in a distributed\nsystem and demonstrate nearly perfect speedups in a 128-processor cluster with\na training set of 100 million high-dimensional points.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 06:31:14 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Carreira-Perpi\u00f1\u00e1n", "Miguel \u00c1.", ""], ["Alizadeh", "Mehdi", ""]]}, {"id": "1605.09219", "submitter": "Sarod Yatawatta", "authors": "Sarod Yatawatta", "title": "Fine tuning consensus optimization for distributed radio interferometric\n  calibration", "comments": "Draft, to be published in the Proceedings of the 24th European Signal\n  Processing Conference (EUSIPCO-2016) in 2016, published by EURASIP", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We recently proposed the use of consensus optimization as a viable and\neffective way to improve the quality of calibration of radio interferometric\ndata. We showed that it is possible to obtain far more accurate calibration\nsolutions and also to distribute the compute load across a network of computers\nby using this technique. A crucial aspect in any consensus optimization problem\nis the selection of the penalty parameter used in the alternating direction\nmethod of multipliers (ADMM) iterations. This affects the convergence speed as\nwell as the accuracy. In this paper, we use the Hessian of the cost function\nused in calibration to appropriately select this penalty. We extend our results\nto a multi-directional calibration setting, where we propose to use a penalty\nscaled by the squared intensity of each direction.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 13:20:46 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Yatawatta", "Sarod", ""]]}, {"id": "1605.09295", "submitter": "Hernan Asorey", "authors": "H. Asorey, R. Mayo-Garc\\'ia, L. A. N\\'u\\~nez, M. Rodr\\'iguez-Pascual,\n  A. J. Rubio Montero, M. Suarez-Dur\\'an and L. A. Torres-Ni\\~no (for the LAGO\n  Collaboration)", "title": "The Latin American Giant Observatory: a successful collaboration in\n  Latin America based on Cosmic Rays and computer science domains", "comments": "to be published in Proccedings of the 16th IEEE/ACM International\n  Symposium on Cluster, Cloud, and Grid Computing", "journal-ref": null, "doi": "10.1109/CCGrid.2016.110", "report-no": null, "categories": "astro-ph.IM cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work the strategy of the Latin American Giant Observatory (LAGO) to\nbuild a Latin American collaboration is presented. Installing Cosmic Rays\ndetectors settled all around the Continent, from Mexico to the Antarctica, this\ncollaboration is forming a community that embraces both high energy physicist\nand computer scientists. This is so because the data that are measured must be\nanalytical processed and due to the fact that \\textit{a priori} and \\textit{a\nposteriori} simulations representing the effects of the radiation must be\nperformed. To perform the calculi, customized codes have been implemented by\nthe collaboration. With regard to the huge amount of data emerging from this\nnetwork of sensors and from the computational simulations performed in a\ndiversity of computing architectures and e-infrastructures, an effort is being\ncarried out to catalog and preserve a vast amount of data produced by the\nwater-Cherenkov Detector network and the complete LAGO simulation workflow that\ncharacterize each site. Metadata, Permanent Identifiers and the facilities from\nthe LAGO Data Repository are described in this work jointly with the simulation\ncodes used. These initiatives allow researchers to produce and find data and to\ndirectly use them in a code running by means of a Science Gateway that provides\naccess to different clusters, Grid and Cloud infrastructures worldwide.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 16:05:28 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Asorey", "H.", "", "for the LAGO\n  Collaboration"], ["Mayo-Garc\u00eda", "R.", "", "for the LAGO\n  Collaboration"], ["N\u00fa\u00f1ez", "L. A.", "", "for the LAGO\n  Collaboration"], ["Rodr\u00edguez-Pascual", "M.", "", "for the LAGO\n  Collaboration"], ["Montero", "A. J. Rubio", "", "for the LAGO\n  Collaboration"], ["Suarez-Dur\u00e1n", "M.", "", "for the LAGO\n  Collaboration"], ["Torres-Ni\u00f1o", "L. A.", "", "for the LAGO\n  Collaboration"]]}, {"id": "1605.09513", "submitter": "Matteo Turilli", "authors": "Matteo Turilli, Yadu Nand Babuji, Andre Merzky, Ming Tai Ha, Michael\n  Wilde, Daniel S. Katz, Shantenu Jha", "title": "Analysis of Distributed Execution of Workloads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resource selection and task placement for distributed execution poses\nconceptual and implementation difficulties. Although resource selection and\ntask placement are at the core of many tools and workflow systems, the models\nand methods are underdeveloped. Consequently, partial and non-interoperable\nimplementations proliferate. We address both the conceptual and implementation\ndifficulties by experimentally characterizing diverse modalities of resource\nselection and task placement. We compare the architectures and capabilities of\ntwo systems: the AIMES middleware and Swift workflow scripting language and\nruntime. We integrate these systems to enable the distributed execution of\nSwift workflows on Pilot-Jobs managed by the AIMES middleware. Our experiments\ncharacterize and compare alternative execution strategies by measuring the time\nto completion of heterogeneous uncoupled workloads executed at diverse scale\nand on multiple resources. We measure the adverse effects of pilot\nfragmentation and early binding of tasks to resources and the benefits of\nbackfill scheduling across pilots on multiple resources. We then use this\ninsight to execute a multi-stage workflow across five production-grade\nresources. We discuss the importance and implications for other tools and\nworkflow systems.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 07:31:44 GMT"}, {"version": "v2", "created": "Sat, 18 Nov 2017 14:13:33 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Turilli", "Matteo", ""], ["Babuji", "Yadu Nand", ""], ["Merzky", "Andre", ""], ["Ha", "Ming Tai", ""], ["Wilde", "Michael", ""], ["Katz", "Daniel S.", ""], ["Jha", "Shantenu", ""]]}, {"id": "1605.09516", "submitter": "Akka Zemmari", "authors": "A. Casteigts, Y. M\\'etivier, J.M. Robson, A. Zemmari", "title": "Counting in One-Hop Beeping Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider networks of processes which interact with beeps. In the basic\nmodel defined by Cornejo and Kuhn, which we refer to as the $BL$ variant,\nprocesses can choose in each round either to beep or to listen. Those who beep\nare unable to detect simultaneous beeps. Those who listen can only distinguish\nbetween silence and the presence of at least one beep. Beeping models are weak\nin essence and even simple tasks may become difficult or unfeasible with them.\n  In this paper, we address the problem of computing how many participants\nthere are in a one-hop network: the {\\em counting} problem. We first observe\nthat no algorithm can compute this number with certainty in $BL$, whether the\nalgorithm be deterministic or even randomised (Las Vegas). We thus consider the\nstronger variant where beeping nodes are able to detect simultaneous beeps,\nreferred to as $B_{cd}L$ (for {\\em collision detection}). We prove that at\nleast $n$ rounds are necessary in $B_{cd}L$, and we present an algorithm whose\nrunning time is $O(n)$ rounds with high probability. Further experimental\nresults show that its expected running time is less than $10n$. Finally, we\ndiscuss how this algorithm can be adapted in other beeping models. In\nparticular, we show that it can be emulated in $BL$, at the cost of a\nlogarithmic slowdown and of trading its Las Vegas nature (result certain, time\nuncertain) against Monte Carlo (time certain, result uncertain).\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 07:40:29 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Casteigts", "A.", ""], ["M\u00e9tivier", "Y.", ""], ["Robson", "J. M.", ""], ["Zemmari", "A.", ""]]}, {"id": "1605.09530", "submitter": "Alina S\\^irbu", "authors": "Alina S\\^irbu and Ozalp Babaoglu", "title": "Predicting System-level Power for a Hybrid Supercomputer", "comments": "8 pages, 8 figures, HPCS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For current High Performance Computing systems to scale towards the holy\ngrail of ExaFLOP performance, their power consumption has to be reduced by at\nleast one order of magnitude. This goal can be achieved only through a\ncombination of hardware and software advances. Being able to model and\naccurately predict the power consumption of large computational systems is\nnecessary for software-level innovations such as proactive and power-aware\nscheduling, resource allocation and fault tolerance techniques. In this paper\nwe present a 2-layer model of power consumption for a hybrid supercomputer\n(which held the top spot of the Green500 list on July 2013) that combines CPU,\nGPU and MIC technologies to achieve higher energy efficiency. Our model takes\nas input workload information - the number and location of resources that are\nused by each job at a certain time - and calculates the resulting system-level\npower consumption. When jobs are submitted to the system, the workload\nconfiguration can be foreseen based on the scheduler policies, and our model\ncan then be applied to predict the ensuing system-level power consumption.\nAdditionally, alternative workload configurations can be evaluated from a power\nperspective and more efficient ones can be selected. Applications of the model\ninclude not only power-aware scheduling but also prediction of anomalous\nbehavior.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 08:55:24 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["S\u00eerbu", "Alina", ""], ["Babaoglu", "Ozalp", ""]]}, {"id": "1605.09619", "submitter": "Mario Lucic", "authors": "Mario Lucic and Olivier Bachem and Morteza Zadimoghaddam and Andreas\n  Krause", "title": "Horizontally Scalable Submodular Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.DM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A variety of large-scale machine learning problems can be cast as instances\nof constrained submodular maximization. Existing approaches for distributed\nsubmodular maximization have a critical drawback: The capacity - number of\ninstances that can fit in memory - must grow with the data set size. In\npractice, while one can provision many machines, the capacity of each machine\nis limited by physical constraints. We propose a truly scalable approach for\ndistributed submodular maximization under fixed capacity. The proposed\nframework applies to a broad class of algorithms and constraints and provides\ntheoretical guarantees on the approximation factor for any available capacity.\nWe empirically evaluate the proposed algorithm on a variety of data sets and\ndemonstrate that it achieves performance competitive with the centralized\ngreedy solution.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 13:18:30 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Lucic", "Mario", ""], ["Bachem", "Olivier", ""], ["Zadimoghaddam", "Morteza", ""], ["Krause", "Andreas", ""]]}, {"id": "1605.09721", "submitter": "Dimitris Papailiopoulos", "authors": "Xinghao Pan, Maximilian Lam, Stephen Tu, Dimitris Papailiopoulos, Ce\n  Zhang, Michael I. Jordan, Kannan Ramchandran, Chris Re, Benjamin Recht", "title": "CYCLADES: Conflict-free Asynchronous Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.DS cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present CYCLADES, a general framework for parallelizing stochastic\noptimization algorithms in a shared memory setting. CYCLADES is asynchronous\nduring shared model updates, and requires no memory locking mechanisms, similar\nto HOGWILD!-type algorithms. Unlike HOGWILD!, CYCLADES introduces no conflicts\nduring the parallel execution, and offers a black-box analysis for provable\nspeedups across a large family of algorithms. Due to its inherent conflict-free\nnature and cache locality, our multi-core implementation of CYCLADES\nconsistently outperforms HOGWILD!-type algorithms on sufficiently sparse\ndatasets, leading to up to 40% speedup gains compared to the HOGWILD!\nimplementation of SGD, and up to 5x gains over asynchronous implementations of\nvariance reduction algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 17:15:01 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Pan", "Xinghao", ""], ["Lam", "Maximilian", ""], ["Tu", "Stephen", ""], ["Papailiopoulos", "Dimitris", ""], ["Zhang", "Ce", ""], ["Jordan", "Michael I.", ""], ["Ramchandran", "Kannan", ""], ["Re", "Chris", ""], ["Recht", "Benjamin", ""]]}, {"id": "1605.09774", "submitter": "Ioannis Mitliagkas", "authors": "Ioannis Mitliagkas, Ce Zhang, Stefan Hadjis, Christopher R\\'e", "title": "Asynchrony begets Momentum, with an Application to Deep Learning", "comments": "Full version of a paper published in Annual Allerton Conference on\n  Communication, Control, and Computing (Allerton) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asynchronous methods are widely used in deep learning, but have limited\ntheoretical justification when applied to non-convex problems. We show that\nrunning stochastic gradient descent (SGD) in an asynchronous manner can be\nviewed as adding a momentum-like term to the SGD iteration. Our result does not\nassume convexity of the objective function, so it is applicable to deep\nlearning systems. We observe that a standard queuing model of asynchrony\nresults in a form of momentum that is commonly used by deep learning\npractitioners. This forges a link between queuing theory and asynchrony in deep\nlearning systems, which could be useful for systems builders. For convolutional\nneural networks, we experimentally validate that the degree of asynchrony\ndirectly correlates with the momentum, confirming our main result. An important\nimplication is that tuning the momentum parameter is important when considering\ndifferent levels of asynchrony. We assert that properly tuned momentum reduces\nthe number of steps required for convergence. Finally, our theory suggests new\nways of counteracting the adverse effects of asynchrony: a simple mechanism\nlike using negative algorithmic momentum can improve performance under high\nasynchrony. Since asynchronous methods have better hardware efficiency, this\nresult may shed light on when asynchronous execution is more efficient for deep\nlearning systems.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 19:16:56 GMT"}, {"version": "v2", "created": "Fri, 25 Nov 2016 12:00:28 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Mitliagkas", "Ioannis", ""], ["Zhang", "Ce", ""], ["Hadjis", "Stefan", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1605.09785", "submitter": "Junyao Guo", "authors": "Junyao Guo, Gabriela Hug, and Ozan Tonguz", "title": "Enabling Distributed Optimization in Large-Scale Power Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed optimization for solving non-convex Optimal Power Flow (OPF)\nproblems in power systems has attracted tremendous attention in the last\ndecade. Most studies are based on the geographical decomposition of IEEE test\nsystems for verifying the feasibility of the proposed approaches. However, it\nis not clear if one can extrapolate from these studies that those approaches\ncan be applied to very large-scale real-world systems. In this paper, we show,\nfor the first time, that distributed optimization can be effectively applied to\na large-scale real transmission network, namely, the Polish 2383-bus system for\nwhich no pre-defined partitions exist, by using a recently developed\npartitioning technique. More specifically, the problem solved is the AC OPF\nproblem with geographical decomposition of the network using the Alternating\nDirection Method of Multipliers (ADMM) method in conjunction with the\npartitioning technique. Through extensive experimental results and analytical\nstudies, we show that with the presented partitioning technique the convergence\nperformance of ADMM can be improved substantially, which enables the\napplication of distributed approaches on very large-scale systems.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 19:41:23 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Guo", "Junyao", ""], ["Hug", "Gabriela", ""], ["Tonguz", "Ozan", ""]]}]