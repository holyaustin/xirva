[{"id": "2008.00018", "submitter": "Homayoun Valafar", "authors": "Michael Bryson, Xijiang Miao, Homayoun Valafar", "title": "Process of Efficiently Parallelizing a Protein Structure Determination\n  Algorithm", "comments": "7 pages published in PDPA2006", "journal-ref": "PDPTA 2006: 320-326", "doi": null, "report-no": null, "categories": "cs.DC cs.CE cs.NA math.NA q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational protein structure determination involves optimization in a\nproblem space much too large to exhaustively search. Existing approaches\ninclude optimization algorithms such as gradient descent and simulated\nannealing, but these typically only find local minima. One novel approach\nimplemented in REDcRAFT is to instead of folding a protein all at the same\ntime, fold it residue by residue. This simulates a protein folding as each\nresidue exits from the generating ribosome. While REDcRAFT exponentially\nreduces the problem space so it can be explored in polynomial time, it is still\nextremely computationally demanding. This algorithm does have the advantage\nthat most of the execution time is spent in inherently parallelizable code.\nHowever, preliminary results from parallel execution indicate that\napproximately two-thirds of execution time is dedicated to system overhead.\nAdditionally, by carefully analyzing and timing the structure of the program\nthe major bottlenecks can be identified. After addressing these issues,\nREDcRAFT becomes a scalable parallel application with nearly two orders of\nmagnitude improvement.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 18:04:39 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Bryson", "Michael", ""], ["Miao", "Xijiang", ""], ["Valafar", "Homayoun", ""]]}, {"id": "2008.00087", "submitter": "Muhammad Kamran Nishat", "authors": "Kamran Nishat, Omprakash Gnawali, and Ahmed Abdelhadi", "title": "Adaptive Bitrate Video Streaming for Wireless nodes: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In today's Internet, video is the most dominant application and in addition\nto this, wireless networks such as WiFi, Cellular, and Bluetooth have become\nubiquitous. Hence, most of the Internet traffic is video over wireless nodes.\nThere is a plethora of research to improve video streaming to achieve high\nQuality of Experience (QoE) over the Internet. Many of them focus on wireless\nnodes. Recent measurement studies often show QoE of video suffers in many\nwireless clients over the Internet. Recently, many research papers have\npresented models and schemes to optimize the Adaptive BitRate (ABR) based video\nstreaming for wireless and mobile users. In this survey, we present a\ncomprehensive overview of recent work in the area of Internet video specially\ndesigned for wireless network. Recent research has suggested that there are\nsome new challenges added by the connectivity of clients through wireless. Also\nthese challenges become more difficult to handle when these nodes are mobile.\nThis survey also discusses new potential areas of future research due to the\nincreasing scarcity of wireless spectrum.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 06:37:02 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Nishat", "Kamran", ""], ["Gnawali", "Omprakash", ""], ["Abdelhadi", "Ahmed", ""]]}, {"id": "2008.00088", "submitter": "Safa Otoum", "authors": "Safa Otoum and Burak Kantarci and Hussein Mouftah", "title": "A Comparative Study of AI-based Intrusion Detection Techniques in\n  Critical Infrastructures", "comments": "ACM Transaction on Internet Technology, 2020 22 pages, 11 Figures, 3\n  Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Volunteer computing uses Internet-connected devices (laptops, PCs, smart\ndevices, etc.), in which their owners volunteer them as storage and computing\npower resources, has become an essential mechanism for resource management in\nnumerous applications. The growth of the volume and variety of data traffic in\nthe Internet leads to concerns on the robustness of cyberphysical systems\nespecially for critical infrastructures. Therefore, the implementation of an\nefficient Intrusion Detection System for gathering such sensory data has gained\nvital importance. In this paper, we present a comparative study of Artificial\nIntelligence (AI)-driven intrusion detection systems for wirelessly connected\nsensors that track crucial applications. Specifically, we present an in-depth\nanalysis of the use of machine learning, deep learning and reinforcement\nlearning solutions to recognize intrusive behavior in the collected traffic. We\nevaluate the proposed mechanisms by using KD'99 as real attack data-set in our\nsimulations. Results present the performance metrics for three different IDSs\nnamely the Adaptively Supervised and Clustered Hybrid IDS (ASCH-IDS),\nRestricted Boltzmann Machine-based Clustered IDS (RBC-IDS) and Q-learning based\nIDS (QL-IDS) to detect malicious behaviors. We also present the performance of\ndifferent reinforcement learning techniques such as\nState-Action-Reward-State-Action Learning (SARSA) and the Temporal Difference\nlearning (TD). Through simulations, we show that QL-IDS performs with 100%\ndetection rate while SARSA-IDS and TD-IDS perform at the order of 99.5%.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 20:55:57 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Otoum", "Safa", ""], ["Kantarci", "Burak", ""], ["Mouftah", "Hussein", ""]]}, {"id": "2008.00216", "submitter": "Aneeqa Fatima", "authors": "Aneeqa Fatima and Igor L. Markov", "title": "Faster Schr\\\"odinger-style simulation of quantum circuits", "comments": "14 pages, 15 figures, 4 tables. Version 2 : Additional optimizations;\n  improved simulation runtimes; profiling data; comparisons with the latest IBM\n  QISKit simulator; dispelled apparent limitations of techniques. Version 3 :\n  Ablation experiments and images for the code snippets", "journal-ref": "HPCA 2021", "doi": null, "report-no": null, "categories": "quant-ph cs.AR cs.DC cs.ET physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent demonstrations of superconducting quantum computers by Google and IBM\nand trapped-ion computers from IonQ fueled new research in quantum algorithms,\ncompilation into quantum circuits, and empirical algorithmics. While online\naccess to quantum hardware remains too limited to meet the demand, simulating\nquantum circuits on conventional computers satisfies many needs. We advance\nSchr\\\"odinger-style simulation of quantum circuits that is useful standalone\nand as a building block in layered simulation algorithms, both cases are\nillustrated in our results. Our algorithmic contributions show how to simulate\nmultiple quantum gates at once, how to avoid floating-point multiplies, how to\nbest use instruction-level and thread-level parallelism as well as CPU cache,\nand how to leverage these optimizations by reordering circuit gates. While not\ndescribed previously, these techniques implemented by us supported published\nhigh-performance distributed simulations up to 64 qubits. To show additional\nimpact, we benchmark our simulator against Microsoft, IBM and Google simulators\non hard circuits from Google.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 08:47:24 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 06:49:12 GMT"}, {"version": "v3", "created": "Tue, 24 Nov 2020 18:20:50 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Fatima", "Aneeqa", ""], ["Markov", "Igor L.", ""]]}, {"id": "2008.00307", "submitter": "Jeremy Kepner", "authors": "Jeremy Kepner, Chad Meiners, Chansup Byun, Sarah McGuire, Timothy\n  Davis, William Arcand, Jonathan Bernays, David Bestor, William Bergeron,\n  Vijay Gadepally, Raul Harnasch, Matthew Hubbell, Micheal Houle, Micheal\n  Jones, Andrew Kirby, Anna Klein, Lauren Milechin, Julie Mullen, Andrew Prout,\n  Albert Reuther, Antonio Rosa, Siddharth Samsi, Doug Stetson, Adam Tse,\n  Charles Yee, Peter Michaleas", "title": "Multi-Temporal Analysis and Scaling Relations of 100,000,000,000 Network\n  Packets", "comments": "6 pages, 6 figures,3 tables, 49 references, accepted to IEEE HPEC\n  2020", "journal-ref": null, "doi": "10.1109/HPEC43674.2020.9286235", "report-no": null, "categories": "cs.NI cs.DC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our society has never been more dependent on computer networks. Effective\nutilization of networks requires a detailed understanding of the normal\nbackground behaviors of network traffic. Large-scale measurements of networks\nare computationally challenging. Building on prior work in interactive\nsupercomputing and GraphBLAS hypersparse hierarchical traffic matrices, we have\ndeveloped an efficient method for computing a wide variety of streaming network\nquantities on diverse time scales. Applying these methods to 100,000,000,000\nanonymized source-destination pairs collected at a network gateway reveals many\npreviously unobserved scaling relationships. These observations provide new\ninsights into normal network background traffic that could be used for anomaly\ndetection, AI feature engineering, and testing theoretical models of streaming\nnetworks.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 17:56:56 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Kepner", "Jeremy", ""], ["Meiners", "Chad", ""], ["Byun", "Chansup", ""], ["McGuire", "Sarah", ""], ["Davis", "Timothy", ""], ["Arcand", "William", ""], ["Bernays", "Jonathan", ""], ["Bestor", "David", ""], ["Bergeron", "William", ""], ["Gadepally", "Vijay", ""], ["Harnasch", "Raul", ""], ["Hubbell", "Matthew", ""], ["Houle", "Micheal", ""], ["Jones", "Micheal", ""], ["Kirby", "Andrew", ""], ["Klein", "Anna", ""], ["Milechin", "Lauren", ""], ["Mullen", "Julie", ""], ["Prout", "Andrew", ""], ["Reuther", "Albert", ""], ["Rosa", "Antonio", ""], ["Samsi", "Siddharth", ""], ["Stetson", "Doug", ""], ["Tse", "Adam", ""], ["Yee", "Charles", ""], ["Michaleas", "Peter", ""]]}, {"id": "2008.00332", "submitter": "Elaine Shi", "authors": "Vijaya Ramachandran and Elaine Shi", "title": "Data Oblivious Algorithms for Multicores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As secure processors such as Intel SGX (with hyperthreading) become widely\nadopted, there is a growing appetite for private analytics on big data. Most\nprior works on data-oblivious algorithms adopt the classical PRAM model to\ncapture parallelism. However, it is widely understood that PRAM does not best\ncapture realistic multicore processors, nor does it reflect parallel\nprogramming models adopted in practice.\n  In this paper, we initiate the study of parallel data oblivious algorithms on\nrealistic multicores, best captured by the binary fork-join model of\ncomputation. We first show that data-oblivious sorting can be accomplished by a\nbinary fork-join algorithm with optimal total work and optimal\n(cache-oblivious) cache complexity, and in O(log n log log n) span (i.e.,\nparallel time) that matches the best-known insecure algorithm. Using our\nsorting algorithm as a core primitive, we show how to data-obliviously simulate\ngeneral PRAM algorithms in the binary fork-join model with non-trivial\nefficiency. We also present results for several applications including list\nranking, Euler tour, tree contraction, connected components, and minimum\nspanning forest. For a subset of these applications, our data-oblivious\nalgorithms asymptotically outperform the best known insecure algorithms. For\nother applications, we show data oblivious algorithms whose performance bounds\nmatch the best known insecure algorithms.\n  Complementing these asymptotically efficient results, we present a practical\nvariant of our sorting algorithm that is self-contained and potentially\nimplementable. It has optimal caching cost, and it is only a log log n factor\noff from optimal work and about a log n factor off in terms of span; moreover,\nit achieves small constant factors in its bounds.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 20:14:10 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 23:25:52 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Ramachandran", "Vijaya", ""], ["Shi", "Elaine", ""]]}, {"id": "2008.00553", "submitter": "Henrik Bengtsson", "authors": "Henrik Bengtsson", "title": "A Unifying Framework for Parallel and Distributed Processing in R using\n  Futures", "comments": "19 pages, 1 figure, accepted The R Journal, 2021", "journal-ref": null, "doi": "10.32614/RJ-2021-048", "report-no": null, "categories": "cs.DC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A future is a programming construct designed for concurrent and asynchronous\nevaluation of code, making it particularly useful for parallel processing. The\nfuture package implements the Future API for programming with futures in R.\nThis minimal API provides sufficient constructs for implementing parallel\nversions of well-established, high-level map-reduce APIs. The future ecosystem\nsupports exception handling, output and condition relaying, parallel random\nnumber generation, and automatic identification of globals lowering the\nthreshold to parallelize code. The Future API bridges parallel frontends with\nparallel backends following the philosophy that end-users are the ones who\nchoose the parallel backend while the developer focuses on what to parallelize.\nA variety of backends exist and third-party contributions meeting the\nspecifications, which ensure that the same code works on all backends, are\nautomatically supported. The future framework solves several problems not\naddressed by other parallel frameworks in R.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 19:53:52 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2020 02:16:02 GMT"}, {"version": "v3", "created": "Sat, 12 Jun 2021 17:35:03 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Bengtsson", "Henrik", ""]]}, {"id": "2008.00581", "submitter": "Mingyue Ji", "authors": "Nicholas Woolsey, Rong-Rong Chen, Mingyue Ji", "title": "A Combinatorial Design for Cascaded Coded Distributed Computing on\n  General Networks", "comments": "30 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CC cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coding theoretic approached have been developed to significantly reduce the\ncommunication load in modern distributed computing system. In particular, coded\ndistributed computing (CDC) introduced by Li et al. can efficiently trade\ncomputation resources to reduce the communication load in MapReduce like\ncomputing systems. For the more general cascaded CDC, Map computations are\nrepeated at r nodes to significantly reduce the communication load among nodes\ntasked with computing Q Reduce functions s times. In this paper, we propose a\nnovel low-complexity combinatorial design for cascaded CDC which 1) determines\nboth input file and output function assignments, 2) requires significantly less\nnumber of input files and output functions, and 3) operates on heterogeneous\nnetworks where nodes have varying storage and computing capabilities. We\nprovide an analytical characterization of the computation-communication\ntradeoff, from which we show the proposed scheme can outperform the\nstate-of-the-art scheme proposed by Li et al. for the homogeneous networks.\nFurther, when the network is heterogeneous, we show that the performance of the\nproposed scheme can be better than its homogeneous counterpart. In addition,\nthe proposed scheme is optimal within a constant factor of the information\ntheoretic converse bound while fixing the input file and the output function\nassignments.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 23:00:11 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Woolsey", "Nicholas", ""], ["Chen", "Rong-Rong", ""], ["Ji", "Mingyue", ""]]}, {"id": "2008.00638", "submitter": "Dibakar Gope", "authors": "Dibakar Gope, Jesse Beu, Matthew Mattina", "title": "High Throughput Matrix-Matrix Multiplication between Asymmetric\n  Bit-Width Operands", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.CV cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix multiplications between asymmetric bit-width operands, especially\nbetween 8- and 4-bit operands are likely to become a fundamental kernel of many\nimportant workloads including neural networks and machine learning. While\nexisting SIMD matrix multiplication instructions for symmetric bit-width\noperands can support operands of mixed precision by zero- or sign-extending the\nnarrow operand to match the size of the other operands, they cannot exploit the\nbenefit of narrow bit-width of one of the operands. We propose a new SIMD\nmatrix multiplication instruction that uses mixed precision on its inputs (8-\nand 4-bit operands) and accumulates product values into narrower 16-bit output\naccumulators, in turn allowing the SIMD operation at 128-bit vector width to\nprocess a greater number of data elements per instruction to improve processing\nthroughput and memory bandwidth utilization without increasing the register\nread- and write-port bandwidth in CPUs. The proposed asymmetric-operand-size\nSIMD instruction offers 2x improvement in throughput of matrix multiplication\nin comparison to throughput obtained using existing symmetric-operand-size\ninstructions while causing negligible (0.05%) overflow from 16-bit accumulators\nfor representative machine learning workloads. The asymmetric-operand-size\ninstruction not only can improve matrix multiplication throughput in CPUs, but\nalso can be effective to support multiply-and-accumulate (MAC) operation\nbetween 8- and 4-bit operands in state-of-the-art DNN hardware accelerators\n(e.g., systolic array microarchitecture in Google TPU, etc.) and offer similar\nimprovement in matrix multiply performance seamlessly without violating the\nvarious implementation constraints. We demonstrate how a systolic array\narchitecture designed for symmetric-operand-size instructions could be modified\nto support an asymmetric-operand-sized instruction.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 04:12:31 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Gope", "Dibakar", ""], ["Beu", "Jesse", ""], ["Mattina", "Matthew", ""]]}, {"id": "2008.00701", "submitter": "Kaustav Bose", "authors": "Archak Das, Kaustav Bose, Buddhadeb Sau", "title": "Memory Optimal Dispersion by Anonymous Mobile Robots", "comments": "This is the full version of the paper, with the same title and\n  authors, that was accepted in the 7th Annual International Conference on\n  Algorithms and Discrete Applied Mathematics (CALDAM 2021), February 11-13,\n  2021, Ropar, India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a team of $k \\leq n$ autonomous mobile robots initially placed at a\nnode of an arbitrary graph $G$ with $n$ nodes. The dispersion problem asks for\na distributed algorithm that allows the robots to reach a configuration in\nwhich each robot is at a distinct node of the graph. If the robots are\nanonymous, i.e., they do not have any unique identifiers, then the problem is\nnot solvable by any deterministic algorithm. However, the problem can be solved\neven by anonymous robots if each robot is given access to a fair coin which\nthey can use to generate random bits. In this setting, it is known that the\nrobots require $\\Omega(\\log{\\Delta})$ bits of memory to achieve dispersion,\nwhere $\\Delta$ is the maximum degree of $G$. On the other hand, the best known\nmemory upper bound is $min \\{\\Delta, max\\{\\log{\\Delta}, \\log{D}\\}\\}$ ($D$ =\ndiameter of $G$), which can be $\\omega(\\log{\\Delta})$, depending on the values\nof $\\Delta$ and $D$. In this paper, we close this gap by presenting an optimal\nalgorithm requiring $O(\\log{\\Delta})$ bits of memory.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 08:12:20 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2020 17:42:09 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Das", "Archak", ""], ["Bose", "Kaustav", ""], ["Sau", "Buddhadeb", ""]]}, {"id": "2008.00739", "submitter": "Kaustav Bose", "authors": "Kaustav Bose, Manash Kumar Kundu, Ranendu Adhikary, Buddhadeb Sau", "title": "Distributed Localization of Wireless Sensor Network Using Communication\n  Wheel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the network localization problem, i.e., the problem of determining\nnode positions of a wireless sensor network modeled as a unit disk graph. In an\narbitrarily deployed network, positions of all nodes of the network may not be\nuniquely determined. It is known that even if the network corresponds to a\nunique solution, no polynomial-time algorithm can solve this problem in the\nworst case, unless RP = NP. So we are interested in algorithms that efficiently\nlocalize the network partially. A widely used technique that can efficiently\nlocalize a uniquely localizable portion of the network is trilateration:\nstarting from three anchors (nodes with known positions), nodes having at least\nthree localized neighbors are sequentially localized. However, the performance\nof trilateration can substantially differ for different choices of the initial\nthree anchors. In this paper, we propose a distributed localization scheme with\na theoretical characterization of nodes that are guaranteed to be localized. In\nparticular, our proposed distributed algorithm starts localization from a\nstrongly interior node and provided that the subgraph induced by the strongly\ninterior nodes is connected, it localizes all nodes of the network except some\nboundary nodes and isolated weakly interior nodes.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 09:37:48 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Bose", "Kaustav", ""], ["Kundu", "Manash Kumar", ""], ["Adhikary", "Ranendu", ""], ["Sau", "Buddhadeb", ""]]}, {"id": "2008.00742", "submitter": "L\\^e-Nguy\\^en Hoang", "authors": "El-Mahdi El-Mhamdi, Sadegh Farhadkhani, Rachid Guerraoui, Arsany\n  Guirguis, L\\^e Nguy\\^en Hoang, S\\'ebastien Rouault", "title": "Collaborative Learning in the Jungle", "comments": "34 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study Byzantine collaborative learning, where $n$ nodes seek to\ncollectively learn from each others' local data. The data distribution may vary\nfrom one node to another. No node is trusted, and $f < n$ nodes can behave\narbitrarily. We prove that collaborative learning is equivalent to a new form\nof agreement, which we call averaging agreement. In this problem, nodes start\neach with an initial vector and seek to approximately agree on a common vector,\nwhich is close to the average of honest nodes' initial vectors. We present two\nasynchronous solutions to averaging agreement, each we prove optimal according\nto some dimension. The first, based on the minimum-diameter averaging, requires\n$ n \\geq 6f+1$, but achieves asymptotically the best-possible averaging\nconstant up to a multiplicative constant. The second, based on reliable\nbroadcast and coordinate-wise trimmed mean, achieves optimal Byzantine\nresilience, i.e., $n \\geq 3f+1$. Each of these algorithms induces an optimal\nByzantine collaborative learning protocol. In particular, our equivalence\nyields new impossibility theorems on what any collaborative learning algorithm\ncan achieve in adversarial and heterogeneous environments.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 09:44:07 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 09:56:27 GMT"}, {"version": "v3", "created": "Mon, 14 Dec 2020 16:30:24 GMT"}, {"version": "v4", "created": "Mon, 7 Jun 2021 15:05:52 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["El-Mhamdi", "El-Mahdi", ""], ["Farhadkhani", "Sadegh", ""], ["Guerraoui", "Rachid", ""], ["Guirguis", "Arsany", ""], ["Hoang", "L\u00ea Nguy\u00ean", ""], ["Rouault", "S\u00e9bastien", ""]]}, {"id": "2008.00787", "submitter": "Abdallah Lakhdari", "authors": "Abdallah Lakhdari, Athman Bouguettaya", "title": "Fluid Composition of Intermittent IoT Energy Services", "comments": "9 pages, Accepted and to appear in 2020 IEEE International Conference\n  on Services Computing (SCC). Content may change prior to final publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel fluid composition approach of wireless energy services in\na crowdsourced IoT environment. The proposed approach selects an optimal set of\ndynamic energy services according to the consumer's requirements. We leverage\nthe mobility patterns of the crowd in confined areas to capture the\nintermittent behavior of IoT energy services. We model the IoT energy services\nbased on their mobility patterns to propose a knapsack-based heuristic for the\nfluid composition. Experimental results demonstrate the efficiency of the\nproposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 11:34:25 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Lakhdari", "Abdallah", ""], ["Bouguettaya", "Athman", ""]]}, {"id": "2008.00793", "submitter": "Guy Goren", "authors": "Guy Goren, Shay Vargaftik, Yoram Moses", "title": "Distributed Dispatching in the Parallel Server Model", "comments": "25 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid increase in the size and volume of cloud services and data\ncenters, architectures with multiple job dispatchers are quickly becoming the\nnorm. Load balancing is a key element of such systems. Nevertheless, current\nsolutions to load balancing in such systems admit a paradoxical behavior in\nwhich more accurate information regarding server queue lengths degrades\nperformance due to herding and detrimental incast effects. Indeed, both in\ntheory and in practice, there is a common doubt regarding the value of\ninformation in the context of multi-dispatcher load balancing. As a result,\nboth researchers and system designers resort to more straightforward solutions,\nsuch as the power-of-two-choices to avoid worst-case scenarios, potentially\nsacrificing overall resource utilization and system performance. A principal\nfocus of our investigation concerns the value of information about queue\nlengths in the multi-dispatcher setting. We argue that, at its core, load\nbalancing with multiple dispatchers is a distributed computing task. In that\nlight, we propose a new job dispatching approach, called Tidal Water Filling,\nwhich addresses the distributed nature of the system. Specifically, by\nincorporating the existence of other dispatchers into the decision-making\nprocess, our protocols outperform previous solutions in many scenarios. In\nparticular, when the dispatchers have complete and accurate information\nregarding the server queues, our policies significantly outperform all existing\nsolutions.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 11:53:01 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Goren", "Guy", ""], ["Vargaftik", "Shay", ""], ["Moses", "Yoram", ""]]}, {"id": "2008.00824", "submitter": "Ahnaf Lodhi", "authors": "Ahnaf Hannan Lodhi, Bar{\\i}\\c{s} Akg\\\"un, \\\"Oznur \\\"Ozkasap", "title": "State-of-the-art Techniques in Deep Edge Intelligence", "comments": "13 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC cs.LG eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The potential held by the gargantuan volumes of data being generated across\nnetworks worldwide has been truly unlocked by machine learning techniques and\nmore recently Deep Learning. The advantages offered by the latter have seen it\nrapidly becoming a framework of choice for various applications. However, the\ncentralization of computational resources and the need for data aggregation\nhave long been limiting factors in the democratization of Deep Learning\napplications. Edge Computing is an emerging paradigm that aims to utilize the\nhitherto untapped processing resources available at the network periphery. Edge\nIntelligence (EI) has quickly emerged as a powerful alternative to enable\nlearning using the concepts of Edge Computing. Deep Learning-based Edge\nIntelligence or Deep Edge Intelligence (DEI) lies in this rapidly evolving\ndomain. In this article, we provide an overview of the major constraints in\noperationalizing DEI. The major research avenues in DEI have been consolidated\nunder Federated Learning, Distributed Computation, Compression Schemes and\nConditional Computation. We also present some of the prevalent challenges and\nhighlight prospective research avenues.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 12:17:23 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 17:07:03 GMT"}, {"version": "v3", "created": "Thu, 24 Dec 2020 07:42:01 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Lodhi", "Ahnaf Hannan", ""], ["Akg\u00fcn", "Bar\u0131\u015f", ""], ["\u00d6zkasap", "\u00d6znur", ""]]}, {"id": "2008.00832", "submitter": "Pavanakumar Mohanamuraly", "authors": "Pavanakumar Mohanamuraly and Gabriel Staffelbach", "title": "Hardware locality-aware partitioning and dynamic load-balancing of\n  unstructured meshes for large-scale scientific applications", "comments": null, "journal-ref": null, "doi": "10.1145/3394277.3401851", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an open-source topology-aware hierarchical unstructured mesh\npartitioning and load-balancing tool TreePart. The framework provides powerful\nabstractions to automatically detect and build hierarchical MPI topology\nresembling the hardware at runtime. Using this information it intelligently\nchooses between shared and distributed parallel algorithms for partitioning and\nload-balancing. It provides a range of partitioning methods by interfacing with\nexisting shared and distributed memory parallel partitioning libraries. It\nprovides powerful and scalable abstractions like one-sided distributed\ndictionaries and MPI3 shared memory based halo communicators for optimising HPC\ncodes. The tool was successfully integrated into our in-house code and we\npresent results from a large-eddy simulation of a combustion problem.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 12:27:08 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Mohanamuraly", "Pavanakumar", ""], ["Staffelbach", "Gabriel", ""]]}, {"id": "2008.00842", "submitter": "Paris Carbone", "authors": "Marios Fragkoulis, Paris Carbone, Vasiliki Kalavri, Asterios\n  Katsifodimos", "title": "A Survey on the Evolution of Stream Processing Systems", "comments": "34 pages, 15 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CL cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stream processing has been an active research field for more than 20 years,\nbut it is now witnessing its prime time due to recent successful efforts by the\nresearch community and numerous worldwide open-source communities. This survey\nprovides a comprehensive overview of fundamental aspects of stream processing\nsystems and their evolution in the functional areas of out-of-order data\nmanagement, state management, fault tolerance, high availability, load\nmanagement, elasticity, and reconfiguration. We review noteworthy past research\nfindings, outline the similarities and differences between early ('00-'10) and\nmodern ('11-'18) streaming systems, and discuss recent trends and open\nproblems.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 12:43:46 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Fragkoulis", "Marios", ""], ["Carbone", "Paris", ""], ["Kalavri", "Vasiliki", ""], ["Katsifodimos", "Asterios", ""]]}, {"id": "2008.00861", "submitter": "Andrew Weinert", "authors": "Andrew Weinert, Ngaire Underhill, Bilal Gill, Ashley Wicks", "title": "Processing of Crowdsourced Observations of Aircraft in a High\n  Performance Computing Environment", "comments": "6 pages, 4 figures, 4 tables", "journal-ref": null, "doi": "10.1109/HPEC43674.2020.9286229", "report-no": null, "categories": "cs.DC cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As unmanned aircraft systems (UASs) continue to integrate into the U.S.\nNational Airspace System (NAS), there is a need to quantify the risk of\nairborne collisions between unmanned and manned aircraft to support regulation\nand standards development. Both regulators and standards developing\norganizations have made extensive use of Monte Carlo collision risk analysis\nsimulations using probabilistic models of aircraft flight. We've previously\ndetermined that the observations of manned aircraft by the OpenSky Network, a\ncommunity network of ground-based sensors, are appropriate to develop models of\nthe low altitude environment. This works overviews the high performance\ncomputing workflow designed and deployed on the Lincoln Laboratory\nSupercomputing Center to process 3.9 billion observations of aircraft. We then\ntrained the aircraft models using more than 250,000 flight hours at 5,000 feet\nabove ground level or below. A key feature of the workflow is that all the\naircraft observations and supporting datasets are available as open source\ntechnologies or been released to the public domain.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 13:29:20 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Weinert", "Andrew", ""], ["Underhill", "Ngaire", ""], ["Gill", "Bilal", ""], ["Wicks", "Ashley", ""]]}, {"id": "2008.00879", "submitter": "Ivo Kubjas", "authors": "Ivo Kubjas and Vitaly Skachek", "title": "Failure Probability Analysis for Partial Extraction from Invertible\n  Bloom Filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Invertible Bloom Filter (IBF) is a data structure, which employs a small set\nof hash functions. An IBF allows for an efficient insertion and, with high\nprobability, for an efficient extraction of the data. However, the success\nprobability of the extraction depends on the storage overhead of an IBF and the\namount of the data stored. In an application, such as set reconciliation, where\nthere is a need to extract data stored in the IBF, the extraction might succeed\nonly partially, by recovering only part of the stored data. In this work, the\nprobability of success for a partial extraction of data from an IBF is\nanalyzed. It is shown that partial extraction could be useful in applications,\nsuch as set reconciliation. In particular, it allows for set reconciliation by\nusing the IBF, where the storage overhead is too small to allow full\nextraction. An upper bound on the number of rounds in an iterative set\nreconciliation protocol is presented. The numerical results are derived\nanalytically, and confirmed by the computer simulations.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 13:58:17 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Kubjas", "Ivo", ""], ["Skachek", "Vitaly", ""]]}, {"id": "2008.00902", "submitter": "Juhyun Bae", "authors": "Juhyun Bae, Gong Su, Arun Iyengar, Yanzhao Wu, Ling Liu", "title": "Efficient Orchestration of Host and Remote Shared Memory for Memory\n  Intensive Workloads", "comments": "13 pages, 23 figures, 8 tables, MemSys '20: The International\n  Symposium on Memory Systems, Sept 2020, Washington, DC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since very few contributions to the development of an unified memory\norchestration framework for efficient management of both host and remote idle\nmemory have been made, we present Valet, an efficient approach to orchestration\nof host and remote shared memory for improving performance of memory intensive\nworkloads. The paper makes three original contributions. First, we redesign the\ndata flow in the critical path by introducing a host-coordinated memory pool\nthat works as a local cache to reduce the latency in the critical path of the\nhost and remote memory orchestration. Second, Valet utilizes unused local\nmemory across containers by managing local memory via Valet host-coordinated\nmemory pool, which allows containers to dynamically expand and shrink their\nmemory allocations according to the workload demands. Third, Valet provides an\nefficient remote memory reclaiming technique on remote peers, based on two\noptimizations: (1) an activity-based victim selection scheme to allow the\nleast-active-chunk of data to be selected for serving the eviction requests and\n(2) a migration protocol to move the least-active-chunk of data to\nless-memory-pressured remote node. As a result, Valet can effectively reduce\nthe performance impact and migration overhead on local nodes. Our extensive\nexperiments on both NoSQL systems and Machine Learning (ML) workloads show that\nValet outperforms existing representative remote paging systems with up to 226X\nthroughput improvement and up to 98% latency decrease over conventional OS swap\nfacility for big data and ML workloads, and by up to 5.5X throughput\nimprovement and up to 78.4% latency decrease over the state-of-the-art remote\npaging systems. Valet is open sourced at https://github.com/git-disl/Valet.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 14:32:39 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 02:18:48 GMT"}, {"version": "v3", "created": "Wed, 19 Aug 2020 17:53:50 GMT"}, {"version": "v4", "created": "Sun, 23 Aug 2020 00:26:03 GMT"}, {"version": "v5", "created": "Fri, 28 Aug 2020 10:15:00 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Bae", "Juhyun", ""], ["Su", "Gong", ""], ["Iyengar", "Arun", ""], ["Wu", "Yanzhao", ""], ["Liu", "Ling", ""]]}, {"id": "2008.01009", "submitter": "Vitaly Aksenov", "authors": "Vitaly Aksenov, Dan Alistarh, Alexandra Drozdova, Amirkeivan\n  Mohtashami", "title": "The Splay-List: A Distribution-Adaptive Concurrent Skip-List", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design and implementation of efficient concurrent data structures have\nseen significant attention. However, most of this work has focused on\nconcurrent data structures providing good \\emph{worst-case} guarantees. In real\nworkloads, objects are often accessed at different rates, since access\ndistributions may be non-uniform. Efficient distribution-adaptive data\nstructures are known in the sequential case, e.g. the splay-trees; however,\nthey often are hard to translate efficiently in the concurrent case.\n  In this paper, we investigate distribution-adaptive concurrent data\nstructures and propose a new design called the splay-list. At a high level, the\nsplay-list is similar to a standard skip-list, with the key distinction that\nthe height of each element adapts dynamically to its access rate: popular\nelements ``move up,'' whereas rarely-accessed elements decrease in height. We\nshow that the splay-list provides order-optimal amortized complexity bounds for\na subset of operations while being amenable to efficient concurrent\nimplementation. Experimental results show that the splay-list can leverage\ndistribution-adaptivity to improve on the performance of classic concurrent\ndesigns, and can outperform the only previously-known distribution-adaptive\ndesign in certain settings.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 16:45:49 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Aksenov", "Vitaly", ""], ["Alistarh", "Dan", ""], ["Drozdova", "Alexandra", ""], ["Mohtashami", "Amirkeivan", ""]]}, {"id": "2008.01124", "submitter": "Jamal Toutouh", "authors": "Jamal Toutouh, Erik Hemberg, and Una-May O'Reilly", "title": "Analyzing the Components of Distributed Coevolutionary GAN Training", "comments": "Accepted as a full paper in Sixteenth International Conference on\n  Parallel Problem Solving from Nature (PPSN XVI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed coevolutionary Generative Adversarial Network (GAN) training has\nempirically shown success in overcoming GAN training pathologies. This is\nmainly due to diversity maintenance in the populations of generators and\ndiscriminators during the training process. The method studied here coevolves\nsub-populations on each cell of a spatial grid organized into overlapping Moore\nneighborhoods. We investigate the impact on the performance of two algorithm\ncomponents that influence the diversity during coevolution: the\nperformance-based selection/replacement inside each sub-population and the\ncommunication through migration of solutions (networks) among overlapping\nneighborhoods. In experiments on MNIST dataset, we find that the combination of\nthese two components provides the best generative models. In addition,\nmigrating solutions without applying selection in the sub-populations achieves\ncompetitive results, while selection without communication between cells\nreduces performance.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 18:35:06 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Toutouh", "Jamal", ""], ["Hemberg", "Erik", ""], ["O'Reilly", "Una-May", ""]]}, {"id": "2008.01144", "submitter": "Minghui Liwang", "authors": "Minghui LiWang, Zhibin Gao, Xianbin Wang", "title": "Energy-aware Graph Job Allocation in Software Defined Air-Ground\n  Integrated Vehicular Networks", "comments": "14 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The software defined air-ground integrated vehicular (SD-AGV) networks have\nemerged as a promising paradigm, which realize the flexible on-ground resource\nsharing to support innovative applications for UAVs with heavy computational\noverhead. In this paper, we investigate a vehicular cloud-assisted graph job\nallocation problem in SD-AGV networks, where the computation-intensive jobs\ncarried by UAVs, and the vehicular cloud are modeled as graphs. To map each\ncomponent of the graph jobs to a feasible vehicle, while achieving the\ntrade-off among minimizing UAVs' job completion time, energy consumption, and\nthe data exchange cost among vehicles, we formulate the problem as a\nmixed-integer non-linear programming problem, which is Np-hard. Moreover, the\nconstraint associated with preserving job structures poses addressing the\nsubgraph isomorphism problem, that further complicates the algorithm design.\nMotivated by which, we propose an efficient decoupled approach by separating\nthe template (feasible mappings between components and vehicles) searching from\nthe transmission power allocation. For the former, we present an efficient\nalgorithm of searching for all the subgraph isomorphisms with low computation\ncomplexity. For the latter, we introduce a power allocation algorithm by\napplying convex optimization techniques. Extensive simulations demonstrate that\nthe proposed approach outperforms the benchmark methods considering various\nproblem sizes.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 19:27:28 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["LiWang", "Minghui", ""], ["Gao", "Zhibin", ""], ["Wang", "Xianbin", ""]]}, {"id": "2008.01215", "submitter": "Laurent Callot", "authors": "Valentin Flunkert, Quentin Rebjock, Joel Castellon, Laurent Callot,\n  Tim Januschowski", "title": "A simple and effective predictive resource scaling heuristic for\n  large-scale cloud applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple yet effective policy for the predictive auto-scaling of\nhorizontally scalable applications running in cloud environments, where compute\nresources can only be added with a delay, and where the deployment throughput\nis limited. Our policy uses a probabilistic forecast of the workload to make\nscaling decisions dependent on the risk aversion of the application owner. We\nshow in our experiments using real-world and synthetic data that this policy\ncompares favorably to mathematically more sophisticated approaches as well as\nto simple benchmark policies.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 21:50:14 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Flunkert", "Valentin", ""], ["Rebjock", "Quentin", ""], ["Castellon", "Joel", ""], ["Callot", "Laurent", ""], ["Januschowski", "Tim", ""]]}, {"id": "2008.01286", "submitter": "Manush Bhatt", "authors": "Manush Bhatt, Rajesh Kalyanam, Gen Nishida, Liu He, Christopher May,\n  Dev Niyogi, Daniel Aliaga", "title": "Design and Deployment of Photo2Building: A Cloud-based Procedural\n  Modeling Tool as a Service", "comments": "7 pages, 7 figures, PEARC '20: Practice and Experience in Advanced\n  Research Computing, July 26--30, 2020, Portland, OR, USA", "journal-ref": "ACM, PEARC 2020", "doi": "10.1145/3311790.3396670", "report-no": null, "categories": "cs.DC cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Photo2Building tool to create a plausible 3D model of a building\nfrom only a single photograph. Our tool is based on a prior desktop version\nwhich, as described in this paper, is converted into a client-server model,\nwith job queuing, web-page support, and support of concurrent usage. The\nreported cloud-based web-accessible tool can reconstruct a building in 40\nseconds on average and costing only 0.60 USD with current pricing. This\nprovides for an extremely scalable and possibly widespread tool for creating\nbuilding models for use in urban design and planning applications. With the\ngrowing impact of rapid urbanization on weather and climate and resource\navailability, access to such a service is expected to help a wide variety of\nusers such as city planners, urban meteorologists worldwide in the quest to\nimproved prediction of urban weather and designing climate-resilient cities of\nthe future.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 02:43:33 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Bhatt", "Manush", ""], ["Kalyanam", "Rajesh", ""], ["Nishida", "Gen", ""], ["He", "Liu", ""], ["May", "Christopher", ""], ["Niyogi", "Dev", ""], ["Aliaga", "Daniel", ""]]}, {"id": "2008.01340", "submitter": "Manish Bhattarai", "authors": "Manish Bhattarai, Gopinath Chennupati, Erik Skau, Raviteja Vangara,\n  Hirsto Djidjev, Boian Alexandrov", "title": "Distributed Non-Negative Tensor Train Decomposition", "comments": "Accepted to IEEE-HPEC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The era of exascale computing opens new venues for innovations and\ndiscoveries in many scientific, engineering, and commercial fields. However,\nwith the exaflops also come the extra-large high-dimensional data generated by\nhigh-performance computing. High-dimensional data is presented as\nmultidimensional arrays, aka tensors. The presence of latent (not directly\nobservable) structures in the tensor allows a unique representation and\ncompression of the data by classical tensor factorization techniques. However,\nthe classical tensor methods are not always stable or they can be exponential\nin their memory requirements, which makes them not suitable for\nhigh-dimensional tensors. Tensor train (TT) is a state-of-the-art tensor\nnetwork introduced for factorization of high-dimensional tensors. TT transforms\nthe initial high-dimensional tensor in a network of three-dimensional tensors\nthat requires only a linear storage. Many real-world data, such as, density,\ntemperature, population, probability, etc., are non-negative and for an easy\ninterpretation, the algorithms preserving non-negativity are preferred. Here,\nwe introduce a distributed non-negative tensor-train and demonstrate its\nscalability and the compression on synthetic and real-world big datasets.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 05:35:57 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Bhattarai", "Manish", ""], ["Chennupati", "Gopinath", ""], ["Skau", "Erik", ""], ["Vangara", "Raviteja", ""], ["Djidjev", "Hirsto", ""], ["Alexandrov", "Boian", ""]]}, {"id": "2008.01425", "submitter": "Thijs Vogels", "authors": "Thijs Vogels and Sai Praneeth Karimireddy and Martin Jaggi", "title": "PowerGossip: Practical Low-Rank Communication Compression in\n  Decentralized Deep Learning", "comments": "To appear in NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lossy gradient compression has become a practical tool to overcome the\ncommunication bottleneck in centrally coordinated distributed training of\nmachine learning models. However, algorithms for decentralized training with\ncompressed communication over arbitrary connected networks have been more\ncomplicated, requiring additional memory and hyperparameters. We introduce a\nsimple algorithm that directly compresses the model differences between\nneighboring workers using low-rank linear compressors applied on model\ndifferences. Inspired by the PowerSGD algorithm for centralized deep learning,\nthis algorithm uses power iteration steps to maximize the information\ntransferred per bit. We prove that our method requires no additional\nhyperparameters, converges faster than prior methods, and is asymptotically\nindependent of both the network and the compression. Out of the box, these\ncompressors perform on par with state-of-the-art tuned compression algorithms\nin a series of deep learning benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 09:14:52 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 15:07:50 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Vogels", "Thijs", ""], ["Karimireddy", "Sai Praneeth", ""], ["Jaggi", "Martin", ""]]}, {"id": "2008.01560", "submitter": "Kostas Kolomvatsos", "authors": "Panagiotis Fountas, Kostas Kolomvatsos, Christos Anagnostopoulos", "title": "Data Synopses Management based on a Deep Learning Model", "comments": "arXiv admin note: substantial text overlap with arXiv:2007.12648", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pervasive computing involves the placement of processing services close to\nend users to support intelligent applications. With the advent of the Internet\nof Things (IoT) and the Edge Computing (EC), one can find room for placing\nservices at various points in the interconnection of the aforementioned\ninfrastructures. Of significant importance is the processing of the collected\ndata. Such a processing can be realized upon the EC nodes that exhibit\nincreased computational capabilities compared to IoT devices. An ecosystem of\nintelligent nodes is created at the EC giving the opportunity to support\ncooperative models. Nodes become the hosts of geo-distributed datasets\nformulated by the IoT devices reports. Upon the datasets, a number of\nqueries/tasks can be executed. Queries/tasks can be offloaded for performance\nreasons. However, an offloading action should be carefully designed being\nalways aligned with the data present to the hosting node. In this paper, we\npresent a model to support the cooperative aspect in the EC infrastructure. We\nargue on the delivery of data synopses to EC nodes making them capable to take\noffloading decisions fully aligned with data present at peers. Nodes exchange\ndata synopses to inform their peers. We propose a scheme that detects the\nappropriate time to distribute synopses trying to avoid the network overloading\nespecially when synopses are frequently extracted due to the high rates at\nwhich IoT devices report data to EC nodes. Our approach involves a Deep\nLearning model for learning the distribution of calculated synopses and\nestimate future trends. Upon these trends, we are able to find the appropriate\ntime to deliver synopses to peer nodes. We provide the description of the\nproposed mechanism and evaluate it based on real datasets. An extensive\nexperimentation upon various scenarios reveals the pros and cons of the\napproach by giving numerical results.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 12:04:21 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Fountas", "Panagiotis", ""], ["Kolomvatsos", "Kostas", ""], ["Anagnostopoulos", "Christos", ""]]}, {"id": "2008.01638", "submitter": "Lorenzo Bacchiani", "authors": "Lorenzo Bacchiani, Mario Bravetti, Saverio Giallorenzo, Jacopo Mauro,\n  Iacopo Talevi, Gianluigi Zavattaro", "title": "Microservice Interface Based Deployment Orchestration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following previous work on the automated deployment orchestration of\ncomponent based applications, where orchestrations are expressed in terms of\nbehaviours satisfying component interface functional dependences, we develop a\nformal model specifically tailored for microservice architectures. The first\nresult that we obtain is decidability of the problem of synthesizing optimal\ndeployment orchestrations for microservice architectures, a problem that is,\ninstead, undecidable for generic component-based applications. We, thus, show\nhow optimal deployment orchestrations can be synthesized and how, by using such\norchestrations we can devise a procedure for run-time adaptation based on\nperforming global reconfigurations. Finally, we evaluate the applicability of\nour approach on a realistic microservice architecture taken from the\nliterature. In particular, we use the high-level object-oriented probabilistic\nand timed process algebra Abstract Behavioural Specification (ABS) to model\nsuch a case study and to simulate it. The results of simulation show the\nadvantages of global reconfiguration w.r.t. local adaptation.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 15:34:06 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 11:31:40 GMT"}, {"version": "v3", "created": "Tue, 29 Sep 2020 18:34:55 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Bacchiani", "Lorenzo", ""], ["Bravetti", "Mario", ""], ["Giallorenzo", "Saverio", ""], ["Mauro", "Jacopo", ""], ["Talevi", "Iacopo", ""], ["Zavattaro", "Gianluigi", ""]]}, {"id": "2008.01742", "submitter": "Mayank Mundhra", "authors": "Mayank Mundhra, Chester Rebeiro", "title": "SISSLE in consensus-based Ripple: Some Improvements in Speed, Security\n  and Last Mile Connectivity", "comments": "11 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cryptocurrencies are rapidly finding application in areas such as Real Time\nGross Settlements and Payments. Ripple is a cryptocurrency that has gained\nprominence with banks and payment providers. It solves the Byzantine General's\nProblem with its Ripple Protocol Consensus Algorithm (RPCA), where each server\nmaintains a list of servers, called the Unique Node List (UNL), that represents\nthe network for that server and will not collectively defraud it. The server\nbelieves that the network has come to a consensus when servers on the UNL come\nto a consensus on a transaction.\n  In this paper we improve Ripple to achieve better speed, security and last\nmile connectivity. We implement guidelines for resilience, robustness, improved\nsecurity, and efficient information propagation (IP). We enhance the system to\nensure that each server receives information from across the whole network\nrather than just from the UNL members. We introduce the paradigm of UNL overlap\nas a function of IP and the trust a server assigns to its own UNL. Our design\nmakes it possible to identify and mitigate some malicious behaviours including\nattempts to fraudulently Double Spend or stall the system. We provide\nexperimental evidence of the benefits of our approach over the current Ripple\nscheme. We observe $\\geq 99.67\\%$ reduction in opportunities for double spend\nattacks and censorship, $1.71x$ increase in fault tolerance to $\\geq 34.21\\%$\nmalicious nodes, $\\geq 4.97x$ and $98.22x$ speedup and success rate for IP\nrespectively, and $\\geq 3.16x$ and $51.70x$ speedup and success rate in\nconsensus respectively.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 18:00:51 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 06:39:57 GMT"}, {"version": "v3", "created": "Sat, 15 May 2021 20:10:31 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Mundhra", "Mayank", ""], ["Rebeiro", "Chester", ""]]}, {"id": "2008.01814", "submitter": "Blesson Varghese", "authors": "Francis McNamee and Schahram Dustadar and Peter Kilpatrick and Weisong\n  Shi and Ivor Spence and Blesson Varghese", "title": "A Case For Adaptive Deep Neural Networks in Edge Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge computing offers an additional layer of compute infrastructure closer to\nthe data source before raw data from privacy-sensitive and performance-critical\napplications is transferred to a cloud data center. Deep Neural Networks (DNNs)\nare one class of applications that are reported to benefit from collaboratively\ncomputing between the edge and the cloud. A DNN is partitioned such that\nspecific layers of the DNN are deployed onto the edge and the cloud to meet\nperformance and privacy objectives. However, there is limited understanding of:\n(a) whether and how evolving operational conditions (increased CPU and memory\nutilization at the edge or reduced data transfer rates between the edge and the\ncloud) affect the performance of already deployed DNNs, and (b) whether a new\npartition configuration is required to maximize performance. A DNN that adapts\nto changing operational conditions is referred to as an 'adaptive DNN'. This\npaper investigates whether there is a case for adaptive DNNs in edge computing\nby considering three questions: (i) Are DNNs sensitive to operational\nconditions? (ii) How sensitive are DNNs to operational conditions? (iii) Do\nindividual or a combination of operational conditions equally affect DNNs? (iv)\nIs DNN partitioning sensitive to hardware architectures on the cloud/edge? The\nexploration is carried out in the context of 8 pre-trained DNN models and the\nresults presented are from analyzing nearly 8 million data points. The results\nhighlight that network conditions affects DNN performance more than CPU or\nmemory related operational conditions. Repartitioning is noted to provide a\nperformance gain in a number of cases, but a specific trend was not noted in\nrelation to its correlation to the underlying hardware architecture.\nNonetheless, the need for adaptive DNNs is confirmed.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 20:23:50 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 14:27:36 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["McNamee", "Francis", ""], ["Dustadar", "Schahram", ""], ["Kilpatrick", "Peter", ""], ["Shi", "Weisong", ""], ["Spence", "Ivor", ""], ["Varghese", "Blesson", ""]]}, {"id": "2008.01827", "submitter": "Somalee Datta", "authors": "Joseph Mesterhazy, Garrick Olson, Somalee Datta", "title": "High performance on-demand de-identification of a petabyte-scale medical\n  imaging data lake", "comments": "11 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increase in Artificial Intelligence driven approaches, researchers\nare requesting unprecedented volumes of medical imaging data which far exceed\nthe capacity of traditional on-premise client-server approaches for making the\ndata research analysis-ready. We are making available a flexible solution for\non-demand de-identification that combines the use of mature software\ntechnologies with modern cloud-based distributed computing techniques to enable\nfaster turnaround in medical imaging research. The solution is part of a\nbroader platform that supports a secure high performance clinical data science\nplatform.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 21:01:30 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Mesterhazy", "Joseph", ""], ["Olson", "Garrick", ""], ["Datta", "Somalee", ""]]}, {"id": "2008.01938", "submitter": "Susumu Matsumae", "authors": "Susumu Matsumae and Makoto Miyazaki", "title": "Solving Dynamic Programming Problem by Pipeline Implementation on GPU", "comments": null, "journal-ref": "International Journal of Advanced Computer Science and\n  Applications(IJACSA), Volume 9 Issue 12, 2018", "doi": "10.14569/IJACSA.2018.091272", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we show the effectiveness of a pipeline implementation of\nDynamic Programming (DP) on GPU. As an example, we explain how to solve a\nmatrix-chain multiplication (MCM) problem by DP on GPU. This problem can be\nsequentially solved in $O(n^3)$ steps by DP where $n$ is the number of\nmatrices, because its solution table is of size $n \\times n$ and each element\nof the table can be computed in $O(n)$ steps. A typical speedup strategy for\nthis is to parallelize the $O(n)$ step computation of each element, which can\nbe easily achieved by parallel prefix computation, i.e., an $O(\\log n)$ step\ncomputation with $n$ threads in a tournament fashion. By such a standard\nparallelizing method, we can solve the MCM problem in $O(n^2 \\log n)$ steps\nwith $n$ threads. In our approach, we solve the MCM problem on GPU in a\npipeline fashion, i.e., we use GPU cores for supporting pipeline-stages so that\nmany elements of the solution table are partially computed in parallel at one\ntime. Our implementation determines one output value per one computational step\nwith $n$ threads in a pipeline fashion and constructs the solution table\ntotally in $O(n^2)$ steps with $n$ threads.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 05:15:00 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Matsumae", "Susumu", ""], ["Miyazaki", "Makoto", ""]]}, {"id": "2008.01960", "submitter": "Kai Sun", "authors": "Kai Sun", "title": "A Novel Approach for the Process Planning and Scheduling Problem Using\n  the Concept of Maximum Weighted Independent Set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process Planning and Scheduling (PPS) is an essential and practical topic but\na very intractable problem in manufacturing systems. Many research use\niterative methods to solve such problems; however, they cannot achieve\nsatisfactory results in both quality and computational speed. Other studies\nformulate scheduling problems as a graph coloring problem (GCP) or its\nextensions, but these formulations are limited to certain types of scheduling\nproblems. In this paper, we propose a novel approach to formulate a general\ntype of the PPS problem with resource allocation and process planning\nintegrated towards a typical objective, minimizing the makespan. The PPS\nproblem is formulated into an undirected weighted conflicting graph, where\nnodes represent operations and their resources; edges represent constraints,\nand weight factors are guidelines for the node selection at each time slot.\nThen, the Maximum Weighted Independent Set (MWIS) problem can be solved to find\nthe best set of operations with their desired resources for each discrete time\nslot. This proposed approach solves the PPS problem directly with minimum\niterations. We establish that the proposed approach always returns a feasible\noptimum or near-optimum solution to the PPS problem. The different weight\nconfigurations of the proposed approach for solving the PPS problem are tested\non a real-world PPS example and further designated test instances to evaluate\nthe scalability, accuracy, and robustness.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 07:02:27 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 03:07:13 GMT"}, {"version": "v3", "created": "Sun, 16 Aug 2020 06:33:33 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Sun", "Kai", ""]]}, {"id": "2008.01990", "submitter": "Shengguo Li", "authors": "Xia Liao, Shengguo Li, Yutong Lu and Jose E. Roman", "title": "A parallel structured divide-and-conquer algorithm for symmetric\n  tridiagonal eigenvalue problems", "comments": "17 pages, 9 figures", "journal-ref": "IEEE Transactions on Parallel and Distributed Systems 32 (2021)\n  367-378", "doi": "10.1109/TPDS.2020.3019471", "report-no": null, "categories": "cs.MS cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, a parallel structured divide-and-conquer (PSDC) eigensolver is\nproposed for symmetric tridiagonal matrices based on ScaLAPACK and a parallel\nstructured matrix multiplication algorithm, called PSMMA. Computing the\neigenvectors via matrix-matrix multiplications is the most computationally\nexpensive part of the divide-and-conquer algorithm, and one of the matrices\ninvolved in such multiplications is a rank-structured Cauchy-like matrix. By\nexploiting this particular property, PSMMA constructs the local matrices by\nusing generators of Cauchy-like matrices without any communication, and further\nreduces the computation costs by using a structured low-rank approximation\nalgorithm. Thus, both the communication and computation costs are reduced.\nExperimental results show that both PSMMA and PSDC are highly scalable and\nscale to 4096 processes at least. PSDC has better scalability than PHDC that\nwas proposed in [J. Comput. Appl. Math. 344 (2018) 512--520] and only scaled to\n300 processes for the same matrices. Comparing with \\texttt{PDSTEDC} in\nScaLAPACK, PSDC is always faster and achieves $1.4$x--$1.6$x speedup for some\nmatrices with few deflations. PSDC is also comparable with ELPA, with PSDC\nbeing faster than ELPA when using few processes and a little slower when using\nmany processes.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 08:23:36 GMT"}, {"version": "v2", "created": "Sat, 28 Nov 2020 15:09:22 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Liao", "Xia", ""], ["Li", "Shengguo", ""], ["Lu", "Yutong", ""], ["Roman", "Jose E.", ""]]}, {"id": "2008.02033", "submitter": "Jin Wang", "authors": "Jin Wang, Jia Hu, Geyong Min, Albert Y. Zomaya, Nektarios Georgalas", "title": "Fast Adaptive Task Offloading in Edge Computing based on Meta\n  Reinforcement Learning", "comments": "Accepted by IEEE Transaction on Parallel and Distributed Systems", "journal-ref": null, "doi": "10.1109/TPDS.2020.3014896", "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-access edge computing (MEC) aims to extend cloud service to the network\nedge to reduce network traffic and service latency. A fundamental problem in\nMEC is how to efficiently offload heterogeneous tasks of mobile applications\nfrom user equipment (UE) to MEC hosts. Recently, many deep reinforcement\nlearning (DRL) based methods have been proposed to learn offloading policies\nthrough interacting with the MEC environment that consists of UE, wireless\nchannels, and MEC hosts. However, these methods have weak adaptability to new\nenvironments because they have low sample efficiency and need full retraining\nto learn updated policies for new environments. To overcome this weakness, we\npropose a task offloading method based on meta reinforcement learning, which\ncan adapt fast to new environments with a small number of gradient updates and\nsamples. We model mobile applications as Directed Acyclic Graphs (DAGs) and the\noffloading policy by a custom sequence-to-sequence (seq2seq) neural network. To\nefficiently train the seq2seq network, we propose a method that synergizes the\nfirst order approximation and clipped surrogate objective. The experimental\nresults demonstrate that this new offloading method can reduce the latency by\nup to 25% compared to three baselines while being able to adapt fast to new\nenvironments.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 10:16:25 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 09:15:59 GMT"}, {"version": "v3", "created": "Fri, 7 Aug 2020 11:05:41 GMT"}, {"version": "v4", "created": "Mon, 14 Sep 2020 14:58:59 GMT"}, {"version": "v5", "created": "Sat, 24 Oct 2020 10:04:20 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Wang", "Jin", ""], ["Hu", "Jia", ""], ["Min", "Geyong", ""], ["Zomaya", "Albert Y.", ""], ["Georgalas", "Nektarios", ""]]}, {"id": "2008.02087", "submitter": "Jiangwei Zhang", "authors": "Jiangwei Zhang, Li Zhang, Vigneshwaran Raveendran, Ziv Ben-Zuk, and\n  Leonard Lu", "title": "PriceAggregator: An Intelligent System for Hotel Price Fetching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the hotel price aggregation system - PriceAggregator,\ndeployed at Agoda, a global online travel agency for hotels, vacation rentals,\nflights and airport transfer. Agoda aggregates non-direct suppliers' hotel\nrooms to ensure that Agoda's customers always have the widest selection of\nhotels, room types and packages. As of today, Agoda aggregates millions of\nhotels. The major challenge is that each supplier only allows Agoda to fetch\nfor the hotel price with a limited amount of Queries Per Second (QPS). Due to\nthe sheer volume of Agoda's user search traffic, this limited amount of QPS is\nnever enough to cover all user searches. Inevitably, many user searches have to\nbe ignored. Hence, booking lost. To overcome the challenge, we built\nPriceAggregator. PriceAggregator intelligently determines when, how and what to\nsend to the suppliers to fetch for price. In this paper, we not only prove\nPriceAggregator is optimal theoretically but also demonstrate that\nPriceAggregator performs well in practice. PriceAggregator has been deployed in\nAgoda. Extensive online A/B experimentation have shown that PriceAggregator\nincreases Agoda's bookings significantly.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 01:28:25 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Zhang", "Jiangwei", ""], ["Zhang", "Li", ""], ["Raveendran", "Vigneshwaran", ""], ["Ben-Zuk", "Ziv", ""], ["Lu", "Leonard", ""]]}, {"id": "2008.02099", "submitter": "Thibault Rieutord", "authors": "Petr Kuznetsov and Thibault Rieutord", "title": "On Decidability of 2-process Affine Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An affine model of computation is defined as a subset of iterated\nimmediate-snapshot runs, capturing a wide variety of shared-memory systems,\nsuch as wait-freedom, t-resilience, k-concurrency, and fair shared-memory\nadversaries. The question of whether a given task is solvable in a given affine\nmodel is, in general, undecidable. In this paper, we focus on affine models\ndefined for a system of two processes. We show that the task computability of\n2-process affine models is decidable and presents a complete hierarchy of the\nfive equivalence classes of 2-process affine models.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 12:58:59 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Kuznetsov", "Petr", ""], ["Rieutord", "Thibault", ""]]}, {"id": "2008.02223", "submitter": "Chansup Byun", "authors": "Chansup Byun, Jeremy Kepner, William Arcand, David Bestor, Bill\n  Bergeron, Vijay Gadepally, Michael Houle, Matthew Hubbell, Michael Jones,\n  Andrew Kirby, Anna Klein, Peter Michaleas, Lauren Milechin, Julie Mullen,\n  Andrew Prout, Antonio Rosa, Siddharth Samsi, Charles Yee, Albert Reuther", "title": "Best of Both Worlds: High Performance Interactive and Batch Launching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid launch of thousands of jobs is essential for effective interactive\nsupercomputing, big data analysis, and AI algorithm development. Achieving\nthousands of launches per second has required hardware to be available to\nreceive these jobs. This paper presents a novel preemptive approach to\nimplement spot jobs on MIT SuperCloud systems allowing the resources to be\nfully utilized for both long running batch jobs while still providing fast\nlaunch for interactive jobs. The new approach separates the job preemption and\nscheduling operations and can achieve 100 times faster performance in the\nscheduling of a job with preemption when compared to using the standard\nscheduler-provided automatic preemption-based capability. The results\ndemonstrate that the new approach can schedule interactive jobs preemptively at\na performance comparable to when the required computing resources are idle and\navailable. The spot job capability can be deployed without disrupting the\ninteractive user experience while increasing the overall system utilization.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 16:45:23 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Byun", "Chansup", ""], ["Kepner", "Jeremy", ""], ["Arcand", "William", ""], ["Bestor", "David", ""], ["Bergeron", "Bill", ""], ["Gadepally", "Vijay", ""], ["Houle", "Michael", ""], ["Hubbell", "Matthew", ""], ["Jones", "Michael", ""], ["Kirby", "Andrew", ""], ["Klein", "Anna", ""], ["Michaleas", "Peter", ""], ["Milechin", "Lauren", ""], ["Mullen", "Julie", ""], ["Prout", "Andrew", ""], ["Rosa", "Antonio", ""], ["Samsi", "Siddharth", ""], ["Yee", "Charles", ""], ["Reuther", "Albert", ""]]}, {"id": "2008.02352", "submitter": "Ashwini Raina", "authors": "Ashwini Raina, Asaf Cidon, Kyle Jamieson, Michael J. Freedman", "title": "PrismDB: Read-aware Log-structured Merge Trees for Heterogeneous Storage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, emerging hardware storage technologies have focused on\ndivergent goals: better performance or lower cost-per-bit of storage.\nCorrespondingly, data systems that employ these new technologies are optimized\neither to be fast (but expensive) or cheap (but slow). We take a different\napproach: by combining multiple tiers of fast and low-cost storage technologies\nwithin the same system, we can achieve a Pareto-efficient balance between\nperformance and cost-per-bit.\n  This paper presents the design and implementation of PrismDB, a novel\nlog-structured merge tree based key-value store that exploits a full spectrum\nof heterogeneous storage technologies (from 3D XPoint to QLC NAND). We\nintroduce the notion of \"read-awareness\" to log-structured merge trees, which\nallows hot objects to be pinned to faster storage, achieving better tiering and\nhot-cold separation of objects. Compared to the standard use of RocksDB on\nflash in datacenters today, PrismDB's average throughput on heterogeneous\nstorage is 2.3$\\times$ faster and its tail latency is more than an order of\nmagnitude better, using hardware than is half the cost.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 20:34:47 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2020 04:05:18 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Raina", "Ashwini", ""], ["Cidon", "Asaf", ""], ["Jamieson", "Kyle", ""], ["Freedman", "Michael J.", ""]]}, {"id": "2008.02452", "submitter": "Dimitrios Dimitriadis", "authors": "Dimitrios Dimitriadis, Kenichi Kumatani, Robert Gmyr, Yashesh Gaur and\n  Sefik Emre Eskimez", "title": "Federated Transfer Learning with Dynamic Gradient Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, a Federated Learning (FL) simulation platform is introduced.\nThe target scenario is Acoustic Model training based on this platform. To our\nknowledge, this is the first attempt to apply FL techniques to Speech\nRecognition tasks due to the inherent complexity. The proposed FL platform can\nsupport different tasks based on the adopted modular design. As part of the\nplatform, a novel hierarchical optimization scheme and two gradient aggregation\nmethods are proposed, leading to almost an order of magnitude improvement in\ntraining convergence speed compared to other distributed or FL training\nalgorithms like BMUF and FedAvg. The hierarchical optimization offers\nadditional flexibility in the training pipeline besides the enhanced\nconvergence speed. On top of the hierarchical optimization, a dynamic gradient\naggregation algorithm is proposed, based on a data-driven weight inference.\nThis aggregation algorithm acts as a regularizer of the gradient quality.\nFinally, an unsupervised training pipeline tailored to FL is presented as a\nseparate training scenario. The experimental validation of the proposed system\nis based on two tasks: first, the LibriSpeech task showing a speed-up of 7x and\n6% Word Error Rate reduction (WERR) compared to the baseline results. The\nsecond task is based on session adaptation providing an improvement of 20% WERR\nover a competitive production-ready LAS model. The proposed Federated Learning\nsystem is shown to outperform the golden standard of distributed training in\nboth convergence speed and overall model performance.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 04:29:01 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Dimitriadis", "Dimitrios", ""], ["Kumatani", "Kenichi", ""], ["Gmyr", "Robert", ""], ["Gaur", "Yashesh", ""], ["Eskimez", "Sefik Emre", ""]]}, {"id": "2008.02512", "submitter": "Pierre Sutra", "authors": "Tuanir Fran\\c{c}a Rezende and Pierre Sutra", "title": "Leaderless State-Machine Replication: Specification, Properties, Limits\n  (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern Internet services commonly replicate critical data across several\ngeographical locations using state-machine replication (SMR). Due to their\nreliance on a leader replica, classical SMR protocols offer limited scalability\nand availability in this setting. To solve this problem, recent protocols\nfollow instead a leaderless approach, in which each replica is able to make\nprogress using a quorum of its peers. In this paper, we study this new emerging\nclass of SMR protocols and states some of their limits. We first propose a\nframework that captures the essence of leaderless state-machine replication\n(Leaderless SMR). Then, we introduce a set of desirable properties for these\nprotocols: (R)eliability, (O)ptimal (L)atency and (L)oad Balancing. We show\nthat protocols matching all of the ROLL properties are subject to a trade-off\nbetween performance and reliability. We also establish a lower bound on the\nmessage delay to execute a command in protocols optimal for the ROLL\nproperties. This lower bound explains the persistent chaining effect observed\nin experimental results.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 08:18:56 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Rezende", "Tuanir Fran\u00e7a", ""], ["Sutra", "Pierre", ""]]}, {"id": "2008.02527", "submitter": "Igor Zablotchi", "authors": "Rachid Guerraoui, Alex Kogan, Virendra J. Marathe, Igor Zablotchi", "title": "Efficient Multi-word Compare and Swap", "comments": "Full version of DISC '20 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Atomic lock-free multi-word compare-and-swap (MCAS) is a powerful tool for\ndesigning concurrent algorithms. Yet, its widespread usage has been limited\nbecause lock-free implementations of MCAS make heavy use of expensive\ncompare-and-swap (CAS) instructions. Existing MCAS implementations indeed use\nat least 2k+1 CASes per k-CAS. This leads to the natural desire to minimize the\nnumber of CASes required to implement MCAS. We first prove in this paper that\nit is impossible to \"pack\" the information required to perform a k-word CAS\n(k-CAS) in less than k locations to be CASed. Then we present the first\nalgorithm that requires k+1 CASes per call to k-CAS in the common uncontended\ncase. We implement our algorithm and show that it outperforms a\nstate-of-the-art baseline in a variety of benchmarks in most considered\nworkloads. We also present a durably linearizable (persistent memory friendly)\nversion of our MCAS algorithm using only 2 persistence fences per call, while\nstill only requiring k+1 CASes per k-CAS.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 08:59:03 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Guerraoui", "Rachid", ""], ["Kogan", "Alex", ""], ["Marathe", "Virendra J.", ""], ["Zablotchi", "Igor", ""]]}, {"id": "2008.02671", "submitter": "Heidi Howard", "authors": "Heidi Howard, Aleksey Charapko, Richard Mortier", "title": "Fast Flexible Paxos: Relaxing Quorum Intersection for Fast Paxos", "comments": "To be published in the Proceedings of International Conference on\n  Distributed Computing and Networking 2021 (ICDCN '21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Paxos, the de facto standard approach to solving distributed consensus,\noperates in two phases, each of which requires an intersecting quorum of nodes.\nMulti-Paxos reduces this to one phase by electing a leader but this leader is\nalso a performance bottleneck. Fast Paxos bypasses the leader but has stronger\nquorum intersection requirements.\n  In this paper we observe that Fast Paxos' intersection requirements can be\nsafely relaxed, reducing to just one additional intersection requirement\nbetween phase-1 quorums and any pair of fast round phase-2 quorums. We thus\nfind that the quorums used with Fast Paxos are larger than necessary, allowing\nalternative quorum systems to obtain new tradeoffs between performance and\nfault-tolerance.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 13:59:42 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 09:30:23 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Howard", "Heidi", ""], ["Charapko", "Aleksey", ""], ["Mortier", "Richard", ""]]}, {"id": "2008.02710", "submitter": "Konstantin Avrachenkov", "authors": "Konstantin Avrachenkov, Patrick Brown, Nelly Litvak", "title": "Red Light Green Light Method for Solving Large Markov Chains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC cs.NA math.OC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete-time discrete-state finite Markov chains are versatile mathematical\nmodels for a wide range of real-life stochastic processes. One of most common\ntasks in studies of Markov chains is computation of the stationary\ndistribution. Without loss of generality, and drawing our motivation from\napplications to large networks, we interpret this problem as one of computing\nthe stationary distribution of a random walk on a graph. We propose a new\ncontrolled, easily distributed algorithm for this task, briefly summarized as\nfollows: at the beginning, each node receives a fixed amount of cash (positive\nor negative), and at each iteration, some nodes receive `green light' to\ndistribute their wealth or debt proportionally to the transition probabilities\nof the Markov chain; the stationary probability of a node is computed as a\nratio of the cash distributed by this a node to the total cash distributed by\nall nodes together. Our method includes as special cases a wide range of known,\nvery different, and previously disconnected methods including power iterations,\nGauss-Southwell, and online distributed algorithms. We prove exponential\nconvergence of our method, demonstrate its high efficiency, and derive\nscheduling strategies for the green-light, that achieve convergence rate faster\nthan state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 15:29:08 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Avrachenkov", "Konstantin", ""], ["Brown", "Patrick", ""], ["Litvak", "Nelly", ""]]}, {"id": "2008.02776", "submitter": "Suryanarayana Murthy Durbhakula", "authors": "Suryanarayana Murthy Durbhakula", "title": "Near Linear OS Scheduling Optimization for Memory Intensive Workloads on\n  Multi-socket Multi-core servers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-socket multi-core servers are used for solving some of the important\nproblems in computing. Remote DRAM accesses can impact performance of certain\napplications running on such servers. This paper presents a new near linear\noperating system (OS) scheduling algorithm to reduce the impact of such remote\nDRAM accesses. By keeping track of the number of local and remote DRAM\naccesses, using performance counters, for every thread and applying this\nalgorithm, I come up with a new schedule of threads for the next quantum. This\nnew schedule reduces remote DRAM accesses and improves overall performance. I\nalso show that this algorithm is actually linear in the best case. As the\nalgorithm is near-linear it is amenable for implementation in a real operating\nsystem.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 17:20:48 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Durbhakula", "Suryanarayana Murthy", ""]]}, {"id": "2008.02782", "submitter": "William Moses Jr.", "authors": "Shay Kutten, William K. Moses Jr., Gopal Pandurangan, David Peleg", "title": "Singularly Optimal Randomized Leader Election", "comments": "24 pages. Full version of paper accepted at DISC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns designing distributed algorithms that are singularly\noptimal, i.e., algorithms that are simultaneously time and message optimal, for\nthe fundamental leader election problem in networks. Our main result is a\nrandomized distributed leader election algorithm for asynchronous complete\nnetworks that is essentially (up to a polylogarithmic factor) singularly\noptimal. Our algorithm uses $O(n)$ messages with high probability and runs in\n$O(\\log^2 n)$ time (with high probability) to elect a unique leader. The $O(n)$\nmessage complexity should be contrasted with the $\\Omega(n \\log n)$ lower\nbounds for the deterministic message complexity of leader election algorithms\n(regardless of time), proven by Korach, Moran, and Zaks (TCS, 1989) for\nasynchronous algorithms and by Afek and Gafni (SIAM J. Comput., 1991) for\nsynchronous networks. Hence, our result also separates the message complexities\nof randomized and deterministic leader election. More importantly, our\n(randomized) time complexity of $O(\\log^2 n)$ for obtaining the optimal $O(n)$\nmessage complexity is significantly smaller than the long-standing\n$\\tilde{\\Theta}(n)$ time complexity obtained by Afek and Gafni and by Singh\n(SIAM J. Comput., 1997) for message optimal (deterministic) election in\nasynchronous networks.\n  In synchronous complete networks, Afek and Gafni showed an essentially\nsingularly optimal deterministic algorithm with $O(\\log n)$ time and $O(n \\log\nn)$ messages. Ramanathan et al. (Distrib. Comput. 2007) used randomization to\nimprove the message complexity, and showed a randomized algorithm with $O(n)$\nmessages and $O(\\log n)$ time (with failure probability $O(1 /\n\\log^{\\Omega(1)}n)$). Our second result is a tightly singularly optimal\nrandomized algorithm, with $O(1)$ time and $O(n)$ messages, for this setting,\nwhose time bound holds with certainty and message bound holds with high\nprobability.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 17:35:38 GMT"}, {"version": "v2", "created": "Sun, 16 Aug 2020 13:17:03 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Kutten", "Shay", ""], ["Moses", "William K.", "Jr."], ["Pandurangan", "Gopal", ""], ["Peleg", "David", ""]]}, {"id": "2008.03066", "submitter": "Babar Shahzaad", "authors": "Babar Shahzaad, Athman Bouguettaya, Sajib Mistry", "title": "A Game-Theoretic Drone-as-a-Service Composition for Delivery", "comments": "5 pages, 3 figures. This is an accepted paper and it is going to\n  appear in the Proceedings of the 2020 IEEE International Conference on Web\n  Services (IEEE ICWS 2020) affiliated with the 2020 IEEE World Congress on\n  Services (IEEE SERVICES 2020), Beijing, China", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel game-theoretic approach for drone service composition\nconsidering recharging constraints. We design a non-cooperative game model for\ndrone services. We propose a non-cooperative game algorithm for the selection\nand composition of optimal drone services. We conduct several experiments on a\nreal drone dataset to demonstrate the efficiency of our proposed approach.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 09:58:58 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Shahzaad", "Babar", ""], ["Bouguettaya", "Athman", ""], ["Mistry", "Sajib", ""]]}, {"id": "2008.03091", "submitter": "Bernhard Haeupler", "authors": "Mohsen Ghaffari and Bernhard Haeupler", "title": "Low-Congestion Shortcuts for Graphs Excluding Dense Minors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that any $n$-node graph $G$ with diameter $D$ admits shortcuts with\ncongestion $O(\\delta D \\log n)$ and dilation $O(\\delta D)$, where $\\delta$ is\nthe maximum edge-density of any minor of $G$. Our proof is simple, elementary,\nand constructive - featuring a $\\tilde{\\Theta}(\\delta D)$-round distributed\nconstruction algorithm. Our results are tight up to $\\tilde{O}(1)$ factors and\ngeneralize, simplify, unify, and strengthen several prior results. For example,\nfor graphs excluding a fixed minor, i.e., graphs with constant $\\delta$, only a\n$\\tilde{O}(D^2)$ bound was known based on a very technical proof that relies on\nthe Robertson-Seymour Graph Structure Theorem.\n  A direct consequence of our result is that many graph families, including any\nminor-excluded ones, have near-optimal $\\tilde{\\Theta}(D)$-round distributed\nalgorithms for many fundamental communication primitives and optimization\nproblems including minimum spanning tree, minimum cut, and shortest-path\napproximations.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 11:23:49 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Ghaffari", "Mohsen", ""], ["Haeupler", "Bernhard", ""]]}, {"id": "2008.03095", "submitter": "Gokhan Gokturk", "authors": "Gokhan Gokturk, Kamer Kaya", "title": "Boosting Parallel Influence-Maximization Kernels for Undirected Networks\n  with Fusing and Vectorization", "comments": "12 pages, 6 figures. Submitted to IEEE Transactions on Parallel and\n  Distributed Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Influence maximization (IM) is the problem of finding a seed vertex set which\nis expected to incur the maximum influence spread on a graph. It has various\napplications in practice such as devising an effective and efficient approach\nto disseminate information, news or ad within a social network. The problem is\nshown to be NP-hard and approximation algorithms with provable quality\nguarantees exist in the literature. However, these algorithms are\ncomputationally expensive even for medium-scaled graphs. Furthermore, graph\nalgorithms usually suffer from spatial and temporal irregularities during\nmemory accesses, and this adds an extra cost on top of the already expensive IM\nkernels. In this work, we leverage fused sampling, memoization, and\nvectorization to restructure, parallelize and boost their performance on\nundirected networks. The proposed approach employs a pseudo-random function and\nperforms multiple Monte-Carlo simulations in parallel to exploit the SIMD lanes\neffectively and efficiently. Besides, it significantly reduces the number of\nedge traversals, hence the amount of data brought from the memory, which is\ncritical for almost all memory-bound graph kernels. We apply the proposed\napproach to the traditional MixGreedy algorithm and propose Infuser which is\nmore than 3000 times faster than the traditional greedy approaches and can run\non large graphs that have been considered as too large in the literature.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 11:48:02 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Gokturk", "Gokhan", ""], ["Kaya", "Kamer", ""]]}, {"id": "2008.03260", "submitter": "Anshumali Shrivastava", "authors": "Nicholas Meisburger, Anshumali Shrivastava", "title": "Distributed Tera-Scale Similarity Search with MPI: Provably Efficient\n  Similarity Search over billions without a Single Distance Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SLASH (Sketched LocAlity Sensitive Hashing), an MPI (Message\nPassing Interface) based distributed system for approximate similarity search\nover terabyte scale datasets. SLASH provides a multi-node implementation of the\npopular LSH (locality sensitive hashing) algorithm, which is generally\nimplemented on a single machine. We show how we can append the LSH algorithm\nwith heavy hitters sketches to provably solve the (high) similarity search\nproblem without a single distance computation. Overall, we mathematically show\nthat, under realistic data assumptions, we can identify the near-neighbor of a\ngiven query $q$ in sub-linear ($ \\ll O(n)$) number of simple sketch aggregation\noperations only. To make such a system practical, we offer a novel design and\nsketching solution to reduce the inter-machine communication overheads\nexponentially. In a direct comparison on comparable hardware, SLASH is more\nthan 10000x faster than the popular LSH package in PySpark. PySpark is a\nwidely-adopted distributed implementation of the LSH algorithm for large\ndatasets and is deployed in commercial platforms. In the end, we show how our\nsystem scale to Tera-scale Criteo dataset with more than 4 billion samples.\nSLASH can index this 2.3 terabyte data over 20 nodes in under an hour, with\nquery times in a fraction of milliseconds. To the best of our knowledge, there\nis no open-source system that can index and perform a similarity search on\nCriteo with a commodity cluster.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 18:15:36 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 22:48:52 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Meisburger", "Nicholas", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "2008.03371", "submitter": "Ang Li", "authors": "Ang Li, Jingwei Sun, Binghui Wang, Lin Duan, Sicheng Li, Yiran Chen,\n  Hai Li", "title": "LotteryFL: Personalized and Communication-Efficient Federated Learning\n  with Lottery Ticket Hypothesis on Non-IID Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning is a popular distributed machine learning paradigm with\nenhanced privacy. Its primary goal is learning a global model that offers good\nperformance for the participants as many as possible. The technology is rapidly\nadvancing with many unsolved challenges, among which statistical heterogeneity\n(i.e., non-IID) and communication efficiency are two critical ones that hinder\nthe development of federated learning. In this work, we propose LotteryFL -- a\npersonalized and communication-efficient federated learning framework via\nexploiting the Lottery Ticket hypothesis. In LotteryFL, each client learns a\nlottery ticket network (i.e., a subnetwork of the base model) by applying the\nLottery Ticket hypothesis, and only these lottery networks will be communicated\nbetween the server and clients. Rather than learning a shared global model in\nclassic federated learning, each client learns a personalized model via\nLotteryFL; the communication cost can be significantly reduced due to the\ncompact size of lottery networks. To support the training and evaluation of our\nframework, we construct non-IID datasets based on MNIST, CIFAR-10 and EMNIST by\ntaking feature distribution skew, label distribution skew and quantity skew\ninto consideration. Experiments on these non-IID datasets demonstrate that\nLotteryFL significantly outperforms existing solutions in terms of\npersonalization and communication cost.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 20:45:12 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Li", "Ang", ""], ["Sun", "Jingwei", ""], ["Wang", "Binghui", ""], ["Duan", "Lin", ""], ["Li", "Sicheng", ""], ["Chen", "Yiran", ""], ["Li", "Hai", ""]]}, {"id": "2008.03433", "submitter": "John Halloran", "authors": "John T. Halloran and David M. Rocke", "title": "GPU-Accelerated Primal Learning for Extremely Fast Large-Scale\n  Classification", "comments": "34th Conference on Neural Information Processing Systems (NeurIPS\n  2020), Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most efficient methods to solve L2-regularized primal problems,\nsuch as logistic regression and linear support vector machine (SVM)\nclassification, is the widely used trust region Newton algorithm, TRON. While\nTRON has recently been shown to enjoy substantial speedups on shared-memory\nmulti-core systems, exploiting graphical processing units (GPUs) to speed up\nthe method is significantly more difficult, owing to the highly complex and\nheavily sequential nature of the algorithm. In this work, we show that using\njudicious GPU-optimization principles, TRON training time for different losses\nand feature representations may be drastically reduced. For sparse feature\nsets, we show that using GPUs to train logistic regression classifiers in\nLIBLINEAR is up to an order-of-magnitude faster than solely using\nmultithreading. For dense feature sets--which impose far more stringent memory\nconstraints--we show that GPUs substantially reduce the lengthy SVM learning\ntimes required for state-of-the-art proteomics analysis, leading to dramatic\nimprovements over recently proposed speedups. Furthermore, we show how GPU\nspeedups may be mixed with multithreading to enable such speedups when the\ndataset is too large for GPU memory requirements; on a massive dense proteomics\ndataset of nearly a quarter-billion data instances, these mixed-architecture\nspeedups reduce SVM analysis time from over half a week to less than a single\nday while using limited GPU memory.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 03:40:27 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 03:16:07 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Halloran", "John T.", ""], ["Rocke", "David M.", ""]]}, {"id": "2008.03485", "submitter": "Leonid Sokolinsky", "authors": "Leonid B. Sokolinsky", "title": "BSF: a parallel computation model for scalability estimation of\n  iterative numerical algorithms on cluster computing systems", "comments": null, "journal-ref": "Journal of Parallel and Distributed Computing, 2021, Volume 149,\n  Pages 193-206", "doi": "10.1016/j.jpdc.2020.12.009", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines a new parallel computation model called bulk synchronous\nfarm (BSF) that focuses on estimating the scalability of compute-intensive\niterative algorithms aimed at cluster computing systems. In the BSF model, a\ncomputer is a set of processor nodes connected by a network and organized\naccording to the master/slave paradigm. A cost metric of the BSF model is\npresented. This cost metric requires the algorithm to be represented in the\nform of operations on lists. This allows us to derive an equation that predicts\nthe scalability boundary of a parallel program: the maximum number of processor\nnodes after which the speedup begins to decrease. The paper includes several\nexamples of applying the BSF model to designing and analyzing parallel\nnu-merical algorithms. The large-scale computational experiments conducted on a\ncluster computing system confirm the adequacy of the analytical estimations\nobtained using the BSF model.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 09:47:23 GMT"}, {"version": "v2", "created": "Sat, 5 Sep 2020 15:09:09 GMT"}, {"version": "v3", "created": "Sun, 27 Sep 2020 07:20:54 GMT"}, {"version": "v4", "created": "Fri, 30 Oct 2020 03:02:42 GMT"}, {"version": "v5", "created": "Sat, 2 Jan 2021 14:50:05 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Sokolinsky", "Leonid B.", ""]]}, {"id": "2008.03523", "submitter": "Blesson Varghese", "authors": "Luke Lockhart and Paul Harvey and Pierre Imai and Peter Willis and\n  Blesson Varghese", "title": "Scission: Performance-driven and Context-aware Cloud-Edge Distribution\n  of Deep Neural Networks", "comments": "Accepted to IEEE/ACM UCC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partitioning and distributing deep neural networks (DNNs) across end-devices,\nedge resources and the cloud has a potential twofold advantage: preserving\nprivacy of the input data, and reducing the ingress bandwidth demand beyond the\nedge. However, for a given DNN, identifying the optimal partition configuration\nfor distributing the DNN that maximizes performance is a significant challenge.\nThis is because the combination of potential target hardware resources that\nmaximizes performance and the sequence of layers of the DNN that should be\ndistributed across the target resources needs to be determined, while\naccounting for user-defined objectives/constraints for partitioning. This paper\npresents Scission, a tool for automated benchmarking of DNNs on a given set of\ntarget device, edge and cloud resources for determining optimal partitions that\nmaximize DNN performance. The decision-making approach is context-aware by\ncapitalizing on hardware capabilities of the target resources, their locality,\nthe characteristics of DNN layers, and the network condition. Experimental\nstudies are carried out on 18 DNNs. The decisions made by Scission cannot be\nmanually made by a human given the complexity and the number of dimensions\naffecting the search space. The benchmarking overheads of Scission allow for\nresponding to operational changes periodically rather than in real-time.\nScission is available for public download at\nhttps://github.com/qub-blesson/Scission.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 13:39:57 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 19:45:55 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Lockhart", "Luke", ""], ["Harvey", "Paul", ""], ["Imai", "Pierre", ""], ["Willis", "Peter", ""], ["Varghese", "Blesson", ""]]}, {"id": "2008.03578", "submitter": "Naorin Hossain", "authors": "Naorin Hossain, Caroline Trippel, Margaret Martonosi", "title": "TransForm: Formally Specifying Transistency Models and Synthesizing\n  Enhanced Litmus Tests", "comments": "*This is an updated version of the TransForm paper that features\n  updated results reflecting performance optimizations and software bug fixes.\n  14 pages, 11 figures, Proceedings of the 47th Annual International Symposium\n  on Computer Architecture (ISCA)", "journal-ref": null, "doi": "10.1109/ISCA45697.2020.00076", "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memory consistency models (MCMs) specify the legal ordering and visibility of\nshared memory accesses in a parallel program. Traditionally, instruction set\narchitecture (ISA) MCMs assume that relevant program-visible memory ordering\nbehaviors only result from shared memory interactions that take place between\nuser-level program instructions. This assumption fails to account for virtual\nmemory (VM) implementations that may result in additional shared memory\ninteractions between user-level program instructions and both 1) system-level\noperations (e.g., address remappings and translation lookaside buffer\ninvalidations initiated by system calls) and 2) hardware-level operations\n(e.g., hardware page table walks and dirty bit updates) during a user-level\nprogram's execution. These additional shared memory interactions can impact the\nobservable memory ordering behaviors of user-level programs. Thus, memory\ntransistency models (MTMs) have been coined as a superset of MCMs to\nadditionally articulate VM-aware consistency rules. However, no prior work has\nenabled formal MTM specifications, nor methods to support their automated\nanalysis.\n  To fill the above gap, this paper presents the TransForm framework. First,\nTransForm features an axiomatic vocabulary for formally specifying MTMs.\nSecond, TransForm includes a synthesis engine to support the automated\ngeneration of litmus tests enhanced with MTM features (i.e., enhanced litmus\ntests, or ELTs) when supplied with a TransForm MTM specification. As a case\nstudy, we formally define an estimated MTM for Intel x86 processors, called\nx86t_elt, that is based on observations made by an ELT-based evaluation of an\nIntel x86 MTM implementation from prior work and available public\ndocumentation. Given x86t_elt and a synthesis bound as input, TransForm's\nsynthesis engine successfully produces a set of ELTs including relevant ELTs\nfrom prior work.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 18:48:01 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 15:01:28 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Hossain", "Naorin", ""], ["Trippel", "Caroline", ""], ["Martonosi", "Margaret", ""]]}, {"id": "2008.03602", "submitter": "Aditya Dhakal", "authors": "Aditya Dhakal, Junguk Cho, Sameer G. Kulkarni, K. K. Ramakrishnan,\n  Puneet Sharma", "title": "Spatial Sharing of GPU for Autotuning DNN models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GPUs are used for training, inference, and tuning the machine learning\nmodels. However, Deep Neural Network (DNN) vary widely in their ability to\nexploit the full power of high-performance GPUs. Spatial sharing of GPU enables\nmultiplexing several DNNs on the GPU and can improve GPU utilization, thus\nimproving throughput and lowering latency. DNN models given just the right\namount of GPU resources can still provide low inference latency, just as much\nas dedicating all of the GPU for their inference task. An approach to improve\nDNN inference is tuning of the DNN model. Autotuning frameworks find the\noptimal low-level implementation for a certain target device based on the\ntrained machine learning model, thus reducing the DNN's inference latency and\nincreasing inference throughput. We observe an interdependency between the\ntuned model and its inference latency. A DNN model tuned with specific GPU\nresources provides the best inference latency when inferred with close to the\nsame amount of GPU resources. While a model tuned with the maximum amount of\nthe GPU's resources has poorer inference latency once the GPU resources are\nlimited for inference. On the other hand, a model tuned with an appropriate\namount of GPU resources still achieves good inference latency across a wide\nrange of GPU resource availability. We explore the causes that impact the\ntuning of a model at different amounts of GPU resources. We present many\ntechniques to maximize resource utilization and improve tuning performance. We\nenable controlled spatial sharing of GPU to multiplex several tuning\napplications on the GPU. We scale the tuning server instances and shard the\ntuning model across multiple client instances for concurrent tuning of\ndifferent operators of a model, achieving better GPU multiplexing. With our\nimprovements, we decrease DNN autotuning time by up to 75 percent and increase\nthroughput by a factor of 5.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 21:27:38 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Dhakal", "Aditya", ""], ["Cho", "Junguk", ""], ["Kulkarni", "Sameer G.", ""], ["Ramakrishnan", "K. K.", ""], ["Sharma", "Puneet", ""]]}, {"id": "2008.03606", "submitter": "Sai Praneeth Karimireddy", "authors": "Sai Praneeth Karimireddy, Martin Jaggi, Satyen Kale, Mehryar Mohri,\n  Sashank J. Reddi, Sebastian U. Stich, Ananda Theertha Suresh", "title": "Mime: Mimicking Centralized Stochastic Algorithms in Federated Learning", "comments": "Version 2 provides stronger theoretical results and more thorough\n  experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) is a challenging setting for optimization due to the\nheterogeneity of the data across different clients which gives rise to the\nclient drift phenomenon. In fact, obtaining an algorithm for FL which is\nuniformly better than simple centralized training has been a major open problem\nthus far. In this work, we propose a general algorithmic framework, Mime, which\ni) mitigates client drift and ii) adapts arbitrary centralized optimization\nalgorithms such as momentum and Adam to the cross-device federated learning\nsetting. Mime uses a combination of control-variates and server-level\nstatistics (e.g. momentum) at every client-update step to ensure that each\nlocal update mimics that of the centralized method run on iid data. We prove a\nreduction result showing that Mime can translate the convergence of a generic\nalgorithm in the centralized setting into convergence in the federated setting.\nFurther, we show that when combined with momentum based variance reduction,\nMime is provably faster than any centralized method--the first such result. We\nalso perform a thorough experimental exploration of Mime's performance on real\nworld datasets.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 21:55:07 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 08:14:57 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Karimireddy", "Sai Praneeth", ""], ["Jaggi", "Martin", ""], ["Kale", "Satyen", ""], ["Mohri", "Mehryar", ""], ["Reddi", "Sashank J.", ""], ["Stich", "Sebastian U.", ""], ["Suresh", "Ananda Theertha", ""]]}, {"id": "2008.03715", "submitter": "Chirag Raman", "authors": "Chirag Raman, Stephanie Tan, Hayley Hung", "title": "A Modular Approach for Synchronized Wireless Multimodal Multisensor Data\n  Acquisition in Highly Dynamic Social Settings", "comments": "9 pages, 8 figures, Proceedings of the 28th ACM International\n  Conference on Multimedia (MM '20), October 12--16, 2020, Seattle, WA, USA.\n  First two authors contributed equally", "journal-ref": null, "doi": "10.1145/3394171.3413697", "report-no": null, "categories": "eess.SP cs.DC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing data acquisition literature for human behavior research provides\nwired solutions, mainly for controlled laboratory setups. In uncontrolled\nfree-standing conversation settings, where participants are free to walk\naround, these solutions are unsuitable. While wireless solutions are employed\nin the broadcasting industry, they can be prohibitively expensive. In this\nwork, we propose a modular and cost-effective wireless approach for\nsynchronized multisensor data acquisition of social human behavior. Our core\nidea involves a cost-accuracy trade-off by using Network Time Protocol (NTP) as\na source reference for all sensors. While commonly used as a reference in\nubiquitous computing, NTP is widely considered to be insufficiently accurate as\na reference for video applications, where Precision Time Protocol (PTP) or\nGlobal Positioning System (GPS) based references are preferred. We argue and\nshow, however, that the latency introduced by using NTP as a source reference\nis adequate for human behavior research, and the subsequent cost and modularity\nbenefits are a desirable trade-off for applications in this domain. We also\ndescribe one instantiation of the approach deployed in a real-world experiment\nto demonstrate the practicality of our setup in-the-wild.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 12:31:05 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Raman", "Chirag", ""], ["Tan", "Stephanie", ""], ["Hung", "Hayley", ""]]}, {"id": "2008.03909", "submitter": "Laxman Dhulipala", "authors": "Laxman Dhulipala, Changwan Hong, Julian Shun", "title": "ConnectIt: A Framework for Static and Incremental Parallel Graph\n  Connectivity Algorithms", "comments": "This is an extended version of a paper in PVLDB (to be presented at\n  VLDB'21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Connected components is a fundamental kernel in graph applications. The\nfastest existing parallel multicore algorithms for connectivity are based on\nsome form of edge sampling and/or linking and compressing trees. However, many\ncombinations of these design choices have been left unexplored. In this paper,\nwe design the ConnectIt framework, which provides different sampling strategies\nas well as various tree linking and compression schemes. ConnectIt enables us\nto obtain several hundred new variants of connectivity algorithms, most of\nwhich extend to computing spanning forest. In addition to static graphs, we\nalso extend ConnectIt to support mixes of insertions and connectivity queries\nin the concurrent setting.\n  We present an experimental evaluation of ConnectIt on a 72-core machine,\nwhich we believe is the most comprehensive evaluation of parallel connectivity\nalgorithms to date. Compared to a collection of state-of-the-art static\nmulticore algorithms, we obtain an average speedup of 12.4x (2.36x average\nspeedup over the fastest existing implementation for each graph). Using\nConnectIt, we are able to compute connectivity on the largest\npublicly-available graph (with over 3.5 billion vertices and 128 billion edges)\nin under 10 seconds using a 72-core machine, providing a 3.1x speedup over the\nfastest existing connectivity result for this graph, in any computational\nsetting. For our incremental algorithms, we show that our algorithms can ingest\ngraph updates at up to several billion edges per second. To guide the user in\nselecting the best variants in ConnectIt for different situations, we provide a\ndetailed analysis of the different strategies. Finally, we show how the\ntechniques in ConnectIt can be used to speed up two important graph\napplications: approximate minimum spanning forest and SCAN clustering.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 05:49:27 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 09:26:09 GMT"}, {"version": "v3", "created": "Sun, 28 Feb 2021 07:18:18 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Dhulipala", "Laxman", ""], ["Hong", "Changwan", ""], ["Shun", "Julian", ""]]}, {"id": "2008.04148", "submitter": "Zachary Langley", "authors": "Sepehr Assadi, Aaron Bernstein, Zachary Langley", "title": "Improved Bounds for Distributed Load Balancing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the load balancing problem, the input is an $n$-vertex bipartite graph $G\n= (C \\cup S, E)$ and a positive weight for each client $c \\in C$. The algorithm\nmust assign each client $c \\in C$ to an adjacent server $s \\in S$. The load of\na server is then the weighted sum of all the clients assigned to it, and the\ngoal is to compute an assignment that minimizes some function of the server\nloads, typically either the maximum server load (i.e., the\n$\\ell_{\\infty}$-norm) or the $\\ell_p$-norm of the server loads.\n  We study load balancing in the distributed setting. There are two existing\nresults in the CONGEST model. Czygrinow et al. [DISC 2012] showed a\n2-approximation for unweighted clients with round-complexity $O(\\Delta^5)$,\nwhere $\\Delta$ is the maximum degree of the input graph. Halld\\'orsson et al.\n[SPAA 2015] showed an $O(\\log{n}/\\log\\log{n})$-approximation for unweighted\nclients and $O(\\log^2\\!{n}/\\log\\log{n})$-approximation for weighted clients\nwith round-complexity polylog$(n)$.\n  In this paper, we show the first distributed algorithms to compute an\n$O(1)$-approximation to the load balancing problem in polylog$(n)$ rounds. In\nthe CONGEST model, we give an $O(1)$-approximation algorithm in polylog$(n)$\nrounds for unweighted clients. For weighted clients, the approximation ratio is\n$O(\\log{n})$. In the less constrained LOCAL model, we give an\n$O(1)$-approximation algorithm for weighted clients in polylog$(n)$ rounds.\n  Our approach also has implications for the standard sequential setting in\nwhich we obtain the first $O(1)$-approximation for this problem that runs in\nnear-linear time. A 2-approximation is already known, but it requires solving a\nlinear program and is hence much slower. Finally, we note that all of our\nresults simultaneously approximate all $\\ell_p$-norms, including the\n$\\ell_{\\infty}$-norm.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 14:21:25 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 02:47:43 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Assadi", "Sepehr", ""], ["Bernstein", "Aaron", ""], ["Langley", "Zachary", ""]]}, {"id": "2008.04164", "submitter": "Alexander Willner", "authors": "Alexander Willner, Varun Gowtham", "title": "Towards a Reference Architecture Model for Industrial Edge Computing", "comments": "10 pages, 6 figures, 15 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of the digital transformation of the industry, whole value\nchains get connected across various application domains; as long as economic,\necologic, or social benefits arise to do so. Under the umbrella of the\nIndustrial Internet of Things (IIoT), traditional Operational Technology (OT)\napproaches are replaced or at least augmented by Information and Communication\nTechnology (ICT) systems to facilitate this development. To meet industrial\nrequirements, for example, related to privacy, determinism, latency, or\nautonomy, established Cloud Computing mechanisms are being moved closer to data\nsources and actuators. Depending on the context, this distributed Cloud\nComputing paradigm is named Edge Computing or Fog Computing and various\nchallenges have been subject to several publications. However, a proper\nreference model that describes the multi-dimensional problem space which is\nbeing spanned by this paradigm, seems still to be undefined. Such a model\nshould provide orientation, put work in relation and support the identification\nof current and future research issues. This paper aims to fill this gap with a\nfocus on industrial automation and follows analog models that have been\ndeveloped for specific domains such as the Smart Grid Architecture Model (SGAM)\nand the Reference Architecture Model Industrie 4.0 (RAMI4.0). The proposed\nReference Architecture Model Edge Computing (RAMEC) identifies 210 views on the\nEdge Computing paradigm in the manufacturing domain. Future iterations of this\nmodel might be used for the classification of relevant research,\nstandardization, and development activities.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 14:45:06 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Willner", "Alexander", ""], ["Gowtham", "Varun", ""]]}, {"id": "2008.04167", "submitter": "Alexey Gotsman", "authors": "Manuel Bravo, Gregory Chockler, Alexey Gotsman", "title": "Making Byzantine Consensus Live (Extended Version)", "comments": "Extended version of a paper from DISC'20: International Symposium on\n  Distributed Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partially synchronous Byzantine consensus protocols typically structure their\nexecution into a sequence of views, each with a designated leader process. The\nkey to guaranteeing liveness in these protocols is to ensure that all correct\nprocesses eventually overlap in a view with a correct leader for long enough to\nreach a decision. We propose a simple view synchronizer abstraction that\nencapsulates the corresponding functionality for Byzantine consensus protocols,\nthus simplifying their design. We present a formal specification of a view\nsynchronizer and its implementation under partial synchrony, which runs in\nbounded space despite tolerating message loss during asynchronous periods. We\nshow that our synchronizer specification is strong enough to guarantee liveness\nfor single-shot versions of several well-known Byzantine consensus protocols,\nincluding HotStuff, Tendermint, PBFT and SBFT. We furthermore give precise\nlatency bounds for these protocols when using our synchronizer. By factoring\nout the functionality of view synchronization we are able to specify and\nanalyze the protocols in a uniform framework, which allows comparing them and\nhighlights trade-offs.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 14:47:59 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Bravo", "Manuel", ""], ["Chockler", "Gregory", ""], ["Gotsman", "Alexey", ""]]}, {"id": "2008.04252", "submitter": "Yuval Emek", "authors": "Xavier D\\'efago, Yuval Emek, Shay Kutten, Toshimitsu Masuzawa,\n  Yasumasa Tamura", "title": "Communication Efficient Self-Stabilizing Leader Election (Full Version)", "comments": "An extended abstract version of this manuscript has been accepted for\n  publication in DISC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a randomized self-stabilizing algorithm that elects a\nleader $r$ in a general $n$-node undirected graph and constructs a spanning\ntree $T$ rooted at $r$. The algorithm works under the synchronous message\npassing network model, assuming that the nodes know a linear upper bound on $n$\nand that each edge has a unique ID known to both its endpoints (or,\nalternatively, assuming the $KT_{1}$ model). The highlight of this algorithm is\nits superior communication efficiency: It is guaranteed to send a total of\n$\\tilde{O} (n)$ messages, each of constant size, till stabilization, while\nstabilizing in $\\tilde{O} (n)$ rounds, in expectation and with high\nprobability. After stabilization, the algorithm sends at most one constant size\nmessage per round while communicating only over the ($n - 1$) edges of $T$. In\nall these aspects, the communication overhead of the new algorithm is far\nsmaller than that of the existing (mostly deterministic) self-stabilizing\nleader election algorithms.\n  The algorithm is relatively simple and relies mostly on known modules that\nare common in the fault free leader election literature; these modules are\nenhanced in various subtle ways in order to assemble them into a communication\nefficient self-stabilizing algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 16:49:15 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2020 11:34:20 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["D\u00e9fago", "Xavier", ""], ["Emek", "Yuval", ""], ["Kutten", "Shay", ""], ["Masuzawa", "Toshimitsu", ""], ["Tamura", "Yasumasa", ""]]}, {"id": "2008.04296", "submitter": "Yuanhao Wei", "authors": "Guy E. Blelloch, Yuanhao Wei", "title": "Concurrent Fixed-Size Allocation and Free in Constant Time", "comments": "To be published as a brief announcement in DISC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to efficiently solve the dynamic memory allocation problem in a\nconcurrent setting where processes run asynchronously. On $p$ processes, we can\nsupport allocation and free for fixed-sized blocks with $O(1)$ worst-case time\nper operation, $\\Theta(p^2)$ additive space overhead, and using only\nsingle-word read, write, and CAS. While many algorithms rely on having\nconstant-time fixed-size allocate and free, we present the first implementation\nof these two operations that is constant time with reasonable space overhead.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 17:47:35 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Blelloch", "Guy E.", ""], ["Wei", "Yuanhao", ""]]}, {"id": "2008.04303", "submitter": "Yannic Maus", "authors": "Magnus M. Halldorsson, Fabian Kuhn, Yannic Maus, Alexandre Nolin", "title": "Coloring Fast Without Learning Your Neighbors' Colors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an improved randomized CONGEST algorithm for distance-$2$ coloring\nthat uses $\\Delta^2+1$ colors and runs in $O(\\log n)$ rounds, improving the\nrecent $O(\\log \\Delta \\cdot \\log n)$-round algorithm in [Halld\\'orsson, Kuhn,\nMaus; PODC '20]. We then improve the time complexity to $O(\\log \\Delta) +\n2^{O(\\sqrt{\\log\\log n})}$.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 17:55:28 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Halldorsson", "Magnus M.", ""], ["Kuhn", "Fabian", ""], ["Maus", "Yannic", ""], ["Nolin", "Alexandre", ""]]}, {"id": "2008.04395", "submitter": "Steven W. D. Chien", "authors": "Steven W. D. Chien, Artur Podobas, Ivy B. Peng, Stefano Markidis", "title": "tf-Darshan: Understanding Fine-grained I/O Performance in Machine\n  Learning Workloads", "comments": "Accepted for publication at the 2020 International Conference on\n  Cluster Computing (CLUSTER 2020)", "journal-ref": null, "doi": "10.1109/CLUSTER49012.2020.00046", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning applications on HPC systems have been gaining popularity in\nrecent years. The upcoming large scale systems will offer tremendous\nparallelism for training through GPUs. However, another heavy aspect of Machine\nLearning is I/O, and this can potentially be a performance bottleneck.\nTensorFlow, one of the most popular Deep-Learning platforms, now offers a new\nprofiler interface and allows instrumentation of TensorFlow operations.\nHowever, the current profiler only enables analysis at the TensorFlow platform\nlevel and does not provide system-level information. In this paper, we extend\nTensorFlow Profiler and introduce tf-Darshan, both a profiler and tracer, that\nperforms instrumentation through Darshan. We use the same Darshan shared\ninstrumentation library and implement a runtime attachment without using a\nsystem preload. We can extract Darshan profiling data structures during\nTensorFlow execution to enable analysis through the TensorFlow profiler. We\nvisualize the performance results through TensorBoard, the web-based TensorFlow\nvisualization tool. At the same time, we do not alter Darshan's existing\nimplementation. We illustrate tf-Darshan by performing two case studies on\nImageNet image and Malware classification. We show that by guiding optimization\nusing data from tf-Darshan, we increase POSIX I/O bandwidth by up to 19% by\nselecting data for staging on fast tier storage. We also show that Darshan has\nthe potential of being used as a runtime library for profiling and providing\ninformation for future optimization.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 20:09:09 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2020 00:40:35 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Chien", "Steven W. D.", ""], ["Podobas", "Artur", ""], ["Peng", "Ivy B.", ""], ["Markidis", "Stefano", ""]]}, {"id": "2008.04397", "submitter": "Steven W. D. Chien", "authors": "Steven W. D. Chien, Jonas Nylund, Gabriel Bengtsson, Ivy B. Peng,\n  Artur Podobas, Stefano Markidis", "title": "sputniPIC: an Implicit Particle-in-Cell Code for Multi-GPU Systems", "comments": "Accepted for publication at the 32nd International Symposium on\n  Computer Architecture and High Performance Computing (SBAC-PAD 2020)", "journal-ref": null, "doi": "10.1109/SBAC-PAD49847.2020.00030", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale simulations of plasmas are essential for advancing our\nunderstanding of fusion devices, space, and astrophysical systems.\nParticle-in-Cell (PIC) codes have demonstrated their success in simulating\nnumerous plasma phenomena on HPC systems. Today, flagship supercomputers\nfeature multiple GPUs per compute node to achieve unprecedented computing power\nat high power efficiency. PIC codes require new algorithm design and\nimplementation for exploiting such accelerated platforms. In this work, we\ndesign and optimize a three-dimensional implicit PIC code, called sputniPIC, to\nrun on a general multi-GPU compute node. We introduce a particle decomposition\ndata layout, in contrast to domain decomposition on CPU-based implementations,\nto use particle batches for overlapping communication and computation on GPUs.\nsputniPIC also natively supports different precision representations to achieve\nspeed up on hardware that supports reduced precision. We validate sputniPIC\nthrough the well-known GEM challenge and provide performance analysis. We test\nsputniPIC on three multi-GPU platforms and report a 200-800x performance\nimprovement with respect to the sputniPIC CPU OpenMP version performance. We\nshow that reduced precision could further improve performance by 45% to 80% on\nthe three platforms. Because of these performance improvements, on a single\nnode with multiple GPUs, sputniPIC enables large-scale three-dimensional PIC\nsimulations that were only possible using clusters.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 20:10:23 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Chien", "Steven W. D.", ""], ["Nylund", "Jonas", ""], ["Bengtsson", "Gabriel", ""], ["Peng", "Ivy B.", ""], ["Podobas", "Artur", ""], ["Markidis", "Stefano", ""]]}, {"id": "2008.04424", "submitter": "Miguel Angel Pi\\~na Avelino", "authors": "Armando Casta\\~neda and Miguel Pi\\~na", "title": "Fully Read/Write Fence-Free Work-Stealing with Multiplicity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Work-stealing is a popular technique to implement dynamic load balancing in a\ndistributed manner. In this approach, each process owns a set of tasks that\nhave to be executed. The owner of the set can put tasks in it and can take\ntasks from it to execute them. When a process runs out of tasks, instead of\nbeing idle, it becomes a thief to steal tasks from a victim. Thus, a\nwork-stealing algorithm provides three high-level operations: Put and Take,\nwhich can be invoked only by the owner, and Steal, which can be invoked by a\nthief. One of the main targets when designing work-stealing algorithms is to\nmake Put and Take as simple and efficient as possible. Unfortunately, it has\nbeen shown that any work-stealing algorithm in the standard asynchronous model\nmust use expensive Read- After-Write synchronization patterns or atomic\nRead-Modify-Write instructions, which may be costly in practice. Thus, prior\nresearch has proposed idempotent work-stealing, a relaxation for which there\nare algorithms with Put and Take devoid of Read-Modify-Write atomic\ninstructions and Read-After-Write synchronization patterns; however, Put uses\nfences among Write instructions, and Steal uses Compare&Swap and fences among\nRead instructions. In the TSO model, in which Write (resp. Read) instructions\ncannot be reordered, there have been proposed fully fence-free work-stealing\nalgorithms whose Put and Take have similar properties but Steal uses\nCompare&Swap or a lock.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 21:30:33 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 18:45:33 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Casta\u00f1eda", "Armando", ""], ["Pi\u00f1a", "Miguel", ""]]}, {"id": "2008.04450", "submitter": "Jelle Hellings", "authors": "Jelle Hellings and Daniel P. Hughes and Joshua Primero and Mohammad\n  Sadoghi", "title": "Cerberus: Minimalistic Multi-shard Byzantine-resilient Transaction\n  Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To enable high-performance and scalable blockchains, we need to step away\nfrom traditional consensus-based fully-replicated designs. One direction is to\nexplore the usage of sharding in which we partition the managed dataset over\nmany shards that independently operate as blockchains. Sharding requires an\nefficient fault-tolerant primitive for the ordering and execution of\nmulti-shard transactions, however.\n  In this work, we seek to design such a primitive suitable for distributed\nledger networks with high transaction throughput. To do so, we propose\nCerberus, a set of minimalistic primitives for processing single-shard and\nmulti-shard UTXO-like transactions. Cerberus aims at maximizing parallel\nprocessing at shards while minimizing coordination within and between shards.\nFirst, we propose Core-Cerberus, that uses strict environmental requirements to\nenable simple yet powerful multi-shard transaction processing. In our intended\nUTXO-environment, Core-Cerberus will operate perfectly with respect to all\ntransactions proposed and approved by well-behaved clients, but does not\nprovide any guarantees for other transactions.\n  To also support more general-purpose environments, we propose two\ngeneralizations of Core-Cerberus: we propose Optimistic-Cerberus, a protocol\nthat does not require any additional coordination phases in the well-behaved\noptimistic case, while requiring intricate coordination when recovering from\nattacks; and we propose Pessimistic-Cerberus, a protocol that adds sufficient\ncoordination to the well-behaved case of Core-Cerberus, allowing it to operate\nin a general-purpose fault-tolerant environments without significant costs to\nrecover from attacks. Finally, we compare the three protocols, showing their\npotential scalability and high transaction throughput in practical\nenvironments.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 23:06:29 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Hellings", "Jelle", ""], ["Hughes", "Daniel P.", ""], ["Primero", "Joshua", ""], ["Sadoghi", "Mohammad", ""]]}, {"id": "2008.04567", "submitter": "Yongchao Liu", "authors": "Yongchao Liu, Yue Jin, Yong Chen, Teng Teng, Hang Ou, Rui Zhao, Yao\n  Zhang", "title": "Woodpecker-DL: Accelerating Deep Neural Networks via Hardware-Aware\n  Multifaceted Optimizations", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accelerating deep model training and inference is crucial in practice.\nExisting deep learning frameworks usually concentrate on optimizing training\nspeed and pay fewer attentions to inference-specific optimizations. Actually,\nmodel inference differs from training in terms of computation, e.g. parameters\nare refreshed each gradient update step during training, but kept invariant\nduring inference. These special characteristics of model inference open new\nopportunities for its optimization. In this paper, we propose a hardware-aware\noptimization framework, namely Woodpecker-DL (WPK), to accelerate inference by\ntaking advantage of multiple joint optimizations from the perspectives of graph\noptimization, automated searches, domain-specific language (DSL) compiler\ntechniques and system-level exploration. In WPK, we investigated two new\nautomated search approaches based on genetic algorithm and reinforcement\nlearning, respectively, to hunt the best operator code configurations targeting\nspecific hardware. A customized DSL compiler is further attached to these\nsearch algorithms to generate efficient codes. To create an optimized inference\nplan, WPK systematically explores high-speed operator implementations from\nthird-party libraries besides our automatically generated codes and singles out\nthe best implementation per operator for use. Extensive experiments\ndemonstrated that on a Tesla P100 GPU, we can achieve the maximum speedup of\n5.40 over cuDNN and 1.63 over TVM on individual convolution operators, and run\nup to 1.18 times faster than TensorRT for end-to-end model inference.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 07:50:34 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Liu", "Yongchao", ""], ["Jin", "Yue", ""], ["Chen", "Yong", ""], ["Teng", "Teng", ""], ["Ou", "Hang", ""], ["Zhao", "Rui", ""], ["Zhang", "Yao", ""]]}, {"id": "2008.04612", "submitter": "Yehuda Afek Prof", "authors": "Shahar Azulay, Lior Raz, Amir Globerson, Tomer Koren, Yehuda Afek", "title": "Holdout SGD: Byzantine Tolerant Federated Learning", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a new distributed Byzantine tolerant federated learning\nalgorithm, HoldOut SGD, for Stochastic Gradient Descent (SGD) optimization.\nHoldOut SGD uses the well known machine learning technique of holdout\nestimation, in a distributed fashion, in order to select parameter updates that\nare likely to lead to models with low loss values. This makes it more effective\nat discarding Byzantine workers inputs than existing methods that eliminate\noutliers in the parameter-space of the learned model. HoldOut SGD first\nrandomly selects a set of workers that use their private data in order to\npropose gradient updates. Next, a voting committee of workers is randomly\nselected, and each voter uses its private data as holdout data, in order to\nselect the best proposals via a voting scheme. We propose two possible\nmechanisms for the coordination of workers in the distributed computation of\nHoldOut SGD. The first uses a truthful central server and corresponds to the\ntypical setting of current federated learning. The second is fully distributed\nand requires no central server, paving the way to fully decentralized federated\nlearning. The fully distributed version implements HoldOut SGD via ideas from\nthe blockchain domain, and specifically the Algorand committee selection and\nconsensus processes. We provide formal guarantees for the HoldOut SGD process\nin terms of its convergence to the optimal model, and its level of resilience\nto the fraction of Byzantine workers. Empirical evaluation shows that HoldOut\nSGD is Byzantine-resilient and efficiently converges to an effectual model for\ndeep-learning tasks, as long as the total number of participating workers is\nlarge and the fraction of Byzantine workers is less than half (<1/3 for the\nfully distributed variant).\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 10:16:37 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Azulay", "Shahar", ""], ["Raz", "Lior", ""], ["Globerson", "Amir", ""], ["Koren", "Tomer", ""], ["Afek", "Yehuda", ""]]}, {"id": "2008.04674", "submitter": "Hossein Ahmadvand", "authors": "Hossein Ahmadvand and Fouzhan Foroutan", "title": "DV-ARPA: Data Variety Aware Resource Provisioning for Big Data\n  Processing in Accumulative Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Cloud Computing, the resource provisioning approach used has a great\nimpact on the processing cost, especially when it is used for Big Data\nprocessing. Due to data variety, the performance of virtual machines (VM) may\ndiffer based on the contents of the data blocks. Data variety-oblivious\nallocation causes a reduction in the performance of VMs and increases the\nprocessing cost. Thus, it is possible to reduce the total cost of the job by\nmatching the VMs with the given data blocks. We use a data-variety-aware\nresource allocation approach to reduce the processing cost of the considered\njob. For this issue, we divide the input data into some data blocks. We define\nthe significance of each data block and based on it we choose the appropriate\nVMs to reduce the cost. For detecting the significance of each data portion, we\nuse a specific sampling method. This approach is applicable to accumulative\napplications. We use some well-known benchmarks and configured servers for our\nevaluations. Based on the results, our provisioning approach improves the\nprocessing cost, up to 35% compared to other approaches.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 12:54:23 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Ahmadvand", "Hossein", ""], ["Foroutan", "Fouzhan", ""]]}, {"id": "2008.04699", "submitter": "Shuo Liu", "authors": "Nirupam Gupta, Shuo Liu and Nitin H. Vaidya", "title": "Byzantine Fault-Tolerant Distributed Machine Learning Using Stochastic\n  Gradient Descent (SGD) and Norm-Based Comparative Gradient Elimination (CGE)", "comments": "The report includes 52 pages, and 16 figures. Extension of our prior\n  work on Byzantine fault-tolerant distribution optimization (arXiv:1903.08752\n  and doi:10.1145/3382734.3405748) to Byzantine fault-tolerant distributed\n  machine learning; Updated to the full version of workshop paper in DSN-DSML\n  '21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the Byzantine fault-tolerance problem in distributed\nstochastic gradient descent (D-SGD) method - a popular algorithm for\ndistributed multi-agent machine learning. In this problem, each agent samples\ndata points independently from a certain data-generating distribution. In the\nfault-free case, the D-SGD method allows all the agents to learn a mathematical\nmodel best fitting the data collectively sampled by all agents. We consider the\ncase when a fraction of agents may be Byzantine faulty. Such faulty agents may\nnot follow a prescribed algorithm correctly, and may render traditional D-SGD\nmethod ineffective by sharing arbitrary incorrect stochastic gradients. We\npropose a norm-based gradient-filter, named comparative gradient elimination\n(CGE), that robustifies the D-SGD method against Byzantine agents. We show that\nthe CGE gradient-filter guarantees fault-tolerance against a bounded fraction\nof Byzantine agents under standard stochastic assumptions, and is\ncomputationally simpler compared to many existing gradient-filters such as\nmulti-KRUM, geometric median-of-means, and the spectral filters. We empirically\nshow, by simulating distributed learning on neural networks, that the\nfault-tolerance of CGE is comparable to that of existing gradient-filters. We\nalso empirically show that exponential averaging of stochastic gradients\nimproves the fault-tolerance of a generic gradient-filter.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 13:51:16 GMT"}, {"version": "v2", "created": "Sun, 18 Apr 2021 00:56:13 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Gupta", "Nirupam", ""], ["Liu", "Shuo", ""], ["Vaidya", "Nitin H.", ""]]}, {"id": "2008.04830", "submitter": "Pawe{\\l} \\.Zuk", "authors": "Pawel Zuk, Krzysztof Rzadca", "title": "Scheduling Methods to Reduce Response Latency of Function as a Service", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Function as a Service (FaaS) permits cloud customers to deploy to cloud\nindividual functions, in contrast to complete virtual machines or Linux\ncontainers. All major cloud providers offer FaaS products (Amazon Lambda,\nGoogle Cloud Functions, Azure Serverless); there are also popular open-source\nimplementations (Apache OpenWhisk) with commercial offerings (Adobe I/O\nRuntime, IBM Cloud Functions). A new feature of FaaS is function composition: a\nfunction may (sequentially) call another function, which, in turn, may call yet\nanother function - forming a chain of invocations. From the perspective of the\ninfrastructure, a composed FaaS is less opaque than a virtual machine or a\ncontainer. We show that this additional information enables the infrastructure\nto reduce the response latency. In particular, knowing the sequence of future\ninvocations, the infrastructure can schedule these invocations along with\nenvironment preparation. We model resource management in FaaS as a scheduling\nproblem combining (1) sequencing of invocations, (2) deploying execution\nenvironments on machines, and (3) allocating invocations to deployed\nenvironments. For each aspect, we propose heuristics. We explore their\nperformance by simulation on a range of synthetic workloads. Our results show\nthat if the setup times are long compared to invocation times, algorithms that\nuse information about the composition of functions consistently outperform\ngreedy, myopic algorithms, leading to significant decrease in response latency.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 16:25:16 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Zuk", "Pawel", ""], ["Rzadca", "Krzysztof", ""]]}, {"id": "2008.05070", "submitter": "Yongjiang He", "authors": "Yongjiang He", "title": "Research on the construction method of vehicle driving cycle based on\n  Mean Shift clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, a novel method for the construction of a driving cycle based\non Mean Shift clustering is proposed to solve the problems existing in the\ntraditional micro-trips method. Firstly, 1701 kinematic segments are obtained\nby processing and dividing the driving data in real road conditions. Secondly,\n12 kinematic parameters are calculated for each segment, and the dimensionality\nof parameters is reduced through principal component analysis (PCA). Three\nprincipal components are chosen to classify all cycles into three types by the\nMean Shift algorithm. Finally, according to the principle of minimum deviation,\nrepresentative micro-trips are selected from each type of cycle to complete the\nconstruction of the final driving cycle. Further, the construction method in\nthis paper is compared with the micro-trips construction method by the K-Means\nclustering. The results show that the construction method by Mean Shift\nclustering can more effectively reflect the real driving data. This study\nrealizes the innovation in the construction method of micro-trips and provides\na preliminary theoretical basis for the formulation of automobile working\ncondition standards, energy management of new-energy vehicles, and optimal\ncontrol of vehicle dynamics in driverless vehicles.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 02:16:49 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["He", "Yongjiang", ""]]}, {"id": "2008.05141", "submitter": "Mingyue Ji", "authors": "Nicholas Woolsey, Rong-Rong Chen, Mingyue Ji", "title": "Coded Elastic Computing on Machines with Heterogeneous Storage and\n  Computation Speed", "comments": "30 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the optimal design of heterogeneous Coded Elastic Computing (CEC)\nwhere machines have varying computation speeds and storage. CEC introduced by\nYang et al. in 2018 is a framework that mitigates the impact of elastic events,\nwhere machines can join and leave at arbitrary times. In CEC, data is\ndistributed among machines using a Maximum Distance Separable (MDS) code such\nthat subsets of machines can perform the desired computations. However,\nstate-of-the-art CEC designs only operate on homogeneous networks where\nmachines have the same speeds and storage. This may not be practical. In this\nwork, based on an MDS storage assignment, we develop a novel computation\nassignment approach for heterogeneous CEC networks to minimize the overall\ncomputation time. We first consider the scenario where machines have\nheterogeneous computing speeds but same storage and then the scenario where\nboth heterogeneities are present. We propose a novel combinatorial optimization\nformulation and solve it exactly by decomposing it into a convex optimization\nproblem for finding the optimal computation load and a \"filling problem\" for\nfinding the exact computation assignment. A low-complexity \"filling algorithm\"\nis adapted and can be completed within a number of iterations equals at most\nthe number of available machines.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 07:24:39 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Woolsey", "Nicholas", ""], ["Chen", "Rong-Rong", ""], ["Ji", "Mingyue", ""]]}, {"id": "2008.05189", "submitter": "Latif U. Khan", "authors": "Latif U. Khan, Walid Saad, Zhu Han, and Choong Seon Hong", "title": "Dispersed Federated Learning: Vision, Taxonomy, and Future Directions", "comments": "This paper has been accepted for publication in IEEE Wireless\n  Communications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ongoing deployment of the Internet of Things (IoT)-based smart\napplications is spurring the adoption of machine learning as a key technology\nenabler. To overcome the privacy and overhead challenges of centralized machine\nlearning, there has been a significant recent interest in the concept of\nfederated learning. Federated learning offers on-device, privacy-preserving\nmachine learning without the need to transfer end-devices data to a third party\nlocation. However, federated learning still has privacy concerns due to\nsensitive information inferring capability of the aggregation server using\nend-devices local learning models. Furthermore, the federated learning process\nmight fail due to a failure in the aggregation server (e.g., due to a malicious\nattack or physical defect). Other than privacy and robustness issues, federated\nlearning over IoT networks requires a significant amount of communication\nresources for training. To cope with these issues, we propose a novel concept\nof dispersed federated learning (DFL) that is based on the true\ndecentralization. We opine that DFL will serve as a practical implementation of\nfederated learning for various IoT-based smart applications such as smart\nindustries and intelligent transportation systems. First, the fundamentals of\nthe DFL are presented. Second, a taxonomy is devised with a qualitative\nanalysis of various DFL schemes. Third, a DFL framework for IoT networks is\nproposed with a matching theory-based solution. Finally, an outlook on future\nresearch directions is presented.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 09:12:04 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 02:44:45 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Khan", "Latif U.", ""], ["Saad", "Walid", ""], ["Han", "Zhu", ""], ["Hong", "Choong Seon", ""]]}, {"id": "2008.05255", "submitter": "Jiangkai Wu", "authors": "Zichuan Xu, Jiangkai Wu, Qiufen Xia, Pan Zhou, Jiankang Ren, Huizhi\n  Liang", "title": "Identity-Aware Attribute Recognition via Real-Time Distributed Inference\n  in Mobile Edge Clouds", "comments": "9 pages, 8 figures, Proceedings of the 28th ACM International\n  Conference on Multimedia (ACM MM'20), Seattle, WA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of deep learning technologies, attribute recognition and\nperson re-identification (re-ID) have attracted extensive attention and\nachieved continuous improvement via executing computing-intensive deep neural\nnetworks in cloud datacenters. However, the datacenter deployment cannot meet\nthe real-time requirement of attribute recognition and person re-ID, due to the\nprohibitive delay of backhaul networks and large data transmissions from\ncameras to datacenters. A feasible solution thus is to employ mobile edge\nclouds (MEC) within the proximity of cameras and enable distributed inference.\nIn this paper, we design novel models for pedestrian attribute recognition with\nre-ID in an MEC-enabled camera monitoring system. We also investigate the\nproblem of distributed inference in the MEC-enabled camera network. To this\nend, we first propose a novel inference framework with a set of distributed\nmodules, by jointly considering the attribute recognition and person re-ID. We\nthen devise a learning-based algorithm for the distributions of the modules of\nthe proposed distributed inference framework, considering the dynamic\nMEC-enabled camera network with uncertainties. We finally evaluate the\nperformance of the proposed algorithm by both simulations with real datasets\nand system implementation in a real testbed. Evaluation results show that the\nperformance of the proposed algorithm with distributed inference framework is\npromising, by reaching the accuracies of attribute recognition and person\nidentification up to 92.9% and 96.6% respectively, and significantly reducing\nthe inference delay by at least 40.6% compared with existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 12:03:27 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Xu", "Zichuan", ""], ["Wu", "Jiangkai", ""], ["Xia", "Qiufen", ""], ["Zhou", "Pan", ""], ["Ren", "Jiankang", ""], ["Liang", "Huizhi", ""]]}, {"id": "2008.05631", "submitter": "Mingyue Ji", "authors": "Nicholas Woolsey, Xingyue Wang, Rong-Rong Chen, Mingyue Ji", "title": "FLCD: A Flexible Low Complexity Design of Coded Distributed Computing", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a flexible low complexity design (FLCD) of coded distributed\ncomputing (CDC) with empirical evaluation on Amazon Elastic Compute Cloud\n(Amazon EC2). CDC can expedite MapReduce like computation by trading increased\nmap computations to reduce communication load and shuffle time. A main novelty\nof FLCD is to utilize the design freedom in defining map and reduce functions\nto develop asymptotic homogeneous systems to support varying intermediate\nvalues (IV) sizes under a general MapReduce framework. Compared to existing\ndesigns with constant IV sizes, FLCD offers greater flexibility in adapting to\nnetwork parameters and significantly reduces the implementation complexity by\nrequiring fewer input files and shuffle groups. The FLCD scheme is the first\nproposed low-complexity CDC design that can operate on a network with an\narbitrary number of nodes and computation load. We perform empirical\nevaluations of the FLCD by executing the TeraSort algorithm on an Amazon EC2\ncluster. This is the first time that theoretical predictions of the CDC shuffle\ntime are validated by empirical evaluations. The evaluations demonstrate a 2.0\nto 4.24x speedup compared to conventional uncoded MapReduce, a 12% to 52%\nreduction in total time, and a wider range of operating network parameters\ncompared to existing CDC schemes.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 01:07:58 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Woolsey", "Nicholas", ""], ["Wang", "Xingyue", ""], ["Chen", "Rong-Rong", ""], ["Ji", "Mingyue", ""]]}, {"id": "2008.05683", "submitter": "Amir Hossein Zolfaghari", "authors": "Amir Hussain Zolfaghari (1), Herbert Daly (2), Mahdi Nasiri (3),\n  Roxana Sharifian (4) ((1) School of Management and Medical Information\n  Sciences, Shiraz University of Medical Sciences, Shiraz, Iran, (2) University\n  of Wolverhampton, UK, (3) School of Management and Medical Information\n  Sciences, Shiraz University of Medical Sciences, Shiraz, Iran, (4) Health\n  Human Resource Research Center, Shiraz University of Medical Sciences,\n  Shiraz, Iran)", "title": "Blockchain applications in Healthcare: A model for research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain technology has rapidly evolved from an enabling technology for\ncryptocurrencies to a potential solution to a wider range of problems found in\ndata-centric and distributed systems. Interest in this area has encouraged many\nrecent innovations to address challenges that traditional approaches of design\nhave been unable to meet. Healthcare Information Systems with issues around\nprivacy, interoperability, data integrity, and access control is potentially an\narea where blockchain technology may have a significant impact. Blockchain,\nhowever, is a meta-technology, combining multiple techniques, as it is often\nimportant to determine how best to separate concerns in the design and\nimplementation of such systems. This paper proposes a layered approach for the\norganization of blockchain in healthcare applications. Key issues driving the\nadoption of this technology are explored. A model presenting the points in each\nlayer is explored. Finally, we present an example of how the perspective we\ndescribe can improve the development of Health Information Systems.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 04:11:30 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Zolfaghari", "Amir Hussain", ""], ["Daly", "Herbert", ""], ["Nasiri", "Mahdi", ""], ["Sharifian", "Roxana", ""]]}, {"id": "2008.05712", "submitter": "Sathish Vadhiyar", "authors": "Vasudevan Rengasamy and Sathish Vadhiyar", "title": "Strategies for Efficient Executions of Irregular Message-Driven Parallel\n  Applications on GPU Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Message-driven executions with over-decomposition of tasks constitute an\nimportant model for parallel programming and have been demonstrated for\nirregular applications. Supporting efficient execution of such message-driven\nirregular applications on GPU systems presents a number of challenges related\nto irregular data accesses and computations. In this work, we have developed\nstrategies including coalescing irregular data accesses and combining with data\nreuse, and adaptive methods for hybrid executions to minimize idling. We have\nintegrated these runtime strategies into our {\\em G-Charm} framework for\nefficient execution of message-driven parallel applications on hybrid GPU\nsystems. We demonstrate our strategies for irregular applications with an\nN-Body simulations and a molecular dynamics application and show that our\ndynamic strategies result in 8-38\\% reduction in execution times for these\nirregular applications over the corresponding static strategies that are\namenable for regular applications.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 06:29:11 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Rengasamy", "Vasudevan", ""], ["Vadhiyar", "Sathish", ""]]}, {"id": "2008.05718", "submitter": "Sathish Vadhiyar", "authors": "Ashirbad Mishra, Sathish Vadhiyar, Rupesh Nasre, Keshav Pingali", "title": "A Fine-Grained Hybrid CPU-GPU Algorithm for Betweenness Centrality\n  Computations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Betweenness centrality (BC) is an important graph analytical application for\nlarge-scale graphs. While there are many efforts for parallelizing betweenness\ncentrality algorithms on multi-core CPUs and many-core GPUs, in this work, we\npropose a novel fine-grained CPU-GPU hybrid algorithm that partitions a graph\ninto CPU and GPU partitions, and performs BC computations for the graph on both\nthe CPU and GPU resources simultaneously with very small number of CPU-GPU\ncommunications. The forward phase in our hybrid BC algorithm leverages the\nmulti-source property inherent in the BC problem. We also perform a novel\nhybrid and asynchronous backward phase that performs minimal CPU-GPU\nsynchronizations. Evaluations using a large number of graphs with different\ncharacteristics show that our hybrid approach gives 80% improvement in\nperformance, and 80-90% less CPU-GPU communications than an existing hybrid\nalgorithm based on the popular Bulk Synchronous Paradigm (BSP) approach.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 06:41:56 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Mishra", "Ashirbad", ""], ["Vadhiyar", "Sathish", ""], ["Nasre", "Rupesh", ""], ["Pingali", "Keshav", ""]]}, {"id": "2008.05722", "submitter": "Yi-Fan Chung", "authors": "Yi-Fan Chung and Solmaz S. Kia", "title": "Dynamic Active Average Consensus and its Application in Containment\n  Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.DC cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a continuous-time dynamic active weighted average\nconsensus algorithm in which the agents can alternate between active and\npassive modes depending on their ability to access to their reference input.\nThe objective is to enable all the agents, both active and passive, to track\nthe weighted average of the reference inputs of the active agents. The\nalgorithm is modeled as a switched linear system whose convergence properties\nare carefully studied considering the agents' piece-wise constant access to the\nreference signals and possible piece-wise constant weights of the agents. We\nalso study the discrete-time implementation of this algorithm. Next, we show\nhow a containment control problem, in which a group of followers should track\nthe convex hull of a set of observed leaders, can be cast as an active average\nconsensus problem, and solved efficiently by our proposed dynamic active\naverage consensus algorithm. Numerical examples demonstrate our results.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 07:00:43 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Chung", "Yi-Fan", ""], ["Kia", "Solmaz S.", ""]]}, {"id": "2008.05823", "submitter": "An Xu", "authors": "An Xu, Zhouyuan Huo, Heng Huang", "title": "Training Faster with Compressed Gradient", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the distributed machine learning methods show the potential for the\nspeed-up of training large deep neural networks, the communication cost has\nbeen the notorious bottleneck to constrain the performance. To address this\nchallenge, the gradient compression based communication-efficient distributed\nlearning methods were designed to reduce the communication cost, and more\nrecently the local error feedback was incorporated to compensate for the\nperformance loss. However, in this paper, we will show the \"gradient mismatch\"\nproblem of the local error feedback in centralized distributed training and\nthis issue can lead to degraded performance compared with full-precision\ntraining. To solve this critical problem, we propose two novel techniques: 1)\nstep ahead; 2) error averaging. Both our theoretical and empirical results show\nthat our new methods can alleviate the \"gradient mismatch\" problem. Experiments\nshow that we can even train \\textbf{faster with compressed gradient} than\nfull-precision training \\textbf{regarding training epochs}.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 11:21:07 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Xu", "An", ""], ["Huo", "Zhouyuan", ""], ["Huang", "Heng", ""]]}, {"id": "2008.05851", "submitter": "Feng Xia", "authors": "Feng Xia, Fangwei Ding, Jie Li, Xiangjie Kong, Laurence T. Yang,\n  Jianhua Ma", "title": "Phone2Cloud: Exploiting Computation Offloading for Energy Saving on\n  Smartphones in Mobile Cloud Computing", "comments": "16 pages, 13 figures", "journal-ref": "Information Systems Frontiers, 16(1): 95-111, 2014", "doi": "10.1007/s10796-013-9458-1", "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With prosperity of applications on smartphones, energy saving for smartphones\nhas drawn increasing attention. In this paper we devise Phone2Cloud, a\ncomputation offloading-based system for energy saving on smartphones in the\ncontext of mobile cloud computing. Phone2Cloud offloads computation of an\napplication running on smartphones to the cloud. The objective is to improve\nenergy efficiency of smartphones and at the same time, enhance the\napplication's performance through reducing its execution time. In this way, the\nuser's experience can be improved. We implement the prototype of Phone2Cloud on\nAndroid and Hadoop environment. Two sets of experiments, including application\nexperiments and scenario experiments, are conducted to evaluate the system. The\nexperimental results show that Phone2Cloud can effectively save energy for\nsmartphones and reduce the application's execution time.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 04:13:41 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Xia", "Feng", ""], ["Ding", "Fangwei", ""], ["Li", "Jie", ""], ["Kong", "Xiangjie", ""], ["Yang", "Laurence T.", ""], ["Ma", "Jianhua", ""]]}, {"id": "2008.05946", "submitter": "Canhui Wang", "authors": "Canhui Wang, Xiaowen Chu", "title": "Performance Characterization and Bottleneck Analysis of Hyperledger\n  Fabric", "comments": null, "journal-ref": null, "doi": "10.1109/ICDCS47774.2020.00165", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperledger Fabric is a popular open-source project for deploying\npermissioned blockchains. Many performance characteristics of the latest\nHyperledger Fabric, such as performance characteristics of each phase, the\nimpacts of ordering services, and bottleneck and scalability, are still not\nwell understood due to the performance complexity of distributed systems. We\nconducted a thorough performance evaluation on the first long term support\nrelease of Hyperledger Fabric. We studied the performance characteristics of\neach phase, including execute, order, and the validate phase, according to\nHyperledger Fabric new execute-order-validate architecture. We also studied the\nordering services, including Solo, Kafka, and Raft. Our experimental results\nshowed some findings as follows. 1) The execution phase exhibited a good\nscalability under the OR endorsement policy but not with the AND endorsement\npolicy. 2) We were not able to find a significant performance difference\nbetween the three ordering services. 3) The validate phase was likely to be the\nsystem bottleneck due to the low validation speed of chaincode. Overall, our\nwork helps to understand and improve Hyperledger Fabric.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 14:53:17 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 04:21:04 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Wang", "Canhui", ""], ["Chu", "Xiaowen", ""]]}, {"id": "2008.06082", "submitter": "Usman Khan", "authors": "Muhammad I. Qureshi and Ran Xin and Soummya Kar and Usman A. Khan", "title": "Push-SAGA: A decentralized stochastic algorithm with variance reduction\n  over directed graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.MA cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose Push-SAGA, a decentralized stochastic first-order\nmethod for finite-sum minimization over a directed network of nodes. Push-SAGA\ncombines node-level variance reduction to remove the uncertainty caused by\nstochastic gradients, network-level gradient tracking to address the\ndistributed nature of the data, and push-sum consensus to tackle the challenge\nof directed communication links. We show that Push-SAGA achieves linear\nconvergence to the exact solution for smooth and strongly convex problems and\nis thus the first linearly-convergent stochastic algorithm over arbitrary\nstrongly connected directed graphs. We also characterize the regimes in which\nPush-SAGA achieves a linear speed-up compared to its centralized counterpart\nand achieves a network-independent convergence rate. We illustrate the behavior\nand convergence properties of Push-SAGA with the help of numerical experiments\non strongly convex and non-convex problems.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 18:52:17 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 20:46:52 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Qureshi", "Muhammad I.", ""], ["Xin", "Ran", ""], ["Kar", "Soummya", ""], ["Khan", "Usman A.", ""]]}, {"id": "2008.06152", "submitter": "Kazuichi Oe", "authors": "Kazuichi Oe", "title": "Consideration for effectively handling parallel workloads on public\n  cloud system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.OS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We retrieved and analyzed parallel storage workloads of the FUJITSU K5 cloud\nservice to clarify how to build cost-effective hybrid storage systems. A hybrid\nstorage system consists of fast but low-capacity tier (first tier) and slow but\nhigh-capacity tier (second tier). And, it typically consists of either SSDs and\nHDDs or NVMs and SSDs. As a result, we found that 1) regions for first tier\nshould be assigned only if a workload includes large number of IO accesses for\na whole day, 2) the regions that include a large number of IO accesses should\nbe dynamically chosen and moved from second tier to first tier for a short\ninterval, and 3) if a cache hit ratio is regularly low, use of the cache for\nthe workload should be cancelled, and the whole workload region should be\nassigned to the region for first tier. These workloads already have been\nreleased from the SNIA web site.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 01:18:04 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Oe", "Kazuichi", ""]]}, {"id": "2008.06180", "submitter": "Sohei Itahara", "authors": "Sohei Itahara, Takayuki Nishio, Yusuke Koda, Masahiro Morikura and\n  Koji Yamamoto", "title": "Distillation-Based Semi-Supervised Federated Learning for\n  Communication-Efficient Collaborative Training with Non-IID Private Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study develops a federated learning (FL) framework overcoming largely\nincremental communication costs due to model sizes in typical frameworks\nwithout compromising model performance. To this end, based on the idea of\nleveraging an unlabeled open dataset, we propose a distillation-based\nsemi-supervised FL (DS-FL) algorithm that exchanges the outputs of local models\namong mobile devices, instead of model parameter exchange employed by the\ntypical frameworks. In DS-FL, the communication cost depends only on the output\ndimensions of the models and does not scale up according to the model size. The\nexchanged model outputs are used to label each sample of the open dataset,\nwhich creates an additionally labeled dataset. Based on the new dataset, local\nmodels are further trained, and model performance is enhanced owing to the data\naugmentation effect. We further highlight that in DS-FL, the heterogeneity of\nthe devices' dataset leads to ambiguous of each data sample and lowing of the\ntraining convergence. To prevent this, we propose entropy reduction averaging,\nwhere the aggregated model outputs are intentionally sharpened. Moreover,\nextensive experiments show that DS-FL reduces communication costs up to 99%\nrelative to those of the FL benchmark while achieving similar or higher\nclassification accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 03:47:27 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 08:35:43 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Itahara", "Sohei", ""], ["Nishio", "Takayuki", ""], ["Koda", "Yusuke", ""], ["Morikura", "Masahiro", ""], ["Yamamoto", "Koji", ""]]}, {"id": "2008.06402", "submitter": "Stefanos Laskaridis", "authors": "Stefanos Laskaridis, Stylianos I. Venieris, Mario Almeida, Ilias\n  Leontiadis, Nicholas D. Lane", "title": "SPINN: Synergistic Progressive Inference of Neural Networks over Device\n  and Cloud", "comments": "Accepted at the 26th Annual International Conference on Mobile\n  Computing and Networking (MobiCom), 2020", "journal-ref": null, "doi": "10.1145/3372224.3419194", "report-no": null, "categories": "cs.LG cs.CV cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the soaring use of convolutional neural networks (CNNs) in mobile\napplications, uniformly sustaining high-performance inference on mobile has\nbeen elusive due to the excessive computational demands of modern CNNs and the\nincreasing diversity of deployed devices. A popular alternative comprises\noffloading CNN processing to powerful cloud-based servers. Nevertheless, by\nrelying on the cloud to produce outputs, emerging mission-critical and\nhigh-mobility applications, such as drone obstacle avoidance or interactive\napplications, can suffer from the dynamic connectivity conditions and the\nuncertain availability of the cloud. In this paper, we propose SPINN, a\ndistributed inference system that employs synergistic device-cloud computation\ntogether with a progressive inference method to deliver fast and robust CNN\ninference across diverse settings. The proposed system introduces a novel\nscheduler that co-optimises the early-exit policy and the CNN splitting at run\ntime, in order to adapt to dynamic conditions and meet user-defined\nservice-level requirements. Quantitative evaluation illustrates that SPINN\noutperforms its state-of-the-art collaborative inference counterparts by up to\n2x in achieved throughput under varying network conditions, reduces the server\ncost by up to 6.8x and improves accuracy by 20.7% under latency constraints,\nwhile providing robust operation under uncertain connectivity conditions and\nsignificant energy savings compared to cloud-centric execution.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 15:00:19 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2020 10:24:41 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Laskaridis", "Stefanos", ""], ["Venieris", "Stylianos I.", ""], ["Almeida", "Mario", ""], ["Leontiadis", "Ilias", ""], ["Lane", "Nicholas D.", ""]]}, {"id": "2008.06571", "submitter": "Xingfu Wu", "authors": "Xingfu Wu and Aniruddha Marathe and Siddhartha Jana and Ondrej Vysocky\n  and Jophin John and Andrea Bartolini and Lubomir Riha and Michael Gerndt and\n  Valerie Taylor and Sridutt Bhalachandra", "title": "Toward an End-to-End Auto-tuning Framework in HPC PowerStack", "comments": "to be published in Energy Efficient HPC State of Practice 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficiently utilizing procured power and optimizing performance of scientific\napplications under power and energy constraints are challenging. The HPC\nPowerStack defines a software stack to manage power and energy of\nhigh-performance computing systems and standardizes the interfaces between\ndifferent components of the stack. This survey paper presents the findings of a\nworking group focused on the end-to-end tuning of the PowerStack. First, we\nprovide a background on the PowerStack layer-specific tuning efforts in terms\nof their high-level objectives, the constraints and optimization goals,\nlayer-specific telemetry, and control parameters, and we list the existing\nsoftware solutions that address those challenges. Second, we propose the\nPowerStack end-to-end auto-tuning framework, identify the opportunities in\nco-tuning different layers in the PowerStack, and present specific use cases\nand solutions. Third, we discuss the research opportunities and challenges for\ncollective auto-tuning of two or more management layers (or domains) in the\nPowerStack. This paper takes the first steps in identifying and aggregating the\nimportant R&D challenges in streamlining the optimization efforts across the\nlayers of the PowerStack.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 20:57:47 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Wu", "Xingfu", ""], ["Marathe", "Aniruddha", ""], ["Jana", "Siddhartha", ""], ["Vysocky", "Ondrej", ""], ["John", "Jophin", ""], ["Bartolini", "Andrea", ""], ["Riha", "Lubomir", ""], ["Gerndt", "Michael", ""], ["Taylor", "Valerie", ""], ["Bhalachandra", "Sridutt", ""]]}, {"id": "2008.06601", "submitter": "Omid Halimi Milani", "authors": "Omid Halimi Milani, S. Ahmad Motamedi and Saeed Sharifian", "title": "Intelligent Service Selection in a Multi-dimensional Environment of\n  Cloud Providers for IoT stream Data through cloudlets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The expansion of the Internet of Things(IoT) services and a huge amount of\ndata generated by different sensors, signify the importance of cloud computing\nservices like Storage as a Service more than ever. IoT traffic imposes such\nextra constraints on the cloud storage service as sensor data preprocessing\ncapability and load-balancing between data centers and servers in each data\ncenter. Also, it should be allegiant to the Quality of Service (QoS). The\nhybrid MWG algorithm has been proposed in this work, which considers different\nobjectives such as energy, processing time, transmission time, and load\nbalancing in both Fog and Cloud Layer. The MATLAB script is used to simulate\nand implement our algorithms, and services of different servers, e.g. Amazon,\nDropbox, Google Drive, etc. have been considered. The MWG has 7%, 13%, and 25%\nimprovement in comparison with MOWCA, KGA, and NSGAII in metric of spacing,\nrespectively. Moreover, the MWG has 4%, 4.7%, and 7.3% optimization in metric\nof quality in comparison to MOWCA, KGA, and NSGAII, respectively. The overall\noptimization shows that the MWG algorithm has 7.8%, 17%, and 21.6% better\nperformance in comparison with MOWCA, KGA, and NSGAII in the obtained best\nresult by considering different objectives, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 23:15:32 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 22:22:42 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Milani", "Omid Halimi", ""], ["Motamedi", "S. Ahmad", ""], ["Sharifian", "Saeed", ""]]}, {"id": "2008.06724", "submitter": "Rahul Verma", "authors": "Rahul Kumar Verma, K. K. Pattanaik, P. B. R. Dissanayake, A. J.\n  Dammika, H. A. D. Samith Buddika, Mosbeh R. Kaloop", "title": "Damage Detection in Bridge Structures: An Edge Computing Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wireless sensor network (WSN) based SHM systems have shown significant\nimprovement as compared to traditional wired-SHM systems in terms of cost,\naccuracy, and reliability of the monitoring. However, due to the\nresource-constrained nature of the sensor nodes, it is a challenge to process a\nlarge amount of sensed vibration data in real-time. Existing mechanisms of data\nprocessing are centralized and use cloud or remote servers to analyze the data\nto characterize the state of the bridge, i.e., healthy or damaged. These\nmethods are feasible for wired-SHM systems, however, transmitting huge\ndata-sets in WSNs has been found to be arduous. In this paper, we propose a\nmechanism named as ``in-network damage detection on edge (INDDE)\" which\nextracts the statistical features from raw acceleration measurements\ncorresponding to the healthy condition of the bridge and use them to train a\nprobabilistic model, i.e., estimating the probability density function (PDF) of\nmultivariate Gaussian distribution. The trained model helps to identify the\nanomalous behaviour of the new data points collected from the unknown condition\nof the bridge in real-time. Each edge device classifies the condition of the\nbridge as either \"healthy\" or \"damaged\" around its deployment region depending\non their respective trained model. Experimentation results showcase a promising\n96-100% damage detection accuracy with the advantage of no data transmission\nfrom sensor nodes to the cloud for processing.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2020 14:23:28 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Verma", "Rahul Kumar", ""], ["Pattanaik", "K. K.", ""], ["Dissanayake", "P. B. R.", ""], ["Dammika", "A. J.", ""], ["Buddika", "H. A. D. Samith", ""], ["Kaloop", "Mosbeh R.", ""]]}, {"id": "2008.06823", "submitter": "Neil J. Gunther", "authors": "Neil J. Gunther", "title": "Erlang Redux: An Ansatz Method for Solving the M/M/m Queue", "comments": "13 pages, 7 figures, 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This exposition presents a novel approach to solving an M/M/m queue for the\nwaiting time and the residence time. The motivation comes from an algebraic\nsolution for the residence time of the M/M/1 queue. The key idea is the\nintroduction of an ansatz transformation, defined in terms of the Erlang B\nfunction, that avoids the more opaque derivation based on applied probability\ntheory. The only prerequisite is an elementary knowledge of the Poisson\ndistribution, which is already necessary for understanding the M/M/1 queue. The\napproach described here supersedes our earlier approximate morphing\ntransformation.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 02:50:19 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Gunther", "Neil J.", ""]]}, {"id": "2008.06991", "submitter": "Ian T Foster", "authors": "Tong Shu, Yanfei Guo, Justin Wozniak, Xiaoning Ding, Ian Foster,\n  Tahsin Kurc", "title": "In-situ Workflow Auto-tuning via Combining Performance Models of\n  Component Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In-situ parallel workflows couple multiple component applications, such as\nsimulation and analysis, via streaming data transfer. in order to avoid data\nexchange via shared file systems. Such workflows are challenging to configure\nfor optimal performance due to the large space of possible configurations.\nExpert experience is rarely sufficient to identify optimal configurations, and\nexisting empirical auto-tuning approaches are inefficient due to the high cost\nof obtaining training data for machine learning models. It is also infeasible\nto optimize individual components independently, due to component interactions.\nWe propose here a new auto-tuning method, Component-based Ensemble Active\nLearning (CEAL), that combines machine learning techniques with knowledge of\nin-situ workflow structure to enable automated workflow configuration with a\nlimited number of performance measurements.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 20:37:02 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Shu", "Tong", ""], ["Guo", "Yanfei", ""], ["Wozniak", "Justin", ""], ["Ding", "Xiaoning", ""], ["Foster", "Ian", ""], ["Kurc", "Tahsin", ""]]}, {"id": "2008.07127", "submitter": "Francesco Conti", "authors": "Alessio Burrello, Angelo Garofalo, Nazareno Bruschi, Giuseppe\n  Tagliavini, Davide Rossi, Francesco Conti", "title": "DORY: Automatic End-to-End Deployment of Real-World DNNs on Low-Cost IoT\n  MCUs", "comments": "14 pages, 12 figures, 4 tables, 2 listings. Accepted for publication\n  in IEEE Transactions on Computers\n  (https://ieeexplore.ieee.org/document/9381618)", "journal-ref": null, "doi": "10.1109/TC.2021.3066883", "report-no": null, "categories": "cs.DC cs.AR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deployment of Deep Neural Networks (DNNs) on end-nodes at the extreme\nedge of the Internet-of-Things is a critical enabler to support pervasive Deep\nLearning-enhanced applications. Low-Cost MCU-based end-nodes have limited\non-chip memory and often replace caches with scratchpads, to reduce area\noverheads and increase energy efficiency -- requiring explicit DMA-based memory\ntransfers between different levels of the memory hierarchy. Mapping modern DNNs\non these systems requires aggressive topology-dependent tiling and\ndouble-buffering. In this work, we propose DORY (Deployment Oriented to memoRY)\n- an automatic tool to deploy DNNs on low cost MCUs with typically less than\n1MB of on-chip SRAM memory. DORY abstracts tiling as a Constraint Programming\n(CP) problem: it maximizes L1 memory utilization under the topological\nconstraints imposed by each DNN layer. Then, it generates ANSI C code to\norchestrate off- and on-chip transfers and computation phases. Furthermore, to\nmaximize speed, DORY augments the CP formulation with heuristics promoting\nperformance-effective tile sizes. As a case study for DORY, we target\nGreenWaves Technologies GAP8, one of the most advanced parallel ultra-low power\nMCU-class devices on the market. On this device, DORY achieves up to 2.5x\nbetter MAC/cycle than the GreenWaves proprietary software solution and 18.1x\nbetter than the state-of-the-art result on an STM32-F746 MCU on single layers.\nUsing our tool, GAP-8 can perform end-to-end inference of a 1.0-MobileNet-128\nnetwork consuming just 63 pJ/MAC on average @ 4.3 fps - 15.4x better than an\nSTM32-F746. We release all our developments - the DORY framework, the optimized\nbackend kernels, and the related heuristics - as open-source software.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 07:30:54 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 15:11:36 GMT"}, {"version": "v3", "created": "Fri, 19 Mar 2021 15:08:55 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Burrello", "Alessio", ""], ["Garofalo", "Angelo", ""], ["Bruschi", "Nazareno", ""], ["Tagliavini", "Giuseppe", ""], ["Rossi", "Davide", ""], ["Conti", "Francesco", ""]]}, {"id": "2008.07141", "submitter": "Zhixiang Ren", "authors": "Zhixiang Ren, Yongheng Liu, Tianhui Shi, Lei Xie, Yue Zhou, Jidong\n  Zhai, Youhui Zhang, Yunquan Zhang, Wenguang Chen", "title": "AIPerf: Automated machine learning as an AI-HPC benchmark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The plethora of complex artificial intelligence (AI) algorithms and available\nhigh performance computing (HPC) power stimulates the expeditious development\nof AI components with heterogeneous designs. Consequently, the need for\ncross-stack performance benchmarking of AI-HPC systems emerges rapidly. The de\nfacto HPC benchmark LINPACK can not reflect AI computing power and I/O\nperformance without representative workload. The current popular AI benchmarks\nlike MLPerf have fixed problem size therefore limited scalability. To address\nthese issues, we propose an end-to-end benchmark suite utilizing automated\nmachine learning (AutoML), which not only represents real AI scenarios, but\nalso is auto-adaptively scalable to various scales of machines. We implement\nthe algorithms in a highly parallel and flexible way to ensure the efficiency\nand optimization potential on diverse systems with customizable configurations.\nWe utilize operations per second (OPS), which is measured in an analytical and\nsystematic approach, as the major metric to quantify the AI performance. We\nperform evaluations on various systems to ensure the benchmark's stability and\nscalability, from 4 nodes with 32 NVIDIA Tesla T4 (56.1 Tera-OPS measured), up\nto 512 nodes with 4096 Huawei Ascend 910 (194.53 Peta-OPS measured), and the\nresults show near-linear weak scalability. With flexible workload and single\nmetric, our benchmark can scale and rank AI-HPC easily.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 08:06:43 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 08:53:50 GMT"}, {"version": "v3", "created": "Wed, 26 Aug 2020 07:29:24 GMT"}, {"version": "v4", "created": "Thu, 8 Oct 2020 13:22:19 GMT"}, {"version": "v5", "created": "Mon, 12 Oct 2020 13:50:56 GMT"}, {"version": "v6", "created": "Tue, 27 Oct 2020 10:02:34 GMT"}, {"version": "v7", "created": "Mon, 15 Mar 2021 02:25:55 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Ren", "Zhixiang", ""], ["Liu", "Yongheng", ""], ["Shi", "Tianhui", ""], ["Xie", "Lei", ""], ["Zhou", "Yue", ""], ["Zhai", "Jidong", ""], ["Zhang", "Youhui", ""], ["Zhang", "Yunquan", ""], ["Chen", "Wenguang", ""]]}, {"id": "2008.07143", "submitter": "Rahim Rahmani", "authors": "Rahim Rahmani, Ramin Firouzi and Theo Kanter", "title": "Distributed Adaptive Formation Control for Multi-UAV to Enable\n  Connectivity", "comments": null, "journal-ref": "IJCSI International Journal of Computer Science Issues, Volume 17,\n  Issue 2, March 2020", "doi": "10.5281/3987125", "report-no": null, "categories": "eess.SP cs.DC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  There is increasing demand for control of multi-robot and as well\ndistributing large amounts of content to cluster of Unmanned Aerial Vehicles\n(UAV) on the operation. In recent years several large-scale accidents have\nhappened. To facilitate rescue operations and gather information, the\ntechnology that can access and map inaccessible areas is needed. This paper\npresents a disruptive approach to address the issues with communication, data\ncollection and data sharing for UAV units in inaccessible or dead zones and We\ndemonstrated feasibility of the approach and evaluate its advantages over the\nAd Hoc architecture involving autonomous gateways\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 08:11:42 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Rahmani", "Rahim", ""], ["Firouzi", "Ramin", ""], ["Kanter", "Theo", ""]]}, {"id": "2008.07159", "submitter": "Chuan-Chi Lai", "authors": "Chuan-Chi Lai, Chuan-Ming Liu, Yan-Lin Chen, Li-Chun Wang", "title": "Probabilistic Skyline Query Processing over Uncertain Data Streams in\n  Edge Computing Environments", "comments": "6 pages, 5 figures, to appear in 2020 IEEE Global Communications\n  Conference: Selected Areas in Communications: Big Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advancement of technology, the data generated in our lives is\ngetting faster and faster, and the amount of data that various applications\nneed to process becomes extremely huge. Therefore, we need to put more effort\ninto analyzing data and extracting valuable information. Cloud computing used\nto be a good technology to solve a large number of data analysis problems.\nHowever, in the era of the popularity of the Internet of Things (IoT),\ntransmitting sensing data back to the cloud for centralized data analysis will\nconsume a lot of wireless communication and network transmission costs. To\nsolve the above problems, edge computing has become a promising solution. In\nthis paper, we propose a new algorithm for processing probabilistic skyline\nqueries over uncertain data streams in an edge computing environment. We use\nthe concept of a second skyline set to filter data that is unlikely to be the\nresult of the skyline. Besides, the edge server only sends the information\nneeded to update the global analysis results on the cloud server, which will\ngreatly reduce the amount of data transmitted over the network. The results\nshow that our proposed method not only reduces the response time by more than\n50% compared with the brute force method on two-dimensional data but also\nmaintains the leading processing speed on high-dimensional data.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 08:53:29 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 04:01:53 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Lai", "Chuan-Chi", ""], ["Liu", "Chuan-Ming", ""], ["Chen", "Yan-Lin", ""], ["Wang", "Li-Chun", ""]]}, {"id": "2008.07290", "submitter": "M Sabbir Salek", "authors": "Hsien-Wen Deng, Mizanur Rahman, Mashrur Chowdhury, M Sabbir Salek, and\n  Mitch Shue", "title": "Commercial Cloud Computing for Connected Vehicle Applications in\n  Transportation Cyber-Physical Systems", "comments": "15 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study focuses on the feasibility of commercial cloud services for\nconnected vehicle (CV) applications in a Transportation Cyber-Physical Systems\n(TCPS) environment. TCPS implies that CVs, in addition to being connected with\neach other, communicates with the transportation and computing infrastructure\nto fulfill application requirements. The motivation of this study is to\naccelerate commercial cloud-based CV application development by presenting the\nlessons learned by implementing a CV mobility application using Amazon Web\nServices (AWS). The feasibility of the cloud-based CV application is assessed\nat three levels: (i) the development of a cloud-based TCPS architecture, (ii)\nthe deployment of a cloud-based CV application using AWS, and (iii) the\nevaluation of the cloud-based CV application. We implemented this CV mobility\napplication using a serverless cloud architecture and found that such a\ncloud-based TCPS environment could meet the permissible delay limits of CV\nmobility applications. Commercial cloud services, as an integral part of TCPS,\ncould reduce costs associated with establishing and maintaining vast computing\ninfrastructure for supporting CV applications. As the CV penetration levels on\nthe surface transportation systems increase significantly over the next several\nyears, scaling the backend infrastructure to support such applications is a\ncritical issue. This study shows how commercial cloud services could\nautomatically scale the backend infrastructure to meet the rapidly changing\ndemands of real-world CV applications. Through real-world experiments, we\ndemonstrate how commercial cloud services along with serverless cloud\narchitecture could advance the transportation digital infrastructure for\nsupporting connected mobility applications in a TCPS environment.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 13:15:44 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Deng", "Hsien-Wen", ""], ["Rahman", "Mizanur", ""], ["Chowdhury", "Mashrur", ""], ["Salek", "M Sabbir", ""], ["Shue", "Mitch", ""]]}, {"id": "2008.07298", "submitter": "Buse Gul Atli Tekgul", "authors": "Buse Gul Atli, Yuxi Xia, Samuel Marchal, N. Asokan", "title": "WAFFLE: Watermarking in Federated Learning", "comments": "Will appear in the proceedings of SRDS 2021; 14 pages, 11 figures, 10\n  tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Federated learning is a distributed learning technique where machine learning\nmodels are trained on client devices in which the local training data resides.\nThe training is coordinated via a central server which is, typically,\ncontrolled by the intended owner of the resulting model. By avoiding the need\nto transport the training data to the central server, federated learning\nimproves privacy and efficiency. But it raises the risk of model theft by\nclients because the resulting model is available on every client device. Even\nif the application software used for local training may attempt to prevent\ndirect access to the model, a malicious client may bypass any such restrictions\nby reverse engineering the application software. Watermarking is a well-known\ndeterrence method against model theft by providing the means for model owners\nto demonstrate ownership of their models. Several recent deep neural network\n(DNN) watermarking techniques use backdooring: training the models with\nadditional mislabeled data. Backdooring requires full access to the training\ndata and control of the training process. This is feasible when a single party\ntrains the model in a centralized manner, but not in a federated learning\nsetting where the training process and training data are distributed among\nseveral client devices. In this paper, we present WAFFLE, the first approach to\nwatermark DNN models trained using federated learning. It introduces a\nretraining step at the server after each aggregation of local models into the\nglobal model. We show that WAFFLE efficiently embeds a resilient watermark into\nmodels incurring only negligible degradation in test accuracy (-0.17%), and\ndoes not require access to training data. We also introduce a novel technique\nto generate the backdoor used as a watermark. It outperforms prior techniques,\nimposing no communication, and low computational (+3.2%) overhead.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 13:27:45 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 13:33:02 GMT"}, {"version": "v3", "created": "Thu, 22 Jul 2021 10:04:49 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Atli", "Buse Gul", ""], ["Xia", "Yuxi", ""], ["Marchal", "Samuel", ""], ["Asokan", "N.", ""]]}, {"id": "2008.07437", "submitter": "Sameh Abdulah", "authors": "Mary Lai O. Salva\\~na, Sameh Abdulah, Huang Huang, Hatem Ltaief, Ying\n  Sun, Marc G. Genton, and David E. Keyes", "title": "High Performance Multivariate Geospatial Statistics on Manycore Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling and inferring spatial relationships and predicting missing values of\nenvironmental data are some of the main tasks of geospatial statisticians.\nThese routine tasks are accomplished using multivariate geospatial models and\nthe cokriging technique. The latter requires the evaluation of the expensive\nGaussian log-likelihood function, which has impeded the adoption of\nmultivariate geospatial models for large multivariate spatial datasets.\nHowever, this large-scale cokriging challenge provides a fertile ground for\nsupercomputing implementations for the geospatial statistics community as it is\nparamount to scale computational capability to match the growth in\nenvironmental data coming from the widespread use of different data collection\ntechnologies. In this paper, we develop and deploy large-scale multivariate\nspatial modeling and inference on parallel hardware architectures. To tackle\nthe increasing complexity in matrix operations and the massive concurrency in\nparallel systems, we leverage low-rank matrix approximation techniques with\ntask-based programming models and schedule the asynchronous computational tasks\nusing a dynamic runtime system. The proposed framework provides both the dense\nand the approximated computations of the Gaussian log-likelihood function. It\ndemonstrates accuracy robustness and performance scalability on a variety of\ncomputer systems. Using both synthetic and real datasets, the low-rank matrix\napproximation shows better performance compared to exact computation, while\npreserving the application requirements in both parameter estimation and\nprediction accuracy. We also propose a novel algorithm to assess the prediction\naccuracy after the online parameter estimation. The algorithm quantifies\nprediction performance and provides a benchmark for measuring the efficiency\nand accuracy of several approximation techniques in multivariate spatial\nmodeling.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 10:24:34 GMT"}, {"version": "v2", "created": "Sun, 4 Apr 2021 11:09:58 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Salva\u00f1a", "Mary Lai O.", ""], ["Abdulah", "Sameh", ""], ["Huang", "Huang", ""], ["Ltaief", "Hatem", ""], ["Sun", "Ying", ""], ["Genton", "Marc G.", ""], ["Keyes", "David E.", ""]]}, {"id": "2008.07455", "submitter": "Michail Theofilatos", "authors": "Othon Michail, Paul G. Spirakis, Michail Theofilatos", "title": "Gathering in 1-Interval Connected Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the problem of gathering $k \\geq 2$ agents (or multi-agent\nrendezvous) in dynamic graphs which may change in every synchronous round but\nremain always connected ($1$-interval connectivity) [KLO10]. The agents are\nidentical and without explicit communication capabilities, and are initially\npositioned at different nodes of the graph. The problem is for the agents to\ngather at the same node, not fixed in advance. We first show that the problem\nbecomes impossible to solve if the graph has a cycle. In light of this, we\nstudy a relaxed version of this problem, called weak gathering. We show that\nonly in unicyclic graphs weak gathering is solvable, and we provide a\ndeterministic algorithm for this problem that runs in polynomial number of\nrounds.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 16:28:36 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Michail", "Othon", ""], ["Spirakis", "Paul G.", ""], ["Theofilatos", "Michail", ""]]}, {"id": "2008.07793", "submitter": "Vipul Gupta", "authors": "Vipul Gupta, Soham Phade, Thomas Courtade, Kannan Ramchandran", "title": "Utility-based Resource Allocation and Pricing for Serverless Computing", "comments": "24 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Serverless computing platforms currently rely on basic pricing schemes that\nare static and do not reflect customer feedback. This leads to significant\ninefficiencies from a total utility perspective. As one of the fastest-growing\ncloud services, serverless computing provides an opportunity to better serve\nboth users and providers through the incorporation of market-based strategies\nfor pricing and resource allocation. With the help of utility functions to\nmodel the delay-sensitivity of customers, we propose a novel scheduler to\nallocate resources for serverless computing. The resulting resource allocation\nscheme is optimal in the sense that it maximizes the aggregate utility of all\nusers across the system, thus maximizing social welfare. Our approach gives\nrise to a dynamic pricing scheme which is obtained by solving an optimization\nproblem in its dual form. We further develop feedback mechanisms that allow the\ncloud provider to converge to optimal resource allocation, even when the users'\nutilities are unknown. Simulations show that our approach can track market\ndemand and achieve significantly higher social welfare (or, equivalently, cost\nsavings for customers) as compared to existing schemes.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 08:05:28 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Gupta", "Vipul", ""], ["Phade", "Soham", ""], ["Courtade", "Thomas", ""], ["Ramchandran", "Kannan", ""]]}, {"id": "2008.07863", "submitter": "Jorge Pe\\~na Queralta", "authors": "Wenshuai Zhao, Jorge Pe\\~na Queralta, Li Qingqing, Tomi Westerlund", "title": "Ubiquitous Distributed Deep Reinforcement Learning at the Edge:\n  Analyzing Byzantine Agents in Discrete Action Spaces", "comments": "Accepted to the 11th International Conference on Emerging Ubiquitous\n  Systems and Pervasive Networks (EUSPN 2020) , Elsevier (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The integration of edge computing in next-generation mobile networks is\nbringing low-latency and high-bandwidth ubiquitous connectivity to a myriad of\ncyber-physical systems. This will further boost the increasing intelligence\nthat is being embedded at the edge in various types of autonomous systems,\nwhere collaborative machine learning has the potential to play a significant\nrole. This paper discusses some of the challenges in multi-agent distributed\ndeep reinforcement learning that can occur in the presence of byzantine or\nmalfunctioning agents. As the simulation-to-reality gap gets bridged, the\nprobability of malfunctions or errors must be taken into account. We show how\nwrong discrete actions can significantly affect the collaborative learning\neffort. In particular, we analyze the effect of having a fraction of agents\nthat might perform the wrong action with a given probability. We study the\nability of the system to converge towards a common working policy through the\ncollaborative learning process based on the number of experiences from each of\nthe agents to be aggregated for each policy update, together with the fraction\nof wrong actions from agents experiencing malfunctions. Our experiments are\ncarried out in a simulation environment using the Atari testbed for the\ndiscrete action spaces, and advantage actor-critic (A2C) for the distributed\nmulti-agent training.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 11:25:39 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Zhao", "Wenshuai", ""], ["Queralta", "Jorge Pe\u00f1a", ""], ["Qingqing", "Li", ""], ["Westerlund", "Tomi", ""]]}, {"id": "2008.07875", "submitter": "Jorge Pe\\~na Queralta", "authors": "Wenshuai Zhao, Jorge Pe\\~na Queralta, Li Qingqing, Tomi Westerlund", "title": "Towards Closing the Sim-to-Real Gap in Collaborative Multi-Robot Deep\n  Reinforcement Learning", "comments": "Accepted to the 5th International Conference on Robotics and\n  Automation Engineering, IEEE, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current research directions in deep reinforcement learning include bridging\nthe simulation-reality gap, improving sample efficiency of experiences in\ndistributed multi-agent reinforcement learning, together with the development\nof robust methods against adversarial agents in distributed learning, among\nmany others. In this work, we are particularly interested in analyzing how\nmulti-agent reinforcement learning can bridge the gap to reality in distributed\nmulti-robot systems where the operation of the different robots is not\nnecessarily homogeneous. These variations can happen due to sensing mismatches,\ninherent errors in terms of calibration of the mechanical joints, or simple\ndifferences in accuracy. While our results are simulation-based, we introduce\nthe effect of sensing, calibration, and accuracy mismatches in distributed\nreinforcement learning with proximal policy optimization (PPO). We discuss on\nhow both the different types of perturbances and how the number of agents\nexperiencing those perturbances affect the collaborative learning effort. The\nsimulations are carried out using a Kuka arm model in the Bullet physics\nengine. This is, to the best of our knowledge, the first work exploring the\nlimitations of PPO in multi-robot systems when considering that different\nrobots might be exposed to different environments where their sensors or\nactuators have induced errors. With the conclusions of this work, we set the\ninitial point for future work on designing and developing methods to achieve\nrobust reinforcement learning on the presence of real-world perturbances that\nmight differ within a multi-robot system.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 11:57:33 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Zhao", "Wenshuai", ""], ["Queralta", "Jorge Pe\u00f1a", ""], ["Qingqing", "Li", ""], ["Westerlund", "Tomi", ""]]}, {"id": "2008.07891", "submitter": "Tobias Pfandzelter", "authors": "Tobias Pfandzelter and Jonathan Hasenburg and David Bermbach", "title": "From Zero to Fog: Efficient Engineering of Fog-Based Internet of Things\n  Applications", "comments": "This work has been published in Wiley - Software: Practice and\n  Experience", "journal-ref": "Software: Practice and Experience, vol. 51, no. 8, pp. 1798-1821,\n  Aug. 2021", "doi": "10.1002/spe.3003", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In IoT data processing, cloud computing alone does not suffice due to latency\nconstraints, bandwidth limitations, and privacy concerns. By introducing\nintermediary nodes closer to the edge of the network that offer compute\nservices in proximity to IoT devices, fog computing can reduce network strain\nand high access latency to application services. While this is the only viable\napproach to enable efficient IoT applications, the issue of component placement\namong cloud and intermediary nodes in the fog adds a new dimension to system\ndesign. State-of-the-art solutions to this issue rely on either simulation or\nsolving a formalized assignment problem through heuristics, which are both\ninaccurate and fail to scale with a solution space that grows exponentially. In\nthis paper, we present a three step process for designing practical fog-based\nIoT applications that uses best practices, simulation, and testbed analysis to\nconverge towards an efficient system architecture. We then apply this process\nin a smart factory case study. By deploying filtered options to a physical\ntestbed, we show that each step of our process converges towards more efficient\napplication designs.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 12:30:26 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 16:40:04 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Pfandzelter", "Tobias", ""], ["Hasenburg", "Jonathan", ""], ["Bermbach", "David", ""]]}, {"id": "2008.08057", "submitter": "Siddharth Samsi", "authors": "Siddharth Samsi, Andrew Prout, Michael Jones, Andrew Kirby, Bill\n  Arcand, Bill Bergeron, David Bestor, Chansup Byun, Vijay Gadepally, Michael\n  Houle, Matthew Hubbell, Anna Klein, Peter Michaleas, Lauren Milechin, Julie\n  Mullen, Antonio Rosa, Charles Yee, Albert Reuther, Jeremy Kepner", "title": "Benchmarking network fabrics for data distributed training of deep\n  neural networks", "comments": "Accepted for publication at IEEE HPEC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Intelligence/Machine Learning applications require the training of\ncomplex models on large amounts of labelled data. The large computational\nrequirements for training deep models have necessitated the development of new\nmethods for faster training. One such approach is the data parallel approach,\nwhere the training data is distributed across multiple compute nodes. This\napproach is simple to implement and supported by most of the commonly used\nmachine learning frameworks. The data parallel approach leverages MPI for\ncommunicating gradients across all nodes. In this paper, we examine the effects\nof using different physical hardware interconnects and network-related software\nprimitives for enabling data distributed deep learning. We compare the effect\nof using GPUDirect and NCCL on Ethernet and OmniPath fabrics. Our results show\nthat using Ethernet-based networking in shared HPC systems does not have a\nsignificant effect on the training times for commonly used deep neural network\narchitectures or traditional HPC applications such as Computational Fluid\nDynamics.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 17:38:30 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Samsi", "Siddharth", ""], ["Prout", "Andrew", ""], ["Jones", "Michael", ""], ["Kirby", "Andrew", ""], ["Arcand", "Bill", ""], ["Bergeron", "Bill", ""], ["Bestor", "David", ""], ["Byun", "Chansup", ""], ["Gadepally", "Vijay", ""], ["Houle", "Michael", ""], ["Hubbell", "Matthew", ""], ["Klein", "Anna", ""], ["Michaleas", "Peter", ""], ["Milechin", "Lauren", ""], ["Mullen", "Julie", ""], ["Rosa", "Antonio", ""], ["Yee", "Charles", ""], ["Reuther", "Albert", ""], ["Kepner", "Jeremy", ""]]}, {"id": "2008.08062", "submitter": "Siddharth Samsi", "authors": "Siddharth Samsi, Michael Jones, Mark M. Veillette", "title": "Compute, Time and Energy Characterization of Encoder-Decoder Networks\n  with Automatic Mixed Precision Training", "comments": "Accepted for publication at IEEE HPEC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have shown great success in many diverse fields. The\ntraining of these networks can take significant amounts of time, compute and\nenergy. As datasets get larger and models become more complex, the exploration\nof model architectures becomes prohibitive. In this paper we examine the\ncompute, energy and time costs of training a UNet based deep neural network for\nthe problem of predicting short term weather forecasts (called precipitation\nNowcasting). By leveraging a combination of data distributed and\nmixed-precision training, we explore the design space for this problem. We also\nshow that larger models with better performance come at a potentially\nincremental cost if appropriate optimizations are used. We show that it is\npossible to achieve a significant improvement in training time by leveraging\nmixed-precision training without sacrificing model performance. Additionally,\nwe find that a 1549% increase in the number of trainable parameters for a\nnetwork comes at a relatively smaller 63.22% increase in energy usage for a\nUNet with 4 encoding layers.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 17:44:24 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Samsi", "Siddharth", ""], ["Jones", "Michael", ""], ["Veillette", "Mark M.", ""]]}, {"id": "2008.08208", "submitter": "Dongfang Zhao", "authors": "Dongfang Zhao", "title": "An Algebraic-Topological Approach to Processing Cross-Blockchain\n  Transactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state-of-the-art techniques for processing cross-blockchain transactions\ntake a simple centralized approach: when the assets on blockchain $X$, say\n$X$-coins, are exchanged with the assets on blockchain $Y$---the $Y$-coins,\nthose $X$-coins need to be exchanged to a \"middle\" medium (such as Bitcoin)\nthat is then exchanged to $Y$-coins. If there are more than two parties\ninvolved in a single global transaction, the global transaction is split into\nmultiple local two-party transactions, each of which follows the above\ncentral-exchange protocol. Unfortunately, the atomicity of the global\ntransaction is violated with the central-exchange approach: those local\ntwo-party transactions, once committed, cannot be rolled back if the global\ntransaction decides to abort. In a more general sense, the graph-based model of\n(two-party) transactions can hardly be extended to an arbitrary number of\nparties in a cross-blockchain transaction. %from why to how In this paper, we\nintroduce a higher-level abstraction of cross-blockchain transactions. We adopt\nthe \\textit{abstract simplicial complex}, an extensively-studied mathematical\nobject in algebraic topology, to represent an arbitrary number of parties\ninvolved in the blockchain transactions. Essentially, each party in the global\ntransaction is modeled as a vertex and the global transaction among $n+1$ ($n\n\\in \\mathbb{Z}$, $n > 0$) parties compose a $n$-dimensional simplex. While this\nhigher-level abstraction seems plausibly trivial, we will show how this simple\nextension leads to a new line of modeling methods and protocols for better\nprocessing cross-blockchain transactions.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 00:38:31 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Zhao", "Dongfang", ""]]}, {"id": "2008.08245", "submitter": "Yepeng Ding", "authors": "Yepeng Ding, Hiroyuki Sato", "title": "Formalizing and Verifying Decentralized Systems with Extended Concurrent\n  Separation Logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decentralized techniques are becoming crucial and ubiquitous with the rapid\nadvancement of distributed ledger technologies such as the blockchain. Numerous\ndecentralized systems have been developed to address security and privacy\nissues with great dependability and reliability via these techniques.\nMeanwhile, formalization and verification of the decentralized systems is the\nkey to ensuring correctness of the design and security properties of the\nimplementation. In this paper, we propose a novel method of formalizing and\nverifying decentralized systems with a kind of extended concurrent separation\nlogic. Our logic extends the standard concurrent separation logic with new\nfeatures including communication encapsulation, environment perception, and\nnode-level reasoning, which enhances modularity and expressiveness. Besides, we\ndevelop our logic with unitarity and compatibility to facilitate\nimplementation. Furthermore, we demonstrate the effectiveness and versatility\nof our method by applying our logic to formalize and verify critical techniques\nin decentralized systems including the consensus mechanism and the smart\ncontract.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 03:40:03 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Ding", "Yepeng", ""], ["Sato", "Hiroyuki", ""]]}, {"id": "2008.08285", "submitter": "Stephen Ash", "authors": "Andrew Borthwick, Stephen Ash, Bin Pang, Shehzad Qureshi, Timothy\n  Jones", "title": "Scalable Blocking for Very Large Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of database deduplication, the goal is to find approximately\nmatching records within a database. Blocking is a typical stage in this process\nthat involves cheaply finding candidate pairs of records that are potential\nmatches for further processing. We present here Hashed Dynamic Blocking, a new\napproach to blocking designed to address datasets larger than those studied in\nmost prior work. Hashed Dynamic Blocking (HDB) extends Dynamic Blocking, which\nleverages the insight that rare matching values and rare intersections of\nvalues are predictive of a matching relationship. We also present a novel use\nof Locality Sensitive Hashing (LSH) to build blocking key values for huge\ndatabases with a convenient configuration to control the trade-off between\nprecision and recall. HDB achieves massive scale by minimizing data movement,\nusing compact block representation, and greedily pruning ineffective candidate\nblocks using a Count-min Sketch approximate counting data structure. We\nbenchmark the algorithm by focusing on real-world datasets in excess of one\nmillion rows, demonstrating that the algorithm displays linear time complexity\nscaling in this range. Furthermore, we execute HDB on a 530 million row\nindustrial dataset, detecting 68 billion candidate pairs in less than three\nhours at a cost of $307 on a major cloud service.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 06:35:37 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Borthwick", "Andrew", ""], ["Ash", "Stephen", ""], ["Pang", "Bin", ""], ["Qureshi", "Shehzad", ""], ["Jones", "Timothy", ""]]}, {"id": "2008.08289", "submitter": "Afshin Abdi", "authors": "Afshin Abdi, Saeed Rashidi, Faramarz Fekri, Tushar Krishna", "title": "Restructuring, Pruning, and Adjustment of Deep Models for Parallel\n  Distributed Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using multiple nodes and parallel computing algorithms has become a principal\ntool to improve training and execution times of deep neural networks as well as\neffective collective intelligence in sensor networks. In this paper, we\nconsider the parallel implementation of an already-trained deep model on\nmultiple processing nodes (a.k.a. workers) where the deep model is divided into\nseveral parallel sub-models, each of which is executed by a worker. Since\nlatency due to synchronization and data transfer among workers negatively\nimpacts the performance of the parallel implementation, it is desirable to have\nminimum interdependency among parallel sub-models. To achieve this goal, we\npropose to rearrange the neurons in the neural network and partition them\n(without changing the general topology of the neural network), such that the\ninterdependency among sub-models is minimized under the computations and\ncommunications constraints of the workers. We propose RePurpose, a layer-wise\nmodel restructuring and pruning technique that guarantees the performance of\nthe overall parallelized model. To efficiently apply RePurpose, we propose an\napproach based on $\\ell_0$ optimization and the Munkres assignment algorithm.\nWe show that, compared to the existing methods, RePurpose significantly\nimproves the efficiency of the distributed inference via parallel\nimplementation, both in terms of communication and computational complexity.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 06:44:41 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Abdi", "Afshin", ""], ["Rashidi", "Saeed", ""], ["Fekri", "Faramarz", ""], ["Krishna", "Tushar", ""]]}, {"id": "2008.08292", "submitter": "Sathish Vadhiyar", "authors": "Swetha Hariharan, Prakash Murali, Abhishek Pasari, Sathish Vadhiyar", "title": "End-to-End Predictions-Based Resource Management Framework for\n  Supercomputer Jobs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Job submissions of parallel applications to production supercomputer systems\nwill have to be carefully tuned in terms of the job submission parameters to\nobtain minimum response times. In this work, we have developed an end-to-end\nresource management framework that uses predictions of queue waiting and\nexecution times to minimize response times of user jobs submitted to\nsupercomputer systems. Our method for predicting queue waiting times adaptively\nchooses a prediction method based on the cluster structure of similar jobs. Our\nstrategy for execution time predictions dynamically learns the impact of load\non execution times and uses this to predict a set of execution time ranges for\nthe target job. We have developed two resource management techniques that\nemploy these predictions, one that selects the number of processors for\nexecution and the other that also dynamically changes the job submission time.\nUsing workload simulations of large supercomputer traces, we show large-scale\nimprovements in predictions and reductions in response times over existing\ntechniques and baseline strategies.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 06:49:44 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Hariharan", "Swetha", ""], ["Murali", "Prakash", ""], ["Pasari", "Abhishek", ""], ["Vadhiyar", "Sathish", ""]]}, {"id": "2008.08445", "submitter": "Hao Wang", "authors": "Hao Wang, Jingrong Chen, Xinchen Wan, Han Tian, Jiacheng Xia, Gaoxiong\n  Zeng, Weiyan Wang, Kai Chen, Wei Bai, Junchen Jiang", "title": "Domain-specific Communication Optimization for Distributed DNN Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communication overhead poses an important obstacle to distributed DNN\ntraining and draws increasing attention in recent years. Despite continuous\nefforts, prior solutions such as gradient compression/reduction,\ncompute/communication overlapping and layer-wise flow scheduling, etc., are\nstill coarse-grained and insufficient for an efficient distributed training\nespecially when the network is under pressure. We present DLCP, a novel\nsolution exploiting the domain-specific properties of deep learning to optimize\ncommunication overhead of DNN training in a fine-grained manner. At its heart,\nDLCP comprises of several key innovations beyond prior work: e.g., it exploits\n{\\em bounded loss tolerance} of SGD-based training to improve tail\ncommunication latency which cannot be avoided purely through gradient\ncompression. It then performs fine-grained packet-level prioritization and\ndropping, as opposed to flow-level scheduling, based on layers and magnitudes\nof gradients to further speedup model convergence without affecting accuracy.\nIn addition, it leverages inter-packet order-independency to perform per-packet\nload balancing without causing classical re-ordering issues. DLCP works with\nboth Parameter Server and collective communication routines. We have\nimplemented DLCP with commodity switches, integrated it with various training\nframeworks including TensorFlow, MXNet and PyTorch, and deployed it in our\nsmall-scale testbed with 10 Nvidia V100 GPUs. Our testbed experiments and\nlarge-scale simulations show that DLCP delivers up to $84.3\\%$ additional\ntraining acceleration over the best existing solutions.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 09:53:21 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Wang", "Hao", ""], ["Chen", "Jingrong", ""], ["Wan", "Xinchen", ""], ["Tian", "Han", ""], ["Xia", "Jiacheng", ""], ["Zeng", "Gaoxiong", ""], ["Wang", "Weiyan", ""], ["Chen", "Kai", ""], ["Bai", "Wei", ""], ["Jiang", "Junchen", ""]]}, {"id": "2008.08509", "submitter": "Haoran Qiu", "authors": "Haoran Qiu, Subho S. Banerjee, Saurabh Jha, Zbigniew T. Kalbarczyk,\n  Ravishankar K. Iyer", "title": "FIRM: An Intelligent Fine-Grained Resource Management Framework for\n  SLO-Oriented Microservices", "comments": "This paper was accepted in OSDI '20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern user-facing latency-sensitive web services include numerous\ndistributed, intercommunicating microservices that promise to simplify software\ndevelopment and operation. However, multiplexing of compute resources across\nmicroservices is still challenging in production because contention for shared\nresources can cause latency spikes that violate the service-level objectives\n(SLOs) of user requests. This paper presents FIRM, an intelligent fine-grained\nresource management framework for predictable sharing of resources across\nmicroservices to drive up overall utilization. FIRM leverages online telemetry\ndata and machine-learning methods to adaptively (a) detect/localize\nmicroservices that cause SLO violations, (b) identify low-level resources in\ncontention, and (c) take actions to mitigate SLO violations via dynamic\nreprovisioning. Experiments across four microservice benchmarks demonstrate\nthat FIRM reduces SLO violations by up to 16x while reducing the overall\nrequested CPU limit by up to 62%. Moreover, FIRM improves performance\npredictability by reducing tail latencies by up to 11x.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 15:37:16 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 23:54:07 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Qiu", "Haoran", ""], ["Banerjee", "Subho S.", ""], ["Jha", "Saurabh", ""], ["Kalbarczyk", "Zbigniew T.", ""], ["Iyer", "Ravishankar K.", ""]]}, {"id": "2008.08519", "submitter": "Esteban Rangel PhD", "authors": "Esteban Rangel, Nicholas Frontiere, Salman Habib, Katrin Heitmann,\n  Wei-keng Liao, Ankit Agrawal, Alok Choudhary", "title": "Building Halo Merger Trees from the Q Continuum Simulation", "comments": "2017 IEEE 24th International Conference on High Performance Computing", "journal-ref": "2017 IEEE 24th International Conference on High Performance\n  Computing (HiPC), pp. 398-407. IEEE, 2017", "doi": "10.1109/HiPC.2017.00052", "report-no": null, "categories": "astro-ph.CO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cosmological N-body simulations rank among the most computationally intensive\nefforts today. A key challenge is the analysis of structure, substructure, and\nthe merger history for many billions of compact particle clusters, called\nhalos. Effectively representing the merging history of halos is essential for\nmany galaxy formation models used to generate synthetic sky catalogs, an\nimportant application of modern cosmological simulations. Generating realistic\nmock catalogs requires computing the halo formation history from simulations\nwith large volumes and billions of halos over many time steps, taking hundreds\nof terabytes of analysis data. We present fast parallel algorithms for\nproducing halo merger trees and tracking halo substructure from a single-level,\ndensity-based clustering algorithm. Merger trees are created from analyzing the\nhalo-particle membership function in adjacent snapshots, and substructure is\nidentified by tracking the \"cores\" of merging halos -- sets of particles near\nthe halo center. Core tracking is performed after creating merger trees and\nuses the relationships found during tree construction to associate\nsubstructures with hosts. The algorithms are implemented with MPI and evaluated\non a Cray XK7 supercomputer using up to 16,384 processes on data from HACC, a\nmodern cosmological simulation framework. We present results for creating\nmerger trees from 101 analysis snapshots taken from the Q Continuum, a large\nvolume, high mass resolution, cosmological simulation evolving half a trillion\nparticles.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 15:57:18 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Rangel", "Esteban", ""], ["Frontiere", "Nicholas", ""], ["Habib", "Salman", ""], ["Heitmann", "Katrin", ""], ["Liao", "Wei-keng", ""], ["Agrawal", "Ankit", ""], ["Choudhary", "Alok", ""]]}, {"id": "2008.08565", "submitter": "Mahdi Soleymani", "authors": "Mahdi Soleymani, Hessam Mahdavifar, A. Salman Avestimehr", "title": "Analog Lagrange Coded Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A distributed computing scenario is considered, where the computational power\nof a set of worker nodes is used to perform a certain computation task over a\ndataset that is dispersed among the workers. Lagrange coded computing (LCC),\nproposed by Yu et al., leverages the well-known Lagrange polynomial to perform\npolynomial evaluation of the dataset in such a scenario in an efficient\nparallel fashion while keeping the privacy of data amidst possible collusion of\nworkers. This solution relies on quantizing the data into a finite field, so\nthat Shamir's secret sharing, as one of its main building blocks, can be\nemployed. Such a solution, however, is not properly scalable with the size of\ndataset, mainly due to computation overflows. To address such a critical issue,\nwe propose a novel extension of LCC to the analog domain, referred to as analog\nLCC (ALCC). All the operations in the proposed ALCC protocol are done over the\ninfinite fields of R/C but for practical implementations floating-point numbers\nare used. We characterize the privacy of data in ALCC, against any subset of\ncolluding workers up to a certain size, in terms of the distinguishing security\n(DS) and the mutual information security (MIS) metrics. Also, the accuracy of\noutcome is characterized in a practical setting assuming operations are\nperformed using floating-point numbers. Consequently, a fundamental trade-off\nbetween the accuracy of the outcome of ALCC and its privacy level is observed\nand is numerically evaluated. Moreover, we implement the proposed scheme to\nperform matrix-matrix multiplication over a batch of matrices. It is observed\nthat ALCC is superior compared to the state-of-the-art LCC, implemented using\nfixed-point numbers, assuming both schemes use an equal number of bits to\nrepresent data symbols.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 17:47:37 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 23:42:14 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Soleymani", "Mahdi", ""], ["Mahdavifar", "Hessam", ""], ["Avestimehr", "A. Salman", ""]]}, {"id": "2008.08636", "submitter": "Fareed Qararyah", "authors": "Fareed Qararyah, Mohamed Wahib, Do\\u{g}a Dikbay{\\i}r, Mehmet Esat\n  Belviranli, Didem Unat", "title": "A Computational-Graph Partitioning Method for Training\n  Memory-Constrained DNNs", "comments": null, "journal-ref": null, "doi": "10.1016/j.parco.2021.102792", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Many state-of-the-art Deep Neural Networks (DNNs) have substantial memory\nrequirements. Limited device memory becomes a bottleneck when training those\nmodels. We propose ParDNN, an automatic, generic, and non-intrusive\npartitioning strategy for DNNs that are represented as computational graphs.\nParDNN decides a placement of DNN's underlying computational graph operations\nacross multiple devices so that the devices' memory constraints are met and the\ntraining time is minimized. ParDNN is completely independent of the deep\nlearning aspects of a DNN. It requires no modification neither at the model nor\nat the systems level implementation of its operation kernels. ParDNN partitions\nDNNs having billions of parameters and hundreds of thousands of operations in\nseconds to few minutes. Our experiments with TensorFlow on 16 GPUs demonstrate\nefficient training of 5 very large models while achieving superlinear scaling\nfor both the batch size and training throughput. ParDNN either outperforms or\nqualitatively improves upon the related work.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 19:09:04 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 11:26:25 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Qararyah", "Fareed", ""], ["Wahib", "Mohamed", ""], ["Dikbay\u0131r", "Do\u011fa", ""], ["Belviranli", "Mehmet Esat", ""], ["Unat", "Didem", ""]]}, {"id": "2008.08656", "submitter": "Anthony Byrne", "authors": "Ozan Tuncer, Anthony Byrne, Nilton Bila, Sastry Duri, Canturk Isci,\n  Ayse K. Coskun", "title": "ConfEx: A Framework for Automating Text-based Software Configuration\n  Analysis in the Cloud", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern cloud services have complex architectures, often comprising many\nsoftware components, and depend on hundreds of configurations parameters to\nfunction correctly, securely, and with high performance. Due to the prevalence\nof open-source software, developers can easily deploy services using\nthird-party software without mastering the configurations of that software. As\na result, configuration errors (i.e., misconfigurations) are among the leading\ncauses of service disruptions and outages. While existing cloud automation\ntools ease the process of service deployment and management, support for\ndetecting misconfigurations in the cloud has not been addressed thoroughly,\nlikely due to the lack of frameworks suitable for consistent parsing of\nunstandardized configuration files. This paper introduces ConfEx, a framework\nthat enables discovery and extraction of text-based software configurations in\nthe cloud. ConfEx uses a novel vocabulary-based technique to identify\nconfiguration files in cloud system instances with unlabeled content. To\nextract the information in these files, ConfEx leverages existing configuration\nparsers and post-processes the extracted data for analysis. We show that ConfEx\nachieves over 99% precision and 100% recall in identifying configuration files\non 7805 popular Docker Hub images. Using two applied examples, we demonstrate\nthat ConfEx also enables detecting misconfigurations in the cloud via existing\ntools that are designed for configurations represented as key-value pairs,\nrevealing 184 errors in public Docker Hub images.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 20:10:35 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2020 16:49:18 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Tuncer", "Ozan", ""], ["Byrne", "Anthony", ""], ["Bila", "Nilton", ""], ["Duri", "Sastry", ""], ["Isci", "Canturk", ""], ["Coskun", "Ayse K.", ""]]}, {"id": "2008.08665", "submitter": "Hyunsung Lee", "authors": "Hyunsung Lee", "title": "Intelligent Replication Management for HDFS Using Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Storage systems for cloud computing merge a large number of commodity\ncomputers into a single large storage pool. It provides high-performance\nstorage over an unreliable, and dynamic network at a lower cost than purchasing\nand maintaining large mainframe. In this paper, we examine whether it is\nfeasible to apply Reinforcement Learning(RL) to system domain problems. Our\nexperiments show that the RL model is comparable, even outperform other\nheuristics for block management problem. However, our experiments are limited\nin terms of scalability and fidelity. Even though our formulation is not very\npractical,applying Reinforcement Learning to system domain could offer good\nalternatives to existing heuristics.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 20:53:13 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Lee", "Hyunsung", ""]]}, {"id": "2008.08708", "submitter": "Zhengyang Liu", "authors": "Zixian Cai, Zhengyang Liu, Saeed Maleki, Madan Musuvathi, Todd\n  Mytkowicz, Jacob Nelson, Olli Saarikivi", "title": "Synthesizing Optimal Collective Algorithms", "comments": "Both Zixian Cai and Zhengyang Liu contributed equally to the paper.\n  The work was done during internships at Microsoft Research. To appear at\n  PPoPP 2021", "journal-ref": null, "doi": "10.1145/3437801.3441620", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collective communication algorithms are an important component of distributed\ncomputation. Indeed, in the case of deep-learning, collective communication is\nthe Amdahl's bottleneck of data-parallel training.\n  This paper introduces SCCL (for Synthesized Collective Communication\nLibrary), a systematic approach to synthesize collective communication\nalgorithms that are explicitly tailored to a particular hardware topology. SCCL\nsynthesizes algorithms along the Pareto-frontier spanning from latency-optimal\nto bandwidth-optimal implementations of a collective. The paper demonstrates\nhow to encode SCCL's synthesis as a quantifier-free SMT formula which can be\ndischarged to a theorem prover. We further demonstrate how to scale our\nsynthesis by exploiting symmetries in topologies and collectives.\n  We synthesize and introduce novel latency and bandwidth optimal algorithms\nnot seen in the literature on two popular hardware topologies. We also show how\nSCCL efficiently lowers algorithms to implementations on two hardware\narchitectures (NVIDIA and AMD) and demonstrate competitive performance with\nhand optimized collective communication libraries.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 23:51:23 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2021 19:04:03 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Cai", "Zixian", ""], ["Liu", "Zhengyang", ""], ["Maleki", "Saeed", ""], ["Musuvathi", "Madan", ""], ["Mytkowicz", "Todd", ""], ["Nelson", "Jacob", ""], ["Saarikivi", "Olli", ""]]}, {"id": "2008.08883", "submitter": "Kris Nikov", "authors": "Kris Nikov (1), Mohammad Hosseinabady (1), Rafael Asenjo (2), Andr\\'es\n  Rodr\\'iguezz (2), Angeles Navarro (2) and Jose Nunez-Yanez (1) ((1)\n  University of Bristol, UK, (2) Universidad de M\\'alaga, Spain)", "title": "High-Performance Simultaneous Multiprocessing for Heterogeneous\n  System-on-Chip", "comments": "7 pages, 5 figures, 1 table Presented at the 13th International\n  Workshop on Programmability and Architectures for Heterogeneous Multicores,\n  2020 (arXiv:2005.07619)", "journal-ref": null, "doi": null, "report-no": "MULTIPROG/2020/4", "categories": "cs.DC cs.AR cs.PF", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a methodology for simultaneous heterogeneous computing,\nnamed ENEAC, where a quad core ARM Cortex-A53 CPU works in tandem with a\npreprogrammed on-board FPGA accelerator. A heterogeneous scheduler distributes\nthe tasks optimally among all the resources and all compute units run\nasynchronously, which allows for improved performance for irregular workloads.\nENEAC achieves up to 17\\% performance improvement \\ignore{and 14\\% energy usage\nreduction,} when using all platform resources compared to just using the FPGA\naccelerators and up to 865\\% performance increase \\ignore{and up to 89\\% energy\nusage decrease} when using just the CPU. The workflow uses existing commercial\ntools and C/C++ as a single programming language for both accelerator design\nand CPU programming for improved productivity and ease of verification.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 10:53:32 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Nikov", "Kris", ""], ["Hosseinabady", "Mohammad", ""], ["Asenjo", "Rafael", ""], ["Rodr\u00edguezz", "Andr\u00e9s", ""], ["Navarro", "Angeles", ""], ["Nunez-Yanez", "Jose", ""]]}, {"id": "2008.08886", "submitter": "Daniele De Sensi PhD", "authors": "Daniele De Sensi, Salvatore Di Girolamo, Kim H. McMahon, Duncan\n  Roweth, Torsten Hoefler", "title": "An In-Depth Analysis of the Slingshot Interconnect", "comments": "To be published in Proceedings of The International Conference for\n  High Performance Computing Networking, Storage, and Analysis (SC '20) (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interconnect is one of the most critical components in large scale\ncomputing systems, and its impact on the performance of applications is going\nto increase with the system size. In this paper, we will describe Slingshot, an\ninterconnection network for large scale computing systems. Slingshot is based\non high-radix switches, which allow building exascale and hyperscale\ndatacenters networks with at most three switch-to-switch hops. Moreover,\nSlingshot provides efficient adaptive routing and congestion control\nalgorithms, and highly tunable traffic classes. Slingshot uses an optimized\nEthernet protocol, which allows it to be interoperable with standard Ethernet\ndevices while providing high performance to HPC applications. We analyze the\nextent to which Slingshot provides these features, evaluating it on\nmicrobenchmarks and on several applications from the datacenter and AI worlds,\nas well as on HPC applications. We find that applications running on Slingshot\nare less affected by congestion compared to previous generation networks.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 10:55:27 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["De Sensi", "Daniele", ""], ["Di Girolamo", "Salvatore", ""], ["McMahon", "Kim H.", ""], ["Roweth", "Duncan", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2008.09048", "submitter": "Cong Luong Nguyen", "authors": "Jer Shyuan Ng, Wei Yang Bryan Lim, Nguyen Cong Luong, Zehui Xiong,\n  Alia Asheralieva, Dusit Niyato, Cyril Leung, Chunyan Miao", "title": "A Survey of Coded Distributed Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed computing has become a common approach for large-scale\ncomputation of tasks due to benefits such as high reliability, scalability,\ncomputation speed, and costeffectiveness. However, distributed computing faces\ncritical issues related to communication load and straggler effects. In\nparticular, computing nodes need to exchange intermediate results with each\nother in order to calculate the final result, and this significantly increases\ncommunication overheads. Furthermore, a distributed computing network may\ninclude straggling nodes that run intermittently slower. This results in a\nlonger overall time needed to execute the computation tasks, thereby limiting\nthe performance of distributed computing. To address these issues, coded\ndistributed computing (CDC), i.e., a combination of coding theoretic techniques\nand distributed computing, has been recently proposed as a promising solution.\nCoding theoretic techniques have proved effective in WiFi and cellular systems\nto deal with channel noise. Therefore, CDC may significantly reduce\ncommunication load, alleviate the effects of stragglers, provide\nfault-tolerance, privacy and security. In this survey, we first introduce the\nfundamentals of CDC, followed by basic CDC schemes. Then, we review and analyze\na number of CDC approaches proposed to reduce the communication costs, mitigate\nthe straggler effects, and guarantee privacy and security. Furthermore, we\npresent and discuss applications of CDC in modern computer networks. Finally,\nwe highlight important challenges and promising research directions related to\nCDC\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 16:02:35 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Ng", "Jer Shyuan", ""], ["Lim", "Wei Yang Bryan", ""], ["Luong", "Nguyen Cong", ""], ["Xiong", "Zehui", ""], ["Asheralieva", "Alia", ""], ["Niyato", "Dusit", ""], ["Leung", "Cyril", ""], ["Miao", "Chunyan", ""]]}, {"id": "2008.09161", "submitter": "Praneeth Vepakomma", "authors": "Praneeth Vepakomma, Abhishek Singh, Otkrist Gupta, Ramesh Raskar", "title": "NoPeek: Information leakage reduction to share activations in\n  distributed deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For distributed machine learning with sensitive data, we demonstrate how\nminimizing distance correlation between raw data and intermediary\nrepresentations reduces leakage of sensitive raw data patterns across client\ncommunications while maintaining model accuracy. Leakage (measured using\ndistance correlation between input and intermediate representations) is the\nrisk associated with the invertibility of raw data from intermediary\nrepresentations. This can prevent client entities that hold sensitive data from\nusing distributed deep learning services. We demonstrate that our method is\nresilient to such reconstruction attacks and is based on reduction of distance\ncorrelation between raw data and learned representations during training and\ninference with image datasets. We prevent such reconstruction of raw data while\nmaintaining information required to sustain good classification accuracies.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 19:03:17 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Vepakomma", "Praneeth", ""], ["Singh", "Abhishek", ""], ["Gupta", "Otkrist", ""], ["Raskar", "Ramesh", ""]]}, {"id": "2008.09209", "submitter": "Massimiliano Morrelli", "authors": "Massimiliano Morrelli", "title": "Addestramento con Dataset Sbilanciati", "comments": "in Italian", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  English. The following document pursues the objective of comparing some\nuseful methods to balance a dataset and obtain a trained model. The dataset\nused for training is made up of short and medium length sentences, such as\nsimple phrases or extracts from conversations that took place on web channels.\nThe training of the models will take place with the help of the structures made\navailable by the Apache Spark framework, the models may subsequently be useful\nfor a possible implementation of a solution capable of classifying sentences\nusing the distributed environment, as described in \"New frontier of textual\nclassification: Big data and distributed calculation\" by Massimiliano Morrelli\net al.\n  Italiano. Il seguente documento persegue l'obiettivo di mettere a confronto\nalcuni metodi utili a bilanciare un dataset e ottenere un modello addestrato.\nIl dataset utilizzato per l'addestramento \\`e composto da frasi di lunghezza\nbreve e media, come frasi semplici o estratte da conversazioni avvenute su\ncanali web. L'addestramento dei modelli avverr\\`a con l'ausilio delle strutture\nmesse a disposizione dal framework Apache Spark, i modelli successivamente\npotranno essere utili a un eventuale implementazione di una soluzione in grado\ndi classificare frasi sfruttando l'ambiente distribuito, come descritto in\n\"Nuova frontiera della classificazione testuale: Big data e calcolo\ndistribuito\" di Massimiliano Morrelli et al.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 07:47:38 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Morrelli", "Massimiliano", ""]]}, {"id": "2008.09213", "submitter": "Deepak Narayanan", "authors": "Deepak Narayanan, Keshav Santhanam, Fiodar Kazhamiaka, Amar\n  Phanishayee, Matei Zaharia", "title": "Heterogeneity-Aware Cluster Scheduling Policies for Deep Learning\n  Workloads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Specialized accelerators such as GPUs, TPUs, FPGAs, and custom ASICs have\nbeen increasingly deployed to train deep learning models. These accelerators\nexhibit heterogeneous performance behavior across model architectures. Existing\nschedulers for clusters of accelerators, which are used to arbitrate these\nexpensive training resources across many users, have shown how to optimize for\nvarious multi-job, multi-user objectives, like fairness and makespan.\nUnfortunately, existing schedulers largely do not consider performance\nheterogeneity. In this paper, we propose Gavel, a heterogeneity-aware scheduler\nthat systematically generalizes a wide range of existing scheduling policies.\nGavel expresses these policies as optimization problems, making it easy to\noptimize for objectives in a heterogeneity-aware way, while also being\ncognizant of performance optimizations like space sharing. Gavel then uses a\nround-based scheduling mechanism to ensure jobs receive their ideal allocation\ngiven the target scheduling policy. Gavel's heterogeneity-aware policies allow\na heterogeneous cluster to sustain higher input load, and improve end\nobjectives such as average job completion time and makespan by up to 3.5x\ncompared to heterogeneity-agnostic policies.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 21:25:31 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Narayanan", "Deepak", ""], ["Santhanam", "Keshav", ""], ["Kazhamiaka", "Fiodar", ""], ["Phanishayee", "Amar", ""], ["Zaharia", "Matei", ""]]}, {"id": "2008.09246", "submitter": "Jie Xu", "authors": "Jie Xu, Wei Zhang, Fei Wang", "title": "A(DP)$^2$SGD: Asynchronous Decentralized Parallel Stochastic Gradient\n  Descent with Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As deep learning models are usually massive and complex, distributed learning\nis essential for increasing training efficiency. Moreover, in many real-world\napplication scenarios like healthcare, distributed learning can also keep the\ndata local and protect privacy. A popular distributed learning strategy is\nfederated learning, where there is a central server storing the global model\nand a set of local computing nodes updating the model parameters with their\ncorresponding data. The updated model parameters will be processed and\ntransmitted to the central server, which leads to heavy communication costs.\nRecently, asynchronous decentralized distributed learning has been proposed and\ndemonstrated to be a more efficient and practical strategy where there is no\ncentral server, so that each computing node only communicates with its\nneighbors. Although no raw data will be transmitted across different local\nnodes, there is still a risk of information leak during the communication\nprocess for malicious participants to make attacks. In this paper, we present a\ndifferentially private version of asynchronous decentralized parallel SGD\n(ADPSGD) framework, or A(DP)$^2$SGD for short, which maintains communication\nefficiency of ADPSGD and prevents the inference from malicious participants.\nSpecifically, R{\\'e}nyi differential privacy is used to provide tighter privacy\nanalysis for our composite Gaussian mechanisms while the convergence rate is\nconsistent with the non-private version. Theoretical analysis shows\nA(DP)$^2$SGD also converges at the optimal $\\mathcal{O}(1/\\sqrt{T})$ rate as\nSGD. Empirically, A(DP)$^2$SGD achieves comparable model accuracy as the\ndifferentially private version of Synchronous SGD (SSGD) but runs much faster\nthan SSGD in heterogeneous computing environments.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 00:56:22 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Xu", "Jie", ""], ["Zhang", "Wei", ""], ["Wang", "Fei", ""]]}, {"id": "2008.09268", "submitter": "Meihui Zhang", "authors": "Meihui Zhang, Zhongle Xie, Cong Yue, Ziyue Zhong", "title": "Spitz: A Verifiable Database System", "comments": null, "journal-ref": null, "doi": "10.14778/3415478.3415567", "report-no": null, "categories": "cs.DB cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Databases in the past have helped businesses maintain and extract insights\nfrom their data. Today, it is common for a business to involve multiple\nindependent, distrustful parties. This trend towards decentralization\nintroduces a new and important requirement to databases: the integrity of the\ndata, the history, and the execution must be protected. In other words, there\nis a need for a new class of database systems whose integrity can be verified\n(or verifiable databases).\n  In this paper, we identify the requirements and the design challenges of\nverifiable databases.We observe that the main challenges come from the need to\nbalance data immutability, tamper evidence, and performance. We first consider\napproaches that extend existing OLTP and OLAP systems with support for\nverification. We next examine a clean-slate approach, by describing a new\nsystem, Spitz, specifically designed for efficiently supporting immutable and\ntamper-evident transaction management. We conduct a preliminary performance\nstudy of both approaches against a baseline system, and provide insights on\ntheir performance.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 02:16:12 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Zhang", "Meihui", ""], ["Xie", "Zhongle", ""], ["Yue", "Cong", ""], ["Zhong", "Ziyue", ""]]}, {"id": "2008.09323", "submitter": "Frank Lin", "authors": "Frank Po-Chen Lin, Christopher G. Brinton, Nicol\\`o Michelusi", "title": "Federated Learning with Communication Delay in Edge Networks", "comments": "Accepted for publication at IEEE Global Communications Conference\n  (Globecom 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning has received significant attention as a potential solution\nfor distributing machine learning (ML) model training through edge networks.\nThis work addresses an important consideration of federated learning at the\nnetwork edge: communication delays between the edge nodes and the aggregator. A\ntechnique called FedDelAvg (federated delayed averaging) is developed, which\ngeneralizes the standard federated averaging algorithm to incorporate a\nweighting between the current local model and the delayed global model received\nat each device during the synchronization step. Through theoretical analysis,\nan upper bound is derived on the global model loss achieved by FedDelAvg, which\nreveals a strong dependency of learning performance on the values of the\nweighting and learning rate. Experimental results on a popular ML task indicate\nsignificant improvements in terms of convergence speed when optimizing the\nweighting scheme to account for delays.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 06:21:35 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Lin", "Frank Po-Chen", ""], ["Brinton", "Christopher G.", ""], ["Michelusi", "Nicol\u00f2", ""]]}, {"id": "2008.09379", "submitter": "Yuichi Sudo", "authors": "Takahiro Shintaku, Yuichi Sudo, Hirotsugu Kakugawa, Toshimitsu\n  Masuzawa", "title": "Efficient Dispersion of Mobile Agents without Global Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the dispersion problem for mobile agents. Initially, k agents are\nlocated at arbitrary nodes in an undirected graph. Agents can migrate from node\nto node via an edge in the graph synchronously. Our goal is to let the k agents\nbe located at different k nodes with minimizing the number of steps before\ndispersion is completed and the working memory space used by the agents.\nKshemkalyani and Ali [ICDCN, 2019] present a fast and space-efficient\ndispersion algorithm with the assumption that each agent has global knowledge\nsuch as the number of edges and the maximum degree of a graph. In this paper,\nwe present a dispersion algorithm that does not require such global knowledge\nbut keeps the asymptotically same running time and slightly smaller memory\nspace.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 08:59:34 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Shintaku", "Takahiro", ""], ["Sudo", "Yuichi", ""], ["Kakugawa", "Hirotsugu", ""], ["Masuzawa", "Toshimitsu", ""]]}, {"id": "2008.09491", "submitter": "Jashwant Raj Gunasekaran", "authors": "Jashwant Raj Gunasekaran, Prashanth Thinakaran, Cyan Subhra Mishra,\n  Mahmut Taylan Kandemir, Chita R. Das", "title": "Towards Designing a Self-Managed Machine Learning Inference Serving\n  System inPublic Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are witnessing an increasing trend towardsusing Machine Learning (ML)\nbased prediction systems, span-ning across different application domains,\nincluding productrecommendation systems, personal assistant devices,\nfacialrecognition, etc. These applications typically have diverserequirements\nin terms of accuracy and response latency, thathave a direct impact on the cost\nof deploying them in a publiccloud. Furthermore, the deployment cost also\ndepends on thetype of resources being procured, which by themselves\nareheterogeneous in terms of provisioning latencies and billingcomplexity.\nThus, it is strenuous for an inference servingsystem to choose from this\nconfounding array of resourcetypes and model types to provide low-latency and\ncost-effectiveinferences. In this work we quantitatively characterize the\ncost,accuracy and latency implications of hosting ML inferenceson different\npublic cloud resource offerings. In addition, wecomprehensively evaluate prior\nwork which tries to achievecost-effective prediction-serving. Our evaluation\nshows that,prior work does not solve the problem from both dimensionsof model\nand resource heterogeneity. Hence, we argue that toaddress this problem, we\nneed to holistically solve the issuesthat arise when trying to combine both\nmodel and resourceheterogeneity towards optimizing for application\nconstraints.Towards this, we envision developing a self-managed\ninferenceserving system, which can optimize the application require-ments based\non public cloud resource characteristics. In orderto solve this complex\noptimization problem, we explore the highlevel design of a\nreinforcement-learning based system that canefficiently adapt to the changing\nneeds of the system at scale.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 14:14:51 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Gunasekaran", "Jashwant Raj", ""], ["Thinakaran", "Prashanth", ""], ["Mishra", "Cyan Subhra", ""], ["Kandemir", "Mahmut Taylan", ""], ["Das", "Chita R.", ""]]}, {"id": "2008.09519", "submitter": "Chuan-Chi Lai", "authors": "Chuan-Chi Lai, Li-Chun Wang, Zhu Han", "title": "The Coverage Overlapping Problem of Serving Arbitrary Crowds in 3D Drone\n  Cellular Networks", "comments": "18 pages, 10 figures, to appear in IEEE Transactions on Mobile\n  Computing. arXiv admin note: text overlap with arXiv:1909.11554", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Providing coverage for flash crowds is an important application for drone\nbase stations (DBSs). However, any arbitrary crowd is likely to be distributed\nat a high density. Under the condition for each DBS to serve the same number of\nground users, multiple DBSs may be placed at the same horizontal location but\ndifferent altitudes and will cause severe co-channel interference, to which we\nrefer as the coverage overlapping problem. To solve this problem, we then\nproposed the data-driven 3D placement (DDP) and the enhanced DDP (eDDP)\nalgorithms. The proposed DDP and eDDP can effectively find the appropriate\nnumber, altitude, location, and coverage of DBSs in the serving area in\npolynomial time to maximize the system sum rate and guarantee the minimum data\nrate requirement of the user equipment. The simulation results show that,\ncompared with the balanced k-means approach, the proposed eDDP can increase the\nsystem sum rate by 200% and reduce the computation time by 50%. In particular,\neDDP can effectively reduce the occurrence of the coverage overlapping problem\nand then outperform DDP by about 100% in terms of system sum rate.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 17:55:19 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Lai", "Chuan-Chi", ""], ["Wang", "Li-Chun", ""], ["Han", "Zhu", ""]]}, {"id": "2008.09591", "submitter": "Ian T Foster", "authors": "Ian Foster, Carl Kesselman", "title": "Translating the Grid: How a Translational Approach Shaped the\n  Development of Grid Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A growing gap between progress in biological knowledge and improved health\noutcomes inspired the new discipline of translational medicine, in which the\napplication of new knowledge is an explicit part of a research plan. Abramson\nand Parashar argue that a similar gap between complex computational\ntechnologies and ever-more-challenging applications demands an analogous\ndiscipline of translational computer science, in which the deliberate movement\nof research results into large-scale practice becomes a central research focus\nrather than an afterthought. We revisit from this perspective the development\nand application of grid computing from the mid-1990s onwards, and find that a\ntranslational framing is useful for understanding the technology's development\nand impact. We discuss how the development of grid computing infrastructure,\nand the Globus Toolkit, in particular, benefited from a translational approach.\nWe identify lessons learned that can be applied to other translational computer\nscience initiatives.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 17:34:58 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Foster", "Ian", ""], ["Kesselman", "Carl", ""]]}, {"id": "2008.09682", "submitter": "Jingji Chen", "authors": "Jingji Chen, Xuehai Qian", "title": "DwarvesGraph: A High-Performance Graph Mining System with Pattern\n  Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents DwarvesGraph, the first compilation-based graph pattern\nmining (GPM) system based on pattern decomposition algorithms, which decompose\na pattern into several subpatterns and find the count of each. Such algorithms\ncan be orders of magnitudes faster than algorithms used in current GPM systems\nbecause the execution time of pattern enumeration drastically increases with\npattern size. We define a novel partial-embedding-centric programming model\nthat supports various applications. We propose an efficient on-the-fly\naggregation of subpatterns embeddings to reduce memory consumption and random\naccesses. DwarvesGraph compiler, using abstract syntax tree (AST) as\nintermediate representation (IR), can apply conventional and a novel\npattern-aware loop rewriting optimization to eliminate redundant computation\nthat cannot be removed with standard methods. To estimate implementation cost\nbased on AST, we propose a simple locality-aware and an advanced\napproximate-mining-based cost model to accurately capture the characteristics\nof real-world graphs. DwarvesGraph fully automates the algorithm generation,\noptimization, and selection in the search space. As a general GPM system,\nDwarvesGraph achieves performance much closer to the best native pattern\ndecomposition algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 21:08:51 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 21:04:57 GMT"}, {"version": "v3", "created": "Sat, 8 May 2021 21:09:06 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Chen", "Jingji", ""], ["Qian", "Xuehai", ""]]}, {"id": "2008.09728", "submitter": "Sumit Mandal", "authors": "Sumit K. Mandal, Umit Y. Ogras, Janardhan Rao Doppa, Raid Z. Ayoub,\n  Michael Kishinevsky, Partha P. Pande", "title": "Online Adaptive Learning for Runtime Resource Management of\n  Heterogeneous SoCs", "comments": "This paper appeared in the Proceedings of Design Automation\n  Conference 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic resource management has become one of the major areas of research in\nmodern computer and communication system design due to lower power consumption\nand higher performance demands. The number of integrated cores, level of\nheterogeneity and amount of control knobs increase steadily. As a result, the\nsystem complexity is increasing faster than our ability to optimize and\ndynamically manage the resources. Moreover, offline approaches are sub-optimal\ndue to workload variations and large volume of new applications unknown at\ndesign time. This paper first reviews recent online learning techniques for\npredicting system performance, power, and temperature. Then, we describe the\nuse of predictive models for online control using two modern approaches:\nimitation learning (IL) and an explicit nonlinear model predictive control\n(NMPC). Evaluations on a commercial mobile platform with 16 benchmarks show\nthat the IL approach successfully adapts the control policy to unknown\napplications. The explicit NMPC provides 25% energy savings compared to a\nstate-of-the-art algorithm for multi-variable power management of modern GPU\nsub-systems.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 01:39:32 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Mandal", "Sumit K.", ""], ["Ogras", "Umit Y.", ""], ["Doppa", "Janardhan Rao", ""], ["Ayoub", "Raid Z.", ""], ["Kishinevsky", "Michael", ""], ["Pande", "Partha P.", ""]]}, {"id": "2008.09735", "submitter": "Yanhong Annie Liu", "authors": "Yanhong A. Liu, Scott D. Stoller", "title": "Assurance of Distributed Algorithms and Systems: Runtime Checking of\n  Safety and Liveness", "comments": "Small fixes to improve property specifications, including\n  improvements not in the RV 2020 final version", "journal-ref": "RV 2020: Proceedings of the 20th International Conference on\n  Runtime Verification. LNCS 12399. Pages 47-66. Springer", "doi": "10.1007/978-3-030-60508-7_3", "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a general framework and methods for complete programming\nand checking of distributed algorithms at a high-level, as in pseudocode\nlanguages, but precisely specified and directly executable, as in formal\nspecification languages and practical programming languages, respectively. The\nchecking framework, as well as the writing of distributed algorithms and\nspecification of their safety and liveness properties, use DistAlgo, a\nhigh-level language for distributed algorithms. We give a complete executable\nspecification of the checking framework, with a complete example algorithm and\nexample safety and liveness properties.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 02:22:52 GMT"}, {"version": "v2", "created": "Thu, 24 Dec 2020 02:19:37 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Liu", "Yanhong A.", ""], ["Stoller", "Scott D.", ""]]}, {"id": "2008.09795", "submitter": "Yi Peng", "authors": "Peng Yi, Jinlong Lei, Yiguang Hong, Jie Chen, Guodong Shi", "title": "Distributed Linear Equations over Random Networks", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed linear algebraic equation over networks, where nodes hold a part\nof problem data and cooperatively solve the equation via node-to-node\ncommunications, is a basic distributed computation task receiving an increasing\nresearch attention. Communications over a network have a stochastic nature,\nwith both temporal and spatial dependence due to link failures, packet dropouts\nor node recreation, etc. In this paper, we study the convergence and\nconvergence rate of distributed linear equation protocols over a $\\ast$-mixing\nrandom network, where the temporal and spatial dependencies between the\nnode-to-node communications are allowed. When the network linear equation\nadmits exact solutions, we prove the mean-squared exponential convergence rate\nof the distributed projection consensus algorithm, while the lower and upper\nbound estimations of the convergence rate are also given for independent and\nidentically distributed (i.i.d.) random graphs. Motivated by the randomized\nKaczmarz algorithm, we also propose a distributed randomized projection\nconsensus algorithm, where each node randomly selects one row of local linear\nequations for projection per iteration, and establish an exponential\nconvergence rate. When the network linear equation admits no exact solution, we\nprove that a distributed gradient-descent-like algorithm with diminishing\nstep-sizes can drive all nodes' states to a least-squares solution at a\nsublinear rate. These results collectively illustrate that distributed\ncomputations may overcome communication correlations if the prototype\nalgorithms enjoy certain contractive properties or are designed with suitable\nparameters.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 10:06:30 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 07:05:36 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Yi", "Peng", ""], ["Lei", "Jinlong", ""], ["Hong", "Yiguang", ""], ["Chen", "Jie", ""], ["Shi", "Guodong", ""]]}, {"id": "2008.09930", "submitter": "Guanjin Qu", "authors": "Guanjin Qu and Huaming Wu", "title": "DMRO:A Deep Meta Reinforcement Learning-based Task Offloading Framework\n  for Edge-Cloud Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the continuous growth of mobile data and the unprecedented demand for\ncomputing power, resource-constrained edge devices cannot effectively meet the\nrequirements of Internet of Things (IoT) applications and Deep Neural Network\n(DNN) computing. As a distributed computing paradigm, edge offloading that\nmigrates complex tasks from IoT devices to edge-cloud servers can break through\nthe resource limitation of IoT devices, reduce the computing burden and improve\nthe efficiency of task processing. However, the problem of optimal offloading\ndecision-making is NP-hard, traditional optimization methods are difficult to\nachieve results efficiently. Besides, there are still some shortcomings in\nexisting deep learning methods, e.g., the slow learning speed and the failure\nof the original network parameters when the environment changes. To tackle\nthese challenges, we propose a Deep Meta Reinforcement Learning-based\noffloading (DMRO) algorithm, which combines multiple parallel DNNs with\nQ-learning to make fine-grained offloading decisions. By aggregating the\nperceptive ability of deep learning, the decision-making ability of\nreinforcement learning, and the rapid environment learning ability of\nmeta-learning, it is possible to quickly and flexibly obtain the optimal\noffloading strategy from the IoT environment. Simulation results demonstrate\nthat the proposed algorithm achieves obvious improvement over the Deep\nQ-Learning algorithm and has strong portability in making real-time offloading\ndecisions even in time-varying IoT environments.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2020 00:29:17 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Qu", "Guanjin", ""], ["Wu", "Huaming", ""]]}, {"id": "2008.10037", "submitter": "Suryanarayana Murthy Durbhakula", "authors": "Suryanarayana Murthy Durbhakula", "title": "ILP Aware Scheduling on Multithreaded Multi-core Processors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multithreaded Multi-core processors are prevalent today and are used for\nsolving some of the important problems in computing. Resource imbalance can\nnegatively impact overall performance in such processors. Hence balanced\nresource utilization is important in such processors. Particularly, it is\nimportant to maximize utilization of available instruction-level-parallelism\n(ILP). In this paper I present an ILP aware operating system (OS) scheduling\nalgorithm for Multithreaded Multi-core processors. By keeping track of\navailable ILP in each thread and by balancing it with available ILP resources\nin the system the OS will come up with a new schedule of threads for the next\nquantum. This new schedule will potentially result in a more balanced\nresource-utilization and improve performance for the next quantum. This work\ncan be extended by doing a detailed quantitative evaluation.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2020 13:14:08 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Durbhakula", "Suryanarayana Murthy", ""]]}, {"id": "2008.10169", "submitter": "Vikram Sharma Mailthody", "authors": "Zaid Qureshi, Vikram Sharma Mailthody, Seung Won Min, I-Hsin Chung,\n  Jinjun Xiong, Wen-mei Hwu", "title": "Tearing Down the Memory Wall", "comments": "SRC Techcon 2020 paper. Discusses vision of GPU-Centric architecture,\n  Erudite", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a vision for the Erudite architecture that redefines the compute\nand memory abstractions such that memory bandwidth and capacity become\nfirst-class citizens along with compute throughput. In this architecture, we\nenvision coupling a high-density, massively parallel memory technology like\nFlash with programmable near-data accelerators, like the streaming\nmultiprocessors in modern GPUs. Each accelerator has a local pool of\nstorage-class memory that it can access at high throughput by initiating very\nlarge numbers of overlapping requests that help to tolerate long access\nlatency. The accelerators can also communicate with each other and remote\nmemory through a high-throughput low-latency interconnect. As a result, systems\nbased on the Erudite architecture scale compute and memory bandwidth at the\nsame rate, tearing down the notorious memory wall that has plagued computer\narchitecture for generations. In this paper, we present the motivation,\nrationale, design, benefit, and research challenges for Erudite.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 03:07:23 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Qureshi", "Zaid", ""], ["Mailthody", "Vikram Sharma", ""], ["Min", "Seung Won", ""], ["Chung", "I-Hsin", ""], ["Xiong", "Jinjun", ""], ["Hwu", "Wen-mei", ""]]}, {"id": "2008.10271", "submitter": "Bharath Comandur", "authors": "Bharath Comandur and Avinash C. Kak", "title": "Semantic Labeling of Large-Area Geographic Regions Using Multi-View and\n  Multi-Date Satellite Images and Noisy OSM Training Labels", "comments": "This work has been accepted by the IEEE for publication. Copyright\n  may be transferred without notice, after which this version may no longer be\n  accessible", "journal-ref": null, "doi": "10.1109/JSTARS.2021.3066944", "report-no": null, "categories": "cs.CV cs.DC cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel multi-view training framework and CNN architecture for\ncombining information from multiple overlapping satellite images and noisy\ntraining labels derived from OpenStreetMap (OSM) to semantically label\nbuildings and roads across large geographic regions (100 km$^2$). Our approach\nto multi-view semantic segmentation yields a 4-7% improvement in the per-class\nIoU scores compared to the traditional approaches that use the views\nindependently of one another. A unique (and, perhaps, surprising) property of\nour system is that modifications that are added to the tail-end of the CNN for\nlearning from the multi-view data can be discarded at the time of inference\nwith a relatively small penalty in the overall performance. This implies that\nthe benefits of training using multiple views are absorbed by all the layers of\nthe network. Additionally, our approach only adds a small overhead in terms of\nthe GPU-memory consumption even when training with as many as 32 views per\nscene. The system we present is end-to-end automated, which facilitates\ncomparing the classifiers trained directly on true orthophotos vis-a-vis first\ntraining them on the off-nadir images and subsequently translating the\npredicted labels to geographical coordinates. With no human supervision, our\nIoU scores for the buildings and roads classes are 0.8 and 0.64 respectively\nwhich are better than state-of-the-art approaches that use OSM labels and that\nare not completely automated.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 09:03:31 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 14:17:24 GMT"}, {"version": "v3", "created": "Sat, 28 Nov 2020 15:43:32 GMT"}, {"version": "v4", "created": "Thu, 1 Apr 2021 07:10:30 GMT"}, {"version": "v5", "created": "Sun, 27 Jun 2021 02:50:21 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Comandur", "Bharath", ""], ["Kak", "Avinash C.", ""]]}, {"id": "2008.10311", "submitter": "Peter Majer", "authors": "Igor Beati, Eliana Andreica, Peter Majer", "title": "ImarisWriter: Open Source Software for Storage of Large Images in\n  Blockwise Multi-Resolution Format", "comments": "7 pages, 3 figures, 2 tables, 2 program code listings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We publish as open source a high performance file writer library to store\nlarge images in the IMS format for high performance visualization and analysis.\nThe library is capable of writing images at high speed. In just over 1 minute\nit can write a 100GB image to disk. The library takes care of all the details\nof multi-resolution resampling, chunking, compression, and multi-threading. It\nmakes high performance writing of images in a useful format very easy.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 10:34:03 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Beati", "Igor", ""], ["Andreica", "Eliana", ""], ["Majer", "Peter", ""]]}, {"id": "2008.10422", "submitter": "Hongchang Gao", "authors": "Hongchang Gao, Heng Huang", "title": "Adaptive Serverless Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the emergence of distributed data, training machine learning models in\nthe serverless manner has attracted increasing attention in recent years.\nNumerous training approaches have been proposed in this regime, such as\ndecentralized SGD. However, all existing decentralized algorithms only focus on\nstandard SGD. It might not be suitable for some applications, such as deep\nfactorization machine in which the feature is highly sparse and categorical so\nthat the adaptive training algorithm is needed. In this paper, we propose a\nnovel adaptive decentralized training approach, which can compute the learning\nrate from data dynamically. To the best of our knowledge, this is the first\nadaptive decentralized training approach. Our theoretical results reveal that\nthe proposed algorithm can achieve linear speedup with respect to the number of\nworkers. Moreover, to reduce the communication-efficient overhead, we further\npropose a communication-efficient adaptive decentralized training approach,\nwhich can also achieve linear speedup with respect to the number of workers. At\nlast, extensive experiments on different tasks have confirmed the effectiveness\nof our proposed two approaches.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 13:23:02 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Gao", "Hongchang", ""], ["Huang", "Heng", ""]]}, {"id": "2008.10596", "submitter": "Gene Cooperman", "authors": "Twinkle Jain, Gene Cooperman", "title": "CRAC: Checkpoint-Restart Architecture for CUDA with Streams and UVM", "comments": "24 pages, 6 figures, 3 tables; to appear in SC'20: The International\n  Conference for High Performance Computing, Networking, Storage, and Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The share of the top 500 supercomputers with NVIDIA GPUs is now over 25% and\ncontinues to grow. While fault tolerance is a critical issue for\nsupercomputing, there does not currently exist an efficient, scalable solution\nfor CUDA applications on NVIDIA GPUs. CRAC (Checkpoint-Restart Architecture for\nCUDA) is new checkpoint-restart solution for fault tolerance that supports the\nfull range of CUDA applications. CRAC combines: low runtime overhead\n(approximately 1% or less); fast checkpoint-restart; support for scalable CUDA\nstreams (for efficient usage of all of the thousands of GPU cores); and support\nfor the full features of Unified Virtual Memory (eliminating the programmer's\nburden of migrating memory between device and host). CRAC achieves its flexible\narchitecture by segregating application code (checkpointed) and its external\nGPU communication via non-reentrant CUDA libraries (not checkpointed) within a\nsingle process's memory. This eliminates the high overhead of inter-process\ncommunication in earlier approaches, and has fewer limitations.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 17:57:55 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Jain", "Twinkle", ""], ["Cooperman", "Gene", ""]]}, {"id": "2008.10604", "submitter": "Kris Nikov", "authors": "Kris Nikov (1), Jose L. Nunez-Yanez (1) and Matthew Horsnell (2) ((1)\n  University of Bristol, UK, (2) ARM Ltd., UK)", "title": "Evaluation of hybrid run-time power models for the ARM big.LITTLE\n  architecture", "comments": "6 pages, 8 fugures. 2015 IEEE 13th International Conference on\n  Embedded and Ubiquitous Computing", "journal-ref": "2015 IEEE 13th International Conference on Embedded and Ubiquitous\n  Computing, Porto, 2015, pp. 205-210", "doi": "10.1109/EUC.2015.32", "report-no": null, "categories": "cs.AR cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Heterogeneous processors, formed by binary compatible CPU cores with\ndifferent microarchitectures, enable energy reductions by better matching\nprocessing capabilities and software application requirements. This new\nhardware platform requires novel techniques to manage power and energy to fully\nutilize its capabilities, particularly regarding the mapping of workloads to\nappropriate cores. In this paper we validate relevant published work related to\npower modelling for heterogeneous systems and propose a new approach for\ndeveloping run-time power models that uses a hybrid set of physical predictors,\nperformance events and CPU state information. We demonstrate the accuracy of\nthis approach compared with the state-of-the-art and its applicability to\nenergy aware scheduling. Our results are obtained on a commercially available\nplatform built around the Samsung Exynos 5 Octa SoC, which features the ARM\nbig.LITTLE heterogeneous architecture.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 15:30:21 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Nikov", "Kris", ""], ["Nunez-Yanez", "Jose L.", ""], ["Horsnell", "Matthew", ""]]}, {"id": "2008.10640", "submitter": "Julian Pistorius", "authors": "Julian L. Pistorius (The University of Arizona), Chris Martin\n  (Exosphere Project), Sanjana Sudarshan (Indiana University), David S. LeBauer\n  (The University of Arizona)", "title": "Exosphere -- Bringing The Cloud Closer", "comments": "6 pages, 4 figures, submitted to \"SC20 Workshop - SuperCompCloud: 3rd\n  Workshop on Interoperability of Supercomputing and Cloud Technologies\",\n  https://sites.google.com/view/supercompcloud", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exosphere provides researcher-friendly software for managing computing\nworkloads on OpenStack cloud infrastructure. Exosphere is a user-friendly\nalternative to Horizon, the default OpenStack graphical interface. Exosphere\ncan be used with most research cloud infrastructure, requiring near-zero custom\nintegration work.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 18:19:20 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 22:36:17 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Pistorius", "Julian L.", "", "The University of Arizona"], ["Martin", "Chris", "", "Exosphere Project"], ["Sudarshan", "Sanjana", "", "Indiana University"], ["LeBauer", "David S.", "", "The University of Arizona"]]}, {"id": "2008.11110", "submitter": "Simon Eismann", "authors": "Simon Eismann, Joel Scheuner, Erwin van Eyk, Maximilian Schwinger,\n  Johannes Grohmann, Nikolas Herbst, Cristina L. Abad, Alexandru Iosup", "title": "A Review of Serverless Use Cases and their Characteristics", "comments": "47 pages, 29 figures, SPEC RG technical report", "journal-ref": null, "doi": null, "report-no": "SPEC-RG-2020-8", "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The serverless computing paradigm promises many desirable properties for\ncloud applications - low-cost, fine-grained deployment, and management-free\noperation. Consequently, the paradigm has underwent rapid growth: there\ncurrently exist tens of serverless platforms and all global cloud providers\nhost serverless operations. To help tune existing platforms, guide the design\nof new serverless approaches, and overall contribute to understanding this\nparadigm, in this work we present a long-term, comprehensive effort to\nidentify, collect, and characterize 89 serverless use cases. We survey use\ncases, sourced from white and grey literature, and from consultations with\nexperts in areas such as scientific computing. We study each use case using 24\ncharacteristics, including general aspects, but also workload, application, and\nrequirements. When the use cases employ workflows, we further analyze their\ncharacteristics. Overall, we hope our study will be useful for both academia\nand industry, and encourage the community to further share and communicate\ntheir use cases.\n  This article appears also as a SPEC Technical Report:\nhttps://research.spec.org/fileadmin/user_upload/documents/rg_cloud/endorsed_publications/SPEC_RG_2020_Serverless_Usecases.pdf\n  The article may be submitted for peer-reviewed publication.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 15:38:34 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 09:43:34 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Eismann", "Simon", ""], ["Scheuner", "Joel", ""], ["van Eyk", "Erwin", ""], ["Schwinger", "Maximilian", ""], ["Grohmann", "Johannes", ""], ["Herbst", "Nikolas", ""], ["Abad", "Cristina L.", ""], ["Iosup", "Alexandru", ""]]}, {"id": "2008.11141", "submitter": "Mohammad Mohammadi Amiri Dr.", "authors": "Mohammad Mohammadi Amiri, Deniz Gunduz, Sanjeev R. Kulkarni, H.\n  Vincent Poor", "title": "Convergence of Federated Learning over a Noisy Downlink", "comments": "submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.LG eess.SP math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study federated learning (FL), where power-limited wireless devices\nutilize their local datasets to collaboratively train a global model with the\nhelp of a remote parameter server (PS). The PS has access to the global model\nand shares it with the devices for local training, and the devices return the\nresult of their local updates to the PS to update the global model. This\nframework requires downlink transmission from the PS to the devices and uplink\ntransmission from the devices to the PS. The goal of this study is to\ninvestigate the impact of the bandwidth-limited shared wireless medium in both\nthe downlink and uplink on the performance of FL with a focus on the downlink.\nTo this end, the downlink and uplink channels are modeled as fading broadcast\nand multiple access channels, respectively, both with limited bandwidth. For\ndownlink transmission, we first introduce a digital approach, where a\nquantization technique is employed at the PS to broadcast the global model\nupdate at a common rate such that all the devices can decode it. Next, we\npropose analog downlink transmission, where the global model is broadcast by\nthe PS in an uncoded manner. We consider analog transmission over the uplink in\nboth cases. We further analyze the convergence behavior of the proposed analog\napproach assuming that the uplink transmission is error-free. Numerical\nexperiments show that the analog downlink approach provides significant\nimprovement over the digital one, despite a significantly lower transmit power\nat the PS. The experimental results corroborate the convergence results, and\nshow that a smaller number of local iterations should be used when the data\ndistribution is more biased, and also when the devices have a better estimate\nof the global model in the analog downlink approach.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 16:15:05 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Amiri", "Mohammad Mohammadi", ""], ["Gunduz", "Deniz", ""], ["Kulkarni", "Sanjeev R.", ""], ["Poor", "H. Vincent", ""]]}, {"id": "2008.11235", "submitter": "Stefan Zellmann", "authors": "Stefan Zellmann and Martin Weier and Ingo Wald", "title": "Accelerating Force-Directed Graph Drawing with RT Cores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph drawing with spring embedders employs a V x V computation phase over\nthe graph's vertex set to compute repulsive forces. Here, the efficacy of\nforces diminishes with distance: a vertex can effectively only influence other\nvertices in a certain radius around its position. Therefore, the algorithm\nlends itself to an implementation using search data structures to reduce the\nruntime complexity. NVIDIA RT cores implement hierarchical tree traversal in\nhardware. We show how to map the problem of finding graph layouts with\nforce-directed methods to a ray tracing problem that can subsequently be\nimplemented with dedicated ray tracing hardware. With that, we observe speedups\nof 4x to 13x over a CUDA software implementation.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 18:57:54 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Zellmann", "Stefan", ""], ["Weier", "Martin", ""], ["Wald", "Ingo", ""]]}, {"id": "2008.11281", "submitter": "Dimitris Stripelis", "authors": "Dimitris Stripelis and Jose Luis Ambite", "title": "Accelerating Federated Learning in Heterogeneous Data and Computational\n  Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are situations where data relevant to a machine learning problem are\ndistributed among multiple locations that cannot share the data due to\nregulatory, competitiveness, or privacy reasons. For example, data present in\nusers' cellphones, manufacturing data of companies in a given industrial\nsector, or medical records located at different hospitals. Moreover,\nparticipating sites often have different data distributions and computational\ncapabilities. Federated Learning provides an approach to learn a joint model\nover all the available data in these environments. In this paper, we introduce\na novel distributed validation weighting scheme (DVW), which evaluates the\nperformance of a learner in the federation against a distributed validation\nset. Each learner reserves a small portion (e.g., 5%) of its local training\nexamples as a validation dataset and allows other learners models to be\nevaluated against it. We empirically show that DVW results in better\nperformance compared to established methods, such as FedAvg, both under\nsynchronous and asynchronous communication protocols in data and\ncomputationally heterogeneous environments.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 21:28:38 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Stripelis", "Dimitris", ""], ["Ambite", "Jose Luis", ""]]}, {"id": "2008.11321", "submitter": "Maciej Besta", "authors": "Maciej Besta, Armon Carigiet, Zur Vonarburg-Shmaria, Kacper Janda,\n  Lukas Gianinazzi, Torsten Hoefler", "title": "High-Performance Parallel Graph Coloring with Strong Guarantees on Work,\n  Depth, and Quality", "comments": null, "journal-ref": "Proceedings of the ACM/IEEE International Conference on High\n  Performance Computing, Networking, Storage and Analysis (SC20), November 2020", "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop the first parallel graph coloring heuristics with strong\ntheoretical guarantees on work and depth and coloring quality. The key idea is\nto design a relaxation of the vertex degeneracy order, a well-known graph\ntheory concept, and to color vertices in the order dictated by this relaxation.\nThis introduces a tunable amount of parallelism into the degeneracy ordering\nthat is otherwise hard to parallelize. This simple idea enables significant\nbenefits in several key aspects of graph coloring. For example, one of our\nalgorithms ensures polylogarithmic depth and a bound on the number of used\ncolors that is superior to all other parallelizable schemes, while maintaining\nwork-efficiency. In addition to provable guarantees, the developed algorithms\nhave competitive run-times for several real-world graphs, while almost always\nproviding superior coloring quality. Our degeneracy ordering relaxation is of\nseparate interest for algorithms outside the context of coloring.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 00:52:33 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 22:56:42 GMT"}, {"version": "v3", "created": "Wed, 11 Nov 2020 15:59:26 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Besta", "Maciej", ""], ["Carigiet", "Armon", ""], ["Vonarburg-Shmaria", "Zur", ""], ["Janda", "Kacper", ""], ["Gianinazzi", "Lukas", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2008.11326", "submitter": "Charlene Yang", "authors": "Charlene Yang", "title": "8 Steps to 3.7 TFLOP/s on NVIDIA V100 GPU: Roofline Analysis and Other\n  Tricks", "comments": "5 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance optimization can be a daunting task especially as the hardware\narchitecture becomes more and more complex. This paper takes a kernel from the\nMaterials Science code BerkeleyGW, and demonstrates a few performance analysis\nand optimization techniques. Despite challenges such as high register usage,\nlow occupancy, complex data access patterns, and the existence of several\nlong-latency instructions, we have achieved 3.7 TFLOP/s of double-precision\nperformance on an NVIDIA V100 GPU, with 8 optimization steps. This is 55% of\nthe theoretical peak, 6.7 TFLOP/s, at nominal frequency 1312 MHz, and 70% of\nthe more customized peak based on our 58% FMA ratio, 5.3 TFLOP/s. An array of\ntechniques used to analyze this OpenACC kernel and optimize its performance are\nshown, including the use of hierarchical Roofline performance model and the\nperformance tool Nsight Compute. This kernel exhibits computational\ncharacteristics that are commonly seen in many high-performance computing (HPC)\napplications, and are expected to be very helpful to a general audience of HPC\ndevelopers and computational scientists, as they pursue more performance on\nNVIDIA GPUs.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 01:09:24 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2020 01:45:05 GMT"}, {"version": "v3", "created": "Mon, 14 Sep 2020 05:08:36 GMT"}, {"version": "v4", "created": "Tue, 22 Sep 2020 20:21:12 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Yang", "Charlene", ""]]}, {"id": "2008.11343", "submitter": "Hanlin Tang", "authors": "Hanlin Tang, Shaoduo Gan, Samyam Rajbhandari, Xiangru Lian, Ji Liu,\n  Yuxiong He, Ce Zhang", "title": "APMSqueeze: A Communication Efficient Adam-Preconditioned Momentum SGD\n  Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adam is the important optimization algorithm to guarantee efficiency and\naccuracy for training many important tasks such as BERT and ImageNet. However,\nAdam is generally not compatible with information (gradient) compression\ntechnology. Therefore, the communication usually becomes the bottleneck for\nparallelizing Adam. In this paper, we propose a communication efficient {\\bf\nA}DAM {\\bf p}reconditioned {\\bf M}omentum SGD algorithm-- named APMSqueeze--\nthrough an error compensated method compressing gradients. The proposed\nalgorithm achieves a similar convergence efficiency to Adam in term of epochs,\nbut significantly reduces the running time per epoch. In terms of end-to-end\nperformance (including the full-precision pre-condition step), APMSqueeze is\nable to provide {sometimes by up to $2-10\\times$ speed-up depending on network\nbandwidth.} We also conduct theoretical analysis on the convergence and\nefficiency.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 02:20:23 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2020 03:59:08 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Tang", "Hanlin", ""], ["Gan", "Shaoduo", ""], ["Rajbhandari", "Samyam", ""], ["Lian", "Xiangru", ""], ["Liu", "Ji", ""], ["He", "Yuxiong", ""], ["Zhang", "Ce", ""]]}, {"id": "2008.11359", "submitter": "Yuwei Hu", "authors": "Yuwei Hu, Zihao Ye, Minjie Wang, Jiali Yu, Da Zheng, Mu Li, Zheng\n  Zhang, Zhiru Zhang, Yida Wang", "title": "FeatGraph: A Flexible and Efficient Backend for Graph Neural Network\n  Systems", "comments": "SC'20; changed all figures to type 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph neural networks (GNNs) are gaining increasing popularity as a promising\napproach to machine learning on graphs. Unlike traditional graph workloads\nwhere each vertex/edge is associated with a scalar, GNNs attach a feature\ntensor to each vertex/edge. This additional feature dimension, along with\nconsequently more complex vertex- and edge-wise computations, has enormous\nimplications on locality and parallelism, which existing graph processing\nsystems fail to exploit.\n  This paper proposes FeatGraph to accelerate GNN workloads by co-optimizing\ngraph traversal and feature dimension computation. FeatGraph provides a\nflexible programming interface to express diverse GNN models by composing\ncoarse-grained sparse templates with fine-grained user-defined functions (UDFs)\non each vertex/edge. FeatGraph incorporates optimizations for graph traversal\ninto the sparse templates and allows users to specify optimizations for UDFs\nwith a feature dimension schedule (FDS). FeatGraph speeds up end-to-end GNN\ntraining and inference by up to 32x on CPU and 7x on GPU.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 03:17:05 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 17:36:15 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Hu", "Yuwei", ""], ["Ye", "Zihao", ""], ["Wang", "Minjie", ""], ["Yu", "Jiali", ""], ["Zheng", "Da", ""], ["Li", "Mu", ""], ["Zhang", "Zheng", ""], ["Zhang", "Zhiru", ""], ["Wang", "Yida", ""]]}, {"id": "2008.11400", "submitter": "Flora D. Salim", "authors": "Manpreet Kaur, Flora D. Salim, Yongli Ren, Jeffrey Chan, Martin Tomko,\n  Mark Sanderson", "title": "Joint Modelling of Cyber Activities and Physical Context to Improve\n  Prediction of Visitor Behaviors", "comments": "Accepted in ACM Transactions on Sensor Networks, 2020", "journal-ref": "ACM Transactions on Sensor Networks, 2020", "doi": "10.1145/3393692", "report-no": null, "categories": "cs.IR cs.DC cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the Cyber-Physical behavior of users in a large\nindoor shopping mall by leveraging anonymized (opt in) Wi-Fi association and\nbrowsing logs recorded by the mall operators. Our analysis shows that many\nusers exhibit a high correlation between their cyber activities and their\nphysical context. To find this correlation, we propose a mechanism to\nsemantically label a physical space with rich categorical information from\nDBPedia concepts and compute a contextual similarity that represents a user's\nactivities with the mall context. We demonstrate the application of\ncyber-physical contextual similarity in two situations: user visit intent\nclassification and future location prediction. The experimental results\ndemonstrate that exploitation of contextual similarity significantly improves\nthe accuracy of such applications.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 06:37:43 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Kaur", "Manpreet", ""], ["Salim", "Flora D.", ""], ["Ren", "Yongli", ""], ["Chan", "Jeffrey", ""], ["Tomko", "Martin", ""], ["Sanderson", "Mark", ""]]}, {"id": "2008.11421", "submitter": "Mohamed Wahib", "authors": "Mohamed Wahib, Haoyu Zhang, Truong Thao Nguyen, Aleksandr Drozd, Jens\n  Domke, Lingqi Zhang, Ryousei Takano, Satoshi Matsuoka", "title": "Scaling Distributed Deep Learning Workloads beyond the Memory Capacity\n  with KARMA", "comments": "ACM/IEEE Proceedings of the International Conference for High\n  Performance Computing, Networking, Storage and Analysis (SC'20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dedicated memory of hardware accelerators can be insufficient to store\nall weights and/or intermediate states of large deep learning models. Although\nmodel parallelism is a viable approach to reduce the memory pressure issue,\nsignificant modification of the source code and considerations for algorithms\nare required. An alternative solution is to use out-of-core methods instead of,\nor in addition to, data parallelism. We propose a performance model based on\nthe concurrency analysis of out-of-core training behavior, and derive a\nstrategy that combines layer swapping and redundant recomputing. We achieve an\naverage of 1.52x speedup in six different models over the state-of-the-art\nout-of-core methods. We also introduce the first method to solve the\nchallenging problem of out-of-core multi-node training by carefully pipelining\ngradient exchanges and performing the parameter updates on the host. Our data\nparallel out-of-core solution can outperform complex hybrid model parallelism\nin training large models, e.g. Megatron-LM and Turning-NLG.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 07:24:34 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Wahib", "Mohamed", ""], ["Zhang", "Haoyu", ""], ["Nguyen", "Truong Thao", ""], ["Drozd", "Aleksandr", ""], ["Domke", "Jens", ""], ["Zhang", "Lingqi", ""], ["Takano", "Ryousei", ""], ["Matsuoka", "Satoshi", ""]]}, {"id": "2008.11476", "submitter": "M. Akif \\\"Ozkan", "authors": "M. Akif \\\"Ozkan, Burak Ok, Bo Qiao, J\\\"urgen Teich, Frank Hannig", "title": "HipaccVX: Wedding of OpenVX and DSL-based Code Generation", "comments": null, "journal-ref": "Journal of Real-Time Image Processing, 2020", "doi": "10.1007/s11554-020-01015-5", "report-no": null, "categories": "cs.CV cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Writing programs for heterogeneous platforms optimized for high performance\nis hard since this requires the code to be tuned at a low level with\narchitecture-specific optimizations that are most times based on fundamentally\ndiffering programming paradigms and languages. OpenVX promises to solve this\nissue for computer vision applications with a royalty-free industry standard\nthat is based on a graph-execution model. Yet, the OpenVX' algorithm space is\nconstrained to a small set of vision functions. This hinders accelerating\ncomputations that are not included in the standard.\n  In this paper, we analyze OpenVX vision functions to find an orthogonal set\nof computational abstractions. Based on these abstractions, we couple an\nexisting Domain-Specific Language (DSL) back end to the OpenVX environment and\nprovide language constructs to the programmer for the definition of\nuser-defined nodes. In this way, we enable optimizations that are not possible\nto detect with OpenVX graph implementations using the standard computer vision\nfunctions. These optimizations can double the throughput on an Nvidia GTX GPU\nand decrease the resource usage of a Xilinx Zynq FPGA by 50% for our\nbenchmarks. Finally, we show that our proposed compiler framework, called\nHipaccVX, can achieve better results than the state-of-the-art approaches\nNvidia VisionWorks and Halide-HLS.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 10:30:18 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["\u00d6zkan", "M. Akif", ""], ["Ok", "Burak", ""], ["Qiao", "Bo", ""], ["Teich", "J\u00fcrgen", ""], ["Hannig", "Frank", ""]]}, {"id": "2008.11560", "submitter": "Weiming Zhuang", "authors": "Weiming Zhuang, Yonggang Wen, Xuesen Zhang, Xin Gan, Daiying Yin,\n  Dongzhan Zhou, Shuai Zhang, Shuai Yi", "title": "Performance Optimization for Federated Person Re-identification via\n  Benchmark Analysis", "comments": "ACMMM'20", "journal-ref": null, "doi": "10.1145/3394171.3413814", "report-no": null, "categories": "cs.CV cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning is a privacy-preserving machine learning technique that\nlearns a shared model across decentralized clients. It can alleviate privacy\nconcerns of personal re-identification, an important computer vision task. In\nthis work, we implement federated learning to person re-identification\n(FedReID) and optimize its performance affected by statistical heterogeneity in\nthe real-world scenario. We first construct a new benchmark to investigate the\nperformance of FedReID. This benchmark consists of (1) nine datasets with\ndifferent volumes sourced from different domains to simulate the heterogeneous\nsituation in reality, (2) two federated scenarios, and (3) an enhanced\nfederated algorithm for FedReID. The benchmark analysis shows that the\nclient-edge-cloud architecture, represented by the federated-by-dataset\nscenario, has better performance than client-server architecture in FedReID. It\nalso reveals the bottlenecks of FedReID under the real-world scenario,\nincluding poor performance of large datasets caused by unbalanced weights in\nmodel aggregation and challenges in convergence. Then we propose two\noptimization methods: (1) To address the unbalanced weight problem, we propose\na new method to dynamically change the weights according to the scale of model\nchanges in clients in each training round; (2) To facilitate convergence, we\nadopt knowledge distillation to refine the server model with knowledge\ngenerated from client models on a public dataset. Experiment results\ndemonstrate that our strategies can achieve much better convergence with\nsuperior performance on all datasets. We believe that our work will inspire the\ncommunity to further explore the implementation of federated learning on more\ncomputer vision tasks in real-world scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 13:41:20 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 17:57:52 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Zhuang", "Weiming", ""], ["Wen", "Yonggang", ""], ["Zhang", "Xuesen", ""], ["Gan", "Xin", ""], ["Yin", "Daiying", ""], ["Zhou", "Dongzhan", ""], ["Zhang", "Shuai", ""], ["Yi", "Shuai", ""]]}, {"id": "2008.11585", "submitter": "Dabas Payal", "authors": "Payal and Sangita Kansal", "title": "Logic Signed Petri Net", "comments": "arXiv admin note: text overlap with arXiv:2001.04374", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper,the authors show the versatility of the Signed Petri Net (SPN)\nintroduced by them by showing the equivalence between a Logic Signed Petri Net\n(LSPN) and Logic Petri Net (LPN).The capacity of each place in all these nets\nis at most one, i.e.,a place has either zero or one token in it.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 04:17:31 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Payal", "", ""], ["Kansal", "Sangita", ""]]}, {"id": "2008.11601", "submitter": "Christian G\\\"ottel", "authors": "Christina M\\\"uller, Marcus Brandenburger, Christian Cachin, Pascal\n  Felber, Christian G\\\"ottel, Valerio Schiavoni", "title": "TZ4Fabric: Executing Smart Contracts with ARM TrustZone", "comments": "European Commission Projet: LEGaTO - Low Energy Toolset for\n  Heterogeneous Computing (EC-H2020-780681)", "journal-ref": "2020 International Symposium on Reliable Distributed Systems\n  (SRDS), Shanghai, China, 2020, pp. 31-40", "doi": "10.1109/SRDS51746.2020.00011", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain technology promises to revolutionize manufacturing industries. For\nexample, several supply-chain use-cases may benefit from transparent asset\ntracking and automated processes using smart contracts. Several real-world\ndeployments exist where the transparency aspect of a blockchain is both an\nadvantage and a disadvantage at the same time. The exposure of assets and\nbusiness interaction represent critical risks. However, there are typically no\nconfidentiality guarantees to protect the smart contract logic as well as the\nprocessed data. Trusted execution environments (TEE) are an emerging technology\navailable in both edge or mobile-grade processors (e.g., Arm TrustZone) and\nserver-grade processors (e.g., Intel SGX). TEEs shield both code and data from\nmalicious attackers. This practical experience report presents TZ4Fabric, an\nextension of Hyperledger Fabric to leverage Arm TrustZone for the secure\nexecution of smart contracts. Our design minimizes the trusted computing base\nexecuted by avoiding the execution of a whole Hyperledger Fabric node inside\nthe TEE, which continues to run in untrusted environment. Instead, we restrict\nit to the execution of only the smart contract. The TZ4Fabric prototype\nexploits the open-source OP-TEE framework, as it supports deployments on cheap\nlow-end devices (e.g., Raspberry Pis). Our experimental results highlight the\nperformance trade-off due to the additional security guarantees provided by Arm\nTrustZone. TZ4Fabric will be released as open-source.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 14:53:51 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 12:14:58 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2020 17:10:56 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["M\u00fcller", "Christina", ""], ["Brandenburger", "Marcus", ""], ["Cachin", "Christian", ""], ["Felber", "Pascal", ""], ["G\u00f6ttel", "Christian", ""], ["Schiavoni", "Valerio", ""]]}, {"id": "2008.11617", "submitter": "Anthony Chronopoulos", "authors": "Zheng Xiao, Dan He, Yu Chen, Anthony Theodore Chronopoulos, Schahram\n  Dustdar, Jiayi Du", "title": "A Bilateral Game Approach for Task Outsourcing in Multi-access Edge\n  Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-access edge computing (MEC) is a promising architecture to provide\nlow-latency applications for future Internet of Things (IoT)-based network\nsystems. Together with the increasing scholarly attention on task offloading,\nthe problem of edge servers' resource allocation has been widely studied. Most\nof previous works focus on a single edge server (ES) serving multiple terminal\nentities (TEs), which restricts their access to sufficient resources. In this\npaper, we consider a MEC resource transaction market with multiple ESs and\nmultiple TEs, which are interdependent and mutually influence each other.\nHowever, this many-to-many interaction requires resolving several problems,\nincluding task allocation, TEs' selection on ESs and conflicting interests of\nboth parties. Game theory can be used as an effective tool to realize the\ninterests of two or more conflicting individuals in the trading market.\nTherefore, we propose a bilateral game framework among multiple ESs and\nmultiple TEs by modeling the task outsourcing problem as two noncooperative\ngames: the supplier and customer side games. In the first game, the supply\nfunction bidding mechanism is employed to model the ESs' profit maximization\nproblem. The ESs submit their bids to the scheduler, where the computing\nservice price is computed and sent to the TEs. While in the second game, TEs\ndetermine the optimal demand profiles according to ESs' bids to maximize their\npayoff. The existence and uniqueness of the Nash equilibrium in the\naforementioned games are proved. A distributed task outsourcing algorithm\n(DTOA) is designed to determine the equilibrium. Simulation results have\ndemonstrated the superior performance of DTOA in increasing the ESs' profit and\nTEs' payoff, as well as flattening the peak and off-peak load.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 15:15:25 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Xiao", "Zheng", ""], ["He", "Dan", ""], ["Chen", "Yu", ""], ["Chronopoulos", "Anthony Theodore", ""], ["Dustdar", "Schahram", ""], ["Du", "Jiayi", ""]]}, {"id": "2008.11675", "submitter": "Karthee Sivalingam", "authors": "Nina Mujkanovic and Karthee Sivalingam and Alfio Lazzaro", "title": "Optimising AI Training Deployments using Graph Compilers and Containers", "comments": "HPEC IEEE, 6 pages, 5 figues, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Intelligence (AI) applications based on Deep Neural Networks (DNN)\nor Deep Learning (DL) have become popular due to their success in solving\nproblems likeimage analysis and speech recognition. Training a DNN is\ncomputationally intensive and High Performance Computing(HPC) has been a key\ndriver in AI growth. Virtualisation and container technology have led to the\nconvergence of cloud and HPC infrastructure. These infrastructures with diverse\nhardware increase the complexity of deploying and optimising AI training\nworkloads. AI training deployments in HPC or cloud can be optimised with\ntarget-specific libraries, graph compilers, andby improving data movement or\nIO. Graph compilers aim to optimise the execution of a DNN graph by generating\nan optimised code for a target hardware/backend. As part of SODALITE (a Horizon\n2020 project), MODAK tool is developed to optimise application deployment in\nsoftware defined infrastructures. Using input from the data scientist and\nperformance modelling, MODAK maps optimal application parameters to a target\ninfrastructure and builds an optimised container. In this paper, we introduce\nMODAK and review container technologies and graph compilers for AI. We\nillustrate optimisation of AI training deployments using graph compilers and\nSingularity containers. Evaluation using MNIST-CNN and ResNet50 training\nworkloads shows that custom built optimised containers outperform the official\nimages from DockerHub. We also found that the performance of graph compilers\ndepends on the target hardware and the complexity of the neural network.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 16:58:32 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 09:23:06 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Mujkanovic", "Nina", ""], ["Sivalingam", "Karthee", ""], ["Lazzaro", "Alfio", ""]]}, {"id": "2008.11799", "submitter": "Robert Haase", "authors": "Daniela Vorkel and Robert Haase", "title": "GPU-accelerating ImageJ Macro image processing workflows using CLIJ", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This chapter introduces GPU-accelerated image processing in ImageJ/FIJI. The\nreader is expected to have some pre-existing knowledge of ImageJ Macro\nprogramming. Core concepts such as variables, for-loops, and functions are\nessential. The chapter provides basic guidelines for improved performance in\ntypical image processing workflows. We present in a step-by-step tutorial how\nto translate a pre-existing ImageJ macro into a GPU-accelerated macro.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 20:38:31 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Vorkel", "Daniela", ""], ["Haase", "Robert", ""]]}, {"id": "2008.11803", "submitter": "Abdullah Yousafzai", "authors": "Abdullah Yousafzai and Choong Seon Hong", "title": "SmartSON:A Smart contract driven incentive management framework for\n  Self-Organizing Networks", "comments": "Incentive Management Framework for Self Organizing Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MA cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proposes a self-organizing collaborative computing network with\nan approach to enhance the expectation of a collaborating node for joining the\nself-organizing network. The proposed approach relies on Ethereum\ncryptocurrency and Smart Contract to enhance the expectation of collaborating\nnodes by monetizing the services provided to the self-organizing network.\nFurthermore, an escrow based smart contract is formalized in the proposed\nframework to sustains the monetary trust issue between collaborating nodes. The\nproposed scheme can enforce an autonomic incentive management mechanism to any\ntype of self-organizing networks such as self-organizing clouds, ad-hoc\nnetworks, self-organizing federated cloud networks, self-organizing federated\nlearning networks, and self-organizing D2D networks to name a few. Considering\nthe distributed nature of these self-organizing networks and the Ethereum\nblockchain network, a distributed agent-based methodology is materialized in\nthe proposed framework. Following this, a proof of concept implementation for\nthe general case of a self-organizing cloud is presented. Lastly, the article\nprovides some insights into possible future directions using the proposed\nframework.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 20:41:22 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Yousafzai", "Abdullah", ""], ["Hong", "Choong Seon", ""]]}, {"id": "2008.11832", "submitter": "Wenqian Dong", "authors": "Wenqian Dong, Jie Liu, Zhen Xie and Dong Li", "title": "Adaptive Neural Network-Based Approximation to Accelerate Eulerian Fluid\n  Simulation", "comments": null, "journal-ref": null, "doi": "10.1145/3295500.3356147", "report-no": null, "categories": "cs.LG cs.DC physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Eulerian fluid simulation is an important HPC application. The neural\nnetwork has been applied to accelerate it. The current methods that accelerate\nthe fluid simulation with neural networks lack flexibility and generalization.\nIn this paper, we tackle the above limitation and aim to enhance the\napplicability of neural networks in the Eulerian fluid simulation. We introduce\nSmartfluidnet, a framework that automates model generation and application.\nGiven an existing neural network as input, Smartfluidnet generates multiple\nneural networks before the simulation to meet the execution time and simulation\nquality requirement. During the simulation, Smartfluidnet dynamically switches\nthe neural networks to make the best efforts to reach the user requirement on\nsimulation quality. Evaluating with 20,480 input problems, we show that\nSmartfluidnet achieves 1.46x and 590x speedup comparing with a state-of-the-art\nneural network model and the original fluid simulation respectively on an\nNVIDIA Titan X Pascal GPU, while providing better simulation quality than the\nstate-of-the-art model.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 21:44:44 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Dong", "Wenqian", ""], ["Liu", "Jie", ""], ["Xie", "Zhen", ""], ["Li", "Dong", ""]]}, {"id": "2008.11837", "submitter": "Xiong Zheng", "authors": "Vijay Garg, Saptaparni Kumar, Lewis Tseng, Xiong Zheng", "title": "Amortized Constant Round Atomic Snapshot in Message-Passing Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the lattice agreement (LA) and atomic snapshot problems in\nasynchronous message-passing systems where up to $f$ nodes may crash. Our main\nresult is a crash-tolerant atomic snapshot algorithm with \\textit{amortized\nconstant round complexity}. To the best of our knowledge, the best prior result\nis given by Delporte et al. [TPDS, 18] with amortized $O(n)$ complexity if\nthere are more scans than updates. Our algorithm achieves amortized constant\nround if there are $\\Omega(\\sqrt{k})$ operations, where $k$ is the number of\nactual failures in an execution and is bounded by $f$. Moreover, when there is\nno failure, our algorithm has $O(1)$ round complexity unconditionally. To\nachieve amortized constant round complexity, we devise a simple\n\\textit{early-stopping} lattice agreement algorithm and use it to \"order\" the\nupdate and scan operations for our snapshot object. Our LA algorithm has\n$O(\\sqrt{k})$ round complexity. It is the first early-stopping LA algorithm in\nasynchronous systems.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 21:48:55 GMT"}, {"version": "v2", "created": "Sat, 29 Aug 2020 20:36:53 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Garg", "Vijay", ""], ["Kumar", "Saptaparni", ""], ["Tseng", "Lewis", ""], ["Zheng", "Xiong", ""]]}, {"id": "2008.11839", "submitter": "Changwan Hong", "authors": "Changwan Hong, Laxman Dhulipala, Julian Shun", "title": "Exploring the Design Space of Static and Incremental Graph Connectivity\n  Algorithms on GPUs", "comments": null, "journal-ref": "Proceedings of the 2020 International Conference on Parallel\n  Architectures and Compilation Techniques", "doi": "10.1145/3410463.3414657", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Connected components and spanning forest are fundamental graph algorithms due\nto their use in many important applications, such as graph clustering and image\nsegmentation. GPUs are an ideal platform for graph algorithms due to their high\npeak performance and memory bandwidth. While there exist several GPU\nconnectivity algorithms in the literature, many design choices have not yet\nbeen explored. In this paper, we explore various design choices in GPU\nconnectivity algorithms, including sampling, linking, and tree compression, for\nboth the static as well as the incremental setting. Our various design choices\nlead to over 300 new GPU implementations of connectivity, many of which\noutperform state-of-the-art. We present an experimental evaluation, and show\nthat we achieve an average speedup of 2.47x speedup over existing static\nalgorithms. In the incremental setting, we achieve a throughput of up to 48.23\nbillion edges per second. Compared to state-of-the-art CPU implementations on a\n72-core machine, we achieve a speedup of 8.26--14.51x for static connectivity\nand 1.85--13.36x for incremental connectivity using a Tesla V100 GPU.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 21:50:11 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Hong", "Changwan", ""], ["Dhulipala", "Laxman", ""], ["Shun", "Julian", ""]]}, {"id": "2008.11868", "submitter": "Landon Cox", "authors": "HyunJong Lee, Shadi Noghabi, Brian Noble, Matthew Furlong, and Landon\n  P. Cox", "title": "BumbleBee: Application-aware adaptation for container orchestration", "comments": "This version fixes problems (e.g., with the video-streaming\n  experiments) from the previous versions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern applications have embraced separation of concerns as a first-order\norganizing principle through the use of containers, container orchestration,\nand service meshes. However, adaptation to unexpected network variation has not\nfollowed suit. We present BumbleBee, a lightweight extension to the container\necosystem that supports application-aware adaptation. BumbleBee provides a\nsimple abstraction for making decisions about network data using application\nsemantics. Because this abstraction is placed within the communications\nframework of a modern service mesh, it is closer to the point at which changes\nare detected, providing more responsive and effective adaptation than possible\nat endpoints.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 00:20:00 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2020 18:18:42 GMT"}, {"version": "v3", "created": "Sun, 9 May 2021 19:21:04 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Lee", "HyunJong", ""], ["Noghabi", "Shadi", ""], ["Noble", "Brian", ""], ["Furlong", "Matthew", ""], ["Cox", "Landon P.", ""]]}, {"id": "2008.11881", "submitter": "Parth Mannan", "authors": "Parth Mannan, Ananda Samajdar and Tushar Krishna", "title": "CLAN: Continuous Learning using Asynchronous Neuroevolution on Commodity\n  Edge Devices", "comments": "Accepted and appears in ISPASS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advancements in machine learning algorithms, especially the\ndevelopment of Deep Neural Networks (DNNs) have transformed the landscape of\nArtificial Intelligence (AI). With every passing day, deep learning based\nmethods are applied to solve new problems with exceptional results. The portal\nto the real world is the edge. The true impact of AI can only be fully realized\nif we can have AI agents continuously interacting with the real world and\nsolving everyday problems. Unfortunately, high compute and memory requirements\nof DNNs acts a huge barrier towards this vision. Today we circumvent this\nproblem by deploying special purpose inference hardware on the edge while\nprocuring trained models from the cloud. This approach, however, relies on\nconstant interaction with the cloud for transmitting all the data, training on\nmassive GPU clusters, and downloading updated models. This is challenging for\nbandwidth, privacy, and constant connectivity concerns that autonomous agents\nmay exhibit. In this paper we evaluate techniques for enabling adaptive\nintelligence on edge devices with zero interaction with any high-end\ncloud/server. We build a prototype distributed system of Raspberry Pis\ncommunicating via WiFi running NeuroEvolutionary (NE) learning and inference.\nWe evaluate the performance of such a collaborative system and detail the\ncompute/communication characteristics of different arrangements of the system\nthat trade-off parallelism versus communication. Using insights from our\nanalysis, we also propose algorithmic modifications to reduce communication by\nup to 3.6x during the learning phase to enhance scalability even further and\nmatch performance of higher end computing devices at scale. We believe that\nthese insights will enable algorithm-hardware co-design efforts for enabling\ncontinuous learning on the edge.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 01:49:21 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Mannan", "Parth", ""], ["Samajdar", "Ananda", ""], ["Krishna", "Tushar", ""]]}, {"id": "2008.11900", "submitter": "Alberto Laender", "authors": "Robson A. Camp\\^elo, Marco A. Casanova, Dorgival O. Guedes, Alberto H.\n  F. Laender", "title": "A Brief Survey on Replica Consistency in Cloud Environments", "comments": "13 pages, 1 figure, 2 tables, 62 references", "journal-ref": "Journal of Internet Services and Applications, Volume 11, Number\n  1, 2020", "doi": "10.1186/s13174-020-0122-y", "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing is a general term that involves delivering hosted services\nover the Internet. With the accelerated growth of the volume of data used by\napplications, many organizations have moved their data into cloud servers to\nprovide scalable, reliable and highly available services. A particularly\nchallenging issue that arises in the context of cloud storage systems with\ngeographically-distributed data replication is how to reach a consistent state\nfor all replicas. This survey reviews major aspects related to consistency\nissues in cloud data storage systems, categorizing recently proposed methods\ninto three categories: (1) fixed consistency methods, (2) configurable\nconsistency methods and (3) consistency monitoring methods.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 03:30:35 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2020 12:03:39 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Camp\u00ealo", "Robson A.", ""], ["Casanova", "Marco A.", ""], ["Guedes", "Dorgival O.", ""], ["Laender", "Alberto H. F.", ""]]}, {"id": "2008.12144", "submitter": "Jesper Larsson Tr\\\"aff", "authors": "Jesper Larsson Tr\\\"aff", "title": "$k$-ported vs. $k$-lane Broadcast, Scatter, and Alltoall Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In $k$-ported message-passing systems, a processor can simultaneously receive\n$k$ different messages from $k$ other processors, and send $k$ different\nmessages to $k$ other processors that may or may not be different from the\nprocessors from which messages are received. Modern clustered systems may not\nhave such capabilities. Instead, compute nodes consisting of $n$ processors can\nsimultaneously send and receive $k$ messages from other nodes, by letting $k$\nprocessors on the nodes concurrently send and receive at most one message. We\npose the question of how to design good algorithms for this $k$-lane model,\npossibly by adapting algorithms devised for the traditional $k$-ported model.\nWe discuss and compare a number of (non-optimal) $k$-lane algorithms for the\nbroadcast, scatter and alltoall collective operations (as found in, e.g., MPI),\nand experimentally evaluate these on a small $36\\times 32$-node cluster with a\ndual OmniPath network (corresponding to $k=2$). Results are preliminary.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 14:14:12 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Tr\u00e4ff", "Jesper Larsson", ""]]}, {"id": "2008.12161", "submitter": "Xinyi Xu Mr", "authors": "Lingjuan Lyu, Xinyi Xu, and Qian Wang", "title": "Collaborative Fairness in Federated Learning", "comments": "accepted to FL-IJCAI'20 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In current deep learning paradigms, local training or the Standalone\nframework tends to result in overfitting and thus poor generalizability. This\nproblem can be addressed by Distributed or Federated Learning (FL) that\nleverages a parameter server to aggregate model updates from individual\nparticipants. However, most existing Distributed or FL frameworks have\noverlooked an important aspect of participation: collaborative fairness. In\nparticular, all participants can receive the same or similar models, regardless\nof their contributions. To address this issue, we investigate the collaborative\nfairness in FL, and propose a novel Collaborative Fair Federated Learning\n(CFFL) framework which utilizes reputation to enforce participants to converge\nto different models, thus achieving fairness without compromising the\npredictive performance. Extensive experiments on benchmark datasets demonstrate\nthat CFFL achieves high fairness, delivers comparable accuracy to the\nDistributed framework, and outperforms the Standalone framework.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 14:39:09 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2020 01:06:55 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Lyu", "Lingjuan", ""], ["Xu", "Xinyi", ""], ["Wang", "Qian", ""]]}, {"id": "2008.12243", "submitter": "Giuseppe Tagliavini", "authors": "Fabio Montagna, Stefan Mach, Simone Benatti, Angelo Garofalo,\n  Gianmarco Ottavi, Luca Benini, Davide Rossi, Giuseppe Tagliavini", "title": "A transprecision floating-point cluster for efficient near-sensor data\n  analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent applications in the domain of near-sensor computing require the\nadoption of floating-point arithmetic to reconcile high precision results with\na wide dynamic range. In this paper, we propose a multi-core computing cluster\nthat leverages the fined-grained tunable principles of transprecision computing\nto provide support to near-sensor applications at a minimum power budget. Our\ndesign - based on the open-source RISC-V architecture - combines\nparallelization and sub-word vectorization with near-threshold operation,\nleading to a highly scalable and versatile system. We perform an exhaustive\nexploration of the design space of the transprecision cluster on a\ncycle-accurate FPGA emulator, with the aim to identify the most efficient\nconfigurations in terms of performance, energy efficiency, and area efficiency.\nWe also provide a full-fledged software stack support, including a parallel\nruntime and a compilation toolchain, to enable the development of end-to-end\napplications. We perform an experimental assessment of our design on a set of\nbenchmarks representative of the near-sensor processing domain, complementing\nthe timing results with a post place-&-route analysis of the power consumption.\nFinally, a comparison with the state-of-the-art shows that our solution\noutperforms the competitors in energy efficiency, reaching a peak of 97\nGflop/s/W on single-precision scalars and 162 Gflop/s/W on half-precision\nvectors.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 16:38:26 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Montagna", "Fabio", ""], ["Mach", "Stefan", ""], ["Benatti", "Simone", ""], ["Garofalo", "Angelo", ""], ["Ottavi", "Gianmarco", ""], ["Benini", "Luca", ""], ["Rossi", "Davide", ""], ["Tagliavini", "Giuseppe", ""]]}, {"id": "2008.12256", "submitter": "Leonid Sokolinsky", "authors": "Leonid B. Sokolinsky", "title": "BSF-skeleton: A Template for Parallelization of Iterative Numerical\n  Algorithms on Cluster Computing Systems", "comments": "Submitted to MethodsX", "journal-ref": "MethodsX, 2021", "doi": "10.1016/j.mex.2021.101437", "report-no": null, "categories": "cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes a method for creating applications for cluster\ncomputing systems using the parallel BSF skeleton based on the original BSF\n(Bulk Synchronous Farm) model of parallel computations developed by the author\nearlier. This model uses the master/slave paradigm. The main advantage of the\nBSF model is that it allows to estimate the scalability of a parallel algorithm\nbefore its implementation. Another important feature of the BSF model is the\nrepresentation of problem data in the form of lists that greatly simplifies the\nlogic of building applications. The BSF skeleton is designed for creating\nparallel programs in C++ using the MPI library. The scope of the BSF skeleton\nis iterative numerical algorithms of high computational complexity. The BSF\nskeleton has the following distinctive features. - The BSF-skeleton completely\nencapsulates all aspects that are associated with parallelizing a program. -\nThe BSF skeleton allows error-free compilation at all stages of application\ndevelopment. - The BSF skeleton supports OpenMP programming model and\nworkflows.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 15:58:24 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2020 06:34:12 GMT"}, {"version": "v3", "created": "Fri, 4 Jun 2021 14:05:06 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Sokolinsky", "Leonid B.", ""]]}, {"id": "2008.12260", "submitter": "Aurick Qiao", "authors": "Aurick Qiao, Sang Keun Choe, Suhas Jayaram Subramanya, Willie\n  Neiswanger, Qirong Ho, Hao Zhang, Gregory R. Ganger, Eric P. Xing", "title": "Pollux: Co-adaptive Cluster Scheduling for Goodput-Optimized Deep\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pollux improves scheduling performance in deep learning (DL) clusters by\nadaptively co-optimizing inter-dependent factors both at the per-job level and\nat the cluster-wide level. Most existing schedulers expect users to specify the\nnumber of resources for each job, often leading to inefficient resource use.\nSome recent schedulers choose job resources for users, but do so without\nawareness of how DL training can be re-optimized to better utilize the provided\nresources.\n  Pollux simultaneously considers both aspects. By monitoring the status of\neach job during training, Pollux models how their goodput (a novel metric we\nintroduce that combines system throughput with statistical efficiency) would\nchange by adding or removing resources. Leveraging these information, Pollux\ndynamically (re-)assigns resources to improve cluster-wide goodput, while\nrespecting fairness and continually optimizing each DL job to better utilize\nthose resources.\n  In experiments with real DL jobs and with trace-driven simulations, Pollux\nreduces average job completion times by 37-50% relative to state-of-the-art DL\nschedulers, even when they are provided with ideal resource and training\nconfigurations for every job. Pollux promotes fairness among DL jobs competing\nfor resources based on a more meaningful measure of useful job progress, and\nreveals a new opportunity for reducing DL cost in cloud environments. Pollux is\nimplemented and publicly available as part of an open-source project at\nhttps://github.com/petuum/adaptdl.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 16:56:48 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 06:08:21 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Qiao", "Aurick", ""], ["Choe", "Sang Keun", ""], ["Subramanya", "Suhas Jayaram", ""], ["Neiswanger", "Willie", ""], ["Ho", "Qirong", ""], ["Zhang", "Hao", ""], ["Ganger", "Gregory R.", ""], ["Xing", "Eric P.", ""]]}, {"id": "2008.12336", "submitter": "Amro Alabsi Aljundi", "authors": "Taha Atahan Akyildiz, Amro Alabsi Aljundi, Kamer Kaya", "title": "GOSH: Embedding Big Graphs on Small Hardware", "comments": "11 pages, 4 figures, published in ICPP 2020: The 49th International\n  Conference on Parallel Processing - Edmonton, AB, Canada", "journal-ref": null, "doi": "10.1145/3404397.3404456", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In graph embedding, the connectivity information of a graph is used to\nrepresent each vertex as a point in a d-dimensional space. Unlike the original,\nirregular structural information, such a representation can be used for a\nmultitude of machine learning tasks. Although the process is extremely useful\nin practice, it is indeed expensive and unfortunately, the graphs are becoming\nlarger and harder to embed. Attempts at scaling up the process to larger graphs\nhave been successful but often at a steep price in hardware requirements. We\npresent GOSH, an approach for embedding graphs of arbitrary sizes on a single\nGPU with minimum constraints. GOSH utilizes a novel graph coarsening approach\nto compress the graph and minimize the work required for embedding, delivering\nhigh-quality embeddings at a fraction of the time compared to the\nstate-of-the-art. In addition to this, it incorporates a decomposition schema\nthat enables any arbitrarily large graph to be embedded using a single GPU with\nminimum constraints on the memory size. With these techniques, GOSH is able to\nembed a graph with over 65 million vertices and 1.8 billion edges in less than\nan hour on a single GPU and obtains a 93% AUCROC for link-prediction which can\nbe increased to 95% by running the tool for 80 minutes.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 18:53:32 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2020 09:38:24 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Akyildiz", "Taha Atahan", ""], ["Aljundi", "Amro Alabsi", ""], ["Kaya", "Kamer", ""]]}, {"id": "2008.12385", "submitter": "Nevin Vunka Jungum", "authors": "Nevin Vunka Jungum, Nawaz Mohamudally, Nimal Nissanke", "title": "A Dynamic Load Balancing Algorithm for Distributing Mobile Codes in\n  Multi-Applications and Multi-Hosts Environment", "comments": null, "journal-ref": "International Journal of Computer Science Issues, Vol 17, Issue 4,\n  July 2020", "doi": "10.5281/zenodo.3991567", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Code offloading refers to partitioning software and migrating the mobile\ncodes to other computational entities for processing. Often when a large number\nof mobile codes need to be distributed to many heterogenous hosts, this can\neasily lead to poor system performance if one host gets too many mobile codes\nto process while others are almost idle. To resolve such situation, we proposed\na proposed a load balancing algorithm to ensure fairness in the distribution of\nthe mobile codes. The algorithm is based on the popular Weighted\nLeast-Connections (WLC) scheduling algorithm while taking into consideration\nthe dynamic recalculation of the hosts weights and system attributes such as\nCPU idle rate and memory idle rate which the WLC algorithm does not take into\nconsideration. Using simulation, various number of mobile codes were\ndistributed to the hosts/servers and the proposed algorithm outperform existing\nLeast-Connections and Weighted Least-Connections scheduling algorithms thus\nimproving system efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 22:07:44 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Jungum", "Nevin Vunka", ""], ["Mohamudally", "Nawaz", ""], ["Nissanke", "Nimal", ""]]}, {"id": "2008.12441", "submitter": "Yingzhou Li", "authors": "Yingzhou Li and Jack Poulson and Lexing Ying", "title": "Distributed-memory $\\mathcal{H}$-matrix Algebra I: Data Distribution and\n  Matrix-vector Multiplication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a data distribution scheme for $\\mathcal{H}$-matrices and a\ndistributed-memory algorithm for $\\mathcal{H}$-matrix-vector multiplication.\nOur data distribution scheme avoids an expensive $\\Omega(P^2)$ scheduling\nprocedure used in previous work, where $P$ is the number of processes, while\ndata balancing is well-preserved. Based on the data distribution, our\ndistributed-memory algorithm evenly distributes all computations among $P$\nprocesses and adopts a novel tree-communication algorithm to reduce the latency\ncost. The overall complexity of our algorithm is $O\\Big(\\frac{N \\log N}{P} +\n\\alpha \\log P + \\beta \\log^2 P \\Big)$ for $\\mathcal{H}$-matrices under weak\nadmissibility condition, where $N$ is the matrix size, $\\alpha$ denotes the\nlatency, and $\\beta$ denotes the inverse bandwidth. Numerically, our algorithm\nis applied to address both two- and three-dimensional problems of various sizes\namong various numbers of processes. On thousands of processes, good parallel\nefficiency is still observed.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 02:18:59 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2020 06:09:25 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Li", "Yingzhou", ""], ["Poulson", "Jack", ""], ["Ying", "Lexing", ""]]}, {"id": "2008.12516", "submitter": "Rohan Garg", "authors": "Rohan Garg", "title": "Fast and Work-Optimal Parallel Algorithms for Predicate Detection", "comments": "Fixed minor bug in JLSDetect from Version 3 with new subroutine FLIS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the predicate detection problem was shown to be in the parallel\ncomplexity class NC. In this paper, we give the first work-optimal parallel\nalgorithm to solve the predicate detection problem on a distributed computation\nwith $n$ processes and at most $m$ states per process. The previous best known\nparallel predicate detection algorithm, ParallelCut, has time complexity\n$O(\\log mn)$ and work complexity $O(m^3n^3\\log mn)$. We give two algorithms, a\ndeterministic algorithm with time complexity $O(mn)$ and work complexity\n$O(mn^2)$, and a randomized algorithm with time complexity $(mn)^{1/2 + o(1)}$\nand work complexity $\\tilde{O}(mn^2)$. Furthermore, our algorithms improve upon\nthe space complexity of ParallelCut. Both of our algorithms have space\ncomplexity $O(mn^2)$ whereas ParallelCut has space complexity $O(m^2n^2)$.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 07:25:42 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2020 04:40:20 GMT"}, {"version": "v3", "created": "Fri, 11 Sep 2020 04:14:46 GMT"}, {"version": "v4", "created": "Wed, 2 Dec 2020 06:01:47 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Garg", "Rohan", ""]]}, {"id": "2008.12586", "submitter": "Mahsa Ghanavatinasab", "authors": "Mahsa Ghanavatinasab, Mastaneh Bahmani, Reza Azmi", "title": "SAF: Simulated Annealing Fair Scheduling for Hadoop Yarn Clusters", "comments": "28 pages, 8 sections, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Apache introduced YARN as the next generation of the Hadoop framework,\nproviding resource management and a central platform to deliver consistent data\ngovernance tools across Hadoop clusters. Hadoop YARN supports multiple\nframeworks like MapReduce to process different types of data and works with\ndifferent scheduling policies such as FIFO, Capacity, and Fair schedulers. DRF\nis the best option that uses short-term, without considering history\ninformation, convergence to fairness for multi-type resource allocation.\nHowever, DRF performance is still not satisfying due to trade-offs between\nfairness and performance regarding resource utilization. To address this\nproblem, we propose Simulated Annealing Fair scheduling, SAF, a long-term fair\nscheme in resource allocation to have fairness and excellent performance in\nterms of resource utilization and MakeSpan. We introduce a new parameter as\nentropy, which is an approach to indicates the disorder in the fairness of\nallocated resources of the whole cluster. We implemented SAF as a pluggable\nscheduler in Hadoop Yarn Cluster and evaluated it with standard MapReduce\nbenchmarks in Yarn Scheduler Load Simulator (SLS) and CloudSim Plus simulation\nframework. Finally, the results of both simulation tools are evidence to prove\nour claim. Compared to DRF, SAF increases resource utilization of YARN clusters\nsignificantly and decreases MakeSpan to an appropriate level.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 11:22:59 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Ghanavatinasab", "Mahsa", ""], ["Bahmani", "Mastaneh", ""], ["Azmi", "Reza", ""]]}, {"id": "2008.12707", "submitter": "Francisco Maturana", "authors": "Francisco Maturana and K. V. Rashmi", "title": "Bandwidth Cost of Code Conversions in Distributed Storage: Fundamental\n  Limits and Optimal Constructions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Erasure codes have become an integral part of distributed storage systems as\na tool for providing data reliability and durability under the constant threat\nof device failures. In such systems, an $[n, k]$ code over a finite field\n$\\mathbb{F}_q$ encodes $k$ message symbols into $n$ codeword symbols from\n$\\mathbb{F}_q$ which are then stored on $n$ different nodes in the system.\nRecent work has shown that significant savings in storage space can be obtained\nby tuning $n$ and $k$ to variations in device failure rates. Such a tuning\nnecessitates code conversion: the process of converting already encoded data\nunder an initial $[n^I, k^I]$ code to its equivalent under a final $[n^F, k^F]$\ncode. The default approach to conversion is to reencode data, which places\nsignificant burden on system resources. Convertible codes are a recently\nproposed class of codes for enabling resource-efficient conversions. Existing\nwork on convertible codes has focused on minimizing access cost, i.e., the\nnumber of code symbols accessed during conversion. Bandwidth, which corresponds\nto the amount of data read and transferred, is another important resource to\noptimize.\n  In this paper, we initiate the study on the fundamental limits on bandwidth\nused during code conversion and present constructions for bandwidth-optimal\nconvertible codes. First, we model the code conversion problem using network\ninformation flow graphs with variable capacity edges. Second, focusing on MDS\ncodes and an important parameter regime called the merge regime, we derive\ntight lower bounds on the bandwidth cost of conversion. The derived bounds show\nthat bandwidth cost can be significantly reduced even in regimes where access\ncost cannot be reduced as compared to the default approach. Third, we present a\nnew construction for MDS convertible codes which matches the proposed lower\nbound and is thus bandwidth-optimal during conversion.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 15:39:43 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Maturana", "Francisco", ""], ["Rashmi", "K. V.", ""]]}, {"id": "2008.12712", "submitter": "Nicholas Smith", "authors": "Nicholas Smith, Lindsey Gray, Matteo Cremonesi, Bo Jayatilaka, Oliver\n  Gutsche, Allison Hall, Kevin Pedro, Maria Acosta, Andrew Melo, Stefano\n  Belforte, Jim Pivarski", "title": "Coffea -- Columnar Object Framework For Effective Analysis", "comments": "To be published in CHEP 2019 proceedings, EPJ Web of Conferences;\n  post-review update", "journal-ref": null, "doi": "10.1051/epjconf/202024506012", "report-no": null, "categories": "cs.DC hep-ex", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The coffea framework provides a new approach to High-Energy Physics analysis,\nvia columnar operations, that improves time-to-insight, scalability,\nportability, and reproducibility of analysis. It is implemented with the Python\nprogramming language, the scientific python package ecosystem, and commodity\nbig data technologies. To achieve this suite of improvements across many use\ncases, coffea takes a factorized approach, separating the analysis\nimplementation and data delivery scheme. All analysis operations are\nimplemented using the NumPy or awkward-array packages which are wrapped to\nyield user code whose purpose is quickly intuited. Various data delivery\nschemes are wrapped into a common front-end which accepts user inputs and code,\nand returns user defined outputs. We will discuss our experience in\nimplementing analysis of CMS data using the coffea framework along with a\ndiscussion of the user experience and future directions.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 15:47:22 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Smith", "Nicholas", ""], ["Gray", "Lindsey", ""], ["Cremonesi", "Matteo", ""], ["Jayatilaka", "Bo", ""], ["Gutsche", "Oliver", ""], ["Hall", "Allison", ""], ["Pedro", "Kevin", ""], ["Acosta", "Maria", ""], ["Melo", "Andrew", ""], ["Belforte", "Stefano", ""], ["Pivarski", "Jim", ""]]}, {"id": "2008.12819", "submitter": "Jashwant Raj Gunasekaran", "authors": "Jashwant Raj Gunasekaran, Prashanth Thinakaran, Nachiappan\n  Chidambaram, Mahmut T. Kandemir, Chita R. Das", "title": "Fifer: Tackling Underutilization in the Serverless Era", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Datacenters are witnessing a rapid surge in the adoption of serverless\nfunctions for microservices-based applications. A vast majority of these\nmicroservices typically span less than a second, have strict SLO requirements,\nand are chained together as per the requirements of an application. The\naforementioned characteristics introduce a new set of challenges, especially in\nterms of container provisioning and management, as the state-of-the-art\nresource management frameworks, employed in serverless platforms, tend to look\nat microservice-based applications similar to conventional monolithic\napplications. Hence, these frameworks suffer from microservice-agnostic\nscheduling and colossal container over-provisioning, especially during workload\nfluctuations, thereby resulting in poor resource utilization.\n  In this work, we quantify the above shortcomings using a variety of workloads\non a multi-node cluster managed by Kubernetes and Brigade serverless framework.\nTo address them, we propose \\emph{Fifer} -- an adaptive resource management\nframework to efficiently manage function-chains on serverless platforms. The\nkey idea is to make \\emph{Fifer} (i) utilization conscious by efficiently bin\npacking jobs to fewer containers using function-aware container scaling and\nintelligent request batching, and (ii) at the same time, SLO-compliant by\nproactively spawning containers to avoid cold-starts, thus minimizing the\noverall response latency. Combining these benefits, \\emph{Fifer} improves\ncontainer utilization and cluster-wide energy consumption by 4x and 31%,\nrespectively, without compromising on SLO's, when compared to the\nstate-of-the-art schedulers employed by serverless platforms.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 19:08:08 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Gunasekaran", "Jashwant Raj", ""], ["Thinakaran", "Prashanth", ""], ["Chidambaram", "Nachiappan", ""], ["Kandemir", "Mahmut T.", ""], ["Das", "Chita R.", ""]]}, {"id": "2008.12820", "submitter": "Andreas Mang", "authors": "Malte Brunn and Naveen Himthani and George Biros and Miriam Mehl and\n  Andreas Mang", "title": "Multi-Node Multi-GPU Diffeomorphic Image Registration for Large-Scale\n  Imaging Problems", "comments": "Proc ACM/IEEE Conference on Supercomputing 2020 (accepted for\n  publication)", "journal-ref": null, "doi": "10.1109/SC41405.2020.00042", "report-no": null, "categories": "cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Gauss-Newton-Krylov solver for large deformation diffeomorphic\nimage registration. We extend the publicly available CLAIRE library to\nmulti-node multi-graphics processing unit (GPUs) systems and introduce novel\nalgorithmic modifications that significantly improve performance. Our\ncontributions comprise ($i$) a new preconditioner for the reduced-space\nGauss-Newton Hessian system, ($ii$) a highly-optimized multi-node multi-GPU\nimplementation exploiting device direct communication for the main\ncomputational kernels (interpolation, high-order finite difference operators\nand Fast-Fourier-Transform), and ($iii$) a comparison with state-of-the-art CPU\nand GPU implementations. We solve a $256^3$-resolution image registration\nproblem in five seconds on a single NVIDIA Tesla V100, with a performance\nspeedup of 70% compared to the state-of-the-art. In our largest run, we\nregister $2048^3$ resolution images (25 B unknowns; approximately 152$\\times$\nlarger than the largest problem solved in state-of-the-art GPU implementations)\non 64 nodes with 256 GPUs on TACC's Longhorn system.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 19:19:54 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Brunn", "Malte", ""], ["Himthani", "Naveen", ""], ["Biros", "George", ""], ["Mehl", "Miriam", ""], ["Mang", "Andreas", ""]]}, {"id": "2008.13006", "submitter": "Cong Guo", "authors": "Cong Guo and Bo Yang Hsueh and Jingwen Leng and Yuxian Qiu and Yue\n  Guan and Zehuan Wang and Xiaoying Jia and Xipeng Li and Minyi Guo and Yuhao\n  Zhu", "title": "Accelerating Sparse DNN Models without Hardware-Support via Tile-Wise\n  Sparsity", "comments": "12pages, ACM/IEEE Proceedings of the International Conference for\n  High Performance Computing, Networking, Storage and Analysis (SC20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network pruning can reduce the high computation cost of deep neural network\n(DNN) models. However, to maintain their accuracies, sparse models often carry\nrandomly-distributed weights, leading to irregular computations. Consequently,\nsparse models cannot achieve meaningful speedup on commodity hardware (e.g.,\nGPU) built for dense matrix computations. As such, prior works usually modify\nor design completely new sparsity-optimized architectures for exploiting\nsparsity. We propose an algorithm-software co-designed pruning method that\nachieves latency speedups on existing dense architectures. Our work builds upon\nthe insight that the matrix multiplication generally breaks the large matrix\ninto multiple smaller tiles for parallel execution. We propose a\ntiling-friendly \"tile-wise\" sparsity pattern, which maintains a regular pattern\nat the tile level for efficient execution but allows for irregular, arbitrary\npruning at the global scale to maintain the high accuracy. We implement and\nevaluate the sparsity pattern on GPU tensor core, achieving a 1.95x speedup\nover the dense model.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 16:27:41 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Guo", "Cong", ""], ["Hsueh", "Bo Yang", ""], ["Leng", "Jingwen", ""], ["Qiu", "Yuxian", ""], ["Guan", "Yue", ""], ["Wang", "Zehuan", ""], ["Jia", "Xiaoying", ""], ["Li", "Xipeng", ""], ["Guo", "Minyi", ""], ["Zhu", "Yuhao", ""]]}, {"id": "2008.13145", "submitter": "John Lawson", "authors": "John Lawson", "title": "Performance portability through machine learning guided kernel selection\n  in SYCL libraries", "comments": "13 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically tuning parallel compute kernels allows libraries and frameworks\nto achieve performance on a wide range of hardware, however these techniques\nare typically focused on finding optimal kernel parameters for particular input\nsizes and parameters. General purpose compute libraries must be able to cater\nto all inputs and parameters provided by a user, and so these techniques are of\nlimited use. Additionally, parallel programming frameworks such as SYCL require\nthat the kernels be deployed in a binary format embedded within the library. As\nsuch it is impractical to deploy a large number of possible kernel\nconfigurations without inflating the library size.\n  Machine learning methods can be used to mitigate against both of these\nproblems and provide performance for general purpose routines with a limited\nnumber of kernel configurations. We show that unsupervised clustering methods\ncan be used to select a subset of the possible kernels that should be deployed\nand that simple classification methods can be trained to select from these\nkernels at runtime to give good performance. As these techniques are fully\nautomated, relying only on benchmark data, the tuning process for new hardware\nor problems does not require any developer effort or expertise.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 11:44:37 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Lawson", "John", ""]]}, {"id": "2008.13225", "submitter": "Tharun Kumar Reddy Medini", "authors": "Tharun Medini, Beidi Chen, Anshumali Shrivastava", "title": "SOLAR: Sparse Orthogonal Learned and Random Embeddings", "comments": "Under review at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense embedding models are commonly deployed in commercial search engines,\nwherein all the document vectors are pre-computed, and near-neighbor search\n(NNS) is performed with the query vector to find relevant documents. However,\nthe bottleneck of indexing a large number of dense vectors and performing an\nNNS hurts the query time and accuracy of these models. In this paper, we argue\nthat high-dimensional and ultra-sparse embedding is a significantly superior\nalternative to dense low-dimensional embedding for both query efficiency and\naccuracy. Extreme sparsity eliminates the need for NNS by replacing them with\nsimple lookups, while its high dimensionality ensures that the embeddings are\ninformative even when sparse. However, learning extremely high dimensional\nembeddings leads to blow up in the model size. To make the training feasible,\nwe propose a partitioning algorithm that learns such high dimensional\nembeddings across multiple GPUs without any communication. This is facilitated\nby our novel asymmetric mixture of Sparse, Orthogonal, Learned and Random\n(SOLAR) Embeddings. The label vectors are random, sparse, and near-orthogonal\nby design, while the query vectors are learned and sparse. We theoretically\nprove that our way of one-sided learning is equivalent to learning both query\nand label embeddings. With these unique properties, we can successfully train\n500K dimensional SOLAR embeddings for the tasks of searching through 1.6M books\nand multi-label classification on the three largest public datasets. We achieve\nsuperior precision and recall compared to the respective state-of-the-art\nbaselines for each of the tasks with up to 10 times faster speed.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 17:35:35 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Medini", "Tharun", ""], ["Chen", "Beidi", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "2008.13292", "submitter": "Rathish Das", "authors": "Zafar Ahmad, Rezaul Chowdhury, Rathish Das, Pramod Ganapathi, Aaron\n  Gregory, and Mohammad Mahdi Javanmard", "title": "Low-Depth Parallel Algorithms for the Binary-Forking Model without\n  Atomics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The binary-forking model is a parallel computation model, formally defined by\nBlelloch et al. very recently, in which a thread can fork a concurrent child\nthread, recursively and asynchronously. The model incurs a cost of $\\Theta(\\log\nn)$ to spawn or synchronize $n$ tasks or threads. The binary-forking model\nrealistically captures the performance of parallel algorithms implemented using\nmodern multithreaded programming languages on multicore shared-memory machines.\nIn contrast, the widely studied theoretical PRAM model does not consider the\ncost of spawning and synchronizing threads, and as a result, algorithms\nachieving optimal performance bounds in the PRAM model may not be optimal in\nthe binary-forking model. Often, algorithms need to be redesigned to achieve\noptimal performance bounds in the binary-forking model and the non-constant\nsynchronization cost makes the task challenging.\n  Though the binary-forking model allows the use of atomic {\\em test-and-set}\n(TS) instructions to reduce some synchronization overhead, assuming the\navailability of such instructions puts a stronger requirement on the hardware\nand may limit the portability of the algorithms using them. In this paper, we\navoid the use of locks and atomic instructions in our algorithms except\npossibly inside the join operation which is implemented by the runtime system.\n  In this paper, we design efficient parallel algorithms in the binary-forking\nmodel without atomics for three fundamental problems: Strassen's (and\nStrassen-like) matrix multiplication (MM), comparison-based sorting, and the\nFast Fourier Transform (FFT). All our results improve over known results for\nthe corresponding problem in the binary-forking model both with and without\natomics.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 22:56:59 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 18:41:25 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Ahmad", "Zafar", ""], ["Chowdhury", "Rezaul", ""], ["Das", "Rathish", ""], ["Ganapathi", "Pramod", ""], ["Gregory", "Aaron", ""], ["Javanmard", "Mohammad Mahdi", ""]]}, {"id": "2008.13312", "submitter": "Minxian Xu", "authors": "Minxian Xu, Adel N. Toosi, Rajkumar Buyya", "title": "A Self-adaptive Approach for Managing Applications and Harnessing\n  Renewable Energy for Sustainable Cloud Computing", "comments": "15 pages, 8 figures, 2 tables. To be appeared in IEEE Transactions on\n  Sustainable Computing (accepted on Aug 4.)", "journal-ref": null, "doi": "10.1109/TSUSC.2020.3014943", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid adoption of Cloud computing for hosting services and its success is\nprimarily attributed to its attractive features such as elasticity,\navailability and pay-as-you-go pricing model. However, the huge amount of\nenergy consumed by cloud data centers makes it to be one of the fastest growing\nsources of carbon emissions. Approaches for improving the energy efficiency\ninclude enhancing the resource utilization to reduce resource wastage and\napplying the renewable energy as the energy supply. This work aims to reduce\nthe carbon footprint of the data centers by reducing the usage of brown energy\nand maximizing the usage of renewable energy. Taking advantage of microservices\nand renewable energy, we propose a self-adaptive approach for the resource\nmanagement of interactive workloads and batch workloads. To ensure the quality\nof service of workloads, a brownout-based algorithm for interactive workloads\nand a deferring algorithm for batch workloads are proposed. We have implemented\nthe proposed approach in a prototype system and evaluated it with web services\nunder real traces. The results illustrate our approach can reduce the brown\nenergy usage by 21% and improve the renewable energy usage by 10%.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 01:24:21 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Xu", "Minxian", ""], ["Toosi", "Adel N.", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "2008.13402", "submitter": "Anastasios Papagiannis", "authors": "Stella Mikrou, Anastasios Papagiannis, Giorgos Saloustros, Manolis\n  Marazakis, Angelos Bilas", "title": "Power and Performance Analysis of Persistent Key-Value Stores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the current rate of data growth, processing needs are becoming difficult\nto fulfill due to CPU power and energy limitations. Data serving systems and\nespecially persistent key-value stores have become a substantial part of data\nprocessing stacks in the data center, providing access to massive amounts of\ndata for applications and services. Key-value stores exhibit high CPU and I/O\noverheads because of their constant need to reorganize data on the devices. In\nthis paper, we examine the efficiency of two key-value stores on four servers\nof different generations and with different CPU architectures. We use RocksDB,\na key-value that is deployed widely, e.g. in Facebook, and Kreon, a research\nkey-value store that has been designed to reduce CPU overhead. We evaluate\ntheir behavior and overheads on an ARM-based microserver and three different\ngenerations of x86 servers. Our findings show that microservers have better\npower efficiency in the range of 0.68-3.6x with a comparable tail latency.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 07:33:18 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Mikrou", "Stella", ""], ["Papagiannis", "Anastasios", ""], ["Saloustros", "Giorgos", ""], ["Marazakis", "Manolis", ""], ["Bilas", "Angelos", ""]]}, {"id": "2008.13456", "submitter": "Lars Kroll", "authors": "Seif Haridi, Lars Kroll, and Paris Carbone", "title": "Lecture Notes on Leader-based Sequence Paxos -- An Understandable\n  Sequence Consensus Algorithm", "comments": "First public draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Agreement among a set of processes and in the presence of partial failures is\none of the fundamental problems of distributed systems. In the most general\ncase, many decisions must be agreed upon over the lifetime of a system with\ndynamically changing membership. Such a sequence of decisions represents a\ndistributed log, and can form the underlying abstraction for driving a\nreplicated state machine. While this abstraction is at the core of many systems\nwith strong consistency requirements, algorithms that achieve such sequence\nconsensus are often poorly understood by developers and have presented a\nsignificant challenge to many students of distributed systems. In these lecture\nnotes we present a complete and practical Paxos-based algorithm for\nreconfigurable sequence consensus in the fail-recovery model, and a clear path\nof simple step-by-step transformations to it from the basic Paxos algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 09:40:46 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Haridi", "Seif", ""], ["Kroll", "Lars", ""], ["Carbone", "Paris", ""]]}, {"id": "2008.13742", "submitter": "Shinjae Yoo", "authors": "Sungsoo Ha, Wonyong Jeong, Gyorgy Matyasfalvi, Cong Xie, Kevin Huck,\n  Jong Youl Choi, Abid Malik, Li Tang, Hubertus Van Dam, Line Pouchard, Wei Xu,\n  Shinjae Yoo, Nicholas D'Imperio, Kerstin Kleese Van Dam", "title": "Chimbuko: A Workflow-Level Scalable Performance Trace Analysis Tool", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because of the limits input/output systems currently impose on\nhigh-performance computing systems, a new generation of workflows that include\nonline data reduction and analysis is emerging. Diagnosing their performance\nrequires sophisticated performance analysis capabilities due to the complexity\nof execution patterns and underlying hardware, and no tool could handle the\nvoluminous performance trace data needed to detect potential problems. This\nwork introduces Chimbuko, a performance analysis framework that provides\nreal-time, distributed, in situ anomaly detection. Data volumes are reduced for\nhuman-level processing without losing necessary details. Chimbuko supports\nonline performance monitoring via a visualization module that presents the\noverall workflow anomaly distribution, call stacks, and timelines. Chimbuko\nalso supports the capture and reduction of performance provenance. To the best\nof our knowledge, Chimbuko is the first online, distributed, and scalable\nworkflow-level performance trace analysis framework, and we demonstrate the\ntool's usefulness on Oak Ridge National Laboratory's Summit system.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 17:06:43 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Ha", "Sungsoo", ""], ["Jeong", "Wonyong", ""], ["Matyasfalvi", "Gyorgy", ""], ["Xie", "Cong", ""], ["Huck", "Kevin", ""], ["Choi", "Jong Youl", ""], ["Malik", "Abid", ""], ["Tang", "Li", ""], ["Van Dam", "Hubertus", ""], ["Pouchard", "Line", ""], ["Xu", "Wei", ""], ["Yoo", "Shinjae", ""], ["D'Imperio", "Nicholas", ""], ["Van Dam", "Kerstin Kleese", ""]]}]