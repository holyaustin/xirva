[{"id": "1901.00041", "submitter": "Paras Jain", "authors": "Paras Jain, Xiangxi Mo, Ajay Jain, Harikaran Subbaraj, Rehan Sohail\n  Durrani, Alexey Tumanov, Joseph Gonzalez, Ion Stoica", "title": "Dynamic Space-Time Scheduling for GPU Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Serving deep neural networks in latency critical interactive settings often\nrequires GPU acceleration. However, the small batch sizes typical in online\ninference results in poor GPU utilization, a potential performance gap which\nGPU resource sharing can address. In this paper, we explore several techniques\nto leverage both temporal and spatial multiplexing to improve GPU utilization\nfor deep learning inference workloads. We evaluate the performance trade-offs\nof each approach with respect to resource-efficiency, latency predictability,\nand isolation when compared with conventional batched inference. Our\nexperimental analysis suggests up to a 5x potential for improved utilization\nthrough the exploration of more advanced spatial and temporal multiplexing\nstrategies. Our preliminary prototype of a dynamic space-time scheduler\ndemonstrates a 3.23x floating-point throughput increase over space-only\nmultiplexing and a 7.73x increase over time-only multiplexing for convolutions,\nwhile also providing better isolation and latency predictability.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 20:50:22 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Jain", "Paras", ""], ["Mo", "Xiangxi", ""], ["Jain", "Ajay", ""], ["Subbaraj", "Harikaran", ""], ["Durrani", "Rehan Sohail", ""], ["Tumanov", "Alexey", ""], ["Gonzalez", "Joseph", ""], ["Stoica", "Ion", ""]]}, {"id": "1901.00113", "submitter": "Jie Song", "authors": "Jie Song, Yichuan Zhang, Yubin Bao, Ge Yu", "title": "Probery: A Probability-based Incomplete Query Optimization for Big Data", "comments": "15 pages under the review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, query optimization has been highly concerned in big data\nmanagement, especially in NoSQL databases. Approximate queries boost query\nperformance by loss of accuracy, for example, sampling approaches trade off\nquery completeness for efficiency. Different from them, we propose an\nuncertainty of query completeness, called Probability of query Completeness (PC\nfor short). PC refers to the possibility that query results contain all\nsatisfied records. For example PC=0.95, it guarantees that there are no more\nthan 5 incomplete queries among 100 ones, but not guarantees how incomplete\nthey are. We trade off PC for query performance, and experiments show that a\nsmall loss of PC doubles query performance. The proposed Probery\n(PROBability-based data quERY) adopts the uncertainty of query completeness to\naccelerate OLTP queries. This paper illustrates the data and probability\nmodels, the probability based data placement and query processing, and the\nApache Drill-based implementation of Probery. In experiments, we first prove\nthat the percentage of complete queries is larger than the given PC confidence\nfor various cases, namely that the PC guarantee is validate. Then Probery is\ncompared with Drill, Impala and Hive in terms of query performance. The results\nindicate that Drill-based Probery performs as fast as Drill with complete\nquery, while averagely 1.8x, 1.3x and 1.6x faster than Drill, Impala and Hive\nwith possible complete query, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jan 2019 08:09:25 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Song", "Jie", ""], ["Zhang", "Yichuan", ""], ["Bao", "Yubin", ""], ["Yu", "Ge", ""]]}, {"id": "1901.00155", "submitter": "Mikhail Zymbler L.", "authors": "Andrey Polyakov, Mikhail Zymbler", "title": "Parallel Algorithm for Time Series Discords Discovery on the Intel Xeon\n  Phi Knights Landing Many-core Processor", "comments": "in Russian", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discord is a refinement of the concept of anomalous subsequence of a time\nseries. The task of discords discovery is applied in a wide range of subject\ndomains related to time series: medicine, economics, climate modeling, etc. In\nthis paper, we propose a novel parallel algorithm for discords discovery for\nthe Intel Xeon Phi Knights Landing (KNL) many-core systems for the case when\ninput data fit in main memory. The algorithm exploits the ability to\nindependently calculate Euclidean distances between the subsequences of the\ntime series. Computations are paralleled through OpenMP technology. The\nalgorithm consists of two stages, namely precomputations and discovery. At the\nprecomputations stage, we construct the auxiliary matrix data structures, which\nensure efficient vectorization of computations on Intel Xeon Phi KNL. At the\ndiscovery stage, the algorithm finds discord based upon the structures above.\nExperimental evaluation confirms the high scalability of the developed\nalgorithm.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jan 2019 14:19:24 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Polyakov", "Andrey", ""], ["Zymbler", "Mikhail", ""]]}, {"id": "1901.00228", "submitter": "Medha Atre", "authors": "Shubham S. Srivastava, Medha Atre, Shubham Sharma, Rahul Gupta,\n  Sandeep K. Shukla", "title": "Verity: Blockchains to Detect Insider Attacks in DBMS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrity and security of the data in database systems are typically\nmaintained with access control policies and firewalls. However, insider attacks\n-- where someone with an intimate knowledge of the system and administrative\nprivileges tampers with the data -- pose a unique challenge. Measures like\nappend only logging prove to be insufficient because an attacker with\nadministrative privileges can alter logs and login records to eliminate the\ntrace of attack, thus making insider attacks hard to detect.\n  In this paper, we propose Verity -- first of a kind system to the best of our\nknowledge. Verity serves as a dataless framework by which any blockchain\nnetwork can be used to store fixed-length metadata about tuples from any SQL\ndatabase, without complete migration of the database. Verity uses a formalism\nfor parsing SQL queries and query results to check the respective tuples'\nintegrity using blockchains to detect insider attacks. We have implemented our\ntechnique using Hyperledger Fabric, Composer REST API, and SQLite database.\nUsing TPC-H data and SQL queries of varying complexity and types, our\nexperiments demonstrate that any overhead of integrity checking remains\nconstant per tuple in a query's results, and scales linearly.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 01:02:56 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Srivastava", "Shubham S.", ""], ["Atre", "Medha", ""], ["Sharma", "Shubham", ""], ["Gupta", "Rahul", ""], ["Shukla", "Sandeep K.", ""]]}, {"id": "1901.00302", "submitter": "Abolfazl Danayi", "authors": "Abolfazl Danayi and Saeed Sharifian", "title": "openCoT: The opensource Cloud of Things platform", "comments": "9 pages, 8 figures, GitHub link", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to address the complexity and extensiveness of technology, Cloud\nComputing is utilized with four main service models. The most recent service\nmodel, function-as-a-service, enables developers to develop their application\nin a function-based structure and then deploy it to the Cloud. Using an optimum\nelastic auto-scaling, the performance of executing an application over FaaS\nCloud, overcomes the extra overhead and reduces the total cost. However,\nresearchers need a simple and well-documented FaaS Cloud manager in order to\nimplement their proposed Auto-scaling algorithms. In this paper, we represent\nthe openCoT platform and explain its building blocks and details. Experimental\nresults show that executing a function (invoking and passing arguments) and\nreturning the result using openCoT takes 21 ms over a remote connection. The\nsource code of openCoT is available in the GitHub repository of the project\n(\\code{www.github.com/adanayi/opencot}) for public usage.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 09:17:51 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Danayi", "Abolfazl", ""], ["Sharifian", "Saeed", ""]]}, {"id": "1901.00342", "submitter": "Suman Sourav", "authors": "Seth Gilbert, Peter Robinson, Suman Sourav", "title": "Leader Election in Well-Connected Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we look at the problem of randomized leader election in\nsynchronous distributed networks with a special focus on the message\ncomplexity. We provide an algorithm that solves the implicit version of leader\nelection (where non-leader nodes need not be aware of the identity of the\nleader) in any general network with $O(\\sqrt{n} \\log^{7/2} n \\cdot t_{mix})$\nmessages and in $O(t_{mix}\\log^2 n)$ time, where $n$ is the number of nodes and\n$t_{mix}$ refers to the mixing time of a random walk in the network graph $G$.\nFor several classes of well-connected networks (that have a large conductance\nor alternatively small mixing times e.g. expanders, hypercubes, etc), the above\nresult implies extremely efficient (sublinear running time and messages) leader\nelection algorithms. Correspondingly, we show that any substantial improvement\nis not possible over our algorithm, by presenting an almost matching lower\nbound for randomized leader election. We show that\n$\\Omega(\\sqrt{n}/\\phi^{3/4})$ messages are needed for any leader election\nalgorithm that succeeds with probability at least $1-o(1)$, where $\\phi$ refers\nto the conductance of a graph. To the best of our knowledge, this is the first\nwork that shows a dependence between the time and message complexity to solve\nleader election and the connectivity of the graph $G$, which is often\ncharacterized by the graph's conductance $\\phi$. Apart from the $\\Omega(m)$\nbound in [Kutten et al., J.ACM 2015] (where $m$ denotes the number of edges of\nthe graph), this work also provides one of the first non-trivial lower bounds\nfor leader election in general networks.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 12:42:39 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Gilbert", "Seth", ""], ["Robinson", "Peter", ""], ["Sourav", "Suman", ""]]}, {"id": "1901.00416", "submitter": "Syed Waqar Nabi Dr", "authors": "Wim Vanderbauwhede and Syed Waqar Nabi", "title": "Towards Automatic Transformation of Legacy Scientific Code into OpenCL\n  for Optimal Performance on FPGAs", "comments": "Presented: HLPGPU at HiPEAC 2018 (Manchester, UK). arXiv admin note:\n  text overlap with arXiv:1711.04471", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a large body of legacy scientific code written in languages like\nFortran that is not optimised to get the best performance out of heterogeneous\nacceleration devices like GPUs and FPGAs, and manually porting such code into\nparallel languages frameworks like OpenCL requires considerable effort. We are\nworking towards developing a turn-key, self-optimising compiler for\naccelerating scientific applications, that can automatically transform legacy\ncode into a solution for heterogeneous targets. In this paper we focus on FPGAs\nas the acceleration devices, and carry out our discussion in the context of the\nOpenCL programming framework. We show a route to automatic creation of kernels\nwhich are optimised for execution in a \"streaming\" fashion, which gives optimal\nperformance on FPGAs. We use a 2D shallow-water model as an illustration;\nspecifically we show how the use of \\emph{channels} to communicate directly\nbetween peer kernels and the use of on-chip memory to create stencil buffers\ncan lead to significant performance improvements. Our results show better FPGA\nperformance against a baseline CPU implementation, and better energy-efficiency\nagainst both CPU and GPU implementations.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 12:52:52 GMT"}, {"version": "v2", "created": "Thu, 24 Jan 2019 13:30:29 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Vanderbauwhede", "Wim", ""], ["Nabi", "Syed Waqar", ""]]}, {"id": "1901.00479", "submitter": "Hoa Vu", "authors": "Hsin-Hao Su and Hoa T. Vu", "title": "Towards the Locality of Vizing's Theorem", "comments": "Extended abstract to appear at STOC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vizing showed that it suffices to color the edges of a simple graph using\n$\\Delta + 1$ colors, where $\\Delta$ is the maximum degree of the graph.\nHowever, up to this date, no efficient distributed edge-coloring algorithms are\nknown for obtaining such a coloring, even for constant degree graphs. The\ncurrent algorithms that get closest to this number of colors are the randomized\n$(\\Delta + \\tilde{\\Theta}(\\sqrt{\\Delta}))$-edge-coloring algorithm that runs in\n$\\text{polylog}(n)$ rounds by Chang et al. (SODA '18) and the deterministic\n$(\\Delta + \\text{polylog}(n))$-edge-coloring algorithm that runs in\n$\\text{poly}(\\Delta, \\log n)$ rounds by Ghaffari et al. (STOC '18).\n  We present two distributed edge-coloring algorithms that run in\n$\\text{poly}(\\Delta,\\log n)$ rounds. The first algorithm, with randomization,\nuses only $\\Delta+2$ colors. The second algorithm is a deterministic algorithm\nthat uses $\\Delta+ O(\\log n/ \\log \\log n)$ colors. Our approach is to reduce\nthe distributed edge-coloring problem into an online, restricted version of\nballs-into-bins problem. If $\\ell$ is the maximum load of the bins, our\nalgorithm uses $\\Delta + 2\\ell - 1$ colors. We show how to achieve $\\ell = 1$\nwith randomization and $\\ell = O(\\log n / \\log \\log n)$ without randomization.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 18:30:14 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 04:27:08 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Su", "Hsin-Hao", ""], ["Vu", "Hoa T.", ""]]}, {"id": "1901.00611", "submitter": "Chang-Shen Lee", "authors": "Chang-Shen Lee, Nicol\\`o Michelusi, Gesualdo Scutari", "title": "Finite rate distributed weight-balancing and average consensus over\n  digraphs", "comments": "A preliminary version arXiv:1809.06440 of this paper has appeared at\n  IEEE CDC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes the first distributed algorithm that solves the\nweight-balancing problem using only finite rate and simplex communications\namong nodes, compliant with the directed nature of the graph edges. It is\nproved that the algorithm converges to a weight-balanced solution at sublinear\nrate. The analysis builds upon a new metric inspired by positional system\nrepresentations, which characterizes the dynamics of information exchange over\nthe network, and on a novel step-size rule. Building on this result, a novel\ndistributed algorithm is proposed that solves the average consensus problem\nover digraphs, using, at each timeslot, finite rate simplex communications\nbetween adjacent nodes -- some bits for the weight-balancing problem and others\nfor the average consensus. Convergence of the proposed quantized consensus\nalgorithm to the average of the node's unquantized initial values is\nestablished, both almost surely and in the moment generating function of the\nerror; and a sublinear convergence rate is proved for sufficiently large\nstep-sizes. Numerical results validate our theoretical findings.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 05:02:54 GMT"}, {"version": "v2", "created": "Sat, 29 Feb 2020 19:36:45 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Lee", "Chang-Shen", ""], ["Michelusi", "Nicol\u00f2", ""], ["Scutari", "Gesualdo", ""]]}, {"id": "1901.00620", "submitter": "Pengfei Zuo", "authors": "Pengfei Zuo, Yu Hua, Yuan Xie", "title": "A Secure and Persistent Memory System for Non-volatile Memory", "comments": "15 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the non-volatile memory, ensuring the security and correctness of\npersistent data is fundamental. However, the security and persistence issues\nare usually studied independently in existing work. To achieve both data\nsecurity and persistence, simply combining existing persistence schemes with\nmemory encryption is inefficient due to crash inconsistency and significant\nperformance degradation. To bridge the gap between security and persistence,\nthis paper proposes SecPM, a Secure and Persistent Memory system, which\nconsists of a counter cache write-through (CWT) scheme and a locality-aware\ncounter write reduction (CWR) scheme. Specifically, SecPM leverages the CWT\nscheme to guarantee the crash consistency via ensuring both the data and its\ncounter are durable before the data flush completes, and leverages the CWR\nscheme to improve the system performance via exploiting the spatial locality of\ncounter storage, log and data writes. We have implemented SecPM in gem5 with\nNVMain and evaluated it using five widely-used workloads. Extensive\nexperimental results demonstrate that SecPM reduces up to half of write\nrequests and speeds up the transaction execution by 1.3-2.0 times via using the\nCWR scheme, and achieves the performance close to an un-encrypted persistent\nmemory system for large transactions.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 06:05:56 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Zuo", "Pengfei", ""], ["Hua", "Yu", ""], ["Xie", "Yuan", ""]]}, {"id": "1901.00622", "submitter": "I-Ting Lee", "authors": "Robert Utterback, Kunal Agrawal, Jeremy Fineman, I-Ting Angelina Lee", "title": "Efficient Race Detection with Futures", "comments": null, "journal-ref": null, "doi": "10.1145/3293883.3295732", "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of provably efficient and practically good\non-the-fly determinacy race detection in task parallel programs that use\nfutures. Prior works determinacy race detection have mostly focused on either\ntask parallel programs that follow a series-parallel dependence structure or\nones with unrestricted use of futures that generate arbitrary dependences. In\nthis work, we consider a restricted use of futures and show that it can be race\ndetected more efficiently than general use of futures.\n  Specifically, we present two algorithms: MultiBags and MultiBags+. MultiBags\ntargets programs that use futures in a restricted fashion and runs in time\n$O(T_1 \\alpha(m,n))$, where $T_1$ is the sequential running time of the\nprogram, $\\alpha$ is the inverse Ackermann's function, $m$ is the total number\nof memory accesses, $n$ is the dynamic count of places at which parallelism is\ncreated. Since $\\alpha$ is a very slowly growing function (upper bounded by $4$\nfor all practical purposes), it can be treated as a close-to-constant overhead.\nMultiBags+ an extension of MultiBags that target programs with general use of\nfutures. It runs in time $O((T_1+k^2)\\alpha(m,n))$ where $T_1$, $\\alpha$, $m$\nand $n$ are defined as before, and $k$ is the number of future operations in\nthe computation. We implemented both algorithms and empirically demonstrate\ntheir efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 06:10:51 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Utterback", "Robert", ""], ["Agrawal", "Kunal", ""], ["Fineman", "Jeremy", ""], ["Lee", "I-Ting Angelina", ""]]}, {"id": "1901.00644", "submitter": "Josef Spillner", "authors": "Josef Spillner", "title": "Quality Assessment and Improvement of Helm Charts for Kubernetes-Based\n  Cloud Applications", "comments": "19 pages, 11 figures, 9 tables, repeatable, preregistered, unreviewed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Helm has recently been proposed by practitioners as technology to package and\ndeploy complex software applications on top of Kubernetes-based cloud computing\nplatforms. Despite growing popularity, little is known about the individual\nso-called Helm Charts and about the emerging ecosystem of charts around the\nKubeApps Hub website and decentralised charts repositories. This article\ncontributes first quantified insights around both the charts and the artefact\ndevelopment community based on metrics automatically gathered by a proposed\nquality assessment tool named HelmQA. The work further identifies quality\ninsufficiencies detectable in public charts, proposes a developer-centric\nhypothesis-based methodology to systematically improve the quality by using\nHelmQA, and finally empirically attempts to validate the methodology and thus\nthe practical usefulness of the tool by presenting results of its application\nover a representative four-month period. Although one of our initial hypotheses\ndoes not statistically hold during the experiment, we still infer that using\nHelmQA regularly in continuous software development would lead to reduced\nquality issues.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 08:08:14 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Spillner", "Josef", ""]]}, {"id": "1901.00844", "submitter": "Mohammad Mohammadi Amiri Dr.", "authors": "Mohammad Mohammadi Amiri and Deniz Gunduz", "title": "Machine Learning at the Wireless Edge: Distributed Stochastic Gradient\n  Descent Over-the-Air", "comments": "IEEE Transactions on Signal Processing, Early Access, Mar. 2020", "journal-ref": null, "doi": "10.1109/TSP.2020.2981904", "report-no": null, "categories": "cs.DC cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study federated machine learning (ML) at the wireless edge, where power-\nand bandwidth-limited wireless devices with local datasets carry out\ndistributed stochastic gradient descent (DSGD) with the help of a remote\nparameter server (PS). Standard approaches assume separate computation and\ncommunication, where local gradient estimates are compressed and transmitted to\nthe PS over orthogonal links. Following this digital approach, we introduce\nD-DSGD, in which the wireless devices employ gradient quantization and error\naccumulation, and transmit their gradient estimates to the PS over a multiple\naccess channel (MAC). We then introduce a novel analog scheme, called A-DSGD,\nwhich exploits the additive nature of the wireless MAC for over-the-air\ngradient computation, and provide convergence analysis for this approach. In\nA-DSGD, the devices first sparsify their gradient estimates, and then project\nthem to a lower dimensional space imposed by the available channel bandwidth.\nThese projections are sent directly over the MAC without employing any digital\ncode. Numerical results show that A-DSGD converges faster than D-DSGD thanks to\nits more efficient use of the limited bandwidth and the natural alignment of\nthe gradient estimates over the channel. The improvement is particularly\ncompelling at low power and low bandwidth regimes. We also illustrate for a\nclassification problem that, A-DSGD is more robust to bias in data distribution\nacross devices, while D-DSGD significantly outperforms other digital schemes in\nthe literature. We also observe that both D-DSGD and A-DSGD perform better by\nincreasing the number of devices (while keeping the total dataset size\nconstant), showing their ability in harnessing the computation power of edge\ndevices.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 18:46:31 GMT"}, {"version": "v2", "created": "Sun, 3 Feb 2019 21:27:55 GMT"}, {"version": "v3", "created": "Tue, 7 Apr 2020 05:09:25 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Amiri", "Mohammad Mohammadi", ""], ["Gunduz", "Deniz", ""]]}, {"id": "1901.00910", "submitter": "Christian Gorenflo", "authors": "Christian Gorenflo, Stephen Lee, Lukasz Golab, S. Keshav", "title": "FastFabric: Scaling Hyperledger Fabric to 20,000 Transactions per Second", "comments": "Minor revisions based on reviewer feedback", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain technologies are expected to make a significant impact on a\nvariety of industries. However, one issue holding them back is their limited\ntransaction throughput, especially compared to established solutions such as\ndistributed database systems. In this paper, we re-architect a modern\npermissioned blockchain system, Hyperledger Fabric, to increase transaction\nthroughput from 3,000 to 20,000 transactions per second. We focus on\nperformance bottlenecks beyond the consensus mechanism, and we propose\narchitectural changes that reduce computation and I/O overhead during\ntransaction ordering and validation to greatly improve throughput. Notably, our\noptimizations are fully plug-and-play and do not require any interface changes\nto Hyperledger Fabric.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 20:35:02 GMT"}, {"version": "v2", "created": "Mon, 4 Mar 2019 20:37:36 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Gorenflo", "Christian", ""], ["Lee", "Stephen", ""], ["Golab", "Lukasz", ""], ["Keshav", "S.", ""]]}, {"id": "1901.01007", "submitter": "Tong Geng", "authors": "Tong Geng, Tianqi Wang, Ang Li, Xi Jin, Martin Herbordt", "title": "FPDeep: Scalable Acceleration of CNN Training on Deeply-Pipelined FPGA\n  Clusters", "comments": "Accepted by IEEE TRANSACTIONS ON COMPUTERS (TC)", "journal-ref": null, "doi": "10.1109/TC.2020.3000118", "report-no": null, "categories": "cs.LG cs.AR cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have revolutionized numerous applications, but\nthe demand for ever more performance remains unabated. Scaling DNN computations\nto larger clusters is generally done by distributing tasks in batch mode using\nmethods such as distributed synchronous SGD. Among the issues with this\napproach is that to make the distributed cluster work with high utilization,\nthe workload distributed to each node must be large, which implies nontrivial\ngrowth in the SGD mini-batch size.\n  In this paper, we propose a framework called FPDeep, which uses a hybrid of\nmodel and layer parallelism to configure distributed reconfigurable clusters to\ntrain DNNs. This approach has numerous benefits. First, the design does not\nsuffer from batch size growth. Second, novel workload and weight partitioning\nleads to balanced loads of both among nodes. And third, the entire system is a\nfine-grained pipeline. This leads to high parallelism and utilization and also\nminimizes the time features need to be cached while waiting for\nback-propagation. As a result, storage demand is reduced to the point where\nonly on-chip memory is used for the convolution layers. We evaluate FPDeep with\nthe Alexnet, VGG-16, and VGG-19 benchmarks. Experimental results show that\nFPDeep has good scalability to a large number of FPGAs, with the limiting\nfactor being the FPGA-to-FPGA bandwidth. With 6 transceivers per FPGA, FPDeep\nshows linearity up to 83 FPGAs. Energy efficiency is evaluated with respect to\nGOPs/J. FPDeep provides, on average, 6.36x higher energy efficiency than\ncomparable GPU servers.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 07:54:46 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 19:03:13 GMT"}, {"version": "v3", "created": "Mon, 25 May 2020 22:33:11 GMT"}, {"version": "v4", "created": "Sun, 21 Jun 2020 04:17:46 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Geng", "Tong", ""], ["Wang", "Tianqi", ""], ["Li", "Ang", ""], ["Jin", "Xi", ""], ["Herbordt", "Martin", ""]]}, {"id": "1901.01331", "submitter": "Carleton Coffrin", "authors": "Carleton Coffrin, James Arnold, Stephan Eidenbenz, Derek Aberle, John\n  Ambrosiano, Zachary Baker, Sara Brambilla, Michael Brown, K. Nolan Carter,\n  Pinghan Chu, Patrick Conry, Keeley Costigan, Ariane Eberhardt, David M.\n  Fobes, Adam Gausmann, Sean Harris, Donovan Heimer, Marlin Holmes, Bill Junor,\n  Csaba Kiss, Steve Linger, Rodman Linn, Li-Ta Lo, Jonathan MacCarthy, Omar\n  Marcillo, Clay McGinnis, Alexander McQuarters, Eric Michalak, Arvind Mohan,\n  Matt Nelson, Diane Oyen, Nidhi Parikh, Donatella Pasqualini, Aaron s. Pope,\n  Reid Porter, Chris Rawlings, Hannah Reinbolt, Reid Rivenburgh, Phil Romero,\n  Kevin Schoonover, Alexei Skurikhin, Daniel Tauritz, Dima Tretiak, Zhehui\n  Wang, James Wernicke, Brad Wolfe, Phillip Wolfram, Jonathan Woodring", "title": "The ISTI Rapid Response on Exploring Cloud Computing 2018", "comments": null, "journal-ref": null, "doi": null, "report-no": "LA-UR-18-31581", "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report describes eighteen projects that explored how commercial cloud\ncomputing services can be utilized for scientific computation at national\nlaboratories. These demonstrations ranged from deploying proprietary software\nin a cloud environment to leveraging established cloud-based analytics\nworkflows for processing scientific datasets. By and large, the projects were\nsuccessful and collectively they suggest that cloud computing can be a valuable\ncomputational resource for scientific computation at national laboratories.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 22:31:55 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Coffrin", "Carleton", ""], ["Arnold", "James", ""], ["Eidenbenz", "Stephan", ""], ["Aberle", "Derek", ""], ["Ambrosiano", "John", ""], ["Baker", "Zachary", ""], ["Brambilla", "Sara", ""], ["Brown", "Michael", ""], ["Carter", "K. Nolan", ""], ["Chu", "Pinghan", ""], ["Conry", "Patrick", ""], ["Costigan", "Keeley", ""], ["Eberhardt", "Ariane", ""], ["Fobes", "David M.", ""], ["Gausmann", "Adam", ""], ["Harris", "Sean", ""], ["Heimer", "Donovan", ""], ["Holmes", "Marlin", ""], ["Junor", "Bill", ""], ["Kiss", "Csaba", ""], ["Linger", "Steve", ""], ["Linn", "Rodman", ""], ["Lo", "Li-Ta", ""], ["MacCarthy", "Jonathan", ""], ["Marcillo", "Omar", ""], ["McGinnis", "Clay", ""], ["McQuarters", "Alexander", ""], ["Michalak", "Eric", ""], ["Mohan", "Arvind", ""], ["Nelson", "Matt", ""], ["Oyen", "Diane", ""], ["Parikh", "Nidhi", ""], ["Pasqualini", "Donatella", ""], ["Pope", "Aaron s.", ""], ["Porter", "Reid", ""], ["Rawlings", "Chris", ""], ["Reinbolt", "Hannah", ""], ["Rivenburgh", "Reid", ""], ["Romero", "Phil", ""], ["Schoonover", "Kevin", ""], ["Skurikhin", "Alexei", ""], ["Tauritz", "Daniel", ""], ["Tretiak", "Dima", ""], ["Wang", "Zhehui", ""], ["Wernicke", "James", ""], ["Wolfe", "Brad", ""], ["Wolfram", "Phillip", ""], ["Woodring", "Jonathan", ""]]}, {"id": "1901.01376", "submitter": "Vikram Saraph", "authors": "Vikram Saraph and Maurice Herlihy", "title": "An Empirical Study of Speculative Concurrency in Ethereum Smart\n  Contracts", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use historical data to estimate the potential benefit of speculative\ntechniques for executing Ethereum smart contracts in parallel. We replay\ntransaction traces of sampled blocks from the Ethereum blockchain over time,\nusing a simple speculative execution engine. In this engine, miners attempt to\nexecute all transactions in a block in parallel, rolling back those that cause\ndata conflicts. Aborted transactions are then executed sequentially. Validators\nexecute the same schedule as miners.\n  We find that our speculative technique yields estimated speed-ups starting at\nabout 8-fold in 2016, declining to about 2-fold at the end of 2017, where\nspeed-up is measured using either gas costs or instruction counts. We also\nobserve that a small set of contracts are responsible for many data conflicts\nresulting from speculative concurrent execution.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jan 2019 07:33:33 GMT"}, {"version": "v2", "created": "Tue, 22 Jan 2019 00:41:42 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Saraph", "Vikram", ""], ["Herlihy", "Maurice", ""]]}, {"id": "1901.01544", "submitter": "Zehua Cheng", "authors": "Zehua Cheng and Zhenghua Xu", "title": "Bandwidth Reduction using Importance Weighted Pruning on Ring AllReduce", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is inevitable to train large deep learning models on a large-scale cluster\nequipped with accelerators system. Deep gradient compression would highly\nincrease the bandwidth utilization and speed up the training process but hard\nto implement on ring structure. In this paper, we find that redundant gradient\nand gradient staleness has negative effect on training. We have observed that\nin different epoch and different steps, the neural networks focus on updating\ndifferent layers and different parameters. In order to save more communication\nbandwidth and preserve the accuracy on ring structure, which break the restrict\nas the node increase, we propose a new algorithm to measure the importance of\ngradients on large-scale cluster implementing ring all-reduce based on the size\nof the ratio of parameter calculation gradient to parameter value. Our\nimportance weighted pruning approach achieved 64X and 58.8X of gradient\ncompression ratio on AlexNet and ResNet50 on ImageNet. Meanwhile, in order to\nmaintain the sparseness of the gradient propagation, we randomly broadcast the\nindex of important gradients on each node. While the remaining nodes are ready\nfor the index gradient and perform all-reduce update. This would speed up the\nconvergence of the model and preserve the training accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jan 2019 13:52:07 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Cheng", "Zehua", ""], ["Xu", "Zhenghua", ""]]}, {"id": "1901.01628", "submitter": "Yiying Zhang", "authors": "Shin-Yeh Tsai, Yiying Zhang", "title": "Building Atomic, Crash-Consistent Data Stores with Disaggregated\n  Persistent Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Byte-addressable persistent memories (PM) has finally made their way into\nproduction. An important and pressing problem that follows is how to deploy\nthem in existing datacenters. One viable approach is to attach PM as\nself-contained devices to the network as disaggregated persistent memory, or\nDPM. DPM requires no changes to existing servers in datacenters; without the\nneed to include a processor, DPM devices are cheap to build; and by sharing DPM\nacross compute servers, they offer great elasticity and efficient resource\npacking.\n  This paper explores different ways to organize DPM and to build data stores\nwith DPM. Specifically, we propose three architectures of DPM: 1) compute nodes\ndirectly access DPM (DPM-Direct); 2) compute nodes send requests to a\ncoordinator server, which then accesses DPM to complete a request\n(DPM-Central); and 3) compute nodes directly access DPM for data operations and\ncommunicate with a global metadata server for the control plane (DPM-Sep).\nBased on these architectures, we built three atomic, crash-consistent data\nstores. We evaluated their performance, scalability, and CPU cost with\nmicro-benchmarks and YCSB. Our evaluation results show that DPM-Direct has\ngreat small-size read but poor write performance; DPM-Central has the best\nwrite performance when the scale of the cluster is small but performs poorly\nwhen the scale increases; and DPM-Sep performs well overall.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 00:02:22 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Tsai", "Shin-Yeh", ""], ["Zhang", "Yiying", ""]]}, {"id": "1901.01630", "submitter": "Ami Paz", "authors": "Amir Abboud and Keren Censor-Hillel and Seri Khoury and Ami Paz", "title": "Smaller Cuts, Higher Lower Bounds", "comments": "This is work is a merger of arXiv:1605.05109 and arXiv:1705.05646", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proves strong lower bounds for distributed computing in the\nCONGEST model, by presenting the bit-gadget: a new technique for constructing\ngraphs with small cuts.\n  The contribution of bit-gadgets is twofold. First, developing careful sparse\ngraph constructions with small cuts extends known techniques to show a\nnear-linear lower bound for computing the diameter, a result previously known\nonly for dense graphs. Moreover, the sparseness of the construction plays a\ncrucial role in applying it to approximations of various distance computation\nproblems, drastically improving over what can be obtained when using dense\ngraphs.\n  Second, small cuts are essential for proving super-linear lower bounds, none\nof which were known prior to this work. In fact, they allow us to show\nnear-quadratic lower bounds for several problems, such as exact minimum vertex\ncover or maximum independent set, as well as for coloring a graph with its\nchromatic number. Such strong lower bounds are not limited to NP-hard problems,\nas given by two simple graph problems in P which are shown to require a\nquadratic and near-quadratic number of rounds. All of the above are optimal up\nto logarithmic factors. In addition, in this context, the complexity of the\nall-pairs-shortest-paths problem is discussed.\n  Finally, it is shown that graph constructions for CONGEST lower bounds\ntranslate to lower bounds for the semi-streaming model, despite being very\ndifferent in its nature.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 00:09:33 GMT"}, {"version": "v2", "created": "Mon, 14 Jan 2019 09:40:21 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Abboud", "Amir", ""], ["Censor-Hillel", "Keren", ""], ["Khoury", "Seri", ""], ["Paz", "Ami", ""]]}, {"id": "1901.01665", "submitter": "Nina Holden", "authors": "Giulia Fanti, Nina Holden, Yuval Peres, and Gireeja Ranade", "title": "Communication cost of consensus for nodes with limited memory", "comments": "62 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications in blockchains and sensor networks, we consider a\nmodel of $n$ nodes trying to reach consensus on their majority bit. Each node\n$i$ is assigned a bit at time zero, and is a finite automaton with $m$ bits of\nmemory (i.e., $2^m$ states) and a Poisson clock. When the clock of $i$ rings,\n$i$ can choose to communicate, and is then matched to a uniformly chosen node\n$j$. The nodes $j$ and $i$ may update their states based on the state of the\nother node. Previous work has focused on minimizing the time to consensus and\nthe probability of error, while our goal is minimizing the number of\ncommunications. We show that when $m>3 \\log\\log\\log(n)$, consensus can be\nreached at linear communication cost, but this is impossible if\n$m<\\log\\log\\log(n)$. We also study a synchronous variant of the model, where\nour upper and lower bounds on $m$ for achieving linear communication cost are\n$2\\log\\log\\log(n)$ and $\\log\\log\\log(n)$, respectively. A key step is to\ndistinguish when nodes can become aware of knowing the majority bit and stop\ncommunicating. We show that this is impossible if their memory is too low.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 04:48:34 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Fanti", "Giulia", ""], ["Holden", "Nina", ""], ["Peres", "Yuval", ""], ["Ranade", "Gireeja", ""]]}, {"id": "1901.01854", "submitter": "Qiaobin Kuang", "authors": "Qiaobin Kuang, Jie Gong, Xiang Chen, and Xiao Ma", "title": "Age-of-Information for Computation-Intensive Messages in Mobile Edge\n  Computing", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Age-of-information (AoI) is a novel metric that measures the freshness of\ninformation in status update scenarios. It is essential for real-time\napplications to transmit status update packets to the destination node as\ntimely as possible. However, for some applications, status information embedded\nin the packets is not revealed until complicated data processing, which is\ncomputational expensive and time consuming. As mobile edge server has\nsufficient computational resource and is placed close to users, mobile edge\ncomputing (MEC) is expected to reduce age for computation-intensive messages.\nIn this paper, we study the AoI for computation-intensive data in MEC, and\nconsider two schemes: local computing by user itself and remote computing at\nMEC server. The two computing models are unified into a two-node tandem queuing\nmodel. Zero-wait policy is adopted, i.e., a new message is generated once the\nprevious one leaves the first node. We consider exponentially distributed\nservice time and infinite queue size, and hence, the second node can be seen as\na First-Come-First-Served (FCFS) M/M/1 system. Closed-form average AoI is\nderived for the two computing schemes. The region where remote computing\noutperforms local computing is characterized. Simulation results show that the\nremote computing is greatly superior to the local computing when the remote\ncomputing rate is large enough, and that there exists an optimal transmission\nrate so that remote computing is better than local computing for a largest\nrange.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 14:57:08 GMT"}, {"version": "v2", "created": "Tue, 8 Jan 2019 03:15:24 GMT"}, {"version": "v3", "created": "Sat, 12 Jan 2019 05:45:07 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Kuang", "Qiaobin", ""], ["Gong", "Jie", ""], ["Chen", "Xiang", ""], ["Ma", "Xiao", ""]]}, {"id": "1901.01908", "submitter": "Petar Maymounkov", "authors": "Petar Maymounkov", "title": "Koji: Automating pipelines with mixed-semantics data sources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new result-oriented semantic for defining data processing\nworkflows that manipulate data in different semantic forms (files or services)\nin a unified manner. This approach enables users to define workflows for a vast\nvariety of reproducible data-processing tasks in a simple declarative manner\nwhich focuses on application-level results, while automating all control-plane\nconsiderations (like failure recovery without loss of progress and computation\nreuse) behind the scenes.\n  The uniform treatment of files and services as data enables easy integration\nwith existing data sources (e.g. data acquisition APIs) and sinks of data (e.g.\ndatabase services). Whereas the focus on containers as transformations enables\nreuse of existing data-processing systems.\n  We describe a declarative configuration mechanism, which can be viewed as an\nintermediate representation (IR) of reproducible data processing pipelines in\nthe same spirit as, for instance, TensorFlow\\cite{tensorflow} and\nONNX\\cite{onnx} utilize IRs for defining tensor-processing pipelines.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2018 13:40:16 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Maymounkov", "Petar", ""]]}, {"id": "1901.01930", "submitter": "Joseph M. Hellerstein", "authors": "Joseph M. Hellerstein and Peter Alvaro", "title": "Keeping CALM: When Distributed Consistency is Easy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.PL cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A key concern in modern distributed systems is to avoid the cost of\ncoordination while maintaining consistent semantics. Until recently, there was\nno answer to the question of when coordination is actually required. In this\npaper we present an informal introduction to the CALM Theorem, which answers\nthis question precisely by moving up from traditional storage consistency to\nconsider properties of programs.\n  CALM is an acronym for \"consistency as logical monotonicity\". The CALM\nTheorem shows that the programs that have consistent, coordination-free\ndistributed implementations are exactly the programs that can be expressed in\nmonotonic logic. This theoretical result has practical implications for\ndevelopers of distributed applications. We show how CALM provides a\nconstructive application-level counterpart to conventional \"systems\" wisdom,\nsuch as the apparently negative results of the CAP Theorem. We also discuss\nways that monotonic thinking can influence distributed systems design, and how\nnew programming language designs and tools can help developers write\nconsistent, coordination-free code.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 17:20:25 GMT"}, {"version": "v2", "created": "Sat, 26 Jan 2019 00:16:08 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Hellerstein", "Joseph M.", ""], ["Alvaro", "Peter", ""]]}, {"id": "1901.01943", "submitter": "Pooja Vyavahare", "authors": "Pooja Vyavahare and Lili Su and Nitin H. Vaidya", "title": "Distributed Learning with Adversarial Agents Under Relaxed Network\n  Condition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies the problem of non-Bayesian learning over multi-agent\nnetwork when there are some adversarial (faulty) agents in the network. At each\ntime step, each non-faulty agent collects partial information about an unknown\nstate of the world and tries to estimate true state of the world by iteratively\nsharing information with its neighbors. Existing algorithms in this setting\nrequire that all non-faulty agents in the network should be able to achieve\nconsensus via local information exchange.\n  In this work, we present an analysis of a distributed algorithm which does\nnot require the network to achieve consensus. We show that if every non-faulty\nagent can receive enough information (via iteratively communicating with\nneighbors) to differentiate the true state of the world from other possible\nstates then it can indeed learn the true state.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 17:49:35 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Vyavahare", "Pooja", ""], ["Su", "Lili", ""], ["Vaidya", "Nitin H.", ""]]}, {"id": "1901.02067", "submitter": "Linghao Song", "authors": "Linghao Song, Jiachen Mao, Youwei Zhuo, Xuehai Qian, Hai Li, Yiran\n  Chen", "title": "HyPar: Towards Hybrid Parallelism for Deep Learning Accelerator Array", "comments": "To appear in the 2019 25th International Symposium on\n  High-Performance Computer Architecture (HPCA 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rise of artificial intelligence in recent years, Deep Neural\nNetworks (DNNs) have been widely used in many domains. To achieve high\nperformance and energy efficiency, hardware acceleration (especially inference)\nof DNNs is intensively studied both in academia and industry. However, we still\nface two challenges: large DNN models and datasets, which incur frequent\noff-chip memory accesses; and the training of DNNs, which is not well-explored\nin recent accelerator designs. To truly provide high throughput and energy\nefficient acceleration for the training of deep and large models, we inevitably\nneed to use multiple accelerators to explore the coarse-grain parallelism,\ncompared to the fine-grain parallelism inside a layer considered in most of the\nexisting architectures. It poses the key research question to seek the best\norganization of computation and dataflow among accelerators. In this paper,\ninspired by recent work in machine learning systems, we propose a solution\nHyPar to determine layer-wise parallelism for deep neural network training with\nan array of DNN accelerators. HyPar partitions the feature map tensors (input\nand output), the kernel tensors, the gradient tensors, and the error tensors\nfor the DNN accelerators. A partition constitutes the choice of parallelism for\nweighted layers. The optimization target is to search a partition that\nminimizes the total communication during training a complete DNN. To solve this\nproblem, we propose a communication model to explain the source and amount of\ncommunications. Then, we use a hierarchical layer-wise dynamic programming\nmethod to search for the partition for each layer.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 21:14:50 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2020 05:25:12 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Song", "Linghao", ""], ["Mao", "Jiachen", ""], ["Zhuo", "Youwei", ""], ["Qian", "Xuehai", ""], ["Li", "Hai", ""], ["Chen", "Yiran", ""]]}, {"id": "1901.02192", "submitter": "Yu Huang", "authors": "Yu Huang, Hengfeng Wei, Maosen Huang, Lingzhi Ouyang", "title": "Inversion-based Measurement of Data Consistency for Read/Write Registers", "comments": "correcting errors in v3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both providers and consumers of distributed storage services benefit from the\nquantification of the severity of consistency violations. However, existing\nmethods fail to capture a typical pattern of violation - the disorder among\noperations with reference to the ideally strong atomic execution. Such disorder\nare often seen in Internet applications based on distributed storage services,\nsuch as instant messaging. To this end, we use inversions in a permutation with\nreference to the totally-ordered permutation to measure the consistency of\ndata. The $i$-atomicity model is proposed and a (pseudo-)polynomial\nverification algorithm for a restricted type of histories is proposed. The\nbasic idea of the verification algorithm is brute-force search of the\npermutation with less than $i$ inversions, which requires exponential cost.\nReasonable assumptions abstracted from applications scenarios can be leveraged\nto prune the search space, which yields an efficient polynomial verification\nalgorithm.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 07:50:40 GMT"}, {"version": "v2", "created": "Thu, 17 Jan 2019 08:32:57 GMT"}, {"version": "v3", "created": "Fri, 18 Jan 2019 02:06:12 GMT"}, {"version": "v4", "created": "Mon, 18 Feb 2019 12:57:50 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Huang", "Yu", ""], ["Wei", "Hengfeng", ""], ["Huang", "Maosen", ""], ["Ouyang", "Lingzhi", ""]]}, {"id": "1901.02213", "submitter": "Thomas Bocek", "authors": "Roman Blum, Thomas Bocek", "title": "Superlight -- A Permissionless, Light-client Only Blockchain with\n  Self-Contained Proofs and BLS Signatures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain protocols are based on a distributed database where stored data is\nguaranteed to be immutable. The requirement that all nodes have to maintain\ntheir own local copy of the database ensures security while consensus\nmechanisms help deciding which data gets added to the database and keep\npowerful adversaries from derailing the system. However, since the database\nthat forms the foundation of a blockchain is a continuously growing list of\nblocks, scalability is an inherent problem of this technology. Some public\nblockchains need a few 100 GB to Terabytes of storage.\n  In this work, we present the concept Superlight with self-contained proofs,\nwhich is designed to improve scalability of a public blockchain, while\npreserving security and decentralization. Instead of all nodes having a local\ncopy of the whole blockchain to verify a transaction, nodes can derive the\nvalidity of a transaction by only using block headers. To keep the block\nheaders compact, BLS signatures are used to combine signatures. We provide a\ndefinition of SCPs and show the required steps of a client to create a proof\nthat is accepted by other nodes for transferring funds. The advantage of such a\nlight-client-only blockchain is the lower storage requirement, while the\ndrawback is an increased computational complexity due to BLS signatures,\nlimited use-cases due to lack of a global state, and the requirement for an\ninteractive protocol between sender, receiver, and miner to create a\ntransaction.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 09:21:47 GMT"}, {"version": "v2", "created": "Thu, 7 Feb 2019 15:52:58 GMT"}, {"version": "v3", "created": "Mon, 16 Sep 2019 15:33:55 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Blum", "Roman", ""], ["Bocek", "Thomas", ""]]}, {"id": "1901.02244", "submitter": "Alexandros Koliousis", "authors": "Alexandros Koliousis, Pijika Watcharapichat, Matthias Weidlich, Luo\n  Mai, Paolo Costa and Peter Pietzuch", "title": "CROSSBOW: Scaling Deep Learning with Small Batch Sizes on Multi-GPU\n  Servers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models are trained on servers with many GPUs, and training must\nscale with the number of GPUs. Systems such as TensorFlow and Caffe2 train\nmodels with parallel synchronous stochastic gradient descent: they process a\nbatch of training data at a time, partitioned across GPUs, and average the\nresulting partial gradients to obtain an updated global model. To fully utilise\nall GPUs, systems must increase the batch size, which hinders statistical\nefficiency. Users tune hyper-parameters such as the learning rate to compensate\nfor this, which is complex and model-specific.\n  We describe CROSSBOW, a new single-server multi-GPU system for training deep\nlearning models that enables users to freely choose their preferred batch size\n- however small - while scaling to multiple GPUs. CROSSBOW uses many parallel\nmodel replicas and avoids reduced statistical efficiency through a new\nsynchronous training method. We introduce SMA, a synchronous variant of model\naveraging in which replicas independently explore the solution space with\ngradient descent, but adjust their search synchronously based on the trajectory\nof a globally-consistent average model. CROSSBOW achieves high hardware\nefficiency with small batch sizes by potentially training multiple model\nreplicas per GPU, automatically tuning the number of replicas to maximise\nthroughput. Our experiments show that CROSSBOW improves the training time of\ndeep learning models on an 8-GPU server by 1.3-4x compared to TensorFlow.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 10:45:24 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Koliousis", "Alexandros", ""], ["Watcharapichat", "Pijika", ""], ["Weidlich", "Matthias", ""], ["Mai", "Luo", ""], ["Costa", "Paolo", ""], ["Pietzuch", "Peter", ""]]}, {"id": "1901.02303", "submitter": "Kishan Prudhvi Guddanti", "authors": "Kishan Prudhvi Guddanti, Amarsagar Reddy Ramapuram Matavalam, Yang\n  Weng", "title": "PMU-based Distributed Non-iterative Algorithm for Real-time Voltage\n  Stability Monitoring", "comments": null, "journal-ref": "IEEE Transactions on Smart Grid, Volume: 11, Issue: 6, Nov. 2020,\n  Page(s): 5203 - 5215", "doi": "10.1109/TSG.2020.3007063", "report-no": null, "categories": "cs.SY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Phasor measurement unit (PMU) measurements are mandatory to monitor the\npower system's voltage stability margin in an online manner. Monitoring is key\nto the secure operation of the grid. Traditionally, online monitoring of\nvoltage stability using synchrophasors required a centralized communication\narchitecture, which leads to high investment cost and cyber-security concerns.\nThe increasing importance of cyber-security and low investment cost have\nrecently led to the development of distributed algorithms for online monitoring\nof the grid that are inherently less prone to malicious attacks. In this work,\na novel distributed non-iterative voltage stability index (VSI) is proposed by\nrecasting the power flow equations as circles. The online computations of VSI\nare simultaneously performed by the processors embedded at each bus in the\nsmart grid with the help of PMUs and communication of voltage phasors between\nneighboring buses. The distributed nature of the index enables the real-time\nidentification of the critical bus of the system with minimal communication\ninfrastructure. The effectiveness of the proposed distributed index is\ndemonstrated on IEEE test systems and contrasted with existing methods to show\nthe benefits of the proposed method in speed, interpretability, identification\nof outage location, and low sensitivity to noisy measurements.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jan 2019 05:41:05 GMT"}, {"version": "v2", "created": "Mon, 2 Sep 2019 21:24:33 GMT"}, {"version": "v3", "created": "Thu, 26 Sep 2019 20:28:57 GMT"}, {"version": "v4", "created": "Mon, 19 Jul 2021 19:07:10 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Guddanti", "Kishan Prudhvi", ""], ["Matavalam", "Amarsagar Reddy Ramapuram", ""], ["Weng", "Yang", ""]]}, {"id": "1901.02436", "submitter": "Layla Majzoobi", "authors": "Layla Majzoobi, Farshad Lahouti, Vahid Shah-Mansouri", "title": "Analysis of Distributed ADMM Algorithm for Consensus Optimization in\n  Presence of Node Error", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2019.2896266", "report-no": null, "categories": "eess.SP cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alternating Direction Method of Multipliers (ADMM) is a popular convex\noptimization algorithm, which can be employed for solving distributed consensus\noptimization problems. In this setting agents locally estimate the optimal\nsolution of an optimization problem and exchange messages with their neighbors\nover a connected network. The distributed algorithms are typically exposed to\ndifferent types of errors in practice, e.g., due to quantization or\ncommunication noise or loss. We here focus on analyzing the convergence of\ndistributed ADMM for consensus optimization in presence of additive random node\nerror, in which case, the nodes communicate a noisy version of their latest\nestimate of the solution to their neighbors in each iteration. We present\nanalytical upper and lower bounds on the mean squared steady state error of the\nalgorithm in case that the local objective functions are strongly convex and\nhave Lipschitz continuous gradients. In addition we show that, when the local\nobjective functions are convex and the additive node error is bounded, the\nestimation error of the noisy ADMM for consensus optimization is also bounded.\nNumerical results are provided which demonstrate the effectiveness of the\npresented analyses and shed light on the role of the system and network\nparameters on performance.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 18:32:17 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Majzoobi", "Layla", ""], ["Lahouti", "Farshad", ""], ["Shah-Mansouri", "Vahid", ""]]}, {"id": "1901.02441", "submitter": "Jukka Suomela", "authors": "Alkida Balliu, Sebastian Brandt, Juho Hirvonen, Dennis Olivetti,\n  Mika\\\"el Rabie, Jukka Suomela", "title": "Lower bounds for maximal matchings and maximal independent sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are distributed graph algorithms for finding maximal matchings and\nmaximal independent sets in $O(\\Delta + \\log^* n)$ communication rounds; here\n$n$ is the number of nodes and $\\Delta$ is the maximum degree. The lower bound\nby Linial (1987, 1992) shows that the dependency on $n$ is optimal: these\nproblems cannot be solved in $o(\\log^* n)$ rounds even if $\\Delta = 2$.\n  However, the dependency on $\\Delta$ is a long-standing open question, and\nthere is currently an exponential gap between the upper and lower bounds.\n  We prove that the upper bounds are tight. We show that maximal matchings and\nmaximal independent sets cannot be found in $o(\\Delta + \\log \\log n / \\log \\log\n\\log n)$ rounds with any randomized algorithm in the LOCAL model of distributed\ncomputing.\n  As a corollary, it follows that there is no deterministic algorithm for\nmaximal matchings or maximal independent sets that runs in $o(\\Delta + \\log n /\n\\log \\log n)$ rounds; this is an improvement over prior lower bounds also as a\nfunction of $n$.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 18:48:04 GMT"}, {"version": "v2", "created": "Fri, 8 Feb 2019 18:05:57 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Balliu", "Alkida", ""], ["Brandt", "Sebastian", ""], ["Hirvonen", "Juho", ""], ["Olivetti", "Dennis", ""], ["Rabie", "Mika\u00ebl", ""], ["Suomela", "Jukka", ""]]}, {"id": "1901.02542", "submitter": "Ovidiu Vaduvescu", "authors": "D. Copandean, C. Nandra, D. Gorgan, O. Vaduvescu", "title": "Asteroids Detection Technique: Classic \"Blink\" An Automated Approch", "comments": "Conference: 2018 IEEE International Conference on Automation, Quality\n  and Testing, Robotics (AQTR), 24-26 May 2018, Cluj-Napoca, Romania", "journal-ref": "IEEE, 2018", "doi": "10.1109/AQTR.2018.8402768", "report-no": null, "categories": "astro-ph.IM cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asteroids detection is a very important research field that received\nincreased attention in the last couple of decades. Some major surveys have\ntheir own dedicated people, equipment and detection applications, so they are\ndiscovering Near Earth Asteroids (NEAs) daily. The interest in asteroids is not\nlimited to those major surveys, it is shared by amateurs and mini-surveys too.\nA couple of them are using the few existent software solutions, most of which\nare developed by amateurs. The rest obtain their results in a visual manner:\nthey \"blink\" a sequence of reduced images of the same field, taken at a\nspecific time interval, and they try to detect a real moving object in the\nresulting animation. Such a technique becomes harder with the increase in size\nof the CCD cameras. Aiming to replace manual detection, we propose an automated\n\"blink\" technique for asteroids detection.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 22:34:18 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Copandean", "D.", ""], ["Nandra", "C.", ""], ["Gorgan", "D.", ""], ["Vaduvescu", "O.", ""]]}, {"id": "1901.02545", "submitter": "Ovidiu Vaduvescu", "authors": "T. Stefanut, V. Bacu, C. Nandra, D. Balasz, D. Gorgan and O. Vaduvescu", "title": "NEARBY Platform: Algorithm for Automated Asteroids Detection in\n  Astronomical Images", "comments": "IEEE 14th International Conference on Intelligent Computer\n  Communication and Processing (ICCP), Sep 6-8, 2018, Cluj-Napoca, Romania", "journal-ref": "IEEE, 2018", "doi": "10.1109/ICCP.2018.8516594", "report-no": null, "categories": "astro-ph.IM cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past two decades an increasing interest in discovering Near Earth\nObjects has been noted in the astronomical community. Dedicated surveys have\nbeen operated for data acquisition and processing, resulting in the present\ndiscovery of over 18.000 objects that are closer than 30 million miles of\nEarth. Nevertheless, recent events have shown that there still are many\nundiscovered asteroids that can be on collision course to Earth. This article\npresents an original NEO detection algorithm developed in the NEARBY research\nobject, that has been integrated into an automated MOPS processing pipeline\naimed at identifying moving space objects based on the blink method. Proposed\nsolution can be considered an approach of Big Data processing and analysis,\nimplementing visual analytics techniques for rapid human data validation.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 22:45:28 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Stefanut", "T.", ""], ["Bacu", "V.", ""], ["Nandra", "C.", ""], ["Balasz", "D.", ""], ["Gorgan", "D.", ""], ["Vaduvescu", "O.", ""]]}, {"id": "1901.02680", "submitter": "Manuel Stein", "authors": "Manuel Stein", "title": "Interim Report on Adaptive Event Dispatching in Serverless Computing\n  Infrastructures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Serverless computing is an emerging service model in distributed computing\nsystems. The term captures cloud-based event-driven distributed application\ndesign and stems from its completely resource-transparent deployment model,\ni.e. serverless. This work thesisizes that adaptive event dispatching can\nimprove current serverless platform resource efficiency by considering locality\nand dependencies. These design contemplations have also been formulated by\nHendrickson et al., which identifies the requirement that \"Serverless load\nbalancers must make low-latency decisions while considering session, code and\ndata locality\". This interim report investigates the economical importance of\nthe emerging trend and asserts that existing serverless platforms still do not\noptimize for data locality, whereas a variety of scheduling methods are\navailable from distributed computing research which have proven to increase\nresource efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 11:13:52 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Stein", "Manuel", ""]]}, {"id": "1901.02712", "submitter": "Merim Dzaferagic", "authors": "Merim Dzaferagic, Neal McBride, Ryan Thomas, Irene Macaluso, and\n  Nicola Marchetti", "title": "Improving In-Network Computing in IoT Through Degeneracy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel way of considering in-network computing (INC), using ideas\nfrom statistical physics. We define degeneracy for INC as the multiplicity of\npossible options available within the network to perform the same function with\na given macroscopic property (e.g. delay). We present an efficient algorithm to\ndetermine all these alternatives. Our results show that by exploiting the set\nof possible degenerate alternatives, we can significantly improve the\nsuccessful computation rate of a symmetric function, while still being able to\nsatisfy requirements such as delay or energy consumption.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 13:11:03 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 14:05:30 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Dzaferagic", "Merim", ""], ["McBride", "Neal", ""], ["Thomas", "Ryan", ""], ["Macaluso", "Irene", ""], ["Marchetti", "Nicola", ""]]}, {"id": "1901.02745", "submitter": "Ghafour Ahani", "authors": "Ghafour Ahani, Di Yuan", "title": "BS-assisted Task Offloading for D2D Networks with Presence of User\n  Mobility", "comments": "accepted in the 2019 IEEE 89th Vehicular Technology Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Task offloading is a key component in mobile edge computing. Offloading a\ntask to a remote server takes communication and networking resources. An\nalternative is device-todevice (D2D) offloading, where a task of a device is\noffloaded to some device having computational resource available. The latter\nrequires that the devices are within the range of each other, first for task\ncollection, and later for result gathering. Hence, in mobility scenarios, the\nperformance of D2D offloading will suffer if the contact rates between the\ndevices are low. We enhance the setup to base station (BS) assisted D2D\noffloading, namely, a BS can act as a relay for task distribution or result\ncollection. However, this would imply additional consumption of wireless\nresource. The associated cost and the improvement in completion time of task\noffloading compose a fundamental trade-off. For the resulting optimization\nproblem, we mathematically prove the complexity, and propose an algorithm using\nLagrangian duality. The simulation results demonstrate not only that the\nalgorithm has close-to-optimal performance, but also provide structural\ninsights of the optimal trade-off.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 13:48:12 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Ahani", "Ghafour", ""], ["Yuan", "Di", ""]]}, {"id": "1901.02750", "submitter": "Ghafour Ahani", "authors": "Ghafour Ahani, Di Yuan", "title": "On Optimal Proactive and Retention-Aware Caching with User Mobility", "comments": "submitted to the 2018 IEEE 88th Vehicular Technology Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Caching popular contents at edge devices is an effective solution to\nalleviate the burden of the backhaul networks. Earlier investigations commonly\nneglected the storage cost in caching. More recently, retention-aware caching,\nwhere both the downloading cost and storage cost are accounted for, is\nattracting attention. Motivated by this, we address proactive and\nretention-aware caching with user mobility, optimizing the sum of the two types\nof costs. This is a combinatorial optimization problem. However, we derive a\nstream of analytical results and they together lead to an algorithm that\nguarantees global optimum with polynomial-time complexity. Numerical results\nshow significant improvements in comparison to popular caching and random\ncaching.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 14:05:38 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Ahani", "Ghafour", ""], ["Yuan", "Di", ""]]}, {"id": "1901.02755", "submitter": "Jiheng Zhang", "authors": "Jiahao He, Guangju Wang, Guangyuan Zhang, Jiheng Zhang", "title": "Consensus Mechanism Design based on Structured Directed Acyclic Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a structure for the directed acyclic graph (DAG) and a mechanism\ndesign based on that structure so that peers can reach consensus at large scale\nbased on proof of work (PoW). We also design a mempool transaction assignment\nmethod based on the DAG structure to render negligible the probability that a\ntransaction being processed by more than one miners. The result is a\nsignificant scale-up of the capacity without sacrificing security and\ndecentralization.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 14:15:20 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["He", "Jiahao", ""], ["Wang", "Guangju", ""], ["Zhang", "Guangyuan", ""], ["Zhang", "Jiheng", ""]]}, {"id": "1901.02773", "submitter": "Ahmed Eleliemy", "authors": "Ahmed Eleliemy and Florina M. Ciorba", "title": "Dynamic Loop Scheduling Using MPI Passive-Target Remote Memory Access", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific applications often contain large computationally-intensive\nparallel loops. Loop scheduling techniques aim to achieve load balanced\nexecutions of such applications. For distributed-memory systems, existing\ndynamic loop scheduling (DLS) libraries are typically MPI-based, and employ a\nmaster-worker execution model to assign variably-sized chunks of loop\niterations. The master-worker execution model may adversely impact performance\ndue to the master-level contention. This work proposes a distributed\nchunk-calculation approach that does not require the master-worker execution\nscheme. Moreover, it considers the novel features in the latest MPI standards,\nsuch as passive-target remote memory access, shared-memory window creation, and\natomic read-modify-write operations. To evaluate the proposed approach, five\nwell-known DLS techniques, two applications, and two heterogeneous hardware\nsetups have been considered. The DLS techniques implemented using the proposed\napproach outperformed their counterparts implemented using the traditional\nmaster-worker execution model.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 09:32:19 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Eleliemy", "Ahmed", ""], ["Ciorba", "Florina M.", ""]]}, {"id": "1901.02774", "submitter": "Akanksha Baranwal", "authors": "Akanksha Baranwal, Ishan Bansal, Roopal Nahar, K. Madhava Krishna", "title": "DeCoILFNet: Depth Concatenation and Inter-Layer Fusion based ConvNet\n  Accelerator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) are rapidly gaining popularity in varied\nfields. Due to their increasingly deep and computationally heavy structures, it\nis difficult to deploy them on energy constrained mobile applications. Hardware\naccelerators such as FPGAs have come up as an attractive alternative. However,\nwith the limited on-chip memory and computation resources of FPGA, meeting the\nhigh memory throughput requirement and exploiting the parallelism of CNNs is a\nmajor challenge. We propose a high-performance FPGA based architecture - Depth\nConcatenation and Inter-Layer Fusion based ConvNet Accelerator - DeCoILFNet\nwhich exploits the intra-layer parallelism of CNNs by flattening across depth\nand combines it with a highly pipelined data flow across the layers enabling\ninter-layer fusion. This architecture significantly reduces off-chip memory\naccesses and maximizes the throughput. Compared to a 3.5GHz hexa-core Intel\nXeon E7 caffe-implementation, our 120MHz FPGA accelerator is 30X faster. In\naddition, our design reduces external memory access by 11.5X along with a\nspeedup of more than 2X in the number of clock cycles compared to\nstate-of-the-art FPGA accelerators.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 12:53:09 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Baranwal", "Akanksha", ""], ["Bansal", "Ishan", ""], ["Nahar", "Roopal", ""], ["Krishna", "K. Madhava", ""]]}, {"id": "1901.02775", "submitter": "Jason Riedy", "authors": "Eric Hein, Srinivas Eswar, Abdurrahman Ya\\c{s}ar, Jiajia Li, Jeffrey\n  S. Young, Thomas M. Conte, \\\"Umit V. \\c{C}ataly\\\"urek, Rich Vuduc, Jason\n  Riedy, Bora U\\c{c}ar", "title": "Programming Strategies for Irregular Algorithms on the Emu Chick", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Emu Chick prototype implements migratory memory-side processing in a\nnovel hardware system. Rather than transferring large amounts of data across\nthe system interconnect, the Emu Chick moves lightweight thread contexts to\nnear-memory cores before the beginning of each remote memory read. Previous\nwork has characterized the performance of the Chick prototype in terms of\nmemory bandwidth and programming differences from more typical, non-migratory\nplatforms, but there has not yet been an analysis of algorithms on this system.\n  This work evaluates irregular algorithms that could benefit from the\nlightweight, memory-side processing of the Chick and demonstrates techniques\nand optimization strategies for achieving performance in sparse matrix-vector\nmultiply operation (SpMV), breadth-first search (BFS), and graph alignment\nacross up to eight distributed nodes encompassing 64 nodelets in the Chick\nsystem. We also define and justify relative metrics to compare prototype\nFPGA-based hardware with established ASIC architectures. The Chick currently\nsupports up to 68x scaling for graph alignment, 80 MTEPS for BFS on balanced\ngraphs, and 50\\% of measured STREAM bandwidth for SpMV.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 16:02:51 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Hein", "Eric", ""], ["Eswar", "Srinivas", ""], ["Ya\u015far", "Abdurrahman", ""], ["Li", "Jiajia", ""], ["Young", "Jeffrey S.", ""], ["Conte", "Thomas M.", ""], ["\u00c7ataly\u00fcrek", "\u00dcmit V.", ""], ["Vuduc", "Rich", ""], ["Riedy", "Jason", ""], ["U\u00e7ar", "Bora", ""]]}, {"id": "1901.03000", "submitter": "Jingzhao Wang", "authors": "Jingzhao Wang, Tinghan Wang, Yuan Luo, and Kenneth W. Shum", "title": "Capacity of Distributed Storage Systems with Clusters and Separate Nodes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In distributed storage systems (DSSs), the optimal tradeoff between node\nstorage and repair bandwidth is an important issue for designing distributed\ncoding strategies to ensure large scale data reliability. The capacity of DSSs\nis obtained as a function of node storage and repair bandwidth parameters,\ncharacterizing the tradeoff. There are lots of works on DSSs with clusters\n(racks) where the repair bandwidths from intra-cluster and cross-cluster are\ndifferentiated. However, separate nodes are also prevalent in the realistic\nDSSs, but the works on DSSs with clusters and separate nodes (CSN-DSSs) are\ninsufficient. In this paper, we formulate the capacity of CSN-DSSs with one\nseparate node for the first time where the bandwidth to repair a separate node\nis of cross-cluster. Consequently, the optimal tradeoff between node storage\nand repair bandwidth are derived and compared with cluster DSSs. A regenerating\ncode instance is constructed based on the tradeoff. Furthermore, the influence\nof adding a separate node is analyzed and formulated theoretically. We prove\nthat when each cluster contains R nodes and any k nodes suffice to recover the\noriginal file (MDS property), adding an extra separate node will keep the\ncapacity if R|k, and reduce the capacity otherwise.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 02:36:34 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Wang", "Jingzhao", ""], ["Wang", "Tinghan", ""], ["Luo", "Yuan", ""], ["Shum", "Kenneth W.", ""]]}, {"id": "1901.03054", "submitter": "Richard Hill Prof", "authors": "Phil Lane, Richard Hill", "title": "Towards Optimised Data Transport and Analytics for Edge Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Industrial organisations, particularly Small and Medium-sized Enterprises\n(SME), face a number of challenges with regard to the adoption of Industrial\nInternet of Things (IIoT) technologies and methods. The scope of analytics\nprocessing that can be performed on data from IIoT-enabled industrial processes\nis typically limited by the compute and storage resources that are available,\nand any investment in additional hardware that is sufficiently flexible and\nscalable is difficult to justify in terms of Return On Investment (ROI). We\ndescribe a distributed model of data transport and processing that eases the\ntake-up of IIoT, whilst also enabling a capability to securely deliver more\ncomplex analysis and future insight discovery, than would be possible with\ntraditional network architectures.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 08:37:02 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Lane", "Phil", ""], ["Hill", "Richard", ""]]}, {"id": "1901.03056", "submitter": "Richard Hill Prof", "authors": "Hussain Al-Aqrabi, Richard Hill", "title": "A Scalable Model for Secure Multiparty Authentication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed system architectures such as cloud computing or the emergent\narchitectures of the Internet Of Things, present significant challenges for\nsecurity and privacy. Specifically, in a complex application there is a need to\nsecurely delegate access control mechanisms to one or more parties, who in turn\ncan govern methods that enable multiple other parties to be authenticated in\nrelation to the services that they wish to consume. We identify shortcomings in\nan existing proposal by Xu et al for multiparty authentication and evaluate a\nnovel model from Al-Aqrabi et al that has been designed specifically for\ncomplex multiple security realm environments. The adoption of a Session\nAuthority Cloud ensures that resources for authentication requests are\nscalable, whilst permitting the necessary architectural abstraction for myriad\nhardware IoT devices such as actuators and sensor networks, etc. In addition,\nthe ability to ensure that session credentials are confirmed with the relevant\nresource principles means that the essential rigour for multiparty\nauthentication is established.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 08:44:10 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Al-Aqrabi", "Hussain", ""], ["Hill", "Richard", ""]]}, {"id": "1901.03086", "submitter": "Manuel Stein", "authors": "Manuel Stein", "title": "Adaptive Event Dispatching in Serverless Computing Infrastructures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Serverless computing is an emerging Cloud service model. It is currently\ngaining momentum as the next step in the evolution of hosted computing from\ncapacitated machine virtualisation and microservices towards utility computing.\nThe term \"serverless\" has become a synonym for the entirely\nresource-transparent deployment model of cloud-based event-driven distributed\napplications. This work investigates how adaptive event dispatching can improve\nserverless platform resource efficiency and contributes a novel approach that\nallows for better scaling and fitting of the platform's resource consumption to\nactual demand.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 10:35:21 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Stein", "Manuel", ""]]}, {"id": "1901.03161", "submitter": "Arda Aytekin", "authors": "Arda Aytekin and Mikael Johansson", "title": "Harnessing the Power of Serverless Runtimes for Large-Scale Optimization", "comments": "9 pages, double column, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The event-driven and elastic nature of serverless runtimes makes them a very\nefficient and cost-effective alternative for scaling up computations. So far,\nthey have mostly been used for stateless, data parallel and ephemeral\ncomputations. In this work, we propose using serverless runtimes to solve\ngeneric, large-scale optimization problems. Specifically, we build a\nmaster-worker setup using AWS Lambda as the source of our workers, implement a\nparallel optimization algorithm to solve a regularized logistic regression\nproblem, and show that relative speedups up to 256 workers and efficiencies\nabove 70% up to 64 workers can be expected. We also identify possible\nalgorithmic and system-level bottlenecks, propose improvements, and discuss the\nlimitations and challenges in realizing these improvements.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 13:57:06 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Aytekin", "Arda", ""], ["Johansson", "Mikael", ""]]}, {"id": "1901.03270", "submitter": "Luiz Fernando Bittencourt", "authors": "Luiz F. Bittencourt, Alfredo Goldman, Edmundo R. M. Madeira, Nelson L.\n  S. da Fonseca, Rizos Sakellariou", "title": "Scheduling in distributed systems: A cloud computing perspective", "comments": null, "journal-ref": "Computer Science Review, Volume 30, 2018, Pages 31-54", "doi": "10.1016/j.cosrev.2018.08.002", "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scheduling is essentially a decision-making process that enables resource\nsharing among a number of activities by determining their execution order on\nthe set of available resources. The emergence of distributed systems brought\nnew challenges on scheduling in computer systems, including clusters, grids,\nand more recently clouds. On the other hand, the plethora of research makes it\nhard for both newcomers researchers to understand the relationship among\ndifferent scheduling problems and strategies proposed in the literature, which\nhampers the identification of new and relevant research avenues. In this paper\nwe introduce a classification of the scheduling problem in distributed systems\nby presenting a taxonomy that incorporates recent developments, especially\nthose in cloud computing. We review the scheduling literature to corroborate\nthe taxonomy and analyze the interest in different branches of the proposed\ntaxonomy. Finally, we identify relevant future directions in scheduling for\ndistributed systems.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 16:56:53 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Bittencourt", "Luiz F.", ""], ["Goldman", "Alfredo", ""], ["Madeira", "Edmundo R. M.", ""], ["da Fonseca", "Nelson L. S.", ""], ["Sakellariou", "Rizos", ""]]}, {"id": "1901.03271", "submitter": "Kevin Sala", "authors": "Kevin Sala (1), Xavier Teruel (1), Josep M. Perez (1), Antonio J.\n  Pe\\~na (1), Vicen\\c{c} Beltran (1), Jesus Labarta (1) ((1) Barcelona\n  Supercomputing Center)", "title": "Integrating Blocking and Non-Blocking MPI Primitives with Task-Based\n  Programming Models", "comments": "European Commission's projects: INTERTWinE (EC-H2020-671602), Marie\n  Sk{\\l}odowska-Curie (EC-H2020-749516). Postprint submitted to the Parallel\n  Computing Journal (Elsevier). Figures from section 7.2 updated, typos\n  corrected", "journal-ref": "Parallel Computing, 85, 153-166 (2019)", "doi": "10.1016/j.parco.2018.12.008", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present the Task-Aware MPI library (TAMPI) that integrates\nboth blocking and non-blocking MPI primitives with task-based programming\nmodels. The TAMPI library leverages two new runtime APIs to improve both\nprogrammability and performance of hybrid applications. The first API allows to\npause and resume the execution of a task depending on external events. This API\nis used to improve the interoperability between blocking MPI communication\nprimitives and tasks. When an MPI operation executed inside a task blocks, the\ntask running is paused so that the runtime system can schedule a new task on\nthe core that became idle. Once the blocked MPI operation is completed, the\npaused task is put again on the runtime system's ready queue, so eventually it\nwill be scheduled again and its execution will be resumed.\n  The second API defers the release of dependencies associated with a task\ncompletion until some external events are fulfilled. This API is composed only\nof two functions, one to bind external events to a running task and another\nfunction to notify about the completion of external events previously bound.\nTAMPI leverages this API to bind non-blocking MPI operations with tasks,\ndeferring the release of their task dependencies until both task execution and\nall its bound MPI operations are completed.\n  Our experiments reveal that the enhanced features of TAMPI not only simplify\nthe development of hybrid MPI+OpenMP applications that use blocking or\nnon-blocking MPI primitives but they also naturally overlap computation and\ncommunication phases, which improves application performance and scalability by\nremoving artificial dependencies across communication tasks.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 17:02:26 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 13:03:53 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Sala", "Kevin", ""], ["Teruel", "Xavier", ""], ["Perez", "Josep M.", ""], ["Pe\u00f1a", "Antonio J.", ""], ["Beltran", "Vicen\u00e7", ""], ["Labarta", "Jesus", ""]]}, {"id": "1901.03279", "submitter": "Roy Friedman", "authors": "Yehonatan Buchnik and Roy Friedman", "title": "FireLedger: A High Throughput Blockchain Consensus Protocol", "comments": "The name of the protocol was changed from TOY to FireLedger. Protocol\n  presentation and related work sections were improved and some typos were\n  fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchains are distributed secure ledgers to which transactions are issued\ncontinuously and each block of transactions is tightly coupled to its\npredecessors. Permissioned blockchains place special emphasis on transactions\nthroughput. In this paper we present FireLedger, which leverages the iterative\nnature of blockchains in order to improve their throughput in optimistic\nexecution scenarios. FireLedger trades latency for throughput in the sense that\nin FireLedger the last f + 1 blocks of each node's blockchain are considered\ntentative, i.e., they may be rescinded in case one of the last f + 1 blocks\nproposers was Byzantine. Yet, when optimistic assumptions are met, a new block\nis decided in each communication step, which consists of a proposer that sends\nonly its proposal and all other participants are sending a single bit each. Our\nperformance study demonstrates that in a single Amazon data-center, FireLedger\nrunning on 10 mid-range Amazon nodes obtains a throughput of up to 160K\ntransactions per second for (typical Bitcoin size) 512 bytes transactions. In a\n10 nodes Amazon geo-distributed setting with 512 bytes transactions, FireLedger\nobtains a throughput of 30K tps. Moreover, on higher end Amazon machines,\nFireLedger obtains $20%-600%$ better throughput than state of the art protocols\nlike HotStuff and BFT-SMaRt, depending on the exact configuration.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 17:16:32 GMT"}, {"version": "v2", "created": "Fri, 11 Jan 2019 08:29:04 GMT"}, {"version": "v3", "created": "Thu, 16 May 2019 13:59:15 GMT"}, {"version": "v4", "created": "Tue, 28 May 2019 09:22:57 GMT"}, {"version": "v5", "created": "Thu, 3 Oct 2019 07:11:14 GMT"}, {"version": "v6", "created": "Fri, 1 Nov 2019 16:46:23 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Buchnik", "Yehonatan", ""], ["Friedman", "Roy", ""]]}, {"id": "1901.03401", "submitter": "Justin Meza", "authors": "Justin Meza", "title": "Large Scale Studies of Memory, Storage, and Network Failures in a Modern\n  Data Center", "comments": "PhD thesis, CMU (Dec 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The workloads running in the modern data centers of large scale Internet\nservice providers (such as Amazon, Baidu, Facebook, Google, and Microsoft)\nsupport billions of users and span globally distributed infrastructure. Yet,\nthe devices used in modern data centers fail due to a variety of causes, from\nfaulty components to bugs to misconfiguration. Faulty devices make operating\nlarge scale data centers challenging because the workloads running in modern\ndata centers consist of interdependent programs distributed across many\nservers, so failures that are isolated to a single device can still have a\nwidespread effect on a workload. In this dissertation, we measure and model the\ndevice failures in a large scale Internet service company, Facebook. We focus\non three device types that form the foundation of Internet service data center\ninfrastructure: DRAM for main memory, SSDs for persistent storage, and switches\nand backbone links for network connectivity. For each of these device types, we\nanalyze long term device failure data broken down by important device\nattributes and operating conditions, such as age, vendor, and workload. We also\nbuild and release statistical models to examine the failure trends for the\ndevices we analyze. Our key conclusion in this dissertation is that we can gain\na deep understanding of why devices fail---and how to predict their\nfailure---using measurement and modeling. We hope that the analysis,\ntechniques, and models we present in this dissertation will enable the\ncommunity to better measure, understand, and prepare for the hardware\nreliability challenges we face in the future.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jan 2019 01:43:38 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Meza", "Justin", ""]]}, {"id": "1901.03587", "submitter": "St\\'ephane Devismes", "authors": "St\\'ephane Devismes and Colette Johnen", "title": "Self-Stabilizing Distributed Cooperative Reset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-stabilization is a versatile fault-tolerance approach that characterizes\nthe ability of a system to eventually resume a correct behavior after any\nfinite number of transient faults. In this paper, we propose a self-stabilizing\nreset algorithm working in anonymous networks. This algorithm resets the\nnetwork in a distributed non-centralized manner, i.e., it is multi-initiator,\nas each process detecting an inconsistency may initiate a reset. It is also\ncooperative in the sense that it coordinates concurrent reset executions in\norder to gain efficiency. Our approach is general since our reset algorithm\nallows to build self-stabilizing solutions for various problems and settings.\nAs a matter of facts, we show that it applies to both static and dynamic\nspecifications since we propose efficient self-stabilizing reset-based\nalgorithms for the (1-minimal) $f,g)-alliance (a generalization of the\ndominating set problem) in identified networks and the unison problem in\nanonymous networks. Notice that these two latter instantiations enhance the\nstate of the art. Indeed, in the former case, our solution is more general than\nthe previous ones; while in the latter case, the complexity of the proposed\nunison algorithm is better than that of previous solutions of the literature.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 14:20:41 GMT"}, {"version": "v2", "created": "Mon, 14 Jan 2019 08:44:12 GMT"}, {"version": "v3", "created": "Thu, 17 Jan 2019 10:10:58 GMT"}, {"version": "v4", "created": "Fri, 19 Apr 2019 19:08:13 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Devismes", "St\u00e9phane", ""], ["Johnen", "Colette", ""]]}, {"id": "1901.03641", "submitter": "Mehmet Ilter", "authors": "Mehmet Cagri Ilter, Halim Yanikomeroglu", "title": "Convolutionally Coded SNR-Adaptive Transmission for Low-Latency\n  Communications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fifth generation new radio aims to facilitate new use cases in wireless\ncommunications. Some of these new use cases have highly de-manding latency\nrequirements; many of the powerful forward error correction codes deployed in\ncurrent systems, such as the turbo and low-density parity-check codes, do not\nperform well when the low-latency requirement does not allow iterative\ndecoding. As such, there is a rejuvenated interest in noniterative/one-shot\ndecoding algorithms. Motivated by this, we propose a signal-to-noise\nratio-adaptive convolutionally coded system with optimized constellations\ndesigned specifically for a particular set of convolutional code parameters.\nNumerical results show that significant performance improvements in terms of\nbit-error-rate and spectral efficiency can be obtained compared to the\ntraditional adaptive modulation and coding systems inlow-latency\ncommunications.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 13:40:58 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Ilter", "Mehmet Cagri", ""], ["Yanikomeroglu", "Halim", ""]]}, {"id": "1901.03744", "submitter": "Soheil Behnezhad", "authors": "Soheil Behnezhad, MohammadTaghi Hajiaghayi, David G. Harris", "title": "Exponentially Faster Massively Parallel Maximal Matching", "comments": "A preliminary version of this paper is to appear in the proceedings\n  of The 60th Annual IEEE Symposium on Foundations of Computer Science (FOCS\n  2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of approximate matching in the Massively Parallel Computations\n(MPC) model has recently seen a burst of breakthroughs. Despite this progress,\nhowever, we still have a far more limited understanding of maximal matching\nwhich is one of the central problems of parallel and distributed computing. All\nknown MPC algorithms for maximal matching either take polylogarithmic time\nwhich is considered inefficient, or require a strictly super-linear space of\n$n^{1+\\Omega(1)}$ per machine.\n  In this work, we close this gap by providing a novel analysis of an extremely\nsimple algorithm a variant of which was conjectured to work by Czumaj et al.\n[STOC'18]. The algorithm edge-samples the graph, randomly partitions the\nvertices, and finds a random greedy maximal matching within each partition. We\nshow that this algorithm drastically reduces the vertex degrees. This, among\nsome other results, leads to an $O(\\log \\log \\Delta)$ round algorithm for\nmaximal matching with $O(n)$ space (or even mildly sublinear in $n$ using\nstandard techniques).\n  As an immediate corollary, we get a $2$ approximate minimum vertex cover in\nessentially the same rounds and space. This is the best possible approximation\nfactor under standard assumptions, culminating a long line of research. It also\nleads to an improved $O(\\log\\log \\Delta)$ round algorithm for $1 + \\varepsilon$\napproximate matching. All these results can also be implemented in the\ncongested clique model within the same number of rounds.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 20:50:30 GMT"}, {"version": "v2", "created": "Sat, 13 Apr 2019 02:37:22 GMT"}, {"version": "v3", "created": "Wed, 16 Oct 2019 22:24:31 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Behnezhad", "Soheil", ""], ["Hajiaghayi", "MohammadTaghi", ""], ["Harris", "David G.", ""]]}, {"id": "1901.03761", "submitter": "Xin Li", "authors": "Xin Li, Yifan Dang, Tefang Chen", "title": "Vehicular Edge Cloud Computing: Depressurize the Intelligent Vehicles\n  Onboard Computational Power", "comments": "2018 IEEE 21st International Conference on Intelligent Transportation\n  Systems (ITSC)", "journal-ref": "21st International Conference on Intelligent Transportation\n  Systems, Maui, HI, USA, 2018, pp. 3421-3426", "doi": "10.1109/ITSC.2018.8569286", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, with the rapid development of autonomous vehicles and connected\nvehicles, the demands of vehicular computing keep continuously growing. We\nnotice a constant and limited onboard computational ability can hardly keep up\nwith the rising requirements of the vehicular system and software application\nduring their long-term lifetime, and also at the same time, the vehicles\nonboard computation causes an increasingly higher vehicular energy consumption.\nTherefore, we suppose to build a vehicular edge cloud computing (VECC)\nframework to resolve such a vehicular computing dilemma. In this framework,\npotential vehicular computing tasks can be executed remotely in an edge cloud\nwithin their time latency constraints. Simultaneously, an effective wireless\nnetwork resources allocation scheme is one of the essential and fundamental\nfactors for the QoS (quality of Service) on the VECC. In this paper, we adopted\na stochastic fair allocation (SFA) algorithm to randomly allocate minimum\nrequired resource blocks to admitted vehicular users. The numerical results\nshow great effectiveness of energy efficiency in VECC.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 22:12:36 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Li", "Xin", ""], ["Dang", "Yifan", ""], ["Chen", "Tefang", ""]]}, {"id": "1901.03772", "submitter": "Roberto Palmieri", "authors": "Masoomeh Javidi Kishi, Sebastiano Peluso, Hank Korth, Roberto Palmieri", "title": "SSS: Scalable Key-Value Store with External Consistent and Abort-free\n  Read-only Transactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SSS, a scalable transactional key-value store deploying a novel\ndistributed concurrency control that provides external consistency for all\ntransactions, never aborts read-only transactions due to concurrency, all\nwithout specialized hardware. SSS ensures the above properties without any\ncentralized source of synchronization. SSS's concurrency control uses a\ncombination of vector clocks and a new technique, called snapshot-queuing, to\nestablish a single transaction serialization order that matches the order of\ntransaction completion observed by clients. We compare SSS against high\nperformance key-value stores, Walter, ROCOCO, and a two-phase commit baseline.\nSSS outperforms 2PC-baseline by as much as 7x using 20 nodes; and ROCOCO by as\nmuch as 2.2x with long read-only transactions using 15 nodes.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 23:46:05 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Kishi", "Masoomeh Javidi", ""], ["Peluso", "Sebastiano", ""], ["Korth", "Hank", ""], ["Palmieri", "Roberto", ""]]}, {"id": "1901.03804", "submitter": "Muhammad Samir Khan", "authors": "Muhammad Samir Khan, Nitin H. Vaidya", "title": "Byzantine Consensus under Local Broadcast Model: Tight Sufficient\n  Condition", "comments": "Some minor typos in arxiv abstract and the report. Also added an NSF\n  grant", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we consider Byzantine Consensus on undirected communication\ngraphs under the local broadcast model. In the classical point-to-point\ncommunication model the messages exchanged between two nodes $u, v$ on an edge\n$uv$ of $G$ are private. This allows a faulty node to send conflicting\ninformation to its different neighbours, a property called equivocation. In\ncontrast, in the local broadcast communication model considered here, a message\nsent by node $u$ is received identically by all of its neighbours. This\nrestriction to broadcast messages provides non-equivocation even for faulty\nnodes. In prior results [10, 11] it was shown that in the local broadcast model\nthe communication graph must be $(\\lfloor 3f/2 \\rfloor +1)$-connected and have\ndegree at least $2f$ to achieve Byzantine Consensus. In this work we show that\nthis network condition is tight.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jan 2019 05:53:08 GMT"}, {"version": "v2", "created": "Tue, 15 Jan 2019 16:49:00 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Khan", "Muhammad Samir", ""], ["Vaidya", "Nitin H.", ""]]}, {"id": "1901.03984", "submitter": "Fabio Petrillo", "authors": "Samuel Lavoie, Anthony Garant, Fabio Petrillo", "title": "Serverless architecture efficiency: an exploratory study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud service provider propose services to insensitive customers to use their\nplatform. Different services can achieve the same result at different cost. In\nthis paper, we study the efficiency of a serverless architecture for running\nhighly parallelizable tasks to compare theses services in order to find the\nmost efficient in term of performance and cost. More precisely, we look at the\ncompute time and at the cost per task for a given task. The tasks studied is\nthe count of the occurrence of a given word in a corpus. We compare the\nserverless architecture to the Apache Spark map reduce technique commonly used\nfor this type of task. Using AWS Lambda for the serverless architecture and\nAmazon EMR for the Apache Spark map reduce, with similar compute power, we show\nthat the serverless technique achieve comparable performance in term of compute\ntime and cost. We observed that the lambda function is a great approach for\nreal time computing, while EMR is preferable for task that require long compute\ntime.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jan 2019 13:54:52 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Lavoie", "Samuel", ""], ["Garant", "Anthony", ""], ["Petrillo", "Fabio", ""]]}, {"id": "1901.04008", "submitter": "Ami Paz", "authors": "Keren Censor-Hillel, Neta Dafni, Victor I. Kolobov, Ami Paz, Gregory\n  Schwartzman", "title": "Fast Deterministic Algorithms for Highly-Dynamic Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides an algorithmic framework for obtaining fast distributed\nalgorithms for a highly-dynamic setting, in which *arbitrarily many* edge\nchanges may occur in each round. Our algorithm significantly improves upon\nprior work in its combination of (1) having an $O(1)$ amortized time\ncomplexity, (2) using only $O(\\log{n})$-bit messages, (3) not posing any\nrestrictions on the dynamic behavior of the environment, (4) being\ndeterministic, (5) having strong guarantees for intermediate solutions, and (6)\nbeing applicable for a wide family of tasks.\n  The tasks for which we deduce such an algorithm are maximal matching,\n$(degree+1)$-coloring, 2-approximation for minimum weight vertex cover, and\nmaximal independent set (which is the most subtle case). For some of these\ntasks, node insertions can also be among the allowed topology changes, and for\nsome of them also abrupt node deletions.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jan 2019 16:11:22 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2019 20:09:12 GMT"}, {"version": "v3", "created": "Sat, 2 Nov 2019 16:16:11 GMT"}, {"version": "v4", "created": "Sun, 23 Feb 2020 21:46:40 GMT"}, {"version": "v5", "created": "Sun, 11 Oct 2020 14:00:04 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Censor-Hillel", "Keren", ""], ["Dafni", "Neta", ""], ["Kolobov", "Victor I.", ""], ["Paz", "Ami", ""], ["Schwartzman", "Gregory", ""]]}, {"id": "1901.04146", "submitter": "Vincent Knapps", "authors": "Vincent Knapps and Karl-Heinz Zimmermann", "title": "Distributed Monitoring of Topological Events via Homology", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topological event detection allows for the distributed computation of\nhomology by focusing on local changes occurring in a network over time. In this\npaper, a model for the monitoring of topological events in dynamically changing\nregions will be developed. Regions are approximated as the connected components\nof the communication graph of a sensor network, reducing homology computation\nto graph homology. Betti number differences together with cyclic neighbor-rings\nare used to categorize topological event types. The focus lies on the correct\ndetection of non-incremental (i.e., multiple concurrently occurring) events and\nthe necessary region update process. Network number differences between a\nnetwork's state before and after events are spread from event nodes into\nnetwork regions, allowing for the conflict-free updating of regions independent\nof the update messages' order of arrival.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 06:35:34 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Knapps", "Vincent", ""], ["Zimmermann", "Karl-Heinz", ""]]}, {"id": "1901.04248", "submitter": "Ovidiu Vaduvescu", "authors": "V. Bacu, A. Sabou, T. Stefanut, D. Gorgan and O. Vaduvescu", "title": "NEARBY Platform for Detecting Asteroids in Astronomical Images Using\n  Cloud-based Containerized Applications", "comments": "IEEE 14th International Conference on Intelligent Computer\n  Communication and Processing (ICCP), Cluj-Napoca, Romania", "journal-ref": "IEEE, 2018", "doi": "10.1109/ICCP.2018.8516578", "report-no": null, "categories": "astro-ph.IM cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The continuing monitoring and surveying of the nearby space to detect Near\nEarth Objects (NEOs) and Near Earth Asteroids (NEAs) are essential because of\nthe threats that this kind of objects impose on the future of our planet. We\nneed more computational resources and advanced algorithms to deal with the\nexponential growth of the digital cameras' performances and to be able to\nprocess (in near real-time) data coming from large surveys. This paper presents\na software platform called NEARBY that supports automated detection of moving\nsources (asteroids) among stars from astronomical images. The detection\nprocedure is based on the classic \"blink\" detection and, after that, the system\nsupports visual analysis techniques to validate the moving sources, assisted by\nstatic and dynamical presentations.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 11:49:06 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Bacu", "V.", ""], ["Sabou", "A.", ""], ["Stefanut", "T.", ""], ["Gorgan", "D.", ""], ["Vaduvescu", "O.", ""]]}, {"id": "1901.04319", "submitter": "Nane Kratzke", "authors": "Kennedy A. Torkura and Christoph Meinel and Nane Kratzke", "title": "Don't Wait to be Breached! Creating Asymmetric Uncertainty of Cloud\n  Applications via Moving Target Defenses", "comments": "Invited paper. arXiv admin note: substantial text overlap with\n  arXiv:1802.03565", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.NI cs.SY", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Cloud applications expose - besides service endpoints - also potential or\nactual vulnerabilities. Therefore, cloud security engineering efforts focus on\nhardening the fortress walls but seldom assume that attacks may be successful.\nAt least against zero-day exploits, this approach is often toothless. Other\nthan most security approaches and comparable to biological systems we accept\nthat defensive \"walls\" can be breached at several layers. Instead of hardening\nthe \"fortress\" walls we propose to make use of an (additional) active and\nadaptive defense system to attack potential intruders - an immune system that\nis inspired by the concept of a moving target defense. This \"immune system\"\nworks on two layers. On the infrastructure layer, virtual machines are\ncontinuously regenerated (cell regeneration) to wipe out even undetected\nintruders. On the application level, the vertical and horizontal attack surface\nis continuously modified to circumvent successful replays of formerly scripted\nattacks. Our evaluations with two common cloud-native reference applications in\npopular cloud service infrastructures (Amazon Web Services, Google Compute\nEngine, Azure and OpenStack) show that it is technically possible to limit the\ntime of attackers acting undetected down to minutes. Further, more than 98% of\nan attack surface can be changed automatically and minimized which makes it\nhard for intruders to replay formerly successful scripted attacks. So, even if\nintruders get a foothold in the system, it is hard for them to maintain it.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 18:28:38 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Torkura", "Kennedy A.", ""], ["Meinel", "Christoph", ""], ["Kratzke", "Nane", ""]]}, {"id": "1901.04359", "submitter": "Shaohuai Shi", "authors": "Shaohuai Shi, Qiang Wang, Kaiyong Zhao, Zhenheng Tang, Yuxin Wang,\n  Xiang Huang, Xiaowen Chu", "title": "A Distributed Synchronous SGD Algorithm with Global Top-$k$\n  Sparsification for Low Bandwidth Networks", "comments": "10 pages. Add discussion with more experimental results. To appear at\n  the ICDCS2019 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed synchronous stochastic gradient descent (S-SGD) has been widely\nused in training large-scale deep neural networks (DNNs), but it typically\nrequires very high communication bandwidth between computational workers (e.g.,\nGPUs) to exchange gradients iteratively. Recently, Top-$k$ sparsification\ntechniques have been proposed to reduce the volume of data to be exchanged\namong workers. Top-$k$ sparsification can zero-out a significant portion of\ngradients without impacting the model convergence. However, the sparse\ngradients should be transferred with their irregular indices, which makes the\nsparse gradients aggregation difficult. Current methods that use AllGather to\naccumulate the sparse gradients have a communication complexity of $O(kP)$,\nwhere $P$ is the number of workers, which is inefficient on low bandwidth\nnetworks with a large number of workers. We observe that not all top-$k$\ngradients from $P$ workers are needed for the model update, and therefore we\npropose a novel global Top-$k$ (gTop-$k$) sparsification mechanism to address\nthe problem. Specifically, we choose global top-$k$ largest absolute values of\ngradients from $P$ workers, instead of accumulating all local top-$k$ gradients\nto update the model in each iteration. The gradient aggregation method based on\ngTop-$k$ sparsification reduces the communication complexity from $O(kP)$ to\n$O(k\\log P)$. Through extensive experiments on different DNNs, we verify that\ngTop-$k$ S-SGD has nearly consistent convergence performance with S-SGD, and it\nhas only slight degradations on generalization performance. In terms of scaling\nefficiency, we evaluate gTop-$k$ on a cluster with 32 GPU machines which are\ninterconnected with 1 Gbps Ethernet. The experimental results show that our\nmethod achieves $2.7-12\\times$ higher scaling efficiency than S-SGD and\n$1.1-1.7\\times$ improvement than the existing Top-$k$ S-SGD.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 15:14:04 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 04:40:22 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Shi", "Shaohuai", ""], ["Wang", "Qiang", ""], ["Zhao", "Kaiyong", ""], ["Tang", "Zhenheng", ""], ["Wang", "Yuxin", ""], ["Huang", "Xiang", ""], ["Chu", "Xiaowen", ""]]}, {"id": "1901.04452", "submitter": "Nicholas Schiefer", "authors": "Christos Chrysafis, Ben Collins, Scott Dugas, Jay Dunkelberger, Moussa\n  Ehsan, Scott Gray, Alec Grieser, Ori Herrnstadt, Kfir Lev-Ari, Tao Lin, Mike\n  McMahon, Nicholas Schiefer, Alexander Shraer", "title": "FoundationDB Record Layer: A Multi-Tenant Structured Datastore", "comments": "16 pages; updated to reflect reviewer suggestions and fix typos", "journal-ref": null, "doi": "10.1145/3299869.3314039", "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The FoundationDB Record Layer is an open source library that provides a\nrecord-oriented data store with semantics similar to a relational database\nimplemented on top of FoundationDB, an ordered, transactional key-value store.\nThe Record Layer provides a lightweight, highly extensible way to store\nstructured data. It offers schema management and a rich set of query and\nindexing facilities, some of which are not usually found in traditional\nrelational databases, such as nested record types, indexes on commit versions,\nand indexes that span multiple record types. The Record Layer is stateless and\nbuilt for massive multi-tenancy, encapsulating and isolating all of a tenant's\nstate, including indexes, into a separate logical database. We demonstrate how\nthe Record Layer is used by CloudKit, Apple's cloud backend service, to provide\npowerful abstractions to applications serving hundreds of millions of users.\nCloudKit uses the Record Layer to host billions of independent databases, many\nwith a common schema. Features provided by the Record Layer enable CloudKit to\nprovide richer APIs and stronger semantics with reduced maintenance overhead\nand improved scalability.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 18:36:25 GMT"}, {"version": "v2", "created": "Fri, 29 Mar 2019 23:52:51 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Chrysafis", "Christos", ""], ["Collins", "Ben", ""], ["Dugas", "Scott", ""], ["Dunkelberger", "Jay", ""], ["Ehsan", "Moussa", ""], ["Gray", "Scott", ""], ["Grieser", "Alec", ""], ["Herrnstadt", "Ori", ""], ["Lev-Ari", "Kfir", ""], ["Lin", "Tao", ""], ["McMahon", "Mike", ""], ["Schiefer", "Nicholas", ""], ["Shraer", "Alexander", ""]]}, {"id": "1901.04620", "submitter": "Jianyu Niu", "authors": "Jianyu Niu and Chen Feng", "title": "Selfish Mining in Ethereum", "comments": "This paper is accepted by 2019 IEEE 39th International Conference on\n  Distributed Computing Systems (ICDCS)", "journal-ref": "IEEE 39th International Conference on Distributed Computing\n  Systems (ICDCS) 2019", "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the second largest cryptocurrency by market capitalization and today's\nbiggest decentralized platform that runs smart contracts, Ethereum has received\nmuch attention from both industry and academia. Nevertheless, there exist very\nfew studies about the security of its mining strategies, especially from the\nselfish mining perspective. In this paper, we aim to fill this research gap by\nanalyzing selfish mining in Ethereum and understanding its potential threat.\nFirst, we introduce a 2-dimensional Markov process to model the behavior of a\nselfish mining strategy inspired by a Bitcoin mining strategy proposed by Eyal\nand Sirer. Second, we derive the stationary distribution of our Markov model\nand compute long-term average mining rewards. This allows us to determine the\nthreshold of computational power that makes selfish mining profitable in\nEthereum. We find that this threshold is lower than that in Bitcoin mining\n(which is 25% as discovered by Eyal and Sirer), suggesting that Ethereum is\nmore vulnerable to selfish mining than Bitcoin.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 00:32:18 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2019 18:04:06 GMT"}, {"version": "v3", "created": "Sun, 21 Apr 2019 21:47:54 GMT"}, {"version": "v4", "created": "Sun, 11 Jul 2021 02:13:36 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Niu", "Jianyu", ""], ["Feng", "Chen", ""]]}, {"id": "1901.04629", "submitter": "Shusen Liu", "authors": "Kan He, Shusen Liu, Jinchuan Hou", "title": "Separation and approximate separation of multipartite quantum gates", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of qubits of current quantum computers is one of the most\ndominating restrictions for applications. So it is naturally conceived to use\ntwo or more small capacity quantum computers to form a larger capacity quantum\ncomputing system by quantum parallel programming. To design the parallel\nprogram for quantum computers, the primary obstacle is to decompose quantum\ngates in the whole circuit to the tensor product of local gates. In the paper,\nwe first devote to analyzing theoretically separability conditions of\nmultipartite quantum gates on finite or infinite dimensional systems.\nFurthermore, we perform the separation experiments for $n$-qubit quantum gates\non the IBM's quantum computers by the software Q$|SI\\rangle$. Not surprisedly,\nit is showed that there exist few separable ones among multipartite quantum\ngates. Therefore, we pay our attention to the approximate separation problems\nof multipartite gates, i.e., how a multipartite gate can be closed to separable\nones.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 01:35:20 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["He", "Kan", ""], ["Liu", "Shusen", ""], ["Hou", "Jinchuan", ""]]}, {"id": "1901.04709", "submitter": "Krzysztof R. Apt", "authors": "Krzysztof R. Apt, Ehsan Shoja", "title": "Self-Stabilization Through the Lens of Game Theory", "comments": "17 pages", "journal-ref": "in: It's All About Coordination, Lecture Notes in Computer Science\n  10865, pp. 21-37, Springer, 2018", "doi": null, "report-no": null, "categories": "cs.GT cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 1974 E.W. Dijkstra introduced the seminal concept of self-stabilization\nthat turned out to be one of the main approaches to fault-tolerant computing.\nWe show here how his three solutions can be formalized and reasoned about using\nthe concepts of game theory. We also determine the precise number of steps\nneeded to reach self-stabilization in his first solution.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 08:42:32 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Apt", "Krzysztof R.", ""], ["Shoja", "Ehsan", ""]]}, {"id": "1901.04797", "submitter": "David Castells-Rufas", "authors": "David Castells-Rufas, C\\'edric Bastoul", "title": "Proceedings of the Workshop on High Performance Energy Efficient\n  Embedded Systems (HIP3ES) 2019", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Proceedings of the Workshop on High Performance Energy Efficient Embedded\nSystems (HIP3ES) 2019. Valencia, Spain, January 22nd. Collocated with HIPEAC\n2019 Conference.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 12:49:52 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Castells-Rufas", "David", ""], ["Bastoul", "C\u00e9dric", ""]]}, {"id": "1901.04830", "submitter": "Mozhdeh Farhadi", "authors": "Mozhdeh Farhadi, Daniele Miorandi, Guillaume Pierre", "title": "Blockchain enabled fog structure to provide data security in IoT\n  applications", "comments": "Proceedings of Middleware 18th, Rennes, France, December 2018, 2\n  pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  IoT provides services by connecting smart devices to the Internet, and\nexploiting data generated by said devices to enable value-added services to\nindividuals and businesses. In such cases, if data is exposed, tampered or\nlost, the service would not behave correctly. In this article, we discuss data\nsecurity in IoT applications across five dimensions: confidentiality,\nintegrity, authenticity, non-repudiation and availability. We discuss how\ndistributed ledger technology could be used to overcome these issues and\npropose to use a fog computing architecture as decentralized computational\nsupport to deploy the ledger.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 14:09:24 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Farhadi", "Mozhdeh", ""], ["Miorandi", "Daniele", ""], ["Pierre", "Guillaume", ""]]}, {"id": "1901.04969", "submitter": "Thiam Khean Hah", "authors": "Thiam Khean Hah, Yeong Tat Liew, Jason Ong", "title": "Low Precision Constant Parameter CNN on FPGA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report FPGA implementation results of low precision CNN convolution layers\noptimized for sparse and constant parameters. We describe techniques that\namortizes the cost of common factor multiplication and automatically leverage\ndense hand tuned LUT structures. We apply this method to corner case residual\nblocks of Resnet on a sparse Resnet50 model to assess achievable utilization\nand frequency and demonstrate an effective performance of 131 and 23 TOP/chip\nfor the corner case blocks. The projected performance on a multichip persistent\nimplementation of all Resnet50 convolution layers is 10k im/s/chip at batch\nsize 2. This is 1.37x higher than V100 GPU upper bound at the same batch size\nafter normalizing for sparsity.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 23:40:35 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Hah", "Thiam Khean", ""], ["Liew", "Yeong Tat", ""], ["Ong", "Jason", ""]]}, {"id": "1901.04976", "submitter": "Leonid Yavits PhD", "authors": "Leonid Yavits, Roman Kaplan and Ran Ginosar", "title": "AIDA: Associative DNN Inference Accelerator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose AIDA, an inference engine for accelerating fully-connected (FC)\nlayers of Deep Neural Network (DNN). AIDA is an associative in-memory\nprocessor, where the bulk of data never leaves the confines of the memory\narrays, and processing is performed in-situ. AIDA area and energy efficiency\nstrongly benefit from sparsity and lower arithmetic precision. We show that\nAIDA outperforms the state of art inference accelerator, EIE, by 14.5x (peak\nperformance) and 2.5x (throughput).\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 09:04:31 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Yavits", "Leonid", ""], ["Kaplan", "Roman", ""], ["Ginosar", "Ran", ""]]}, {"id": "1901.04977", "submitter": "Michael Hopfengaertner", "authors": "Michael Hopfengaertner", "title": "An open-source sensor platform for analysis of group dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The collaboration of several people in groups is becoming more and more\nimportant nowadays. Teamwork is often used for decision-making processes and\nfor solving complex problems. Research in this area focuses on the\nquantification and analysis of behavior within and between groups. By using\nwearable electronic devices, such as badges, this quantification can be\nperformed automatically and with high accuracy. The goal of this work is the\ndesign and implementation of a new firmware for the badges of the Rhythm\nproject - an open-source project of the Human Dynamics Group of the MIT Media\nLab. The firmware is characterized by a modular and extensible architecture and\ncombines different techniques. These include a filesystem, which efficiently\nstores sequentially generated data based on a virtual memory abstraction, a\nserialization library, which enables a platform-independent exchange of\nstructured data, and a time synchronization technique, which compensates\nfrequency deviations of the oscillator. In addition, an automated test\nenvironment was designed for the verification of individual functional software\ncomponents of the application. As a result of detailed analysis and special\nmeasures, the power consumption could be significantly reduced compared to the\nprevious implementation. Due to the hierarchical and modular structure and the\nhigh degree of abstraction, the developed techniques can also be integrated\ninto other projects and platforms.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 00:02:37 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Hopfengaertner", "Michael", ""]]}, {"id": "1901.04978", "submitter": "Shaoshan Liu", "authors": "Jie Tang, Shaoshan Liu, Bo Yu, Weisong Shi", "title": "PI-Edge: A Low-Power Edge Computing System for Real-Time Autonomous\n  Driving Services", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To simultaneously enable multiple autonomous driving services on affordable\nembedded systems, we designed and implemented {\\pi}-Edge, a complete edge\ncomputing framework for autonomous robots and vehicles. The contributions of\nthis paper are three-folds: first, we developed a runtime layer to fully\nutilize the heterogeneous computing resources of low-power edge computing\nsystems; second, we developed an extremely lightweight operating system to\nmanage multiple autonomous driving services and their communications; third, we\ndeveloped an edge-cloud coordinator to dynamically offload tasks to the cloud\nto optimize client system energy consumption. To the best of our knowledge,\nthis is the first complete edge computing system of a production autonomous\nvehicle. In addition, we successfully implemented {\\pi}-Edge on a Nvidia Jetson\nand demonstrated that we could successfully support multiple autonomous driving\nservices with only 11 W of power consumption, and hence proving the\neffectiveness of the proposed {\\pi}-Edge system.\n", "versions": [{"version": "v1", "created": "Sat, 29 Dec 2018 09:29:24 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Tang", "Jie", ""], ["Liu", "Shaoshan", ""], ["Yu", "Bo", ""], ["Shi", "Weisong", ""]]}, {"id": "1901.04982", "submitter": "Mathias Gottschlag", "authors": "Mathias Gottschlag, Frank Bellosa", "title": "Mechanism to Mitigate AVX-Induced Frequency Reduction", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern Intel CPUs reduce their frequency when executing wide vector\noperations (AVX2 and AVX-512 instructions), as these instructions increase\npower consumption. The frequency is only increased again two milliseconds after\nthe last code section containing such instructions has been executed in order\nto prevent excessive numbers of frequency changes. Due to this delay,\nintermittent use of wide vector operations can slow down the rest of the system\nsignificantly. For example, previous work has shown the performance of web\nservers to be reduced by up to 10% if the SSL library uses AVX-512 vector\ninstructions. These performance variations are hard to predict during software\ndevelopment as the performance impact of vectorization depends on the specific\nworkload.\n  We describe a mechanism to reduce the slowdown caused by wide vector\ninstructions without requiring extensive changes to existing software. Our\ndesign allows the developer to mark problematic AVX code regions. The scheduler\nthen restricts execution of this code to a subset of the cores so that only\nthese cores' frequency is affected. Threads are automatically migrated to a\nsuitable core whenever necessary. We identify a suitable load balancing policy\nto ensure good utilization of all available cores. Our approach is able to\nreduce the performance variability caused by AVX2 and AVX-512 instructions by\nover 70%.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 11:57:48 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Gottschlag", "Mathias", ""], ["Bellosa", "Frank", ""]]}, {"id": "1901.04983", "submitter": "Gheorghe Stefanescu", "authors": "Ciprian Ionut Paduraru, Gheorghe Stefanescu", "title": "Adaptive virtual organisms: A compositional model for complex\n  hardware-software binding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The relation between a structure and the function running on that structure\nis of central interest in many fields, including computer science, biology\n(organ vs. function), psychology (body vs. mind), architecture (designs vs.\nfunctionality), etc. Our paper addresses this question with reference to\ncomputer science recent hardware and software advances, particularly in areas\nas robotics, AI-hardware, self-adaptive systems, IoT, CPS, etc. At the\nmodelling, conceptual level, our main contribution is the introduction of the\nconcept of \"virtual organism\" (VO), to populate the intermediary level between\nrigid, slightly reconfigurable, hardware agents and abstract, intelligent,\nadaptive software agents. A virtual organism has a structure, resembling the\nhardware capabilities, and it runs low-level functions, implementing the\nsoftware requirements. The model is compositional in space (allowing the\nvirtual organisms to aggregate into larger organisms) and in time (allowing the\nvirtual organisms to get composed functionalities). Technically, the virtual\norganisms studied here are in 2D and their structures are described by regular\n2D pattens; adding the time dimension, we conclude the VO model is in 3D. By\nreconfiguration, an organism may change its structure to another structure\nbelonging to the same 2D pattern. We illustrate the VO concept with three\nincreasingly more complex VOs: (1) a tree collector; (2) a feeding cell; and\n(3) a collection of connected feeding cells. We implemented a simulator for\ntree collector organisms and the quantitative results show reconfigurable\nstructures are better suited than fixed structures in dynamically changing\nenvironments. We briefly show how Agapia - a structured parallel, interactive\nprogramming language where dataflow and control flow structures can be freely\nmixed - may be used for getting quick implementations for VO's simulation.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 06:23:13 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Paduraru", "Ciprian Ionut", ""], ["Stefanescu", "Gheorghe", ""]]}, {"id": "1901.04985", "submitter": "MyungJoo Ham", "authors": "MyungJoo Ham and Ji Joong Moon and Geunsik Lim and Wook Song and\n  Jaeyun Jung and Hyoungjoo Ahn and Sangjung Woo and Youngchul Cho and Jinhyuck\n  Park and Sewon Oh and Hong-Seok Kim", "title": "NNStreamer: Stream Processing Paradigm for Neural Networks, Toward\n  Efficient Development and Execution of On-Device AI Applications", "comments": "This is a technical report from Samsung Electronics, which will be\n  published via conference proceedings (ACM/IEEE/USENIX, to be determined)\n  later", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose nnstreamer, a software system that handles neural networks as\nfilters of stream pipelines, applying the stream processing paradigm to neural\nnetwork applications. A new trend with the wide-spread of deep neural network\napplications is on-device AI; i.e., processing neural networks directly on\nmobile devices or edge/IoT devices instead of cloud servers. Emerging privacy\nissues, data transmission costs, and operational costs signifies the need for\non-device AI especially when a huge number of devices with real-time data\nprocessing are deployed. Nnstreamer efficiently handles neural networks with\ncomplex data stream pipelines on devices, improving the overall performance\nsignificantly with minimal efforts. Besides, nnstreamer simplifies the neural\nnetwork pipeline implementations and allows reusing off-shelf multimedia stream\nfilters directly; thus it reduces the developmental costs significantly.\nNnstreamer is already being deployed with a product releasing soon and is open\nsource software applicable to a wide range of hardware architectures and\nsoftware platforms.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jan 2019 06:00:05 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Ham", "MyungJoo", ""], ["Moon", "Ji Joong", ""], ["Lim", "Geunsik", ""], ["Song", "Wook", ""], ["Jung", "Jaeyun", ""], ["Ahn", "Hyoungjoo", ""], ["Woo", "Sangjung", ""], ["Cho", "Youngchul", ""], ["Park", "Jinhyuck", ""], ["Oh", "Sewon", ""], ["Kim", "Hong-Seok", ""]]}, {"id": "1901.04986", "submitter": "Hazoor Ahmad", "authors": "Hazoor Ahmad, Muhammad Tanvir, Muhammad Abdullah Hanif, Muhammad Usama\n  Javed, Rehan Hafiz, Muhammad Shafique", "title": "Systimator: A Design Space Exploration Methodology for Systolic Array\n  based CNNs Acceleration on the FPGA-based Edge Nodes", "comments": "5 Pages, 3 Figures, work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The evolution of IoT based smart applications demand porting of artificial\nintelligence algorithms to the edge computing devices. CNNs form a large part\nof these AI algorithms. Systolic array based CNN acceleration is being widely\nadvocated due its ability to allow scalable architectures. However, CNNs are\ninherently memory and compute intensive algorithms, and hence pose significant\nchallenges to be implemented on the resource-constrained edge computing\ndevices. Memory-constrained low-cost FPGA based devices form a substantial\nfraction of these edge computing devices. Thus, when porting to such\nedge-computing devices, the designer is left unguided as to how to select a\nsuitable systolic array configuration that could fit in the available hardware\nresources. In this paper we propose Systimator, a design space exploration\nbased methodology that provides a set of design points that can be mapped\nwithin the memory bounds of the target FPGA device. The methodology is based\nupon an analytical model that is formulated to estimate the required resources\nfor systolic arrays, assuming multiple data reuse patterns. The methodology\nfurther provides the performance estimates for each of the candidate design\npoints. We show that Systimator provides an in-depth analysis of\nresource-requirement of systolic array based CNNs. We provide our resource\nestimation results for porting of convolutional layers of TINY YOLO, a CNN\nbased object detector, on a Xilinx ARTIX 7 FPGA.\n", "versions": [{"version": "v1", "created": "Sat, 15 Dec 2018 12:05:03 GMT"}, {"version": "v2", "created": "Fri, 8 Feb 2019 07:18:07 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Ahmad", "Hazoor", ""], ["Tanvir", "Muhammad", ""], ["Hanif", "Muhammad Abdullah", ""], ["Javed", "Muhammad Usama", ""], ["Hafiz", "Rehan", ""], ["Shafique", "Muhammad", ""]]}, {"id": "1901.04987", "submitter": "Hyeran Jeon", "authors": "Aajna Karki, Chethan Palangotu Keshava, Spoorthi Mysore Shivakumar,\n  Joshua Skow, Goutam Madhukeshwar Hegde, Hyeran Jeon", "title": "Tango: A Deep Neural Network Benchmark Suite for Various Accelerators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have been proving the effectiveness in various\ncomputing fields. To provide more efficient computing platforms for DNN\napplications, it is essential to have evaluation environments that include\nassorted benchmark workloads. Though a few DNN benchmark suites have been\nrecently released, most of them require to install proprietary DNN libraries or\nresource-intensive DNN frameworks, which are hard to run on resource-limited\nmobile platforms or architecture simulators. To provide a more scalable\nevaluation environment, we propose a new DNN benchmark suite that can run on\nany platform that supports CUDA and OpenCL. The proposed benchmark suite\nincludes the most widely used five convolution neural networks and two\nrecurrent neural networks. We provide in-depth architectural statistics of\nthese networks while running them on an architecture simulator, a server- and a\nmobile-GPU, and a mobile FPGA.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 09:53:19 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Karki", "Aajna", ""], ["Keshava", "Chethan Palangotu", ""], ["Shivakumar", "Spoorthi Mysore", ""], ["Skow", "Joshua", ""], ["Hegde", "Goutam Madhukeshwar", ""], ["Jeon", "Hyeran", ""]]}, {"id": "1901.04988", "submitter": "Teng Wang", "authors": "Teng Wang, Chao Wang, Xuehai Zhou, Huaping Chen", "title": "A Survey of FPGA Based Deep Learning Accelerators: Challenges and\n  Opportunities", "comments": "Some part in the section of introduction dont have the labeling\n  reference. And there are some wrong of data in figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of in-depth learning, neural network and deep\nlearning algorithms have been widely used in various fields, e.g., image, video\nand voice processing. However, the neural network model is getting larger and\nlarger, which is expressed in the calculation of model parameters. Although a\nwealth of existing efforts on GPU platforms currently used by researchers for\nimproving computing performance, dedicated hardware solutions are essential and\nemerging to provide advantages over pure software solutions. In this paper, we\nsystematically investigate the neural network accelerator based on FPGA.\nSpecifically, we respectively review the accelerators designed for specific\nproblems, specific algorithms, algorithm features, and general templates. We\nalso compared the design and implementation of the accelerator based on FPGA\nunder different devices and network models and compared it with the versions of\nCPU and GPU. Finally, we present to discuss the advantages and disadvantages of\naccelerators on FPGA platforms and to further explore the opportunities for\nfuture research.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2018 04:43:25 GMT"}, {"version": "v2", "created": "Wed, 25 Dec 2019 01:34:12 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Wang", "Teng", ""], ["Wang", "Chao", ""], ["Zhou", "Xuehai", ""], ["Chen", "Huaping", ""]]}, {"id": "1901.04989", "submitter": "Marcelo Fernandes", "authors": "Carlos E. B. S. J\\'unior, Matheus F. Torquato and Marcelo A. C.\n  Fernandes", "title": "Application-Specific System Processor for the SHA-1 Hash Algorithm", "comments": "20 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.CR eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes an Application-Specific System Processor (ASSP) hardware\nfor the Secure Hash Algorithm 1 (SHA-1) algorithm. The proposed hardware was\nimplemented in a Field Programmable Gate Array (FPGA) Xilinx Virtex 6\nxc6vlx240t-1ff1156. The throughput and the occupied area were analyzed for\nseveral implementations in parallel instances of the hash algorithm. The\nresults showed that the hardware proposed for the SHA-1 achieved a throughput\nof 0.644 Gbps for a single instance and slightly more than 28 Gbps for 48\ninstances in a single FPGA. Various applications such as password recovery,\npassword validation, and high volume data integrity checking can be performed\nefficiently and quickly with an ASSP for SHA1.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 23:27:37 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["J\u00fanior", "Carlos E. B. S.", ""], ["Torquato", "Matheus F.", ""], ["Fernandes", "Marcelo A. C.", ""]]}, {"id": "1901.05030", "submitter": "Igor Kopestenski", "authors": "Kopestenski Igor, Peter Van Roy", "title": "Achlys : Towards a framework for distributed storage and generic\n  computing applications for wireless IoT edge networks with Lasp on GRiSP", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet of Things (IoT) has gained substantial attention over the past\nyears. And the main discussion has been how to process the amount of data that\nit generates which has lead to the edge computing paradigm. Wether it is called\nfog1, edge or mist, the principle remains that cloud services must become\navailable closer to clients. This documents presents ongoing work on future\nedge systems that are built to provide steadfast IoT services to users by\nbringing storage and processing power closer to peripheral parts of networks.\nDesigning such infrastructures is becoming much more challenging as the number\nof IoT devices keeps growing. Production grade deployments have to meet very\nhigh performance requirements, and end-to-end solutions involve significant\ninvestments. In this paper, we aim at providing a solution to extend the range\nof the edge model to the very farthest nodes in the network. Specifically, we\nfocus on providing reliable storage and computation capabilities immediately on\nwireless IoT sensor nodes. This extended edge model will allow end users to\nmanage their IoT ecosystem without forcibly relying on gateways or Internet\nprovider solutions. In this document, we introduce Achlys, a prototype\nimplementation of an edge node that is a concrete port of the Lasp programming\nlibrary on the GRiSP Erlang embedded system. This way, we aim at addressing the\nneed for a general purpose edge that is both resilient and consistent in terms\nof storage and network. Finally, we study example use cases that could take\nadvantage of integrating the Achlys framework and discuss future work for the\nlatter.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 19:53:04 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Igor", "Kopestenski", ""], ["Van Roy", "Peter", ""]]}, {"id": "1901.05049", "submitter": "Miguel de Prado", "authors": "Miguel de Prado, Jing Su, Rabia Saeed, Lorenzo Keller, Noelia Vallez,\n  Andrew Anderson, David Gregg, Luca Benini, Tim Llewellynn, Nabil Ouerhani,\n  Rozenn Dahyot and, Nuria Pazos", "title": "Bonseyes AI Pipeline -- bringing AI to you. End-to-end integration of\n  data, algorithms and deployment tools", "comments": null, "journal-ref": null, "doi": "10.1145/3403572", "report-no": null, "categories": "cs.LG cs.DC cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Next generation of embedded Information and Communication Technology (ICT)\nsystems are collaborative systems able to perform autonomous tasks. The\nremarkable expansion of the embedded ICT market, together with the rise and\nbreakthroughs of Artificial Intelligence (AI), have put the focus on the Edge\nas it stands as one of the keys for the next technological revolution: the\nseamless integration of AI in our daily life. However, training and deployment\nof custom AI solutions on embedded devices require a fine-grained integration\nof data, algorithms, and tools to achieve high accuracy. Such integration\nrequires a high level of expertise that becomes a real bottleneck for small and\nmedium enterprises wanting to deploy AI solutions on the Edge which,\nultimately, slows down the adoption of AI on daily-life applications. In this\nwork, we present a modular AI pipeline as an integrating framework to bring\ndata, algorithms, and deployment tools together. By removing the integration\nbarriers and lowering the required expertise, we can interconnect the different\nstages of tools and provide a modular end-to-end development of AI products for\nembedded devices. Our AI pipeline consists of four modular main steps: i) data\ningestion, ii) model training, iii) deployment optimization and, iv) the IoT\nhub integration. To show the effectiveness of our pipeline, we provide examples\nof different AI applications during each of the steps. Besides, we integrate\nour deployment framework, LPDNN, into the AI pipeline and present its\nlightweight architecture and deployment capabilities for embedded devices.\nFinally, we demonstrate the results of the AI pipeline by showing the\ndeployment of several AI applications such as keyword spotting, image\nclassification and object detection on a set of well-known embedded platforms,\nwhere LPDNN consistently outperforms all other popular deployment frameworks.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 21:27:28 GMT"}, {"version": "v2", "created": "Wed, 20 May 2020 08:12:04 GMT"}, {"version": "v3", "created": "Thu, 11 Jun 2020 06:41:02 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["de Prado", "Miguel", ""], ["Su", "Jing", ""], ["Saeed", "Rabia", ""], ["Keller", "Lorenzo", ""], ["Vallez", "Noelia", ""], ["Anderson", "Andrew", ""], ["Gregg", "David", ""], ["Benini", "Luca", ""], ["Llewellynn", "Tim", ""], ["Ouerhani", "Nabil", ""], ["and", "Rozenn Dahyot", ""], ["Pazos", "Nuria", ""]]}, {"id": "1901.05182", "submitter": "Mustafa Bal", "authors": "Mustafa Bal, Rangel Milushev, Kaan Armagan", "title": "VeriSign: A Secure Contract Consensus Platform on the Blockchain with\n  Amendment Functionality", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While electronic signatures are widespread, there currently exists no viable\nsigning solutions that can track amendments. We proposed VeriSign, a secure\ncontract consensus platform where amendments to contracts can be tracked in a\ndecentralized medium. We demonstrate a user-facing app where signatories can\nvote on original contracts and amendments, and incorporate a Blockchain where\nwe store the transaction history of original contracts and amendments. This\nplatform has possible applications in tracking the history of legislation, and\namendments to legislation.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 08:54:36 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Bal", "Mustafa", ""], ["Milushev", "Rangel", ""], ["Armagan", "Kaan", ""]]}, {"id": "1901.05423", "submitter": "Alexander Keller", "authors": "Nikolaus Binder and Alexander Keller", "title": "Massively Parallel Construction of Radix Tree Forests for the Efficient\n  Sampling of Discrete Probability Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare different methods for sampling from discrete probability\ndistributions and introduce a new algorithm which is especially efficient on\nmassively parallel processors, such as GPUs. The scheme preserves the\ndistribution properties of the input sequence, exposes constant time complexity\non the average, and significantly lowers the average number of operations for\ncertain distributions when sampling is performed in a parallel algorithm that\nrequires synchronization afterwards. Avoiding load balancing issues of na\\\"ive\napproaches, a very efficient massively parallel construction algorithm for the\nrequired auxiliary data structure is complemented.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 13:17:06 GMT"}, {"version": "v2", "created": "Fri, 30 Aug 2019 14:28:09 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Binder", "Nikolaus", ""], ["Keller", "Alexander", ""]]}, {"id": "1901.05520", "submitter": "Marcelo Ponce", "authors": "Ramses van Zon, Marcelo Ponce, Erik Spence, Daniel Gruner", "title": "Trends in Demand, Growth, and Breadth in Scientific Computing Training\n  Delivered by a High-Performance Computing Center", "comments": "Presented at the Fifth Workshop on Best Practices for Enhancing HPC\n  Training and Education (BPHTE18) @ SC18", "journal-ref": "Journal of Computational Science Education Volume 10, Issue 1, pp.\n  53-60 (2019)", "doi": "10.22369/issn.2153-4136/10/1/9", "report-no": null, "categories": "cs.CY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the changes in the training and educational efforts of the SciNet\nHPC Consortium, a Canadian academic High Performance Computing center, in the\nareas of Scientific Computing and High-Performance Computing, over the last six\nyears. Initially, SciNet offered isolated training events on how to use HPC\nsystems and write parallel code, but the training program now consists of a\nbroad range of workshops and courses that users can take toward certificates in\nscientific computing, data science, or high-performance computing. Using data\non enrollment, attendence, and certificate numbers from SciNet's education\nwebsite, used by almost 1800 users so far, we extract trends on the growth,\ndemand, and breadth of SciNet's training program. Among the results are a\nsteady overall growth, a sharp and steady increase in the demand for data\nscience training, and a wider participation of 'non-traditional' computing\ndisciplines, which has motivated an increasingly broad spectrum of training\nofferings. Of interest is also that many of the training initiatives have\nevolved into courses that can be taken as part of the graduate curriculum at\nthe University of Toronto.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 20:36:54 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["van Zon", "Ramses", ""], ["Ponce", "Marcelo", ""], ["Spence", "Erik", ""], ["Gruner", "Daniel", ""]]}, {"id": "1901.05717", "submitter": "Stefano Forti", "authors": "Antonio Brogi and Stefano Forti and Carlos Guerrero and Isaac Lera", "title": "How to Place Your Apps in the Fog -- State of the Art and Open\n  Challenges", "comments": null, "journal-ref": "Software Practice and Experience, 1-22, 2019", "doi": "10.1002/spe.2766", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fog computing aims at extending the Cloud towards the IoT so to achieve\nimproved QoS and to empower latency-sensitive and bandwidth-hungry\napplications. The Fog calls for novel models and algorithms to distribute\nmulti-service applications in such a way that data processing occurs wherever\nit is best-placed, based on both functional and non-functional requirements.\nThis survey reviews the existing methodologies to solve the application\nplacement problem in the Fog, while pursuing three main objectives. First, it\noffers a comprehensive overview on the currently employed algorithms, on the\navailability of open-source prototypes, and on the size of test use cases.\nSecond, it classifies the literature based on the application and Fog\ninfrastructure characteristics that are captured by available models, with a\nfocus on the considered constraints and the optimised metrics. Finally, it\nidentifies some open challenges in application placement in the Fog.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 10:21:27 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2019 18:13:00 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Brogi", "Antonio", ""], ["Forti", "Stefano", ""], ["Guerrero", "Carlos", ""], ["Lera", "Isaac", ""]]}, {"id": "1901.05758", "submitter": "Myeongjae Jeon", "authors": "Myeongjae Jeon, Shivaram Venkataraman, Amar Phanishayee, Junjie Qian,\n  Wencong Xiao, Fan Yang", "title": "Analysis of Large-Scale Multi-Tenant GPU Clusters for DNN Training\n  Workloads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With widespread advances in machine learning, a number of large enterprises\nare beginning to incorporate machine learning models across a number of\nproducts. These models are typically trained on shared, multi-tenant GPU\nclusters. Similar to existing cluster computing workloads, scheduling\nframeworks aim to provide features like high efficiency, resource isolation,\nfair sharing across users, etc. However Deep Neural Network (DNN) based\nworkloads, predominantly trained on GPUs, differ in two significant ways from\ntraditional big data analytics workloads. First, from a cluster utilization\nperspective, GPUs represent a monolithic resource that cannot be shared at a\nfine granularity across users. Second, from a workload perspective, deep\nlearning frameworks require gang scheduling reducing the flexibility of\nscheduling and making the jobs themselves inelastic to failures at runtime. In\nthis paper we present a detailed workload characterization of a two-month long\ntrace from a multi-tenant GPU cluster in a large enterprise. By correlating\nscheduler logs with logs from individual jobs, we study three distinct issues\nthat affect cluster utilization for DNN training workloads on multi-tenant\nclusters: (1) the effect of gang scheduling and locality constraints on\nqueuing, (2) the effect of locality on GPU utilization, and (3) failures during\ntraining. Based on our experience running a large-scale operation, we provide\ndesign guidelines pertaining to next-generation cluster schedulers for DNN\ntraining workloads.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 12:28:09 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2019 13:44:53 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Jeon", "Myeongjae", ""], ["Venkataraman", "Shivaram", ""], ["Phanishayee", "Amar", ""], ["Qian", "Junjie", ""], ["Xiao", "Wencong", ""], ["Yang", "Fan", ""]]}, {"id": "1901.05803", "submitter": "Myeongjae Jeon", "authors": "Jay H. Park, Sunghwan Kim, Jinwon Lee, Myeongjae Jeon, Sam H. Noh", "title": "Accelerated Training for CNN Distributed Deep Learning through Automatic\n  Resource-Aware Layer Placement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Convolutional Neural Network (CNN) model, often used for image\nclassification, requires significant training time to obtain high accuracy. To\nthis end, distributed training is performed with the parameter server (PS)\narchitecture using multiple servers. Unfortunately, scalability has been found\nto be poor in existing architectures. We find that the PS network is the\nbottleneck as it communicates a large number of gradients and parameters with\nthe many workers. This is because synchronization with the many workers has to\noccur at every step of training. Depending on the model, communication can be\nin the several hundred MBs per synchronization. In this paper, we propose a\nscheme to reduce network traffic through layer placement that considers the\nresources that each layer uses. Through analysis of the characteristics of CNN,\nwe find that placement of layers can be done in an effective manner. We then\nincorporate this observation within the TensorFlow framework such that layers\ncan be automatically placed for more efficient training. Our evaluation making\nuse of this placement scheme show that training time can be significantly\nreduced without loss of accuracy for many CNN models.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 14:19:49 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Park", "Jay H.", ""], ["Kim", "Sunghwan", ""], ["Lee", "Jinwon", ""], ["Jeon", "Myeongjae", ""], ["Noh", "Sam H.", ""]]}, {"id": "1901.05836", "submitter": "Luisa D'Amore", "authors": "Luisa D'Amore, Valeria Mele, Diego Romano and Giuliano Laccetti", "title": "A Multilevel Approach for the Performance Analysis of Parallel\n  Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a multilevel approach for analysing performances of parallel\nalgorithms. The main outcome of such approach is that the algorithm is\ndescribed by using a set of operators which are related to each other according\nto the problem decomposition. Decomposition level determines the granularity of\nthe algorithm. A set of block matrices (decomposition and execution) highlights\nfundamental characteristics of the algorithm, such as inherent parallelism and\nsources of overheads.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 17:18:26 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["D'Amore", "Luisa", ""], ["Mele", "Valeria", ""], ["Romano", "Diego", ""], ["Laccetti", "Giuliano", ""]]}, {"id": "1901.05907", "submitter": "Miquel Pericas", "authors": "Agnes Rohlin, Henrik Fahlgren, Miquel Pericas", "title": "High performance scheduling of mixed-mode DAGs on heterogeneous\n  multicores", "comments": "Presented at HIP3ES, 2019. European Commission Project: LEGaTO - Low\n  Energy Toolset for Heterogeneous Computing (EC-H2020-780681)", "journal-ref": null, "doi": null, "report-no": "HIP3ES/2019/1", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many HPC applications can be expressed as mixed-mode computations, in which\neach node of a computational DAG is itself a parallel computation that can be\nmolded at runtime to allocate different amounts of processing resources. At the\nsame time, modern HPC systems are becoming increasingly heterogeneous to\naddress the requirements of energy efficiency. Effectively using heterogeneous\ndevices is complex, requiring the developer to be aware of each DAG nodes'\ncriticality, and the relative performance of the underlying heterogeneous\nresources.\n  This paper studies how irregular mixed-mode parallel computations can be\nmapped on a single-ISA heterogeneous architecture with the goals of performance\nand portability. To achieve high performance we analyze various schemes for\nheterogeneous scheduling, including both criticality-aware and performance-only\nschemes, and extend them with task molding to dynamically adjust the amount of\nresources used for each task. To achieve performance portability we track each\nDAG nodes' performance and construct an online model of the system and its\nperformance. Using a HiKey960 big.LITTLE board as experimental system, the\nresulting scheduler implementations achieve large speed-ups when executing\nirregular DAGs compared to traditional random work stealing.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 17:15:29 GMT"}, {"version": "v2", "created": "Mon, 8 Jul 2019 19:39:51 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Rohlin", "Agnes", ""], ["Fahlgren", "Henrik", ""], ["Pericas", "Miquel", ""]]}, {"id": "1901.05998", "submitter": "Konstantinos Psychas", "authors": "Konstantinos Psychas, Javad Ghaderi", "title": "Scheduling Jobs with Random Resource Requirements in Computing Clusters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a natural scheduling problem which arises in many distributed\ncomputing frameworks. Jobs with diverse resource requirements (e.g. memory\nrequirements) arrive over time and must be served by a cluster of servers, each\nwith a finite resource capacity. To improve throughput and delay, the scheduler\ncan pack as many jobs as possible in the servers subject to their capacity\nconstraints. Motivated by the ever-increasing complexity of workloads in shared\nclusters, we consider a setting where the jobs' resource requirements belong to\na very large number of diverse types or, in the extreme, even infinitely many\ntypes, e.g. when resource requirements are drawn from an unknown distribution\nover a continuous support. The application of classical scheduling approaches\nthat crucially rely on a predefined finite set of types is discouraging in this\nhigh (or infinite) dimensional setting. We first characterize a fundamental\nlimit on the maximum throughput in such setting, and then develop oblivious\nscheduling algorithms that have low complexity and can achieve at least 1/2 and\n2/3 of the maximum throughput, without the knowledge of traffic or resource\nrequirement distribution. Extensive simulation results, using both synthetic\nand real traffic traces, are presented to verify the performance of our\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 20:17:33 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Psychas", "Konstantinos", ""], ["Ghaderi", "Javad", ""]]}, {"id": "1901.06175", "submitter": "Stefano Cherubin", "authors": "Cristina Silvano, Giovanni Agosta, Andrea Bartolini, Andrea R.\n  Beccari, Luca Benini, Lo\\\"ic Besnard, Jo\\~ao Bispo, Radim Cmar, Jo\\~ao M. P.\n  Cardoso, Carlo Cavazzoni, Daniele Cesarini, Stefano Cherubin, Federico\n  Ficarelli, Davide Gadioli, Martin Golasowski, Antonio Libri, Jan\n  Martinovi\\v{c}, Gianluca Palermo, Pedro Pinto, Erven Rohou, Kate\\v{r}ina\n  Slaninov\\'a, Emanuele Vitali", "title": "The ANTAREX Domain Specific Language for High Performance Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ANTAREX project relies on a Domain Specific Language (DSL) based on\nAspect Oriented Programming (AOP) concepts to allow applications to enforce\nextra functional properties such as energy-efficiency and performance and to\noptimize Quality of Service (QoS) in an adaptive way. The DSL approach allows\nthe definition of energy-efficiency, performance, and adaptivity strategies as\nwell as their enforcement at runtime through application autotuning and\nresource and power management. In this paper, we present an overview of the key\noutcome of the project, the ANTAREX DSL, and some of its capabilities through a\nnumber of examples, including how the DSL is applied in the context of the\nproject use cases.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 10:46:47 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Silvano", "Cristina", ""], ["Agosta", "Giovanni", ""], ["Bartolini", "Andrea", ""], ["Beccari", "Andrea R.", ""], ["Benini", "Luca", ""], ["Besnard", "Lo\u00efc", ""], ["Bispo", "Jo\u00e3o", ""], ["Cmar", "Radim", ""], ["Cardoso", "Jo\u00e3o M. P.", ""], ["Cavazzoni", "Carlo", ""], ["Cesarini", "Daniele", ""], ["Cherubin", "Stefano", ""], ["Ficarelli", "Federico", ""], ["Gadioli", "Davide", ""], ["Golasowski", "Martin", ""], ["Libri", "Antonio", ""], ["Martinovi\u010d", "Jan", ""], ["Palermo", "Gianluca", ""], ["Pinto", "Pedro", ""], ["Rohou", "Erven", ""], ["Slaninov\u00e1", "Kate\u0159ina", ""], ["Vitali", "Emanuele", ""]]}, {"id": "1901.06210", "submitter": "Gianluca Palermo", "authors": "Emanuele Vitali, Davide Gadioli, Gianluca Palermo, Martin Golasowski,\n  Joao Bispo, Pedro Pinto, Jan Martinovic, Katerina Slaninova, Joao M. P.\n  Cardoso, Cristina Silvano", "title": "An Efficient Monte Carlo-based Probabilistic Time-Dependent Routing\n  Calculation Targeting a Server-Side Car Navigation System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incorporating speed probability distribution to the computation of the route\nplanning in car navigation systems guarantees more accurate and precise\nresponses. In this paper, we propose a novel approach for dynamically selecting\nthe number of samples used for the Monte Carlo simulation to solve the\nProbabilistic Time-Dependent Routing (PTDR) problem, thus improving the\ncomputation efficiency. The proposed method is used to determine in a proactive\nmanner the number of simulations to be done to extract the travel-time\nestimation for each specific request while respecting an error threshold as\noutput quality level. The methodology requires a reduced effort on the\napplication development side. We adopted an aspect-oriented programming\nlanguage (LARA) together with a flexible dynamic autotuning library (mARGOt)\nrespectively to instrument the code and to take tuning decisions on the number\nof samples improving the execution efficiency. Experimental results demonstrate\nthat the proposed adaptive approach saves a large fraction of simulations\n(between 36% and 81%) with respect to a static approach while considering\ndifferent traffic situations, paths and error requirements. Given the\nnegligible runtime overhead of the proposed approach, it results in an\nexecution-time speedup between 1.5x and 5.1x. This speedup is reflected at\ninfrastructure-level in terms of a reduction of around 36% of the computing\nresources needed to support the whole navigation pipeline.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 13:06:38 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Vitali", "Emanuele", ""], ["Gadioli", "Davide", ""], ["Palermo", "Gianluca", ""], ["Golasowski", "Martin", ""], ["Bispo", "Joao", ""], ["Pinto", "Pedro", ""], ["Martinovic", "Jan", ""], ["Slaninova", "Katerina", ""], ["Cardoso", "Joao M. P.", ""], ["Silvano", "Cristina", ""]]}, {"id": "1901.06228", "submitter": "Gianluca Palermo", "authors": "Tomas Martinovic, Davide Gadioli, Gianluca Palermo, Cristina Silvano", "title": "On-line Application Autotuning Exploiting Ensemble Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Application autotuning is a promising path investigated in literature to\nimprove computation efficiency. In this context, the end-users define\nhigh-level requirements and an autonomic manager is able to identify and seize\noptimization opportunities by leveraging trade-offs between extra-functional\nproperties of interest, such as execution time, power consumption or quality of\nresults. The relationship between an application configuration and the\nextra-functional properties might depend on the underlying architecture, on the\nsystem workload and on features of the current input. For these reasons,\nautotuning frameworks rely on application knowledge to drive the adaptation\nstrategies. The autotuning task is typically done offline because having it in\nproduction requires significant effort to reduce its overhead. In this paper,\nwe enhance a dynamic autotuning framework with a module for learning the\napplication knowledge during the production phase, in a distributed fashion. We\nleverage two strategies to limit the overhead introduced at the production\nphase. On one hand, we use a scalable infrastructure capable of leveraging the\nparallelism of the underlying platform. On the other hand, we use ensemble\nmodels to speed up the predictive capabilities, while iteratively gathering\nproduction data. Experimental results on synthetic applications and on a use\ncase show how the proposed approach is able to learn the application knowledge,\nby exploring a small fraction of the design space.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 13:47:07 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Martinovic", "Tomas", ""], ["Gadioli", "Davide", ""], ["Palermo", "Gianluca", ""], ["Silvano", "Cristina", ""]]}, {"id": "1901.06229", "submitter": "Gianluca Palermo", "authors": "Emanuele Vitali, Davide Gadioli, Gianluca Palermo, Andrea Beccari,\n  Carlo Cavazzoni, Cristina Silvano", "title": "Exploiting OpenMP & OpenACC to Accelerate a Molecular Docking Mini-App\n  in Heterogeneous HPC Nodes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In drug discovery, molecular docking is the task in charge of estimating the\nposition of a molecule when interacting with the docking site. This task is\nusually used to perform screening of a large library of molecules, in the early\nphase of the process. Given the amount of candidate molecules and the\ncomplexity of the application, this task is usually performed using\nHigh-Performance Computing (HPC) platforms. In modern HPC systems,\nheterogeneous platforms provide a better throughput with respect to homogeneous\nplatforms. In this work, we ported and optimized a molecular docking\napplication to a heterogeneous system, with one or more GPU accelerators,\nleveraging a hybrid OpenMP and OpenACC approach. We prove that our approach has\na better exploitation of the node compared to pure CPU/GPU data splitting\napproaches, reaching a throughput improvement up to 36% while considering the\nsame computing node.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 13:48:26 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Vitali", "Emanuele", ""], ["Gadioli", "Davide", ""], ["Palermo", "Gianluca", ""], ["Beccari", "Andrea", ""], ["Cavazzoni", "Carlo", ""], ["Silvano", "Cristina", ""]]}, {"id": "1901.06236", "submitter": "Michael Kuperberg", "authors": "Michael Kuperberg, Daniel Kindler, Sabina Jeschke", "title": "Are Smart Contracts and Blockchains Suitable for Decentralized Railway\n  Control?", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional railway operations employ specialized software and hardware to\nensure safe and secure train operations. Track occupation and signaling are\ngoverned by central control offices, while trains (and their drivers) receive\ninstructions. To make this setup more dynamic, the train operations can be\ndecentralized by enabling the trains to find routes and make decisions which\nare safeguarded and protocolled in an auditable manner. In this paper, we\npresent the findings of a first-of-its-kind blockchain-based prototype\nimplementation for railway control, based on decentralization but also ensuring\nthat the overall system state remains conflict-free and safe. We also show how\na blockchain-based approach simplifies usage billing and enables a\ntrain-to-train/machine-to-machine economy. Finally, first ideas addressing the\nuse of blockchain as a life-cycle approach for condition based monitoring and\npredictive maintenance in train operations are outlined.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 14:01:59 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Kuperberg", "Michael", ""], ["Kindler", "Daniel", ""], ["Jeschke", "Sabina", ""]]}, {"id": "1901.06331", "submitter": "Mohammad Hosseinabady", "authors": "Mohammad Hosseinabady, Mohd Amiruddin Bin Zainol, Jose Nunez-Yanez", "title": "Heterogeneous FPGA+GPU Embedded Systems: Challenges and Opportunities", "comments": "Presented at HIP3ES, 2019", "journal-ref": null, "doi": null, "report-no": "HIP3ES/2019/4", "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The edge computing paradigm has emerged to handle cloud computing issues such\nas scalability, security and low response time among others. This new computing\ntrend heavily relies on ubiquitous embedded systems on the edge. Performance\nand energy consumption are two main factors that should be considered during\nthe design of such systems. Focusing on performance and energy consumption,\nthis paper studies the opportunities and challenges that a heterogeneous\nembedded system consisting of embedded FPGAs and GPUs (as accelerators) can\nprovide for applications. We study three design, modeling and scheduling\nchallenges throughout the paper. We also propose three techniques to cope with\nthese three challenges. Applying the proposed techniques to three applications\nincluding image histogram, dense matrix-vector multiplication and sparse\nmatrix-vector multiplications show 1.79x and 2.29x improvements in performance\nand energy consumption, respectively, when both FPGA and GPU execute the\ncorresponding application in parallel.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 16:37:06 GMT"}, {"version": "v2", "created": "Fri, 25 Jan 2019 12:50:40 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Hosseinabady", "Mohammad", ""], ["Zainol", "Mohd Amiruddin Bin", ""], ["Nunez-Yanez", "Jose", ""]]}, {"id": "1901.06347", "submitter": "Zohar Kapach", "authors": "Zohar Kapach, Andrew Ulmer, Daniel Merrick, Arshad Alikhan,\n  Yung-Hsiang Lu, Anup Mohan, Ahmed S. Kaseb, George K. Thiruvathukal", "title": "Cloud Resource Optimization for Processing Multiple Streams of Visual\n  Data", "comments": "IEEE MultiMedia Magazine", "journal-ref": null, "doi": "10.1109/MMUL.2018.2890255", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hundreds of millions of network cameras have been installed throughout the\nworld. Each is capable of providing a vast amount of real-time data. Analyzing\nthe massive data generated by these cameras requires significant computational\nresources and the demands may vary over time. Cloud computing shows the most\npromise to provide the needed resources on demand. In this article, we\ninvestigate how to allocate cloud resources when analyzing real-time data\nstreams from network cameras. A resource manager considers many factors that\naffect its decisions, including the types of analysis, the number of data\nstreams, and the locations of the cameras. The manager then selects the most\ncost-efficient types of cloud instances (e.g. CPU vs. GPGPU) to meet the\ncomputational demands for analyzing streams. We evaluate the effectiveness of\nour approach using Amazon Web Services. Experiments demonstrate more than 50%\ncost reduction for real workloads.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 17:21:52 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Kapach", "Zohar", ""], ["Ulmer", "Andrew", ""], ["Merrick", "Daniel", ""], ["Alikhan", "Arshad", ""], ["Lu", "Yung-Hsiang", ""], ["Mohan", "Anup", ""], ["Kaseb", "Ahmed S.", ""], ["Thiruvathukal", "George K.", ""]]}, {"id": "1901.06363", "submitter": "Gianluca Palermo", "authors": "Davide Gadioli, Gianluca Palermo, Stefano Cherubin, Emanuele Vitali,\n  Giovanni Agosta, Candida Manelfi, Andrea R. Beccari, Carlo Cavazzoni, Nico\n  Sanna, and Cristina Silvano", "title": "Tunable Approximations to Control Time-to-Solution in an HPC Molecular\n  Docking Mini-App", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The drug discovery process involves several tasks to be performed in vivo, in\nvitro and in silico. Molecular docking is a task typically performed in silico.\nIt aims at finding the three-dimensional pose of a given molecule when it\ninteracts with the target protein binding site. This task is often done for\nvirtual screening a huge set of molecules to find the most promising ones,\nwhich will be forwarded to the later stages of the drug discovery process.\nGiven the huge complexity of the problem, molecular docking cannot be solved by\nexploring the entire space of the ligand poses. State-of-the-art approaches\nface the problem by sampling the space of the ligand poses to generate results\nin a reasonable time budget. In this work, we improve the geometric approach to\nmolecular docking by introducing tunable approximations. In particular, we\nanalyzed and enriched the original implementation with tunable software knobs\nto explore and control the performance-accuracy tradeoffs. We modeled\ntime-to-solution of the virtual screening task as a function of software knobs,\ninput data features, and available computational resources. Therefore, the\napplication can autotune its configuration according to a user-defined time\nbudget. We used a Mini-App derived by LiGenDock - a state-of-the-art molecular\ndocking application - to validate the proposed approach. We run the enhanced\nMini-App on an HPC system by using a very large database of pockets and\nligands. The proposed approach exposes a time-to-solution interval spanning\nmore than one order of magnitude with accuracy degradation up to 30%, more in\ngeneral providing different accuracy levels according to the needs of the\nvirtual screening campaign.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 18:13:01 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Gadioli", "Davide", ""], ["Palermo", "Gianluca", ""], ["Cherubin", "Stefano", ""], ["Vitali", "Emanuele", ""], ["Agosta", "Giovanni", ""], ["Manelfi", "Candida", ""], ["Beccari", "Andrea R.", ""], ["Cavazzoni", "Carlo", ""], ["Sanna", "Nico", ""], ["Silvano", "Cristina", ""]]}, {"id": "1901.06455", "submitter": "Boyi Liu", "authors": "Boyi Liu, Lujia Wang, Ming Liu", "title": "Lifelong Federated Reinforcement Learning: A Learning Architecture for\n  Navigation in Cloud Robotic Systems", "comments": null, "journal-ref": "IEEE Robotics and Automation Letters, Volume: 4, Issue:4, On\n  Page(s): 4555-4562, 2019", "doi": "10.1109/LRA.2019.2931179", "report-no": null, "categories": "cs.RO cs.AI cs.DC cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper was motivated by the problem of how to make robots fuse and\ntransfer their experience so that they can effectively use prior knowledge and\nquickly adapt to new environments. To address the problem, we present a\nlearning architecture for navigation in cloud robotic systems: Lifelong\nFederated Reinforcement Learning (LFRL). In the work, We propose a knowledge\nfusion algorithm for upgrading a shared model deployed on the cloud. Then,\neffective transfer learning methods in LFRL are introduced. LFRL is consistent\nwith human cognitive science and fits well in cloud robotic systems.\nExperiments show that LFRL greatly improves the efficiency of reinforcement\nlearning for robot navigation. The cloud robotic system deployment also shows\nthat LFRL is capable of fusing prior knowledge. In addition, we release a cloud\nrobotic navigation-learning website based on LFRL.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jan 2019 02:09:14 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 08:32:03 GMT"}, {"version": "v3", "created": "Mon, 13 May 2019 07:09:50 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Liu", "Boyi", ""], ["Wang", "Lujia", ""], ["Liu", "Ming", ""]]}, {"id": "1901.06545", "submitter": "Xiong Zheng", "authors": "Xiong Zheng, Vijay Garg", "title": "An Optimal Vector Clock Algorithm for Multithreaded Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking causality (or happened-before relation) between events is useful for\nmany applications such as debugging and recovery from failures. Consider a\nconcurrent system with $n$ threads and $m$ objects. For such systems, either a\nvector clock of size $n$ is used with one component per thread or a vector\nclock of size $m$ is used with one component per object. A natural question is\nwhether one can use a vector clock of size strictly less than the minimum of\n$m$ and $n$ to timestamp events. We give an algorithm in this paper that uses a\nhybrid of thread and object components. Our algorithm is guaranteed to return\nthe minimum number of components necessary for vector clocks. We first consider\nthe case when the interaction between objects and threads is statically known.\nThis interaction is modeled by a thread-object bipartite graph. Our algorithm\nis based on finding the maximum bipartite matching of such a graph and then\napplying K\\\"{o}nig-Egerv\\'{a}ry Theorem to compute the minimum vertex cover to\ndetermine the optimal number of components necessary for the vector clock. We\nalso propose two mechanisms to compute such an vector clock when computation is\nrevealed in an online fashion. Evaluation on different types of graphs\nindicates that our offline algorithm generates a size vector clock which is\nsignificantly less than the minimum of $m$ and $n$. These mechanisms are more\neffective when the underlying bipartite graph is not dense.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jan 2019 16:08:08 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Zheng", "Xiong", ""], ["Garg", "Vijay", ""]]}, {"id": "1901.06549", "submitter": "Iliad Ramezani", "authors": "Emanuele Natale and Iliad Ramezani", "title": "On the Necessary Memory to Compute the Plurality in Multi-Agent Systems", "comments": "14 pages, accepted at CIAC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Relative-Majority Problem (also known as Plurality), in\nwhich, given a multi-agent system where each agent is initially provided an\ninput value out of a set of $k$ possible ones, each agent is required to\neventually compute the input value with the highest frequency in the initial\nconfiguration. We consider the problem in the general Population Protocols\nmodel in which, given an underlying undirected connected graph whose nodes\nrepresent the agents, edges are selected by a globally fair scheduler.\n  The state complexity that is required for solving the Plurality Problem\n(i.e., the minimum number of memory states that each agent needs to have in\norder to solve the problem), has been a long-standing open problem. The best\nprotocol so far for the general multi-valued case requires polynomial memory:\nSalehkaleybar et al. (2015) devised a protocol that solves the problem by\nemploying $O(k 2^k)$ states per agent, and they conjectured their upper bound\nto be optimal. On the other hand, under the strong assumption that agents\ninitially agree on a total ordering of the initial input values, Gasieniec et\nal. (2017), provided an elegant logarithmic-memory plurality protocol.\n  In this work, we refute Salehkaleybar et al.'s conjecture, by providing a\nplurality protocol which employs $O(k^{11})$ states per agent. Central to our\nresult is an ordering protocol which allows to leverage on the plurality\nprotocol by Gasieniec et al., of independent interest. We also provide a\n$\\Omega(k^2)$-state lower bound on the necessary memory to solve the problem,\nproving that the Plurality Problem cannot be solved within the mere memory\nnecessary to encode the output.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jan 2019 16:25:55 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Natale", "Emanuele", ""], ["Ramezani", "Iliad", ""]]}, {"id": "1901.06587", "submitter": "Seyed Mohammadreza Mousavi Kalan", "authors": "Seyed Mohammadreza Mousavi Kalan, Mahdi Soltanolkotabi, and A. Salman\n  Avestimehr", "title": "Fitting ReLUs via SGD and Quantized SGD", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we focus on the problem of finding the optimal weights of the\nshallowest of neural networks consisting of a single Rectified Linear Unit\n(ReLU). These functions are of the form $\\mathbf{x}\\rightarrow\n\\max(0,\\langle\\mathbf{w},\\mathbf{x}\\rangle)$ with $\\mathbf{w}\\in\\mathbb{R}^d$\ndenoting the weight vector. We focus on a planted model where the inputs are\nchosen i.i.d. from a Gaussian distribution and the labels are generated\naccording to a planted weight vector. We first show that mini-batch stochastic\ngradient descent when suitably initialized, converges at a geometric rate to\nthe planted model with a number of samples that is optimal up to numerical\nconstants. Next we focus on a parallel implementation where in each iteration\nthe mini-batch gradient is calculated in a distributed manner across multiple\nprocessors and then broadcast to a master or all other processors. To reduce\nthe communication cost in this setting we utilize a Quanitzed Stochastic\nGradient Scheme (QSGD) where the partial gradients are quantized. Perhaps\nunexpectedly, we show that QSGD maintains the fast convergence of SGD to a\nglobally optimal model while significantly reducing the communication cost. We\nfurther corroborate our numerical findings via various experiments including\ndistributed implementations over Amazon EC2.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jan 2019 21:21:54 GMT"}, {"version": "v2", "created": "Mon, 1 Apr 2019 17:48:22 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Kalan", "Seyed Mohammadreza Mousavi", ""], ["Soltanolkotabi", "Mahdi", ""], ["Avestimehr", "A. Salman", ""]]}, {"id": "1901.06811", "submitter": "Burak Bartan", "authors": "Burak Bartan, Mert Pilanci", "title": "Straggler Resilient Serverless Computing Based on Polar Codes", "comments": "New results added in the new version. More discussion on serverless\n  computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a serverless computing mechanism for distributed computation based\non polar codes. Serverless computing is an emerging cloud based computation\nmodel that lets users run their functions on the cloud without provisioning or\nmanaging servers. Our proposed approach is a hybrid computing framework that\ncarries out computationally expensive tasks such as linear algebraic operations\ninvolving large-scale data using serverless computing and does the rest of the\nprocessing locally. We address the limitations and reliability issues of\nserverless platforms such as straggling workers using coding theory, drawing\nideas from recent literature on coded computation. The proposed mechanism uses\npolar codes to ensure straggler-resilience in a computationally effective\nmanner. We provide extensive evidence showing polar codes outperform other\ncoding methods. We have designed a sequential decoder specifically for polar\ncodes in erasure channels with full-precision input and outputs. In addition,\nwe have extended the proposed method to the matrix multiplication case where\nboth matrices being multiplied are coded. The proposed coded computation scheme\nis implemented for AWS Lambda. Experiment results are presented where the\nperformance of the proposed coded computation technique is tested in\noptimization via gradient descent. Finally, we introduce the idea of partial\npolarization which reduces the computational burden of encoding and decoding at\nthe expense of straggler-resilience.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 07:15:24 GMT"}, {"version": "v2", "created": "Sat, 13 Jul 2019 00:49:41 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Bartan", "Burak", ""], ["Pilanci", "Mert", ""]]}, {"id": "1901.06824", "submitter": "Kyrill Winkler", "authors": "Matthias F\\\"ugger, Thomas Nowak, Kyrill Winkler", "title": "On the Radius of Nonsplit Graphs and Information Dissemination in\n  Dynamic Networks", "comments": "12 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A nonsplit graph is a directed graph where each pair of nodes has a common\nincoming neighbor. We show that the radius of such graphs is in $O(\\log \\log\nn)$, where $n$ is the number of nodes. We then generalize the result to\nproducts of nonsplit graphs.\n  The analysis of nonsplit graph products has direct implications in the\ncontext of distributed systems, where processes operate in rounds and\ncommunicate via message passing in each round: communication graphs in several\ndistributed systems naturally relate to nonsplit graphs and the graph product\nconcisely represents relaying messages in such networks. Applying our results,\nwe obtain improved bounds on the dynamic radius of such networks, i.e., the\nmaximum number of rounds until all processes have received a message from a\ncommon process, if all processes relay messages in each round. We finally\nconnect the dynamic radius to lower bounds for achieving consensus in dynamic\nnetworks.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 08:45:38 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["F\u00fcgger", "Matthias", ""], ["Nowak", "Thomas", ""], ["Winkler", "Kyrill", ""]]}, {"id": "1901.06887", "submitter": "Jonathan Mace", "authors": "Amit Samanta and Suhas Shrinivasan and Antoine Kaufmann and Jonathan\n  Mace", "title": "No DNN Left Behind: Improving Inference in the Cloud with Multi-Tenancy", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rise of machine learning, inference on deep neural networks (DNNs)\nhas become a core building block on the critical path for many cloud\napplications. Applications today rely on isolated ad-hoc deployments that force\nusers to compromise on consistent latency, elasticity, or cost-efficiency,\ndepending on workload characteristics. We propose to elevate DNN inference to\nbe a first class cloud primitive provided by a shared multi-tenant system, akin\nto cloud storage, and cloud databases. A shared system enables cost-efficient\noperation with consistent performance across the full spectrum of workloads. We\nargue that DNN inference is an ideal candidate for a multi-tenant system\nbecause of its narrow and well-defined interface and predictable resource\nrequirements.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 11:40:21 GMT"}, {"version": "v2", "created": "Wed, 23 Jan 2019 10:38:15 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Samanta", "Amit", ""], ["Shrinivasan", "Suhas", ""], ["Kaufmann", "Antoine", ""], ["Mace", "Jonathan", ""]]}, {"id": "1901.06899", "submitter": "Michael Orr", "authors": "Michael Orr and Oliver Sinnen", "title": "Optimal Task Scheduling Benefits From a Duplicate-Free State-Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The NP-hard problem of task scheduling with communication delays\n(P|prec,c_{ij}|C_{\\mathrm{max}}) is often tackled using approximate methods,\nbut guarantees on the quality of these heuristic solutions are hard to come by.\nOptimal schedules are therefore invaluable for properly evaluating these\nheuristics, as well as being very useful for applications in time critical\nsystems. Optimal solving using branch-and-bound algorithms like A* has been\nshown to be promising in the past, with a state-space model we refer to as\nexhaustive list scheduling (ELS). The obvious weakness of this model is that it\nleads to the production of large numbers of duplicate states during a search,\nrequiring special techniques to mitigate this which cost additional time and\nmemory. In this paper we define a new state-space model (AO) in which we divide\nthe problem into two distinct sub-problems: first we decide the allocations of\nall tasks to processors, and then we order the tasks on their allocated\nprocessors in order to produce a complete schedule. This two-phase state-space\nmodel offers no potential for the production of duplicates. We also describe\nhow the pruning techniques and optimisations developed for the ELS model were\nadapted or made obsolete by the AO model. An experimental evaluation shows that\nthe use of this new state-space model leads to a significant increase in the\nnumber of task graphs able to be scheduled within a feasible time-frame,\nparticularly for task graphs with a high communication-to-computation ratio.\nFinally, some advanced lower bound heuristics are proposed for the AO model,\nand evaluation demonstrates that significant gains can be achieved from the\nconsideration of necessary idle time.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 12:12:06 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Orr", "Michael", ""], ["Sinnen", "Oliver", ""]]}, {"id": "1901.06918", "submitter": "Siavash Ghiasvand", "authors": "Siavash Ghiasvand, Florina M. Ciorba, Wolfgang E. Nagel", "title": "Turning Privacy Constraints into Syslog Analysis Advantage", "comments": "This document is mistakenly submitted to arXiv", "journal-ref": "29th ACM/IEEE International Conference for High Performance\n  Computing, Networking, Storage and Analysis (SC 2016)", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mean time between failures (MTBF) of HPC systems is rapidly reducing, and\nthat current failure recovery mechanisms e.g., checkpoint-restart, will no\nlonger be able to recover the systems from failures. Early failure detection is\na new class of failure recovery methods that can be beneficial for HPC systems\nwith short MTBF. System logs (syslogs) are invaluable source of information\nwhich give us a deep insight about system behavior, and make the early failure\ndetection possible. Beside normal information, syslogs contain sensitive data\nwhich might endanger users' privacy. Even though analyzing various syslogs is\nnecessary for creating a general failure detection/prediction method, privacy\nconcerns discourage system administrators to publish syslogs. Herein, we ensure\nuser privacy via de-identifying syslogs, and then turning the applied\nconstraint for addressing users' privacy into an advantage for system behavior\nanalysis. Results indicate significant reduction in required storage space and\n3 times shorter processing time.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 13:10:27 GMT"}, {"version": "v2", "created": "Thu, 14 Mar 2019 08:02:38 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Ghiasvand", "Siavash", ""], ["Ciorba", "Florina M.", ""], ["Nagel", "Wolfgang E.", ""]]}, {"id": "1901.06995", "submitter": "Usman Khan", "authors": "Ran Xin, Dusan Jakovetic, Usman A. Khan", "title": "Distributed Nesterov gradient methods over arbitrary graphs", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2019.2925537", "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this letter, we introduce a distributed Nesterov method, termed as\n$\\mathcal{ABN}$, that does not require doubly-stochastic weight matrices.\nInstead, the implementation is based on a simultaneous application of both row-\nand column-stochastic weights that makes this method applicable to arbitrary\n(strongly-connected) graphs. Since constructing column-stochastic weights needs\nadditional information (the number of outgoing neighbors at each agent), not\navailable in certain communication protocols, we derive a variation, termed as\nFROZEN, that only requires row-stochastic weights but at the expense of\nadditional iterations for eigenvector learning. We numerically study these\nalgorithms for various objective functions and network parameters and show that\nthe proposed distributed Nesterov methods achieve acceleration compared to the\ncurrent state-of-the-art methods for distributed optimization.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 16:43:31 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Xin", "Ran", ""], ["Jakovetic", "Dusan", ""], ["Khan", "Usman A.", ""]]}, {"id": "1901.07023", "submitter": "Michael Garvie", "authors": "Michael Garvie and Phil Husbands", "title": "Automatic Synthesis of Totally Self-Checking Circuits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Totally self-checking (TSC) circuits are synthesised with a grid of computers\nrunning a distributed population based stochastic optimisation algorithm. The\npresented method is the first to automatically synthesise TSC circuits from\narbitrary logic as all previous methods fail to guarantee the checker is\nself-testing (ST) for circuits with limited output codespaces. The circuits\nsynthesised by the presented method have significantly lower overhead than the\npreviously reported best for every one of a set of 11 frequently used\nbenchmarks. Average overhead across the entire set is 23% of duplication and\ncomparison overhead, compared with an average of 69% for the previous best\nreported values across the set. The methodology presented represents a\nbreakthrough in concurrent error detection (CED). The highly efficient, novel\ndesigns produced are tailored to each circuit's function, rather than being\nconstrained by a particular modular CED design methodology. Results are\nsynthesised using two-input gates and are TSC with respect to all gate input\nand output stuck-at faults. The method can be used to add CED with or without\nmodifications to the original logic, and can be generalised to any\nimplementation technology and fault model. An example circuit is analysed and\nrigorously proven to be TSC.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 18:26:46 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Garvie", "Michael", ""], ["Husbands", "Phil", ""]]}, {"id": "1901.07038", "submitter": "Eliu Huerta", "authors": "E. A. Huerta, Roland Haas, Sarah Habib, Anushri Gupta, Adam Rebei,\n  Vishnu Chavva, Daniel Johnson, Shawn Rosofsky, Erik Wessel, Bhanu Agarwal,\n  Diyu Luo and Wei Ren", "title": "Physics of eccentric binary black hole mergers: A numerical relativity\n  perspective", "comments": "11 pages, 5 figures, 2 appendices. A visualization of this numerical\n  relativity waveform catalog is available at\n  https://gravity.ncsa.illinois.edu/products/outreach/; v2: 13 pages, 5\n  figures, calculations for angular momentum emission and recoil velocities are\n  now included, references added. Accepted to Phys. Rev. D", "journal-ref": "Phys. Rev. D 100, 064003 (2019)", "doi": "10.1103/PhysRevD.100.064003", "report-no": null, "categories": "gr-qc astro-ph.CO astro-ph.HE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gravitational wave observations of eccentric binary black hole mergers will\nprovide unequivocal evidence for the formation of these systems through\ndynamical assembly in dense stellar environments. The study of these\nastrophysically motivated sources is timely in view of electromagnetic\nobservations, consistent with the existence of stellar mass black holes in the\nglobular cluster M22 and in the Galactic center, and the proven detection\ncapabilities of ground-based gravitational wave detectors. In order to get\ninsights into the physics of these objects in the dynamical, strong-field\ngravity regime, we present a catalog of 89 numerical relativity waveforms that\ndescribe binary systems of non-spinning black holes with mass-ratios $1\\leq q\n\\leq 10$, and initial eccentricities as high as $e_0=0.18$ fifteen cycles\nbefore merger. We use this catalog to quantify the loss of energy and angular\nmomentum through gravitational radiation, and the astrophysical properties of\nthe black hole remnant, including its final mass and spin, and recoil velocity.\nWe discuss the implications of these results for gravitational wave source\nmodeling, and the design of algorithms to search for and identify eccentric\nbinary black hole mergers in realistic detection scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 19:00:03 GMT"}, {"version": "v2", "created": "Thu, 5 Sep 2019 16:56:25 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Huerta", "E. A.", ""], ["Haas", "Roland", ""], ["Habib", "Sarah", ""], ["Gupta", "Anushri", ""], ["Rebei", "Adam", ""], ["Chavva", "Vishnu", ""], ["Johnson", "Daniel", ""], ["Rosofsky", "Shawn", ""], ["Wessel", "Erik", ""], ["Agarwal", "Bhanu", ""], ["Luo", "Diyu", ""], ["Ren", "Wei", ""]]}, {"id": "1901.07143", "submitter": "Matteo Cremonesi", "authors": "Matteo Cremonesi, Claudio Bellini, Bianny Bian, Luca Canali, Vasileios\n  Dimakopoulos, Peter Elmer, Ian Fisk, Maria Girone, Oliver Gutsche, Siew-Yan\n  Hoh, Bo Jayatilaka, Viktor Khristenko, Andrea Luiselli, Andrew Melo,\n  Evangelos Evangelos, Dominick Olivito, Jacopo Pazzini, Jim Pivarski, Alexey\n  Svyatkovskiy, Marco Zanetti", "title": "Using Big Data Technologies for HEP Analysis", "comments": null, "journal-ref": null, "doi": "10.1051/epjconf/201921406030", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The HEP community is approaching an era were the excellent performances of\nthe particle accelerators in delivering collision at high rate will force the\nexperiments to record a large amount of information. The growing size of the\ndatasets could potentially become a limiting factor in the capability to\nproduce scientific results timely and efficiently. Recently, new technologies\nand new approaches have been developed in industry to answer to the necessity\nto retrieve information as quickly as possible to analyze PB and EB datasets.\nProviding the scientists with these modern computing tools will lead to\nrethinking the principles of data analysis in HEP, making the overall\nscientific process faster and smoother.\n  In this paper, we are presenting the latest developments and the most recent\nresults on the usage of Apache Spark for HEP analysis. The study aims at\nevaluating the efficiency of the application of the new tools both\nquantitatively, by measuring the performances, and qualitatively, focusing on\nthe user experience. The first goal is achieved by developing a data reduction\nfacility: working together with CERN Openlab and Intel, CMS replicates a real\nphysics search using Spark-based technologies, with the ambition of reducing 1\nPB of public data in 5 hours, collected by the CMS experiment, to 1 TB of data\nin a format suitable for physics analysis.\n  The second goal is achieved by implementing multiple physics use-cases in\nApache Spark using as input preprocessed datasets derived from official CMS\ndata and simulation. By performing different end-analyses up to the publication\nplots on different hardware, feasibility, usability and portability are\ncompared to the ones of a traditional ROOT-based workflow.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 01:35:59 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Cremonesi", "Matteo", ""], ["Bellini", "Claudio", ""], ["Bian", "Bianny", ""], ["Canali", "Luca", ""], ["Dimakopoulos", "Vasileios", ""], ["Elmer", "Peter", ""], ["Fisk", "Ian", ""], ["Girone", "Maria", ""], ["Gutsche", "Oliver", ""], ["Hoh", "Siew-Yan", ""], ["Jayatilaka", "Bo", ""], ["Khristenko", "Viktor", ""], ["Luiselli", "Andrea", ""], ["Melo", "Andrew", ""], ["Evangelos", "Evangelos", ""], ["Olivito", "Dominick", ""], ["Pazzini", "Jacopo", ""], ["Pivarski", "Jim", ""], ["Svyatkovskiy", "Alexey", ""], ["Zanetti", "Marco", ""]]}, {"id": "1901.07160", "submitter": "Roberto Saltini", "authors": "Roberto Saltini and David Hyland-Wood", "title": "Correctness Analysis of IBFT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we analyse the correctness of Istanbul BFT (IBFT), which is a\nByzantine-fault-tolerant (BFT) proof-of-authority (PoA) blockchain consensus\nprotocol that ensures immediate finality. We show that the IBFT protocol does\nnot guarantee Byzantine-fault-tolerant consistency and liveness when operating\nin an eventually synchronous network, and we propose modifications to the\nprotocol to ensure both Byzantine-fault-tolerant consistency and liveness in\neventually synchronous settings.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 03:22:46 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 06:24:47 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Saltini", "Roberto", ""], ["Hyland-Wood", "David", ""]]}, {"id": "1901.07216", "submitter": "Bing Lin", "authors": "Bing Lin, Fangning Zhu, Jianshan Zhang, Jiaqing Chen, Xing Chen, Neal\n  N. Xiong, Jaime Lloret Mauri", "title": "A Time-driven Data Placement Strategy for a Scientific Workflow\n  Combining Edge Computing and Cloud Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared to traditional distributed computing environments such as grids,\ncloud computing provides a more cost-effective way to deploy scientific\nworkflows. Each task of a scientific workflow requires several large datasets\nthat are located in different datacenters from the cloud computing environment,\nresulting in serious data transmission delays. Edge computing reduces the data\ntransmission delays and supports the fixed storing manner for scientific\nworkflow private datasets, but there is a bottleneck in its storage capacity.\nIt is a challenge to combine the advantages of both edge computing and cloud\ncomputing to rationalize the data placement of scientific workflow, and\noptimize the data transmission time across different datacenters. Traditional\ndata placement strategies maintain load balancing with a given number of\ndatacenters, which results in a large data transmission time. In this study, a\nself-adaptive discrete particle swarm optimization algorithm with genetic\nalgorithm operators (GA-DPSO) was proposed to optimize the data transmission\ntime when placing data for a scientific workflow. This approach considered the\ncharacteristics of data placement combining edge computing and cloud computing.\nIn addition, it considered the impact factors impacting transmission delay,\nsuch as the band-width between datacenters, the number of edge datacenters, and\nthe storage capacity of edge datacenters. The crossover operator and mutation\noperator of the genetic algorithm were adopted to avoid the premature\nconvergence of the traditional particle swarm optimization algorithm, which\nenhanced the diversity of population evolution and effectively reduced the data\ntransmission time. The experimental results show that the data placement\nstrategy based on GA-DPSO can effectively reduce the data transmission time\nduring workflow execution combining edge computing and cloud computing.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 09:04:48 GMT"}, {"version": "v2", "created": "Thu, 24 Jan 2019 06:49:42 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Lin", "Bing", ""], ["Zhu", "Fangning", ""], ["Zhang", "Jianshan", ""], ["Chen", "Jiaqing", ""], ["Chen", "Xing", ""], ["Xiong", "Neal N.", ""], ["Mauri", "Jaime Lloret", ""]]}, {"id": "1901.07294", "submitter": "Tilo Wettig", "authors": "Nils Meyer, Peter Georg, Dirk Pleiter, Stefan Solbrig, Tilo Wettig", "title": "SVE-enabling Lattice QCD Codes", "comments": "6 pages", "journal-ref": "2018 IEEE International Conference on Cluster Computing (CLUSTER),\n  p. 623", "doi": "10.1109/CLUSTER.2018.00079", "report-no": null, "categories": "cs.DC hep-lat", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization of applications for supercomputers of the highest performance\nclass requires parallelization at multiple levels using different techniques.\nIn this contribution we focus on parallelization of particle physics\nsimulations through vector instructions. With the advent of the Scalable Vector\nExtension (SVE) ISA, future ARM-based processors are expected to provide a\nsignificant level of parallelism at this level.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 13:47:50 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Meyer", "Nils", ""], ["Georg", "Peter", ""], ["Pleiter", "Dirk", ""], ["Solbrig", "Stefan", ""], ["Wettig", "Tilo", ""]]}, {"id": "1901.07302", "submitter": "Pietro Ferraro", "authors": "Pietro Ferraro, Christopher King and Robert Shorten", "title": "IOTA-based Directed Acyclic Graphs without Orphans", "comments": "This paper has been published with the title \"On the stability of\n  unverified transactions in a DAG-based Distributed Ledger\"", "journal-ref": "in IEEE Transactions on Automatic Control, vol. 65, no. 9, pp.\n  3772-3783, Sept. 2020", "doi": "10.1109/TAC.2019.2950873", "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Directed Acylic Graphs (DAGs) are emerging as an attractive alternative to\ntraditional blockchain architectures for distributed ledger technology (DLT).\nIn particular DAG ledgers with stochastic attachment mechanisms potentially\noffer many advantages over blockchain, including scalability and faster\ntransaction speeds. However, the random nature of the attachment mechanism\ncoupled with the requirement of protection against double-spend transactions\nleaves open the possibility that not all transactions will be eventually\nvalidated. Such transactions are said to be orphaned, and will never be\nvalidated. Our principal contribution is to propose a simple modification to\nthe attachment mechanism for the Tangle (the IOTA DAG architecture). This\nmodification ensures that all transactions are validated in finite time, and\npreserves essential features of the popular Monte-Carlo selection algorithm. In\norder to demonstrate these results we derive a fluid approximation for the\nTangle (in the limit of infinite arrival rate) and prove that this fluid model\nexhibits the desired behavior. We also present simulations which validate the\nresults for finite arrival rates.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 01:57:02 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 19:37:28 GMT"}, {"version": "v3", "created": "Thu, 12 Nov 2020 10:59:43 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Ferraro", "Pietro", ""], ["King", "Christopher", ""], ["Shorten", "Robert", ""]]}, {"id": "1901.07306", "submitter": "Jie Xu", "authors": "Jie Xu, Wei Ding and Xiaoyan Hu", "title": "Most memory efficient distributed super points detection on core\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The super point, a host which communicates with lots of others, is a kind of\nspecial hosts gotten great focus. Mining super point at the edge of a network\nis the foundation of many network research fields. In this paper, we proposed\nthe most memory efficient super points detection scheme. This scheme contains a\nsuper points reconstruction algorithm called short estimator and a super points\nfilter algorithm called long estimator. Short estimator gives a super points\ncandidate list using thousands of bytes memory and long estimator improves the\naccuracy of detection result using millions of bytes memory. Combining short\nestimator and long estimator, our scheme acquires the highest accuracy using\nthe smallest memory than other algorithms. There is no data conflict and\nfloating operation in our scheme. This ensures that our scheme is suitable for\nparallel running and we deploy our scheme on a common GPU to accelerate\nprocessing speed. We also describe how to extend our algorithm to sliding time.\nExperiments on several real-world core network traffics show that our algorithm\nacquires the highest accuracy with only consuming littler than one-fifth memory\nof other algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 07:16:51 GMT"}, {"version": "v2", "created": "Sat, 26 Jan 2019 23:34:59 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Xu", "Jie", ""], ["Ding", "Wei", ""], ["Hu", "Xiaoyan", ""]]}, {"id": "1901.07309", "submitter": "Inayat Ali", "authors": "Inayat Ali, Sonia Sabir, Zahid Ullah", "title": "Internet of Things Security, Device Authentication and Access Control: A\n  Review", "comments": null, "journal-ref": "International Journal of Computer Science and Information Security\n  (IJCSIS), Vol. 14, No. 8, August 2016", "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet of Things (IoT) is one of the emerging technologies that has\ngrabbed the attention of researchers from academia and industry. The idea\nbehind Internet of things is the interconnection of internet enabled things or\ndevices to each other and to humans, to achieve some common goals. In near\nfuture IoT is expected to be seamlessly integrated into our environment and\nhuman will be wholly solely dependent on this technology for comfort and easy\nlife style. Any security compromise of the system will directly affect human\nlife. Therefore security and privacy of this technology is foremost important\nissue to resolve. In this paper we present a thorough study of security\nproblems in IoT and classify possible cyberattacks on each layer of IoT\narchitecture. We also discuss challenges to traditional security solutions such\nas cryptographic solutions, authentication mechanisms and key management in\nIoT. Device authentication and access controls is an essential area of IoT\nsecurity, which is not surveyed so far. We spent our efforts to bring the state\nof the art device authentication and access control techniques on a single\npaper.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 02:01:23 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Ali", "Inayat", ""], ["Sabir", "Sonia", ""], ["Ullah", "Zahid", ""]]}, {"id": "1901.07317", "submitter": "Jose Nunez-Yanez Dr", "authors": "William Beasley, Brenda Gatusch, Daniel Connolly-Taylor, Chenyuan\n  Teng, Asier Marzo and Jose Nunez-Yanez", "title": "High-Performance Ultrasonic Levitation with FPGA-based Phased Arrays", "comments": "Presented at HIP3ES, 2019", "journal-ref": null, "doi": null, "report-no": "HIP3ES/2019/2", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a flexible and self-contained platform for acoustic levitation\nresearch based on the Xilinx Zynq SoC using an array of ultrasonic emitters.\nThe platform employs an inexpensive ZedBoard and provides fast movement of the\nlevitated objects as well as object detection based on the produced echo.\nSeveral features available in the Zynq device are of benefit for this platform:\nhardware acceleration for the phase calculations, large number of parallel I/Os\nconnected through the FPGA Mezzanine connector (FMC), integrated ADC\ncapabilities to capture echo signals and ease of programmability due to a\nC-based design flow for both CPU and FPGA. A planar and spherical cap phased\narrays are created and we investigate the capabilities and limitations of the\ndifferent designs to improve the stability of the levitation process.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 12:29:34 GMT"}, {"version": "v2", "created": "Thu, 24 Jan 2019 17:30:56 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Beasley", "William", ""], ["Gatusch", "Brenda", ""], ["Connolly-Taylor", "Daniel", ""], ["Teng", "Chenyuan", ""], ["Marzo", "Asier", ""], ["Nunez-Yanez", "Jose", ""]]}, {"id": "1901.07335", "submitter": "Ben Blamey", "authors": "Ben Blamey, Fredrik Wrede, Johan Karlsson, Andreas Hellander, and\n  Salman Toor", "title": "Adapting The Secretary Hiring Problem for Optimal Hot-Cold Tier\n  Placement under Top-$K$ Workloads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Top-K queries are an established heuristic in information retrieval. This\npaper presents an approach for optimal tiered storage allocation under stream\nprocessing workloads using this heuristic: those requiring the analysis of only\nthe top-$K$ ranked most relevant, or most interesting, documents from a\nfixed-length stream, stream window, or batch job. In this workflow, documents\nare analyzed relevance with a user-specified interestingness function, on which\nthey are ranked, the top-$K$ being selected (and hence stored) for further\nprocessing. This workflow allows human in the loop systems, including\nsupervised machine learning, to prioritize documents. This scenario bears\nsimilarity to the classic Secretary Hiring Problem (SHP), and the expected rate\nof document writes, and document lifetime, can be modelled as a function of\ndocument index. We present parameter-based algorithms for storage tier\nplacement, minimizing document storage and transport costs. We show that\noptimal parameter values are a function of these costs. It is possible to model\napplication IO behaviour a priori for this class of workloads. When combined\nwith tiered storage, the tractability of the probabilistic model of IO makes it\neasy to optimize tier allocation, yielding simple analytic solutions. This\ncontrasts with (often complex) existing work on tiered storage optimization,\nwhich is either tightly coupled to specific use cases, or requires active\nmonitoring of application IO load (a reactive approach) -- ill-suited to\nlong-running or one-off operations common in the scientific computing domain.We\nvalidate our model with a trace-driven simulation of a bio-chemical model\nexploration, and give worked examples for two cloud storage case studies.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 14:38:15 GMT"}, {"version": "v2", "created": "Tue, 12 Mar 2019 13:05:19 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Blamey", "Ben", ""], ["Wrede", "Fredrik", ""], ["Karlsson", "Johan", ""], ["Hellander", "Andreas", ""], ["Toor", "Salman", ""]]}, {"id": "1901.07418", "submitter": "Konstantinos Konstantinidis", "authors": "Konstantinos Konstantinidis and Aditya Ramamoorthy", "title": "CAMR: Coded Aggregated MapReduce", "comments": "6 pages, 2 figures, full paper for ISIT 2019 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many big data algorithms executed on MapReduce-like systems have a shuffle\nphase that often dominates the overall job execution time. Recent work has\ndemonstrated schemes where the communication load in the shuffle phase can be\ntraded off for the computation load in the map phase. In this work, we focus on\na class of distributed algorithms, broadly used in deep learning, where\nintermediate computations of the same task can be combined. Even though prior\ntechniques reduce the communication load significantly, they require a number\nof jobs that grows exponentially in the system parameters. This limitation is\ncrucial and may diminish the load gains as the algorithm scales. We propose a\nnew scheme which achieves the same load as the state-of-the-art while ensuring\nthat the number of jobs as well as the number of subfiles that the data set\nneeds to be split into remain small.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 15:35:11 GMT"}, {"version": "v2", "created": "Thu, 25 Apr 2019 18:53:08 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Konstantinidis", "Konstantinos", ""], ["Ramamoorthy", "Aditya", ""]]}, {"id": "1901.07497", "submitter": "Jiaxiao Zheng", "authors": "Jiaxiao Zheng, Gustavo de Veciana", "title": "Elastic Multi-resource Network Slicing: Can Protection Lead to Improved\n  Performance?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to meet the performance/privacy requirements of future\ndata-intensive mobile applications, e.g., self-driving cars, mobile data\nanalytics, and AR/VR, service providers are expected to draw on shared\nstorage/computation/connectivity resources at the network \"edge\". To be\ncost-effective, a key functional requirement for such infrastructure is\nenabling the sharing of heterogeneous resources amongst tenants/service\nproviders supporting spatially varying and dynamic user demands. This paper\nproposes a resource allocation criterion, namely, Share Constrained Slicing\n(SCS), for slices allocated predefined shares of the network's resources, which\nextends the traditional alpha-fairness criterion, by striking a balance among\ninter- and intra-slice fairness vs. overall efficiency. We show that SCS has\nseveral desirable properties including slice-level protection, envyfreeness,\nand load driven elasticity. In practice, mobile users' dynamics could make the\ncost of implementing SCS high, so we discuss the feasibility of using a simpler\n(dynamically) weighted max-min as a surrogate resource allocation scheme. For a\nsetting with stochastic loads and elastic user requirements, we establish a\nsufficient condition for the stability of the associated coupled network\nsystem. Finally, and perhaps surprisingly, we show via extensive simulations\nthat while SCS (and/or the surrogate weighted max-min allocation) provides\ninter-slice protection, they can achieve improved job delay and/or perceived\nthroughput, as compared to other weighted max-min based allocation schemes\nwhose intra-slice weight allocation is not share-constrained, e.g., traditional\nmax-min or discriminatory processor sharing.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 18:16:25 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Zheng", "Jiaxiao", ""], ["de Veciana", "Gustavo", ""]]}, {"id": "1901.07499", "submitter": "Bhargav Gokalgandhi", "authors": "Bhargav Gokalgandhi, Christina Segerholm, Nilanjan Paul, Ivan Seskar", "title": "Accelerating Channel Estimation and Demodulation of Uplink OFDM symbols\n  for Large Scale Antenna Systems using GPU", "comments": "This paper has been accepted at IEEE ICNC 2019 conference. The IEEE\n  copyright notice has been attached to the paper. DOI and full reference for\n  the paper will be added after it is published", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increase in the number of antennas in the front-end increases the volume of\ndata to be processed at the back-end. This establishes a need for acceleration\nin back-end processing. To solve the issue of high volume data processing at\nback-end, a GPU is utilized. Acceleration for Least Squares channel estimation\nand demodulation of uplink OFDM symbols is provided by using a combination of\nCPU and GPU at the back-end. Single user uplink scenario is implemented in near\nreal-time manner using the USRP platform present in the Large scale antenna\nsystems in ORBIT Testbed. The number of antennas and FFT length are varied to\nprovide different scenarios for comparison. The performance of both CPU and GPU\nis compared for each process.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 18:19:50 GMT"}, {"version": "v2", "created": "Wed, 23 Jan 2019 18:56:40 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Gokalgandhi", "Bhargav", ""], ["Segerholm", "Christina", ""], ["Paul", "Nilanjan", ""], ["Seskar", "Ivan", ""]]}, {"id": "1901.07715", "submitter": "Amit Kumar Nath", "authors": "Huansong Fu, Yue Zhu, Amit Kumar Nath, Md. Muhib Khan, Weikuan Yu", "title": "Enhancing MapReduce Fault Recovery Through Binocular Speculation", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MapReduce speculation plays an important role in finding potential task\nstragglers and failures. But a tacit dichotomy exists in MapReduce due to its\ninherent two-phase (map and reduce) management scheme in which map tasks and\nreduce tasks have distinctly different execution behaviors, yet reduce tasks\nare dependent on the results of map tasks. We reveal that speculation policies\nfor fault handling in MapReduce do not recognize this dichotomy between map and\nreduce tasks, which leads to an issue of speculation myopia for MapReduce fault\nrecovery. These issues cause significant performance degradation upon network\nand node failures. To address the speculation myopia caused by MapReduce\ndichotomy, we introduce a new scheme called binocular speculation to help\nMapReduce increase its assessment scope for speculation. As part of the scheme,\nwe also design three component techniques including neighborhood glance,\ncollective speculation and speculative rollback. Our evaluation shows that,\nwith these techniques, binocular speculation can increase the coordination of\nmap and reduce phases, and enhance the efficiency of MapReduce fault recovery.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 04:15:47 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Fu", "Huansong", ""], ["Zhu", "Yue", ""], ["Nath", "Amit Kumar", ""], ["Khan", "Md. Muhib", ""], ["Yu", "Weikuan", ""]]}, {"id": "1901.08151", "submitter": "Richard Hill Prof", "authors": "Hussain Al-Aqrabi, Lu Liu, Richard Hill, Nick Antonopoulos", "title": "Cloud BI: Future of Business Intelligence in the Cloud", "comments": "12 pages, Journal of Computer and System Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing is gradually gaining popularity among businesses due to its\ndistinct advantages over self-hosted IT infrastructures. Business Intelligence\n(BI) is a highly resource intensive system requiring large-scale parallel\nprocessing and significant storage capacities to host data warehouses. In\nself-hosted environments it was feared that BI will eventually face a resource\ncrunch situation because it will not be feasible for companies to keep adding\nresources to host a neverending expansion of data warehouses and the online\nanalytical processing (OLAP) demands on the underlying networking. Cloud\ncomputing has instigated a new hope for future prospects of BI. However, how\nwill BI be implemented on cloud and how will the traffic and demand profile\nlook like? This research attempts to answer these key questions in regards to\ntaking BI to the cloud. The cloud hosting of BI has been demonstrated with the\nhelp of a simulation on OPNET comprising a cloud model with multiple OLAP\napplication servers applying parallel query loads on an array of servers\nhosting relational databases. The simulation results have reflected that true\nand extensible parallel processing of database servers on the cloud can\nefficiently process OLAP application demands on cloud computing. Hence, the BI\ndesigner needs to plan for a highly partitioned database running on massively\nparallel database servers in which, each server hosts at least one partition of\nthe underlying database serving the OLAP demands.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 22:11:24 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Al-Aqrabi", "Hussain", ""], ["Liu", "Lu", ""], ["Hill", "Richard", ""], ["Antonopoulos", "Nick", ""]]}, {"id": "1901.08166", "submitter": "Sinong Wang", "authors": "Sinong Wang, Jiashang Liu and Ness Shroff", "title": "Fundamental Limits of Approximate Gradient Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been established that when the gradient coding problem is distributed\namong $n$ servers, the computation load (number of stored data partitions) of\neach worker is at least $s+1$ in order to resists $s$ stragglers. This scheme\nincurs a large overhead when the number of stragglers $s$ is large. In this\npaper, we focus on a new framework called \\emph{approximate gradient coding} to\nmitigate stragglers in distributed learning. We show that, to exactly recover\nthe gradient with high probability, the computation load is lower bounded by\n$O(\\log(n)/\\log(n/s))$. We also propose a code that exactly matches such lower\nbound. We identify a fundamental three-fold tradeoff for any approximate\ngradient coding scheme $d\\geq O(\\log(1/\\epsilon)/\\log(n/s))$, where $d$ is the\ncomputation load, $\\epsilon$ is the error of gradient. We give an explicit code\nconstruction based on random edge removal process that achieves the derived\ntradeoff. We implement our schemes and demonstrate the advantage of the\napproaches over the current fastest gradient coding strategies.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 23:26:14 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Wang", "Sinong", ""], ["Liu", "Jiashang", ""], ["Shroff", "Ness", ""]]}, {"id": "1901.08200", "submitter": "Zaoxing Liu", "authors": "Zaoxing Liu, Zhihao Bai, Zhenming Liu, Xiaozhou Li, Changhoon Kim,\n  Vladimir Braverman, Xin Jin, Ion Stoica", "title": "DistCache: Provable Load Balancing for Large-Scale Storage Systems with\n  Distributed Caching", "comments": "Technical Report. Conference version accepted to USENIX FAST'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Load balancing is critical for distributed storage to meet strict\nservice-level objectives (SLOs). It has been shown that a fast cache can\nguarantee load balancing for a clustered storage system. However, when the\nsystem scales out to multiple clusters, the fast cache itself would become the\nbottleneck. Traditional mechanisms like cache partition and cache replication\neither result in load imbalance between cache nodes or have high overhead for\ncache coherence.\n  We present DistCache, a new distributed caching mechanism that provides\nprovable load balancing for large-scale storage systems. DistCache co-designs\ncache allocation with cache topology and query routing. The key idea is to\npartition the hot objects with independent hash functions between cache nodes\nin different layers, and to adaptively route queries with the\npower-of-two-choices. We prove that DistCache enables the cache throughput to\nincrease linearly with the number of cache nodes, by unifying techniques from\nexpander graphs, network flows, and queuing theory. DistCache is a general\nsolution that can be applied to many storage systems. We demonstrate the\nbenefits of DistCache by providing the design, implementation, and evaluation\nof the use case for emerging switch-based caching.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 02:23:36 GMT"}, {"version": "v2", "created": "Thu, 14 Feb 2019 20:03:14 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Liu", "Zaoxing", ""], ["Bai", "Zhihao", ""], ["Liu", "Zhenming", ""], ["Li", "Xiaozhou", ""], ["Kim", "Changhoon", ""], ["Braverman", "Vladimir", ""], ["Jin", "Xin", ""], ["Stoica", "Ion", ""]]}, {"id": "1901.08215", "submitter": "Jiaqi Zhang", "authors": "Jiaqi Zhang and Keyou You", "title": "Fully Asynchronous Distributed Optimization with Linear Convergence in\n  Directed Networks", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the distributed optimization problem, the goal of which is to\nminimize the sum of local objective functions over a directed network. Though\nit has been widely studied recently, most of the existing algorithms are\ndesigned for synchronized or randomly activated implementation, which may\ncreate deadlocks in practice. In sharp contrast, we propose a \\emph{fully}\nasynchronous push-pull gradient algorithm (APPG) where each node updates\nwithout waiting for any other node by using (possibly stale) information from\nneighbors. Thus, it is both deadlock-free and robust to any bounded\ncommunication delay. Moreover, we construct two novel augmented networks to\ntheoretically evaluate its performance from the worst-case point of view and\nshow that if local functions have Lipschitz-continuous gradients and their sum\nsatisfies the Polyak-\\L ojasiewicz condition (convexity is not required), each\nnode of APPG converges to the same optimal solution at a linear rate of\n$\\mathcal{O}(\\lambda^k)$, where $\\lambda\\in(0,1)$ and the virtual counter $k$\nincreases by one no matter which node updates. This largely elucidates its\nlinear speedup efficiency and shows its advantage over the synchronous version.\nFinally, the performance of APPG is numerically validated via a logistic\nregression problem on the \\emph{Covertype} dataset.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 03:31:45 GMT"}, {"version": "v2", "created": "Mon, 18 Feb 2019 11:56:55 GMT"}, {"version": "v3", "created": "Mon, 4 Jan 2021 11:54:50 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Zhang", "Jiaqi", ""], ["You", "Keyou", ""]]}, {"id": "1901.08227", "submitter": "Jianqiao Wangni", "authors": "Jianqiao Wangni, Ke Li, Jianbo Shi, Jitendra Malik", "title": "Trajectory Normalized Gradients for Distributed Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, researchers proposed various low-precision gradient compression,\nfor efficient communication in large-scale distributed optimization. Based on\nthese work, we try to reduce the communication complexity from a new direction.\nWe pursue an ideal bijective mapping between two spaces of gradient\ndistribution, so that the mapped gradient carries greater information entropy\nafter the compression. In our setting, all servers should share a reference\ngradient in advance, and they communicate via the normalized gradients, which\nare the subtraction or quotient, between current gradients and the reference.\nTo obtain a reference vector that yields a stronger signal-to-noise ratio,\ndynamically in each iteration, we extract and fuse information from the past\ntrajectory in hindsight, and search for an optimal reference for compression.\nWe name this to be the trajectory-based normalized gradients (TNG). It bridges\nthe research from different societies, like coding, optimization, systems, and\nlearning. It is easy to implement and can universally combine with existing\nalgorithms. Our experiments on benchmarking hard non-convex functions, convex\nproblems like logistic regression demonstrate that TNG is more\ncompression-efficient for communication of distributed optimization of general\nfunctions.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 04:24:31 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Wangni", "Jianqiao", ""], ["Li", "Ke", ""], ["Shi", "Jianbo", ""], ["Malik", "Jitendra", ""]]}, {"id": "1901.08326", "submitter": "Mohamed Lamine Lamali", "authors": "Mohamed Lamine Lamali (LaBRI), Simon Lassourreuille (LaBRI), Stephan\n  Kunne (GALaC - LRI), Johanne Cohen (LRI)", "title": "A stack-vector routing protocol for automatic tunneling", "comments": null, "journal-ref": "IEEE INFOCOM 2019, Apr 2019, Paris, France.\n  https://infocom2019.ieee-infocom.org/", "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a network, a tunnel is a part of a path where a protocol is encapsulated\nin another one. A tunnel starts with an encapsulation and ends with the\ncorresponding decapsulation. Several tunnels can be nested at some stage,\nforming a protocol stack. Tunneling is very important nowadays and it is\ninvolved in several tasks: IPv4/IPv6 transition, VPNs, security (IPsec, onion\nrouting), etc. However, tunnel establishment is mainly performed manually or by\nscript, which present obvious scalability issues. Some works attempt to\nautomate a part of the process (e.g., TSP, ISATAP, etc.). However, the\ndetermination of the tunnel(s) endpoints is not fully automated, especially in\nthe case of an arbitrary number of nested tunnels. The lack of routing\nprotocols performing automatic tunneling is due to the unavailability of path\ncomputation algorithms taking into account encapsulations and decapsulations.\nThere is a polynomial centralized algorithm to perform the task. However, to\nthe best of our knowledge, no fully distributed path computation algorithm is\nknown. Here, we propose the first fully distributed algorithm for path\ncomputation with automatic tunneling, i.e., taking into account encapsulation,\ndecapsulation and conversion of protocols. Our algorithm is a generalization of\nthe distributed Bellman-Ford algorithm, where the distance vector is replaced\nby a protocol stack vector. This allows to know how to route a packet with some\nprotocol stack. We prove that the messages size of our algorithm is polynomial,\neven if the shortest path can be of exponential length. We also prove that the\nalgorithm converges after a polynomial number of steps in a synchronized\nsetting. We adapt our algorithm into a proto-protocol for routing with\nautomatic tunneling and we show its efficiency through simulations.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 10:08:18 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Lamali", "Mohamed Lamine", "", "LaBRI"], ["Lassourreuille", "Simon", "", "LaBRI"], ["Kunne", "Stephan", "", "GALaC - LRI"], ["Cohen", "Johanne", "", "LRI"]]}, {"id": "1901.08435", "submitter": "Egor Zuev", "authors": "Egor Zuev", "title": "Mokka: BFT consensus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mokka is a PC (CAP theorem) log-less BFT consensus algorithm for reaching the\nconsensus about a certain value in open networks. This algorithm has some\ncommon approaches nested from RAFT, but its nature and design make Mokka a\nbetter solution in the following cases: trustless environment (where voting\nshould be somehow validated), and where network split attack is possible.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 14:49:31 GMT"}, {"version": "v2", "created": "Wed, 8 May 2019 16:54:44 GMT"}, {"version": "v3", "created": "Thu, 5 Sep 2019 20:10:22 GMT"}, {"version": "v4", "created": "Mon, 13 Apr 2020 18:31:09 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Zuev", "Egor", ""]]}, {"id": "1901.08460", "submitter": "Valentina Zantedeschi Dr", "authors": "Valentina Zantedeschi, Aur\\'elien Bellet, Marc Tommasi", "title": "Fully Decentralized Joint Learning of Personalized Models and\n  Collaboration Graphs", "comments": "To appear in the proceedings of the 23rd International Conference on\n  Artificial Intelligence and Statistics (AISTATS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the fully decentralized machine learning scenario where many\nusers with personal datasets collaborate to learn models through local\npeer-to-peer exchanges, without a central coordinator. We propose to train\npersonalized models that leverage a collaboration graph describing the\nrelationships between user personal tasks, which we learn jointly with the\nmodels. Our fully decentralized optimization procedure alternates between\ntraining nonlinear models given the graph in a greedy boosting manner, and\nupdating the collaboration graph (with controlled sparsity) given the models.\nThroughout the process, users exchange messages only with a small number of\npeers (their direct neighbors when updating the models, and a few random users\nwhen updating the graph), ensuring that the procedure naturally scales with the\nnumber of users. Overall, our approach is communication-efficient and avoids\nexchanging personal data. We provide an extensive analysis of the convergence\nrate, memory and communication complexity of our approach, and demonstrate its\nbenefits compared to competing techniques on synthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 15:44:42 GMT"}, {"version": "v2", "created": "Fri, 25 Jan 2019 13:33:17 GMT"}, {"version": "v3", "created": "Mon, 3 Jun 2019 21:56:19 GMT"}, {"version": "v4", "created": "Thu, 26 Mar 2020 09:34:43 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Zantedeschi", "Valentina", ""], ["Bellet", "Aur\u00e9lien", ""], ["Tommasi", "Marc", ""]]}, {"id": "1901.08666", "submitter": "Boris Glavic", "authors": "Jason Arnold and Boris Glavic and Ioan Raicu", "title": "HRDBMS: Combining the Best of Modern and Traditional Relational\n  Databases", "comments": "Oral Ph.D. Qualifier Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  HRDBMS is a novel distributed relational database that uses a hybrid model\ncombining the best of traditional distributed relational databases and Big Data\nanalytics platforms such as Hive. This allows HRDBMS to leverage years worth of\nresearch regarding query optimization, while also taking advantage of the\nscalability of Big Data platforms. The system uses an execution framework that\nis tailored for relational processing, thus addressing some of the performance\nchallenges of running SQL on top of platforms such as MapReduce and Spark.\nThese include excessive materialization of intermediate results, lack of a\nglobal cost-based optimization, unnecessary sorting, lack of index support, no\nstatistics, no support for DML and ACID, and excessive communication caused by\nthe rigid communication patterns enforced by these platforms.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 22:20:46 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Arnold", "Jason", ""], ["Glavic", "Boris", ""], ["Raicu", "Ioan", ""]]}, {"id": "1901.08705", "submitter": "Hatef Monajemi Dr.", "authors": "Hatef Monajemi, Riccardo Murri, Eric Jonas, Percy Liang, Victoria\n  Stodden and David L. Donoho", "title": "Ambitious Data Science Can Be Painless", "comments": "Submitted to Harvard Data Science Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern data science research can involve massive computational\nexperimentation; an ambitious PhD in computational fields may do experiments\nconsuming several million CPU hours. Traditional computing practices, in which\nresearchers use laptops or shared campus-resident resources, are inadequate for\nexperiments at the massive scale and varied scope that we now see in data\nscience. On the other hand, modern cloud computing promises seemingly unlimited\ncomputational resources that can be custom configured, and seems to offer a\npowerful new venue for ambitious data-driven science. Exploiting the cloud\nfully, the amount of work that could be completed in a fixed amount of time can\nexpand by several orders of magnitude.\n  As potentially powerful as cloud-based experimentation may be in the\nabstract, it has not yet become a standard option for researchers in many\nacademic disciplines. The prospect of actually conducting massive computational\nexperiments in today's cloud systems confronts the potential user with daunting\nchallenges. Leading considerations include: (i) the seeming complexity of\ntoday's cloud computing interface, (ii) the difficulty of executing an\noverwhelmingly large number of jobs, and (iii) the difficulty of monitoring and\ncombining a massive collection of separate results. Starting a massive\nexperiment `bare-handed' seems therefore highly problematic and prone to rapid\n`researcher burn out'.\n  New software stacks are emerging that render massive cloud experiments\nrelatively painless. Such stacks simplify experimentation by systematizing\nexperiment definition, automating distribution and management of tasks, and\nallowing easy harvesting of results and documentation. In this article, we\ndiscuss several painless computing stacks that abstract away the difficulties\nof massive experimentation, thereby allowing a proliferation of ambitious\nexperiments for scientific discovery.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 01:10:55 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Monajemi", "Hatef", ""], ["Murri", "Riccardo", ""], ["Jonas", "Eric", ""], ["Liang", "Percy", ""], ["Stodden", "Victoria", ""], ["Donoho", "David L.", ""]]}, {"id": "1901.08716", "submitter": "Anindya Bijoy Das", "authors": "Anindya Bijoy Das and Aditya Ramamoorthy", "title": "Distributed Matrix-Vector Multiplication: A Convolutional Coding\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed computing systems are well-known to suffer from the problem of\nslow or failed nodes; these are referred to as stragglers. Straggler mitigation\n(for distributed matrix computations) has recently been investigated from the\nstandpoint of erasure coding in several works. In this work we present a\nstrategy for distributed matrix-vector multiplication based on convolutional\ncoding. Our scheme can be decoded using a low-complexity peeling decoder. The\nrecovery process enjoys excellent numerical stability as compared to\nReed-Solomon coding based approaches (which exhibit significant problems owing\ntheir badly conditioned decoding matrices). Finally, our schemes are better\nmatched to the practically important case of sparse matrix-vector\nmultiplication as compared to many previous schemes. Extensive simulation\nresults corroborate our findings.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 02:36:44 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Das", "Anindya Bijoy", ""], ["Ramamoorthy", "Aditya", ""]]}, {"id": "1901.08806", "submitter": "Huynh Tu Dang Mr", "authors": "Huynh Tu Dang, Pietro Bressana, Han Wang, Ki Suh Lee, Noa Zilberman,\n  Hakim Weatherspoon, Marco Canini, Fernando Pedone, and Robert Soul\\'e", "title": "Partitioned Paxos via the Network Data Plane", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": "USI-INF-TR-2019-01", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consensus protocols are the foundation for building fault-tolerant,\ndistributed systems, and services. They are also widely acknowledged as\nperformance bottlenecks. Several recent systems have proposed accelerating\nthese protocols using the network data plane. But, while network-accelerated\nconsensus shows great promise, current systems suffer from an important\nlimitation: they assume that the network hardware also accelerates the\napplication itself. Consequently, they provide a specialized replicated\nservice, rather than providing a general-purpose high-performance consensus\nthat fits any off-the-shelf application.\n  To address this problem, this paper proposes Partitioned Paxos, a novel\napproach to network-accelerated consensus. The key insight behind Partitioned\nPaxos is to separate the two aspects of Paxos, agreement, and execution, and\noptimize them separately. First, Partitioned Paxos uses the network forwarding\nplane to accelerate agreement. Then, it uses state partitioning and\nparallelization to accelerate execution at the replicas. Our experiments show\nthat using this combination of data plane acceleration and parallelization,\nPartitioned Paxos is able to provide at least x3 latency improvement and x11\nthroughput improvement for a replicated instance of a RocksDB key-value store.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 10:02:51 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Dang", "Huynh Tu", ""], ["Bressana", "Pietro", ""], ["Wang", "Han", ""], ["Lee", "Ki Suh", ""], ["Zilberman", "Noa", ""], ["Weatherspoon", "Hakim", ""], ["Canini", "Marco", ""], ["Pedone", "Fernando", ""], ["Soul\u00e9", "Robert", ""]]}, {"id": "1901.09235", "submitter": "Thomas Moreau", "authors": "Thomas Moreau, Alexandre Gramfort", "title": "Distributed Convolutional Dictionary Learning (DiCoDiLe): Pattern\n  Discovery in Large Images and Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional dictionary learning (CDL) estimates shift invariant basis\nadapted to multidimensional data. CDL has proven useful for image denoising or\ninpainting, as well as for pattern discovery on multivariate signals. As\nestimated patterns can be positioned anywhere in signals or images,\noptimization techniques face the difficulty of working in extremely high\ndimensions with millions of pixels or time samples, contrarily to standard\npatch-based dictionary learning. To address this optimization problem, this\nwork proposes a distributed and asynchronous algorithm, employing locally\ngreedy coordinate descent and an asynchronous locking mechanism that does not\nrequire a central server. This algorithm can be used to distribute the\ncomputation on a number of workers which scales linearly with the encoded\nsignal's size. Experiments confirm the scaling properties which allows us to\nlearn patterns on large scales images from the Hubble Space Telescope.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jan 2019 15:48:38 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Moreau", "Thomas", ""], ["Gramfort", "Alexandre", ""]]}, {"id": "1901.09312", "submitter": "Mohsen Amini Salehi", "authors": "James Gentry, Chavit Denninnart, Mohsen Amini Salehi", "title": "Robust Dynamic Resource Allocation via Probabilistic Task Pruning in\n  Heterogeneous Computing Systems", "comments": null, "journal-ref": "33rd IEEE International Parallel & Distributed Processing\n  Symposium, 2019, (IPDPS '19)", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In heterogeneous distributed computing (HC) systems, diversity can exist in\nboth computational resources and arriving tasks. In an inconsistently\nheterogeneous computing system, task types have different execution times on\nheterogeneous machines. A method is required to map arriving tasks to machines\nbased on machine availability and performance, maximizing the number of tasks\nmeeting deadlines (defined as robustness). For tasks with hard deadlines (eg\nthose in live video streaming), tasks that miss their deadlines are dropped.\nThe problem investigated in this research is maximizing the robustness of an\noversubscribed HC system. A way to maximize this robustness is to prune (ie\ndefer or drop) tasks with low probability of meeting their deadlines to\nincrease the probability of other tasks meeting their deadlines. In this paper,\nwe first provide a mathematical model to estimate a task's probability of\nmeeting its deadline in the presence of task dropping. We then investigate\nmethods for engaging probabilistic dropping and we find thresholds for dropping\nand deferring. Next, we develop a pruning-aware mapping heuristic and extend it\nto engender fairness across various task types. We show the cost benefit of\nusing probabilistic pruning in an HC system. Simulation results, harnessing a\nselection of mapping heuristics, show efficacy of the pruning mechanism in\nimproving robustness (on average by 25%) and cost in an oversubscribed HC\nsystem by up to 40%.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jan 2019 04:10:01 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Gentry", "James", ""], ["Denninnart", "Chavit", ""], ["Salehi", "Mohsen Amini", ""]]}, {"id": "1901.09339", "submitter": "Bin Tang", "authors": "Haozhao Wang, Song Guo, Bin Tang, Ruixuan Li, Chengjie Li", "title": "Heterogeneity-aware Gradient Coding for Straggler Tolerance", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient descent algorithms are widely used in machine learning. In order to\ndeal with huge volume of data, we consider the implementation of gradient\ndescent algorithms in a distributed computing setting where multiple workers\ncompute the gradient over some partial data and the master node aggregates\ntheir results to obtain the gradient over the whole data. However, its\nperformance can be severely affected by straggler workers. Recently, some\ncoding-based approaches are introduced to mitigate the straggler problem, but\nthey are efficient only when the workers are homogeneous, i.e., having the same\ncomputation capabilities. In this paper, we consider that the workers are\nheterogeneous which are common in modern distributed systems. We propose a\nnovel heterogeneity-aware gradient coding scheme which can not only tolerate a\npredetermined number of stragglers but also fully utilize the computation\ncapabilities of heterogeneous workers. We show that this scheme is optimal when\nthe computation capabilities of workers are estimated accurately. A variant of\nthis scheme is further proposed to improve the performance when the estimations\nof the computation capabilities are not so accurate. We conduct our schemes for\ngradient descent based image classification on QingCloud clusters. Evaluation\nresults show that our schemes can reduce the whole computation time by up to\n$3\\times$ compared with a state-of-the-art coding scheme.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jan 2019 09:05:50 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Wang", "Haozhao", ""], ["Guo", "Song", ""], ["Tang", "Bin", ""], ["Li", "Ruixuan", ""], ["Li", "Chengjie", ""]]}, {"id": "1901.09367", "submitter": "Nicolas Loizou", "authors": "Filip Hanzely, Jakub Kone\\v{c}n\\'y, Nicolas Loizou, Peter Richt\\'arik,\n  Dmitry Grishchenko", "title": "A Privacy Preserving Randomized Gossip Algorithm via Controlled Noise\n  Insertion", "comments": "NeurIPS 2018, Privacy Preserving Machine Learning Workshop (camera\n  ready version). The full-length paper, which includes a number of additional\n  algorithms and results (including proofs of statements and experiments), is\n  available in arXiv:1706.07636", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG cs.MA cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a randomized gossip algorithm for solving the average\nconsensus problem while at the same time protecting the information about the\ninitial private values stored at the nodes. We give iteration complexity bounds\nfor the method and perform extensive numerical experiments.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jan 2019 12:40:31 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Hanzely", "Filip", ""], ["Kone\u010dn\u00fd", "Jakub", ""], ["Loizou", "Nicolas", ""], ["Richt\u00e1rik", "Peter", ""], ["Grishchenko", "Dmitry", ""]]}, {"id": "1901.09671", "submitter": "Zachary Charles", "authors": "Hongyi Wang, Zachary Charles, Dimitris Papailiopoulos", "title": "ErasureHead: Distributed Gradient Descent without Delays Using\n  Approximate Gradient Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.IT math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ErasureHead, a new approach for distributed gradient descent (GD)\nthat mitigates system delays by employing approximate gradient coding. Gradient\ncoded distributed GD uses redundancy to exactly recover the gradient at each\niteration from a subset of compute nodes. ErasureHead instead uses approximate\ngradient codes to recover an inexact gradient at each iteration, but with\nhigher delay tolerance. Unlike prior work on gradient coding, we provide a\nperformance analysis that combines both delay and convergence guarantees. We\nestablish that down to a small noise floor, ErasureHead converges as quickly as\ndistributed GD and has faster overall runtime under a probabilistic delay\nmodel. We conduct extensive experiments on real world datasets and distributed\nclusters and demonstrate that our method can lead to significant speedups over\nboth standard and gradient coded GD.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 14:21:32 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Wang", "Hongyi", ""], ["Charles", "Zachary", ""], ["Papailiopoulos", "Dimitris", ""]]}, {"id": "1901.09716", "submitter": "Henriette R\\\"oger", "authors": "Henriette R\\\"oger and Ruben Mayer", "title": "A Comprehensive Survey on Parallelization and Elasticity in Stream\n  Processing", "comments": "37 Pages, to be published in ACM Computing Surveys", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stream Processing (SP) has evolved as the leading paradigm to process and\ngain value from the high volume of streaming data produced e.g. in the domain\nof the Internet of Things. An SP system is a middleware that deploys a network\nof operators between data sources, such as sensors, and the consuming\napplications. SP systems typically face intense and highly dynamic data\nstreams. Parallelization and elasticity enables SP systems to process these\nstreams with continuously high quality of service. The current research\nlandscape provides a broad spectrum of methods for parallelization and\nelasticity in SP. Each method makes specific assumptions and focuses on\nparticular aspects of the problem. However, the literature lacks a\ncomprehensive overview and categorization of the state of the art in SP\nparallelization and elasticity, which is necessary to consolidate the state of\nthe research and to plan future research directions on this basis. Therefore,\nin this survey, we study the literature and develop a classification of current\nmethods for both parallelization and elasticity in SP systems.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 14:56:49 GMT"}, {"version": "v2", "created": "Tue, 29 Jan 2019 14:25:03 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["R\u00f6ger", "Henriette", ""], ["Mayer", "Ruben", ""]]}, {"id": "1901.09777", "submitter": "Kazuyuki Shudo", "authors": "Yusuke Aoki, Kai Otsuki, Takeshi Kaneko, Ryohei Banno, Kazuyuki Shudo", "title": "SimBlock: A Blockchain Network Simulator", "comments": "Proc. 2nd Workshop on Cryptocurrencies and Blockchains for\n  Distributed Systems (CryBlock 2019) (in conj. with INFOCOM 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain, which is a technology for distributedly managing ledger\ninformation over multiple nodes without a centralized system, has elicited\nincreasing attention. Performing experiments on actual blockchains are\ndifficult because a large number of nodes in wide areas are necessary. In this\nstudy, we developed a blockchain network simulator SimBlock for such\nexperiments. Unlike the existing simulators, SimBlock can easily change\nbehavior of node, so that it enables to investigate the influence of nodes'\nbehavior on blockchains. We compared some simulation results with the measured\nvalues in actual blockchains to demonstrate the validity of this simulator.\nFurthermore, to show practical usage, we conducted two experiments which\nclarify the influence of neighbor node selection algorithms and relay networks\non the block propagation time. The simulator could depict the effects of the\ntwo techniques on block propagation time. The simulator will be publicly\navailable in a few months.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 16:25:32 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2019 10:05:58 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Aoki", "Yusuke", ""], ["Otsuki", "Kai", ""], ["Kaneko", "Takeshi", ""], ["Banno", "Ryohei", ""], ["Shudo", "Kazuyuki", ""]]}, {"id": "1901.09782", "submitter": "Saverio Giallorenzo", "authors": "Mario Bravetti, Saverio Giallorenzo, Jacopo Mauro, Iacopo Talevi, and\n  Gianluigi Zavattaro", "title": "Optimal and Automated Deployment for Microservices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microservices are highly modular and scalable Service Oriented Architectures.\nThey underpin automated deployment practices like Continuous Deployment and\nAutoscaling. In this paper, we formalize these practices and show that\nautomated deployment - proven undecidable in the general case - is\nalgorithmically treatable for microservices. Our key assumption is that the\nconfiguration life-cycle of a microservice is split into two phases: (i)\ncreation, which entails establishing initial connections with already available\nmicroservices, and (ii) subsequent binding/unbinding with other microservices.\nTo illustrate the applicability of our approach, we implement an automatic\noptimal deployment tool and compute deployment plans for a realistic\nmicroservice architecture, modeled in the Abstract Behavioral Specification\n(ABS) language.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 16:30:22 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Bravetti", "Mario", ""], ["Giallorenzo", "Saverio", ""], ["Mauro", "Jacopo", ""], ["Talevi", "Iacopo", ""], ["Zavattaro", "Gianluigi", ""]]}, {"id": "1901.09865", "submitter": "Hadrien Hendrikx", "authors": "Hadrien Hendrikx, Francis Bach and Laurent Massouli\\'e", "title": "Asynchronous Accelerated Proximal Stochastic Gradient for Strongly\n  Convex Distributed Finite Sums", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the problem of minimizing the sum of strongly convex\nfunctions split over a network of $n$ nodes. We propose the decentralized and\nasynchronous algorithm ADFS to tackle the case when local functions are\nthemselves finite sums with $m$ components. ADFS converges linearly when local\nfunctions are smooth, and matches the rates of the best known finite sum\nalgorithms when executed on a single machine. On several machines, ADFS enjoys\na $O (\\sqrt{n})$ or $O(n)$ speed-up depending on the leading complexity term as\nlong as the diameter of the network is not too big with respect to $m$. This\nalso leads to a $\\sqrt{m}$ speed-up over state-of-the-art distributed batch\nmethods, which is the expected speed-up for finite sum algorithms. In terms of\ncommunication times and network parameters, ADFS scales as well as optimal\ndistributed batch algorithms. As a side contribution, we give a generalized\nversion of the accelerated proximal coordinate gradient algorithm using\narbitrary sampling that we apply to a well-chosen dual problem to derive ADFS.\nYet, ADFS uses primal proximal updates that only require solving\none-dimensional problems for many standard machine learning applications.\nFinally, ADFS can be formulated for non-smooth objectives with equally good\nscaling properties. We illustrate the improvement of ADFS over state-of-the-art\napproaches with simulations.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 18:12:37 GMT"}, {"version": "v2", "created": "Thu, 6 Jun 2019 09:36:32 GMT"}, {"version": "v3", "created": "Wed, 17 Jul 2019 12:21:59 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Hendrikx", "Hadrien", ""], ["Bach", "Francis", ""], ["Massouli\u00e9", "Laurent", ""]]}, {"id": "1901.09873", "submitter": "Sara Rouhani", "authors": "Sara Rouhani, Vahid Pourheidari, Ralph Deters", "title": "Physical Access Control Management System Based on Permissioned\n  Blockchain", "comments": "2018 IEEE Confs on Internet of Things, Green Computing and\n  Communications, Cyber, Physical and Social Computing,Smart Data, Blockchain,\n  Computer and Information Technology, Congress on Cybermatics", "journal-ref": null, "doi": "10.1109/Cybermatics_2018.2018.00198", "report-no": null, "categories": "cs.SY cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using blockchain as a decentralized backend infrastructure has grabbed the\nattention of many startups entrepreneurs and developers. Blockchain records\ntransactions permanently and protects them from undesirable tampering. It\nprovides a reliable tamper-proof database which can be considered as a\ntrustable source for tracking the previous system state. In this paper, we\npresent our access control application based on Hyperledger Fabric Blockchain\nand Hyperledger Composer to control access to physical places. The system\ncomponents and modular architecture are illustrated, and we have extracted\nmetadata include historian transactions details arising from our demo test.\nFinally, the performance metrics and resources consumption are provided using\nHyperledger Caliper, a benchmark framework for measuring Hyperledger\nblockchains performance.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 18:27:21 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Rouhani", "Sara", ""], ["Pourheidari", "Vahid", ""], ["Deters", "Ralph", ""]]}, {"id": "1901.09942", "submitter": "Nadi Sarrar", "authors": "Nadi Sarrar", "title": "On transaction parallelizability in Ethereum", "comments": "2 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.PF", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Ethereum clients execute transactions in a sequential order prescribed by the\nconsensus protocol. This is a safe and conservative approach to blockchain\ntransaction processing which forgoes running transactions in parallel even when\ndoing so would be beneficial and safe, e.g., when there is no intersection in\nthe sets of accounts that the transactions read or modify. In this work we\nstudy the degree of transaction parallelizability and present results from\nthree different simulations using real Ethereum transaction data. Our\nsimulations demonstrate that notable gains are achievable with parallelization,\nand suggest that the potential for parallelizability improves as transaction\nrates increase.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 19:02:44 GMT"}, {"version": "v2", "created": "Wed, 30 Jan 2019 13:40:45 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Sarrar", "Nadi", ""]]}, {"id": "1901.10008", "submitter": "Paras Jain", "authors": "Paras Jain, Xiangxi Mo, Ajay Jain, Alexey Tumanov, Joseph E. Gonzalez,\n  Ion Stoica", "title": "The OoO VLIW JIT Compiler for GPU Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current trends in Machine Learning~(ML) inference on hardware accelerated\ndevices (e.g., GPUs, TPUs) point to alarmingly low utilization. As ML inference\nis increasingly time-bounded by tight latency SLOs, increasing data parallelism\nis not an option. The need for better efficiency motivates GPU multiplexing.\nFurthermore, existing GPU programming abstractions force programmers to\nmicro-manage GPU resources in an early-binding, context-free fashion. We\npropose a VLIW-inspired Out-of-Order (OoO) Just-in-Time (JIT) compiler that\ncoalesces and reorders execution kernels at runtime for throughput-optimal\ndevice utilization while satisfying latency SLOs. We quantify the\ninefficiencies of space-only and time-only multiplexing alternatives and\ndemonstrate an achievable 7.7x opportunity gap through spatial coalescing.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 21:34:59 GMT"}, {"version": "v2", "created": "Thu, 31 Jan 2019 04:41:58 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Jain", "Paras", ""], ["Mo", "Xiangxi", ""], ["Jain", "Ajay", ""], ["Tumanov", "Alexey", ""], ["Gonzalez", "Joseph E.", ""], ["Stoica", "Ion", ""]]}, {"id": "1901.10023", "submitter": "Jungyeon Baek", "authors": "Jung-yeon Baek, Georges Kaddoum, Sahil Garg, Kuljeet Kaur, and\n  Vivianne Gravel", "title": "Managing Fog Networks using Reinforcement Learning Based Load Balancing\n  Algorithm", "comments": "7 pages, 4 figures, IEEE Wireless Communications and Networking\n  Conference, 15- 18 April 2019, Marrakesh, Morocco", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The powerful paradigm of Fog computing is currently receiving major interest,\nas it provides the possibility to integrate virtualized servers into networks\nand brings cloud service closer to end devices. To support this distributed\nintelligent platform, Software-Defined Network (SDN) has emerged as a viable\nnetwork technology in the Fog computing environment. However, uncertainties\nrelated to task demands and the different computing capacities of Fog nodes,\ninquire an effective load balancing algorithm. In this paper, the load\nbalancing problem has been addressed under the constraint of achieving the\nminimum latency in Fog networks. To handle this problem, a reinforcement\nlearning based decision-making process has been proposed to find the optimal\noffloading decision with unknown reward and transition functions. The proposed\nprocess allows Fog nodes to offload an optimal number of tasks among incoming\ntasks by selecting an available neighboring Fog node under their respective\nresource capabilities with the aim to minimize the processing time and the\noverall overloading probability. Compared with the traditional approaches, the\nproposed scheme not only simplifies the algorithmic framework without imposing\nany specific assumption on the network model but also guarantees convergence in\npolynomial time. The results show that, during average delays, the proposed\nreinforcement learning-based offloading method achieves significant performance\nimprovements over the variation of service rate and traffic arrival rate. The\nproposed algorithm achieves 1.17%, 1.02%, and 3.21% lower overload probability\nrelative to random, least-queue and nearest offloading selection schemes,\nrespectively.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 22:46:22 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Baek", "Jung-yeon", ""], ["Kaddoum", "Georges", ""], ["Garg", "Sahil", ""], ["Kaur", "Kuljeet", ""], ["Gravel", "Vivianne", ""]]}, {"id": "1901.10084", "submitter": "Nate Veldt", "authors": "Cameron Ruggles and Nate Veldt and David F. Gleich", "title": "A Parallel Projection Method for Metric Constrained Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many clustering applications in machine learning and data mining rely on\nsolving metric-constrained optimization problems. These problems are\ncharacterized by $O(n^3)$ constraints that enforce triangle inequalities on\ndistance variables associated with $n$ objects in a large dataset. Despite its\nusefulness, metric-constrained optimization is challenging in practice due to\nthe cubic number of constraints and the high-memory requirements of standard\noptimization software. Recent work has shown that iterative projection methods\nare able to solve metric-constrained optimization problems on a much larger\nscale than was previously possible, thanks to their comparatively low memory\nrequirement. However, the major limitation of projection methods is their slow\nconvergence rate. In this paper we present a parallel projection method for\nmetric-constrained optimization which allows us to speed up the convergence\nrate in practice. The key to our approach is a new parallel execution schedule\nthat allows us to perform projections at multiple metric constraints\nsimultaneously without any conflicts or locking of variables. We illustrate the\neffectiveness of this execution schedule by implementing and testing a parallel\nprojection method for solving the metric-constrained linear programming\nrelaxation of correlation clustering. We show numerous experimental results on\nproblems involving up to 2.9 trillion constraints.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 03:14:24 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Ruggles", "Cameron", ""], ["Veldt", "Nate", ""], ["Gleich", "David F.", ""]]}, {"id": "1901.10183", "submitter": "Tal Ben-Nun", "authors": "Tal Ben-Nun, Maciej Besta, Simon Huber, Alexandros Nikolaos Ziogas,\n  Daniel Peter, Torsten Hoefler", "title": "A Modular Benchmarking Infrastructure for High-Performance and\n  Reproducible Deep Learning", "comments": "Accepted to IPDPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Deep500: the first customizable benchmarking infrastructure that\nenables fair comparison of the plethora of deep learning frameworks,\nalgorithms, libraries, and techniques. The key idea behind Deep500 is its\nmodular design, where deep learning is factorized into four distinct levels:\noperators, network processing, training, and distributed training. Our\nevaluation illustrates that Deep500 is customizable (enables combining and\nbenchmarking different deep learning codes) and fair (uses carefully selected\nmetrics). Moreover, Deep500 is fast (incurs negligible overheads), verifiable\n(offers infrastructure to analyze correctness), and reproducible. Finally, as\nthe first distributed and reproducible benchmarking system for deep learning,\nDeep500 provides software infrastructure to utilize the most powerful\nsupercomputers for extreme-scale workloads.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 09:03:41 GMT"}, {"version": "v2", "created": "Thu, 13 Jun 2019 07:59:35 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Ben-Nun", "Tal", ""], ["Besta", "Maciej", ""], ["Huber", "Simon", ""], ["Ziogas", "Alexandros Nikolaos", ""], ["Peter", "Daniel", ""], ["Hoefler", "Torsten", ""]]}, {"id": "1901.10387", "submitter": "Nima Anari", "authors": "Nima Anari and Vijay V. Vazirani", "title": "Matching is as Easy as the Decision Problem, in the NC Model", "comments": "Appeared in ITCS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Is matching in NC, i.e., is there a deterministic fast parallel algorithm for\nit? This has been an outstanding open question in TCS for over three decades,\never since the discovery of randomized NC matching algorithms [KUW85, MVV87].\nOver the last five years, the theoretical computer science community has\nlaunched a relentless attack on this question, leading to the discovery of\nseveral powerful ideas. We give what appears to be the culmination of this line\nof work: An NC algorithm for finding a minimum-weight perfect matching in a\ngeneral graph with polynomially bounded edge weights, provided it is given an\noracle for the decision problem. Consequently, for settling the main open\nproblem, it suffices to obtain an NC algorithm for the decision problem. We\nbelieve this new fact has qualitatively changed the nature of this open\nproblem.\n  All known efficient matching algorithms for general graphs follow one of two\napproaches: given by Edmonds [Edm65] and Lov\\'asz [Lov79]. Our oracle-based\nalgorithm follows a new approach and uses many of the ideas discovered in the\nlast five years.\n  The difficulty of obtaining an NC perfect matching algorithm led researchers\nto study matching vis-a-vis clever relaxations of the class NC. In this vein,\nrecently Goldwasser and Grossman [GG15] gave a pseudo-deterministic RNC\nalgorithm for finding a perfect matching in a bipartite graph, i.e., an RNC\nalgorithm with the additional requirement that on the same graph, it should\nreturn the same (i.e., unique) perfect matching for almost all choices of\nrandom bits. A corollary of our reduction is an analogous algorithm for general\ngraphs.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 16:53:57 GMT"}, {"version": "v2", "created": "Thu, 31 Jan 2019 07:15:56 GMT"}, {"version": "v3", "created": "Tue, 19 Feb 2019 17:33:14 GMT"}, {"version": "v4", "created": "Fri, 23 Aug 2019 12:48:35 GMT"}, {"version": "v5", "created": "Mon, 9 Nov 2020 00:36:43 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Anari", "Nima", ""], ["Vazirani", "Vijay V.", ""]]}, {"id": "1901.10403", "submitter": "Ali Vatankhah Barenji Dr", "authors": "Ali Vatankhah Barenji, Hanyang Guo, Zonggui Tian, Zhi Li, W.M. Wang\n  and George Q. Huang", "title": "Blockchain-Based Cloud Manufacturing: Decentralization", "comments": "9 pages, 2 figures", "journal-ref": "Transdisciplinary Engineering Methods for Social Innovation of\n  Industry 4.0, 2018", "doi": "10.3233/978-1-61499-898-3-1003", "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been growing interest in the field of cloud manufacturing\n(CM) amongst researchers in the manufacturing community. Cloud manufacturing is\na customer-driven manufacturing model that was inspired by cloud computing, and\nits major objective was to provide ubiquitous on-demand access to services.\nHowever, the current CM architecture suffers from problems that are associated\nwith a centralized based industrial network framework and third part operation.\nIn a nutshell, centralized networking has had issues with flexibility,\nefficiency, availability, and security. Therefore, this paper aims to tackle\nthese problems by introducing an ongoing project to a decentralized network\narchitecture for cloud manufacturing which is based on the blockchain\ntechnology. In essence, this research paper introduces the blockchain\ntechnology as a decentralized peer to peer network for multiple cloud\nmanufacturing providers.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jan 2019 05:12:39 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Barenji", "Ali Vatankhah", ""], ["Guo", "Hanyang", ""], ["Tian", "Zonggui", ""], ["Li", "Zhi", ""], ["Wang", "W. M.", ""], ["Huang", "George Q.", ""]]}, {"id": "1901.10645", "submitter": "Sara Rouhani", "authors": "Sara Rouhani, Luke Butterworth, Adam D. Simmons, Darryl G. Humphery,\n  and Ralph Deters", "title": "MediChainTM: A Secure Decentralized Medical Data Asset Management System", "comments": "2018 IEEE Confs on Internet of Things, Green Computing and\n  Communications, Cyber, Physical and Social Computing, Smart Data, Blockchain,\n  Computer and Information Technology, Congress on Cybermatics", "journal-ref": null, "doi": "10.1109/Cybermatics_2018.2018.00258", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The set of distributed ledger architectures known as blockchain is best known\nfor cryptocurrency applications such as Bitcoin and Ethereum. These\npermissionless block chains are showing the potential to be disruptive to the\nfinancial services industry. Their broader adoption is likely to be limited by\nthe maximum block size, the cost of the Proof of Work consensus mechanism, and\nthe increasing size of any given chain overwhelming most of the participating\nnodes. These factors have led to many cryptocurrency blockchains to become\ncentralized in the nodes with enough computing power and storage to be a\ndominant miner and validator. Permissioned chains operate in trusted\nenvironments and can, therefore, avoid the computationally expensive consensus\nmechanisms. Permissioned chains are still susceptible to asset storage demands\nand non-standard user interfaces that will impede their adoption. This paper\ndescribes an approach to addressing these limitations: permissioned blockchain\nthat uses off-chain storage of the data assets and this is accessed through a\nstandard browser and mobile app. The implementation in the Hyperledger\nframework is described as is an example use of patient-centered health data\nmanagement.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 02:22:07 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Rouhani", "Sara", ""], ["Butterworth", "Luke", ""], ["Simmons", "Adam D.", ""], ["Humphery", "Darryl G.", ""], ["Deters", "Ralph", ""]]}, {"id": "1901.11204", "submitter": "Matheus Saldanha", "authors": "Matheus Henrique Junqueira Saldanha, Paulo S\\'ergio Lopes de Souza", "title": "High Performance Algorithms for Counting Collisions and Pairwise\n  Interactions", "comments": "Accepted in ICCS 2019 and published in Springer's LNCS series.\n  Supplementary content at https://mjsaldanha.com/articles/1-hpc-sspi", "journal-ref": "Rodrigues J. et al. (eds) Computational Science - ICCS 2019. ICCS\n  2019. Lecture Notes in Computer Science, vol 11536. Springer, Cham", "doi": "10.1007/978-3-030-22734-0_14", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of counting collisions or interactions is common in areas as\ncomputer graphics and scientific simulations. Since it is a major bottleneck in\napplications of these areas, a lot of research has been carried out on such\nsubject, mainly focused on techniques that allow calculations to be performed\nwithin pruned sets of objects. This paper focuses on how interaction\ncalculation (such as collisions) within these sets can be done more efficiently\nthan existing approaches. Two algorithms are proposed: a sequential algorithm\nthat has linear complexity at the cost of high memory usage; and a parallel\nalgorithm, mathematically proved to be correct, that manages to use GPU\nresources more efficiently than existing approaches. The proposed and existing\nalgorithms were implemented, and experiments show a speedup of 21.7 for the\nsequential algorithm (on small problem size), and 1.12 for the parallel\nproposal (large problem size). By improving interaction calculation, this work\ncontributes to research areas that promote interconnection in the modern world,\nsuch as computer graphics and robotics.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 04:10:21 GMT"}, {"version": "v2", "created": "Sat, 31 Aug 2019 23:03:27 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Saldanha", "Matheus Henrique Junqueira", ""], ["de Souza", "Paulo S\u00e9rgio Lopes", ""]]}, {"id": "1901.11211", "submitter": "Weiwen Jiang", "authors": "Weiwen Jiang, Xinyi Zhang, Edwin H.-M. Sha, Lei Yang, Qingfeng Zhuge,\n  Yiyu Shi, Jingtong Hu", "title": "Accuracy vs. Efficiency: Achieving Both through FPGA-Implementation\n  Aware Neural Architecture Search", "comments": "6 pages, 8 figures, 1 table, accepted by DAC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental question lies in almost every application of deep neural\nnetworks: what is the optimal neural architecture given a specific dataset?\nRecently, several Neural Architecture Search (NAS) frameworks have been\ndeveloped that use reinforcement learning and evolutionary algorithm to search\nfor the solution. However, most of them take a long time to find the optimal\narchitecture due to the huge search space and the lengthy training process\nneeded to evaluate each candidate. In addition, most of them aim at accuracy\nonly and do not take into consideration the hardware that will be used to\nimplement the architecture. This will potentially lead to excessive latencies\nbeyond specifications, rendering the resulting architectures useless. To\naddress both issues, in this paper we use Field Programmable Gate Arrays\n(FPGAs) as a vehicle to present a novel hardware-aware NAS framework, namely\nFNAS, which will provide an optimal neural architecture with latency guaranteed\nto meet the specification. In addition, with a performance abstraction model to\nanalyze the latency of neural architectures without training, our framework can\nquickly prune architectures that do not satisfy the specification, leading to\nhigher efficiency. Experimental results on common data set such as ImageNet\nshow that in the cases where the state-of-the-art generates architectures with\nlatencies 7.81x longer than the specification, those from FNAS can meet the\nspecs with less than 1% accuracy loss. Moreover, FNAS also achieves up to\n11.13x speedup for the search process. To the best of the authors' knowledge,\nthis is the very first hardware aware NAS.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 04:57:16 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Jiang", "Weiwen", ""], ["Zhang", "Xinyi", ""], ["Sha", "Edwin H. -M.", ""], ["Yang", "Lei", ""], ["Zhuge", "Qingfeng", ""], ["Shi", "Yiyu", ""], ["Hu", "Jingtong", ""]]}, {"id": "1901.11249", "submitter": "Tatsuya Sato", "authors": "Tatsuya Sato, Yosuke Himura and Jun Nemoto", "title": "Design and Evaluation of Smart-Contract-based System Operations for\n  Permissioned Blockchain-based Systems", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, enterprises have paid attention to permissioned blockchain (BC),\nwhere business transactions among inter-authorized organizations (forming a\nconsortium) can automatically be executed on the basis of a distributed\nconsensus protocol, and applications of BC have expanded as permissioned BC has\nadopted the features of the smart contract (SC), which is programmable\nuser-defined business logic deployed in BC and executed with the consensus\nprotocol. A single BC-based system will be built across multiple management\ndomains (e.g., the data centers of each organization) having different\noperational policies (e.g., operational procedures, timing, etc.); although\nestablishing system management and operations over BC-based systems (e.g., SC\ninstallation for updates) will be important for production uses, such\nmulti-domain formation will trigger a problem in that executing system\noperations over BC-based systems will become time-consuming and costly due to\nthe difficulty in unifying and/or adjusting operational policies. Toward\nsolving the problem, we propose an operations execution method for BC-based\nsystems; the primary idea is to define operations as a smart contract so that\nunified and synchronized cross-organizational operations can be executed\neffectively by using BC-native features. For the recent BC architecture in\nwhich participating nodes have different types of roles, we designed the\nproposed method as a hybrid architecture characterized with in-BC consensus\nestablishment and execution status management and out-BC operations execution\nfor all types of nodes operated by agents that listen to triggered events\nincluding operational instructions defined in SCs. We implemented a prototype\nwith Hyperledger Fabric v1.2.0. A cost estimation with the prototype shows that\nthe total yearly cost of SC installation operations could be reduced by 74\npercent compared with a manual method.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 07:50:09 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Sato", "Tatsuya", ""], ["Himura", "Yosuke", ""], ["Nemoto", "Jun", ""]]}, {"id": "1901.11282", "submitter": "Keisuke Okumura", "authors": "Keisuke Okumura, Manao Machida, Xavier D\\'efago, Yasumasa Tamura", "title": "Priority Inheritance with Backtracking for Iterative Multi-agent Path\n  Finding", "comments": "8 pages, 2 figures, 2 tables, to be presented at IJCAI-19, Aug 2019,\n  Macao", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.DC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Multi-agent Path Finding (MAPF) problem consists in all agents having to\nmove to their own destinations while avoiding collisions. In practical\napplications to the problem, such as for navigation in an automated warehouse,\nMAPF must be solved iteratively. We present here a novel approach to iterative\nMAPF, that we call Priority Inheritance with Backtracking (PIBT). PIBT gives a\nunique priority to each agent every timestep, so that all movements are\nprioritized. Priority inheritance, which aims at dealing effectively with\npriority inversion in path adjustment within a small time window, can be\napplied iteratively and a backtracking protocol prevents agents from being\nstuck. We prove that, regardless of their number, all agents are guaranteed to\nreach their destination within finite time, when the environment is a graph\nsuch that all pairs of adjacent nodes belong to a simple cycle of length 3 or\nmore (e.g., biconnected). Our implementation of PIBT can be fully decentralized\nwithout global communication. Experimental results over various scenarios\nconfirm that PIBT is adequate both for finding paths in large environments with\nmany agents, as well as for conveying packages in an automated warehouse.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 09:27:22 GMT"}, {"version": "v2", "created": "Thu, 7 Mar 2019 09:27:33 GMT"}, {"version": "v3", "created": "Sun, 23 Jun 2019 04:03:27 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Okumura", "Keisuke", ""], ["Machida", "Manao", ""], ["D\u00e9fago", "Xavier", ""], ["Tamura", "Yasumasa", ""]]}, {"id": "1901.11286", "submitter": "Daniel Rodriguez", "authors": "Raul-Jose Palma-Mendoza and Luis de-Marcos and Daniel Rodriguez and\n  Amparo Alonso-Betanzos", "title": "Distributed Correlation-Based Feature Selection in Spark", "comments": "25 pages, 5 figures", "journal-ref": null, "doi": "10.1016/j.ins.2018.10.052", "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CFS (Correlation-Based Feature Selection) is an FS algorithm that has been\nsuccessfully applied to classification problems in many domains. We describe\nDistributed CFS (DiCFS) as a completely redesigned, scalable, parallel and\ndistributed version of the CFS algorithm, capable of dealing with the large\nvolumes of data typical of big data applications. Two versions of the algorithm\nwere implemented and compared using the Apache Spark cluster computing model,\ncurrently gaining popularity due to its much faster processing times than\nHadoop's MapReduce model. We tested our algorithms on four publicly available\ndatasets, each consisting of a large number of instances and two also\nconsisting of a large number of features. The results show that our algorithms\nwere superior in terms of both time-efficiency and scalability. In leveraging a\ncomputer cluster, they were able to handle larger datasets than the\nnon-distributed WEKA version while maintaining the quality of the results,\ni.e., exactly the same features were returned by our algorithms when compared\nto the original algorithm available in WEKA.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 09:36:04 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Palma-Mendoza", "Raul-Jose", ""], ["de-Marcos", "Luis", ""], ["Rodriguez", "Daniel", ""], ["Alonso-Betanzos", "Amparo", ""]]}, {"id": "1901.11334", "submitter": "Ga\\'etan Hains", "authors": "Gaetan J.D.R. Hains and Arvid Jakobsson and Youry Khmelevsky", "title": "Formal methods and software engineering for DL. Security, safety and\n  productivity for DL systems development", "comments": "Submitted to IEEE-CCECE2019", "journal-ref": null, "doi": null, "report-no": "Huawei CSI/DPSL technical report DPSL-PARIS-TR-2019-02", "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning (DL) techniques are now widespread and being integrated into\nmany important systems. Their classification and recognition abilities ensure\ntheir relevance for multiple application domains. As machine-learning that\nrelies on training instead of algorithm programming, they offer a high degree\nof productivity. But they can be vulnerable to attacks and the verification of\ntheir correctness is only just emerging as a scientific and engineering\npossibility. This paper is a major update of a previously-published survey,\nattempting to cover all recent publications in this area. It also covers an\neven more recent trend, namely the design of domain-specific languages for\nproducing and training neural nets.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 12:49:48 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Hains", "Gaetan J. D. R.", ""], ["Jakobsson", "Arvid", ""], ["Khmelevsky", "Youry", ""]]}, {"id": "1901.11374", "submitter": "Bal\\'azs Gerencs\\'er", "authors": "Bal\\'azs Gerencs\\'er, L\\'aszl\\'o Gerencs\\'er", "title": "Tight bounds on the convergence rate of generalized ratio consensus\n  algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problems discussed in this paper are motivated by general ratio consensus\nalgorithms, introduced by Kempe, Dobra, and Gehrke (2003) in a simple form as\nthe push-sum algorithm, later extended by B\\'en\\'ezit et al. (2010) under the\nname weighted gossip algorithm. We consider a communication protocol described\nby a strictly stationary, ergodic, sequentially primitive sequence of\nnon-negative matrices, applied iteratively to a pair of fixed initial vectors,\nthe components of which are called values and weights defined at the nodes of a\nnetwork. The subject of ratio consensus problems is to study the asymptotic\nproperties of ratios of values and weights at each node, expecting convergence\nto the same limit for all nodes. The main results of the paper provide upper\nbounds for the rate of the almost sure exponential convergence in terms of the\nspectral gap associated with the given sequence of random matrices. It will be\nshown that these upper bounds are sharp. Our results complement previous\nresults of Picci and Taylor (2013) and Iutzeler, Ciblat and Hachem (2013).\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 14:38:15 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 18:52:38 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Gerencs\u00e9r", "Bal\u00e1zs", ""], ["Gerencs\u00e9r", "L\u00e1szl\u00f3", ""]]}]