[{"id": "1903.00045", "submitter": "Tian Guo", "authors": "Shijian Li and Robert J. Walls and Lijie Xu and Tian Guo", "title": "Speeding up Deep Learning with Transient Servers", "comments": "Accepted to ICAC'19. 11 pages, 8 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.CV cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed training frameworks, like TensorFlow, have been proposed as a\nmeans to reduce the training time of deep learning models by using a cluster of\nGPU servers. While such speedups are often desirable---e.g., for rapidly\nevaluating new model designs---they often come with significantly higher\nmonetary costs due to sublinear scalability. In this paper, we investigate the\nfeasibility of using training clusters composed of cheaper transient GPU\nservers to get the benefits of distributed training without the high costs.\n  We conduct the first large-scale empirical analysis, launching more than a\nthousand GPU servers of various capacities, aimed at understanding the\ncharacteristics of transient GPU servers and their impact on distributed\ntraining performance. Our study demonstrates the potential of transient servers\nwith a speedup of 7.7X with more than 62.9% monetary savings for some cluster\nconfigurations. We also identify a number of important challenges and\nopportunities for redesigning distributed training frameworks to be\ntransient-aware. For example, the dynamic cost and availability characteristics\nof transient servers suggest the need for frameworks to dynamically change\ncluster configurations to best take advantage of current conditions.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 19:47:59 GMT"}, {"version": "v2", "created": "Sun, 5 May 2019 07:20:37 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Li", "Shijian", ""], ["Walls", "Robert J.", ""], ["Xu", "Lijie", ""], ["Guo", "Tian", ""]]}, {"id": "1903.00182", "submitter": "Junhao Hua", "authors": "Junhao Hua, Chunguang Li", "title": "Distributed Variational Bayesian Algorithms for Extended Object Tracking", "comments": "14 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with the problem of distributed extended object\ntracking, which aims to collaboratively estimate the state and extension of an\nobject by a network of nodes. In traditional tracking applications, most\napproaches consider an object as a point source of measurements due to limited\nsensor resolution capabilities. Recently, some studies consider the extended\nobjects, which are spatially structured, i.e., multiple resolution cells are\noccupied by an object. In this setting, multiple measurements are generated by\neach object per time step. In this paper, we present a Bayesian model for\nextended object tracking problem in a sensor network. In this model, the object\nextension is represented by a symmetric positive definite random matrix, and we\nassume that the measurement noise exists but is unknown. Using this Bayesian\nmodel, we first propose a novel centralized algorithm for extended object\ntracking based on variational Bayesian methods. Then, we extend it to the\ndistributed scenario based on the alternating direction method of multipliers\n(ADMM) technique. The proposed algorithms can simultaneously estimate the\nextended object state (the kinematic state and extension) and the measurement\nnoise covariance. Simulations on both extended object tracking and group target\ntracking are given to verify the effectiveness of the proposed model and\nalgorithms.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 07:32:48 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Hua", "Junhao", ""], ["Li", "Chunguang", ""]]}, {"id": "1903.00227", "submitter": "Lorenz H\\\"ubschle-Schneider", "authors": "Lorenz H\\\"ubschle-Schneider and Peter Sanders", "title": "Parallel Weighted Random Sampling", "comments": "Preliminary versions of this paper were presented at ESA 2019 and\n  SPAA 2020. Includes material from arXiv:1910.11069", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data structures for efficient sampling from a set of weighted items are an\nimportant building block of many applications. However, few parallel solutions\nare known. We close many of these gaps both for shared-memory and\ndistributed-memory machines. We give efficient, fast, and practicable parallel\nalgorithms for building data structures that support sampling single items\n(alias tables, compressed data structures). This also yields a simplified and\nmore space-efficient sequential algorithm for alias table construction. Our\napproaches to sampling $k$ out of $n$ items with/without replacement and to\nsubset (Poisson) sampling are output-sensitive, i.e., the sampling algorithms\nuse work linear in the number of different samples. This is also interesting in\nthe sequential case. Weighted random permutation can be done by sorting\nappropriate random deviates. We show that this is possible with linear work\nusing a nonlinear transformation of these deviates. Finally, we give a\ncommunication-efficient, highly scalable approach to (weighted and unweighted)\nreservoir sampling. This algorithm is based on a fully distributed model of\nstreaming algorithms that might be of independent interest. Experiments for\nalias tables and sampling with replacement show near linear speedups both for\nconstruction and queries using up to 158 threads of shared-memory machines. An\nexperimental evaluation of distributed weighted reservoir sampling on up to 256\nnodes (5120 cores) also shows good speedups.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 09:52:11 GMT"}, {"version": "v2", "created": "Fri, 12 Jul 2019 15:23:15 GMT"}, {"version": "v3", "created": "Mon, 19 Jul 2021 15:01:54 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["H\u00fcbschle-Schneider", "Lorenz", ""], ["Sanders", "Peter", ""]]}, {"id": "1903.00308", "submitter": "Klaus Reuter", "authors": "Katharina Kormann, Klaus Reuter, Markus Rampp", "title": "A massively parallel semi-Lagrangian solver for the six-dimensional\n  Vlasov-Poisson equation", "comments": null, "journal-ref": null, "doi": "10.1177/1094342019834644", "report-no": null, "categories": "physics.comp-ph cs.DC physics.plasm-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an optimized and scalable semi-Lagrangian solver for the\nVlasov-Poisson system in six-dimensional phase space. Grid-based solvers of the\nVlasov equation are known to give accurate results. At the same time, these\nsolvers are challenged by the curse of dimensionality resulting in very high\nmemory requirements, and moreover, requiring highly efficient parallelization\nschemes. In this paper, we consider the 6d Vlasov-Poisson problem discretized\nby a split-step semi-Lagrangian scheme, using successive 1d interpolations on\n1d stripes of the 6d domain. Two parallelization paradigms are compared, a\nremapping scheme and a classical domain decomposition approach applied to the\nfull 6d problem. From numerical experiments, the latter approach is found to be\nsuperior in the massively parallel case in various respects. We address the\nchallenge of artificial time step restrictions due to the decomposition of the\ndomain by introducing a blocked one-sided communication scheme for the purely\nelectrostatic case and a rotating mesh for the case with a constant magnetic\nfield. In addition, we propose a pipelining scheme that enables to hide the\ncosts for the halo communication between neighbor processes efficiently behind\nuseful computation. Parallel scalability on up to 65k processes is demonstrated\nfor benchmark problems on a supercomputer.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 14:12:52 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Kormann", "Katharina", ""], ["Reuter", "Klaus", ""], ["Rampp", "Markus", ""]]}, {"id": "1903.00505", "submitter": "Sebastian Siebertz", "authors": "Sebastian Siebertz and Alexandre Vigny", "title": "Parameterized Distributed Complexity Theory: A logical approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameterized complexity theory offers a framework for a refined analysis of\nhard algorithmic problems. Instead of expressing the running time of an\nalgorithm as a function of the input size only, running times are expressed\nwith respect to one or more parameters of the input instances. In this work we\nfollow the approach of parameterized complexity to provide a framework of\nparameterized distributed complexity. The central notion of efficiency in\nparameterized complexity is fixed-parameter tractability and we define the\ndistributed analogue Distributed-FPT (for Distributed in $\\{Local, Congest,\nCongested-Clique\\}$) as the class of problems that can be solved in $f(k)$\ncommunication rounds in the Distributed model of distributed computing, where\n$k$ is the parameter of the problem instance and $f$ is an arbitrary computable\nfunction. To classify hardness we introduce three hierarchies. The\nDistributed-WEFT-hierarchy is defined analogously to the W-hierarchy in\nparameterized complexity theory via reductions to the weighted circuit\nsatisfiability problem, but it turns out that this definition does not lead to\nsatisfying frameworks for the Local and Congest models. We then follow a\nlogical approach that leads to a more robust theory. We define the levels of\nthe Distributed-W-hierarchy and the Distributed-A-hierarchy that have\nfirst-order model-checking problems as their complete problems via suitable\nreductions.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 19:32:05 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 10:56:11 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Siebertz", "Sebastian", ""], ["Vigny", "Alexandre", ""]]}, {"id": "1903.00581", "submitter": "Klaus-Tycho Foerster", "authors": "Sebastian Brandt and Klaus-Tycho Foerster and Jonathan Maurer and\n  Roger Wattenhofer", "title": "Online Graph Exploration on a Restricted Graph Class: Optimal Solutions\n  for Tadpole Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of online graph exploration on undirected graphs, where\na searcher has to visit every vertex and return to the origin. Once a new\nvertex is visited, the searcher learns of all neighboring vertices and the\nconnecting edge weights. The goal such an exploration is to minimize its total\ncost, where each edge traversal incurs a cost of the corresponding edge weight.\nWe investigate the problem on tadpole graphs (also known as dragons, kites),\nwhich consist of a cycle with an attached path. Miyazaki et al. (The online\ngraph exploration problem on restricted graphs, IEICE Transactions 92-D (9),\n2009) showed that every online algorithm on these graphs must have a\ncompetitive ratio of 2-epsilon, but did not provide upper bounds for non-unit\nedge weights. We show via amortized analysis that a greedy approach yields a\nmatching competitive ratio of 2 on tadpole graphs, for arbitrary non-negative\nedge weights.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 23:51:01 GMT"}, {"version": "v2", "created": "Fri, 22 Mar 2019 13:48:06 GMT"}, {"version": "v3", "created": "Sat, 18 Apr 2020 19:33:12 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Brandt", "Sebastian", ""], ["Foerster", "Klaus-Tycho", ""], ["Maurer", "Jonathan", ""], ["Wattenhofer", "Roger", ""]]}, {"id": "1903.00757", "submitter": "Zhaocheng Zhu", "authors": "Zhaocheng Zhu, Shizhen Xu, Meng Qu, Jian Tang", "title": "GraphVite: A High-Performance CPU-GPU Hybrid System for Node Embedding", "comments": "accepted at WWW 2019", "journal-ref": null, "doi": "10.1145/3308558.3313508", "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning continuous representations of nodes is attracting growing interest\nin both academia and industry recently, due to their simplicity and\neffectiveness in a variety of applications. Most of existing node embedding\nalgorithms and systems are capable of processing networks with hundreds of\nthousands or a few millions of nodes. However, how to scale them to networks\nthat have tens of millions or even hundreds of millions of nodes remains a\nchallenging problem. In this paper, we propose GraphVite, a high-performance\nCPU-GPU hybrid system for training node embeddings, by co-optimizing the\nalgorithm and the system. On the CPU end, augmented edge samples are parallelly\ngenerated by random walks in an online fashion on the network, and serve as the\ntraining data. On the GPU end, a novel parallel negative sampling is proposed\nto leverage multiple GPUs to train node embeddings simultaneously, without much\ndata transfer and synchronization. Moreover, an efficient collaboration\nstrategy is proposed to further reduce the synchronization cost between CPUs\nand GPUs. Experiments on multiple real-world networks show that GraphVite is\nsuper efficient. It takes only about one minute for a network with 1 million\nnodes and 5 million edges on a single machine with 4 GPUs, and takes around 20\nhours for a network with 66 million nodes and 1.8 billion edges. Compared to\nthe current fastest system, GraphVite is about 50 times faster without any\nsacrifice on performance.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2019 20:06:58 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Zhu", "Zhaocheng", ""], ["Xu", "Shizhen", ""], ["Qu", "Meng", ""], ["Tang", "Jian", ""]]}, {"id": "1903.00843", "submitter": "Xiang Liu", "authors": "Xiang Liu, Ziyang Tang, Huyunting Huang, Tonglin Zhang, Baijian Yang", "title": "Multiple Learning for Regression in big data", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression problems that have closed-form solutions are well understood and\ncan be easily implemented when the dataset is small enough to be all loaded\ninto the RAM. Challenges arise when data is too big to be stored in RAM to\ncompute the closed form solutions. Many techniques were proposed to overcome or\nalleviate the memory barrier problem but the solutions are often local optimal.\nIn addition, most approaches require accessing the raw data again when updating\nthe models. Parallel computing clusters are also expected if multiple models\nneed to be computed simultaneously. We propose multiple learning approaches\nthat utilize an array of sufficient statistics (SS) to address this big data\nchallenge. This memory oblivious approach breaks the memory barrier when\ncomputing regressions with closed-form solutions, including but not limited to\nlinear regression, weighted linear regression, linear regression with Box-Cox\ntransformation (Box-Cox regression) and ridge regression models. The\ncomputation and update of the SS array can be handled at per row level or per\nmini-batch level. And updating a model is as easy as matrix addition and\nsubtraction. Furthermore, multiple SS arrays for different models can be easily\ncomputed simultaneously to obtain multiple models at one pass through the\ndataset. We implemented our approaches on Spark and evaluated over the\nsimulated datasets. Results showed our approaches can achieve closed-form\nsolutions of multiple models at the cost of half training time of the\ntraditional methods for a single model.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2019 06:34:24 GMT"}, {"version": "v2", "created": "Sat, 5 Oct 2019 00:39:30 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Liu", "Xiang", ""], ["Tang", "Ziyang", ""], ["Huang", "Huyunting", ""], ["Zhang", "Tonglin", ""], ["Yang", "Baijian", ""]]}, {"id": "1903.00872", "submitter": "Shaked Matar", "authors": "Michael Elkin and Shaked Matar", "title": "Near-Additive Spanners In Low Polynomial Deterministic CONGEST Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given parameters $\\alpha\\geq 1,\\beta\\geq 0$, a subgraph $G'=(V,H)$ of an\n$n$-vertex unweighted undirected graph $G=(V,E)$ is called an\n$(\\alpha,\\beta)$-spanner if for every pair $u,v\\in V$ of vertices,\n$d_{G'}(u,v)\\leq \\alpha d_{G}(u,v)+\\beta$. If $\\beta=0$ the spanner is called a\nmultiplicative $\\alpha$-spanner, and if $\\alpha = 1+\\epsilon$, for an\narbitrarily small $\\epsilon>0$, the spanner is said to be a near-additive one.\nGraph spanners are a fundamental and extremely well-studied combinatorial\nconstruct, with a multitude of applications in distributed computing and in\nother areas. Near-additive spanners, introduced in [EP01], preserve large\ndistances much more faithfully than multiplicative spanners. Also, recent lower\nbounds [AB15] ruled out the existence of arbitrarily sparse purely additive\nspanners (i.e., spanners with $\\alpha=1$), and therefore near-additive spanners\nprovide the best approximation of distances that one can hope for. Numerous\ndistributed algorithms for constructing sparse near-additive spanners exist. In\nparticular, there are now known efficient randomized algorithms in the CONGEST\nmodel that construct such spanners [EN17], and also there are efficient\ndeterministic algorithms in the LOCAL model [DGPV09]. The only known\ndeterministic CONGEST-model algorithm for the problem [Elk01] requires\nsuperlinear time in $n$. We remedy the situation and devise an efficient\ndeterministic CONGEST-model algorithm for constructing arbitrarily sparse\nnear-additive spanners. The running time of our algorithm is low polynomial,\ni.e., roughly $O(\\beta \\cdot n^\\rho)$, where $\\rho > 0$ is an arbitrarily small\npositive constant that affects the additive term $\\beta$. In general, the\nparameters of our algorithm and of the resulting spanner are at the same\nballpark as the respective parameters of the state-of-the-art randomized\nalgorithm for the problem due to [EN17].\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2019 09:47:30 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Elkin", "Michael", ""], ["Matar", "Shaked", ""]]}, {"id": "1903.00999", "submitter": "Waqar Ali", "authors": "Waqar Ali, Heechul Yun", "title": "RT-Gang: Real-Time Gang Scheduling Framework for Safety-Critical Systems", "comments": "11 pages, 6 figures, To appear in IEEE Real-Time Application\n  Symposium (RTAS), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present RT-Gang: a novel real-time gang scheduling\nframework that enforces a one-gang-at-a-time policy. We find that, in a\nmulticore platform, co-scheduling multiple parallel real-time tasks would\nrequire highly pessimistic worst-case execution time (WCET) and schedulability\nanalysis - even when there are enough cores - due to contention in shared\nhardware resources such as cache and DRAM controller. In RT-Gang, all threads\nof a parallel real-time task form a real-time gang and the scheduler globally\nenforces the one-gang-at-a-time scheduling policy to guarantee tight and\naccurate task WCET. To minimize under-utilization, we integrate a\nstate-of-the-art memory bandwidth throttling framework to allow safe execution\nof best-effort tasks. Specifically, any idle cores, if exist, are used to\nschedule best-effort tasks but their maximum memory bandwidth usages are\nstrictly throttled to tightly bound interference to real-time gang tasks. We\nimplement RT-Gang in the Linux kernel and evaluate it on two representative\nembedded multicore platforms using both synthetic and real-world DNN workloads.\nThe results show that RT-Gang dramatically improves system predictability and\nthe overhead is negligible.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2019 21:52:55 GMT"}, {"version": "v2", "created": "Sun, 17 Mar 2019 00:11:36 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Ali", "Waqar", ""], ["Yun", "Heechul", ""]]}, {"id": "1903.01042", "submitter": "Sanghamitra Dutta", "authors": "Sanghamitra Dutta, Ziqian Bai, Tze Meng Low, Pulkit Grover", "title": "CodeNet: Training Large Scale Neural Networks in Presence of Soft-Errors", "comments": "Currently under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.LG cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes the first strategy to make distributed training of neural\nnetworks resilient to computing errors, a problem that has remained unsolved\ndespite being first posed in 1956 by von Neumann. He also speculated that the\nefficiency and reliability of the human brain is obtained by allowing for low\npower but error-prone components with redundancy for error-resilience. It is\nsurprising that this problem remains open, even as massive artificial neural\nnetworks are being trained on increasingly low-cost and unreliable processing\nunits. Our coding-theory-inspired strategy, \"CodeNet,\" solves this problem by\naddressing three challenges in the science of reliable computing: (i) Providing\nthe first strategy for error-resilient neural network training by encoding each\nlayer separately; (ii) Keeping the overheads of coding\n(encoding/error-detection/decoding) low by obviating the need to re-encode the\nupdated parameter matrices after each iteration from scratch. (iii) Providing a\ncompletely decentralized implementation with no central node (which is a single\npoint of failure), allowing all primary computational steps to be error-prone.\nWe theoretically demonstrate that CodeNet has higher error tolerance than\nreplication, which we leverage to speed up computation time. Simultaneously,\nCodeNet requires lower redundancy than replication, and equal computational and\ncommunication costs in scaling sense. We first demonstrate the benefits of\nCodeNet in reducing expected computation time over replication when accounting\nfor checkpointing. Our experiments show that CodeNet achieves the best\naccuracy-runtime tradeoff compared to both replication and uncoded strategies.\nCodeNet is a significant step towards biologically plausible neural network\ntraining, that could hold the key to orders of magnitude efficiency\nimprovements.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 01:45:14 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Dutta", "Sanghamitra", ""], ["Bai", "Ziqian", ""], ["Low", "Tze Meng", ""], ["Grover", "Pulkit", ""]]}, {"id": "1903.01081", "submitter": "Yankan Song", "authors": "Yankan Song, Ying Chen, Zhitong Yu, Shaowei Huang, Chen Shen", "title": "CloudPSS: A High-Performance Power System Simulator Based on Cloud\n  Computing", "comments": null, "journal-ref": "Energy Rep. 6(2020) 1611-1618", "doi": "10.1016/j.egyr.2020.12.028", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing computations in power system simulations,\nhigh-performance and cost-effective power system simulator is highly required.\nIn this paper, a cloud-computing based power system simulator, namely CloudPSS,\nis designed. Based on an open service integrating framework, a self-developed\nelectromagnetic transients (EMT) simulator with an automatic code generator is\nprovided to accelerate EMT simulations using heterogeneous devices in the\ncloud, such as CPU and GPU. Test results show that CloudPSS can achieve\nsignificant speedups for both large-scale simulation tasks and multi-scenario\ntasks. Moreover, benefit from the time-sharing of cloud computing resources,\nthe computational cost can be greatly reduced.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 05:41:10 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2020 06:39:39 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Song", "Yankan", ""], ["Chen", "Ying", ""], ["Yu", "Zhitong", ""], ["Huang", "Shaowei", ""], ["Shen", "Chen", ""]]}, {"id": "1903.01113", "submitter": "Muhammad Hilman", "authors": "Muhammad H. Hilman, Maria A. Rodriguez, Rajkumar Buyya", "title": "Resource-sharing Policy in Multi-tenant Scientific Workflow-as-a-Service\n  Cloud Platform", "comments": "The manuscript has been revised in several sections. Add one\n  subsection of experimental validation. Submitted to Journal of Computer and\n  System Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increased adoption of scientific workflows in the community has urged for the\ndevelopment of multi-tenant platforms that provide these workflow executions as\na service. As a result, Workflow-as-a-Service (WaaS) concept has been created\nby researchers to address the future design of Workflow Management Systems\n(WMS) that can serve a large number of users from a single point of service.\nThese platforms differ from traditional WMS in that they handle a workload of\nworkflows at runtime. A traditional WMS is usually designed to execute a single\nworkflow in a dedicated process while WaaS cloud platforms enhance the process\nby exploiting multiple workflows execution in a multi-tenant environment model.\nIn this paper, we explore a novel resource-sharing policy to improve system\nutilization and to fulfil various Quality of Service (QoS) requirements from\nmultiple users in WaaS cloud platforms. We propose an Elastic\nBudget-constrained resource Provisioning and Scheduling algorithm for Multiple\nworkflows that can reduce the computational overhead by encouraging resource\nsharing to minimize workflows' makespan while meeting a user-defined budget.\nOur experiments show that the EBPSM algorithm can utilize the resource-sharing\npolicy to achieve higher performance in terms of minimizing the makespan\ncompared to the state-of-the-art budget-constraint scheduling algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 08:00:26 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2019 06:29:24 GMT"}, {"version": "v3", "created": "Tue, 23 Jun 2020 22:10:18 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Hilman", "Muhammad H.", ""], ["Rodriguez", "Maria A.", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "1903.01154", "submitter": "Xiaoying Zheng", "authors": "Hang Zhang, Xiaoying Zheng, Ye Xia, and Mingqi Li", "title": "Workflow Scheduling in the Cloud with Weighted Upward-rank Priority\n  Scheme Using Random Walk and Uniform Spare Budget Splitting", "comments": "16 pages, submitted to IEEE ACCESS journal on Feb. 28", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We study a difficult problem of how to schedule complex workflows with\nprecedence constraints under a limited budget in the cloud environment. We\nfirst formulate the scheduling problem as an integer programming problem, which\ncan be optimized and used as the baseline of performance. We then consider the\ntraditional approach of scheduling jobs in a prioritized order based on the\nupward-rank of each job. For those jobs with no precedence constraints among\nthemselves, the plain upward-rank priority scheme assigns priorities in an\narbitrary way. We propose a job prioritization scheme that uses Markovian chain\nstationary probabilities as a measure of importance of jobs. The scheme keeps\nthe precedence order for the jobs that have precedence constraints between each\nother, and assigns priorities according to the jobs' importance for the jobs\nwithout precedence constraints. We finally design a uniform spare budget\nsplitting strategy, which splits the spare budget uniformly across all the\njobs. We test our algorithms on a variety of workflows, including FFT, Gaussian\nelimination, typical scientific workflows, randomly generated workflows and\nworkflows from an in-production cluster of an online streaming service company.\nWe compare our algorithms with the-state-of-art algorithms. The empirical\nresults show that the uniform spare budget splitting scheme outperforms the\nsplitting scheme in proportion to extra demand in average for most cases, and\nthe Markovian based prioritization further improves the workflow makespan.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 10:03:22 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Zhang", "Hang", ""], ["Zheng", "Xiaoying", ""], ["Xia", "Ye", ""], ["Li", "Mingqi", ""]]}, {"id": "1903.01217", "submitter": "Mien Brabeeba Wang", "authors": "Nancy Lynch and Mien Brabeeba Wang", "title": "Integrating Temporal Information to Spatial Information in a Neural\n  Circuit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider networks of deterministic spiking neurons, firing\nsynchronously at discrete times; such spiking neural networks are inspired by\nnetworks of neurons and synapses that occur in brains. We consider the problem\nof translating temporal information into spatial information in such networks,\nan important task that is carried out by actual brains.\n  Specifically, we define two problems: \"First Consecutive Spikes Counting\n(FCSC)\" and \"Total Spikes Counting (TSC)\", which model spike and rate coding\naspects of translating temporal information into spatial information\nrespectively. Assuming an upper bound of $T$ on the length of the temporal\ninput signal, we design two networks that solve these two problems, each using\n$O(\\log T)$ neurons and terminating in time $1$. We also prove that there is no\nnetwork with less than $T$ neurons that solves either question in time $0$.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 03:59:59 GMT"}, {"version": "v2", "created": "Sat, 20 Jul 2019 19:17:14 GMT"}, {"version": "v3", "created": "Mon, 15 Jun 2020 18:20:39 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Lynch", "Nancy", ""], ["Wang", "Mien Brabeeba", ""]]}, {"id": "1903.01699", "submitter": "David Anderson", "authors": "David P. Anderson", "title": "BOINC: A Platform for Volunteer Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"Volunteer computing\" is the use of consumer digital devices for\nhigh-throughput scientific computing. It can provide large computing capacity\nat low cost, but presents challenges due to device heterogeneity,\nunreliability, and churn. BOINC, a widely-used open-source middleware system\nfor volunteer computing, addresses these challenges. We describe its features,\narchitecture, and implementation.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 06:56:34 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Anderson", "David P.", ""]]}, {"id": "1903.01758", "submitter": "Anders E. Kal{\\o}r", "authors": "Pietro Danzi, Anders E. Kal{\\o}r, Ren\\'e B. S{\\o}rensen, Alexander K.\n  Hagelskj{\\ae}r, Lam D. Nguyen, \\v{C}edomir Stefanovi\\'c and Petar Popovski", "title": "Communication Aspects of the Integration of Wireless IoT Devices with\n  Distributed Ledger Technology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pervasive need to safely share and store information between devices\ncalls for the replacement of centralized trust architectures with the\ndecentralized ones. Distributed Ledger Technologies (DLTs) are seen as the most\npromising enabler of decentralized trust, but they still lack technological\nmaturity and their successful adoption depends on the understanding of the\nfundamental design trade-offs and their reflection in the actual technical\ndesign. This work focuses on the challenges and potential solutions for an\neffective integration of DLTs in the context of Internet-of-Things (IoT). We\nfirst introduce the landscape of IoT applications and discuss the limitations\nand opportunities offered by DLTs. Then, we review the technical challenges\nencountered in the integration of resource-constrained devices with distributed\ntrust networks. We describe the common traits of lightweight synchronization\nprotocols, and propose a novel classification, rooted in the IoT perspective.\nWe identify the need of receiving ledger information at the endpoint devices,\nimplying a two-way data exchange that contrasts with the conventional\nuplink-oriented communication technologies intended for IoT systems.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 10:27:53 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Danzi", "Pietro", ""], ["Kal\u00f8r", "Anders E.", ""], ["S\u00f8rensen", "Ren\u00e9 B.", ""], ["Hagelskj\u00e6r", "Alexander K.", ""], ["Nguyen", "Lam D.", ""], ["Stefanovi\u0107", "\u010cedomir", ""], ["Popovski", "Petar", ""]]}, {"id": "1903.01794", "submitter": "Miltiadis Filippou", "authors": "Miltiades C. Filippou, Dario Sabella, Vincenzo Riccobene", "title": "Flexible MEC service consumption through edge host zoning in 5G networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-access Edge Computing (MEC) is commonly recognized as a key supporting\ntechnology for the emerging 5G systems. When deployed in fully virtualized\nnetworks, i.e., following the Network Function Virtualization (NFV) paradigm,\nit will enable a multitude of new applications and use cases. However, the\ngrowing number of devices, combined with the vastly increasing traffic demand,\ncall for low End-to-End (E2E) latency packet transfer and processing in an NFV\nenvironment, both in user and control plane. In this paper, focusing on control\nplane packet traffic, we investigate the general case of a MEC application\nconsuming a MEC service running on a different MEC host. To enable flexible MEC\nplatform service consumption at different localities, based on a\nstate-of-the-art statistical model of the total processing time, we define\nlatency-aware proximity zones around MEC servers hosting MEC application\ninstances. Exemplary scenarios exhibit the E2E performance benefit of\nintroducing the awareness of proximity zones around MEC hosts and service\nproducing MEC application instances. This performance-aware service consumption\nwill be beneficial in view of the future evolution towards distributed\ncomputing systems.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 12:48:52 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Filippou", "Miltiades C.", ""], ["Sabella", "Dario", ""], ["Riccobene", "Vincenzo", ""]]}, {"id": "1903.01919", "submitter": "Senthil Nathan", "authors": "Senthil Nathan, Chander Govindarajan, Adarsh Saraf, Manish Sethi,\n  Praveen Jayachandran", "title": "Blockchain Meets Database: Design and Implementation of a Blockchain\n  Relational Database", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we design and implement the first-ever decentralized\nreplicated relational database with blockchain properties that we term\nblockchain relational database. We highlight several similarities between\nfeatures provided by blockchain platforms and a replicated relational database,\nalthough they are conceptually different, primarily in their trust model.\nMotivated by this, we leverage the rich features, decades of research and\noptimization, and available tooling in relational databases to build a\nblockchain relational database. We consider a permissioned blockchain model of\nknown, but mutually distrustful organizations each operating their own database\ninstance that are replicas of one another. The replicas execute transactions\nindependently and engage in decentralized consensus to determine the commit\norder for transactions. We design two approaches, the first where the commit\norder for transactions is agreed upon prior to executing them, and the second\nwhere transactions are executed without prior knowledge of the commit order\nwhile the ordering happens in parallel. We leverage serializable snapshot\nisolation (SSI) to guarantee that the replicas across nodes remain consistent\nand respect the ordering determined by consensus, and devise a new variant of\nSSI based on block height for the latter approach. We implement our system on\nPostgreSQL and present detailed performance experiments analyzing both\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 16:13:50 GMT"}, {"version": "v2", "created": "Fri, 31 May 2019 10:21:48 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Nathan", "Senthil", ""], ["Govindarajan", "Chander", ""], ["Saraf", "Adarsh", ""], ["Sethi", "Manish", ""], ["Jayachandran", "Praveen", ""]]}, {"id": "1903.01930", "submitter": "Matteo Stefanini", "authors": "Matteo Stefanini, Riccardo Lancellotti, Lorenzo Baraldi, Simone\n  Calderara", "title": "A Deep Learning based approach to VM behavior identification in cloud\n  systems", "comments": "Accepted at CLOSER2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing data centers are growing in size and complexity to the point\nwhere monitoring and management of the infrastructure become a challenge due to\nscalability issues. A possible approach to cope with the size of such data\ncenters is to identify VMs exhibiting a similar behavior. Existing literature\ndemonstrated that clustering together VMs that show a similar behavior may\nimprove the scalability of both monitoring andmanagement of a data center.\nHowever, available techniques suffer from a trade-off between accuracy and time\nto achieve this result. Throughout this paper we propose a different approach\nwhere, instead of an unsupervised clustering, we rely on classifiers based on\ndeep learning techniques to assigna newly deployed VMs to a cluster of\nalready-known VMs. The two proposed classifiers, namely DeepConv and DeepFFT\nuse a convolution neural network and (in the latter model) exploits Fast\nFourier Transformation to classify the VMs. Our proposal is validated using a\nset of traces describing the behavior of VMs from a realcloud data center. The\nexperiments compare our proposal with state-of-the-art solutions available in\nliterature, demonstrating that our proposal achieve better performance.\nFurthermore, we show that our solution issignificantly faster than the\nalternatives as it can produce a perfect classification even with just a few\nsamples of data, making our proposal viable also toclassify on-demand VMs that\nare characterized by a short life span.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 16:49:00 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Stefanini", "Matteo", ""], ["Lancellotti", "Riccardo", ""], ["Baraldi", "Lorenzo", ""], ["Calderara", "Simone", ""]]}, {"id": "1903.01974", "submitter": "Mehmet Emre Ozfatura", "authors": "Emre Ozfatura and Deniz Gunduz and Sennur Ulukus", "title": "Gradient Coding with Clustering and Multi-message Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.LG eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient descent (GD) methods are commonly employed in machine learning\nproblems to optimize the parameters of the model in an iterative fashion. For\nproblems with massive datasets, computations are distributed to many parallel\ncomputing servers (i.e., workers) to speed up GD iterations. While distributed\ncomputing can increase the computation speed significantly, the per-iteration\ncompletion time is limited by the slowest straggling workers. Coded distributed\ncomputing can mitigate straggling workers by introducing redundant\ncomputations; however, existing coded computing schemes are mainly designed\nagainst persistent stragglers, and partial computations at straggling workers\nare discarded, leading to wasted computational capacity. In this paper, we\npropose a novel gradient coding (GC) scheme which allows multiple coded\ncomputations to be conveyed from each worker to the master per iteration. We\nnumerically show that the proposed GC with multi-message communication (MMC)\ntogether with clustering provides significant improvements in the average\ncompletion time (of each iteration), with minimal or no increase in the\ncommunication load.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 18:38:15 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Ozfatura", "Emre", ""], ["Gunduz", "Deniz", ""], ["Ulukus", "Sennur", ""]]}, {"id": "1903.01982", "submitter": "Albert Reuther PhD", "authors": "Julia Mullen, Albert Reuther, William Arcand, Bill Bergeron, David\n  Bestor, Chansup Byun, Vijay Gadepally, Michael Houle, Matthew Hubbell,\n  Michael Jones, Anna Klein, Peter Michaleas, Lauren Milechin, Andrew Prout,\n  Antonio Rosa, Siddharth Samsi, Charles Yee, Jeremy Kepner", "title": "Lessons Learned from a Decade of Providing Interactive, On-Demand High\n  Performance Computing to Scientists and Engineers", "comments": "15 pages, 3 figures, First Workshop on Interactive High Performance\n  Computing (WIHPC) 2018 held in conjunction with ISC High Performance 2018 in\n  Frankfurt, Germany", "journal-ref": null, "doi": "10.1007/978-3-030-02465-9_47", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For decades, the use of HPC systems was limited to those in the physical\nsciences who had mastered their domain in conjunction with a deep understanding\nof HPC architectures and algorithms. During these same decades, consumer\ncomputing device advances produced tablets and smartphones that allow millions\nof children to interactively develop and share code projects across the globe.\nAs the HPC community faces the challenges associated with guiding researchers\nfrom disciplines using high productivity interactive tools to effective use of\nHPC systems, it seems appropriate to revisit the assumptions surrounding the\nnecessary skills required for access to large computational systems. For over a\ndecade, MIT Lincoln Laboratory has been supporting interactive, on-demand high\nperformance computing by seamlessly integrating familiar high productivity\ntools to provide users with an increased number of design turns, rapid\nprototyping capability, and faster time to insight. In this paper, we discuss\nthe lessons learned while supporting interactive, on-demand high performance\ncomputing from the perspectives of the users and the team supporting the users\nand the system. Building on these lessons, we present an overview of current\nneeds and the technical solutions we are building to lower the barrier to entry\nfor new users from the humanities, social, and biological sciences.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 18:53:08 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Mullen", "Julia", ""], ["Reuther", "Albert", ""], ["Arcand", "William", ""], ["Bergeron", "Bill", ""], ["Bestor", "David", ""], ["Byun", "Chansup", ""], ["Gadepally", "Vijay", ""], ["Houle", "Michael", ""], ["Hubbell", "Matthew", ""], ["Jones", "Michael", ""], ["Klein", "Anna", ""], ["Michaleas", "Peter", ""], ["Milechin", "Lauren", ""], ["Prout", "Andrew", ""], ["Rosa", "Antonio", ""], ["Samsi", "Siddharth", ""], ["Yee", "Charles", ""], ["Kepner", "Jeremy", ""]]}, {"id": "1903.02139", "submitter": "Xiaoying Zheng", "authors": "Xiaoying Zheng and Ye Xia", "title": "Exploring Mixed Integer Programming Reformulations for Virtual Machine\n  Placement with Disk Anti-Colocation Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the important problems for datacenter resource management is to place\nvirtual machines (VMs) to physical machines (PMs) such that certain cost,\nprofit or performance objective is optimized, subject to various constraints.\nIn this paper, we consider an interesting and difficult VM placement problem\nwith disk anti-colocation constraints: a VM's virtual disks should be spread\nout across the physical disks of its assigned PM. For solutions, we use the\nmixed integer programming (MIP) formulations and algorithms. However, a\nchallenge is the potentially long computation time of the MIP algorithms. In\nthis paper, we explore how reformulation of the problem can help to reduce the\ncomputation time. We develop two reformulations, by redefining the variables,\nfor our VM placement problem and evaluate the computation time of all three\nformulations. We show that they have vastly different computation time. All\nthree formulations can be useful, but for different problem instances. They all\nshould be kept in the toolbox for tackling the problem. Out of the three,\nformulation COMB is especially flexible and versatile, and it can solve large\nproblem instances.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 02:15:31 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Zheng", "Xiaoying", ""], ["Xia", "Ye", ""]]}, {"id": "1903.02294", "submitter": "Antoine Paris", "authors": "Antoine Paris, Hamed Mirghasemi, Ivan Stupia, Luc Vandendorpe", "title": "Energy-Efficient Edge-Facilitated Wireless Collaborative Computing using\n  Map-Reduce", "comments": "5 pages, 5 figures, submitted to SPAWC19", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, a heterogeneous set of wireless devices sharing a common access\npoint collaborates to perform a set of tasks. Using the Map-Reduce distributed\ncomputing framework, the tasks are optimally distributed amongst the nodes with\nthe objective of minimizing the total energy consumption of the nodes while\nsatisfying a latency constraint. The derived optimal collaborative-computing\nscheme takes into account both the computing capabilities of the nodes and the\nstrength of their communication links. Numerical simulations illustrate the\nbenefits of the proposed optimal collaborative-computing scheme over a blind\ncollaborative-computing scheme and the non-collaborative scenario, both in term\nof energy savings and achievable latency. The proposed optimal scheme also\nexhibits the interesting feature of allowing to trade energy for latency, and\nvice versa.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 10:33:38 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Paris", "Antoine", ""], ["Mirghasemi", "Hamed", ""], ["Stupia", "Ivan", ""], ["Vandendorpe", "Luc", ""]]}, {"id": "1903.02352", "submitter": "Michel Fliess", "authors": "Michel Fliess, C\\'edric Join, Maria Bekcheva, Alireza Moradi, Hugues\n  Mounier", "title": "Easily implementable time series forecasting techniques for resource\n  provisioning in cloud computing", "comments": "6th Conference on Control, Decision and Information Technologies\n  (CoDIT 2019), April 2019, Paris", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Workload predictions in cloud computing is obviously an important topic. Most\nof the existing publications employ various time series techniques, that might\nbe difficult to implement. We suggest here another route, which has already\nbeen successfully used in financial engineering and photovoltaic energy. No\nmathematical modeling and machine learning procedures are needed. Our computer\nsimulations via realistic data, which are quite convincing, show that a setting\nmixing algebraic estimation techniques and the daily seasonality behaves much\nbetter. An application to the computing resource allocation, via virtual\nmachines, is sketched out.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 09:24:44 GMT"}, {"version": "v2", "created": "Mon, 11 Mar 2019 10:27:50 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Fliess", "Michel", ""], ["Join", "C\u00e9dric", ""], ["Bekcheva", "Maria", ""], ["Moradi", "Alireza", ""], ["Mounier", "Hugues", ""]]}, {"id": "1903.02550", "submitter": "Junzhong Shen", "authors": "Deguang Wang, Junzhong Shen, Mei Wen, Chunyuan Zhang", "title": "Towards a Uniform Architecture for the Efficient Implementation of 2D\n  and 3D Deconvolutional Neural Networks on FPGAs", "comments": "5 pages, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three-dimensional deconvolution is widely used in many computer vision\napplications. However, most previous works have only focused on accelerating 2D\ndeconvolutional neural networks (DCNNs) on FPGAs, while the acceleration of 3D\nDCNNs has not been studied in depth as they have higher computational\ncomplexity and sparsity than 2D DCNNs. In this paper, we focus on the\nacceleration of both 2D and 3D DCNNs on FPGAs by proposing efficient schemes\nfor mapping 2D and 3D DCNNs on a uniform architecture. By implementing our\ndesign on the Xilinx VC709 platform for four real-life 2D and 3D DCNNs, we can\nachieve up to 3.0 TOPS with high hardware efficiency. Comparisons with CPU and\nGPU solutions demonstrate that we can achieve an improvement of up to 63.3X in\nthroughput relative to a CPU solution and an improvement of up to 8.3X in\nenergy efficiency compared to a GPU solution.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 00:27:08 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Wang", "Deguang", ""], ["Shen", "Junzhong", ""], ["Wen", "Mei", ""], ["Zhang", "Chunyuan", ""]]}, {"id": "1903.02637", "submitter": "Eric Severson", "authors": "Eric E. Severson, David Haley, David Doty", "title": "Composable computation in discrete chemical reaction networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the composability of discrete chemical reaction networks (CRNs) that\nstably compute (i.e., with probability 0 of error) integer-valued functions\n$f:\\mathbb{N}^d\\to\\mathbb{N}$. We consider output-oblivious CRNs in which the\noutput species is never a reactant (input) to any reaction. The class of\noutput-oblivious CRNs is fundamental, appearing in earlier studies of CRN\ncomputation, because it is precisely the class of CRNs that can be composed by\nsimply renaming the output of the upstream CRN to match the input of the\ndownstream CRN.\n  Our main theorem precisely characterizes the functions $f$ stably computable\nby output-oblivious CRNs with an initial leader. The key necessary condition is\nthat for sufficiently large inputs, $f$ is the minimum of a finite number of\nnondecreasing quilt-affine functions. (An affine function is linear with a\nconstant offset; a quilt-affine function is linear with a periodic offset).\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 22:45:08 GMT"}, {"version": "v2", "created": "Sat, 1 Jun 2019 01:34:58 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Severson", "Eric E.", ""], ["Haley", "David", ""], ["Doty", "David", ""]]}, {"id": "1903.02724", "submitter": "Seyyedali Hosseinalipour", "authors": "Minghui LiWang, Seyyedali Hosseinalipour, Zhibin Gao, Yuliang Tang,\n  Lianfen Huang, Huaiyu Dai", "title": "Allocation of Computation-Intensive Graph Jobs over Vehicular Clouds in\n  IoV", "comments": "11 pages, 6 Figures", "journal-ref": "IEEE Internet of Things Journal, 2019", "doi": "10.1109/JIOT.2019.2949602", "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph jobs represent a wide variety of computation-intensive tasks in which\ncomputations are represented by graphs consisting of components (denoting\neither data sources or data processing) and edges (corresponding to data flows\nbetween the components). Recent years have witnessed dramatic growth in smart\nvehicles and computation-intensive graph jobs, which pose new challenges to the\nprovision of efficient services related to the Internet of Vehicles.\nFortunately, vehicular clouds formed by a collection of vehicles, which allows\njobs to be offloaded among vehicles, can substantially alleviate heavy on-board\nworkloads and enable on-demand provisioning of computational resources. In this\npaper, we present a novel framework for vehicular clouds that maps components\nof graph jobs to service providers via opportunistic vehicle-to-vehicle\ncommunication. Then, graph job allocation over vehicular clouds is formulated\nas a non-linear integer programming with respect to vehicles' contact duration\nand available resources, aiming to minimize job completion time and data\nexchange cost. The problem is addressed for two scenarios: low-traffic and\nrush-hours. For the former, we determine the optimal solutions for the problem.\nIn the latter case, given the intractable computations for deriving feasible\nallocations, we propose a novel low complexity randomized graph job allocation\nmechanism by considering hierarchical tree based subgraph isomorphism. We\nevaluate the performance of our proposed algorithms through extensive\nsimulations.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 04:36:06 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 13:44:57 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["LiWang", "Minghui", ""], ["Hosseinalipour", "Seyyedali", ""], ["Gao", "Zhibin", ""], ["Tang", "Yuliang", ""], ["Huang", "Lianfen", ""], ["Dai", "Huaiyu", ""]]}, {"id": "1903.02759", "submitter": "Marc Shapiro", "authors": "Sreeja Nair (DELYS), Gustavo Petri, Marc Shapiro (DELYS)", "title": "Invariant Safety for Distributed Applications", "comments": "Workshop on Principles and Practice of Consistency for Distributed\n  Data (PaPoC), Mar 2019, Dresden, Germany.\n  https://novasys.di.fct.unl.pt/conferences/papoc19/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a proof methodology for verifying the safety of data invariants of\nhighly-available distributed applications that replicate state. The proof is\n(1) modular: one can reason about each individual operation separately, and (2)\nsequential: one can reason about a distributed application as if it were\nsequential. We automate the methodology and illustrate the use of the tool with\na representative example.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 07:40:22 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Nair", "Sreeja", "", "DELYS"], ["Petri", "Gustavo", "", "DELYS"], ["Shapiro", "Marc", "", "DELYS"]]}, {"id": "1903.02843", "submitter": "Yoshiaki Katayama", "authors": "Shlomi Dolev, Sayaka Kamei, Yoshiaki Katayama, Fukuhito Ooshita and\n  Koichi Wada", "title": "Neighborhood Mutual Remainder: Self-Stabilizing Implementation of\n  Look-Compute-Move Robots (Extended Abstract)", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local mutual exclusion guarantees that no two neighboring processes enter a\ncritical section at the same time while satisfying both mutual exclusion and no\nstarvation properties. On the other hand, processes may want to execute some\noperation simultaneously with the neighbors. Of course, we can use a globally\nsynchronized clock to achieve the task but it is very expensive to realize it\nin a distributed system in general.\n  In this paper, we define a new concept neighborhood mutual remainder. A\ndistributed algorithm that satisfies the neighborhood mutual remainder\nrequirement should satisfy global fairness, l-exclusion and repeated local\nrendezvous requirements. Global fairness is satisfied when each process (that\nrequests to enter the critical section infinitely often) executes the critical\nsection infinitely often, l-exclusion is satisfied when at most l neighboring\nprocesses enter the critical section at the same time, and repeated local\nrendezvous is satisfied when for each process infinitely often no process in\nthe closed neighborhood is in the critical or trying section.\n  We first formalize the concept of neighborhood mutual remainder, and give a\nsimple self-stabilizing algorithm to demonstrate the design paradigm to achieve\nneighborhood mutual remainder. We also present two applications of neighborhood\nmutual remainder to a Look-Compute-Move robot system. One is for implementing a\nmove-atomic property and the other is for implementing FSYNC scheduler, where\nrobots possess an independent clock that is advanced in the same speed. These\nare the first self-stabilizing implementations of the LCM synchronization.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 11:25:37 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Dolev", "Shlomi", ""], ["Kamei", "Sayaka", ""], ["Katayama", "Yoshiaki", ""], ["Ooshita", "Fukuhito", ""], ["Wada", "Koichi", ""]]}, {"id": "1903.02891", "submitter": "Felix Sattler", "authors": "Felix Sattler, Simon Wiedemann, Klaus-Robert M\\\"uller, Wojciech Samek", "title": "Robust and Communication-Efficient Federated Learning from Non-IID Data", "comments": "17 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning allows multiple parties to jointly train a deep learning\nmodel on their combined data, without any of the participants having to reveal\ntheir local data to a centralized server. This form of privacy-preserving\ncollaborative learning however comes at the cost of a significant communication\noverhead during training. To address this problem, several compression methods\nhave been proposed in the distributed training literature that can reduce the\namount of required communication by up to three orders of magnitude. These\nexisting methods however are only of limited utility in the Federated Learning\nsetting, as they either only compress the upstream communication from the\nclients to the server (leaving the downstream communication uncompressed) or\nonly perform well under idealized conditions such as iid distribution of the\nclient data, which typically can not be found in Federated Learning. In this\nwork, we propose Sparse Ternary Compression (STC), a new compression framework\nthat is specifically designed to meet the requirements of the Federated\nLearning environment. Our experiments on four different learning tasks\ndemonstrate that STC distinctively outperforms Federated Averaging in common\nFederated Learning scenarios where clients either a) hold non-iid data, b) use\nsmall batch sizes during training, or where c) the number of clients is large\nand the participation rate in every communication round is low. We furthermore\nshow that even if the clients hold iid data and use medium sized batches for\ntraining, STC still behaves pareto-superior to Federated Averaging in the sense\nthat it achieves fixed target accuracies on our benchmarks within both fewer\ntraining iterations and a smaller communication budget.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 13:10:30 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Sattler", "Felix", ""], ["Wiedemann", "Simon", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Samek", "Wojciech", ""]]}, {"id": "1903.03008", "submitter": "Tahar Kechadi M", "authors": "Lamine M. Aouad, Nhien-An Le-Khac, Tahar M. Kechadi", "title": "Performance study of distributed Apriori-like frequent itemsets mining", "comments": null, "journal-ref": "Knowledge and Information Systems April 2010, Volume 23, Issue 1", "doi": "10.1007/s10115-009-0205-3", "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we focus on distributed Apriori-based frequent itemsets\nmining. We present a new distributed approach which takes into account inherent\ncharacteristics of this algorithm. We study the distribution aspect of this\nalgorithm and give a comparison of the proposed approach with a classical\nApriori-like distributed algorithm, using both analytical and experimental\nstudies. We find that under a wide range of conditions and datasets, the\nperformance of a distributed Apriori-like algorithm is not related to global\nstrategies of pruning since the performance of the local Apriori generation is\nusually characterized by relatively high success rates of candidate sets\nfrequency at low levels which switch to very low rates at some stage, and often\ndrops to zero. This means that the intermediate communication steps and remote\nsupport counts computation and collection in classical distributed schemes are\ncomputationally inefficient locally, and then constrains the global\nperformance. Our performance evaluation is done on a large cluster of\nworkstations using the Condor system and its workflow manager DAGMan. The\nresults show that the presented approach greatly enhances the performance and\nachieves good scalability compared to a typical distributed Apriori founded\nalgorithm.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 13:47:35 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Aouad", "Lamine M.", ""], ["Le-Khac", "Nhien-An", ""], ["Kechadi", "Tahar M.", ""]]}, {"id": "1903.03023", "submitter": "Patrick Diehl", "authors": "Tianyi Zhang, Shahrzad Shirzad, Patrick Diehl, R. Tohid, Weile Wei,\n  Hartmut Kaiser", "title": "An Introduction to hpxMP: A Modern OpenMP Implementation Leveraging HPX,\n  An Asynchronous Many-Task System", "comments": null, "journal-ref": null, "doi": "10.1145/3318170.3318191", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Asynchronous Many-task (AMT) runtime systems have gained increasing\nacceptance in the HPC community due to the performance improvements offered by\nfine-grained tasking runtime systems. At the same time, C++ standardization\nefforts are focused on creating higher-level interfaces able to replace OpenMP\nor OpenACC in modern C++ codes. These higher level functions have been adopted\nin standards conforming runtime systems such as HPX, giving users the ability\nto simply utilize fork-join parallelism in their own codes. Despite innovations\nin runtime systems and standardization efforts users face enormous challenges\nporting legacy applications. Not only must users port their own codes, but\noften users rely on highly optimized libraries such as BLAS and LAPACK which\nuse OpenMP for parallization. Current efforts to create smooth migration paths\nhave struggled with these challenges, especially as the threading systems of\nAMT libraries often compete with the treading system of OpenMP.\n  To overcome these issues, our team has developed hpxMP, an implementation of\nthe OpenMP standard, which utilizes the underlying AMT system to schedule and\nmanage tasks. This approach leverages the C++ interfaces exposed by HPX and\nallows users to execute their applications on an AMT system without changing\ntheir code.\n  In this work, we compare hpxMP with Clang's OpenMP library with four linear\nalgebra benchmarks of the Blaze C++ library. While hpxMP is often not able to\nreach the same performance, we demonstrate viability for providing a smooth\nmigration for applications but have to be extended to benefit from a more\ngeneral task based programming model.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 16:22:02 GMT"}, {"version": "v2", "created": "Wed, 20 Mar 2019 20:19:35 GMT"}, {"version": "v3", "created": "Fri, 5 Jul 2019 19:09:31 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Zhang", "Tianyi", ""], ["Shirzad", "Shahrzad", ""], ["Diehl", "Patrick", ""], ["Tohid", "R.", ""], ["Wei", "Weile", ""], ["Kaiser", "Hartmut", ""]]}, {"id": "1903.03051", "submitter": "Navjot Kukreja", "authors": "Navjot Kukreja, Alena Shilova, Olivier Beaumont, Jan Huckelheim,\n  Nicola Ferrier, Paul Hovland, Gerard Gorman", "title": "Training on the Edge: The why and the how", "comments": "Submitted to PAISE 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Edge computing is the natural progression from Cloud computing, where,\ninstead of collecting all data and processing it centrally, like in a cloud\ncomputing environment, we distribute the computing power and try to do as much\nprocessing as possible, close to the source of the data. There are various\nreasons this model is being adopted quickly, including privacy, and reduced\npower and bandwidth requirements on the Edge nodes. While it is common to see\ninference being done on Edge nodes today, it is much less common to do training\non the Edge. The reasons for this range from computational limitations, to it\nnot being advantageous in reducing communications between the Edge nodes. In\nthis paper, we explore some scenarios where it is advantageous to do training\non the Edge, as well as the use of checkpointing strategies to save memory.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 11:04:14 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Kukreja", "Navjot", ""], ["Shilova", "Alena", ""], ["Beaumont", "Olivier", ""], ["Huckelheim", "Jan", ""], ["Ferrier", "Nicola", ""], ["Hovland", "Paul", ""], ["Gorman", "Gerard", ""]]}, {"id": "1903.03129", "submitter": "Beidi Chen", "authors": "Beidi Chen, Tharun Medini, James Farwell, Sameh Gobriel, Charlie Tai,\n  Anshumali Shrivastava", "title": "SLIDE : In Defense of Smart Algorithms over Hardware Acceleration for\n  Large-Scale Deep Learning Systems", "comments": "Published at MLSys 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning (DL) algorithms are the central focus of modern machine\nlearning systems. As data volumes keep growing, it has become customary to\ntrain large neural networks with hundreds of millions of parameters to maintain\nenough capacity to memorize these volumes and obtain state-of-the-art accuracy.\nTo get around the costly computations associated with large models and data,\nthe community is increasingly investing in specialized hardware for model\ntraining. However, specialized hardware is expensive and hard to generalize to\na multitude of tasks. The progress on the algorithmic front has failed to\ndemonstrate a direct advantage over powerful hardware such as NVIDIA-V100 GPUs.\nThis paper provides an exception. We propose SLIDE (Sub-LInear Deep learning\nEngine) that uniquely blends smart randomized algorithms, with multi-core\nparallelism and workload optimization. Using just a CPU, SLIDE drastically\nreduces the computations during both training and inference outperforming an\noptimized implementation of Tensorflow (TF) on the best available GPU. Our\nevaluations on industry-scale recommendation datasets, with large fully\nconnected architectures, show that training with SLIDE on a 44 core CPU is more\nthan 3.5 times (1 hour vs. 3.5 hours) faster than the same network trained\nusing TF on Tesla V100 at any given accuracy level. On the same CPU hardware,\nSLIDE is over 10x faster than TF. We provide codes and scripts for\nreproducibility.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 19:12:07 GMT"}, {"version": "v2", "created": "Sun, 1 Mar 2020 03:17:52 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Chen", "Beidi", ""], ["Medini", "Tharun", ""], ["Farwell", "James", ""], ["Gobriel", "Sameh", ""], ["Tai", "Charlie", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "1903.03164", "submitter": "Hao Tan", "authors": "Hao Tan, Wojciech Golab", "title": "Shallow Overlay Trees Suffice for High-Throughput Consensus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  All-to-all data transmission is a typical data transmission pattern in\nblockchain systems. Developing an optimization scheme that provides high\nthroughput and low latency data transmission can significantly benefit the\nperformance of those systems. In this work, we consider the problem of\noptimizing all-to-all data transmission in a wide area network(WAN) using\noverlay multicast. We prove that in a congestion-free core network model, using\nshallow broadcast trees with heights up to two is sufficient for all-to-all\ndata transmission to achieve the optimal throughput allowed by the available\nnetwork resources.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 20:15:45 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Tan", "Hao", ""], ["Golab", "Wojciech", ""]]}, {"id": "1903.03337", "submitter": "Marco Landoni Dr", "authors": "M. Landoni, G. Taffoni, A. Bignamini and R. Smareglia", "title": "Application of Google Cloud Platform in Astrophysics", "comments": "ADASS XXVIII Conference paper - Published by the Astronomical Society\n  of the Pacific (ASP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of new Cloud Platform offered by Google motivated us to\npropose nine Proof of Concepts (PoC) aiming to demonstrated and test the\ncapabilities of the platform in the context of scientifically-driven tasks and\nrequirements. We review the status of our initiative by illustrating 3 out of 9\nsuccessfully closed PoC that we implemented on Google Cloud Platform. In\nparticular, we illustrate a cloud architecture for deployment of scientific\nsoftware as microservice coupling Google Compute Engine with Docker and Pub/Sub\nto dispatch heavily parallel simulations. We detail also an experiment for HPC\nbased simulation and workflow executions of data reduction pipelines (for the\nTNG-GIANO-B spectrograph) deployed on GCP. We compare and contrast our\nexperience with on-site facilities comparing advantages and disadvantages both\nin terms of total cost of ownership and reached performances.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 09:32:51 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Landoni", "M.", ""], ["Taffoni", "G.", ""], ["Bignamini", "A.", ""], ["Smareglia", "R.", ""]]}, {"id": "1903.03501", "submitter": "Aurojit Panda", "authors": "Aurojit Panda", "title": "Certifying Safety when Implementing Consensus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Ensuring the correctness of distributed system implementations remains a\nchallenging and largely unaddressed problem. In this paper we present a\nprotocol that can be used to certify the safety of consensus implementations.\nOur proposed protocol is efficient both in terms of the number of additional\nmessages sent and their size, and is designed to operate correctly in the\npresence of $n-1$ nodes failing in an $n$ node distributed system (assuming\nfail-stop failures). We also comment on how our construction might be\ngeneralized to certify other protocols and invariants.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 15:28:44 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Panda", "Aurojit", ""]]}, {"id": "1903.03640", "submitter": "Crist\\'obal A. Navarro", "authors": "Roberto Carrasco, Raimundo Vega, Crist\\'obal A. Navarro", "title": "Analyzing GPU Tensor Core Potential for Fast Reductions", "comments": "This paper was presented in the SCCC 2018 Conference, November 5", "journal-ref": "37th Internatioinal Conference of the Chilean Computer Science\n  Society, SCCC 2018, November 5-9, Santiago, Chile, 2018", "doi": "10.29007/zlmg", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Nvidia GPU architecture has introduced new computing elements such as the\n\\textit{tensor cores}, which are special processing units dedicated to perform\nfast matrix-multiply-accumulate (MMA) operations and accelerate \\textit{Deep\nLearning} applications. In this work we present the idea of using tensor cores\nfor a different purpose such as the parallel arithmetic reduction problem, and\npropose a new GPU tensor-core based algorithm as well as analyze its potential\nperformance benefits in comparison to a traditional GPU-based one. The proposed\nmethod, encodes the reduction of $n$ numbers as a set of $m\\times m$ MMA\ntensor-core operations (for Nvidia's Volta architecture $m=16$) and takes\nadvantage from the fact that each MMA operation takes just one GPU cycle. When\nanalyzing the cost under a simplified GPU computing model, the result is that\nthe new algorithm manages to reduce a problem of $n$ numbers in $T(n) =\n5\\log_{m^2}(n)$ steps with a speedup of $S = \\frac{4}{5}\\log_2(m^2)$.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 19:33:04 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Carrasco", "Roberto", ""], ["Vega", "Raimundo", ""], ["Navarro", "Crist\u00f3bal A.", ""]]}, {"id": "1903.03659", "submitter": "Fande Kong", "authors": "Fande Kong, Yaqi Wang, Derek R. Gaston, Cody J. Permann, Andrew E.\n  Slaughter, Alexander D. Lindsay, Richard C. Martineau", "title": "A highly parallel multilevel Newton-Krylov-Schwarz method with\n  subspace-based coarsening and partition-based balancing for the multigroup\n  neutron transport equations on 3D unstructured meshes", "comments": "Submitted to SIAM Journal on Scientific Computing. 25 pages and 9\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multigroup neutron transport equations have been widely used to study the\nmotion of neutrons and their interactions with the background materials.\nNumerical simulation of the multigroup neutron transport equations is\ncomputationally challenging because the equations is defined on a high\ndimensional phase space (1D in energy, 2D in angle, and 3D in spatial space),\nand furthermore, for realistic applications, the computational spatial domain\nis complex and the materials are heterogeneous. The multilevel domain\ndecomposition methods is one of the most popular algorithms for solving the\nmultigroup neutron transport equations, but the construction of coarse spaces\nis expensive and often not strongly scalable when the number of processor cores\nis large. In this paper, we study a highly parallel multilevel\nNewton-Krylov-Schwarz method equipped with several novel components, such as\nsubspace-based coarsening, partition-based balancing and hierarchical mesh\npartitioning, that enable the overall simulation strongly scalable in terms of\nthe compute time. Compared with the traditional coarsening method, the\nsubspace-based coarsening algorithm significantly reduces the cost of the\npreconditioner setup that is often unscalable. In addition, the partition-based\nbalancing strategy enhances the parallel efficiency of the overall solver by\nassigning a nearly-equal amount of work to each processor core. The\nhierarchical mesh partitioning is able to generate a large number of subdomains\nand meanwhile minimizes the off-node communication. We numerically show that\nthe proposed algorithm is scalable with more than 10,000 processor cores for a\nrealistic application with a few billions unknowns on 3D unstructured meshes.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 20:25:48 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Kong", "Fande", ""], ["Wang", "Yaqi", ""], ["Gaston", "Derek R.", ""], ["Permann", "Cody J.", ""], ["Slaughter", "Andrew E.", ""], ["Lindsay", "Alexander D.", ""], ["Martineau", "Richard C.", ""]]}, {"id": "1903.03934", "submitter": "Cong Xie", "authors": "Cong Xie, Sanmi Koyejo, Indranil Gupta", "title": "Asynchronous Federated Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning enables training on a massive number of edge devices. To\nimprove flexibility and scalability, we propose a new asynchronous federated\noptimization algorithm. We prove that the proposed approach has near-linear\nconvergence to a global optimum, for both strongly convex and a restricted\nfamily of non-convex problems. Empirical results show that the proposed\nalgorithm converges quickly and tolerates staleness in various applications.\n", "versions": [{"version": "v1", "created": "Sun, 10 Mar 2019 06:19:38 GMT"}, {"version": "v2", "created": "Wed, 13 Mar 2019 14:19:49 GMT"}, {"version": "v3", "created": "Mon, 27 May 2019 05:17:39 GMT"}, {"version": "v4", "created": "Thu, 26 Sep 2019 21:54:02 GMT"}, {"version": "v5", "created": "Sat, 5 Dec 2020 01:33:57 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Xie", "Cong", ""], ["Koyejo", "Sanmi", ""], ["Gupta", "Indranil", ""]]}, {"id": "1903.03936", "submitter": "Cong Xie", "authors": "Cong Xie, Sanmi Koyejo, Indranil Gupta", "title": "Fall of Empires: Breaking Byzantine-tolerant SGD by Inner Product\n  Manipulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, new defense techniques have been developed to tolerate Byzantine\nfailures for distributed machine learning. The Byzantine model captures workers\nthat behave arbitrarily, including malicious and compromised workers. In this\npaper, we break two prevailing Byzantine-tolerant techniques. Specifically we\nshow robust aggregation methods for synchronous SGD -- coordinate-wise median\nand Krum -- can be broken using new attack strategies based on inner product\nmanipulation. We prove our results theoretically, as well as show empirical\nvalidation.\n", "versions": [{"version": "v1", "created": "Sun, 10 Mar 2019 06:26:01 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Xie", "Cong", ""], ["Koyejo", "Sanmi", ""], ["Gupta", "Indranil", ""]]}, {"id": "1903.04134", "submitter": "Mohammad Jalalzai", "authors": "Mohammad M. Jalalzai, Costas Busch and Golden Richard III", "title": "Proteus: A Scalable BFT Consesus Protocol for Blockchains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Byzantine Fault Tolerant (BFT) consensus exhibits higher throughput in\ncomparison to Proof of Work (PoW) in blockchains. But BFT-based protocols\nsuffer from scalability problems with respect to the number of replicas in the\nnetwork. The main reason for this limitation is the quadratic message\ncomplexity of BFT protocols. Previously, proposed solutions improve BFT\nperformance for normal operation, but will fall back to quadratic message\ncomplexity once the protocol observes a certain number of failures. This makes\nthe protocol performance unpredictable as it is not guaranteed that the network\nwill face a a certain number of failures.\n  As a result, such protocols are only scalable when conditions are favorable\n(i.e., the number of failures are less than a given threshold). To address this\nissue we propose Proteus, a new BFT-based consensus protocol which elects a\nsubset of nodes $c$ as a root committee. Proteus guarantees stable performance,\nregardless of the number of failures in the network and it improves on the\nquadratic message complexity of typical BFT-based protocols to $O(cn)$, where\n$c<<n$, for large $n$.\n  Thus, message complexity remains small and less than quadratic when $c$ is\nasymptotically smaller than $n$, and this helps the protocol to provide stable\nperformance even during the view change process (change of root committee). Our\nview change process is different than typical BFT protocols as it replaces the\nwhole root committee compared to replacing a single primary in other protocols.\nWe deployed and tested our protocol on $200$ Amazon $EC2$ instances, with two\ndifferent baseline BFT protocols (PBFT and Bchain) for comparison. In these\ntests, our protocol outperformed the baselines by more than $2\\times$ in terms\nof throughput as well as latency.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 05:41:42 GMT"}, {"version": "v2", "created": "Mon, 18 Mar 2019 16:58:42 GMT"}, {"version": "v3", "created": "Fri, 26 Apr 2019 21:27:08 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Jalalzai", "Mohammad M.", ""], ["Busch", "Costas", ""], ["Richard", "Golden", "III"]]}, {"id": "1903.04203", "submitter": "Rafael Pereira  Pires", "authors": "Christian G\\\"ottel, Rafael Pires, Isabelly Rocha, S\\'ebastien Vaucher,\n  Pascal Felber, Marcelo Pasin, Valerio Schiavoni", "title": "Security, Performance and Energy Trade-offs of Hardware-assisted Memory\n  Protection Mechanisms", "comments": "European Commission Project: LEGaTO - Low Energy Toolset for\n  Heterogeneous Computing (EC-H2020-780681)", "journal-ref": "2018 IEEE 37th Symposium on Reliable Distributed Systems (SRDS),\n  Salvador, Brazil, 2018, pp. 133-142", "doi": "10.1109/SRDS.2018.00024", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deployment of large-scale distributed systems, e.g., publish-subscribe\nplatforms, that operate over sensitive data using the infrastructure of public\ncloud providers, is nowadays heavily hindered by the surging lack of trust\ntoward the cloud operators. Although purely software-based solutions exist to\nprotect the confidentiality of data and the processing itself, such as\nhomomorphic encryption schemes, their performance is far from being practical\nunder real-world workloads.\n  The performance trade-offs of two novel hardware-assisted memory protection\nmechanisms, namely AMD SEV and Intel SGX - currently available on the market to\ntackle this problem, are described in this practical experience.\n  Specifically, we implement and evaluate a publish/subscribe use-case and\nevaluate the impact of the memory protection mechanisms and the resulting\nperformance. This paper reports on the experience gained while building this\nsystem, in particular when having to cope with the technical limitations\nimposed by SEV and SGX.\n  Several trade-offs that provide valuable insights in terms of latency,\nthroughput, processing time and energy requirements are exhibited by means of\nmicro- and macro-benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 10:30:07 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2019 15:28:32 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["G\u00f6ttel", "Christian", ""], ["Pires", "Rafael", ""], ["Rocha", "Isabelly", ""], ["Vaucher", "S\u00e9bastien", ""], ["Felber", "Pascal", ""], ["Pasin", "Marcelo", ""], ["Schiavoni", "Valerio", ""]]}, {"id": "1903.04205", "submitter": "Stefanos Leonardos Mr.", "authors": "Vitalik Buterin and Daniel Reijsbergen and Stefanos Leonardos and\n  Georgios Piliouras", "title": "Incentives in Ethereum's Hybrid Casper Protocol", "comments": "Conference version: IEEE International Conference on Blockchain and\n  Cryptocurrency (2019)", "journal-ref": "International Journal of Network Management, Vol. 30(5),\n  pp.:e2098, (2020)", "doi": "10.1002/nem.2098", "report-no": null, "categories": "cs.CR cs.DC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an overview of hybrid Casper the Friendly Finality Gadget (FFG): a\nProof-of-Stake checkpointing protocol overlaid onto Ethereum's Proof-of-Work\nblockchain. We describe its core functionalities and reward scheme, and explore\nits properties. Our findings indicate that Casper's implemented incentives\nmechanism ensures liveness, while providing safety guarantees that improve over\nstandard Proof-of-Work protocols. Based on a minimal-impact implementation of\nthe protocol as a smart contract on the blockchain, we discuss additional\nissues related to parametrisation, funding, throughput and network overhead and\ndetect potential limitations.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 10:33:13 GMT"}, {"version": "v2", "created": "Sat, 21 Mar 2020 09:29:36 GMT"}, {"version": "v3", "created": "Sun, 18 Jul 2021 13:03:19 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Buterin", "Vitalik", ""], ["Reijsbergen", "Daniel", ""], ["Leonardos", "Stefanos", ""], ["Piliouras", "Georgios", ""]]}, {"id": "1903.04243", "submitter": "Ashish Agarwal", "authors": "Ashish Agarwal, Igor Ganichev", "title": "Auto-Vectorizing TensorFlow Graphs: Jacobians, Auto-Batching And Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a static loop vectorization optimization on top of high level\ndataflow IR used by frameworks like TensorFlow. A new statically vectorized\nparallel-for abstraction is provided on top of TensorFlow, and used for\napplications ranging from auto-batching and per-example gradients, to jacobian\ncomputation, optimized map functions and input pipeline optimization. We report\nhuge speedups compared to both loop based implementations, as well as run-time\nbatching adopted by the DyNet framework.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 03:11:02 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Agarwal", "Ashish", ""], ["Ganichev", "Igor", ""]]}, {"id": "1903.04277", "submitter": "Xinlei Yi", "authors": "Xinlei Yi, Xiuxian Li, Lihua Xie, and Karl H. Johansson", "title": "Distributed Online Convex Optimization with Time-Varying Coupled\n  Inequality Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers distributed online optimization with time-varying\ncoupled inequality constraints. The global objective function is composed of\nlocal convex cost and regularization functions and the coupled constraint\nfunction is the sum of local convex functions. A distributed online primal-dual\ndynamic mirror descent algorithm is proposed to solve this problem, where the\nlocal cost, regularization, and constraint functions are held privately and\nrevealed only after each time slot. Without assuming Slater's condition, we\nfirst derive regret and constraint violation bounds for the algorithm and show\nhow they depend on the stepsize sequences, the accumulated dynamic variation of\nthe comparator sequence, the number of agents, and the network connectivity. As\na result, under some natural decreasing stepsize sequences, we prove that the\nalgorithm achieves sublinear dynamic regret and constraint violation if the\naccumulated dynamic variation of the optimal sequence also grows sublinearly.\nWe also prove that the algorithm achieves sublinear static regret and\nconstraint violation under mild conditions. Assuming Slater's condition, we\nshow that the algorithm achieves smaller bounds on the constraint violation. In\naddition, smaller bounds on the static regret are achieved when the objective\nfunction is strongly convex. Finally, numerical simulations are provided to\nillustrate the effectiveness of the theoretical results.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 20:29:00 GMT"}, {"version": "v2", "created": "Wed, 5 Jun 2019 10:09:40 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Yi", "Xinlei", ""], ["Li", "Xiuxian", ""], ["Xie", "Lihua", ""], ["Johansson", "Karl H.", ""]]}, {"id": "1903.04364", "submitter": "Steven W. D. Chien", "authors": "Steven W. D. Chien, Stefano Markidis, Vyacheslav Olshevsky, Yaroslav\n  Bulatov, Erwin Laure, Jeffrey S. Vetter", "title": "TensorFlow Doing HPC", "comments": "Accepted for publication at The Ninth International Workshop on\n  Accelerators and Hybrid Exascale Systems (AsHES'19)", "journal-ref": null, "doi": "10.1109/IPDPSW.2019.00092", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  TensorFlow is a popular emerging open-source programming framework supporting\nthe execution of distributed applications on heterogeneous hardware. While\nTensorFlow has been initially designed for developing Machine Learning (ML)\napplications, in fact TensorFlow aims at supporting the development of a much\nbroader range of application kinds that are outside the ML domain and can\npossibly include HPC applications. However, very few experiments have been\nconducted to evaluate TensorFlow performance when running HPC workloads on\nsupercomputers. This work addresses this lack by designing four traditional HPC\nbenchmark applications: STREAM, matrix-matrix multiply, Conjugate Gradient (CG)\nsolver and Fast Fourier Transform (FFT). We analyze their performance on two\nsupercomputers with accelerators and evaluate the potential of TensorFlow for\ndeveloping HPC applications. Our tests show that TensorFlow can fully take\nadvantage of high performance networks and accelerators on supercomputers.\nRunning our TensorFlow STREAM benchmark, we obtain over 50% of theoretical\ncommunication bandwidth on our testing platform. We find an approximately 2x,\n1.7x and 1.8x performance improvement when increasing the number of GPUs from\ntwo to four in the matrix-matrix multiply, CG and FFT applications\nrespectively. All our performance results demonstrate that TensorFlow has high\npotential of emerging also as HPC programming framework for heterogeneous\nsupercomputers.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 15:20:01 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Chien", "Steven W. D.", ""], ["Markidis", "Stefano", ""], ["Olshevsky", "Vyacheslav", ""], ["Bulatov", "Yaroslav", ""], ["Laure", "Erwin", ""], ["Vetter", "Jeffrey S.", ""]]}, {"id": "1903.04394", "submitter": "Gennadi I. Malaschonok", "authors": "Gennadi Malaschonok and Evgeni Ilchenko", "title": "Recursive Matrix Algorithms in Commutative Domain for Cluster with\n  Distributed Memory", "comments": "8 pages, 9 figures", "journal-ref": "2018 Ivannikov Memorial Workshop (IVMEM), (Yerevan, Armenia, 3-4\n  May 2018) Publisher: IEEE 2019, p.40-47", "doi": "10.1109/IVMEM.2018.00015", "report-no": null, "categories": "cs.SC cs.DC math.RA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an overview of the theoretical results for matrix block-recursive\nalgorithms in commutative domains and present the results of experiments that\nwe conducted with new parallel programs based on these algorithms on a\nsupercomputer MVS-10P at the Joint Supercomputer Center of the Russian Academy\nof Science. To demonstrate a scalability of these programs we measure the\nrunning time of the program for a different number of processors and plot the\ngraphs of efficiency factor. Also we present the main application areas in\nwhich such parallel algorithms are used. It is concluded that this class of\nalgorithms allows to obtain efficient parallel programs on clusters with\ndistributed memory.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 16:07:22 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Malaschonok", "Gennadi", ""], ["Ilchenko", "Evgeni", ""]]}, {"id": "1903.04395", "submitter": "Langshi Chen", "authors": "Langshi Chen, Jiayu Li, Ariful Azad, Lei Jiang, Madhav Marathe, Anil\n  Vullikanti, Andrey Nikolaev, Egor Smirnov, Ruslan Israfilov, Judy Qiu", "title": "A GraphBLAS Approach for Subgraph Counting", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subgraph counting aims to count the occurrences of a subgraph template T in a\ngiven network G. The basic problem of computing structural properties such as\ncounting triangles and other subgraphs has found applications in diverse\ndomains. Recent biological, social, cybersecurity and sensor network\napplications have motivated solving such problems on massive networks with\nbillions of vertices. The larger subgraph problem is known to be memory bounded\nand computationally challenging to scale; the complexity grows both as a\nfunction of T and G. In this paper, we study the non-induced tree subgraph\ncounting problem, propose a novel layered softwarehardware co-design approach,\nand implement a shared-memory multi-threaded algorithm: 1) reducing the\ncomplexity of the parallel color-coding algorithm by identifying and pruning\nredundant graph traversal; 2) achieving a fully-vectorized implementation upon\nlinear algebra kernels inspired by GraphBLAS, which significantly improves\ncache usage and maximizes memory bandwidth utilization. Experiments show that\nour implementation improves the overall performance over the state-of-the-art\nwork by orders of magnitude and up to 660x for subgraph templates with size\nover 12 on a dual-socket Intel(R) Xeon(R) Platinum 8160 server. We believe our\napproach using GraphBLAS with optimized sparse linear algebra can be applied to\nother massive subgraph counting problems and emerging high-memory bandwidth\nhardware architectures.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 16:08:40 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Chen", "Langshi", ""], ["Li", "Jiayu", ""], ["Azad", "Ariful", ""], ["Jiang", "Lei", ""], ["Marathe", "Madhav", ""], ["Vullikanti", "Anil", ""], ["Nikolaev", "Andrey", ""], ["Smirnov", "Egor", ""], ["Israfilov", "Ruslan", ""], ["Qiu", "Judy", ""]]}, {"id": "1903.04488", "submitter": "Enayat Ullah", "authors": "Nikita Ivkin, Daniel Rothchild, Enayat Ullah, Vladimir Braverman, Ion\n  Stoica, Raman Arora", "title": "Communication-efficient distributed SGD with Sketching", "comments": "19 pages, 6 figures, published at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale distributed training of neural networks is often limited by\nnetwork bandwidth, wherein the communication time overwhelms the local\ncomputation time. Motivated by the success of sketching methods in\nsub-linear/streaming algorithms, we introduce Sketched SGD, an algorithm for\ncarrying out distributed SGD by communicating sketches instead of full\ngradients. We show that Sketched SGD has favorable convergence rates on several\nclasses of functions. When considering all communication -- both of gradients\nand of updated model weights -- Sketched SGD reduces the amount of\ncommunication required compared to other gradient compression methods from\n$\\mathcal{O}(d)$ or $\\mathcal{O}(W)$ to $\\mathcal{O}(\\log d)$, where $d$ is the\nnumber of model parameters and $W$ is the number of workers participating in\ntraining. We run experiments on a transformer model, an LSTM, and a residual\nnetwork, demonstrating up to a 40x reduction in total communication cost with\nno loss in final model performance. We also show experimentally that Sketched\nSGD scales to at least 256 workers without increasing communication cost or\ndegrading model performance.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 17:59:48 GMT"}, {"version": "v2", "created": "Thu, 6 Jun 2019 21:49:03 GMT"}, {"version": "v3", "created": "Thu, 23 Jan 2020 15:59:50 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Ivkin", "Nikita", ""], ["Rothchild", "Daniel", ""], ["Ullah", "Enayat", ""], ["Braverman", "Vladimir", ""], ["Stoica", "Ion", ""], ["Arora", "Raman", ""]]}, {"id": "1903.04563", "submitter": "Yu Chen", "authors": "Seyed Yahya Nikouei, Ronghua Xu, Yu Chen, Alex Aved, Erik Blasch", "title": "Decentralized Smart Surveillance through Microservices Platform", "comments": "2019 SPIE Defense + Commercial Sensing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Connected societies require reliable measures to assure the safety, privacy,\nand security of members. Public safety technology has made fundamental\nimprovements since the first generation of surveillance cameras were\nintroduced, which aims to reduce the role of observer agents so that no\nabnormality goes unnoticed. While the edge computing paradigm promises\nsolutions to address the shortcomings of cloud computing, e.g., the extra\ncommunication delay and network security issues, it also introduces new\nchallenges. One of the main concerns is the limited computing power at the edge\nto meet the on-site dynamic data processing. In this paper, a Lightweight IoT\n(Internet of Things) based Smart Public Safety (LISPS) framework is proposed on\ntop of microservices architecture. As a computing hierarchy at the edge, the\nLISPS system possesses high flexibility in the design process, loose coupling\nto add new services or update existing functions without interrupting the\nnormal operations, and efficient power balancing. A real-world public safety\nmonitoring scenario is selected to verify the effectiveness of LISPS, which\ndetects, tracks human objects and identify suspicious activities. The\nexperimental results demonstrate the feasibility of the approach.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 19:46:59 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Nikouei", "Seyed Yahya", ""], ["Xu", "Ronghua", ""], ["Chen", "Yu", ""], ["Aved", "Alex", ""], ["Blasch", "Erik", ""]]}, {"id": "1903.04611", "submitter": "Ang Li", "authors": "Ang Li and Shuaiwen Leon Song and Jieyang Chen and Jiajia Li and Xu\n  Liu and Nathan Tallent and Kevin Barker", "title": "Evaluating Modern GPU Interconnect: PCIe, NVLink, NV-SLI, NVSwitch and\n  GPUDirect", "comments": "15 pages. The paper is going to be submitted to TPDS", "journal-ref": null, "doi": "10.1109/TPDS.2019.2928289", "report-no": null, "categories": "cs.AR cs.DC cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High performance multi-GPU computing becomes an inevitable trend due to the\never-increasing demand on computation capability in emerging domains such as\ndeep learning, big data and planet-scale simulations. However, the lack of deep\nunderstanding on how modern GPUs can be connected and the real impact of\nstate-of-the-art interconnect technology on multi-GPU application performance\nbecome a hurdle. In this paper, we fill the gap by conducting a thorough\nevaluation on five latest types of modern GPU interconnects: PCIe, NVLink-V1,\nNVLink-V2, NVLink-SLI and NVSwitch, from six high-end servers and HPC\nplatforms: NVIDIA P100-DGX-1, V100-DGX-1, DGX-2, OLCF's SummitDev and Summit\nsupercomputers, as well as an SLI-linked system with two NVIDIA Turing RTX-2080\nGPUs. Based on the empirical evaluation, we have observed four new types of GPU\ncommunication network NUMA effects: three are triggered by NVLink's topology,\nconnectivity and routing, while one is caused by PCIe chipset design issue.\nThese observations indicate that, for an application running in a multi-GPU\nnode, choosing the right GPU combination can impose considerable impact on GPU\ncommunication efficiency, as well as the application's overall performance. Our\nevaluation can be leveraged in building practical multi-GPU performance models,\nwhich are vital for GPU task allocation, scheduling and migration in a shared\nenvironment (e.g., AI cloud and HPC centers), as well as communication-oriented\nperformance tuning.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 21:21:21 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Li", "Ang", ""], ["Song", "Shuaiwen Leon", ""], ["Chen", "Jieyang", ""], ["Li", "Jiajia", ""], ["Liu", "Xu", ""], ["Tallent", "Nathan", ""], ["Barker", "Kevin", ""]]}, {"id": "1903.04709", "submitter": "Wei Du", "authors": "Wei Du, Tao Lei, Qiang He, Wei Liu, Qiwang Lei, Hailiang Zhao, Wei\n  Wang", "title": "Service Capacity Enhanced Task Offloading and Resource Allocation in\n  Multi-Server Edge Computing Environment", "comments": "This paper has been accepted by Early Submission Phase of ICWS2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An edge computing environment features multiple edge servers and multiple\nservice clients. In this environment, mobile service providers can offload\nclient-side computation tasks from service clients' devices onto edge servers\nto reduce service latency and power consumption experienced by the clients. A\ncritical issue that has yet to be properly addressed is how to allocate edge\ncomputing resources to achieve two optimization objectives: 1) minimize the\nservice cost measured by the service latency and the power consumption\nexperienced by service clients; and 2) maximize the service capacity measured\nby the number of service clients that can offload their computation tasks in\nthe long term. This paper formulates this long-term problem as a stochastic\noptimization problem and solves it with an online algorithm based on Lyapunov\noptimization. This NP-hard problem is decomposed into three sub-problems, which\nare then solved with a suite of techniques. The experimental results show that\nour approach significantly outperforms two baseline approaches.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 03:22:20 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Du", "Wei", ""], ["Lei", "Tao", ""], ["He", "Qiang", ""], ["Liu", "Wei", ""], ["Lei", "Qiwang", ""], ["Zhao", "Hailiang", ""], ["Wang", "Wei", ""]]}, {"id": "1903.04880", "submitter": "Supreeth Shastri", "authors": "Aashaka Shah, Vinay Banakar, Supreeth Shastri, Melissa Wasserman, and\n  Vijay Chidambaram", "title": "Analyzing the Impact of GDPR on Storage Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently introduced General Data Protection Regulation (GDPR) is forcing\nseveral companies to make significant changes to their systems to achieve\ncompliance. Motivated by the finding that more than 30% of GDPR articles are\nrelated to storage, we investigate the impact of GDPR compliance on storage\nsystems. We illustrate the challenges of retrofitting existing systems into\ncompliance by modifying Redis to be GDPR-compliant. We show that despite\nneeding to introduce a small set of new features, a strict real-time compliance\n(eg logging every user request synchronously) lowers Redis' throughput by 20x.\nOur work reveals how GDPR allows compliance to be a spectrum, and what its\nimplications are for system designers. We discuss the technical challenges that\nneed to be solved before strict compliance can be efficiently achieved.\n", "versions": [{"version": "v1", "created": "Sat, 9 Mar 2019 02:31:56 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 19:10:32 GMT"}, {"version": "v3", "created": "Thu, 16 May 2019 15:53:44 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Shah", "Aashaka", ""], ["Banakar", "Vinay", ""], ["Shastri", "Supreeth", ""], ["Wasserman", "Melissa", ""], ["Chidambaram", "Vijay", ""]]}, {"id": "1903.05133", "submitter": "Fan Zhou", "authors": "Fan Zhou and Guojing Cong", "title": "A Distributed Hierarchical SGD Algorithm with Sparse Global Reduction", "comments": "38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reducing communication in training large-scale machine learning applications\non distributed platform is still a big challenge. To address this issue, we\npropose a distributed hierarchical averaging stochastic gradient descent\n(Hier-AVG) algorithm with infrequent global reduction by introducing local\nreduction. As a general type of parallel SGD, Hier-AVG can reproduce several\npopular synchronous parallel SGD variants by adjusting its parameters. We show\nthat Hier-AVG with infrequent global reduction can still achieve standard\nconvergence rate for non-convex optimization problems. In addition, we show\nthat more frequent local averaging with more participants involved can lead to\nfaster training convergence. By comparing Hier-AVG with another popular\ndistributed training algorithm K-AVG, we show that through deploying local\naveraging with fewer number of global averaging, Hier-AVG can still achieve\ncomparable training speed while frequently get better test accuracy. This\nindicates that local averaging can serve as an alternative remedy to\neffectively reduce communication overhead when the number of learners is large.\nExperimental results of Hier-AVG with several state-of-the-art deep neural nets\non CIFAR-10 and IMAGENET-1K are presented to validate our analysis and show its\nsuperiority.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 18:34:49 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 06:18:49 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Zhou", "Fan", ""], ["Cong", "Guojing", ""]]}, {"id": "1903.05474", "submitter": "R. Ghosh", "authors": "Nikita Bhagatkar and Kapil Dolas and R. K. Ghosh", "title": "An Integrated P2P Framework for E-Learning", "comments": "17 pages, 14 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper is concerned with the design and development of a P2P Presentation\nSystem (P2P-PS) for live streaming of lectures coupled with a shareable\nwhiteboard. Video streaming uses a data-driven modified mesh architecture which\nsupports dynamic fanout. The whiteboard may be used in synchronization with an\nactive video streaming session. It is shared by both the presenter and the\nparticipants in the audience for graphic illustrations during the lecture.\nP2P-PS provides an important feature called {\\em ask doubt} that allows any\nparticipant to send a query (or a doubt) during an active presentation session.\nThe query is disseminated to all other peers by preserving the causality\nbetween {\\em ask doubt} and its resolution. A buffered approach is employed to\nboost the performance of the whiteboard. The simulations experiments were\nperformed with over 1000 peer nodes on Emulab using 200 physical nodes. The\nresults show that the proposed overlay eventually stabilizes even in the\npresence of a churning rate of up to 30\\%. The maximum path length being just\nsix hops. The estimated throughput is found to be close to our theoretical\nresults.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 13:22:28 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Bhagatkar", "Nikita", ""], ["Dolas", "Kapil", ""], ["Ghosh", "R. K.", ""]]}, {"id": "1903.05488", "submitter": "Jakub Krzywda", "authors": "Jakub Krzywda and Ahmed Ali-Eldin and Trevor E. Carlson and Per-Olov\n  \\\"Ostberg and Erik Elmroth", "title": "Power-Performance Tradeoffs in Data Center Servers: DVFS, CPU pinning,\n  Horizontal, and Vertical Scaling", "comments": "31 pages", "journal-ref": "Future Generation Computer Systems, Elsevier, Vol. 81, pp.\n  114-128, 2018", "doi": "10.1016/j.future.2017.10.044", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic Voltage and Frequency Scaling (DVFS), CPU pinning, horizontal, and\nvertical scaling, are four techniques that have been proposed as actuators to\ncontrol the performance and energy consumption on data center servers. This\nwork investigates the utility of these four actuators, and quantifies the\npower-performance tradeoffs associated with them. Using replicas of the German\nWikipedia running on our local testbed, we perform a set of experiments to\nquantify the influence of DVFS, vertical and horizontal scaling, and CPU\npinning on end-to-end response time (average and tail), throughput, and power\nconsumption with different workloads. Results of the experiments show that DVFS\nrarely reduces the power consumption of underloaded servers by more than 5%,\nbut it can be used to limit the maximal power consumption of a saturated server\nby up to 20% (at a cost of performance degradation). CPU pinning reduces the\npower consumption of underloaded server (by up to 7%) at the cost of\nperformance degradation, which can be limited by choosing an appropriate CPU\npinning scheme. Horizontal and vertical scaling improves both the average and\ntail response time, but the improvement is not proportional to the amount of\nresources added. The load balancing strategy has a big impact on the tail\nresponse time of horizontally scaled applications.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 13:50:27 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Krzywda", "Jakub", ""], ["Ali-Eldin", "Ahmed", ""], ["Carlson", "Trevor E.", ""], ["\u00d6stberg", "Per-Olov", ""], ["Elmroth", "Erik", ""]]}, {"id": "1903.05599", "submitter": "Ruwan Wickramarachchi", "authors": "Sameh M. Saad, Terrence Perera and Ruwan Wickramarachchi", "title": "Simulation of distributed manufacturing enterprises: A new approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The globalization of markets and world-wide competition forces manufacturing\nenterprises to enter into alliances leading to the creation of distributed\nmanufacturing enterprises. Before forming a partnership it is essential to\nevaluate viability of proposed enterprise as well as how a companys operations\nare affected by the proposed virtual enterprise. Distributed simulation\nprovides an attractive tool to make decisions on such situations. However, due\nto its complexity and high cost distributed simulation failed to gain a wide\nacceptance from industrial users. This paper presents a new approach for\ndistributed manufacturing simulation (DMS) including a formal methodology for\nDMS and, implementation approach using current commercial simulation software,\nemploying widely available and cost effective technologies. The main objective\nof this work is to promote the use of distributed simulation particularly in\ndistributed manufacturing by making it fast to develop and less complicated for\nimplementation.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jan 2019 17:53:59 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Saad", "Sameh M.", ""], ["Perera", "Terrence", ""], ["Wickramarachchi", "Ruwan", ""]]}, {"id": "1903.05714", "submitter": "Joseph Izraelevitz", "authors": "Joseph Izraelevitz, Jian Yang, Lu Zhang, Juno Kim, Xiao Liu, Amirsaman\n  Memaripour, Yun Joon Soh, Zixuan Wang, Yi Xu, Subramanya R. Dulloor, Jishen\n  Zhao and Steven Swanson", "title": "Basic Performance Measurements of the Intel Optane DC Persistent Memory\n  Module", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scalable nonvolatile memory DIMMs will finally be commercially available with\nthe release of the Intel Optane DC Persistent Memory Module (or just \"Optane DC\nPMM\"). This new nonvolatile DIMM supports byte-granularity accesses with access\ntimes on the order of DRAM, while also providing data storage that survives\npower outages. This work comprises the first in-depth, scholarly, performance\nreview of Intel's Optane DC PMM, exploring its capabilities as a main memory\ndevice, and as persistent, byte-addressable memory exposed to user-space\napplications. This report details the technologies performance under a number\nof modes and scenarios, and across a wide variety of macro-scale benchmarks.\nOptane DC PMMs can be used as large memory devices with a DRAM cache to hide\ntheir lower bandwidth and higher latency. When used in this Memory (or cached)\nmode, Optane DC memory has little impact on applications with small memory\nfootprints. Applications with larger memory footprints may experience some\nslow-down relative to DRAM, but are now able to keep much more data in memory.\nWhen used under a file system, Optane DC PMMs can result in significant\nperformance gains, especially when the file system is optimized to use the\nload/store interface of the Optane DC PMM and the application uses many small,\npersistent writes. For instance, using the NOVA-relaxed NVMM file system, we\ncan improve the performance of Kyoto Cabinet by almost 2x. Optane DC PMMs can\nalso enable user-space persistence where the application explicitly controls\nits writes into persistent Optane DC media. In our experiments, modified\napplications that used user-space Optane DC persistence generally outperformed\ntheir file system counterparts. For instance, the persistent version of RocksDB\nperformed almost 2x faster than the equivalent program utilizing an NVMM-aware\nfile system.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 21:14:40 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 20:19:20 GMT"}, {"version": "v3", "created": "Fri, 9 Aug 2019 18:41:24 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Izraelevitz", "Joseph", ""], ["Yang", "Jian", ""], ["Zhang", "Lu", ""], ["Kim", "Juno", ""], ["Liu", "Xiao", ""], ["Memaripour", "Amirsaman", ""], ["Soh", "Yun Joon", ""], ["Wang", "Zixuan", ""], ["Xu", "Yi", ""], ["Dulloor", "Subramanya R.", ""], ["Zhao", "Jishen", ""], ["Swanson", "Steven", ""]]}, {"id": "1903.05838", "submitter": "Jude Tchaye-Kondi", "authors": "Jude Tchaye-Kondi, Yanlong Zhai, Kwei-Jay Lin, Wenjun Tao, and Kai\n  Yang", "title": "Hadoop Perfect File: A fast access container for small files with direct\n  in disc metadata access", "comments": null, "journal-ref": null, "doi": "10.1016/j.jpdc.2021.05.011", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Storing and processing massive small files is one of the major challenges for\nthe Hadoop Distributed File System (HDFS). In order to provide fast data\naccess, the NameNode (NN) in HDFS maintains the metadata of all files in its\nmain-memory. Hadoop performs well with a small number of large files that\nrequire relatively little metadata in the NN s memory. But for a large number\nof small files, Hadoop has problems such as NN memory overload caused by the\nhuge metadata size of these small files. We present a new type of archive file,\nHadoop Perfect File (HPF), to solve HDFS s small files problem by merging small\nfiles into a large file on HDFS. Existing archive files offer limited\nfunctionality and have poor performance when accessing a file in the merged\nfile due to the fact that during metadata lookup it is necessary to read and\nprocess the entire index file(s). In contrast, HPF file can directly access the\nmetadata of a particular file from its index file without having to process it\nentirely. The HPF index system uses two hash functions: file s metadata are\ndistributed through index files by using a dynamic hash function and, for each\nindex file, we build an order preserving perfect hash function that preserves\nthe position of each file s metadata in the index file. The HPF design will\nonly read the part of the index file that contains the metadata of the searched\nfile during its access. HPF file also supports the file appending functionality\nafter its creation. Our experiments show that HPF can be more than 40% faster\nfile s access from the original HDFS. If we don t consider the caching effect,\nHPF s file access is around 179% faster than MapFile and 11294% faster than HAR\nfile. If we consider caching effect, HPF is around 35% faster than MapFile and\n105% faster than HAR file.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 07:12:48 GMT"}, {"version": "v2", "created": "Fri, 26 Apr 2019 03:59:10 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Tchaye-Kondi", "Jude", ""], ["Zhai", "Yanlong", ""], ["Lin", "Kwei-Jay", ""], ["Tao", "Wenjun", ""], ["Yang", "Kai", ""]]}, {"id": "1903.05898", "submitter": "Siqi Wang", "authors": "Siqi Wang, Gayathri Ananthanarayanan, Yifan Zeng, Neeraj Goel, Anuj\n  Pathania, Tulika Mitra", "title": "High-Throughput CNN Inference on Embedded ARM big.LITTLE Multi-Core\n  Processors", "comments": "Accepted to IEEE Transactions on Computer-Aided Design of Integrated\n  Circuits and Systems", "journal-ref": "in IEEE Transactions on Computer-Aided Design of Integrated\n  Circuits and Systems, vol. 39, no. 10, pp. 2254-2267, Oct. 2020", "doi": "10.1109/TCAD.2019.2944584", "report-no": null, "categories": "cs.LG cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  IoT Edge intelligence requires Convolutional Neural Network (CNN) inference\nto take place in the edge devices itself. ARM big.LITTLE architecture is at the\nheart of prevalent commercial edge devices. It comprises of single-ISA\nheterogeneous cores grouped into multiple homogeneous clusters that enable\npower and performance trade-offs. All cores are expected to be simultaneously\nemployed in inference to attain maximal throughput. However, high communication\noverhead involved in parallelization of computations from convolution kernels\nacross clusters is detrimental to throughput. We present an alternative\nframework called Pipe-it that employs pipelined design to split convolutional\nlayers across clusters while limiting parallelization of their respective\nkernels to the assigned cluster. We develop a performance-prediction model that\nutilizes only the convolutional layer descriptors to predict the execution time\nof each layer individually on all permitted core configurations (type and\ncount). Pipe-it then exploits the predictions to create a balanced pipeline\nusing an efficient design space exploration algorithm. Pipe-it on average\nresults in a 39% higher throughput than the highest antecedent throughput.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 10:24:57 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 07:43:41 GMT"}, {"version": "v3", "created": "Wed, 22 Jan 2020 15:46:27 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Wang", "Siqi", ""], ["Ananthanarayanan", "Gayathri", ""], ["Zeng", "Yifan", ""], ["Goel", "Neeraj", ""], ["Pathania", "Anuj", ""], ["Mitra", "Tulika", ""]]}, {"id": "1903.05900", "submitter": "Alexander Stannat", "authors": "Alexander Stannat, Johan Pouwelse", "title": "A Random Walk based Trust Ranking in Distributed Systems", "comments": "13 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Honest cooperation among individuals in a network can be achieved in\ndifferent ways. In online networks with some kind of central authority, such as\nEbay, Airbnb, etc. honesty is achieved through a reputation system, which is\nmaintained and secured by the central authority. These systems usually rely on\nreview mechanisms, through which agents can evaluate the trustworthiness of\ntheir interaction partners. These reviews are stored centrally and are\ntamper-proof. In decentralized peer-to-peer networks, enforcing cooperation\nturns out to be more difficult. One way of approaching this problem is by\nobserving cooperative biological communities in nature. One finds that\ncooperation among biological organisms is achieved through a mechanism called\nindirect reciprocity. This mechanism for cooperation relies on some shared\nnotion of trust. In this work we aim to facilitate communal cooperation in a\npeer-to-peer file sharing network called Tribler, by introducing a mechanism\nfor evaluating the trustworthiness of agents. We determine a trust ranking of\nall nodes in the network based on the Monte Carlo algorithm estimating the\nvalues of Google's personalized PageRank vector. We go on to evaluate the\nalgorithm's resistance to Sybil attacks, whereby our aim is for sybils to be\nassigned low trust scores.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 10:29:12 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Stannat", "Alexander", ""], ["Pouwelse", "Johan", ""]]}, {"id": "1903.05918", "submitter": "Carsten Kutzner", "authors": "Carsten Kutzner, Szil\\'ard P\\'all, Martin Fechner, Ansgar Esztermann,\n  Bert L. de Groot, Helmut Grubm\\\"uller", "title": "More Bang for Your Buck: Improved use of GPU Nodes for GROMACS 2018", "comments": "41 pages, 13 figures, 4 tables. This updated version includes the\n  following improvements: - most notably, added benchmarks for two coarse grain\n  MARTINI systems VES and BIG, resulting in a new Figure 13 - fixed typos -\n  made text clearer in some places - added two more benchmarks for MEM and RIB\n  systems (E3-1240v6 + RTX 2080 / 2080Ti)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF physics.bio-ph physics.comp-ph q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We identify hardware that is optimal to produce molecular dynamics\ntrajectories on Linux compute clusters with the GROMACS 2018 simulation\npackage. Therefore, we benchmark the GROMACS performance on a diverse set of\ncompute nodes and relate it to the costs of the nodes, which may include their\nlifetime costs for energy and cooling. In agreement with our earlier\ninvestigation using GROMACS 4.6 on hardware of 2014, the performance to price\nratio of consumer GPU nodes is considerably higher than that of CPU nodes.\nHowever, with GROMACS 2018, the optimal CPU to GPU processing power balance has\nshifted even more towards the GPU. Hence, nodes optimized for GROMACS 2018 and\nlater versions enable a significantly higher performance to price ratio than\nnodes optimized for older GROMACS versions. Moreover, the shift towards GPU\nprocessing allows to cheaply upgrade old nodes with recent GPUs, yielding\nessentially the same performance as comparable brand-new hardware.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 11:06:54 GMT"}, {"version": "v2", "created": "Thu, 13 Jun 2019 09:59:55 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Kutzner", "Carsten", ""], ["P\u00e1ll", "Szil\u00e1rd", ""], ["Fechner", "Martin", ""], ["Esztermann", "Ansgar", ""], ["de Groot", "Bert L.", ""], ["Grubm\u00fcller", "Helmut", ""]]}, {"id": "1903.05956", "submitter": "Janne H. Korhonen", "authors": "Keren Censor-Hillel, Michal Dory, Janne H. Korhonen, Dean Leitersdorf", "title": "Fast Approximate Shortest Paths in the Congested Clique", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design fast deterministic algorithms for distance computation in the\ncongested clique model. Our key contributions include:\n  -- A $(2+\\epsilon)$-approximation for all-pairs shortest paths in\n$O(\\log^2{n} / \\epsilon)$ rounds on unweighted undirected graphs. With a small\nadditional additive factor, this also applies for weighted graphs. This is the\nfirst sub-polynomial constant-factor approximation for APSP in this model.\n  -- A $(1+\\epsilon)$-approximation for multi-source shortest paths from\n$O(\\sqrt{n})$ sources in $O(\\log^2{n} / \\epsilon)$ rounds on weighted\nundirected graphs. This is the first sub-polynomial algorithm obtaining this\napproximation for a set of sources of polynomial size.\n  Our main techniques are new distance tools that are obtained via improved\nalgorithms for sparse matrix multiplication, which we leverage to construct\nefficient hopsets and shortest paths. Furthermore, our techniques extend to\nadditional distance problems for which we improve upon the state-of-the-art,\nincluding diameter approximation, and an exact single-source shortest paths\nalgorithm for weighted undirected graphs in $\\tilde{O}(n^{1/6})$ rounds.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 12:54:11 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2019 16:18:59 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Censor-Hillel", "Keren", ""], ["Dory", "Michal", ""], ["Korhonen", "Janne H.", ""], ["Leitersdorf", "Dean", ""]]}, {"id": "1903.05992", "submitter": "Michail Theofilatos", "authors": "Othon Michail, Paul G. Spirakis, Michail Theofilatos", "title": "Fault Tolerant Network Constructors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider adversarial crash faults of nodes in the network\nconstructors model $[$Michail and Spirakis, 2016$]$. We first show that,\nwithout further assumptions, the class of graph languages that can be (stably)\nconstructed under crash faults is non-empty but small. In particular, if an\nunbounded number of crash faults may occur, we prove that (i) the only\nconstructible graph language is that of spanning cliques and (ii) a strong\nimpossibility result holds even if the size of the graphs that the protocol\noutputs in populations of size $n$ need only grow with $n$ (the remaining nodes\nbeing waste). When there is a finite upper bound $f$ on the number of faults,\nwe show that it is impossible to construct any non-hereditary graph language.\nOn the positive side, by relaxing our requirements we prove that: (i)\npermitting linear waste enables to construct on $n/(2f)-f$ nodes, any graph\nlanguage that is constructible in the fault-free case, (ii) partial\nconstructibility (i.e. not having to generate all graphs in the language)\nallows the construction of a large class of graph languages. We then extend the\noriginal model with a minimal form of fault notifications. Our main result here\nis a fault-tolerant universal constructor: We develop a fault-tolerant protocol\nfor spanning line and use it to simulate a linear-space Turing Machine $M$.\nThis allows a fault-tolerant construction of any graph accepted by $M$ in\nlinear space, with waste $min\\{n/2+f(n),\\; n\\}$, where $f(n)$ is the number of\nfaults in the execution. We then prove that increasing the permissible waste to\n$min\\{2n/3+f(n),\\; n\\}$ allows the construction of graphs accepted by an\n$O(n^2)$-space Turing Machine, which is asymptotically the maximum simulation\nspace that we can hope for in this model. Finally, we show that logarithmic\nlocal memories can be exploited for a no-waste fault-tolerant simulation of any\nsuch protocol.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 13:40:25 GMT"}, {"version": "v2", "created": "Sat, 18 May 2019 12:18:58 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Michail", "Othon", ""], ["Spirakis", "Paul G.", ""], ["Theofilatos", "Michail", ""]]}, {"id": "1903.06226", "submitter": "Kai Wu", "authors": "Kai Wu, Jie Ren, and Dong Li", "title": "Architecture-Aware, High Performance Transaction for Persistent Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Byte-addressable non-volatile main memory (NVM) demands transactional\nmechanisms to access and manipulate data on NVM atomically. Those transaction\nmechanisms often employ a logging mechanism (undo logging or redo logging).\nHowever, the logging mechanisms can bring large runtime overhead (8%-49% in our\nevaluation), and 41%-78% of the overhead attributes to the frequent cache-line\nflushing. Such large overhead significantly diminishes the performance benefits\noffered by NVM. In this paper, we introduce a new method to reduce the overhead\nof cache-line flushing for logging-based transactions. Different from the\ntraditional method that works at the program level and leverages program\nsemantics to reduce the logging overhead, we introduce architecture awareness.\nIn particular, we do not flush certain cache blocks, as long as they are\nestimated to be eliminated out of the cache because of the hardware caching\nmechanism (e.g., the cache replacement algorithm). Furthermore, we coalesce\nthose cache blocks with low dirtiness to improve the efficiency of cache-line\nflushing. We implement an architecture-aware, high performance transaction\nruntime system for persistent memory, Archapt. Our results show that comparing\nwith an undo logging (PMDK) and a redo logging (Mnemosyne), Archapt reduces\ncache-line flushing by 66% and improves system throughput by 19% on average\n(42% at most). Our crash tests with four hardware caching policies show that\nArchapt provides a strong guarantee on crash consistency.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 19:35:17 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2019 17:06:16 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Wu", "Kai", ""], ["Ren", "Jie", ""], ["Li", "Dong", ""]]}, {"id": "1903.06495", "submitter": "Farzad Farshchi", "authors": "Farzad Farshchi, Qijing Huang, and Heechul Yun", "title": "Integrating NVIDIA Deep Learning Accelerator (NVDLA) with RISC-V SoC on\n  FireSim", "comments": "Presented at the 2nd Workshop on Energy Efficient Machine Learning\n  and Cognitive Computing for Embedded Applications (EMC2'19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NVDLA is an open-source deep neural network (DNN) accelerator which has\nreceived a lot of attention by the community since its introduction by Nvidia.\nIt is a full-featured hardware IP and can serve as a good reference for\nconducting research and development of SoCs with integrated accelerators.\nHowever, an expensive FPGA board is required to do experiments with this IP in\na real SoC. Moreover, since NVDLA is clocked at a lower frequency on an FPGA,\nit would be hard to do accurate performance analysis with such a setup. To\novercome these limitations, we integrate NVDLA into a real RISC-V SoC on the\nAmazon cloud FPGA using FireSim, a cycle-exact FPGA-accelerated simulator. We\nthen evaluate the performance of NVDLA by running YOLOv3 object-detection\nalgorithm. Our results show that NVDLA can sustain 7.5 fps when running YOLOv3.\nWe further analyze the performance by showing that sharing the last-level cache\nwith NVDLA can result in up to 1.56x speedup. We then identify that sharing the\nmemory system with the accelerator can result in unpredictable execution time\nfor the real-time tasks running on this platform. We believe this is an\nimportant issue that must be addressed in order for on-chip DNN accelerators to\nbe incorporated in real-time embedded systems.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 01:35:31 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2019 21:52:55 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Farshchi", "Farzad", ""], ["Huang", "Qijing", ""], ["Yun", "Heechul", ""]]}, {"id": "1903.06498", "submitter": "Tim Zerrell", "authors": "Tim Zerrell and Jeremy Bruestle", "title": "Stripe: Tensor Compilation via the Nested Polyhedral Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hardware architectures and machine learning (ML) libraries evolve rapidly.\nTraditional compilers often fail to generate high-performance code across the\nspectrum of new hardware offerings. To mitigate, engineers develop hand-tuned\nkernels for each ML library update and hardware upgrade. Unfortunately, this\napproach requires excessive engineering effort to scale or maintain with any\ndegree of state-of-the-art performance. Here we present a Nested Polyhedral\nModel for representing highly parallelizable computations with limited\ndependencies between iterations. This model provides an underlying framework\nfor an intermediate representation (IR) called Stripe, amenable to standard\ncompiler techniques while naturally modeling key aspects of modern ML\ncomputing. Stripe represents parallelism, efficient memory layout, and multiple\ncompute units at a level of abstraction amenable to automatic optimization. We\ndescribe how Stripe enables a compiler for ML in the style of LLVM that allows\nindependent development of algorithms, optimizations, and hardware\naccelerators. We also discuss the design exploration advantages of Stripe over\nkernel libraries and schedule-based or schedule-space-based code generation.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 17:49:48 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Zerrell", "Tim", ""], ["Bruestle", "Jeremy", ""]]}, {"id": "1903.06630", "submitter": "Guy Lemieux", "authors": "Guy G.F. Lemieux, Joe Edwards, Joel Vandergriendt, Aaron Severance,\n  Ryan De Iaco, Abdullah Raouf, Hussein Osman, Tom Watzka, Satwant Singh", "title": "TinBiNN: Tiny Binarized Neural Network Overlay in about 5,000 4-LUTs and\n  5mW", "comments": "Presented at 3rd International Workshop on Overlay Architectures for\n  FPGAs (OLAF 2017) arXiv:1704.08802", "journal-ref": null, "doi": null, "report-no": "OLAF/2017/06", "categories": "cs.DC cs.CV cs.OH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reduced-precision arithmetic improves the size, cost, power and performance\nof neural networks in digital logic. In convolutional neural networks, the use\nof 1b weights can achieve state-of-the-art error rates while eliminating\nmultiplication, reducing storage and improving power efficiency. The\nBinaryConnect binary-weighted system, for example, achieves 9.9% error using\nfloating-point activations on the CIFAR-10 dataset. In this paper, we introduce\nTinBiNN, a lightweight vector processor overlay for accelerating inference\ncomputations with 1b weights and 8b activations. The overlay is very small --\nit uses about 5,000 4-input LUTs and fits into a low cost iCE40 UltraPlus FPGA\nfrom Lattice Semiconductor. To show this can be useful, we build two embedded\n'person detector' systems by shrinking the original BinaryConnect network. The\nfirst is a 10-category classifier with a 89% smaller network that runs in\n1,315ms and achieves 13.6% error. The other is a 1-category classifier that is\neven smaller, runs in 195ms, and has only 0.4% error. In both classifiers, the\nerror can be attributed entirely to training and not reduced precision.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 14:51:36 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Lemieux", "Guy G. F.", ""], ["Edwards", "Joe", ""], ["Vandergriendt", "Joel", ""], ["Severance", "Aaron", ""], ["De Iaco", "Ryan", ""], ["Raouf", "Abdullah", ""], ["Osman", "Hussein", ""], ["Watzka", "Tom", ""], ["Singh", "Satwant", ""]]}, {"id": "1903.06631", "submitter": "Junzhe Zhang Mr", "authors": "Junzhe Zhang, Sai Ho Yeung, Yao Shu, Bingsheng He, Wei Wang", "title": "Efficient Memory Management for GPU-based Deep Learning Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GPU (graphics processing unit) has been used for many data-intensive\napplications. Among them, deep learning systems are one of the most important\nconsumer systems for GPU nowadays. As deep learning applications impose deeper\nand larger models in order to achieve higher accuracy, memory management\nbecomes an important research topic for deep learning systems, given that GPU\nhas limited memory size. Many approaches have been proposed towards this issue,\ne.g., model compression and memory swapping. However, they either degrade the\nmodel accuracy or require a lot of manual intervention. In this paper, we\npropose two orthogonal approaches to reduce the memory cost from the system\nperspective. Our approaches are transparent to the models, and thus do not\naffect the model accuracy. They are achieved by exploiting the iterative nature\nof the training algorithm of deep learning to derive the lifetime and\nread/write order of all variables. With the lifetime semantics, we are able to\nimplement a memory pool with minimal fragments. However, the optimization\nproblem is NP-complete. We propose a heuristic algorithm that reduces up to\n13.3% of memory compared with Nvidia's default memory pool with equal time\ncomplexity. With the read/write semantics, the variables that are not in use\ncan be swapped out from GPU to CPU to reduce the memory footprint. We propose\nmultiple swapping strategies to automatically decide which variable to swap and\nwhen to swap out (in), which reduces the memory cost by up to 34.2% without\ncommunication overhead.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 08:33:04 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Zhang", "Junzhe", ""], ["Yeung", "Sai Ho", ""], ["Shu", "Yao", ""], ["He", "Bingsheng", ""], ["Wang", "Wei", ""]]}, {"id": "1903.06648", "submitter": "Basit Qureshi", "authors": "Basit Qureshi, Anis Koubaa", "title": "On Energy Efficiency and Performance Evaluation of SBC based Clusters: A\n  Hadoop case study", "comments": "12 pages. Submitted to Electronics Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy efficiency in a data center is a challenge and has garnered\nresearchers interest. In this paper we address the energy efficiency issue of a\nsmall scale data center by utilizing Single Board Computer (SBC) based\nclusters. A compact design layout is presented to build two clusters using 20\nnodes each. Extensive testing was carried out to analyze the performance of\nthese clusters using popular performance benchmarks for task execution time,\nmemory/storage utilization, network throughput and energy consumption. Further,\nwe investigate the cost of operating SBC based clusters by correlating energy\nutilization for the execution time of various benchmarks using workloads of\ndifferent sizes. Results show that, although the low-cost benefit of a cluster\nbuilt with ARM-based SBCs is desirable, these clusters yield low comparable\nperformance and energy efficiency due to limited onboard capabilities. It is\npossible to tweak Hadoop configuration parameters for an ARM-based SBC cluster\nto efficiently utilize resources. We present, a discussion on the effectiveness\nof the SBC-based clusters as a testbed for inexpensive and green cloud\ncomputing research.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 08:32:25 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Qureshi", "Basit", ""], ["Koubaa", "Anis", ""]]}, {"id": "1903.06649", "submitter": "Qiuwen Lou", "authors": "Qiuwen Lou, Indranil Palit, Tang Li, Andras Horvath, Michael Niemier,\n  X. Sharon Hu", "title": "Application-level Studies of Cellular Neural Network-based Hardware\n  Accelerators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As cost and performance benefits associated with Moore's Law scaling slow,\nresearchers are studying alternative architectures (e.g., based on analog\nand/or spiking circuits) and/or computational models (e.g., convolutional and\nrecurrent neural networks) to perform application-level tasks faster, more\nenergy efficiently, and/or more accurately. We investigate cellular neural\nnetwork (CeNN)-based co-processors at the application-level for these metrics.\nWhile it is well-known that CeNNs can be well-suited for spatio-temporal\ninformation processing, few (if any) studies have quantified the\nenergy/delay/accuracy of a CeNN-friendly algorithm and compared the CeNN-based\napproach to the best von Neumann algorithm at the application level. We present\nan evaluation framework for such studies. As a case study, a CeNN-friendly\ntarget-tracking algorithm was developed and mapped to an array architecture\ndeveloped in conjunction with the algorithm. We compare the energy, delay, and\naccuracy of our architecture/algorithm (assuming all overheads) to the most\naccurate von Neumann algorithm (Struck). Von Neumann CPU data is measured on an\nIntel i5 chip. The CeNN approach is capable of matching the accuracy of Struck,\nand can offer approximately 1000x improvements in energy-delay product.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 17:07:33 GMT"}, {"version": "v2", "created": "Wed, 12 Jun 2019 21:23:42 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Lou", "Qiuwen", ""], ["Palit", "Indranil", ""], ["Li", "Tang", ""], ["Horvath", "Andras", ""], ["Niemier", "Michael", ""], ["Hu", "X. Sharon", ""]]}, {"id": "1903.06681", "submitter": "Nikoli Dryden", "authors": "Nikoli Dryden, Naoya Maruyama, Tom Benson, Tim Moon, Marc Snir, Brian\n  Van Essen", "title": "Improving Strong-Scaling of CNN Training by Exploiting Finer-Grained\n  Parallelism", "comments": "To appear at IPDPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scaling CNN training is necessary to keep up with growing datasets and reduce\ntraining time. We also see an emerging need to handle datasets with very large\nsamples, where memory requirements for training are large. Existing training\nframeworks use a data-parallel approach that partitions samples within a\nmini-batch, but limits to scaling the mini-batch size and memory consumption\nmakes this untenable for large samples. We describe and implement new\napproaches to convolution, which parallelize using spatial decomposition or a\ncombination of sample and spatial decomposition. This introduces many\nperformance knobs for a network, so we develop a performance model for CNNs and\npresent a method for using it to automatically determine efficient\nparallelization strategies.\n  We evaluate our algorithms with microbenchmarks and image classification with\nResNet-50. Our algorithms allow us to prototype a model for a mesh-tangling\ndataset, where sample sizes are very large. We show that our parallelization\nachieves excellent strong and weak scaling and enables training for previously\nunreachable datasets.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 17:25:01 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Dryden", "Nikoli", ""], ["Maruyama", "Naoya", ""], ["Benson", "Tom", ""], ["Moon", "Tim", ""], ["Snir", "Marc", ""], ["Van Essen", "Brian", ""]]}, {"id": "1903.06693", "submitter": "Jeferson Santiago da Silva", "authors": "Jeferson Santiago da Silva, Fran\\c{c}ois-Raymond Boyer and J.M. Pierre\n  Langlois", "title": "Module-per-Object: a Human-Driven Methodology for C++-based High-Level\n  Synthesis Design", "comments": "9 pages. Paper accepted for publication at The 27th IEEE\n  International Symposium on Field-Programmable Custom Computing Machines, San\n  Diego CA, April 28 - May 1, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-Level Synthesis (HLS) brings FPGAs to audiences previously unfamiliar to\nhardware design. However, achieving the highest Quality-of-Results (QoR) with\nHLS is still unattainable for most programmers. This requires detailed\nknowledge of FPGA architecture and hardware design in order to produce\nFPGA-friendly codes. Moreover, these codes are normally in conflict with best\ncoding practices, which favor code reuse, modularity, and conciseness.\n  To overcome these limitations, we propose Module-per-Object (MpO), a\nhuman-driven HLS design methodology intended for both hardware designers and\nsoftware developers with limited FPGA expertise. MpO exploits modern C++ to\nraise the abstraction level while improving QoR, code readability and\nmodularity. To guide HLS designers, we present the five characteristics of MpO\nclasses. Each characteristic exploits the power of HLS-supported modern C++\nfeatures to build C++-based hardware modules. These characteristics lead to\nhigh-quality software descriptions and efficient hardware generation. We also\npresent a use case of MpO, where we use C++ as the intermediate language for\nFPGA-targeted code generation from P4, a packet processing domain specific\nlanguage. The MpO methodology is evaluated using three design experiments: a\npacket parser, a flow-based traffic manager, and a digital up-converter. Based\non experiments, we show that MpO can be comparable to hand-written VHDL code\nwhile keeping a high abstraction level, human-readable coding style and\nmodularity. Compared to traditional C-based HLS design, MpO leads to more\nefficient circuit generation, both in terms of performance and resource\nutilization. Also, the MpO approach notably improves software quality,\naugmenting parametrization while eliminating the incidence of code duplication.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 00:37:01 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 14:35:04 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["da Silva", "Jeferson Santiago", ""], ["Boyer", "Fran\u00e7ois-Raymond", ""], ["Langlois", "J. M. Pierre", ""]]}, {"id": "1903.06695", "submitter": "Dorian Cazau", "authors": "Paul Nguyen Hong Duc and Dorian Cazau", "title": "Development details and computational benchmarking of DEPAM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In the big data era of observational oceanography, passive acoustics datasets\nare becoming too high volume to be processed on local computers due to their\nprocessor and memory limitations. As a result there is a current need for our\ncommunity to turn to cloud-based distributed computing. We present a scalable\ncomputing system for FFT (Fast Fourier Transform)-based features (e.g., Power\nSpectral Density) based on the Apache distributed frameworks Hadoop and Spark.\nThese features are at the core of many different types of acoustic analysis\nwhere the need of processing data at scale with speed is evident, e.g. serving\nas long-term averaged learning representations of soundscapes to identify\nperiods of acoustic interest. In addition to provide a complete description of\nour system implementation, we also performed a computational benchmark\ncomparing our system to three other Scala-only, Matlab and Python based systems\nin standalone executions, and evaluated its scalability using the speed up\nmetric. Our current results are very promising in terms of computational\nperformance, as we show that our proposed Hadoop/Spark system performs\nreasonably well on a single node setup comparatively to state-of-the-art\nprocessing tools used by the PAM community, and that it could also fully\nleverage more intensive cluster resources with a almost-linear scalability\nbehaviour above a certain dataset volume.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2019 21:32:52 GMT"}, {"version": "v2", "created": "Fri, 7 Jun 2019 14:04:46 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Duc", "Paul Nguyen Hong", ""], ["Cazau", "Dorian", ""]]}, {"id": "1903.06697", "submitter": "Maciej Besta", "authors": "Maciej Besta, Dimitri Stanojevic, Johannes De Fine Licht, Tal Ben-Nun,\n  Torsten Hoefler", "title": "Graph Processing on FPGAs: Taxonomy, Survey, Challenges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph processing has become an important part of various areas, such as\nmachine learning, computational sciences, medical applications, social network\nanalysis, and many others. Various graphs, for example web or social networks,\nmay contain up to trillions of edges. The sheer size of such datasets, combined\nwith the irregular nature of graph processing, poses unique challenges for the\nruntime and the consumed power. Field Programmable Gate Arrays (FPGAs) can be\nan energy-efficient solution to deliver specialized hardware for graph\nprocessing. This is reflected by the recent interest in developing various\ngraph algorithms and graph processing frameworks on FPGAs. To facilitate\nunderstanding of this emerging domain, we present the first survey and taxonomy\non graph computations on FPGAs. Our survey describes and categorizes existing\nschemes and explains key ideas. Finally, we discuss research and engineering\nchallenges to outline the future of graph computations on FPGAs.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 04:46:07 GMT"}, {"version": "v2", "created": "Sat, 23 Mar 2019 22:29:18 GMT"}, {"version": "v3", "created": "Sat, 27 Apr 2019 16:57:25 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Besta", "Maciej", ""], ["Stanojevic", "Dimitri", ""], ["Licht", "Johannes De Fine", ""], ["Ben-Nun", "Tal", ""], ["Hoefler", "Torsten", ""]]}, {"id": "1903.06701", "submitter": "Marco Canini", "authors": "Amedeo Sapio, Marco Canini, Chen-Yu Ho, Jacob Nelson, Panos Kalnis,\n  Changhoon Kim, Arvind Krishnamurthy, Masoud Moshref, Dan R. K. Ports, Peter\n  Richt\\'arik", "title": "Scaling Distributed Machine Learning with In-Network Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.NI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training machine learning models in parallel is an increasingly important\nworkload. We accelerate distributed parallel training by designing a\ncommunication primitive that uses a programmable switch dataplane to execute a\nkey step of the training process. Our approach, SwitchML, reduces the volume of\nexchanged data by aggregating the model updates from multiple workers in the\nnetwork. We co-design the switch processing with the end-host protocols and ML\nframeworks to provide an efficient solution that speeds up training by up to\n5.5$\\times$ for a number of real-world benchmark models.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2019 15:10:21 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 09:26:58 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Sapio", "Amedeo", ""], ["Canini", "Marco", ""], ["Ho", "Chen-Yu", ""], ["Nelson", "Jacob", ""], ["Kalnis", "Panos", ""], ["Kim", "Changhoon", ""], ["Krishnamurthy", "Arvind", ""], ["Moshref", "Masoud", ""], ["Ports", "Dan R. K.", ""], ["Richt\u00e1rik", "Peter", ""]]}, {"id": "1903.06702", "submitter": "Cristiano Arbex Valle", "authors": "Cristiano Arbex Valle and John E Beasley", "title": "Order allocation, rack allocation and rack sequencing for pickers in a\n  mobile rack environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate the problem of simultaneously allocating orders\nand mobile storage racks to static pickers. Here storage racks are allocated to\npickers to enable them to pick all of the products for the orders that have\nbeen allocated to them. Problems of the type considered here arise in\nfacilities operating as robotic mobile fulfilment systems.\n  We present a formulation of the problem of allocating orders and racks to\npickers as an integer program and discuss the complexity of the problem. We\npresent two heuristics (matheuristics) for the problem, one using partial\ninteger optimisation, that are directly based upon our formulation.\n  We also consider the problem of how to sequence the racks for presentation at\neach individual picker and formulate this problem as an integer program. We\nprove that, subject to certain conditions being satisfied, a feasible rack\nsequence for all orders can be produced by focusing on just a subset of the\norders to be dealt with by the picker.\n  Computational results are presented, both for order and rack allocation, and\nfor rack sequencing, for randomly generated test problems (that are made\npublicly available) involving up to 500 products, 150 orders, 150 racks and 10\npickers.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 16:44:43 GMT"}, {"version": "v2", "created": "Fri, 12 Jul 2019 17:56:36 GMT"}, {"version": "v3", "created": "Thu, 12 Dec 2019 19:13:00 GMT"}, {"version": "v4", "created": "Sat, 2 May 2020 21:19:40 GMT"}, {"version": "v5", "created": "Wed, 17 Jun 2020 16:46:56 GMT"}, {"version": "v6", "created": "Wed, 22 Jul 2020 16:54:03 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Valle", "Cristiano Arbex", ""], ["Beasley", "John E", ""]]}, {"id": "1903.06798", "submitter": "Karl Mason", "authors": "Karl Mason, Sadegh Vejdan, Santiago Grijalva", "title": "An \"On The Fly\" Framework for Efficiently Generating Synthetic Big Data\n  Sets", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collecting, analyzing and gaining insight from large volumes of data is now\nthe norm in an ever increasing number of industries. Data analytics techniques,\nsuch as machine learning, are powerful tools used to analyze these large\nvolumes of data. Synthetic data sets are routinely relied upon to train and\ndevelop such data analytics methods for several reasons: to generate larger\ndata sets than are available, to generate diverse data sets, to preserve\nanonymity in data sets with sensitive information, etc. Processing,\ntransmitting and storing data is a key issue faced when handling large data\nsets. This paper presents an \"On the fly\" framework for generating big\nsynthetic data sets, suitable for these data analytics methods, that is both\ncomputationally efficient and applicable to a diverse set of problems. An\nexample application of the proposed framework is presented along with a\nmathematical analysis of its computational efficiency, demonstrating its\neffectiveness.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 20:50:56 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Mason", "Karl", ""], ["Vejdan", "Sadegh", ""], ["Grijalva", "Santiago", ""]]}, {"id": "1903.06801", "submitter": "Luca Piccolboni", "authors": "Luca Piccolboni and Giuseppe Di Guglielmo and Luca Carloni", "title": "Securing Accelerators with Dynamic Information Flow Tracking", "comments": "IEEE International Symposium on Hardware Oriented Security and Trust\n  (HOST) - Hardware Demo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systems-on-chip (SoCs) are becoming heterogeneous: they combine\ngeneral-purpose processor cores with application-specific hardware components,\nalso known as accelerators, to improve performance and energy efficiency. The\nadvantages of heterogeneity, however, come at a price of threatening security.\nThe architectural dissimilarities of processors and accelerators require\nrevisiting the current security techniques. With this hardware demo, we show\nhow accelerators can break dynamic information flow tracking (DIFT), a\nwell-known security technique that protects systems against software-based\nattacks. We also describe how the security guarantees of DIFT can be\nre-established with a hardware solution that has low performance and area\npenalties.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 22:32:55 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Piccolboni", "Luca", ""], ["Di Guglielmo", "Giuseppe", ""], ["Carloni", "Luca", ""]]}, {"id": "1903.06802", "submitter": "Daniel Crawl", "authors": "Ilkay Altintas, Kyle Marcus, Isaac Nealey, Scott L. Sellars, John\n  Graham, Dima Mishin, Joel Polizzi, Daniel Crawl, Thomas DeFanti, Larry Smarr", "title": "Workflow-Driven Distributed Machine Learning in CHASE-CI: A Cognitive\n  Hardware and Software Ecosystem Community Infrastructure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advances in data, computing and networking over the last two decades led\nto a shift in many application domains that includes machine learning on big\ndata as a part of the scientific process, requiring new capabilities for\nintegrated and distributed hardware and software infrastructure. This paper\ncontributes a workflow-driven approach for dynamic data-driven application\ndevelopment on top of a new kind of networked Cyberinfrastructure called\nCHASE-CI. In particular, we present: 1) The architecture for CHASE-CI, a\nnetwork of distributed fast GPU appliances for machine learning and storage\nmanaged through Kubernetes on the high-speed (10-100Gbps) Pacific Research\nPlatform (PRP); 2) A machine learning software containerization approach and\nlibraries required for turning such a network into a distributed computer for\nbig data analysis; 3) An atmospheric science case study that can only be made\nscalable with an infrastructure like CHASE-CI; 4) Capabilities for virtual\ncluster management for data communication and analysis in a dynamically\nscalable fashion, and visualization across the network in specialized\nvisualization facilities in near real-time; and, 5) A step-by-step workflow and\nperformance measurement approach that enables taking advantage of the dynamic\narchitecture of the CHASE-CI network and container management infrastructure.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 02:09:31 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Altintas", "Ilkay", ""], ["Marcus", "Kyle", ""], ["Nealey", "Isaac", ""], ["Sellars", "Scott L.", ""], ["Graham", "John", ""], ["Mishin", "Dima", ""], ["Polizzi", "Joel", ""], ["Crawl", "Daniel", ""], ["DeFanti", "Thomas", ""], ["Smarr", "Larry", ""]]}, {"id": "1903.06854", "submitter": "Yoji Yamato", "authors": "Yoji Yamato, Hirofumi Noguchi, Misao Kataoka, Takuma Isoda", "title": "Study to achieve environment adaptive software", "comments": "6 pages, 1 figure, in Japanese, IEICE Technical Report, SC2018-37", "journal-ref": "IEICE Technical Report, SC2018-37, Mar. 2019. (c) 2019 IEICE", "doi": null, "report-no": "IEICE Technical Report, SC2018-37, Mar. 2019", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, heterogeneous hardware such as GPU and FPGA is used in many systems\nand also IoT devices are increased repidly. However, to utilize heterogeneous\nhardware, the hurdles are high because of much technical skills. In order to\nbreak down such a situation, we think it is required in the future that\napplication programmers only need to write logics to be processed, then\nsoftware will adapt to the environments with heterogeneous hardware, to make it\neasy to utilize heterogeneous hardware and IoT devices. Therefore, in this\npaper, we propose environment adaptive software to operate an once written\napplication with high performance by automatically converting the code and\nconfiguring setting so that we can utilize GPU, FPGA and IoT devices in the\nlocation to be deployed. We explain a proceesing flow and elemental\ntechnologies to achieve environment adaptive software.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2019 01:02:08 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Yamato", "Yoji", ""], ["Noguchi", "Hirofumi", ""], ["Kataoka", "Misao", ""], ["Isoda", "Takuma", ""]]}, {"id": "1903.06934", "submitter": "Jiarui Fang", "authors": "Jiarui Fang, Liandeng Li, Haohuan Fu, Jinlei Jiang, Wenlai Zhao,\n  Conghui He, Xin You, Guangwen Yang", "title": "swCaffe: a Parallel Framework for Accelerating Deep Learning\n  Applications on Sunway TaihuLight", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports our efforts on swCaffe, a highly efficient parallel\nframework for accelerating deep neural networks (DNNs) training on Sunway\nTaihuLight, the current fastest supercomputer in the world that adopts a unique\nmany-core heterogeneous architecture, with 40,960 SW26010 processors connected\nthrough a customized communication network. First, we point out some insightful\nprinciples to fully exploit the performance of the innovative many-core\narchitecture. Second, we propose a set of optimization strategies for\nredesigning a variety of neural network layers based on Caffe. Third, we put\nforward a topology-aware parameter synchronization scheme to scale the\nsynchronous Stochastic Gradient Descent (SGD) method to multiple processors\nefficiently. We evaluate our framework by training a variety of widely used\nneural networks with the ImageNet dataset. On a single node, swCaffe can\nachieve 23\\%\\~{}119\\% overall performance compared with Caffe running on K40m\nGPU. As compared with the Caffe on CPU, swCaffe runs 3.04\\~{}7.84x faster on\nall the networks. Finally, we present the scalability of swCaffe for the\ntraining of ResNet-50 and AlexNet on the scale of 1024 nodes.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2019 14:53:35 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Fang", "Jiarui", ""], ["Li", "Liandeng", ""], ["Fu", "Haohuan", ""], ["Jiang", "Jinlei", ""], ["Zhao", "Wenlai", ""], ["He", "Conghui", ""], ["You", "Xin", ""], ["Yang", "Guangwen", ""]]}, {"id": "1903.06950", "submitter": "Siddharth Jaiswal", "authors": "Siddharth D Jaiswal and Yogesh Simmhan", "title": "A Partition-centric Distributed Algorithm for Identifying Euler Circuits\n  in Large Graphs", "comments": "To appear in Proceedings of 5th IEEE International Workshop on\n  High-Performance Big Data, Deep Learning, and Cloud Computing, In conjunction\n  with The 33rd IEEE International Parallel and Distributed Processing\n  Symposium (IPDPS 2019), Rio de Janeiro, Brazil, May 20th, 2019", "journal-ref": "Proceedings of the IEEE International Parallel and Distributed\n  Processing Symposium Workshops, IPDPSW 2019, Rio de Janeiro, Brazil, May\n  20-24, 2019. IEEE 2019, ISBN 978-1-7281-3510-6", "doi": "10.1109/IPDPSW.2019.00085", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the Eulerian circuit in graphs is a classic problem, but inadequately\nexplored for parallel computation. With such cycles finding use in neuroscience\nand Internet of Things for large graphs, designing a distributed algorithm for\nfinding the Euler circuit is important. Existing parallel algorithms are\nimpractical for commodity clusters and Clouds. We propose a novel\npartition-centric algorithm to find the Euler circuit, over large graphs\npartitioned across distributed machines and executed iteratively using a Bulk\nSynchronous Parallel (BSP) model. The algorithm finds partial paths and cycles\nwithin each partition, and refines these into longer paths by recursively\nmerging the partitions. We describe the algorithm, analyze its complexity,\nvalidate it on Apache Spark for large graphs, and offer experimental results.\nWe also identify memory bottlenecks in the algorithm and propose an enhanced\ndesign to address it.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2019 15:57:50 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Jaiswal", "Siddharth D", ""], ["Simmhan", "Yogesh", ""]]}, {"id": "1903.06996", "submitter": "Cong Xie", "authors": "Cong Xie, Sanmi Koyejo, Indranil Gupta", "title": "SLSGD: Secure and Efficient Distributed On-device Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider distributed on-device learning with limited communication and\nsecurity requirements. We propose a new robust distributed optimization\nalgorithm with efficient communication and attack tolerance. The proposed\nalgorithm has provable convergence and robustness under non-IID settings.\nEmpirical results show that the proposed algorithm stabilizes the convergence\nand tolerates data poisoning on a small number of workers.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2019 22:25:20 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 16:53:40 GMT"}, {"version": "v3", "created": "Tue, 1 Oct 2019 20:12:18 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Xie", "Cong", ""], ["Koyejo", "Sanmi", ""], ["Gupta", "Indranil", ""]]}, {"id": "1903.07001", "submitter": "Ali Dauda Baba", "authors": "Ali Baba Dauda, Zerdoumi Saber, Faiz Alotaibi, Muhammad A. Mustapha,\n  Muhamad Taufik Abdullah", "title": "Effect of Serialized Messaging on Web Services Performance", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Serialized messages are processed at the server and sent as objects over the\nnetwork to the client to be consumed.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2019 23:01:42 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Dauda", "Ali Baba", ""], ["Saber", "Zerdoumi", ""], ["Alotaibi", "Faiz", ""], ["Mustapha", "Muhammad A.", ""], ["Abdullah", "Muhamad Taufik", ""]]}, {"id": "1903.07020", "submitter": "Cong Xie", "authors": "Cong Xie, Sanmi Koyejo, Indranil Gupta", "title": "Zeno++: Robust Fully Asynchronous SGD", "comments": "ICML version with some additional remarks related to the acceptance\n  rate of Byzantine validation, and also with the full version of error bounds\n  in the theorems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Zeno++, a new robust asynchronous Stochastic Gradient\nDescent~(SGD) procedure which tolerates Byzantine failures of the workers. In\ncontrast to previous work, Zeno++ removes some unrealistic restrictions on\nworker-server communications, allowing for fully asynchronous updates from\nanonymous workers, arbitrarily stale worker updates, and the possibility of an\nunbounded number of Byzantine workers. The key idea is to estimate the descent\nof the loss value after the candidate gradient is applied, where large descent\nvalues indicate that the update results in optimization progress. We prove the\nconvergence of Zeno++ for non-convex problems under Byzantine failures.\nExperimental results show that Zeno++ outperforms existing approaches.\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2019 03:02:32 GMT"}, {"version": "v2", "created": "Mon, 27 May 2019 05:22:43 GMT"}, {"version": "v3", "created": "Thu, 13 Jun 2019 08:19:39 GMT"}, {"version": "v4", "created": "Thu, 26 Sep 2019 22:01:24 GMT"}, {"version": "v5", "created": "Sun, 9 May 2021 17:42:44 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Xie", "Cong", ""], ["Koyejo", "Sanmi", ""], ["Gupta", "Indranil", ""]]}, {"id": "1903.07038", "submitter": "Marcelo Pereira Novaes", "authors": "Marcelo Novaes, Vin\\'icius Petrucci, Abdoulaye Gamati\\'e and Fernando\n  Quint\\~ao", "title": "Compiler-assisted Adaptive Program Scheduling in big.LITTLE Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy-aware architectures provide applications with a mix of low (LITTLE)\nand high (big) frequency cores. Choosing the best hardware configuration for a\nprogram running on such an architecture is difficult, because program parts\nbenefit differently from the same hardware configuration. State-of-the-art\ntechniques to solve this problem adapt the program's execution to dynamic\ncharacteristics of the runtime environment, such as energy consumption and\nthroughput. We claim that these purely dynamic techniques can be improved if\nthey are aware of the program's syntactic structure. To support this claim, we\nshow how to use the compiler to partition source code into program phases:\nregions whose syntactic characteristics lead to similar runtime behavior. We\nuse reinforcement learning to map pairs formed by a program phase and a\nhardware state to the configuration that best fit this setup. To demonstrate\nthe effectiveness of our ideas, we have implemented the Astro system. Astro\nuses Q-learning to associate syntactic features of programs with hardware\nconfigurations. As a proof of concept, we provide evidence that Astro\noutperforms GTS, the ARM-based Linux scheduler tailored for heterogeneous\narchitectures, on the parallel benchmarks from Rodinia and Parsec.\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2019 07:31:02 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Novaes", "Marcelo", ""], ["Petrucci", "Vin\u00edcius", ""], ["Gamati\u00e9", "Abdoulaye", ""], ["Quint\u00e3o", "Fernando", ""]]}, {"id": "1903.07114", "submitter": "Diana Andreea Popescu", "authors": "Diana Andreea Popescu and Andrew W. Moore", "title": "No Delay: Latency-Driven, Application Performance-Aware, Cluster\n  Scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the network latency variability observed in data centers, applications'\nperformance is also determined by their placement within the data centre. We\npresent NoMora, a cluster scheduling architecture whose core is represented by\na latency-driven, application performance-aware, cluster scheduling policy. The\npolicy places the tasks of an application taking into account the expected\nperformance based on the measured network latency between pairs of hosts in the\ndata center. Furthermore, if a tenant's application experiences increased\nnetwork latency, and thus lower application performance, their application may\nbe migrated to a better placement. Preliminary results show that our policy\nimproves the overall average application performance by up to 13.4% and by up\nto 42% if preemption is enabled, and improves the task placement latency by a\nfactor of 1.79x and the median algorithm runtime by 1.16x compared to a random\npolicy on the Google cluster workload. This demonstrates that application\nperformance can be improved by exploiting the relationship between network\nlatency and application performance, and the current network conditions in a\ndata center, while preserving the demands of low-latency cluster scheduling.\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2019 15:52:15 GMT"}, {"version": "v2", "created": "Thu, 21 Mar 2019 18:55:02 GMT"}, {"version": "v3", "created": "Sun, 18 Aug 2019 10:22:30 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Popescu", "Diana Andreea", ""], ["Moore", "Andrew W.", ""]]}, {"id": "1903.07266", "submitter": "Usman Khan", "authors": "Ran Xin, Anit Kumar Sahu, Usman A. Khan, and Soummya Kar", "title": "Distributed stochastic optimization with gradient tracking over\n  strongly-connected networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.MA cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study distributed stochastic optimization to minimize a sum\nof smooth and strongly-convex local cost functions over a network of agents,\ncommunicating over a strongly-connected graph. Assuming that each agent has\naccess to a stochastic first-order oracle ($\\mathcal{SFO}$), we propose a novel\ndistributed method, called $\\mathcal{S}$-$\\mathcal{AB}$, where each agent uses\nan auxiliary variable to asymptotically track the gradient of the global cost\nin expectation. The $\\mathcal{S}$-$\\mathcal{AB}$ algorithm employs row- and\ncolumn-stochastic weights simultaneously to ensure both consensus and\noptimality. Since doubly-stochastic weights are not used,\n$\\mathcal{S}$-$\\mathcal{AB}$ is applicable to arbitrary strongly-connected\ngraphs. We show that under a sufficiently small constant step-size,\n$\\mathcal{S}$-$\\mathcal{AB}$ converges linearly (in expected mean-square sense)\nto a neighborhood of the global minimizer. We present numerical simulations\nbased on real-world data sets to illustrate the theoretical results.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 06:29:08 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 22:23:17 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Xin", "Ran", ""], ["Sahu", "Anit Kumar", ""], ["Khan", "Usman A.", ""], ["Kar", "Soummya", ""]]}, {"id": "1903.07289", "submitter": "Yahya Hassanzadeh Nazarabadi", "authors": "Yahya Hassanzadeh-Nazarabadi, Alptekin K\\\"up\\c{c}\\\"u and \\\"Oznur\n  \\\"Ozkasap", "title": "Interlaced: Fully decentralized churn stabilization for Skip Graph-based\n  DHTs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a distributed hash table (DHT) routing overlay, Skip Graph is used in a\nvariety of peer-to-peer (P2P) systems including cloud storage, social networks,\nand search engines. The overlay connectivity of P2P systems is negatively\naffected by the arrivals and departures of nodes to and from the system that is\nknown as churn. Preserving connectivity of the overlay network (i.e., the\nreachability of every pair of nodes) under churn is a performance challenge in\nevery P2P system including the Skip Graph-based ones. The existing\ndecentralized churn stabilization solutions that are applicable on Skip Graphs\nhave intensive communication complexities, which leave them unable to provide a\nstrong overlay connectivity, especially under high rates of churn.\n  In this paper, we propose Interlaced, a fully decentralized churn\nstabilization mechanism for Skip Graphs that provides drastically stronger\noverlay connectivity without changing the asymptotic complexity of the Skip\nGraph in terms of storage, computation, and communication. We also propose the\nSliding Window De Bruijn Graph (SW-DBG) as a tool to predict the availability\nof nodes with high accuracy. Our simulation results show that in comparison to\nthe best existing DHT-based solutions, Interlaced improves the overlay\nconnectivity of Skip Graph under churn with the gain of about 1.81 times. A\nSkip Graph that benefits from Interlaced and SW-DBG is about 2.47 times faster\non average in routing the queries under churn compared to the best existing\nsolutions. We also present an adaptive extension of Interlaced to be applied on\nother DHTs, for example Kademlia.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 07:56:01 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Hassanzadeh-Nazarabadi", "Yahya", ""], ["K\u00fcp\u00e7\u00fc", "Alptekin", ""], ["\u00d6zkasap", "\u00d6znur", ""]]}, {"id": "1903.07424", "submitter": "Yang Chen Mr", "authors": "Yang Chen, Xiaoyan Sun, Yaochu Jin", "title": "Communication-Efficient Federated Deep Learning with Asynchronous Model\n  Update and Temporally Weighted Aggregation", "comments": null, "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems, 2019", "doi": "10.1109/TNNLS.2019.2953131", "report-no": null, "categories": "cs.LG cs.AI cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning obtains a central model on the server by aggregating\nmodels trained locally on clients. As a result, federated learning does not\nrequire clients to upload their data to the server, thereby preserving the data\nprivacy of the clients. One challenge in federated learning is to reduce the\nclient-server communication since the end devices typically have very limited\ncommunication bandwidth. This paper presents an enhanced federated learning\ntechnique by proposing a synchronous learning strategy on the clients and a\ntemporally weighted aggregation of the local models on the server. In the\nasynchronous learning strategy, different layers of the deep neural networks\nare categorized into shallow and deeps layers and the parameters of the deep\nlayers are updated less frequently than those of the shallow layers.\nFurthermore, a temporally weighted aggregation strategy is introduced on the\nserver to make use of the previously trained local models, thereby enhancing\nthe accuracy and convergence of the central model. The proposed algorithm is\nempirically on two datasets with different deep neural networks. Our results\ndemonstrate that the proposed asynchronous federated deep learning outperforms\nthe baseline algorithm both in terms of communication cost and model accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 13:27:42 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Chen", "Yang", ""], ["Sun", "Xiaoyan", ""], ["Jin", "Yaochu", ""]]}, {"id": "1903.07486", "submitter": "Daniele Scarpazza", "authors": "Zhe Jia, Marco Maggioni, Jeffrey Smith, Daniele Paolo Scarpazza", "title": "Dissecting the NVidia Turing T4 GPU via Microbenchmarking", "comments": "65 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2019, the rapid rate at which GPU manufacturers refresh their designs,\ncoupled with their reluctance to disclose microarchitectural details, is still\na hurdle for those software designers who want to extract the highest possible\nperformance. Last year, these very reasons motivated us to dissect the Volta\nGPU architecture using microbenchmarks.\n  The introduction in August 2018 of Turing, NVidia's latest architecture,\npressed us to update our study. In this report, we examine Turing and compare\nit quantitatively against previous NVidia GPU generations. Specifically, we\nstudy the T4 GPU: a low-power board aiming at inference applications. We\ndescribe its improvements against its inference-oriented predecessor: the P4\nGPU based on the Pascal architecture. Both T4 and P4 GPUs achieve significantly\nhigher frequency-per-Watt figures than their full-size counterparts.\n  We study the performance of the T4's TensorCores, finding a much higher\nthroughput on low-precision operands than on the P4 GPU. We reveal that Turing\nintroduces new instructions that express matrix math more succinctly. We map\nTuring's instruction space, finding the same encoding as Volta, and additional\ninstructions. We reveal that the Turing TU104 chip has the same memory\nhierarchy depth as the Volta GV100; cache levels sizes on the TU104 are\nfrequently twice as large as those found on the Pascal GP104. We benchmark each\nconstituent of the T4 memory hierarchy and find substantial overall performance\nimprovements over its P4 predecessor. We studied how clock throttling affects\ncompute-intensive workloads that hit power or thermal limits.\n  Many of our findings are novel, published here for the first time. All of\nthem can guide high-performance software developers get closer to the GPU's\npeak performance.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 14:45:46 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Jia", "Zhe", ""], ["Maggioni", "Marco", ""], ["Smith", "Jeffrey", ""], ["Scarpazza", "Daniele Paolo", ""]]}, {"id": "1903.07557", "submitter": "Xiuqin Shang", "authors": "Xiuqin Shang and Dayong Shen and Fei-Yue Wang and Timo R. Nyberg", "title": "A Heuristic Algorithm for the Fabric Spreading and Cutting Problem in\n  Apparel Factories", "comments": "accepted by IEEE/CAA Journal of Automatica Sinica", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the fabric spreading and cutting problem in apparel factories. For\nthe sake of saving the material costs, the cutting requirement should be met\nexactly without producing additional garment components. For reducing the\nproduction costs, the number of lays that corresponds to the frequency of using\nthe cutting beds should be minimized. We propose an iterated greedy algorithm\nfor solving the fabric spreading and cutting problem. This algorithm contains a\nconstructive procedure and an improving loop. Firstly the constructive\nprocedure creates a set of lays in sequence, and then the improving loop tries\nto pick each lay from the lay set and rearrange the remaining lays into a\nsmaller lay set. The improving loop will run until it cannot obtain any small\nlay set or the time limit is due. The experiment results on 500 cases shows\nthat the proposed algorithm is effective and efficient.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 07:51:27 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Shang", "Xiuqin", ""], ["Shen", "Dayong", ""], ["Wang", "Fei-Yue", ""], ["Nyberg", "Timo R.", ""]]}, {"id": "1903.07676", "submitter": "Ye Yu", "authors": "Ye Yu, Yingmin Li, Shuai Che, Niraj K. Jha, and Weifeng Zhang", "title": "Software-defined Design Space Exploration for an Efficient DNN\n  Accelerator Architecture", "comments": null, "journal-ref": null, "doi": "10.1109/TC.2020.2983694", "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have been shown to outperform conventional\nmachine learning algorithms across a wide range of applications, e.g., image\nrecognition, object detection, robotics, and natural language processing.\nHowever, the high computational complexity of DNNs often necessitates extremely\nfast and efficient hardware. The problem gets worse as the size of neural\nnetworks grows exponentially. As a result, customized hardware accelerators\nhave been developed to accelerate DNN processing without sacrificing model\naccuracy. However, previous accelerator design studies have not fully\nconsidered the characteristics of the target applications, which may lead to\nsub-optimal architecture designs. On the other hand, new DNN models have been\ndeveloped for better accuracy, but their compatibility with the underlying\nhardware accelerator is often overlooked. In this article, we propose an\napplication-driven framework for architectural design space exploration of DNN\naccelerators. This framework is based on a hardware analytical model of\nindividual DNN operations. It models the accelerator design task as a\nmulti-dimensional optimization problem. We demonstrate that it can be\nefficaciously used in application-driven accelerator architecture design. Given\na target DNN, the framework can generate efficient accelerator design solutions\nwith optimized performance and area. Furthermore, we explore the opportunity to\nuse the framework for accelerator configuration optimization under simultaneous\ndiverse DNN applications. The framework is also capable of improving neural\nnetwork models to best fit the underlying hardware resources.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 19:03:50 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2020 20:44:29 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Yu", "Ye", ""], ["Li", "Yingmin", ""], ["Che", "Shuai", ""], ["Jha", "Niraj K.", ""], ["Zhang", "Weifeng", ""]]}, {"id": "1903.07712", "submitter": "David Bermbach", "authors": "David Bermbach and Erik Wittern", "title": "Benchmarking Web API Quality -- Revisited", "comments": "Accepted for publication in the Rivers Journal of Web Engineering.\n  The paper is text-wise identical to the camera-ready version but uses a\n  different template", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern applications increasingly interact with web APIs -- reusable\ncomponents, deployed and operated outside the application, and accessed over\nthe network. Their existence, arguably, spurs application innovations, making\nit easy to integrate data or functionalities. While previous work has analyzed\nthe ecosystem of web APIs and their design, little is known about web API\nquality at runtime. This gap is critical, as qualities including availability,\nlatency, or provider security preferences can severely impact applications and\nuser experience. In this paper, we revisit a 3-month, geo-distributed benchmark\nof popular web APIs, originally performed in 2015. We repeat this benchmark in\n2018 and compare results from these two benchmarks regarding availability and\nlatency. We furthermore introduce new results from assessing provider security\npreferences, collected both in 2015 and 2018, and results from our attempts to\nreach out to API providers with the results from our 2015 experiments. Our\nextensive experiments show that web API qualities vary 1.) based on the\ngeo-distribution of clients, 2.) during our individual experiments, and 3.)\nbetween the two experiments. Our findings provide evidence to foster the\ndiscussion around web API quality, and can act as a basis for the creation of\ntools and approaches to mitigate quality issues.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 20:50:25 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 13:27:11 GMT"}, {"version": "v3", "created": "Fri, 3 Jul 2020 15:24:21 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Bermbach", "David", ""], ["Wittern", "Erik", ""]]}, {"id": "1903.07748", "submitter": "Panagiotis Tampakis", "authors": "Panagiotis Tampakis, Christos Doulkeridis, Nikos Pelekis and Yannis\n  Theodoridis", "title": "Distributed Subtrajectory Join on Massive Datasets", "comments": null, "journal-ref": "ACM Transactions on Spatial Algorithms and Systems (TSAS), Volume\n  6, Issue 2, (2020), Article No.: 8", "doi": "10.1145/3373642", "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joining trajectory datasets is a significant operation in mobility data\nanalytics and the cornerstone of various methods that aim to extract knowledge\nout of them. In the era of Big Data, the production of mobility data has become\nmassive and, consequently, performing such an operation in a centralized way is\nnot feasible. In this paper, we address the problem of Distributed\nSubtrajectory Join processing by utilizing the MapReduce programming model.\nCompared to traditional trajectory join queries, this problem is even more\nchallenging since the goal is to retrieve all the \"maximal\" portions of\ntrajectories that are \"similar\". We propose three solutions: (i) a\nwell-designed basic solution, coined DTJb, (ii) a solution that uses a\npreprocessing step that repartitions the data, labeled DTJr, and (iii) a\nsolution that, additionally, employs an indexing scheme, named DTJi. In our\nexperimental study, we utilize a 56GB dataset of real trajectories from the\nmaritime domain, which, to the best of our knowledge, is the largest real\ndataset used for experimentation in the literature of trajectory data\nmanagement. The results show that DTJi performs up to 16x faster compared with\nDTJb, 10x faster than DTJr and 3x faster than the closest related state of the\nart algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 22:39:30 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Tampakis", "Panagiotis", ""], ["Doulkeridis", "Christos", ""], ["Pelekis", "Nikos", ""], ["Theodoridis", "Yannis", ""]]}, {"id": "1903.07754", "submitter": "Samuel Grossman", "authors": "Samuel Grossman, Christos Kozyrakis", "title": "A New Frontier for Pull-Based Graph Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The trade-off between pull-based and push-based graph processing engines is\nwell-understood. On one hand, pull-based engines can achieve higher throughput\nbecause their workloads are read-dominant, rather than write-dominant, and can\nproceed without synchronization between threads. On the other hand, push-based\nengines are much better able to take advantage of the frontier optimization,\nwhich leverages the fact that often only a small subset of the graph needs to\nbe accessed to complete an iteration of a graph processing application. Hybrid\nengines attempt to overcome this trade-off by dynamically switching between\npush and pull, but there are two key disadvantages with this approach. First,\napplications must be implemented twice (once for push and once for pull), and\nsecond, processing throughput is reduced for iterations that run with push.\n  We propose a radically different solution: rebuild the frontier optimization\nentirely such that it is well-suited for a pull-based engine. In doing so, we\nremove the only advantage that a push-based engine had over a pull-based\nengine, making it possible to eliminate the push-based engine entirely. We\nintroduce Wedge, a pull-only graph processing framework that transforms the\ntraditional source-oriented vertex-based frontier into a pull-friendly format\ncalled the Wedge Frontier. The transformation itself is expensive even when\nparallelized, so we introduce two key optimizations to make it practical.\nFirst, we perform the transformation only when the resulting Wedge Frontier is\nsufficiently sparse. Second, we coarsen the granularity of the representation\nof elements in the Wedge Frontier. These optimizations respectively improve\nWedge's performance by up to 5x and 2x, enabling it to outperform Grazelle,\nLigra, and GraphMat respectively by up to 2.8x, 4.9x, and 185.5x.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 22:56:34 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Grossman", "Samuel", ""], ["Kozyrakis", "Christos", ""]]}, {"id": "1903.07761", "submitter": "Panagiotis Hadjidoukas", "authors": "Panagiotis Hadjidoukas and Fabian Wermelinger", "title": "A Parallel Data Compression Framework for Large Scale 3D Scientific Data", "comments": "26 pages, 12 figures, open-source software", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale simulations of complex systems ranging from climate and\nastrophysics to crowd dynamics, produce routinely petabytes of data and are\nprojected to reach the zettabytes level in the coming decade. These simulations\nenable unprecedented insights but at the same their effectiveness is hindered\nby the enormous data sizes associated with the computational elements and\nrespective output quantities of interest that impose severe constraints on\nstorage and I/O time. In this work, we address these challenges through a novel\nsoftware framework for scientific data compression. The software (CubismZ)\nincorporates efficient wavelet based techniques and the state-of-the-art ZFP,\nSZ and FPZIP floating point compressors. The framework relies on a\nblock-structured data layout, benefits from OpenMP and MPI and targets\nsupercomputers based on multicores. CubismZ can be used as a tool for ex situ\n(offline) compression of scientific datasets and supports conventional\nComputational Fluid Dynamics (CFD) file formats. Moreover, it provides a\ntestbed of comparison, in terms of compression factor and peak signal-to-noise\nratio, for a number of available data compression methods. The software yields\nin situ compression ratios of 100x or higher for fluid dynamics data produced\nby petascale simulations of cloud cavitation collapse using\n$\\mathcal{O}(10^{11})$ grid cells, with negligible impact on the total\nsimulation time.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 23:19:49 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Hadjidoukas", "Panagiotis", ""], ["Wermelinger", "Fabian", ""]]}, {"id": "1903.08114", "submitter": "Ke Alexander Wang", "authors": "Ke Alexander Wang, Geoff Pleiss, Jacob R. Gardner, Stephen Tyree,\n  Kilian Q. Weinberger, Andrew Gordon Wilson", "title": "Exact Gaussian Processes on a Million Data Points", "comments": "Published at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes (GPs) are flexible non-parametric models, with a capacity\nthat grows with the available data. However, computational constraints with\nstandard inference procedures have limited exact GPs to problems with fewer\nthan about ten thousand training points, necessitating approximations for\nlarger datasets. In this paper, we develop a scalable approach for exact GPs\nthat leverages multi-GPU parallelization and methods like linear conjugate\ngradients, accessing the kernel matrix only through matrix multiplication. By\npartitioning and distributing kernel matrix multiplies, we demonstrate that an\nexact GP can be trained on over a million points, a task previously thought to\nbe impossible with current computing hardware, in less than 2 hours. Moreover,\nour approach is generally applicable, without constraints to grid data or\nspecific kernel classes. Enabled by this scalability, we perform the first-ever\ncomparison of exact GPs against scalable GP approximations on datasets with\n$10^4 \\!-\\! 10^6$ data points, showing dramatic performance improvements.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 17:10:28 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 18:44:52 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Wang", "Ke Alexander", ""], ["Pleiss", "Geoff", ""], ["Gardner", "Jacob R.", ""], ["Tyree", "Stephen", ""], ["Weinberger", "Kilian Q.", ""], ["Wilson", "Andrew Gordon", ""]]}, {"id": "1903.08228", "submitter": "Olaf Witkowski", "authors": "Olaf Witkowski, Takashi Ikegami", "title": "How to Make Swarms Open-Ended? Evolving Collective Intelligence Through\n  a Constricted Exploration of Adjacent Possibles", "comments": "40 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.DC cs.NE nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach of open-ended evolution via the simulation of swarm\ndynamics. In nature, swarms possess remarkable properties, which allow many\norganisms, from swarming bacteria to ants and flocking birds, to form\nhigher-order structures that enhance their behavior as a group. Swarm\nsimulations highlight three important factors to create novelty and diversity:\n(a) communication generates combinatorial cooperative dynamics, (b) concurrency\nallows for separation of timescales, and (c) complexity and size increases push\nthe system towards transitions in innovation. We illustrate these three\ncomponents in a model computing the continuous evolution of a swarm of agents.\nThe results, divided in three distinct applications, show how emergent\nstructures are capable of filtering information through the bottleneck of their\nmemory, to produce meaningful novelty and diversity within their simulated\nenvironment.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 19:34:49 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Witkowski", "Olaf", ""], ["Ikegami", "Takashi", ""]]}, {"id": "1903.08252", "submitter": "Martin \\v{S}urkovsk\\'y", "authors": "Martin \\v{S}urkovsk\\'y", "title": "MP net as Abstract Model of Communication for Message-passing\n  Applications", "comments": "Fix typos; Modify layout of figures 1. and 4. and add figure 13.\n  (message broker); Add a paragraph to conclusion", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MP net is a formal model specifically designed for the field of parallel\napplications that use a message passing interface. The main idea is to use MP\nnet as a comprehensible way of presenting the actual structure of communication\nwithin MPI applications. The goal is to provide users with the kind of feedback\nthat can help them to check quickly whether or not the actual communication\nwithin their application corresponds to the intended one. This paper introduces\nMP net that focuses on the communication part of parallel applications and\nemphasizes its spatial character, which is rather hidden in sequential\n(textual) form.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 20:42:09 GMT"}, {"version": "v2", "created": "Fri, 29 Mar 2019 07:34:25 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["\u0160urkovsk\u00fd", "Martin", ""]]}, {"id": "1903.08351", "submitter": "Mehrdad Ghadiri", "authors": "Mehrdad Ghadiri, Mark Schmidt", "title": "Distributed Maximization of Submodular plus Diversity Functions for\n  Multi-label Feature Selection on Huge Datasets", "comments": "17 pages, accepted in AISTATS'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many problems in machine learning and data mining which are\nequivalent to selecting a non-redundant, high \"quality\" set of objects.\nRecommender systems, feature selection, and data summarization are among many\napplications of this. In this paper, we consider this problem as an\noptimization problem that seeks to maximize the sum of a sum-sum diversity\nfunction and a non-negative monotone submodular function. The diversity\nfunction addresses the redundancy, and the submodular function controls the\npredictive quality. We consider the problem in big data settings (in other\nwords, distributed and streaming settings) where the data cannot be stored on a\nsingle machine or the process time is too high for a single machine. We show\nthat a greedy algorithm achieves a constant factor approximation of the optimal\nsolution in these settings. Moreover, we formulate the multi-label feature\nselection problem as such an optimization problem. This formulation combined\nwith our algorithm leads to the first distributed multi-label feature selection\nmethod. We compare the performance of this method with centralized multi-label\nfeature selection methods in the literature, and we show that its performance\nis comparable or in some cases is even better than current centralized\nmulti-label feature selection methods.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 06:08:03 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2019 05:29:43 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Ghadiri", "Mehrdad", ""], ["Schmidt", "Mark", ""]]}, {"id": "1903.08752", "submitter": "Nirupam Gupta", "authors": "Nirupam Gupta and Nitin H. Vaidya", "title": "Byzantine Fault Tolerant Distributed Linear Regression", "comments": "Manuscript revised by adding; a new improved filtering technique, and\n  convergence analysis with noise", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of Byzantine fault tolerance in distributed\nlinear regression in a multi-agent system. However, the proposed algorithms are\ngiven for a more general class of distributed optimization problems, of which\ndistributed linear regression is a special case. The system comprises of a\nserver and multiple agents, where each agent is holding a certain number of\ndata points and responses that satisfy a linear relationship (could be noisy).\nThe objective of the server is to determine this relationship, given that some\nof the agents in the system (up to a known number) are Byzantine faulty (aka.\nactively adversarial). We show that the server can achieve this objective, in a\ndeterministic manner, by robustifying the original distributed gradient descent\nmethod using norm based filters, namely 'norm filtering' and 'norm-cap\nfiltering', incurring an additional log-linear computation cost in each\niteration. The proposed algorithms improve upon the existing methods on three\nlevels: i) no assumptions are required on the probability distribution of data\npoints, ii) system can be partially asynchronous, and iii) the computational\noverhead (in order to handle Byzantine faulty agents) is log-linear in number\nof agents and linear in dimension of data points. The proposed algorithms\ndiffer from each other in the assumptions made for their correctness, and the\ngradient filter they use.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 21:37:42 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 15:05:46 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Gupta", "Nirupam", ""], ["Vaidya", "Nitin H.", ""]]}, {"id": "1903.08781", "submitter": "Christian M. Fuchs", "authors": "Christian M. Fuchs, Nadia Murillo, Aske Plaat, Erik Van der Kouwe,\n  Daniel Harsono, and Todor Stefanov", "title": "Fault-Tolerant Nanosatellite Computing on a Budget", "comments": null, "journal-ref": "Conference on Radiation Effects on Components and Systems 2018\n  (RADECS)", "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.RO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Micro- and nanosatellites have become popular platforms for a variety of\ncommercial and scientific applications, but today are considered suitable\nmainly for short and low-priority space missions due to their low reliability.\nIn part, this can be attributed to their reliance upon cheap, low-feature size,\nCOTS components originally designed for embedded and mobile-market\napplications, for which traditional hardware-voting concepts are ineffective.\nSoftware-fault-tolerance concepts have been shown effective for such systems,\nbut have largely been ignored by the space industry due to low maturity, as\nmost have only been researched in theory. In practice, designers of payload\ninstruments and miniaturized satellites are usually forced to sacrifice\nreliability in favor deliver the level of performance necessary for\ncutting-edge science and innovative commercial applications. Thus, we developed\na software-fault-tolerance-approach based upon thread-level coarse-grain\nlockstep, which was validated using fault-injection. To offer strong long-term\nfault coverage, our architecture is implemented as tiled MPSoC on an FPGA,\nutilizing partial reconfiguration, as well as mixed criticality. This\narchitecture can satisfy the high performance requirements of current and\nfuture scientific and commercial space missions at very low cost, while\noffering the strong fault-coverage guarantees necessary for platform control\neven for missions with a long duration. This architecture was developed for a\n4-year ESA project. Together with two industrial partners, we are developing a\nprototype to then undergo radiation testing.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 00:00:34 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Fuchs", "Christian M.", ""], ["Murillo", "Nadia", ""], ["Plaat", "Aske", ""], ["Van der Kouwe", "Erik", ""], ["Harsono", "Daniel", ""], ["Stefanov", "Todor", ""]]}, {"id": "1903.08857", "submitter": "Vipul Gupta", "authors": "Vipul Gupta, Swanand Kadhe, Thomas Courtade, Michael W. Mahoney,\n  Kannan Ramchandran", "title": "OverSketched Newton: Fast Convex Optimization for Serverless Systems", "comments": "37 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by recent developments in serverless systems for large-scale\ncomputation as well as improvements in scalable randomized matrix algorithms,\nwe develop OverSketched Newton, a randomized Hessian-based optimization\nalgorithm to solve large-scale convex optimization problems in serverless\nsystems. OverSketched Newton leverages matrix sketching ideas from Randomized\nNumerical Linear Algebra to compute the Hessian approximately. These sketching\nmethods lead to inbuilt resiliency against stragglers that are a characteristic\nof serverless architectures. Depending on whether the problem is strongly\nconvex or not, we propose different iteration updates using the approximate\nHessian. For both cases, we establish convergence guarantees for OverSketched\nNewton and empirically validate our results by solving large-scale supervised\nlearning problems on real-world datasets. Experiments demonstrate a reduction\nof ~50% in total running time on AWS Lambda, compared to state-of-the-art\ndistributed optimization schemes.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 07:33:11 GMT"}, {"version": "v2", "created": "Fri, 14 Jun 2019 17:22:45 GMT"}, {"version": "v3", "created": "Thu, 27 Aug 2020 08:26:33 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Gupta", "Vipul", ""], ["Kadhe", "Swanand", ""], ["Courtade", "Thomas", ""], ["Mahoney", "Michael W.", ""], ["Ramchandran", "Kannan", ""]]}, {"id": "1903.08969", "submitter": "Sayed Chhattan Shah", "authors": "Sayed Chhattan Shah", "title": "An Energy-Efficient Resource Management System for a Mobile Ad Hoc Cloud", "comments": "19 Pages", "journal-ref": "IEEE ACCESS 2019", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, mobile ad hoc clouds have emerged as a promising technology for\nmobile cyber-physical system applications, such as mobile intelligent video\nsurveillance and smart homes. Resource management plays a key role in\nmaximizing resource utilization and application performance in mobile ad hoc\nclouds. Unlike resource management in traditional distributed computing\nsystems, such as clouds, resource management in a mobile ad hoc cloud poses\nnumerous challenges owing to the node mobility, limited battery power, high\nlatency, and the dynamic network environment. The real-time requirements\nassociated with mobile cyber-physical system applications make the problem even\nmore challenging. Currently, existing resource management systems for mobile ad\nhoc clouds are not designed to support mobile cyber-physical system\napplications and energy-efficient communication between application tasks. In\nthis paper, we propose a new energy-efficient resource management system for\nmobile ad hoc clouds. The proposed system consists of two layers: a network\nlayer and a middleware layer. The network layer provides ad hoc network and\ncommunication services to the middleware layer and shares the collected\ninformation in order to allow efficient and robust resource management\ndecisions. It uses (1) a transmission power control mechanism to improve energy\nefficiency and network capacity, (2) link lifetimes to reduce communication and\nenergy consumption costs, and (3) link quality to estimate data transfer times.\nThe middleware layer is responsible for the discovery, monitoring, migration,\nand allocation of resources. It receives application tasks from users and\nallocates tasks to nodes on the basis of network and node-level information.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 13:03:53 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Shah", "Sayed Chhattan", ""]]}, {"id": "1903.08988", "submitter": "Giovanni Farina", "authors": "Silvia Bonomi, Giovanni Farina (NPA), S\\'ebastien Tixeuil (NPA, LINCS)", "title": "Multi-hop Byzantine Reliable Broadcast with Honest Dealer Made Practical", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit Byzantine tolerant reliable broadcast with honest dealer\nalgorithms in multi-hop networks. To tolerate Byzantine faulty nodes\narbitrarily spread over the network, previous solutions require a factorial\nnumber of messages to be sent over the network if the messages are not\nauthenticated (e.g. digital signatures are not available). We propose\nmodifications that preserve the safety and liveness properties of the original\nunauthenticated protocols, while highly decreasing their observed message\ncomplexity when simulated on several classes of graph topologies, potentially\nopening to their employment.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 13:34:17 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 14:37:18 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Bonomi", "Silvia", "", "NPA"], ["Farina", "Giovanni", "", "NPA"], ["Tixeuil", "S\u00e9bastien", "", "NPA, LINCS"]]}, {"id": "1903.09106", "submitter": "Diego Didona Dr", "authors": "Diego Didona, Panagiota Fatourou, Rachid Guerraoui, Jingjing Wang,\n  Willy Zwaenepoel", "title": "Distributed Transactional Systems Cannot Be Fast", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that no fully transactional system can provide fast read\ntransactions (including read-only ones that are considered the most frequent in\npractice). Specifically, to achieve fast read transactions, the system has to\ngive up support of transactions that write more than one object. We prove this\nimpossibility result for distributed storage systems that are causally\nconsistent, i.e., they do not require to ensure any strong form of consistency.\nTherefore, our result holds also for any system that ensures a consistency\nlevel stronger than causal consistency, e.g., strict serializability. The\nimpossibility result holds even for systems that store only two objects (and\nsupport at least two servers and at least four clients). It also holds for\nsystems that are partially replicated. Our result justifies the design choices\nof state-of-the-art distributed transactional systems and insists that system\ndesigners should not put more effort to design fully-functional systems that\nsupport both fast read transactions and ensure causal or any stronger form of\nconsistency.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 16:43:03 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 05:59:22 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Didona", "Diego", ""], ["Fatourou", "Panagiota", ""], ["Guerraoui", "Rachid", ""], ["Wang", "Jingjing", ""], ["Zwaenepoel", "Willy", ""]]}, {"id": "1903.09276", "submitter": "Zhongli Dong", "authors": "Zhongli Dong, Young Choon Lee, Albert Y. Zomaya", "title": "Proofware: Proof of Useful Work Blockchain Consensus Protocol for\n  Decentralized Applications", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a blockchain system, consensus protocol as an incentive and security\nmechanism, is to ensure the participants to build the block honestly and\neffectively. There are different consensus protocols for blockchain, like Proof\nof work (PoW), Proof of Stake (PoS), Proof of Space (PoSpace), Proof of\nActivities etc. But most of these consensus protocols are not designed for\ndoing some useful jobs for society because of too much competition and\nscalability limitation. Massive electric power and computing resources,\nincluding CPU, RAM, storage and sensors have been wasted to run blockchain\nnetwork based on these consensus protocols. Current frameworks and middleware\nfor building decentralised applications (dApps) are largely limited to simple\nand less useful jobs. In this paper, we present Proofware which is designed for\ndevelopers to build their dApps easily with existing public/crowd-based\ncomputing resources. Under Proofware, developers can develop and test their own\nProof of Useful Work (PoUW) consensus protocols. Also, rather than depending on\na centralised accounting system, each dApp has an embedded currency system to\nkeep the whole incentive system decentralised, fair, transparent, stable and\nsustainable. Based on Proofware, we have built a crowd based video sharing\napplication, called OurTube, as a case study. By the OurTube example, it has\nshown Proofware significantly improves the productivity to build crowd-based\ncomputing system with the features of cost-effectiveness, anti-censorship,\nelasticity and financial sustainability.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 00:12:15 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Dong", "Zhongli", ""], ["Lee", "Young Choon", ""], ["Zomaya", "Albert Y.", ""]]}, {"id": "1903.09310", "submitter": "Fabien Bouquillon", "authors": "Fabien Bouquillon, Cl\\'ement Ballabriga, Giuseppe Lipari, Smail Niar", "title": "A WCET-aware cache coloring technique for reducing interference in\n  real-time systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The predictability of a system is the condition to give saferbound on worst\ncase execution timeof real-time tasks which are running on it. Commercial\noff-the-shelf(COTS) processors are in-creasingly used in embedded systems and\ncontain shared cache memory. This component hasa hard predictable behavior\nbecause its state depends of theexecution history of the systems.To increase\npredictability of COTS component we use cache coloring, a technique widely\nusedto partition cache memory. Our main contribution is a WCET aware heuristic\nwhich parti-tion task according to the needs of each task. Our experiments are\nmade with CPLEX an ILPsolver with random tasks set generated running on\npreemptive system scheduled with earliestdeadline first(EDF).\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 13:08:13 GMT"}, {"version": "v2", "created": "Tue, 26 Mar 2019 15:56:22 GMT"}, {"version": "v3", "created": "Wed, 27 Mar 2019 16:00:02 GMT"}, {"version": "v4", "created": "Mon, 20 May 2019 16:35:52 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Bouquillon", "Fabien", ""], ["Ballabriga", "Cl\u00e9ment", ""], ["Lipari", "Giuseppe", ""], ["Niar", "Smail", ""]]}, {"id": "1903.09321", "submitter": "Edgar Dobriban", "authors": "Edgar Dobriban, Yue Sheng", "title": "WONDER: Weighted one-shot distributed ridge regression in high\n  dimensions", "comments": "Gave the name \"Wonder\" to the algorithm, updated title, added\n  algorithm for general non-isotropic design", "journal-ref": null, "doi": null, "report-no": "Journal of Machine Learning Research 21(66) p. 1-52 2020. Short\n  version at ICML 2020", "categories": "math.ST cs.DC cs.LG stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many areas, practitioners need to analyze large datasets that challenge\nconventional single-machine computing. To scale up data analysis, distributed\nand parallel computing approaches are increasingly needed.\n  Here we study a fundamental and highly important problem in this area: How to\ndo ridge regression in a distributed computing environment? Ridge regression is\nan extremely popular method for supervised learning, and has several optimality\nproperties, thus it is important to study. We study one-shot methods that\nconstruct weighted combinations of ridge regression estimators computed on each\nmachine. By analyzing the mean squared error in a high dimensional\nrandom-effects model where each predictor has a small effect, we discover\nseveral new phenomena.\n  1. Infinite-worker limit: The distributed estimator works well for very large\nnumbers of machines, a phenomenon we call \"infinite-worker limit\".\n  2. Optimal weights: The optimal weights for combining local estimators sum to\nmore than unity, due to the downward bias of ridge. Thus, all averaging methods\nare suboptimal.\n  We also propose a new Weighted ONe-shot DistributEd Ridge regression (WONDER)\nalgorithm. We test WONDER in simulation studies and using the Million Song\nDataset as an example. There it can save at least 100x in computation time,\nwhile nearly preserving test accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 02:26:29 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 20:25:58 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Dobriban", "Edgar", ""], ["Sheng", "Yue", ""]]}, {"id": "1903.09422", "submitter": "Alexander van der Grinten", "authors": "Alexander van der Grinten, Eugenio Angriman, Henning Meyerhenke", "title": "Parallel Adaptive Sampling with almost no Synchronization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximation via sampling is a widespread technique whenever exact solutions\nare too expensive. In this paper, we present techniques for an efficient\nparallelization of adaptive (a. k. a. progressive) sampling algorithms on\nmulti-threaded shared-memory machines. Our basic algorithmic technique requires\nno synchronization except for atomic load-acquire and store-release operations.\nIt does, however, require O(n) memory per thread, where n is the size of the\nsampling state. We present variants of the algorithm that either reduce this\nmemory consumption to O(1) or ensure that deterministic results are obtained.\nUsing the KADABRA algorithm for betweenness centrality (a popular measure in\nnetwork analysis) approximation as a case study, we demonstrate the empirical\nperformance of our techniques. In particular, on a 32-core machine, our best\nalgorithm is 2.9x faster than what we could achieve using a straightforward\nOpenMP-based parallelization and 65.3x faster than the existing implementation\nof KADABRA.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 09:52:17 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["van der Grinten", "Alexander", ""], ["Angriman", "Eugenio", ""], ["Meyerhenke", "Henning", ""]]}, {"id": "1903.09477", "submitter": "Gregor Ulm", "authors": "Gregor Ulm, Simon Smith, Adrian Nilsson, Emil Gustavsson, Mats\n  Jirstrand", "title": "Facilitating Rapid Prototyping in the OODIDA Data Analytics Platform via\n  Active-Code Replacement", "comments": "24 pages, 4 figures, 3 code listings, 1 table", "journal-ref": "Array Vol. 8, December 2020 (100043)", "doi": "10.1016/j.array.2020.100043", "report-no": null, "categories": "cs.DC cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  OODIDA (On-board/Off-board Distributed Data Analytics) is a platform for\ndistributed real-time analytics, targeting fleets of reference vehicles in the\nautomotive industry. Its users are data analysts. The bulk of the data\nanalytics tasks are performed by clients (on-board), while a central cloud\nserver performs supplementary tasks (off-board). OODIDA can be automatically\npackaged and deployed, which necessitates restarting parts of the system, or\nall of it. As this is potentially disruptive, we added the ability to execute\nuser-defined Python modules on clients as well as the server. These modules can\nbe replaced without restarting any part of the system; they can even be\nreplaced between iterations of an ongoing assignment. This feature is referred\nto as active-code replacement. It facilitates use cases such as iterative A/B\ntesting of machine learning algorithms or modifying experimental algorithms\non-the-fly. Consistency of results is achieved by majority vote, which prevents\ntainted state. Active-code replacement can be done in less than a second in an\nidealized setting whereas a standard deployment takes many orders of magnitude\nmore time. The main contribution of this paper is the description of a\nrelatively straightforward approach to active-code replacement that is very\nuser-friendly. It enables a data analyst to quickly execute custom code on the\ncloud server as well as on client devices. Sensible safeguards and design\ndecisions ensure that this feature can be used by non-specialists who are not\nfamiliar with the implementation of OODIDA in general or this feature in\nparticular. As a consequence of adding the active-code replacement feature,\nOODIDA is now very well-suited for rapid prototyping.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 12:46:34 GMT"}, {"version": "v2", "created": "Thu, 5 Sep 2019 13:26:01 GMT"}, {"version": "v3", "created": "Tue, 25 Feb 2020 10:36:43 GMT"}, {"version": "v4", "created": "Wed, 30 Dec 2020 10:34:47 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Ulm", "Gregor", ""], ["Smith", "Simon", ""], ["Nilsson", "Adrian", ""], ["Gustavsson", "Emil", ""], ["Jirstrand", "Mats", ""]]}, {"id": "1903.09507", "submitter": "Grigore Stamatescu", "authors": "Viorel Mihai, Cristina Elena Hanganu, Grigore Stamatescu, Dan Popescu", "title": "WSN and Fog Computing Integration for Intelligent Data Processing", "comments": "4 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networked embedded systems endowed with sensing, computing, control and\ncommunication capabilities allow the development of various application\nscenarios and represent the building blocks of the Internet of Things (IoT)\nparadigm. Traditional data collection methods include multiple field level IoT\nsystems that can relay data stemming from a network of distributed ground\nsensors directly to a cloud platform for storage, analysis and processing. In\nsuch applications however, rapid sensor deployment in unstructured environments\nrepresents a challenge to the overall robustness of the system. We discuss the\nfog and mist computing approaches to hierarchically process data along its path\nfrom source to destination. The several stages of intermediate data processing\nreduce the computational and communication effort in a gradual manner. A\nthree-layer topology for smart data monitoring and processing is thus proposed\nand illustrated to improve the information to noise ratio in a reference\nscenario.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 13:41:27 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Mihai", "Viorel", ""], ["Hanganu", "Cristina Elena", ""], ["Stamatescu", "Grigore", ""], ["Popescu", "Dan", ""]]}, {"id": "1903.09510", "submitter": "Ahmed Eleliemy", "authors": "Ahmed Eleliemy and Florina M. Ciorba", "title": "Hierarchical Dynamic Loop Self-Scheduling on Distributed-Memory Systems\n  Using an MPI+MPI Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computationally-intensive loops are the primary source of parallelism in\nscientific applications. Such loops are often irregular and a balanced\nexecution of their loop iterations is critical for achieving high performance.\nHowever, several factors may lead to an imbalanced load execution, such as\nproblem characteristics, algorithmic, and systemic variations. Dynamic loop\nself-scheduling (DLS) techniques are devised to mitigate these factors, and\nconsequently, improve application performance. On distributed-memory systems,\nDLS techniques can be implemented using a hierarchical master-worker execution\nmodel and are, therefore, called hierarchical DLS techniques. These techniques\nself-schedule loop iterations at two levels of hardware parallelism: across and\nwithin compute nodes. Hybrid programming approaches that combine the message\npassing interface (MPI) with open multi-processing (OpenMP) dominate the\nimplementation of hierarchical DLS techniques. The MPI-3 standard includes the\nfeature of sharing memory regions among MPI processes. This feature introduced\nthe MPI+MPI approach that simplifies the implementation of parallel scientific\napplications. The present work designs and implements hierarchical DLS\ntechniques by exploiting the MPI+MPI approach. Four well-known DLS techniques\nare considered in the evaluation proposed herein. The results indicate certain\nperformance advantages of the proposed approach compared to the hybrid\nMPI+OpenMP approach.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 13:51:32 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Eleliemy", "Ahmed", ""], ["Ciorba", "Florina M.", ""]]}, {"id": "1903.09524", "submitter": "Nicolas Rivera Nicol\\'as Rivera", "authors": "Nan Kang and Nicolas Rivera", "title": "Best-of-Three Voting on Dense Graphs", "comments": "Accepted at ACM SPAA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph $G$ of $n$ vertices, where each vertex is initially attached an\nopinion of either red or blue. We investigate a random process known as the\nBest-of-three voting. In this process, at each time step, every vertex chooses\nthree neighbours at random and adopts the majority colour. We study this\nprocess for a class of graphs with minimum degree $d = n^{\\alpha}$\\,, where\n$\\alpha = \\Omega\\left( (\\log \\log n)^{-1} \\right)$. We prove that if initially\neach vertex is red with probability greater than $1/2+\\delta$, and blue\notherwise, where $\\delta \\geq (\\log d)^{-C}$ for some $C>0$, then with high\nprobability this dynamic reaches a final state where all vertices are red\nwithin $O\\left( \\log \\log n\\right) + O\\left( \\log \\left( \\delta^{-1} \\right)\n\\right)$ steps.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 14:22:31 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Kang", "Nan", ""], ["Rivera", "Nicolas", ""]]}, {"id": "1903.09548", "submitter": "Martin Geier", "authors": "Martin Geier (1), Dominik Faller (1), Marian Br\\\"andle (1) and\n  Samarjit Chakraborty (1) ((1) Technical University of Munich)", "title": "Cost-effective Energy Monitoring of a Zynq-based Real-time System\n  including dual Gigabit Ethernet", "comments": "4 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ongoing integration of fine-grained power management features already\nestablished in CPU-driven Systems-on-Chip (SoCs) enables both traditional Field\nProgrammable Gate Arrays (FPGAs) and, more recently, hybrid Programmable SoCs\n(pSoCs) to reach more energy-sensitive application domains (such as, e.g.,\nautomotive and robotics). By combining a fixed-function multi-core SoC with\nflexible, configurable FPGA fabric, the latter can be used to realize\nheterogeneous Real-time Systems (RTSs) commonly implementing complex\napplication-specific architectures with high computation and communication\n(I/O) densities. Their dynamic changes in workload, currently active power\nsaving features and thus power consumption require precise voltage and current\nsensing on all relevant supply rails to enable dependable evaluation of the\nvarious power management techniques. In this paper, we propose a low-cost\n18-channel 16-bit-resolution measurement (sub-)system capable of 200 kSPS\n(kilo-samples per second) for instrumentation of current pSoC development\nboards. To this end, we join simultaneously sampling analog-to-digital\nconverters (ADCs) and analog voltage/current sensing circuitry with a Cortex M7\nmicrocontroller using an SD card for storage. In addition, we propose to\ninclude crucial I/O components such as Ethernet PHYs into the power monitoring\nto gain a holistic view on the RTS's temporal behavior covering not only\ncomputation on FPGA and CPUs, but also communication in terms of, e.g.,\nreception of sensor values and transmission of actuation signals. We present an\nFMC-sized implementation of our measurement system combined with two Gigabit\nEthernet PHYs and one HDMI input. Paired with Xilinx' ZC702 development board,\nwe are able to synchronously acquire power traces of a Zynq pSoC and the two\nPHYs precise enough to identify individual Ethernet frames.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 15:15:42 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Geier", "Martin", "", "Technical University of Munich"], ["Faller", "Dominik", "", "Technical University of Munich"], ["Br\u00e4ndle", "Marian", "", "Technical University of Munich"], ["Chakraborty", "Samarjit", "", "Technical University of Munich"]]}, {"id": "1903.10036", "submitter": "Christina Peterson", "authors": "Zachary Painter, Christina Peterson, Damian Dechev", "title": "Lock-Free Transactional Adjacency List", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adjacency lists are frequently used in graphing or map based applications.\nAlthough efficient concurrent linked-list algorithms are well known, it can be\ndifficult to adapt these approaches to build a high-performance adjacency list.\nFurthermore, it can often be desirable to execute operations in these data\nstructures transactionally, or perform a sequence of operations in one atomic\nstep. In this paper, we present a lock-free transactional adjacency list based\non a multi-dimensional list (MDList). We are able to combine known linked list\nstrategies with the capability of the MDList in order to efficiently organize\ngraph vertexes and their edges. We design our underlying data structure to be\nnode-based and linearizable, then use the Lock-Free Transactional\nTransformation (LFTT) methodology to efficiently enable transactional\nexecution. In our performance evaluation, our lock-free transactional adjacency\nlist achieves an average of 50% speedup over a transactional boosting\nimplementation.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2019 18:33:58 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Painter", "Zachary", ""], ["Peterson", "Christina", ""], ["Dechev", "Damian", ""]]}, {"id": "1903.10057", "submitter": "Matteo Turilli", "authors": "Matteo Turilli, Vivek Balasubramanian, Andre Merzky, Ioannis\n  Paraskevakos, Shantenu Jha", "title": "Middleware Building Blocks for Workflow Systems", "comments": null, "journal-ref": null, "doi": "10.1109/MCSE.2019.2920048", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a building blocks approach to the design of scientific\nworkflow systems. We discuss RADICAL-Cybertools as one implementation of the\nbuilding blocks concept, showing how they are designed and developed in\naccordance with this approach. This paper offers three main contributions: (i)\nshowing the relevance of the design principles underlying the building blocks\napproach to support scientific workflows on high performance computing\nplatforms; (ii) illustrating a set of building blocks that enable multiple\npoints of integration, \"unifying\" conceptual reasoning across otherwise very\ndifferent tools and systems; and (iii) case studies discussing how\nRADICAL-Cybertools are integrated with existing workflow, workload, and general\npurpose computing systems and used to develop domain-specific workflow systems.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2019 20:46:39 GMT"}, {"version": "v2", "created": "Thu, 27 Jun 2019 17:44:07 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Turilli", "Matteo", ""], ["Balasubramanian", "Vivek", ""], ["Merzky", "Andre", ""], ["Paraskevakos", "Ioannis", ""], ["Jha", "Shantenu", ""]]}, {"id": "1903.10402", "submitter": "Jordi Bataller Mascarell", "authors": "Jordi Bataller Mascarell", "title": "Two Mutual Exclusion Algorithms for Shared Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce two algorithms that solve the mutual exclusion\nproblem for concurrent processes that communicate through shared variables,\n[2]. Our algorithms guarantee that any process trying to enter the critical\nsection, eventually, does enter it. They are formally proven to be correct. The\nfirst algorithm uses a special coordinator process in order to ensure equal\nchances to processes waiting for the critical section. In the second algorithm,\nwith no coordinator, the process exiting the critical section is in charge to\nfairly elect the following one. In the case that no process is waiting, the\nturn is marked free and will be determined by future waiting processes. The\ntype of shared variables used are a turn variable, readable and writable by all\nprocesses; and a flag array, readable by all with flag[i] writable only by\nprocess i. There is a version of the first algorithm where no writable by all\nvariable is used. The bibliography reviewed for this paper is [4] and [3], all\nthe rest is original work.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 15:36:47 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Mascarell", "Jordi Bataller", ""]]}, {"id": "1903.10407", "submitter": "Eddie Hung", "authors": "David Shah, Eddie Hung, Clifford Wolf, Serge Bazanski, Dan Gisselquist\n  and Miodrag Milanovi\\'c", "title": "Yosys+nextpnr: an Open Source Framework from Verilog to Bitstream for\n  Commercial FPGAs", "comments": "4 page short paper to appear at IEEE FCCM 2019 (https://www.fccm.org)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a fully free and open source software (FOSS)\narchitecture-neutral FPGA framework comprising of Yosys for Verilog synthesis,\nand nextpnr for placement, routing, and bitstream generation. Currently, this\nflow supports two commercially available FPGA families, Lattice iCE40 (up to 8K\nlogic elements) and Lattice ECP5 (up to 85K elements) and has been\nhardware-proven for custom-computing machines including a low-power\nneural-network accelerator and an OpenRISC system-on-chip capable of booting\nLinux. Both Yosys and nextpnr have been engineered in a highly flexible manner\nto support many of the features present in modern FPGAs by separating\narchitecture-specific details from the common mapping algorithms. This\nframework is demonstrated on a longest-path case study to find an atypical\nsingle source-sink path occupying up to 45% of all on-chip wiring.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 15:40:21 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Shah", "David", ""], ["Hung", "Eddie", ""], ["Wolf", "Clifford", ""], ["Bazanski", "Serge", ""], ["Gisselquist", "Dan", ""], ["Milanovi\u0107", "Miodrag", ""]]}, {"id": "1903.10584", "submitter": "Zachariah Carmichael", "authors": "Zachariah Carmichael, Hamed F.Langroudi, Char Khazanov, Jeffrey\n  Lillie, John L. Gustafson, Dhireesha Kudithipudi", "title": "Performance-Efficiency Trade-off of Low-Precision Numerical Formats in\n  Deep Neural Networks", "comments": "9 pages, Proceedings of the ACM Conference for Next Generation\n  Arithmetic (CoNGA) 2019", "journal-ref": null, "doi": "10.1145/3316279.3316282", "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have been demonstrated as effective prognostic\nmodels across various domains, e.g. natural language processing, computer\nvision, and genomics. However, modern-day DNNs demand high compute and memory\nstorage for executing any reasonably complex task. To optimize the inference\ntime and alleviate the power consumption of these networks, DNN accelerators\nwith low-precision representations of data and DNN parameters are being\nactively studied. An interesting research question is in how low-precision\nnetworks can be ported to edge-devices with similar performance as\nhigh-precision networks. In this work, we employ the fixed-point, floating\npoint, and posit numerical formats at $\\leq$8-bit precision within a DNN\naccelerator, Deep Positron, with exact multiply-and-accumulate (EMAC) units for\ninference. A unified analysis quantifies the trade-offs between overall network\nefficiency and performance across five classification tasks. Our results\nindicate that posits are a natural fit for DNN inference, outperforming at\n$\\leq$8-bit precision, and can be realized with competitive resource\nrequirements relative to those of floating point.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 20:21:45 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Carmichael", "Zachariah", ""], ["Langroudi", "Hamed F.", ""], ["Khazanov", "Char", ""], ["Lillie", "Jeffrey", ""], ["Gustafson", "John L.", ""], ["Kudithipudi", "Dhireesha", ""]]}, {"id": "1903.10722", "submitter": "Didier El Baz", "authors": "Jia Luo (LAAS-CDA), Didier El Baz", "title": "A Dual Heterogeneous Island Genetic Algorithm for Solving Large Size\n  Flexible Flow Shop Scheduling Problems on Hybrid multi-core CPU and GPU\n  Platforms", "comments": null, "journal-ref": "Mathematical Problems in Engineering, 2019", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The flexible flow shop scheduling problem is an NP-hard problem and it\nrequires significant resolution time to find optimal or even adequate solutions\nwhen dealing with large size instances. Thus, this paper proposes a dual island\ngenetic algorithm consisting of a parallel cellular model and a parallel pseudo\nmodel. This is a two-level parallelization highly consistent with the\nunderlying architecture and is well suited for parallelizing inside or between\nGPUs and a multi-core CPU. At the higher level, the efficiency of island GAs is\nimproved by exploring new regions within the search space utilizing different\nmethods. In the meantime, the cellular model keeps the population diversity by\ndecentralization and the pseudo model enhances the search ability by the\ncomplementary parent strategy at the lower level. To encourage the information\nsharing between islands, a penetration inspired migration policy is designed\nwhich sets the topology, the rate, the interval and the strategy adaptively.\nFinally, the proposed method is tested on some large size flexible flow shop\nscheduling instances in comparison with other parallel algorithms. The\ncomputational results show that it cannot only obtain competitive results but\nalso reduces execution time.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 08:08:20 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Luo", "Jia", "", "LAAS-CDA"], ["Baz", "Didier El", ""]]}, {"id": "1903.10741", "submitter": "Didier El Baz", "authors": "Jia Luo (LAAS-CDA), Shigeru Fujimura, Didier El Baz (LAAS-CDA)", "title": "GPU based parallel genetic algorithm for solving an energy efficient\n  dynamic flexible flow shop scheduling problem", "comments": "Journal of Parallel and Distributed Computing, Elsevier, 2018", "journal-ref": null, "doi": "10.1016/j.jpdc.2018.07.022", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to new government legislation, customers' environmental concerns and\ncontinuously rising cost of energy, energy efficiency is becoming an essential\nparameter of industrial manufacturing processes in recent years. Most efforts\nconsidering energy issues in scheduling problems have focused on static\nscheduling. But in fact, scheduling problems are dynamic in the real world with\nuncertain new arrival jobs after the execution time. This paper proposes a\ndynamic energy efficient flexible flow shop scheduling model using peak power\nvalue with the consideration of new arrival jobs. As the problem is strongly\nNP-hard, a priority based hybrid parallel Genetic Algorithm with a predictive\nreactive complete rescheduling approach is developed. In order to achieve a\nspeedup to meet the short response in the dynamic environment, the proposed\nmethod is designed to be highly consistent with NVIDIA CUDA software model.\nFinally, numerical experiments are conducted and show that our approach can not\nonly achieve better performance than the traditional static approach, but also\ngain competitive results by reducing the time requirements dramatically.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 09:00:57 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Luo", "Jia", "", "LAAS-CDA"], ["Fujimura", "Shigeru", "", "LAAS-CDA"], ["Baz", "Didier El", "", "LAAS-CDA"]]}, {"id": "1903.10956", "submitter": "Kun Yuan", "authors": "Kun Yuan, Sulaiman A. Alghunaim, Bicheng Ying, Ali H. Sayed", "title": "On the Influence of Bias-Correction on Distributed Stochastic\n  Optimization", "comments": "17 pages, 9 figure, submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various bias-correction methods such as EXTRA, gradient tracking methods, and\nexact diffusion have been proposed recently to solve distributed {\\em\ndeterministic} optimization problems. These methods employ constant step-sizes\nand converge linearly to the {\\em exact} solution under proper conditions.\nHowever, their performance under stochastic and adaptive settings is less\nexplored. It is still unknown {\\em whether}, {\\em when} and {\\em why} these\nbias-correction methods can outperform their traditional counterparts (such as\nconsensus and diffusion) with noisy gradient and constant step-sizes.\n  This work studies the performance of exact diffusion under the stochastic and\nadaptive setting, and provides conditions under which exact diffusion has\nsuperior steady-state mean-square deviation (MSD) performance than traditional\nalgorithms without bias-correction. In particular, it is proven that this\nsuperiority is more evident over sparsely-connected network topologies such as\nlines, cycles, or grids. Conditions are also provided under which exact\ndiffusion method match or may even degrade the performance of traditional\nmethods. Simulations are provided to validate the theoretical findings.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 15:28:36 GMT"}, {"version": "v2", "created": "Thu, 11 Jul 2019 06:54:02 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Yuan", "Kun", ""], ["Alghunaim", "Sulaiman A.", ""], ["Ying", "Bicheng", ""], ["Sayed", "Ali H.", ""]]}, {"id": "1903.11280", "submitter": "Timm Faulwasser", "authors": "Alexander Engelmann, Yuning Jiang, Boris Houska, Timm Faulwasser", "title": "Decomposition of non-convex optimization via bi-level distributed ALADIN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decentralized optimization algorithms are important in different contexts,\nsuch as distributed optimal power flow or distributed model predictive control,\nas they avoid central coordination and enable decomposition of large-scale\nproblems. In case of constrained non-convex optimization only a few algorithms\nare currently are available; often their performance is limited, or they lack\nconvergence guarantees. This paper proposes a framework for decentralized\nnon-convex optimization via bi-level distribution of the Augmented Lagrangian\nAlternating Direction Inexact Newton (ALADIN) algorithm. Bi-level distribution\nmeans that the outer ALADIN structure is combined with an inner\ndistribution/decentralization level solving a condensed variant of ALADIN's\nconvex coordination QP by decentralized algorithms. We prove sufficient\nconditions ensuring local convergence while allowing for inexact\ndecentralized/distributed solutions of the coordination QP. Moreover, we show\nhow a decentralized variant of conjugate gradient or decentralized ADMM schemes\ncan be employed at the inner level. We draw upon case studies from power\nsystems and robotics to illustrate the performance of the proposed framework.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 07:48:57 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Engelmann", "Alexander", ""], ["Jiang", "Yuning", ""], ["Houska", "Boris", ""], ["Faulwasser", "Timm", ""]]}, {"id": "1903.11314", "submitter": "Ruben Mayer", "authors": "Ruben Mayer and Hans-Arno Jacobsen", "title": "Scalable Deep Learning on Distributed Infrastructures: Challenges,\n  Techniques and Tools", "comments": "accepted at ACM Computing Surveys, to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning (DL) has had an immense success in the recent past, leading to\nstate-of-the-art results in various domains such as image recognition and\nnatural language processing. One of the reasons for this success is the\nincreasing size of DL models and the proliferation of vast amounts of training\ndata being available. To keep on improving the performance of DL, increasing\nthe scalability of DL systems is necessary. In this survey, we perform a broad\nand thorough investigation on challenges, techniques and tools for scalable DL\non distributed infrastructures. This incorporates infrastructures for DL,\nmethods for parallel DL training, multi-tenant resource scheduling and the\nmanagement of training and model data. Further, we analyze and compare 11\ncurrent open-source DL frameworks and tools and investigate which of the\ntechniques are commonly implemented in practice. Finally, we highlight future\nresearch trends in DL systems that deserve further research.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 09:46:52 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 08:51:24 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Mayer", "Ruben", ""], ["Jacobsen", "Hans-Arno", ""]]}, {"id": "1903.11409", "submitter": "Yusuke Nagasaka", "authors": "Yusuke Nagasaka, Akira Nukada, Ryosuke Kojima, Satoshi Matsuoka", "title": "Batched Sparse Matrix Multiplication for Accelerating Graph\n  Convolutional Networks", "comments": "10 pages, 19th IEEE/ACM International Symposium on Cluster, Cloud and\n  Grid Computing (CCGRID)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Convolutional Networks (GCNs) are recently getting much attention in\nbioinformatics and chemoinformatics as a state-of-the-art machine learning\napproach with high accuracy. GCNs process convolutional operations along with\ngraph structures, and GPUs are used to process enormous operations including\nsparse-dense matrix multiplication (SpMM) when the graph structure is expressed\nas an adjacency matrix with sparse matrix format. However, the SpMM operation\non small graph, where the number of nodes is tens or hundreds, hardly exploits\nhigh parallelism or compute power of GPU. Therefore, SpMM becomes a bottleneck\nof training and inference in GCNs applications. In order to improve the\nperformance of GCNs applications, we propose new SpMM algorithm especially for\nsmall sparse matrix and Batched SpMM, which exploits high parallelism of GPU by\nprocessing multiple SpMM operations with single CUDA kernel. To the best of our\nknowledge, this is the first work of batched approach for SpMM. We evaluated\nthe performance of the GCNs application on TSUBAME3.0 implementing NVIDIA Tesla\nP100 GPU, and our batched approach shows significant speedups of up to 1.59x\nand 1.37x in training and inference, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 13:18:54 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Nagasaka", "Yusuke", ""], ["Nukada", "Akira", ""], ["Kojima", "Ryosuke", ""], ["Matsuoka", "Satoshi", ""]]}, {"id": "1903.11434", "submitter": "Jan Hackfeld", "authors": "Jan Hackfeld", "title": "A lightweight BFT consensus protocol for blockchains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We present a general consensus framework that allows to easily introduce a\ncustomizable Byzantine fault tolerant consensus algorithm to an existing\n(Delegated) Proof-of-Stake blockchain. We prove the safety of the protocol\nunder the assumption that less than 1/3 of the validators are Byzantine. The\nframework further allows for consensus participants to choose subjective\ndecision thresholds in order to obtain safety even in the case of a larger\nproportion of Byzantine validators. Moreover, the liveness of the protocol is\nshown if less than 1/3 of the validators crash.\n  Based on the framework, we introduce Lisk-BFT, a Byzantine fault tolerant\nconsensus algorithm for the Lisk ecosystem. Lisk-BFT integrates with the\nexisting block proposal mechanism, requires only two additional integers in\nblocks and no additional messages. The protocol is simple and provides safety\nin the case of static validators if less than 1/3 of the validators are\nByzantine. For the case of dynamically changing validators, we prove the safety\nof the protocol assuming a bound on the number of Byzantine validators and the\nnumber of honest validators that can change at one time. We further show the\nliveness of the Lisk-BFT protocol for less than 1/3 crashing validators.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 14:03:15 GMT"}, {"version": "v2", "created": "Fri, 14 Jun 2019 14:17:11 GMT"}, {"version": "v3", "created": "Tue, 4 May 2021 22:13:51 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Hackfeld", "Jan", ""]]}, {"id": "1903.11441", "submitter": "Boris Bauermeister", "authors": "Daniel Ahlin, Boris Bauermeister, Jan Conrad, Robert Gardner, Luca\n  Grandi, Benedikt Riedel, Evan Shockley, Judith Stephen, Ragnar Sundblad,\n  Suchandra Thapa and Christopher Tunnell", "title": "The XENON1T Data Distribution and Processing Scheme", "comments": "8 pages, 2 figures, CHEP 2018 proceedings", "journal-ref": "EPJ Web of Conferences 214, 03015 (2019)", "doi": "10.1051/epjconf/201921403015", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The XENON experiment is looking for non-baryonic particle dark matter in the\nuniverse. The setup is a dual phase time projection chamber (TPC) filled with\n3200 kg of ultra-pure liquid xenon. The setup is operated at the Laboratori\nNazionali del Gran Sasso (LNGS) in Italy. We present a full overview of the\ncomputing scheme for data distribution and job management in XENON1T. The\nsoftware package Rucio, which is developed by the ATLAS collaboration,\nfacilitates data handling on Open Science Grid (OSG) and European Grid\nInfrastructure (EGI) storage systems. A tape copy at the Center for High\nPerformance Computing (PDC) is managed by the Tivoli Storage Manager (TSM).\nData reduction and Monte Carlo production are handled by CI Connect which is\nintegrated into the OSG network. The job submission system connects resources\nat the EGI, OSG, SDSC's Comet, and the campus HPC resources for distributed\ncomputing. The previous success in the XENON1T computing scheme is also the\nstarting point for its successor experiment XENONnT, which starts to take data\nin autumn 2019.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 14:19:12 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Ahlin", "Daniel", ""], ["Bauermeister", "Boris", ""], ["Conrad", "Jan", ""], ["Gardner", "Robert", ""], ["Grandi", "Luca", ""], ["Riedel", "Benedikt", ""], ["Shockley", "Evan", ""], ["Stephen", "Judith", ""], ["Sundblad", "Ragnar", ""], ["Thapa", "Suchandra", ""], ["Tunnell", "Christopher", ""]]}, {"id": "1903.11521", "submitter": "Carsten Uphoff", "authors": "Carsten Uphoff and Michael Bader", "title": "Yet Another Tensor Toolbox for discontinuous Galerkin methods and other\n  applications", "comments": "Submitted to ACM TOMS", "journal-ref": null, "doi": "10.1145/3406835", "report-no": null, "categories": "cs.MS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The numerical solution of partial differential equations is at the heart of\nmany grand challenges in supercomputing. Solvers based on high-order\ndiscontinuous Galerkin (DG) discretisation have been shown to scale on large\nsupercomputers with excellent performance and efficiency, if the implementation\nexploits all levels of parallelism and is tailored to the specific\narchitecture. However, every year new supercomputers emerge and the list of\nhardware-specific considerations grows, simultaneously with the list of desired\nfeatures in a DG code. Thus we believe that a sustainable DG code needs an\nabstraction layer to implement the numerical scheme in a suitable language. We\nexplore the possibility to abstract the numerical scheme as small tensor\noperations, describe them in a domain-specific language (DSL) resembling the\nEinstein notation, and to map them to existing code generators which generate\nsmall matrix matrix multiplication routines. The compiler for our DSL\nimplements classic optimisations that are used for large tensor contractions,\nand we present novel optimisation techniques such as equivalent sparsity\npatterns and optimal index permutations for temporary tensors. Our application\nexamples, which include the earthquake simulation software SeisSol, show that\nthe generated kernels achieve over 50 % peak performance while the DSL\nconsiderably simplifies the implementation.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 16:17:01 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Uphoff", "Carsten", ""], ["Bader", "Michael", ""]]}, {"id": "1903.11596", "submitter": "Johanne Cohen", "authors": "Johanne Cohen, Daniel Cordeiro, and Loubna Echabbi", "title": "Detecting service provider alliances", "comments": "arXiv admin note: text overlap with arXiv:1606.07111", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm for detecting service provider alliances. To perform\nthis, we modelize a cooperative game-theoretic model for competitor service\nproviders. A choreography (a peer-to-peer service composition model) needs a\nset of services to fulfill its requirements. Users must choose, for each\nrequirement, which service providers will be used to enact the choreography at\nlowest cost. Due to the lack of centralization, vendors can form alliances to\ncontrol the market. We propose a novel algorithm capable of detecting alliances\namong service providers, based on our findings showing that this game has an\nempty core, but a non-empty bargaining set.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 09:48:07 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Cohen", "Johanne", ""], ["Cordeiro", "Daniel", ""], ["Echabbi", "Loubna", ""]]}, {"id": "1903.11677", "submitter": "Muhammad Samir Khan", "authors": "Muhammad Samir Khan, Syed Shalan Naqvi, Nitin H. Vaidya", "title": "Exact Byzantine Consensus on Undirected Graphs under Local Broadcast\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the Byzantine consensus problem for nodes with binary\ninputs. The nodes are interconnected by a network represented as an undirected\ngraph, and the system is assumed to be synchronous. Under the classical\npoint-to-point communication model, it is well-known [7] that the following two\nconditions are both necessary and sufficient to achieve Byzantine consensus\namong $n$ nodes in the presence of up to $f$ Byzantine faulty nodes: $n \\ge\n3f+1$ and vertex connectivity at least $2f+1$. In the classical point-to-point\ncommunication model, it is possible for a faulty node to equivocate, i.e.,\ntransmit conflicting information to different neighbors. Such equivocation is\npossible because messages sent by a node to one of its neighbors are not\noverheard by other neighbors.\n  This paper considers the local broadcast model. In contrast to the\npoint-to-point communication model, in the local broadcast model, messages sent\nby a node are received identically by all of its neighbors. Thus, under the\nlocal broadcast model, attempts by a node to send conflicting information can\nbe detected by its neighbors. Under this model, we show that the following two\nconditions are both necessary and sufficient for Byzantine consensus: vertex\nconnectivity at least $\\lfloor 3f/2 \\rfloor + 1$ and minimum node degree at\nleast $2f$. Observe that the local broadcast model results in a lower\nrequirement for connectivity and the number of nodes $n$, as compared to the\npoint-to-point communication model.\n  We extend the above results to a hybrid model that allows some of the\nByzantine faulty nodes to equivocate. The hybrid model bridges the gap between\nthe point-to-point and local broadcast models, and helps to precisely\ncharacterize the trade-off between equivocation and network requirements.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 19:52:25 GMT"}, {"version": "v2", "created": "Mon, 27 May 2019 20:44:24 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Khan", "Muhammad Samir", ""], ["Naqvi", "Syed Shalan", ""], ["Vaidya", "Nitin H.", ""]]}, {"id": "1903.11694", "submitter": "Joshua Davis", "authors": "Joshua Hoke Davis, Tao Gao, Sunita Chandresekaran, Michela Taufer", "title": "Studying the Impact of Power Capping on MapReduce-based, Data-intensive\n  Mini-applications on Intel KNL and KNM Architectures", "comments": "Extended abstract submitted for the ACM Student Research Competition\n  at SC18. 2nd place undergraduate poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this poster, we quantitatively measure the impacts of data movement on\nperformance in MapReduce-based applications when executed on HPC systems. We\nleverage the PAPI 'powercap' component to identify ideal conditions for\nexecution of our applications in terms of (1) dataset characteristics (i.e.,\nunique words); (2) HPC system (i.e., KNL and KNM); and (3) implementation of\nthe MapReduce programming model (i.e., with or without combiner optimizations).\nResults confirm the high energy and runtime costs of data movement, and the\nbenefits of the combiner optimization on these costs.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 19:40:16 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Davis", "Joshua Hoke", ""], ["Gao", "Tao", ""], ["Chandresekaran", "Sunita", ""], ["Taufer", "Michela", ""]]}, {"id": "1903.11714", "submitter": "Yi-Fan Chen", "authors": "Kun Yang, Yi-Fan Chen, Georgios Roumpos, Chris Colby, John Anderson", "title": "High Performance Monte Carlo Simulation of Ising Model on TPU Clusters", "comments": "26 pages, 8 figures, 7 tables. Results (excluding new results in 7.2)\n  published in ACM SC2019 Proceedings: ACM ISBN 978-1-4503-6229-0/19/11.\n  https://doi.org/10.1145/3295500.3356149", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale deep learning benefits from an emerging class of AI accelerators.\nSome of these accelerators' designs are general enough for compute-intensive\napplications beyond AI and Cloud TPU is one such example. In this paper, we\ndemonstrate a novel approach using TensorFlow on Cloud TPU to simulate the\ntwo-dimensional Ising Model. TensorFlow and Cloud TPU framework enable the\nsimple and readable code to express the complicated distributed algorithm\nwithout compromising the performance. Our code implementation fits into a small\nJupyter Notebook and fully utilizes Cloud TPU's efficient matrix operation and\ndedicated high speed inter-chip connection. The performance is highly\ncompetitive: it outperforms the best published benchmarks to our knowledge by\n60% in single-core and 250% in multi-core with good linear scaling. When\ncompared to Tesla V100 GPU, the single-core performance maintains a ~10% gain.\nWe also demonstrate that using low precision arithmetic---bfloat16---does not\ncompromise the correctness of the simulation results.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 21:52:54 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 01:52:06 GMT"}, {"version": "v3", "created": "Wed, 28 Aug 2019 07:02:07 GMT"}, {"version": "v4", "created": "Sun, 17 Nov 2019 16:18:16 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Yang", "Kun", ""], ["Chen", "Yi-Fan", ""], ["Roumpos", "Georgios", ""], ["Colby", "Chris", ""], ["Anderson", "John", ""]]}, {"id": "1903.11749", "submitter": "Wenqing Lin", "authors": "Wenqing Lin", "title": "Distributed Algorithms for Fully Personalized PageRank on Large Graphs", "comments": "Full Research Paper accepted in the proceedings of the 30th World\n  Wide Web Conference (WWW 2019)", "journal-ref": null, "doi": "10.1145/3308558.3313555", "report-no": null, "categories": "cs.SI cs.DB cs.DC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Personalized PageRank (PPR) has enormous applications, such as link\nprediction and recommendation systems for social networks, which often require\nthe fully PPR to be known. Besides, most of real-life graphs are edge-weighted,\ne.g., the interaction between users on the Facebook network. However, it is\ncomputationally difficult to compute the fully PPR, especially on large graphs,\nnot to mention that most existing approaches do not consider the weights of\nedges. In particular, the existing approach cannot handle graphs with billion\nedges on a moderate-size cluster. To address this problem, this paper presents\na novel study on the computation of fully edge-weighted PPR on large graphs\nusing the distributed computing framework. Specifically, we employ the Monte\nCarlo approximation that performs a large number of random walks from each node\nof the graph, and exploits the parallel pipeline framework to reduce the\noverall running time of the fully PPR. Based on that, we develop several\noptimization techniques which (i) alleviate the issue of large nodes that could\nexplode the memory space, (ii) pre-compute short walks for small nodes that\nlargely speedup the computation of random walks, and (iii) optimize the amount\nof random walks to compute in each pipeline that significantly reduces the\noverhead. With extensive experiments on a variety of real-life graph datasets,\nwe demonstrate that our solution is several orders of magnitude faster than the\nstate-of-the-arts, and meanwhile, largely outperforms the baseline algorithms\nin terms of accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 01:14:37 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Lin", "Wenqing", ""]]}, {"id": "1903.11874", "submitter": "Yushan Gao", "authors": "Yushan Gao, Ander Biguri, Thomas Blumensath", "title": "Block stochastic gradient descent for large-scale tomographic\n  reconstruction in a parallel network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iterative algorithms have many advantages for linear tomographic image\nreconstruction when compared to back-projection based methods. However,\niterative methods tend to have significantly higher computational complexity.\nTo overcome this, parallel processing schemes that can utilise several\ncomputing nodes are desirable. Popular methods here are row action methods,\nwhich update the entire image simultaneously and column action methods, which\nrequire access to all measurements at each node. In large scale tomographic\nreconstruction with limited storage capacity of each node, data communication\noverheads between nodes becomes a significant performance limiting factor. To\nreduce this overhead, we proposed a row action method BSGD. The method is based\non the stochastic gradient descent method but it does not update the entire\nimage at each iteration, which reduces between node communication. To further\nincrease convergence speeds, an importance sampling strategy is proposed. We\ncompare BSGD to other existing stochastic methods and show its effectiveness\nand efficiency. Other properties of BSGD are also explored, including its\nability to incorporate total variation (TV) regularization and automatic\nparameter tuning.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 10:10:27 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Gao", "Yushan", ""], ["Biguri", "Ander", ""], ["Blumensath", "Thomas", ""]]}, {"id": "1903.12065", "submitter": "Srikanta Tirthapura", "authors": "Srikanta Tirthapura and David P. Woodruff", "title": "Optimal Random Sampling from Distributed Streams Revisited", "comments": "This writeup is a revised version of a paper with the same title and\n  authors, which appeared in the Proceedings of the International Conference on\n  Distributed Computing (DISC) 2011", "journal-ref": "DISC 2011: 283-297", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an improved algorithm for drawing a random sample from a large data\nstream when the input elements are distributed across multiple sites which\ncommunicate via a central coordinator. At any point in time the set of elements\nheld by the coordinator represent a uniform random sample from the set of all\nthe elements observed so far. When compared with prior work, our algorithms\nasymptotically improve the total number of messages sent in the system as well\nas the computation required of the coordinator. We also present a matching\nlower bound, showing that our protocol sends the optimal number of messages up\nto a constant factor with large probability. As a byproduct, we obtain an\nimproved algorithm for finding the heavy hitters across multiple distributed\nsites.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 15:51:22 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Tirthapura", "Srikanta", ""], ["Woodruff", "David P.", ""]]}, {"id": "1903.12085", "submitter": "Jesper Larsson Tr\\\"aff", "authors": "Michael Kainer, Jesper Larsson Tr\\\"aff", "title": "More Parallelism in Dijkstra's Single-Source Shortest Path Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dijkstra's algorithm for the Single-Source Shortest Path (SSSP) problem is\nnotoriously hard to parallelize in $o(n)$ depth, $n$ being the number of\nvertices in the input graph, without increasing the required parallel work\nunreasonably. Crauser et al.\\ (1998) presented observations that allow to\nidentify more than a single vertex at a time as correct and correspondingly\nmore edges to be relaxed simultaneously. Their algorithm runs in parallel\nphases, and for certain random graphs they showed that the number of phases is\n$O(n^{1/3})$ with high probability. A work-efficient CRCW PRAM with this depth\nwas given, but no implementation on a real, parallel system.\n  In this paper we strengthen the criteria of Crauser et al., and discuss\ntradeoffs between work and number of phases in their implementation. We present\nsimulation results with a range of common input graphs for the depth that an\nideal parallel algorithm that can apply the criteria at no cost and parallelize\nrelaxations without conflicts can achieve. These results show that the number\nof phases is indeed a small root of $n$, but still off from the shortest path\nlength lower bound that can also be computed.\n  We give a shared-memory parallel implementation of the most work-efficient\nversion of a Dijkstra's algorithm running in parallel phases, which we compare\nto an own implementation of the well-known $\\Delta$-stepping algorithm. We can\nshow that the work-efficient SSSP algorithm applying the criteria of Crauser et\nal. is competitive to and often better than $\\Delta$-stepping on our chosen\ninput graphs. Despite not providing an $o(n)$ guarantee on the number of\nrequired phases, criteria allowing concurrent relaxation of many correct\nvertices may be a viable approach to practically fast, parallel SSSP\nimplementations.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 16:06:55 GMT"}, {"version": "v2", "created": "Wed, 8 May 2019 11:12:09 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Kainer", "Michael", ""], ["Tr\u00e4ff", "Jesper Larsson", ""]]}, {"id": "1903.12204", "submitter": "Damien Imbs", "authors": "Emmanuel Godard, Damien Imbs, Michel Raynal, Gadi Taubenfeld", "title": "Mutex-based Desanonymization of an Anonymous Read/Write Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anonymous shared memory is a memory in which processes use different names\nfor the same shared read/write register. As an example, a shared register named\n$A$ by a process $p$ and a shared register named $B$ by another process $q$ can\ncorrespond to the very same register $X$, and similarly for the names $B$ at\n$p$ and $A$ at $q$ which can correspond to the same register $Y\\neq X$. Hence,\nthere is a permanent disagreement on the register names among the processes.\nThis new notion of anonymity was recently introduced by G. Taubenfeld (PODC\n2017), who presented several memory-anonymous algorithms and impossibility\nresults.\n  This paper introduces a new problem (new to our knowledge), that consists in\n\"desanonymizing\" an anonymous shared memory. To this end, it presents an\nalgorithm that, starting with a shared memory made up of $m$ anonymous\nread/write atomic registers (i.e., there is no a priori agreement on their\nnames), allows each process to compute a local addressing mapping, such that\nall the processes agree on the names of each register. The proposed\nconstruction is based on an underlying deadlock-free mutex algorithm for $n\\geq\n2$ processes (recently proposed in a paper co-authored by some of the authors\nof this paper), and consequently inherits its necessary and sufficient\ncondition on the size $m$ of the anonymous memory, namely $m$ must belongs to\nthe set $M(n)=\\{m:~$ such that $\\forall~ \\ell: 1<\\ell \\leq n:~\n\\gcd(\\ell,m)=1\\}\\setminus \\{1\\}$. This algorithm, which is also symmetric in\nthe sense process identities can only be compared by equality, requires the\nparticipation of all the processes; hence it can be part of the system\ninitialization. Last but not least, the proposed algorithm has a first-class\nnoteworthy property, namely, its simplicity.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 18:15:37 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Godard", "Emmanuel", ""], ["Imbs", "Damien", ""], ["Raynal", "Michel", ""], ["Taubenfeld", "Gadi", ""]]}, {"id": "1903.12221", "submitter": "Alex Glikson", "authors": "Ping-Min Lin, Alex Glikson", "title": "Mitigating Cold Starts in Serverless Platforms: A Pool-Based Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid adoption of the serverless (or Function-as-a-Service, FaaS) paradigm,\npioneered by Amazon with AWS Lambda and followed by numerous commercial\nofferings and open source projects, introduces new challenges in designing the\ncloud infrastructure, balancing between performance and cost. While instant\nper-request elasticity that FaaS platforms typically offer application\ndevelopers makes it possible to achieve high performance of bursty workloads\nwithout over-provisioning, such elasticity often involves extra latency\nassociated with on-demand provisioning of individual runtime containers that\nserve the functions. This phenomenon is often called cold starts, as opposed to\nthe situation when a function is served by a pre-provisioned \"warm\" container,\nready to serve requests with close to zero overhead. Providers are constantly\nworking on techniques aimed at reducing cold starts. A common approach to\nreduce cold starts is to maintain a pool of warm containers, in anticipation of\nfuture requests. In this report, we address the cold start problem in\nserverless architectures, specifically under the Knative Serving FaaS platform.\nWe describe our implementation leveraging a pool of function instances, and\nevaluate the latency compared to the original implementation, resulting in a\n85% reduction of P99 response time for a single instance pool.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 18:55:30 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Lin", "Ping-Min", ""], ["Glikson", "Alex", ""]]}, {"id": "1903.12226", "submitter": "Daniel Bittman", "authors": "Daniel Bittman, Ethan L. Miller, Peter Alvaro", "title": "Co-evolving Tracing and Fault Injection with Box of Pain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed systems are hard to reason about largely because of uncertainty\nabout what may go wrong in a particular execution, and about whether the system\nwill mitigate those faults. Tools that perturb executions can help test whether\na system is robust to faults, while tools that observe executions can help\nbetter understand their system-wide effects. We present Box of Pain, a tracer\nand fault injector for unmodified distributed systems that addresses both\nconcerns by interposing at the system call level and dynamically reconstructing\nthe partial order of communication events based on causal relationships. Box of\nPain's lightweight approach to tracing and focus on simulating the effects of\npartial failures on communication rather than the failures themselves sets it\napart from other tracing and fault injection systems. We present evidence of\nthe promise of Box of Pain and its approach to lightweight observation and\nperturbation of distributed systems.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 19:05:25 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Bittman", "Daniel", ""], ["Miller", "Ethan L.", ""], ["Alvaro", "Peter", ""]]}, {"id": "1903.12287", "submitter": "Adam Lerer", "authors": "Adam Lerer, Ledell Wu, Jiajun Shen, Timothee Lacroix, Luca Wehrstedt,\n  Abhijit Bose, Alex Peysakhovich", "title": "PyTorch-BigGraph: A Large-scale Graph Embedding System", "comments": null, "journal-ref": "Proceedings of The Conference on Systems and Machine Learning,\n  2019", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph embedding methods produce unsupervised node features from graphs that\ncan then be used for a variety of machine learning tasks. Modern graphs,\nparticularly in industrial applications, contain billions of nodes and\ntrillions of edges, which exceeds the capability of existing embedding systems.\nWe present PyTorch-BigGraph (PBG), an embedding system that incorporates\nseveral modifications to traditional multi-relation embedding systems that\nallow it to scale to graphs with billions of nodes and trillions of edges. PBG\nuses graph partitioning to train arbitrarily large embeddings on either a\nsingle machine or in a distributed environment. We demonstrate comparable\nperformance with existing embedding systems on common benchmarks, while\nallowing for scaling to arbitrarily large graphs and parallelization on\nmultiple machines. We train and evaluate embeddings on several large social\nnetwork graphs as well as the full Freebase dataset, which contains over 100\nmillion nodes and 2 billion edges.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 21:51:09 GMT"}, {"version": "v2", "created": "Mon, 1 Apr 2019 16:48:00 GMT"}, {"version": "v3", "created": "Tue, 9 Apr 2019 15:41:25 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Lerer", "Adam", ""], ["Wu", "Ledell", ""], ["Shen", "Jiajun", ""], ["Lacroix", "Timothee", ""], ["Wehrstedt", "Luca", ""], ["Bose", "Abhijit", ""], ["Peysakhovich", "Alex", ""]]}, {"id": "1903.12359", "submitter": "Gary Pui-Tung Choi", "authors": "Gary P. T. Choi, Yusan Leung-Liu, Xianfeng Gu, Lok Ming Lui", "title": "Parallelizable global conformal parameterization of simply-connected\n  surfaces via partial welding", "comments": null, "journal-ref": "SIAM Journal on Imaging Sciences 13(3), 1049-1083 (2020)", "doi": "10.1137/19M125337X", "report-no": null, "categories": "cs.CG cs.DC cs.GR math.CV math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conformal surface parameterization is useful in graphics, imaging and\nvisualization, with applications to texture mapping, atlas construction,\nregistration, remeshing and so on. With the increasing capability in scanning\nand storing data, dense 3D surface meshes are common nowadays. While meshes\nwith higher resolution better resemble smooth surfaces, they pose computational\ndifficulties for the existing parameterization algorithms. In this work, we\npropose a novel parallelizable algorithm for computing the global conformal\nparameterization of simply-connected surfaces via partial welding maps. A given\nsimply-connected surface is first partitioned into smaller subdomains. The\nlocal conformal parameterizations of all subdomains are then computed in\nparallel. The boundaries of the parameterized subdomains are subsequently\nintegrated consistently using a novel technique called partial welding, which\nis developed based on conformal welding theory. Finally, by solving the Laplace\nequation for each subdomain using the updated boundary conditions, we obtain a\nglobal conformal parameterization of the given surface, with bijectivity\nguaranteed by quasi-conformal theory. By including additional shape\nconstraints, our method can be easily extended to achieve disk conformal\nparameterization for simply-connected open surfaces and spherical conformal\nparameterization for genus-0 closed surfaces. Experimental results are\npresented to demonstrate the effectiveness of our proposed algorithm. When\ncompared to the state-of-the-art conformal parameterization methods, our method\nachieves a significant improvement in both computational time and accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 05:43:33 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 05:56:11 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Choi", "Gary P. T.", ""], ["Leung-Liu", "Yusan", ""], ["Gu", "Xianfeng", ""], ["Lui", "Lok Ming", ""]]}]