[{"id": "1910.00178", "submitter": "Wenlei Bao", "authors": "Wenlei Bao, Li-Wen Chang, Yang Chen, Ke Deng, Amit Agarwal, Emad\n  Barsoum, Abe Taha", "title": "NGEMM: Optimizing GEMM for Deep Learning via Compiler-based Techniques", "comments": "Comments: performance figure updated, experiments added, formula\n  corrected, references added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantization has emerged to be an effective way to significantly boost the\nperformance of deep neural networks (DNNs) by utilizing low-bit computations.\nDespite having lower numerical precision, quantized DNNs are able to reduce\nboth memory bandwidth and computation cycles with little losses of accuracy.\nInteger GEMM (General Matrix Multiplication) is critical to running quantized\nDNN models efficiently, as GEMM operations often dominate the computations in\nthese models. Various approaches have been developed by leveraging techniques\nsuch as vectorization and memory layout to improve the performance of integer\nGEMM. However, these existing approaches are not fast enough in certain\nscenarios. We developed NGEMM, a compiler-based GEMM implementation for\naccelerating lower-precision training and inference. NGEMM has better use of\nthe vector units by avoiding unnecessary vector computation that is introduced\nduring tree reduction. We compared NGEMM's performance with the state-of-art\nBLAS libraries such as MKL. Our experimental results showed that NGEMM\noutperformed MKL non-pack and pack version by an average of 1.86x and 1.16x,\nrespectively. We have applied NGEMM to a number of production services in\nMicrosoft.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 02:27:17 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2019 23:12:33 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Bao", "Wenlei", ""], ["Chang", "Li-Wen", ""], ["Chen", "Yang", ""], ["Deng", "Ke", ""], ["Agarwal", "Amit", ""], ["Barsoum", "Emad", ""], ["Taha", "Abe", ""]]}, {"id": "1910.00643", "submitter": "Jianyu Wang", "authors": "Jianyu Wang, Vinayak Tantia, Nicolas Ballas, Michael Rabbat", "title": "SlowMo: Improving Communication-Efficient Distributed SGD with Slow\n  Momentum", "comments": "Accepted to ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed optimization is essential for training large models on large\ndatasets. Multiple approaches have been proposed to reduce the communication\noverhead in distributed training, such as synchronizing only after performing\nmultiple local SGD steps, and decentralized methods (e.g., using gossip\nalgorithms) to decouple communications among workers. Although these methods\nrun faster than AllReduce-based methods, which use blocking communication\nbefore every update, the resulting models may be less accurate after the same\nnumber of updates. Inspired by the BMUF method of Chen & Huo (2016), we propose\na slow momentum (SlowMo) framework, where workers periodically synchronize and\nperform a momentum update, after multiple iterations of a base optimization\nalgorithm. Experiments on image classification and machine translation tasks\ndemonstrate that SlowMo consistently yields improvements in optimization and\ngeneralization performance relative to the base optimizer, even when the\nadditional overhead is amortized over many updates so that the SlowMo runtime\nis on par with that of the base optimizer. We provide theoretical convergence\nguarantees showing that SlowMo converges to a stationary point of smooth\nnon-convex losses. Since BMUF can be expressed through the SlowMo framework,\nour results also correspond to the first theoretical convergence guarantees for\nBMUF.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 20:06:48 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 20:00:02 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Wang", "Jianyu", ""], ["Tantia", "Vinayak", ""], ["Ballas", "Nicolas", ""], ["Rabbat", "Michael", ""]]}, {"id": "1910.00731", "submitter": "Jaafar Elmirghani", "authors": "Sanaa Hamid Mohamed, Taisir E.H. El-Gorashi and Jaafar M.H. Elmirghani", "title": "A Survey of Big Data Machine Learning Applications Optimization in Cloud\n  Data Centers and Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This survey article reviews the challenges associated with deploying and\noptimizing big data applications and machine learning algorithms in cloud data\ncenters and networks. The MapReduce programming model and its widely-used\nopen-source platform; Hadoop, are enabling the development of a large number of\ncloud-based services and big data applications. MapReduce and Hadoop thus\nintroduce innovative, efficient, and accelerated intensive computations and\nanalytics. These services usually utilize commodity clusters within\ngeographically-distributed data centers and provide cost-effective and elastic\nsolutions. However, the increasing traffic between and within the data centers\nthat migrate, store, and process big data, is becoming a bottleneck that calls\nfor enhanced infrastructures capable of reducing the congestion and power\nconsumption. Moreover, enterprises with multiple tenants requesting various big\ndata services are challenged by the need to optimize leasing their resources at\nreduced running costs and power consumption while avoiding under or over\nutilization. In this survey, we present a summary of the characteristics of\nvarious big data programming models and applications and provide a review of\ncloud computing infrastructures, and related technologies such as\nvirtualization, and software-defined networking that increasingly support big\ndata systems. Moreover, we provide a brief review of data centers topologies,\nrouting protocols, and traffic characteristics, and emphasize the implications\nof big data on such cloud data centers and their supporting networks. Wide\nranging efforts were devoted to optimize systems that handle big data in terms\nof various applications performance metrics and/or infrastructure energy\nefficiency. Finally, some insights and future research directions are provided.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 08:05:34 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Mohamed", "Sanaa Hamid", ""], ["El-Gorashi", "Taisir E. H.", ""], ["Elmirghani", "Jaafar M. H.", ""]]}, {"id": "1910.00739", "submitter": "Harish Anand", "authors": "Harish Anand, Stephen A. Rees, Zhiang Chen, Ashwin Jose, Sarah\n  Bearman, Prasad Antervedi, Jnaneshwar Das", "title": "OpenUAV Cloud Testbed: a Collaborative Design Studio for Field Robotics", "comments": "8 pages, Submitted to IEEE CASE 2021 for review, GitHub:\n  https://github.com/Open-UAV/openuav-turbovnc Webpage: https://openuav.us", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.DC cs.SE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Simulations play a crucial role in robotics research and education. This\npaper presents the OpenUAV testbed, an open-source, easy-to-use, web-based, and\nreproducible software system that enables students and researchers to run\nrobotic simulations on the cloud. We have built upon our previous work and have\naddressed some of the educational and research challenges associated with the\nprior work. The critical contributions of the paper to the robotics and\nautomation community are threefold: First, OpenUAV saves students and\nresearchers from tedious and complicated software setups by providing\nweb-browser-based Linux desktop sessions with standard robotics software like\nGazebo, ROS, and flight autonomy stack. Second, a method for saving an\nindividual's research work with its dependencies for the work's future\nreproducibility. Third, the platform provides a mechanism to support\nphotorealistic robotics simulations by combining Unity game engine-based camera\nrendering and Gazebo physics. The paper addresses a research need for\nphotorealistic simulations and describes a methodology for creating a\nphotorealistic aquatic simulation. We also present the various academic and\nresearch use-cases of this platform to improve robotics education and research,\nespecially during times like the COVID-19 pandemic, when virtual collaboration\nis necessary.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 01:33:53 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 01:48:09 GMT"}, {"version": "v3", "created": "Mon, 2 Nov 2020 19:38:42 GMT"}, {"version": "v4", "created": "Thu, 6 May 2021 16:54:01 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Anand", "Harish", ""], ["Rees", "Stephen A.", ""], ["Chen", "Zhiang", ""], ["Jose", "Ashwin", ""], ["Bearman", "Sarah", ""], ["Antervedi", "Prasad", ""], ["Das", "Jnaneshwar", ""]]}, {"id": "1910.00742", "submitter": "Gang Wang", "authors": "Gang Wang, Zhijie Jerry Shi, Mark Nixon, Song Han", "title": "ChainSplitter: Towards Blockchain-based Industrial IoT Architecture for\n  Supporting Hierarchical Storage", "comments": "10 pages, 5 figures, 2019 IEEE International Conference on Blockchain\n  (Blockchain) 978-1-7281-4693-5/19/$31.00 \\c{opyright}2019 IEEE", "journal-ref": "2019 IEEE International Conference on Blockchain (Blockchain)", "doi": "10.1109/Blockchain.2019.00030", "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fast developing Industrial Internet of Things (IIoT) technologies provide\na promising opportunity to build large-scale systems to connect numerous\nheterogeneous devices into the Internet. Most existing IIoT infrastructures are\nbased on a centralized architecture, which is easier for management but cannot\neffectively support immutable and verifiable services among multiple parties.\nBlockchain technology provides many desired features for large-scale IIoT\ninfrastructures, such as decentralization, trustworthiness, trackability, and\nimmutability. This paper presents a blockchain-based IIoT architecture to\nsupport immutable and verifiable services. However, when applying blockchain\ntechnology to the IIoT infrastructure, the required storage space posts a grant\nchallenge to resource-constrained IIoT infrastructures. To address the storage\nissue, this paper proposes a hierarchical blockchain storage structure,\n\\textit{ChainSplitter}. Specially, the proposed architecture features a\nhierarchical storage structure where the majority of the blockchain is stored\nin the clouds, while the most recent blocks are stored in the overlay network\nof the individual IIoT networks. The proposed architecture seamlessly binds\nlocal IIoT networks, the blockchain overlay network, and the cloud\ninfrastructure together through two connectors, the \\textit{blockchain\nconnector} and the \\textit{cloud connector}, to construct the hierarchical\nblockchain storage. The blockchain connector in the overlay network builds\nblocks in blockchain from data generated in IIoT networks, and the cloud\nconnector resolves the blockchain synchronization issues between the overlay\nnetwork and the clouds. We also provide a case study to show the efficiency of\nthe proposed hierarchical blockchain storage in a practical Industrial IoT\ncase.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 01:37:20 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Wang", "Gang", ""], ["Shi", "Zhijie Jerry", ""], ["Nixon", "Mark", ""], ["Han", "Song", ""]]}, {"id": "1910.00765", "submitter": "Mohammad Javad Amiri", "authors": "Mohammad Javad Amiri, Divyakant Agrawal, Amr El Abbadi", "title": "SharPer: Sharding Permissioned Blockchains Over Network Clusters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scalability is one of the main roadblocks to business adoption of blockchain\nsystems. Despite recent intensive research on using sharding techniques to\nenhance the scalability of blockchain systems, existing solutions do not\nefficiently address cross-shard transactions. In this paper, we introduce\nSharPer, a permissioned blockchain system that improves scalability by\nclustering (partitioning) the nodes and assigning different data shards to\ndifferent clusters where each data shard is replicated on the nodes of a\ncluster. SharPer supports both intra-shard and cross-shard transactions and\nprocesses intra-shard transactions of different clusters as well as cross-shard\ntransactions with non-overlapping clusters simultaneously. In SharPer, the\nblockchain ledger is formed as a directed acyclic graph where each cluster\nmaintains only a view of the ledger. SharPer also incorporates a flattened\nprotocol to establish consensus among clusters on the order of cross-shard\ntransactions. The experimental results reveal the efficiency of SharPer in\nterms of performance and scalability especially in workloads with a low\npercentage of cross-shard transactions.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 03:51:37 GMT"}, {"version": "v2", "created": "Sun, 16 Feb 2020 23:11:37 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Amiri", "Mohammad Javad", ""], ["Agrawal", "Divyakant", ""], ["Abbadi", "Amr El", ""]]}, {"id": "1910.00788", "submitter": "Hossein Esfandiari", "authors": "Hossein Esfandiari, Vahab Mirrokni, Peilin Zhong", "title": "Streaming Balanced Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering of data points in metric space is among the most fundamental\nproblems in computer science with plenty of applications in data mining,\ninformation retrieval and machine learning. Due to the necessity of clustering\nof large datasets, several streaming algorithms have been developed for\ndifferent variants of clustering problems such as $k$-median and $k$-means\nproblems. However, despite the importance of the context, the current\nunderstanding of balanced clustering (or more generally capacitated clustering)\nin the streaming setting is very limited. The only previously known streaming\napproximation algorithm for capacitated clustering requires three passes and\nonly handles insertions.\n  In this work, we develop \\emph{the first single pass streaming algorithm} for\na general class of clustering problems that includes capacitated $k$-median and\ncapacitated $k$-means in Euclidean space, using only poly$( k d \\log \\Delta)$\nspace, where $k$ is the number of clusters, $d$ is the dimension and $\\Delta$\nis the maximum relative range of a coordinate. (Note that $d\\log \\Delta$ is the\nspace required to represent one point.) This algorithm only violates the\ncapacity constraint by a $1+\\epsilon$ factor. Interestingly, unlike the\nprevious algorithm, our algorithm handles both insertions and deletions of\npoints. To provide this result we define a decomposition of the space via some\ncurved half-spaces. We used this decomposition to design a strong coreset of\nsize poly$( k d \\log \\Delta)$ for balanced clustering. Then, we show that this\ncoreset is implementable in the streaming and distributed settings.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 05:59:51 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Esfandiari", "Hossein", ""], ["Mirrokni", "Vahab", ""], ["Zhong", "Peilin", ""]]}, {"id": "1910.00849", "submitter": "Davide Basile", "authors": "Davide Basile, Maurice H. ter Beek, and Rosario Pugliese", "title": "Synthesis of Orchestrations and Choreographies: Bridging the Gap between\n  Supervisory Control and Coordination of Services", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 16, Issue 2 (June 3,\n  2020) lmcs:6527", "doi": "10.23638/LMCS-16(2:9)2020", "report-no": null, "categories": "eess.SY cs.DC cs.FL cs.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a number of contributions to bridging the gap between supervisory\ncontrol theory and coordination of services in order to explore the frontiers\nbetween coordination and control systems. Firstly, we modify the classical\nsynthesis algorithm from supervisory control theory for obtaining the so-called\nmost permissive controller in order to synthesise orchestrations and\nchoreographies of service contracts formalised as contract automata. The key\ningredient to make this possible is a novel notion of controllability. Then, we\npresent an abstract parametric synthesis algorithm and show that it generalises\nthe classical synthesis as well as the orchestration and choreography\nsyntheses. Finally, through the novel abstract synthesis, we show that the\nconcrete syntheses are in a refinement order. A running example from the\nservice domain illustrates our contributions.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 09:47:29 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 11:44:35 GMT"}, {"version": "v3", "created": "Mon, 4 May 2020 10:22:19 GMT"}, {"version": "v4", "created": "Tue, 2 Jun 2020 07:48:43 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Basile", "Davide", ""], ["ter Beek", "Maurice H.", ""], ["Pugliese", "Rosario", ""]]}, {"id": "1910.00862", "submitter": "Joachim Schopfel", "authors": "Otmane Azeroual (DZHW), Joachim Sch\\\"opfel (GERIICO)", "title": "Research Intelligence (CRIS) and the Cloud: A Review", "comments": null, "journal-ref": "Computer & Information Science, 2019, 12 (4), pp.40-55", "doi": "10.5539/cis.v12n4p40", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is to explore the impact of the cloud technology on\ncurrent research information systems (CRIS). Based on an overview of published\nliterature and on empirical evidence from surveys, the paper presents main\ncharacteristics, delivery models, service levels and general benefits of cloud\ncomputing. The second part assesses how the cloud computing challenges the\nresearch information management, from three angles: networking, specific\nbenefits, and the ingestion of data in the cloud. The third part describes\nthree aspects of the implementation of current research systems in the clouds,\ni.e. service models, requirements and potential risks and barriers. The paper\nconcludes with some perspectives for future work. The paper is written for CRIS\nadministrators and users, in order to improve research information management\nand to contribute to future development and implementation of these systems,\nbut also for scholars and students who want to have detailed knowledge on this\ntopic.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 10:22:33 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Azeroual", "Otmane", "", "DZHW"], ["Sch\u00f6pfel", "Joachim", "", "GERIICO"]]}, {"id": "1910.00916", "submitter": "Yu-Pin Hsu", "authors": "Yu-Pin Hsu, Yu-Chih Huang, and Shin-Lin Shieh", "title": "Scheduling Stochastic Real-Time Jobs in Unreliable Workers", "comments": "8 pages, technical report for the WCNC paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a distributed computing network consisting of a master and\nmultiple workers processing tasks of different types. The master is running\nmultiple applications. Each application stochastically generates real-time jobs\nwith a strict job deadline, where each job is a collection of tasks of some\ntypes specified by the application. A real-time job is completed only when all\nits tasks are completed by the corresponding workers within the deadline.\nMoreover, we consider unreliable workers, whose processing speeds are\nuncertain. Because of the limited processing abilities of the workers, an\nalgorithm for scheduling the jobs in the workers is needed to maximize the\naverage number of completed jobs for each application. The scheduling problem\nis not only critical but also practical in distributed computing networks. In\nthis paper, we develop two scheduling algorithms, namely, a feasibility-optimal\nscheduling algorithm and an approximate scheduling algorithm. The\nfeasibility-optimal scheduling algorithm can fulfill the largest region of\napplications' requirements for the average number of completed jobs. However,\nthe feasibility-optimal scheduling algorithm suffers from high computational\ncomplexity when the number of applications is large. To address the issue, the\napproximate scheduling algorithm is proposed with a guaranteed approximation\nratio in the worst-case scenario. The approximate scheduling algorithm is also\nvalidated in the average-case scenario via computer simulations.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 12:48:25 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 12:54:19 GMT"}, {"version": "v3", "created": "Fri, 4 Oct 2019 02:40:39 GMT"}, {"version": "v4", "created": "Fri, 25 Oct 2019 02:53:08 GMT"}, {"version": "v5", "created": "Thu, 30 Jan 2020 05:05:09 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Hsu", "Yu-Pin", ""], ["Huang", "Yu-Chih", ""], ["Shieh", "Shin-Lin", ""]]}, {"id": "1910.00928", "submitter": "Mathis Bode", "authors": "Mathis Bode and Michael Gauding and Konstantin Kleinheinz and Heinz\n  Pitsch", "title": "Deep learning at scale for subgrid modeling in turbulent flows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.DC cs.LG physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling of turbulent flows is still challenging. One way to deal with the\nlarge scale separation due to turbulence is to simulate only the large scales\nand model the unresolved contributions as done in large-eddy simulation (LES).\nThis paper focuses on two deep learning (DL) strategies, regression and\nreconstruction, which are data-driven and promising alternatives to classical\nmodeling concepts. Using three-dimensional (3-D) forced turbulence direct\nnumerical simulation (DNS) data, subgrid models are evaluated, which predict\nthe unresolved part of quantities based on the resolved solution. For\nregression, it is shown that feedforward artificial neural networks (ANNs) are\nable to predict the fully-resolved scalar dissipation rate using filtered input\ndata. It was found that a combination of a large-scale quantity, such as the\nfiltered passive scalar itself, and a small-scale quantity, such as the\nfiltered energy dissipation rate, gives the best agreement with the actual DNS\ndata. Furthermore, a DL network motivated by enhanced super-resolution\ngenerative adversarial networks (ESRGANs) was used to reconstruct\nfully-resolved 3-D velocity fields from filtered velocity fields. The energy\nspectrum shows very good agreement. As size of scientific data is often in the\norder of terabytes or more, DL needs to be combined with high performance\ncomputing (HPC). Necessary code improvements for HPC-DL are discussed with\nrespect to the supercomputer JURECA. After optimizing the training code, 396.2\nTFLOPS were achieved.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 15:50:10 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Bode", "Mathis", ""], ["Gauding", "Michael", ""], ["Kleinheinz", "Konstantin", ""], ["Pitsch", "Heinz", ""]]}, {"id": "1910.00985", "submitter": "Anh Dinh", "authors": "Tien Tuan Anh Dinh, Anwitaman Datta, Beng Chin Ooi", "title": "A Blueprint for Interoperable Blockchains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in blockchain systems has mainly focused on improving security and\nbridging the performance gaps between blockchains and databases. Despite many\npromising results, we observe a worrying trend that the blockchain landscape is\nfragmented in which many systems exist in silos. Apart from a handful of\ngeneral-purpose blockchains, such as Ethereum or Hyperledger Fabric, there are\nhundreds of others designed for specific applications and typically do not talk\nto each other. In this paper, we describe our vision of interoperable\nblockchains. We argue that supporting interaction among different blockchains\nrequires overcoming challenges that go beyond data standardization. The\nunderlying problem is to allow smart contracts running in different blockchains\nto communicate. We discuss three open problems: access control, general\ncross-chain transactions, and cross-chain communication. We describe partial\nsolutions to some of these problems in the literature. Finally, we propose a\nnovel design to overcome these challenges.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 14:44:47 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2019 02:56:25 GMT"}, {"version": "v3", "created": "Tue, 22 Oct 2019 12:46:10 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Dinh", "Tien Tuan Anh", ""], ["Datta", "Anwitaman", ""], ["Ooi", "Beng Chin", ""]]}, {"id": "1910.01012", "submitter": "Anil Somayaji", "authors": "Mohamed Alsharnouby and Anil Somayaji", "title": "Thread Homeostasis: Real-Time Anomalous Behavior Detection for\n  Safety-Critical Software", "comments": "draft report version 0.5, based on Mohamed Alsharnouby's MCS thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Safety-critical systems must always have predictable and reliable behavior,\notherwise systems fail and lives are put at risk. Even with the most rigorous\ntesting it is impossible to test systems using all possible inputs. Complex\nsoftware systems will often fail when given novel sets of inputs; thus,\nsafety-critical systems may behave in unintended, dangerous ways when subject\nto inputs combinations that were not seen in development. Safety critical\nsystems are normally designed to be fault tolerant so they do not fail when\ngiven unexpected inputs.\n  Anomaly detection has been proposed as a technique for improving the fault\ntolerance of safety-critical systems. Past work, however, has been largely\nlimited to behavioral parameter thresholds that miss many kinds of system\ndeviations. Here we propose a novel approach to anomaly detection in\nfault-tolerant safety critical systems using patterns of messages between\nthreads. This approach is based on techniques originally developed for\ndetecting security violations on systems with UNIX-like system call APIs; here\nwe show that they can be adapted to the constraints of safety critical\nmicrokernel-based hard real-time systems. We present the design,\nimplementation, and initial evaluation of tH (thread Homeostasis) implemented\non a QNX-based self-driving car platform.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 03:30:41 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Alsharnouby", "Mohamed", ""], ["Somayaji", "Anil", ""]]}, {"id": "1910.01173", "submitter": "Cody Bumgardner", "authors": "V. K. Cody Bumgardner, Nima Seyedtalebi, Caylin Hickey", "title": "Toward Edge-enabled Cyber-Physical Systems Testbeds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of edge computing can be extremely valuable in support of CPS\nefforts. However, few if any testbeds provide the type of resource control and\nprovisioning required to support edge-enabled CPS experimentation. Likewise,\ncommercial offerings provide operational capabilities, but lack the distributed\ninfrastructure and transparency provided by research testbed. In this paper we\npropose methods to develop new and augment existing testbeds to better support\nthe challenges of edge computing and CPS research. The proposed network is\nspecifically designed to address the challenges associated with edge-based\nprovisioning, data collection, analysis, monitoring, and measurement across\nislands of edge and data center resources.\n  We present the purpose of our work, the basic architecture, initial results,\nthe relationship to the existing software, and the potential of an existing\nedge-focused framework to support the foundations of edge-focused CPS testbeds.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 19:15:48 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Bumgardner", "V. K. Cody", ""], ["Seyedtalebi", "Nima", ""], ["Hickey", "Caylin", ""]]}, {"id": "1910.01196", "submitter": "Chih-Chieh Yang", "authors": "Chih-Chieh Yang and Guojing Cong", "title": "Accelerating Data Loading in Deep Neural Network Training", "comments": "11 pages, 12 figures, accepted for publication in IEEE International\n  Conference on High Performance Computing, Data and Analytics (HiPC) 2019", "journal-ref": null, "doi": "10.1109/HiPC.2019.00037", "report-no": null, "categories": "cs.LG cs.DC cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data loading can dominate deep neural network training time on large-scale\nsystems. We present a comprehensive study on accelerating data loading\nperformance in large-scale distributed training. We first identify performance\nand scalability issues in current data loading implementations. We then propose\noptimizations that utilize CPU resources to the data loader design. We use an\nanalytical model to characterize the impact of data loading on the overall\ntraining time and establish the performance trend as we scale up distributed\ntraining. Our model suggests that I/O rate limits the scalability of\ndistributed training, which inspires us to design a locality-aware data loading\nmethod. By utilizing software caches, our method can drastically reduce the\ndata loading communication volume in comparison with the original data loading\nimplementation. Finally, we evaluate the proposed optimizations with various\nexperiments. We achieved more than 30x speedup in data loading using 256 nodes\nwith 1,024 learners.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 20:03:02 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Yang", "Chih-Chieh", ""], ["Cong", "Guojing", ""]]}, {"id": "1910.01250", "submitter": "Oscar Morales-Ponce", "authors": "Oscar Morales-Ponce", "title": "Optimal Patrolling of High Priority Segments While Visiting the Unit\n  Interval with a Set of Mobile Robots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a region that requires to be protected from unauthorized\npenetrations. The border of the region, modeled as a unit line segment,\nconsists of high priority segments that require the highest level of protection\nseparated by low priority segments that require to be visited infinitely often.\nWe study the problem of patrolling the border with a set of $k$ robots. The\ngoal is to obtain a strategy that minimizes the maximum idle time (the time\nthat a point is left unattended) of the high priority points while visiting the\nlow priority points infinitely often. We use the concept of single lid cover\n(segments of fixed length) where each high priority point is covered with at\nleast one lid, and then we extend it to strong double-lid cover where each high\npriority point is covered with at least two lids, and the unit line segment is\nfully covered. Let $\\lambda_{k-1}$ be the minimum lid length that accepts a\nsingle $\\lambda_{k-1}$-lid cover with $k-1$ lids and $\\Lambda_{2k}$ be the\nminimum lid length that accepts a strong double $\\Lambda_{2k}$-lid cover with\n$2k$ lids. We show that $2\\min(\\Lambda_{2k}, \\lambda_{k-1})$ is the lower bound\nof the idle time when the max speed of the robots is one. To compute\n$\\Lambda_{2k}$ and $\\lambda_{k-1}$, we present an algorithm with time\ncomplexity $O(\\max(k, n)\\log{n})$ where $n$ is the number of high priority\nsections. For the upper bound, first we present a strategy with idle time\n$\\lambda_{k-1}$ where one robot covers the unit line, and the remaining robots\ncover the lids of a single $\\lambda_{k-1}$-lid cover with $k-1$ lids. Then, we\npresent a simple strategy with idle time $3\\Lambda_{2k}$ that splits the unit\nline into not-disjoint $k$ segments of equal length that robots synchronously\ncover. Then, we present a complex strategy that split the unit line into $k$\nnon-disjoint segments that robots asynchronously cover.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 23:20:18 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Morales-Ponce", "Oscar", ""]]}, {"id": "1910.01354", "submitter": "Kai Rothauge", "authors": "Kai Rothauge, Haripriya Ayyalasomayajula, Kristyn J. Maschhoff,\n  Michael Ringenburg, Michael W. Mahoney", "title": "Running Alchemist on Cray XC and CS Series Supercomputers: Dask and\n  PySpark Interfaces, Deployment Options, and Data Transfer Times", "comments": "This work appeared at CUG 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alchemist is a system that allows Apache Spark to achieve better performance\nby interfacing with HPC libraries for large-scale distributed computations. In\nthis paper, we highlight some recent developments in Alchemist that are of\ninterest to Cray users and the scientific community in general. We discuss our\nexperience porting Alchemist to container images and deploying it on Cray XC\n(using Shifter) and CS (using Singularity) series supercomputers and on a local\nKubernetes cluster.\n  Newly developed interfaces for Python, Dask, and PySpark enable the use of\nAlchemist with additional data analysis frameworks. We also briefly discuss the\ncombination of Alchemist with RLlib, an increasingly popular library for\nreinforcement learning, and consider the benefits of leveraging HPC simulations\nin reinforcement learning. Finally, since data transfer between the client\napplications and Alchemist are the main overhead Alchemist encounters, we give\na qualitative assessment of these transfer times with respect to\ndifferent~factors.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 08:42:51 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2019 06:20:59 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Rothauge", "Kai", ""], ["Ayyalasomayajula", "Haripriya", ""], ["Maschhoff", "Kristyn J.", ""], ["Ringenburg", "Michael", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1910.01355", "submitter": "Wentai Wu", "authors": "Wentai Wu, Ligang He, Weiwei Lin, Rui Mao, Carsten Maple, Stephen\n  Jarvis", "title": "SAFA: a Semi-Asynchronous Protocol for Fast Federated Learning with Low\n  Overhead", "comments": "16 pages, 8 figures", "journal-ref": "IEEE Transactions on Computers. vol. 70, pp. 655-668 (2020)", "doi": "10.1109/TC.2020.2994391", "report-no": null, "categories": "cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Federated learning (FL) has attracted increasing attention as a promising\napproach to driving a vast number of end devices with artificial intelligence.\nHowever, it is very challenging to guarantee the efficiency of FL considering\nthe unreliable nature of end devices while the cost of device-server\ncommunication cannot be neglected. In this paper, we propose SAFA, a\nsemi-asynchronous FL protocol, to address the problems in federated learning\nsuch as low round efficiency and poor convergence rate in extreme conditions\n(e.g., clients dropping offline frequently). We introduce novel designs in the\nsteps of model distribution, client selection and global aggregation to\nmitigate the impacts of stragglers, crashes and model staleness in order to\nboost efficiency and improve the quality of the global model. We have conducted\nextensive experiments with typical machine learning tasks. The results\ndemonstrate that the proposed protocol is effective in terms of shortening\nfederated round duration, reducing local resource wastage, and improving the\naccuracy of the global model at an acceptable communication cost.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 08:43:09 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 08:58:39 GMT"}, {"version": "v3", "created": "Sat, 16 May 2020 09:00:09 GMT"}, {"version": "v4", "created": "Fri, 23 Apr 2021 10:22:34 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Wu", "Wentai", ""], ["He", "Ligang", ""], ["Lin", "Weiwei", ""], ["Mao", "Rui", ""], ["Maple", "Carsten", ""], ["Jarvis", "Stephen", ""]]}, {"id": "1910.01483", "submitter": "Vincenzo De Florio", "authors": "Vincenzo De Florio and Susanna Donatelli and Giovanna Dondossola", "title": "Flexible Development of Dependability Services: An Experience Derived\n  from Energy Automation Systems", "comments": "The final version of this paper appeared in the Proceedings of the\n  2002 Conference on the Engineering of Computer Based Systems (ECBS-2002).\n  arXiv admin note: text overlap with arXiv:1611.02273", "journal-ref": null, "doi": "10.1109/ECBS.2002.999826", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a novel approach for the flexible development of\ndependable automation services applied to a case study taken from requirements\nof energy automation systems. It shows first how the use of a custom\ncompositional recovery language can be exploited to achieve a flexible and\ndependable functionality in software. Then it is shown how modeling techniques\nbased on Petri nets can be used to assess the properties that different\nconfigurations of the addressed service can achieve.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 13:22:33 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["De Florio", "Vincenzo", ""], ["Donatelli", "Susanna", ""], ["Dondossola", "Giovanna", ""]]}, {"id": "1910.01834", "submitter": "Joachim Neu", "authors": "Vivek Bagaria, Joachim Neu, David Tse", "title": "Boomerang: Redundancy Improves Latency and Throughput in Payment-Channel\n  Networks", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-51280-4_17", "report-no": null, "categories": "cs.CR cs.DC cs.IT cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multi-path routing schemes for payment-channel networks, Alice transfers\nfunds to Bob by splitting them into partial payments and routing them along\nmultiple paths. Undisclosed channel balances and mismatched transaction fees\ncause delays and failures on some payment paths. For atomic transfer schemes,\nthese straggling paths stall the whole transfer. We show that the latency of\ntransfers reduces when redundant payment paths are added. This frees up\nliquidity in payment channels and hence increases the throughput of the\nnetwork. We devise Boomerang, a generic technique to be used on top of\nmulti-path routing schemes to construct redundant payment paths free of\ncounterparty risk. In our experiments, applying Boomerang to a baseline routing\nscheme leads to 40% latency reduction and 2x throughput increase. We build on\nideas from publicly verifiable secret sharing, such that Alice learns a secret\nof Bob iff Bob overdraws funds from the redundant paths. Funds are forwarded\nusing Boomerang contracts, which allow Alice to revert the transfer iff she has\nlearned Bob's secret. We implement the Boomerang contract in Bitcoin Script.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 08:26:52 GMT"}, {"version": "v2", "created": "Sun, 29 Mar 2020 04:47:15 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Bagaria", "Vivek", ""], ["Neu", "Joachim", ""], ["Tse", "David", ""]]}, {"id": "1910.01972", "submitter": "Karel Ad\\'amek", "authors": "Karel Ad\\'amek, Sofia Dimoudi, Mike Giles, Wesley Armour", "title": "GPU Fast Convolution via the Overlap-and-Save Method in Shared Memory", "comments": "accepted to ACM TACO", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an implementation of the overlap-and-save method, a method for the\nconvolution of very long signals with short response functions, which is\ntailored to GPUs. We have implemented several FFT algorithms (using the CUDA\nprogramming language) which exploit GPU shared memory, allowing for GPU\naccelerated convolution. We compare our implementation with an implementation\nof the overlap-and-save algorithm utilizing the NVIDIA FFT library (cuFFT). We\ndemonstrate that by using a shared memory based FFT we can achieved significant\nspeed-ups for certain problem sizes and lower the memory requirements of the\noverlap-and-save method on GPUs.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 14:41:10 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2020 15:04:13 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Ad\u00e1mek", "Karel", ""], ["Dimoudi", "Sofia", ""], ["Giles", "Mike", ""], ["Armour", "Wesley", ""]]}, {"id": "1910.01991", "submitter": "Felix Sattler", "authors": "Felix Sattler, Klaus-Robert M\\\"uller, Wojciech Samek", "title": "Clustered Federated Learning: Model-Agnostic Distributed Multi-Task\n  Optimization under Privacy Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning (FL) is currently the most widely adopted framework for\ncollaborative training of (deep) machine learning models under privacy\nconstraints. Albeit it's popularity, it has been observed that Federated\nLearning yields suboptimal results if the local clients' data distributions\ndiverge. To address this issue, we present Clustered Federated Learning (CFL),\na novel Federated Multi-Task Learning (FMTL) framework, which exploits\ngeometric properties of the FL loss surface, to group the client population\ninto clusters with jointly trainable data distributions. In contrast to\nexisting FMTL approaches, CFL does not require any modifications to the FL\ncommunication protocol to be made, is applicable to general non-convex\nobjectives (in particular deep neural networks) and comes with strong\nmathematical guarantees on the clustering quality. CFL is flexible enough to\nhandle client populations that vary over time and can be implemented in a\nprivacy preserving way. As clustering is only performed after Federated\nLearning has converged to a stationary point, CFL can be viewed as a\npost-processing method that will always achieve greater or equal performance\nthan conventional FL by allowing clients to arrive at more specialized models.\nWe verify our theoretical analysis in experiments with deep convolutional and\nrecurrent neural networks on commonly used Federated Learning datasets.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 15:31:09 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Sattler", "Felix", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Samek", "Wojciech", ""]]}, {"id": "1910.02054", "submitter": "Jeff Rasley", "authors": "Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He", "title": "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large deep learning models offer significant accuracy gains, but training\nbillions to trillions of parameters is challenging. Existing solutions such as\ndata and model parallelisms exhibit fundamental limitations to fit these models\ninto limited device memory, while obtaining computation, communication and\ndevelopment efficiency. We develop a novel solution, Zero Redundancy Optimizer\n(ZeRO), to optimize memory, vastly improving training speed while increasing\nthe model size that can be efficiently trained. ZeRO eliminates memory\nredundancies in data- and model-parallel training while retaining low\ncommunication volume and high computational granularity, allowing us to scale\nthe model size proportional to the number of devices with sustained high\nefficiency. Our analysis on memory requirements and communication volume\ndemonstrates: ZeRO has the potential to scale beyond 1 Trillion parameters\nusing today's hardware.\n  We implement and evaluate ZeRO: it trains large models of over 100B parameter\nwith super-linear speedup on 400 GPUs, achieving throughput of 15 Petaflops.\nThis represents an 8x increase in model size and 10x increase in achievable\nperformance over state-of-the-art. In terms of usability, ZeRO can train large\nmodels of up to 13B parameters (e.g., larger than Megatron GPT 8.3B and T5 11B)\nwithout requiring model parallelism which is harder for scientists to apply.\nLast but not the least, researchers have used the system breakthroughs of ZeRO\nto create the world's largest language model (Turing-NLG, 17B parameters) with\nrecord breaking accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 17:29:39 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 16:55:13 GMT"}, {"version": "v3", "created": "Wed, 13 May 2020 06:45:15 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Rajbhandari", "Samyam", ""], ["Rasley", "Jeff", ""], ["Ruwase", "Olatunji", ""], ["He", "Yuxiong", ""]]}, {"id": "1910.02100", "submitter": "Abishek Sankararaman", "authors": "Abishek Sankararaman, Ayalvadi Ganesh, Sanjay Shakkottai", "title": "Social Learning in Multi Agent Multi Armed Bandits", "comments": "Minor Corrections from before", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NI cs.SI math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a distributed version of the classical stochastic\nMulti-Arm Bandit (MAB) problem. Our setting consists of a large number of\nagents $n$ that collaboratively and simultaneously solve the same instance of\n$K$ armed MAB to minimize the average cumulative regret over all agents. The\nagents can communicate and collaborate among each other \\emph{only} through a\npairwise asynchronous gossip based protocol that exchange a limited number of\nbits. In our model, agents at each point decide on (i) which arm to play, (ii)\nwhether to, and if so (iii) what and whom to communicate with. Agents in our\nmodel are decentralized, namely their actions only depend on their observed\nhistory in the past.\n  We develop a novel algorithm in which agents, whenever they choose,\ncommunicate only arm-ids and not samples, with another agent chosen uniformly\nand independently at random. The per-agent regret scaling achieved by our\nalgorithm is $O \\left( \\frac{\\lceil\\frac{K}{n}\\rceil+\\log(n)}{\\Delta}\n  \\log(T) + \\frac{\\log^3(n) \\log \\log(n)}{\\Delta^2}\n  \\right)$. Furthermore, any agent in our algorithm communicates only a total\nof $\\Theta(\\log(T))$ times over a time interval of $T$.\n  We compare our results to two benchmarks - one where there is no\ncommunication among agents and one corresponding to complete interaction. We\nshow both theoretically and empirically, that our algorithm experiences a\nsignificant reduction both in per-agent regret when compared to the case when\nagents do not collaborate and in communication complexity when compared to the\nfull interaction setting which requires $T$ communication attempts by an agent\nover $T$ arm pulls.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 18:34:04 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 15:12:18 GMT"}, {"version": "v3", "created": "Tue, 5 Nov 2019 01:20:10 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Sankararaman", "Abishek", ""], ["Ganesh", "Ayalvadi", ""], ["Shakkottai", "Sanjay", ""]]}, {"id": "1910.02158", "submitter": "Benjamin Brock", "authors": "Benjamin Brock, Yuxin Chen, Jiakun Yan, John D. Owens, Ayd{\\i}n\n  Bulu\\c{c}, and Katherine Yelick", "title": "RDMA vs. RPC for Implementing Distributed Data Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed data structures are key to implementing scalable applications for\nscientific simulations and data analysis. In this paper we look at two\nimplementation styles for distributed data structures: remote direct memory\naccess (RDMA) and remote procedure call (RPC). We focus on operations that\nrequire individual accesses to remote portions of a distributed data structure,\ne.g., accessing a hash table bucket or distributed queue, rather than global\noperations in which all processors collectively exchange information. We look\nat the trade-offs between the two styles through microbenchmarks and a\nperformance model that approximates the cost of each. The RDMA operations have\ndirect hardware support in the network and therefore lower latency and\noverhead, while the RPC operations are more expressive but higher cost and can\nsuffer from lack of attentiveness from the remote side. We also run experiments\nto compare the real-world performance of RDMA- and RPC-based data structure\noperations with the predicted performance to evaluate the accuracy of our\nmodel, and show that while the model does not always precisely predict running\ntime, it allows us to choose the best implementation in the examples shown. We\nbelieve this analysis will assist developers in designing data structures that\nwill perform well on current network architectures, as well as network\narchitects in providing better support for this class of distributed data\nstructures.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 22:11:24 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 18:05:51 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Brock", "Benjamin", ""], ["Chen", "Yuxin", ""], ["Yan", "Jiakun", ""], ["Owens", "John D.", ""], ["Bulu\u00e7", "Ayd\u0131n", ""], ["Yelick", "Katherine", ""]]}, {"id": "1910.02239", "submitter": "Moshe Sulamy", "authors": "Yehuda Afek, Yishay Mansour, Shaked Rafaeli, and Moshe Sulamy", "title": "The Role of A-priori Information in Networks of Rational Agents", "comments": "This paper is the full version of the DISC 2018 paper. arXiv admin\n  note: substantial text overlap with arXiv:1711.04728", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Until now, distributed algorithms for rational agents have assumed a-priori\nknowledge of $n$, the size of the network. This assumption is challenged here\nby proving how much a-priori knowledge is necessary for equilibrium in\ndifferent distributed computing problems. Duplication - pretending to be more\nthan one agent - is the main tool used by agents to deviate and increase their\nutility when not enough knowledge about $n$ is given. The a-priori knowledge of\n$n$ is formalized as a Bayesian setting where at the beginning of the algorithm\nagents only know a prior $\\sigma$, a distribution from which they know $n$\noriginates. We begin by providing new algorithms for the Knowledge Sharing and\nColoring problems when $n$ is a-priori known to all agents. We then prove that\nwhen agents have no a-priori knowledge of $n$, i.e., the support for $\\sigma$\nis infinite, equilibrium is impossible for the Knowledge Sharing problem.\nFinally, we consider priors with finite support and find bounds on the\nnecessary interval $[\\alpha,\\beta]$ that contains the support of $\\sigma$,\ni.e., $\\alpha \\leq n \\leq \\beta$, for which we have an equilibrium. When\npossible, we extend these bounds to hold for any possible protocol.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 09:30:31 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Afek", "Yehuda", ""], ["Mansour", "Yishay", ""], ["Rafaeli", "Shaked", ""], ["Sulamy", "Moshe", ""]]}, {"id": "1910.02270", "submitter": "Sam Ade Jacobs", "authors": "Sam Ade Jacobs, Brian Van Essen, David Hysom, Jae-Seung Yeom, Tim\n  Moon, Rushil Anirudh, Jayaraman J. Thiagaranjan, Shusen Liu, Peer-Timo\n  Bremer, Jim Gaffney, Tom Benson, Peter Robinson, Luc Peterson, Brian Spears", "title": "Parallelizing Training of Deep Generative Models on Massive Scientific\n  Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG hep-ex physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep neural networks on large scientific data is a challenging task\nthat requires enormous compute power, especially if no pre-trained models exist\nto initialize the process. We present a novel tournament method to train\ntraditional as well as generative adversarial networks built on LBANN, a\nscalable deep learning framework optimized for HPC systems. LBANN combines\nmultiple levels of parallelism and exploits some of the worlds largest\nsupercomputers. We demonstrate our framework by creating a complex predictive\nmodel based on multi-variate data from high-energy-density physics containing\nhundreds of millions of images and hundreds of millions of scalar values\nderived from tens of millions of simulations of inertial confinement fusion.\nOur approach combines an HPC workflow and extends LBANN with optimized data\ningestion and the new tournament-style training algorithm to produce a scalable\nneural network architecture using a CORAL-class supercomputer. Experimental\nresults show that 64 trainers (1024 GPUs) achieve a speedup of 70.2 over a\nsingle trainer (16 GPUs) baseline, and an effective 109% parallel efficiency.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 13:39:07 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Jacobs", "Sam Ade", ""], ["Van Essen", "Brian", ""], ["Hysom", "David", ""], ["Yeom", "Jae-Seung", ""], ["Moon", "Tim", ""], ["Anirudh", "Rushil", ""], ["Thiagaranjan", "Jayaraman J.", ""], ["Liu", "Shusen", ""], ["Bremer", "Peer-Timo", ""], ["Gaffney", "Jim", ""], ["Benson", "Tom", ""], ["Robinson", "Peter", ""], ["Peterson", "Luc", ""], ["Spears", "Brian", ""]]}, {"id": "1910.02276", "submitter": "Quan-Lin Li", "authors": "Rui-Na Fan, Quan-Lin Li, Xiaole Wu, Zhe George Zhang", "title": "Dockless Bike-Sharing Systems with Unusable Bikes: Removing, Repair and\n  Redistribution under Batch Policies", "comments": "51 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC cs.SI math.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses a large-scale dockless bike-sharing system (DBSS) with\nunusable bikes, which can be removed, repaired, redistributed and reused under\ntwo batch policies: One for removing the unusable bikes from each parking\nregion to a maintenance shop, and the other for redistributing the repaired\nbikes from the maintenance shop to some suitable parking regions. For such a\nbike-sharing system, this paper proposes and develops a new computational\nmethod by applying the RG-factorizations of block-structured Markov processes\nin the closed queueing networks. Different from previous works in the\nliterature of queueing networks, a key contribution of our computational method\nis to set up a new nonlinear matrix equation to determine the relative arrival\nrates, and to show that the nonlinearity comes from two different groups of\nprocesses: The failure and removing processes; and the repair and\nredistributing processes. Once the relative arrival rate is introduced to each\nnode, these nodes are isolated from each other, so that the Markov processes of\nall the nodes are independent of each other, thus the Markov system of each\nnode is described as an elegant block-structured Markov process whose\nstationary probabilities can be easily computed by the RG-factorizations. Based\non this, this paper can establish a more general product-form solution of the\nclosed queueing network, and provides performance analysis of the DBSS through\na comprehensive discussion for the bikes' failure, removing, repair,\nredistributing and reuse processes under two batch policies. We hope that our\nmethod opens a new avenue to quantitative evaluation of more general DBSSs with\nunusable bikes.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 14:25:06 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 08:43:35 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Fan", "Rui-Na", ""], ["Li", "Quan-Lin", ""], ["Wu", "Xiaole", ""], ["Zhang", "Zhe George", ""]]}, {"id": "1910.02312", "submitter": "Vivek Sharma", "authors": "Vivek Sharma, Praneeth Vepakomma, Tristan Swedish, Ken Chang,\n  Jayashree Kalpathy-Cramer, Ramesh Raskar", "title": "ExpertMatcher: Automating ML Model Selection for Users in Resource\n  Constrained Countries", "comments": "In NeurIPS Workshop on Machine learning for the Developing World\n  (ML4D)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work we introduce ExpertMatcher, a method for automating deep\nlearning model selection using autoencoders. Specifically, we are interested in\nperforming inference on data sources that are distributed across many clients\nusing pretrained expert ML networks on a centralized server. The ExpertMatcher\nassigns the most relevant model(s) in the central server given the client's\ndata representation. This allows resource-constrained clients in developing\ncountries to utilize the most relevant ML models for their given task without\nhaving to evaluate the performance of each ML model. The method is generic and\ncan be beneficial in any setup where there are local clients and numerous\ncentralized expert ML models.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 18:58:59 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Sharma", "Vivek", ""], ["Vepakomma", "Praneeth", ""], ["Swedish", "Tristan", ""], ["Chang", "Ken", ""], ["Kalpathy-Cramer", "Jayashree", ""], ["Raskar", "Ramesh", ""]]}, {"id": "1910.02371", "submitter": "Edgar Solomonik", "authors": "Navjot Singh, Zecheng Zhang, Xiaoxiao Wu, Naijing Zhang, Siyuan Zhang,\n  and Edgar Solomonik", "title": "Distributed-Memory Tensor Completion for Generalized Loss Functions in\n  Python using New Sparse Tensor Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor computations are increasingly prevalent numerical techniques in data\nscience, but pose unique challenges for high-performance implementation. We\nprovide novel algorithms and systems infrastructure which enable efficient\nparallel implementation of algorithms for tensor completion with generalized\nloss functions. Specifically, we consider alternating minimization, coordinate\nminimization, and a quasi-Newton (generalized Gauss-Newton) method. By\nextending the Cyclops library, we implement all of these methods in high-level\nPython syntax. To make possible tensor completion for very sparse tensors, we\nintroduce new multi-tensor primitives, for which we provide specialized\nparallel implementations. We compare these routines to pairwise contraction of\nsparse tensors by reduction to hypersparse matrix formats, and find that the\nmulti-tensor routines are more efficient in theoretical cost and execution time\nin experiments. We provide microbenchmarking results on the Stampede2\nsupercomputer to demonstrate the efficiency of the new primitives and Cyclops\nfunctionality. We then study the performance of the tensor completion methods\nfor a synthetic tensor with 10 billion nonzeros and the Netflix dataset,\nconsidering both least squares and Poisson loss functions.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 04:48:05 GMT"}, {"version": "v2", "created": "Mon, 4 Nov 2019 05:36:52 GMT"}, {"version": "v3", "created": "Mon, 24 May 2021 22:31:55 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Singh", "Navjot", ""], ["Zhang", "Zecheng", ""], ["Wu", "Xiaoxiao", ""], ["Zhang", "Naijing", ""], ["Zhang", "Siyuan", ""], ["Solomonik", "Edgar", ""]]}, {"id": "1910.02434", "submitter": "Yuguang Wang", "authors": "Shao-Bo Lin, Yu Guang Wang, Ding-Xuan Zhou", "title": "Distributed filtered hyperinterpolation for noisy data on the sphere", "comments": "26 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CA cs.DC cs.LG cs.NA math.NA math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Problems in astrophysics, space weather research and geophysics usually need\nto analyze noisy big data on the sphere. This paper develops distributed\nfiltered hyperinterpolation for noisy data on the sphere, which assigns the\ndata fitting task to multiple servers to find a good approximation of the\nmapping of input and output data. For each server, the approximation is a\nfiltered hyperinterpolation on the sphere by a small proportion of quadrature\nnodes. The distributed strategy allows parallel computing for data processing\nand model selection and thus reduces computational cost for each server while\npreserves the approximation capability compared to the filtered\nhyperinterpolation. We prove quantitative relation between the approximation\ncapability of distributed filtered hyperinterpolation and the numbers of input\ndata and servers. Numerical examples show the efficiency and accuracy of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 12:28:08 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Lin", "Shao-Bo", ""], ["Wang", "Yu Guang", ""], ["Zhou", "Ding-Xuan", ""]]}, {"id": "1910.02639", "submitter": "Aur\\'elien Cavelan", "authors": "Aur\\'elien Cavelan, Rub\\'en M. Cabez\\'on, Jonas H. M. Korndorfer and\n  Florina M. Ciorba", "title": "Finding Neighbors in a Forest: A b-tree for Smoothed Particle\n  Hydrodynamics Simulations", "comments": "Adding a few references in Related Work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the exact close neighbors of each fluid element in mesh-free\ncomputational hydrodynamical methods, such as the Smoothed Particle\nHydrodynamics (SPH), often becomes a main bottleneck for scaling their\nperformance beyond a few million fluid elements per computing node. Tree\nstructures are particularly suitable for SPH simulation codes, which rely on\nfinding the exact close neighbors of each fluid element (or SPH particle). In\nthis work we present a novel tree structure, named \\textit{$b$-tree}, which\nfeatures an adaptive branching factor to reduce the depth of the neighbor\nsearch. Depending on the particle spatial distribution, finding neighbors using\n\\tree has an asymptotic best case complexity of $O(n)$, as opposed to $O(n \\log\nn)$ for other classical tree structures such as octrees and quadtrees. We also\npresent the proposed tree structure as well as the algorithms to build it and\nto find the exact close neighbors of all particles. We assess the scalability\nof the proposed tree-based algorithms through an extensive set of performance\nexperiments in a shared-memory system. Results show that b-tree is up to\n$12\\times$ faster for building the tree and up to $1.6\\times$ faster for\nfinding the exact neighbors of all particles when compared to its octree form.\nMoreover, we apply b-tree to a SPH code and show its usefulness over the\nexisting octree implementation, where b-tree is up to $5\\times$ faster for\nfinding the exact close neighbors compared to the legacy code.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 07:20:05 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 13:20:04 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Cavelan", "Aur\u00e9lien", ""], ["Cabez\u00f3n", "Rub\u00e9n M.", ""], ["Korndorfer", "Jonas H. M.", ""], ["Ciorba", "Florina M.", ""]]}, {"id": "1910.02653", "submitter": "Ajay Jain", "authors": "Paras Jain, Ajay Jain, Aniruddha Nrusimha, Amir Gholami, Pieter\n  Abbeel, Kurt Keutzer, Ion Stoica, Joseph E. Gonzalez", "title": "Checkmate: Breaking the Memory Wall with Optimal Tensor\n  Rematerialization", "comments": "In Proceedings of 3rd Conference Machine Learning and Systems 2020\n  (MLSys 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We formalize the problem of trading-off DNN training time and memory\nrequirements as the tensor rematerialization optimization problem, a\ngeneralization of prior checkpointing strategies. We introduce Checkmate, a\nsystem that solves for optimal rematerialization schedules in reasonable times\n(under an hour) using off-the-shelf MILP solvers or near-optimal schedules with\nan approximation algorithm, then uses these schedules to accelerate millions of\ntraining iterations. Our method scales to complex, realistic architectures and\nis hardware-aware through the use of accelerator-specific, profile-based cost\nmodels. In addition to reducing training cost, Checkmate enables real-world\nnetworks to be trained with up to 5.1x larger input sizes. Checkmate is an\nopen-source project, available at https://github.com/parasj/checkmate.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 07:54:06 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2020 17:57:45 GMT"}, {"version": "v3", "created": "Thu, 14 May 2020 17:46:43 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Jain", "Paras", ""], ["Jain", "Ajay", ""], ["Nrusimha", "Aniruddha", ""], ["Gholami", "Amir", ""], ["Abbeel", "Pieter", ""], ["Keutzer", "Kurt", ""], ["Stoica", "Ion", ""], ["Gonzalez", "Joseph E.", ""]]}, {"id": "1910.02706", "submitter": "Kaustav Bose", "authors": "Kaustav Bose, Ranendu Adhikary, Manash Kumar Kundu, Buddhadeb Sau", "title": "Arbitrary Pattern Formation by Opaque Fat Robots with Lights", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arbitrary Pattern Formation is a widely studied problem in autonomous robot\nsystems. The problem asks to design a distributed algorithm that moves a team\nof autonomous, anonymous and identical mobile robots to form any arbitrary\npattern given as input. The majority of the existing literature investigates\nthis problem for robots with unobstructed visibility. In a few recent works,\nthe problem has been studied in the obstructed visibility model, where the view\nof a robot can be obstructed by the presence of other robots. However, in these\nworks, the robots have been modelled as dimensionless points in the plane. In\nthis paper, we have considered the problem in the more realistic setting where\nthe robots have a physical extent. In particular, the robots are modelled as\nopaque disks. Furthermore, the robots operate under a fully asynchronous\nscheduler. They do not have access to any global coordinate system, but agree\non the direction and orientation of one coordinate axis. Each robot is equipped\nwith an externally visible light which can assume a constant number of\npredefined colors. In this setting, we have given a complete characterization\nof initial configurations from where any arbitrary pattern can be formed by a\ndeterministic distributed algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 10:31:31 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Bose", "Kaustav", ""], ["Adhikary", "Ranendu", ""], ["Kundu", "Manash Kumar", ""], ["Sau", "Buddhadeb", ""]]}, {"id": "1910.02794", "submitter": "Ben Wiederhake", "authors": "Saeed Akhoondian Amiri, Ben Wiederhake", "title": "Distributed Distance-$r$ Dominating Set on Sparse High-Girth Graphs", "comments": "23 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DM math.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The dominating set problem and its generalization, the distance-$r$\ndominating set problem, are among the well-studied problems in the sequential\nsettings. In distributed models of computation, unlike for domination, not much\nis known about distance-r domination. This is actually the case for other\nimportant closely-related covering problem, namely, the distance-$r$\nindependent set problem. By result of Kuhn et al. we know the distributed\ndomination problem is hard on high girth graphs; we study the problem on a\nslightly restricted subclass of these graphs: graphs of bounded expansion with\nhigh girth, i.e. their girth should be at least $4r + 3$. We show that in such\ngraphs, for every constant $r$, a simple greedy CONGEST algorithm provides a\nconstant-factor approximation of the minimum distance-$r$ dominating set\nproblem, in a constant number of rounds. More precisely, our constants are\ndependent to $r$, not to the size of the graph. This is the first algorithm\nthat shows there are non-trivial constant factor approximations in constant\nnumber of rounds for any distance $r$-covering problem in distributed settings.\nTo show the dependency on r is inevitable, we provide an unconditional lower\nbound showing the same problem is hard already on rings. We also show that our\nanalysis of the algorithm is relatively tight, that is any significant\nimprovement to the approximation factor requires new algorithmic ideas.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 13:50:50 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Amiri", "Saeed Akhoondian", ""], ["Wiederhake", "Ben", ""]]}, {"id": "1910.02803", "submitter": "Mohammed Khatiri", "authors": "Mohammed Khatiri, Denis Trystram and Fr\\'ed\\'eric Wagner", "title": "Work Stealing Simulator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present in this paper a Work Stealing lightweight PYTHON simulator. Our\nsimulator is used to execute an application (list of tasks with or without\ndependencies), on a multiple processors platform linked by specific topology.\nWe first give an overview of the different variants of the work stealing\nalgorithm, then we present the architecture of our light Work Stealing\nsimulator. Its architecture facilitates the development of other types of\napplications and other topologies for interconnecting the processors. We\npresent the use cases of the simulator and the different types of results.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 14:05:56 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Khatiri", "Mohammed", ""], ["Trystram", "Denis", ""], ["Wagner", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1910.02863", "submitter": "Ana Trisovic", "authors": "Ana Trisovic, Chris R. Jones, Ben Couturier, Marco Clemencic", "title": "Provenance tracking in the LHCb software", "comments": null, "journal-ref": "Computing in Science & Engineering ( Volume: 22 , Issue: 2 ,\n  March-April 2020 )", "doi": "10.1109/MCSE.2020.2970625", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even though computational reproducibility is widely accepted as necessary for\nresearch validation and reuse, it is often not considered during the research\nprocess. This is because reproducibility tools are typically stand-alone and\nrequire additional training to be employed. In this article, we present a\nsolution to foster reproducibility, which is integrated within existing\nscientific software that is actively used in the LHCb collaboration. Our\nprovenance tracking service captures metadata of a dataset, which is then saved\ninside the output data file on the disk. The captured information allows a\ncomplete understanding of how the file was produced and enables a user to\nreproduce the dataset, even when the original input code (that was used to\ninitially produce the dataset) is altered or lost. This article describes the\nimplementation of the service and gives examples of its application.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 18:00:09 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 22:18:33 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Trisovic", "Ana", ""], ["Jones", "Chris R.", ""], ["Couturier", "Ben", ""], ["Clemencic", "Marco", ""]]}, {"id": "1910.02874", "submitter": "Thorsten Wissmann", "authors": "Giorgio Audrito, Jacob Beal, Ferruccio Damiani, Danilo Pianini, Mirko\n  Viroli", "title": "Field-based Coordination with the Share Operator", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 16, Issue 4 (October\n  2, 2020) lmcs:6816", "doi": "10.23638/LMCS-16(4:1)2020", "report-no": null, "categories": "cs.DC cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Field-based coordination has been proposed as a model for coordinating\ncollective adaptive systems, promoting a view of distributed computations as\nfunctions manipulating data structures spread over space and evolving over\ntime, called computational fields. The field calculus is a formal foundation\nfor field computations, providing specific constructs for evolution (time) and\nneighbor interaction (space), which are handled by separate operators (called\nrep and nbr, respectively). This approach, however, intrinsically limits the\nspeed of information propagation that can be achieved by their combined use. In\nthis paper, we propose a new field-based coordination operator called share,\nwhich captures the space-time nature of field computations in a single operator\nthat declaratively achieves: (i) observation of neighbors' values; (ii)\nreduction to a single local value; and (iii) update and converse sharing to\nneighbors of a local variable. We show that for an important class of\nself-stabilising computations, share can replace all occurrences of rep and nbr\nconstructs. In addition to conceptual economy, use of the share operator also\nallows many prior field calculus algorithms to be greatly accelerated, which we\nvalidate empirically with simulations of frequently used network propagation\nand collection algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 15:54:03 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2020 16:15:57 GMT"}, {"version": "v3", "created": "Sat, 12 Sep 2020 08:33:19 GMT"}, {"version": "v4", "created": "Wed, 30 Sep 2020 19:45:45 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Audrito", "Giorgio", ""], ["Beal", "Jacob", ""], ["Damiani", "Ferruccio", ""], ["Pianini", "Danilo", ""], ["Viroli", "Mirko", ""]]}, {"id": "1910.03026", "submitter": "Deepak Puthal", "authors": "Devki Nandan Jha, Khaled Alwasel, Areeb Alshoshan, Xianghua Huang,\n  Ranesh Kumar Naha, Sudheer Kumar Battula, Saurabh Garg, Deepak Puthal, Philip\n  James, Albert Y. Zomaya, Schahram Dustdar and Rajiv Ranjan", "title": "IoTSim-Edge: A Simulation Framework for Modeling the Behaviour of IoT\n  and Edge Computing Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel simulator IoTSim-Edge, which captures the\nbehavior of heterogeneous IoT and edge computing infrastructure and allows\nusers to test their infrastructure and framework in an easy and configurable\nmanner. IoTSim-Edge extends the capability of CloudSim to incorporate the\ndifferent features of edge and IoT devices. The effectiveness of IoTSim-Edge is\ndescribed using three test cases. The results show the varying capability of\nIoTSim-Edge in terms of application composition, battery-oriented modeling,\nheterogeneous protocols modeling and mobility modeling along with the resources\nprovisioning for IoT applications.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 19:10:19 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Jha", "Devki Nandan", ""], ["Alwasel", "Khaled", ""], ["Alshoshan", "Areeb", ""], ["Huang", "Xianghua", ""], ["Naha", "Ranesh Kumar", ""], ["Battula", "Sudheer Kumar", ""], ["Garg", "Saurabh", ""], ["Puthal", "Deepak", ""], ["James", "Philip", ""], ["Zomaya", "Albert Y.", ""], ["Dustdar", "Schahram", ""], ["Ranjan", "Rajiv", ""]]}, {"id": "1910.03028", "submitter": "Guido Schryen", "authors": "Guido Schryen", "title": "Parallel computational optimization in operations research: A new\n  integrative framework, literature review and research directions", "comments": "submitted to an OR journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving optimization problems with parallel algorithms has a long tradition\nin OR. Its future relevance for solving hard optimization problems in many\nfields, including finance, logistics, production and design, is leveraged\nthrough the increasing availability of powerful computing capabilities.\nAcknowledging the existence of several literature reviews on parallel\noptimization, we did not find reviews that cover the most recent literature on\nthe parallelization of both exact and (meta)heuristic methods. However, in the\npast decade substantial advancements in parallel computing capabilities have\nbeen achieved and used by OR scholars so that an overview of modern parallel\noptimization in OR that accounts for these advancements is beneficial. Another\nissue from previous reviews results from their adoption of different foci so\nthat concepts used to describe and structure prior literature differ. This\nheterogeneity is accompanied by a lack of unifying frameworks for parallel\noptimization across methodologies, application fields and problems, and it has\nfinally led to an overall fragmented picture of what has been achieved and\nstill needs to be done in parallel optimization in OR. This review addresses\nthe aforementioned issues with three contributions: First, we suggest a new\nintegrative framework of parallel computational optimization across\noptimization problems, algorithms and application domains. The framework\nintegrates the perspectives of algorithmic design and computational\nimplementation of parallel optimization. Second, we apply the framework to\nsynthesize prior research on parallel optimization in OR, focusing on\ncomputational studies published in the period 2008-2017. Finally, we suggest\nresearch directions for parallel optimization in OR.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 13:50:52 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Schryen", "Guido", ""]]}, {"id": "1910.03060", "submitter": "Dibyajyoti Pati", "authors": "Dibyajyoti Pati, Caroline Favart, Purujit Bahl, Vivek Soni, Yun-chan\n  Tsai, Michael Potter, Jiahui Guan, Xiaomeng Dong, V. Ratna Saripalli", "title": "Impact of Inference Accelerators on hardware selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As opportunities for AI-assisted healthcare grow steadily, model deployment\nfaces challenges due to the specific characteristics of the industry. The\nconfiguration choice for a production device can impact model performance while\ninfluencing operational costs. Moreover, in healthcare some situations might\nrequire fast, but not real time, inference. We study different configurations\nand conduct a cost-performance analysis to determine the optimized hardware for\nthe deployment of a model subject to healthcare domain constraints. We observe\nthat a naive performance comparison may not lead to an optimal configuration\nselection. In fact, given realistic domain constraints, CPU execution might be\npreferable to GPU accelerators. Hence, defining beforehand precise expectations\nfor model deployment is crucial.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 20:06:38 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Pati", "Dibyajyoti", ""], ["Favart", "Caroline", ""], ["Bahl", "Purujit", ""], ["Soni", "Vivek", ""], ["Tsai", "Yun-chan", ""], ["Potter", "Michael", ""], ["Guan", "Jiahui", ""], ["Dong", "Xiaomeng", ""], ["Saripalli", "V. Ratna", ""]]}, {"id": "1910.03122", "submitter": "Zhuwei Qin", "authors": "Zhuwei Qin, Fuxun Yu, Xiang Chen", "title": "Task-Adaptive Incremental Learning for Intelligent Edge Devices", "comments": "2 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) are used for a wide range of\nimage-related tasks such as image classification and object detection. However,\na large pre-trained CNN model contains a lot of redundancy considering the\ntask-specific edge applications. Also, the statically pre-trained model could\nnot efficiently handle the dynamic data in the real-world application. The CNN\ntraining data and their labels are collected in an incremental manner. To\ntackle the above two challenges, we proposed TeAM a task-adaptive incremental\nlearning framework for CNNs in intelligent edge devices. Given a pre-trained\nlarge model, TeAM can configure it into any specialized model for dedicated\nedge applications. The specialized model can be quickly fine-tuned with local\ndata to achieve very high accuracy. Also, with our global aggregation and\nincremental learning scheme, the specialized CNN models can be collaboratively\naggregated to an enhanced global model with new training data.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 22:57:31 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Qin", "Zhuwei", ""], ["Yu", "Fuxun", ""], ["Chen", "Xiang", ""]]}, {"id": "1910.03280", "submitter": "Gabriele D'Angelo", "authors": "Mirko Zichichi, Stefano Ferretti, Gabriele D'Angelo", "title": "A Distributed Ledger Based Infrastructure for Smart Transportation\n  System and Social Good", "comments": "Proceedings of the IEEE Consumer Communications and Networking\n  Conference 2020 (CCNC 2020)", "journal-ref": null, "doi": "10.1109/CCNC46108.2020.9045640", "report-no": null, "categories": "cs.CR cs.CY cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a system architecture to promote the development of smart\ntransportation systems. Thanks to the use of distributed ledgers and related\ntechnologies, it is possible to create, store and share data generated by users\nthrough their sensors, while moving. In particular, IOTA and IPFS are used to\nstore and certify data (and their related metadata) coming from sensors or by\nthe users themselves. Ethereum is exploited as the smart contract platform that\ncoordinates the data sharing and provisioning. The necessary privacy guarantees\nare provided by the usage of Zero Knowledge Proof. We show some results\nobtained from some use case scenarios that demonstrate how such technologies\ncan be integrated to build novel smart services and to promote social good in\nuser mobility.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 08:53:18 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 06:19:41 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Zichichi", "Mirko", ""], ["Ferretti", "Stefano", ""], ["D'Angelo", "Gabriele", ""]]}, {"id": "1910.03281", "submitter": "Gabriele D'Angelo", "authors": "Gyordan Caminati, Sara Kiade, Gabriele D'Angelo, Stefano Ferretti,\n  Vittorio Ghini", "title": "Fast Session Resumption in DTLS for Mobile Communications", "comments": "Proceedings of the IEEE Consumer Communications and Networking\n  Conference 2020 (CCNC 2020)", "journal-ref": null, "doi": "10.1109/CCNC46108.2020.9045119", "report-no": null, "categories": "cs.NI cs.DC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DTLS is a protocol that provides security guarantees to Internet\ncommunications. It can operate on top of both TCP and UDP transport protocols.\nThus, it is particularly suited for peer-to-peer and distributed multimedia\napplications. The same holds if the endpoints are mobile devices. In this\nscenario, mechanisms are needed to surmount possible network disconnections,\noften arising due to the mobility or the scarce resources of devices, that can\njeopardize the quality of the communications. Session resumption is thus a main\nissue to deal with. To this aim, we propose a fast reconnection scheme that\nemploys non-connected sockets to quickly resume DTLS communication sessions.\nThe proposed scheme is assessed in a performance evaluation that confirms its\nviability.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 08:56:05 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 06:22:48 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Caminati", "Gyordan", ""], ["Kiade", "Sara", ""], ["D'Angelo", "Gabriele", ""], ["Ferretti", "Stefano", ""], ["Ghini", "Vittorio", ""]]}, {"id": "1910.03322", "submitter": "Piotr Dziurzanski", "authors": "Piotr Dziurzanski, Shuai Zhao, Leandro Soares Indrusiak", "title": "Integrated Process Planning and Scheduling in Commercial Smart Kitchens", "comments": "International Workshop on Reconfigurable and Communication-centric\n  Cyber-Physical Systems ReCoCyPS 2019 (arXiv:1909.05617)", "journal-ref": null, "doi": null, "report-no": "ReCoCyPS/2019/03", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the possibility of applying a generic, cloud-based\nOptimisation as a Service facility to food cooking planning and scheduling in a\ncommercial kitchen. We propose a chromosome encoding and customisation of the\nclassic MOEA/D multi-objective genetic algorithm. The applicability of the\nproposed approach is evaluated experimentally for two scenarios different with\nrespect to the number of cooking appliances and the amount of the ordered food.\nThe proposed system managed to determine the trade-offs between cooking time,\nenergy dissipation and food quality.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 10:30:30 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Dziurzanski", "Piotr", ""], ["Zhao", "Shuai", ""], ["Indrusiak", "Leandro Soares", ""]]}, {"id": "1910.03331", "submitter": "Piotr Dziurzanski", "authors": "Shuai Zhao, Piotr Dziurzanski, Leandro Soares Indrusiak", "title": "An XML-based Factory Description Language for Smart Manufacturing Plants\n  in Industry 4.0", "comments": "International Workshop on Reconfigurable and Communication-centric\n  Cyber-Physical Systems ReCoCyPS 2019 (arXiv:1909.05617)", "journal-ref": null, "doi": null, "report-no": "ReCoCyPS/2019/05", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Industry 4.0 revolution concerns the digital transformation of manufacturing\nand promises to answer the ever-increasing demand of product customisation and\nmanufacturing flexibility while incurring low costs. To perform the required\nfactory reconfiguration, a computationally demanding optimisation process has\nto be executed to find favourable solutions in a relatively short time. While\nprevious research focused on planning and scheduling of smart factories based\non cloud-based optimisation, little attention has been paid to effective\napproaches to describe the targeted factory, the required products and the\nproduction processes. However, these matters are fundamental for the\noptimisation engine to be correctly and efficiently performed. This paper\npresents an XML-based factory modelling language to effectively describe the\nabove data for a given factory and commodity order and to provide a convenient\ninterface for altering the input information. Finally, two real-world\nmanufacturing plants are provided to illustrate the feasibility of the proposed\ndescription language.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 10:46:17 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Zhao", "Shuai", ""], ["Dziurzanski", "Piotr", ""], ["Indrusiak", "Leandro Soares", ""]]}, {"id": "1910.03445", "submitter": "Isabelly Rocha", "authors": "Isabelly Rocha, Gabriel Vinha, Andrey Brito, Pascal Felber, Marcelo\n  Pasin, Valerio Schiavoni", "title": "ABEONA: an Architecture for Energy-Aware Task Migrations from the Edge\n  to the Cloud", "comments": "European Commission Project: LEGaTO - Low Energy Toolset for\n  Heterogeneous Computing (EC-H2020-780681)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents our preliminary results with ABEONA, an edge-to-cloud\narchitecture that allows migrating tasks from low-energy, resource-constrained\ndevices on the edge up to the cloud. Our preliminary results on artificial and\nreal world datasets show that it is possible to execute workloads in a more\nefficient manner energy-wise by scaling horizontally at the edge, without\nnegatively affecting the execution runtime.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 15:07:43 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Rocha", "Isabelly", ""], ["Vinha", "Gabriel", ""], ["Brito", "Andrey", ""], ["Felber", "Pascal", ""], ["Pasin", "Marcelo", ""], ["Schiavoni", "Valerio", ""]]}, {"id": "1910.03575", "submitter": "Gregor Ulm", "authors": "Gregor Ulm and Emil Gustavsson and Mats Jirstrand", "title": "Active-Code Replacement in the OODIDA Data Analytics Platform", "comments": "6 pages, 2 figures; Published in Euro-Par 2019: Parallel Processing\n  Workshops proceedings; DOI was added to the PDF. There is also an extended\n  version of this paper, cf. arXiv admin note: text overlap with\n  arXiv:1903.09477", "journal-ref": "Euro-Par 2019: Parallel Processing Workshops Proceedings (Springer\n  LNCS 11997)", "doi": "10.1007/978-3-030-48340-1_55", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  OODIDA (On-board/Off-board Distributed Data Analytics) is a platform for\ndistributing and executing concurrent data analytics tasks. It targets fleets\nof reference vehicles in the automotive industry and has a particular focus on\nrapid prototyping. Its underlying message-passing infrastructure has been\nimplemented in Erlang/OTP. External Python applications perform data analytics\ntasks. Most work is performed by clients (on-board). A central cloud server\nperforms supplementary tasks (off-board). OODIDA can be automatically packaged\nand deployed, which necessitates restarting parts of the system, or all of it.\nThis is potentially disruptive. To address this issue, we added the ability to\nexecute user-defined Python modules on clients as well as the server. These\nmodules can be replaced without restarting any part of the system and they can\neven be replaced between iterations of an ongoing assignment. This facilitates\nuse cases such as iterative A/B testing of machine learning algorithms or\nmodifying experimental algorithms on-the-fly.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 09:06:06 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 22:23:38 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Ulm", "Gregor", ""], ["Gustavsson", "Emil", ""], ["Jirstrand", "Mats", ""]]}, {"id": "1910.03679", "submitter": "Oded Green", "authors": "Oded Green, James Fox, Jeffrey Young, Jun Shirako, David Bader", "title": "Performance Impact of Memory Channels on Sparse and Irregular Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph processing is typically considered to be a memory-bound rather than\ncompute-bound problem. One common line of thought is that more available memory\nbandwidth corresponds to better graph processing performance. However, in this\nwork we demonstrate that the key factor in the utilization of the memory system\nfor graph algorithms is not necessarily the raw bandwidth or even the latency\nof memory requests. Instead, we show that performance is proportional to the\nnumber of memory channels available to handle small data transfers with limited\nspatial locality.\n  Using several widely used graph frameworks, including Gunrock (on the GPU)\nand GAPBS \\& Ligra (for CPUs), we evaluate key graph analytics kernels using\ntwo unique memory hierarchies, DDR-based and HBM/MCDRAM. Our results show that\nthe differences in the peak bandwidths of several Pascal-generation GPU memory\nsubsystems aren't reflected in the performance of various analytics.\nFurthermore, our experiments on CPU and Xeon Phi systems demonstrate that the\nnumber of memory channels utilized can be a decisive factor in performance\nacross several different applications. For CPU systems with smaller thread\ncounts, the memory channels can be underutilized while systems with high thread\ncounts can oversaturate the memory subsystem, which leads to limited\nperformance. Finally, we model the potential performance improvements of adding\nmore memory channels with narrower access widths than are found in current\nplatforms, and we analyze performance trade-offs for the two most prominent\ntypes of memory accesses found in graph algorithms, streaming and random\naccesses.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 20:39:14 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Green", "Oded", ""], ["Fox", "James", ""], ["Young", "Jeffrey", ""], ["Shirako", "Jun", ""], ["Bader", "David", ""]]}, {"id": "1910.03731", "submitter": "Vivek Sharma", "authors": "Vivek Sharma, Praneeth Vepakomma, Tristan Swedish, Ken Chang,\n  Jayashree Kalpathy-Cramer, Ramesh Raskar", "title": "ExpertMatcher: Automating ML Model Selection for Clients using Hidden\n  Representations", "comments": "In NeurIPS Workshop on Robust AI in Financial Services: Data,\n  Fairness, Explainability, Trustworthiness, and Privacy, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, there has been the development of Split Learning, a framework for\ndistributed computation where model components are split between the client and\nserver (Vepakomma et al., 2018b). As Split Learning scales to include many\ndifferent model components, there needs to be a method of matching client-side\nmodel components with the best server-side model components. A solution to this\nproblem was introduced in the ExpertMatcher (Sharma et al., 2019) framework,\nwhich uses autoencoders to match raw data to models. In this work, we propose\nan extension of ExpertMatcher, where matching can be performed without the need\nto share the client's raw data representation. The technique is applicable to\nsituations where there are local clients and centralized expert ML models, but\nthe sharing of raw data is constrained.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 00:42:16 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Sharma", "Vivek", ""], ["Vepakomma", "Praneeth", ""], ["Swedish", "Tristan", ""], ["Chang", "Ken", ""], ["Kalpathy-Cramer", "Jayashree", ""], ["Raskar", "Ramesh", ""]]}, {"id": "1910.03736", "submitter": "Vladyslav Oles", "authors": "Vladyslav Oles, Anton Kukushkin", "title": "BoolSi: a tool for distributed simulations and analysis of Boolean\n  networks", "comments": "Added asynchronous BNs to the introduction, added figure showing an\n  attractor, updated figure node correlations, explicitly mentioned if a figure\n  is BoolSi output, stated correlations of CK and TDIF on proliferation in case\n  study", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.MN cs.DC math.DS q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present BoolSi, an open-source cross-platform command line tool for\ndistributed simulations of deterministic Boolean networks with synchronous\nupdate. It uses MPI standard to support execution on computational clusters, as\nwell as parallel processing on a single computer. BoolSi can be used to model\nthe behavior of complex dynamic networks, such as gene regulatory networks. In\nparticular, it allows for identification and statistical analysis of network\nattractors. We perform a case study of the activity of a cambium cell to\ndemonstrate the capabilities of the tool.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 01:06:47 GMT"}, {"version": "v2", "created": "Fri, 18 Oct 2019 20:29:56 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Oles", "Vladyslav", ""], ["Kukushkin", "Anton", ""]]}, {"id": "1910.04032", "submitter": "David Bermbach", "authors": "Frank Pallas, Philip Raschke, David Bermbach", "title": "Fog Computing as Privacy Enabler", "comments": "Preprint, accepted for publication in IEEE Internet Computing. This\n  is the authors' own version before final copy-editing by IEEE", "journal-ref": null, "doi": "10.1109/MIC.2020.2979161", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite broad discussions on privacy challenges arising from fog computing,\nthe authors argue that privacy and security requirements might actually drive\nthe adoption of fog computing. They present four patterns of fog computing\nfostering data privacy and the security of business secrets, complementing\nexisting cryptographic approaches. Their practical application is illuminated\non the basis of three case studies.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 14:49:02 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 16:40:57 GMT"}, {"version": "v3", "created": "Fri, 13 Dec 2019 16:14:40 GMT"}, {"version": "v4", "created": "Tue, 3 Mar 2020 10:48:22 GMT"}, {"version": "v5", "created": "Fri, 13 Mar 2020 17:53:34 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Pallas", "Frank", ""], ["Raschke", "Philip", ""], ["Bermbach", "David", ""]]}, {"id": "1910.04041", "submitter": "Ramy E. Ali", "authors": "Ramy E. Ali, Bilgehan Erman, Ejder Ba\\c{s}tu\\u{g} and Bruce Cilli", "title": "Hierarchical Deep Double Q-Routing", "comments": "IEEE ICC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.IT cs.LG cs.MA math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores a deep reinforcement learning approach applied to the\npacket routing problem with high-dimensional constraints instigated by dynamic\nand autonomous communication networks. Our approach is motivated by the fact\nthat centralized path calculation approaches are often not scalable, whereas\nthe distributed approaches with locally acting nodes are not fully aware of the\nend-to-end performance. We instead hierarchically distribute the path\ncalculation over designated nodes in the network while taking into account the\nend-to-end performance. Specifically, we develop a hierarchical\ncluster-oriented adaptive per-flow path calculation mechanism by leveraging the\nDeep Double Q-network (DDQN) algorithm, where the end-to-end paths are\ncalculated by the source nodes with the assistance of cluster (group) leaders\nat different hierarchical levels. In our approach, a deferred composite reward\nis designed to capture the end-to-end performance through a feedback signal\nfrom the source nodes to the group leaders and captures the local network\nperformance through the local resource assessments by the group leaders. This\napproach scales in large networks, adapts to the dynamic demand, utilizes the\nnetwork resources efficiently and can be applied to segment routing.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 15:03:07 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 19:15:59 GMT"}, {"version": "v3", "created": "Tue, 3 Mar 2020 20:41:06 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Ali", "Ramy E.", ""], ["Erman", "Bilgehan", ""], ["Ba\u015ftu\u011f", "Ejder", ""], ["Cilli", "Bruce", ""]]}, {"id": "1910.04054", "submitter": "Olivier Delalleau", "authors": "Viswanath Sivakumar, Olivier Delalleau, Tim Rockt\\\"aschel, Alexander\n  H. Miller, Heinrich K\\\"uttler, Nantas Nardelli, Mike Rabbat, Joelle Pineau,\n  Sebastian Riedel", "title": "MVFST-RL: An Asynchronous RL Framework for Congestion Control with\n  Delayed Actions", "comments": "Workshop on ML for Systems at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective network congestion control strategies are key to keeping the\nInternet (or any large computer network) operational. Network congestion\ncontrol has been dominated by hand-crafted heuristics for decades. Recently,\nReinforcementLearning (RL) has emerged as an alternative to automatically\noptimize such control strategies. Research so far has primarily considered RL\ninterfaces which block the sender while an agent considers its next action.\nThis is largely an artifact of building on top of frameworks designed for RL in\ngames (e.g. OpenAI Gym). However, this does not translate to real-world\nnetworking environments, where a network sender waiting on a policy without\nsending data leads to under-utilization of bandwidth. We instead propose to\nformulate congestion control with an asynchronous RL agent that handles delayed\nactions. We present MVFST-RL, a scalable framework for congestion control in\nthe QUIC transport protocol that leverages state-of-the-art in asynchronous RL\ntraining with off-policy correction. We analyze modeling improvements to\nmitigate the deviation from Markovian dynamics, and evaluate our method on\nemulated networks from the Pantheon benchmark platform. The source code is\npublicly available at https://github.com/facebookresearch/mvfst-rl.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 15:12:30 GMT"}, {"version": "v2", "created": "Sat, 12 Oct 2019 14:49:47 GMT"}, {"version": "v3", "created": "Wed, 30 Oct 2019 17:02:53 GMT"}, {"version": "v4", "created": "Wed, 26 May 2021 21:52:10 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Sivakumar", "Viswanath", ""], ["Delalleau", "Olivier", ""], ["Rockt\u00e4schel", "Tim", ""], ["Miller", "Alexander H.", ""], ["K\u00fcttler", "Heinrich", ""], ["Nardelli", "Nantas", ""], ["Rabbat", "Mike", ""], ["Pineau", "Joelle", ""], ["Riedel", "Sebastian", ""]]}, {"id": "1910.04223", "submitter": "Renan Souza", "authors": "Renan Souza, Leonardo Azevedo, V\\'itor Louren\\c{c}o, Elton Soares,\n  Raphael Thiago, Rafael Brand\\~ao, Daniel Civitarese, Emilio Vital Brazil,\n  Marcio Moreno, Patrick Valduriez, Marta Mattoso, Renato Cerqueira, Marco A.\n  S. Netto", "title": "Provenance Data in the Machine Learning Lifecycle in Computational\n  Science and Engineering", "comments": "10 pages, 7 figures, Accepted at Workflows in Support of Large-scale\n  Science (WORKS) co-located with the ACM/IEEE International Conference for\n  High Performance Computing, Networking, Storage, and Analysis (SC) 2019,\n  Denver, Colorado", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning (ML) has become essential in several industries. In\nComputational Science and Engineering (CSE), the complexity of the ML lifecycle\ncomes from the large variety of data, scientists' expertise, tools, and\nworkflows. If data are not tracked properly during the lifecycle, it becomes\nunfeasible to recreate a ML model from scratch or to explain to stakeholders\nhow it was created. The main limitation of provenance tracking solutions is\nthat they cannot cope with provenance capture and integration of domain and ML\ndata processed in the multiple workflows in the lifecycle while keeping the\nprovenance capture overhead low. To handle this problem, in this paper we\ncontribute with a detailed characterization of provenance data in the ML\nlifecycle in CSE; a new provenance data representation, called PROV-ML, built\non top of W3C PROV and ML Schema; and extensions to a system that tracks\nprovenance from multiple workflows to address the characteristics of ML and\nCSE, and to allow for provenance queries with a standard vocabulary. We show a\npractical use in a real case in the Oil and Gas industry, along with its\nevaluation using 48 GPUs in parallel.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 19:52:40 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2019 15:19:18 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Souza", "Renan", ""], ["Azevedo", "Leonardo", ""], ["Louren\u00e7o", "V\u00edtor", ""], ["Soares", "Elton", ""], ["Thiago", "Raphael", ""], ["Brand\u00e3o", "Rafael", ""], ["Civitarese", "Daniel", ""], ["Brazil", "Emilio Vital", ""], ["Moreno", "Marcio", ""], ["Valduriez", "Patrick", ""], ["Mattoso", "Marta", ""], ["Cerqueira", "Renato", ""], ["Netto", "Marco A. S.", ""]]}, {"id": "1910.04436", "submitter": "Johannes de Fine Licht", "authors": "Johannes de Fine Licht, Torsten Hoefler", "title": "hlslib: Software Engineering for Hardware Design", "comments": "4 pages extended abstract accepted to H2RC'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-level synthesis (HLS) tools have brought FPGA development into the\nmainstream, by allowing programmers to design architectures using familiar\nlanguages such as C, C++, and OpenCL. While the move to these languages has\nbrought significant benefits, many aspects of traditional software engineering\nare still unsupported, or not exploited by developers in practice. Furthermore,\ndesigning reconfigurable architectures requires support for hardware\nconstructs, such as FIFOs and shift registers, that are not native to\nCPU-oriented languages. To address this gap, we have developed hlslib, a\ncollection of software tools, plug-in hardware modules, and code samples,\ndesigned to enhance the productivity of HLS developers. The goal of hlslib is\ntwo-fold: first, create a community-driven arena of bleeding edge development,\nwhich can move quicker, and provides more powerful abstractions than what is\nprovided by vendors; and second, collect a wide range of example codes, both\nminimal proofs of concept, and larger, real-world applications, that can be\nreused directly or inspire other work. hlslib is offered as an open source\nlibrary, containing CMake files, C++ headers, convenience scripts, and examples\ncodes, and is receptive to any contribution that can benefit HLS developers,\nthrough general functionality or examples.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 08:44:41 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Licht", "Johannes de Fine", ""], ["Hoefler", "Torsten", ""]]}, {"id": "1910.04493", "submitter": "Kevin Gomez", "authors": "Kevin Gomez, Matthias T\\\"aschner, M. Ali Rostami, Christopher Rost,\n  Erhard Rahm", "title": "Graph Sampling with Distributed In-Memory Dataflow Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a large graph, a graph sample determines a subgraph with similar\ncharacteristics for certain metrics of the original graph. The samples are much\nsmaller thereby accelerating and simplifying the analysis and visualization of\nlarge graphs. We focus on the implementation of distributed graph sampling for\nBig Data frameworks and in-memory dataflow systems such as Apache Spark or\nApache Flink. We evaluate the scalability of the new implementations and\nanalyze to what degree the sampling approaches preserve certain graph metrics\ncompared to the original graph. The latter analysis also uses comparative graph\nvisualizations. The presented methods will be open source and be integrated\ninto Gradoop, a system for distributed graph analytics.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 11:44:59 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Gomez", "Kevin", ""], ["T\u00e4schner", "Matthias", ""], ["Rostami", "M. Ali", ""], ["Rost", "Christopher", ""], ["Rahm", "Erhard", ""]]}, {"id": "1910.04517", "submitter": "Khaled Alwasel", "authors": "Khaled Alwasel, Rodrigo N. Calheiros, Saurabh Garg, Rajkumar Buyya,\n  Rajiv Ranjan", "title": "BigDataSDNSim: A Simulator for Analyzing Big Data Applications in\n  Software-Defined Cloud Data Centers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emerging paradigms of big data and Software-Defined Networking (SDN) in cloud\ndata centers have gained significant attention from industry and academia. The\nintegration and coordination of big data and SDN are required to improve the\napplication and network performance of big data applications. While empirical\nevaluation and analysis of big data and SDN can be one way of observing\nproposed solutions, it is often impractical or difficult to apply for several\nreasons, such as expensive undertakings, time consuming, and complexity; in\naddition, it is beyond the reach of most individuals. Thus, simulation tools\nare preferable options for performing costeffective, scalable experimentation\nin a controllable, repeatable, and configurable manner. To fill this gap, we\npresent a new, self-contained simulation tool named BigDataSDNSim that enables\nthe modeling and simulating of big data management systems (YARN), big data\nprogramming models (MapReduce), and SDN-enabled networks within cloud computing\nenvironments. To demonstrate the efficacy, effectiveness, and features of\nBigDataSDNSim, a use-case that compares SDN-enabled networks with legacy\nnetworks in terms of the performance and energy consumption is presented.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 12:38:03 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Alwasel", "Khaled", ""], ["Calheiros", "Rodrigo N.", ""], ["Garg", "Saurabh", ""], ["Buyya", "Rajkumar", ""], ["Ranjan", "Rajiv", ""]]}, {"id": "1910.04600", "submitter": "Martin Helfrich", "authors": "Michael Blondin, Javier Esparza, Blaise Genest, Martin Helfrich and\n  Stefan Jaax", "title": "Succinct Population Protocols for Presburger Arithmetic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC cs.LO cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Angluin et al. proved that population protocols compute exactly the\npredicates definable in Presburger arithmetic (PA), the first-order theory of\naddition. As part of this result, they presented a procedure that translates\nany formula $\\varphi$ of quantifier-free PA with remainder predicates (which\nhas the same expressive power as full PA) into a population protocol with\n$2^{O(\\text{poly}(|\\varphi|))}$ states that computes $\\varphi$. More precisely,\nthe number of states of the protocol is exponential in both the bit length of\nthe largest coefficient in the formula, and the number of nodes of its syntax\ntree.\n  In this paper, we prove that every formula $\\varphi$ of quantifier-free PA\nwith remainder predicates is computable by a leaderless population protocol\nwith $O(\\text{poly}(|\\varphi|))$ states. Our proof is based on several new\nconstructions, which may be of independent interest. Given a formula $\\varphi$\nof quantifier-free PA with remainder predicates, a first construction produces\na succinct protocol (with $O(|\\varphi|^3)$ leaders) that computes $\\varphi$;\nthis completes the work initiated in [STACS'18], where we constructed such\nprotocols for a fragment of PA. For large enough inputs, we can get rid of\nthese leaders. If the input is not large enough, then it is small, and we\ndesign another construction producing a succinct protocol with one leader that\ncomputes $\\varphi$. Our last construction gets rid of this leader for small\ninputs.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 14:28:13 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 15:07:23 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Blondin", "Michael", ""], ["Esparza", "Javier", ""], ["Genest", "Blaise", ""], ["Helfrich", "Martin", ""], ["Jaax", "Stefan", ""]]}, {"id": "1910.04796", "submitter": "Alfio Lazzaro", "authors": "Ilia Sivkov, Alfio Lazzaro, Juerg Hutter", "title": "DBCSR: A Library for Dense Matrix Multiplications on Distributed\n  GPU-Accelerated Systems", "comments": "5 pages, 4 figures, proceeding of the 2019 International\n  Multi-Conference on Engineering, Computer and Information Sciences (SIBIRCON)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most, if not all the modern scientific simulation packages utilize matrix\nalgebra operations. Among the operation of the linear algebra, one of the most\nimportant kernels is the multiplication of matrices, dense and sparse. Examples\nof application of such a kernel are in electronic structure calculations,\nmachine learning, data mining, graph processing, and digital signal processing.\nSeveral optimized libraries exist that can achieve high-performance on\ndistributed systems. Only a few of them target distributed GPU-accelerated\nsystems. In most of the cases, these libraries are provided and optimized by\nsystem vendors for their specific computer systems. In this paper, we present\nthe DBCSR library (Distributed Block Compressed Sparse Row) for the distributed\ndense matrix-matrix multiplications. Although the library is specifically\ndesigned for block-sparse matrix-matrix multiplications, we optimized it for\nthe dense case on GPU-accelerated systems. We show that the DBCSR outperforms\nthe multiplication of matrices of different sizes and shapes provided by a\nvendor optimized GPU version of the ScaLAPACK library up to 2.5x (1.4x on\naverage).\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 18:23:26 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Sivkov", "Ilia", ""], ["Lazzaro", "Alfio", ""], ["Hutter", "Juerg", ""]]}, {"id": "1910.04940", "submitter": "Guanhua Wang", "authors": "Guanhua Wang, Shivaram Venkataraman, Amar Phanishayee, Jorgen Thelin,\n  Nikhil Devanur, Ion Stoica", "title": "Blink: Fast and Generic Collectives for Distributed ML", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model parameter synchronization across GPUs introduces high overheads for\ndata-parallel training at scale. Existing parameter synchronization protocols\ncannot effectively leverage available network resources in the face of ever\nincreasing hardware heterogeneity. To address this, we propose Blink, a\ncollective communication library that dynamically generates optimal\ncommunication primitives by packing spanning trees. We propose techniques to\nminimize the number of trees generated and extend Blink to leverage\nheterogeneous communication channels for faster data transfers. Evaluations\nshow that compared to the state-of-the-art (NCCL), Blink can achieve up to 8x\nfaster model synchronization, and reduce end-to-end training time for image\nclassification tasks by up to 40%.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 02:13:34 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Wang", "Guanhua", ""], ["Venkataraman", "Shivaram", ""], ["Phanishayee", "Amar", ""], ["Thelin", "Jorgen", ""], ["Devanur", "Nikhil", ""], ["Stoica", "Ion", ""]]}, {"id": "1910.04991", "submitter": "Santhilata Kuppili Venkata", "authors": "Santhilata Kuppili Venkata and Katarzyna Musial", "title": "Sub-query Fragmentation for Query Analysis and Data Caching in the\n  Distributed Environment", "comments": "29 pages, 18 figures, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When data stores and users are distributed geographically, it is essential to\norganize distributed data cache points at ideal locations to minimize data\ntransfers. To answer this, we are developing an adaptive distributed data\ncaching framework that can identify suitable data chunks to cache and move\nacross a network of community cache locations.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 06:41:09 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Venkata", "Santhilata Kuppili", ""], ["Musial", "Katarzyna", ""]]}, {"id": "1910.05104", "submitter": "Igor Colin", "authors": "Igor Colin, Ludovic Dos Santos, Kevin Scaman", "title": "Theoretical Limits of Pipeline Parallel Optimization and Application to\n  Distributed Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the theoretical limits of pipeline parallel learning of deep\nlearning architectures, a distributed setup in which the computation is\ndistributed per layer instead of per example. For smooth convex and non-convex\nobjective functions, we provide matching lower and upper complexity bounds and\nshow that a naive pipeline parallelization of Nesterov's accelerated gradient\ndescent is optimal. For non-smooth convex functions, we provide a novel\nalgorithm coined Pipeline Parallel Random Smoothing (PPRS) that is within a\n$d^{1/4}$ multiplicative factor of the optimal convergence rate, where $d$ is\nthe underlying dimension. While the convergence rate still obeys a slow\n$\\varepsilon^{-2}$ convergence rate, the depth-dependent part is accelerated,\nresulting in a near-linear speed-up and convergence time that only slightly\ndepends on the depth of the deep learning architecture. Finally, we perform an\nempirical analysis of the non-smooth non-convex case and show that, for\ndifficult and highly non-smooth problems, PPRS outperforms more traditional\noptimization algorithms such as gradient descent and Nesterov's accelerated\ngradient descent for problems where the sample size is limited, such as\nfew-shot or adversarial learning.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 12:18:56 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Colin", "Igor", ""], ["Santos", "Ludovic Dos", ""], ["Scaman", "Kevin", ""]]}, {"id": "1910.05106", "submitter": "Simon Peter", "authors": "Thomas E. Anderson, Marco Canini, Jongyul Kim, Dejan Kosti\\'c,\n  Youngjin Kwon, Simon Peter, Waleed Reda, Henry N. Schuh, Emmett Witchel", "title": "Assise: Performance and Availability via NVM Colocation in a Distributed\n  File System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The adoption of very low latency persistent memory modules (PMMs) upends the\nlong-established model of disaggregated file system access. Instead, by\ncolocating computation and PMM storage, we can provide applications much higher\nI/O performance, sub-second application failover, and strong consistency. To\ndemonstrate this, we built the Assise distributed file system, based on a\npersistent, replicated coherence protocol for managing a set of\nserver-colocated PMMs as a fast, crash-recoverable cache between applications\nand slower disaggregated storage, such as SSDs. Unlike disaggregated file\nsystems, Assise maximizes locality for all file IO by carrying out IO on\ncolocated PMM whenever possible and minimizes coherence overhead by maintaining\nconsistency at IO operation granularity, rather than at fixed block sizes.\n  We compare Assise to Ceph/Bluestore, NFS, and Octopus on a cluster with Intel\nOptane DC PMMs and SSDs for common cloud applications and benchmarks, such as\nLevelDB, Postfix, and FileBench. We find that Assise improves write latency up\nto 22x, throughput up to 56x, fail-over time up to 103x, and scales up to 6x\nbetter than its counterparts, while providing stronger consistency semantics.\nAssise promises to beat the MinuteSort world record by 1.5x.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 02:13:19 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 02:54:13 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Anderson", "Thomas E.", ""], ["Canini", "Marco", ""], ["Kim", "Jongyul", ""], ["Kosti\u0107", "Dejan", ""], ["Kwon", "Youngjin", ""], ["Peter", "Simon", ""], ["Reda", "Waleed", ""], ["Schuh", "Henry N.", ""], ["Witchel", "Emmett", ""]]}, {"id": "1910.05109", "submitter": "Daniel Lemire", "authors": "Wojciech Mu{\\l}a, Daniel Lemire", "title": "Base64 encoding and decoding at almost the speed of a memory copy", "comments": null, "journal-ref": "Software: Practice and Experience 50 (2), 2020", "doi": "10.1002/spe.2777", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many common document formats on the Internet are text-only such as email\n(MIME) and the Web (HTML, JavaScript, JSON and XML). To include images or\nexecutable code in these documents, we first encode them as text using base64.\nStandard base64 encoding uses 64~ASCII characters: both lower and upper case\nLatin letters, digits and two other symbols. We show how we can encode and\ndecode base64 data at nearly the speed of a memory copy (memcpy) on recent\nIntel processors, as long as the data does not fit in the first-level (L1)\ncache. We use the SIMD (Single Instruction Multiple Data) instruction set\nAVX-512 available on commodity processors. Our implementation generates several\ntimes fewer instructions than previous SIMD-accelerated base64 codecs. It is\nalso more versatile, as it can be adapted---even at runtime---to any base64\nvariant by only changing constants.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 15:39:28 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Mu\u0142a", "Wojciech", ""], ["Lemire", "Daniel", ""]]}, {"id": "1910.05124", "submitter": "Christopher De Sa", "authors": "Bowen Yang, Jian Zhang, Jonathan Li, Christopher R\\'e, Christopher R.\n  Aberger and Christopher De Sa", "title": "PipeMare: Asynchronous Pipeline Parallel DNN Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pipeline parallelism (PP) when training neural networks enables larger models\nto be partitioned spatially, leading to both lower network communication and\noverall higher hardware utilization. Unfortunately, to preserve the statistical\nefficiency of sequential training, existing PP techniques sacrifice hardware\nefficiency by decreasing pipeline utilization or incurring extra memory costs.\nIn this paper, we investigate to what extent these sacrifices are necessary. We\ndevise PipeMare, a simple yet robust training method that tolerates\nasynchronous updates during PP execution without sacrificing utilization or\nmemory, which allows efficient use of fine-grained pipeline parallelism.\nConcretely, when tested on ResNet and Transformer networks, asynchrony enables\nPipeMare to use up to $2.7\\times$ less memory or get $4.3\\times$ higher\npipeline utilization, with similar model quality, when compared to\nstate-of-the-art synchronous PP training techniques.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 19:20:24 GMT"}, {"version": "v2", "created": "Sun, 9 Feb 2020 01:34:36 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Yang", "Bowen", ""], ["Zhang", "Jian", ""], ["Li", "Jonathan", ""], ["R\u00e9", "Christopher", ""], ["Aberger", "Christopher R.", ""], ["De Sa", "Christopher", ""]]}, {"id": "1910.05316", "submitter": "Xu Chen", "authors": "En Li and Liekang Zeng and Zhi Zhou and Xu Chen", "title": "Edge AI: On-Demand Accelerating Deep Neural Network Inference via Edge\n  Computing", "comments": "Accepted by IEEE Transactions on Wireless Communications, Sept 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.CV cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a key technology of enabling Artificial Intelligence (AI) applications in\n5G era, Deep Neural Networks (DNNs) have quickly attracted widespread\nattention. However, it is challenging to run computation-intensive DNN-based\ntasks on mobile devices due to the limited computation resources. What's worse,\ntraditional cloud-assisted DNN inference is heavily hindered by the significant\nwide-area network latency, leading to poor real-time performance as well as low\nquality of user experience. To address these challenges, in this paper, we\npropose Edgent, a framework that leverages edge computing for DNN collaborative\ninference through device-edge synergy. Edgent exploits two design knobs: (1)\nDNN partitioning that adaptively partitions computation between device and edge\nfor purpose of coordinating the powerful cloud resource and the proximal edge\nresource for real-time DNN inference; (2) DNN right-sizing that further reduces\ncomputing latency via early exiting inference at an appropriate intermediate\nDNN layer. In addition, considering the potential network fluctuation in\nreal-world deployment, Edgentis properly design to specialize for both static\nand dynamic network environment. Specifically, in a static environment where\nthe bandwidth changes slowly, Edgent derives the best configurations with the\nassist of regression-based prediction models, while in a dynamic environment\nwhere the bandwidth varies dramatically, Edgent generates the best execution\nplan through the online change point detection algorithm that maps the current\nbandwidth state to the optimal configuration. We implement Edgent prototype\nbased on the Raspberry Pi and the desktop PC and the extensive experimental\nevaluations demonstrate Edgent's effectiveness in enabling on-demand\nlow-latency edge intelligence.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 00:53:44 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Li", "En", ""], ["Zeng", "Liekang", ""], ["Zhou", "Zhi", ""], ["Chen", "Xu", ""]]}, {"id": "1910.05339", "submitter": "Chetan Bansal", "authors": "Chetan Bansal, Sundararajan Renganathan, Ashima Asudani, Olivier Midy,\n  Mathru Janakiraman", "title": "DeCaf: Diagnosing and Triaging Performance Issues in Large-Scale Cloud\n  Services", "comments": "To be published in the proceedings of ICSE-SEIP '20, Seoul, Republic\n  of Korea", "journal-ref": null, "doi": "10.1145/3377813.3381353", "report-no": null, "categories": "cs.DC cs.SE cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale cloud services use Key Performance Indicators (KPIs) for tracking\nand monitoring performance. They usually have Service Level Objectives (SLOs)\nbaked into the customer agreements which are tied to these KPIs. Dependency\nfailures, code bugs, infrastructure failures, and other problems can cause\nperformance regressions. It is critical to minimize the time and manual effort\nin diagnosing and triaging such issues to reduce customer impact. Large volume\nof logs and mixed type of attributes (categorical, continuous) in the logs\nmakes diagnosis of regressions non-trivial.\n  In this paper, we present the design, implementation and experience from\nbuilding and deploying DeCaf, a system for automated diagnosis and triaging of\nKPI issues using service logs. It uses machine learning along with pattern\nmining to help service owners automatically root cause and triage performance\nissues. We present the learnings and results from case studies on two large\nscale cloud services in Microsoft where DeCaf successfully diagnosed 10 known\nand 31 unknown issues. DeCaf also automatically triages the identified issues\nby leveraging historical data. Our key insights are that for any such diagnosis\ntool to be effective in practice, it should a) scale to large volumes of\nservice logs and attributes, b) support different types of KPIs and ranking\nfunctions, c) be integrated into the DevOps processes.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 21:49:22 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 17:23:39 GMT"}, {"version": "v3", "created": "Sat, 21 Dec 2019 16:01:25 GMT"}, {"version": "v4", "created": "Wed, 1 Jan 2020 21:49:56 GMT"}, {"version": "v5", "created": "Sun, 2 Feb 2020 08:19:52 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Bansal", "Chetan", ""], ["Renganathan", "Sundararajan", ""], ["Asudani", "Ashima", ""], ["Midy", "Olivier", ""], ["Janakiraman", "Mathru", ""]]}, {"id": "1910.05340", "submitter": "Skanda Koppula", "authors": "Skanda Koppula, Lois Orosa, Abdullah Giray Ya\\u{g}l{\\i}k\\c{c}{\\i},\n  Roknoddin Azizi, Taha Shahroodi, Konstantinos Kanellopoulos, Onur Mutlu", "title": "EDEN: Enabling Energy-Efficient, High-Performance Deep Neural Network\n  Inference Using Approximate DRAM", "comments": "This work is to appear at MICRO 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effectiveness of deep neural networks (DNN) in vision, speech, and\nlanguage processing has prompted a tremendous demand for energy-efficient\nhigh-performance DNN inference systems. Due to the increasing memory intensity\nof most DNN workloads, main memory can dominate the system's energy consumption\nand stall time. One effective way to reduce the energy consumption and increase\nthe performance of DNN inference systems is by using approximate memory, which\noperates with reduced supply voltage and reduced access latency parameters that\nviolate standard specifications. Using approximate memory reduces reliability,\nleading to higher bit error rates. Fortunately, neural networks have an\nintrinsic capacity to tolerate increased bit errors. This can enable\nenergy-efficient and high-performance neural network inference using\napproximate DRAM devices.\n  Based on this observation, we propose EDEN, a general framework that reduces\nDNN energy consumption and DNN evaluation latency by using approximate DRAM\ndevices, while strictly meeting a user-specified target DNN accuracy. EDEN\nrelies on two key ideas: 1) retraining the DNN for a target approximate DRAM\ndevice to increase the DNN's error tolerance, and 2) efficient mapping of the\nerror tolerance of each individual DNN data type to a corresponding approximate\nDRAM partition in a way that meets the user-specified DNN accuracy\nrequirements.\n  We evaluate EDEN on multi-core CPUs, GPUs, and DNN accelerators with error\nmodels obtained from real approximate DRAM devices. For a target accuracy\nwithin 1% of the original DNN, our results show that EDEN enables 1) an average\nDRAM energy reduction of 21%, 37%, 31%, and 32% in CPU, GPU, and two DNN\naccelerator architectures, respectively, across a variety of DNNs, and 2) an\naverage (maximum) speedup of 8% (17%) and 2.7% (5.5%) in CPU and GPU\narchitectures, respectively, when evaluating latency-bound DNNs.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 00:56:59 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Koppula", "Skanda", ""], ["Orosa", "Lois", ""], ["Ya\u011fl\u0131k\u00e7\u0131", "Abdullah Giray", ""], ["Azizi", "Roknoddin", ""], ["Shahroodi", "Taha", ""], ["Kanellopoulos", "Konstantinos", ""], ["Mutlu", "Onur", ""]]}, {"id": "1910.05341", "submitter": "Sebastian Scholze", "authors": "Sebastian Scholze, Fulya Feryal Horozal, Marie-Saphira Flug, Ana\n  Teresa Correia", "title": "Persistence and Big Data Analytics Architectures for Smart Connected\n  Vehicles", "comments": "International Workshop on Reconfigurable and Communicationcentric\n  Cyber-Physical Systems ReCoCyPS 2019 (arXiv:1909.05617)", "journal-ref": null, "doi": null, "report-no": "ReCoCyPS/2019/04", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Up until recently, relational databases were considered as the de-facto\ntechnology for persisting and managing large volumes of data. This came to\nchange with the emergence of enterprises producing extremely large datasets and\nhaving unprecedentedly high availability requirements. The need for levels of\navailability beyond those supported by relational databases and the challenges\ninvolved in scaling such databases horizontally led to the emergence of a new\ngeneration of purpose-specific databases grouped under the term NoSQL. The\nNoSQL databases despite being designed with horizontal scalability as a primary\nconcern and deliver increased availability and fault-tolerance, they have the\ncost of temporary inconsistency and reduced durability of data. The present\npaper presents a concept and application of how TYPHON aims to face the\nchallenges emerging from the attempts to balance the best world of relational\nand NoSQL databases, by supporting the migration towards hybrid data\npersistence architectures.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 13:26:42 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Scholze", "Sebastian", ""], ["Horozal", "Fulya Feryal", ""], ["Flug", "Marie-Saphira", ""], ["Correia", "Ana Teresa", ""]]}, {"id": "1910.05357", "submitter": "Sebastian Scholze", "authors": "Sebastian Scholze, Rebecca Siafaka, Kevin Nagorny, Albert Zilverberg,\n  Karl Krone", "title": "Situation-aware Re-configuration of Production Processes", "comments": "International Workshop on Reconfigurable and Communicationcentric\n  Cyber-Physical Systems ReCoCyPS 2019 (arXiv:1909.05617)", "journal-ref": null, "doi": null, "report-no": "ReCoCyPS/2019/01", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manufacturing of products, nowadays, due to the advances in Information\nTechnologies (IT) that turn products into smart devices, and the evolution in\ndevice connectivity, becomes increasingly complex. Connected product networks\n(CPN) and cyber-physical systems (CPS) are bringing new challenges to\nmanufacturing companies. These challenges, among others things, lead to\nincreased demands for optimisation and personalisation of production processes.\nThis move, which as a result drives the market of machines and products, poses\nnew challenges to the manufacturing domain that needs flexible solutions to\nreplace the rigid traditional methodologies and tools. The current work\nsuggests a cloud-based platform for optimisation of machines, products and\nprocesses, allowing for situational awareness on the users' site, involving\ntechnologies for event prediction and optimisation.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 18:12:29 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Scholze", "Sebastian", ""], ["Siafaka", "Rebecca", ""], ["Nagorny", "Kevin", ""], ["Zilverberg", "Albert", ""], ["Krone", "Karl", ""]]}, {"id": "1910.05385", "submitter": "Soheil Behnezhad", "authors": "Soheil Behnezhad, Laxman Dhulipala, Hossein Esfandiari, Jakub\n  {\\L}\\k{a}cki, Vahab Mirrokni", "title": "Near-Optimal Massively Parallel Graph Connectivity", "comments": "A preliminary version of this paper is to appear in the proceedings\n  of The 60th Annual IEEE Symposium on Foundations of Computer Science (FOCS\n  2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying the connected components of a graph, apart from being a\nfundamental problem with countless applications, is a key primitive for many\nother algorithms. In this paper, we consider this problem in parallel settings.\nParticularly, we focus on the Massively Parallel Computations (MPC) model,\nwhich is the standard theoretical model for modern parallel frameworks such as\nMapReduce, Hadoop, or Spark. We consider the truly sublinear regime of MPC for\ngraph problems where the space per machine is $n^\\delta$ for some desirably\nsmall constant $\\delta \\in (0, 1)$.\n  We present an algorithm that for graphs with diameter $D$ in the wide range\n$[\\log^{\\epsilon} n, n]$, takes $O(\\log D)$ rounds to identify the connected\ncomponents and takes $O(\\log \\log n)$ rounds for all other graphs. The\nalgorithm is randomized, succeeds with high probability, does not require prior\nknowledge of $D$, and uses an optimal total space of $O(m)$. We complement this\nby showing a conditional lower-bound based on the widely believed TwoCycle\nconjecture that $\\Omega(\\log D)$ rounds are indeed necessary in this setting.\n  Studying parallel connectivity algorithms received a resurgence of interest\nafter the pioneering work of Andoni et al. [FOCS 2018] who presented an\nalgorithm with $O(\\log D \\cdot \\log \\log n)$ round-complexity. Our algorithm\nimproves this result for the whole range of values of $D$ and almost settles\nthe problem due to the conditional lower-bound.\n  Additionally, we show that with minimal adjustments, our algorithm can also\nbe implemented in a variant of the (CRCW) PRAM in asymptotically the same\nnumber of rounds.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 19:51:13 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2020 23:51:43 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Behnezhad", "Soheil", ""], ["Dhulipala", "Laxman", ""], ["Esfandiari", "Hossein", ""], ["\u0141\u0105cki", "Jakub", ""], ["Mirrokni", "Vahab", ""]]}, {"id": "1910.05433", "submitter": "Jie Su", "authors": "Bin Qian, Jie Su, Zhenyu Wen, Devki Nandan Jha, Yinhao Li, Yu Guan,\n  Deepak Puthal, Philip James, Renyu Yang, Albert Y. Zomaya, Omer Rana, Lizhe\n  Wang, Maciej Koutny, Rajiv Ranjan", "title": "Orchestrating the Development Lifecycle of Machine Learning-Based IoT\n  Applications: A Taxonomy and Survey", "comments": "50 pages, Accepted by ACM Computing Surveys (CSUR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning (ML) and Internet of Things (IoT) are complementary\nadvances: ML techniques unlock complete potentials of IoT with intelligence,\nand IoT applications increasingly feed data collected by sensors into ML\nmodels, thereby employing results to improve their business processes and\nservices. Hence, orchestrating ML pipelines that encompasses model training and\nimplication involved in holistic development lifecycle of an IoT application\noften leads to complex system integration. This paper provides a comprehensive\nand systematic survey on the development lifecycle of ML-based IoT application.\nWe outline core roadmap and taxonomy, and subsequently assess and compare\nexisting standard techniques used in individual stage.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 23:04:22 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 10:44:31 GMT"}, {"version": "v3", "created": "Thu, 30 Jan 2020 11:54:30 GMT"}, {"version": "v4", "created": "Sat, 9 May 2020 13:36:49 GMT"}, {"version": "v5", "created": "Fri, 29 May 2020 21:59:18 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Qian", "Bin", ""], ["Su", "Jie", ""], ["Wen", "Zhenyu", ""], ["Jha", "Devki Nandan", ""], ["Li", "Yinhao", ""], ["Guan", "Yu", ""], ["Puthal", "Deepak", ""], ["James", "Philip", ""], ["Yang", "Renyu", ""], ["Zomaya", "Albert Y.", ""], ["Rana", "Omer", ""], ["Wang", "Lizhe", ""], ["Koutny", "Maciej", ""], ["Ranjan", "Rajiv", ""]]}, {"id": "1910.05482", "submitter": "Yuqing Zhu", "authors": "Yuqing Zhu and Jianxun Liu", "title": "ClassyTune: A Performance Auto-Tuner for Systems in the Cloud", "comments": "12 pages, Journal paper", "journal-ref": "ClassyTune: A Performance Auto-Tuner for Systems in the Cloud.\n  IEEE Transactions on Cloud Computing, 2019. doi: 10.1109/TCC.2019.2936567", "doi": "10.1109/TCC.2019.2936567", "report-no": null, "categories": "cs.PF cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance tuning can improve the system performance and thus enable the\nreduction of cloud computing resources needed to support an application. Due to\nthe ever increasing number of parameters and complexity of systems, there is a\nnecessity to automate performance tuning for the complicated systems in the\ncloud. The state-of-the-art tuning methods are adopting either the\nexperience-driven tuning approach or the data-driven one. Data-driven tuning is\nattracting increasing attentions, as it has wider applicability. But existing\ndata-driven methods cannot fully address the challenges of sample scarcity and\nhigh dimensionality simultaneously. We present ClassyTune, a data-driven\nautomatic configuration tuning tool for cloud systems. ClassyTune exploits the\nmachine learning model of classification for auto-tuning. This exploitation\nenables the induction of more training samples without increasing the input\ndimension. Experiments on seven popular systems in the cloud show that\nClassyTune can effectively tune system performance to seven times higher for\nhigh-dimensional configuration space, outperforming expert tuning and the\nstate-of-the-art auto-tuning solutions. We also describe a use case in which\nperformance tuning enables the reduction of 33% computing resources needed to\nrun an online stateless service.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 04:05:24 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Zhu", "Yuqing", ""], ["Liu", "Jianxun", ""]]}, {"id": "1910.05680", "submitter": "Chao-Tsung Huang", "authors": "Chao-Tsung Huang, Yu-Chun Ding, Huan-Ching Wang, Chi-Wen Weng,\n  Kai-Ping Lin, Li-Wei Wang, Li-De Chen", "title": "eCNN: A Block-Based and Highly-Parallel CNN Accelerator for Edge\n  Inference", "comments": "14 pages; appearing in IEEE/ACM International Symposium on\n  Microarchitecture (MICRO), 2019", "journal-ref": null, "doi": "10.1145/3352460.3358263", "report-no": null, "categories": "cs.DC cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have recently demonstrated superior\nquality for computational imaging applications. Therefore, they have great\npotential to revolutionize the image pipelines on cameras and displays.\nHowever, it is difficult for conventional CNN accelerators to support\nultra-high-resolution videos at the edge due to their considerable DRAM\nbandwidth and power consumption. Therefore, finding a further memory- and\ncomputation-efficient microarchitecture is crucial to speed up this coming\nrevolution.\n  In this paper, we approach this goal by considering the inference flow,\nnetwork model, instruction set, and processor design jointly to optimize\nhardware performance and image quality. We apply a block-based inference flow\nwhich can eliminate all the DRAM bandwidth for feature maps and accordingly\npropose a hardware-oriented network model, ERNet, to optimize image quality\nbased on hardware constraints. Then we devise a coarse-grained instruction set\narchitecture, FBISA, to support power-hungry convolution by massive\nparallelism. Finally,we implement an embedded processor---eCNN---which\naccommodates to ERNet and FBISA with a flexible processing architecture. Layout\nresults show that it can support high-quality ERNets for super-resolution and\ndenoising at up to 4K Ultra-HD 30 fps while using only DDR-400 and consuming\n6.94W on average. By comparison, the state-of-the-art Diffy uses dual-channel\nDDR3-2133 and consumes 54.3W to support lower-quality VDSR at Full HD 30 fps.\nLastly, we will also present application examples of high-performance style\ntransfer and object recognition to demonstrate the flexibility of eCNN.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 03:54:25 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Huang", "Chao-Tsung", ""], ["Ding", "Yu-Chun", ""], ["Wang", "Huan-Ching", ""], ["Weng", "Chi-Wen", ""], ["Lin", "Kai-Ping", ""], ["Wang", "Li-Wei", ""], ["Chen", "Li-De", ""]]}, {"id": "1910.05768", "submitter": "Giuseppe Antonio Di Luna", "authors": "Giuseppe Antonio Di Luna, Emmanuelle Anceaume, Leonardo Querzoni", "title": "Byzantine Generalized Lattice Agreement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper investigates the Lattice Agreement (LA) problem in asynchronous\nsystems. In LA each process proposes an element $e$ from a predetermined\nlattice, and has to decide on an element $e'$ of the lattice such that $e \\leq\ne'$. Moreover, decisions of different processes have to be comparable (no two\nprocesses can decide two elements $e$ and $e'$ such that $(e \\not\\leq e') \\land\n(e' \\not\\leq e)$). It has been shown that Generalized LA (i.e., a version of LA\nproposing and deciding on sequences of values) can be used to build a\nReplicated State Machine (RSM) with commutative update operations. The key\nadvantage of LA and Generalized LA is that they can be solved in asynchronous\nsystems prone to crash-failures (this is not the case with standard Consensus).\nIn this paper we assume Byzantine failures. We propose the Wait Till Safe (WTS)\nalgorithm for LA, and we show that its resilience to $f \\leq (n-1)/3$\nByzantines is optimal. We then generalize WTS obtaining a Generalized LA\nalgorithm, namely GWTS. We use GWTS to build a RSM with commutative updates.\nOur RSM works in asynchronous systems and tolerates $f \\leq (n-1)/3$ malicious\nentities. All our algorithms use the minimal assumption of authenticated\nchannels. When the more powerful public signatures are available, we discuss\nhow to improve the message complexity of our results (from quadratic to linear,\nwhen $f={\\cal O}(1)$). At the best of our knowledge this is the first paper\nproposing a solution for Byzantine LA that works on any possible lattice, and\nit is the first work proposing a Byzantine tolerant RSM built on it.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 15:17:13 GMT"}, {"version": "v2", "created": "Sat, 26 Oct 2019 09:15:10 GMT"}, {"version": "v3", "created": "Thu, 13 Feb 2020 10:22:15 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Di Luna", "Giuseppe Antonio", ""], ["Anceaume", "Emmanuelle", ""], ["Querzoni", "Leonardo", ""]]}, {"id": "1910.05779", "submitter": "Amin M Khan", "authors": "Petter Tunstad, Amin M. Khan, Phuong Hoai Ha", "title": "HyperProv: Decentralized Resilient Data Provenance at the Edge with\n  Blockchains", "comments": null, "journal-ref": null, "doi": "10.1145/3366627.3368105", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data provenance and lineage are critical for ensuring integrity and\nreproducibility of information in research and application. This is\nparticularly challenging for distributed scenarios, where data may be\noriginating from decentralized sources without any central control by a single\ntrusted entity. We present HyperProv, a general framework for data provenance\nbased on the permissioned blockchain Hyperledger Fabric (HLF), and to the best\nof our knowledge, the first system that is ported to ARM based devices such as\nRaspberry Pi (RPi). HyperProv tracks the metadata, operation history and data\nlineage through a set of built-in queries using smart contracts, enabling\nlightweight retrieval of provenance data. HyperProv provides convenient\nintegration through a NodeJS client library, and also includes off-chain\nstorage through the SSH file system. We evaluate HyperProv's performance,\nthroughput, resource consumption, and energy efficiency on x86-64 machines, as\nwell as on RPi devices for IoT use cases at the edge.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 16:16:52 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Tunstad", "Petter", ""], ["Khan", "Amin M.", ""], ["Ha", "Phuong Hoai", ""]]}, {"id": "1910.05857", "submitter": "Haoran Sun", "authors": "Haoran Sun, Songtao Lu, Mingyi Hong", "title": "Improving the Sample and Communication Complexity for Decentralized\n  Non-Convex Optimization: A Joint Gradient Estimation and Tracking Approach", "comments": null, "journal-ref": "Published at the International Conference on Machine Learning\n  (ICML 2020)", "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern large-scale machine learning problems benefit from decentralized\nand stochastic optimization. Recent works have shown that utilizing both\ndecentralized computing and local stochastic gradient estimates can outperform\nstate-of-the-art centralized algorithms, in applications involving highly\nnon-convex problems, such as training deep neural networks.\n  In this work, we propose a decentralized stochastic algorithm to deal with\ncertain smooth non-convex problems where there are $m$ nodes in the system, and\neach node has a large number of samples (denoted as $n$). Differently from the\nmajority of the existing decentralized learning algorithms for either\nstochastic or finite-sum problems, our focus is given to both reducing the\ntotal communication rounds among the nodes, while accessing the minimum number\nof local data samples. In particular, we propose an algorithm named D-GET\n(decentralized gradient estimation and tracking), which jointly performs\ndecentralized gradient estimation (which estimates the local gradient using a\nsubset of local samples) and gradient tracking (which tracks the global full\ngradient using local estimates). We show that, to achieve certain $\\epsilon$\nstationary solution of the deterministic finite sum problem, the proposed\nalgorithm achieves an $\\mathcal{O}(mn^{1/2}\\epsilon^{-1})$ sample complexity\nand an $\\mathcal{O}(\\epsilon^{-1})$ communication complexity. These bounds\nsignificantly improve upon the best existing bounds of\n$\\mathcal{O}(mn\\epsilon^{-1})$ and $\\mathcal{O}(\\epsilon^{-1})$, respectively.\nSimilarly, for online problems, the proposed method achieves an $\\mathcal{O}(m\n\\epsilon^{-3/2})$ sample complexity and an $\\mathcal{O}(\\epsilon^{-1})$\ncommunication complexity, while the best existing bounds are\n$\\mathcal{O}(m\\epsilon^{-2})$ and $\\mathcal{O}(\\epsilon^{-2})$, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 22:57:33 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Sun", "Haoran", ""], ["Lu", "Songtao", ""], ["Hong", "Mingyi", ""]]}, {"id": "1910.05885", "submitter": "Lucas A. Wilson", "authors": "Pei Yang, Srinivas Varadharajan, Lucas A. Wilson, Don D. Smith II,\n  John A Lockman III, Vineet Gundecha, Quy Ta", "title": "Parallelized Training of Restricted Boltzmann Machines using\n  Markov-Chain Monte Carlo Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Restricted Boltzmann Machine (RBM) is a generative stochastic neural network\nthat can be applied to collaborative filtering technique used by recommendation\nsystems. Prediction accuracy of the RBM model is usually better than that of\nother models for recommendation systems. However, training the RBM model\ninvolves Markov-Chain Monte Carlo (MCMC) method, which is computationally\nexpensive. In this paper, we have successfully applied distributed parallel\ntraining using Horovod framework to improve the training time of the RBM model.\nOur tests show that the distributed training approach of the RBM model has a\ngood scaling efficiency. We also show that this approach effectively reduces\nthe training time to little over 12 minutes on 64 CPU nodes compared to 5 hours\non a single CPU node. This will make RBM models more practically applicable in\nrecommendation systems.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 01:50:57 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Yang", "Pei", ""], ["Varadharajan", "Srinivas", ""], ["Wilson", "Lucas A.", ""], ["Smith", "Don D.", "II"], ["Lockman", "John A", "III"], ["Gundecha", "Vineet", ""], ["Ta", "Quy", ""]]}, {"id": "1910.05896", "submitter": "Yue Cheng", "authors": "Benjamin Carver, Jingyuan Zhang, Ao Wang, Yue Cheng", "title": "In Search of a Fast and Efficient Serverless DAG Engine", "comments": "Appears at PDSW 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Python-written data analytics applications can be modeled as and compiled\ninto a directed acyclic graph (DAG) based workflow, where the nodes are\nfine-grained tasks and the edges are task dependencies. Such analytics workflow\njobs are increasingly characterized by short, fine-grained tasks with large\nfan-outs. These characteristics make them well-suited for a new cloud computing\nmodel called serverless computing or Function-as-a-Service (FaaS), which has\nbecome prevalent in recent years. The auto-scaling property of serverless\ncomputing platforms accommodates short tasks and bursty workloads, while the\npay-per-use billing model of serverless computing providers keeps the cost of\nshort tasks low.\n  In this paper, we thoroughly investigate the problem space of DAG scheduling\nin serverless computing. We identify and evaluate a set of techniques to make\nDAG schedulers serverless-aware. These techniques have been implemented in\nWukong, a serverless, DAG scheduler attuned to AWS Lambda. Wukong provides\ndecentralized scheduling through a combination of static and dynamic\nscheduling. We present the results of an empirical study in which Wukong is\napplied to a range of microbenchmark and real-world DAG applications. Results\ndemonstrate the efficacy of Wukong in minimizing the performance overhead\nintroduced by AWS Lambda --- Wukong achieves competitive performance compared\nto a serverful DAG scheduler, while improving the performance of real-world DAG\njobs by as much as 3.1X at larger scale.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 02:48:50 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 19:02:11 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Carver", "Benjamin", ""], ["Zhang", "Jingyuan", ""], ["Wang", "Ao", ""], ["Cheng", "Yue", ""]]}, {"id": "1910.05971", "submitter": "Yongzhe Zhang", "authors": "Yongzhe Zhang, Ariful Azad, Zhenjiang Hu", "title": "FastSV: A Distributed-Memory Connected Component Algorithm with Fast\n  Convergence", "comments": "SIAM Conference on Parallel Processing for Scientific Computing\n  (PP20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new distributed-memory algorithm called FastSV for\nfinding connected components in an undirected graph. Our algorithm simplifies\nthe classic Shiloach-Vishkin algorithm and employs several novel and efficient\nhooking strategies for faster convergence. We map different steps of FastSV to\nlinear algebraic operations and implement them with the help of scalable graph\nlibraries. FastSV uses sparse operations to avoid redundant work and optimized\nMPI communication to avoid bottlenecks. The resultant algorithm shows\nhigh-performance and scalability as it can find the connected components of a\nhyperlink graph with over 134B edges in 30 seconds using 262K cores on a Cray\nXC40 supercomputer. FastSV outperforms the state-of-the-art algorithm by an\naverage speedup of 2.21x (max 4.27x) on a variety of real-world graphs.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 08:13:41 GMT"}, {"version": "v2", "created": "Fri, 3 Jan 2020 20:33:37 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Zhang", "Yongzhe", ""], ["Azad", "Ariful", ""], ["Hu", "Zhenjiang", ""]]}, {"id": "1910.06093", "submitter": "Jy-yong Sohn", "authors": "Jy-yong Sohn, Dong-Jun Han, Beongjun Choi and Jaekyun Moon", "title": "Election Coding for Distributed Learning: Protecting SignSGD against\n  Byzantine Attacks", "comments": "Accepted at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in large-scale distributed learning algorithms have enabled\ncommunication-efficient training via SignSGD. Unfortunately, a major issue\ncontinues to plague distributed learning: namely, Byzantine failures may incur\nserious degradation in learning accuracy. This paper proposes Election Coding,\na coding-theoretic framework to guarantee Byzantine-robustness for SignSGD with\nMajority Vote, which uses minimum worker-master communication in both\ndirections. The suggested framework explores new information-theoretic limits\nof finding the majority opinion when some workers could be malicious, and paves\nthe road to implement robust and efficient distributed learning algorithms.\nUnder this framework, we construct two types of explicit codes, random\nBernoulli codes and deterministic algebraic codes, that can tolerate Byzantine\nattacks with a controlled amount of computational redundancy. For the Bernoulli\ncodes, we provide upper bounds on the error probability in estimating the\nmajority opinion, which give useful insights into code design for tolerating\nByzantine attacks. As for deterministic codes, we construct an explicit code\nwhich perfectly tolerates Byzantines, and provide tight upper/lower bounds on\nthe minimum required computational redundancy. Finally, the Byzantine-tolerance\nof the suggested coding schemes is confirmed by deep learning experiments on\nAmazon EC2 using Python with MPI4py package.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 12:27:20 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 14:19:31 GMT"}, {"version": "v3", "created": "Fri, 16 Oct 2020 01:35:37 GMT"}, {"version": "v4", "created": "Sat, 24 Oct 2020 05:54:53 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Sohn", "Jy-yong", ""], ["Han", "Dong-Jun", ""], ["Choi", "Beongjun", ""], ["Moon", "Jaekyun", ""]]}, {"id": "1910.06156", "submitter": "Alessio Netti", "authors": "Alessio Netti, Micha Mueller, Carla Guillen, Michael Ott, Daniele\n  Tafani, Gence Ozer and Martin Schulz", "title": "DCDB Wintermute: Enabling Online and Holistic Operational Data Analytics\n  on HPC Systems", "comments": "Accepted for publication at the 29th ACM International Symposium on\n  High-Performance Parallel and Distributed Computing (HPDC 2020)", "journal-ref": null, "doi": "10.1145/3369583.3392674", "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As we approach the exascale era, the size and complexity of HPC systems\ncontinues to increase, raising concerns about their manageability and\nsustainability. For this reason, more and more HPC centers are experimenting\nwith fine-grained monitoring coupled with Operational Data Analytics (ODA) to\noptimize efficiency and effectiveness of system operations. However, while\nmonitoring is a common reality in HPC, there is no well-stated and\ncomprehensive list of requirements, nor matching frameworks, to support\nholistic and online ODA. This leads to insular ad-hoc solutions, each\naddressing only specific aspects of the problem.\n  In this paper we propose Wintermute, a novel generic framework to enable\nonline ODA on large-scale HPC installations. Its design is based on the results\nof a literature survey of common operational requirements. We implement\nWintermute on top of the holistic DCDB monitoring system, offering a large\nvariety of configuration options to accommodate the varying requirements of ODA\napplications. Moreover, Wintermute is based on a set of logical abstractions to\nease the configuration of models at a large scale and maximize code re-use. We\nhighlight Wintermute's flexibility through a series of practical case studies,\neach targeting a different aspect of the management of HPC systems, and then\ndemonstrate the small resource footprint of our implementation.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 14:05:52 GMT"}, {"version": "v2", "created": "Sat, 18 Apr 2020 18:04:48 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Netti", "Alessio", ""], ["Mueller", "Micha", ""], ["Guillen", "Carla", ""], ["Ott", "Michael", ""], ["Tafani", "Daniele", ""], ["Ozer", "Gence", ""], ["Schulz", "Martin", ""]]}, {"id": "1910.06310", "submitter": "Yu-Hang Tang", "authors": "Yu-Hang Tang, Oguz Selvitopi, Doru Popovici, Ayd{\\i}n Bulu\\c{c}", "title": "A High-Throughput Solver for Marginalized Graph Kernels on GPU", "comments": null, "journal-ref": null, "doi": "10.1109/IPDPS47924.2020.00080", "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the design and optimization of a linear solver on General Purpose\nGPUs for the efficient and high-throughput evaluation of the marginalized graph\nkernel between pairs of labeled graphs. The solver implements a preconditioned\nconjugate gradient (PCG) method to compute the solution to a generalized\nLaplacian equation associated with the tensor product of two graphs. To cope\nwith the gap between the instruction throughput and the memory bandwidth of\ncurrent generation GPUs, our solver forms the tensor product linear system\non-the-fly without storing it in memory when performing matrix-vector dot\nproduct operations in PCG. Such on-the-fly computation is accomplished by using\nthreads in a warp to cooperatively stream the adjacency and edge label matrices\nof individual graphs by small square matrix blocks called tiles, which are then\nstaged in registers and the shared memory for later reuse. Warps across a\nthread block can further share tiles via the shared memory to increase data\nreuse. We exploit the sparsity of the graphs hierarchically by storing only\nnon-empty tiles using a coordinate format and nonzero elements within each tile\nusing bitmaps. Besides, we propose a new partition-based reordering algorithm\nfor aggregating nonzero elements of the graphs into fewer but denser tiles to\nimprove the efficiency of the sparse format.\n  We carry out extensive theoretical analyses on the graph tensor product\nprimitives for tiles of various density and evaluate their performance on\nsynthetic and real-world datasets. Our solver delivers three to four orders of\nmagnitude speedup over existing CPU-based solvers such as GraKeL and\nGraphKernels. The capability of the solver enables kernel-based learning tasks\nat unprecedented scales.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 17:46:47 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 08:58:01 GMT"}, {"version": "v3", "created": "Mon, 16 Dec 2019 08:09:42 GMT"}, {"version": "v4", "created": "Tue, 25 Feb 2020 23:28:34 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Tang", "Yu-Hang", ""], ["Selvitopi", "Oguz", ""], ["Popovici", "Doru", ""], ["Bulu\u00e7", "Ayd\u0131n", ""]]}, {"id": "1910.06313", "submitter": "Thanakorn Khamvilai", "authors": "Thanakorn Khamvilai, Louis Sutter, Eric Feron, Philippe Baufreton,\n  Francois Neumann", "title": "Decentralized On-line Task Reallocation on Parallel Computing\n  Architectures with Safety-Critical Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SY eess.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a decentralized allocation algorithm of safety-critical\napplication on parallel computing architectures, where individual Computational\nUnits can be affected by faults.\n  The described method consists in representing the architecture by an abstract\ngraph where each node represents a Computational Unit. Applications are also\nrepresented by the graph of Computational Units they require for execution. The\nproblem is then to decide how to allocate Computational Units to applications\nto guarantee execution of the safety-critical application. The problem is\nformulated as an optimization problem, with the form of an Integer Linear\nProgram. A state-of-the-art solver is then used to solve the problem.\n  Decentralizing the allocation process is achieved through redundancy of the\nallocator executed on the architecture. No centralized element decides on the\nallocation of the entire architecture, thus improving the reliability of the\nsystem.\n  Experimental reproduction of a multi-core architecture is also presented. It\nis used to demonstrate the capabilities of the proposed allocation process to\nmaintain the operation of a physical system in a decentralized way while\nindividual component fails.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 18:55:50 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 06:43:46 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Khamvilai", "Thanakorn", ""], ["Sutter", "Louis", ""], ["Feron", "Eric", ""], ["Baufreton", "Philippe", ""], ["Neumann", "Francois", ""]]}, {"id": "1910.06378", "submitter": "Sai Praneeth Karimireddy", "authors": "Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank J.\n  Reddi, Sebastian U. Stich, Ananda Theertha Suresh", "title": "SCAFFOLD: Stochastic Controlled Averaging for Federated Learning", "comments": "v2 contains analysis of FedAvg, non-convex rates of Scaffold, and\n  experimental evaluation. v3 fixes typos, ICML version. v4 slightly improves\n  rate of SCAFFOLD for general convex functions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Averaging (FedAvg) has emerged as the algorithm of choice for\nfederated learning due to its simplicity and low communication cost. However,\nin spite of recent research efforts, its performance is not fully understood.\nWe obtain tight convergence rates for FedAvg and prove that it suffers from\n`client-drift' when the data is heterogeneous (non-iid), resulting in unstable\nand slow convergence.\n  As a solution, we propose a new algorithm (SCAFFOLD) which uses control\nvariates (variance reduction) to correct for the `client-drift' in its local\nupdates. We prove that SCAFFOLD requires significantly fewer communication\nrounds and is not affected by data heterogeneity or client sampling. Further,\nwe show that (for quadratics) SCAFFOLD can take advantage of similarity in the\nclient's data yielding even faster convergence. The latter is the first result\nto quantify the usefulness of local-steps in distributed optimization.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 18:49:20 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2020 15:57:57 GMT"}, {"version": "v3", "created": "Sun, 9 Aug 2020 23:37:28 GMT"}, {"version": "v4", "created": "Fri, 9 Apr 2021 16:21:59 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Karimireddy", "Sai Praneeth", ""], ["Kale", "Satyen", ""], ["Mohri", "Mehryar", ""], ["Reddi", "Sashank J.", ""], ["Stich", "Sebastian U.", ""], ["Suresh", "Ananda Theertha", ""]]}, {"id": "1910.06403", "submitter": "Maximilian Balandat", "authors": "Maximilian Balandat, Brian Karrer, Daniel R. Jiang, Samuel Daulton,\n  Benjamin Letham, Andrew Gordon Wilson, Eytan Bakshy", "title": "BoTorch: A Framework for Efficient Monte-Carlo Bayesian Optimization", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems 33, 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization provides sample-efficient global optimization for a\nbroad range of applications, including automatic machine learning, engineering,\nphysics, and experimental design. We introduce BoTorch, a modern programming\nframework for Bayesian optimization that combines Monte-Carlo (MC) acquisition\nfunctions, a novel sample average approximation optimization approach,\nauto-differentiation, and variance reduction techniques. BoTorch's modular\ndesign facilitates flexible specification and optimization of probabilistic\nmodels written in PyTorch, simplifying implementation of new acquisition\nfunctions. Our approach is backed by novel theoretical convergence results and\nmade practical by a distinctive algorithmic foundation that leverages fast\npredictive distributions, hardware acceleration, and deterministic\noptimization. We also propose a novel \"one-shot\" formulation of the Knowledge\nGradient, enabled by a combination of our theoretical and software\ncontributions. In experiments, we demonstrate the improved sample efficiency of\nBoTorch relative to other popular libraries.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 20:11:30 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 17:28:05 GMT"}, {"version": "v3", "created": "Tue, 8 Dec 2020 19:31:38 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Balandat", "Maximilian", ""], ["Karrer", "Brian", ""], ["Jiang", "Daniel R.", ""], ["Daulton", "Samuel", ""], ["Letham", "Benjamin", ""], ["Wilson", "Andrew Gordon", ""], ["Bakshy", "Eytan", ""]]}, {"id": "1910.06415", "submitter": "Gal Oren", "authors": "Matan Rusanovsky, Re'em Harel, Lee-or Alon, Idan Mosseri, Harel Levin,\n  Gal Oren", "title": "BACKUS: Comprehensive High-Performance Research Software Engineering\n  Approach for Simulations in Supercomputing Systems", "comments": "19 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-Performance Computing (HPC) platforms enable scientific software to\nachieve breakthroughs in many research fields such as physics, biology, and\nchemistry, by employing Research Software Engineering (RSE) techniques. These\ninclude 1) novel parallelism paradigms such as Shared Memory Parallelism (with\ne.g. OpenMP 4.5); Distributed Memory Parallelism (with e.g. MPI 4); Hybrid\nParallelism which combines them; and Heterogeneous Parallelism (for CPUs,\nco-processors and accelerators), 2) introducing advanced Software Engineering\nconcepts such as Object Oriented Parallel Programming (OOPP); Parallel Unit\ntesting; Parallel I/O Formats; Hybrid Parallel Visualization; and 3) Selecting\nthe Best Practices in other necessary areas such as User Interface; Automatic\nDocumentation; Version Control and Project Management. In this work we present\nBACKUS: Comprehensive High-Performance Research Software Engineering Approach\nfor Simulations in Supercomputing Systems, which we found to fit best for\nlong-lived parallel scientific codes.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 20:49:26 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Rusanovsky", "Matan", ""], ["Harel", "Re'em", ""], ["Alon", "Lee-or", ""], ["Mosseri", "Idan", ""], ["Levin", "Harel", ""], ["Oren", "Gal", ""]]}, {"id": "1910.06477", "submitter": "Kenneth Duru", "authors": "Kenneth Duru, Leonhard Rannabauer, Alice-Agnes Gabriel, Gunilla\n  Kreiss, and Michael Bader", "title": "A stable discontinuous Galerkin method for the perfectly matched layer\n  for elastodynamics in first order form", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC cs.NA physics.comp-ph physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a stable discontinuous Galerkin (DG) method with a perfectly\nmatched layer (PML) for three and two space dimensional linear elastodynamics,\nin velocity-stress formulation, subject to well-posed linear boundary\nconditions. First, we consider the elastodynamics equation, in a cuboidal\ndomain, and derive an unsplit PML truncating the domain using complex\ncoordinate stretching. Leveraging the hyperbolic structure of the underlying\nsystem, we construct continuous energy estimates, in the time domain for the\nelastic wave equation, and in the Laplace space for a sequence of PML model\nproblems, with variations in one, two and three space dimensions, respectively.\nThey correspond to PMLs normal to boundary faces, along edges and in corners.\nSecond, we develop a DG numerical method for the linear elastodynamics equation\nusing physically motivated numerical flux and penalty parameters, which are\ncompatible with all well-posed, internal and external, boundary conditions.\nWhen the PML damping vanishes, by construction, our choice of penalty\nparameters yield an upwind scheme and a discrete energy estimate analogous to\nthe continuous energy estimate. Third, to ensure numerical stability of the\ndiscretization when PML damping is present, it is necessary to extend the\nnumerical DG fluxes, and the numerical inter-element and boundary procedures,\nto the PML auxiliary differential equations. This is crucial for deriving\ndiscrete energy estimates analogous to the continuous energy estimates. By\ncombining the DG spatial approximation with the high order ADER time stepping\nscheme and the accuracy of the PML we obtain an arbitrarily accurate wave\npropagation solver in the time domain. Numerical experiments are presented in\ntwo and three space dimensions corroborating the theoretical results.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 01:53:44 GMT"}, {"version": "v2", "created": "Mon, 6 Jan 2020 21:54:19 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Duru", "Kenneth", ""], ["Rannabauer", "Leonhard", ""], ["Gabriel", "Alice-Agnes", ""], ["Kreiss", "Gunilla", ""], ["Bader", "Michael", ""]]}, {"id": "1910.06567", "submitter": "Jing Fu", "authors": "Jing Fu and Bill Moran", "title": "Energy-Efficient Job-Assignment Policy with Asymptotically Guaranteed\n  Performance Deviation", "comments": "14 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a job-assignment problem in a large-scale server farm system with\ngeographically deployed servers as abstracted computer components (e.g.,\nstorage, network links, and processors) that are potentially diverse. We aim to\nmaximize the energy efficiency of the entire system by effectively controlling\ncarried load on networked servers. A scalable, near-optimal job-assignment\npolicy is proposed. The optimality is gauged as, roughly speaking, energy cost\nper job. Our key result is an upper bound on the deviation between the proposed\npolicy and the asymptotically optimal energy efficiency, when job sizes are\nexponentially distributed and blocking probabilities are positive. Relying on\nWhittle relaxation and the asymptotic optimality theorem of Weber and Weiss,\nthis bound is shown to decrease exponentially as the number of servers and the\narrival rates of jobs increase arbitrarily and in proportion. In consequence,\nthe proposed policy is asymptotically optimal and, more importantly, approaches\nasymptotic optimality quickly (exponentially). This suggests that the proposed\npolicy is close to optimal even for relatively small systems (and indeed any\nlarger systems), and this is consistent with the results of our simulations.\nSimulations indicate that the policy is effective, and robust to variations in\njob-size distributions.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 07:34:15 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 03:00:43 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Fu", "Jing", ""], ["Moran", "Bill", ""]]}, {"id": "1910.06674", "submitter": "Alexey Lastovetsky", "authors": "Semyon Khokhriakov, Ravi Reddy Manumachu, Alexey Lastovetsky", "title": "Modern Multicore CPUs are not Energy Proportional: Opportunity for\n  Bi-objective Optimization for Performance and Energy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.PF cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy proportionality is the key design goal followed by architects of\nmodern multicore CPUs. One of its implications is that optimization of an\napplication for performance will also optimize it for energy. In this work, we\nshow that energy proportionality does not hold true for multicore CPUs. This\nfinding creates the opportunity for bi-objective optimization of applications\nfor performance and energy. We propose and study the first application-level\nmethod for bi-objective optimization of multithreaded data-parallel\napplications for performance and energy. The method uses two decision\nvariables, the number of identical multithreaded kernels (threadgroups)\nexecuting the application and the number of threads in each threadgroup, with\nthe workload always partitioned equally between the threadgroups. We\nexperimentally demonstrate the efficiency of the method using four highly\noptimized multithreaded data-parallel applications, 2D fast Fourier transform\nbased on FFTW and Intel MKL, and dense matrix-matrix multiplication using\nOpenBLAS and Intel MKL. Four modern multicore CPUs are used in the experiments.\nThe experiments show that optimization for performance alone results in the\nincrease in dynamic energy consumption by up to 89% and optimization for\ndynamic energy alone degrades the performance by up to 49%. By solving the\nbi-objective optimization problem, the method determines up to 11 globally\nPareto-optimal solutions. Finally, we propose a qualitative dynamic energy\nmodel employing performance monitoring counters as parameters, which we use to\nexplain the discovered energy nonproportionality and the Pareto-optimal\nsolutions determined by our method. The model shows that the energy\nnonproportionality in our case is due to the activity of the data translation\nlookaside buffer (dTLB), which is disproportionately energy expensive.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 12:17:21 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Khokhriakov", "Semyon", ""], ["Manumachu", "Ravi Reddy", ""], ["Lastovetsky", "Alexey", ""]]}, {"id": "1910.06682", "submitter": "Tommy Koens", "authors": "Rowel G\\\"undlach, Jaap-Henk Hoepman, Remco van der Hofstad, Tommy\n  Koens, Stijn Meijer", "title": "Hydra: A Multiple Blockchain Protocol for Improving Transaction\n  Throughput", "comments": "Preprint, 12 pages, 2 figures, short version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Improving transaction throughput is one of the main challenges in\ndecentralized payment systems. Attempts to improve transaction throughput in\ncryptocurrencies are usually a trade-off between throughput and security or\nintroduce a central component.\n  We propose Hydra, a decentralized protocol that improves transaction\nthroughput without the security trade-off and has no central component. Our\nnovel approach distributes blocks over multiple blockchains. Hydra makes a\ntrade-off between transaction throughput and finality, the time it takes to\nstabilize the record of a transaction in the blockchain. We rigorously analyze\nthe double spend attack in a multiple-blockchain protocol. Our analysis shows\nthat the number of transactions per second can be increased significantly while\nfinality is within acceptable boundaries.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 12:30:42 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["G\u00fcndlach", "Rowel", ""], ["Hoepman", "Jaap-Henk", ""], ["van der Hofstad", "Remco", ""], ["Koens", "Tommy", ""], ["Meijer", "Stijn", ""]]}, {"id": "1910.06716", "submitter": "Saptaparni Kumar", "authors": "Saptaparni Kumar and Jennifer L. Welch", "title": "Byzantine-Tolerant Register in a System with Continuous Churn", "comments": "arXiv admin note: text overlap with arXiv:1708.03274", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A shared read/write register emulation provides the illusion of shared-memory\non top of message-passing models. The main hurdle with such emulations is\ndealing with server faults in the system. Several crash-tolerant register\nemulations in static systems require algorithms to replicate the value of the\nshared register onto a majority of servers. Majority correctness is necessary\nfor such emulations. Byzantine faults are considered to be the worst kind of\nfaults that can happen in any distributed system. Emulating a\nByzantine-tolerant register requires replicating the register value on to more\nthan two-thirds of the servers. Emulating a register in a dynamic system where\nservers and clients can enter and leave the system and be faulty is harder than\nin static systems. There are several crash-tolerant register emulations for\ndynamic systems. This paper presents the first emulation of a multi-reader\nmulti-writer atomic register in a system that can withstand nodes continually\nentering and leaving, imposes no upper bound on the system size and can\ntolerate Byzantine servers. The algorithm works as long as the number of\nservers entering and leaving during a fixed time interval is at most a constant\nfraction of the system size at the beginning of the interval, and as long as\nthe number of Byzantine servers in the system is at most f. Although our\nalgorithm requires that there be a constant known upper bound on the number of\nByzantine servers, this restriction is unavoidable, as we show that it is\nimpossible to emulate an atomic register if the system size and maximum number\nof servers that can be Byzantine in the system is unknown to the nodes.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 22:43:07 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Kumar", "Saptaparni", ""], ["Welch", "Jennifer L.", ""]]}, {"id": "1910.06726", "submitter": "Hamid Reza Zohouri", "authors": "Hamid Reza Zohouri, Satoshi Matsuoka", "title": "The Memory Controller Wall: Benchmarking the Intel FPGA SDK for OpenCL\n  Memory Interface", "comments": "Published at H2RC'19: Fifth International Workshop on Heterogeneous\n  High-performance Reconfigurable Computing held in conjunction with SC'19", "journal-ref": null, "doi": "10.1109/H2RC49586.2019.00007", "report-no": null, "categories": "cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supported by their high power efficiency and recent advancements in High\nLevel Synthesis (HLS), FPGAs are quickly finding their way into HPC and cloud\nsystems. Large amounts of work have been done so far on loop and area\noptimizations for different applications on FPGAs using HLS. However, a\ncomprehensive analysis of the behavior and efficiency of the memory controller\nof FPGAs is missing in literature, which becomes even more crucial when the\nlimited memory bandwidth of modern FPGAs compared to their GPU counterparts is\ntaken into account. In this work, we will analyze the memory interface\ngenerated by Intel FPGA SDK for OpenCL with different configurations for\ninput/output arrays, vector size, interleaving, kernel programming model,\non-chip channels, operating frequency, padding, and multiple types of\noverlapped blocking. Our results point to multiple shortcomings in the memory\ncontroller of Intel FPGAs, especially with respect to memory access alignment,\nthat can hinder the programmer's ability in maximizing memory performance in\ntheir design. For some of these cases, we will provide work-arounds to improve\nmemory bandwidth efficiency; however, a general solution will require major\nchanges in the memory controller itself.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 13:32:19 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 11:55:15 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Zohouri", "Hamid Reza", ""], ["Matsuoka", "Satoshi", ""]]}, {"id": "1910.06799", "submitter": "Dinesh Verma", "authors": "D. Verma, S. Calo, S. Witherspoon, E. Bertino, A. Abu Jabal, A. Swami,\n  G. Cirincione, S. Julier, G. White, G. de Mel, G. Pearson", "title": "Federated Learning for Coalition Operations", "comments": "Presented at AAAI FSS-19: Artificial Intelligence in Government and\n  Public Sector, Arlington, Virginia, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning in coalition settings requires combining insights available\nfrom data assets and knowledge repositories distributed across multiple\ncoalition partners. In tactical environments, this requires sharing the assets,\nknowledge and models in a bandwidth-constrained environment, while staying in\nconformance with the privacy, security and other applicable policies for each\ncoalition member. Federated Machine Learning provides an approach for such\nsharing. In its simplest version, federated machine learning could exchange\ntraining data available among the different coalition members, with each\npartner deciding which part of the training data from other partners to accept\nbased on the quality and value of the offered data. In a more sophisticated\nversion, coalition partners may exchange models learnt locally, which need to\nbe transformed, accepted in entirety or in part based on the quality and value\noffered by each model, and fused together into an integrated model. In this\npaper, we examine the challenges present in creating federated learning\nsolutions in coalition settings, and present the different flavors of federated\nlearning that we have created as part of our research in the DAIS ITA. The\nchallenges addressed include dealing with varying quality of data and models,\ndetermining the value offered by the data/model of each coalition partner,\naddressing the heterogeneity in data representation, labeling and AI model\narchitecture selected by different coalition members, and handling the varying\nlevels of trust present among members of the coalition. We also identify some\nopen problems that remain to be addressed to create a viable solution for\nfederated learning in coalition environments.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 16:40:10 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Verma", "D.", ""], ["Calo", "S.", ""], ["Witherspoon", "S.", ""], ["Bertino", "E.", ""], ["Jabal", "A. Abu", ""], ["Swami", "A.", ""], ["Cirincione", "G.", ""], ["Julier", "S.", ""], ["White", "G.", ""], ["de Mel", "G.", ""], ["Pearson", "G.", ""]]}, {"id": "1910.06844", "submitter": "Ali Mohammed", "authors": "Ali Mohammed, Ahmed Eleliemy, Florina M. Ciorba, Franziska Kasielke,\n  Ioana Banicescu", "title": "An Approach for Realistically Simulating the Performance of Scientific\n  Applications on High Performance Computing Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific applications often contain large, computationally-intensive, and\nirregular parallel loops or tasks that exhibit stochastic characteristics.\nApplications may suffer from load imbalance during their execution on\nhigh-performance computing (HPC) systems due to such characteristics. Dynamic\nloop self-scheduling (DLS) techniques are instrumental in improving the\nperformance of scientific applications on HPC systems via load balancing.\nSelecting a DLS technique that results in the best performance for different\nproblems and system sizes requires a large number of exploratory experiments. A\ntheoretical model that can be used to predict the scheduling technique that\nyields the best performance for a given problem and system has not yet been\nidentified. Therefore, simulation is the most appropriate approach for\nconducting such exploratory experiments with reasonable costs. This work\ndevises an approach to realistically simulate computationally-intensive\nscientific applications that employ DLS and execute on HPC systems. Several\napproaches to represent the application tasks (or loop iterations) are compared\nto establish their influence on the simulative application performance. A novel\nsimulation strategy is introduced, which transforms a native application code\ninto a simulative code. The native and simulative performance of two\ncomputationally-intensive scientific applications are compared to evaluate the\nrealism of the proposed simulation approach. The comparison of the performance\ncharacteristics extracted from the native and simulative performance shows that\nthe proposed simulation approach fully captured most of the performance\ncharacteristics of interest. This work shows and establishes the importance of\nsimulations that realistically predict the performance of DLS techniques for\ndifferent applications and system configurations.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 15:08:16 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Mohammed", "Ali", ""], ["Eleliemy", "Ahmed", ""], ["Ciorba", "Florina M.", ""], ["Kasielke", "Franziska", ""], ["Banicescu", "Ioana", ""]]}, {"id": "1910.06898", "submitter": "Mahdi Miraz", "authors": "Mahdi H. Miraz", "title": "Blockchain of Things (BCoT): The Fusion of Blockchain and IoT\n  Technologies", "comments": null, "journal-ref": "Blockchain of Things (BCoT): The Fusion of Blockchain and IoT\n  Technologies. In: Advanced Applications of Blockchain Technology. Studies in\n  Big Data, vol 60, Chapter 7, DOI: 10.1007/978-981-13-8775-3_7, 2019, Springer\n  Nature", "doi": "10.1007/978-981-13-8775-3_7", "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain, as well as Internet of Things (IoT), is considered as two major\ndisruptive emerging technologies. However, both of them suffer from innate\ntechnological limitations to some extent. IoT requires strengthening its\nsecurity features while Blockchain inherently possesses them due to its\nextensive use of cryptographic mechanisms and Blockchain, in an inverted\nmanner, needs contributions from the distributed nodes for its P2P\n(Peer-to-Peer) consensus model while IoT rudimentarily embodies them within its\narchitecture. This chapter, therefore, acutely dissects the viability, along\nwith prospective challenges, of incorporating Blockchain with IoT\ntechnologies,inducing the notion of Blockchain of Things (BCoT), as well as the\nbenefits such consolidation can offer.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 03:31:01 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Miraz", "Mahdi H.", ""]]}, {"id": "1910.07045", "submitter": "Stephane Gaiffas Pr", "authors": "Emmanuel Bacry, St\\'ephane Ga\\\"iffas, Fanny Leroy, Maryan Morel, Dinh\n  Phong Nguyen, Youcef Sebiat, Dian Sun", "title": "SCALPEL3: a scalable open-source library for healthcare claims databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces SCALPEL3, a scalable open-source framework for\nstudies involving Large Observational Databases (LODs). Its design eases\nmedical observational studies thanks to abstractions allowing concept\nextraction, high-level cohort manipulation, and production of data formats\ncompatible with machine learning libraries. SCALPEL3 has successfully been used\non the SNDS database (see Tuppin et al. (2017)), a huge healthcare claims\ndatabase that handles the reimbursement of almost all French citizens.\n  SCALPEL3 focuses on scalability, easy interactive analysis and helpers for\ndata flow analysis to accelerate studies performed on LODs. It consists of\nthree open-source libraries based on Apache Spark. SCALPEL-Flattening allows\ndenormalization of the LOD (only SNDS for now) by joining tables sequentially\nin a big table. SCALPEL-Extraction provides fast concept extraction from a big\ntable such as the one produced by SCALPEL-Flattening. Finally, SCALPEL-Analysis\nallows interactive cohort manipulations, monitoring statistics of cohort flows\nand building datasets to be used with machine learning libraries. The first two\nprovide a Scala API while the last one provides a Python API that can be used\nin an interactive environment. Our code is available on GitHub.\n  SCALPEL3 allowed to extract successfully complex concepts for studies such as\nMorel et al (2017) or studies with 14.5 million patients observed over three\nyears (corresponding to more than 15 billion healthcare events and roughly 15\nTeraBytes of data) in less than 49 minutes on a small 15 nodes HDFS cluster.\nSCALPEL3 provides a sharp interactive control of data processing through\nlegible code, which helps to build studies with full reproducibility, leading\nto improved maintainability and audit of studies performed on LODs.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 20:36:08 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2020 17:01:48 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Bacry", "Emmanuel", ""], ["Ga\u00efffas", "St\u00e9phane", ""], ["Leroy", "Fanny", ""], ["Morel", "Maryan", ""], ["Nguyen", "Dinh Phong", ""], ["Sebiat", "Youcef", ""], ["Sun", "Dian", ""]]}, {"id": "1910.07055", "submitter": "Hui Zhao", "authors": "Xianwei Cheng, Hui Zhao, Mahmut Kandemir, Saraju Mohanty, Beilei Jiang", "title": "Alleviating Bottlenecks for DNN Execution on GPUs via Opportunistic\n  Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge computing and IoT applications are severely constrained by limited\nhardware resource. This makes memory consuming DNN frameworks not applicable to\nedge computing. Simple algorithms such as direct convolution are finding their\nway in embedded machine learning. As one of the most widely used platforms for\nDNN acceleration, GPUs face the bottleneck of on-chip bandwidth. This work\nintroduces a GPU DNN execution architecture that targets on relieving the\non-chip bandwidth bottleneck by reducing data movement through opportunistic\ncomputing. We first investigate data access patterns in the hardware view\nrather than the software view. Then we propose two opportunistic computing\ntechniques to predictably perform computation when data is available with the\nhelp of assistant warps. By moving computation to data, our techniques are able\nto significantly reduce data movement and relieve the DNN execution bottleneck.\nOur evaluation results show that the proposed technique can improve DNN\napplication performance as much as 55%.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 21:03:15 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Cheng", "Xianwei", ""], ["Zhao", "Hui", ""], ["Kandemir", "Mahmut", ""], ["Mohanty", "Saraju", ""], ["Jiang", "Beilei", ""]]}, {"id": "1910.07110", "submitter": "Rishav Agarwal", "authors": "Rishav Raj Agarwal, Dhruv Kumar, Lukasz Golab and Srinivasan Keshav", "title": "Consentio: Managing Consent to Data Access using Permissioned\n  Blockchains", "comments": "minor changes after reviewwe comments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The increasing amount of personal data is raising serious issues in the\ncontext of privacy, security, and data ownership. Entities whose data are being\ncollected can benefit from mechanisms to manage the parties that can access\ntheir data and to audit who has accessed their data. Consent management systems\naddress these issues. We present Consentio, a scalable consent management\nsystem based on the Hyperledger Fabric permissioned blockchain. The data\nmanagement challenge we address is to ensure high throughput and low latency of\nendorsing data access requests and granting or revoking consent. Experimental\nresults show that our system can handle as many as 6,000 access requests per\nsecond, allowing it to scale to very large deployments.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 00:48:03 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 19:42:31 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Agarwal", "Rishav Raj", ""], ["Kumar", "Dhruv", ""], ["Golab", "Lukasz", ""], ["Keshav", "Srinivasan", ""]]}, {"id": "1910.07144", "submitter": "Guangyan Hu", "authors": "Guangyan Hu, Yongfeng Zhang, Sandro Rigo, Thu D. Nguyen", "title": "Similarity Driven Approximation for Text Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text analytics has become an important part of business intelligence as\nenterprises increasingly seek to extract insights for decision making from text\ndata sets. Processing large text data sets can be computationally expensive,\nhowever, especially if it involves sophisticated algorithms. This challenge is\nexacerbated when it is desirable to run different types of queries against a\ndata set, making it expensive to build multiple indices to speed up query\nprocessing. In this paper, we propose and evaluate a framework called EmApprox\nthat uses approximation to speed up the processing of a wide range of queries\nover large text data sets. The key insight is that different types of queries\ncan be approximated by processing subsets of data that are most similar to the\nqueries. EmApprox builds a general index for a data set by learning a natural\nlanguage processing model, producing a set of highly compressed vectors\nrepresenting words and subcollections of documents. Then, at query processing\ntime, EmApprox uses the index to guide sampling of the data set, with the\nprobability of selecting each subcollection of documents being proportional to\nits {\\em similarity} to the query as computed using the vector representations.\nWe have implemented a prototype of EmApprox as an extension of the Apache Spark\nsystem, and used it to approximate three types of queries: aggregation,\ninformation retrieval, and recommendation. Experimental results show that\nEmApprox's similarity-guided sampling achieves much better accuracy than random\nsampling. Further, EmApprox can achieve significant speedups if users can\ntolerate small amounts of inaccuracies. For example, when sampling at 10\\%,\nEmApprox speeds up a set of queries counting phrase occurrences by almost 10x\nwhile achieving estimated relative errors of less than 22\\% for 90\\% of the\nqueries.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 03:13:23 GMT"}, {"version": "v2", "created": "Sun, 12 Jan 2020 19:54:48 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Hu", "Guangyan", ""], ["Zhang", "Yongfeng", ""], ["Rigo", "Sandro", ""], ["Nguyen", "Thu D.", ""]]}, {"id": "1910.07172", "submitter": "Davit Buniatyan", "authors": "Davit Buniatyan", "title": "Hyper: Distributed Cloud Processing for Large-Scale Deep Learning Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Training and deploying deep learning models in real-world applications\nrequire processing large amounts of data. This is a challenging task when the\namount of data grows to a hundred terabytes, or even, petabyte-scale. We\nintroduce a hybrid distributed cloud framework with a unified view to multiple\nclouds and an on-premise infrastructure for processing tasks using both CPU and\nGPU compute instances at scale. The system implements a distributed file system\nand failure-tolerant task processing scheduler, independent of the language and\nDeep Learning framework used. It allows to utilize unstable cheap resources on\nthe cloud to significantly reduce costs. We demonstrate the scalability of the\nframework on running pre-processing, distributed training, hyperparameter\nsearch and large-scale inference tasks utilizing 10,000 CPU cores and 300 GPU\ninstances with the overall processing power of 30 petaflops.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 05:23:11 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Buniatyan", "Davit", ""]]}, {"id": "1910.07203", "submitter": "Margaux Nattaf", "authors": "Arnaud Malapert, Margaux Nattaf (G-SCOP)", "title": "A new CP-approach for a parallel machine scheduling problem with time\n  constraints on machine qualifications", "comments": null, "journal-ref": "Integration of Constraint Programming, Artificial Intelligence,\n  and Operations Research, pp.426-442, 2019", "doi": "10.1007/978-3-030-19212-9_28", "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the scheduling of job families on parallel machines with\ntime constraints on machine qualifications. In this problem, each job belongs\nto a family and a family can only be executed on a subset of qualified\nmachines. In addition, machines can lose their qualifications during the\nschedule. Indeed, if no job of a family is scheduled on a machine during a\ngiven amount of time, the machine loses its qualification for this family. The\ngoal is to minimize the sum of job completion times, i.e. the flow time, while\nmaximizing the number of qualifications at the end of the schedule. The paper\npresents a new Constraint Programming (CP) model taking more advantages of the\nCP feature to model machine disqualifications. This model is compared with two\nexisting models: an Integer Linear Programming (ILP) model and a Constraint\nProgramming model. The experiments show that the new CP model outperforms the\nother model when the priority is given to the number of disqualifications\nobjective. Furthermore, it is competitive with the other model when the flow\ntime objective is prioritized.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 08:03:30 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Malapert", "Arnaud", "", "G-SCOP"], ["Nattaf", "Margaux", "", "G-SCOP"]]}, {"id": "1910.07402", "submitter": "Jos\\'e \\'Angel Morell Mart\\'inez", "authors": "Jos\\'e \\'A. Morell, Andr\\'es Camero and Enrique Alba", "title": "JSDoop and TensorFlow.js: Volunteer Distributed Web Browser-Based Neural\n  Network Training", "comments": null, "journal-ref": "IEEE Access 7 (2019): 158671-158684", "doi": "10.1109/ACCESS.2019.2950287", "report-no": null, "categories": "cs.DC cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2019, around 57\\% of the population of the world has broadband access to\nthe Internet. Moreover, there are 5.9 billion mobile broadband subscriptions,\ni.e., 1.3 subscriptions per user. So there is an enormous interconnected\ncomputational power held by users all around the world. Also, it is estimated\nthat Internet users spend more than six and a half hours online every day. But\nin spite of being a great amount of time, those resources are idle most of the\nday. Therefore, taking advantage of them presents an interesting opportunity.\nIn this study, we introduce JSDoop, a prototype implementation to profit from\nthis opportunity. In particular, we propose a volunteer web browser-based\nhigh-performance computing library. JSdoop divides a problem into tasks and\nuses different queues to distribute the computation. Then, volunteers access\nthe web page of the problem and start processing the tasks in their web\nbrowsers. We conducted a proof-of-concept using our proposal and TensorFlow.js\nto train a recurrent neural network that predicts text. We tested it in a\ncomputer cluster and with up to 32 volunteers. The experimental results show\nthat training a neural network in distributed web browsers is feasible and\naccurate, has a high scalability, and it is an interesting area for research.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 19:39:01 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Morell", "Jos\u00e9 \u00c1.", ""], ["Camero", "Andr\u00e9s", ""], ["Alba", "Enrique", ""]]}, {"id": "1910.07408", "submitter": "Nina Engelhardt", "authors": "Nina Engelhardt and Hayden K.-H. So", "title": "GraVF-M: Graph Processing System Generation for Multi-FPGA Platforms", "comments": null, "journal-ref": "ACM Trans. Reconfigurable Technol. Syst. 12, 4, Article 21\n  (November 2019)", "doi": "10.1145/3357596", "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the irregular nature of connections in most graph datasets,\npartitioning graph analysis algorithms across multiple computational nodes that\ndo not share a common memory inevitably leads to large amounts of interconnect\ntraffic. Previous research has shown that FPGAs can outcompete software-based\ngraph processing in shared memory contexts, but it remains an open question if\nthis advantage can be maintained in distributed systems.\n  In this work, we present GraVF-M, a framework designed to ease the\nimplementation of FPGA-based graph processing accelerators for multi-FPGA\nplatforms with distributed memory. Based on a lightweight description of the\nalgorithm kernel, the framework automatically generates optimized RTL code for\nthe whole multi-FPGA design. We exploit an aspect of the programming model to\npresent a familiar message-passing paradigm to the user, while under the hood\nimplementing a more efficient architecture that can reduce the necessary\ninter-FPGA network traffic by a factor equal to the average degree of the input\ngraph. A performance model based on a theoretical analysis of the factors\ninfluencing performance serves to evaluate the efficiency of our\nimplementation. With a throughput of up to 5.8 GTEPS (billions of traversed\nedges per second) on a 4-FPGA system, the designs generated by GraVF-M compare\nfavorably to state-of-the-art frameworks from the literature and reach 94% of\nthe projected performance limit of the system.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 10:09:14 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Engelhardt", "Nina", ""], ["So", "Hayden K. -H.", ""]]}, {"id": "1910.07561", "submitter": "Xiaorui Liu", "authors": "Xiaorui Liu, Yao Li, Jiliang Tang and Ming Yan", "title": "A Double Residual Compression Algorithm for Efficient Distributed\n  Learning", "comments": null, "journal-ref": "PMLR 108:133-143, 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale machine learning models are often trained by parallel stochastic\ngradient descent algorithms. However, the communication cost of gradient\naggregation and model synchronization between the master and worker nodes\nbecomes the major obstacle for efficient learning as the number of workers and\nthe dimension of the model increase. In this paper, we propose DORE, a DOuble\nREsidual compression stochastic gradient descent algorithm, to reduce over\n$95\\%$ of the overall communication such that the obstacle can be immensely\nmitigated. Our theoretical analyses demonstrate that the proposed strategy has\nsuperior convergence properties for both strongly convex and nonconvex\nobjective functions. The experimental results validate that DORE achieves the\nbest communication efficiency while maintaining similar model accuracy and\nconvergence speed in comparison with start-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 18:19:19 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Liu", "Xiaorui", ""], ["Li", "Yao", ""], ["Tang", "Jiliang", ""], ["Yan", "Ming", ""]]}, {"id": "1910.07566", "submitter": "Ivy Peng", "authors": "Ivy B. Peng, Marty McFadden, Eric Green, Keita Iwabuchi, Kai Wu, Dong\n  Li, Roger Pearce, Maya Gokhale", "title": "UMap: Enabling Application-driven Optimizations for Page Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leadership supercomputers feature a diversity of storage, from node-local\npersistent memory and NVMe SSDs to network-interconnected flash memory and HDD.\nMemory mapping files on different tiers of storage provides a uniform interface\nin applications. However, system-wide services like mmap are optimized for\ngenerality and lack flexibility for enabling application-specific\noptimizations. In this work, we present Umap to enable user-space page\nmanagement that can be easily adapted to access patterns in applications and\nstorage characteristics. Umap uses the userfaultfd mechanism to handle page\nfaults in multi-threaded applications efficiently. By providing a data object\nabstraction layer, Umap is extensible to support various backing stores. The\ndesign of Umap supports dynamic load balancing and I/O decoupling for scalable\nperformance. Umap also uses application hints to improve the selection of\ncaching, prefetching, and eviction policies. We evaluate Umap in five\nbenchmarks and real applications on two systems. Our results show that\nleveraging application knowledge for page management could substantially\nimprove performance. On average, Umap achieved 1.25 to 2.5 times improvement\nusing the adapted configurations compared to the system service.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 18:41:58 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Peng", "Ivy B.", ""], ["McFadden", "Marty", ""], ["Green", "Eric", ""], ["Iwabuchi", "Keita", ""], ["Wu", "Kai", ""], ["Li", "Dong", ""], ["Pearce", "Roger", ""], ["Gokhale", "Maya", ""]]}, {"id": "1910.07696", "submitter": "Vibhuti Gupta", "authors": "Vibhuti Gupta, Rattikorn Hewett", "title": "Adaptive Normalization in Streaming Data", "comments": null, "journal-ref": null, "doi": "10.1145/3372454.3372466", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In todays digital era, data are everywhere from Internet of Things to health\ncare or financial applications. This leads to potentially unbounded\never-growing Big data streams and it needs to be utilized effectively. Data\nnormalization is an important preprocessing technique for data analytics. It\nhelps prevent mismodeling and reduce the complexity inherent in the data\nespecially for data integrated from multiple sources and contexts.\nNormalization of Big Data stream is challenging because of evolving\ninconsistencies, time and memory constraints, and non-availability of whole\ndata beforehand. This paper proposes a distributed approach to adaptive\nnormalization for Big data stream. Using sliding windows of fixed size, it\nprovides a simple mechanism to adapt the statistics for normalizing changing\ndata in each window. Implemented on Apache Storm, a distributed real-time\nstream data framework, our approach exploits distributed data processing for\nefficient normalization. Unlike other existing adaptive approaches that\nnormalize data for a specific use (e.g., classification), ours does not.\nMoreover, our adaptive mechanism allows flexible controls, via user-specified\nthresholds, for normalization tradeoffs between time and precision. The paper\nillustrates our proposed approach along with a few other techniques and\nexperiments on both synthesized and real-world data. The normalized data\nobtained from our proposed approach, on 160,000 instances of data stream,\nimproves over the baseline by 89% with 0.0041 root-mean-square error compared\nwith the actual data.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 03:40:52 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Gupta", "Vibhuti", ""], ["Hewett", "Rattikorn", ""]]}, {"id": "1910.07700", "submitter": "Arjun Singhvi", "authors": "Arjun Singhvi, Junaid Khalid, Aditya Akella, Sujata Banerjee", "title": "SNF: Serverless Network Functions", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is increasingly common to outsource network functions (NFs) to the cloud.\nHowever, no cloud providers offer NFs-as-a-Service (NFaaS) that allows users to\nrun custom NFs. Our work addresses how a cloud provider can offer NFaaS. We use\nthe emerging serverless computing paradigm as it has the right building blocks\n- usage-based billing, convenient event-driven programming model and automatic\ncompute elasticity. Towards this end, we identify two core limitations of\nexisting serverless platforms to support demanding stateful NFs - coupling of\nthe billing and work assignment granularities, and state sharing via an\nexternal store. We develop a novel NFaaS framework, SNF, that overcomes these\nissues using two ideas. SNF allocates work at the granularity of flowlets\nobserved in network traffic, whereas billing and programming occur on the basis\nof packets. SNF embellishes serverless platforms with ephemeral state that\nlasts for the duration of the flowlet and supports high performance state\noperations between compute units in a peer-to-peer manner. We present\nalgorithms for work allocation and state maintenance, and demonstrate that our\nSNF prototype dynamically adapts compute resources for various stateful NFs\nbased on traffic demand at very fine time scales, with minimal overheads.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 03:56:06 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Singhvi", "Arjun", ""], ["Khalid", "Junaid", ""], ["Akella", "Aditya", ""], ["Banerjee", "Sujata", ""]]}, {"id": "1910.07703", "submitter": "Jiacheng Zhuo", "authors": "Jiacheng Zhuo, Qi Lei, Alexandros G. Dimakis, Constantine Caramanis", "title": "Communication-Efficient Asynchronous Stochastic Frank-Wolfe over\n  Nuclear-norm Balls", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale machine learning training suffers from two prior challenges,\nspecifically for nuclear-norm constrained problems with distributed systems:\nthe synchronization slowdown due to the straggling workers, and high\ncommunication costs. In this work, we propose an asynchronous Stochastic Frank\nWolfe (SFW-asyn) method, which, for the first time, solves the two problems\nsimultaneously, while successfully maintaining the same convergence rate as the\nvanilla SFW. We implement our algorithm in python (with MPI) to run on Amazon\nEC2, and demonstrate that SFW-asyn yields speed-ups almost linear to the number\nof machines compared to the vanilla SFW.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 04:22:21 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Zhuo", "Jiacheng", ""], ["Lei", "Qi", ""], ["Dimakis", "Alexandros G.", ""], ["Caramanis", "Constantine", ""]]}, {"id": "1910.07776", "submitter": "Saeed Taheri", "authors": "Saeed Taheri and Apan Qasem and Martin Burtscher", "title": "A Tool for Automatically Suggesting Source-Code Optimizations for\n  Complex GPU Kernels", "comments": null, "journal-ref": "proceedings of the 2015 International Conference on Parallel and\n  Distributed Processing Techniques and Applications, page 589-599 :\n  WORLDCOMP'15, July 27-30, 2015, Las Vegas, Nevada", "doi": null, "report-no": null, "categories": "cs.DC cs.PF cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future computing systems, from handhelds to supercomputers, will undoubtedly\nbe more parallel and heterogeneous than todays systems to provide more\nperformance and energy efficiency. Thus, GPUs are increasingly being used to\naccelerate general purpose applications, including applications with data\ndependent, irregular control flow and memory access patterns. However, the\ngrowing complexity, exposed memory hierarchy, incoherence, heterogeneity, and\nparallelism will make accelerator based systems progressively more difficult to\nprogram. In the foreseeable future, the vast majority of programmers will no\nlonger be able to extract additional performance or energy savings from next\ngeneration systems be-cause the programming will be too difficult. Automatic\nperformance analysis and optimization recommendation tools have the potential\nto avert this situation. They embody expert knowledge and make it available to\nsoftware developers when needed. In this paper, we describe and evaluate such a\ntool.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 08:57:01 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Taheri", "Saeed", ""], ["Qasem", "Apan", ""], ["Burtscher", "Martin", ""]]}, {"id": "1910.07782", "submitter": "Lum Ramabaja", "authors": "Lum Ramabaja, Arber Avdullahu", "title": "The Distributed Bloom Filter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Distributed Bloom Filter is a space-efficient, probabilistic data\nstructure designed to perform more efficient set reconciliations in distributed\nsystems. It guarantees eventual consistency of states between nodes in a\nsystem, while still keeping bloom filter sizes as compact as possible. The\neventuality can be tweaked as desired, by tweaking the distributed bloom\nfilter's parameters. The scalability, as well as accuracy of the data structure\nis made possible by combining two novel ideas: The first idea introduces a new,\ncomputationally inexpensive way for populating bloom filters, making it\npossible to quickly compute new bloom filters when interacting with peers. The\nsecond idea introduces the concept of unique bloom filter mappings between\npeers. By applying these two simple ideas, one can achieve incredibly\nbandwidth-efficient set reconciliation in networks. Instead of trying to\nminimize the false positive rate of a single bloom filter, we use the unique\nbloom filter mappings to increase the probability for an element to propagate\nthrough a network. We compare the standard bloom filter with the distributed\nbloom filter and show that even with a false positive rate of 50%, i.e. even\nwith a very small bloom filter size, the distributed bloom filter still manages\nto reach complete set reconciliation across the network in a highly\nspace-efficient, as well as time-efficient way.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 09:21:17 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 11:57:57 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Ramabaja", "Lum", ""], ["Avdullahu", "Arber", ""]]}, {"id": "1910.07850", "submitter": "Salvatore Cielo", "authors": "Salvatore Cielo, Luigi Iapichino, Johannes G\\\"unther, Christoph\n  Federrath, Elisabeth Mayer, Markus Wiedemann", "title": "Visualizing the world's largest turbulence simulation", "comments": "6 pages, 5 figures, accompanying paper of SC19 visualization showcase\n  finalist. The full video is publicly available under\n  https://www.youtube.com/watch?v=EPe1Ho5qRuM", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph astro-ph.GA astro-ph.IM astro-ph.SR cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this exploratory submission we present the visualization of the largest\ninterstellar turbulence simulations ever performed, unravelling key\nastrophysical processes concerning the formation of stars and the relative role\nof magnetic fields. The simulations, including pure hydrodynamical (HD) and\nmagneto-hydrodynamical (MHD) runs, up to a size of $10048^3$ grid elements,\nwere produced on the supercomputers of the Leibniz Supercomputing Centre and\nvisualized using the hybrid parallel (MPI+TBB) ray-tracing engine OSPRay\nassociated with VisIt. Besides revealing features of turbulence with an\nunprecedented resolution, the visualizations brilliantly showcase the\nstretching-and-folding mechanisms through which astrophysical processes such as\nsupernova explosions drive turbulence and amplify the magnetic field in the\ninterstellar gas, and how the first structures, the seeds of newborn stars are\nshaped by this process.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 12:16:48 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Cielo", "Salvatore", ""], ["Iapichino", "Luigi", ""], ["G\u00fcnther", "Johannes", ""], ["Federrath", "Christoph", ""], ["Mayer", "Elisabeth", ""], ["Wiedemann", "Markus", ""]]}, {"id": "1910.07855", "submitter": "Salvatore Cielo", "authors": "Salvatore Cielo, Luigi Iapichino, Fabio Baruffa", "title": "Speeding simulation analysis up with yt and Intel Distribution for\n  Python", "comments": "3 pages, 1 figure, published on Intel Parallel Universe Magazine", "journal-ref": "Issue 38, 2019, p. 27-32", "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As modern scientific simulations grow ever more in size and complexity, even\ntheir analysis and post-processing becomes increasingly demanding, calling for\nthe use of HPC resources and methods. yt is a parallel, open source\npost-processing python package for numerical simulations in astrophysics, made\npopular by its cross-format compatibility, its active community of developers\nand its integration with several other professional Python instruments. The\nIntel Distribution for Python enhances yt's performance and parallel\nscalability, through the optimization of lower-level libraries Numpy and Scipy,\nwhich make use of the optimized Intel Math Kernel Library (Intel-MKL) and the\nIntel MPI library for distributed computing. The library package yt is used for\nseveral analysis tasks, including integration of derived quantities, volumetric\nrendering, 2D phase plots, cosmological halo analysis and production of\nsynthetic X-ray observation. In this paper, we provide a brief tutorial for the\ninstallation of yt and the Intel Distribution for Python, and the execution of\neach analysis task. Compared to the Anaconda python distribution, using the\nprovided solution one can achieve net speedups up to 4.6x on Intel Xeon\nScalable processors (codename Skylake).\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 12:28:46 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Cielo", "Salvatore", ""], ["Iapichino", "Luigi", ""], ["Baruffa", "Fabio", ""]]}, {"id": "1910.07867", "submitter": "Ankit Chaudhary", "authors": "Steffen Zeuch and Ankit Chaudhary and Bonaventura Del Monte and\n  Haralampos Gavriilidis and Dimitrios Giouroukis and Philipp M. Grulich and\n  Sebastian Bress and Jonas Traub and Volker Markl", "title": "The NebulaStream Platform: Data and Application Management for the\n  Internet of Things", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet of Things (IoT) presents a novel computing architecture for data\nmanagement: a distributed, highly dynamic, and heterogeneous environment of\nmassive scale. Applications for the IoT introduce new challenges for\nintegrating the concepts of fog and cloud computing as well as sensor networks\nin one unified environment. In this paper, we highlight these major challenges\nand outline how existing systems handle them. To address these challenges, we\nintroduce the NebulaStream platform, a general purpose, endto-end data\nmanagement system for the IoT. NebulaStream addresses the heterogeneity and\ndistribution of compute and data, supports diverse data and programming models\ngoing beyond relational algebra, deals with potentially unreliable\ncommunication, and enables constant evolution under continuous operation. In\nour evaluation, we demonstrate the effectiveness of our approach by providing\nearly results on partial aspects.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 12:49:22 GMT"}, {"version": "v2", "created": "Fri, 15 Nov 2019 10:49:20 GMT"}, {"version": "v3", "created": "Wed, 4 Mar 2020 09:03:43 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Zeuch", "Steffen", ""], ["Chaudhary", "Ankit", ""], ["Del Monte", "Bonaventura", ""], ["Gavriilidis", "Haralampos", ""], ["Giouroukis", "Dimitrios", ""], ["Grulich", "Philipp M.", ""], ["Bress", "Sebastian", ""], ["Traub", "Jonas", ""], ["Markl", "Volker", ""]]}, {"id": "1910.08038", "submitter": "Lichao Sun", "authors": "Lichao Sun, Ji Wang, Philip S. Yu, Lifang He", "title": "Not Just Cloud Privacy: Protecting Client Privacy in Teacher-Student\n  Learning", "comments": "some parts incorrect and unclear about the definition LDP for\n  protection", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensuring the privacy of sensitive data used to train modern machine learning\nmodels is of paramount importance in many areas of practice. One recent popular\napproach to study these concerns is using the differential privacy via a\n\"teacher-student\" model, wherein the teacher provides the student with useful,\nbut noisy, information, hopefully allowing the student model to perform well on\na given task. However, these studies only solve the privacy concerns of the\nteacher by assuming the student owns a public but unlabelled dataset. In real\nlife, the student also has privacy concerns on its unlabelled data, so as to\ninquire about privacy protection on any data sent to the teacher. In this work,\nwe re-design the privacy-preserving \"teacher-student\" model consisting of\nadopting both private arbitrary masking and local differential privacy, which\nprotects the sensitive information of each student sample. However, the\ntraditional training of teacher model is not robust on any perturbed data. We\nuse the adversarial learning techniques to improve the robustness of the\nperturbed sample that supports returning good feedback without having all\nprivate information of each student sample. The experimental results\ndemonstrate the effectiveness of our new privacy-preserving \"teacher-student\"\nmodel.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 17:06:41 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 15:37:16 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Sun", "Lichao", ""], ["Wang", "Ji", ""], ["Yu", "Philip S.", ""], ["He", "Lifang", ""]]}, {"id": "1910.08232", "submitter": "Shahzad Shahzad", "authors": "Shahzad and Eun-Sung Jung", "title": "FLIP:FLexible IoT Path Programming Framework for Large-scale IoT", "comments": "10 pages, 8 Figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid increase in smart objects forming IoT fabric, it is inevitable\nto see billions of devices connected together, forming large-scale IoT\nnetworks. This expeditious increase in IoT devices is giving rise to increased\nuser requirements and network complexity. Collecting big data from these IoT\ndevices with optimal network utilization and simplicity is becoming more and\nmore challenging. This paper proposes FLIP- FLexible IoT Path Programming\nFramework for Large-scale IoT. The distinctive feature of FLIP is that it\nfocuses on the IoT fabric from the perspective of user requirements and uses\nSDN techniques along with DPI technology to efficiently fulfill the user\nrequirements and establish datapath in the network in an automated and\ndistributed manner. FLIP utilizes SDN structure to optimize network utilization\nthrough in-network computing and automated datapath establishment, also hiding\nnetwork complexity along the way. We evaluated our framework through\nexperiments, and results indicate that FLIP has the potential to fulfill user\nrequirements in an automated fashion and optimize network utilization.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 03:12:17 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Shahzad", "", ""], ["Jung", "Eun-Sung", ""]]}, {"id": "1910.08234", "submitter": "Xin Yao", "authors": "Xin Yao, Tianchi Huang, Rui-Xiao Zhang, Ruiyu Li, Lifeng Sun", "title": "Federated Learning with Unbiased Gradient Aggregation and Controllable\n  Meta Updating", "comments": "This manuscript has been accepted to the Workshop on Federated\n  Learning for Data Privacy and Confidentiality (FL - NeurIPS 2019, in\n  Conjunction with NeurIPS 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) aims to train machine learning models in the\ndecentralized system consisting of an enormous amount of smart edge devices.\nFederated averaging (FedAvg), the fundamental algorithm in FL settings,\nproposes on-device training and model aggregation to avoid the potential heavy\ncommunication costs and privacy concerns brought by transmitting raw data.\nHowever, through theoretical analysis we argue that 1) the multiple steps of\nlocal updating will result in gradient biases and 2) there is an inconsistency\nbetween the expected target distribution and the optimization objectives\nfollowing the training paradigm in FedAvg. To tackle these problems, we first\npropose an unbiased gradient aggregation algorithm with the keep-trace gradient\ndescent and the gradient evaluation strategy. Then we introduce an additional\ncontrollable meta updating procedure with a small set of data samples,\nindicating the expected target distribution, to provide a clear and consistent\noptimization objective. Both the two improvements are model- and task-agnostic\nand can be applied individually or together. Experimental results demonstrate\nthat the proposed methods are faster in convergence and achieve higher accuracy\nwith different network architectures in various FL settings.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 03:17:22 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 09:17:12 GMT"}, {"version": "v3", "created": "Wed, 16 Dec 2020 06:26:36 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Yao", "Xin", ""], ["Huang", "Tianchi", ""], ["Zhang", "Rui-Xiao", ""], ["Li", "Ruiyu", ""], ["Sun", "Lifeng", ""]]}, {"id": "1910.08248", "submitter": "Duong Nguyen", "authors": "Duong Nguyen, Sandeep S. Kulkarni", "title": "Benefits of Stabilization versus Rollback in Eventually Consistent\n  Key-Value Stores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we evaluate and compare the performance of two approaches,\nnamely self-stabilization and rollback, to handling consistency violation\nfaults (cvf) that occurred when a distributed program is executed on eventually\nconsistent key-value store. We observe that self-stabilization is usually\nbetter than rollbacks in our experiments. Moreover, when we aggressively allow\nmore cvf in exchange of eliminating mechanisms for guaranteeing atomicity\nrequirements of actions, we observe the programs in our case studies achieve a\nspeedup between 2--15 times compared with the standard implementation. We also\nanalyze different factors that contribute to the results. Our results and\nanalysis are useful in helping a system designer choose proper design options\nfor their program.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 03:53:11 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Nguyen", "Duong", ""], ["Kulkarni", "Sandeep S.", ""]]}, {"id": "1910.08385", "submitter": "Aleksei Triastcyn", "authors": "Aleksei Triastcyn, Boi Faltings", "title": "Federated Generative Privacy", "comments": "IJCAI Workshop on Federated Machine Learning for User Privacy and\n  Data Confidentiality (FL-IJCAI 2019). 7 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose FedGP, a framework for privacy-preserving data\nrelease in the federated learning setting. We use generative adversarial\nnetworks, generator components of which are trained by FedAvg algorithm, to\ndraw privacy-preserving artificial data samples and empirically assess the risk\nof information disclosure. Our experiments show that FedGP is able to generate\nlabelled data of high quality to successfully train and validate supervised\nmodels. Finally, we demonstrate that our approach significantly reduces\nvulnerability of such models to model inversion attacks.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 12:41:15 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Triastcyn", "Aleksei", ""], ["Faltings", "Boi", ""]]}, {"id": "1910.08494", "submitter": "Xiaoke Zhu", "authors": "Xiaoke Zhu and Qi Zhang and Ling Liu and Taining Cheng and Shaowen Yao\n  and Wei Zhou and and Jing He", "title": "DLB: Deep Learning Based Load Balancing", "comments": "6 pages, IEEE CLOUD2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce DLB, a Deep Learning based load Balancing\nmechanism, to effectively address the data skew problem. The key idea of DLB is\nto replace hash functions in the load balancing mechanisms with deep learning\nmodels, which are trained to be able to map different distributions of\nworkloads and data to the servers in a uniformed manner. We implemented DLB and\ndeployed it on a practical Cloud environment using CloudSim. Experimental\nresults using both synthetic and real-world data sets show that compared with\ntraditional hash function based load balancing methods, DLB is able to achieve\nmore balanced mappings, especially when the workload is highly skewed.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 16:30:09 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2019 07:59:16 GMT"}, {"version": "v3", "created": "Wed, 21 Jul 2021 15:00:34 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Zhu", "Xiaoke", ""], ["Zhang", "Qi", ""], ["Liu", "Ling", ""], ["Cheng", "Taining", ""], ["Yao", "Shaowen", ""], ["Zhou", "Wei", ""], ["He", "and Jing", ""]]}, {"id": "1910.08498", "submitter": "Ji\\v{r}\\'i Filipovi\\v{c}", "authors": "Filip Petrovi\\v{c}, David St\\v{r}el\\'ak, Jana Hozzov\\'a, Jaroslav\n  O\\v{l}ha, Richard Trembeck\\'y, Siegfried Benkner, Ji\\v{r}\\'i Filipovi\\v{c}", "title": "A Benchmark Set of Highly-efficient CUDA and OpenCL Kernels and its\n  Dynamic Autotuning with Kernel Tuning Toolkit", "comments": null, "journal-ref": "Petrovic et al., A benchmark set of highly-efficient CUDA and\n  OpenCL kernels and its dynamic autotuning with Kernel Tuning Toolkit. In\n  Future Generation Computer Systems, Vol. 108, pages 161-177. 2020", "doi": "10.1016/j.future.2020.02.069", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autotuning of performance-relevant source-code parameters allows to\nautomatically tune applications without hard coding optimizations and thus\nhelps with keeping the performance portable. In this paper, we introduce a\nbenchmark set of ten autotunable kernels for important computational problems\nimplemented in OpenCL or CUDA. Using our Kernel Tuning Toolkit, we show that\nwith autotuning most of the kernels reach near-peak performance on various GPUs\nand outperform baseline implementations on CPUs and Xeon Phis. Our evaluation\nalso demonstrates that autotuning is key to performance portability. In\naddition to offline tuning, we also introduce dynamic autotuning of code\noptimization parameters during application runtime. With dynamic tuning, the\nKernel Tuning Toolkit enables applications to re-tune performance-critical\nkernels at runtime whenever needed, for example, when input data changes.\nAlthough it is generally believed that autotuning spaces tend to be too large\nto be searched during application runtime, we show that it is not necessarily\nthe case when tuning spaces are designed rationally. Many of our kernels reach\nnear peak-performance with moderately sized tuning spaces that can be searched\nat runtime with acceptable overhead. Finally we demonstrate, how dynamic\nperformance tuning can be integrated into a real-world application from\ncryo-electron microscopy domain.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 16:46:23 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 11:26:30 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Petrovi\u010d", "Filip", ""], ["St\u0159el\u00e1k", "David", ""], ["Hozzov\u00e1", "Jana", ""], ["O\u013eha", "Jaroslav", ""], ["Trembeck\u00fd", "Richard", ""], ["Benkner", "Siegfried", ""], ["Filipovi\u010d", "Ji\u0159\u00ed", ""]]}, {"id": "1910.08510", "submitter": "Jianyu Niu", "authors": "Jianyu Niu, Chen Feng, Hoang Dau, Yu-Chih Huang, Jingge Zhu", "title": "Analysis of Nakamoto Consensus, Revisited", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Bitcoin white paper, Nakamoto proposed a very simple Byzantine fault\ntolerant consensus algorithm that is also known as Nakamoto consensus. Despite\nits simplicity, some existing analysis of Nakamoto consensus appears to be long\nand involved. In this technical report, we aim to make such analysis simple and\ntransparent so that we can teach senior undergraduate students and graduate\nstudents in our institutions. This report is largely based on a 3-hour tutorial\ngiven by one of the authors in June 2019.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 17:13:47 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2019 05:40:54 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Niu", "Jianyu", ""], ["Feng", "Chen", ""], ["Dau", "Hoang", ""], ["Huang", "Yu-Chih", ""], ["Zhu", "Jingge", ""]]}, {"id": "1910.08547", "submitter": "Himanshu Gupta", "authors": "Himanshu Gupta (IIT Madras), Dharanipragada Janakiram (IIT Madras)", "title": "CDAG: A Serialized blockDAG for Permissioned Blockchain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Blockchain is maintained as a global log between a network of nodes and uses\ncryptographic distributed protocols to synchronize the updates. As adopted by\nBitcoin and Ethereum these update operations to the ledger are serialized, and\nexecuted in batches. To safeguard the system against the generation of\nconflicting sets of updates and maintain the consistency of the ledger, the\nfrequency of the updates is controlled, which severely affects the performance\nof the system.\n  This paper presents Converging Directed Acyclic Graph (CDAG), as a substitute\nfor the chain and DAG structures used in other blockchain protocols. CDAG\nallows multiple parallel updates to the ledger and converges them at the next\nstep providing finality to the blocks. It partitions the updates into\nnon-intersecting buckets of transactions to prevent the generation of\nconflicting blocks and divide the time into slots to provide enough time for\nthem to propagate in the network. Multiple simultaneous updates improve the\nthroughput of CDAG, and the converging step helps to finalize them faster, even\nin the presence of conflicts. Moreover, CDAG provides a total order among the\nblocks of the ledger to support smart contracts, unlike some of the other\nblockDAG protocols.\n  We evaluate the performance of CDAG on Google Cloud Platform using Google\nKubernetes Engine, simulating a real-time network. Experimental results show\nthat CDAG achieves a throughput of more than 2000 transactions per second and\nconfirms them well in under 2 minutes. Also, the protocol scales well in\ncomparison to other permissioned protocols, and the capacity of the network\nonly limits the performance.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 13:26:52 GMT"}, {"version": "v2", "created": "Thu, 24 Oct 2019 06:59:30 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Gupta", "Himanshu", "", "IIT Madras"], ["Janakiram", "Dharanipragada", "", "IIT Madras"]]}, {"id": "1910.08625", "submitter": "Bruce Cox", "authors": "Petar D. Jackovich, Bruce A. Cox, and Raymond R. Hill", "title": "Comparing Greedy Constructive Heuristic Subtour Elimination Methods for\n  the Traveling Salesman Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper further defines the class of fragment constructive heuristics used\nto compute feasible solutions for the Traveling Salesman Problem into\narc-greedy and node-greedy subclasses. Since these subclasses of heuristics can\ncreate subtours, two known methodologies for subtour elimination on symmetric\ninstances are reviewed and are expanded to cover asymmetric problem instances.\nThis paper introduces a third novel methodology, the Greedy Tracker, and\ncompares it to both known methodologies. Computational results are generated\nacross multiple symmetric and asymmetric instances. The results demonstrate the\nGreedy Tracker is the fastest method for preventing subtours for instances\nbelow 400 nodes. A distinction between fragment constructive heuristics and the\nsubtour elimination methodology used to ensure the feasibility of resulting\nsolutions enables the introduction of a new node-greedy fragment heuristic\ncalled Ordered Greedy.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 19:13:27 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Jackovich", "Petar D.", ""], ["Cox", "Bruce A.", ""], ["Hill", "Raymond R.", ""]]}, {"id": "1910.08663", "submitter": "Kevin Hsieh", "authors": "Kevin Hsieh", "title": "Machine Learning Systems for Highly-Distributed and Rapidly-Growing Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The usability and practicality of any machine learning (ML) applications are\nlargely influenced by two critical but hard-to-attain factors: low latency and\nlow cost. Unfortunately, achieving low latency and low cost is very challenging\nwhen ML depends on real-world data that are highly distributed and rapidly\ngrowing (e.g., data collected by mobile phones and video cameras all over the\nworld). Such real-world data pose many challenges in communication and\ncomputation. For example, when training data are distributed across data\ncenters that span multiple continents, communication among data centers can\neasily overwhelm the limited wide-area network bandwidth, leading to\nprohibitively high latency and high cost.\n  In this dissertation, we demonstrate that the latency and cost of ML on\nhighly-distributed and rapidly-growing data can be improved by one to two\norders of magnitude by designing ML systems that exploit the characteristics of\nML algorithms, ML model structures, and ML training/serving data. We support\nthis thesis statement with three contributions. First, we design a system that\nprovides both low-latency and low-cost ML serving (inferencing) over\nlarge-scale and continuously-growing datasets, such as videos. Second, we build\na system that makes ML training over geo-distributed datasets as fast as\ntraining within a single data center. Third, we present a first detailed study\nand a system-level solution on a fundamental and largely overlooked problem: ML\ntraining over non-IID (i.e., not independent and identically distributed) data\npartitions (e.g., facial images collected by cameras varies according to the\ndemographics of each camera's location).\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 23:59:00 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Hsieh", "Kevin", ""]]}, {"id": "1910.08713", "submitter": "Muhammad Aslam Jarwar", "authors": "Sajjad Ali, Muhammad Aslam Jarwar, Ilyoung Chong", "title": "Microservices based Framework to Support Interoperable IoT Applications\n  for Enhanced Data Analytics", "comments": "Proceedings of Symposium of the Korean Institute of communications\n  and Information Sciences , 2019.1, 636-639", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Internet of things is growing with a large number of diverse objects which\ngenerate billions of data streams by sensing, actuating and communicating.\nManagement of heterogeneous IoT objects with existing approaches and processing\nof myriads of data from these objects using monolithic services have become\nmajor challenges in developing effective IoT applications. The heterogeneity\ncan be resolved by providing interoperability with semantic virtualization of\nobjects. Moreover, monolithic services can be substituted with modular\nmicroservices. This article presents an architecture that enables the\ndevelopment of IoT applications using semantically interoperable microservices\nand virtual objects. The proposed framework supports analytic features with\nknowledge-driven and data-driven techniques to provision intelligent services\non top of interoperable microservices in Web Objects enabled IoT environment.\nThe knowledge-driven aspects are supported with reasoning on semantic ontology\nmodels and the data-driven aspects are realized with machine learning pipeline.\nThe development of service functionalities is supported with microservices to\nenhance modularity and reusability. To evaluate the proposed framework a proof\nof concept implementation with a use case is discussed.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 06:59:16 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Ali", "Sajjad", ""], ["Jarwar", "Muhammad Aslam", ""], ["Chong", "Ilyoung", ""]]}, {"id": "1910.08925", "submitter": "Dong Dai", "authors": "Di Zhang, Dong Dai, Youbiao He, Forrest Sheng Bao, Bing Xie", "title": "RLScheduler: An Automated HPC Batch Job Scheduler Using Reinforcement\n  Learning", "comments": "14 pages; conference accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today high-performance computing (HPC) platforms are still dominated by batch\njobs. Accordingly, effective batch job scheduling is crucial to obtain high\nsystem efficiency. Existing HPC batch job schedulers typically leverage\nheuristic priority functions to prioritize and schedule jobs. But, once\nconfigured and deployed by the experts, such priority functions can hardly\nadapt to the changes of job loads, optimization goals, or system settings,\npotentially leading to degraded system efficiency when changes occur. To\naddress this fundamental issue, we present RLScheduler, an automated HPC batch\njob scheduler built on reinforcement learning. RLScheduler relies on minimal\nmanual interventions or expert knowledge, but can learn high-quality scheduling\npolicies via its own continuous 'trial and error'. We introduce a new\nkernel-based neural network structure and trajectory filtering mechanism in\nRLScheduler to improve and stabilize the learning process. Through extensive\nevaluations, we confirm that RLScheduler can learn high-quality scheduling\npolicies towards various workloads and various optimization goals with\nrelatively low computation cost. Moreover, we show that the learned models\nperform stably even when applied to unseen workloads, making them practical for\nproduction use.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 08:14:28 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2020 15:11:44 GMT"}, {"version": "v3", "created": "Wed, 2 Sep 2020 00:58:23 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Zhang", "Di", ""], ["Dai", "Dong", ""], ["He", "Youbiao", ""], ["Bao", "Forrest Sheng", ""], ["Xie", "Bing", ""]]}, {"id": "1910.08951", "submitter": "Kleomenis Katevas", "authors": "Matteo Varvello, Kleomenis Katevas, Mihai Plesa, Hamed Haddadi,\n  Benjamin Livshits", "title": "BatteryLab, A Distributed Power Monitoring Platform For Mobile Devices", "comments": "8 pages, 8 figures, HotNets 2019 paper", "journal-ref": "HotNets 2019", "doi": "10.1145/3365609.3365852", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in cloud computing have simplified the way that both software\ndevelopment and testing are performed. Unfortunately, this is not true for\nbattery testing for which state of the art test-beds simply consist of one\nphone attached to a power meter. These test-beds have limited resources,\naccess, and are overall hard to maintain; for these reasons, they often sit\nidle with no experiment to run. In this paper, we propose to share existing\nbattery testing setups and build BatteryLab, a distributed platform for battery\nmeasurements. Our vision is to transform independent battery testing setups\ninto vantage points of a planetary-scale measurement platform offering\nheterogeneous devices and testing conditions. In the paper, we design and\ndeploy a combination of hardware and software solutions to enable BatteryLab's\nvision. We then preliminarily evaluate BatteryLab's accuracy of battery\nreporting, along with some system benchmarking. We also demonstrate how\nBatteryLab can be used by researchers to investigate a simple research\nquestion.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 11:23:34 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Varvello", "Matteo", ""], ["Katevas", "Kleomenis", ""], ["Plesa", "Mihai", ""], ["Haddadi", "Hamed", ""], ["Livshits", "Benjamin", ""]]}, {"id": "1910.09017", "submitter": "Maciej Besta", "authors": "Maciej Besta, Emanuel Peter, Robert Gerstenberger, Marc Fischer,\n  Micha{\\l} Podstawski, Claude Barthels, Gustavo Alonso, Torsten Hoefler", "title": "Demystifying Graph Databases: Analysis and Taxonomy of Data\n  Organization, System Designs, and Graph Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph processing has become an important part of multiple areas of computer\nscience, such as machine learning, computational sciences, medical\napplications, social network analysis, and many others. Numerous graphs such as\nweb or social networks may contain up to trillions of edges. Often, these\ngraphs are also dynamic (their structure changes over time) and have\ndomain-specific rich data associated with vertices and edges. Graph database\nsystems such as Neo4j enable storing, processing, and analyzing such large,\nevolving, and rich datasets. Due to the sheer size of such datasets, combined\nwith the irregular nature of graph processing, these systems face unique design\nchallenges. To facilitate the understanding of this emerging domain, we present\nthe first survey and taxonomy of graph database systems. We focus on\nidentifying and analyzing fundamental categories of these systems (e.g., triple\nstores, tuple stores, native graph database systems, or object-oriented\nsystems), the associated graph models (e.g., RDF or Labeled Property Graph),\ndata organization techniques (e.g., storing graph data in indexing structures\nor dividing data into records), and different aspects of data distribution and\nquery execution (e.g., support for sharding and ACID). 45 graph database\nsystems are presented and compared, including Neo4j, OrientDB, or Virtuoso. We\noutline graph database queries and relationships with associated domains (NoSQL\nstores, graph streaming, and dynamic graph algorithms). Finally, we describe\nresearch and engineering challenges to outline the future of graph databases.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 16:45:15 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 13:15:02 GMT"}, {"version": "v3", "created": "Thu, 2 Apr 2020 14:19:20 GMT"}, {"version": "v4", "created": "Fri, 3 Apr 2020 09:54:33 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Besta", "Maciej", ""], ["Peter", "Emanuel", ""], ["Gerstenberger", "Robert", ""], ["Fischer", "Marc", ""], ["Podstawski", "Micha\u0142", ""], ["Barthels", "Claude", ""], ["Alonso", "Gustavo", ""], ["Hoefler", "Torsten", ""]]}, {"id": "1910.09020", "submitter": "Mohammed Alser", "authors": "Mohammed Alser, Taha Shahroodi, Juan Gomez-Luna, Can Alkan, and Onur\n  Mutlu", "title": "SneakySnake: A Fast and Accurate Universal Genome Pre-Alignment Filter\n  for CPUs, GPUs, and FPGAs", "comments": "To appear in Bioinformatics", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.AR cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: We introduce SneakySnake, a highly parallel and highly accurate\npre-alignment filter that remarkably reduces the need for computationally\ncostly sequence alignment. The key idea of SneakySnake is to reduce the\napproximate string matching (ASM) problem to the single net routing (SNR)\nproblem in VLSI chip layout. In the SNR problem, we are interested in finding\nthe optimal path that connects two terminals with the least routing cost on a\nspecial grid layout that contains obstacles. The SneakySnake algorithm quickly\nsolves the SNR problem and uses the found optimal path to decide whether or not\nperforming sequence alignment is necessary. Reducing the ASM problem into SNR\nalso makes SneakySnake efficient to implement on CPUs, GPUs, and FPGAs.\nResults: SneakySnake significantly improves the accuracy of pre-alignment\nfiltering by up to four orders of magnitude compared to the state-of-the-art\npre-alignment filters, Shouji, GateKeeper, and SHD. For short sequences,\nSneakySnake accelerates Edlib (state-of-the-art implementation of Myers's\nbit-vector algorithm) and Parasail (state-of-the-art sequence aligner with a\nconfigurable scoring function), by up to 37.7x and 43.9x (>12x on average),\nrespectively, with its CPU implementation, and by up to 413x and 689x (>400x on\naverage), respectively, with FPGA and GPU acceleration. For long sequences, the\nCPU implementation of SneakySnake accelerates Parasail and KSW2 (sequence\naligner of minimap2) by up to 979x (276.9x on average) and 91.7x (31.7x on\naverage), respectively. As SneakySnake does not replace sequence alignment,\nusers can still obtain all capabilities (e.g., configurable scoring functions)\nof the aligner of their choice, unlike existing acceleration efforts that\nsacrifice some aligner capabilities. Availability:\nhttps://github.com/CMU-SAFARI/SneakySnake\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 16:48:05 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2020 10:47:00 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2020 20:38:49 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Alser", "Mohammed", ""], ["Shahroodi", "Taha", ""], ["Gomez-Luna", "Juan", ""], ["Alkan", "Can", ""], ["Mutlu", "Onur", ""]]}, {"id": "1910.09264", "submitter": "Thibault Rieutord", "authors": "Petr Kuznetsov, Thibault Rieutord and Sara Tucci-Piergiovanni", "title": "Reconfigurable Lattice Agreement and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconfiguration is one of the central mechanisms in distributed systems. Due\nto failures and connectivity disruptions, the very set of service replicas (or\nservers) and their roles in the computation may have to be reconfigured over\ntime. To provide the desired level of consistency and availability to\napplications running on top of these servers, the clients of the service should\nbe able to reach some form of agreement on the system configuration. We observe\nthat this agreement is naturally captured via a lattice partial order on the\nsystem states. We propose an asynchronous implementation of reconfigurable\nlattice agreement that implies elegant reconfigurable versions of a large class\nof lattice abstract data types, such as max-registers and conflict detectors,\nas well as popular distributed programming abstractions, such as atomic\nsnapshot and commit-adopt.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 11:16:56 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 09:28:17 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Kuznetsov", "Petr", ""], ["Rieutord", "Thibault", ""], ["Tucci-Piergiovanni", "Sara", ""]]}, {"id": "1910.09305", "submitter": "Richard Barnard", "authors": "Richard C Barnard and Kai Huang and Cory Hauck", "title": "A mathematical model of asynchronous data flow in parallel computers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simplified model of data flow on processors in a high\nperformance computing framework involving computations necessitating\ninter-processor communications. From this ordinary differential model, we take\nits asymptotic limit, resulting in a model which treats the computer as a\ncontinuum of processors and data flow as an Eulerian fluid governed by a\nconservation law. We derive a Hamilton-Jacobi equation associated with this\nconservation law for which the existence and uniqueness of solutions can be\nproven. We then present the results of numerical experiments for both discrete\nand continuum models; these show a qualitative agreement between the two and\nthe effect of variations in the computing environment's processing capabilities\non the progress of the modeled computation.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 17:29:27 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 20:10:19 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Barnard", "Richard C", ""], ["Huang", "Kai", ""], ["Hauck", "Cory", ""]]}, {"id": "1910.09598", "submitter": "Steven W. D. Chien", "authors": "Steven W. D. Chien, Ivy B. Peng, Stefano Markidis", "title": "Performance Evaluation of Advanced Features in CUDA Unified Memory", "comments": "Accepted for publication at Workshop on Memory Centric High\n  Performance Computing (MCHPC'19) in SC19", "journal-ref": null, "doi": "10.1109/MCHPC49590.2019.00014", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CUDA Unified Memory improves the GPU programmability and also enables GPU\nmemory oversubscription. Recently, two advanced memory features, memory advises\nand asynchronous prefetch, have been introduced. In this work, we evaluate the\nnew features on two platforms that feature different CPUs, GPUs, and\ninterconnects. We derive a benchmark suite for the experiments and stress the\nmemory system to evaluate both in-memory and oversubscription performance.\n  The results show that memory advises on the Intel-Volta/Pascal-PCIe platform\nbring negligible improvement for in-memory executions. However, when GPU memory\nis oversubscribed by about 50%, using memory advises results in up to 25%\nperformance improvement compared to the basic CUDA Unified Memory. In contrast,\nthe Power9-Volta-NVLink platform can substantially benefit from memory advises,\nachieving up to 34% performance gain for in-memory executions. However, when\nGPU memory is oversubscribed on this platform, using memory advises increases\nGPU page faults and results in considerable performance loss. The CUDA prefetch\nalso shows different performance impact on the two platforms. It improves\nperformance by up to 50% on the Intel-Volta/Pascal-PCI-E platform but brings\nlittle benefit to the Power9-Volta-NVLink platform.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 18:43:00 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Chien", "Steven W. D.", ""], ["Peng", "Ivy B.", ""], ["Markidis", "Stefano", ""]]}, {"id": "1910.09650", "submitter": "Amanda Bienz", "authors": "Amanda Bienz, Luke N. Olson, and William D. Gropp", "title": "Node-Aware Improvements to Allreduce", "comments": "10 pages, 11 figures, ExaMPI Workshop at SC19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The \\texttt{MPI\\_Allreduce} collective operation is a core kernel of many\nparallel codebases, particularly for reductions over a single value per\nprocess. The commonly used allreduce recursive-doubling algorithm obtains the\nlower bound message count, yielding optimality for small reduction sizes based\non node-agnostic performance models. However, this algorithm yields duplicate\nmessages between sets of nodes. Node-aware optimizations in MPICH remove\nduplicate messages through use of a single master process per node, yielding a\nlarge number of inactive processes at each inter-node step. In this paper, we\npresent an algorithm that uses the multiple processes available per node to\nreduce the maximum number of inter-node messages communicated by a single\nprocess, improving the performance of allreduce operations, particularly for\nsmall message sizes.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 20:59:31 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Bienz", "Amanda", ""], ["Olson", "Luke N.", ""], ["Gropp", "William D.", ""]]}, {"id": "1910.09684", "submitter": "Yehuda Afek Prof", "authors": "Yehuda Afek, Eli Gafni and Nati Linial", "title": "A King in every two consecutive tournaments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We think of a tournament $T=([n], E)$ as a communication network where in\neach round of communication processor $P_i$ sends its information to $P_j$, for\nevery directed edge $ij \\in E(T)$. By Landau's theorem (1953) there is a King\nin $T$, i.e., a processor whose initial input reaches every other processor in\ntwo rounds or less. Namely, a processor $P_{\\nu}$ such that after two rounds of\ncommunication along $T$'s edges, the initial information of $P_{\\nu}$ reaches\nall other processors. Here we consider a more general scenario where an\nadversary selects an arbitrary series of tournaments $T_1, T_2,\\ldots$, so that\nin each round $s=1, 2, \\ldots$, communication is governed by the corresponding\ntournament $T_s$. We prove that for every series of tournaments that the\nadversary selects, it is still true that after two rounds of communication, the\ninitial input of at least one processor reaches everyone. Concretely, we show\nthat for every two tournaments $T_1, T_2$ there is a vertex in $[n]$ that can\nreach all vertices via (i) A step in $T_1$, or (ii) A step in $T_2$ or (iii) A\nstep in $T_1$ followed by a step in $T_2$. }\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 22:41:38 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Afek", "Yehuda", ""], ["Gafni", "Eli", ""], ["Linial", "Nati", ""]]}, {"id": "1910.09727", "submitter": "Hasan Al Maruf", "authors": "Youngmoon Lee, Hasan Al Maruf, Mosharaf Chowdhury, Asaf Cidon, Kang G.\n  Shin", "title": "Mitigating the Performance-Efficiency Tradeoff in Resilient Memory\n  Disaggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the design and implementation of a low-latency, low-overhead, and\nhighly available resilient disaggregated cluster memory. Our proposed framework\ncan access erasure-coded remote memory within a single-digit {\\mu}s read/write\nlatency, significantly improving the performance-efficiency tradeoff over the\nstate-of-the-art - it performs similar to in-memory replication with 1.6x lower\nmemory overhead. We also propose a novel coding group placement algorithm for\nerasure-coded data, that provides load balancing while reducing the probability\nof data loss under correlated failures by an order of magnitude.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 02:12:55 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 16:35:44 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Lee", "Youngmoon", ""], ["Maruf", "Hasan Al", ""], ["Chowdhury", "Mosharaf", ""], ["Cidon", "Asaf", ""], ["Shin", "Kang G.", ""]]}, {"id": "1910.09775", "submitter": "Ivan Homoliak Ph.D.", "authors": "Ivan Homoliak, Sarad Venugopalan, Qingze Hum, Daniel Reijsbergen,\n  Richard Schumi, Pawel Szalachowski", "title": "The Security Reference Architecture for Blockchains: Towards a\n  Standardized Model for Studying Vulnerabilities, Threats, and Defenses", "comments": null, "journal-ref": null, "doi": "10.1109/COMST.2020.3033665", "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchains are distributed systems, in which security is a critical factor\nfor their success. However, despite their increasing popularity and adoption,\nthere is a lack of standardized models that study blockchain-related security\nthreats. To fill this gap, the main focus of our work is to systematize and\nextend the knowledge about the security and privacy aspects of blockchains and\ncontribute to the standardization of this domain.\n  We propose the security reference architecture (SRA) for blockchains, which\nadopts a stacked model (similar to the ISO/OSI) describing the nature and\nhierarchy of various security and privacy aspects. The SRA contains four\nlayers: (1) the network layer, (2) the consensus layer, (3) the replicated\nstate machine layer, and (4) the application layer. At each of these layers, we\nidentify known security threats, their origin, and countermeasures, while we\nalso analyze several cross-layer dependencies. Next, to enable better reasoning\nabout security aspects of blockchains by the practitioners, we propose a\nblockchain-specific version of the threat-risk assessment standard ISO/IEC\n15408 by embedding the stacked model into this standard. Finally, we provide\ndesigners of blockchain platforms and applications with a design methodology\nfollowing the model of SRA and its hierarchy.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 05:44:19 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 14:34:51 GMT"}, {"version": "v3", "created": "Wed, 28 Oct 2020 13:25:39 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Homoliak", "Ivan", ""], ["Venugopalan", "Sarad", ""], ["Hum", "Qingze", ""], ["Reijsbergen", "Daniel", ""], ["Schumi", "Richard", ""], ["Szalachowski", "Pawel", ""]]}, {"id": "1910.09786", "submitter": "Yackolley Amoussou-Guenou", "authors": "Yackolley Amoussou-Guenou (DILS, NPA), Antonella del Pozzo (DILS),\n  Maria Potop-Butucaru (NPA, LINCS), Sara Tucci-Piergiovanni (DILS)", "title": "On Fairness in Committee-based Blockchains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Committee-based blockchains are among the most popular alternatives of\nproof-of-work based blockchains, such as Bitcoin. They provide strong\nconsistency (no fork) under classical assumptions, and avoid using\nenergy-consuming mechanisms to add new blocks in the blockchain. For each\nblock, these blockchains use a committee that executes Byzantine-fault tolerant\ndistributed consensus to decide the next block they will add in the blockchain.\nUnlike Bitcoin, where there is only one creator per block with high\nprobability, in committee-based blockchain any block is cooperatively created.\nIn order to incentivize committee members to participate to the creation of new\nblocks rewarding schemes have to be designed. In this paper, we study the\nfairness of rewarding in committee-based blockchains and we provide necessary\nand sufficient conditions on the system communication under which it is\npossible to have a fair reward mechanism.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 06:36:50 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Amoussou-Guenou", "Yackolley", "", "DILS, NPA"], ["del Pozzo", "Antonella", "", "DILS"], ["Potop-Butucaru", "Maria", "", "NPA, LINCS"], ["Tucci-Piergiovanni", "Sara", "", "DILS"]]}, {"id": "1910.09882", "submitter": "Benyamin Ghojogh", "authors": "Benyamin Ghojogh, Saber Salehkaleybar", "title": "Distributed Voting in Beep Model", "comments": "Published in Signal Processing journal, Elsevier, 2020", "journal-ref": "Signal Processing, Elsevier, vol. 177, 2020", "doi": "10.1016/j.sigpro.2020.107732", "report-no": null, "categories": "cs.DC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of distributed multi-choice voting in a setting that\neach node can communicate with its neighbors merely by sending beep signals.\nGiven its simplicity, the beep communication model is of practical importance\nin different applications such as system biology and wireless sensor networks.\nYet, the distributed majority voting has not been resolved in this setting. In\nthis paper, we propose two algorithms, named Distributed Voting with Beeps, to\nresolve this problem. In the first proposed algorithm, the adjacent nodes\nhaving the same value form a set called spot. Afterwards, the spots with\nmajority value try to corrode the spots with non-majority values. The second\nproposed algorithm is based on pairwise interactions between nodes. The\nproposed algorithms have a termination detection procedure to check whether\nvoting is achieved. We establish theoretical guarantees for the convergence of\nthese algorithms. In particular, we show that the success probability of the\nfirst algorithm tends to one as the percentage of the initial votes in majority\nincreases. For the second algorithm, we show that it returns the correct output\nwith high probability. Our experiments show that the algorithms are fairly\ninvariant to the network topology, initial distribution of values, and network\nsize.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 10:38:43 GMT"}, {"version": "v2", "created": "Sat, 8 Aug 2020 03:31:39 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Ghojogh", "Benyamin", ""], ["Salehkaleybar", "Saber", ""]]}, {"id": "1910.09941", "submitter": "Michael Breza", "authors": "Alberto Spina, Michael Breza, Naranker Dulay, Julie McCann", "title": "XPC: Fast and Reliable Synchronous Transmission Protocols for 2-Phase\n  Commit and 3-Phase Commit", "comments": "13 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the major challenges for the engineering of wireless sensing systems\nis to improve the software abstractions and frameworks that are available to\nprogrammers while ensuring system reliability and efficiency. The distributed\nsystems community have developed a rich set of such abstractions for building\ndependable distributed systems connected using wired networks, however after 20\nyears research many of these elude wireless sensor systems. In this paper we\npresent X Process Commit (XPC) an atomic commit protocol framework that\nutilizes Synchronous Transmission (ST). We also introduce Hybrid, a technique\nthat allows us to exploit the advantages of the Glossy and Chaos Synchronous\nTransmission primitives to get lower latency and higher reliability than\neither. Using XPC and Hybrid we demonstrate how to build protocols for the\nclassical 2-phase and 3-phase commit abstractions and evaluate these\ndemonstrating significantly improved performance and reliability than the use\nof Glossy or Chaos individually as dissemination primitives. We address how we\novercame the timing challenges of bringing Glossy and Chaos together to form\nHybrid and through extensive experimentation demonstrate that it is robust to\nin-network radio interference caused by multiple sources. We are first to\npresent testbed results that show that Hybrid can provide almost 100%\nreliability in a network of nodes suffering from various levels of radio\ninterference.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 12:59:11 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Spina", "Alberto", ""], ["Breza", "Michael", ""], ["Dulay", "Naranker", ""], ["McCann", "Julie", ""]]}, {"id": "1910.10190", "submitter": "Ahmad Banijamali", "authors": "Ahmad Banijamali, Pooyan Jamshidi, Pasi Kuvaja, Markku Oivo", "title": "Kuksa: A Cloud-Native Architecture for Enabling Continuous Delivery in\n  the Automotive Domain", "comments": "Accepted for publication at the 20th International Conference on\n  Product-Focused Software Process Improvement (PROFES19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Connecting vehicles to cloud platforms has enabled innovative business\nscenarios while raising new quality concerns, such as reliability and\nscalability, which must be addressed by research. Cloud-native architectures\nbased on microservices are a recent approach to enable continuous delivery and\nto improve service reliability and scalability. We propose an approach for\nrestructuring cloud platform architectures in the automotive domain into a\nmicroservices architecture. To this end, we adopted and implemented\nmicroservices patterns from literature to design the cloud-native automotive\narchitecture and conducted a laboratory experiment to evaluate the reliability\nand scalability of microservices in the context of a real-world project in the\nautomotive domain called Eclipse Kuksa. Findings indicated that the proposed\narchitecture could handle the continuous software delivery over-the-air by\nsending automatic control messages to a vehicular setting. Different patterns\nenabled us to make changes or interrupt services without extending the impact\nto others. The results of this study provide evidences that microservices are a\npotential design solution when dealing with service failures and high payload\non cloud-based services in the automotive domain.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 18:33:28 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Banijamali", "Ahmad", ""], ["Jamshidi", "Pooyan", ""], ["Kuvaja", "Pasi", ""], ["Oivo", "Markku", ""]]}, {"id": "1910.10283", "submitter": "Hema Venkata Krishna Giri Narra", "authors": "Zhifeng Lin, Krishna Giri Narra, Mingchao Yu, Salman Avestimehr,\n  Murali Annavaram", "title": "Train Where the Data is: A Case for Bandwidth Efficient Coded Training", "comments": "10 pages, Under submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training a machine learning model is both compute and data-intensive. Most of\nthe model training is performed on high performance compute nodes and the\ntraining data is stored near these nodes for faster training. But there is a\ngrowing interest in enabling training near the data. For instance, mobile\ndevices are rich sources of training data. It may not be feasible to\nconsolidate the data from mobile devices into a cloud service, due to bandwidth\nand data privacy reasons. Training at mobile devices is however fraught with\nchallenges. First mobile devices may join or leave the distributed setting,\neither voluntarily or due to environmental uncertainties, such as lack of\npower. Tolerating uncertainties is critical to the success of distributed\nmobile training. One proactive approach to tolerate computational uncertainty\nis to store data in a coded format and perform training on coded data. Encoding\ndata is a challenging task since erasure codes require multiple devices to\nexchange their data to create a coded data partition, which places a\nsignificant bandwidth constraint. Furthermore, coded computing traditionally\nrelied on a central node to encode and distribute data to all the worker nodes,\nwhich is not practical in a distributed mobile setting.\n  In this paper, we tackle the uncertainty in distributed mobile training using\na bandwidth-efficient encoding strategy. We use a Random Linear Network coding\n(RLNC) which reduces the need to exchange data partitions across all\nparticipating mobile devices, while at the same time preserving the property of\ncoded computing to tolerate uncertainties. We implement gradient descent for\nlogistic regression and SVM to evaluate the effectiveness of our mobile\ntraining framework. We demonstrate a 50% reduction in total required\ncommunication bandwidth compared to MDS coded computation, one of the popular\nerasure codes.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 23:48:05 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Lin", "Zhifeng", ""], ["Narra", "Krishna Giri", ""], ["Yu", "Mingchao", ""], ["Avestimehr", "Salman", ""], ["Annavaram", "Murali", ""]]}, {"id": "1910.10434", "submitter": "Zeta Avarikioti", "authors": "Georgia Avarikioti, Eleftherios Kokoris-Kogias, Roger Wattenhofer", "title": "Divide and Scale: Formalization of Distributed Ledger Sharding Protocols", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sharding distributed ledgers is the most promising on-chain solution for\nscaling blockchain technology. In this work, we define and analyze the\nproperties a sharded distributed ledger should fulfill. More specifically, we\nshow that a sharded blockchain cannot be scalable under a fully adaptive\nadversary, but it can scale up to $O(n/\\log n)$ under an epoch-adaptive\nadversary. This is possible only if the distributed ledger creates succinct\nproofs of the valid state updates at the end of each epoch. Our model builds\nupon and extends the Bitcoin backbone protocol by defining consistency and\nscalability. Consistency encompasses the need for atomic execution of\ncross-shard transactions to preserve safety, whereas scalability encapsulates\nthe speedup a sharded system can gain in comparison to a non-sharded system. We\nintroduce a protocol abstraction and highlight the sufficient components for\nsecure and efficient sharding in our model. In order to show the power of our\nframework, we analyze the most prominent shared blockchains (Elastico,\nMonoxide, OmniLedger, RapidChain) and pinpoint where they fail to meet the\ndesired properties.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 09:41:41 GMT"}, {"version": "v2", "created": "Mon, 4 Nov 2019 13:58:11 GMT"}, {"version": "v3", "created": "Mon, 20 Jan 2020 12:15:42 GMT"}, {"version": "v4", "created": "Fri, 19 Jun 2020 14:29:00 GMT"}, {"version": "v5", "created": "Mon, 8 Mar 2021 09:33:40 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Avarikioti", "Georgia", ""], ["Kokoris-Kogias", "Eleftherios", ""], ["Wattenhofer", "Roger", ""]]}, {"id": "1910.10453", "submitter": "Jihong Park", "authors": "Anis Elgabli, Jihong Park, Amrit S. Bedi, Chaouki Ben Issaid, Mehdi\n  Bennis, Vaneet Aggarwal", "title": "Q-GADMM: Quantized Group ADMM for Communication Efficient Decentralized\n  Machine Learning", "comments": "19 pages, 8 figures; to appear in IEEE Transactions on Communications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.IT cs.NI math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose a communication-efficient decentralized machine\nlearning (ML) algorithm, coined quantized group ADMM (Q-GADMM). To reduce the\nnumber of communication links, every worker in Q-GADMM communicates only with\ntwo neighbors, while updating its model via the group alternating direction\nmethod of multipliers (GADMM). Moreover, each worker transmits the quantized\ndifference between its current model and its previously quantized model,\nthereby decreasing the communication payload size. However, due to the lack of\ncentralized entity in decentralized ML, the spatial sparsity and payload\ncompression may incur error propagation, hindering model training convergence.\nTo overcome this, we develop a novel stochastic quantization method to\nadaptively adjust model quantization levels and their probabilities, while\nproving the convergence of Q-GADMM for convex objective functions. Furthermore,\nto demonstrate the feasibility of Q-GADMM for non-convex and stochastic\nproblems, we propose quantized stochastic GADMM (Q-SGADMM) that incorporates\ndeep neural network architectures and stochastic sampling. Simulation results\ncorroborate that Q-GADMM significantly outperforms GADMM in terms of\ncommunication efficiency while achieving the same accuracy and convergence\nspeed for a linear regression task. Similarly, for an image classification task\nusing DNN, Q-SGADMM achieves significantly less total communication cost with\nidentical accuracy and convergence speed compared to its counterpart without\nquantization, i.e., stochastic GADMM (SGADMM).\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 10:47:06 GMT"}, {"version": "v2", "created": "Sun, 23 Feb 2020 13:40:21 GMT"}, {"version": "v3", "created": "Fri, 13 Mar 2020 17:31:00 GMT"}, {"version": "v4", "created": "Mon, 27 Jul 2020 19:33:13 GMT"}, {"version": "v5", "created": "Mon, 3 Aug 2020 09:37:46 GMT"}, {"version": "v6", "created": "Sat, 3 Oct 2020 18:28:18 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Elgabli", "Anis", ""], ["Park", "Jihong", ""], ["Bedi", "Amrit S.", ""], ["Issaid", "Chaouki Ben", ""], ["Bennis", "Mehdi", ""], ["Aggarwal", "Vaneet", ""]]}, {"id": "1910.10638", "submitter": "Yu Chen", "authors": "Erik Blasch, Ronghua Xu, Yu Chen, Genshe Chen, Dan Shen", "title": "Blockchain Methods for Trusted Avionics Systems", "comments": "Accepted and presented at 2019 IEEE NAECON Conference. arXiv admin\n  note: text overlap with arXiv:1902.10567", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain is a popular method to ensure security for trusted systems. The\nbenefits include an auditable method to provide decentralized security without\na trusted third party, but the drawback is the large computational resources\nneeded to process and store the ever-expanding chain of security blocks. The\npromise of blockchain for edge devices (e.g., internet of things) poses a\nvariety of challenges and strategies before adoption. In this paper, we explore\nblockchain methods and examples, with experimental data to determine the merits\nof the capabilities. As for an aerospace example, we address a notional example\nfor Automatic dependent surveillance-broadcast (ADS-B) from Flight24 data\n(https://www.flightradar24.com/) to determine whether blockchain is feasible\nfor avionics systems. The methods are incorporated into the Lightweight\nInternet of Things (IoT) based Smart Public Safety (LISPS) framework. By\ndecoupling a complex system into independent sub-tasks, the LISPS system\npossesses high flexibility in the design process and online maintenance. The\nBlockchain-enabled decentralized avionics services provide a secured data\nsharing and access control mechanism. The experimental results demonstrate the\nfeasibility of the approach.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 00:53:38 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Blasch", "Erik", ""], ["Xu", "Ronghua", ""], ["Chen", "Yu", ""], ["Chen", "Genshe", ""], ["Shen", "Dan", ""]]}, {"id": "1910.10641", "submitter": "Johannes Holke", "authors": "Johannes Holke and David Knapp and Carsten Burstedde", "title": "An Optimized, Parallel Computation of the Ghost Layer for Adaptive\n  Hybrid Forest Meshes", "comments": "33 pages, 12 figures, 13 tables. arXiv admin note: substantial text\n  overlap with arXiv:1803.04970", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss parallel algorithms to gather topological information about\noff-process mesh neighbor elements. This information is commonly called the\nghost layer, whose creation is a fundamental, necessary task in executing most\nparallel, element-based computer simulations. Approaches differ in that the\nghost layer may either be inherently part of the mesh data structure that is\nmaintained and modified, or kept separate and constructed/deleted as needed.\n  In this work, we present an updated design following the latter approach,\nwhich we favor for its modularity of algorithms and data structures. We target\narbitrary adaptive, non-conforming forest-of-(oc)trees meshes of mixed element\nshapes, such as cubes, prisms, and tetrahedra, and restrict ourselves to\nface-ghosts. Our algorithm has low complexity and redundancy since we reduce it\nto generic codimension-1 subalgorithms that can be flexibly combined. We cover\nseveral existing solutions as special cases and optimize further using\nrecursive, amortized tree searches and traversals.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 11:54:16 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Holke", "Johannes", ""], ["Knapp", "David", ""], ["Burstedde", "Carsten", ""]]}, {"id": "1910.10666", "submitter": "Jinming Xu", "authors": "Jinming Xu, Ye Tian, Ying Sun, Gesualdo Scutari", "title": "Accelerated Primal-Dual Algorithms for Distributed Smooth Convex\n  Optimization over Networks", "comments": "final version for AISTATS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel family of primal-dual-based distributed\nalgorithms for smooth, convex, multi-agent optimization over networks that uses\nonly gradient information and gossip communications. The algorithms can also\nemploy acceleration on the computation and communications. We provide a unified\nanalysis of their convergence rate, measured in terms of the Bregman distance\nassociated to the saddle point reformation of the distributed optimization\nproblem. When acceleration is employed, the rate is shown to be optimal, in the\nsense that it matches (under the proposed metric) existing complexity lower\nbounds of distributed algorithms applicable to such a class of problem and\nusing only gradient information and gossip communications. Preliminary\nnumerical results on distributed least-square regression problems show that the\nproposed algorithm compares favorably on existing distributed schemes.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 17:05:00 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 01:15:54 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Xu", "Jinming", ""], ["Tian", "Ye", ""], ["Sun", "Ying", ""], ["Scutari", "Gesualdo", ""]]}, {"id": "1910.10776", "submitter": "Onur Mutlu", "authors": "Konstantinos Kanellopoulos, Nandita Vijaykumar, Christina Giannoula,\n  Roknoddin Azizi, Skanda Koppula, Nika Mansouri Ghiasi, Taha Shahroodi, Juan\n  Gomez Luna and Onur Mutlu", "title": "SMASH: Co-designing Software Compression and Hardware-Accelerated\n  Indexing for Efficient Sparse Matrix Operations", "comments": null, "journal-ref": null, "doi": "10.1145/3352460.3358286", "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Important workloads, such as machine learning and graph analytics\napplications, heavily involve sparse linear algebra operations. These\noperations use sparse matrix compression as an effective means to avoid storing\nzeros and performing unnecessary computation on zero elements. However,\ncompression techniques like Compressed Sparse Row (CSR) that are widely used\ntoday introduce significant instruction overhead and expensive pointer-chasing\noperations to discover the positions of the non-zero elements. In this paper,\nwe identify the discovery of the positions (i.e., indexing) of non-zero\nelements as a key bottleneck in sparse matrix-based workloads, which greatly\nreduces the benefits of compression. We propose SMASH, a hardware-software\ncooperative mechanism that enables highly-efficient indexing and storage of\nsparse matrices. The key idea of SMASH is to explicitly enable the hardware to\nrecognize and exploit sparsity in data. To this end, we devise a novel software\nencoding based on a hierarchy of bitmaps. This encoding can be used to\nefficiently compress any sparse matrix, regardless of the extent and structure\nof sparsity. At the same time, the bitmap encoding can be directly interpreted\nby the hardware. We design a lightweight hardware unit, the Bitmap Management\nUnit (BMU), that buffers and scans the bitmap hierarchy to perform\nhighly-efficient indexing of sparse matrices. SMASH exposes an expressive and\nrich ISA to communicate with the BMU, which enables its use in accelerating any\nsparse matrix computation. We demonstrate the benefits of SMASH on four use\ncases that include sparse matrix kernels and graph analytics applications.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 19:37:12 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Kanellopoulos", "Konstantinos", ""], ["Vijaykumar", "Nandita", ""], ["Giannoula", "Christina", ""], ["Azizi", "Roknoddin", ""], ["Koppula", "Skanda", ""], ["Ghiasi", "Nika Mansouri", ""], ["Shahroodi", "Taha", ""], ["Luna", "Juan Gomez", ""], ["Mutlu", "Onur", ""]]}, {"id": "1910.10929", "submitter": "Zijie Yan", "authors": "Zijie Yan", "title": "Gradient Sparification for Asynchronous Distributed Training", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern large scale machine learning applications require stochastic\noptimization algorithms to be implemented on distributed computational\narchitectures. A key bottleneck is the communication overhead for exchanging\ninformation, such as stochastic gradients, among different nodes. Recently,\ngradient sparsification techniques have been proposed to reduce communications\ncost and thus alleviate the network overhead. However, most of gradient\nsparsification techniques consider only synchronous parallelism and cannot be\napplied in asynchronous scenarios, such as asynchronous distributed training\nfor federated learning at mobile devices.\n  In this paper, we present a dual-way gradient sparsification approach (DGS)\nthat is suitable for asynchronous distributed training. We let workers download\nmodel difference, instead of the global model, from the server, and the model\ndifference information is also sparsified so that the information exchanged\noverhead is reduced by sparsifying the dual-way communication between the\nserver and workers. To preserve accuracy under dual-way sparsification, we\ndesign a sparsification aware momentum (SAMomentum) to turn sparsification into\nadaptive batch size between each parameter. We conduct experiments at a cluster\nof 32 workers, and the results show that, with the same compression ratio but\nmuch lower communication cost, our approach can achieve better scalability and\ngeneralization ability.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 06:16:28 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Yan", "Zijie", ""]]}, {"id": "1910.11039", "submitter": "Alexander van der Grinten", "authors": "Alexander van der Grinten, Henning Meyerhenke", "title": "Scaling Betweenness Approximation to Billions of Edges by MPI-based\n  Adaptive Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Betweenness centrality is one of the most popular vertex centrality measures\nin network analysis. Hence, many (sequential and parallel) algorithms to\ncompute or approximate betweenness have been devised. Recent algorithmic\nadvances have made it possible to approximate betweenness very efficiently on\nshared-memory architectures. Yet, the best shared-memory algorithms can still\ntake hours of running time for large graphs, especially for graphs with a high\ndiameter or when a small relative error is required.\n  In this work, we present an MPI-based generalization of the state-of-the-art\nshared-memory algorithm for betweenness approximation. This algorithm is based\non adaptive sampling; our parallelization strategy can be applied in the same\nmanner to adaptive sampling algorithms for other problems. In experiments on a\n16-node cluster, our MPI-based implementation is by a factor of 16.1x faster\nthan the state-of-the-art shared-memory implementation when considering our\nparallelization focus -- the adaptive sampling phase -- only. For the complete\nalgorithm, we obtain an average (geom. mean) speedup factor of 7.4x over the\nstate of the art. For some previously very challenging inputs, this speedup is\nmuch higher. As a result, our algorithm is the first to approximate betweenness\ncentrality on graphs with several billion edges in less than ten minutes with\nhigh accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 11:48:28 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["van der Grinten", "Alexander", ""], ["Meyerhenke", "Henning", ""]]}, {"id": "1910.11069", "submitter": "Lorenz H\\\"ubschle-Schneider", "authors": "Lorenz H\\\"ubschle-Schneider and Peter Sanders", "title": "Communication-Efficient (Weighted) Reservoir Sampling from Fully\n  Distributed Data Streams", "comments": "A previous version of this paper was titled \"Communication-Efficient\n  (Weighted) Reservoir Sampling\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider communication-efficient weighted and unweighted (uniform) random\nsampling from distributed data streams presented as a sequence of mini-batches\nof items. This is a natural model for distributed streaming computation, and\nour goal is to showcase its usefulness. We present and analyze fully\ndistributed, communication-efficient algorithms for both versions of the\nproblem. An experimental evaluation of weighted reservoir sampling on up to 256\nnodes (5120 processors) shows good speedups, while theoretical analysis\npromises further scaling to much larger machines.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 13:21:00 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 13:54:24 GMT"}, {"version": "v3", "created": "Tue, 25 Feb 2020 13:53:52 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["H\u00fcbschle-Schneider", "Lorenz", ""], ["Sanders", "Peter", ""]]}, {"id": "1910.11074", "submitter": "Chengyi Zhang", "authors": "Chengyi Zhang and Jianguo Xie", "title": "Adaptive-time Synchronization Algorithm for Superlattice Key\n  Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a synchronization algorithm for superlattice key\ndistribution, which is a symmetric encryption solution, by optimizing the\nEuclidian distance between the two chaotic waveforms generated in the receiver\nand the sender, respectively. This algorithm based on time synchronization is\ncapable of reconstructing the generated waveforms in the receiver and the\nsender within the error of 5% to 6% (given the fact that the waveforms were not\nperfectly congruent when they were originally created).\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 13:09:21 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Zhang", "Chengyi", ""], ["Xie", "Jianguo", ""]]}, {"id": "1910.11110", "submitter": "Ludovic Henrio", "authors": "Ludovic Henrio (CASH), Christoph Kessler (LIU), Lu Li (LIU)", "title": "Leveraging access mode declarations in a model for memory consistency in\n  heterogeneous systems", "comments": null, "journal-ref": "Journal of Logical and Algebraic Methods in Programming, Elsevier,\n  2020, 110, pp.100498", "doi": "10.1016/j.jlamp.2019.100498", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On a system that exposes disjoint memory spaces to the software, a program\nhas to address memory consistency issues and perform data transfers so that it\nalways accesses valid data. Several approaches exist to ensure the consistency\nof the memory accessed. Here we are interested in the verification of a\ndeclarative approach where each component of a computation is annotated with an\naccess mode declaring which part of the memory is read or written by the\ncomponent. The programming framework uses the component annotations to\nguarantee the validity of the memory accesses. This is the mechanism used in\nVectorPU, a C++ library for programming CPU-GPU heterogeneous systems. This\narticle proves the correctness of the software cache-coherence mechanism used\nin VectorPU. Beyond the scope of VectorPU, this article provides a simple and\neffective formalisation of memory consistency mechanisms based on the explicit\ndeclaration of the effect of each component on each memory space. The formalism\nwe propose also takes into account arrays for which a single validity status is\nstored for the whole array; additional mechanisms for dealing with overlapping\narrays are also studied.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 13:49:15 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Henrio", "Ludovic", "", "CASH"], ["Kessler", "Christoph", "", "LIU"], ["Li", "Lu", "", "LIU"]]}, {"id": "1910.11125", "submitter": "Amit Kumar Mondal", "authors": "Amit Kumar Mondal, Banani Roy, Chanchal K. Roy, Kevin A. Schneider", "title": "Micro-level Modularity of Computaion-intensive Programs in Big Data\n  Platforms: A Case Study with Image Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid advancement of Big Data platforms such as Hadoop, Spark, and\nDataflow, many tools are being developed that are intended to provide end users\nwith an interactive environment for large-scale data analysis (e.g., IQmulus).\nHowever, there are challenges using these platforms. For example, developers\nfind it difficult to use these platforms when developing interactive and\nreusable data analytic tools. One approach to better support interactivity and\nreusability is the use of microlevel modularisation for computation-intensive\ntasks, which splits data operations into independent, composable modules.\nHowever, modularizing data and computation-intensive tasks into independent\ncomponents differs from traditional programming, e.g., when accessing large\nscale data, controlling data-flow among components, and structuring computation\nlogic. In this paper, we present a case study on modularizing real world\ncomputationintensive tasks that investigates the impact of modularization on\nprocessing large scale image data. To that end, we synthesize image\ndata-processing patterns and propose a unified modular model for the effective\nimplementation of computation-intensive tasks on data-parallel frameworks\nconsidering reproducibility, reusability, and customization. We present various\ninsights of using the modularity model based on our experimental results from\nrunning image processing tasks on Spark and Hadoop clusters.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 02:14:13 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Mondal", "Amit Kumar", ""], ["Roy", "Banani", ""], ["Roy", "Chanchal K.", ""], ["Schneider", "Kevin A.", ""]]}, {"id": "1910.11141", "submitter": "Alexey Radul", "authors": "Alexey Radul, Brian Patton, Dougal Maclaurin, Matthew D. Hoffman and\n  Rif A. Saurous", "title": "Automatically Batching Control-Intensive Programs for Modern\n  Accelerators", "comments": "10 pages; Machine Learning and Systems 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a general approach to batching arbitrary computations for\naccelerators such as GPUs. We show orders-of-magnitude speedups using our\nmethod on the No U-Turn Sampler (NUTS), a workhorse algorithm in Bayesian\nstatistics. The central challenge of batching NUTS and other Markov chain Monte\nCarlo algorithms is data-dependent control flow and recursion. We overcome this\nby mechanically transforming a single-example implementation into a form that\nexplicitly tracks the current program point for each batch member, and only\nsteps forward those in the same place. We present two different batching\nalgorithms: a simpler, previously published one that inherits recursion from\nthe host Python, and a more complex, novel one that implemenents recursion\ndirectly and can batch across it. We implement these batching methods as a\ngeneral program transformation on Python source. Both the batching system and\nthe NUTS implementation presented here are available as part of the popular\nTensorFlow Probability software package.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 14:06:18 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2020 15:56:56 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Radul", "Alexey", ""], ["Patton", "Brian", ""], ["Maclaurin", "Dougal", ""], ["Hoffman", "Matthew D.", ""], ["Saurous", "Rif A.", ""]]}, {"id": "1910.11143", "submitter": "Bernd Burgstaller", "authors": "Kirk Baird, Seongho Jeong, Yeonsoo Kim, Bernd Burgstaller, Bernhard\n  Scholz", "title": "The Economics of Smart Contracts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ethereum is a distributed blockchain that can execute smart contracts, which\ninter-communicate and perform transactions automatically. The execution of\nsmart contracts is paid in the form of gas, which is a monetary unit used in\nthe Ethereum blockchain. The Ethereum Virtual Machine (EVM) provides the\nmetering capability for smart contract execution. Instruction costs vary\ndepending on the instruction type and the approximate computational resources\nrequired to execute the instruction on the network. The cost of gas is adjusted\nusing transaction fees to ensure adequate payment of the network. In this work,\nwe highlight the \"real\" economics of smart contracts. We show that the actual\ncosts of executing smart contracts are disproportionate to the computational\ncosts and that this gap is continuously widening. We show that the gas\ncost-model of the underlying EVM instruction-set is wrongly modeled.\nSpecifically, the computational cost for the SLOAD instruction increases with\nthe length of the blockchain. Our proposed performance model estimates gas\nusage and execution time of a smart contract at a given block-height. The new\ngas-cost model incorporates the block-height to eliminate irregularities in the\nEthereum gas calculations. Our findings are based on extensive experiments over\nthe entire history of the EVM blockchain.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 03:36:37 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Baird", "Kirk", ""], ["Jeong", "Seongho", ""], ["Kim", "Yeonsoo", ""], ["Burgstaller", "Bernd", ""], ["Scholz", "Bernhard", ""]]}, {"id": "1910.11160", "submitter": "Rulin Shao", "authors": "Rulin Shao, Hongyu He, Hui Liu, Dianbo Liu", "title": "Stochastic Channel-Based Federated Learning for Medical Data Privacy\n  Preserving", "comments": "6 pages including references, 2 figures, Machine Learning for Health\n  (ML4H) at NeurIPS 2019 - Extended Abstract. arXiv admin note: substantial\n  text overlap with arXiv:1910.02115", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Artificial neural network has achieved unprecedented success in the medical\ndomain. This success depends on the availability of massive and representative\ndatasets. However, data collection is often prevented by privacy concerns and\npeople want to take control over their sensitive information during both\ntraining and using processes. To address this problem, we propose a\nprivacy-preserving method for the distributed system, Stochastic Channel-Based\nFederated Learning (SCBF), which enables the participants to train a\nhigh-performance model cooperatively without sharing their inputs.\nSpecifically, we design, implement and evaluate a channel-based update\nalgorithm for the central server in a distributed system, which selects the\nchannels with regard to the most active features in a training loop and uploads\nthem as learned information from local datasets. A pruning process is applied\nto the algorithm based on the validation set, which serves as a model\naccelerator. In the experiment, our model presents better performances and\nhigher saturating speed than the Federated Averaging method which reveals all\nthe parameters of local models to the server when updating. We also demonstrate\nthat the saturating rate of performance could be promoted by introducing a\npruning process. And further improvement could be achieved by tuning the\npruning rate. Our experiment shows that 57% of the time is saved by the pruning\nprocess with only a reduction of 0.0047 in AUCROC performance and a reduction\nof 0.0068 in AUCPR.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 05:08:55 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 08:26:57 GMT"}, {"version": "v3", "created": "Fri, 15 Nov 2019 15:00:57 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Shao", "Rulin", ""], ["He", "Hongyu", ""], ["Liu", "Hui", ""], ["Liu", "Dianbo", ""]]}, {"id": "1910.11510", "submitter": "Daning Cheng", "authors": "Daning Cheng, Hanping Zhang, Fen Xia, Shigang Li, and Yunquan Zhang", "title": "The Scalability for Parallel Machine Learning Training Algorithm:\n  Dataset Matters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To gain a better performance, many researchers put more computing resource\ninto an application. However, in the AI area, there is still a lack of a\nsuccessful large-scale machine learning training application: The scalability\nand performance reproducibility of parallel machine learning training algorithm\nare limited and there are a few pieces of research focusing on why these\nindexes are limited but there are very few research efforts explaining the\nreasons in essence. In this paper, we propose that the sample difference in\ndataset plays a more prominent role in parallel machine learning algorithm\nscalability. Dataset characters can measure sample difference. These characters\ninclude the variance of the sample in a dataset, sparsity, sample diversity and\nsimilarity in sampling sequence. To match our proposal, we choose four kinds of\nparallel machine learning training algorithms as our research objects: (1)\nAsynchronous parallel SGD algorithm (Hogwild! algorithm) (2) Parallel model\naverage SGD algorithm (Mini-batch SGD algorithm) (3) Decenterilization\noptimization algorithm, (4) Dual Coordinate Optimization (DADM algorithm).\nThese algorithms cover different types of machine learning optimization\nalgorithms. We present the analysis of their convergence proof and design\nexperiments. Our results show that the characters datasets decide the\nscalability of the machine learning algorithm. What is more, there is an upper\nbound of parallel scalability for machine learning algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 03:15:49 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 12:26:27 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Cheng", "Daning", ""], ["Zhang", "Hanping", ""], ["Xia", "Fen", ""], ["Li", "Shigang", ""], ["Zhang", "Yunquan", ""]]}, {"id": "1910.11613", "submitter": "Mattia Bianchi", "authors": "Mattia Bianchi, Giuseppe Belgioioso and Sergio Grammatico", "title": "A fully-distributed proximal-point algorithm for Nash equilibrium\n  seeking with linear convergence rate", "comments": "To appear in the 59th Conference on Decision and Control (CDC 2020)", "journal-ref": null, "doi": "10.1109/CDC42340.2020.9304145", "report-no": null, "categories": "math.OC cs.DC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the Nash equilibrium problem in a partial-decision information\nscenario, where each agent can only observe the actions of some neighbors,\nwhile its cost possibly depends on the strategies of other agents. Our main\ncontribution is the design of a fully-distributed, single-layer, fixed-step\nalgorithm, based on a proximal best-response augmented with consensus terms. To\nderive our algorithm, we follow an operator-theoretic approach. First, we\nrecast the Nash equilibrium problem as that of finding a zero of a monotone\noperator. Then, we demonstrate that the resulting inclusion can be solved in a\nfully-distributed way via a proximal-point method, thanks to the use of a novel\npreconditioning matrix. Under strong monotonicity and Lipschitz continuity of\nthe game mapping, We prove linear convergence of our algorithm to a Nash\nequilibrium. Furthermore, we show that our method outperforms the fastest known\ngradient-based schemes, both in terms of guaranteed convergence rate, via\ntheoretical analysis, and in practice, via numerical simulations.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 10:45:26 GMT"}, {"version": "v2", "created": "Sat, 23 May 2020 14:06:30 GMT"}, {"version": "v3", "created": "Thu, 10 Sep 2020 16:34:11 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Bianchi", "Mattia", ""], ["Belgioioso", "Giuseppe", ""], ["Grammatico", "Sergio", ""]]}, {"id": "1910.11632", "submitter": "Michael J. Klaiber", "authors": "Michael J. Klaiber, Sebastian Vogel, Axel Acosta, Robert Korn,\n  Leonardo Ecco, Kristine Back, Andre Guntoro, Ingo Feldner", "title": "An End-to-End HW/SW Co-Design Methodology to Design Efficient Deep\n  Neural Network Systems using Virtual Models", "comments": null, "journal-ref": "Embedded Systems Week 2019, INTelligent Embedded Systems\n  Architectures and Applications Workshop 2019", "doi": "10.1145/3372394.3372396", "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end performance estimation and measurement of deep neural network\n(DNN) systems become more important with increasing complexity of DNN systems\nconsisting of hardware and software components. The methodology proposed in\nthis paper aims at a reduced turn-around time for evaluating different design\nchoices of hardware and software components of DNN systems. This reduction is\nachieved by moving the performance estimation from the implementation phase to\nthe concept phase by employing virtual hardware models instead of gathering\nmeasurement results from physical prototypes. Deep learning compilers introduce\nhardware-specific transformations and are, therefore, considered a part of the\ndesign flow of virtual system models to extract end-to-end performance\nestimations. To validate the run-time accuracy of the proposed methodology, a\nsystem processing the DilatedVGG DNN is realized both as virtual system model\nand as hardware implementation. The results show that up to 92 % accuracy can\nbe reached in predicting the processing time of the DNN inference.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 11:42:00 GMT"}, {"version": "v2", "created": "Mon, 18 Nov 2019 11:35:18 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Klaiber", "Michael J.", ""], ["Vogel", "Sebastian", ""], ["Acosta", "Axel", ""], ["Korn", "Robert", ""], ["Ecco", "Leonardo", ""], ["Back", "Kristine", ""], ["Guntoro", "Andre", ""], ["Feldner", "Ingo", ""]]}, {"id": "1910.11672", "submitter": "Carlos E. Budde", "authors": "Carlos E. Budde, Marco Biagi, Ra\\'ul E. Monti, Pedro R. D'Argenio,\n  Mari\\\"elle Stoelinga", "title": "Rare Event Simulation for non-Markovian repairable Fault Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.FL cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Dynamic Fault Trees (DFT) are widely adopted in industry to assess the\ndependability of safety-critical equipment. Since many systems are too large to\nbe studied numerically, DFTs dependability is often analysed using Monte Carlo\nsimulation. A bottleneck here is that many simulation samples are required in\nthe case of rare events, e.g. in highly reliable systems where components fail\nseldomly. Rare Event Simulation (RES) provides techniques to reduce the number\nof samples in the case of rare events. We present a RES technique based on\nimportance splitting, to study failures in highly reliable DFTs. Whereas RES\nusually requires meta-information from an expert, our method is fully\nautomatic: by cleverly exploiting the fault tree structure we extract the\nso-called importance function. We handle DFTs with Markovian and non-Markovian\nfailure and repair distributions (for which no numerical methods exist) and\nshow the efficiency of our approach on several case studies.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 19:20:42 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 14:56:27 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Budde", "Carlos E.", ""], ["Biagi", "Marco", ""], ["Monti", "Ra\u00fal E.", ""], ["D'Argenio", "Pedro R.", ""], ["Stoelinga", "Mari\u00eblle", ""]]}, {"id": "1910.11829", "submitter": "Hamed Saleh", "authors": "MohammadTaghi Hajiaghayi, Hamed Saleh, Saeed Seddighin, Xiaorui Sun", "title": "String Matching with Wildcards in the Massively Parallel Computation\n  Model", "comments": null, "journal-ref": null, "doi": "10.1145/3409964.3461793", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study distributed algorithms for string matching problem in presence of\nwildcard characters. Given a string T (a text), we look for all occurrences of\nanother string P (a pattern) as a substring of string T . Each wildcard\ncharacter in the pattern matches a specific class of strings based on its type.\nString matching is one of the most fundamental problems in computer science,\nespecially in the fields of bioinformatics and machine learning. Persistent\neffort has led to a variety of algorithms for the problem since 1960s.\n  With rise of big data and the inevitable demand to solve problems on huge\ndata sets, there have been many attempts to adapt classic algorithms into the\nMPC framework to obtain further efficiency. MPC is a recent framework for\nparallel computation of big data, which is designed to capture the\nMapReduce-like algorithms. In this paper, we study the string matching problem\nusing a set of tools translated to MPC model. We consider three types of\nwildcards in string matching:\n  - '?' wildcard: In this setting, the pattern is allowed to contain special\n'?' characters or don't cares that match any character of the text. String\nmatching with don't cares could be solved by fast convolutions, and we give a\nconstant round MPC algorithm for which by utilizing FFT in a constant number of\nMPC rounds.\n  - '+' wildcard: '+' wildcard is a special character that allows for arbitrary\nrepetitions of a character. When the pattern contains '+' wildcard characters,\nour algorithm runs in a constant number of MPC rounds by a reduction from\nsubset matching problem.\n  - '*' wildcard: '*' is a special character that matches with any substring of\nthe text. When '*' is allowed in the pattern, we solve two special cases of the\nproblem in logarithmic rounds.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 16:18:36 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 01:45:35 GMT"}, {"version": "v3", "created": "Mon, 4 Nov 2019 03:41:10 GMT"}, {"version": "v4", "created": "Fri, 4 Jun 2021 19:32:07 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Hajiaghayi", "MohammadTaghi", ""], ["Saleh", "Hamed", ""], ["Seddighin", "Saeed", ""], ["Sun", "Xiaorui", ""]]}, {"id": "1910.12200", "submitter": "Aerin Kim", "authors": "Rohit Pandey, Yingnong Dang, Ali Vira, Aerin Kim, Gil Lapid Shafriri,\n  Murali Chintalapati", "title": "Annual Interruption Rate as a KPI, its measurement and comparison", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is divided into two chapters. The first chapter describes the\nfailure rate as a KPI and studies its properties. The second one goes over ways\nto compare this KPI across two groups using the concepts of statistical\nhypothesis testing.\n  In section 1., we will motivate the failure rate as a KPI (in Azure, it is\ndubbed `Annual Interruption Rate' or AIR. In section 3, we will discuss\nmeasuring failure rate from logs machines typically generate. In section 1.2,\nwe will discuss the problem of measuring it from real-world data.\n  In section 2.1, we will discuss the general concepts of hypothesis testing.\nIn section 2.2, we will go over some general count distributions for modeling\nAzure reboots. In section 2.3, we will go over some experiments on applying\nvarious hypothesis tests to simulated data. In section 2.4, we will discuss\nsome applications of this work like using these statistical methods to catch\nregressions in failure rate and how long we need to let changes to the system\n`bake' before we are reasonably sure they didn't regress failure rate.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 07:33:22 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Pandey", "Rohit", ""], ["Dang", "Yingnong", ""], ["Vira", "Ali", ""], ["Kim", "Aerin", ""], ["Shafriri", "Gil Lapid", ""], ["Chintalapati", "Murali", ""]]}, {"id": "1910.12298", "submitter": "Mohsin Ur Rahman", "authors": "Mohsin Ur Rahman, Fabrizio Baiardi, Barbara Guidi and Laura Ricci", "title": "Protecting Personal Data using Smart Contracts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decentralized Online Social Networks (DOSNs) have been proposed as an\nalternative solution to the current centralized Online Social Networks (OSNs).\nOnline Social Networks are based on centralized architecture (e.g., Facebook,\nTwitter, or Google+), while DOSNs do not have a service provider that acts as\ncentral authority and users have more control over their information. Several\nDOSNs have been proposed during the last years. However, the decentralization\nof the OSN requires efficient solutions for protecting the privacy of users,\nand to evaluate the trust between users. Blockchain represents a disruptive\ntechnology which has been applied to several fields, among these also to Social\nNetworks. In this paper, we propose a manageable, user-driven and auditable\naccess control framework for DOSNs using blockchain technology. In the proposed\napproach, the blockchain is used as a support for the definition of privacy\npolicies. The resource owner uses the public key of the subject to define\nflexible role-based access control policies, while the private key associated\nwith the subject's Ethereum account is used to decrypt the private data once\naccess permission is validated on the blockchain. We evaluate our solution by\nexploiting the Rinkeby Ethereum testnet to deploy the smart contract, and to\nevaluate its performance. Experimental results show the feasibility of the\nproposed scheme in achieving auditable and user-driven access control via smart\ncontract deployed on the Blockchain.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 16:37:24 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Rahman", "Mohsin Ur", ""], ["Baiardi", "Fabrizio", ""], ["Guidi", "Barbara", ""], ["Ricci", "Laura", ""]]}, {"id": "1910.12308", "submitter": "Giorgi Nadiradze", "authors": "Giorgi Nadiradze, Amirmojtaba Sabour, Peter Davies, Ilia Markov,\n  Shigang Li, Dan Alistarh", "title": "Decentralized SGD with Asynchronous, Local and Quantized Updates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to scale distributed optimization to large node counts has been\none of the main enablers of recent progress in machine learning. To this end,\nseveral techniques have been explored, such as asynchronous, decentralized, or\nquantized communication--which significantly reduce the cost of\nsynchronization, and the ability for nodes to perform several local model\nupdates before communicating--which reduces the frequency of synchronization.\n  In this paper, we show that these techniques, which have so far been\nconsidered independently, can be jointly leveraged to minimize distribution\ncost for training neural network models via stochastic gradient descent (SGD).\nWe consider a setting with minimal coordination: we have a large number of\nnodes on a communication graph, each with a local subset of data, performing\nindependent SGD updates onto their local models. After some number of local\nupdates, each node chooses an interaction partner uniformly at random from its\nneighbors, and averages a possibly quantized version of its local model with\nthe neighbor's model. Our first contribution is in proving that, even under\nsuch a relaxed setting, SGD can still be guaranteed to converge under standard\nassumptions. The proof is based on a new connection with parallel\nload-balancing processes, and improves existing techniques by jointly handling\ndecentralization, asynchrony, quantization, and local updates, and by bounding\ntheir impact. On the practical side, we implement variants of our algorithm and\ndeploy them onto distributed environments, and show that they can successfully\nconverge and scale for large-scale image classification and translation tasks,\nmatching or even slightly improving the accuracy of previous methods.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 17:40:26 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 14:24:00 GMT"}, {"version": "v3", "created": "Mon, 14 Dec 2020 18:32:01 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Nadiradze", "Giorgi", ""], ["Sabour", "Amirmojtaba", ""], ["Davies", "Peter", ""], ["Markov", "Ilia", ""], ["Li", "Shigang", ""], ["Alistarh", "Dan", ""]]}, {"id": "1910.12310", "submitter": "Laxman Dhulipala", "authors": "Laxman Dhulipala, Charlie McGuffey, Hongbo Kang, Yan Gu, Guy E.\n  Blelloch, Phillip B. Gibbons, Julian Shun", "title": "Sage: Parallel Semi-Asymmetric Graph Algorithms for NVRAMs", "comments": "This is an extended version of a paper in PVLDB (to be presented at\n  VLDB'20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-volatile main memory (NVRAM) technologies provide an attractive set of\nfeatures for large-scale graph analytics, including byte-addressability, low\nidle power, and improved memory-density. NVRAM systems today have an order of\nmagnitude more NVRAM than traditional memory (DRAM). NVRAM systems could\ntherefore potentially allow very large graph problems to be solved on a single\nmachine, at a modest cost. However, a significant challenge in achieving high\nperformance is in accounting for the fact that NVRAM writes can be much more\nexpensive than NVRAM reads.\n  In this paper, we propose an approach to parallel graph analytics using the\nParallel Semi-Asymmetric Model (PSAM), in which the graph is stored as a\nread-only data structure (in NVRAM), and the amount of mutable memory is kept\nproportional to the number of vertices. Similar to the popular semi-external\nand semi-streaming models for graph analytics, the PSAM approach assumes that\nthe vertices of the graph fit in a fast read-write memory (DRAM), but the edges\ndo not. In NVRAM systems, our approach eliminates writes to the NVRAM, among\nother benefits.\n  To experimentally study this new setting, we develop Sage, a parallel\nsemi-asymmetric graph engine with which we implement provably-efficient (and\noften work-optimal) PSAM algorithms for over a dozen fundamental graph\nproblems. We experimentally study Sage using a 48-core machine on the largest\npublicly-available real-world graph (the Hyperlink Web graph with over 3.5\nbillion vertices and 128 billion edges) equipped with Optane DC Persistent\nMemory, and show that Sage outperforms the fastest prior systems designed for\nNVRAM. Importantly, we also show that Sage nearly matches the fastest prior\nsystems running solely in DRAM, by effectively hiding the costs of repeatedly\naccessing NVRAM versus DRAM.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 17:46:20 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 02:34:14 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Dhulipala", "Laxman", ""], ["McGuffey", "Charlie", ""], ["Kang", "Hongbo", ""], ["Gu", "Yan", ""], ["Blelloch", "Guy E.", ""], ["Gibbons", "Phillip B.", ""], ["Shun", "Julian", ""]]}, {"id": "1910.12331", "submitter": "Linjian Ma", "authors": "Navjot Singh, Linjian Ma, Hongru Yang, Edgar Solomonik", "title": "Comparison of Accuracy and Scalability of Gauss-Newton and Alternating\n  Least Squares for CP Decomposition", "comments": "20 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alternating least squares is the most widely used algorithm for CP tensor\ndecomposition. However, alternating least squares may exhibit slow or no\nconvergence, especially when high accuracy is required. An alternative approach\nis to regard CP decomposition as a nonlinear least squares problem and employ\nNewton-like methods. Direct solution of linear systems involving an\napproximated Hessian is generally expensive. However, recent advancements have\nshown that use of an implicit representation of the linear system makes these\nmethods competitive with alternating least squares. We provide the first\nparallel implementation of a Gauss-Newton method for CP decomposition, which\niteratively solves linear least squares problems at each Gauss-Newton step. In\nparticular, we leverage a formulation that employs tensor contractions for\nimplicit matrix-vector products within the conjugate gradient method. The use\nof tensor contractions enables us to employ the Cyclops library for\ndistributed-memory tensor computations to parallelize the Gauss-Newton approach\nwith a high-level Python implementation. In addition, we propose a\nregularization scheme for Gauss-Newton method to improve convergence properties\nwithout any additional cost. We study the convergence of variants of the\nGauss-Newton method relative to ALS for finding exact CP decompositions as well\nas approximate decompositions of real-world tensors. We evaluate the\nperformance of sequential and parallel versions of both approaches, and study\nthe parallel scalability on the Stampede2 supercomputer.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 19:43:58 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 00:24:45 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Singh", "Navjot", ""], ["Ma", "Linjian", ""], ["Yang", "Hongru", ""], ["Solomonik", "Edgar", ""]]}, {"id": "1910.12340", "submitter": "William Kuszmaul", "authors": "Tim Kaler, William Kuszmaul, Tao B. Schardl, Daniele Vettorel", "title": "Cilkmem: Algorithms for Analyzing the Memory High-Water Mark of\n  Fork-Join Parallel Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software engineers designing recursive fork-join programs destined to run on\nmassively parallel computing systems must be cognizant of how their program's\nmemory requirements scale in a many-processor execution. Although tools exist\nfor measuring memory usage during one particular execution of a parallel\nprogram, such tools cannot bound the worst-case memory usage over all possible\nparallel executions.\n  This paper introduces Cilkmem, a tool that analyzes the execution of a\ndeterministic Cilk program to determine its $p$-processor memory high-water\nmark (MHWM), which is the worst-case memory usage of the program over \\emph{all\npossible} $p$-processor executions. Cilkmem employs two new algorithms for\ncomputing the $p$-processor MHWM. The first algorithm calculates the exact\n$p$-processor MHWM in $O(T_1 \\cdot p)$ time, where $T_1$ is the total work of\nthe program. The second algorithm solves, in $O(T_1)$ time, the approximate\nthreshold problem, which asks, for a given memory threshold $M$, whether the\n$p$-processor MHWM exceeds $M/2$ or whether it is guaranteed to be less than\n$M$. Both algorithms are memory efficient, requiring $O(p \\cdot D)$ and $O(D)$\nspace, respectively, where $D$ is the maximum call-stack depth of the program's\nexecution on a single thread.\n  Our empirical studies show that Cilkmem generally exhibits low overheads.\nAcross ten application benchmarks from the Cilkbench suite, the exact algorithm\nincurs a geometric-mean multiplicative overhead of $1.54$ for $p=128$, whereas\nthe approximation-threshold algorithm incurs an overhead of $1.36$ independent\nof $p$. In addition, we use Cilkmem to reveal and diagnose a previously unknown\nissue in a large image-alignment program contributing to unexpectedly high\nmemory usage under parallel executions.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 20:22:26 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Kaler", "Tim", ""], ["Kuszmaul", "William", ""], ["Schardl", "Tao B.", ""], ["Vettorel", "Daniele", ""]]}, {"id": "1910.12579", "submitter": "Abhishek Dubey", "authors": "Scott Eisele and Taha Eghtesad and Keegan Campanelli and Prakhar\n  Agrawal and Aron Laszka and Abhishek Dubey", "title": "Safe and Private Forward-Trading Platform for Transactive Microgrids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.DC cs.MA cs.SY eess.SP eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transactive microgrids have emerged as a transformative solution for the\nproblems faced by distribution system operators due to an increase in the use\nof distributed energy resources and rapid growth in renewable energy\ngeneration. Transactive microgrids are tightly coupled cyber and physical\nsystems, which require resilient and robust financial markets where\ntransactions can be submitted and cleared, while ensuring that erroneous or\nmalicious transactions cannot destabilize the grid. In this paper, we introduce\nTRANSAX, a novel decentralized platform for transactive microgrids. TRANSAX\nenables participants to trade in an energy futures market, which improves\nefficiency by finding feasible matches for energy trades, reducing the load on\nthe distribution system operator. TRANSAX provides privacy to participants by\nanonymizing their trading activity using a distributed mixing service, while\nalso enforcing constraints that limit trading activity based on safety\nrequirements, such as keeping power flow below line capacity. We show that\nTRANSAX can satisfy the seemingly conflicting requirements of efficiency,\nsafety, and privacy, and we demonstrate its performance using simulation\nresults\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 21:13:18 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Eisele", "Scott", ""], ["Eghtesad", "Taha", ""], ["Campanelli", "Keegan", ""], ["Agrawal", "Prakhar", ""], ["Laszka", "Aron", ""], ["Dubey", "Abhishek", ""]]}, {"id": "1910.12747", "submitter": "Laurent Feuilloley", "authors": "Laurent Feuilloley", "title": "Introduction to local certification", "comments": "Last update: minor editing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A distributed graph algorithm is basically an algorithm where every node of a\ngraph can look at its neighborhood at some distance in the graph and chose its\noutput. As distributed environment are subject to faults, an important issue is\nto be able to check that the output is correct, or in general that the network\nis in proper configuration with respect to some predicate. One would like this\nchecking to be very local, to avoid using too much resources. Unfortunately\nmost predicates cannot be checked this way, and that is where certification\ncomes into play. Local certification (also known as proof-labeling schemes,\nlocally checkable proofs or distributed verification) consists in assigning\nlabels to the nodes, that certify that the configuration is correct. There are\nseveral point of view on this topic: it can be seen as a part of\nself-stabilizing algorithms, as labeling problem, or as a non-deterministic\ndistributed decision.\n  This paper is an introduction to the domain of local certification, giving an\noverview of the history, the techniques and the current research directions.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 15:16:15 GMT"}, {"version": "v2", "created": "Thu, 2 Jan 2020 15:30:14 GMT"}, {"version": "v3", "created": "Mon, 1 Feb 2021 16:19:39 GMT"}, {"version": "v4", "created": "Tue, 1 Jun 2021 17:08:12 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Feuilloley", "Laurent", ""]]}, {"id": "1910.12897", "submitter": "Maciej Besta", "authors": "Maciej Besta, Torsten Hoefler", "title": "Active Access: A Mechanism for High-Performance Distributed Data-Centric\n  Computations", "comments": null, "journal-ref": "Proceedings of the 29th ACM International Conference on\n  Supercomputing (ACM ICS'15), 2015", "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote memory access (RMA) is an emerging high-performance programming model\nthat uses RDMA hardware directly. Yet, accessing remote memories cannot invoke\nactivities at the target which complicates implementation and limits\nperformance of data-centric algorithms. We propose Active Access (AA), a\nmechanism that integrates well-known active messaging (AM) semantics with RMA\nto enable high-performance distributed data-centric computations. AA supports a\nnew programming model where the user specifies handlers that are triggered when\nincoming puts and gets reference designated addresses. AA is based on a set of\nextensions to the Input/Output Memory Management Unit (IOMMU), a unit that\nprovides high-performance hardware support for remapping I/O accesses to\nmemory. We illustrate that AA outperforms existing AM and RMA designs,\naccelerates various codes such as distributed hashtables or logging schemes,\nand enables new protocols such as incremental checkpointing for RMA.We also\ndiscuss how extended IOMMUs can support a virtualized global address space in a\ndistributed system that offers features known from on-node memory\nvirtualization. We expect that AA can enhance the design of HPC operating and\nruntime systems in large computing centers.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 18:09:23 GMT"}, {"version": "v2", "created": "Sun, 18 Oct 2020 23:13:52 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Besta", "Maciej", ""], ["Hoefler", "Torsten", ""]]}, {"id": "1910.13056", "submitter": "Sebastian Angel", "authors": "Sebastian Angel, Mihir Nanavati, Siddhartha Sen", "title": "Disaggregation and the Application", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines disaggregated data center architectures from the\nperspective of the applications that would run on these data centers, and\nchallenges the abstractions that have been proposed to date. In particular, we\nargue that operating systems for disaggregated data centers should not abstract\ndisaggregated hardware resources, such as memory, compute, and storage away\nfrom applications, but should instead give them information about, and control\nover, these resources. To this end, we propose additional OS abstractions and\ninterfaces for disaggregation and show how they can improve data transfer in\ndata parallel frameworks and speed up failure recovery in replicated,\nfault-tolerant applications. This paper studies the technical challenges in\nproviding applications with this additional functionality and advances several\npreliminary proposals to overcome these challenges.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 02:56:07 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Angel", "Sebastian", ""], ["Nanavati", "Mihir", ""], ["Sen", "Siddhartha", ""]]}, {"id": "1910.13067", "submitter": "The Canh Dinh", "authors": "Canh T. Dinh, Nguyen H. Tran, Minh N. H. Nguyen, Choong Seon Hong, Wei\n  Bao, Albert Y. Zomaya, Vincent Gramoli", "title": "Federated Learning over Wireless Networks: Convergence Analysis and\n  Resource Allocation", "comments": null, "journal-ref": null, "doi": "10.1109/TNET.2020.3035770", "report-no": null, "categories": "cs.LG cs.DC cs.NI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is an increasing interest in a fast-growing machine learning technique\ncalled Federated Learning, in which the model training is distributed over\nmobile user equipments (UEs), exploiting UEs' local computation and training\ndata. Despite its advantages in data privacy-preserving, Federated Learning\n(FL) still has challenges in heterogeneity across UEs' data and physical\nresources. We first propose a FL algorithm which can handle the heterogeneous\nUEs' data challenge without further assumptions except strongly convex and\nsmooth loss functions. We provide the convergence rate characterizing the\ntrade-off between local computation rounds of UE to update its local model and\nglobal communication rounds to update the FL global model. We then employ the\nproposed FL algorithm in wireless networks as a resource allocation\noptimization problem that captures the trade-off between the FL convergence\nwall clock time and energy consumption of UEs with heterogeneous computing and\npower resources. Even though the wireless resource allocation problem of FL is\nnon-convex, we exploit this problem's structure to decompose it into three\nsub-problems and analyze their closed-form solutions as well as insights to\nproblem design. Finally, we illustrate the theoretical analysis for the new\nalgorithm with Tensorflow experiments and extensive numerical results for the\nwireless resource allocation sub-problems. The experiment results not only\nverify the theoretical convergence but also show that our proposed algorithm\noutperforms the vanilla FedAvg algorithm in terms of convergence rate and\ntesting accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 03:31:28 GMT"}, {"version": "v2", "created": "Fri, 1 Nov 2019 03:31:59 GMT"}, {"version": "v3", "created": "Thu, 12 Mar 2020 00:16:29 GMT"}, {"version": "v4", "created": "Thu, 29 Oct 2020 03:09:33 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Dinh", "Canh T.", ""], ["Tran", "Nguyen H.", ""], ["Nguyen", "Minh N. H.", ""], ["Hong", "Choong Seon", ""], ["Bao", "Wei", ""], ["Zomaya", "Albert Y.", ""], ["Gramoli", "Vincent", ""]]}, {"id": "1910.13194", "submitter": "Ghafour Ahani", "authors": "Ghafour Ahani and Di Yuan", "title": "Accounting for Information Freshness in Scheduling of Content Caching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of optimal scheduling of content\nplacement along time in a base station with limited cache capacity, taking into\naccount jointly the offloading effect and freshness of information. We model\noffloading based on popularity in terms of the number of requests and\ninformation freshness based on the notion of age of information (AoI). The\nobjective is to reduce the load of backhaul links as well as the AoI of\ncontents in the cache via a joint cost function. For the resulting optimization\nproblem, we prove its hardness via a reduction from the Partition problem.\nNext, via a mathematical reformulation, we derive a solution approach based on\ncolumn generation and a tailored rounding mechanism. Finally, we provide\nperformance evaluation results showing that our algorithm provides near-optimal\nsolutions.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 11:06:00 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Ahani", "Ghafour", ""], ["Yuan", "Di", ""]]}, {"id": "1910.13346", "submitter": "Changxi Liu", "authors": "Changxi Liu and Hailong Yang and Xu Liu and Zhongzhi Luan and Depei\n  Qian", "title": "Intelligent-Unrolling: Exploiting Regular Patterns in Irregular\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern optimizing compilers are able to exploit memory access or computation\npatterns to generate vectorization codes. However, such patterns in irregular\napplications are unknown until runtime due to the input dependence. Thus,\neither compiler's static optimization or profile-guided optimization based on\nspecific inputs cannot predict the patterns for any common input, which leads\nto suboptimal code generation. To address this challenge, we develop\nIntelligent-Unroll, a framework to automatically optimize irregular\napplications with vectorization. Intelligent-Unroll allows the users to depict\nthe computation task using \\textit{code seed} with the memory access and\ncomputation patterns represented in \\textit{feature table} and\n\\textit{information-code tree}, and generates highly efficient codes.\nFurthermore, Intelligent-Unroll employs several novel optimization techniques\nto optimize reduction operations and gather/scatter instructions. We evaluate\nIntelligent-Unroll with sparse matrix-vector multiplication (SpMV) and graph\napplications. Experimental results show that Intelligent-Unroll is able to\ngenerate more efficient vectorization codes compared to the state-of-the-art\nimplementations.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 04:23:49 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Liu", "Changxi", ""], ["Yang", "Hailong", ""], ["Liu", "Xu", ""], ["Luan", "Zhongzhi", ""], ["Qian", "Depei", ""]]}, {"id": "1910.13373", "submitter": "Jesper Larsson Tr\\\"aff", "authors": "Jesper Larsson Tr\\\"aff", "title": "Decomposing Collectives for Exploiting Multi-lane Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern, high-performance systems increase the cumulated node-bandwidth\nby offering more than a single communication network and/or by having multiple\nconnections to the network. Efficient algorithms and implementations for\ncollective operations as found in, e.g., MPI must be explicitly designed for\nsuch multi-lane capabilities. We discuss a model for the design of multi-lane\nalgorithms, and in particular give a recipe for converting any standard,\none-ported, (pipelined) communication tree algorithm into a multi-lane\nalgorithm that can effectively use $k$ lanes simultaneously.\n  We first examine the problem from the perspective of \\emph{self-consistent\nperformance guidelines}, and give simple, \\emph{full-lane, mock-up\nimplementations} of the MPI broadcast, reduction, scan, gather, scatter,\nallgather, and alltoall operations using only similar operations of the given\nMPI library itself in such a way that multi-lane capabilities can be exploited.\nThese implementations which rely on a decomposition of the communication domain\ninto communicators for nodes and lanes are full-fledged and readily usable\nimplementations of the MPI collectives. The mock-up implementations, contrary\nto expectation, in many cases show surprising performance improvements with\ndifferent MPI libraries on a small 36-node dual-socket, dual-lane Intel\nOmniPath cluster, indicating severe problems with the native MPI library\nimplementations. Our full-lane implementations are in many cases considerably\nmore than a factor of two faster than the corresponding MPI collectives. We see\nsimilar results on the larger Vienna Scientific Cluster, VSC-3. These\nexperiments indicate considerable room for improvement of the MPI collectives\nin current libraries including more efficient use of multi-lane communication.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 16:36:51 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2019 15:21:21 GMT"}, {"version": "v3", "created": "Wed, 15 Jan 2020 13:34:10 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Tr\u00e4ff", "Jesper Larsson", ""]]}, {"id": "1910.13386", "submitter": "Changyong Hu", "authors": "Changyong Hu, Vijay K. Garg", "title": "NC Algorithms for Popular Matchings in One-Sided Preference Systems and\n  Related Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The popular matching problem is of matching a set of applicants to a set of\nposts, where each applicant has a preference list, ranking a non-empty subset\nof posts in the order of preference, possibly with ties. A matching M is\npopular if there is no other matching M' such that more applicants prefer M' to\nM. We give the first NC algorithm to solve the popular matching problem without\nties. We also give an NC algorithm that solves the maximum-cardinality popular\nmatching problem. No NC or RNC algorithms were known for the matching problem\nin preference systems prior to this work. Moreover, we give an NC algorithm for\na weaker version of the stable matching problem, that is, the problem of\nfinding the \"next\" stable matching given a stable matching.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 16:07:40 GMT"}, {"version": "v2", "created": "Fri, 20 Dec 2019 18:45:37 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Hu", "Changyong", ""], ["Garg", "Vijay K.", ""]]}, {"id": "1910.13397", "submitter": "Feng Zhao", "authors": "Feng Zhao, Xingzhi Niu, Shao-Lun Huang, Lin Zhang", "title": "Reproducing Scientific Experiment with Cloud DevOps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reproducibility of scientific experiment is vital for the advancement of\ndisciplines based on previous work. To achieve this goal, many researchers\nfocus on complex methodology and self-invented tools which have difficulty in\npractical usage. In this article, we introduce the Cloud DevOps infrastructure\nfrom software engineering community and shows how it can be used effectively\nfor heterogeneous agents to reproduce experiments for computer science related\ndisciplines. DevOps can be enabled using freely available cloud computing\nmachines for medium-sized experiment and self-hosted computing engines for\nlarge-scale computing, thus powering researchers to share their experiment\nresult with others in a more reliable way.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 07:47:20 GMT"}, {"version": "v2", "created": "Mon, 28 Dec 2020 08:26:14 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Zhao", "Feng", ""], ["Niu", "Xingzhi", ""], ["Huang", "Shao-Lun", ""], ["Zhang", "Lin", ""]]}, {"id": "1910.13555", "submitter": "Alfio Lazzaro", "authors": "Ilia Sivkov, Patrick Seewald, Alfio Lazzaro, Juerg Hutter", "title": "DBCSR: A Blocked Sparse Tensor Algebra Library", "comments": "10 pages, 3 figures, submitted at the ParCO2019 conference, Prague,\n  Czech Republic, 10-13 September 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advanced algorithms for large-scale electronic structure calculations are\nmostly based on processing multi-dimensional sparse data. Examples are sparse\nmatrix-matrix multiplications in linear-scaling Kohn-Sham calculations or the\nefficient determination of the exact exchange energy. When going beyond mean\nfield approaches, e.g. for Moller-Plesset perturbation theory, RPA and\nCoupled-Cluster methods, or the GW methods, it becomes necessary to manipulate\nhigher-order sparse tensors. Very similar problems are also encountered in\nother domains, like signal processing, data mining, computer vision, and\nmachine learning. With the idea that the most of the tensor operations can be\nmapped to matrices, we have implemented sparse tensor algebra functionalities\nin the frames of the sparse matrix linear algebra library DBCSR (Distributed\nBlock Compressed Sparse Row). DBCSR has been specifically designed to\nefficiently perform blocked-sparse matrix operations, so it becomes natural to\nextend its functionality to include tensor operations. We describe the newly\ndeveloped tensor interface and algorithms. In particular, we introduce the\ntensor contraction based on a fast rectangular sparse matrix multiplication\nalgorithm.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 21:52:30 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Sivkov", "Ilia", ""], ["Seewald", "Patrick", ""], ["Lazzaro", "Alfio", ""], ["Hutter", "Juerg", ""]]}, {"id": "1910.13598", "submitter": "Farzin Haddadpour", "authors": "Farzin Haddadpour, Mohammad Mahdi Kamani, Mehrdad Mahdavi, Viveck R.\n  Cadambe", "title": "Local SGD with Periodic Averaging: Tighter Analysis and Adaptive\n  Synchronization", "comments": "Paper accepted to NeurIPS 2019 - We fixed a flaw in the earlier\n  version regarding the dependency on constants but this change does not affect\n  the communication complexity", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communication overhead is one of the key challenges that hinders the\nscalability of distributed optimization algorithms. In this paper, we study\nlocal distributed SGD, where data is partitioned among computation nodes, and\nthe computation nodes perform local updates with periodically exchanging the\nmodel among the workers to perform averaging. While local SGD is empirically\nshown to provide promising results, a theoretical understanding of its\nperformance remains open. We strengthen convergence analysis for local SGD, and\nshow that local SGD can be far less expensive and applied far more generally\nthan current theory suggests. Specifically, we show that for loss functions\nthat satisfy the Polyak-{\\L}ojasiewicz condition, $O((pT)^{1/3})$ rounds of\ncommunication suffice to achieve a linear speed up, that is, an error of\n$O(1/pT)$, where $T$ is the total number of model updates at each worker. This\nis in contrast with previous work which required higher number of communication\nrounds, as well as was limited to strongly convex loss functions, for a similar\nasymptotic performance. We also develop an adaptive synchronization scheme that\nprovides a general condition for linear speed up. Finally, we validate the\ntheory with experimental results, running over AWS EC2 clouds and an internal\nGPU cluster.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 00:35:38 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 16:04:26 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Haddadpour", "Farzin", ""], ["Kamani", "Mohammad Mahdi", ""], ["Mahdavi", "Mehrdad", ""], ["Cadambe", "Viveck R.", ""]]}, {"id": "1910.13683", "submitter": "Sasindu Wijeratne", "authors": "Sasindu Wijeratne, Ashen Ekanayake, Sandaruwan Jayaweera, Danuka\n  Ravishan, Ajith Pasqual", "title": "Scalable High Performance SDN Switch Architecture on FPGA for Core\n  Networks", "comments": "6 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the increasing heterogeneity in network user requirements, dynamically\nvarying day to day network traffic patterns and delay in-network service\ndeployment, there is a huge demand for scalability and flexibility in modern\nnetworking infrastructure, which in return has paved way for the introduction\nof Software Defined Networking (SDN) in core networks. In this paper, we\npresent an FPGA-based switch that is fully compliant with OpenFlow; the\npioneering protocol for southbound interface of SDN. The switch architecture is\ncompletely implemented on hardware. The design consists of an OpenFlow\nSouthbound agent which can process OpenFlow packets at a rate of 10Gbps. The\nproposed architecture speed scales up to 400Gbps while it consumes only 60% of\nresources on a Xilinx Virtex-7 featuring XC7VX485T FPGA.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 05:45:37 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Wijeratne", "Sasindu", ""], ["Ekanayake", "Ashen", ""], ["Jayaweera", "Sandaruwan", ""], ["Ravishan", "Danuka", ""], ["Pasqual", "Ajith", ""]]}, {"id": "1910.13784", "submitter": "Abigail Goldsteen", "authors": "Abigail Goldsteen, Tomer Douek, Yaniv Cohen, Igor Gokhman, Ofir\n  Keren-Ackerman, Gadi Katsovich, Grisha Weintraub, and Doron Ben-Ari", "title": "Forgotten @ Scale: A Practical Solution for Implementing the Right To Be\n  Forgotten in Large-Scale Systems", "comments": null, "journal-ref": "1st International Workshop on Security and Privacy in Models and\n  Data (TRIDENT 2019)", "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The European General Data Protection Regulation asserts data subjects' right\nto be forgotten, i.e., their right to request that all their personal data be\ndeleted from an organizations' data stores. However, fulfilling such requests\nin large-scale systems is technically challenging. It requires that\norganizations keep track of all locations in which an individual's data is\nstored, be able to access and delete it in a reasonable time frame, and be able\nto prove that all such data was in fact deleted. In addition, organizations\nmust cope with complexities such as multiple, distributed, and continuously\nevolving systems of record, complex data retention policies and deletion\napproval workflows. We present a first design pattern and practical\nimplementation of the right to be forgotten on a large scale in Big Data and\ncloud environments.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 11:48:47 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Goldsteen", "Abigail", ""], ["Douek", "Tomer", ""], ["Cohen", "Yaniv", ""], ["Gokhman", "Igor", ""], ["Keren-Ackerman", "Ofir", ""], ["Katsovich", "Gadi", ""], ["Weintraub", "Grisha", ""], ["Ben-Ari", "Doron", ""]]}, {"id": "1910.13849", "submitter": "Jaber Kakar", "authors": "Jaber Kakar, Anton Khristoforov, Seyedhamed Ebadifar, Aydin Sezgin", "title": "Uplink-Downlink Tradeoff in Secure Distributed Matrix Multiplication", "comments": "Amazon EC2 results includes now encoding time. Second-Order Encoding\n  Strategy added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.DC cs.IR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In secure distributed matrix multiplication (SDMM) the multiplication\n$\\mathbf{A}\\mathbf{B}$ from two private matrices $\\mathbf{A}$ and $\\mathbf{B}$\nis outsourced by a user to $N$ distributed servers. In $\\ell$-SDMM, the goal is\nto a design a joint communication-computation procedure that optimally balances\nconflicting communication and computation metrics without leaking any\ninformation on both $\\mathbf{A}$ and $\\mathbf{B}$ to any set of $\\ell\\leq N$\nservers. To this end, the user applies coding with $\\tilde{\\mathbf{A}}_i$ and\n$\\tilde{\\mathbf{B}}_i$ representing encoded versions of $\\mathbf{A}$ and\n$\\mathbf{B}$ destined to the $i$-th server. Now, SDMM involves multiple\ntradeoffs. One such tradeoff is the tradeoff between uplink (UL) and downlink\n(DL) costs. To find a good balance between these two metrics, we propose two\nschemes which we term USCSA and GSCSA that are based on secure cross subspace\nalignment (SCSA). We show that there are various scenarios where they\noutperform existing SDMM schemes from the literature with respect to the UL-DL\nefficiency. Next, we implement schemes from the literature, including USCSA and\nGSCSA, and test their performance on Amazon EC2. Our numerical results show\nthat USCSA and GSCSA establish a good balance between the time spend on the\ncommunication and computation in SDMMs. This is because they combine advantages\nof polynomial codes, namely low time for the upload of\n$\\left(\\tilde{\\mathbf{A}}_i,\\tilde{\\mathbf{B}}_i\\right)_{i=1}^{N}$ and the\ncomputation of $\\mathbf{O}_i=\\tilde{\\mathbf{A}}_i\\tilde{\\mathbf{B}}_i$, with\nthose of SCSA, being a low timing overhead for the download of\n$\\left(\\mathbf{O}_i\\right)_{i=1}^{N}$ and the decoding of\n$\\mathbf{A}\\mathbf{B}$.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 13:46:55 GMT"}, {"version": "v2", "created": "Mon, 4 Nov 2019 15:34:14 GMT"}, {"version": "v3", "created": "Mon, 2 Dec 2019 13:22:00 GMT"}, {"version": "v4", "created": "Sat, 2 May 2020 12:45:03 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Kakar", "Jaber", ""], ["Khristoforov", "Anton", ""], ["Ebadifar", "Seyedhamed", ""], ["Sezgin", "Aydin", ""]]}, {"id": "1910.13875", "submitter": "Vasisht Duddu", "authors": "Vasisht Duddu, N. Rajesh Pillai, D. Vijay Rao, Valentina E. Balas", "title": "Fault Tolerance of Neural Networks in Adversarial Settings", "comments": null, "journal-ref": "Journal of Intelligent and Fuzzy Systems (JIFS) 2020", "doi": "10.3233/JIFS-179677", "report-no": null, "categories": "cs.CR cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Artificial Intelligence systems require a through assessment of different\npillars of trust, namely, fairness, interpretability, data and model privacy,\nreliability (safety) and robustness against against adversarial attacks. While\nthese research problems have been extensively studied in isolation, an\nunderstanding of the trade-off between different pillars of trust is lacking.\nTo this extent, the trade-off between fault tolerance, privacy and adversarial\nrobustness is evaluated for the specific case of Deep Neural Networks, by\nconsidering two adversarial settings under a security and a privacy threat\nmodel. Specifically, this work studies the impact of the fault tolerance of the\nNeural Network on training the model by adding noise to the input (Adversarial\nRobustness) and noise to the gradients (Differential Privacy). While training\nmodels with noise to inputs, gradients or weights enhances fault tolerance, it\nis observed that adversarial robustness and fault tolerance are at odds with\neach other. On the other hand, ($\\epsilon,\\delta$)-Differentially Private\nmodels enhance the fault tolerance, measured using generalisation error,\ntheoretically has an upper bound of $e^{\\epsilon} - 1 + \\delta$. This novel\nstudy of the trade-off between different elements of trust is pivotal for\ntraining a model which satisfies the requirements for different pillars of\ntrust simultaneously.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 14:22:22 GMT"}, {"version": "v2", "created": "Sat, 7 Mar 2020 16:21:15 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Duddu", "Vasisht", ""], ["Pillai", "N. Rajesh", ""], ["Rao", "D. Vijay", ""], ["Balas", "Valentina E.", ""]]}, {"id": "1910.13939", "submitter": "Andreas Naumann", "authors": "Janine Gl\\\"anzel and Andreas Naumann and Tharun Suresh Kumar", "title": "Parallel computing in automation of decoupled fluid-thermostructural\n  simulation approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decoupling approach presents a novel solution/alternative to the highly\ntime-consuming fluid-thermal-structural simulation procedures when thermal\neffects and resultant displacements on machine tools are analyzed. Using high\ndimensional Characteristic Diagrams (CDs) along with a Clustering Algorithm\nthat immensely reduces the data needed for training, a limited number of CFD\nsimulations can suffice in effectively decoupling fluid and thermal-structural\nsimulations. This approach becomes highly significant when complex geometries\nor dynamic components are considered. However, there is still scope for\nimprovement in the reduction of time needed to train CDs. Parallel computation\ncan be effectively utilized in decoupling approach in simultaneous execution of\n(i) CFD simulations and data export, and (ii) Clustering technique involving\nGenetic Algorithm and Radial Basis Function interpolation, which clusters and\noptimizes the training data for CDs. Parallelization reduces the entire\ncomputation duration from several days to a few hours and thereby, improving\nthe efficiency and ease-of-use of decoupling simulation approach.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 15:49:44 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Gl\u00e4nzel", "Janine", ""], ["Naumann", "Andreas", ""], ["Kumar", "Tharun Suresh", ""]]}, {"id": "1910.14029", "submitter": "Stefan Engblom", "authors": "Jing Liu and Stefan Engblom and Carl Nettelblad", "title": "Flash X-ray diffraction imaging in 3D: a proposed analysis pipeline", "comments": null, "journal-ref": null, "doi": "10.1364/JOSAA.390384", "report-no": null, "categories": "eess.IV cs.CV cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern Flash X-ray diffraction Imaging (FXI) acquires diffraction signals\nfrom single biomolecules at a high repetition rate from X-ray Free Electron\nLasers (XFELs), easily obtaining millions of 2D diffraction patterns from a\nsingle experiment. Due to the stochastic nature of FXI experiments and the\nmassive volumes of data, retrieving 3D electron densities from raw 2D\ndiffraction patterns is a challenging and time-consuming task.\n  We propose a semi-automatic data analysis pipeline for FXI experiments, which\nincludes four steps: hit finding and preliminary filtering, pattern\nclassification, 3D Fourier reconstruction, and post analysis. We also include a\nrecently developed bootstrap methodology in the post-analysis step for\nuncertainty analysis and quality control. To achieve the best possible\nresolution, we further suggest using background subtraction, signal windowing,\nand convex optimization techniques when retrieving the Fourier phases in the\npost-analysis step.\n  As an application example, we quantified the 3D electron structure of the\nPR772 virus using the proposed data-analysis pipeline. The retrieved structure\nwas above the detector-edge resolution and clearly showed the\npseudo-icosahedral capsid of the PR772.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 13:29:54 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 09:04:33 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Liu", "Jing", ""], ["Engblom", "Stefan", ""], ["Nettelblad", "Carl", ""]]}, {"id": "1910.14127", "submitter": "Shawkat Khairullah", "authors": "Shawkat Khairullah and Carl Elks", "title": "Self-Repairing Hardware Architecture for Safety-Critical\n  Cyber-Physical-Systems", "comments": "10 pages. IET Cyber-Physical Systems: Theory & Applications 2020", "journal-ref": "in IET Cyber-Physical Systems: Theory & Applications, vol. 5, no.\n  1, pp. 92-99, 3 2020", "doi": "10.1049/iet-cps.2019.0022", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital embedded systems in safety-critical cyber-physical-systems (CPSs)\nrequire high levels of resilience and robustness against different fault\nclasses. In recent years, self-healing concepts based on biological physiology\nhave received attention for the design and implementation of reliable systems.\nHowever, many of these approaches have not been architected from the outset\nwith safety in mind, nor have they been targeted for the safety-related\nautomation industry where the significant need exists. This study presents a\nnew self-healing hardware architecture inspired by integrating biological\nconcepts, fault tolerance techniques, and IEC 61131-3 operational schematics to\nfacilitate adaption in automation and critical infrastructure. The proposed\narchitecture is organised in two levels: the critical functions layer used for\nproviding the intended service of the application and the healing layer that\ncontinuously monitors the correct execution of that application and generates\nhealth syndromes to heal any failure occurrence inside the functions layer.\nFinally, two industrial applications have been mapped on this architecture to\ndate, and the authors believe the nexus of its concepts can positively impact\nthe next generation of critical CPSs in industrial automation.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 20:50:15 GMT"}, {"version": "v2", "created": "Wed, 25 Mar 2020 18:21:24 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Khairullah", "Shawkat", ""], ["Elks", "Carl", ""]]}, {"id": "1910.14139", "submitter": "Andrew Davison", "authors": "Andrew J. Davison and Joseph Ortiz", "title": "FutureMapping 2: Gaussian Belief Propagation for Spatial AI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.DC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We argue the case for Gaussian Belief Propagation (GBP) as a strong\nalgorithmic framework for the distributed, generic and incremental\nprobabilistic estimation we need in Spatial AI as we aim at high performance\nsmart robots and devices which operate within the constraints of real products.\nProcessor hardware is changing rapidly, and GBP has the right character to take\nadvantage of highly distributed processing and storage while estimating global\nquantities, as well as great flexibility. We present a detailed tutorial on\nGBP, relating to the standard factor graph formulation used in robotics and\ncomputer vision, and give several simulation examples with code which\ndemonstrate its properties.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 21:12:14 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Davison", "Andrew J.", ""], ["Ortiz", "Joseph", ""]]}, {"id": "1910.14141", "submitter": "Xiong Zheng", "authors": "Xiong Zheng, Vijay Garg", "title": "Byzantine Lattice Agreement in Synchronous Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the Byzantine lattice agreement problem in\nsynchronous systems. The lattice agreement problem in crash failure model has\nbeen studied both in synchronous and asynchronous systems, which leads to the\ncurrent best upper bound of $O(\\log f)$ rounds in both systems. However, very\nfew algorithmic results are known for the lattice agreement problem in\nByzantine failure model. The paper [Nowak et al., DISC, 2019] first gives an\nalgorithm for a variant of the lattice agreement problem on cycle-free lattices\nthat tolerates up to $f < n/(h(X) + 1)$ Byzantine faults, where $n$ is the\nnumber of processes and $h(X)$ is the height of the input lattice $X$. The\nrecent preprint by Di et al. studies this problem with a slightly modified\nvalidity condition in asynchronous systems. They present a $O(f)$ rounds\nalgorithm by using the reliable broadcast primitive as a first step and\nfollowing the similar algorithmic framework as the algorithms in crash failure\nmodel.\n  In this paper, we propose three algorithms for the Byzantine lattice\nagreement problem in synchronous systems. The first algorithm takes $\\min\n\\{3h(X) + 6,6\\sqrt{f} + 6\\})$ rounds and $O(n^2 \\min\\{h(X), \\sqrt{f}\\})$\nmessages, where $h(X)$ is the height of the input lattice $X$, $n$ is the total\nnumber of processes. The second algorithm runs in $3\\log n + 3$ rounds and\ntakes $O(n^2 \\log n)$ messages. The third algorithm takes $4 \\log f + 3$ rounds\nand takes $O(n^2 \\log f)$ messages. All algorithms can tolerate up to $f <\n\\frac{n}{3}$ Byzantine failures.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 21:17:06 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2019 17:42:15 GMT"}, {"version": "v3", "created": "Mon, 17 Feb 2020 04:28:54 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Zheng", "Xiong", ""], ["Garg", "Vijay", ""]]}, {"id": "1910.14154", "submitter": "Christoph Grunau", "authors": "Christoph Grunau, Slobodan Mitrovi\\'c, Ronitt Rubinfeld, Ali Vakilian", "title": "Improved Local Computation Algorithm for Set Cover via Sparsification", "comments": "To appear in ACM-SIAM Symposium on Discrete Algorithms (SODA 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design a Local Computation Algorithm (LCA) for the set cover problem.\nGiven a set system where each set has size at most $s$ and each element is\ncontained in at most $t$ sets, the algorithm reports whether a given set is in\nsome fixed set cover whose expected size is $O(\\log{s})$ times the minimum\nfractional set cover value. Our algorithm requires $s^{O(\\log{s})} t^{O(\\log{s}\n\\cdot (\\log \\log{s} + \\log \\log{t}))}$ queries. This result improves upon the\napplication of the reduction of [Parnas and Ron, TCS'07] on the result of [Kuhn\net al., SODA'06], which leads to a query complexity of $(st)^{O(\\log{s} \\cdot\n\\log{t})}$.\n  To obtain this result, we design a parallel set cover algorithm that admits\nan efficient simulation in the LCA model by using a sparsification technique\nintroduced in [Ghaffari and Uitto, SODA'19] for the maximal independent set\nproblem. The parallel algorithm adds a random subset of the sets to the\nsolution in a style similar to the PRAM algorithm of [Berger et al., FOCS'89].\nHowever, our algorithm differs in the way that it never revokes its decisions,\nwhich results in a fewer number of adaptive rounds. This requires a novel\napproximation analysis which might be of independent interest.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 22:07:55 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 13:39:13 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Grunau", "Christoph", ""], ["Mitrovi\u0107", "Slobodan", ""], ["Rubinfeld", "Ronitt", ""], ["Vakilian", "Ali", ""]]}, {"id": "1910.14280", "submitter": "Navjot Singh", "authors": "Navjot Singh, Deepesh Data, Jemin George, Suhas Diggavi", "title": "SPARQ-SGD: Event-Triggered and Compressed Communication in Decentralized\n  Stochastic Optimization", "comments": "41 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose and analyze SPARQ-SGD, which is an event-triggered\nand compressed algorithm for decentralized training of large-scale machine\nlearning models. Each node can locally compute a condition (event) which\ntriggers a communication where quantized and sparsified local model parameters\nare sent. In SPARQ-SGD each node takes at least a fixed number ($H$) of local\ngradient steps and then checks if the model parameters have significantly\nchanged compared to its last update; it communicates further compressed model\nparameters only when there is a significant change, as specified by a (design)\ncriterion. We prove that the SPARQ-SGD converges as $O(\\frac{1}{nT})$ and\n$O(\\frac{1}{\\sqrt{nT}})$ in the strongly-convex and non-convex settings,\nrespectively, demonstrating that such aggressive compression, including\nevent-triggered communication, model sparsification and quantization does not\naffect the overall convergence rate as compared to uncompressed decentralized\ntraining; thereby theoretically yielding communication efficiency for \"free\".\nWe evaluate SPARQ-SGD over real datasets to demonstrate significant amount of\nsavings in communication over the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 06:58:38 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 23:23:41 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Singh", "Navjot", ""], ["Data", "Deepesh", ""], ["George", "Jemin", ""], ["Diggavi", "Suhas", ""]]}, {"id": "1910.14425", "submitter": "Farzin Haddadpour", "authors": "Farzin Haddadpour, Mehrdad Mahdavi", "title": "On the Convergence of Local Descent Methods in Federated Learning", "comments": "47 pages, \"Updates from v1: A technical error in Lemma B3 is\n  corrected\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In federated distributed learning, the goal is to optimize a global training\nobjective defined over distributed devices, where the data shard at each device\nis sampled from a possibly different distribution (a.k.a., heterogeneous or non\ni.i.d. data samples). In this paper, we generalize the local stochastic and\nfull gradient descent with periodic averaging-- originally designed for\nhomogeneous distributed optimization, to solve nonconvex optimization problems\nin federated learning. Although scant research is available on the\neffectiveness of local SGD in reducing the number of communication rounds in\nhomogeneous setting, its convergence and communication complexity in\nheterogeneous setting is mostly demonstrated empirically and lacks through\ntheoretical understating. To bridge this gap, we demonstrate that by properly\nanalyzing the effect of unbiased gradients and sampling schema in federated\nsetting, under mild assumptions, the implicit variance reduction feature of\nlocal distributed methods generalize to heterogeneous data shards and exhibits\nthe best known convergence rates of homogeneous setting both in general\nnonconvex and under {\\pl}~ condition (generalization of strong-convexity). Our\ntheoretical results complement the recent empirical studies that demonstrate\nthe applicability of local GD/SGD to federated learning. We also specialize the\nproposed local method for networked distributed optimization. To the best of\nour knowledge, the obtained convergence rates are the sharpest known to date on\nthe convergence of local decant methods with periodic averaging for solving\nnonconvex federated optimization in both centralized and networked distributed\noptimization.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 12:52:55 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2019 21:16:41 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Haddadpour", "Farzin", ""], ["Mahdavi", "Mehrdad", ""]]}, {"id": "1910.14548", "submitter": "George Teodoro", "authors": "Eduardo Scartezini, Willian Barreiros Jr., Tahsin Kurc, Jun Kong, Alba\n  C. M. A. Melo, Joel Saltz and George Teodoro", "title": "Run-time Parameter Sensitivity Analysis Optimizations", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient execution of parameter sensitivity analysis (SA) is critical to\nallow for its routinely use. The pathology image processing application\ninvestigated in this work processes high-resolution whole-slide cancer tissue\nimages from large datasets to characterize and classify the disease. However,\nthe application is parameterized and changes in parameter values may\nsignificantly affect its results. Thus, understanding the impact of parameters\nto the output using SA is important to draw reliable scientific conclusions.\nThe execution of the application is rather compute intensive, and a SA requires\nit to process the input data multiple times as parameter values are\nsystematically varied. Optimizing this process is then important to allow for\nSA to be executed with large datasets. In this work, we employ a distributed\ncomputing system with novel computation reuse optimizations to accelerate SA.\nThe new computation reuse strategy can maximize reuse even with limited memory\navailability where previous approaches would not be able to fully take\nadvantage of reuse. The proposed solution was evaluated on an environment with\n256 nodes (7168 CPU-cores) attaining a parallel efficiency of over 92%, and\nimproving the previous reuse strategies in up to 2.8x.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 15:50:09 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Scartezini", "Eduardo", ""], ["Barreiros", "Willian", "Jr."], ["Kurc", "Tahsin", ""], ["Kong", "Jun", ""], ["Melo", "Alba C. M. A.", ""], ["Saltz", "Joel", ""], ["Teodoro", "George", ""]]}, {"id": "1910.14614", "submitter": "Peilin Zheng", "authors": "Peilin Zheng, Zibin Zheng, Liang Chen", "title": "Selecting Reliable Blockchain Peers via Hybrid Blockchain Reliability\n  Prediction", "comments": "10 pages, Blockchain Reliability, Blockchain QoS,\n  http://www.xblock.pro", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain and blockchain-based decentralized applications are attracting\nincreasing attentions recently. In public blockchain systems, users usually\nconnect to third-party peers or run a peer to join the P2P blockchain network.\nHowever, connecting to unreliable blockchain peers will make users waste\nresources and even lose millions of dollars of cryptocurrencies. In order to\nselect the reliable blockchain peers, it is urgently needed to evaluate and\npredict the reliability of them. Faced with this problem, we propose H-BRP,\nHybrid Blockchain Reliability Prediction model to extract the blockchain\nreliability factors then make personalized prediction for each user.\nLarge-scale real-world experiments are conducted on 100 blockchain requesters\nand 200 blockchain peers. The implement and dataset of 2,000,000 test cases are\nreleased. The experimental results show that the proposed model obtains better\naccuracy than other approaches.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 17:02:07 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Zheng", "Peilin", ""], ["Zheng", "Zibin", ""], ["Chen", "Liang", ""]]}]