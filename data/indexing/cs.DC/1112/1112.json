[{"id": "1112.0384", "submitter": "Chinmoy Dutta", "authors": "Chinmoy Dutta and Gopal Pandurangan and Rajmohan Rajaraman and Zhifeng\n  Sun", "title": "Information Spreading in Dynamic Networks", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the fundamental problem of information spreading (also known as\ngossip) in dynamic networks. In gossip, or more generally, $k$-gossip, there\nare $k$ pieces of information (or tokens) that are initially present in some\nnodes and the problem is to disseminate the $k$ tokens to all nodes. The goal\nis to accomplish the task in as few rounds of distributed computation as\npossible. The problem is especially challenging in dynamic networks where the\nnetwork topology can change from round to round and can be controlled by an\non-line adversary.\n  The focus of this paper is on the power of token-forwarding algorithms, which\ndo not manipulate tokens in any way other than storing and forwarding them. We\nfirst consider a worst-case adversarial model first studied by Kuhn, Lynch, and\nOshman~\\cite{kuhn+lo:dynamic} in which the communication links for each round\nare chosen by an adversary, and nodes do not know who their neighbors for the\ncurrent round are before they broadcast their messages. Our main result is an\n$\\Omega(nk/\\log n)$ lower bound on the number of rounds needed for any\ndeterministic token-forwarding algorithm to solve $k$-gossip. This resolves an\nopen problem raised in~\\cite{kuhn+lo:dynamic}, improving their lower bound of\n$\\Omega(n \\log k)$, and matching their upper bound of $O(nk)$ to within a\nlogarithmic factor.\n  We next show that token-forwarding algorithms can achieve subquadratic time\nin the offline version of the problem where the adversary has to commit all the\ntopology changes in advance at the beginning of the computation, and present\ntwo polynomial-time offline token-forwarding algorithms. Our results are a step\ntowards understanding the power and limitation of token-forwarding algorithms\nin dynamic networks.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2011 04:56:12 GMT"}], "update_date": "2011-12-05", "authors_parsed": [["Dutta", "Chinmoy", ""], ["Pandurangan", "Gopal", ""], ["Rajaraman", "Rajmohan", ""], ["Sun", "Zhifeng", ""]]}, {"id": "1112.0416", "submitter": "Stefano Ferretti Stefano Ferretti", "authors": "Stefano Ferretti", "title": "Publish-Subscribe Systems via Gossip: a Study based on Complex Networks", "comments": "To appear in: Proc. of the 4th International Workshop on Simplifying\n  Complex Networks for Pratictioners (SIMPLEX 2012) - World Wide Web Conference\n  (WWW 2012), ACM, Lyon (France), April 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper analyzes the adoption of unstructured P2P overlay networks to\nbuild publish-subscribe systems. We consider a very simple distributed\ncommunication protocol, based on gossip and on the local knowledge each node\nhas about subscriptions made by its neighbours. In particular, upon reception\n(or generation) of a novel event, a node sends it to those neighbours whose\nsubscriptions match that event. Moreover, the node gossips the event to its\n\"non-interested\" neighbours, so that the event can be spread through the\noverlay. A mathematical analysis is provided to estimate the number of nodes\nreceiving the event, based on the network topology, the amount of subscribers\nand the gossip probability. These outcomes are compared to those obtained via\nsimulation. Results show even when the amount of subscribers represents a very\nsmall (yet non-negligible) portion of network nodes, by tuning the gossip\nprobability the event can percolate through the overlay. Hence, the use of\nunstructured networks. coupled with simple dissemination protocols, represents\na viable approach to build peer-to-peer publish-subscribe applications.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2011 10:15:56 GMT"}, {"version": "v2", "created": "Wed, 20 Jun 2012 11:46:19 GMT"}], "update_date": "2012-06-21", "authors_parsed": [["Ferretti", "Stefano", ""]]}, {"id": "1112.1141", "submitter": "Fabiano Botelho Dr.", "authors": "Nitin Garg and Ed Zhu and Fabiano C. Botelho", "title": "Highly-Concurrent Doubly-Linked Lists", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As file systems are increasingly being deployed on ever larger systems with\nmany cores and multi-gigabytes of memory, scaling the internal data structures\nof file systems has taken greater importance and urgency. A doubly-linked list\nis a simple and very commonly used data structure in file systems but it is not\nvery friendly to multi-threaded use. While special cases of lists, such as\nqueues and stacks, have lock-free versions that scale reasonably well, the\ngeneral form of a doubly-linked list offers no such solution. Using a mutex to\nserialize all operations remains the de-facto method of maintaining a doubly\nlinked list. This severely limits the scalability of the list and developers\nmust resort to ad-hoc workarounds that involve using multiple smaller lists\n(with individual locks) and deal with the resulting complexity of the system.\nIn this paper, we present an approach to building highly concurrent data\nstructures, with special focus on the implementation of highly concurrent\ndoubly-linked lists. Dubbed \"advanced doubly-linked list\" or \"adlist\" for\nshort, our list allows iteration in any direction, and insert/delete operations\nover non-overlapping nodes to execute in parallel. Operations with common nodes\nget serialized so as to always present a locally consistent view to the\ncallers. An adlist node needs an additional 8 bytes of space for keeping\nsynchronization information. The Data Domain File System makes extensive use of\nadlists which has allowed for significant scaling of the system without\nsacrificing simplicity.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2011 01:37:51 GMT"}], "update_date": "2011-12-07", "authors_parsed": [["Garg", "Nitin", ""], ["Zhu", "Ed", ""], ["Botelho", "Fabiano C.", ""]]}, {"id": "1112.1210", "submitter": "Michael Dinitz", "authors": "Atish Das Sarma, Michael Dinitz and Gopal Pandurangan", "title": "Efficient Computation of Distance Sketches in Distributed Networks", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distance computation is one of the most fundamental primitives used in\ncommunication networks. The cost of effectively and accurately computing\npairwise network distances can become prohibitive in large-scale networks such\nas the Internet and Peer-to-Peer (P2P) networks. To negotiate the rising need\nfor very efficient distance computation, approximation techniques for numerous\nvariants of this question have recently received significant attention in the\nliterature. The goal is to preprocess the graph and store a small amount of\ninformation such that whenever a query for any pairwise distance is issued, the\ndistance can be well approximated (i.e., with small stretch) very quickly in an\nonline fashion. Specifically, the pre-processing (usually) involves storing a\nsmall sketch with each node, such that at query time only the sketches of the\nconcerned nodes need to be looked up to compute the approximate distance. In\nthis paper, we present the first theoretical study of distance sketches derived\nfrom distance oracles in a distributed network. We first present a fast\ndistributed algorithm for computing approximate distance sketches, based on a\ndistributed implementation of the distance oracle scheme of [Thorup-Zwick, JACM\n2005]. We also show how to modify this basic construction to achieve different\ntradeoffs between the number of pairs for which the distance estimate is\naccurate and other parameters. These tradeoffs can then be combined to give an\nefficient construction of small sketches with provable average-case as well as\nworst-case performance. Our algorithms use only small-sized messages and hence\nare suitable for bandwidth-constrained networks, and can be used in various\nnetworking applications such as topology discovery and construction, token\nmanagement, load balancing, monitoring overlays, and several other problems in\ndistributed algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2011 10:01:28 GMT"}], "update_date": "2011-12-07", "authors_parsed": [["Sarma", "Atish Das", ""], ["Dinitz", "Michael", ""], ["Pandurangan", "Gopal", ""]]}, {"id": "1112.1245", "submitter": "Mikael Vejdemo-Johansson", "authors": "David Lipsky, Primoz Skraba, Mikael Vejdemo-Johansson", "title": "A spectral sequence for parallelized persistence", "comments": "15 pages, 10 figures, submitted to the ACM Symposium on Computational\n  Geometry", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DC math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We approach the problem of the computation of persistent homology for large\ndatasets by a divide-and-conquer strategy. Dividing the total space into\nseparate but overlapping components, we are able to limit the total memory\nresidency for any part of the computation, while not degrading the overall\ncomplexity much. Locally computed persistence information is then merged from\nthe components and their intersections using a spectral sequence generalizing\nthe Mayer-Vietoris long exact sequence.\n  We describe the Mayer-Vietoris spectral sequence and give details on how to\ncompute with it. This allows us to merge local homological data into the global\npersistent homology. Furthermore, we detail how the classical topology\nconstructions inherent in the spectral sequence adapt to a persistence\nperspective, as well as describe the techniques from computational commutative\nalgebra necessary for this extension.\n  The resulting computational scheme suggests a parallelization scheme, and we\ndiscuss the communication steps involved in this scheme. Furthermore, the\ncomputational scheme can also serve as a guideline for which parts of the\nboundary matrix manipulation need to co-exist in primary memory at any given\ntime allowing for stratified memory access in single-core computation. The\nspectral sequence viewpoint also provides easy proofs of a homology nerve lemma\nas well as a persistent homology nerve lemma. In addition, the algebraic tools\nwe develop to approch persistent homology provide a purely algebraic\nformulation of kernel, image and cokernel persistence (D. Cohen-Steiner, H.\nEdelsbrunner, J. Harer, and D. Morozov. Persistent homology for kernels,\nimages, and cokernels. In Proceedings of the twentieth Annual ACM-SIAM\nSymposium on Discrete Algorithms, pages 1011-1020. Society for Industrial and\nApplied Mathematics, 2009.)\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2011 12:01:16 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Lipsky", "David", ""], ["Skraba", "Primoz", ""], ["Vejdemo-Johansson", "Mikael", ""]]}, {"id": "1112.1336", "submitter": "Guodong Shi", "authors": "Guodong Shi, Brian D. O. Anderson, Karl Henrik Johansson", "title": "Consensus over Random Graph Processes: Network Borel-Cantelli Lemmas for\n  Almost Sure Convergence", "comments": "IEEE Transactions on Information Theory, In Press", "journal-ref": null, "doi": "10.1109/TIT.2015.2468584", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed consensus computation over random graph processes is considered.\nThe random graph process is defined as a sequence of random variables which\ntake values from the set of all possible digraphs over the node set. At each\ntime step, every node updates its state based on a Bernoulli trial, independent\nin time and among different nodes: either averaging among the neighbor set\ngenerated by the random graph, or sticking with its current state.\nConnectivity-independence and arc-independence are introduced to capture the\nfundamental influence of the random graphs on the consensus convergence.\nNecessary and/or sufficient conditions are presented on the success\nprobabilities of the Bernoulli trials for the network to reach a global almost\nsure consensus, with some sharp threshold established revealing a consensus\nzero-one law. Convergence rates are established by lower and upper bounds of\nthe $\\epsilon$-computation time. We also generalize the concepts of\nconnectivity/arc independence to their analogues from the $*$-mixing point of\nview, so that our results apply to a very wide class of graphical models,\nincluding the majority of random graph models in the literature, e.g.,\nErd\\H{o}s-R\\'{e}nyi, gossiping, and Markovian random graphs. We show that under\n$*$-mixing, our convergence analysis continues to hold and the corresponding\nalmost sure consensus conditions are established. Finally, we further\ninvestigate almost sure finite-time convergence of random gossiping algorithms,\nand prove that the Bernoulli trials play a key role in ensuring finite-time\nconvergence. These results add to the understanding of the interplay between\nrandom graphs, random computations, and convergence probability for distributed\ninformation processing.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2011 16:33:04 GMT"}, {"version": "v2", "created": "Thu, 29 Dec 2011 20:00:37 GMT"}, {"version": "v3", "created": "Mon, 7 May 2012 15:27:47 GMT"}, {"version": "v4", "created": "Tue, 9 Dec 2014 12:28:06 GMT"}, {"version": "v5", "created": "Wed, 12 Aug 2015 02:45:59 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Shi", "Guodong", ""], ["Anderson", "Brian D. O.", ""], ["Johansson", "Karl Henrik", ""]]}, {"id": "1112.1710", "submitter": "Jonathan Carroll-Nellenback", "authors": "Jonathan J. Carroll-Nellenback, Brandon Shroyer, Adam Frank, Chen Ding", "title": "Efficient Parallelization for AMR MHD Multiphysics Calculations;\n  Implementation in AstroBEAR", "comments": "Updated version of paper with improved scaling results", "journal-ref": "Journal of Computational Physics, Volume 236, 1 March 2013, Pages\n  461-476", "doi": null, "report-no": null, "categories": "astro-ph.SR cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current Adaptive Mesh Refinement (AMR) simulations require algorithms that\nare highly parallelized and manage memory efficiently. As compute engines grow\nlarger, AMR simulations will require algorithms that achieve new levels of\nefficient parallelization and memory management. We have attempted to employ\nnew techniques to achieve both of these goals. Patch or grid based AMR often\nemploys ghost cells to decouple the hyperbolic advances of each grid on a given\nrefinement level. This decoupling allows each grid to be advanced\nindependently. In AstroBEAR we utilize this independence by threading the grid\nadvances on each level with preference going to the finer level grids. This\nallows for global load balancing instead of level by level load balancing and\nallows for greater parallelization across both physical space and AMR level.\nThreading of level advances can also improve performance by interleaving\ncommunication with computation, especially in deep simulations with many levels\nof refinement. While we see improvements of up to 30% on deep simulations run\non a few cores, the speedup is typically more modest (5-20%) for larger scale\nsimulations. To improve memory management we have employed a distributed tree\nalgorithm that requires processors to only store and communicate local sections\nof the AMR tree structure with neighboring processors. Using this distributed\napproach we are able to get reasonable scaling efficiency (> 80%) out to 12288\ncores and up to 8 levels of AMR - independent of the use of threading.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2011 21:25:04 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2013 17:54:05 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Carroll-Nellenback", "Jonathan J.", ""], ["Shroyer", "Brandon", ""], ["Frank", "Adam", ""], ["Ding", "Chen", ""]]}, {"id": "1112.1851", "submitter": "Michael Menzel", "authors": "Michael Menzel and Marten Sch\\\"onherr and Jens Nimis and Stefan Tai", "title": "(MC2)2: A Generic Decision-Making Framework and its Application to Cloud\n  Computing", "comments": "short version, full version available in proceedings of International\n  Conference on Cloud Computing and Virtualization (CCV) 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing is a disruptive technology, representing a new model for\ninformation technology (IT) solution engineering and management that promises\nto introduce significant cost savings and other benefits. The adoption of Cloud\ncomputing requires a detailed comparison of infrastructure alternatives, taking\na number of aspects into careful consideration. Existing methods of evaluation,\nhowever, limit decision making to the relative costs of cloud computing, but do\nnot take a broader range of criteria into account. In this paper, we introduce\na generic, multi-criteria-based decision framework and an application for Cloud\nComputing, the Multi-Criteria Comparison Method for Cloud Computing ((MC2)2).\nThe framework and method allow organizations to determine what infrastructure\nbest suits their needs by evaluating and ranking infrastructure alternatives\nusing multiple criteria. Therefore, (MC2)2 offers a way to differentiate\ninfrastructures not only by costs, but also in terms of benefits, opportunities\nand risks. (MC2)2 can be adapted to facilitate a wide array of decision-making\nscenarios within the domain of information technology infrastructures,\ndepending on the criteria selected to support the framework.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2011 14:46:08 GMT"}, {"version": "v2", "created": "Fri, 16 Dec 2011 16:34:59 GMT"}], "update_date": "2011-12-19", "authors_parsed": [["Menzel", "Michael", ""], ["Sch\u00f6nherr", "Marten", ""], ["Nimis", "Jens", ""], ["Tai", "Stefan", ""]]}, {"id": "1112.2021", "submitter": "Debasis  Das", "authors": "Debasis Das and Rajiv Misra", "title": "Programmable Cellular Automata Based Efficient Parallel AES Encryption\n  Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI nlin.CG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Cellular Automata(CA) is a discrete computing model which provides simple,\nflexible and efficient platform for simulating complicated systems and\nperforming complex computation based on the neighborhoods information. CA\nconsists of two components 1) a set of cells and 2) a set of rules .\nProgrammable Cellular Automata(PCA) employs some control signals on a Cellular\nAutomata(CA) structure. Programmable Cellular Automata were successfully\napplied for simulation of biological systems, physical systems and recently to\ndesign parallel and distributed algorithms for solving task density and\nsynchronization problems. In this paper PCA is applied to develop cryptography\nalgorithms. This paper deals with the cryptography for a parallel AES\nencryption algorithm based on programmable cellular automata. This proposed\nalgorithm based on symmetric key systems.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2011 05:47:31 GMT"}], "update_date": "2011-12-12", "authors_parsed": [["Das", "Debasis", ""], ["Misra", "Rajiv", ""]]}, {"id": "1112.2025", "submitter": "Tin Tin  Yee", "authors": "Tin Tin Yee and Thinn Thu Naing", "title": "PC-Cluster based Storage System Architecture for Cloud Storage", "comments": null, "journal-ref": "International Journal on Cloud Computing: Services and\n  Architecture(IJCCSA),Vol.1, No.3, November 2011", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Design and architecture of cloud storage system plays a vital role in cloud\ncomputing infrastructure in order to improve the storage capacity as well as\ncost effectiveness. Usually cloud storage system provides users to efficient\nstorage space with elasticity feature. One of the challenges of cloud storage\nsystem is difficult to balance the providing huge elastic capacity of storage\nand investment of expensive cost for it. In order to solve this issue in the\ncloud storage infrastructure, low cost PC cluster based storage server is\nconfigured to be activated for large amount of data to provide cloud users.\nMoreover, one of the contributions of this system is proposed an analytical\nmodel using M/M/1 queuing network model, which is modeled on intended\narchitecture to provide better response time, utilization of storage as well as\npending time when the system is running. According to the analytical result on\nexperimental testing, the storage can be utilized more than 90% of storage\nspace. In this paper, two parts have been described such as (i) design and\narchitecture of PC cluster based cloud storage system. On this system, related\nto cloud applications, services configurations are explained in detailed. (ii)\nAnalytical model has been enhanced to be increased the storage utilization on\nthe target architecture.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2011 06:58:13 GMT"}], "update_date": "2011-12-12", "authors_parsed": [["Yee", "Tin Tin", ""], ["Naing", "Thinn Thu", ""]]}, {"id": "1112.2254", "submitter": "Abedelaziz  Mohaisen", "authors": "Abedelaziz Mohaisen and Huy Tran and Abhishek Chandra and Yongdae Kim", "title": "SocialCloud: Using Social Networks for Building Distributed Computing\n  Services", "comments": "15 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate a new computing paradigm, called SocialCloud, in\nwhich computing nodes are governed by social ties driven from a bootstrapping\ntrust-possessing social graph. We investigate how this paradigm differs from\nexisting computing paradigms, such as grid computing and the conventional cloud\ncomputing paradigms. We show that incentives to adopt this paradigm are\nintuitive and natural, and security and trust guarantees provided by it are\nsolid. We propose metrics for measuring the utility and advantage of this\ncomputing paradigm, and using real-world social graphs and structures of social\ntraces; we investigate the potential of this paradigm for ordinary users. We\nstudy several design options and trade-offs, such as scheduling algorithms,\ncentralization, and straggler handling, and show how they affect the utility of\nthe paradigm. Interestingly, we conclude that whereas graphs known in the\nliterature for high trust properties do not serve distributed trusted computing\nalgorithms, such as Sybil defenses---for their weak algorithmic properties,\nsuch graphs are good candidates for our paradigm for their self-load-balancing\nfeatures.\n", "versions": [{"version": "v1", "created": "Sat, 10 Dec 2011 05:52:58 GMT"}], "update_date": "2011-12-13", "authors_parsed": [["Mohaisen", "Abedelaziz", ""], ["Tran", "Huy", ""], ["Chandra", "Abhishek", ""], ["Kim", "Yongdae", ""]]}, {"id": "1112.2431", "submitter": "Arash Mohammadi ARASH MOHAMMADI", "authors": "Arash Mohammadi and Amir Asif", "title": "Distributed Particle Filter Implementation with Intermittent/Irregular\n  Consensus Convergence", "comments": "Revised Version Submitted to IEEE Transaction on Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by non-linear, non-Gaussian, distributed multi-sensor/agent\nnavigation and tracking applications, we propose a multi-rate consensus/fusion\nbased framework for distributed implementation of the particle filter (CF/DPF).\nThe CF/DPF framework is based on running localized particle filters to estimate\nthe overall state vector at each observation node. Separate fusion filters are\ndesigned to consistently assimilate the local filtering distributions into the\nglobal posterior by compensating for the common past information between\nneighbouring nodes. The CF/DPF offers two distinct advantages over its\ncounterparts. First, the CF/DPF framework is suitable for scenarios where\nnetwork connectivity is intermittent and consensus can not be reached between\ntwo consecutive observations. Second, the CF/DPF is not limited to the Gaussian\napproximation for the global posterior density. A third contribution of the\npaper is the derivation of the exact expression for computing the posterior\nCramer-Rao lower bound (PCRLB) for the distributed architecture based on a\nrecursive procedure involving the local Fisher information matrices (FIM) of\nthe distributed estimators. The performance of the CF/DPF algorithm closely\nfollows the centralized particle filter approaching the PCRLB at the signal to\nnoise ratios that we tested.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2011 03:23:47 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2012 20:55:56 GMT"}], "update_date": "2012-09-06", "authors_parsed": [["Mohammadi", "Arash", ""], ["Asif", "Amir", ""]]}, {"id": "1112.2444", "submitter": "Steffen Schreiner", "authors": "Steffen Schreiner, Latchezar Betev, Costin Grigoras, Maarten Litmaath", "title": "A Mediated Definite Delegation Model allowing for Certified Grid Job\n  Submission", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grid computing infrastructures need to provide traceability and accounting of\ntheir users\" activity and protection against misuse and privilege escalation. A\ncentral aspect of multi-user Grid job environments is the necessary delegation\nof privileges in the course of a job submission. With respect to these generic\nrequirements this document describes an improved handling of multi-user Grid\njobs in the ALICE (\"A Large Ion Collider Experiment\") Grid Services. A security\nanalysis of the ALICE Grid job model is presented with derived security\nobjectives, followed by a discussion of existing approaches of unrestricted\ndelegation based on X.509 proxy certificates and the Grid middleware gLExec.\nUnrestricted delegation has severe security consequences and limitations, most\nimportantly allowing for identity theft and forgery of delegated assignments.\nThese limitations are discussed and formulated, both in general and with\nrespect to an adoption in line with multi-user Grid jobs. Based on the\narchitecture of the ALICE Grid Services, a new general model of mediated\ndefinite delegation is developed and formulated, allowing a broker to assign\ncontext-sensitive user privileges to agents. The model provides strong\naccountability and long- term traceability. A prototype implementation allowing\nfor certified Grid jobs is presented including a potential interaction with\ngLExec. The achieved improvements regarding system security, malicious job\nexploitation, identity protection, and accountability are emphasized, followed\nby a discussion of non- repudiation in the face of malicious Grid jobs.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2011 05:16:51 GMT"}], "update_date": "2011-12-13", "authors_parsed": [["Schreiner", "Steffen", ""], ["Betev", "Latchezar", ""], ["Grigoras", "Costin", ""], ["Litmaath", "Maarten", ""]]}, {"id": "1112.2584", "submitter": "Mahmoud Mahmoud BMathsSci (Hons.)", "authors": "Mahmoud S. Mahmoud, Andrew Ensor, Alain Biem, Bruce Elmegreen and\n  Sergei Gulyaev", "title": "Data Provenance and Management in Radio Astronomy: A Stream Computing\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New approaches for data provenance and data management (DPDM) are required\nfor mega science projects like the Square Kilometer Array, characterized by\nextremely large data volume and intense data rates, therefore demanding\ninnovative and highly efficient computational paradigms. In this context, we\nexplore a stream-computing approach with the emphasis on the use of\naccelerators. In particular, we make use of a new generation of high\nperformance stream-based parallelization middleware known as InfoSphere\nStreams. Its viability for managing and ensuring interoperability and integrity\nof signal processing data pipelines is demonstrated in radio astronomy. IBM\nInfoSphere Streams embraces the stream-computing paradigm. It is a shift from\nconventional data mining techniques (involving analysis of existing data from\ndatabases) towards real-time analytic processing. We discuss using InfoSphere\nStreams for effective DPDM in radio astronomy and propose a way in which\nInfoSphere Streams can be utilized for large antennae arrays. We present a\ncase-study: the InfoSphere Streams implementation of an autocorrelating\nspectrometer, and using this example we discuss the advantages of the\nstream-computing approach and the utilization of hardware accelerators.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2011 15:16:41 GMT"}], "update_date": "2011-12-13", "authors_parsed": [["Mahmoud", "Mahmoud S.", ""], ["Ensor", "Andrew", ""], ["Biem", "Alain", ""], ["Elmegreen", "Bruce", ""], ["Gulyaev", "Sergei", ""]]}, {"id": "1112.2792", "submitter": "Masoomeh Sanei", "authors": "Masoomeh sanei and Nasrollah Moghaddam Charkari", "title": "Hybrid Heuristic-Based Artificial Immune System for Task Scheduling", "comments": "12 pages, 8 figures; International Journal of Distributed and\n  Parallel Systems (IJDPS) Vol.2, No.6, November 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Task scheduling problem in heterogeneous systems is the process of allocating\ntasks of an application to heterogeneous processors interconnected by\nhigh-speed networks, so that minimizing the finishing time of application as\nmuch as possible. Tasks are processing units of application and have\nprecedenceconstrained, communication and also, are presented by Directed\nAcyclic Graphs (DAGs). Evolutionary algorithms are well suited for solving task\nscheduling problem in heterogeneous environment. In this paper, we propose a\nhybrid heuristic-based Artificial Immune System (AIS) algorithm for solving the\nscheduling problem. In this regard, AIS with some heuristics and Single\nNeighbourhood Search (SNS) technique are hybridized. Clonning and immune-remove\noperators of AIS provide diversity, while heuristics and SNS provide\nconvergence of algorithm into good solutions, that is balancing between\nexploration and exploitation. We have compared our method with some\nstate-of-the art algorithms. The results of the experiments show the validity\nand efficiency of our method.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2011 04:41:39 GMT"}], "update_date": "2011-12-14", "authors_parsed": [["sanei", "Masoomeh", ""], ["Charkari", "Nasrollah Moghaddam", ""]]}, {"id": "1112.3062", "submitter": "Guy Kloss", "authors": "Miriam Ney and Guy K. Kloss and Andreas Schreiber", "title": "Using Provenance to support Good Laboratory Practice in Grid\n  Environments", "comments": "Book Chapter for \"Data Provenance and Data Management for eScience,\"\n  of Studies in Computational Intelligence series, Springer. 25 pages, 8\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Conducting experiments and documenting results is daily business of\nscientists. Good and traceable documentation enables other scientists to\nconfirm procedures and results for increased credibility. Documentation and\nscientific conduct are regulated and termed as \"good laboratory practice.\"\nLaboratory notebooks are used to record each step in conducting an experiment\nand processing data. Originally, these notebooks were paper based. Due to\ncomputerised research systems, acquired data became more elaborate, thus\nincreasing the need for electronic notebooks with data storage, computational\nfeatures and reliable electronic documentation. As a new approach to this, a\nscientific data management system (DataFinder) is enhanced with features for\ntraceable documentation. Provenance recording is used to meet requirements of\ntraceability, and this information can later be queried for further analysis.\nDataFinder has further important features for scientific documentation: It\nemploys a heterogeneous and distributed data storage concept. This enables\naccess to different types of data storage systems (e. g. Grid data\ninfrastructure, file servers). In this chapter we describe a number of building\nblocks that are available or close to finished development. These components\nare intended for assembling an electronic laboratory notebook for use in Grid\nenvironments, while retaining maximal flexibility on usage scenarios as well as\nmaximal compatibility overlap towards each other. Through the usage of such a\nsystem, provenance can successfully be used to trace the scientific workflow of\npreparation, execution, evaluation, interpretation and archiving of research\ndata. The reliability of research results increases and the research process\nremains transparent to remote research partners.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2011 22:30:36 GMT"}], "update_date": "2011-12-15", "authors_parsed": [["Ney", "Miriam", ""], ["Kloss", "Guy K.", ""], ["Schreiber", "Andreas", ""]]}, {"id": "1112.3516", "submitter": "Zhong Fan", "authors": "Zhong Fan, Parag Kulkarni, Sedat Gormus, Costas Efthymiou, Georgios\n  Kalogridis, Mahesh Sooriyabandara, Ziming Zhu, Sangarapillai Lambotharan, and\n  Woon Hau Chin", "title": "Smart Grid Communications: Overview of Research Challenges, Solutions,\n  and Standardization Activities", "comments": "To be published in IEEE Communications Surveys and Tutorials", "journal-ref": null, "doi": "10.1109/SURV.2011.122211.00021", "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization of energy consumption in future intelligent energy networks (or\nSmart Grids) will be based on grid-integrated near-real-time communications\nbetween various grid elements in generation, transmission, distribution and\nloads. This paper discusses some of the challenges and opportunities of\ncommunications research in the areas of smart grid and smart metering. In\nparticular, we focus on some of the key communications challenges for realizing\ninteroperable and future-proof smart grid/metering networks, smart grid\nsecurity and privacy, and how some of the existing networking technologies can\nbe applied to energy management. Finally, we also discuss the coordinated\nstandardization efforts in Europe to harmonize communications standards and\nprotocols.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2011 14:20:51 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Fan", "Zhong", ""], ["Kulkarni", "Parag", ""], ["Gormus", "Sedat", ""], ["Efthymiou", "Costas", ""], ["Kalogridis", "Georgios", ""], ["Sooriyabandara", "Mahesh", ""], ["Zhu", "Ziming", ""], ["Lambotharan", "Sangarapillai", ""], ["Chin", "Woon Hau", ""]]}, {"id": "1112.3725", "submitter": "Mamoun Jamous", "authors": "Mamoun M. Jamous, Safaai Bin Deris", "title": "Web Services Non-Functional Classification to Enhance Discovery Speed", "comments": "6 pages, 6 figures, 1 table", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 8,\n  Issue 4, July 2011", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Recently, the use and deployment of web services has dramatically increased.\nThis is due to the easiness, interoperability, and flexibility that web\nservices offer to the software systems, which other software structures don't\nsupport or support poorly. Web services discovery became more important and\nresearch conducted in this area became more critical. With the increasing\nnumber of published and publicly available web services, speed in web service\ndiscovery process is becoming an issue which cannot be neglected. This paper\nproposes a generic non-functional based web services classification algorithm.\nClassification algorithm depends on information supplied by web service\nprovider at the registration time. Authors have proved mathematically and\nexperimentally the usefulness and efficiency of proposed algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2011 07:39:02 GMT"}], "update_date": "2011-12-19", "authors_parsed": [["Jamous", "Mamoun M.", ""], ["Deris", "Safaai Bin", ""]]}, {"id": "1112.3765", "submitter": "Gero Greiner", "authors": "Gero Greiner and Riko Jacob", "title": "The Efficiency of MapReduce in Parallel External Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since its introduction in 2004, the MapReduce framework has become one of the\nstandard approaches in massive distributed and parallel computation. In\ncontrast to its intensive use in practise, theoretical footing is still limited\nand only little work has been done yet to put MapReduce on a par with the major\ncomputational models. Following pioneer work that relates the MapReduce\nframework with PRAM and BSP in their macroscopic structure, we focus on the\nfunctionality provided by the framework itself, considered in the parallel\nexternal memory model (PEM). In this, we present upper and lower bounds on the\nparallel I/O-complexity that are matching up to constant factors for the\nshuffle step. The shuffle step is the single communication phase where all\ninformation of one MapReduce invocation gets transferred from map workers to\nreduce workers. Hence, we move the focus towards the internal communication\nstep in contrast to previous work. The results we obtain further carry over to\nthe BSP* model. On the one hand, this shows how much complexity can be \"hidden\"\nfor an algorithm expressed in MapReduce compared to PEM. On the other hand, our\nresults bound the worst-case performance loss of the MapReduce approach in\nterms of I/O-efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2011 11:27:07 GMT"}], "update_date": "2011-12-19", "authors_parsed": [["Greiner", "Gero", ""], ["Jacob", "Riko", ""]]}, {"id": "1112.3844", "submitter": "\\\"Ozg\\\"ur Sa\\u{g}lam", "authors": "\\\"Ozg\\\"ur Sa\\u{g}lam and Mehmet Emin Dalkili\\c{c}", "title": "Cross Layer Implementation of Key Establishment and Configuration\n  Protocols in WSN", "comments": "16 pages, 12 figures", "journal-ref": "IJCSI, Vol. 8 Issue 6 No 2, November 2011, pp. 86-101", "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Security in Wireless Sensor Networks (WSN) can be achieved by establishing\nshared keys among the neighbor sensor nodes to create secure communication\nlinks. The protocol to be used for such a pairwise key establishment is a key\nfactor determining the energy to be consumed by each sensor node during the\nsecure network configuration. On the other hand, to achieve the optimum network\nconfiguration, nodes may not need to establish pairwise keys with all of their\nneighbors. Because, links to be established are defined by the network\nconfiguration protocol and as long as the network connectivity requirements are\nsatisfied, number of links to be secured can be limited accordingly. In this\nsense, key establishment and network configuration performances are related to\neach other and this cross relation should be taken into consideration while\nimplementing security for WSN. In this paper, we have investigated the cross\nlayer relations and performance figures of the selected randomized\npre-distribution and public key based key establishment protocols with the\nconfiguration protocol we proposed in a separate publication. Simulation\nresults indicate that total network configuration energy cost can be reduced by\nreducing the number of links to be secured without affecting the global network\nconnectivity performance. Results also show that the energy and resilience\nperformances of the public key establishment can be better than the key\npre-distribution for a given set of network configuration parameters.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2011 15:05:14 GMT"}], "update_date": "2011-12-19", "authors_parsed": [["Sa\u011flam", "\u00d6zg\u00fcr", ""], ["Dalkili\u00e7", "Mehmet Emin", ""]]}, {"id": "1112.3880", "submitter": "Michael Menzel", "authors": "Michael Menzel and Rajiv Ranjan", "title": "CloudGenius: Automated Decision Support for Migrating Multi-Component\n  Enterprise Applications to Clouds", "comments": "technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key problems in migrating multi-component enterprise applications\nto Clouds is selecting the best mix of VM images and Cloud infrastructure\nservices. A migration process has to ensure that Quality of Service (QoS)\nrequirements are met, while satisfying conflicting selection criteria, e.g.\nthroughput and cost. When selecting Cloud services, application engineers must\nconsider heterogeneous sets of criteria and complex dependencies across\nmultiple layers impossible to resolve manually. To overcome this challenge, we\npresent the generic recommender framework CloudGenius and an implementation\nthat leverage well known multi-criteria decision making technique Analytic\nHierarchy Process to automate the selection process based on a model, factors,\nand QoS requirements related to enterprise applications. In particular, we\nintroduce a structured migration process for multi-component enterprise\napplications, clearly identify the most important criteria relevant to the\nselection problem and present a multi-criteria-based selection algorithm.\nExperiments with the software prototype CumulusGenius show time complexities.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2011 16:24:34 GMT"}, {"version": "v2", "created": "Mon, 6 Feb 2012 12:30:38 GMT"}], "update_date": "2012-02-07", "authors_parsed": [["Menzel", "Michael", ""], ["Ranjan", "Rajiv", ""]]}, {"id": "1112.3972", "submitter": "Serguei Mokhov", "authors": "Emil Vassev and Serguei A. Mokhov", "title": "Developing Autonomic Properties for Distributed Pattern-Recognition\n  Systems with ASSL: A Distributed MARF Case Study", "comments": "28 pages; 16 figures; Submitted and accepted in 2010; to appear in\n  \"E. Vassev and S. A. Mokhov. Development and evaluation of autonomic\n  properties for pattern-recognition systems with ASSL -- a distributed MARF\n  case study. Transactions on Computational Science, Special Issue on Advances\n  in Autonomic Computing: Formal Engineering Methods for Nature-Inspired\n  Computing Systems, XV (LNCS7050).\"", "journal-ref": "J. Trans. on Comput. Sci. XV, Springer-Verlag,130-157", "doi": "10.1007/978-3-642-28525-7_5", "report-no": null, "categories": "cs.DC cs.CV cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we discuss our research towards developing special properties\nthat introduce autonomic behavior in pattern-recognition systems. In our\napproach we use ASSL (Autonomic System Specification Language) to formally\ndevelop such properties for DMARF (Distributed Modular Audio Recognition\nFramework). These properties enhance DMARF with an autonomic middleware that\nmanages the four stages of the framework's pattern-recognition pipeline. DMARF\nis a biologically inspired system employing pattern recognition, signal\nprocessing, and natural language processing helping us process audio, textual,\nor imagery data needed by a variety of scientific applications, e.g., biometric\napplications. In that context, the notion go autonomic DMARF (ADMARF) can be\nemployed by autonomous and robotic systems that theoretically require\nless-to-none human intervention other than data collection for pattern analysis\nand observing the results. In this article, we explain the ASSL specification\nmodels for the autonomic properties of DMARF.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2011 21:12:17 GMT"}], "update_date": "2013-07-08", "authors_parsed": [["Vassev", "Emil", ""], ["Mokhov", "Serguei A.", ""]]}, {"id": "1112.4303", "submitter": "Antun Balaz", "authors": "Antun Balaz, Ognjen Prnjat, Dusan Vudragovic, Vladimir Slavnic,\n  Ioannis Liabotis, Emanouil Atanassov, Boro Jakimovski, Mihajlo Savic", "title": "Development of Grid e-Infrastructure in South-Eastern Europe", "comments": "22 pages, 12 figures, 4 tables", "journal-ref": "J. Grid Comput. 9, 135 (2011)", "doi": "10.1007/s10723-011-9185-0", "report-no": null, "categories": "cs.DC cs.NI cs.SI physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the period of 6 years and three phases, the SEE-GRID programme has\nestablished a strong regional human network in the area of distributed\nscientific computing and has set up a powerful regional Grid infrastructure. It\nattracted a number of user communities and applications from diverse fields\nfrom countries throughout the South-Eastern Europe. From the infrastructure\npoint view, the first project phase has established a pilot Grid infrastructure\nwith more than 20 resource centers in 11 countries. During the subsequent two\nphases of the project, the infrastructure has grown to currently 55 resource\ncenters with more than 6600 CPUs and 750 TBs of disk storage, distributed in 16\nparticipating countries. Inclusion of new resource centers to the existing\ninfrastructure, as well as a support to new user communities, has demanded\nsetup of regionally distributed core services, development of new monitoring\nand operational tools, and close collaboration of all partner institution in\nmanaging such a complex infrastructure. In this paper we give an overview of\nthe development and current status of SEE-GRID regional infrastructure and\ndescribe its transition to the NGI-based Grid model in EGI, with the strong SEE\nregional collaboration.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2011 11:03:54 GMT"}], "update_date": "2011-12-20", "authors_parsed": [["Balaz", "Antun", ""], ["Prnjat", "Ognjen", ""], ["Vudragovic", "Dusan", ""], ["Slavnic", "Vladimir", ""], ["Liabotis", "Ioannis", ""], ["Atanassov", "Emanouil", ""], ["Jakimovski", "Boro", ""], ["Savic", "Mihajlo", ""]]}, {"id": "1112.4428", "submitter": "Ido Ben-Zvi", "authors": "Ido Ben-Zvi", "title": "Causality, Knowledge and Coordination in Distributed Systems", "comments": "PhD Dissertation", "journal-ref": null, "doi": null, "report-no": "PHD-2011-09", "categories": "cs.LO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effecting coordination across remote sites in a distributed system is an\nessential part of distributed computing, and also an inherent challenge. In\n1978, an analysis of communication in asynchronous systems was suggested by\nLeslie Lamport. Lamport's analysis determines a notion of temporal precedence,\na sort of weak notion of time, which is otherwise missing in asynchronous\nsystems. This notion has been extensively utilized in various applications.\n  Yet the analysis is limited to systems that are asynchronous. In this thesis\nwe go beyond by investigating causality in synchronous systems. In such\nsystems, the boundaries of causal influence are not charted out exclusively by\nmessage passing. Here time itself, passing at a uniform (or almost uniform)\nrate for all processes, is also a medium by which causal influence may fan out.\nThis thesis studies, and characterizes, the combinations of time and message\npassing that govern causal influence in synchronous systems.\n  It turns out that knowledge based analysis [FHMV] provides a well tailored\nformal framework within which causal notions can be studied. As we show, the\nformal notion of knowledge is highly appropriate for characterizing causal\ninfluence in terms of information flow, broadening the analysis of Chandy and\nMisra in [ChM].\n  We define several generic classes of coordination problems that pose various\ntemporal ordering requirements on the participating processes. These\ncoordination problems provide natural generalizations of real life\nrequirements. We then analyze the causal conditions that underlie suitable\nsolutions to these problems. The analysis is conducted in two stages: first,\nthe temporal ordering requirements are reduced to epistemic conditions. Then,\nthese epistemic conditions are characterized in terms of the causal\ncommunication patterns that are necessary and sufficient to bring them about.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2011 18:27:05 GMT"}], "update_date": "2011-12-20", "authors_parsed": [["Ben-Zvi", "Ido", ""]]}, {"id": "1112.4604", "submitter": "Jose Gracia", "authors": "Steffen Brinkmann, Jos\\'e Gracia, Christoph Niethammer, Rainer Keller", "title": "TEMANEJO - a debugger for task based parallel programming models", "comments": "8 pages, presented at ParCO 2011, Ghent, Belgium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the program Temanejo, a debugger for task based parallelisation\nmodels such as StarSs. The challenge in debugging StarSs applications lies in\nthe fact that tasks are scheduled at runtime, i.e dynamically in accordance to\nthe data dependencies between them. Our tool assists the programmer in the\ndebugging process by visualising the task dependency graph and allowing to\ncontrol the scheduling of tasks. The toolset consists of the library Ayudame\nwhich communicates with the StarSs runtime on one side and of the debugger\nTemanejo on the other side which communicates with Ayudame. Temanejo provides a\ngraphical user interface with which the application can be analysed and\ncontrolled.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2011 08:41:56 GMT"}], "update_date": "2011-12-22", "authors_parsed": [["Brinkmann", "Steffen", ""], ["Gracia", "Jos\u00e9", ""], ["Niethammer", "Christoph", ""], ["Keller", "Rainer", ""]]}, {"id": "1112.4980", "submitter": "Meni Rosenfeld", "authors": "Meni Rosenfeld", "title": "Analysis of Bitcoin Pooled Mining Reward Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe the various scoring systems used to calculate\nrewards of participants in Bitcoin pooled mining, explain the problems each\nwere designed to solve and analyze their respective advantages and\ndisadvantages.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2011 10:40:38 GMT"}], "update_date": "2011-12-22", "authors_parsed": [["Rosenfeld", "Meni", ""]]}, {"id": "1112.5200", "submitter": "Marc-Thierry Jaekel", "authors": "Philippe Matherat and Marc-Thierry Jaekel", "title": "Relativistic causality and clockless circuits", "comments": "25 pages, 5 figures", "journal-ref": "ACM J. Emerg. Technol. Comput. Syst. 7, 4, Article 20 (December\n  2011)", "doi": "10.1145/2043643.2043650", "report-no": "LPTENS 11/32", "categories": "cs.DC gr-qc", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time plays a crucial role in the performance of computing systems. The\naccurate modelling of logical devices, and of their physical implementations,\nrequires an appropriate representation of time and of all properties that\ndepend on this notion. The need for a proper model, particularly acute in the\ndesign of clockless delay-insensitive (DI) circuits, leads one to reconsider\nthe classical descriptions of time and of the resulting order and causal\nrelations satisfied by logical operations. This questioning meets the\ncriticisms of classical spacetime formulated by Einstein when founding\nrelativity theory and is answered by relativistic conceptions of time and\ncausality. Applying this approach to clockless circuits and considering the\ntrace formalism, we rewrite Udding's rules which characterize communications\nbetween DI components. We exhibit their intrinsic relation with relativistic\ncausality. For that purpose, we introduce relativistic generalizations of\ntraces, called R-traces, which provide a pertinent description of\ncommunications and compositions of DI components.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2011 22:58:53 GMT"}], "update_date": "2011-12-23", "authors_parsed": [["Matherat", "Philippe", ""], ["Jaekel", "Marc-Thierry", ""]]}, {"id": "1112.5505", "submitter": "Nikzad Babaii-Rizvandi", "authors": "Nikzad Babaii Rizvandi, Javid Taheri, Albert Y. Zomaya, Reza Moraveji", "title": "A Study on Using Uncertain Time Series Matching Algorithms in MapReduce\n  Applications", "comments": "12 pages a version has been accepted to journal of \"Concurrency and\n  Computation: Practice and Experience\", available online from the University\n  of Sydney at http://www.nicta.com.au/pub?doc=4744", "journal-ref": null, "doi": null, "report-no": "TR672- University of Sydney", "categories": "cs.DC cs.AI cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study CPU utilization time patterns of several Map-Reduce\napplications. After extracting running patterns of several applications, the\npatterns with their statistical information are saved in a reference database\nto be later used to tweak system parameters to efficiently execute unknown\napplications in future. To achieve this goal, CPU utilization patterns of new\napplications along with its statistical information are compared with the\nalready known ones in the reference database to find/predict their most\nprobable execution patterns. Because of different patterns lengths, the Dynamic\nTime Warping (DTW) is utilized for such comparison; a statistical analysis is\nthen applied to DTWs' outcomes to select the most suitable candidates.\nMoreover, under a hypothesis, another algorithm is proposed to classify\napplications under similar CPU utilization patterns. Three widely used text\nprocessing applications (WordCount, Distributed Grep, and Terasort) and another\napplication (Exim Mainlog parsing) are used to evaluate our hypothesis in\ntweaking system parameters in executing similar applications. Results were very\npromising and showed effectiveness of our approach on 5-node Map-Reduce\nplatform\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2011 02:38:42 GMT"}, {"version": "v2", "created": "Mon, 16 Jan 2012 00:30:11 GMT"}, {"version": "v3", "created": "Fri, 20 Jan 2012 00:28:52 GMT"}, {"version": "v4", "created": "Wed, 13 Jun 2012 03:33:54 GMT"}, {"version": "v5", "created": "Fri, 18 Jan 2013 03:54:34 GMT"}], "update_date": "2013-01-30", "authors_parsed": [["Rizvandi", "Nikzad Babaii", ""], ["Taheri", "Javid", ""], ["Zomaya", "Albert Y.", ""], ["Moraveji", "Reza", ""]]}, {"id": "1112.5588", "submitter": "Georg Hager", "authors": "Moritz Kreutzer, Georg Hager, Gerhard Wellein, Holger Fehske, Achim\n  Basermann, Alan R. Bishop", "title": "Sparse matrix-vector multiplication on GPGPU clusters: A new storage\n  format and a scalable implementation", "comments": "10 pages, 5 figures. Added reference to other recent sparse matrix\n  formats", "journal-ref": null, "doi": "10.1109/IPDPSW.2012.211", "report-no": null, "categories": "cs.DC cs.MS cs.NA cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse matrix-vector multiplication (spMVM) is the dominant operation in many\nsparse solvers. We investigate performance properties of spMVM with matrices of\nvarious sparsity patterns on the nVidia \"Fermi\" class of GPGPUs. A new \"padded\njagged diagonals storage\" (pJDS) format is proposed which may substantially\nreduce the memory overhead intrinsic to the widespread ELLPACK-R scheme. In our\ntest scenarios the pJDS format cuts the overall spMVM memory footprint on the\nGPGPU by up to 70%, and achieves 95% to 130% of the ELLPACK-R performance.\nUsing a suitable performance model we identify performance bottlenecks on the\nnode level that invalidate some types of matrix structures for efficient\nmulti-GPGPU parallelization. For appropriate sparsity patterns we extend\nprevious work on distributed-memory parallel spMVM to demonstrate a scalable\nhybrid MPI-GPGPU code, achieving efficient overlap of communication and\ncomputation.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2011 14:03:56 GMT"}, {"version": "v2", "created": "Wed, 29 Feb 2012 07:40:22 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Kreutzer", "Moritz", ""], ["Hager", "Georg", ""], ["Wellein", "Gerhard", ""], ["Fehske", "Holger", ""], ["Basermann", "Achim", ""], ["Bishop", "Alan R.", ""]]}, {"id": "1112.5917", "submitter": "Julia Myint Ms.", "authors": "Julia Myint and Thinn Thu Naing", "title": "Management of Data Replication for PC Cluster-based Cloud Storage System", "comments": null, "journal-ref": "International Journal on Cloud Computing: Services and\n  Architecture (IJCCSA), Vol.1,No.3, November 2011, 31-41", "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Storage systems are essential building blocks for cloud computing\ninfrastructures. Although high performance storage servers are the ultimate\nsolution for cloud storage, the implementation of inexpensive storage system\nremains an open issue. To address this problem, the efficient cloud storage\nsystem is implemented with inexpensive and commodity computer nodes that are\norganized into PC cluster based datacenter. Hadoop Distributed File System\n(HDFS) is an open source cloud based storage platform and designed to be\ndeployed in low-cost hardware. PC Cluster based Cloud Storage System is\nimplemented with HDFS by enhancing replication management scheme. Data objects\nare distributed and replicated in a cluster of commodity nodes located in the\ncloud. This system provides optimum replica number as well as weighting and\nbalancing among the storage server nodes. The experimental results show that\nstorage can be balanced depending on the available disk space, expected\navailability and failure probability of each node in PC cluster.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2011 03:58:36 GMT"}], "update_date": "2011-12-30", "authors_parsed": [["Myint", "Julia", ""], ["Naing", "Thinn Thu", ""]]}, {"id": "1112.6128", "submitter": "Stefano Gallozzi", "authors": "Stefano Gallozzi", "title": "Holographic Grid Cloud, a futurable high storage technology for the next\n  generation astronomical facilities", "comments": "15 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the immediate future holographic technology will be available to store a\nvery large amount of data in HVD (Holographic Versatile Disk) devices. This\ntechnology make extensive use of the WORM (Write-Once-Read-Many) paradigm: this\nmeans that such devices allow for a simultaneous and parallel reading of\nmillions of volumetric pixels (i.e. voxels). This characteristic will make\naccessible wherever the acquired data from a telescope (or satellite) in a\nquite-simultaneous way.\n  With the support of this new technology the aim of this paper is to identify\nthe guidelines for the implementation of a distributed RAID system, a sort of\n\"storage block\" to distribute astronomical data over different geographical\nsites acting as a single remote device as an effect of a property of\ndistributed computing, the abstraction of resources. The end user will only\nhave to take care on connecting in a opportune and secure mode (using personal\ncertificates) to the remote device and will have access to all (or part) of\nthis potential technology.\n  A Storage-Block+Services engineered on such a platform will allow rapid\nscalability of resources, creating a \"network-distributed cloud\" of services\nfor an instrument or a mission. It is recommended the use of a dedicated\ngrid-infrastructure within each single cloud to enhance some critical tasks and\nto speed-up services working on the redundant, encrypted and compressed\nscientific data. The power, the accessibility, the degree of parallelism and of\nredundancy will only depend on the number of distributed storage-blocks: the\nhigher this amount, the greater will be throughput of the IT-system. A\nstorage-block of this kind is a meeting point between two technologies and two\nantithetical computing paradigms: the Grid-Computing and Cloud-Computing.\n", "versions": [{"version": "v1", "created": "Wed, 28 Dec 2011 14:37:36 GMT"}, {"version": "v2", "created": "Mon, 9 Jan 2012 16:34:30 GMT"}], "update_date": "2012-01-10", "authors_parsed": [["Gallozzi", "Stefano", ""]]}, {"id": "1112.6254", "submitter": "Soumitra Pal", "authors": "Soumitra Pal and Abhiram Ranade", "title": "Scheduling Light-trails in WDM Rings", "comments": "19 pages, 4 figures, Submitted to Journal of Parallel and Distributed\n  Computing (JPDC) on June 22, 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of scheduling communication on optical WDM\n(wavelength division multiplexing) networks using the light-trails technology.\nWe seek to design scheduling algorithms such that the given transmission\nrequests can be scheduled using minimum number of wavelengths (optical\nchannels). We provide algorithms and close lower bounds for two versions of the\nproblem on an $n$ processor linear array/ring network. In the {\\em stationary}\nversion, the pattern of transmissions (given) is assumed to not change over\ntime. For this, a simple lower bound is $c$, the congestion or the maximum\ntotal traffic required to pass through any link. We give an algorithm that\nschedules the transmissions using $O(c+\\log{n})$ wavelengths. We also show a\npattern for which $\\Omega(c+\\log{n}/\\log\\log{n})$ wavelengths are needed. In\nthe {\\em on-line} version, the transmissions arrive and depart dynamically, and\nmust be scheduled without upsetting the previously scheduled transmissions. For\nthis case we give an on-line algorithm which has competitive ratio\n$\\Theta(\\log{n})$. We show that this is optimal in the sense that every on-line\nalgorithm must have competitive ratio $\\Omega(\\log{n})$. We also give an\nalgorithm that appears to do well in simulation (for the classes of traffic we\nconsider), but which has competitive ratio between $\\Omega(\\log^2n/\\log\n\\log{n})$ and $O(\\log^2n)$. We present detailed simulations of both our\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2011 08:56:42 GMT"}], "update_date": "2011-12-30", "authors_parsed": [["Pal", "Soumitra", ""], ["Ranade", "Abhiram", ""]]}]