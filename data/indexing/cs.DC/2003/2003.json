[{"id": "2003.00010", "submitter": "S\\'ebastien Rouault", "authors": "El-Mahdi El-Mhamdi, Rachid Guerraoui, S\\'ebastien Rouault", "title": "Distributed Momentum for Byzantine-resilient Learning", "comments": "Source code (for academic use only):\n  https://github.com/LPD-EPFL/ByzantineMomentum", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Momentum is a variant of gradient descent that has been proposed for its\nbenefits on convergence. In a distributed setting, momentum can be implemented\neither at the server or the worker side. When the aggregation rule used by the\nserver is linear, commutativity with addition makes both deployments\nequivalent. Robustness and privacy are however among motivations to abandon\nlinear aggregation rules. In this work, we demonstrate the benefits on\nrobustness of using momentum at the worker side. We first prove that computing\nmomentum at the workers reduces the variance-norm ratio of the gradient\nestimation at the server, strengthening Byzantine resilient aggregation rules.\nWe then provide an extensive experimental demonstration of the robustness\neffect of worker-side momentum on distributed SGD.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 16:57:27 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 09:24:24 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["El-Mhamdi", "El-Mahdi", ""], ["Guerraoui", "Rachid", ""], ["Rouault", "S\u00e9bastien", ""]]}, {"id": "2003.00103", "submitter": "Nikos Batsaras", "authors": "Nikos Batsaras, Giorgos Saloustros, Anastasios Papagiannis, Panagiota\n  Fatourou, and Angelos Bilas", "title": "VAT: Asymptotic Cost Analysis for Multi-Level Key-Value Stores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past years, there has been an increasing number of key-value (KV)\nstore designs, each optimizing for a different set of requirements.\nFurthermore, with the advancements of storage technology the design space of KV\nstores has become even more complex. More recent KV-store designs target fast\nstorage devices, such as SSDs and NVM. Most of these designs aim to reduce\namplification during data reorganization by taking advantage of device\ncharacteristics. However, until today most analysis of KV-store designs is\nexperimental and limited to specific design points. This makes it difficult to\ncompare tradeoffs across different designs, find optimal configurations and\nguide future KV-store design. In this paper, we introduce the Variable\nAmplification- Throughput analysis (VAT) to calculate insert-path amplification\nand its impact on multi-level KV-store performance.We use VAT to express the\nbehavior of several existing design points and to explore tradeoffs that are\nnot possible or easy to measure experimentally. VAT indicates that by inserting\nrandomness in the insert-path, KV stores can reduce amplification by more than\n10x for fast storage devices. Techniques, such as key-value separation and\ntiering compaction, reduce amplification by 10x and 5x, respectively.\nAdditionally, VAT predicts that the advancements in device technology towards\nNVM, reduces the benefits from both using key-value separation and tiering.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 22:54:18 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Batsaras", "Nikos", ""], ["Saloustros", "Giorgos", ""], ["Papagiannis", "Anastasios", ""], ["Fatourou", "Panagiota", ""], ["Bilas", "Angelos", ""]]}, {"id": "2003.00138", "submitter": "Luting Yang", "authors": "Luting Yang, Bingqian Lu and Shaolei Ren", "title": "A Note on Latency Variability of Deep Neural Networks for Mobile\n  Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Running deep neural network (DNN) inference on mobile devices, i.e., mobile\ninference, has become a growing trend, making inference less dependent on\nnetwork connections and keeping private data locally. The prior studies on\noptimizing DNNs for mobile inference typically focus on the metric of average\ninference latency, thus implicitly assuming that mobile inference exhibits\nlittle latency variability. In this note, we conduct a preliminary measurement\nstudy on the latency variability of DNNs for mobile inference. We show that the\ninference latency variability can become quite significant in the presence of\nCPU resource contention. More interestingly, unlike the common belief that the\nrelative performance superiority of DNNs on one device can carry over to\nanother device and/or another level of resource contention, we highlight that a\nDNN model with a better latency performance than another model can become\noutperformed by the other model when resource contention be more severe or\nrunning on another device. Thus, when optimizing DNN models for mobile\ninference, only measuring the average latency may not be adequate; instead,\nlatency variability under various conditions should be accounted for, including\nbut not limited to different devices and different levels of CPU resource\ncontention considered in this note.\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 00:30:52 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Yang", "Luting", ""], ["Lu", "Bingqian", ""], ["Ren", "Shaolei", ""]]}, {"id": "2003.00239", "submitter": "Cao Vien Phung", "authors": "Cao Vien Phung, Anna Engelmann, Thomas Kuerner and Admela Jukan", "title": "Improving THz Quality-of-Transmission with Systematic RLNC and Auxiliary\n  Channels", "comments": "7 pages, 6 figures, accepted at IEEE ICC'20 Workshop - TeraCom", "journal-ref": null, "doi": "10.1109/ICCWorkshops49005.2020.9145148", "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel solution that can improve the quality of\nTHz transmission with systematic random linear network coding (sRLNC) and a\nlow-bitrate auxiliary channel. To minimize complexity of channel coding, we\ncomplement a generic low complexity FEC code by a low complexity sRLNC. To\nincrease the overall throughput of THz transmission, we propose to send the\nnative data and coding redundancy in parallel over 2 differently configured THz\nchannels, i.e., over 1 high bit rate main channel and 1 low bit rate low error\nrate auxiliary channel. The results show, that the main THz channel supported\nby low bit rate auxiliary channel can use a higher level modulation format and\nsent over longer distances with a higher throughput.\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 11:34:36 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 16:13:10 GMT"}, {"version": "v3", "created": "Sat, 5 Jun 2021 08:22:39 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Phung", "Cao Vien", ""], ["Engelmann", "Anna", ""], ["Kuerner", "Thomas", ""], ["Jukan", "Admela", ""]]}, {"id": "2003.00290", "submitter": "Gus Henry Smith", "authors": "Gus Smith, Zachary Tatlock and Luis Ceze (University of Washington)", "title": "Enumerating Hardware-Software Splits with Program Rewriting", "comments": "Accepted in the Second Young Architect Workshop, in conjunction with\n  ASPLOS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A core problem in hardware-software codesign is in the sheer size of the\ndesign space. Without a set ISA to constrain the hardware-software interface,\nthe design space explodes. This work presents a strategy for managing the\nmassive hardware-software design space within the domain of machine learning\ninference workloads and accelerators. We first propose EngineIR, a new language\nfor representing machine learning hardware and software in a single program.\nThen, using equality graphs -- a data structure from the compilers literature\n-- we suggest a method for efficiently enumerating the design space by\nperforming rewrites over our representation.\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 16:15:51 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Smith", "Gus", "", "University of Washington"], ["Tatlock", "Zachary", "", "University of Washington"], ["Ceze", "Luis", "", "University of Washington"]]}, {"id": "2003.00294", "submitter": "Suat Mercan", "authors": "Suat Mercan, Enes Erdin, and Kemal Akkaya", "title": "Improving Sustainability of Cryptocurrency Payment Networks for IoT\n  Applications", "comments": null, "journal-ref": null, "doi": "10.1109/ICCWorkshops49005.2020.9145389", "report-no": null, "categories": "cs.CR cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain-based cryptocurrencies received a lot of attention recently for\ntheir applications in many domains. IoT domain is one of such applications,\nwhich can utilize cryptocur-rencies for micro payments without compromising\ntheir payment privacy. However, long confirmation times of transactions and\nrelatively high fees hinder the adoption of cryptoccurency based\nmicro-payments. The payment channel networks is one of the proposed solutions\nto address these issue where nodes establish payment channels among themselves\nwithout writing on blockchain. IoT devices can benefit from such payment\nnetworks as long as they are capable of sustaining their overhead. Payment\nchannel networks pose unique characteristics as far as the routing problem is\nconcerned. Specifically, they should stay balanced to have a sustainable\nnetwork for maintaining payments for longer times, which is crucial for IoT\ndevices once they are deployed.In this paper, we present a payment channel\nnetwork design that aims to keep the channels balanced by using a common weight\npolicy across the network. We additionally propose using multi-point\nconnections to nodes for each IoT device for unbalanced payment scenarios. The\nexperiment results show that we can keep the channels in the network more\nequally balanced compared to the minimal fee approach. In addition, multiple\nconnections from IoT devices to nodes increase the success ratio significantly.\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 16:33:25 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Mercan", "Suat", ""], ["Erdin", "Enes", ""], ["Akkaya", "Kemal", ""]]}, {"id": "2003.00295", "submitter": "Zachary Charles", "authors": "Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith\n  Rush, Jakub Kone\\v{c}n\\'y, Sanjiv Kumar, H. Brendan McMahan", "title": "Adaptive Federated Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning is a distributed machine learning paradigm in which a\nlarge number of clients coordinate with a central server to learn a model\nwithout sharing their own training data. Standard federated optimization\nmethods such as Federated Averaging (FedAvg) are often difficult to tune and\nexhibit unfavorable convergence behavior. In non-federated settings, adaptive\noptimization methods have had notable success in combating such issues. In this\nwork, we propose federated versions of adaptive optimizers, including Adagrad,\nAdam, and Yogi, and analyze their convergence in the presence of heterogeneous\ndata for general non-convex settings. Our results highlight the interplay\nbetween client heterogeneity and communication efficiency. We also perform\nextensive experiments on these methods and show that the use of adaptive\noptimizers can significantly improve the performance of federated learning.\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 16:37:29 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2020 23:31:57 GMT"}, {"version": "v3", "created": "Wed, 2 Dec 2020 17:37:11 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Reddi", "Sashank", ""], ["Charles", "Zachary", ""], ["Zaheer", "Manzil", ""], ["Garrett", "Zachary", ""], ["Rush", "Keith", ""], ["Kone\u010dn\u00fd", "Jakub", ""], ["Kumar", "Sanjiv", ""], ["McMahan", "H. Brendan", ""]]}, {"id": "2003.00331", "submitter": "Michael Whittaker", "authors": "Michael Whittaker, Neil Giridharan, Adriana Szekeres, Joseph M.\n  Hellerstein, Ion Stoica", "title": "Bipartisan Paxos: A Modular State Machine Replication Protocol", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is no shortage of state machine replication protocols. From Generalized\nPaxos to EPaxos, a huge number of replication protocols have been proposed that\nachieve high throughput and low latency. However, these protocols all have two\nproblems. First, they do not scale. Many protocols actually slow down when you\nscale them, instead of speeding up. For example, increasing the number of\nMultiPaxos acceptors increases quorum sizes and slows down the protocol.\nSecond, they are too complicated. This is not a secret; state machine\nreplication is notoriously difficult to understand.\n  In this paper, we tackle both problems with a single solution: modularity. We\npresent Bipartisan Paxos (BPaxos), a modular state machine replication\nprotocol. Modularity yields high throughput via scaling. We note that while\nmany replication protocol components do not scale, some do. By modularizing\nBPaxos, we are able to disentangle the two and scale the bottleneck components\nto increase the protocol's throughput. Modularity also yields simplicity.\nBPaxos is divided into a number of independent modules that can be understood\nand proven correct in isolation.\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 19:37:44 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Whittaker", "Michael", ""], ["Giridharan", "Neil", ""], ["Szekeres", "Adriana", ""], ["Hellerstein", "Joseph M.", ""], ["Stoica", "Ion", ""]]}, {"id": "2003.00395", "submitter": "Ranesh Kumar Naha", "authors": "Abdullah Al-Noman Patwary, Anmin Fu, Ranesh Kumar Naha, Sudheer Kumar\n  Battula, Saurabh Garg, Md Anwarul Kaium Patwary, and Erfan Aghasian", "title": "Authentication, Access Control, Privacy, Threats and Trust Management\n  Towards Securing Fog Computing Environments: A Review", "comments": "34 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fog computing is an emerging computing paradigm that has come into\nconsideration for the deployment of IoT applications amongst researchers and\ntechnology industries over the last few years. Fog is highly distributed and\nconsists of a wide number of autonomous end devices, which contribute to the\nprocessing. However, the variety of devices offered across different users are\nnot audited. Hence, the security of Fog devices is a major concern in the Fog\ncomputing environment. Furthermore, mitigating and preventing those security\nmeasures is a research issue. Therefore, to provide the necessary security for\nFog devices, we need to understand what the security concerns are with regards\nto Fog. All aspects of Fog security, which have not been covered by other\nliterature works needs to be identified and need to be aggregate all issues in\nFog security. It needs to be noted that computation devices consist of many\nordinary users, and are not managed by any central entity or managing body.\nTherefore, trust and privacy is also a key challenge to gain market adoption\nfor Fog. To provide the required trust and privacy, we need to also focus on\nauthentication, threats and access control mechanisms as well as techniques in\nFog computing. In this paper, we perform a survey and propose a taxonomy, which\npresents an overview of existing security concerns in the context of the Fog\ncomputing paradigm. We discuss the Blockchain-based solutions towards a secure\nFog computing environment and presented various research challenges and\ndirections for future research.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 04:08:18 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Patwary", "Abdullah Al-Noman", ""], ["Fu", "Anmin", ""], ["Naha", "Ranesh Kumar", ""], ["Battula", "Sudheer Kumar", ""], ["Garg", "Saurabh", ""], ["Patwary", "Md Anwarul Kaium", ""], ["Aghasian", "Erfan", ""]]}, {"id": "2003.00465", "submitter": "Hamza Ali Imran", "authors": "Hamza Ali Imran and Saad Wazir and Ahmed Jamal Ikram and Ataul Aziz\n  Ikram and Hanif Ullah and Maryam Ehsan", "title": "HPC as a Service: A naive model", "comments": "2019 8th International Conference on Information and Communication\n  Technologies (ICICT), Karachi, Pakistan, 2019", "journal-ref": null, "doi": "10.1109/ICICT47744.2019.9001912", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications like Big Data, Machine Learning, Deep Learning and even other\nEngineering and Scientific research requires a lot of computing power; making\nHigh-Performance Computing (HPC) an important field. But access to\nSupercomputers is out of range from the majority. Nowadays Supercomputers are\nactually clusters of computers usually made-up of commodity hardware. Such\nclusters are called Beowulf Clusters. The history of which goes back to 1994\nwhen NASA built a Supercomputer by creating a cluster of commodity hardware. In\nrecent times a lot of effort has been done in making HPC Clusters of even\nsingle board computers (SBCs). Although the creation of clusters of commodity\nhardware is possible but is a cumbersome task. Moreover, the maintenance of\nsuch systems is also difficult and requires special expertise and time. The\nconcept of cloud is to provide on-demand resources that can be services,\nplatform or even infrastructure and this is done by sharing a big resource\npool. Cloud computing has resolved problems like maintenance of hardware and\nrequirement of having expertise in networking etc. An effort is made of\nbringing concepts from cloud computing to HPC in order to get benefits of\ncloud. The main target is to create a system which can develop a capability of\nproviding computing power as a service which to further be referred to as\nSupercomputer as a service. A prototype was made using Raspberry Pi (RPi) 3B\nand 3B+ Single Board Computers. The reason for using RPi boards was increasing\npopularity of ARM processors in the field of HPC\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 11:54:31 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Imran", "Hamza Ali", ""], ["Wazir", "Saad", ""], ["Ikram", "Ahmed Jamal", ""], ["Ikram", "Ataul Aziz", ""], ["Ullah", "Hanif", ""], ["Ehsan", "Maryam", ""]]}, {"id": "2003.00468", "submitter": "Moti Medina", "authors": "Reut Levi and Moti Medina", "title": "Distributed Testing of Graph Isomorphism in the CONGEST model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the problem of testing graph isomorphism (GI) in the\nCONGEST distributed model. In this setting we test whether the distributive\nnetwork, $G_U$, is isomorphic to $G_K$ which is given as an input to all the\nnodes in the network, or alternatively, only to a single node.\n  We first consider the decision variant of the problem in which the algorithm\ndistinguishes $G_U$ and $G_K$ which are isomorphic from $G_U$ and $G_K$ which\nare not isomorphic. We provide a randomized algorithm with $O(n)$ rounds for\nthe setting in which $G_K$ is given only to a single node. We prove that for\nthis setting the number of rounds of any deterministic algorithm is\n$\\tilde{\\Omega}(n^2)$ rounds, where $n$ denotes the number of nodes, which\nimplies a separation between the randomized and the deterministic complexities\nof deciding GI.\n  We then consider the \\emph{property testing} variant of the problem, where\nthe algorithm is only required to distinguish the case that $G_U$ and $G_K$ are\nisomorphic from the case that $G_U$ and $G_K$ are \\emph{far} from being\nisomorphic (according to some predetermined distance measure). We show that\nevery algorithm requires $\\Omega(D)$ rounds, where $D$ denotes the diameter of\nthe network. This lower bound holds even if all the nodes are given $G_K$ as an\ninput, and even if the message size is unbounded. We provide a randomized\nalgorithm with an almost matching round complexity of $O(D+(\\epsilon^{-1}\\log\nn)^2)$ rounds that is suitable for dense graphs.\n  We also show that with the same number of rounds it is possible that each\nnode outputs its mapping according to a bijection which is an\n\\emph{approximated} isomorphism.\n  We conclude with simple simulation arguments that allow us to obtain\nessentially tight algorithms with round complexity $\\tilde{O}(D)$ for special\nfamilies of sparse graphs.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 12:03:17 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Levi", "Reut", ""], ["Medina", "Moti", ""]]}, {"id": "2003.00614", "submitter": "Sixue Liu", "authors": "S. Cliff Liu, Robert E. Tarjan, Peilin Zhong", "title": "Connected Components on a PRAM in Log Diameter Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an $O(\\log d + \\log\\log_{m/n} n)$-time randomized PRAM algorithm\nfor computing the connected components of an $n$-vertex, $m$-edge undirected\ngraph with maximum component diameter $d$. The algorithm runs on an ARBITRARY\nCRCW (concurrent-read, concurrent-write with arbitrary write resolution) PRAM\nusing $O(m)$ processors. The time bound holds with good probability.\n  Our algorithm is based on the breakthrough results of Andoni et al. [FOCS'18]\nand Behnezhad et al. [FOCS'19]. Their algorithms run on the more powerful MPC\nmodel and rely on sorting and computing prefix sums in $O(1)$ time, tasks that\ntake $\\Omega(\\log n / \\log\\log n)$ time on a CRCW PRAM with $\\text{poly}(n)$\nprocessors. Our simpler algorithm uses limited-collision hashing and does not\nsort or do prefix sums. It matches the time and space bounds of the algorithm\nof Behnezhad et al., who improved the time bound of Andoni et al.\n  It is widely believed that the larger private memory per processor and\nunbounded local computation of the MPC model admit algorithms faster than that\non a PRAM. Our result suggests that such additional power might not be\nnecessary, at least for fundamental graph problems like connected components\nand spanning forest.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 00:00:11 GMT"}, {"version": "v2", "created": "Wed, 20 May 2020 22:56:31 GMT"}, {"version": "v3", "created": "Wed, 21 Apr 2021 13:03:59 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Liu", "S. Cliff", ""], ["Tarjan", "Robert E.", ""], ["Zhong", "Peilin", ""]]}, {"id": "2003.00671", "submitter": "Qijing Huang", "authors": "Qijing Huang, Ameer Haj-Ali, William Moses, John Xiang, Ion Stoica,\n  Krste Asanovic, John Wawrzynek", "title": "AutoPhase: Juggling HLS Phase Orderings in Random Forests with Deep\n  Reinforcement Learning", "comments": "arXiv admin note: text overlap with arXiv:1901.04615", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of the code a compiler generates depends on the order in\nwhich it applies the optimization passes. Choosing a good order--often referred\nto as the phase-ordering problem, is an NP-hard problem. As a result, existing\nsolutions rely on a variety of heuristics. In this paper, we evaluate a new\ntechnique to address the phase-ordering problem: deep reinforcement learning.\nTo this end, we implement AutoPhase: a framework that takes a program and uses\ndeep reinforcement learning to find a sequence of compilation passes that\nminimizes its execution time. Without loss of generality, we construct this\nframework in the context of the LLVM compiler toolchain and target high-level\nsynthesis programs. We use random forests to quantify the correlation between\nthe effectiveness of a given pass and the program's features. This helps us\nreduce the search space by avoiding phase orderings that are unlikely to\nimprove the performance of a given program. We compare the performance of\nAutoPhase to state-of-the-art algorithms that address the phase-ordering\nproblem. In our evaluation, we show that AutoPhase improves circuit performance\nby 28% when compared to using the -O3 compiler flag, and achieves competitive\nresults compared to the state-of-the-art solutions, while requiring fewer\nsamples. Furthermore, unlike existing state-of-the-art solutions, our deep\nreinforcement learning solution shows promising result in generalizing to real\nbenchmarks and 12,874 different randomly generated programs, after training on\na hundred randomly generated programs.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 05:35:32 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 19:48:50 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Huang", "Qijing", ""], ["Haj-Ali", "Ameer", ""], ["Moses", "William", ""], ["Xiang", "John", ""], ["Stoica", "Ion", ""], ["Asanovic", "Krste", ""], ["Wawrzynek", "John", ""]]}, {"id": "2003.00680", "submitter": "Xubo Wang", "authors": "Xubo Wang, Lu Qin, Lijun Chang, Ying Zhang, Dong Wen, Xuemin Lin", "title": "Graph3S: A Simple, Speedy and Scalable Distributed Graph Processing\n  System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph is a ubiquitous structure in many domains. The rapidly increasing data\nvolume calls for efficient and scalable graph data processing. In recent years,\ndesigning distributed graph processing systems has been an increasingly\nimportant area to fulfil the demands of processing big graphs in a distributed\nenvironment. Though a variety of distributed graph processing systems have been\ndeveloped, very little attention has been paid to achieving a good\ncombinational system performance in terms of usage simplicity, efficiency and\nscalability. To contribute to the study of distributed graph processing system,\nthis work tries to fill this gap by designing a simple, speedy and scalable\nsystem. Our observation is that enforcing the communication flexibility of a\nsystem leads to the gains of both system efficiency and scalability as well as\nsimple usage. We realize our idea in a system Graph3S and conduct extensive\nexperiments with diverse algorithms over big graphs from different domains to\ntest its performance. The results show that, besides simple usage, our system\nhas outstanding performance over various graph algorithms and can even reach up\nto two orders of magnitude speedup over existing in-memory systems when\napplying to some algorithms. Also, its scalability is competitive to disk-based\nsystems and even better when less machines are used.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 05:51:38 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Wang", "Xubo", ""], ["Qin", "Lu", ""], ["Chang", "Lijun", ""], ["Zhang", "Ying", ""], ["Wen", "Dong", ""], ["Lin", "Xuemin", ""]]}, {"id": "2003.00925", "submitter": "Andreas Fischbach", "authors": "Andreas Fischbach, Jan Strohschein, Andreas Bunte, J\\\"org Stork, Heide\n  Faeskorn-Woyke, Natalia Moriz, Thomas Bartz-Beielstein", "title": "CAAI -- A Cognitive Architecture to Introduce Artificial Intelligence in\n  Cyber-Physical Production Systems", "comments": null, "journal-ref": null, "doi": "10.1007/s00170-020-06094-z", "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces CAAI, a novel cognitive architecture for artificial\nintelligence in cyber-physical production systems. The goal of the architecture\nis to reduce the implementation effort for the usage of artificial intelligence\nalgorithms. The core of the CAAI is a cognitive module that processes\ndeclarative goals of the user, selects suitable models and algorithms, and\ncreates a configuration for the execution of a processing pipeline on a big\ndata platform. Constant observation and evaluation against performance criteria\nassess the performance of pipelines for many and varying use cases. Based on\nthese evaluations, the pipelines are automatically adapted if necessary. The\nmodular design with well-defined interfaces enables the reusability and\nextensibility of pipeline components. A big data platform implements this\nmodular design supported by technologies such as Docker, Kubernetes, and Kafka\nfor virtualization and orchestration of the individual components and their\ncommunication. The implementation of the architecture is evaluated using a\nreal-world use case.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 16:27:07 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Fischbach", "Andreas", ""], ["Strohschein", "Jan", ""], ["Bunte", "Andreas", ""], ["Stork", "J\u00f6rg", ""], ["Faeskorn-Woyke", "Heide", ""], ["Moriz", "Natalia", ""], ["Bartz-Beielstein", "Thomas", ""]]}, {"id": "2003.00926", "submitter": "Sajjad Arshad", "authors": "Hossein Rahmani, Sajjad Arshad, Mohsen Ebrahimi Moghaddam", "title": "A Disk Scheduling Algorithm Based on ANT Colony Optimization", "comments": "ISCA Conference on Parallel and Distributed Computing and\n  Communication Systems (PDCCS) Louisville, KY, USA, September 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audio, animations and video belong to a class of data known as delay\nsensitive because they are sensitive to delays in presentation to the users.\nAlso, because of huge data in such items, disk is an important device in\nmanaging them. In order to have an acceptable presentation, disk requests\ndeadlines must be met, and a real-time scheduling approach should be used to\nguarantee the timing requirements for such environment. However, some disk\nscheduling algorithms have been proposed since now to optimize scheduling\nreal-time disk requests, but improving the results is a challenge yet. In this\npaper, we propose a new disk scheduling method based on Ant Colony Optimization\n(ACO) approach. In this approach, ACO models the tasks and finds the best\nsequence to minimize number of missed tasks and maximize throughput.\nExperimental results showed that the proposed method worked very well and\nexcelled other related ones in terms of miss ratio and throughput in most\ncases.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 22:21:03 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Rahmani", "Hossein", ""], ["Arshad", "Sajjad", ""], ["Moghaddam", "Mohsen Ebrahimi", ""]]}, {"id": "2003.00986", "submitter": "Sarod Yatawatta", "authors": "Sarod Yatawatta", "title": "Stochastic Calibration of Radio Interferometers", "comments": "MNRAS Accepted 2020 March 2. Received 2020 March 2 ; in original form\n  2020 January 27", "journal-ref": null, "doi": "10.1093/mnras/staa648", "report-no": null, "categories": "astro-ph.IM cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With ever increasing data rates produced by modern radio telescopes like\nLOFAR and future telescopes like the SKA, many data processing steps are\noverwhelmed by the amount of data that needs to be handled using limited\ncompute resources. Calibration is one such operation that dominates the overall\ndata processing computational cost, nonetheless, it is an essential operation\nto reach many science goals. Calibration algorithms do exist that scale well\nwith the number of stations of an array and the number of directions being\ncalibrated. However, the remaining bottleneck is the raw data volume, which\nscales with the number of baselines, and which is proportional to the square of\nthe number of stations. We propose a 'stochastic' calibration strategy where we\nonly read in a mini-batch of data for obtaining calibration solutions, as\nopposed to reading the full batch of data being calibrated. Nonetheless, we\nobtain solutions that are valid for the full batch of data. Normally, data need\nto be averaged before calibration is performed to accommodate the data in\nsize-limited compute memory. Stochastic calibration overcomes the need for data\naveraging before any calibration can be performed, and offers many advantages\nincluding: enabling the mitigation of faint radio frequency interference;\nbetter removal of strong celestial sources from the data; and better detection\nand spatial localization of fast radio transients.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 16:04:38 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 10:42:20 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Yatawatta", "Sarod", ""]]}, {"id": "2003.01081", "submitter": "Camille Coti", "authors": "Joseph Ben Geloun, Camille Coti, Allen D. Malony", "title": "On-the-fly Optimization of Parallel Computation of Symbolic Symplectic\n  Invariants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Group invariants are used in high energy physics to define quantum field\ntheory interactions. In this paper, we are presenting the parallel algebraic\ncomputation of special invariants called symplectic and even focusing on one\nparticular invariant that finds recent interest in physics. Our results will\nexport to other invariants. The cost of performing basic computations on the\nmultivariate polynomials involved evolves during the computation, as the\npolynomials get larger or with an increasing number of terms. However, in some\ncases, they stay small. Traditionally, high-performance software is optimized\nby running it on a smaller data set in order to use profiling information to\nset some tuning parameters. Since the (communication and computation) costs\nevolve during the computation, the first iterations of the computation might\nnot be representative of the rest of the computation and this approach cannot\nbe applied in this case. To cope with this evolution, we are presenting an\napproach to get performance data and tune the algorithm during the execution.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 18:14:17 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Geloun", "Joseph Ben", ""], ["Coti", "Camille", ""], ["Malony", "Allen D.", ""]]}, {"id": "2003.01107", "submitter": "Prajoy Podder", "authors": "Arnab Paul, Mamdudul Haque Khan, M. Muktadir Rahman, Tanvir Zaman\n  Khan, Prajoy Podder and Md. Yeasir Akram Khan", "title": "Reconfigurable Parallel Architecture of High Speed Round Robin Arbiter", "comments": "Published in 2015 International Conference on Electrical,\n  Electronics, Signals, Communication and Optimization (EESCO)", "journal-ref": "2015 International Conference on Electrical, Electronics, Signals,\n  Communication and Optimization (EESCO)", "doi": "10.1109/EESCO.2015.7253744", "report-no": "15438813", "categories": "cs.DC cs.NI cs.OS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With a view to managing the increasing traffic in computer networks, round\nrobin arbiter has been proposed to work with packet switching system to have\nincreased speed in providing access and scheduling. Round robin arbiter is a\ndoorway to a particular bus based on request along with equal priority and\ngives turns to devices connected to it in a cyclic order. Considering the rapid\ngrowth in computer networking and the emergence of computer automation which\nwill need much more access to the existing limited resources, this paper\nemphasizes on designing a reconfigurable round robin arbiter over FPGA which\ntakes parallel requests and processes them with high efficiency and less delay\nthan existing designs. Proposed round robin arbiter encounters with 4 to 12\ndevices. Results show that with 200% increment in the number of connected\ndevices, only 2.69% increment has been found in the delay. With less delay,\nproposed round robin arbiter exhibits high speed performance with higher\ntraffic, which is a new feature in comparison with the existing designs.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 05:33:23 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Paul", "Arnab", ""], ["Khan", "Mamdudul Haque", ""], ["Rahman", "M. Muktadir", ""], ["Khan", "Tanvir Zaman", ""], ["Podder", "Prajoy", ""], ["Khan", "Md. Yeasir Akram", ""]]}, {"id": "2003.01203", "submitter": "Siddhartha Jayanti", "authors": "Siddhartha V. Jayanti and Robert E. Tarjan", "title": "Concurrent Disjoint Set Union", "comments": "40 pages, combines ideas in two previous PODC papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop and analyze concurrent algorithms for the disjoint set union\n(union-find) problem in the shared memory, asynchronous multiprocessor model of\ncomputation, with CAS (compare and swap) or DCAS (double compare and swap) as\nthe synchronization primitive. We give a deterministic bounded wait-free\nalgorithm that uses DCAS and has a total work bound of $O(m \\cdot (\\log(np/m +\n1) + \\alpha(n, m/(np)))$ for a problem with $n$ elements and $m$ operations\nsolved by $p$ processes, where $\\alpha$ is a functional inverse of Ackermann's\nfunction. We give two randomized algorithms that use only CAS and have the same\nwork bound in expectation. The analysis of the second randomized algorithm is\nvalid even if the scheduler is adversarial. Our DCAS and randomized algorithms\ntake $O(\\log n)$ steps per operation, worst-case for the DCAS algorithm,\nhigh-probability for the randomized algorithms. Our work and step bounds grow\nonly logarithmically with $p$, making our algorithms truly scalable. We prove\nthat for a class of symmetric algorithms that includes ours, no better step or\nwork bound is possible.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 21:43:46 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Jayanti", "Siddhartha V.", ""], ["Tarjan", "Robert E.", ""]]}, {"id": "2003.01216", "submitter": "Thoria Alghamdi", "authors": "Thoria Alghamdi, Gita Alaghband", "title": "High Performance Parallel Sort for Shared and Distributed Memory MIMD", "comments": null, "journal-ref": "16th International Conference on Applied Computing 2019 113-122", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present four high performance hybrid sorting methods developed for various\nparallel platforms: shared memory multiprocessors, distributed multiprocessors,\nand clusters taking advantage of existence of both shared and distributed\nmemory. Merge sort, known for its stability, is used to design several of our\nalgorithms. We improve its parallel performance by combining it with Quicksort.\nWe present two models designed for shared memory MIMD (OpenMP): (a) a\nnon-recursive Merge sort and (b) a hybrid Quicksort and Merge sort. The third\nmodel presented is designed for distributed memory MIMD (MPI) using a hybrid\nQuicksort and Merge sort. Our fourth model is designed to take advantage of the\nshared memory within individual nodes of todays cluster systems, and to\neliminate all internal data transfers between different nodes, Our model\nimplements a one-step MSD-Radix to distribute data in ten packets (MPI) while\nparallel cores of each node use Quicksort to sort their data partitions\nsequentially then merge and sort them in parallel employing the OpenMp. The\nperformances of all developed models outperform the baseline performance.\nHybrid Quicksort and Merge sort outperformed Hybrid Memory Parallel Merge Sort\nusing Hybrid MSD-Radix and Quicksort in Cluster Platforms when sorting small\nsize data, but with larger data the speedup of Hybrid Memory Parallel Merge\nSort Using Hybrid MSD-Radix and Quicksort in Cluster Platforms becomes bigger\nand it keeps improving. The speedup of Distributed Memory Parallel Hybrid\nQuicksort and Merge Sort is the best.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 22:07:17 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Alghamdi", "Thoria", ""], ["Alaghband", "Gita", ""]]}, {"id": "2003.01254", "submitter": "Yasamin Nazari", "authors": "Amartya Shankha Biswas, Michal Dory, Mohsen Ghaffari, Slobodan\n  Mitrovi\\'c, Yasamin Nazari", "title": "Massively Parallel Algorithms for Distance Approximation and Spanners", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over the past decade, there has been increasing interest in\ndistributed/parallel algorithms for processing large-scale graphs. By now, we\nhave quite fast algorithms -- usually sublogarithmic-time and often\n$poly(\\log\\log n)$-time, or even faster -- for a number of fundamental graph\nproblems in the massively parallel computation (MPC) model. This model is a\nwidely-adopted theoretical abstraction of MapReduce style settings, where a\nnumber of machines communicate in an all-to-all manner to process large-scale\ndata. Contributing to this line of work on MPC graph algorithms, we present\n$poly(\\log k) \\in poly(\\log\\log n)$ round MPC algorithms for computing\n$O(k^{1+{o(1)}})$-spanners in the strongly sublinear regime of local memory. To\nthe best of our knowledge, these are the first sublogarithmic-time MPC\nalgorithms for spanner construction. As primary applications of our spanners,\nwe get two important implications, as follows:\n  -For the MPC setting, we get an $O(\\log^2\\log n)$-round algorithm for\n$O(\\log^{1+o(1)} n)$ approximation of all pairs shortest paths (APSP) in the\nnear-linear regime of local memory. To the best of our knowledge, this is the\nfirst sublogarithmic-time MPC algorithm for distance approximations.\n  -Our result above also extends to the Congested Clique model of distributed\ncomputing, with the same round complexity and approximation guarantee. This\ngives the first sub-logarithmic algorithm for approximating APSP in weighted\ngraphs in the Congested Clique model.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 23:52:06 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2020 22:16:35 GMT"}, {"version": "v3", "created": "Sun, 17 May 2020 16:51:13 GMT"}, {"version": "v4", "created": "Tue, 26 Jan 2021 18:13:31 GMT"}, {"version": "v5", "created": "Mon, 1 Feb 2021 01:52:33 GMT"}, {"version": "v6", "created": "Mon, 28 Jun 2021 20:08:50 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Biswas", "Amartya Shankha", ""], ["Dory", "Michal", ""], ["Ghaffari", "Mohsen", ""], ["Mitrovi\u0107", "Slobodan", ""], ["Nazari", "Yasamin", ""]]}, {"id": "2003.01310", "submitter": "Anirban Das", "authors": "Anirban Das, Shigeru Imai, Mike P. Wittie, Stacy Patterson", "title": "Performance Optimization for Edge-Cloud Serverless Platforms via Dynamic\n  Task Placement", "comments": "10 pages, 6 figures, 20th IEEE/ACM International Symposium on\n  Cluster, Cloud and Internet Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for performance optimization in serverless edge-cloud\nplatforms using dynamic task placement. We focus on applications for smart edge\ndevices, for example, smart cameras or speakers, that need to perform\nprocessing tasks on input data in real to near-real time. Our framework allows\nthe user to specify cost and latency requirements for each application task,\nand for each input, it determines whether to execute the task on the edge\ndevice or in the cloud. Further, for cloud executions, the framework identifies\nthe container resource configuration needed to satisfy the performance goals.\nWe have evaluated our framework in simulation using measurements collected from\nserverless applications in AWS Lambda and AWS Greengrass. In addition, we have\nimplemented a prototype of our framework that runs in these same platforms. In\nexperiments with our prototype, our models can predict average end-to-end\nlatency with less than 6% error, and we obtain almost three orders of magnitude\nreduction in end-to-end latency compared to edge-only execution.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 03:18:45 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 20:57:25 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Das", "Anirban", ""], ["Imai", "Shigeru", ""], ["Wittie", "Mike P.", ""], ["Patterson", "Stacy", ""]]}, {"id": "2003.01527", "submitter": "Leyuan Wang", "authors": "Leyuan Wang and John D. Owens", "title": "Fast Gunrock Subgraph Matching (GSM) on GPUs", "comments": "arXiv admin note: text overlap with arXiv:1909.02127", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a GPU-efficient subgraph isomorphism algorithm\nusing the Gunrock graph analytic framework, GSM (Gunrock Subgraph Matching), to\ncompute graph matching on GPUs. In contrast to previous approaches on the CPU\nwhich are based on depth-first traversal, GSM is BFS-based: possible matches\nare explored simultaneously in a breadth-first strategy. The advantage of using\nBFS-based traversal is that we can leverage the massively parallel processing\ncapabilities of the GPU. The disadvantage is the generation of more\nintermediate results. We propose several optimization techniques to cope with\nthe problem. Our implementation follows a filtering-and-verification strategy.\nWhile most previous work on GPUs requires one-/two-step joining, we use\none-step verification to decide the candidates in current frontier of nodes.\nOur implementation has a speedup up to 4x over previous GPU state-of-the-art\nimplementation.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 04:58:54 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 19:46:58 GMT"}, {"version": "v3", "created": "Wed, 11 Mar 2020 04:33:11 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Wang", "Leyuan", ""], ["Owens", "John D.", ""]]}, {"id": "2003.01538", "submitter": "M. G. Sarwar Murshed", "authors": "Edward Verenich, Alvaro Velasquez, M.G. Sarwar Murshed, Faraz Hussain", "title": "FlexServe: Deployment of PyTorch Models as Flexible REST Endpoints", "comments": "3 pages, 1 figure, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The integration of artificial intelligence capabilities into modern software\nsystems is increasingly being simplified through the use of cloud-based machine\nlearning services and representational state transfer architecture design.\nHowever, insufficient information regarding underlying model provenance and the\nlack of control over model evolution serve as an impediment to the more\nwidespread adoption of these services in many operational environments which\nhave strict security requirements. Furthermore, tools such as TensorFlow\nServing allow models to be deployed as RESTful endpoints, but require\nerror-prone transformations for PyTorch models as these dynamic computational\ngraphs. This is in contrast to the static computational graphs of TensorFlow.\nTo enable rapid deployments of PyTorch models without intermediate\ntransformations we have developed FlexServe, a simple library to deploy\nmulti-model ensembles with flexible batching.\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 18:51:09 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Verenich", "Edward", ""], ["Velasquez", "Alvaro", ""], ["Murshed", "M. G. Sarwar", ""], ["Hussain", "Faraz", ""]]}, {"id": "2003.01551", "submitter": "Yang Zhao", "authors": "Hongjie Wang, Yang Zhao, Chaojian Li, Yue Wang, Yingyan Lin", "title": "A New MRAM-based Process In-Memory Accelerator for Efficient Neural\n  Network Training with Floating Point Precision", "comments": "Accepted by the IEEE International Symposium on Circuits and Systems\n  2020 (ISCAS'2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.ET cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The excellent performance of modern deep neural networks (DNNs) comes at an\noften prohibitive training cost, limiting the rapid development of DNN\ninnovations and raising various environmental concerns. To reduce the dominant\ndata movement cost of training, process in-memory (PIM) has emerged as a\npromising solution as it alleviates the need to access DNN weights. However,\nstate-of-the-art PIM DNN training accelerators employ either analog/mixed\nsignal computing which has limited precision or digital computing based on a\nmemory technology that supports limited logic functions and thus requires\ncomplicated procedure to realize floating point computation. In this paper, we\npropose a spin orbit torque magnetic random access memory (SOT-MRAM) based\ndigital PIM accelerator that supports floating point precision. Specifically,\nthis new accelerator features an innovative (1) SOT-MRAM cell, (2) full\naddition design, and (3) floating point computation. Experiment results show\nthat the proposed SOT-MRAM PIM based DNN training accelerator can achieve\n3.3$\\times$, 1.8$\\times$, and 2.5$\\times$ improvement in terms of energy,\nlatency, and area, respectively, compared with a state-of-the-art PIM based DNN\ntraining accelerator.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 04:58:54 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 16:49:47 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Wang", "Hongjie", ""], ["Zhao", "Yang", ""], ["Li", "Chaojian", ""], ["Wang", "Yue", ""], ["Lin", "Yingyan", ""]]}, {"id": "2003.01575", "submitter": "Lifeng Liu", "authors": "Lifeng Liu, Fengda Zhang, Jun Xiao, and Chao Wu", "title": "Evaluation Framework For Large-scale Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning is proposed as a machine learning setting to enable\ndistributed edge devices, such as mobile phones, to collaboratively learn a\nshared prediction model while keeping all the training data on device, which\ncan not only take full advantage of data distributed across millions of nodes\nto train a good model but also protect data privacy. However, learning in\nscenario above poses new challenges. In fact, data across a massive number of\nunreliable devices is likely to be non-IID (identically and independently\ndistributed), which may make the performance of models trained by federated\nlearning unstable. In this paper, we introduce a framework designed for\nlarge-scale federated learning which consists of approaches to generating\ndataset and modular evaluation framework. Firstly, we construct a suite of\nopen-source non-IID datasets by providing three respects including covariate\nshift, prior probability shift, and concept shift, which are grounded in\nreal-world assumptions. In addition, we design several rigorous evaluation\nmetrics including the number of network nodes, the size of datasets, the number\nof communication rounds and communication resources etc. Finally, we present an\nopen-source benchmark for large-scale federated learning research.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 15:12:13 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2020 02:14:05 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Liu", "Lifeng", ""], ["Zhang", "Fengda", ""], ["Xiao", "Jun", ""], ["Wu", "Chao", ""]]}, {"id": "2003.01648", "submitter": "Pedro Casas Dr.", "authors": "Alessandro D'Alconzo, Idilio Drago, Andrea Morichetta, Marco Mellia,\n  Pedro Casas", "title": "A Survey on Big Data for Network Traffic Monitoring and Analysis", "comments": null, "journal-ref": "IEEE Transactions on Network and Service Management, vol. 16, no.\n  3, pp. 800-813, Sept. 2019", "doi": "10.1109/TNSM.2019.2933358", "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network Traffic Monitoring and Analysis (NTMA) represents a key component for\nnetwork management, especially to guarantee the correct operation of\nlarge-scale networks such as the Internet. As the complexity of Internet\nservices and the volume of traffic continue to increase, it becomes difficult\nto design scalable NTMA applications. Applications such as traffic\nclassification and policing require real-time and scalable approaches. Anomaly\ndetection and security mechanisms require to quickly identify and react to\nunpredictable events while processing millions of heterogeneous events. At\nlast, the system has to collect, store, and process massive sets of historical\ndata for post-mortem analysis. Those are precisely the challenges faced by\ngeneral big data approaches: Volume, Velocity, Variety, and Veracity. This\nsurvey brings together NTMA and big data. We catalog previous work on NTMA that\nadopt big data approaches to understand to what extent the potential of big\ndata is being explored in NTMA. This survey mainly focuses on approaches and\ntechnologies to manage the big NTMA data, additionally briefly discussing big\ndata analytics (e.g., machine learning) for the sake of NTMA. Finally, we\nprovide guidelines for future work, discussing lessons learned, and research\ndirections.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 17:17:36 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["D'Alconzo", "Alessandro", ""], ["Drago", "Idilio", ""], ["Morichetta", "Andrea", ""], ["Mellia", "Marco", ""], ["Casas", "Pedro", ""]]}, {"id": "2003.01697", "submitter": "Muktikanta Sa", "authors": "Bapi Chatterjee and Sathya Peri and Muktikanta Sa", "title": "Dynamic Graph Operations: A Consistent Non-blocking Approach", "comments": "27 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph algorithms enormously contribute to the domains such as blockchains,\nsocial networks, biological networks, telecommunication networks, and several\nothers. The ever-increasing demand of data-volume, as well as speed of such\napplications, have essentially transported these applications from their\ncomfort zone: static setting, to a challenging territory of dynamic updates. At\nthe same time, mainstreaming of multi-core processors have entailed that the\ndynamic applications should be able to exploit concurrency as soon as\nparallelization gets inhibited. Thus, the design and implementation of\nefficient concurrent dynamic graph algorithms have become significant.\n  This paper reports a novel library of concurrent shared-memory algorithms for\nbreadth-first search (BFS), single-source shortest-path (SSSP), and betweenness\ncentrality (BC) in a dynamic graph. The presented algorithms are provably\nnon-blocking and linearizable. We extensively evaluate C++ implementations of\nthe algorithms through several micro-benchmarks. The experimental results\ndemonstrate the scalability with the number of threads. Our experiments also\nhighlight the limitations of static graph analytics methods in a dynamic\nsetting.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 18:40:05 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Chatterjee", "Bapi", ""], ["Peri", "Sathya", ""], ["Sa", "Muktikanta", ""]]}, {"id": "2003.01836", "submitter": "Nathan Vaughn", "authors": "Nathan Vaughn, Leighton Wilson, Robert Krasny", "title": "A GPU-Accelerated Barycentric Lagrange Treecode", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an MPI + OpenACC implementation of the kernel-independent\nbarycentric Lagrange treecode (BLTC) for fast summation of particle\ninteractions on GPUs. The distributed memory parallelization uses recursive\ncoordinate bisection for domain decomposition and MPI remote memory access to\nbuild locally essential trees on each rank. The particle interactions are\norganized into target batch/source cluster interactions which efficiently map\nonto the GPU; target batching provides an outer level of parallelism, while the\ndirect sum form of the barycentric particle-cluster approximation provides an\ninner level of parallelism. The GPU-accelerated BLTC performance is\ndemonstrated on several test cases up to 1~billion particles interacting via\nthe Coulomb potential and Yukawa potential.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 23:57:40 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 21:01:02 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Vaughn", "Nathan", ""], ["Wilson", "Leighton", ""], ["Krasny", "Robert", ""]]}, {"id": "2003.01841", "submitter": "Nishant Budhdev", "authors": "Nishant Budhdev, Mun Choon Chan, Tulika Mitra", "title": "IsoRAN: Isolation and Scaling for 5G RANvia User-Level Data Plane\n  Virtualization", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  5G presents a unique set of challenges for cellular network architecture. The\narchitecture needs to be versatile in order to handle a variety of use cases.\nWhile network slicing has been proposed as a way to provide such versatility,\nit is also important to ensure that slices do not adversely interfere with each\nother. In other words, isolation among network slices is needed. Additionally,\nthe large number of use cases also implies a large number of users, making it\nimperative that 5G architectures scale efficiently. In this paper we propose\nIsoRAN, which provides isolation and scaling along with the flexibility needed\nfor 5G architecture. In IsoRAN, users are processed by daemon threads in the\nCloud Radio Access Network (CRAN) architecture. Our design allows users from\ndifferent use cases to be executed, in a distributed manner, on the most\nefficient hardware to ensure that the Service Level Agreements (SLAs) are met\nwhile minimising power consumption. Our experiments show that IsoRAN handles\nusers with different SLA while providing isolation to reduce interference. This\nincreased isolation reduces the drop rate for different users from 42% to\nnearly 0% in some cases. Finally, we run large scale simulations on real traces\nto show the benefits for power consumption and cost reduction scale while\nincreasing the number of base stations.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 00:39:59 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Budhdev", "Nishant", ""], ["Chan", "Mun Choon", ""], ["Mitra", "Tulika", ""]]}, {"id": "2003.01914", "submitter": "Debasish Pattanayak", "authors": "Debasish Pattanayak, Klaus-Tycho Foerster, Partha Sarathi Mandal,\n  Stefan Schmid", "title": "Conic Formation in Presence of Faulty Robots", "comments": "Full version of the conference paper in ALGOSENSORS, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pattern formation is one of the most fundamental problems in distributed\ncomputing, which has recently received much attention. In this paper, we\ninitiate the study of distributed pattern formation in situations when some\nrobots can be \\textit{faulty}. In particular, we consider the well-established\n\\textit{look-compute-move} model with oblivious, anonymous robots. We first\npresent lower bounds and show that any deterministic algorithm takes at least\ntwo rounds to form simple patterns in the presence of faulty robots. We then\npresent distributed algorithms for our problem which match this bound,\n\\textit{for conic sections}: in at most two rounds, robots form lines, circles\nand parabola tolerating $f=2,3$ and $4$ faults, respectively. For $f=5$, the\ntarget patterns are parabola, hyperbola and ellipse. We show that the resulting\npattern includes the $f$ faulty robots in the pattern of $n$ robots, where $n\n\\geq 2f+1$, and that $f < n < 2f+1$ robots cannot form such patterns. We\nconclude by discussing several relaxations and extensions.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 06:44:27 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2020 17:59:25 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Pattanayak", "Debasish", ""], ["Foerster", "Klaus-Tycho", ""], ["Mandal", "Partha Sarathi", ""], ["Schmid", "Stefan", ""]]}, {"id": "2003.02200", "submitter": "Andres Jesus Sanchez Fernandez", "authors": "A. J. Sanchez-Fernandez, L. F. Romero, G. Bandera and S. Tabik", "title": "A data relocation approach for terrain surface analysis on multi-GPU\n  systems: a case study on the total viewshed problem", "comments": null, "journal-ref": null, "doi": "10.1080/13658816.2020.1844207", "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital Elevation Models (DEMs) are important datasets for modelling the line\nof sight, such as radio signals, sound waves and human vision. These are\ncommonly analyzed using rotational sweep algorithms. However, such algorithms\nrequire large numbers of memory accesses to 2D arrays which, despite being\nregular, result in poor data locality in memory. Here, we propose a new\nmethodology called skewed Digital Elevation Model (sDEM), which substantially\nimproves the locality of memory accesses and increases the inherent parallelism\ninvolved in the computation of rotational sweep-based algorithms. In\nparticular, sDEM applies a data restructuring technique before accessing the\nmemory and performing the computation. To demonstrate the high efficiency of\nsDEM, we use the problem of total viewshed computation as a case study\nconsidering different implementations for single-core, multi-core, single-GPU\nand multi-GPU platforms. We conducted two experiments to compare sDEM with (i)\nthe most commonly used geographic information systems (GIS) software and (ii)\nthe state-of-the-art algorithm. In the first experiment, sDEM is on average\n8.8x faster than current GIS software despite being able to consider only few\npoints because of their limitations. In the second experiment, sDEM is 827.3x\nfaster than the state-of-the-art algorithm in the best case.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 17:13:17 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 14:21:11 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Sanchez-Fernandez", "A. J.", ""], ["Romero", "L. F.", ""], ["Bandera", "G.", ""], ["Tabik", "S.", ""]]}, {"id": "2003.02256", "submitter": "Joseph Kump", "authors": "Joseph Kump, Eileen R. Martin", "title": "Multichannel Analysis of Surface Waves Accelerated (MASWAccelerated):\n  Software for Efficient Surface Wave Inversion Using MPI and GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.DC cs.NA math.NA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Multichannel Analysis of Surface Waves (MASW) is a technique frequently used\nin geotechnical engineering and engineering geophysics to infer layered models\nof seismic shear wave velocities in the top tens to hundreds of meters of the\nsubsurface. We aim to accelerate MASW calculations by capitalizing on modern\ncomputer hardware available in the workstations of most engineers: multiple\ncores and graphics processing units (GPUs). We propose new parallel and GPU\naccelerated algorithms for evaluating MASW data, and provide software\nimplementations in C using Message Passing Interface (MPI) and CUDA. These\nalgorithms take advantage of sparsity that arises in the problem, and the work\nbalance between processes considers typical data trends. We compare our methods\nto an existing open source Matlab MASW tool. Our serial C implementation\nachieves a 2x speedup over the Matlab software, and we continue to see\nimprovements by parallelizing the problem with MPI. We see nearly perfect\nstrong and weak scaling for uniform data, and improve strong scaling for\nrealistic data by repartitioning the problem to process mapping. By utilizing\nGPUs available on most modern workstations, we observe an additional 1.3x\nspeedup over the serial C implementation on the first use of the method. We\ntypically repeatedly evaluate theoretical dispersion curves as part of an\noptimization procedure, and on the GPU the kernel can be cached for faster\nreuse on later runs. We observe a 3.2x speedup on the cached GPU runs compared\nto the serial C runs. This work is the first open-source parallel or\nGPU-accelerated software tool for MASW imaging, and should enable geotechnical\nengineers to fully utilize all computer hardware at their disposal.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 18:54:14 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Kump", "Joseph", ""], ["Martin", "Eileen R.", ""]]}, {"id": "2003.02291", "submitter": "Bryan Ford", "authors": "Bryan Ford, Philipp Jovanovic, Ewa Syta", "title": "Que Sera Consensus: Simple Asynchronous Agreement with Private Coins and\n  Threshold Logical Clocks", "comments": "6 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is commonly held that asynchronous consensus is much more complex,\ndifficult, and costly than partially-synchronous algorithms, especially without\nusing common coins. This paper challenges that conventional wisdom with que\nsera consensus QSC, an approach to consensus that cleanly decomposes the\nagreement problem from that of network asynchrony. QSC uses only private coins\nand reaches consensus in $O(1)$ expected communication rounds. It relies on\n\"lock-step\" synchronous broadcast, but can run atop a threshold logical clock\n(TLC) algorithm to time and pace partially-reliable communication atop an\nunderlying asynchronous network. This combination is arguably simpler than\npartially-synchronous consensus approaches like (Multi-)Paxos or Raft with\nleader election, and is more robust to slow leaders or targeted network\ndenial-of-service attacks. The simplest formulations of QSC atop TLC incur\nexpected $O(n^2)$ messages and $O(n^4)$ bits per agreement, or $O(n^3)$ bits\nwith straightforward optimizations. An on-demand implementation, in which\nclients act as \"natural leaders\" to execute the protocol atop stateful servers\nthat merely implement passive key-value stores, can achieve $O(n^2)$ expected\ncommunication bits per client-driven agreement.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 19:06:58 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Ford", "Bryan", ""], ["Jovanovic", "Philipp", ""], ["Syta", "Ewa", ""]]}, {"id": "2003.02319", "submitter": "Edgar Fajardo", "authors": "Edgar Fajardo, Matevz Tadel, Justas Balcas, Alja Tadel, Frank\n  Wuerthwein, Diego Davila, Jonathan Guiang, and Igor Sfiligoi", "title": "Moving the California distributed CMS xcache from bare metal into\n  containers using Kubernetes", "comments": null, "journal-ref": null, "doi": "10.1051/epjconf/202024504042", "report-no": null, "categories": "cs.DC hep-ex", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The University of California system has excellent networking between all of\nits campuses as well as a number of other Universities in CA, including\nCaltech, most of them being connected at 100 Gbps. UCSD and Caltech have thus\njoined their disk systems into a single logical xcache system, with worker\nnodes from both sites accessing data from disks at either site. This setup has\nbeen in place for a couple years now and has shown to work very well.\nCoherently managing nodes at multiple physical locations has however not been\ntrivial, and we have been looking for ways to improve operations. With the\nPacific Research Platform (PRP) now providing a Kubernetes resource pool\nspanning resources in the science DMZs of all the UC campuses, we have recently\nmigrated the xcache services from being hosted bare-metal into containers. This\npaper presents our experience in both migrating to and operating in the new\nenvironment.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 20:19:58 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Fajardo", "Edgar", ""], ["Tadel", "Matevz", ""], ["Balcas", "Justas", ""], ["Tadel", "Alja", ""], ["Wuerthwein", "Frank", ""], ["Davila", "Diego", ""], ["Guiang", "Jonathan", ""], ["Sfiligoi", "Igor", ""]]}, {"id": "2003.02351", "submitter": "Jiayi Xu", "authors": "Jiayi Xu, Hanqi Guo, Han-Wei Shen, Mukund Raj, Xueyun Wang, Xueqiao\n  Xu, Zhehui Wang, Tom Peterka", "title": "Asynchronous and Load-Balanced Union-Find for Distributed and Parallel\n  Scientific Data Visualization and Analysis", "comments": "Accepted by IEEE Transactions on Visualization and Computer Graphics\n  (IEEE TVCG Special Issue on IEEE PacificVis 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel distributed union-find algorithm that features\nasynchronous parallelism and k-d tree based load balancing for scalable\nvisualization and analysis of scientific data. Applications of union-find\ninclude level set extraction and critical point tracking, but distributed\nunion-find can suffer from high synchronization costs and imbalanced workloads\nacross parallel processes. In this study, we prove that global synchronizations\nin existing distributed union-find can be eliminated without changing final\nresults, allowing overlapped communications and computations for scalable\nprocessing. We also use a k-d tree decomposition to redistribute inputs, in\norder to improve workload balancing. We benchmark the scalability of our\nalgorithm with up to 1,024 processes using both synthetic and application data.\nWe demonstrate the use of our algorithm in critical point tracking and\nsuper-level set extraction with high-speed imaging experiments and fusion\nplasma simulations, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 22:26:01 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 18:46:42 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Xu", "Jiayi", ""], ["Guo", "Hanqi", ""], ["Shen", "Han-Wei", ""], ["Raj", "Mukund", ""], ["Wang", "Xueyun", ""], ["Xu", "Xueqiao", ""], ["Wang", "Zhehui", ""], ["Peterka", "Tom", ""]]}, {"id": "2003.02368", "submitter": "Shay Vargaftik", "authors": "Shay Vargaftik, Isaac Keslassy, Ariel Orda", "title": "LSQ: Load Balancing in Large-Scale Heterogeneous Systems with Multiple\n  Dispatchers", "comments": "Preprint of this work has been available online since 27.12.2018.\n  This manuscript is an extended version of an IEEE/ACM ToN paper (submitted\n  15.4.2019, revised 12.8.2019, 19.11.2019, 9.1.2020, accepted 26.2.2020). It\n  is based on the Ph.D. thesis of Shay Vargaftik, submitted to the Technion in\n  Apr 2019$.$", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, the efficiency and even the feasibility of traditional\nload-balancing policies are challenged by the rapid growth of cloud\ninfrastructure and the increasing levels of server heterogeneity. In such\nheterogeneous systems with many load-balancers, traditional solutions, such as\nJSQ, incur a prohibitively large communication overhead and detrimental incast\neffects due to herd behavior. Alternative low-communication policies, such as\nJSQ(d) and the recently proposed JIQ, are either unstable or provide poor\nperformance.\n  We introduce the Local Shortest Queue (LSQ) family of load balancing\nalgorithms. In these algorithms, each dispatcher maintains its own, local, and\npossibly outdated view of the server queue lengths, and keeps using JSQ on its\nlocal view. A small communication overhead is used infrequently to update this\nlocal view. We formally prove that as long as the error in these local\nestimates of the server queue lengths is bounded in expectation, the entire\nsystem is strongly stable. Finally, in simulations, we show how simple and\nstable LSQ policies exhibit appealing performance and significantly outperform\nexisting low-communication policies, while using an equivalent communication\nbudget. In particular, our simple policies often outperform even JSQ due to\ntheir reduction of herd behavior. We further show how, by relying on smart\nservers (i.e., advanced pull-based communication), we can further improve\nperformance and lower communication overhead.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 23:38:41 GMT"}, {"version": "v2", "created": "Sun, 8 Mar 2020 09:10:40 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Vargaftik", "Shay", ""], ["Keslassy", "Isaac", ""], ["Orda", "Ariel", ""]]}, {"id": "2003.02369", "submitter": "Byung Hoon Ahn", "authors": "Byung Hoon Ahn, Jinwon Lee, Jamie Menjay Lin, Hsin-Pai Cheng, Jilei\n  Hou, Hadi Esmaeilzadeh", "title": "Ordering Chaos: Memory-Aware Scheduling of Irregularly Wired Neural\n  Networks for Edge Devices", "comments": "Published as a conference paper at MLSys 2020 (Oral Presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances demonstrate that irregularly wired neural networks from\nNeural Architecture Search (NAS) and Random Wiring can not only automate the\ndesign of deep neural networks but also emit models that outperform previous\nmanual designs. These designs are especially effective while designing neural\narchitectures under hard resource constraints (memory, MACs, . . . ) which\nhighlights the importance of this class of designing neural networks. However,\nsuch a move creates complication in the previously streamlined pattern of\nexecution. In fact one of the main challenges is that the order of such nodes\nin the neural network significantly effects the memory footprint of the\nintermediate activations. Current compilers do not schedule with regard to\nactivation memory footprint that it significantly increases its peak compared\nto the optimum, rendering it not applicable for edge devices. To address this\nstanding issue, we present a memory-aware compiler, dubbed SERENITY, that\nutilizes dynamic programming to find a sequence that finds a schedule with\noptimal memory footprint. Our solution also comprises of graph rewriting\ntechnique that allows further reduction beyond the optimum. As such, SERENITY\nachieves optimal peak memory, and the graph rewriting technique further\nimproves this resulting in 1.68x improvement with dynamic programming-based\nscheduler and 1.86x with graph rewriting, against TensorFlow Lite with less\nthan one minute overhead.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 23:38:54 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Ahn", "Byung Hoon", ""], ["Lee", "Jinwon", ""], ["Lin", "Jamie Menjay", ""], ["Cheng", "Hsin-Pai", ""], ["Hou", "Jilei", ""], ["Esmaeilzadeh", "Hadi", ""]]}, {"id": "2003.02450", "submitter": "Edric Matwiejew Mr.", "authors": "Edric Matwiejew and Jingbo Wang", "title": "QSW_MPI: a framework for parallel simulation of quantum stochastic walks", "comments": "15 pages, 15 Figures", "journal-ref": null, "doi": "10.1016/j.cpc.2020.107724", "report-no": null, "categories": "quant-ph cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  QSW_MPI is a python package developed for time-series simulation of\ncontinuous-time quantum stochastic walks. This model allows for the study of\nMarkovian open quantum systems in the Lindblad formalism, including a\ngeneralisation of the continuous-time random walk and continuous-time quantum\nwalk. Consisting of a python interface accessing parallelised Fortran libraries\nutilising sparse data structures, QSW_MPI is scalable to massively parallel\ncomputers, which makes possible the simulation of a wide range of walk dynamics\non directed and undirected graphs of arbitrary complexity.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 06:27:00 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 05:05:15 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Matwiejew", "Edric", ""], ["Wang", "Jingbo", ""]]}, {"id": "2003.02793", "submitter": "Hangyu Zhu", "authors": "Hangyu Zhu and Yaochu Jin", "title": "Real-time Federated Evolutionary Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning is a distributed machine learning approach to privacy\npreservation and two major technical challenges prevent a wider application of\nfederated learning. One is that federated learning raises high demands on\ncommunication, since a large number of model parameters must be transmitted\nbetween the server and the clients. The other challenge is that training large\nmachine learning models such as deep neural networks in federated learning\nrequires a large amount of computational resources, which may be unrealistic\nfor edge devices such as mobile phones. The problem becomes worse when deep\nneural architecture search is to be carried out in federated learning. To\naddress the above challenges, we propose an evolutionary approach to real-time\nfederated neural architecture search that not only optimize the model\nperformance but also reduces the local payload. During the search, a\ndouble-sampling technique is introduced, in which for each individual, a\nrandomly sampled sub-model of a master model is transmitted to a number of\nrandomly sampled clients for training without reinitialization. This way, we\neffectively reduce computational and communication costs required for\nevolutionary optimization and avoid big performance fluctuations of the local\nmodels, making the proposed framework well suited for real-time federated\nneural architecture search.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 17:03:28 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Zhu", "Hangyu", ""], ["Jin", "Yaochu", ""]]}, {"id": "2003.02820", "submitter": "Mostafa Hadadian Nejad Yousefi", "authors": "Mostafa Hadadian Nejad Yousefi, Amirmasoud Ghiassi, Boshra Sadat\n  Hashemi, Maziar Goudarzi", "title": "Workload Scheduling on heterogeneous Mobile Edge Cloud in 5G networks to\n  Minimize SLA Violation", "comments": "12 pages, 8 figures, 4 tables contact: hadadian AT ce DOT sharif DOT\n  edu", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart devices have become an indispensable part of our lives and gain\nincreasing applicability in almost every area. Latency-aware applications such\nas Augmented Reality (AR), autonomous driving, and online gaming demand more\nresources such as network bandwidth and computational capabilities. Since the\ntraditional mobile networks cannot fulfill the required bandwidth and latency,\nMobile Edge Cloud (MEC) emerged to provide cloud computing capabilities in the\nproximity of users on 5G networks. In this paper, we consider a heterogeneous\nMEC network with numerous mobile users that send their tasks to MEC servers.\nEach task has a maximum acceptable response time. Non-uniform distribution of\nusers makes some MEC servers hotspots that cannot take more. A solution is to\nrelocate the tasks among MEC servers, called Workload Migration. We formulate\nthis problem of task scheduling as a mixed-integer non-linear optimization\nproblem to minimize the number of Service Level Agreement (SLA) violations.\nSince solving this optimization problem has high computational complexity, we\nintroduce a greedy algorithm called MESA, Migration Enabled Scheduling\nAlgorithm, which reaches a near-optimal solution quickly. Our experiments show\nthat in the term of SLA violation, MESA is only 8% and 11% far from the optimal\nchoice on the average and the worst-case, respectively. Moreover, the migration\nenabled solution can reduce SLA violations by about 30% compare to assigning\ntasks to MEC servers without migration.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 18:43:24 GMT"}, {"version": "v2", "created": "Sat, 21 Mar 2020 11:19:49 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Yousefi", "Mostafa Hadadian Nejad", ""], ["Ghiassi", "Amirmasoud", ""], ["Hashemi", "Boshra Sadat", ""], ["Goudarzi", "Maziar", ""]]}, {"id": "2003.02869", "submitter": "Adam Shimi", "authors": "Adam Shimi and Armando Casta\\~neda", "title": "K set-agreement bounds in round-based models through combinatorial\n  topology", "comments": "23 pages, accepted at PODC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Round-based models are very common message-passing models; combinatorial\ntopology applied to distributed computing provides sweeping results like\ngeneral lower bounds. We combine both to study the computability of k-set\nagreement. Among all the possible round-based models, we consider oblivious\nones, where the constraints are given only round per round by a set of allowed\ngraphs. And among oblivious models, we focus on closed-above ones, that is\nmodels where the set of possible graphs contains all graphs with more edges\nthan some starting graphs. These capture intuitively the underlying structure\nrequired by some communication model, like containing a ring. We then derive\nlower bounds and upper bounds in one round for k-set agreement, such that these\nbounds are proved using combinatorial topology but stated only in terms of\ngraph properties. These bounds extend to multiple rounds when limiting our\nalgorithms to be oblivious -- recalling only pairs of processes and initial\nvalue, not who send what and when.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 19:07:10 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 12:51:46 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Shimi", "Adam", ""], ["Casta\u00f1eda", "Armando", ""]]}, {"id": "2003.02972", "submitter": "Cyrus Rashtchian", "authors": "Cyrus Rashtchian, Aneesh Sharma, David P. Woodruff", "title": "LSF-Join: Locality Sensitive Filtering for Distributed All-Pairs Set\n  Similarity Under Skew", "comments": "WWW (The Web Conference) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  All-pairs set similarity is a widely used data mining task, even for large\nand high-dimensional datasets. Traditionally, similarity search has focused on\ndiscovering very similar pairs, for which a variety of efficient algorithms are\nknown. However, recent work highlights the importance of finding pairs of sets\nwith relatively small intersection sizes. For example, in a recommender system,\ntwo users may be alike even though their interests only overlap on a small\npercentage of items. In such systems, some dimensions are often highly skewed\nbecause they are very popular. Together these two properties render previous\napproaches infeasible for large input sizes. To address this problem, we\npresent a new distributed algorithm, LSF-Join, for approximate all-pairs set\nsimilarity. The core of our algorithm is a randomized selection procedure based\non Locality Sensitive Filtering. Our method deviates from prior approximate\nalgorithms, which are based on Locality Sensitive Hashing. Theoretically, we\nshow that LSF-Join efficiently finds most close pairs, even for small\nsimilarity thresholds and for skewed input sets. We prove guarantees on the\ncommunication, work, and maximum load of LSF-Join, and we also experimentally\ndemonstrate its accuracy on multiple graphs.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 00:06:20 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Rashtchian", "Cyrus", ""], ["Sharma", "Aneesh", ""], ["Woodruff", "David P.", ""]]}, {"id": "2003.02978", "submitter": "Markus Foote", "authors": "Markus D. Foote (1), Philip E. Dennison (2), Andrew K. Thorpe (3),\n  David R. Thompson (3), Siraput Jongaramrungruang (4), Christian Frankenberg\n  (3 and 4), Sarang C. Joshi (1) ((1) Scientific Computing and Imaging\n  Institute, University of Utah, Salt Lake City, UT (2) Department of\n  Geography, University of Utah, Salt Lake City, UT, (3) Jet Propulsion\n  Laboratory, California Institute of Technology, Pasadena, CA, (4) Division of\n  Geological and Planetary Sciences, California Institute of Technology,\n  Pasadena, CA)", "title": "Fast and Accurate Retrieval of Methane Concentration from Imaging\n  Spectrometer Data Using Sparsity Prior", "comments": "13 pages, 11 figures", "journal-ref": "IEEE Transactions on Geoscience and Remote Sensing, 2020, pp. 1-13", "doi": "10.1109/TGRS.2020.2976888", "report-no": null, "categories": "eess.IV cs.DC physics.ao-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The strong radiative forcing by atmospheric methane has stimulated interest\nin identifying natural and anthropogenic sources of this potent greenhouse gas.\nPoint sources are important targets for quantification, and anthropogenic\ntargets have potential for emissions reduction. Methane point source plume\ndetection and concentration retrieval have been previously demonstrated using\ndata from the Airborne Visible InfraRed Imaging Spectrometer Next Generation\n(AVIRIS-NG). Current quantitative methods have tradeoffs between computational\nrequirements and retrieval accuracy, creating obstacles for processing\nreal-time data or large datasets from flight campaigns. We present a new\ncomputationally efficient algorithm that applies sparsity and an albedo\ncorrection to matched filter retrieval of trace gas concentration-pathlength.\nThe new algorithm was tested using AVIRIS-NG data acquired over several point\nsource plumes in Ahmedabad, India. The algorithm was validated using simulated\nAVIRIS-NG data including synthetic plumes of known methane concentration.\nSparsity and albedo correction together reduced the root mean squared error of\nretrieved methane concentration-pathlength enhancement by 60.7% compared with a\nprevious robust matched filter method. Background noise was reduced by a factor\nof 2.64. The new algorithm was able to process the entire 300 flightline 2016\nAVIRIS-NG India campaign in just over 8 hours on a desktop computer with GPU\nacceleration.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 00:31:42 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Foote", "Markus D.", "", "3 and 4"], ["Dennison", "Philip E.", "", "3 and 4"], ["Thorpe", "Andrew K.", "", "3 and 4"], ["Thompson", "David R.", "", "3 and 4"], ["Jongaramrungruang", "Siraput", "", "3 and 4"], ["Frankenberg", "Christian", "", "3 and 4"], ["Joshi", "Sarang C.", ""]]}, {"id": "2003.03009", "submitter": "Shuo Ouyang", "authors": "Shuo Ouyang, Dezun Dong, Yemao Xu, Liquan Xiao", "title": "Communication optimization strategies for distributed deep neural\n  network training: A survey", "comments": null, "journal-ref": "Journal of Parallel and Distributed Computing 149 (2021) pp. 52-65", "doi": "10.1016/j.jpdc.2020.11.005", "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent trends in high-performance computing and deep learning have led to the\nproliferation of studies on large-scale deep neural network training. However,\nthe frequent communication requirements among computation nodes drastically\nslows the overall training speeds, which causes bottlenecks in distributed\ntraining, particularly in clusters with limited network bandwidths. To mitigate\nthe drawbacks of distributed communications, researchers have proposed various\noptimization strategies. In this paper, we provide a comprehensive survey of\ncommunication strategies from both an algorithm viewpoint and a computer\nnetwork perspective. Algorithm optimizations focus on reducing the\ncommunication volumes used in distributed training, while network optimizations\nfocus on accelerating the communications between distributed devices. At the\nalgorithm level, we describe how to reduce the number of communication rounds\nand transmitted bits per round. In addition, we elucidate how to overlap\ncomputation and communication. At the network level, we discuss the effects\ncaused by network infrastructures, including logical communication schemes and\nnetwork protocols. Finally, we extrapolate the potential future challenges and\nnew research directions to accelerate communications for distributed deep\nneural network training.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 02:32:54 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 02:48:04 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Ouyang", "Shuo", ""], ["Dong", "Dezun", ""], ["Xu", "Yemao", ""], ["Xiao", "Liquan", ""]]}, {"id": "2003.03043", "submitter": "Seyedramin Rasoulinezhad", "authors": "SeyedRamin Rasoulinezhad, Siddhartha, Hao Zhou, Lingli Wang, David\n  Boland, Philip H.W. Leong", "title": "LUXOR: An FPGA Logic Cell Architecture for Efficient Compressor Tree\n  Implementations", "comments": "In Proceedings of the 2020 ACM/SIGDA International Symposium on\n  Field-Programmable Gate Arrays (FPGA'20), February 23-25, 2020, Seaside, CA,\n  USA", "journal-ref": null, "doi": "10.1145/3373087.3375303", "report-no": null, "categories": "cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two tiers of modifications to FPGA logic cell architecture to\ndeliver a variety of performance and utilization benefits with only minor area\noverheads. In the irst tier, we augment existing commercial logic cell\ndatapaths with a 6-input XOR gate in order to improve the expressiveness of\neach element, while maintaining backward compatibility. This new architecture\nis vendor-agnostic, and we refer to it as LUXOR. We also consider a secondary\ntier of vendor-speciic modifications to both Xilinx and Intel FPGAs, which we\nrefer to as X-LUXOR+ and I-LUXOR+ respectively. We demonstrate that compressor\ntree synthesis using generalized parallel counters (GPCs) is further improved\nwith the proposed modifications. Using both the Intel adaptive logic module and\nthe Xilinx slice at the 65nm technology node for a comparative study, it is\nshown that the silicon area overhead is less than 0.5% for LUXOR and 5-6% for\nLUXOR+, while the delay increments are 1-6% and 3-9% respectively. We\ndemonstrate that LUXOR can deliver an average reduction of 13-19% in logic\nutilization on micro-benchmarks from a variety of domains.BNN benchmarks\nbenefit the most with an average reduction of 37-47% in logic utilization,\nwhich is due to the highly-efficient mapping of the XnorPopcount operation on\nour proposed LUXOR+ logic cells.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 05:54:11 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Rasoulinezhad", "SeyedRamin", ""], ["Siddhartha", "", ""], ["Zhou", "Hao", ""], ["Wang", "Lingli", ""], ["Boland", "David", ""], ["Leong", "Philip H. W.", ""]]}, {"id": "2003.03058", "submitter": "Michal Dory", "authors": "Michal Dory, Merav Parter", "title": "Exponentially Faster Shortest Paths in the Congested Clique", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present improved deterministic algorithms for approximating shortest paths\nin the Congested Clique model of distributed computing. We obtain\n$poly(\\log\\log n)$-round algorithms for the following problems in unweighted\nundirected $n$-vertex graphs:\n  -- $(1+\\epsilon)$-approximation of multi-source shortest paths (MSSP) from\n$O(\\sqrt{n})$ sources.\n  -- $(2+\\epsilon)$-approximation of all pairs shortest paths (APSP).\n  -- $(1+\\epsilon,\\beta)$-approximation of APSP where $\\beta=O(\\frac{\\log\\log\nn}{\\epsilon})^{\\log\\log n}$.\n  These bounds improve exponentially over the state-of-the-art poly-logarithmic\nbounds due to [Censor-Hillel et al., PODC19]. It also provides the first\nnearly-additive bounds for the APSP problem in sub-polynomial time. Our\napproach is based on distinguishing between short and long distances based on\nsome distance threshold $t = O(\\frac{\\beta}{\\epsilon})$ where\n$\\beta=O(\\frac{\\log\\log n}{\\epsilon})^{\\log\\log n}$. Handling the long\ndistances is done by devising a new algorithm for computing sparse\n$(1+\\epsilon,\\beta)$ emulator with $O(n\\log\\log n)$ edges. For the short\ndistances, we provide distance-sensitive variants for the distance tool-kit of\n[Censor-Hillel et al., PODC19]. By exploiting the fact that this tool-kit\nshould be applied only on local balls of radius $t$, their round complexities\nget improved from $poly(\\log n)$ to $poly(\\log t)$.\n  Finally, our deterministic solutions for these problems are based on a\nderandomization scheme of a novel variant of the hitting set problem, which\nmight be of independent interest.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 07:13:41 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Dory", "Michal", ""], ["Parter", "Merav", ""]]}, {"id": "2003.03134", "submitter": "Joseph Ortiz", "authors": "Joseph Ortiz, Mark Pupilli, Stefan Leutenegger, Andrew J. Davison", "title": "Bundle Adjustment on a Graph Processor", "comments": "Published in Proceedings of the IEEE Conference on Computer Vision\n  and Pattern Recognition (CVPR 2020). Video:\n  https://www.youtube.com/watch?v=TqeN8aQNgd0", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph processors such as Graphcore's Intelligence Processing Unit (IPU) are\npart of the major new wave of novel computer architecture for AI, and have a\ngeneral design with massively parallel computation, distributed on-chip memory\nand very high inter-core communication bandwidth which allows breakthrough\nperformance for message passing algorithms on arbitrary graphs. We show for the\nfirst time that the classical computer vision problem of bundle adjustment (BA)\ncan be solved extremely fast on a graph processor using Gaussian Belief\nPropagation. Our simple but fully parallel implementation uses the 1216 cores\non a single IPU chip to, for instance, solve a real BA problem with 125\nkeyframes and 1919 points in under 40ms, compared to 1450ms for the Ceres CPU\nlibrary. Further code optimisation will surely increase this difference on\nstatic problems, but we argue that the real promise of graph processing is for\nflexible in-place optimisation of general, dynamically changing factor graphs\nrepresenting Spatial AI problems. We give indications of this with experiments\nshowing the ability of GBP to efficiently solve incremental SLAM problems, and\ndeal with robust cost functions and different types of factors.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 11:05:55 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 16:59:24 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Ortiz", "Joseph", ""], ["Pupilli", "Mark", ""], ["Leutenegger", "Stefan", ""], ["Davison", "Andrew J.", ""]]}, {"id": "2003.03147", "submitter": "Tao Gu", "authors": "Tao Gu, Hung Keng Pung, Daqing Zhang", "title": "A P2P Context Lookup Service for Multiple Smart Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context information has emerged as an important resource to enable autonomy\nand flexibility of pervasive applications. The widespread use of context\ninformation necessitates efficient wide-area lookup services. In this paper, we\npresent the design and implementation of a peer-to-peer context lookup system\nto support contextaware applications over multiple smart spaces. Our system\nprovides a distributed repository for context storage, and a semantic\npeer-to-peer network for context lookup. Collaborative context-aware\napplications that utilize different context information in multiple smart\nspaces can be easily built by invoking a pull or push service provided by our\nsystem. We outline the design and implementation of our system, and validate\nour system through the development of cross-domain applications\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 12:02:00 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Gu", "Tao", ""], ["Pung", "Hung Keng", ""], ["Zhang", "Daqing", ""]]}, {"id": "2003.03217", "submitter": "Pau Tallada Cresp\\'i", "authors": "Pau Tallada, Jorge Carretero, Jordi Casals, Carles Acosta-Silva,\n  Santiago Serrano, Marc Caubet, Francisco J. Castander, Eduardo C\\'esar,\n  Mart\\'in Crocce, Manuel Delfino, Martin Eriksen, Pablo Fosalba, Enrique\n  Gazta\\~naga, Gonzalo Merino, Christian Neissner, Nadia Tonello", "title": "CosmoHub: Interactive exploration and distribution of astronomical data\n  on Hadoop", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DC physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present CosmoHub (https://cosmohub.pic.es), a web application based on\nHadoop to perform interactive exploration and distribution of massive\ncosmological datasets. Recent Cosmology seeks to unveil the nature of both dark\nmatter and dark energy mapping the large-scale structure of the Universe,\nthrough the analysis of massive amounts of astronomical data, progressively\nincreasing during the last (and future) decades with the digitization and\nautomation of the experimental techniques.\n  CosmoHub, hosted and developed at the Port d'Informaci\\'o Cient\\'ifica (PIC),\nprovides support to a worldwide community of scientists, without requiring the\nend user to know any Structured Query Language (SQL). It is serving data of\nseveral large international collaborations such as the Euclid space mission,\nthe Dark Energy Survey (DES), the Physics of the Accelerating Universe Survey\n(PAUS) and the Marenostrum Institut de Ci\\`encies de l'Espai (MICE) numerical\nsimulations. While originally developed as a PostgreSQL relational database web\nfrontend, this work describes the current version of CosmoHub, built on top of\nApache Hive, which facilitates scalable reading, writing and managing huge\ndatasets. As CosmoHub's datasets are seldomly modified, Hive it is a better\nfit.\n  Over 60 TiB of catalogued information and $50 \\times 10^9$ astronomical\nobjects can be interactively explored using an integrated visualization tool\nwhich includes 1D histogram and 2D heatmap plots. In our current\nimplementation, online exploration of datasets of $10^9$ objects can be done in\na timescale of tens of seconds. Users can also download customized subsets of\ndata in standard formats generated in few minutes.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 08:52:36 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 11:40:38 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Tallada", "Pau", ""], ["Carretero", "Jorge", ""], ["Casals", "Jordi", ""], ["Acosta-Silva", "Carles", ""], ["Serrano", "Santiago", ""], ["Caubet", "Marc", ""], ["Castander", "Francisco J.", ""], ["C\u00e9sar", "Eduardo", ""], ["Crocce", "Mart\u00edn", ""], ["Delfino", "Manuel", ""], ["Eriksen", "Martin", ""], ["Fosalba", "Pablo", ""], ["Gazta\u00f1aga", "Enrique", ""], ["Merino", "Gonzalo", ""], ["Neissner", "Christian", ""], ["Tonello", "Nadia", ""]]}, {"id": "2003.03255", "submitter": "Ami Paz", "authors": "Pierre Fraigniaud and Ami Paz", "title": "The Topology of Local Computing in Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling distributed computing in a way enabling the use of formal methods is\na challenge that has been approached from different angles, among which two\ntechniques emerged at the turn of the century: protocol complexes, and directed\nalgebraic topology. In both cases, the considered computational model generally\nassumes communication via shared objects, typically a shared memory consisting\nof a collection of read-write registers. Our paper is concerned with network\ncomputing, where the processes are located at the nodes of a network, and\ncommunicate by exchanging messages along the edges of that network. Applying\nthe topological approach for verification in network computing is a\nconsiderable challenge, mainly because the presence of identifiers assigned to\nthe nodes yields protocol complexes whose size grows exponentially with the\nsize of the underlying network. However, many of the problems studied in this\ncontext are of local nature, and their definitions do not depend on the\nidentifiers or on the size of the network. We leverage this independence in\norder to meet the above challenge, and present $\\textit{local}$ protocol\ncomplexes, whose sizes do not depend on the size of the network. As an\napplication of the design of \"compact\" protocol complexes, we reformulate the\ncelebrated lower bound of $\\Omega(\\log^*n)$ rounds for 3-coloring the $n$-node\nring, in the algebraic topology framework.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 14:53:44 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Fraigniaud", "Pierre", ""], ["Paz", "Ami", ""]]}, {"id": "2003.03304", "submitter": "Joao Barreto", "authors": "David Gureya, Jo\\~ao Neto, Reza Karimi, Jo\\~ao Barreto, Pramod\n  Bhatotia, Vivien Quema, Rodrigo Rodrigues, Paolo Romano, Vladimir Vlassov", "title": "Bandwidth-Aware Page Placement in NUMA", "comments": "Accepted at 34th IEEE International Parallel & Distributed Processing\n  Symposium (IPDPS), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Page placement is a critical problem for memoryintensive applications running\non a shared-memory multiprocessor with a non-uniform memory access (NUMA)\narchitecture. State-of-the-art page placement mechanisms interleave pages\nevenly across NUMA nodes. However, this approach fails to maximize memory\nthroughput in modern NUMA systems, characterised by asymmetric bandwidths and\nlatencies, and sensitive to memory contention and interconnect congestion\nphenomena. We propose BWAP, a novel page placement mechanism based on\nasymmetric weighted page interleaving. BWAP combines an analytical performance\nmodel of the target NUMA system with on-line iterative tuning of page\ndistribution for a given memory-intensive application. Our experimental\nevaluation with representative memory-intensive workloads shows that BWAP\nperforms up to 66% better than state-of-the-art techniques. These gains are\nparticularly relevant when multiple co-located applications run in disjoint\npartitions of a large NUMA machine or when applications do not scale up to the\ntotal number of cores.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 16:34:01 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Gureya", "David", ""], ["Neto", "Jo\u00e3o", ""], ["Karimi", "Reza", ""], ["Barreto", "Jo\u00e3o", ""], ["Bhatotia", "Pramod", ""], ["Quema", "Vivien", ""], ["Rodrigues", "Rodrigo", ""], ["Romano", "Paolo", ""], ["Vlassov", "Vladimir", ""]]}, {"id": "2003.03317", "submitter": "Joao Barreto", "authors": "Ricardo Filipe, Shady Issa, Paolo Romano, Jo\\~ao Barreto", "title": "Stretching the capacity of Hardware Transactional Memory in IBM POWER\n  architectures", "comments": null, "journal-ref": "PPoPP '19: Proceedings of the 24th Symposium on Principles and\n  Practice of Parallel Programming, February 2019, Pages 107-119", "doi": "10.1145/3293883.3295714", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The hardware transactional memory (HTM) implementations in commercially\navailable processors are significantly hindered by their tight capacity\nconstraints. In practice, this renders current HTMs unsuitable to many\nreal-world workloads of in-memory databases. This paper proposes SI-HTM, which\nstretches the capacity bounds of the underlying HTM, thus opening HTM to a much\nbroader class of applications. SI-HTM leverages the HTM implementation of the\nIBM POWER architecture with a software layer to offer a single-version\nimplementation of Snapshot Isolation. When compared to HTM- and software-based\nconcurrency control alternatives, SI-HTM exhibits improved scalability,\nachieving speedups of up to 300% relatively to HTM on in-memory database\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 17:25:13 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Filipe", "Ricardo", ""], ["Issa", "Shady", ""], ["Romano", "Paolo", ""], ["Barreto", "Jo\u00e3o", ""]]}, {"id": "2003.03320", "submitter": "Felix Sattler", "authors": "Felix Sattler, Thomas Wiegand, Wojciech Samek", "title": "Trends and Advancements in Deep Neural Network Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.MM cs.NI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to their great performance and scalability properties neural networks\nhave become ubiquitous building blocks of many applications. With the rise of\nmobile and IoT, these models now are also being increasingly applied in\ndistributed settings, where the owners of the data are separated by limited\ncommunication channels and privacy constraints. To address the challenges of\nthese distributed environments, a wide range of training and evaluation schemes\nhave been developed, which require the communication of neural network\nparametrizations. These novel approaches, which bring the \"intelligence to the\ndata\" have many advantages over traditional cloud solutions such as\nprivacy-preservation, increased security and device autonomy, communication\nefficiency and high training speed. This paper gives an overview over the\nrecent advancements and challenges in this new field of research at the\nintersection of machine learning and communications.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 17:34:15 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Sattler", "Felix", ""], ["Wiegand", "Thomas", ""], ["Samek", "Wojciech", ""]]}, {"id": "2003.03355", "submitter": "George Skretas", "authors": "Othon Michail (1), George Skretas (1), Paul G. Spirakis (1 and 2) ((1)\n  Department of Computer Science, University of Liverpool, UK, (2) Computer\n  Engineering and Informatics Department, University of Patras, Greece)", "title": "Distributed Computation and Reconfiguration in Actively Dynamic Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study systems of distributed entities that can actively\nmodify their communication network. This gives rise to distributed algorithms\nthat apart from communication can also exploit network reconfiguration in order\nto carry out a given task. At the same time, the distributed task itself may\nnow require global reconfiguration from a given initial network $G_s$ to a\ntarget network $G_f$ from a family of networks having some good properties,\nlike small diameter. With reasonably powerful computational entities, there is\na straightforward algorithm that transforms any $G_s$ into a spanning clique in\n$O(\\log n)$ time. The algorithm can then compute any global function on inputs\nand reconfigure to any target network in one round. We argue that such a\nstrategy is impractical for real applications. In real dynamic networks there\nis a cost associated with creating and maintaining connections. To formally\ncapture such costs, we define three edge-complexity measures: the \\emph{total\nedge activations}, the \\emph{maximum activated edges per round}, and the\n\\emph{maximum activated degree of a node}. The clique formation strategy\nhighlighted above, maximizes all of them. We aim at improved algorithms that\nachieve (poly)log$(n)$ time while minimizing the edge-complexity for the\ngeneral task of transforming any $G_s$ into a $G_f$ of diameter (poly)log$(n)$.\nWe give three distributed algorithms. The first runs in $O(\\log n)$ time, with\nat most $2n$ active edges per round, an optimal total of $O(n\\log n)$ edge\nactivations, a maximum degree $n-1$, and a target network of diameter 2. The\nsecond achieves bounded degree by paying an additional logarithmic factor in\ntime and in total edge activations and gives a target network of diameter\n$O(\\log n)$. Our third algorithm shows that if we slightly increase the maximum\ndegree to polylog$(n)$ then we can achieve a running time of $o(\\log^2 n)$.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 18:39:51 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Michail", "Othon", "", "1 and 2"], ["Skretas", "George", "", "1 and 2"], ["Spirakis", "Paul G.", "", "1 and 2"]]}, {"id": "2003.03398", "submitter": "Gabriel Gomes", "authors": "Gabriel Gomes, Juliette Ugirumurera, Xiaoye Li", "title": "Distributed macroscopic traffic simulation with Open Traffic Models", "comments": "6 pages, 6 figures, 1 table, submitted to IEEE ITSC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents OTM-MPI, an extension of the Open Traffic Models platform\n(OTM) for running macroscopic traffic simulations in high-performance computing\nenvironments. Macroscopic simulations are appropriate for studying regional\ntraffic scenarios when aggregate trends are of interest, rather than individual\nvehicle traces. They are also appropriate for studying the routing behavior of\nclasses of vehicles, such as app-informed vehicles. The network partitioning\nwas performed with METIS. Inter-process communication was done with MPI\n(message-passing interface). Results are provided for two networks: one\nrealistic network which was obtained from Open Street Maps for Chattanooga, TN,\nand another larger synthetic grid network. The software recorded a speed-up\nratio of 198 using 256 cores for Chattanooga, and 475 with 1,024 cores for the\nsynthetic network.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 19:14:54 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Gomes", "Gabriel", ""], ["Ugirumurera", "Juliette", ""], ["Li", "Xiaoye", ""]]}, {"id": "2003.03423", "submitter": "Rodrigo Fonseca", "authors": "Mohammad Shahrad, Rodrigo Fonseca, \\'I\\~nigo Goiri, Gohar Chaudhry,\n  Paul Batum, Jason Cooke, Eduardo Laureano, Colby Tresness, Mark Russinovich,\n  and Ricardo Bianchini", "title": "Serverless in the Wild: Characterizing and Optimizing the Serverless\n  Workload at a Large Cloud Provider", "comments": "14 pages, 20 figures. Corrected and published in USENIX ATC, July\n  2020. For accompanying dataset, see\n  https://github.com/Azure/AzurePublicDataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Function as a Service (FaaS) has been gaining popularity as a way to deploy\ncomputations to serverless backends in the cloud. This paradigm shifts the\ncomplexity of allocating and provisioning resources to the cloud provider,\nwhich has to provide the illusion of always-available resources (i.e., fast\nfunction invocations without cold starts) at the lowest possible resource cost.\nDoing so requires the provider to deeply understand the characteristics of the\nFaaS workload. Unfortunately, there has been little to no public information on\nthese characteristics. Thus, in this paper, we first characterize the entire\nproduction FaaS workload of Azure Functions. We show for example that most\nfunctions are invoked very infrequently, but there is an 8-order-of-magnitude\nrange of invocation frequencies. Using observations from our characterization,\nwe then propose a practical resource management policy that significantly\nreduces the number of function coldstarts,while spending fewerresources than\nstate-of-the-practice policies.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 20:23:04 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 12:16:16 GMT"}, {"version": "v3", "created": "Sat, 6 Jun 2020 03:35:14 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Shahrad", "Mohammad", ""], ["Fonseca", "Rodrigo", ""], ["Goiri", "\u00cd\u00f1igo", ""], ["Chaudhry", "Gohar", ""], ["Batum", "Paul", ""], ["Cooke", "Jason", ""], ["Laureano", "Eduardo", ""], ["Tresness", "Colby", ""], ["Russinovich", "Mark", ""], ["Bianchini", "Ricardo", ""]]}, {"id": "2003.03477", "submitter": "Qinqing Zheng", "authors": "Qinqing Zheng, Bor-Yiing Su, Jiyan Yang, Alisson Azzolini, Qiang Wu,\n  Ou Jin, Shri Karandikar, Hagay Lupesko, Liang Xiong, Eric Zhou", "title": "ShadowSync: Performing Synchronization in the Background for Highly\n  Scalable Distributed Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation systems are often trained with a tremendous amount of data,\nand distributed training is the workhorse to shorten the training time. While\nthe training throughput can be increased by simply adding more workers, it is\nalso increasingly challenging to preserve the model quality. In this paper, we\npresent \\shadowsync, a distributed framework specifically tailored to modern\nscale recommendation system training. In contrast to previous works where\nsynchronization happens as part of the training process, \\shadowsync separates\nthe synchronization from training and runs it in the background. Such isolation\nsignificantly reduces the synchronization overhead and increases the\nsynchronization frequency, so that we are able to obtain both high throughput\nand excellent model quality when training at scale. The superiority of our\nprocedure is confirmed by experiments on training deep neural networks for\nclick-through-rate prediction tasks. Our framework is capable to express data\nparallelism and/or model parallelism, generic to host various types of\nsynchronization algorithms, and readily applicable to large scale problems in\nother areas.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 00:26:26 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 18:29:21 GMT"}, {"version": "v3", "created": "Tue, 23 Feb 2021 18:23:31 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Zheng", "Qinqing", ""], ["Su", "Bor-Yiing", ""], ["Yang", "Jiyan", ""], ["Azzolini", "Alisson", ""], ["Wu", "Qiang", ""], ["Jin", "Ou", ""], ["Karandikar", "Shri", ""], ["Lupesko", "Hagay", ""], ["Xiong", "Liang", ""], ["Zhou", "Eric", ""]]}, {"id": "2003.03479", "submitter": "Daniel Perez", "authors": "Sam M. Werner and Paul J. Pritz and Daniel Perez", "title": "Step on the Gas? A Better Approach for Recommending the Ethereum Gas\n  Price", "comments": null, "journal-ref": "in proceedings of The 2nd International Conference on Mathematical\n  Research for Blockchain Economy (MARBLE 2020)", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In the Ethereum network, miners are incentivized to include transactions in a\nblock depending on the gas price specified by the sender. The sender of a\ntransaction therefore faces a trade-off between timely inclusion and cost of\nhis transaction. Existing recommendation mechanisms aggregate recent gas price\ndata on a per-block basis to suggest a gas price. We perform an empirical\nanalysis of historic block data to motivate the use of a predictive model for\ngas price recommendation. Subsequently, we propose a novel mechanism that\ncombines a deep-learning based price forecasting model as well as an algorithm\nparameterized by a user-specific urgency value to recommend gas prices. In a\ncomprehensive evaluation on real-world data, we show that our approach results\non average in costs savings of more than 50% while only incurring an inclusion\ndelay of 1.3 blocks, when compared to the gas price recommendation mechanism of\nthe most widely used Ethereum client.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 01:08:44 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 12:35:25 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Werner", "Sam M.", ""], ["Pritz", "Paul J.", ""], ["Perez", "Daniel", ""]]}, {"id": "2003.03564", "submitter": "Jinjin Xu", "authors": "Jinjin Xu, Wenli Du, Ran Cheng, Wangli He, Yaochu Jin", "title": "Ternary Compression for Communication-Efficient Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning over massive data stored in different locations is essential in many\nreal-world applications. However, sharing data is full of challenges due to the\nincreasing demands of privacy and security with the growing use of smart mobile\ndevices and IoT devices. Federated learning provides a potential solution to\nprivacy-preserving and secure machine learning, by means of jointly training a\nglobal model without uploading data distributed on multiple devices to a\ncentral server. However, most existing work on federated learning adopts\nmachine learning models with full-precision weights, and almost all these\nmodels contain a large number of redundant parameters that do not need to be\ntransmitted to the server, consuming an excessive amount of communication\ncosts. To address this issue, we propose a federated trained ternary\nquantization (FTTQ) algorithm, which optimizes the quantized networks on the\nclients through a self-learning quantization factor. A convergence proof of the\nquantization factor and the unbiasedness of FTTQ is given. In addition, we\npropose a ternary federated averaging protocol (T-FedAvg) to reduce the\nupstream and downstream communication of federated learning systems. Empirical\nexperiments are conducted to train widely used deep learning models on publicly\navailable datasets, and our results demonstrate the effectiveness of FTTQ and\nT-FedAvg compared with the canonical federated learning algorithms in reducing\ncommunication costs and maintaining the learning performance.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 11:55:34 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Xu", "Jinjin", ""], ["Du", "Wenli", ""], ["Cheng", "Ran", ""], ["He", "Wangli", ""], ["Jin", "Yaochu", ""]]}, {"id": "2003.03572", "submitter": "Thirunavukarasu Balasubramaniam", "authors": "Thirunavukarasu Balasubramaniam, Richi Nayak, Chau Yuen", "title": "Efficient Nonnegative Tensor Factorization via Saturating Coordinate\n  Descent", "comments": "Accepted for publication in ACM Transactions on Knowledge Discovery\n  from Data", "journal-ref": null, "doi": "10.1145/3385654", "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advancements in computing technology and web-based applications,\ndata is increasingly generated in multi-dimensional form. This data is usually\nsparse due to the presence of a large number of users and fewer user\ninteractions. To deal with this, the Nonnegative Tensor Factorization (NTF)\nbased methods have been widely used. However existing factorization algorithms\nare not suitable to process in all three conditions of size, density, and rank\nof the tensor. Consequently, their applicability becomes limited. In this\npaper, we propose a novel fast and efficient NTF algorithm using the element\nselection approach. We calculate the element importance using Lipschitz\ncontinuity and propose a saturation point based element selection method that\nchooses a set of elements column-wise for updating to solve the optimization\nproblem. Empirical analysis reveals that the proposed algorithm is scalable in\nterms of tensor size, density, and rank in comparison to the relevant\nstate-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 12:51:52 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Balasubramaniam", "Thirunavukarasu", ""], ["Nayak", "Richi", ""], ["Yuen", "Chau", ""]]}, {"id": "2003.03576", "submitter": "Jussi Hanhirova", "authors": "Jussi Hanhirova, Anton Debner, Matias Hyypp\\\"a, Vesa Hirvisalo", "title": "A machine learning environment for evaluating autonomous driving\n  software", "comments": "8 pages, 13 figures", "journal-ref": "Embedded World Conference 2019 Proceedings", "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous vehicles need safe development and testing environments. Many\ntraffic scenarios are such that they cannot be tested in the real world. We see\nhybrid photorealistic simulation as a viable tool for developing AI (artificial\nintelligence) software for autonomous driving. We present a machine learning\nenvironment for detecting autonomous vehicle corner case behavior. Our\nenvironment is based on connecting the CARLA simulation software to TensorFlow\nmachine learning framework and custom AI client software. The AI client\nsoftware receives data from a simulated world via virtual sensors and\ntransforms the data into information using machine learning models. The AI\nclients control vehicles in the simulated world. Our environment monitors the\nstate assumed by the vehicle AIs to the ground truth state derived from the\nsimulation model. Our system can search for corner cases where the vehicle AI\nis unable to correctly understand the situation. In our paper, we present the\noverall hybrid simulator architecture and compare different configurations. We\npresent performance measurements from real setups, and outline the main\nparameters affecting the hybrid simulator performance.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 13:05:03 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Hanhirova", "Jussi", ""], ["Debner", "Anton", ""], ["Hyypp\u00e4", "Matias", ""], ["Hirvisalo", "Vesa", ""]]}, {"id": "2003.03604", "submitter": "Sergio Esteves", "authors": "S\\'ergio Esteves, Gianmarco De Francisci Morales, Rodrigo Rodrigues,\n  Marco Serafini, Lu\\'is Veiga", "title": "Aion: Better Late than Never in Event-Time Streams", "comments": "14 pages, 28 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Processing data streams in near real-time is an increasingly important task.\nIn the case of event-timestamped data, the stream processing system must\npromptly handle late events that arrive after the corresponding window has been\nprocessed. To enable this late processing, the window state must be maintained\nfor a long period of time. However, current systems maintain this state in\nmemory, which either imposes a maximum period of tolerated lateness, or causes\nthe system to degrade performance or even crash when the system memory runs\nout.\n  In this paper, we propose AION, a comprehensive solution for handling late\nevents in an efficient manner, implemented on top of Flink. In designing AION,\nwe go beyond a naive solution that transfers state between memory and\npersistent storage on demand. In particular, we introduce a proactive caching\nscheme, where we leverage the semantics of stream processing to anticipate the\nneed for bringing data to memory. Furthermore, we propose a predictive cleanup\nscheme to permanently discard window state based on the likelihood of receiving\nmore late events, to prevent storage consumption from growing without bounds.\n  Our evaluation shows that AION is capable of maintaining sustainable levels\nof memory utilization while still preserving high throughput, low latency, and\nlow staleness.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 16:41:09 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 20:04:37 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Esteves", "S\u00e9rgio", ""], ["Morales", "Gianmarco De Francisci", ""], ["Rodrigues", "Rodrigo", ""], ["Serafini", "Marco", ""], ["Veiga", "Lu\u00eds", ""]]}, {"id": "2003.03674", "submitter": "Cao Vien Phung", "authors": "Cao Vien Phung, Anna Engelmann and Admela Jukan", "title": "Error Correction with Systematic RLNC in Multi-Channel THz Communication\n  Systems", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": "10.23919/MIPRO48935.2020.9245338", "report-no": null, "categories": "cs.NI cs.DC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The terahertz (THz) frequency band (0.3-10THz) has the advantage of large\navailable bandwidth and is a candidate to satisfy the ever increasing mobile\ntraffic in wireless communications. However, the THz channels are often\nabsorbed by molecules in the atmosphere, which can decrease the signal quality\nresulting in high bit error rate of received data. In this paper, we study the\nusage of systematic random linear network coding (sRLNC) for error correction\nin generic THz systems with with 2N parallel channels, whereby N main\nhigh-bitrate channels are used in parallel with N auxiliary channels with lower\nbit rate. The idea behind this approach is to use coded low-bit rate channels\nto carry redundant information from high-bit rate channels, and thus compensate\nfor errors in THz transmission. The analytical results evaluate and compare the\ndifferent scenarios of the THz system in term of the amount of coding\nredundancy, a code rate, transmission rate of auxiliary channels, the number of\nTHz channels, the modulation format and transmission distance as required\nsystem configurations for a fault tolerant THz transmission.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 22:41:00 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 16:20:15 GMT"}, {"version": "v3", "created": "Sat, 5 Jun 2021 08:11:44 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Phung", "Cao Vien", ""], ["Engelmann", "Anna", ""], ["Jukan", "Admela", ""]]}, {"id": "2003.03697", "submitter": "Feng Yin", "authors": "Feng Yin, Zhidi Lin, Yue Xu, Qinglei Kong, Deshi Li, Sergios\n  Theodoridis, Shuguang (Robert) Cui", "title": "FedLoc: Federated Learning Framework for Data-Driven Cooperative\n  Localization and Location Data Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.SY eess.SP eess.SY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this overview paper, data-driven learning model-based cooperative\nlocalization and location data processing are considered, in line with the\nemerging machine learning and big data methods. We first review (1)\nstate-of-the-art algorithms in the context of federated learning, (2) two\nwidely used learning models, namely the deep neural network model and the\nGaussian process model, and (3) various distributed model hyper-parameter\noptimization schemes. Then, we demonstrate various practical use cases that are\nsummarized from a mixture of standard, newly published, and unpublished works,\nwhich cover a broad range of location services, including collaborative static\nlocalization/fingerprinting, indoor target tracking, outdoor navigation using\nlow-sampling GPS, and spatio-temporal wireless traffic data modeling and\nprediction. Experimental results show that near centralized data fitting- and\nprediction performance can be achieved by a set of collaborative mobile users\nrunning distributed algorithms. All the surveyed use cases fall under our newly\nproposed Federated Localization (FedLoc) framework, which targets on\ncollaboratively building accurate location services without sacrificing user\nprivacy, in particular, sensitive information related to their geographical\ntrajectories. Future research directions are also discussed at the end of this\npaper.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 01:51:56 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 04:21:47 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Yin", "Feng", "", "Robert"], ["Lin", "Zhidi", "", "Robert"], ["Xu", "Yue", "", "Robert"], ["Kong", "Qinglei", "", "Robert"], ["Li", "Deshi", "", "Robert"], ["Theodoridis", "Sergios", "", "Robert"], ["Shuguang", "", "", "Robert"], ["Cui", "", ""]]}, {"id": "2003.03794", "submitter": "Philip Heinisch", "authors": "Philip Heinisch, Katharina Ostaszewski, Hendrik Ranocha", "title": "Towards Green Computing: A Survey of Performance and Energy Efficiency\n  of Different Platforms using OpenCL", "comments": null, "journal-ref": null, "doi": "10.1145/3388333.3403035", "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When considering different hardware platforms, not just the time-to-solution\ncan be of importance but also the energy necessary to reach it. This is not\nonly the case with battery powered and mobile devices but also with\nhigh-performance parallel cluster systems due to financial and practical limits\non power consumption and cooling. Recent developments in hard- and software\nhave given programmers the ability to run the same code on a range of different\ndevices giving rise to the concept of heterogeneous computing. Many of these\ndevices are optimized for certain types of applications. To showcase the\ndifferences and give a basic outlook on the applicability of different\narchitectures for specific problems, the cross-platform OpenCL framework was\nused to compare both time- and energy-to-solution. A large set of devices\nranging from ARM processors to server CPUs and consumer and enterprise level\nGPUs has been used with different benchmarking testcases taken from applied\nresearch applications. While the results show the overall advantages of GPUs in\nterms of both runtime and energy efficiency compared to CPUs, ARM devices show\npotential for certain applications in massively parallel systems. This study\nalso highlights how OpenCL enables the use of the same codebase on many\ndifferent systems and hardware platforms without specific code adaptations.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 15:06:27 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Heinisch", "Philip", ""], ["Ostaszewski", "Katharina", ""], ["Ranocha", "Hendrik", ""]]}, {"id": "2003.03810", "submitter": "Kaihua Qin", "authors": "Kaihua Qin, Liyi Zhou, Benjamin Livshits and Arthur Gervais", "title": "Attacking the DeFi Ecosystem with Flash Loans for Fun and Profit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Credit allows a lender to loan out surplus capital to a borrower. In the\ntraditional economy, credit bears the risk that the borrower may default on its\ndebt, the lender hence requires upfront collateral from the borrower, plus\ninterest fee payments. Due to the atomicity of blockchain transactions, lenders\ncan offer flash loans, i.e., loans that are only valid within one transaction\nand must be repaid by the end of that transaction. This concept has lead to a\nnumber of interesting attack possibilities, some of which were exploited in\nFebruary 2020.\n  This paper is the first to explore the implication of transaction atomicity\nand flash loans for the nascent decentralized finance (DeFi) ecosystem. We show\nquantitatively how transaction atomicity increases the arbitrage revenue. We\nmoreover analyze two existing attacks with ROIs beyond 500k%. We formulate\nfinding the attack parameters as an optimization problem over the state of the\nunderlying Ethereum blockchain and the state of the DeFi ecosystem. We show how\nmalicious adversaries can efficiently maximize an attack profit and hence\ndamage the DeFi ecosystem further. Specifically, we present how two previously\nexecuted attacks can be \"boosted\" to result in a profit of 829.5k USD and 1.1M\nUSD, respectively, which is a boost of 2.37x and 1.73x, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 16:52:34 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2020 20:28:58 GMT"}, {"version": "v3", "created": "Sat, 23 Jan 2021 17:13:59 GMT"}, {"version": "v4", "created": "Sat, 20 Mar 2021 10:42:45 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Qin", "Kaihua", ""], ["Zhou", "Liyi", ""], ["Livshits", "Benjamin", ""], ["Gervais", "Arthur", ""]]}, {"id": "2003.03837", "submitter": "Marcelo Fernandes", "authors": "Lucileide M. D. da Silva, Maria G. F. Coutinho, Carlos E. B. Santos,\n  Mailson R. Santos, Luiz Affonso Guedes, M. Dolores Ruiz, Marcelo A. C.\n  Fernandes", "title": "Hardware Architecture Proposal for TEDA algorithm to Data Streaming\n  Anomaly Detection", "comments": null, "journal-ref": null, "doi": "10.1109/ACCESS.2021.3098004", "report-no": null, "categories": "cs.DC cs.LG eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The amount of data in real-time, such as time series and streaming data,\navailable today continues to grow. Being able to analyze this data the moment\nit arrives can bring an immense added value. However, it also requires a lot of\ncomputational effort and new acceleration techniques. As a possible solution to\nthis problem, this paper proposes a hardware architecture for Typicality and\nEccentricity Data Analytic (TEDA) algorithm implemented on Field Programmable\nGate Arrays (FPGA) for use in data streaming anomaly detection. TEDA is based\non a new approach to outlier detection in the data stream context. In order to\nvalidate the proposals, results of the occupation and throughput of the\nproposed hardware are presented. Besides, the bit accurate simulation results\nare also presented. The project aims to Xilinx Virtex-6 xc6vlx240t-1ff1156 as\nthe target FPGA.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 19:28:53 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["da Silva", "Lucileide M. D.", ""], ["Coutinho", "Maria G. F.", ""], ["Santos", "Carlos E. B.", ""], ["Santos", "Mailson R.", ""], ["Guedes", "Luiz Affonso", ""], ["Ruiz", "M. Dolores", ""], ["Fernandes", "Marcelo A. C.", ""]]}, {"id": "2003.03931", "submitter": "Hao-Ran Yu", "authors": "Shenggan Cheng, Hao-Ran Yu, Derek Inman, Qiucheng Liao, Qiaoya Wu,\n  James Lin", "title": "CUBE -- Towards an Optimal Scaling of Cosmological N-body Simulations", "comments": "6 pages, 5 figures. Accepted for SCALE 2020, co-located as part of\n  the proceedings of CCGRID 2020", "journal-ref": "2020 20th IEEE/ACM International Symposium on Cluster, Cloud and\n  Internet Computing (CCGRID)", "doi": "10.1109/CCGrid49817.2020.00-22", "report-no": null, "categories": "physics.comp-ph astro-ph.CO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  N-body simulations are essential tools in physical cosmology to understand\nthe large-scale structure (LSS) formation of the Universe. Large-scale\nsimulations with high resolution are important for exploring the substructure\nof universe and for determining fundamental physical parameters like neutrino\nmass. However, traditional particle-mesh (PM) based algorithms use considerable\namounts of memory, which limits the scalability of simulations. Therefore, we\ndesigned a two-level PM algorithm CUBE towards optimal performance in memory\nconsumption reduction. By using the fixed-point compression technique, CUBE\nreduces the memory consumption per N-body particle toward 6 bytes, an order of\nmagnitude lower than the traditional PM-based algorithms. We scaled CUBE to 512\nnodes (20,480 cores) on an Intel Cascade Lake based supercomputer with\n$\\simeq$95\\% weak-scaling efficiency. This scaling test was performed in\n\"Cosmo-$\\pi$\" -- a cosmological LSS simulation using $\\simeq$4.4 trillion\nparticles, tracing the evolution of the universe over $\\simeq$13.7 billion\nyears. To our best knowledge, Cosmo-$\\pi$ is the largest completed cosmological\nN-body simulation. We believe CUBE has a huge potential to scale on exascale\nsupercomputers for larger simulations.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 05:57:27 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Cheng", "Shenggan", ""], ["Yu", "Hao-Ran", ""], ["Inman", "Derek", ""], ["Liao", "Qiucheng", ""], ["Wu", "Qiaoya", ""], ["Lin", "James", ""]]}, {"id": "2003.04150", "submitter": "Pulkit Misra", "authors": "Pulkit A. Misra, Srihari Radhakrishnan, Jeffrey S. Chase, Johannes\n  Gehrke, Alvin R. Lebeck", "title": "Lightweight Inter-transaction Caching with Precise Clocks and Dynamic\n  Self-invalidation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed, transactional storage systems scale by sharding data across\nservers. However, workload-induced hotspots result in contention, leading to\nhigher abort rates and performance degradation.\n  We present KAIROS, a transactional key-value storage system that leverages\nclient-side inter-transaction caching and sharded transaction validation to\nbalance the dynamic load and alleviate workload-induced hotspots in the system.\nKAIROS utilizes precise synchronized clocks to implement self-invalidating\nleases for cache consistency and avoids the overhead and potential hotspots due\nto maintaining sharing lists or sending invalidations.\n  Experiments show that inter-transaction caching alone provides 2.35x the\nthroughput of a baseline system with only intra-transaction caching; adding\nsharded validation further improves the throughput by a factor of 3.1 over\nbaseline. We also show that lease-based caching can operate at a 30% higher\nscale while providing 1.46x the throughput of the state-of-the-art explicit\ninvalidation-based caching.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 13:50:08 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Misra", "Pulkit A.", ""], ["Radhakrishnan", "Srihari", ""], ["Chase", "Jeffrey S.", ""], ["Gehrke", "Johannes", ""], ["Lebeck", "Alvin R.", ""]]}, {"id": "2003.04163", "submitter": "Newton Carlos Will", "authors": "Marciano da Rocha and Dalton C\\'ezane Gomes Valadares and Angelo\n  Perkusich and Kyller Costa Gorgonio and Rodrigo Tomaz Pagno and Newton Carlos\n  Will", "title": "Secure Cloud Storage with Client-Side Encryption Using a Trusted\n  Execution Environment", "comments": null, "journal-ref": null, "doi": "10.5220/0009130600310043", "report-no": null, "categories": "cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the evolution of computer systems, the amount of sensitive data to be\nstored as well as the number of threats on these data grow up, making the data\nconfidentiality increasingly important to computer users. Currently, with\ndevices always connected to the Internet, the use of cloud data storage\nservices has become practical and common, allowing quick access to such data\nwherever the user is. Such practicality brings with it a concern, precisely the\nconfidentiality of the data which is delivered to third parties for storage. In\nthe home environment, disk encryption tools have gained special attention from\nusers, being used on personal computers and also having native options in some\nsmartphone operating systems. The present work uses the data sealing, feature\nprovided by the Intel Software Guard Extensions (Intel SGX) technology, for\nfile encryption. A virtual file system is created in which applications can\nstore their data, keeping the security guarantees provided by the Intel SGX\ntechnology, before send the data to a storage provider. This way, even if the\nstorage provider is compromised, the data are safe. To validate the proposal,\nthe Cryptomator software, which is a free client-side encryption tool for cloud\nfiles, was integrated with an Intel SGX application (enclave) for data sealing.\nThe results demonstrate that the solution is feasible, in terms of performance\nand security, and can be expanded and refined for practical use and integration\nwith cloud synchronization services.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 14:18:57 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["da Rocha", "Marciano", ""], ["Valadares", "Dalton C\u00e9zane Gomes", ""], ["Perkusich", "Angelo", ""], ["Gorgonio", "Kyller Costa", ""], ["Pagno", "Rodrigo Tomaz", ""], ["Will", "Newton Carlos", ""]]}, {"id": "2003.04169", "submitter": "Yu Chen", "authors": "Seyed Yahya Nikouei, Yu Chen, Alexander Aved, Erik Blasch", "title": "I-ViSE: Interactive Video Surveillance as an Edge Service using\n  Unsupervised Feature Queries", "comments": "R1 is under review by the IEEE Internet of Things Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Situation AWareness (SAW) is essential for many mission critical\napplications. However, SAW is very challenging when trying to immediately\nidentify objects of interest or zoom in on suspicious activities from thousands\nof video frames. This work aims at developing a queryable system to instantly\nselect interesting content. While face recognition technology is mature, in\nmany scenarios like public safety monitoring, the features of objects of\ninterest may be much more complicated than face features. In addition, human\noperators may not be always able to provide a descriptive, simple, and accurate\nquery. Actually, it is more often that there are only rough, general\ndescriptions of certain suspicious objects or accidents. This paper proposes an\nInteractive Video Surveillance as an Edge service (I-ViSE) based on\nunsupervised feature queries. Adopting unsupervised methods that do not reveal\nany private information, the I-ViSE scheme utilizes general features of a human\nbody and color of clothes. An I-ViSE prototype is built following the edge-fog\ncomputing paradigm and the experimental results verified the I-ViSE scheme\nmeets the design goal of scene recognition in less than two seconds.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 14:26:45 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Nikouei", "Seyed Yahya", ""], ["Chen", "Yu", ""], ["Aved", "Alexander", ""], ["Blasch", "Erik", ""]]}, {"id": "2003.04216", "submitter": "Mehmet Emre Ozfatura", "authors": "Emre Ozfatura, Stefano Rini, Deniz Gunduz", "title": "Decentralized SGD with Over-the-Air Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.DC cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the performance of decentralized stochastic gradient descent (DSGD)\nin a wireless network, where the nodes collaboratively optimize an objective\nfunction using their local datasets. Unlike the conventional setting, where the\nnodes communicate over error-free orthogonal communication links, we assume\nthat transmissions are prone to additive noise and interference.We first\nconsider a point-to-point (P2P) transmission strategy, termed the OAC-P2P\nscheme, in which the node pairs are scheduled in an orthogonal fashion to\nminimize interference. Since in the DSGD framework, each node requires a linear\ncombination of the neighboring models at the consensus step, we then propose\nthe OAC-MAC scheme, which utilizes the signal superposition property of the\nwireless medium to achieve over-the-air computation (OAC). For both schemes, we\ncast the scheduling problem as a graph coloring problem. We numerically\nevaluate the performance of these two schemes for the MNIST image\nclassification task under various network conditions. We show that the OAC-MAC\nscheme attains better convergence performance with a fewer communication\nrounds.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 15:33:59 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Ozfatura", "Emre", ""], ["Rini", "Stefano", ""], ["Gunduz", "Deniz", ""]]}, {"id": "2003.04280", "submitter": "Gwena\\\"el Joret", "authors": "Vida Dujmovi\\'c, Louis Esperet, Gwena\\\"el Joret, Cyril Gavoille, Piotr\n  Micek, and Pat Morin", "title": "Adjacency Labelling for Planar Graphs (and Beyond)", "comments": "v4: referees' comments incorporated v3: minor changes v2: significant\n  revision v1: 35 pages; 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that there exists an adjacency labelling scheme for planar graphs\nwhere each vertex of an $n$-vertex planar graph $G$ is assigned a\n$(1+o(1))\\log_2 n$-bit label and the labels of two vertices $u$ and $v$ are\nsufficient to determine if $uv$ is an edge of $G$. This is optimal up to the\nlower order term and is the first such asymptotically optimal result. An\nalternative, but equivalent, interpretation of this result is that, for every\n$n$, there exists a graph $U_n$ with $n^{1+o(1)}$ vertices such that every\n$n$-vertex planar graph is an induced subgraph of $U_n$. These results\ngeneralize to bounded genus graphs, apex-minor-free graphs, bounded-degree\ngraphs from minor closed families, and $k$-planar graphs.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 17:39:55 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 17:03:15 GMT"}, {"version": "v3", "created": "Mon, 8 Jun 2020 19:41:42 GMT"}, {"version": "v4", "created": "Thu, 4 Feb 2021 19:39:41 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Dujmovi\u0107", "Vida", ""], ["Esperet", "Louis", ""], ["Joret", "Gwena\u00ebl", ""], ["Gavoille", "Cyril", ""], ["Micek", "Piotr", ""], ["Morin", "Pat", ""]]}, {"id": "2003.04293", "submitter": "Kornilios Kourtis", "authors": "Kornilios Kourtis and Martino Dazzi and Nikolas Ioannou and Tobias\n  Grosser and Abu Sebastian and Evangelos Eleftheriou", "title": "Compiling Neural Networks for a Computational Memory Accelerator", "comments": "Accepted at SPMA '20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational memory (CM) is a promising approach for accelerating inference\non neural networks (NN) by using enhanced memories that, in addition to storing\ndata, allow computations on them. One of the main challenges of this approach\nis defining a hardware/software interface that allows a compiler to map NN\nmodels for efficient execution on the underlying CM accelerator. This is a\nnon-trivial task because efficiency dictates that the CM accelerator is\nexplicitly programmed as a dataflow engine where the execution of the different\nNN layers form a pipeline. In this paper, we present our work towards a\nsoftware stack for executing ML models on such a multi-core CM accelerator. We\ndescribe an architecture for the hardware and software, and focus on the\nproblem of implementing the appropriate control logic so that data dependencies\nare respected. We propose a solution to the latter that is based on polyhedral\ncompilation.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 14:42:56 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 13:59:56 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Kourtis", "Kornilios", ""], ["Dazzi", "Martino", ""], ["Ioannou", "Nikolas", ""], ["Grosser", "Tobias", ""], ["Sebastian", "Abu", ""], ["Eleftheriou", "Evangelos", ""]]}, {"id": "2003.04294", "submitter": "Zheng Wang", "authors": "Peng Zhang, Jianbin Fang, Canqun Yang, Chun Huang, Tao Tang, Zheng\n  Wang", "title": "Optimizing Streaming Parallelism on Heterogeneous Many-Core\n  Architectures: A Machine Learning Based Approach", "comments": "Accepted to be published at IEEE TPDS. arXiv admin note: substantial\n  text overlap with arXiv:1802.02760", "journal-ref": null, "doi": "10.1109/TPDS.2020.2978045", "report-no": null, "categories": "cs.DC cs.LG cs.PF cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article presents an automatic approach to quickly derive a good solution\nfor hardware resource partition and task granularity for task-based parallel\napplications on heterogeneous many-core architectures. Our approach employs a\nperformance model to estimate the resulting performance of the target\napplication under a given resource partition and task granularity\nconfiguration. The model is used as a utility to quickly search for a good\nconfiguration at runtime. Instead of hand-crafting an analytical model that\nrequires expert insights into low-level hardware details, we employ machine\nlearning techniques to automatically learn it. We achieve this by first\nlearning a predictive model offline using training programs. The learnt model\ncan then be used to predict the performance of any unseen program at runtime.\nWe apply our approach to 39 representative parallel applications and evaluate\nit on two representative heterogeneous many-core platforms: a CPU-XeonPhi\nplatform and a CPU-GPU platform. Compared to the single-stream version, our\napproach achieves, on average, a 1.6x and 1.1x speedup on the XeonPhi and the\nGPU platform, respectively. These results translate to over 93% of the\nperformance delivered by a theoretically perfect predictor.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 21:18:21 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Zhang", "Peng", ""], ["Fang", "Jianbin", ""], ["Yang", "Canqun", ""], ["Huang", "Chun", ""], ["Tang", "Tao", ""], ["Wang", "Zheng", ""]]}, {"id": "2003.04345", "submitter": "Yusaku Yamamoto Dr.", "authors": "Tsubasa Sakai, Shuhei Kudo, Hiroto Imachi, Yuto Miyatake, Takeo Hoshi\n  and Yusaku Yamamoto", "title": "A Parallelizable Energy-Preserving Integrator MB4 and Its Application to\n  Quantum-Mechanical Wavepacket Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In simulating physical systems, conservation of the total energy is often\nessential, especially when energy conversion between different forms of energy\noccurs frequently. Recently, a new fourth order energy-preserving integrator\nnamed MB4 was proposed based on the so-called continuous stage Runge--Kutta\nmethods (Y.~Miyatake and J.~C.~Butcher, SIAM J.~Numer.~Anal., 54(3),\n1993-2013). A salient feature of this method is that it is parallelizable,\nwhich makes its computational time for one time step comparable to that of\nsecond order methods. In this paper, we illustrate how to apply the MB4 method\nto a concrete ordinary differential equation using the nonlinear\nSchr\\\"{o}dinger-type equation on a two-dimensional grid as an example. This\nsystem is a prototypical model of two-dimensional disordered organic material\nand is difficult to solve with standard methods like the classical Runge--Kutta\nmethods due to the nonlinearity and the $\\delta$-function like potential coming\nfrom defects. Numerical tests show that the method can solve the equation\nstably and preserves the total energy to 16-digit accuracy throughout the\nsimulation. It is also shown that parallelization of the method yields up to\n2.8 times speedup using 3 computational nodes.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 18:18:48 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Sakai", "Tsubasa", ""], ["Kudo", "Shuhei", ""], ["Imachi", "Hiroto", ""], ["Miyatake", "Yuto", ""], ["Hoshi", "Takeo", ""], ["Yamamoto", "Yusaku", ""]]}, {"id": "2003.04364", "submitter": "David Grimsman", "authors": "Haoyuan Sun, David Grimsman, Jason R Marden", "title": "Distributed Submodular Maximization with Parallel Execution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MA cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The submodular maximization problem is widely applicable in many engineering\nproblems where objectives exhibit diminishing returns. While this problem is\nknown to be NP-hard for certain subclasses of objective functions, there is a\ngreedy algorithm which guarantees approximation at least 1/2 of the optimal\nsolution. This greedy algorithm can be implemented with a set of agents, each\nmaking a decision sequentially based on the choices of all prior agents. In\nthis paper, we consider a generalization of the greedy algorithm in which\nagents can make decisions in parallel, rather than strictly in sequence. In\nparticular, we are interested in partitioning the agents, where a set of agents\nin the partition all make a decision simultaneously based on the choices of\nprior agents, so that the algorithm terminates in limited iterations. We\nprovide bounds on the performance of this parallelized version of the greedy\nalgorithm and show that dividing the agents evenly among the sets in the\npartition yields an optimal structure. We additionally show that this optimal\nstructure is still near-optimal when the objective function exhibits a certain\nmonotone property. Lastly, we show that the same performance guarantees can be\nachieved in the parallelized greedy algorithm even when agents can only observe\nthe decisions of a subset of prior agents.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 19:08:20 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 16:28:05 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Sun", "Haoyuan", ""], ["Grimsman", "David", ""], ["Marden", "Jason R", ""]]}, {"id": "2003.04470", "submitter": "Vuong M. Ngo", "authors": "V.M. Ngo, N.A. Le-Khac, and M.T. Kechadi", "title": "Data Warehouse and Decision Support on Integrated Crop Big Data", "comments": "13 pages, 11 figures. arXiv admin note: text overlap with\n  arXiv:1905.12411", "journal-ref": "International Journal of Business Process Integration and\n  Management 2020 Vol.10 No.1", "doi": "10.1504/IJBPIM.2020.113115", "report-no": null, "categories": "cs.DB cs.DC cs.LG cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, precision agriculture is becoming very popular. The\nintroduction of modern information and communication technologies for\ncollecting and processing Agricultural data revolutionise the agriculture\npractises. This has started a while ago (early 20th century) and it is driven\nby the low cost of collecting data about everything; from information on fields\nsuch as seed, soil, fertiliser, pest, to weather data, drones and satellites\nimages. Specially, the agricultural data mining today is considered as Big Data\napplication in terms of volume, variety, velocity and veracity. Hence it leads\nto challenges in processing vast amounts of complex and diverse information to\nextract useful knowledge for the farmer, agronomist, and other businesses. It\nis a key foundation to establishing a crop intelligence platform, which will\nenable efficient resource management and high quality agronomy decision making\nand recommendations. In this paper, we designed and implemented a continental\nlevel agricultural data warehouse (ADW). ADW is characterised by its (1)\nflexible schema; (2) data integration from real agricultural multi datasets;\n(3) data science and business intelligent support; (4) high performance; (5)\nhigh storage; (6) security; (7) governance and monitoring; (8) consistency,\navailability and partition tolerant; (9) cloud compatibility. We also evaluate\nthe performance of ADW and present some complex queries to extract and return\nnecessary knowledge about crop management.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 00:10:22 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 08:45:11 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Ngo", "V. M.", ""], ["Le-Khac", "N. A.", ""], ["Kechadi", "M. T.", ""]]}, {"id": "2003.04510", "submitter": "Jung Ho Ahn", "authors": "Wonkyung Jung, Eojin Lee, Sangpyo Kim, Keewoo Lee, Namhoon Kim,\n  Chohong Min, Jung Hee Cheon, Jung Ho Ahn", "title": "HEAAN Demystified: Accelerating Fully Homomorphic Encryption Through\n  Architecture-centric Analysis and Optimization", "comments": null, "journal-ref": "IEEE Access 2021", "doi": "10.1109/ACCESS.2021.3096189", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Homomorphic Encryption (HE) draws a significant attention as a\nprivacy-preserving way for cloud computing because it allows computation on\nencrypted messages called ciphertexts. Among numerous HE schemes proposed, HE\nfor Arithmetic of Approximate Numbers (HEAAN) is rapidly gaining popularity\nacross a wide range of applications because it supports messages that can\ntolerate approximate computation with no limit on the number of arithmetic\noperations applicable to the corresponding ciphertexts. A critical shortcoming\nof HE is the high computation complexity of ciphertext arithmetic; especially,\nHE multiplication (HE Mul) is more than 10,000 times slower than the\ncorresponding multiplication between unencrypted messages. This leads to a\nlarge body of HE acceleration studies, including ones exploiting FPGAs;\nhowever, those did not conduct a rigorous analysis of computational complexity\nand data access patterns of HE Mul. Moreover, the proposals mostly focused on\ndesigns with small parameter sizes, making it difficult to accurately estimate\ntheir performance in conducting a series of complex arithmetic operations. In\nthis paper, we first describe how HE Mul of HEAAN is performed in a manner\nfriendly to computer architects. Then we conduct a disciplined analysis on its\ncomputational and memory access characteristics, through which we (1) extract\nparallelism in the key functions composing HE Mul and (2) demonstrate how to\neffectively map the parallelism to the popular parallel processing platforms,\nmulticore CPUs and GPUs, by applying a series of optimization techniques such\nas transposing matrices and pinning data to threads. This leads to the\nperformance improvement of HE Mul on a CPU and a GPU by 42.9x and 134.1x,\nrespectively, over the single-thread reference HEAAN running on a CPU. The\nconducted analysis and optimization would set a new foundation for future HE\nacceleration research.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 02:38:34 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Jung", "Wonkyung", ""], ["Lee", "Eojin", ""], ["Kim", "Sangpyo", ""], ["Lee", "Keewoo", ""], ["Kim", "Namhoon", ""], ["Min", "Chohong", ""], ["Cheon", "Jung Hee", ""], ["Ahn", "Jung Ho", ""]]}, {"id": "2003.04544", "submitter": "Dingzhu Wen", "authors": "Dingzhu Wen, Mehdi Bennis, and Kaibin Huang", "title": "Joint Parameter-and-Bandwidth Allocation for Improving the Efficiency of\n  Partitioned Edge Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To leverage data and computation capabilities of mobile devices, machine\nlearning algorithms are deployed at the network edge for training artificial\nintelligence (AI) models, resulting in the new paradigm of edge learning. In\nthis paper, we consider the framework of partitioned edge learning for\niteratively training a large-scale model using many resource-constrained\ndevices (called workers). To this end, in each iteration, the model is\ndynamically partitioned into parametric blocks, which are downloaded to worker\ngroups for updating using data subsets. Then, the local updates are uploaded to\nand cascaded by the server for updating a global model. To reduce resource\nusage by minimizing the total learning-and-communication latency, this work\nfocuses on the novel joint design of parameter (computation load) allocation\nand bandwidth allocation (for downloading and uploading). Two design approaches\nare adopted. First, a practical sequential approach, called partially\nintegrated parameter-and-bandwidth allocation (PABA), yields two schemes,\nnamely bandwidth aware parameter allocation and parameter aware bandwidth\nallocation. The former minimizes the load for the slowest (in computing) of\nworker groups, each training a same parametric block. The latter allocates the\nlargest bandwidth to the worker being the latency bottleneck. Second, PABA are\njointly optimized. Despite its being a nonconvex problem, an efficient and\noptimal solution algorithm is derived by intelligently nesting a bisection\nsearch and solving a convex problem. Experimental results using real data\ndemonstrate that integrating PABA can substantially improve the performance of\npartitioned edge learning in terms of latency (by e.g., 46%) and accuracy (by\ne.g., 4%).\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 05:52:15 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2020 02:00:56 GMT"}, {"version": "v3", "created": "Tue, 30 Jun 2020 01:47:27 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Wen", "Dingzhu", ""], ["Bennis", "Mehdi", ""], ["Huang", "Kaibin", ""]]}, {"id": "2003.04565", "submitter": "Ois\\'in Creaner", "authors": "Ois\\'n Creaner, Kevin Nolan, John Walsh, Eugene Hickey", "title": "The Locus Algorithm III: A Grid Computing system to generate catalogues\n  of optimised pointings for Differential Photometry", "comments": "12 Pages, 9 Figures, 1 reference corrected, cross-references for\n  arxiv links updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the hardware and software components of the Grid\nComputing system used to implement the Locus Algorithm to identify optimum\npointings for differential photometry of 61,662,376 stars and 23,799 quasars.\nThe scale of the data, together with initial operational assessments demanded a\nHigh Performance Computing (HPC) system to complete the data analysis. Grid\ncomputing was chosen as the HPC solution as the optimum choice available within\nthis project. The physical and logical structure of the National Grid computing\nInfrastructure informed the approach that was taken. That approach was one of\nlayered separation of the different project components to enable maximum\nflexibility and extensibility.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 08:10:08 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2020 08:53:36 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Creaner", "Ois\u0144", ""], ["Nolan", "Kevin", ""], ["Walsh", "John", ""], ["Hickey", "Eugene", ""]]}, {"id": "2003.04574", "submitter": "Ois\\'in Creaner", "authors": "Kevin Nolan, Eugene Hickey, Ois\\'in Creaner", "title": "The Locus Algorithm II: A robust software system to maximise the quality\n  of fields of view for Differential Photometry", "comments": "11 Pages, 13 Figures, Updated arxiv references with URLs for other\n  papers in series", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the software system developed to implement the Locus Algorithm, a\nnovel algorithm designed to maximise the performance of differential photometry\nsystems by optimising the number and quality of reference stars in the Field of\nView with the target. Firstly, we state the design requirements, constraints\nand ambitions for the software system required to implement this algorithm.\nThen, a detailed software design is presented for the system in operation.\nNext, the data design including file structures used and the data environment\nrequired for the system are defined. Finally, we conclude by illustrating the\nscaling requirements which mandate a high-performance computing implementation\nof this system, which is discussed in the other papers in this series.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 08:38:25 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2020 08:44:52 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Nolan", "Kevin", ""], ["Hickey", "Eugene", ""], ["Creaner", "Ois\u00edn", ""]]}, {"id": "2003.04590", "submitter": "Ois\\'in Creaner", "authors": "Ois\\'in Creaner, Kevin Nolan, David Grennan, Niall Smith and Eugene\n  Hickey", "title": "A Catalogue of Locus Algorithm Pointings for Optimal Differential\n  Photometry for 23,779 Quasars", "comments": "7 pages, 5 figures, cross-references to other papers in the series\n  edited with arxiv links", "journal-ref": null, "doi": "10.1093/mnras/staa2494", "report-no": null, "categories": "astro-ph.GA astro-ph.IM cs.DC cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a catalogue of optimised pointings for differential\nphotometry of 23,779 quasars extracted from the Sloan Digital Sky Survey (SDSS)\nCatalogue and a score for each indicating the quality of the Field of View\n(FoV) associated with that pointing. Observation of millimagnitude variability\non a timescale of minutes typically requires differential observations with\nreference to an ensemble of reference stars. For optimal performance, these\nreference stars should have similar colour and magnitude to the target quasar.\nIn addition, the greatest quantity and quality of suitable reference stars may\nbe found by using a telescope pointing which offsets the target object from the\ncentre of the field of view. By comparing each quasar with the stars which\nappear close to it on the sky in the SDSS Catalogue, an optimum pointing can be\ncalculated, and a figure of merit, referred to as the \"score\" calculated for\nthat pointing. Highly flexible software has been developed to enable this\nprocess to be automated and implemented in a distributed computing paradigm,\nwhich enables the creation of catalogues of pointings given a set of input\ntargets. Applying this technique to a sample of 40,000 targets from the 4th\nSDSS quasar catalogue resulted in the production of pointings and scores for\n23,779 quasars. This catalogue is a useful resource for observers planning\ndifferential photometry studies and surveys of quasars to select those which\nhave many suitable celestial neighbours for differential photometry\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 09:08:10 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2020 08:25:33 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Creaner", "Ois\u00edn", ""], ["Nolan", "Kevin", ""], ["Grennan", "David", ""], ["Smith", "Niall", ""], ["Hickey", "Eugene", ""]]}, {"id": "2003.04686", "submitter": "Hossein Shokri Ghadikolaei", "authors": "Hossein S. Ghadikolaei and Sindri Magnusson", "title": "Communication-efficient Variance-reduced Stochastic Gradient Descent", "comments": "IFAC World Congress 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of communication efficient distributed optimization\nwhere multiple nodes exchange important algorithm information in every\niteration to solve large problems. In particular, we focus on the stochastic\nvariance-reduced gradient and propose a novel approach to make it\ncommunication-efficient. That is, we compress the communicated information to a\nfew bits while preserving the linear convergence rate of the original\nuncompressed algorithm. Comprehensive theoretical and numerical analyses on\nreal datasets reveal that our algorithm can significantly reduce the\ncommunication complexity, by as much as 95\\%, with almost no noticeable\npenalty. Moreover, it is much more robust to quantization (in terms of\nmaintaining the true minimizer and the convergence rate) than the\nstate-of-the-art algorithms for solving distributed optimization problems. Our\nresults have important implications for using machine learning over\ninternet-of-things and mobile networks.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 13:22:16 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Ghadikolaei", "Hossein S.", ""], ["Magnusson", "Sindri", ""]]}, {"id": "2003.04776", "submitter": "Mirko Myllykoski", "authors": "Carl Christian Kjelgaard Mikkelsen and Mirko Myllykoski", "title": "Parallel Robust Computation of Generalized Eigenvectors of Matrix\n  Pencils", "comments": "This manuscript was accepted to 13th International Conference on\n  Parallel Processing and Applied Mathematics (PPAM2019), Bialystok, Poland,\n  September 8-11, 2019. The final authenticated version is available online at\n  https://doi.org/10.1007/978-3-030-43229-4_6 (DOI valid from May 8, 2020\n  onward). First author's first name is \"Carl Christian\" and last name\n  \"Kjelgaard Mikkelsen\"", "journal-ref": "LNCS 12043 (2020) 58-69", "doi": "10.1007/978-3-030-43229-4_6", "report-no": null, "categories": "cs.MS cs.DC cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of computing generalized eigenvectors\nof a matrix pencil in real Schur form. In exact arithmetic, this problem can be\nsolved using substitution. In practice, substitution is vulnerable to\nfloating-point overflow. The robust solvers xTGEVC in LAPACK prevent overflow\nby dynamically scaling the eigenvectors. These subroutines are sequential\nscalar codes which compute the eigenvectors one by one. In this paper we\ndiscuss how to derive robust blocked algorithms. The new StarNEig library\ncontains a robust task-parallel solver Zazamoukh which runs on top of StarPU.\nOur numerical experiments show that Zazamoukh achieves a super-linear speedup\ncompared with DTGEVC for sufficiently large matrices.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 14:37:39 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Mikkelsen", "Carl Christian Kjelgaard", ""], ["Myllykoski", "Mirko", ""]]}, {"id": "2003.04824", "submitter": "Dmitry Duplyakin", "authors": "Dmitry Duplyakin and Alexandru Uta and Aleksander Maricq and Robert\n  Ricci", "title": "In Datacenter Performance, The Only Constant Is Change", "comments": "To be presented at the 20th IEEE/ACM International Symposium on\n  Cluster, Cloud and Internet Computing (CCGrid,\n  http://cloudbus.org/ccgrid2020/) on May 11-14, 2020 in Melbourne, Victoria,\n  Australia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  All computing infrastructure suffers from performance variability, be it\nbare-metal or virtualized. This phenomenon originates from many sources: some\ntransient, such as noisy neighbors, and others more permanent but sudden, such\nas changes or wear in hardware, changes in the underlying hypervisor stack, or\neven undocumented interactions between the policies of the computing resource\nprovider and the active workloads. Thus, performance measurements obtained on\nclouds, HPC facilities, and, more generally, datacenter environments are almost\nguaranteed to exhibit performance regimes that evolve over time, which leads to\nundesirable nonstationarities in application performance. In this paper, we\npresent our analysis of performance of the bare-metal hardware available on the\nCloudLab testbed where we focus on quantifying the evolving performance regimes\nusing changepoint detection. We describe our findings, backed by a dataset with\nnearly 6.9M benchmark results collected from over 1600 machines over a period\nof 2 years and 9 months. These findings yield a comprehensive characterization\nof real-world performance variability patterns in one computing facility, a\nmethodology for studying such patterns on other infrastructures, and contribute\nto a better understanding of performance variability in general.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 16:04:33 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Duplyakin", "Dmitry", ""], ["Uta", "Alexandru", ""], ["Maricq", "Aleksander", ""], ["Ricci", "Robert", ""]]}, {"id": "2003.04826", "submitter": "Syed Mohammed Arshad Zaidi", "authors": "Anuj Sharma, Syed Mohammed Arshad Zaidi", "title": "Optimizations to the Parallel Breath First Search on Distributed Memory", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs and their traversal is becoming significant as it is applicable to\nvarious areas of mathematics, science and technology. Various problems in\nfields as varied as biochemistry (genomics), electrical engineering\n(communication networks), computer science (algorithms and computation) can be\nmodeled as Graph problems. Real world scenarios including communities their\ninterconnections and related properties can be studied using graphs. So fast,\nscalable, low-cost execution of parallel graph algorithms is very important. In\nthis implementation of parallel breadth first search of graphs, we implemented\nParallel BFS algorithm with 1-D partitioning of graph as described in [2] and\nhave reduced execution time by optimizing communication for local buffers.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 16:11:46 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Sharma", "Anuj", ""], ["Zaidi", "Syed Mohammed Arshad", ""]]}, {"id": "2003.04892", "submitter": "Yatin Manerkar", "authors": "Yatin A. Manerkar, Daniel Lustig, Margaret Martonosi", "title": "RealityCheck: Bringing Modularity, Hierarchy, and Abstraction to\n  Automated Microarchitectural Memory Consistency Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern SoCs are heterogeneous parallel systems comprised of components\ndeveloped by distinct teams and possibly even different vendors. The memory\nconsistency model (MCM) of processors in such SoCs specifies the ordering rules\nwhich constrain the values that can be read by load instructions in parallel\nprograms running on such systems. The implementation of required MCM orderings\ncan span components which may be designed and implemented by many different\nteams. Ideally, each team would be able to specify the orderings enforced by\ntheir components independently and then connect them together when conducting\nMCM verification. However, no prior automated approach for formal hardware MCM\nverification provided this.\n  To bring automated hardware MCM verification in line with the realities of\nthe design process, we present RealityCheck, a methodology and tool for\nautomated formal MCM verification of modular microarchitectural ordering\nspecifications. RealityCheck allows users to specify their designs as a\nhierarchy of distinct modules connected to each other rather than a single flat\nspecification. It can then automatically verify litmus test programs against\nthese modular specifications. RealityCheck also provides support for\nabstraction, which enables scalable verification by breaking up the\nverification of the entire design into smaller verification problems. We\npresent results for verifying litmus tests on 7 different designs using\nRealityCheck. These include in-order and out-of-order pipelines, a non-blocking\ncache, and a heterogeneous processor. Our case studies cover the TSO and RISC-V\n(RVWMO) weak memory models. RealityCheck is capable of verifying 98 RVWMO\nlitmus tests in under 4 minutes each, and its capability for abstraction\nenables up to a 32.1% reduction in litmus test verification time for RVWMO.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 22:48:37 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Manerkar", "Yatin A.", ""], ["Lustig", "Daniel", ""], ["Martonosi", "Margaret", ""]]}, {"id": "2003.04915", "submitter": "Renan Souza", "authors": "Raphael Thiago, Renan Souza, L. Azevedo, E. Soares, Rodrigo Santos,\n  Wallas Santos, Max De Bayser, M. Cardoso, M. Moreno, and Renato Cerqueira", "title": "Managing Data Lineage of O&G Machine Learning Models: The Sweet Spot for\n  Shale Use Case", "comments": "Author preprint of paper accepted at the 2020 European Association of\n  Geoscientists and Engineers (EAGE) Digitalization Conference and Exhibition", "journal-ref": "2020 European Association of Geoscientists and Engineers (EAGE)\n  Digitalization Conference and Exhibition", "doi": null, "report-no": null, "categories": "cs.DB cs.CY cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning (ML) has increased its role, becoming essential in several\nindustries. However, questions around training data lineage, such as \"where has\nthe dataset used to train this model come from?\"; the introduction of several\nnew data protection legislation; and, the need for data governance\nrequirements, have hindered the adoption of ML models in the real world. In\nthis paper, we discuss how data lineage can be leveraged to benefit the ML\nlifecycle to build ML models to discover sweet-spots for shale oil and gas\nproduction, a major application in the Oil and Gas O&G Industry.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 18:10:16 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Thiago", "Raphael", ""], ["Souza", "Renan", ""], ["Azevedo", "L.", ""], ["Soares", "E.", ""], ["Santos", "Rodrigo", ""], ["Santos", "Wallas", ""], ["De Bayser", "Max", ""], ["Cardoso", "M.", ""], ["Moreno", "M.", ""], ["Cerqueira", "Renato", ""]]}, {"id": "2003.04969", "submitter": "Shantanu Sharma", "authors": "Nisha Panwar, Shantanu Sharma, Peeyush Gupta, Dhrubajyoti Ghosh,\n  Sharad Mehrotra, Nalini Venkatasubramanian", "title": "IoT Expunge: Implementing Verifiable Retention of IoT Data", "comments": "This paper has been accepted in 10th ACM Conference on Data and\n  Application Security and Privacy (CODASPY), 2020", "journal-ref": null, "doi": "10.1145/3374664.3375737", "report-no": null, "categories": "cs.CR cs.DB cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing deployment of Internet of Things (IoT) systems aims to ease the\ndaily life of end-users by providing several value-added services. However, IoT\nsystems may capture and store sensitive, personal data about individuals in the\ncloud, thereby jeopardizing user-privacy. Emerging legislation, such as\nCalifornia's CalOPPA and GDPR in Europe, support strong privacy laws to protect\nan individual's data in the cloud. One such law relates to strict enforcement\nof data retention policies. This paper proposes a framework, entitled IoT\nExpunge that allows sensor data providers to store the data in cloud platforms\nthat will ensure enforcement of retention policies. Additionally, the cloud\nprovider produces verifiable proofs of its adherence to the retention policies.\nExperimental results on a real-world smart building testbed show that IoT\nExpunge imposes minimal overheads to the user to verify the data against data\nretention policies.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 20:55:01 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Panwar", "Nisha", ""], ["Sharma", "Shantanu", ""], ["Gupta", "Peeyush", ""], ["Ghosh", "Dhrubajyoti", ""], ["Mehrotra", "Sharad", ""], ["Venkatasubramanian", "Nalini", ""]]}, {"id": "2003.05000", "submitter": "Tao Gu", "authors": "Zheng Yang, Bin Xu, Jingyao Dai, Tao Gu", "title": "PAS: Prediction-based Adaptive Sleeping for Environment Monitoring in\n  Sensor Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy efficiency has proven to be an important factor dominating the working\nperiod of WSN surveillance systems. Intensive studies have been done to provide\nenergy efficient power management mechanisms. In this paper, we present PAS, a\nPrediction-based Adaptive Sleeping mechanism for environment monitoring sensor\nnetworks to conserve energy. PAS focuses on the diffusion stimulus (DS)\nscenario, which is very common and important in the application of environment\nmonitoring. Different with most of previous works, PAS explores the features of\nDS spreading process to obtain higher energy efficiency. In PAS, sensors\ndetermine their sleeping schedules based on the observed emergency of DS\nspreading. While sensors near the DS boundary stay awake to accurately capture\nthe possible stimulus arrival, the far away sensors turn into sleeping mode to\nconserve energy. Simulation experiment shows that PAS largely reduces the\nenergy cost without decreasing system performance\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 12:52:36 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Yang", "Zheng", ""], ["Xu", "Bin", ""], ["Dai", "Jingyao", ""], ["Gu", "Tao", ""]]}, {"id": "2003.05001", "submitter": "Tao Gu", "authors": "Tao Gu, Hung Keng Pung, Daqing Zhang", "title": "A Hierarchical Semantic Overlay for P2P Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a hierarchical semantic overlay network for\nsearching heterogeneous data over wide-area networks. In this system, data are\nrepresented as RDF triples based on ontologies. Peers that have the same\nsemantics are organized into a semantic cluster, and the semantic clusters are\nself-organized into a one-dimensional ring space to form the toplevel semantic\noverlay network. Each semantic cluster has its low-level overlay network which\ncan be built using an unstructured overlay or a DHT-based overlay. A search is\nfirst forwarded to the appropriate semantic cluster, and then routed to the\nspecific peers that hold the relevant data using a parallel flooding algorithm\nor a DHT-based routing algorithm. By combining the advantages of both\nunstructured and structured overlay networks, we are able to achieve a better\ntradeoff in terms of search efficiency, search cost and overlay maintenance\ncost.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 13:01:24 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Gu", "Tao", ""], ["Pung", "Hung Keng", ""], ["Zhang", "Daqing", ""]]}, {"id": "2003.05055", "submitter": "Tao Gu", "authors": "Tao Gu, Xiao Hang Wang, Hung Keng Pung, Da Qing Zhang", "title": "An Ontology-based Context Model in Intelligent Environments", "comments": "arXiv admin note: text overlap with arXiv:0906.3925 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing becomes increasingly mobile and pervasive today; these changes\nimply that applications and services must be aware of and adapt to their\nchanging contexts in highly dynamic environments. Today, building context-aware\nsystems is a complex task due to lack of an appropriate infrastructure support\nin intelligent environments. A context-aware infrastructure requires an\nappropriate context model to represent, manipulate and access context\ninformation. In this paper, we propose a formal context model based on ontology\nusing OWL to address issues including semantic context representation, context\nreasoning and knowledge sharing, context classification, context dependency and\nquality of context. The main benefit of this model is the ability to reason\nabout various contexts. Based on our context model, we also present a\nService-Oriented Context-Aware Middleware (SOCAM) architecture for building of\ncontext-aware services.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 12:15:20 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Gu", "Tao", ""], ["Wang", "Xiao Hang", ""], ["Pung", "Hung Keng", ""], ["Zhang", "Da Qing", ""]]}, {"id": "2003.05113", "submitter": "Senthil Nathan", "authors": "Parth Thakkar, Senthilnathan Natarajan", "title": "Scaling Hyperledger Fabric Using Pipelined Execution and Sparse Peers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Permissioned blockchains are becoming popular as data management systems in\nthe enterprise setting. Compared to traditional distributed databases,\nblockchain platforms provide increased security guarantees but significantly\nlower performance. Further, these platforms are quite expensive to run for the\nlow throughput they provide. The following are two ways to improve performance\nand reduce cost: (1) make the system utilize allocated resources efficiently;\n(2) allow rapid and dynamic scaling of allocated resources based on load. We\nexplore both of these in this work.\n  We first investigate the reasons for the poor performance and scalability of\nthe dominant permissioned blockchain flavor called Execute-Order-Validate\n(EOV). We do this by studying the scaling characteristics of Hyperledger\nFabric, a popular EOV platform, using vertical scaling and horizontal scaling.\nWe find that the transaction throughput scales very poorly with these\ntechniques. At least in the permissioned setting, the real bottleneck is\ntransaction processing, not the consensus protocol. With vertical scaling, the\nallocated vCPUs go under-utilized. In contrast, with horizontal scaling, the\nallocated resources get wasted due to redundant work across nodes within an\norganization.\n  To mitigate the above concerns, we first improve resource efficiency by (a)\nimproving CPU utilization with a pipelined execution of validation & commit\nphases; (b) avoiding redundant work across nodes by introducing a new type of\npeer node called sparse peer that selectively commits transactions. We further\npropose a technique that enables the rapid scaling of resources. Our\nimplementation - SmartFabric, built on top of Hyperledger Fabric demonstrates\n3x higher throughput, 12-26x faster scale-up time, and provides Fabric's\nthroughput at 50% to 87% lower cost.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 05:02:12 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 06:36:48 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Thakkar", "Parth", ""], ["Natarajan", "Senthilnathan", ""]]}, {"id": "2003.05324", "submitter": "Sameh Abdulah", "authors": "Sameh Abdulah, Hatem Ltaief, Ying Sun, Marc G. Genton, and David E.\n  Keyes", "title": "Geostatistical Modeling and Prediction Using Mixed-Precision Tile\n  Cholesky Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geostatistics represents one of the most challenging classes of scientific\napplications due to the desire to incorporate an ever increasing number of\ngeospatial locations to accurately model and predict environmental phenomena.\nFor example, the evaluation of the Gaussian log-likelihood function, which\nconstitutes the main computational phase, involves solving systems of linear\nequations with a large dense symmetric and positive definite covariance matrix.\nCholesky, the standard algorithm, requires O(n^3) floating point operators and\nhas an O(n^2) memory footprint, where n is the number of geographical\nlocations. Here, we present a mixed-precision tile algorithm to accelerate the\nCholesky factorization during the log-likelihood function evaluation. Under an\nappropriate ordering, it operates with double-precision arithmetic on tiles\naround the diagonal, while reducing to single-precision arithmetic for tiles\nsufficiently far off. This translates into an improvement of the performance\nwithout any deterioration of the numerical accuracy of the application. We rely\non the StarPU dynamic runtime system to schedule the tasks and to overlap them\nwith data movement. To assess the performance and the accuracy of the proposed\nmixed-precision algorithm, we use synthetic and real datasets on various shared\nand distributed-memory systems possibly equipped with hardware accelerators. We\ncompare our mixed-precision Cholesky factorization against the double-precision\nreference implementation as well as an independent block approximation method.\nWe obtain an average of 1.6X performance speedup on massively parallel\narchitectures while maintaining the accuracy necessary for modeling and\nprediction.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 08:00:17 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Abdulah", "Sameh", ""], ["Ltaief", "Hatem", ""], ["Sun", "Ying", ""], ["Genton", "Marc G.", ""], ["Keyes", "David E.", ""]]}, {"id": "2003.05361", "submitter": "Pratik Nayak", "authors": "Pratik Nayak, Terry Cojean, Hartwig Anzt", "title": "Evaluating Abstract Asynchronous Schwarz solvers on GPUs", "comments": "Preprint submitted to IJHPCA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the commencement of the exascale computing era, we realize that the\nmajority of the leadership supercomputers are heterogeneous and massively\nparallel even on a single node with multiple co-processors such as GPUs and\nmultiple cores on each node. For example, ORNLs Summit accumulates six NVIDIA\nTesla V100s and 42 core IBM Power9s on each node. Synchronizing across all\nthese compute resources in a single node or even across multiple nodes is\nprohibitively expensive. Hence it is necessary to develop and study\nasynchronous algorithms that circumvent this issue of bulk-synchronous\ncomputing for massive parallelism. In this study, we examine the asynchronous\nversion of the abstract Restricted Additive Schwarz method as a solver where we\ndo not explicitly synchronize, but allow for communication of the data between\nthe sub-domains to be completely asynchronous thereby removing the bulk\nsynchronous nature of the algorithm.\n  We accomplish this by using the onesided RMA functions of the MPI standard.\nWe study the benefits of using such an asynchronous solver over its synchronous\ncounterpart on both multi-core architectures and on multiple GPUs. We also\nstudy the communication patterns and local solvers and their effect on the\nglobal solver. Finally, we show that this concept can render attractive runtime\nbenefits over the synchronous counterparts.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 15:28:53 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 12:14:52 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Nayak", "Pratik", ""], ["Cojean", "Terry", ""], ["Anzt", "Hartwig", ""]]}, {"id": "2003.05542", "submitter": "Matthias F\\\"ugger", "authors": "Johannes Bund, Matthias F\\\"ugger, Christoph Lenzen, Moti Medina, Will\n  Rosenbaum", "title": "PALS: Plesiochronous and Locally Synchronous Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider an arbitrary network of communicating modules on a chip, each\nrequiring a local signal telling it when to execute a computational step. There\nare three common solutions to generating such a local clock signal: (i) by\nderiving it from a single, central clock source, (ii) by local, free-running\noscillators, or (iii) by handshaking between neighboring modules. Conceptually,\neach of these solutions is the result of a perceived dichotomy in which\n(sub)systems are either clocked or fully asynchronous, suggesting that the\ndesigner's choice is limited to deciding where to draw the line between\nsynchronous and asynchronous design. In contrast, we take the view that the\nbetter question to ask is how synchronous the system can and should be. Based\non a distributed clock synchronization algorithm, we present a novel design\nproviding modules with local clocks whose frequency bounds are almost as good\nas those of corresponding free-running oscillators, yet neighboring modules are\nguaranteed to have a phase offset substantially smaller than one clock cycle.\nConcretely, parameters obtained from a 15nm ASIC implementation running at 2GHz\nyield mathematical worst-case bounds of 30ps on phase offset for a 32x32 node\ngrid network.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 22:24:44 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Bund", "Johannes", ""], ["F\u00fcgger", "Matthias", ""], ["Lenzen", "Christoph", ""], ["Medina", "Moti", ""], ["Rosenbaum", "Will", ""]]}, {"id": "2003.05575", "submitter": "Sai Vikneshwar Mani Jayaraman", "authors": "Michael Langberg, Shi Li, Sai Vikneshwar Mani Jayaraman, and Atri\n  Rudra", "title": "Topology Dependent Bounds For FAQs", "comments": "A conference version was presented at PODS 2019", "journal-ref": null, "doi": "10.1145/3294052.3319686", "report-no": null, "categories": "cs.DC cs.CC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we prove topology dependent bounds on the number of rounds\nneeded to compute Functional Aggregate Queries (FAQs) studied by Abo Khamis et\nal. [PODS 2016] in a synchronous distributed network under the model considered\nby Chattopadhyay et al. [FOCS 2014, SODA 2017]. Unlike the recent work on\ncomputing database queries in the Massively Parallel Computation model, in the\nmodel of Chattopadhyay et al., nodes can communicate only via private\npoint-to-point channels and we are interested in bounds that work over an {\\em\narbitrary} communication topology. This is the first work to consider more\npractically motivated problems in this distributed model. For the sake of\nexposition, we focus on two special problems in this paper: Boolean Conjunctive\nQuery (BCQ) and computing variable/factor marginals in Probabilistic Graphical\nModels (PGMs). We obtain tight bounds on the number of rounds needed to compute\nsuch queries as long as the underlying hypergraph of the query is\n$O(1)$-degenerate and has $O(1)$-arity. In particular, the $O(1)$-degeneracy\ncondition covers most well-studied queries that are efficiently computable in\nthe centralized computation model like queries with constant treewidth. These\ntight bounds depend on a new notion of `width' (namely internal-node-width) for\nGeneralized Hypertree Decompositions (GHDs) of acyclic hypergraphs, which\nminimizes the number of internal nodes in a sub-class of GHDs. To the best of\nour knowledge, this width has not been studied explicitly in the theoretical\ndatabase literature. Finally, we consider the problem of computing the product\nof a vector with a chain of matrices and prove tight bounds on its round\ncomplexity (over the finite field of two elements) using a novel min-entropy\nbased argument.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 02:23:26 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Langberg", "Michael", ""], ["Li", "Shi", ""], ["Jayaraman", "Sai Vikneshwar Mani", ""], ["Rudra", "Atri", ""]]}, {"id": "2003.05622", "submitter": "Ping Li", "authors": "Weijie Zhao, Deping Xie, Ronglai Jia, Yulei Qian, Ruiquan Ding,\n  Mingming Sun, Ping Li", "title": "Distributed Hierarchical GPU Parameter Server for Massive Scale Deep\n  Learning Ads Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks of ads systems usually take input from multiple resources,\ne.g., query-ad relevance, ad features and user portraits. These inputs are\nencoded into one-hot or multi-hot binary features, with typically only a tiny\nfraction of nonzero feature values per example. Deep learning models in online\nadvertising industries can have terabyte-scale parameters that do not fit in\nthe GPU memory nor the CPU main memory on a computing node. For example, a\nsponsored online advertising system can contain more than $10^{11}$ sparse\nfeatures, making the neural network a massive model with around 10 TB\nparameters. In this paper, we introduce a distributed GPU hierarchical\nparameter server for massive scale deep learning ads systems. We propose a\nhierarchical workflow that utilizes GPU High-Bandwidth Memory, CPU main memory\nand SSD as 3-layer hierarchical storage. All the neural network training\ncomputations are contained in GPUs. Extensive experiments on real-world data\nconfirm the effectiveness and the scalability of the proposed system. A 4-node\nhierarchical GPU parameter server can train a model more than 2X faster than a\n150-node in-memory distributed parameter server in an MPI cluster. In addition,\nthe price-performance ratio of our proposed system is 4-9 times better than an\nMPI-cluster solution.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 05:15:48 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Zhao", "Weijie", ""], ["Xie", "Deping", ""], ["Jia", "Ronglai", ""], ["Qian", "Yulei", ""], ["Ding", "Ruiquan", ""], ["Sun", "Mingming", ""], ["Li", "Ping", ""]]}, {"id": "2003.05649", "submitter": "Xiaoxi Zhang", "authors": "Xiaoxi Zhang, Jianyu Wang, Gauri Joshi, and Carlee Joe-Wong", "title": "Machine Learning on Volatile Instances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the massive size of the neural network models and training datasets\nused in machine learning today, it is imperative to distribute stochastic\ngradient descent (SGD) by splitting up tasks such as gradient evaluation across\nmultiple worker nodes. However, running distributed SGD can be prohibitively\nexpensive because it may require specialized computing resources such as GPUs\nfor extended periods of time. We propose cost-effective strategies to exploit\nvolatile cloud instances that are cheaper than standard instances, but may be\ninterrupted by higher priority workloads. To the best of our knowledge, this\nwork is the first to quantify how variations in the number of active worker\nnodes (as a result of preemption) affects SGD convergence and the time to train\nthe model. By understanding these trade-offs between preemption probability of\nthe instances, accuracy, and training time, we are able to derive practical\nstrategies for configuring distributed SGD jobs on volatile instances such as\nAmazon EC2 spot instances and other preemptible cloud instances. Experimental\nresults show that our strategies achieve good training performance at\nsubstantially lower cost.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 07:47:34 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Zhang", "Xiaoxi", ""], ["Wang", "Jianyu", ""], ["Joshi", "Gauri", ""], ["Joe-Wong", "Carlee", ""]]}, {"id": "2003.05687", "submitter": "Mayank Raikwar", "authors": "Mayank Raikwar, Danilo Gligoroski, Goran Velinov", "title": "Trends in Development of Databases and Blockchain", "comments": "Accepted in The Second International Workshop on Blockchain\n  Applications and Theory (BAT 2020)", "journal-ref": "2020 Seventh International Conference on Software Defined Systems\n  (SDS)", "doi": "10.1109/SDS49854.2020.9143893", "report-no": null, "categories": "cs.DC cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is about the mutual influence between two technologies: Databases\nand Blockchain. It addresses two questions: 1. How the database technology has\ninfluenced the development of blockchain technology?, and 2. How blockchain\ntechnology has influenced the introduction of new functionalities in some\nmodern databases? For the first question, we explain how database technology\ncontributes to blockchain technology by unlocking different features such as\nACID (Atomicity, Consistency, Isolation, and Durability) transactional\nconsistency, rich queries, real-time analytics, and low latency. We explain how\nthe CAP (Consistency, Availability, Partition tolerance) theorem known for\ndatabases influenced the DCS (Decentralization, Consistency, Scalability)\ntheorem for the blockchain systems. By using an analogous relaxation approach\nas it was used for the proof of the CAP theorem, we postulate a\n\"DCS-satisfiability conjecture.\" For the second question, we review different\ndatabases that are designed specifically for blockchain and provide most of the\nblockchain functionality like immutability, privacy, censorship resistance,\nalong with database features.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 10:04:41 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Raikwar", "Mayank", ""], ["Gligoroski", "Danilo", ""], ["Velinov", "Goran", ""]]}, {"id": "2003.05731", "submitter": "Yue Zhao", "authors": "Yue Zhao, Xiyang Hu, Cheng Cheng, Cong Wang, Changlin Wan, Wen Wang,\n  Jianing Yang, Haoping Bai, Zheng Li, Cao Xiao, Yunlong Wang, Zhi Qiao, Jimeng\n  Sun, Leman Akoglu", "title": "SUOD: Accelerating Large-Scale Unsupervised Heterogeneous Outlier\n  Detection", "comments": "Proceedings of the 4th Conference on Machine Learning and Systems\n  (MLSys). The code is available at see http://github.com/yzhao062/SUOD. arXiv\n  admin note: text overlap with arXiv:2002.03222", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outlier detection (OD) is a key machine learning (ML) task for identifying\nabnormal objects from general samples with numerous high-stake applications\nincluding fraud detection and intrusion detection. Due to the lack of ground\ntruth labels, practitioners often have to build a large number of unsupervised,\nheterogeneous models (i.e., different algorithms with varying hyperparameters)\nfor further combination and analysis, rather than relying on a single model.\nHow to accelerate the training and scoring on new-coming samples by\noutlyingness (referred as prediction throughout the paper) with a large number\nof unsupervised, heterogeneous OD models? In this study, we propose a modular\nacceleration system, called SUOD, to address it. The proposed system focuses on\nthree complementary acceleration aspects (data reduction for high-dimensional\ndata, approximation for costly models, and taskload imbalance optimization for\ndistributed environment), while maintaining performance accuracy. Extensive\nexperiments on more than 20 benchmark datasets demonstrate SUOD's effectiveness\nin heterogeneous OD acceleration, along with a real-world deployment case on\nfraudulent claim analysis at IQVIA, a leading healthcare firm. We open-source\nSUOD for reproducibility and accessibility.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 00:22:50 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2020 21:57:38 GMT"}, {"version": "v3", "created": "Tue, 19 Jan 2021 14:38:38 GMT"}, {"version": "v4", "created": "Fri, 5 Mar 2021 01:55:27 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Zhao", "Yue", ""], ["Hu", "Xiyang", ""], ["Cheng", "Cheng", ""], ["Wang", "Cong", ""], ["Wan", "Changlin", ""], ["Wang", "Wen", ""], ["Yang", "Jianing", ""], ["Bai", "Haoping", ""], ["Li", "Zheng", ""], ["Xiao", "Cao", ""], ["Wang", "Yunlong", ""], ["Qiao", "Zhi", ""], ["Sun", "Jimeng", ""], ["Akoglu", "Leman", ""]]}, {"id": "2003.06007", "submitter": "Vikram Sreekanti", "authors": "Vikram Sreekanti, Chenggang Wu, Saurav Chhatrapati, Joseph E.\n  Gonzalez, Joseph M. Hellerstein, Jose M. Faleiro", "title": "A Fault-Tolerance Shim for Serverless Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Serverless computing has grown in popularity in recent years, with an\nincreasing number of applications being built on Functions-as-a-Service (FaaS)\nplatforms. By default, FaaS platforms support retry-based fault tolerance, but\nthis is insufficient for programs that modify shared state, as they can\nunwittingly persist partial sets of updates in case of failures. To address\nthis challenge, we would like atomic visibility of the updates made by a FaaS\napplication.\n  In this paper, we present AFT, an atomic fault tolerance shim for serverless\napplications. AFT interposes between a commodity FaaS platform and storage\nengine and ensures atomic visibility of updates by enforcing the read atomic\nisolation guarantee. AFT supports new protocols to guarantee read atomic\nisolation in the serverless setting. We demonstrate that aft introduces minimal\noverhead relative to existing storage engines and scales smoothly to thousands\nof requests per second, while preventing a significant number of consistency\nanomalies.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 20:28:55 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Sreekanti", "Vikram", ""], ["Wu", "Chenggang", ""], ["Chhatrapati", "Saurav", ""], ["Gonzalez", "Joseph E.", ""], ["Hellerstein", "Joseph M.", ""], ["Faleiro", "Jose M.", ""]]}, {"id": "2003.06064", "submitter": "Aditya Chilukuri", "authors": "Aditya Chilukuri, Josh Milthorpe, Beau Johnston", "title": "Characterizing Optimizations to Memory Access Patterns using\n  Architecture-Independent Program Features", "comments": "11 pages, 5 figures. To be published in The 8th International\n  Workshop on OpenCL, SYCL, Vulkan and SPIR-V, Munich 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-performance computing developers are faced with the challenge of\noptimizing the performance of OpenCL workloads on diverse architectures. The\nArchitecture-Independent Workload Characterization (AIWC) tool is a plugin for\nthe Oclgrind OpenCL simulator that gathers metrics of OpenCL programs that can\nbe used to understand and predict program performance on an arbitrary given\nhardware architecture. However, AIWC metrics are not always easily interpreted\nand do not reflect some important memory access patterns affecting efficiency\nacross architectures. We propose a new metric of parallel spatial locality --\nthe closeness of memory accesses simultaneously issued by OpenCL work-items\n(threads). We implement the parallel spatial locality metric in the AIWC\nframework, and analyse gathered results on matrix multiply and the Extended\nOpenDwarfs OpenCL benchmarks. The differences in the observed parallel spatial\nlocality metric across implementations of matrix multiply reflect the\noptimizations performed. The new metric can be used to distinguish between the\nOpenDwarfs benchmarks based on the memory access patterns affecting their\nperformance on various architectures. The improvements suggested to AIWC will\nhelp HPC developers better understand memory access patterns of complex codes\nand guide optimization of codes for arbitrary hardware targets.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 23:43:53 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Chilukuri", "Aditya", ""], ["Milthorpe", "Josh", ""], ["Johnston", "Beau", ""]]}, {"id": "2003.06128", "submitter": "Daniel Reijsbergen", "authors": "Dani\\\"el Reijsbergen and Tien Tuan Anh Dinh", "title": "On Exploiting Transaction Concurrency To Speed Up Blockchains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consensus protocols are currently the bottlenecks that prevent blockchain\nsystems from scaling. However, we argue that transaction execution is also\nimportant to the performance and security of blockchains. In other words, there\nare ample opportunities to speed up and further secure blockchains by reducing\nthe cost of transaction execution.\n  Our goal is to understand how much we can speed up blockchains by exploiting\ntransaction concurrency available in blockchain workloads. To this end, we\nfirst analyze historical data of seven major public blockchains, namely\nBitcoin, Bitcoin Cash, Litecoin, Dogecoin, Ethereum, Ethereum Classic, and\nZilliqa. We consider two metrics for concurrency, namely the single-transaction\nconflict rate per block, and the group conflict rate per block. We find that\nthere is more concurrency in UTXO-based blockchains than in account-based ones,\nalthough the amount of concurrency in the former is lower than expected.\nAnother interesting finding is that some blockchains with larger blocks have\nmore concurrency than blockchains with smaller blocks. Next, we propose an\nanalytical model for estimating the transaction execution speed-up given an\namount of concurrency. Using results from our empirical analysis, the model\nestimates that 6x speed-ups in Ethereum can be achieved if all available\nconcurrency is exploited.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 06:16:34 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 13:48:10 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Reijsbergen", "Dani\u00ebl", ""], ["Dinh", "Tien Tuan Anh", ""]]}, {"id": "2003.06307", "submitter": "Zhenheng Tang", "authors": "Zhenheng Tang, Shaohuai Shi, Xiaowen Chu, Wei Wang, Bo Li", "title": "Communication-Efficient Distributed Deep Learning: A Comprehensive\n  Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed deep learning becomes very common to reduce the overall training\ntime by exploiting multiple computing devices (e.g., GPUs/TPUs) as the size of\ndeep models and data sets increases. However, data communication between\ncomputing devices could be a potential bottleneck to limit the system\nscalability. How to address the communication problem in distributed deep\nlearning is becoming a hot research topic recently. In this paper, we provide a\ncomprehensive survey of the communication-efficient distributed training\nalgorithms in both system-level and algorithmic-level optimizations. In the\nsystem-level, we demystify the system design and implementation to reduce the\ncommunication cost. In algorithmic-level, we compare different algorithms with\ntheoretical convergence bounds and communication complexity. Specifically, we\nfirst propose the taxonomy of data-parallel distributed training algorithms,\nwhich contains four main dimensions: communication synchronization, system\narchitectures, compression techniques, and parallelism of communication and\ncomputing. Then we discuss the studies in addressing the problems of the four\ndimensions to compare the communication cost. We further compare the\nconvergence rates of different algorithms, which enable us to know how fast the\nalgorithms can converge to the solution in terms of iterations. According to\nthe system-level communication cost analysis and theoretical convergence speed\ncomparison, we provide the readers to understand what algorithms are more\nefficient under specific distributed environments and extrapolate potential\ndirections for further optimizations.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 05:42:44 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Tang", "Zhenheng", ""], ["Shi", "Shaohuai", ""], ["Chu", "Xiaowen", ""], ["Wang", "Wei", ""], ["Li", "Bo", ""]]}, {"id": "2003.06452", "submitter": "Guenter Hesse", "authors": "Guenter Hesse, Christoph Matthies, Matthias Uflacker", "title": "How Fast Can We Insert? An Empirical Performance Evaluation of Apache\n  Kafka", "comments": "IEEE International Conference on Parallel and Distributed Systems\n  (ICPADS) 2020", "journal-ref": null, "doi": "10.1109/ICPADS51040.2020.00089", "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Message brokers see widespread adoption in modern IT landscapes, with Apache\nKafka being one of the most employed platforms. These systems feature\nwell-defined APIs for use and configuration and present flexible solutions for\nvarious data storage scenarios. Their ability to scale horizontally enables\nusers to adapt to growing data volumes and changing environments. However, one\nof the main challenges concerning message brokers is the danger of them\nbecoming a bottleneck within an IT architecture. To prevent this, knowledge\nabout the amount of data a message broker using a specific configuration can\nhandle needs to be available. In this paper, we propose a monitoring\narchitecture for message brokers and similar Java Virtual Machine-based\nsystems. We present a comprehensive performance analysis of the popular Apache\nKafka platform using our approach. As part of the benchmark, we study selected\ndata ingestion scenarios with respect to their maximum data ingestion rates.\nThe results show that we can achieve an ingestion rate of about 420,000\nmessages/second on the used commodity hardware and with the developed data\nsender tool.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 19:19:45 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 14:15:36 GMT"}, {"version": "v3", "created": "Wed, 3 Mar 2021 14:06:36 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Hesse", "Guenter", ""], ["Matthies", "Christoph", ""], ["Uflacker", "Matthias", ""]]}, {"id": "2003.06454", "submitter": "Xingyu Zhou", "authors": "Xingyu Zhou and Ness Shroff", "title": "A Note on Stein's Method for Heavy-Traffic Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this note, we apply Stein's method to analyze the steady-state\ndistribution of queueing systems in the traditional heavy-traffic regime.\nCompared to previous methods (e.g., drift method and transform method), Stein's\nmethod allows us to establish stronger results with simple and template proofs.\nIn particular, we consider discrete-time systems in this note. We first\nintroduce the key ideas of Stein's method for heavy-traffic analysis through a\nsingle-server system. Then, we apply the developed template to analyze both\nload balancing problems and scheduling problems. All these three examples\ndemonstrate the power and flexibility of Stein's method in heavy-traffic\nanalysis. In particular, we can see that one appealing property of Stein's\nmethod is that it combines the advantages of both the drift method and the\ntransform method.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 19:26:25 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 02:40:54 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Zhou", "Xingyu", ""], ["Shroff", "Ness", ""]]}, {"id": "2003.06485", "submitter": "Przemys{\\l}aw Uzna\\'nski", "authors": "Dan Alistarh, Martin T\\\"opfer, Przemys{\\l}aw Uzna\\'nski", "title": "Robust Comparison in Population Protocols", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has recently been a surge of interest in the computational and\ncomplexity properties of the population model, which assumes $n$ anonymous,\ncomputationally-bounded nodes, interacting at random, and attempting to jointly\ncompute global predicates. In particular, a significant amount of work, has\ngone towards investigating majority and consensus dynamics in this model:\nassuming that each node is initially in one of two states $X$ or $Y$, determine\nwhich state had higher initial count.\n  In this paper, we consider a natural generalization of majority/consensus,\nwhich we call comparison. We are given two baseline states, $X_0$ and $Y_0$,\npresent in any initial configuration in fixed, possibly small counts.\nImportantly, one of these states has higher count than the other: we will\nassume $|X_0| \\ge C |Y_0|$ for some constant $C$. The challenge is to design a\nprotocol which can quickly and reliably decide on which of the baseline states\n$X_0$ and $Y_0$ has higher initial count.\n  We propose a simple algorithm solving comparison: the baseline algorithm uses\n$O(\\log n)$ states per node, and converges in $O(\\log n)$ (parallel) time, with\nhigh probability, to a state where whole population votes on opinions $X$ or\n$Y$ at rates proportional to initial $|X_0|$ vs. $|Y_0|$ concentrations. We\nthen describe how such output can be then used to solve comparison. The\nalgorithm is self-stabilizing, in the sense that it converges to the correct\ndecision even if the relative counts of baseline states $X_0$ and $Y_0$ change\ndynamically during the execution, and leak-robust, in the sense that it can\nwithstand spurious faulty reactions. Our analysis relies on a new martingale\nconcentration result which relates the evolution of a population protocol to\nits expected (steady-state) analysis, which should be broadly applicable in the\ncontext of population protocols and opinion dynamics.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 21:10:12 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 09:36:45 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Alistarh", "Dan", ""], ["T\u00f6pfer", "Martin", ""], ["Uzna\u0144ski", "Przemys\u0142aw", ""]]}, {"id": "2003.06612", "submitter": "Eugene Bagdasaryan", "authors": "Kleomenis Katevas, Eugene Bagdasaryan, Jason Waterman, Mohamad Mounir\n  Safadieh, Eleanor Birrell, Hamed Haddadi, Deborah Estrin", "title": "Policy-Based Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present PoliFL, a decentralized, edge-based framework that\nsupports heterogeneous privacy policies for federated learning. We evaluate our\nsystem on three use cases that train models with sensitive user data collected\nby mobile phones - predictive text, image classification, and notification\nengagement prediction - on a Raspberry Pi edge device. We find that PoliFL is\nable to perform accurate model training and inference within reasonable\nresource and time budgets while also enforcing heterogeneous privacy policies.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 12:04:36 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 23:01:17 GMT"}, {"version": "v3", "created": "Tue, 16 Feb 2021 02:31:06 GMT"}, {"version": "v4", "created": "Wed, 17 Feb 2021 14:36:48 GMT"}, {"version": "v5", "created": "Fri, 19 Feb 2021 02:09:31 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Katevas", "Kleomenis", ""], ["Bagdasaryan", "Eugene", ""], ["Waterman", "Jason", ""], ["Safadieh", "Mohamad Mounir", ""], ["Birrell", "Eleanor", ""], ["Haddadi", "Hamed", ""], ["Estrin", "Deborah", ""]]}, {"id": "2003.06718", "submitter": "Wentao Cai", "authors": "Wentao Cai, Haosen Wen, H. Alan Beadle, Chris Kjellqvist, Mohammad\n  Hedayati, Michael L. Scott", "title": "Understanding and Optimizing Persistent Memory Allocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of fast, dense, byte-addressable nonvolatile memory\nsuggests that data might be kept in pointer-rich \"in-memory\" format across\nprogram runs and even process and system crashes. For full generality, such\ndata requires dynamic memory allocation, and while the allocator could in\nprinciple \"rolled into\" each data structure, it is desirable to make it a\nseparate abstraction.\n  Toward this end, we introduce recoverability, a correctness criterion for\npersistent allocators, together with a nonblocking allocator, Ralloc, that\nsatisfies this criterion. Ralloc is based on the LRMalloc of Leite and Rocha,\nwith three key innovations. First, we persist just enough information during\nnormal operation to permit correct reconstruction of the heap after a\nfull-system crash. Our reconstruction mechanism performs garbage collection\n(GC) to identify and remedy any failure-induced memory leaks. Second, we\nintroduce the notion of filter functions, which identify the locations of\npointers within persistent blocks to mitigate the limitations of conservative\nGC. Third, to allow persistent regions to be mapped at an arbitrary address, we\nemploy position-independent (offset-based) pointers for both data and metadata.\n  Experiments show Ralloc to be performance-competitive with both Makalu, the\nstate-of-the-art lock-based persistent allocator, and such transient allocators\nas LRMalloc and JEMalloc. In particular, reliance on GC and offline metadata\nreconstruction allows Ralloc to pay almost nothing for persistence during\nnormal operation.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 22:59:12 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Cai", "Wentao", ""], ["Wen", "Haosen", ""], ["Beadle", "H. Alan", ""], ["Kjellqvist", "Chris", ""], ["Hedayati", "Mohammad", ""], ["Scott", "Michael L.", ""]]}, {"id": "2003.06862", "submitter": "Nelson Bore K", "authors": "Nelson Bore, Andrew Kinai, Peninah Waweru, Isaac Wambugu, Juliet\n  Mutahi, Everlyne Kemunto, Reginald Bryant, Komminist Weldemariam", "title": "ADW: Blockchain-enabled Small-scale Farm Digitization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Farm records hold the static, temporal, and longitudinal details of the\nfarms. For small-scale farming, the ability to accurately capture these records\nplays a critical role in formalizing and digitizing the agriculture industry.\nReliable exchange of these record through a trusted platform could unlock\ncritical and valuable insights to different stakeholders across the value chain\nin agriculture eco-system. Lately, there has been increasing attention on\ndigitization of small scale farming with the objective of providing farm-level\ntransparency, accountability, visibility, access to farm loans, etc. using\nthese farm records. However, most solutions proposed so far have the\nshortcoming of providing detailed, reliable and trusted small-scale farm\ndigitization information in real time. To address these challenges, we present\na system, called Agribusiness Digital Wallet (ADW), which leverages blockchain\nto formalize the interactions and enable seamless data flow in small-scale\nfarming ecosystem. Utilizing instrumentation of farm tractors, we demonstrate\nthe ability to utilize farm activities to create trusted electronic field\nrecords (EFR) with automated valuable insights. Using ADW, we processed several\nthousands of small-scale farm-level activity events for which we also performed\nautomated farm boundary detection of a number of farms in different\ngeographies.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 16:15:20 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Bore", "Nelson", ""], ["Kinai", "Andrew", ""], ["Waweru", "Peninah", ""], ["Wambugu", "Isaac", ""], ["Mutahi", "Juliet", ""], ["Kemunto", "Everlyne", ""], ["Bryant", "Reginald", ""], ["Weldemariam", "Komminist", ""]]}, {"id": "2003.06990", "submitter": "Yibin Xu", "authors": "Yibin Xu, Yangyu Huang, Jianhua Shao, George Theodorakopoulos", "title": "A Flexible n/2 Adversary Node Resistant and Halting Recoverable\n  Blockchain Sharding Protocol", "comments": "Accepted by Concurrency and Computation Practice and Experience", "journal-ref": null, "doi": "10.1002/CPE.5773", "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain sharding is a promising approach to solving the dilemma between\ndecentralisation and high performance (transaction throughput) for blockchain.\nThe main challenge of Blockchain sharding systems is how to reach a decision on\na statement among a sub-group (shard) of people while ensuring the whole\npopulation recognises this statement. Namely, the challenge is to prevent an\nadversary who does not have the majority of nodes globally but have the\nmajority of nodes inside a shard. Most Blockchain sharding approaches can only\nreach a correct consensus inside a shard with at most $n/3$ evil nodes in a $n$\nnode system. There is a blockchain sharding approach which can prevent an\nincorrect decision to be reached when the adversary does not have $n/2$ nodes\nglobally. However, the system can be stopped from reaching consensus (become\ndeadlocked) if the adversary controls a smaller number of nodes.\n  In this paper, we present an improved Blockchain sharding approach that can\nwithstand $n/2$ adversarial nodes and recover from deadlocks. The recovery is\nmade by dynamically adjusting the number of shards and the shard size. A\nperformance analysis suggests our approach has a high performance (transaction\nthroughput) while requiring little bandwidth for synchronisation.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 03:21:01 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 08:16:05 GMT"}, {"version": "v3", "created": "Thu, 19 Mar 2020 11:05:19 GMT"}, {"version": "v4", "created": "Mon, 23 Mar 2020 13:16:28 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Xu", "Yibin", ""], ["Huang", "Yangyu", ""], ["Shao", "Jianhua", ""], ["Theodorakopoulos", "George", ""]]}, {"id": "2003.07131", "submitter": "Ionut Anghel", "authors": "Claudia Pop, Tudor Cioara, Ionut Anghel, Marcel Antal and Ioan Salomie", "title": "Blockchain based Decentralized Applications: Technology Review and\n  Development Guidelines", "comments": "30 pages, 8 figures, 9 tables, 121 references", "journal-ref": "Future Internet 2021, 13(3), 62", "doi": "10.3390/fi13030062", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain or Distributed Ledger Technology is a disruptive technology that\nprovides the infrastructure for developing decentralized applications enabling\nthe implementation of novel business models even in traditionally centralized\ndomains. In the last years it has drawn high interest from the academic\ncommunity, technology developers and startups thus lots of solutions have been\ndeveloped to address blockchain technology limitations and the requirements of\napplications software engineering. In this paper, we provide a comprehensive\noverview of DLT solutions analyzing the addressed challenges, provided\nsolutions and their usage for developing decentralized applications. Our study\nreviews over 100 blockchain papers and startup initiatives from which we\nconstruct a 3-tier based architecture for decentralized applications and we use\nit to systematically classify the technology solutions. Protocol and Network\nTier solutions address the digital assets registration, transactions, data\nstructure, and privacy and business rules implementation and the creation of\npeer-to-peer networks, ledger replication, and consensus-based state\nvalidation. Scaling Tier solutions address the scalability problems in terms of\nstorage size, transaction throughput, and computational capability. Finally,\nFederated Tier aggregates integrative solutions across multiple blockchain\napplications deployments. The paper closes with a discussion on challenges and\nopportunities for developing decentralized applications by providing a\nmulti-step guideline for decentralizing the design of traditional systems and\nimplementing decentralized applications.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 11:50:57 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Pop", "Claudia", ""], ["Cioara", "Tudor", ""], ["Anghel", "Ionut", ""], ["Antal", "Marcel", ""], ["Salomie", "Ioan", ""]]}, {"id": "2003.07395", "submitter": "Thomas Dickerson", "authors": "Thomas Dickerson", "title": "Adapting Persistent Data Structures for Concurrency and Speculation", "comments": "PhD Thesis, Brown University (2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work unifies insights from the systems and functional programming\ncommunities, in order to enable compositional reasoning about software which is\nnonetheless efficiently realizable in hardware. It exploits a correspondence\nbetween design goals for efficient concurrent data structures and efficient\nimmutable persistent data structures, to produce novel implementations of\nmutable concurrent trees with low contention and an efficient snapshot\noperation to support speculative execution models. It also exploits\ncommutativity to characterize a design space for integrating traditional\nhigh-performance concurrent data structures into Software Transactional Memory\n(STM) runtimes, and extends this technique to yield a novel algorithm for\nconcurrent execution of so-called ``smart contracts'' (specialized programs\nwhich manipulate the state of blockchain ledgers).\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 18:26:08 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Dickerson", "Thomas", ""]]}, {"id": "2003.07427", "submitter": "Seri Khoury", "authors": "Yuval Efron, Ofer Grossman, and Seri Khoury", "title": "Beyond Alice and Bob: Improved Inapproximability for Maximum Independent\n  Set in CONGEST", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By far the most fruitful technique for showing lower bounds for the CONGEST\nmodel is reductions to two-party communication complexity. This technique has\nyielded nearly tight results for various fundamental problems such as distance\ncomputations, minimum spanning tree, minimum vertex cover, and more.\n  In this work, we take this technique a step further, and we introduce a\nframework of reductions to $t$-party communication complexity, for every $t\\geq\n2$. Our framework enables us to show improved hardness results for maximum\nindependent set. Recently, Bachrach et al.[PODC 2019] used the two-party\nframework to show hardness of approximation for maximum independent set. They\nshow that finding a $(5/6+\\epsilon)$-approximation requires $\\Omega(n/\\log^6\nn)$ rounds, and finding a $(7/8+\\epsilon)$-approximation requires\n$\\Omega(n^2/\\log^7 n)$ rounds, in the CONGEST model where $n$ in the number of\nnodes in the network.\n  We improve the results of Bachrach et al. by using reductions to multi-party\ncommunication complexity. Our results:\n  (1) Any algorithm that finds a $(1/2+\\epsilon)$-approximation for maximum\nindependent set in the CONGEST model requires $\\Omega(n/\\log^3 n)$ rounds.\n  (2) Any algorithm that finds a $(3/4+\\epsilon)$-approximation for maximum\nindependent set in the CONGEST model requires $\\Omega(n^2/\\log^3 n)$ rounds.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 20:07:35 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 16:21:20 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Efron", "Yuval", ""], ["Grossman", "Ofer", ""], ["Khoury", "Seri", ""]]}, {"id": "2003.07491", "submitter": "Yuichi Sudo", "authors": "Yuichi Sudo, Masahiro Shibata, Junya Nakamura, Yonghwan Kim,\n  Toshimitsu Masuzawa", "title": "The Power of Global Knowledge on Self-stabilizing Population Protocols", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the population protocol model, many problems cannot be solved in a\nself-stabilizing way. However, global knowledge, such as the number of nodes in\na network, sometimes allows us to design a self-stabilizing protocol for such\nproblems. In this paper, we investigate the effect of global knowledge on the\npossibility of self-stabilizing population protocols in arbitrary graphs.\nSpecifically, we clarify the solvability of the leader election problem, the\nranking problem, the degree recognition problem, and the neighbor recognition\nproblem by self-stabilizing population protocols with knowledge of the number\nof nodes and/or the number of edges in a network.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 01:42:51 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 02:28:50 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Sudo", "Yuichi", ""], ["Shibata", "Masahiro", ""], ["Nakamura", "Junya", ""], ["Kim", "Yonghwan", ""], ["Masuzawa", "Toshimitsu", ""]]}, {"id": "2003.07760", "submitter": "Aleksey Charapko", "authors": "Aleksey Charapko, Ailidani Ailijiang, Murat Demirbas", "title": "Scaling Strongly Consistent Replication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Strong consistency replication helps keep application logic simple and\nprovides significant benefits for correctness and manageability. Unfortunately,\nthe adoption of strongly-consistent replication protocols has been curbed due\nto their limited scalability and performance. To alleviate the leader\nbottleneck in strongly-consistent replication protocols, we introduce Pig, an\nin-protocol communication aggregation and piggybacking technique. Pig employs\nrandomly selected nodes from follower subgroups to relay the leader's message\nto the rest of the followers in the subgroup, and to perform in-network\naggregation of acknowledgments back from these followers. By randomly\nalternating the relay nodes across replication operations, Pig shields the\nrelay nodes as well as the leader from becoming hotspots and improves\nthroughput scalability.\n  We showcase Pig in the context of classical Paxos protocols employed for\nstrongly consistent replication by many cloud computing services and databases.\nWe implement and evaluate PigPaxos, in comparison to Paxos and EPaxos protocols\nunder various workloads over clusters of size 5 to 25 nodes. We show that the\naggregation at the relay has little latency overhead, and PigPaxos can provide\nmore than 3 folds improved throughput over Paxos and EPaxos with little latency\ndeterioration. We support our experimental observations with the analytical\nmodeling of the bottlenecks and show that the rotating of the relay nodes\nprovides the most benefit for reducing the bottlenecks and that the throughput\nis maximized when employing only 1 randomly rotating relay node.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 15:17:02 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2021 17:36:29 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Charapko", "Aleksey", ""], ["Ailijiang", "Ailidani", ""], ["Demirbas", "Murat", ""]]}, {"id": "2003.07787", "submitter": "Archit Somani", "authors": "Hagit Attiya, Sweta Kumari, Archit Somani, and Jennifer L. Welch", "title": "Store-Collect in the Presence of Continuous Churn with Application to\n  Snapshots and Lattice Agreement", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm for implementing a store-collect object in an\nasynchronous crash-prone message-passing dynamic system, where nodes\ncontinually enter and leave. The algorithm is very simple and efficient,\nrequiring just one round trip for a store operation and two for a collect. We\nthen show the versatility of the store-collect object for implementing\nchurn-tolerant versions of useful data structures, while shielding the user\nfrom the complications of the underlying churn. In particular, we present\nelegant and efficient implementations of atomic snapshot and generalized\nlattice agreement objects that use store-collect.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 16:11:29 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 11:03:56 GMT"}, {"version": "v3", "created": "Thu, 5 Nov 2020 09:57:17 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Attiya", "Hagit", ""], ["Kumari", "Sweta", ""], ["Somani", "Archit", ""], ["Welch", "Jennifer L.", ""]]}, {"id": "2003.07940", "submitter": "Luca Mazzola", "authors": "Luca Mazzola, Alexander Denzler and Ramon Christen", "title": "Towards a Peer-to-Peer Energy Market: an Overview", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.CY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work focuses on the electric power market, comparing the status quo with\nthe recent trend towards the increase in distributed self-generation\ncapabilities by prosumers. Starting from the existing tension between the\nintrinsically hierarchical current structure of the electricity distribution\nnetwork and the substantially distributed and self-organising nature of the\nself-generation, we explore the limitations imposed by the current conditions.\nInitially, we introduce a potential multi-layered architecture for a\nPeer-to-Peer (P2P) energy market, discussing the fundamental aspects of local\nproduction and local consumption as part of a microgrid. Secondly, we analyse\nthe consequent changes for the different users' roles, also in connection with\nsome incentive models connected with the decentralisation of the power\nproduction. To give a full picture to the reader, we also scrutinise relevant\nelements of energy trading, such as Smart Contract and grid stability. Thirdly,\nwe present an example of a typical P2P settlement, showcasing the role of all\nthe previously analysed aspects. To conclude, we performed a review of relevant\nactivities in this domain, to showcase where existing projects are going and\nwhat are the most important themes covered. Being this a work in progress, many\nopen questions are still on the table and will be addressed in the next stages\nof the research. Eventually, by providing a reference model as base for further\ndiscussions and improvements, we would like to engage ourselves in a dialog\nwith the different users and the broad community, oriented towards a more fair\nand ecological-friendly solution for the electricity market of the future.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 20:32:10 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2020 20:02:37 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Mazzola", "Luca", ""], ["Denzler", "Alexander", ""], ["Christen", "Ramon", ""]]}, {"id": "2003.07945", "submitter": "Yang Hu", "authors": "Soroush Bateni, Zhendong Wang, Yuankun Zhu, Yang Hu, Cong Liu", "title": "Co-Optimizing Performance and Memory FootprintVia Integrated CPU/GPU\n  Memory Management, anImplementation on Autonomous Driving Platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.OS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cutting-edge embedded system applications, such as self-driving cars and\nunmanned drone software, are reliant on integrated CPU/GPU platforms for their\nDNNs-driven workload, such as perception and other highly parallel components.\nIn this work, we set out to explore the hidden performance implication of GPU\nmemory management methods of integrated CPU/GPU architecture. Through a series\nof experiments on micro-benchmarks and real-world workloads, we find that the\nperformance under different memory management methods may vary according to\napplication characteristics. Based on this observation, we develop a\nperformance model that can predict system overhead for each memory management\nmethod based on application characteristics. Guided by the performance model,\nwe further propose a runtime scheduler. By conducting per-task memory\nmanagement policy switching and kernel overlapping, the scheduler can\nsignificantly relieve the system memory pressure and reduce the multitasking\nco-run response time. We have implemented and extensively evaluated our system\nprototype on the NVIDIA Jetson TX2, Drive PX2, and Xavier AGX platforms, using\nboth Rodinia benchmark suite and two real-world case studies of drone software\nand autonomous driving software.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 21:09:24 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2020 02:43:49 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Bateni", "Soroush", ""], ["Wang", "Zhendong", ""], ["Zhu", "Yuankun", ""], ["Hu", "Yang", ""], ["Liu", "Cong", ""]]}, {"id": "2003.08011", "submitter": "Guang Chao Wang", "authors": "Guang Chao Wang, Kenny Gross, and Akshay Subramaniam", "title": "ContainerStress: Autonomous Cloud-Node Scoping Framework for Big-Data ML\n  Use Cases", "comments": "To be published in 6th Annual Conf. on Computational Science &\n  Computational Intelligence (CSCI'19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deploying big-data Machine Learning (ML) services in a cloud environment\npresents a challenge to the cloud vendor with respect to the cloud container\nconfiguration sizing for any given customer use case. OracleLabs has developed\nan automated framework that uses nested-loop Monte Carlo simulation to\nautonomously scale any size customer ML use cases across the range of cloud\nCPU-GPU \"Shapes\" (configurations of CPUs and/or GPUs in Cloud containers\navailable to end customers). Moreover, the OracleLabs and NVIDIA authors have\ncollaborated on a ML benchmark study which analyzes the compute cost and GPU\nacceleration of any ML prognostic algorithm and assesses the reduction of\ncompute cost in a cloud container comprising conventional CPUs and NVIDIA GPUs.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 01:51:42 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Wang", "Guang Chao", ""], ["Gross", "Kenny", ""], ["Subramaniam", "Akshay", ""]]}, {"id": "2003.08059", "submitter": "Yo-Seb Jeon", "authors": "Yo-Seb Jeon, Mohammad Mohammadi Amiri, Jun Li, and H. Vincent Poor", "title": "A Compressive Sensing Approach for Federated Learning over Massive MIMO\n  Communication Systems", "comments": "The title of the paper has been changed from \"Gradient Estimation for\n  Federated Learning over Massive MIMO Communication Systems\" to \"A Compressive\n  Sensing Approach for Federated Learning over Massive MIMO Communication\n  Systems\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning is a privacy-preserving approach to train a global model\nat a central server by collaborating with wireless devices, each with its own\nlocal training data set. In this paper, we present a compressive sensing\napproach for federated learning over massive multiple-input multiple-output\ncommunication systems in which the central server equipped with a massive\nantenna array communicates with the wireless devices. One major challenge in\nsystem design is to reconstruct local gradient vectors accurately at the\ncentral server, which are computed-and-sent from the wireless devices. To\novercome this challenge, we first establish a transmission strategy to\nconstruct sparse transmitted signals from the local gradient vectors at the\ndevices. We then propose a compressive sensing algorithm enabling the server to\niteratively find the linear minimum-mean-square-error (LMMSE) estimate of the\ntransmitted signal by exploiting its sparsity. We also derive an analytical\nthreshold for the residual error at each iteration, to design the stopping\ncriterion of the proposed algorithm. We show that for a sparse transmitted\nsignal, the proposed algorithm requires less computationally complexity than\nLMMSE. Simulation results demonstrate that the presented approach outperforms\nconventional linear beamforming approaches and reduces the performance gap\nbetween federated learning and centralized learning with perfect\nreconstruction.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 05:56:27 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 13:29:42 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Jeon", "Yo-Seb", ""], ["Amiri", "Mohammad Mohammadi", ""], ["Li", "Jun", ""], ["Poor", "H. Vincent", ""]]}, {"id": "2003.08233", "submitter": "Yang Wang", "authors": "Xu Jiang, Nan Guan, He Du, Weichen Liu, Wang Yi", "title": "On the Analysis of Parallel Real-Time Tasks with Spin Locks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locking protocol is an essential component in resource management of\nreal-time systems, which coordinates mutually exclusive accesses to shared\nresources from different tasks. Although the design and analysis of locking\nprotocols have been intensively studied for sequential real-time tasks, there\nhas been little work on this topic for parallel real-time tasks. In this paper,\nwe study the analysis of parallel real-time tasks using spin locks to protect\naccesses to shared resources in three commonly used request serving orders\n(unordered, FIFO-order and priority-order). A remarkable feature making our\nanalysis method more accurate is to systematically analyze the blocking time\nwhich may delay a task's finishing time, where the impact to the total workload\nand the longest path length is jointly considered, rather than analyzing them\nseparately and counting all blocking time as the workload that delays a task's\nfinishing time, as commonly assumed in the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 14:08:35 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Jiang", "Xu", ""], ["Guan", "Nan", ""], ["Du", "He", ""], ["Liu", "Weichen", ""], ["Yi", "Wang", ""]]}, {"id": "2003.08301", "submitter": "Luca Ballotta", "authors": "Luca Ballotta, Luca Schenato, Luca Carlone", "title": "From Sensor to Processing Networks: Optimal Estimation with Computation\n  and Communication Latency", "comments": "8 pages, 8 figures To be published in Proceedings of IFAC 2020 World\n  Congress. arXiv admin note: substantial text overlap with arXiv:1911.05859", "journal-ref": null, "doi": "10.1016/j.ifacol.2020.12.223", "report-no": null, "categories": "math.OC cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the use of a networked system ($e.g.$, swarm of\nrobots, smart grid, sensor network) to monitor a time-varying phenomenon of\ninterest in the presence of communication and computation latency. Recent\nadvances in edge computing have enabled processing to be spread across the\nnetwork, hence we investigate the fundamental computation-communication\ntrade-off, arising when a sensor has to decide whether to transmit raw data\n(incurring communication delay) or preprocess them (incurring computational\ndelay) in order to compute an accurate estimate of the state of the phenomenon\nof interest. We propose two key contributions. First, we formalize the notion\nof $processing$ $network$. Contrarily to $sensor$ $and$ $communication$\n$networks$, where the designer is concerned with the design of a suitable\ncommunication policy, in a processing network one can also control when and\nwhere the computation occurs in the network. The second contribution is to\nprovide analytical results on the optimal preprocessing delay ($i.e.$, the\noptimal time spent on computations at each sensor) for the case with a single\nsensor and multiple homogeneous sensors. Numerical results substantiate our\nclaims that accounting for computation latencies (both at sensor and estimator\nside) and communication delays can largely impact the estimation accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 21:03:29 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Ballotta", "Luca", ""], ["Schenato", "Luca", ""], ["Carlone", "Luca", ""]]}, {"id": "2003.08305", "submitter": "Blesson Varghese", "authors": "Kai Chen and Peter Kilpatrick and Dimitrios S. Nikolopoulos and\n  Blesson Varghese", "title": "Cross Architectural Power Modelling", "comments": "10 pages; IEEE/ACM CCGrid 2020. arXiv admin note: text overlap with\n  arXiv:1710.10325", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing power modelling research focuses on the model rather than the\nprocess for developing models. An automated power modelling process that can be\ndeployed on different processors for developing power models with high accuracy\nis developed. For this, (i) an automated hardware performance counter selection\nmethod that selects counters best correlated to power on both ARM and Intel\nprocessors, (ii) a noise filter based on clustering that can reduce the mean\nerror in power models, and (iii) a two stage power model that surmounts\nchallenges in using existing power models across multiple architectures are\nproposed and developed. The key results are: (i) the automated hardware\nperformance counter selection method achieves comparable selection to the\nmanual method reported in the literature, (ii) the noise filter reduces the\nmean error in power models by up to 55%, and (iii) the two stage power model\ncan predict dynamic power with less than 8% error on both ARM and Intel\nprocessors, which is an improvement over classic models.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 09:51:12 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Chen", "Kai", ""], ["Kilpatrick", "Peter", ""], ["Nikolopoulos", "Dimitrios S.", ""], ["Varghese", "Blesson", ""]]}, {"id": "2003.08361", "submitter": "Poorna Chandra Tejasvi", "authors": "Poorna Chandra Tejasvi, Vasanth Rajaraman, Arun Babu Puthuparambil,\n  Akhil Pankaj, Bharadwaj Amrutur", "title": "Vermillion: A High-Performance Scalable IoT Middleware for Smart Cities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the massive increase in the number of IoT devices being deployed in\nsmart cities, it becomes paramount for middlewares to be able to handle very\nhigh loads and support demanding use-cases. In order to do so, middlewares must\nscale horizontally while providing a commensurate increase in availability and\nthroughput. Currently, most open-source IoT middlewares do not provide\nout-of-the-box support for scaling horizontally. In this paper, we present\n\"Vermillion'', a scalable, secure and open-source IoT middleware for smart\ncities which provides in-built support for scaling-out. We make three\ncontributions in this paper. Firstly, the middleware platform itself along with\na formal process for data exchange between data producers and consumers.\nSecondly, we propose the use of hash-based federation to distribute and manage\nload across various message broker nodes while eliminating inter-node\nsynchronisation overheads. Thirdly, we discuss a case study where Vermillion\nwas deployed in a city and briefly discuss about deployment considerations\nusing the obtained results.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 13:09:55 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Tejasvi", "Poorna Chandra", ""], ["Rajaraman", "Vasanth", ""], ["Puthuparambil", "Arun Babu", ""], ["Pankaj", "Akhil", ""], ["Amrutur", "Bharadwaj", ""]]}, {"id": "2003.08364", "submitter": "Arvind Easwaran", "authors": "Xiaozhe Gu, Arvind Easwaran", "title": "Dynamic Budget Management with Service Guarantees for Mixed-Criticality\n  Systems", "comments": "\\copyright 2016 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": "IEEE Real-Time Systems Symposium (RTSS), Porto, Portugal, 2016,\n  pages 47-56", "doi": "10.1109/RTSS.2016.014", "report-no": null, "categories": "cs.DC cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many existing studies on mixed-criticality (MC) scheduling assume that\nlow-criticality budgets for high-criticality applications are known apriori.\nThese budgets are primarily used as guidance to determine when the scheduler\nshould switch the system mode from low to high. Based on this key observation,\nin this paper we propose a dynamic MC scheduling model under which\nlow-criticality budgets for individual high-criticality applications are\ndetermined at runtime as opposed to being fixed offline. To ensure sufficient\nbudget for high-criticality applications at all times, we use offline\nschedulability analysis to determine a system-wide total low-criticality budget\nallocation for all the high-criticality applications combined. This total\nbudget is used as guidance in our model to determine the need for a\nmode-switch. The runtime strategy then distributes this total budget among the\nvarious applications depending on their execution requirement and with the\nobjective of postponing mode-switch as much as possible. We show that this\nruntime strategy is able to postpone mode-switches for a longer time than any\nstrategy that uses a fixed low-criticality budget allocation for each\napplication. Finally, since we are able to control the total budget allocation\nfor high-criticality applications before mode-switch, we also propose\ntechniques to determine these budgets considering system-wide objectives such\nas schedulability and service guarantee for low-criticality applications.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 04:03:40 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Gu", "Xiaozhe", ""], ["Easwaran", "Arvind", ""]]}, {"id": "2003.08567", "submitter": "Praneeth Vepakomma", "authors": "Ramesh Raskar, Isabel Schunemann, Rachel Barbar, Kristen Vilcans, Jim\n  Gray, Praneeth Vepakomma, Suraj Kapa, Andrea Nuzzo, Rajiv Gupta, Alex Berke,\n  Dazza Greenwood, Christian Keegan, Shriank Kanaparti, Robson Beaudry, David\n  Stansbury, Beatriz Botero Arcila, Rishank Kanaparti, Vitor Pamplona,\n  Francesco M Benedetti, Alina Clough, Riddhiman Das, Kaushal Jain, Khahlil\n  Louisy, Greg Nadeau, Vitor Pamplona, Steve Penrod, Yasaman Rajaee, Abhishek\n  Singh, Greg Storm, John Werner", "title": "Apps Gone Rogue: Maintaining Personal Privacy in an Epidemic", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Containment, the key strategy in quickly halting an epidemic, requires rapid\nidentification and quarantine of the infected individuals, determination of\nwhom they have had close contact with in the previous days and weeks, and\ndecontamination of locations the infected individual has visited. Achieving\ncontainment demands accurate and timely collection of the infected individual's\nlocation and contact history. Traditionally, this process is labor intensive,\nsusceptible to memory errors, and fraught with privacy concerns. With the\nrecent almost ubiquitous availability of smart phones, many people carry a tool\nwhich can be utilized to quickly identify an infected individual's contacts\nduring an epidemic, such as the current 2019 novel Coronavirus crisis.\nUnfortunately, the very same first-generation contact tracing tools have been\nused to expand mass surveillance, limit individual freedoms and expose the most\nprivate details about individuals. We seek to outline the different\ntechnological approaches to mobile-phone based contact-tracing to date and\nelaborate on the opportunities and the risks that these technologies pose to\nindividuals and societies. We describe advanced security enhancing approaches\nthat can mitigate these risks and describe trade-offs one must make when\ndeveloping and deploying any mass contact-tracing technology. With this paper,\nour aim is to continue to grow the conversation regarding contact-tracing for\nepidemic and pandemic containment and discuss opportunities to advance this\nspace. We invite feedback and discussion.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 04:22:24 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Raskar", "Ramesh", ""], ["Schunemann", "Isabel", ""], ["Barbar", "Rachel", ""], ["Vilcans", "Kristen", ""], ["Gray", "Jim", ""], ["Vepakomma", "Praneeth", ""], ["Kapa", "Suraj", ""], ["Nuzzo", "Andrea", ""], ["Gupta", "Rajiv", ""], ["Berke", "Alex", ""], ["Greenwood", "Dazza", ""], ["Keegan", "Christian", ""], ["Kanaparti", "Shriank", ""], ["Beaudry", "Robson", ""], ["Stansbury", "David", ""], ["Arcila", "Beatriz Botero", ""], ["Kanaparti", "Rishank", ""], ["Pamplona", "Vitor", ""], ["Benedetti", "Francesco M", ""], ["Clough", "Alina", ""], ["Das", "Riddhiman", ""], ["Jain", "Kaushal", ""], ["Louisy", "Khahlil", ""], ["Nadeau", "Greg", ""], ["Pamplona", "Vitor", ""], ["Penrod", "Steve", ""], ["Rajaee", "Yasaman", ""], ["Singh", "Abhishek", ""], ["Storm", "Greg", ""], ["Werner", "John", ""]]}, {"id": "2003.08746", "submitter": "Carlos Junqueira Junior PhD", "authors": "Carlos Junqueira-Junior, Jo\\~ao Luiz F. Azevedo, Jairo Panetta,\n  William R. Wolf, and Sami Yamouni", "title": "Strong Scaling of Numerical Solver for Supersonic Jet Flow Configuration", "comments": "Journal article from Journal of the Brazilian Society of Mechanical\n  Sciences and Engineering (41, Article number: 547 (2019))", "journal-ref": "J Braz. Soc. Mech. Sci. Eng. 41, 547 (2019)", "doi": "10.1007/s40430-019-2055-6", "report-no": null, "categories": "cs.CE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acoustics loads are rocket design constraints which push researches and\nengineers to invest efforts in the aeroacoustics phenomena which is present on\nlaunch vehicles. Therefore, an in-house computational fluid dynamics tool is\ndeveloped in order to reproduce high-fidelity results of supersonic jet flows\nfor aeroacoustic analogy applications. The solver is written using the large\neddy simulation formulation that is discretized using a finite-difference\napproach and an explicit time integration. Numerical simulations of supersonic\njet flows are very expensive and demand efficient high-performance computing.\nTherefore, non-blocking message passage interface protocols and parallel\ninput/output features are implemented into the code in order to perform\nsimulations which demand up to one billion degrees of freedom. The present work\nevaluates the parallel efficiency of the solver when running on a supercomputer\nwith a maximum theoretical peak of 127.4 TFLOPS. Speedup curves are generated\nusing nine different workloads. Moreover, the validation results of a realistic\nflow condition are also presented in the current work.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 14:09:20 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Junqueira-Junior", "Carlos", ""], ["Azevedo", "Jo\u00e3o Luiz F.", ""], ["Panetta", "Jairo", ""], ["Wolf", "William R.", ""], ["Yamouni", "Sami", ""]]}, {"id": "2003.09074", "submitter": "Ed Upchurch", "authors": "Ed T. Upchurch", "title": "A Migratory Near Memory Processing Architecture Applied to Big Data\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Servers produced by mainstream vendors are inefficient in processing Big Data\nqueries due to bottlenecks inherent in the fundamental architecture of these\nsystems. Current server blades contain multicore processors connected to DRAM\nmemory and disks by an interconnection chipset. The multicore processor chips\nperform all the computations while the DRAM and disks store the data but have\nno processing capability. To perform a database query, data must be moved back\nand forth between DRAM and a small cache as well as between DRAM and disks. For\nBig Data applications this data movement in onerous. Migratory Near Memory\nServers address this bottleneck by placing large numbers of lightweight\nprocessors directly into the memory system. These processors operate directly\non the relations, vertices and edges of Big Data applications in place without\nhaving to shuttle large quantities of data back and forth between DRAM, cache\nand heavyweight multicore processors. This paper addresses the application of\nsuch an architecture to relational database SELECT and JOIN queries.\nPreliminary results indicate end-to-end orders of magnitude speedup.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 02:37:40 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Upchurch", "Ed T.", ""]]}, {"id": "2003.09269", "submitter": "Jeremy Kepner", "authors": "Siddharth Samsi, Jeremy Kepner, Vijay Gadepally, Michael Hurley,\n  Michael Jones, Edward Kao, Sanjeev Mohindra, Albert Reuther, Steven Smith,\n  William Song, Diane Staheli, Paul Monticciolo", "title": "GraphChallenge.org Triangle Counting Performance", "comments": "10 pages, 8 figures, 121 references, to be submitted to IEEE HPEC\n  2020. This work reports new updated results on prior work reported in\n  arXiv:1805.09675 & arXiv:1708.06866", "journal-ref": null, "doi": "10.1109/HPEC43674.2020.9286166", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of graph analytic systems has created a need for new ways to measure\nand compare the capabilities of graph processing systems. The MIT/Amazon/IEEE\nGraph Challenge has been developed to provide a well-defined community venue\nfor stimulating research and highlighting innovations in graph analysis\nsoftware, hardware, algorithms, and systems. GraphChallenge.org provides a wide\nrange of pre-parsed graph data sets, graph generators, mathematically defined\ngraph algorithms, example serial implementations in a variety of languages, and\nspecific metrics for measuring performance. The triangle counting component of\nGraphChallenge.org tests the performance of graph processing systems to count\nall the triangles in a graph and exercises key graph operations found in many\ngraph algorithms. In 2017, 2018, and 2019 many triangle counting submissions\nwere received from a wide range of authors and organizations. This paper\npresents a performance analysis of the best performers of these submissions.\nThese submissions show that their state-of-the-art triangle counting execution\ntime, $T_{\\rm tri}$, is a strong function of the number of edges in the graph,\n$N_e$, which improved significantly from 2017 ($T_{\\rm tri} \\approx\n(N_e/10^8)^{4/3}$) to 2018 ($T_{\\rm tri} \\approx N_e/10^9$) and remained\ncomparable from 2018 to 2019. Graph Challenge provides a clear picture of\ncurrent graph analysis systems and underscores the need for new innovations to\nachieve high performance on very large graphs.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 20:36:29 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Samsi", "Siddharth", ""], ["Kepner", "Jeremy", ""], ["Gadepally", "Vijay", ""], ["Hurley", "Michael", ""], ["Jones", "Michael", ""], ["Kao", "Edward", ""], ["Mohindra", "Sanjeev", ""], ["Reuther", "Albert", ""], ["Smith", "Steven", ""], ["Song", "William", ""], ["Staheli", "Diane", ""], ["Monticciolo", "Paul", ""]]}, {"id": "2003.09363", "submitter": "Giorgi Nadiradze", "authors": "Dan Alistarh, Nikita Koval, Giorgi Nadiradze", "title": "Efficiency Guarantees for Parallel Incremental Algorithms under Relaxed\n  Schedulers", "comments": null, "journal-ref": null, "doi": "10.1145/3323165.3323201", "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several classic problems in graph processing and computational geometry are\nsolved via incremental algorithms, which split computation into a series of\nsmall tasks acting on shared state, which gets updated progressively.\n  While the sequential variant of such algorithms usually specifies a fixed\n(but sometimes random) order in which the tasks should be performed, a standard\napproach to parallelizing such algorithms is to relax this constraint to allow\nfor out-of-order parallel execution. This is the case for parallel\nimplementations of Dijkstra's single-source shortest-paths algorithm (SSSP),\nand for parallel Delaunay mesh triangulation.\n  While many software frameworks parallelize incremental computation in this\nway, it is still not well understood whether this relaxed ordering approach can\nstill provide any complexity guarantees.\n  In this paper, we address this problem, and analyze the efficiency guarantees\nprovided by a range of incremental algorithms when parallelized via relaxed\nschedulers.\n  We show that, for algorithms such as Delaunay mesh triangulation and sorting\nby insertion, schedulers with a maximum relaxation factor of $k$ in terms of\nthe maximum priority inversion allowed will introduce a maximum amount of\nwasted work of $O(log(n) poly (k) ), $ where $n$ is the number of tasks to be\nexecuted.\n  For SSSP, we show that the additional work is $O(poly (k) d_{max} / w_{min}),\n$ where $d_{\\max}$ is the maximum distance between two nodes, and $w_{min}$ is\nthe minimum such distance. In practical settings where $n \\gg k$, this suggests\nthat the overheads of relaxation will be outweighed by the improved scalability\nof the relaxed scheduler.\n  On the negative side, we provide lower bounds showing that certain algorithms\nwill inherently incur a non-trivial amount of wasted work due to scheduler\nrelaxation, even for relatively benign relaxed schedulers.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 16:33:29 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 00:31:32 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Alistarh", "Dan", ""], ["Koval", "Nikita", ""], ["Nadiradze", "Giorgi", ""]]}, {"id": "2003.09518", "submitter": "Maxim Naumov", "authors": "Maxim Naumov, John Kim, Dheevatsa Mudigere, Srinivas Sridharan,\n  Xiaodong Wang, Whitney Zhao, Serhat Yilmaz, Changkyu Kim, Hector Yuen,\n  Mustafa Ozdal, Krishnakumar Nair, Isabel Gao, Bor-Yiing Su, Jiyan Yang and\n  Mikhail Smelyanskiy", "title": "Deep Learning Training in Facebook Data Centers: Design of Scale-up and\n  Scale-out Systems", "comments": "10 pages, 14 figures; adjusted Fig. 10, added reference; fixed typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale training is important to ensure high performance and accuracy of\nmachine-learning models. At Facebook we use many different models, including\ncomputer vision, video and language models. However, in this paper we focus on\nthe deep learning recommendation models (DLRMs), which are responsible for more\nthan 50% of the training demand in our data centers. Recommendation models\npresent unique challenges in training because they exercise not only compute\nbut also memory capacity as well as memory and network bandwidth. As model size\nand complexity increase, efficiently scaling training becomes a challenge. To\naddress it we design Zion - Facebook's next-generation large-memory training\nplatform that consists of both CPUs and accelerators. Also, we discuss the\ndesign requirements of future scale-out training systems.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 22:18:35 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 06:39:48 GMT"}, {"version": "v3", "created": "Tue, 18 Aug 2020 06:22:42 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Naumov", "Maxim", ""], ["Kim", "John", ""], ["Mudigere", "Dheevatsa", ""], ["Sridharan", "Srinivas", ""], ["Wang", "Xiaodong", ""], ["Zhao", "Whitney", ""], ["Yilmaz", "Serhat", ""], ["Kim", "Changkyu", ""], ["Yuen", "Hector", ""], ["Ozdal", "Mustafa", ""], ["Nair", "Krishnakumar", ""], ["Gao", "Isabel", ""], ["Su", "Bor-Yiing", ""], ["Yang", "Jiyan", ""], ["Smelyanskiy", "Mikhail", ""]]}, {"id": "2003.09526", "submitter": "Sumit Mandal", "authors": "Sumit K. Mandal, Ganapati Bhat, Janardhan Rao Doppa, Partha Pratim\n  Pande, Umit Y. Ogras", "title": "An Energy-Aware Online Learning Framework for Resource Management in\n  Heterogeneous Platforms", "comments": "This paper has been accepted to be published in a future issue of ACM\n  TODAES", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile platforms must satisfy the contradictory requirements of fast response\ntime and minimum energy consumption as a function of dynamically changing\napplications. To address this need, system-on-chips (SoC) that are at the heart\nof these devices provide a variety of control knobs, such as the number of\nactive cores and their voltage/frequency levels. Controlling these knobs\noptimally at runtime is challenging for two reasons. First, the large\nconfiguration space prohibits exhaustive solutions. Second, control policies\ndesigned offline are at best sub-optimal since many potential new applications\nare unknown at design-time. We address these challenges by proposing an online\nimitation learning approach. Our key idea is to construct an offline policy and\nadapt it online to new applications to optimize a given metric (e.g., energy).\nThe proposed methodology leverages the supervision enabled by power-performance\nmodels learned at runtime. We demonstrate its effectiveness on a commercial\nmobile platform with 16 diverse benchmarks. Our approach successfully adapts\nthe control policy to an unknown application after executing less than 25% of\nits instructions.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 22:59:35 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Mandal", "Sumit K.", ""], ["Bhat", "Ganapati", ""], ["Doppa", "Janardhan Rao", ""], ["Pande", "Partha Pratim", ""], ["Ogras", "Umit Y.", ""]]}, {"id": "2003.09532", "submitter": "Mahsa Eftekhari Hesari", "authors": "Talley Amir, James Aspnes, David Doty, Mahsa Eftekhari, Eric Severson", "title": "Message complexity of population protocols", "comments": null, "journal-ref": null, "doi": "10.4230/LIPIcs.DISC.2020.5", "report-no": null, "categories": "cs.DC cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard population protocol model assumes that when two agents interact,\neach observes the entire state of the other agent. We initiate the study of\n$\\textit{message complexity}$ for population protocols, where the state of an\nagent is divided into an externally-visible $\\textit{message}$ and an internal\ncomponent, where only the message can be observed by the other agent in an\ninteraction.\n  We consider the case of $O(1)$ message complexity. When time is unrestricted,\nwe obtain an exact characterization of the stably computable predicates based\non the number of internal states $s(n)$: If $s(n) = o(n)$ then the protocol\ncomputes semilinear predicates (unlike the original model, which can compute\nnon-semilinear predicates with $s(n) = O(\\log n)$), and otherwise it computes a\npredicate decidable by a nondeterministic $O(n \\log s(n))$-space-bounded Turing\nmachine. We then introduce novel $O(\\mathrm{polylog}(n))$ expected time\nprotocols for junta/leader election and general purpose broadcast correct with\nhigh probability, and approximate and exact population size counting correct\nwith probability 1. Finally, we show that the main constraint on the power of\nbounded-message-size protocols is the size of the internal states: with\nunbounded internal states, any computable function can be computed with\nprobability 1 in the limit by a protocol that uses only $\\textit{1-bit}$\nmessages.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 23:53:43 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 20:31:47 GMT"}, {"version": "v3", "created": "Wed, 23 Sep 2020 17:53:58 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Amir", "Talley", ""], ["Aspnes", "James", ""], ["Doty", "David", ""], ["Eftekhari", "Mahsa", ""], ["Severson", "Eric", ""]]}, {"id": "2003.09675", "submitter": "Nirupam Gupta", "authors": "Nirupam Gupta and Nitin H. Vaidya", "title": "Resilience in Collaborative Optimization: Redundant and Independent Cost\n  Functions", "comments": "This revised version contains additional generalizations. Comprises\n  30 pages, and 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report considers the problem of Byzantine fault-tolerance in multi-agent\ncollaborative optimization. In this problem, each agent has a local cost\nfunction. The goal of a collaborative optimization algorithm is to compute a\nminimum of the aggregate of the agents' cost functions. We consider the case\nwhen a certain number of agents may be Byzantine faulty. Such faulty agents may\nnot follow a prescribed algorithm, and they may send arbitrary or incorrect\ninformation regarding their local cost functions. A reasonable goal in presence\nof such faulty agents is to minimize the aggregate cost of the non-faulty\nagents. In this report, we show that this goal can be achieved if and only if\nthe cost functions of the non-faulty agents have a minimal redundancy property.\nWe present different algorithms that achieve such tolerance against faulty\nagents, and demonstrate a trade-off between the complexity of an algorithm and\nthe properties of the agents' cost functions.\n  Further, we also consider the case when the cost functions are independent or\ndo not satisfy the minimal redundancy property. In that case, we quantify the\ntolerance against faulty agents by introducing a metric called weak resilience.\nWe present an algorithm that attains weak resilience when the faulty agents are\nin the minority and the cost functions are non-negative.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2020 15:00:27 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 14:43:29 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Gupta", "Nirupam", ""], ["Vaidya", "Nitin H.", ""]]}, {"id": "2003.09744", "submitter": "Philipp Brune", "authors": "Philipp Brune (Neu-Ulm University of Applied Sciences, Neu-Ulm,\n  Germany)", "title": "Towards an Enterprise-Ready Implementation of Artificial\n  Intelligence-Enabled, Blockchain-Based Smart Contracts", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain technology and artificial intelligence (AI) are current hot topics\nin research and practice. However, the potentials of their combination have\nbeen studied just recently to a larger extend. While different use cases for\ncombining AI and blockchain have been discussed, the idea of enabling\nblockchain-based smart contracts to perform \"smarter\" decisions by using AI or\nmachine learning (ML) models has only been considered on the conceptual level\nso far. It remained open, how such AI-enabled smart contracts could be\nimplemented in a robust way for real-world applications. Therefore, in this\npaper a new, enterprise-class implementation of AI-enabled smart contracts is\npresented and first insights regarding its feasibility are discussed.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2020 20:21:01 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Brune", "Philipp", "", "Neu-Ulm University of Applied Sciences, Neu-Ulm,\n  Germany"]]}, {"id": "2003.09876", "submitter": "Xu Chen", "authors": "Deyin Liu and Xu Chen and Zhi Zhou and Qing Ling", "title": "HierTrain: Fast Hierarchical Edge AI Learning with Hybrid Parallelism in\n  Mobile-Edge-Cloud Computing", "comments": "Submitted for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, deep neural networks (DNNs) are the core enablers for many emerging\nedge AI applications. Conventional approaches to training DNNs are generally\nimplemented at central servers or cloud centers for centralized learning, which\nis typically time-consuming and resource-demanding due to the transmission of a\nlarge amount of data samples from the device to the remote cloud. To overcome\nthese disadvantages, we consider accelerating the learning process of DNNs on\nthe Mobile-Edge-Cloud Computing (MECC) paradigm. In this paper, we propose\nHierTrain, a hierarchical edge AI learning framework, which efficiently deploys\nthe DNN training task over the hierarchical MECC architecture. We develop a\nnovel \\textit{hybrid parallelism} method, which is the key to HierTrain, to\nadaptively assign the DNN model layers and the data samples across the three\nlevels of edge device, edge server and cloud center. We then formulate the\nproblem of scheduling the DNN training tasks at both layer-granularity and\nsample-granularity. Solving this optimization problem enables us to achieve the\nminimum training time. We further implement a hardware prototype consisting of\nan edge device, an edge server and a cloud server, and conduct extensive\nexperiments on it. Experimental results demonstrate that HierTrain can achieve\nup to 6.9x speedup compared to the cloud-based hierarchical training approach.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 12:40:06 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Liu", "Deyin", ""], ["Chen", "Xu", ""], ["Zhou", "Zhi", ""], ["Ling", "Qing", ""]]}, {"id": "2003.09895", "submitter": "Peter Robinson", "authors": "Peter Robinson", "title": "Being Fast Means Being Chatty: The Local Information Cost of Graph\n  Spanners", "comments": "An earlier version of this paper appeared in the proceedings of SODA\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new measure for quantifying the amount of information that the\nnodes in a network need to learn to jointly solve a graph problem. We show that\nthe local information cost ($\\textsf{LIC}$) presents a natural lower bound on\nthe communication complexity of distributed algorithms. For the synchronous\nCONGEST-KT1 model, where each node has initial knowledge of its neighbors' IDs,\nwe prove that $\\Omega(\\textsf{LIC}_\\gamma(P)/ \\log\\tau \\log n)$ bits are\nrequired for solving a graph problem $P$ with a $\\tau$-round algorithm that\nerrs with probability at most $\\gamma$. Our result is the first lower bound\nthat yields a general trade-off between communication and time for graph\nproblems in the CONGEST-KT1 model.\n  We demonstrate how to apply the local information cost by deriving a lower\nbound on the communication complexity of computing a $(2t-1)$-spanner that\nconsists of at most $O(n^{1+1/t + \\epsilon})$ edges, where $\\epsilon =\n\\Theta(1/t^2)$. Our main result is that any $O(\\textsf{poly}(n))$-time\nalgorithm must send at least $\\tilde\\Omega((1/t^2) n^{1+1/2t})$ bits in the\nCONGEST model under the KT1 assumption. Previously, only a trivial lower bound\nof $\\tilde \\Omega(n)$ bits was known for this problem.\n  A consequence of our lower bound is that achieving both time- and\ncommunication-optimality is impossible when designing a distributed spanner\nalgorithm. In light of the work of King, Kutten, and Thorup (PODC 2015), this\nshows that computing a minimum spanning tree can be done significantly faster\nthan finding a spanner when considering algorithms with $\\tilde O(n)$\ncommunication complexity. Our result also implies time complexity lower bounds\nfor constructing a spanner in the node-congested clique of Augustine et al.\n(2019) and in the push-pull gossip model with limited bandwidth.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 13:54:26 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2020 06:42:25 GMT"}, {"version": "v3", "created": "Mon, 15 Mar 2021 22:36:09 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Robinson", "Peter", ""]]}, {"id": "2003.09926", "submitter": "Carlos Junqueira Junior PhD", "authors": "Carlos Junqueira-Junior, Jo\\~ao Luiz F. Azevedo, Jairo Panetta,\n  William R. Wolf, and Sami Yamouni", "title": "On the scalability of CFD tool for supersonic jet flow configurations", "comments": "13 pages journal article. arXiv admin note: text overlap with\n  arXiv:2003.08746", "journal-ref": "Parallel Computing, Volume 93, May 2020, 102620", "doi": "10.1016/j.parco.2020.102620", "report-no": null, "categories": "cs.DC cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New regulations are imposing noise emissions limitations for the aviation\nindustry which are pushing researchers and engineers to invest efforts in\nstudying the aeroacoustics phenomena. Following this trend, an in-house\ncomputational fluid dynamics tool is build to reproduce high fidelity results\nof supersonic jet flows for aeroacoustic analogy applications. The solver is\nwritten using the large eddy simulation formulation that is discretized using a\nfinite difference approach and an explicit time integration. Numerical\nsimulations of supersonic jet flows are very expensive and demand efficient\nhigh-performance computing. Therefore, non-blocking message passage interface\nprotocols and parallel Input/Output features are implemented into the code in\norder to perform simulations which demand up to one billion grid points. The\npresent work addresses the evaluation of code improvements along with the\ncomputational performance of the solver running on a computer with maximum\ntheoretical peak of 2.727 PFlops. Different mesh configurations, whose size\nvaries from a few hundred thousand to approximately one billion grid points,\nare evaluated in the present paper. Calculations are performed using different\nworkloads in order to assess the strong and weak scalability of the parallel\ncomputational tool. Moreover, validation results of a realistic flow condition\nare also presented in the current work.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 17:49:52 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Junqueira-Junior", "Carlos", ""], ["Azevedo", "Jo\u00e3o Luiz F.", ""], ["Panetta", "Jairo", ""], ["Wolf", "William R.", ""], ["Yamouni", "Sami", ""]]}, {"id": "2003.09972", "submitter": "Matthias F\\\"ugger", "authors": "Da-Jung Cho, Matthias F\\\"ugger, Corbin Hopper, Manish Kushwaha, Thomas\n  Nowak, Quentin Soubeyran", "title": "Distributed Computation with Continual Population Growth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing with synthetically engineered bacteria is a vibrant and active\nfield with numerous applications in bio-production, bio-sensing, and medicine.\nMotivated by the lack of robustness and by resource limitation inside single\ncells, distributed approaches with communication among bacteria have recently\ngained in interest. In this paper, we focus on the problem of population growth\nhappening concurrently, and possibly interfering, with the desired\nbio-computation. Specifically, we present a fast protocol in systems with\ncontinuous population growth for the majority consensus problem and prove that\nit correctly identifies the initial majority among two inputs with high\nprobability if the initial difference is $\\Omega(\\sqrt{n\\log n})$ where $n$ is\nthe total initial population. We also present a fast protocol that correctly\ncomputes the NAND of two inputs with high probability. We demonstrate that\ncombining the NAND gate protocol with the continuous-growth majority consensus\nprotocol, using the latter as an amplifier, it is possible to implement\ncircuits computing arbitrary Boolean functions.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 19:05:53 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 19:30:18 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Cho", "Da-Jung", ""], ["F\u00fcgger", "Matthias", ""], ["Hopper", "Corbin", ""], ["Kushwaha", "Manish", ""], ["Nowak", "Thomas", ""], ["Soubeyran", "Quentin", ""]]}, {"id": "2003.10064", "submitter": "Pingcheng Ruan", "authors": "Pingcheng Ruan, Dumitrel Loghin, Quang-Trung Ta, Meihui Zhang, Gang\n  Chen, Beng Chin Ooi", "title": "A Transactional Perspective on Execute-order-validate Blockchains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart contracts have enabled blockchain systems to evolve from simple\ncryptocurrency platforms, such as Bitcoin, to general transactional systems,\nsuch as Ethereum. Catering for emerging business requirements, a new\narchitecture called execute-order-validate has been proposed in Hyperledger\nFabric to support parallel transactions and improve the blockchain's\nthroughput. However, this new architecture might render many invalid\ntransactions when serializing them. This problem is further exaggerated as the\nblock formation rate is inherently limited due to other factors beside data\nprocessing, such as cryptography and consensus.\n  In this work, we propose a novel method to enhance the execute-order-validate\narchitecture, by reducing invalid transactions to improve the throughput of\nblockchains. Our method is inspired by state-of-the-art optimistic concurrency\ncontrol techniques in modern database systems. In contrast to existing\nblockchains that adopt database's preventive approaches which might abort\nserializable transactions, our method is theoretically more fine-grained.\nSpecifically, unserializable transactions are aborted before ordering and the\nremaining transactions are guaranteed to be serializable. For evaluation, we\nimplement our method in two blockchains respectively, FabricSharp on top of\nHyperledger Fabric, and FastFabricSharp on top of FastFabric. We compare the\nperformance of FabricSharp with vanilla Fabric and three related systems, two\nof which are respectively implemented with one standard and one\nstate-of-the-art concurrency control techniques from databases. The results\ndemonstrate that FabricSharp achieves 25% higher throughput compared to the\nother systems in nearly all experimental scenarios. Moreover, the\nFastFabricSharp's improvement over FastFabric is up to 66%.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 03:30:45 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Ruan", "Pingcheng", ""], ["Loghin", "Dumitrel", ""], ["Ta", "Quang-Trung", ""], ["Zhang", "Meihui", ""], ["Chen", "Gang", ""], ["Ooi", "Beng Chin", ""]]}, {"id": "2003.10092", "submitter": "Victor A. Melent'ev", "authors": "Victor A. Melent'ev", "title": "Author's approach to the topological modeling of parallel computing\n  systems", "comments": "27 pages", "journal-ref": null, "doi": "10.26782/JMCMS.SPL.8/2020.04.00017", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The author's research of topologies of parallel computing systems and the\ntasks solved with them, including the corresponding tools of their modeling, is\nsummarized in the present paper. The original topological model of such systems\nis presented based on the modified Amdahl law. It allowed formalizing the\ndependence of the necessary number of processors and the maximal distance\nbetween information-adjacent vertices in a graph on the directive values of\nacceleration or efficiency. The dependences of these values on the system\ninterconnection topology and on the information graph of the parallel task are\nalso formalized. The tools for a comparative evaluation of these dependences,\ntopological criteria and the functions of scaling and fault-tolerant operation\nof parallel systems are based on the author|s technique of projective\ndescription of graphs and the algorithms used in it.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 06:07:29 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Melent'ev", "Victor A.", ""]]}, {"id": "2003.10128", "submitter": "Yi-Shan Lin", "authors": "Wei-Kang Fu, Yi-Shan Lin, Giovanni Campagna, De-Yi Tsai, Chun-Ting\n  Liu, Chung-Huan Mei, Edward Y. Chang, Monica S. Lam, Shih-Wei Liao", "title": "Soteria: A Provably Compliant User Right Manager Using a Novel Two-Layer\n  Blockchain Technology", "comments": "12 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Soteria is a user right management system designed to safeguard user-data\nprivacy in a transparent and provable manner in compliance to regulations such\nas GDPR and CCPA. Soteria represents user data rights as formal executable\nsharing agreements, which can automatically be translated into a human readable\nform and enforced as data are queried. To support revocation and to prove\ncompliance, an indelible, audited trail of the hash of data access and sharing\nagreements are stored on a two-layer distributed ledger. The main chain ensures\npartition tolerance and availability (PA) properties while side chains ensure\nconsistency and availability (CA), thus providing the three properties of the\nCAP (consistency, availability, and partition tolerance) theorem. Besides\ndepicting the two-layer architecture of Soteria, this paper evaluates\nrepresentative consensus protocols and reports performance statistics.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 08:26:54 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 04:31:54 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Fu", "Wei-Kang", ""], ["Lin", "Yi-Shan", ""], ["Campagna", "Giovanni", ""], ["Tsai", "De-Yi", ""], ["Liu", "Chun-Ting", ""], ["Mei", "Chung-Huan", ""], ["Chang", "Edward Y.", ""], ["Lam", "Monica S.", ""], ["Liao", "Shih-Wei", ""]]}, {"id": "2003.10422", "submitter": "Anastasiia Koloskova", "authors": "Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi,\n  Sebastian U. Stich", "title": "A Unified Theory of Decentralized SGD with Changing Topology and Local\n  Updates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decentralized stochastic optimization methods have gained a lot of attention\nrecently, mainly because of their cheap per iteration cost, data locality, and\ntheir communication-efficiency. In this paper we introduce a unified\nconvergence analysis that covers a large variety of decentralized SGD methods\nwhich so far have required different intuitions, have different applications,\nand which have been developed separately in various communities.\n  Our algorithmic framework covers local SGD updates and synchronous and\npairwise gossip updates on adaptive network topology. We derive universal\nconvergence rates for smooth (convex and non-convex) problems and the rates\ninterpolate between the heterogeneous (non-identically distributed data) and\niid-data settings, recovering linear convergence rates in many special cases,\nfor instance for over-parametrized models. Our proofs rely on weak assumptions\n(typically improving over prior work in several aspects) and recover (and\nimprove) the best known complexity results for a host of important scenarios,\nsuch as for instance coorperative SGD and federated averaging (local SGD).\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 17:49:15 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 17:24:03 GMT"}, {"version": "v3", "created": "Tue, 2 Mar 2021 14:07:36 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Koloskova", "Anastasia", ""], ["Loizou", "Nicolas", ""], ["Boreiri", "Sadra", ""], ["Jaggi", "Martin", ""], ["Stich", "Sebastian U.", ""]]}, {"id": "2003.10486", "submitter": "Jovonni Pharr", "authors": "Jovonni L. Pharr", "title": "AfricaOS: Using a distributed, proposal-based, replicated state machine\n  towards liberation from the Berlin Conference of 1885", "comments": "v0.0.1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CR cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Berlin Conference of 1885 has influenced the way native Africans, and the\nAfrican Diaspora live their daily lives. France contractually controls several\nresources generated by the continent of Africa. Herein lies a technical\nproposal to free Africa from the financial and economic agreements coerced upon\nthe continent over a century ago by utilizing decentralized collaboration\nthrough advanced technology. AfricaOS (AOS) aims to provide a philosophical,\nand fundamental framework for implementing a simple, distributed, collaborative\ncomputer for agreement amongst peers. The work also demonstrates an algebra\nover transactions, use of the protocol for privatization, a method for\ntokenizing barter economies, and methods to design mechanisms for use in\nimplementing protocol behavior.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 18:34:52 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Pharr", "Jovonni L.", ""]]}, {"id": "2003.10554", "submitter": "Peter Alvaro", "authors": "Kyle Kingsbury and Peter Alvaro", "title": "Elle: Inferring Isolation Anomalies from Experimental Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Users who care about their data store it in databases, which (at least in\nprinciple) guarantee some form of transactional isolation. However, experience\nshows [Kleppmann 2019, Kingsbury and Patella 2019a] that many databases do not\nprovide the isolation guarantees they claim. With the recent proliferation of\nnew distributed databases, demand has grown for checkers that can, by\ngenerating client workloads and injecting faults, produce anomalies that\nwitness a violation of a stated guarantee. An ideal checker would be sound (no\nfalse positives), efficient (polynomial in history length and concurrency),\neffective (finding violations in real databases), general (analyzing many\npatterns of transactions), and informative (justifying the presence of an\nanomaly with understandable counterexamples). Sadly, we are aware of no\ncheckers that satisfy these goals.\n  We present Elle: a novel checker which infers an Adya-style dependency graph\nbetween client-observed transactions. It does so by carefully selecting\ndatabase objects and operations when generating histories, so as to ensure that\nthe results of database reads reveal information about their version history.\nElle can detect every anomaly in Adya et al's formalism [Adya et al. 2000]\n(except for predicates), discriminate between them, and provide concise\nexplanations of each. This paper makes the following contributions: we present\nElle, demonstrate its soundness, measure its efficiency against the current\nstate of the art, and give evidence of its effectiveness via a case study of\nfour real databases.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 21:11:06 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Kingsbury", "Kyle", ""], ["Alvaro", "Peter", ""]]}, {"id": "2003.10579", "submitter": "Sanghamitra Dutta", "authors": "Sanghamitra Dutta, Jianyu Wang, Gauri Joshi", "title": "Slow and Stale Gradients Can Win the Race", "comments": "Some of the results have appeared in AISTATS 2018. This is an\n  extended version with additional results, in particular, an adaptive\n  synchronicity strategy called AdaSync. arXiv admin note: substantial text\n  overlap with arXiv:1803.01113", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed Stochastic Gradient Descent (SGD) when run in a synchronous\nmanner, suffers from delays in runtime as it waits for the slowest workers\n(stragglers). Asynchronous methods can alleviate stragglers, but cause gradient\nstaleness that can adversely affect the convergence error. In this work, we\npresent a novel theoretical characterization of the speedup offered by\nasynchronous methods by analyzing the trade-off between the error in the\ntrained model and the actual training runtime(wallclock time). The main novelty\nin our work is that our runtime analysis considers random straggling delays,\nwhich helps us design and compare distributed SGD algorithms that strike a\nbalance between straggling and staleness. We also provide a new error\nconvergence analysis of asynchronous SGD variants without bounded or\nexponential delay assumptions. Finally, based on our theoretical\ncharacterization of the error-runtime trade-off, we propose a method of\ngradually varying synchronicity in distributed SGD and demonstrate its\nperformance on CIFAR10 dataset.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 23:27:50 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Dutta", "Sanghamitra", ""], ["Wang", "Jianyu", ""], ["Joshi", "Gauri", ""]]}, {"id": "2003.10615", "submitter": "Yu Ye", "authors": "Yu Ye, Hao Chen, Ming Xiao, Mikael Skoglund, H. Vincent Poor", "title": "Privacy-preserving Incremental ADMM for Decentralized Consensus\n  Optimization", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2020.3027917", "report-no": null, "categories": "math.OC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The alternating direction method of multipliers (ADMM) has been recently\nrecognized as a promising optimizer for large-scale machine learning models.\nHowever, there are very few results studying ADMM from the aspect of\ncommunication costs, especially jointly with privacy preservation, which are\ncritical for distributed learning. We investigate the communication efficiency\nand privacy-preservation of ADMM by solving the consensus optimization problem\nover decentralized networks. Since walk algorithms can reduce communication\nload, we first propose incremental ADMM (I-ADMM) based on the walk algorithm,\nthe updating order of which follows a Hamiltonian cycle instead. However,\nI-ADMM cannot guarantee the privacy for agents against external eavesdroppers\neven if the randomized initialization is applied. To protect privacy for\nagents, we then propose two privacy-preserving incremental ADMM algorithms,\ni.e., PI-ADMM1 and PI-ADMM2, where perturbation over step sizes and primal\nvariables is adopted, respectively. Through theoretical analyses, we prove the\nconvergence and privacy preservation for PI-ADMM1, which are further supported\nby numerical experiments. Besides, simulations demonstrate that the proposed\nPI-ADMM1 and PI-ADMM2 algorithms are communication efficient compared with\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 02:02:25 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Ye", "Yu", ""], ["Chen", "Hao", ""], ["Xiao", "Ming", ""], ["Skoglund", "Mikael", ""], ["Poor", "H. Vincent", ""]]}, {"id": "2003.10688", "submitter": "Nicolas Weber", "authors": "Nicolas Weber and Felipe Huici", "title": "SOL: Effortless Device Support for AI Frameworks without Source Code\n  Changes", "comments": "HPML Workshop 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern high performance computing clusters heavily rely on accelerators to\novercome the limited compute power of CPUs. These supercomputers run various\napplications from different domains such as simulations, numerical applications\nor artificial intelligence (AI). As a result, vendors need to be able to\nefficiently run a wide variety of workloads on their hardware. In the AI domain\nthis is in particular exacerbated by the existence of a number of popular\nframeworks (e.g, PyTorch, TensorFlow, etc.) that have no common code base, and\ncan vary in functionality. The code of these frameworks evolves quickly, making\nit expensive to keep up with all changes and potentially forcing developers to\ngo through constant rounds of upstreaming. In this paper we explore how to\nprovide hardware support in AI frameworks without changing the framework's\nsource code in order to minimize maintenance overhead. We introduce SOL, an AI\nacceleration middleware that provides a hardware abstraction layer that allows\nus to transparently support heterogeneous hardware. As a proof of concept, we\nimplemented SOL for PyTorch with three backends: CPUs, GPUs and vector\nprocessors.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 07:03:09 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Weber", "Nicolas", ""], ["Huici", "Felipe", ""]]}, {"id": "2003.10735", "submitter": "Jae-Won Chung", "authors": "Jae-Won Chung, Jae-Yun Kim, Soo-Mook Moon", "title": "ShadowTutor: Distributed Partial Distillation for Mobile Video DNN\n  Inference", "comments": "Accepted at ICPP 2020", "journal-ref": null, "doi": "10.1145/3404397.3404404", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the recent success of deep neural networks (DNN) on video computer\nvision tasks, performing DNN inferences on videos that originate from mobile\ndevices has gained practical significance. As such, previous approaches\ndeveloped methods to offload DNN inference computations for images to cloud\nservers to manage the resource constraints of mobile devices. However, when it\ncomes to video data, communicating information of every frame consumes\nexcessive network bandwidth and renders the entire system susceptible to\nadverse network conditions such as congestion. Thus, in this work, we seek to\nexploit the temporal coherence between nearby frames of a video stream to\nmitigate network pressure. That is, we propose ShadowTutor, a distributed video\nDNN inference framework that reduces the number of network transmissions\nthrough intermittent knowledge distillation to a student model. Moreover, we\nupdate only a subset of the student's parameters, which we call partial\ndistillation, to reduce the data size of each network transmission.\nSpecifically, the server runs a large and general teacher model, and the mobile\ndevice only runs an extremely small but specialized student model. On sparsely\nselected key frames, the server partially trains the student model by targeting\nthe teacher's response and sends the updated part to the mobile device. We\ninvestigate the effectiveness of ShadowTutor with HD video semantic\nsegmentation. Evaluations show that network data transfer is reduced by 95% on\naverage. Moreover, the throughput of the system is improved by over three times\nand shows robustness to changes in network bandwidth.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 09:50:38 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2020 06:39:06 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Chung", "Jae-Won", ""], ["Kim", "Jae-Yun", ""], ["Moon", "Soo-Mook", ""]]}, {"id": "2003.10745", "submitter": "Kashyap Thimmaraju", "authors": "Kashyap Thimmaraju and Stefan Schmid", "title": "Towards Fine-Grained Billing For Cloud Networking", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.CR cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit multi-tenant network virtualization in data centers, and make the\ncase for tenant-specific virtual switches. In particular, tenant-specific\nvirtual switches allow cloud providers to extend fine-grained billing (known,\ne.g., from serverless architectures) to the network, accounting not only for\nIO, but also CPU or energy. We sketch an architecture and present economical\nmotivation and recent technological enablers. We also find that virtual\nswitches today do not offer sufficient multi-tenancy and can introduce\nartificial performance bottlenecks, e.g., in load balancers. We conclude by\ndiscussing additional use cases for tentant-specific switches.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 10:06:06 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Thimmaraju", "Kashyap", ""], ["Schmid", "Stefan", ""]]}, {"id": "2003.11178", "submitter": "Alexis Asseman", "authors": "Pritish Narayanan, Charles E. Cox, Alexis Asseman, Nicolas Antoine,\n  Harald Huels, Winfried W. Wilcke and Ahmet S. Ozcan", "title": "Overview of the IBM Neural Computer Architecture", "comments": "8 pages, 5 figures. Submitted to IEEE Transactions on Parallel and\n  Distributed Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The IBM Neural Computer (INC) is a highly flexible, re-configurable parallel\nprocessing system that is intended as a research and development platform for\nemerging machine intelligence algorithms and computational neuroscience. It\nconsists of hundreds of programmable nodes, primarily based on Xilinx's Field\nProgrammable Gate Array (FPGA) technology. The nodes are interconnected in a\nscalable 3d mesh topology. We overview INC, emphasizing unique features such as\nflexibility and scalability both in the types of computations performed and in\nthe available modes of communication, enabling new machine intelligence\napproaches and learning strategies not well suited to the matrix\nmanipulation/SIMD libraries that GPUs are optimized for. This paper describes\nthe architecture of the machine and applications are to be described in detail\nelsewhere.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 01:51:59 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Narayanan", "Pritish", ""], ["Cox", "Charles E.", ""], ["Asseman", "Alexis", ""], ["Antoine", "Nicolas", ""], ["Huels", "Harald", ""], ["Wilcke", "Winfried W.", ""], ["Ozcan", "Ahmet S.", ""]]}, {"id": "2003.11267", "submitter": "Nikhil Khatri", "authors": "Nikhil Khatri, Shirshendu Chakrabarti", "title": "NVMe and PCIe SSD Monitoring in Hyperscale Data Centers", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With low latency, high throughput and enterprise-grade reliability, SSDs have\nbecome the de-facto choice for storage in the data center. As a result, SSDs\nare used in all online data stores in LinkedIn. These apps persist and serve\ncritical user data and have millisecond latencies. For the hosts serving these\napplications, SSD faults are the single largest cause of failure. Frequent SSD\nfailures result in significant downtime for critical applications. They also\ngenerate a significant downstream RCA (Root Cause Analysis) load for systems\noperations teams. A lack of insight into the runtime characteristics of these\ndrives results in limited ability to provide accurate RCAs for such issues and\nhinders the ability to provide credible, long term fixes to such issues. In\nthis paper we describe the system developed at LinkedIn to facilitate the\nreal-time monitoring of SSDs and the insights we gained into failure\ncharacteristics. We describe how we used that insight to perform predictive\nmaintenance and present the resulting reduction of man-hours spent on\nmaintenance.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 08:29:44 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Khatri", "Nikhil", ""], ["Chakrabarti", "Shirshendu", ""]]}, {"id": "2003.11332", "submitter": "Dieter Weber", "authors": "Dieter Weber, Alexander Clausen, Rafal E. Dunin-Borkowski", "title": "Next-Generation Information Technology Systems for Fast Detectors in\n  Electron Microscop", "comments": null, "journal-ref": "Handbook on Big Data and Machine Learning in the Physical\n  Sciences, World Scientific, 2020, 83-120", "doi": "10.1142/9789811204579_0005", "report-no": null, "categories": "cs.DC cond-mat.mtrl-sci cs.PF physics.ins-det", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The Gatan K2 IS direct electron detector (Gatan Inc., 2018), which was\nintroduced in 2014, marked a watershed moment in the development of cameras for\ntransmission electron microscopy (TEM) (Pan & Czarnik, 2016). Its pixel\nfrequency, i.e. the number of data points (pixels) recorded per second, was two\norders of magnitude higher than the fastest cameras available only five years\nbefore. Starting from 2009, the data rate of TEM cameras has outpaced the\ndevelopment of network, mass storage and memory bandwidth by almost two orders\nof magnitude. Consequently, solutions based on personal computers (PCs) that\nwere adequate until then are no longer able to handle the resulting data rates.\nInstead, tailored high-performance setups are necessary. Similar developments\nhave occurred for advanced X-ray sources such as the European XFEL, requiring\nspecial information technology (IT) systems for data handling (Sauter, Hattne,\nGrosse-Kunstleve, & Echols, 2013) (Fangohr, et al., 2018). Information and\ndetector technology are currently under rapid development and involve\ndisruptive technological innovations. This chapter briefly reviews the\ntechnological developments of the past 20 years, presents a snapshot of the\ncurrent situation at the beginning of 2019 with many practical considerations,\nand looks forward to future developments.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 11:24:42 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Weber", "Dieter", ""], ["Clausen", "Alexander", ""], ["Dunin-Borkowski", "Rafal E.", ""]]}, {"id": "2003.11409", "submitter": "Yutaro Iiyama", "authors": "Yutaro Iiyama and Benedikt Maier and Daniel Abercrombie and Maxim\n  Goncharov and Christoph Paus", "title": "Dynamo -- Handling Scientific Data Across Sites and Storage Media", "comments": "18 pages, 9 figures", "journal-ref": "Computing and Software for Big Science 5, 11 (2021)", "doi": "10.1007/s41781-021-00054-2", "report-no": null, "categories": "cs.DC hep-ex", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamo is a full-stack software solution for scientific data management.\nDynamo's architecture is modular, extensible, and customizable, making the\nsoftware suitable for managing data in a wide range of installation scales,\nfrom a few terabytes stored at a single location to hundreds of petabytes\ndistributed across a worldwide computing grid. This article documents the core\nsystem design of Dynamo and describes the applications that implement various\ndata management tasks. A brief report is also given on the operational\nexperiences of the system at the CMS experiment at the CERN Large Hadron\nCollider and at a small scale analysis facility.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 13:52:50 GMT"}, {"version": "v2", "created": "Sun, 16 May 2021 14:40:09 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Iiyama", "Yutaro", ""], ["Maier", "Benedikt", ""], ["Abercrombie", "Daniel", ""], ["Goncharov", "Maxim", ""], ["Paus", "Christoph", ""]]}, {"id": "2003.11468", "submitter": "James Willis", "authors": "James S. Willis, Matthieu Schaller, Pedro Gonnet, John C. Helly", "title": "A Hybrid MPI+Threads Approach to Particle Group Finding Using Union-Find", "comments": "12 pages, 4 figures. Proceedings of the ParCo 2019 conference,\n  Prague, Czech Republic, September 10-13th, 2019", "journal-ref": "Advances in Parallel Computing, Volume 36: Parallel Computing:\n  Technology Trends (2020), Pages: 263 - 274, ISBN: 978-1-64368-070-5", "doi": "10.3233/APC200050", "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Friends-of-Friends (FoF) algorithm is a standard technique used in\ncosmological $N$-body simulations to identify structures. Its goal is to find\nclusters of particles (called groups) that are separated by at most a cut-off\nradius. $N$-body simulations typically use most of the memory present on a\nnode, leaving very little free for a FoF algorithm to run on-the-fly. We\npropose a new method that utilises the common Union-Find data structure and a\nhybrid MPI+threads approach. The algorithm can also be expressed elegantly in a\ntask-based formalism if such a framework is used in the rest of the\napplication. We have implemented our algorithm in the open-source cosmological\ncode, SWIFT. Our implementation displays excellent strong- and weak-scaling\nbehaviour on realistic problems and compares favourably (speed-up of 18x) over\nother methods commonly used in the $N$-body community.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 16:00:08 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Willis", "James S.", ""], ["Schaller", "Matthieu", ""], ["Gonnet", "Pedro", ""], ["Helly", "John C.", ""]]}, {"id": "2003.11666", "submitter": "Vitaliy Chiley", "authors": "Atli Kosson, Vitaliy Chiley, Abhinav Venigalla, Joel Hestness, Urs\n  K\\\"oster", "title": "Pipelined Backpropagation at Scale: Training Large Models without\n  Batches", "comments": "Proceedings of the 4th MLSys Conference, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New hardware can substantially increase the speed and efficiency of deep\nneural network training. To guide the development of future hardware\narchitectures, it is pertinent to explore the hardware and machine learning\nproperties of alternative training algorithms. In this work we evaluate the use\nof small batch, fine-grained Pipelined Backpropagation, an asynchronous\npipeline parallel training algorithm that has significant hardware advantages.\nWe introduce two methods, Spike Compensation and Linear Weight Prediction, that\neffectively mitigate the downsides caused by the asynchronicity of Pipelined\nBackpropagation and outperform existing techniques in our setting. We show that\nappropriate normalization and small batch sizes can also aid training. With our\nmethods, fine-grained Pipelined Backpropagation using a batch size of one can\nmatch the accuracy of SGD for multiple networks trained on CIFAR-10 and\nImageNet. Simple scaling rules allow the use of existing hyperparameters for\ntraditional training without additional tuning.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 22:26:28 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 04:32:28 GMT"}, {"version": "v3", "created": "Sat, 10 Apr 2021 00:50:11 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Kosson", "Atli", ""], ["Chiley", "Vitaliy", ""], ["Venigalla", "Abhinav", ""], ["Hestness", "Joel", ""], ["K\u00f6ster", "Urs", ""]]}, {"id": "2003.11789", "submitter": "Vitor Enes", "authors": "Vitor Enes, Carlos Baquero, Tuanir Fran\\c{c}a Rezende, Alexey Gotsman,\n  Matthieu Perrin, Pierre Sutra", "title": "State-Machine Replication for Planet-Scale Systems (Extended Version)", "comments": "Extended version of a EuroSys'20 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online applications now routinely replicate their data at multiple sites\naround the world. In this paper we present Atlas, the first state-machine\nreplication protocol tailored for such planet-scale systems. Atlas does not\nrely on a distinguished leader, so clients enjoy the same quality of service\nindependently of their geographical locations. Furthermore, client-perceived\nlatency improves as we add sites closer to clients. To achieve this, Atlas\nminimizes the size of its quorums using an observation that concurrent data\ncenter failures are rare. It also processes a high percentage of accesses in a\nsingle round trip, even when these conflict. We experimentally demonstrate that\nAtlas consistently outperforms state-of-the-art protocols in planet-scale\nscenarios. In particular, Atlas is up to two times faster than Flexible Paxos\nwith identical failure assumptions, and more than doubles the performance of\nEgalitarian Paxos in the YCSB benchmark.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 08:24:26 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 13:19:10 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Enes", "Vitor", ""], ["Baquero", "Carlos", ""], ["Rezende", "Tuanir Fran\u00e7a", ""], ["Gotsman", "Alexey", ""], ["Perrin", "Matthieu", ""], ["Sutra", "Pierre", ""]]}, {"id": "2003.11859", "submitter": "Chiara De Luca", "authors": "Bruno Golosio, Chiara De Luca, Cristiano Capone, Elena Pastorelli,\n  Giovanni Stegel, Gianmarco Tiddia, Giulia De Bonis and Pier Stanislao\n  Paolucci", "title": "Thalamo-cortical spiking model of incremental learning combining\n  perception, context and NREM-sleep-mediated noise-resilience", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The brain exhibits capabilities of fast incremental learning from few noisy\nexamples, as well as the ability to associate similar memories in\nautonomously-created categories and to combine contextual hints with sensory\nperceptions. Together with sleep, these mechanisms are thought to be key\ncomponents of many high-level cognitive functions. Yet, little is known about\nthe underlying processes and the specific roles of different brain states. In\nthis work, we exploited the combination of context and perception in a\nthalamo-cortical model based on a soft winner-take-all circuit of excitatory\nand inhibitory spiking neurons. After calibrating this model to express awake\nand deep-sleep states with features comparable with biological measures, we\ndemonstrate the model capability of fast incremental learning from few\nexamples, its resilience when proposed with noisy perceptions and contextual\nsignals, and an improvement in visual classification after sleep due to induced\nsynaptic homeostasis and association of similar memories.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 12:18:35 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 07:42:16 GMT"}, {"version": "v3", "created": "Fri, 19 Mar 2021 10:30:45 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Golosio", "Bruno", ""], ["De Luca", "Chiara", ""], ["Capone", "Cristiano", ""], ["Pastorelli", "Elena", ""], ["Stegel", "Giovanni", ""], ["Tiddia", "Gianmarco", ""], ["De Bonis", "Giulia", ""], ["Paolucci", "Pier Stanislao", ""]]}, {"id": "2003.11902", "submitter": "Rafa{\\l} Skinderowicz", "authors": "Rafa{\\l} Skinderowicz", "title": "Implementing a GPU-based parallel MAX-MIN Ant System", "comments": null, "journal-ref": "Future Generation Computer Systems, Volume 106, 2020, Pages\n  277-295, ISSN 0167-739X", "doi": "10.1016/j.future.2020.01.011", "report-no": null, "categories": "cs.NE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The MAX-MIN Ant System (MMAS) is one of the best-known Ant Colony\nOptimization (ACO) algorithms proven to be efficient at finding satisfactory\nsolutions to many difficult combinatorial optimization problems. The slow-down\nin Moore's law, and the availability of graphics processing units (GPUs)\ncapable of conducting general-purpose computations at high speed, has sparked\nconsiderable research efforts into the development of GPU-based ACO\nimplementations. In this paper, we discuss a range of novel ideas for improving\nthe GPU-based parallel MMAS implementation, allowing it to better utilize the\ncomputing power offered by two subsequent Nvidia GPU architectures.\nSpecifically, based on the weighted reservoir sampling algorithm we propose a\nnovel parallel implementation of the node selection procedure, which is at the\nheart of the MMAS and other ACO algorithms. We also present a memory-efficient\nimplementation of another key-component -- the tabu list structure -- which is\nused in the ACO's solution construction stage. The proposed implementations,\ncombined with the existing approaches, lead to a total of six MMAS variants,\nwhich are evaluated on a set of Traveling Salesman Problem (TSP) instances\nranging from 198 to 3,795 cities. The results show that our MMAS implementation\nis competitive with state-of-the-art GPU-based and multi-core CPU-based\nparallel ACO implementations: in fact, the times obtained for the Nvidia V100\nVolta GPU were up to 7.18x and 21.79x smaller, respectively. The fastest of the\nproposed MMAS variants is able to generate over 1 million candidate solutions\nper second when solving a 1,002-city instance. Moreover, we show that, combined\nwith the 2-opt local search heuristic, the proposed parallel MMAS finds\nhigh-quality solutions for the TSP instances with up to 18,512 nodes.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2020 14:18:34 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Skinderowicz", "Rafa\u0142", ""]]}, {"id": "2003.12091", "submitter": "Jesmin Jahan Tithi", "authors": "Jesmin Jahan Tithi, Sriram Aananthakrishnan, Fabrizio Petrini", "title": "Online and Real-time Object Tracking Algorithm with Extremely Small\n  Matrices", "comments": "5 Pages (4 Pages main paper, 5th page for reference), Accepted for\n  presentation in WHPC 2020 Summit which got canceled for Corona. But it will\n  not be published in Digital Library", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online and Real-time Object Tracking is an interesting workload that can be\nused to track objects (e.g., car, human, animal) in a series of video sequences\nin real-time. For simple object tracking on edge devices, the output of object\ntracking could be as simple as drawing a bounding box around a detected object\nand in some cases, the input matrices used in such computation are quite small\n(e.g., 4x7, 3x3, 5x5, etc). As a result, the amount of actual work is low.\nTherefore, a typical multi-threading based parallelization technique can not\naccelerate the tracking application; instead, a throughput based\nparallelization technique where each thread operates on independent video\nsequences is more rewarding. In this paper, we share our experience in\nparallelizing a Simple Online and Real-time Tracking (SORT) application on\nshared-memory multicores.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 18:22:47 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 18:03:09 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Tithi", "Jesmin Jahan", ""], ["Aananthakrishnan", "Sriram", ""], ["Petrini", "Fabrizio", ""]]}, {"id": "2003.12101", "submitter": "Shulin Zeng", "authors": "Shulin Zeng, Guohao Dai, Hanbo Sun, Kai Zhong, Guangjun Ge, Kaiyuan\n  Guo, Yu Wang, Huazhong Yang", "title": "Enabling Efficient and Flexible FPGA Virtualization for Deep Learning in\n  the Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FPGAs have shown great potential in providing low-latency and\nenergy-efficient solutions for deep neural network (DNN) inference\napplications. Currently, the majority of FPGA-based DNN accelerators in the\ncloud run in a time-division multiplexing way for multiple users sharing a\nsingle FPGA, and require re-compilation with $\\sim$100 s overhead. Such designs\nlead to poor isolation and heavy performance loss for multiple users, which are\nfar away from providing efficient and flexible FPGA virtualization for neither\npublic nor private cloud scenarios.\n  To solve these problems, we introduce a novel virtualization framework for\ninstruction architecture set (ISA) based on DNN accelerators by sharing a\nsingle FPGA. We enable the isolation by introducing a two-level instruction\ndispatch module and a multi-core based hardware resources pool. Such designs\nprovide isolated and runtime-programmable hardware resources, further leading\nto performance isolation for multiple users. On the other hand, to overcome the\nheavy re-compilation overheads, we propose a tiling-based instruction frame\npackage design and two-stage static-dynamic compilation. Only the light-weight\nruntime information is re-compiled with $\\sim$1 ms overhead, thus the\nperformance is guaranteed for the private cloud. Our extensive experimental\nresults show that the proposed virtualization design achieves 1.07-1.69x and\n1.88-3.12x throughput improvement over previous static designs using the\nsingle-core and the multi-core architectures, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 18:34:11 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Zeng", "Shulin", ""], ["Dai", "Guohao", ""], ["Sun", "Hanbo", ""], ["Zhong", "Kai", ""], ["Ge", "Guangjun", ""], ["Guo", "Kaiyuan", ""], ["Wang", "Yu", ""], ["Yang", "Huazhong", ""]]}, {"id": "2003.12141", "submitter": "Francesco Fusco", "authors": "Bradley Eck, Francesco Fusco, Robert Gormally, Mark Purcell, Seshu\n  Tirupathi", "title": "Scalable Deployment of AI Time-series Models for IoT", "comments": null, "journal-ref": "Workshop AI for Internet of Things, IJCAI 2019", "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  IBM Research Castor, a cloud-native system for managing and deploying large\nnumbers of AI time-series models in IoT applications, is described. Modelling\ncode templates, in Python and R, following a typical machine-learning workflow\nare supported. A knowledge-based approach to managing model and time-series\ndata allows the use of general semantic concepts for expressing feature\nengineering tasks. Model templates can be programmatically deployed against\nspecific instances of semantic concepts, thus supporting model reuse and\nautomated replication as the IoT application grows. Deployed models are\nautomatically executed in parallel leveraging a serverless cloud computing\nframework. The complete history of trained model versions and rolling-horizon\npredictions is persisted, thus enabling full model lineage and traceability.\nResults from deployments in real-world smart-grid live forecasting applications\nare reported. Scalability of executing up to tens of thousands of AI modelling\ntasks is also evaluated.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 14:27:25 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Eck", "Bradley", ""], ["Fusco", "Francesco", ""], ["Gormally", "Robert", ""], ["Purcell", "Mark", ""], ["Tirupathi", "Seshu", ""]]}, {"id": "2003.12203", "submitter": "Kai Zhao", "authors": "Kai Zhao, Sheng Di, Sihuan Li, Xin Liang, Yujia Zhai, Jieyang Chen,\n  Kaiming Ouyang, Franck Cappello, Zizhong Chen", "title": "FT-CNN: Algorithm-Based Fault Tolerance for Convolutional Neural\n  Networks", "comments": "13 pages", "journal-ref": "IEEE Transactions on Parallel and Distributed Systems, 2020", "doi": "10.1109/TPDS.2020.3043449", "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) are becoming more and more important for\nsolving challenging and critical problems in many fields. CNN inference\napplications have been deployed in safety-critical systems, which may suffer\nfrom soft errors caused by high-energy particles, high temperature, or abnormal\nvoltage. Of critical importance is ensuring the stability of the CNN inference\nprocess against soft errors. Traditional fault tolerance methods are not\nsuitable for CNN inference because error-correcting code is unable to protect\ncomputational components, instruction duplication techniques incur high\noverhead, and existing algorithm-based fault tolerance (ABFT) techniques cannot\nprotect all convolution implementations. In this paper, we focus on how to\nprotect the CNN inference process against soft errors as efficiently as\npossible, with the following three contributions. (1) We propose several\nsystematic ABFT schemes based on checksum techniques and analyze their fault\nprotection ability and runtime thoroughly.Unlike traditional ABFT based on\nmatrix-matrix multiplication, our schemes support any convolution\nimplementations. (2) We design a novel workflow integrating all the proposed\nschemes to obtain a high detection/correction ability with limited total\nruntime overhead. (3) We perform our evaluation using ImageNet with well-known\nCNN models including AlexNet, VGG-19, ResNet-18, and YOLOv2. Experimental\nresults demonstrate that our implementation can handle soft errors with very\nlimited runtime overhead (4%~8% in both error-free and error-injected\nsituations).\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 02:01:54 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 16:13:33 GMT"}, {"version": "v3", "created": "Wed, 15 Jul 2020 02:27:25 GMT"}, {"version": "v4", "created": "Mon, 7 Sep 2020 20:34:16 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Zhao", "Kai", ""], ["Di", "Sheng", ""], ["Li", "Sihuan", ""], ["Liang", "Xin", ""], ["Zhai", "Yujia", ""], ["Chen", "Jieyang", ""], ["Ouyang", "Kaiming", ""], ["Cappello", "Franck", ""], ["Chen", "Zizhong", ""]]}, {"id": "2003.12375", "submitter": "Bryan Ford", "authors": "Bryan Ford", "title": "Democratic Value and Money for Decentralized Digital Society", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical monetary systems regularly subject the most vulnerable majority of\nthe world's population to debilitating financial shocks, and have manifestly\nallowed uncontrolled global inequality over the long term. Given these basic\nfailures, how can we avoid asking whether mainstream macroeconomic principles\nare actually compatible with democratic principles such as equality or the\nprotection of human rights and dignity? This idea paper takes a constructive\nlook at this question, by exploring how alternate monetary principles might\nresult in a form of money more compatible with democratic principles -- dare we\ncall it \"democratic money\"? In this alternative macroeconomic philosophy, both\nthe supply of and the demand for money must be rooted in people, so as to give\nall people both equal opportunities for economic participation. Money must be\ndesigned around equality, not only across all people alive at a given moment,\nbut also across past and future generations of people, guaranteeing that our\ndescendants cannot be enslaved by their ancestors' economic luck or misfortune.\nDemocratic money must reliably give all people a means to enable everyday\ncommerce, investment, and value creation in good times and bad, and must impose\nhard limits on financial inequality. Democratic money must itself be governed\ndemocratically, and must economically facilitate the needs of citizens in a\ndemocracy for trustworthy and unbiased information with which to make wise\ncollective decisions. An intriguing approach to implementing and deploying\ndemocratic money is via a cryptocurrency built on a proof-of-personhood\nfoundation, giving each opt-in human participant one equal unit of stake. Such\na cryptocurrency would have both interesting similarities to, and important\ndifferences from, a Universal Basic Income (UBI) denominated in an existing\ncurrency.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 09:30:47 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Ford", "Bryan", ""]]}, {"id": "2003.12423", "submitter": "Naeimeh Omidvar", "authors": "Naeimeh Omidvar, Mohammad Ali Maddah-Ali, Hamed Mahdavi", "title": "A Hybrid-Order Distributed SGD Method for Non-Convex Optimization to\n  Balance Communication Overhead, Computational Complexity, and Convergence\n  Rate", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.IT math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a method of distributed stochastic gradient descent\n(SGD), with low communication load and computational complexity, and still fast\nconvergence. To reduce the communication load, at each iteration of the\nalgorithm, the worker nodes calculate and communicate some scalers, that are\nthe directional derivatives of the sample functions in some \\emph{pre-shared\ndirections}. However, to maintain accuracy, after every specific number of\niterations, they communicate the vectors of stochastic gradients. To reduce the\ncomputational complexity in each iteration, the worker nodes approximate the\ndirectional derivatives with zeroth-order stochastic gradient estimation, by\nperforming just two function evaluations rather than computing a first-order\ngradient vector. The proposed method highly improves the convergence rate of\nthe zeroth-order methods, guaranteeing order-wise faster convergence. Moreover,\ncompared to the famous communication-efficient methods of model averaging (that\nperform local model updates and periodic communication of the gradients to\nsynchronize the local models), we prove that for the general class of\nnon-convex stochastic problems and with reasonable choice of parameters, the\nproposed method guarantees the same orders of communication load and\nconvergence rate, while having order-wise less computational complexity.\nExperimental results on various learning problems in neural networks\napplications demonstrate the effectiveness of the proposed approach compared to\nvarious state-of-the-art distributed SGD methods.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 14:02:15 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Omidvar", "Naeimeh", ""], ["Maddah-Ali", "Mohammad Ali", ""], ["Mahdavi", "Hamed", ""]]}, {"id": "2003.12448", "submitter": "Lev Mukhanov", "authors": "Lev Mukhanov, Konstantinos Tovletoglou, Hans Vandierendonck, Dimitrios\n  S. Nikolopoulos, Georgios Karakonstantis", "title": "Workload-Aware DRAM Error Prediction using Machine Learning", "comments": null, "journal-ref": "In Proceedings of the IEEE International Symposium on Workload\n  Characterization (IISWC), Orlando, Florida, USA, 2019", "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aggressive scaling of technology may have helped to meet the growing\ndemand for higher memory capacity and density, but has also made DRAM cells\nmore prone to errors. Such a reality triggered a lot of interest in modeling\nDRAM behavior for either predicting the errors in advance or for adjusting DRAM\ncircuit parameters to achieve a better trade-off between energy efficiency and\nreliability. Existing modeling efforts may have studied the impact of few\noperating parameters and temperature on DRAM reliability using custom FPGAs\nsetups, however they neglected the combined effect of workload-specific\nfeatures that can be systematically investigated only on a real system. In this\npaper, we present the results of our study on workload-dependent DRAM error\nbehavior within a real server considering various operating parameters, such as\nthe refresh rate, voltage and temperature. We show that the rate of single- and\nmulti-bit errors may vary across workloads by 8x, indicating that program\ninherent features can affect DRAM reliability significantly. Based on this\nobservation, we extract 249 features, such as the memory access rate, the rate\nof cache misses, the memory reuse time and data entropy, from various\ncompute-intensive, caching and analytics benchmarks. We apply several\nsupervised learning methods to construct the DRAM error behavior model for 72\nserver-grade DRAM chips using the memory operating parameters and extracted\nprogram inherent features. Our results show that, with an appropriate choice of\nprogram features and supervised learning method, the rate of single- and\nmulti-bit errors can be predicted for a specific DRAM module with an average\nerror of less than 10.5 %, as opposed to the 2.9x estimation error obtained for\na conventional workload-unaware error model.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 05:09:17 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Mukhanov", "Lev", ""], ["Tovletoglou", "Konstantinos", ""], ["Vandierendonck", "Hans", ""], ["Nikolopoulos", "Dimitrios S.", ""], ["Karakonstantis", "Georgios", ""]]}, {"id": "2003.12449", "submitter": "Lucian Petrica", "authors": "Mairin Kroes, Lucian Petrica, Sorin Cotofana, Michaela Blott", "title": "Evolutionary Bin Packing for Memory-Efficient Dataflow Inference\n  Acceleration on FPGA", "comments": "To appear in GECCO 2020 Proceedings", "journal-ref": null, "doi": "10.1145/3377930.3389808", "report-no": null, "categories": "cs.DC cs.NE eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural network (CNN) dataflow inference accelerators\nimplemented in Field Programmable Gate Arrays (FPGAs) have demonstrated\nincreased energy efficiency and lower latency compared to CNN execution on CPUs\nor GPUs. However, the complex shapes of CNN parameter memories do not typically\nmap well to FPGA on-chip memories (OCM), which results in poor OCM utilization\nand ultimately limits the size and types of CNNs which can be effectively\naccelerated on FPGAs. In this work, we present a design methodology that\nimproves the mapping efficiency of CNN parameters to FPGA OCM. We frame the\nmapping as a bin packing problem and determine that traditional bin packing\nalgorithms are not well suited to solve the problem within FPGA- and\nCNN-specific constraints. We hybridize genetic algorithms and simulated\nannealing with traditional bin packing heuristics to create flexible mappers\ncapable of grouping parameter memories such that each group optimally fits FPGA\non-chip memories. We evaluate these algorithms on a variety of FPGA inference\naccelerators. Our hybrid mappers converge to optimal solutions in a matter of\nseconds for all CNN use-cases, achieve an increase of up to 65% in OCM\nutilization efficiency for deep CNNs, and are up to 200$\\times$ faster than\ncurrent state-of-the-art simulated annealing approaches.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 09:55:08 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Kroes", "Mairin", ""], ["Petrica", "Lucian", ""], ["Cotofana", "Sorin", ""], ["Blott", "Michaela", ""]]}, {"id": "2003.12452", "submitter": "George K. Thiruvathukal", "authors": "Jack West and Neil Kingensmith and George K. Thiruvathukal", "title": "FLIC: A Distributed Fog Cache for City-Scale Applications", "comments": "Accepted at 2020 IEEE International Conference on Fog Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.OS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present FLIC, a distributed software data caching framework for fogs that\nreduces network traffic and latency. FLICis targeted toward city-scale\ndeployments of cooperative IoT devices in which each node gathers and shares\ndata with surrounding devices. As machine learning and other data processing\ntechniques that require large volumes of training data are ported to low-cost\nand low-power IoT systems, we expect that data analysis will be moved away from\nthe cloud. Separation from the cloud will reduce reliance on power-hungry\ncentralized cloud-based infrastructure. However, city-scale deployments of\ncooperative IoT devices often connect to the Internet with cellular service, in\nwhich service charges are proportional to network usage. IoT system architects\nmust be clever in order to keep costs down in these scenarios. To reduce the\nnetwork bandwidth required to operate city-scale deployments of cooperative IoT\nsystems, FLIC implements a distributed cache on the IoT nodes in the fog. FLIC\nallows the IoT network to share its data without repetitively interacting with\na simple cloud storage service reducing calls out to a backing store. Our\nresults displayed a less than 2% miss rate on reads. Thus, allowing for only 5%\nof requests needing the backing store. We were also able to achieve more than\n50% reduction in bytes transmitted per second.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 00:37:56 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["West", "Jack", ""], ["Kingensmith", "Neil", ""], ["Thiruvathukal", "George K.", ""]]}, {"id": "2003.12476", "submitter": "Sebastiaan Huber", "authors": "Sebastiaan. P. Huber, Spyros Zoupanos, Martin Uhrin, Leopold Talirz,\n  Leonid Kahle, Rico H\\\"auselmann, Dominik Gresch, Tiziano M\\\"uller, Aliaksandr\n  V. Yakutovich, Casper W. Andersen, Francisco F. Ramirez, Carl S. Adorf,\n  Fernando Gargiulo, Snehal Kumbhar, Elsa Passaro, Conrad Johnston, Andrius\n  Merkys, Andrea Cepellotti, Nicolas Mounet, Nicola Marzari, Boris Kozinsky,\n  Giovanni Pizzi", "title": "AiiDA 1.0, a scalable computational infrastructure for automated\n  reproducible workflows and data provenance", "comments": null, "journal-ref": "Scientific Data 7, 300 (2020)", "doi": "10.1038/s41597-020-00638-4", "report-no": null, "categories": "cs.DC cond-mat.mtrl-sci", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ever-growing availability of computing power and the sustained\ndevelopment of advanced computational methods have contributed much to recent\nscientific progress. These developments present new challenges driven by the\nsheer amount of calculations and data to manage. Next-generation exascale\nsupercomputers will harden these challenges, such that automated and scalable\nsolutions become crucial. In recent years, we have been developing AiiDA\n(http://www.aiida.net), a robust open-source high-throughput infrastructure\naddressing the challenges arising from the needs of automated workflow\nmanagement and data provenance recording. Here, we introduce developments and\ncapabilities required to reach sustained performance, with AiiDA supporting\nthroughputs of tens of thousands processes/hour, while automatically preserving\nand storing the full data provenance in a relational database making it\nqueryable and traversable, thus enabling high-performance data analytics.\nAiiDA's workflow language provides advanced automation, error handling features\nand a flexible plugin model to allow interfacing with any simulation software.\nThe associated plugin registry enables seamless sharing of extensions,\nempowering a vibrant user community dedicated to making simulations more\nrobust, user-friendly and reproducible.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 12:06:12 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Huber", "Sebastiaan. P.", ""], ["Zoupanos", "Spyros", ""], ["Uhrin", "Martin", ""], ["Talirz", "Leopold", ""], ["Kahle", "Leonid", ""], ["H\u00e4uselmann", "Rico", ""], ["Gresch", "Dominik", ""], ["M\u00fcller", "Tiziano", ""], ["Yakutovich", "Aliaksandr V.", ""], ["Andersen", "Casper W.", ""], ["Ramirez", "Francisco F.", ""], ["Adorf", "Carl S.", ""], ["Gargiulo", "Fernando", ""], ["Kumbhar", "Snehal", ""], ["Passaro", "Elsa", ""], ["Johnston", "Conrad", ""], ["Merkys", "Andrius", ""], ["Cepellotti", "Andrea", ""], ["Mounet", "Nicolas", ""], ["Marzari", "Nicola", ""], ["Kozinsky", "Boris", ""], ["Pizzi", "Giovanni", ""]]}, {"id": "2003.12477", "submitter": "Jan Baumeister", "authors": "Jan Baumeister and Bernd Finkbeiner and Maximilian Schwenger and Hazem\n  Torfah", "title": "FPGA Stream-Monitoring of Real-time Properties", "comments": null, "journal-ref": "ACM Transactions on Embedded Computing Systems 2019", "doi": null, "report-no": null, "categories": "cs.DC cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An essential part of cyber-physical systems is the online evaluation of\nreal-time data streams. Especially in systems that are intrinsically\nsafety-critical, a dedicated monitoring component inspecting data streams to\ndetect problems at runtime greatly increases the confidence in a safe\nexecution. Such a monitor needs to be based on a specification language capable\nof expressing complex, high-level properties using only the accessible\nlow-level signals. Moreover, tight constraints on computational resources\nexacerbate the requirements on the monitor. Thus, several existing approaches\nto monitoring are not applicable due to their dependence on an operating\nsystem. We present an FPGA-based monitoring approach by compiling an RTLola\nspecification into synthesizable VHDL code. RTLola is a stream-based\nspecification language capable of expressing complex real-time properties while\nproviding an upper bound on the execution time and memory requirements. The\nstatically determined memory bound allows for a compilation to an FPGA with a\nfixed size. An advantage of FPGAs is a simple integration process in existing\nsystems and superb executing time. The compilation results in a highly parallel\nimplementation thanks to the modular nature of RTLola specifications. This\nfurther increases the maximal event rate the monitor can handle.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 14:49:20 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Baumeister", "Jan", ""], ["Finkbeiner", "Bernd", ""], ["Schwenger", "Maximilian", ""], ["Torfah", "Hazem", ""]]}, {"id": "2003.12488", "submitter": "Qianlin Liang", "authors": "Qianlin Liang, Prashant Shenoy, David Irwin", "title": "AI on the Edge: Rethinking AI-based IoT Applications Using Specialized\n  Edge Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge computing has emerged as a popular paradigm for supporting mobile and\nIoT applications with low latency or high bandwidth needs. The attractiveness\nof edge computing has been further enhanced due to the recent availability of\nspecial-purpose hardware to accelerate specific compute tasks, such as deep\nlearning inference, on edge nodes. In this paper, we experimentally compare the\nbenefits and limitations of using specialized edge systems, built using edge\naccelerators, to more traditional forms of edge and cloud computing. Our\nexperimental study using edge-based AI workloads shows that today's edge\naccelerators can provide comparable, and in many cases better, performance,\nwhen normalized for power or cost, than traditional edge and cloud servers.\nThey also provide latency and bandwidth benefits for split processing, across\nand within tiers, when using model compression or model splitting, but require\ndynamic methods to determine the optimal split across tiers. We find that edge\naccelerators can support varying degrees of concurrency for multi-tenant\ninference applications, but lack isolation mechanisms necessary for edge cloud\nmulti-tenant hosting.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 15:43:01 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Liang", "Qianlin", ""], ["Shenoy", "Prashant", ""], ["Irwin", "David", ""]]}, {"id": "2003.12558", "submitter": "Sangamesh Kodge", "authors": "Mustafa Ali, Akhilesh Jaiswal, Sangamesh Kodge, Amogh Agrawal,\n  Indranil Chakraborty, and Kaushik Roy", "title": "IMAC: In-memory multi-bit Multiplication andACcumulation in 6T SRAM\n  Array", "comments": "(In-proceedings)2020 Accepted in IEEE Transactions on Circuits and\n  Systems-I Journal", "journal-ref": null, "doi": "10.1109/TCSI.2020.2981901", "report-no": null, "categories": "cs.ET cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  `In-memory computing' is being widely explored as a novel computing paradigm\nto mitigate the well known memory bottleneck. This emerging paradigm aims at\nembedding some aspects of computations inside the memory array, thereby\navoiding frequent and expensive movement of data between the compute unit and\nthe storage memory. In-memory computing with respect to Silicon memories has\nbeen widely explored on various memory bit-cells. Embedding computation inside\nthe 6 transistor (6T) SRAM array is of special interest since it is the most\nwidely used on-chip memory. In this paper, we present a novel in-memory\nmultiplication followed by accumulation operation capable of performing\nparallel dot products within 6T SRAM without any changes to the standard\nbitcell. We, further, study the effect of circuit non-idealities and process\nvariations on the accuracy of the LeNet-5 and VGG neural network architectures\nagainst the MNIST and CIFAR-10 datasets, respectively. The proposed in-memory\ndot-product mechanism achieves 88.8% and 99% accuracy for the CIFAR-10 and\nMNIST, respectively. Compared to the standard von Neumann system, the proposed\nsystem is 6.24x better in energy consumption and 9.42x better in delay.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 17:43:19 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Ali", "Mustafa", ""], ["Jaiswal", "Akhilesh", ""], ["Kodge", "Sangamesh", ""], ["Agrawal", "Amogh", ""], ["Chakraborty", "Indranil", ""], ["Roy", "Kaushik", ""]]}, {"id": "2003.12631", "submitter": "Minghui LiWang", "authors": "Zhibin Gao, Minghui LiWang, Seyyedali Hosseinalipour, Huaiyu Dai,\n  Xianbin Wang", "title": "A Truthful Auction for Graph Job Allocation in Vehicular Cloud-assisted\n  Networks", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicular cloud computing has emerged as a promising solution to fulfill\nusers' demands on processing computation-intensive applications in modern\ndriving environments. Such applications are commonly represented by graphs\nconsisting of components and edges. However, encouraging vehicles to share\nresources poses significant challenges owing to users' selfishness. In this\npaper, an auction-based graph job allocation problem is studied in vehicular\ncloud-assisted networks considering resource reutilization. Our goal is to map\neach buyer (component) to a feasible seller (virtual machine) while maximizing\nthe buyers' utility-of-service, which concerns the execution time and\ncommission cost. First, we formulate the auction-based graph job allocation as\nan integer programming (IP) problem. Then, a Vickrey-Clarke-Groves based\npayment rule is proposed which satisfies the desired economical properties,\ntruthfulness and individual rationality. We face two challenges: 1) the\nabove-mentioned IP problem is NP-hard; 2) one constraint associated with the IP\nproblem poses addressing the subgraph isomorphism problem. Thus, obtaining the\noptimal solution is practically infeasible in large-scale networks. Motivated\nby which, we develop a structure-preserved matching algorithm by maximizing the\nutility-of-service-gain, and the corresponding payment rule which offers\neconomical properties and low computation complexity. Extensive simulations\ndemonstrate that the proposed algorithm outperforms the benchmark methods\nconsidering various problem sizes.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 20:43:19 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 02:38:00 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Gao", "Zhibin", ""], ["LiWang", "Minghui", ""], ["Hosseinalipour", "Seyyedali", ""], ["Dai", "Huaiyu", ""], ["Wang", "Xianbin", ""]]}, {"id": "2003.12671", "submitter": "Phuong-Duy Nguyen", "authors": "Phuong-Duy Nguyen and Long Bao Le", "title": "Joint Computation Offloading, SFC Placement, and Resource Allocation for\n  Multi-Site MEC Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network function Virtualization (NFV) and Mobile Edge Computing (MEC) are\npromising 5G technologies to support resource-demanding mobile applications. In\nNFV, one must process the service function chain (SFC) in which a set of\nnetwork functions must be executed in a specific order. Moreover, the MEC\ntechnology enables computation offloading of service requests from mobile users\nto remote servers to potentially reduce energy consumption and processing delay\nfor the mobile application. This paper considers the optimization of the\ncomputation offloading, resource allocation, and SFC placement in the\nmulti-site MEC system. Our design objective is to minimize the weighted\nnormalized energy consumption and computing cost subject to the maximum\ntolerable delay constraint. To solve the underlying mixed integer and\nnon-linear optimization problem, we employ the decomposition approach where we\niteratively optimize the computation offloading, SFC placement and computing\nresource allocation to obtain an efficient solution. Numerical results show the\nimpacts of different parameters on the system performance and the superior\nperformance of the proposed algorithm compared to benchmarking algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2020 00:27:07 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Nguyen", "Phuong-Duy", ""], ["Le", "Long Bao", ""]]}, {"id": "2003.12677", "submitter": "Anuradha Trivedi", "authors": "Stefano Marchesini (1), Anuradha Trivedi (2), Pablo Enfedaque (3),\n  Talita Perciano (3), Dilworth Parkinson (4) ((1) Sigray, Inc., (2) Virginia\n  Polytechnic Institute and State University, (3) Computational Research\n  Division, Lawrence Berkeley National Laboratory, (4) Advanced Light Source,\n  Lawrence Berkeley National Laboratory)", "title": "Sparse Matrix-Based HPC Tomography", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-50371-0_18", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tomographic imaging has benefited from advances in X-ray sources, detectors\nand optics to enable novel observations in science, engineering and medicine.\nThese advances have come with a dramatic increase of input data in the form of\nfaster frame rates, larger fields of view or higher resolution, so high\nperformance solutions are currently widely used for analysis. Tomographic\ninstruments can vary significantly from one to another, including the hardware\nemployed for reconstruction: from single CPU workstations to large scale hybrid\nCPU/GPU supercomputers. Flexibility on the software interfaces and\nreconstruction engines are also highly valued to allow for easy development and\nprototyping. This paper presents a novel software framework for tomographic\nanalysis that tackles all aforementioned requirements. The proposed solution\ncapitalizes on the increased performance of sparse matrix-vector multiplication\nand exploits multi-CPU and GPU reconstruction over MPI. The solution is\nimplemented in Python and relies on CuPy for fast GPU operators and CUDA kernel\nintegration, and on SciPy for CPU sparse matrix computation. As opposed to\nprevious tomography solutions that are tailor-made for specific use cases or\nhardware, the proposed software is designed to provide flexible, portable and\nhigh-performance operators that can be used for continuous integration at\ndifferent production environments, but also for prototyping new experimental\nsettings or for algorithmic development. The experimental results demonstrate\nhow our implementation can even outperform state-of-the-art software packages\nused at advanced X-ray sources worldwide.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2020 01:20:15 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 04:43:56 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Marchesini", "Stefano", ""], ["Trivedi", "Anuradha", ""], ["Enfedaque", "Pablo", ""], ["Perciano", "Talita", ""], ["Parkinson", "Dilworth", ""]]}, {"id": "2003.13083", "submitter": "Hao Xu", "authors": "Hao Xu, Paulo Valente Klaine, Oluwakayode Onireti, Bin Cao, Muhammad\n  Imran, Lei Zhang", "title": "Blockchain-enabled Resource Management and Sharing for 6G Communications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.SY eess.SP eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The sixth generation (6G) network must provide performance superior to\nprevious generations in order to meet the requirements of emerging services and\napplications, such as multi-gigabit transmission rate, even higher reliability,\nsub 1 millisecond latency and ubiquitous connection for Internet of Everything.\nHowever, with the scarcity of spectrum resources, efficient resource management\nand sharing is crucial to achieve all these ambitious requirements. One\npossible technology to enable all of this is blockchain, which has recently\ngained significance and will be of paramount importance to 6G networks and\nbeyond due to its inherent properties. In particular, the integration of\nblockchain in 6G will enable the network to monitor and manage resource\nutilization and sharing efficiently. Hence, in this article, we discuss the\npotentials of blockchain for resource management and sharing in 6G using\nmultiple application scenarios namely, Internet of things, device-to-device\ncommunications, network slicing, and inter-domain blockchain ecosystems.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 17:38:49 GMT"}, {"version": "v2", "created": "Sat, 11 Apr 2020 11:57:16 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Xu", "Hao", ""], ["Klaine", "Paulo Valente", ""], ["Onireti", "Oluwakayode", ""], ["Cao", "Bin", ""], ["Imran", "Muhammad", ""], ["Zhang", "Lei", ""]]}, {"id": "2003.13155", "submitter": "Zhuolun Xiang", "authors": "Ittai Abraham, Kartik Nayak, Ling Ren, Zhuolun Xiang", "title": "Byzantine Agreement, Broadcast and State Machine Replication with\n  Near-optimal Good-case Latency", "comments": "A brief announcement appeared in DISC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the problem \\textit{good-case latency} of Byzantine\nagreement, broadcast and state machine replication in the synchronous\nauthenticated setting. The good-case latency measure captures the time it takes\nto reach agreement when all non-faulty parties have the same input (or in\nBB/SMR when the sender/leader is non-faulty). Previous result implies a lower\nbound showing that any Byzantine agreement or broadcast protocol tolerating\nmore than $n/3$ faults must have a good-case latency of at least $\\Delta$,\nwhere $\\Delta$ is the assumed maximum message delay bound. Our first result is\na family of protocols we call $1\\Delta$ that have near-optimal good-case\nlatency. We propose a protocol $1\\Delta$-BA that solves Byzantine agreement in\nthe synchronous and authenticated setting with near-optimal good-case latency\nof $\\Delta+2\\delta$ and optimal resilience $f<n/2$, where $\\delta$ is the\nactual (unknown) delay bound. We then extend our protocol and present\n$1\\Delta$-BB and $1\\Delta$-SMR for Byzantine fault tolerant broadcast and state\nmachine replication, respectively, in the same setting and with the same\ngood-case latency of $\\Delta+2\\delta$ and $f<n/2$ fault tolerance. Our\n$1\\Delta$-SMR upper bound improves the gap between the best current solution,\nSync HotStuff, which obtains a good-case latency of $2\\Delta$ per command and\nthe lower bound of $\\Delta$ on good-case latency. Finally, we investigate\nweaker notions of the synchronous setting and show how to adopt the $1\\Delta$\napproach to these models.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 22:53:34 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 15:57:07 GMT"}, {"version": "v3", "created": "Thu, 17 Dec 2020 07:19:15 GMT"}, {"version": "v4", "created": "Sat, 19 Dec 2020 21:13:55 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Abraham", "Ittai", ""], ["Nayak", "Kartik", ""], ["Ren", "Ling", ""], ["Xiang", "Zhuolun", ""]]}, {"id": "2003.13174", "submitter": "Massimiliano Dibitonto", "authors": "Pasquale Giampa (1 and 2) and Massimiliano Dibitonto (1 and 2) ((1)\n  Redev Technology Ltd London UK, (2) Link Campus University Rome IT)", "title": "MIP An AI Distributed Architectural Model to Introduce Cognitive\n  computing capabilities in Cyber Physical Systems (CPS)", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the MIP Platform architecture model, a novel AI-based\ncognitive computing platform architecture. The goal of the proposed application\nof MIP is to reduce the implementation burden for the usage of AI algorithms\napplied to cognitive computing and fluent HMI interactions within the\nmanufacturing process in a cyber-physical production system. The cognitive\ninferencing engine of MIP is a deterministic cognitive module that processes\ndeclarative goals, identifies Intents and Entities, selects suitable actions\nand associated algorithms, and invokes for the execution a processing logic\n(Function) configured in the internal Function-as-aService or Connectivity\nEngine. Constant observation and evaluation against performance criteria assess\nthe performance of Lambda(s) for many and varying scenarios. The modular design\nwith well-defined interfaces enables the reusability and extensibility of FaaS\ncomponents. An integrated BigData platform implements this modular design\nsupported by technologies such as Docker, Kubernetes for virtualization and\norchestration of the individual components and their communication. The\nimplementation of the architecture is evaluated using a real-world use case\nlater discussed in this paper.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 00:59:31 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Giampa", "Pasquale", "", "1 and 2"], ["Dibitonto", "Massimiliano", "", "1 and 2"]]}, {"id": "2003.13376", "submitter": "Yansong Gao Dr", "authors": "Yansong Gao, Minki Kim, Sharif Abuadbba, Yeonjae Kim, Chandra Thapa,\n  Kyuyeon Kim, Seyit A. Camtepe, Hyoungshick Kim, Surya Nepal", "title": "End-to-End Evaluation of Federated Learning and Split Learning for\n  Internet of Things", "comments": "10 pages, 12 figures", "journal-ref": "The 39th International Symposium on Reliable Distributed Systems\n  (SRDS) 2020", "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is the first attempt to evaluate and compare felderated learning\n(FL) and split neural networks (SplitNN) in real-world IoT settings in terms of\nlearning performance and device implementation overhead. We consider a variety\nof datasets, different model architectures, multiple clients, and various\nperformance metrics. For learning performance, which is specified by the model\naccuracy and convergence speed metrics, we empirically evaluate both FL and\nSplitNN under different types of data distributions such as imbalanced and\nnon-independent and identically distributed (non-IID) data. We show that the\nlearning performance of SplitNN is better than FL under an imbalanced data\ndistribution, but worse than FL under an extreme non-IID data distribution. For\nimplementation overhead, we end-to-end mount both FL and SplitNN on Raspberry\nPis, and comprehensively evaluate overheads including training time,\ncommunication overhead under the real LAN setting, power consumption and memory\nusage. Our key observations are that under IoT scenario where the communication\ntraffic is the main concern, the FL appears to perform better over SplitNN\nbecause FL has the significantly lower communication overhead compared with\nSplitNN, which empirically corroborate previous statistical analysis. In\naddition, we reveal several unrecognized limitations about SplitNN, forming the\nbasis for future research.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 12:12:51 GMT"}, {"version": "v2", "created": "Sun, 2 Aug 2020 08:51:07 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Gao", "Yansong", ""], ["Kim", "Minki", ""], ["Abuadbba", "Sharif", ""], ["Kim", "Yeonjae", ""], ["Thapa", "Chandra", ""], ["Kim", "Kyuyeon", ""], ["Camtepe", "Seyit A.", ""], ["Kim", "Hyoungshick", ""], ["Nepal", "Surya", ""]]}, {"id": "2003.13461", "submitter": "Mohammad Mahdi Kamani", "authors": "Yuyang Deng, Mohammad Mahdi Kamani, Mehrdad Mahdavi", "title": "Adaptive Personalized Federated Learning", "comments": "[v3] Added convergence analysis for nonconvex losses and additional\n  experiments along with new baselines [v2] A new generalization analysis is\n  provided. Also, additional experiments are added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Investigation of the degree of personalization in federated learning\nalgorithms has shown that only maximizing the performance of the global model\nwill confine the capacity of the local models to personalize. In this paper, we\nadvocate an adaptive personalized federated learning (APFL) algorithm, where\neach client will train their local models while contributing to the global\nmodel. We derive the generalization bound of mixture of local and global\nmodels, and find the optimal mixing parameter. We also propose a\ncommunication-efficient optimization method to collaboratively learn the\npersonalized models and analyze its convergence in both smooth strongly convex\nand nonconvex settings. The extensive experiments demonstrate the effectiveness\nof our personalization schema, as well as the correctness of established\ngeneralization theories.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 13:19:37 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 18:10:22 GMT"}, {"version": "v3", "created": "Fri, 6 Nov 2020 04:07:31 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Deng", "Yuyang", ""], ["Kamani", "Mohammad Mahdi", ""], ["Mahdavi", "Mehrdad", ""]]}, {"id": "2003.13585", "submitter": "Quanquan C. Liu", "authors": "Laxman Dhulipala, Quanquan C. Liu, Julian Shun, Shangdi Yu", "title": "Parallel Batch-Dynamic $k$-Clique Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study new batch-dynamic algorithms for the $k$-clique\ncounting problem, which are dynamic algorithms where the updates are batches of\nedge insertions and deletions. We study this problem in the parallel setting,\nwhere the goal is to obtain algorithms with low (polylogarithmic) depth. Our\nfirst result is a new parallel batch-dynamic triangle counting algorithm with\n$O(\\Delta\\sqrt{\\Delta+m})$ amortized work and $O(\\log^* (\\Delta+m))$ depth with\nhigh probability, and $O(\\Delta+m)$ space for a batch of $\\Delta$ edge\ninsertions or deletions. Our second result is an algebraic algorithm based on\nparallel fast matrix multiplication. Assuming that a parallel fast matrix\nmultiplication algorithm exists with parallel matrix multiplication constant\n$\\omega_p$, the same algorithm solves dynamic $k$-clique counting with\n$O\\left(\\min\\left(\\Delta m^{\\frac{(2k - 1)\\omega_p}{3(\\omega_p + 1)}},\n(\\Delta+m)^{\\frac{2(k + 1)\\omega_p}{3(\\omega_p + 1)}}\\right)\\right)$ amortized\nwork and $O(\\log (\\Delta+m))$ depth with high probability, and\n$O\\left((\\Delta+m)^{\\frac{2(k + 1)\\omega_p}{3(\\omega_p + 1)}}\\right)$ space.\nUsing a recently developed parallel $k$-clique counting algorithm, we also\nobtain a simple batch-dynamic algorithm for $k$-clique counting on graphs with\narboricity $\\alpha$ running in $O(\\Delta(m+\\Delta)\\alpha^{k-4})$ expected work\nand $O(\\log^{k-2} n)$ depth with high probability, and $O(m + \\Delta)$ space.\nFinally, we present a multicore CPU implementation of our parallel\nbatch-dynamic triangle counting algorithm. On a 72-core machine with two-way\nhyper-threading, our implementation achieves 36.54--74.73x parallel speedup,\nand in certain cases achieves significant speedups over existing parallel\nalgorithms for the problem, which are not theoretically-efficient.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 16:02:14 GMT"}, {"version": "v2", "created": "Sun, 13 Dec 2020 08:08:41 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Dhulipala", "Laxman", ""], ["Liu", "Quanquan C.", ""], ["Shun", "Julian", ""], ["Yu", "Shangdi", ""]]}, {"id": "2003.13617", "submitter": "Javad Ghofrani", "authors": "Kirill Loisha, Javad Ghofrani, Dirk Reichelt", "title": "A Systematic Mapping Study on Blockchain Technology for Digital\n  Protection of Communication with Industrial Control", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the next few years, Blockchain will play a central role in IoT as a\ntechnology. It enables the traceability of processes between multiple parties\nindependent of a central instance. Blockchain allows to make the processes more\ntransparent, cheaper, and safer. This research paper was conducted as\nsystematic literature search. Our aim is to understand current state of\nimplementation in context of Blockchain Technology for digital protection of\ncommunication in industrial cyber-physical systems. We have extracted 28\nprimary papers from scientific databases and classified into different\ncategories using visualizations. The results show that the focus in around 14\\%\npapers is on solution proposal and implementation of use cases \"Secure transfer\nof order data\" using Ethereum Blockchain, 7\\% papers applying Hyperledger\nFabric and Multichain. The majority of research (around 43\\%) is focusing on\nsolution development for supply chain and process traceability.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 16:49:11 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Loisha", "Kirill", ""], ["Ghofrani", "Javad", ""], ["Reichelt", "Dirk", ""]]}, {"id": "2003.13618", "submitter": "Javad Ghofrani", "authors": "Javad Ghofrani, Paul Patoola, Daniel Richter, Dirk Reichelt", "title": "Conceptualizing A Configuration Service for Complex Automation Systems", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arrowhead Framework (AHF) is being developed to enable large-scale IoT based\nautomation by providing an interoperability layer for local clouds. This\nframework aims to create an abstract model for distributed, heterogeneous, and\nnon-linear systems. Managing the variability in such environments plays a key\nrole in handling complex automation tasks such as in smart production systems.\nHowever, there is no standard solution available for handling the variability\nand configuration specifications in such environments. In this paper, we\nanalyze the existing solutions for configuration management in industrial\nautomation frameworks and provide leverage points for a standardization\nframework for handling configurations of automated production systems based on\nthe concept of industrial internet of things.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 16:49:18 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Ghofrani", "Javad", ""], ["Patoola", "Paul", ""], ["Richter", "Daniel", ""], ["Reichelt", "Dirk", ""]]}, {"id": "2003.13629", "submitter": "Glen MacLachlan", "authors": "Glen MacLachlan, Jason Hurlburt, Marco Suarez, Kai Leung Wong, William\n  Burke, Terrence Lewis, Andrew Gallo, Jaroslav Flidr, Raoul Gabiam, Janis\n  Nicholas, Brian Ensor", "title": "Building a Shared Resource HPC Center Across University Schools and\n  Institutes: A Case Study", "comments": "5 pages, 2 tables. Submitted to SC16 and XSEDE16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past several years, The George Washington University has recruited a\nsignificant number of researchers in a wide variety of domains requiring the\navailability of advanced computational resources. We discuss the challenges and\nobstacles encountered planning and establishing a first-time high performance\ncomputing center at the university level and present a set of solutions that\nwill be useful for any university developing a fledgling high performance\ncomputing center. We focus on justification and cost model, strategies for\ndetermining anticipated use cases, planning appropriate resources, staffing,\nuser engagement, and metrics for gauging success.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 17:03:48 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 16:48:22 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["MacLachlan", "Glen", ""], ["Hurlburt", "Jason", ""], ["Suarez", "Marco", ""], ["Wong", "Kai Leung", ""], ["Burke", "William", ""], ["Lewis", "Terrence", ""], ["Gallo", "Andrew", ""], ["Flidr", "Jaroslav", ""], ["Gabiam", "Raoul", ""], ["Nicholas", "Janis", ""], ["Ensor", "Brian", ""]]}, {"id": "2003.13846", "submitter": "Abdullah Alourani", "authors": "Abdullah Alourani and Ajay D. Kshemkalyani", "title": "Provisioning Spot Instances Without Employing Fault-Tolerance Mechanisms", "comments": null, "journal-ref": "International Symposium on Parallel and Distributed Computing,\n  2020, pp. 126-133", "doi": "10.1109/ISPDC51135.2020.00026", "report-no": "ISSN 2379-5352", "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing offers a variable-cost payment scheme that allows cloud\ncustomers to specify the price they are willing to pay for renting spot\ninstances to run their applications at much lower costs than fixed payment\nschemes, and depending on the varying demand from cloud customers, cloud\nplatforms could revoke spot instances at any time. To alleviate the effect of\nspot instance revocations, applications often employ different fault-tolerance\nmechanisms to minimize or even eliminate the lost work for each spot instance\nrevocation. However, these fault-tolerance mechanisms incur additional overhead\nrelated to application completion time and deployment cost. We propose a novel\ncloud market-based approach that leverages cloud spot market features to\nprovision spot instances without employing fault-tolerance mechanisms to reduce\nthe deployment cost and completion time of applications. We evaluate our\napproach in simulations and use Amazon spot instances that contain jobs in\nDocker containers and realistic price traces from EC2 markets. Our simulation\nresults show that our approach reduces the deployment cost and completion time\ncompared to approaches based on fault-tolerance mechanisms.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 22:20:22 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Alourani", "Abdullah", ""], ["Kshemkalyani", "Ajay D.", ""]]}, {"id": "2003.13980", "submitter": "Shi Pu", "authors": "Shi Pu", "title": "A Robust Gradient Tracking Method for Distributed Optimization over\n  Directed Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.MA cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of distributed consensus optimization\nover multi-agent networks with directed network topology. Assuming each agent\nhas a local cost function that is smooth and strongly convex, the global\nobjective is to minimize the average of all the local cost functions. To solve\nthe problem, we introduce a robust gradient tracking method (R-Push-Pull)\nadapted from the recently proposed Push-Pull/AB algorithm. R-Push-Pull inherits\nthe advantages of Push-Pull and enjoys linear convergence to the optimal\nsolution with exact communication. Under noisy information exchange,\nR-Push-Pull is more robust than the existing gradient tracking based\nalgorithms; the solutions obtained by each agent reach a neighborhood of the\noptimum in expectation exponentially fast under a constant stepsize policy. We\nprovide a numerical example that demonstrate the effectiveness of R-Push-Pull.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 06:48:04 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 04:09:18 GMT"}, {"version": "v3", "created": "Thu, 20 Aug 2020 09:40:12 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Pu", "Shi", ""]]}, {"id": "2003.13999", "submitter": "Weilin Zheng", "authors": "Weilin Zheng, Xu Chen, Zibin Zheng, Xiapu Luo, Jiahui Cui", "title": "AxeChain: A Secure and Decentralized blockchain for solving\n  Easily-Verifiable problems", "comments": "14 pages, 14 figures, 3 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Proof-of-Work (PoW) is the most widely used consensus mechanism for\nblockchain, it received harsh criticism due to its massive waste of energy for\nmeaningless hash calculation. Some studies have introduced Proof-of-Stake to\naddress this issue. However, such protocols widen the gap between rich and poor\nand in the worst case lead to an oligopoly, where the rich control the entire\nnetwork. Other studies have attempted to translate the energy consumption of\nPoW into useful work, but they have many limitations, such as narrow\napplication scope, serious security issues and impractical incentive model. In\nthis paper, we introduce AxeChain, which can use the computing power of\nblockchain to solve practical problems raised by users without greatly\ncompromising decentralization or security. AxeChain achieves this by coupling\nhard problem solving with PoW mining. We model the security of AxeChain and\nderive a balance curve between power utilization and system security. That is,\nunder the reasonable assumption that the attack power does not exceed 1/3 of\nthe total power, 1/2 of total power can be safely used to solve practical\nproblems. We also design a novel incentive model based on the amount of work\ninvolved in problem solving, balancing the interests of both the users and\nminers. Moreover, our experimental results show that AxeChain provides strong\nsecurity guarantees, no matter what kind of problem is submitted.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 07:40:16 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Zheng", "Weilin", ""], ["Chen", "Xu", ""], ["Zheng", "Zibin", ""], ["Luo", "Xiapu", ""], ["Cui", "Jiahui", ""]]}, {"id": "2003.14099", "submitter": "Do Le Quoc", "authors": "Franz Gregor and Wojciech Ozga and S\\'ebastien Vaucher and Rafael\n  Pires and Do Le Quoc and Sergei Arnautov and Andr\\'e Martin and Valerio\n  Schiavoni and Pascal Felber and Christof Fetzer", "title": "Trust Management as a Service: Enabling Trusted Execution in the Face of\n  Byzantine Stakeholders", "comments": "European Commission Project: LEGaTO - Low Energy Toolset for\n  Heterogeneous Computing (EC-H2020-780681)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Trust is arguably the most important challenge for critical services both\ndeployed as well as accessed remotely over the network. These systems are\nexposed to a wide diversity of threats, ranging from bugs to exploits, active\nattacks, rogue operators, or simply careless administrators. To protect such\napplications, one needs to guarantee that they are properly configured and\nsecurely provisioned with the \"secrets\" (e.g., encryption keys) necessary to\npreserve not only the confidentiality, integrity and freshness of their data\nbut also their code. Furthermore, these secrets should not be kept under the\ncontrol of a single stakeholder - which might be compromised and would\nrepresent a single point of failure - and they must be protected across\nsoftware versions in the sense that attackers cannot get access to them via\nmalicious updates. Traditional approaches for solving these challenges often\nuse ad hoc techniques and ultimately rely on a hardware security module (HSM)\nas root of trust. We propose a more powerful and generic approach to trust\nmanagement that instead relies on trusted execution environments (TEEs) and a\nset of stakeholders as root of trust. Our system, PALAEMON, can operate as a\nmanaged service deployed in an untrusted environment, i.e., one can delegate\nits operations to an untrusted cloud provider with the guarantee that data will\nremain confidential despite not trusting any individual human (even with root\naccess) nor system software. PALAEMON addresses in a secure, efficient and\ncost-effective way five main challenges faced when developing trusted networked\napplications and services. Our evaluation on a range of benchmarks and real\napplications shows that PALAEMON performs efficiently and can protect secrets\nof services without any change to their source code.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 11:13:05 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Gregor", "Franz", ""], ["Ozga", "Wojciech", ""], ["Vaucher", "S\u00e9bastien", ""], ["Pires", "Rafael", ""], ["Quoc", "Do Le", ""], ["Arnautov", "Sergei", ""], ["Martin", "Andr\u00e9", ""], ["Schiavoni", "Valerio", ""], ["Felber", "Pascal", ""], ["Fetzer", "Christof", ""]]}]