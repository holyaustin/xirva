[{"id": "1907.00048", "submitter": "Georg Hager", "authors": "Johannes Hofmann, Christie L. Alappat, Georg Hager, Dietmar Fey,\n  Gerhard Wellein", "title": "Bridging the Architecture Gap: Abstracting Performance-Relevant\n  Properties of Modern Server Processors", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": "10.14529/jsfi200204", "report-no": null, "categories": "cs.DC cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a universal modeling approach for predicting single- and\nmulticore runtime of steady-state loops on server processors. To this end we\nstrictly differentiate between application and machine models: An application\nmodel comprises the loop code, problem sizes, and other runtime parameters,\nwhile a machine model is an abstraction of all performance-relevant properties\nof a CPU. We introduce a generic method for determining machine models and\npresent results for relevant server-processor architectures by Intel, AMD, IBM,\nand Marvell/Cavium. Considering this wide range of architectures, the set of\nfeatures required for adequate performance modeling is surprisingly small. To\nvalidate our approach, we compare performance predictions to empirical data for\nan OpenMP-parallel preconditioned CG algorithm, which includes compute- and\nmemory-bound kernels. Both single- and multicore analysis shows that the model\nexhibits average and maximum relative errors of 5% and 10%. Deviations from the\nmodel and insights gained are discussed in detail.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 15:04:32 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Hofmann", "Johannes", ""], ["Alappat", "Christie L.", ""], ["Hager", "Georg", ""], ["Fey", "Dietmar", ""], ["Wellein", "Gerhard", ""]]}, {"id": "1907.00050", "submitter": "Youry Khmelevsky", "authors": "Bernd Amann, Youry Khmelevsky and Gaetan Hains", "title": "State-of-the-Art on Query & Transaction Processing Acceleration", "comments": "7 pages, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vast amount of processing power and memory bandwidth provided by modern\nGraphics Processing Units (GPUs) make them a platform for data-intensive\napplications. The database community identified GPUs as effective co-processors\nfor data processing. In the past years, there were many approaches to make use\nof GPUs at different levels of a database system. In this Internal Technical\nReport, based on the [1] and some other research papers, we identify possible\nresearch areas at LIP6 for GPU-accelerated database management systems. We\ndescribe some key properties, typical challenges of GPU-aware database\narchitectures, and identify major open challenges.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 01:46:35 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Amann", "Bernd", ""], ["Khmelevsky", "Youry", ""], ["Hains", "Gaetan", ""]]}, {"id": "1907.00053", "submitter": "Niels Kornerup", "authors": "Cameron Chalk, Niels Kornerup, Wyatt Reeves, David Soloveichik", "title": "Composable Rate-Independent Computation in Continuous Chemical Reaction\n  Networks", "comments": "Appeared at Computational Methods in Systems Biology (CMSB) 2018\n  (best paper award) To appear in IEEE/ACM Transactions on Computational\n  Biology and Bioinformatics", "journal-ref": "Computational Methods in Systems Biology 1 (2018) 256-273", "doi": "10.1007/978-3-319-99429-1_15", "report-no": null, "categories": "cs.ET cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biological regulatory networks depend upon chemical interactions to process\ninformation. Engineering such molecular computing systems is a major challenge\nfor synthetic biology and related fields. The chemical reaction network (CRN)\nmodel idealizes chemical interactions, allowing rigorous reasoning about the\ncomputational power of chemical kinetics. Here we focus on function computation\nwith CRNs, where we think of the initial concentrations of some species as the\ninput and the equilibrium concentration of another species as the output.\nSpecifically, we are concerned with CRNs that are rate-independent (the\ncomputation must be correct independent of the reaction rate law) and\ncomposable ($f\\circ g$ can be computed by concatenating the CRNs computing $f$\nand $g$). Rate independence and composability are important engineering\ndesiderata, permitting implementations that violate mass-action kinetics, or\neven \"well-mixedness\", and allowing the systematic construction of complex\ncomputation via modular design. We show that to construct composable\nrate-independent CRNs, it is necessary and sufficient to ensure that the output\nspecies of a module is not a reactant in any reaction within the module. We\nthen exactly characterize the functions computable by such CRNs as\nsuperadditive, positive-continuous, and piecewise rational linear. Thus\ncomposability severely limits rate-independent computation unless more\nsophisticated input/output encodings are used.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 00:20:11 GMT"}, {"version": "v2", "created": "Sun, 26 Jan 2020 18:40:06 GMT"}, {"version": "v3", "created": "Tue, 22 Sep 2020 00:54:16 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Chalk", "Cameron", ""], ["Kornerup", "Niels", ""], ["Reeves", "Wyatt", ""], ["Soloveichik", "David", ""]]}, {"id": "1907.00096", "submitter": "Jan Verschelde", "authors": "Jasmine Otto, Angus Forbes, and Jan Verschelde", "title": "Solving Polynomial Systems with phcpy", "comments": "Accepted for publication in the SciPy 2019 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC cs.SC math.AG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The solutions of a system of polynomials in several variables are often\nneeded, e.g.: in the design of mechanical systems, and in phase-space analyses\nof nonlinear biological dynamics. Reliable, accurate, and comprehensive\nnumerical solutions are available through PHCpack, a FOSS package for solving\npolynomial systems with homotopy continuation. This paper explores new\ndevelopments in phcpy, a scripting interface for PHCpack, over the past five\nyears. For instance, phcpy is now available online through a JupyterHub server\nfeaturing Python2, Python3, and SageMath kernels. As small systems are solved\nin real-time by phcpy, they are suitable for interactive exploration through\nthe notebook interface. Meanwhile, phcpy supports GPU parallelization,\nimproving the speed and quality of solutions to much larger polynomial systems.\nFrom various model design and analysis problems in STEM, certain classes of\npolynomial system frequently arise, to which phcpy is well-suited.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 22:19:04 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Otto", "Jasmine", ""], ["Forbes", "Angus", ""], ["Verschelde", "Jan", ""]]}, {"id": "1907.00097", "submitter": "Oliver Beckstein", "authors": "Mahzad Khoshlessan and Ioannis Paraskevakos and Geoffrey C. Fox and\n  Shantenu Jha and Oliver Beckstein", "title": "Parallel Performance of Molecular Dynamics Trajectory Analysis", "comments": "accepted manuscript, to appear in 'Concurrency and Computation:\n  Practice and Experience'", "journal-ref": null, "doi": "10.1002/CPE.5789", "report-no": null, "categories": "cs.DC q-bio.QM", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The performance of biomolecular molecular dynamics simulations has steadily\nincreased on modern high performance computing resources but acceleration of\nthe analysis of the output trajectories has lagged behind so that analyzing\nsimulations is becoming a bottleneck. To close this gap, we studied the\nperformance of parallel trajectory analysis with MPI and the Python MDAnalysis\nlibrary on three different XSEDE supercomputers where trajectories were read\nfrom a Lustre parallel file system. Strong scaling performance was impeded by\nstragglers, MPI processes that were slower than the typical process. Stragglers\nwere less prevalent for compute-bound workloads, thus pointing to file reading\nas a bottleneck for scaling. However, a more complicated picture emerged in\nwhich both the computation and the data ingestion exhibited close to ideal\nstrong scaling behavior whereas stragglers were primarily caused by either\nlarge MPI communication costs or long times to open the single shared\ntrajectory file. We improved overall strong scaling performance by either\nsubfiling (splitting the trajectory into separate files) or MPI-IO with\nParallel HDF5 trajectory files. The parallel HDF5 approach resulted in near\nideal strong scaling on up to 384 cores (16 nodes), thus reducing trajectory\nanalysis times by two orders of magnitude compared to the serial approach.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 22:22:24 GMT"}, {"version": "v2", "created": "Sat, 31 Aug 2019 00:05:16 GMT"}, {"version": "v3", "created": "Sun, 2 Feb 2020 20:00:05 GMT"}, {"version": "v4", "created": "Fri, 27 Mar 2020 23:32:52 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Khoshlessan", "Mahzad", ""], ["Paraskevakos", "Ioannis", ""], ["Fox", "Geoffrey C.", ""], ["Jha", "Shantenu", ""], ["Beckstein", "Oliver", ""]]}, {"id": "1907.00140", "submitter": "Kartik Lakhotia", "authors": "Kartik Lakhotia, Qing Dong, Rajgopal Kannan, Viktor Prasanna", "title": "Planting Trees for scalable and efficient Canonical Hub Labeling", "comments": "14 pages, 9 figures, 4 tables", "journal-ref": "Proceedings of the VLDB Endowment, 2020", "doi": "10.14778/3372716.3372722", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point-to-Point Shortest Distance (PPSD) query is a crucial primitive in graph\ndatabase applications. Hub labeling algorithms compute a labeling that converts\na PPSD query into a list intersection problem (over a pre-computed indexing)\nenabling swift query response. However, constructing hub labeling is\ncomputationally challenging. Even state-of-the-art parallel algorithms based on\nPruned Landmark Labeling (PLL) [3], are plagued by large label size, violation\nof given network hierarchy, poor scalability and inability to process large\ngraphs.\n  In this paper, we develop novel parallel shared-memory and distributed-memory\nalgorithms for constructing the Canonical Hub Labeling (CHL) that is minimal in\nsize for a given network hierarchy. To the best of our knowledge, none of the\nexisting parallel algorithms guarantee canonical labeling. Our key\ncontribution, the PLaNT algorithm, scales well beyond the limits of current\npractice by completely avoiding inter-node communication. PLaNT also enables\nthe design of a collaborative label partitioning scheme across multiple nodes\nfor completely in-memory processing of massive graphs whose labels cannot fit\non a single machine.\n  Compared to the sequential PLL, we empirically demonstrate upto 47.4x speedup\non a 72 thread shared-memory platform. On a 64-node cluster, PLaNT achieves an\naverage 42x speedup over single node execution. Finally, we show how our\napproach demonstrates superior scalability - we can process 14x larger graphs\n(in terms of label size) and construct hub labeling orders of magnitude faster\ncompared to state-of-the-art distributed paraPLL algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 03:59:40 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Lakhotia", "Kartik", ""], ["Dong", "Qing", ""], ["Kannan", "Rajgopal", ""], ["Prasanna", "Viktor", ""]]}, {"id": "1907.00194", "submitter": "Adam Lev-Libfeld", "authors": "Adam Lev-Libfeld, Alex Margolin, Amnon Barak", "title": "Open-MPI over MOSIX: paralleled computing in a clustered world", "comments": "Engineering School graduation paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent increased interest in Cloud computing emphasizes the need to find an\nadequate solution to the load-balancing problem in parallel computing --\nefficiently running several jobs concurrently on a cluster of shared computers\n(nodes). One approach to solve this problem is by preemptive process migration\n-- the transfer of running processes between nodes. A possible drawback of this\napproach is the increased overhead between heavily communicating processes.\nThis project presents a solution to this last problem by incorporating the\nprocess migration capability of MOSIX into Open-MPI and by reducing the\nresulting communication overhead. Specifically, we developed a module for\ndirect communication (DiCOM) between migrated Open-MPI processes, to overcome\nthe increased communication latency of TCP/IP between such processes. The\noutcome is reduced run-time by improved resource allocation.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 12:14:04 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Lev-Libfeld", "Adam", ""], ["Margolin", "Alex", ""], ["Barak", "Amnon", ""]]}, {"id": "1907.00434", "submitter": "Raajay Viswanathan", "authors": "Raajay Viswanathan and Aditya Akella", "title": "Network-accelerated Distributed Machine Learning Using MLFabric", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing distributed machine learning (DML) systems focus on improving the\ncomputational efficiency of distributed learning, whereas communication aspects\nhave received less attention. Many DML systems treat the network as a blackbox.\nThus, DML algorithms' performance is impeded by network bottlenecks, and DML\nsystems end up sacrificing important algorithmic and system-level benefits. We\npresent MLfabric, a communication library that manages all network transfers in\na DML system, and holistically determines the communication pattern of a DML\nalgorithm at any point in time. This allows MLfabric to carefully order\ntransfers (i.e., gradient updates) to improve convergence, opportunistically\naggregate updates in-network to improve efficiency, and proactively replicate\nsome of them to support new notions of fault tolerance. We empirically find\nthat MLfabric achieves up to 3X speed-up in training large deep learning models\nin realistic dynamic cluster settings.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 19:12:09 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Viswanathan", "Raajay", ""], ["Akella", "Aditya", ""]]}, {"id": "1907.00498", "submitter": "Evangelos Pournaras", "authors": "Evangelos Pournaras", "title": "Proof of Witness Presence: Blockchain Consensus for Augmented Democracy\n  in Smart Cities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart Cities evolve into complex and pervasive urban environments with a\ncitizens' mandate to meet sustainable development goals. Repositioning\ndemocratic values of citizens' choices in these complex ecosystems has turned\nout to be imperative in an era of social media filter bubbles, fake news and\nopportunities for manipulating electoral results with such means. This paper\nintroduces a new paradigm of augmented democracy that promises actively\nengaging citizens in a more informed decision-making augmented into public\nurban space. The proposed concept is inspired by a digital revive of the\nAncient Agora of Athens, an arena of public discourse, a Polis where citizens\nassemble to actively deliberate and collectively decide about public matters.\nThe core contribution of the proposed paradigm is the concept of proving\nwitness presence: making decision-making subject of providing secure evidence\nand testifying for choices made in the physical space. This paper shows how the\nchallenge of proving witness presence can be tackled with blockchain consensus\nto empower citizens' trust and overcome security vulnerabilities of GPS\nlocalization. Moreover, a novel platform for collective decision-making and\ncrowd-sensing in urban space is introduced: Smart Agora. It is shown how\nreal-time collective measurements over citizens' choices can be made in a fully\ndecentralized and privacy-preserving way. Witness presence is tested by\ndeploying a decentralized system for crowd-sensing the sustainable use of\ntransport means. Furthermore, witness presence of cycling risk is validated\nusing official accident data from public authorities, which are compared\nagainst wisdom of the crowd. The paramount role of dynamic consensus,\nself-governance and ethically aligned artificial intelligence in the augmented\ndemocracy paradigm is outlined.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 23:46:30 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 19:24:32 GMT"}, {"version": "v3", "created": "Mon, 20 Apr 2020 22:07:40 GMT"}, {"version": "v4", "created": "Wed, 8 Jul 2020 22:48:11 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Pournaras", "Evangelos", ""]]}, {"id": "1907.00667", "submitter": "Sebastian G\\\"otschel", "authors": "Sebastian G\\\"otschel and Martin Weiser", "title": "Compression challenges in large scale PDE solvers", "comments": "revised version", "journal-ref": "Algorithms 2019, 12(9), 197", "doi": "10.3390/a12090197", "report-no": "ZIB Report 19-32", "categories": "math.NA cs.DC cs.IT cs.NA math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solvers for partial differential equations (PDE) are one of the cornerstones\nof computational science. For large problems, they involve huge amounts of data\nthat needs to be stored and transmitted on all levels of the memory hierarchy.\nOften, bandwidth is the limiting factor due to relatively small arithmetic\nintensity, and increasingly so due to the growing disparity between computing\npower and bandwidth. Consequently, data compression techniques have been\ninvestigated and tailored towards the specific requirements of PDE solvers\nduring the last decades. This paper surveys data compression challenges and\ncorresponding solution approaches for PDE problems, covering all levels of the\nmemory hierarchy from mass storage up to main memory. Exemplarily, we\nillustrate concepts at particular methods, and give references to alternatives.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 11:26:26 GMT"}, {"version": "v2", "created": "Mon, 8 Jul 2019 07:57:38 GMT"}, {"version": "v3", "created": "Wed, 18 Sep 2019 11:27:05 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["G\u00f6tschel", "Sebastian", ""], ["Weiser", "Martin", ""]]}, {"id": "1907.00670", "submitter": "Bruno Magalhaes", "authors": "Bruno Magalh\\~aes, Michael Hines, Thomas Sterling, Felix Schuermann", "title": "Fully-Asynchronous Fully-Implicit Variable-Order Variable-Timestep\n  Simulation of Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art simulations of detailed neural models follow the Bulk\nSynchronous Parallel execution model. Execution is divided in equidistant\ncommunication intervals, equivalent to the shortest synaptic delay in the\nnetwork. Neurons stepping is performed independently, with collective\ncommunication guiding synchronization and exchange of synaptic events.\n  The interpolation step size is fixed and chosen based on some prior knowledge\nof the fastest possible dynamics in the system. However, simulations driven by\nstiff dynamics or a wide range of time scales - such as multiscale simulations\nof neural networks - struggle with fixed step interpolation methods, yielding\nexcessive computation of intervals of quasi-constant activity, inaccurate\ninterpolation of periods of high volatility solution, and being incapable of\nhandling unknown or distinct time constants. A common alternative is the usage\nof adaptive stepping methods, however they have been deemed inefficient in\nparallel executions due to computational load imbalance at the synchronization\nbarriers that characterize the BSP execution model.\n  We introduce a distributed fully-asynchronous execution model that removes\nglobal synchronization, allowing for longer variable timestep interpolations.\nAsynchronicity is provided by active point-to-point communication notifying\nneurons' time advancement to synaptic connectivities. Time stepping is driven\nby scheduled neuron advancements based on synaptic delays across neurons,\nyielding an \"exhaustive yet not speculative\" adaptive-step execution. Execution\nbenchmarks on 64 Cray XE6 compute nodes demonstrate a reduced number of\ninterpolation steps, higher numerical accuracy and lower time to solution,\ncompared to state-of-the-art methods. Efficiency is shown to be\nactivity-dependent, with scaling of the algorithm demonstrated on a simulation\nof a laboratory experiment.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 11:38:03 GMT"}, {"version": "v2", "created": "Tue, 24 Dec 2019 00:16:26 GMT"}, {"version": "v3", "created": "Wed, 3 Jun 2020 23:16:17 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Magalh\u00e3es", "Bruno", ""], ["Hines", "Michael", ""], ["Sterling", "Thomas", ""], ["Schuermann", "Felix", ""]]}, {"id": "1907.00748", "submitter": "Tadeusz Kobus", "authors": "Tadeusz Kobus, Maciej Kokoci\\'nski, and Pawe{\\l} T. Wojciechowski", "title": "Creek: Low-latency, Mixed-Consistency Transactional Replication Scheme", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce Creek, a low-latency, eventually consistent\nreplication scheme that also enables execution of strongly consistent\noperations (akin to ACID transactions). Operations can have arbitrary complex\n(but deterministic) semantics. Similarly to state machine replication (SMR),\nCreek totally-orders all operations, but does so using two different broadcast\nmechanisms: a timestamp-based one and our novel conditional atomic broadcast\n(CAB). The former is used to establish a tentative order of all operations for\nspeculative execution, and it can tolerate network partitions. On the other\nhand, CAB is only used to ensure linearizable execution of the strongly\nconsistent operations, whenever distributed consensus can be solved. The\nexecution of strongly consistent operations also stabilizes the execution order\nof the causally related weakly consistent operations. Creek uses multiversion\nconcurrency control to efficiently handle operations' rollbacks and\nreexecutions resulting from the mismatch between the tentative and the final\nexecution orders. In the TPC-C benchmark, Creek offers up to 2.5 times lower\nlatency in returning client responses compared to the state-of-the-art\nspeculative SMR scheme, while maintaining high accuracy of the speculative\nexecution (92-100%).\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 13:05:47 GMT"}, {"version": "v2", "created": "Mon, 16 Dec 2019 16:24:19 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Kobus", "Tadeusz", ""], ["Kokoci\u0144ski", "Maciej", ""], ["Wojciechowski", "Pawe\u0142 T.", ""]]}, {"id": "1907.00879", "submitter": "\\'Italo Assis", "authors": "\\'Italo A. S. Assis, Ant\\^onio D. S. Oliveira, Tiago Barros, Idalmis\n  M. Sardina, Calebe P. Bianchini, Samuel Xavier-de-Souza", "title": "Distributed-Memory Load Balancing with Cyclic Token-based Work-Stealing\n  Applied to Reverse Time Migration", "comments": null, "journal-ref": null, "doi": "10.1109/ACCESS.2019.2939100", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reverse time migration (RTM) is a prominent technique in seismic imaging. Its\nresulting subsurface images are used in the industry to investigate with higher\nconfidence the existence and the conditions of oil and gas reservoirs. Because\nof its high computational cost, RTM must make use of parallel computers.\nBalancing the workload distribution of an RTM is a growing challenge in\ndistributed computing systems. The competition for shared resources and the\ndifferently-sized tasks of the RTM are some of the possible sources of load\nimbalance. Although many load balancing techniques exist, scaling up for large\nproblems and large systems remains a challenge because synchronization overhead\nalso scales. This paper proposes a cyclic token-based work-stealing (CTWS)\nalgorithm for distributed memory systems applied to RTM. The novel cyclic token\napproach reduces the number of failed steals, avoids communication overhead,\nand simplifies the victim selection and the termination strategy. The proposed\nmethod is implemented as a C library using the one-sided communication feature\nof the message passing interface (MPI) standard. Results obtained by applying\nthe proposed technique to balance the workload of a 3D RTM system present a\nfactor of 14.1% speedup and reductions of the load imbalance of 78.4% when\ncompared to the conventional static distribution.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 15:49:38 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 14:59:06 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Assis", "\u00cdtalo A. S.", ""], ["Oliveira", "Ant\u00f4nio D. S.", ""], ["Barros", "Tiago", ""], ["Sardina", "Idalmis M.", ""], ["Bianchini", "Calebe P.", ""], ["Xavier-de-Souza", "Samuel", ""]]}, {"id": "1907.01019", "submitter": "Saurabh Jha", "authors": "Valerio Formicola and Saurabh Jha and Daniel Chen and Fei Deng and\n  Amanda Bonnie and Mike Mason and Jim Brandt and Ann Gentile and Larry Kaplan\n  and Jason Repik and Jeremy Enos and Mike Showerman and Annette Greiner and\n  Zbigniew Kalbarczyk and Ravishankar K. Iyer and Bill Krammer", "title": "Understanding Fault Scenarios and Impacts through Fault Injection\n  Experiments in Cielo", "comments": "Presented at Cray User Group 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a set of fault injection experiments performed on the ACES\n(LANL/SNL) Cray XE supercomputer Cielo. We use this experimental campaign to\nimprove the understanding of failure causes and propagation that we observed in\nthe field failure data analysis of NCSA's Blue Waters. We use the data\ncollected from the logs and from network performance counter data 1) to\ncharacterize the fault-error-failure sequence and recovery mechanisms in the\nGemini network and in the Cray compute nodes, 2) to understand the impact of\nfailures on the system and the user applications at different scale, and 3) to\nidentify and recreate fault scenarios that induce unrecoverable failures, in\norder to create new tests for system and application design. The faults were\ninjected through special input commands to bring down network links,\ndirectional connections, nodes, and blades. We present extensions that will be\nneeded to apply our methodologies of injection and analysis to the Cray XC\n(Aries) systems.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 19:07:35 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Formicola", "Valerio", ""], ["Jha", "Saurabh", ""], ["Chen", "Daniel", ""], ["Deng", "Fei", ""], ["Bonnie", "Amanda", ""], ["Mason", "Mike", ""], ["Brandt", "Jim", ""], ["Gentile", "Ann", ""], ["Kaplan", "Larry", ""], ["Repik", "Jason", ""], ["Enos", "Jeremy", ""], ["Showerman", "Mike", ""], ["Greiner", "Annette", ""], ["Kalbarczyk", "Zbigniew", ""], ["Iyer", "Ravishankar K.", ""], ["Krammer", "Bill", ""]]}, {"id": "1907.01046", "submitter": "Wilhelm Hasselbring", "authors": "S\\\"oren Henning and Wilhelm Hasselbring and Armin M\\\"obius", "title": "A Scalable Architecture for Power Consumption Monitoring in Industrial\n  Production Environments", "comments": "10 pages", "journal-ref": "2019 IEEE International Conference on Fog Computing (ICFC)", "doi": "10.1109/ICFC.2019.00024", "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detailed knowledge about the electrical power consumption in industrial\nproduction environments is a prerequisite to reduce and optimize their power\nconsumption. Today's industrial production sites are equipped with a variety of\nsensors that, inter alia, monitor electrical power consumption in detail.\nHowever, these environments often lack an automated data collation and\nanalysis.\n  We present a system architecture that integrates different sensors and\nanalyzes and visualizes the power consumption of devices, machines, and\nproduction plants. It is designed with a focus on scalability to support\nproduction environments of various sizes and to handle varying loads. We argue\nthat a scalable architecture in this context must meet requirements for fault\ntolerance, extensibility, real-time data processing, and resource efficiency.\nAs a solution, we propose a microservice-based architecture augmented by big\ndata and stream processing techniques. Applying the fog computing paradigm,\nparts of it are deployed in an elastic, central cloud while other parts run\ndirectly, decentralized in the production environment.\n  A prototype implementation of this architecture presents solutions how\ndifferent kinds of sensors can be integrated and their measurements can be\ncontinuously aggregated. In order to make analyzed data comprehensible, it\nfeatures a single-page web application that provides different forms of data\nvisualization. We deploy this pilot implementation in the data center of a\nmedium-sized enterprise, where we successfully monitor the power consumption of\n16~servers. Furthermore, we show the scalability of our architecture with\n20,000~simulated sensors.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 20:06:50 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Henning", "S\u00f6ren", ""], ["Hasselbring", "Wilhelm", ""], ["M\u00f6bius", "Armin", ""]]}, {"id": "1907.01063", "submitter": "Erik \\v{S}trumbelj", "authors": "Rok \\v{C}e\\v{s}novar, Steve Bronder, Davor Sluga, Jure Dem\\v{s}ar,\n  Tadej Ciglari\\v{c}, Sean Talts, Erik \\v{S}trumbelj", "title": "GPU-based Parallel Computation Support for Stan", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper details an extensible OpenCL framework that allows Stan to utilize\nheterogeneous compute devices. It includes GPU-optimized routines for the\nCholesky decomposition, its derivative, other matrix algebra primitives and\nsome commonly used likelihoods, with more additions planned for the near\nfuture. Stan users can now benefit from large speedups offered by GPUs with\nlittle effort and without changes to their existing Stan code. We demonstrate\nthe practical utility of our work with two examples - logistic regression and\nGaussian Process regression.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 20:36:16 GMT"}, {"version": "v2", "created": "Sat, 16 May 2020 23:23:03 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["\u010ce\u0161novar", "Rok", ""], ["Bronder", "Steve", ""], ["Sluga", "Davor", ""], ["Dem\u0161ar", "Jure", ""], ["Ciglari\u010d", "Tadej", ""], ["Talts", "Sean", ""], ["\u0160trumbelj", "Erik", ""]]}, {"id": "1907.01132", "submitter": "Moming Duan", "authors": "Moming Duan, Duo Liu, Xianzhang Chen, Yujuan Tan, Jinting Ren, Lei\n  Qiao, Liang Liang", "title": "Astraea: Self-balancing Federated Learning for Improving Classification\n  Accuracy of Mobile Deep Learning Applications", "comments": "Published as a conference paper at IEEE 37th International Conference\n  on Computer Design (ICCD) 2019", "journal-ref": null, "doi": "10.1109/ICCD46524.2019.00038", "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) is a distributed deep learning method which enables\nmultiple participants, such as mobile phones and IoT devices, to contribute a\nneural network model while their private training data remains in local\ndevices. This distributed approach is promising in the edge computing system\nwhere have a large corpus of decentralized data and require high privacy.\nHowever, unlike the common training dataset, the data distribution of the edge\ncomputing system is imbalanced which will introduce biases in the model\ntraining and cause a decrease in accuracy of federated learning applications.\nIn this paper, we demonstrate that the imbalanced distributed training data\nwill cause accuracy degradation in FL. To counter this problem, we build a\nself-balancing federated learning framework call Astraea, which alleviates the\nimbalances by 1) Global data distribution based data augmentation, and 2)\nMediator based multi-client rescheduling. The proposed framework relieves\nglobal imbalance by runtime data augmentation, and for averaging the local\nimbalance, it creates the mediator to reschedule the training of clients based\non Kullback-Leibler divergence (KLD) of their data distribution. Compared with\nFedAvg, the state-of-the-art FL algorithm, Astraea shows +5.59% and +5.89%\nimprovement of top-1 accuracy on the imbalanced EMNIST and imbalanced CINIC-10\ndatasets, respectively. Meanwhile, the communication traffic of Astraea can be\n82% lower than that of FedAvg.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 02:44:36 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 07:01:56 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Duan", "Moming", ""], ["Liu", "Duo", ""], ["Chen", "Xianzhang", ""], ["Tan", "Yujuan", ""], ["Ren", "Jinting", ""], ["Qiao", "Lei", ""], ["Liang", "Liang", ""]]}, {"id": "1907.01198", "submitter": "Qinmeng Zou", "authors": "Qinmeng Zou and Guillaume Gbikpi-Benissan and Frederic Magoules", "title": "Asynchronous Parareal Algorithm Applied to European Option Pricing", "comments": null, "journal-ref": "16th International Symposium on Distributed Computing and\n  Applications for Business Engineering and Science (DCABES), 2017, IEEE", "doi": "10.1109/dcabes.2017.15", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asynchronous iterations arise naturally in parallel computing if one wants to\nsolve large problems with a minimization of the idle times. This paper presents\nan original model of asynchronous iterations for a time-domain decomposition\nmethod, namely the parareal method. The asynchronous parareal algorithm is here\napplied to European option pricing, and numerical experiments performed on a\nparallel supercomputer, illustrate the performance and efficiency of this new\nmethod.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 07:01:19 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Zou", "Qinmeng", ""], ["Gbikpi-Benissan", "Guillaume", ""], ["Magoules", "Frederic", ""]]}, {"id": "1907.01199", "submitter": "Qinmeng Zou", "authors": "Qinmeng Zou and Guillaume Gbikpi-Benissan and Frederic Magoules", "title": "Asynchronous Communications Library for the Parallel-in-Time Solution of\n  Black-Scholes Equation", "comments": null, "journal-ref": "16th International Symposium on Distributed Computing and\n  Applications for Business Engineering and Science (DCABES), 2017, IEEE", "doi": "10.1109/dcabes.2017.17", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of asynchronous iterative scheme gives high efficiency to\nnumerical computations. However, it is generally difficult to handle the\nproblems of resource management and convergence detection. This paper uses\nJACK2, an asynchronous communication kernel library for iterative algorithms,\nto implement both classical and asynchronous parareal algorithms, especially\nthe latter. We illustrate the measures whereby one can tackle the problems\nabove elegantly for the time-dependent case. Finally, experiments are presented\nto prove the availability and efficiency of such application.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 07:02:26 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Zou", "Qinmeng", ""], ["Gbikpi-Benissan", "Guillaume", ""], ["Magoules", "Frederic", ""]]}, {"id": "1907.01201", "submitter": "Qinmeng Zou", "authors": "Qinmeng Zou and Frederic Magoules", "title": "Convergence Detection of Asynchronous Iterations based on Modified\n  Recursive Doubling", "comments": null, "journal-ref": "17th International Symposium on Distributed Computing and\n  Applications for Business Engineering and Science (DCABES), 2018, IEEE", "doi": "10.1109/dcabes.2018.00081", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the distributed convergence detection problem in\nasynchronous iterations. A modified recursive doubling algorithm is\ninvestigated in order to adapt to the non-power-of-two case. Some convergence\ndetection algorithms are illustrated based on the reduction operation. Finally,\na concluding discussion about the implementation and the applicability is\npresented.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 07:04:31 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Zou", "Qinmeng", ""], ["Magoules", "Frederic", ""]]}, {"id": "1907.01293", "submitter": "Dirk Trossen", "authors": "Dirk Trossen, Sebastian Robitzsch, Scott Hergenhan, Janne Riihijarvi,\n  Martin Reed, Mays Al-Naday", "title": "Service-based Routing at the Edge", "comments": "under submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future scenarios, such as AR/VR, pose challenging latency and bandwidth\nrequirements in 5G. This need is complemented by the adoption of cloud\nprinciples for providing services, particularly for virtualizing service\ncomponents with which virtualized instances can appear rapidly at different\nexecution points in the network. While providing service endpoints close to the\nend user appears straightforward, this early service break-out is currently\nlimited to routing requests to Point-of-Presence (POP) nodes provided by a few\nglobal CDN players deep in the customer network. In this paper, we propose\ninstead to turn the edge of the Internet into a rich service-based routing\ninfrastructure with services being provided through edge compute nodes, without\nneeding indirect routing. Our approach interprets every IP-based service as a\nnamed service over a (L2 or similar) transport network, requiring no per-flow\nstate in the network, while natively supporting both unicast and multicast\ndelivery. The solution allows route adjustments in time scales of few tens of\nmilliseconds, enabling rapid failure recovery, extremely responsive load\nbalancing, efficient mobility support, and more. We implemented our solution on\nstandard SDN-based infrastructure and in mobile terminals in a\nbackwards-compatible manner, enabling a performance evaluation that shows\nsignificant improvements in network utilization as well as flow setup times.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 10:47:03 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Trossen", "Dirk", ""], ["Robitzsch", "Sebastian", ""], ["Hergenhan", "Scott", ""], ["Riihijarvi", "Janne", ""], ["Reed", "Martin", ""], ["Al-Naday", "Mays", ""]]}, {"id": "1907.01484", "submitter": "Kshiteej Mahajan", "authors": "Kshiteej Mahajan, Arjun Balasubramanian, Arjun Singhvi, Shivaram\n  Venkataraman, Aditya Akella, Amar Phanishayee, Shuchi Chawla", "title": "Themis: Fair and Efficient GPU Cluster Scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern distributed machine learning (ML) training workloads benefit\nsignificantly from leveraging GPUs. However, significant contention ensues when\nmultiple such workloads are run atop a shared cluster of GPUs. A key question\nis how to fairly apportion GPUs across workloads. We find that established\ncluster scheduling disciplines are a poor fit because of ML workloads' unique\nattributes: ML jobs have long-running tasks that need to be gang-scheduled, and\ntheir performance is sensitive to tasks' relative placement.\n  We propose Themis, a new scheduling framework for ML training workloads. It's\nGPU allocation policy enforces that ML workloads complete in a finish-time fair\nmanner, a new notion we introduce. To capture placement sensitivity and ensure\nefficiency, Themis uses a two-level scheduling architecture where ML workloads\nbid on available resources that are offered in an auction run by a central\narbiter. Our auction design allocates GPUs to winning bids by trading off\nefficiency for fairness in the short term but ensuring finish-time fairness in\nthe long term. Our evaluation on a production trace shows that Themis can\nimprove fairness by more than 2.25X and is ~5% to 250% more cluster efficient\nin comparison to state-of-the-art schedulers.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 16:45:22 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 16:15:01 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Mahajan", "Kshiteej", ""], ["Balasubramanian", "Arjun", ""], ["Singhvi", "Arjun", ""], ["Venkataraman", "Shivaram", ""], ["Akella", "Aditya", ""], ["Phanishayee", "Amar", ""], ["Chawla", "Shuchi", ""]]}, {"id": "1907.01578", "submitter": "Eberle A. Rambo", "authors": "Eberle A. Rambo, Bryan Donyanavard, Minjun Seo, Florian Maurer, Thawra\n  Kadeed, Caio B. de Melo, Biswadip Maity, Anmol Surhonne, Andreas Herkersdorf,\n  Fadi Kurdahi, Nikil Dutt, and Rolf Ernst", "title": "The Information Processing Factory: Organization, Terminology, and\n  Definitions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Information Processing Factory (IPF) project has recently introduced the\nabstraction of complex architectures as self-aware information processing\nfactories. These factories consist of a set of highly configurable resources,\ne.g., processing elements and interconnects, whose use is monitored, planned,\nand configured during runtime. Managing a factory involves multiple facets,\nsuch as efficiency, availability, reliability, integrity, and timing. IPF\nconquers the complexity of managing facets in digital systems by hierarchically\ndecomposing the challenges and addressing them with different co-existing\nentities in the factory. This paper introduces the organization, terminology,\nand definitions of IPF.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 18:25:02 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Rambo", "Eberle A.", ""], ["Donyanavard", "Bryan", ""], ["Seo", "Minjun", ""], ["Maurer", "Florian", ""], ["Kadeed", "Thawra", ""], ["de Melo", "Caio B.", ""], ["Maity", "Biswadip", ""], ["Surhonne", "Anmol", ""], ["Herkersdorf", "Andreas", ""], ["Kurdahi", "Fadi", ""], ["Dutt", "Nikil", ""], ["Ernst", "Rolf", ""]]}, {"id": "1907.01624", "submitter": "Fabian Frei", "authors": "Fabian Frei and Koichi Wada", "title": "Efficient Circuit Simulation in MapReduce", "comments": "This is the full version of the preliminary paper with the same title\n  presented at the 30th International Symposium on Algorithms and Computation\n  (ISAAC 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The MapReduce framework has firmly established itself as one of the most\nwidely used parallel computing platforms for processing big data on tera- and\npeta-byte scale. Approaching it from a theoretical standpoint has proved to be\nnotoriously difficult, however. In continuation of Goodrich et al.'s early\nefforts, explicitly espousing the goal of putting the MapReduce framework on\nfooting equal to that of long-established models such as the PRAM, we\ninvestigate the obvious complexity question of how the computational power of\nMapReduce algorithms compares to that of combinational Boolean circuits\ncommonly used for parallel computations. Relying on the standard MapReduce\nmodel introduced by Karloff et al. a decade ago, we develop an intricate\nsimulation technique to show that any problem in NC (i.e., a problem solved by\na logspace-uniform family of Boolean circuits of polynomial size and a depth\npolylogarithmic in the input size) can be solved by a MapReduce computation in\nO(T(n)/ log n) rounds, where n is the input size and T(n) is the depth of the\nwitnessing circuit family. Thus, we are able to closely relate the standard,\nuniform NC hierarchy modeling parallel computations to the deterministic\nMapReduce hierarchy DMRC by proving that NC^(i+1) is contained in DMRC^i for\nall natural i, including 0. Besides the theoretical significance, this result\nthat has important applied aspects as well. In particular, we show for all\nproblems in NC^1---many practically relevant ones such as integer\nmultiplication and division, the parity function, and recognizing balanced\nstrings of parentheses being among these---how to solve them in a constant\nnumber of deterministic MapReduce rounds.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 20:32:26 GMT"}, {"version": "v2", "created": "Wed, 25 Dec 2019 16:12:20 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Frei", "Fabian", ""], ["Wada", "Koichi", ""]]}, {"id": "1907.01796", "submitter": "Mark Burgess", "authors": "Mark Burgess and Ewout Prangsma", "title": "Koalja: from Data Plumbing to Smart Workspaces in the Extended Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MA cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Koalja describes a generalized data wiring or `pipeline' platform, built on\ntop of Kubernetes, for plugin user code. Koalja makes the Kubernetes underlay\ntransparent to users (for a `serverless' experience), and offers a\nbreadboarding experience for development of data sharing circuitry, to\ncommoditize its gradual promotion to a production system, with a minimum of\ninfrastructure knowledge. Enterprise grade metadata are captured as data\npayloads flow through the circuitry, allowing full tracing of provenance and\nforensic reconstruction of transactional processes, down to the versions of\nsoftware that led to each outcome. Koalja attends to optimizations for avoiding\nunwanted processing and transportation of data, that are rapidly becoming\nsustainability imperatives. Thus one can minimize energy expenditure and waste,\nand design with scaling in mind, especially with regard to edge computing, to\naccommodate an Internet of Things, Network Function Virtualization, and more.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 08:52:42 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Burgess", "Mark", ""], ["Prangsma", "Ewout", ""]]}, {"id": "1907.01891", "submitter": "M\\'onica Chillar\\'on", "authors": "M\\'onica Chillar\\'on, Gregorio Quintana-Ort\\'i, Vicente Vidal, and\n  Gumersindo Verd\\'u", "title": "Computed tomography medical image reconstruction on affordable equipment\n  by using out-of-core techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As Computed Tomography (CT) scans are an essential medical test, many\ntechniques have been proposed to reconstruct high-quality images using a\nsmaller amount of radiation. One approach is to employ algebraic factorization\nmethods to reconstruct the images, using fewer views than the traditional\nanalytical methods. However, their main drawback is the high computational cost\nand hence the time needed to obtain the images, which is critical in the daily\nclinical practice. For this reason, faster methods for solving this problem are\nrequired. In this paper, we propose a new reconstruction method based on the QR\nfactorization that is very efficient on affordable equipment (standard\nmulticore processors and standard Solid-State Drives) by using out-of-core\ntechniques. Combining both affordable hardware and the new software, we can\nboost the performance of the reconstructions and implement a reliable and\ncompetitive method that reconstructs high-quality CT images quickly.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 10:37:37 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Chillar\u00f3n", "M\u00f3nica", ""], ["Quintana-Ort\u00ed", "Gregorio", ""], ["Vidal", "Vicente", ""], ["Verd\u00fa", "Gumersindo", ""]]}, {"id": "1907.01989", "submitter": "Juhyun Lee", "authors": "Juhyun Lee, Nikolay Chirkov, Ekaterina Ignasheva, Yury Pisarchyk,\n  Mogan Shieh, Fabio Riccardi, Raman Sarokin, Andrei Kulik, and Matthias\n  Grundmann", "title": "On-Device Neural Net Inference with Mobile GPUs", "comments": "Computer Vision and Pattern Recognition Workshop: Efficient Deep\n  Learning for Computer Vision 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On-device inference of machine learning models for mobile phones is desirable\ndue to its lower latency and increased privacy. Running such a\ncompute-intensive task solely on the mobile CPU, however, can be difficult due\nto limited computing power, thermal constraints, and energy consumption. App\ndevelopers and researchers have begun exploiting hardware accelerators to\novercome these challenges. Recently, device manufacturers are adding neural\nprocessing units into high-end phones for on-device inference, but these\naccount for only a small fraction of hand-held devices. In this paper, we\npresent how we leverage the mobile GPU, a ubiquitous hardware accelerator on\nvirtually every phone, to run inference of deep neural networks in real-time\nfor both Android and iOS devices. By describing our architecture, we also\ndiscuss how to design networks that are mobile GPU-friendly. Our\nstate-of-the-art mobile GPU inference engine is integrated into the open-source\nproject TensorFlow Lite and publicly available at https://tensorflow.org/lite.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 15:23:20 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Lee", "Juhyun", ""], ["Chirkov", "Nikolay", ""], ["Ignasheva", "Ekaterina", ""], ["Pisarchyk", "Yury", ""], ["Shieh", "Mogan", ""], ["Riccardi", "Fabio", ""], ["Sarokin", "Raman", ""], ["Kulik", "Andrei", ""], ["Grundmann", "Matthias", ""]]}, {"id": "1907.02064", "submitter": "Mark Hill", "authors": "Mark D. Hill and Vijay Janapa Reddi", "title": "Accelerator-level Parallelism", "comments": "6 pages, 3 figures, & 7 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.OS cs.PF cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future applications demand more performance, but technology advances have\nbeen faltering. A promising approach to further improve computer system\nperformance under energy constraints is to employ hardware accelerators.\nAlready today, mobile systems concurrently employ multiple accelerators in what\nwe call accelerator-level parallelism (ALP). To spread the benefits of ALP more\nbroadly, we charge computer scientists to develop the science needed to best\nachieve the performance and cost goals of ALP hardware and software.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 21:04:47 GMT"}, {"version": "v2", "created": "Mon, 8 Jul 2019 14:53:53 GMT"}, {"version": "v3", "created": "Sat, 24 Aug 2019 20:11:05 GMT"}, {"version": "v4", "created": "Wed, 19 Aug 2020 14:58:25 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Hill", "Mark D.", ""], ["Reddi", "Vijay Janapa", ""]]}, {"id": "1907.02154", "submitter": "Yida Wang", "authors": "Leyuan Wang, Zhi Chen, Yizhi Liu, Yao Wang, Lianmin Zheng, Mu Li, Yida\n  Wang", "title": "A Unified Optimization Approach for CNN Model Inference on Integrated\n  GPUs", "comments": "10 pages, 3 figures, 48th International Conference on Parallel\n  Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern deep learning applications urge to push the model inference taking\nplace at the edge devices for multiple reasons such as achieving shorter\nlatency, relieving the burden of the network connecting to the cloud, and\nprotecting user privacy. The Convolutional Neural Network (\\emph{CNN}) is one\nof the most widely used model family in the applications. Given the high\ncomputational complexity of the CNN models, it is favorable to execute them on\nthe integrated GPUs at the edge devices, which are ubiquitous and have more\npower and better energy efficiency than the accompanying CPUs. However,\nprogramming on integrated GPUs efficiently is challenging due to the variety of\ntheir architectures and programming interfaces. This paper proposes an\nend-to-end solution to execute CNN model inference on the integrated GPUs at\nthe edge, which uses a unified IR to represent and optimize vision-specific\noperators on integrated GPUs from multiple vendors, as well as leverages\nmachine learning-based scheduling search schemes to optimize\ncomputationally-intensive operators like convolution. Our solution even\nprovides a fallback mechanism for operators not suitable or convenient to run\non GPUs. The evaluation results suggest that compared to state-of-the-art\nsolutions backed up by the vendor-provided high-performance libraries on Intel\nGraphics, ARM Mali GPU, and Nvidia integrated Maxwell GPU, our solution\nachieves similar, or even better (up to 1.62$\\times$), performance on a number\nof popular image classification and object detection models. In addition, our\nsolution has a wider model coverage and is more flexible to embrace new models.\nOur solution has been adopted in production services in AWS and is\nopen-sourced.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 22:49:11 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Wang", "Leyuan", ""], ["Chen", "Zhi", ""], ["Liu", "Yizhi", ""], ["Wang", "Yao", ""], ["Zheng", "Lianmin", ""], ["Li", "Mu", ""], ["Wang", "Yida", ""]]}, {"id": "1907.02162", "submitter": "Tian Guo", "authors": "Samuel S. Ogden and Tian Guo", "title": "CloudCoaster: Transient-aware Bursty Datacenter Workload Scheduling", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's clusters often have to divide resources among a diverse set of jobs.\nThese jobs are heterogeneous both in execution time and in their rate of\narrival. Execution time heterogeneity has lead to the development of hybrid\nschedulers that can schedule both short and long jobs to ensure good task\nplacement. However, arrival rate heterogeneity, or burstiness, remains a\nproblem in existing schedulers. These hybrid schedulers manage resources on\nstatically provisioned cluster, which can quickly be overwhelmed by bursts in\nthe number of arriving jobs.\n  In this paper we propose CloudCoaster, a hybrid scheduler that dynamically\nresizes the cluster by leveraging cheap transient servers. CloudCoaster\nschedules jobs in an intelligent way that increases job performance while\nreducing overall resource cost. We evaluate the effectiveness of CloudCoaster\nthrough simulations on real-world traces and compare it against a state-of-art\nhybrid scheduler. CloudCoaster improves the average queueing delay time of\nshort jobs by 4.8X while maintaining long job performance. In addition,\nCloudCoaster reduces the short partition budget by over 29.5%.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 23:50:26 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Ogden", "Samuel S.", ""], ["Guo", "Tian", ""]]}, {"id": "1907.02335", "submitter": "Victoria Tokareva A.", "authors": "V. Tokareva, A. Haungs, D. Kang, D. Kostunin, F. Polgart, D. Wochele,\n  J. Wochele", "title": "Development of a data infrastructure for a global data and analysis\n  center in astroparticle physics", "comments": "8 pages, 2 figures, The III International Workshop \"Data life cycle\n  in physics\" (DLC-2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays astroparticle physics faces a rapid data volume increase. Meanwhile,\nthere are still challenges of testing the theoretical models for clarifying the\norigin of cosmic rays by applying a multi-messenger approach, machine learning\nand investigation of the phenomena related to the rare statistics in detecting\nincoming particles. The problems are related to the accurate data mapping and\ndata management as well as to the distributed storage and high-performance data\nprocessing. In particular, one could be interested in employing such solutions\nin study of air-showers induced by ultra-high energy cosmic and gamma rays,\ntesting new hypotheses of hadronic interaction or cross-calibration of\ndifferent experiments. KASCADE (Karlsruhe, Germany) and TAIGA (Tunka valley,\nRussia) are experiments in the field of astroparticle physics, aiming at the\ndetection of cosmic-ray air-showers, induced by the primaries in the energy\nrange of about hundreds TeVs to hundreds PeVs. They are located at the same\nlatitude and have an overlap in operation runs. These factors determine the\ninterest in performing a joint analysis of these data. In the German-Russian\nAstroparticle Data Life Cycle Initiative (GRADLCI), modern technologies of the\ndistributed data management are being employed for establishing a reliable open\naccess to the experimental cosmic-ray physics data collected by KASCADE and the\nTunka-133 setup of TAIGA.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 11:42:10 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Tokareva", "V.", ""], ["Haungs", "A.", ""], ["Kang", "D.", ""], ["Kostunin", "D.", ""], ["Polgart", "F.", ""], ["Wochele", "D.", ""], ["Wochele", "J.", ""]]}, {"id": "1907.02394", "submitter": "Herodotos Herodotou", "authors": "Herodotos Herodotou and Elena Kakoulli", "title": "Automating Distributed Tiered Storage Management in Cluster Computing", "comments": "16 pages, 17 figures, 4 tables", "journal-ref": null, "doi": "10.14778/3357377.3357381", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-intensive platforms such as Hadoop and Spark are routinely used to\nprocess massive amounts of data residing on distributed file systems like HDFS.\nIncreasing memory sizes and new hardware technologies (e.g., NVRAM, SSDs) have\nrecently led to the introduction of storage tiering in such settings. However,\nusers are now burdened with the additional complexity of managing the multiple\nstorage tiers and the data residing on them while trying to optimize their\nworkloads. In this paper, we develop a general framework for automatically\nmoving data across the available storage tiers in distributed file systems.\nMoreover, we employ machine learning for tracking and predicting file access\npatterns, which we use to decide when and which data to move up or down the\nstorage tiers for increasing system performance. Our approach uses incremental\nlearning to dynamically refine the models with new file accesses, allowing them\nto naturally adjust and adapt to workload changes over time. Our extensive\nevaluation using realistic workloads derived from Facebook and CMU traces\ncompares our approach with several other policies and showcases significant\nbenefits in terms of both workload performance and cluster efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 13:30:33 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 13:03:09 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Herodotou", "Herodotos", ""], ["Kakoulli", "Elena", ""]]}, {"id": "1907.02506", "submitter": "Jun Zhao", "authors": "Binbin Huang, Zhongjin Lia, Peng Tang, Shangguang Wang, Jun Zhao,\n  Haiyang Hua, Wanqing Lia, Victor Chang", "title": "Security modeling and efficient computation offloading for service\n  workflow in mobile edge computing", "comments": "published in journal \"Future Generation Computer Systems\":\n  https://doi.org/10.1016/j.future.2019.03.011", "journal-ref": null, "doi": "10.1016/j.future.2019.03.011", "report-no": null, "categories": "cs.DC cs.CR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a big challenge for resource-limited mobile devices (MDs) to execute\nvarious complex and energy-consumed mobile applications. Fortunately, as a\nnovel computing paradigm, edge computing (MEC) can provide abundant computing\nresources to execute all or parts of the tasks of MDs and thereby can greatly\nreduce the energy of MD and improve the QoS of applications. However,\noffloading workflow tasks to the MEC servers are liable to external security\nthreats (e.g., snooping, alteration). In this paper, we propose a security and\nenergy efficient computation offloading (SEECO) strategy for service workflows\nin MEC environment, the goal of which is to optimize the energy consumption\nunder the risk probability and deadline constraints. First, we build a security\noverhead model to measure the execution time of security services. Then, we\nformulate the computation offloading problem by incorporating the security,\nenergy consumption and execution time of workflow application. Finally, based\non the genetic algorithm (GA), the corresponding coding strategies of SEECO are\ndevised by considering tasks execution order and location and security services\nselection. Extensive experiments with the variety of workflow parameters\ndemonstrate that SEECO strategy can achieve the security and energy efficiency\nfor the mobile applications.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 17:40:30 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Huang", "Binbin", ""], ["Lia", "Zhongjin", ""], ["Tang", "Peng", ""], ["Wang", "Shangguang", ""], ["Zhao", "Jun", ""], ["Hua", "Haiyang", ""], ["Lia", "Wanqing", ""], ["Chang", "Victor", ""]]}, {"id": "1907.02658", "submitter": "Kenneth Duru", "authors": "Kenneth Duru, Leonhard Rannabauer, Alice-Agnes Gabriel, On Ki Angel\n  Ling, Heiner Igel, Michael Bader", "title": "A stable discontinuous Galerkin method for linear elastodynamics in 3D\n  geometrically complex media using physics based numerical fluxes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC cs.NA physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High order accurate and explicit time-stable solvers are well suited for\nhyperbolic wave propagation problems. As a result of the complexities of real\ngeometries, internal interfaces and nonlinear boundary and interface\nconditions, discontinuities and sharp wave fronts may become fundamental\nfeatures of the solution. Thus, geometrically flexible and adaptive numerical\nalgorithms are critical for high fidelity and efficient simulations of wave\nphenomena in many applications. Adaptive curvilinear meshes hold promise to\nminimise the effort to represent complicated geometries or heterogeneous\nmaterial data avoiding the bottleneck of feature-preserving meshing. To enable\nthe design of stable DG methods on three space dimensional (3D) curvilinear\nelements we construct a structure preserving anti-symmetric coordinate\ntransformation motivated by the underlying physics. Using a physics-based\nnumerical penalty-flux, we develop a 3D provably energy-stable discontinuous\nGalerkin finite element approximation of the elastic wave equation in\ngeometrically complex and heterogenous media. By construction, our numerical\nflux is upwind and yields a discrete energy estimate analogous to the\ncontinuous energy estimate. The ability to treat conforming and non-conforming\ncurvilinear elements allows for flexible adaptive mesh refinement strategies.\nThe numerical scheme has been implemented in ExaHyPE, a simulation engine for\nparallel dynamically adaptive simulations of wave problems on adaptive\nCartesian meshes. We present 3D numerical experiments of wave propagation in\nheterogeneous isotropic and anisotropic elastic solids demonstrating stability\nand high order accuracy. We demonstrate the potential of our approach for\ncomputational seismology in a regional wave propagation scenario in a\ngeologically constrained 3D model including the geometrically complex\nfree-surface topography of Mount Zugspitze, Germany.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 02:41:46 GMT"}, {"version": "v2", "created": "Sat, 10 Apr 2021 08:42:36 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Duru", "Kenneth", ""], ["Rannabauer", "Leonhard", ""], ["Gabriel", "Alice-Agnes", ""], ["Ling", "On Ki Angel", ""], ["Igel", "Heiner", ""], ["Bader", "Michael", ""]]}, {"id": "1907.02664", "submitter": "Deepesh Data", "authors": "Deepesh Data, Linqi Song, Suhas Diggavi", "title": "Data Encoding for Byzantine-Resilient Distributed Optimization", "comments": "38 pages, Accepted for publication in the IEEE Transactions on\n  Information Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study distributed optimization in the presence of Byzantine adversaries,\nwhere both data and computation are distributed among $m$ worker machines, $t$\nof which may be corrupt. The compromised nodes may collaboratively and\narbitrarily deviate from their pre-specified programs, and a designated\n(master) node iteratively computes the model/parameter vector for generalized\nlinear models. In this work, we primarily focus on two iterative algorithms:\nProximal Gradient Descent (PGD) and Coordinate Descent (CD). Gradient descent\n(GD) is a special case of these algorithms. PGD is typically used in the\ndata-parallel setting, where data is partitioned across different samples,\nwhereas, CD is used in the model-parallelism setting, where data is partitioned\nacross the parameter space.\n  In this paper, we propose a method based on data encoding and error\ncorrection over real numbers to combat adversarial attacks. We can tolerate up\nto $t\\leq \\lfloor\\frac{m-1}{2}\\rfloor$ corrupt worker nodes, which is\ninformation-theoretically optimal. We give deterministic guarantees, and our\nmethod does not assume any probability distribution on the data. We develop a\n{\\em sparse} encoding scheme which enables computationally efficient data\nencoding and decoding. We demonstrate a trade-off between the corruption\nthreshold and the resource requirements (storage, computational, and\ncommunication complexity). As an example, for $t\\leq\\frac{m}{3}$, our scheme\nincurs only a {\\em constant} overhead on these resources, over that required by\nthe plain distributed PGD/CD algorithms which provide no adversarial\nprotection. To the best of our knowledge, ours is the first paper that makes CD\nsecure against adversarial attacks.\n  Our encoding scheme extends efficiently to the data streaming model and for\nstochastic gradient descent (SGD). We also give experimental results to show\nthe efficacy of our proposed schemes.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 03:31:43 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 05:57:11 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Data", "Deepesh", ""], ["Song", "Linqi", ""], ["Diggavi", "Suhas", ""]]}, {"id": "1907.02669", "submitter": "Srivatsan Ravi Mr", "authors": "Trevor Brown and Srivatsan Ravi", "title": "On the Cost of Concurrency in Hybrid Transactional Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art \\emph{software transactional memory (STM)} implementations\nachieve good performance by carefully avoiding the overhead of\n\\emph{incremental validation} (i.e., re-reading previously read data items to\navoid inconsistency) while still providing \\emph{progressiveness} (allowing\ntransactional aborts only due to \\emph{data conflicts}). Hardware transactional\nmemory (HTM) implementations promise even better performance, but offer no\nprogress guarantees. Thus, they must be combined with STMs, leading to\n\\emph{hybrid} TMs (HyTMs) in which hardware transactions must be\n\\emph{instrumented} (i.e., access metadata) to detect contention with software\ntransactions.\n  We show that, unlike in progressive STMs, software transactions in\nprogressive HyTMs cannot avoid incremental validation. In fact, this result\nholds even if hardware transactions can \\emph{read} metadata\n\\emph{non-speculatively}. We then present \\emph{opaque} HyTM algorithms\nproviding \\emph{progressiveness for a subset of transactions} that are optimal\nin terms of hardware instrumentation. We explore the concurrency vs. hardware\ninstrumentation vs. software validation trade-offs for these algorithms. Our\nexperiments with Intel and IBM POWER8 HTMs seem to suggest that (i) the\n\\emph{cost of concurrency} also exists in practice, (ii) it is important to\nimplement HyTMs that provide progressiveness for a maximal set of transactions\nwithout incurring high hardware instrumentation overhead or using global\ncontending bottlenecks and (iii) there is no easy way to derive more efficient\nHyTMs by taking advantage of non-speculative accesses within hardware.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 04:01:30 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Brown", "Trevor", ""], ["Ravi", "Srivatsan", ""]]}, {"id": "1907.02761", "submitter": "Abdulkadir Celik", "authors": "Abdulkadir Celik, Ming-Cheng Tsai, Redha M. Radaydeh, Fawaz S.\n  Al-Qahtani, Mohamed-Slim Alouini", "title": "Distributed User Clustering and Resource Allocation for Imperfect NOMA\n  in Heterogeneous Networks", "comments": null, "journal-ref": "IEEE Transactions on Communications, 2019", "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a distributed cluster formation (CF) and resource\nallocation (RA) framework for non-ideal non-orthogonal multiple access (NOMA)\nschemes in heterogeneous networks. The imperfection of the underlying NOMA\nscheme is due to the receiver sensitivity and interference residue from\nnon-ideal successive interference cancellation (SIC), which is generally\ncharacterized by a fractional error factor (FEF). Our analytical findings first\nshow that several factors have a significant impact on the achievable NOMA\ngain. Then, we investigate fundamental limits on NOMA cluster size as a\nfunction of FEF levels, cluster bandwidth, and quality of service (QoS) demands\nof user equipments (UEs). Thereafter, a clustering algorithm is developed by\ntaking feasible cluster size and channel gain disparity of UEs into account.\nFinally, we develop a distributed alpha-fair RA framework where alpha governs\nthe trade-off between maximum throughput and proportional fairness objectives.\nBased on the derived closed-form optimal power levels, the proposed distributed\nsolution iteratively updates bandwidths, clusters, and UEs' transmission\npowers. Numerical results demonstrate that proposed solutions deliver a higher\nspectral and energy efficiency than traditionally adopted basic NOMA cluster\nsize of two. We also show that an imperfect NOMA cannot always provide better\nperformance than orthogonal multiple access under certain conditions. Finally,\nour numerical investigations reveal that NOMA gain is maximized under\ndownlink/uplink decoupled (DUDe) UE association.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 10:34:49 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Celik", "Abdulkadir", ""], ["Tsai", "Ming-Cheng", ""], ["Radaydeh", "Redha M.", ""], ["Al-Qahtani", "Fawaz S.", ""], ["Alouini", "Mohamed-Slim", ""]]}, {"id": "1907.02805", "submitter": "Alexey Lastovetsky", "authors": "Arsalan Shahid, Muhammad Fahad, Ravi Reddy Manumachu, Alexey\n  Lastovetsky", "title": "Energy of Computing on Multicore CPUs: Predictive Models and Energy\n  Conservation Law", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy is now a first-class design constraint along with performance in all\ncomputing settings. Energy predictive modelling based on performance monitoring\ncounts (PMCs) is the leading method used for prediction of energy consumption\nduring an application execution. We use a model-theoretic approach to formulate\nthe assumed properties of existing models in a mathematical form. We extend the\nformalism by adding properties, heretofore unconsidered, that account for a\nlimited form of energy conservation law. The extended formalism defines our\ntheory of energy of computing. By applying the basic practical implications of\nthe theory, we improve the prediction accuracy of state-of-the-art energy\nmodels from 31% to 18%. We also demonstrate that use of state-of-the-art\nmeasurement tools for energy optimisation may lead to significant losses of\nenergy (ranging from 56% to 65% for applications used in experiments) since\nthey do not take into account the energy conservation properties.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 12:56:57 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Shahid", "Arsalan", ""], ["Fahad", "Muhammad", ""], ["Manumachu", "Ravi Reddy", ""], ["Lastovetsky", "Alexey", ""]]}, {"id": "1907.02818", "submitter": "Jan H\\\"uckelheim", "authors": "Jan H\\\"uckelheim, Navjot Kukreja, Sri Hari Krishna Narayanan, Fabio\n  Luporini, Gerard Gorman, Paul Hovland", "title": "Automatic Differentiation for Adjoint Stencil Loops", "comments": "ICPP 2019", "journal-ref": null, "doi": "10.1145/3337821.3337906", "report-no": null, "categories": "cs.DC cs.LG cs.PF cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stencil loops are a common motif in computations including convolutional\nneural networks, structured-mesh solvers for partial differential equations,\nand image processing. Stencil loops are easy to parallelise, and their fast\nexecution is aided by compilers, libraries, and domain-specific languages.\nReverse-mode automatic differentiation, also known as algorithmic\ndifferentiation, autodiff, adjoint differentiation, or back-propagation, is\nsometimes used to obtain gradients of programs that contain stencil loops.\nUnfortunately, conventional automatic differentiation results in a memory\naccess pattern that is not stencil-like and not easily parallelisable.\n  In this paper we present a novel combination of automatic differentiation and\nloop transformations that preserves the structure and memory access pattern of\nstencil loops, while computing fully consistent derivatives. The generated\nloops can be parallelised and optimised for performance in the same way and\nusing the same tools as the original computation. We have implemented this new\ntechnique in the Python tool PerforAD, which we release with this paper along\nwith test cases derived from seismic imaging and computational fluid dynamics\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 13:33:34 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["H\u00fcckelheim", "Jan", ""], ["Kukreja", "Navjot", ""], ["Narayanan", "Sri Hari Krishna", ""], ["Luporini", "Fabio", ""], ["Gorman", "Gerard", ""], ["Hovland", "Paul", ""]]}, {"id": "1907.02894", "submitter": "Putt Sakdhnagool", "authors": "Putt Sakdhnagool, Amit Sabne, Rudolf Eigenmann", "title": "RegDem: Increasing GPU Performance via Shared Memory Register Spilling", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GPU utilization, measured as occupancy, is limited by the parallel threads'\ncombined usage of on-chip resources, such as registers and the\nprogrammer-managed shared memory. Higher resource demand means lower effective\nparallel thread count, and therefore lower program performance. Our\ninvestigation found that registers are often the occupancy limiters.\n  The de-facto nvcc compiler-based approach spills excessive registers to the\noff-chip memory, ignoring the shared memory and leaving the on-chip resources\nunderutilized. To mitigate the register demand, this paper presents a binary\ntranslation technique, called RegDem, that spills excessive registers to the\nunderutilized shared memory by transforming the GPU assembly code (SASS). Most\nGPU programs do not fully use shared memory, thus allowing RegDem to use it for\nregister spilling. The higher occupancy achieved by RegDem outweighs the\nslightly higher cost of accessing shared memory instead of placing data in\nregisters. The paper also presents a compile-time performance predictor that\nmodels instructions stalls to choose the best version from a set of program\nvariants. Cumulatively, these techniques outperform the nvcc compiler with a 9%\ngeometric mean, the highest observed being 18%.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 15:30:04 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Sakdhnagool", "Putt", ""], ["Sabne", "Amit", ""], ["Eigenmann", "Rudolf", ""]]}, {"id": "1907.02900", "submitter": "Oded Green", "authors": "Oded Green", "title": "HashGraph -- Scalable Hash Tables Using A Sparse Graph Data Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hash tables are ubiquitous and used in a wide range of applications for\nefficient probing of large and unsorted data. If designed properly, hash-tables\ncan enable efficients look ups in a constant number of operations or commonly\nreferred to as O(1) operations. As data sizes continue to grow and data becomes\nless structured (as is common for big-data applications), the need for\nefficient and scalable hash table also grows. In this paper we introduce\nHashGraph, a new scalable approach for building hash tables that uses concepts\ntaken from sparse graph representations--hence the name HashGraph. We show two\ndifferent variants of HashGraph, a simple algorithm that outlines the method to\ncreate the hash-table and an advanced method that creates the hash table in a\nmore efficient manner (with an improved memory access pattern). HashGraph shows\na new way to deal with hash-collisions that does not use \"open-addressing\" or\n\"chaining\", yet has all the benefits of both these approaches. HashGraph\ncurrently works for static inputs, though recent progress with dynamic graph\ndata structures suggest that HashGraph might be extended to dynamic inputs as\nwell. We show that HashGraph can deal with a large number of hash-values per\nentry without loss of performance as most open-addressing and chaining\napproaches have. Further, we show that HashGraph is indifferent to the\nload-factor. Lastly, we show a new probing algorithm for the second phase of\nvalue lookups. Given the above, HashGraph is extremely fast and outperforms\nseveral state of the art hash-table implementations. The implementation of\nHashGraph in this paper is for NVIDIA GPUs, though HashGraph is not\narchitecture dependent. Using a NVIDIA GV100 GPU, HashGraph is anywhere from\n2X-8X faster than cuDPP, WarpDrive, and cuDF. HashGraph is able to build a\nhash-table at a rate of 2.5 billion keys per second and can probe at nearly the\nsame rate.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 15:44:27 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Green", "Oded", ""]]}, {"id": "1907.03081", "submitter": "Behnam Dezfouli", "authors": "Colton Powell, Christopher Desiniotis, and Behnam Dezfouli", "title": "The Fog Development Kit: A Development Platform for SDN-based Edge-Fog\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": "SIOTLAB-TECHNICAL-REPORT-FDK-Fall2019", "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rise of the Internet of Things (IoT), fog computing has emerged to\nhelp traditional cloud computing in meeting scalability demands. Fog computing\nmakes it possible to fulfill real-time requirements of applications by bringing\nmore processing, storage, and control power geographically closer to\nend-devices. However, since fog computing is a relatively new field, there is\nno standard platform for research and development in a realistic environment,\nand this dramatically inhibits innovation and development of fog-based\napplications. In response to these challenges, we propose the Fog Development\nKit (FDK). By providing high-level interfaces for allocating computing and\nnetworking resources, the FDK abstracts the complexities of fog computing from\ndevelopers and enables the rapid development of fog systems. In addition to\nsupporting application development on a physical deployment, the FDK supports\nthe use of emulation tools (e.g., GNS3 and Mininet) to create realistic\nenvironments, allowing fog application prototypes to be built with zero\nadditional costs and enabling seamless portability to a physical\ninfrastructure. Using a physical testbed and various kinds of applications\nrunning on it, we verify the operation and study the performance of the FDK.\nSpecifically, we demonstrate that resource allocations are appropriately\nenforced and guaranteed, even amidst extreme network congestion. We also\npresent simulation-based scalability analysis of the FDK versus the number of\nswitches, the number of end-devices, and the number of fog-devices.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 05:50:57 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 06:50:38 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Powell", "Colton", ""], ["Desiniotis", "Christopher", ""], ["Dezfouli", "Behnam", ""]]}, {"id": "1907.03103", "submitter": "Vasisht Duddu", "authors": "Vasisht Duddu, D. Vijay Rao, Valentina E. Balas", "title": "Towards Enhancing Fault Tolerance in Neural Networks", "comments": "MobiQuitous 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC cs.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning Accelerators are prone to faults which manifest in the form of\nerrors in Neural Networks. Fault Tolerance in Neural Networks is crucial in\nreal-time safety critical applications requiring computation for long\ndurations. Neural Networks with high regularisation exhibit superior fault\ntolerance, however, at the cost of classification accuracy. In the view of\ndifference in functionality, a Neural Network is modelled as two separate\nnetworks, i.e, the Feature Extractor with unsupervised learning objective and\nthe Classifier with a supervised learning objective. Traditional approaches of\ntraining the entire network using a single supervised learning objective is\ninsufficient to achieve the objectives of the individual components optimally.\nIn this work, a novel multi-criteria objective function, combining unsupervised\ntraining of the Feature Extractor followed by supervised tuning with Classifier\nNetwork is proposed. The unsupervised training solves two games simultaneously\nin the presence of adversary neural networks with conflicting objectives to the\nFeature Extractor. The first game minimises the loss in reconstructing the\ninput image for indistinguishability given the features from the Extractor, in\nthe presence of a generative decoder. The second game solves a minimax\nconstraint optimisation for distributional smoothening of feature space to\nmatch a prior distribution, in the presence of a Discriminator network. The\nresultant strongly regularised Feature Extractor is combined with the\nClassifier Network for supervised fine-tuning. The proposed Adversarial Fault\nTolerant Neural Network Training is scalable to large networks and is\nindependent of the architecture. The evaluation on benchmarking datasets:\nFashionMNIST and CIFAR10, indicates that the resultant networks have high\naccuracy with superior tolerance to stuck at \"0\" faults compared to widely used\nregularisers.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 09:39:26 GMT"}, {"version": "v2", "created": "Tue, 9 Jul 2019 05:37:21 GMT"}, {"version": "v3", "created": "Sun, 30 May 2021 03:31:58 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Duddu", "Vasisht", ""], ["Rao", "D. Vijay", ""], ["Balas", "Valentina E.", ""]]}, {"id": "1907.03195", "submitter": "Jeremy Kepner", "authors": "Chansup Byun, Jeremy Kepner, William Arcand, David Bestor, William\n  Bergeron, Matthew Hubbell, Vijay Gadepally, Michael Houle, Michael Jones,\n  Anne Klein, Lauren Milechin, Peter Michaleas, Julie Mullen, Andrew Prout,\n  Antonio Rosa, Siddharth Samsi, Charles Yee, Albert Reuther", "title": "Optimizing Xeon Phi for Interactive Data Analysis", "comments": "6 pages, 5 figures, accepted in IEEE High Performance Extreme\n  Computing (HPEC) conference 2019", "journal-ref": null, "doi": "10.1109/HPEC.2019.8916300", "report-no": null, "categories": "cs.PF cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Intel Xeon Phi manycore processor is designed to provide high performance\nmatrix computations of the type often performed in data analysis. Common data\nanalysis environments include Matlab, GNU Octave, Julia, Python, and R.\nAchieving optimal performance of matrix operations within data analysis\nenvironments requires tuning the Xeon Phi OpenMP settings, process pinning, and\nmemory modes. This paper describes matrix multiplication performance results\nfor Matlab and GNU Octave over a variety of combinations of process counts and\nOpenMP threads and Xeon Phi memory modes. These results indicate that using\nKMP_AFFINITY=granlarity=fine, taskset pinning, and all2all cache memory mode\nallows both Matlab and GNU Octave to achieve 66% of the practical peak\nperformance for process counts ranging from 1 to 64 and OpenMP threads ranging\nfrom 1 to 64. These settings have resulted in generally improved performance\nacross a range of applications and has enabled our Xeon Phi system to deliver\nsignificant results in a number of real-world applications.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 22:04:25 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Byun", "Chansup", ""], ["Kepner", "Jeremy", ""], ["Arcand", "William", ""], ["Bestor", "David", ""], ["Bergeron", "William", ""], ["Hubbell", "Matthew", ""], ["Gadepally", "Vijay", ""], ["Houle", "Michael", ""], ["Jones", "Michael", ""], ["Klein", "Anne", ""], ["Milechin", "Lauren", ""], ["Michaleas", "Peter", ""], ["Mullen", "Julie", ""], ["Prout", "Andrew", ""], ["Rosa", "Antonio", ""], ["Samsi", "Siddharth", ""], ["Yee", "Charles", ""], ["Reuther", "Albert", ""]]}, {"id": "1907.03331", "submitter": "Alex Manuskin", "authors": "Alex Manuskin, Michael Mirkin, Ittay Eyal", "title": "Ostraka: Secure Blockchain Scaling by Node Sharding", "comments": "In proceedings of IEEE Security & Privacy on The Blockchain (IEEE S&B\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Cryptocurrencies, implemented with blockchain protocols, promise to become a\nglobal payment system if they can overcome performance limitations. Rapidly\nadvancing architectures improve on latency and throughput, but most require all\nparticipating servers to process all transactions. Several recent works propose\nto shard the system, such that each machine would only process a subset of the\ntransactions. However, we identify a denial-of-service attack that is exposed\nby these solutions - an attacker can generate transactions that would overload\na single shard, thus delaying processing in the entire system. Moreover, we\nshow that in common scenarios, these protocols require most node operators to\nprocess almost all blockchain transactions. We present Ostraka, a blockchain\nnode architecture that shards (parallelizes) the nodes themselves. We prove\nthat replacing a unified node with an Ostraka node does not affect the security\nof the underlying consensus mechanism. We evaluate analytically and\nexperimentally block propagation and processing in various settings. Ostraka\nallows nodes in the network to scale, without costly coordination. In our\nexperiments, Ostraka nodes' transaction processing rate grows linearly with the\naddition of resources.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2019 18:55:03 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2020 08:18:43 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Manuskin", "Alex", ""], ["Mirkin", "Michael", ""], ["Eyal", "Ittay", ""]]}, {"id": "1907.03335", "submitter": "Disa Mhembere", "authors": "Disa Mhembere, Da Zheng, Carey E. Priebe, Joshua T. Vogelstein and\n  Randal Burns", "title": "Graphyti: A Semi-External Memory Graph Library for FlashGraph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph datasets exceed the in-memory capacity of most standalone machines.\nTraditionally, graph frameworks have overcome memory limitations through\nscale-out, distributing computing. Emerging frameworks avoid the network\nbottleneck of distributed data with Semi-External Memory (SEM) that uses a\nsingle multicore node and operates on graphs larger than memory. In SEM,\n$\\mathcal{O}(m)$ data resides on disk and $\\mathcal{O}(n)$ data in memory, for\na graph with $n$ vertices and $m$ edges. For developers, this adds complexity\nbecause they must explicitly encode I/O within applications. We present\nprinciples that are critical for application developers to adopt in order to\nachieve state-of-the-art performance, while minimizing I/O and memory for\nalgorithms in SEM. We present them in Graphyti, an extensible parallel SEM\ngraph library built on FlashGraph and available in Python via pip. In SEM,\nGraphyti achieves 80% of the performance of in-memory execution and retains the\nperformance of FlashGraph, which outperforms distributed engines, such as\nPowerGraph and Galois.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2019 19:13:24 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Mhembere", "Disa", ""], ["Zheng", "Da", ""], ["Priebe", "Carey E.", ""], ["Vogelstein", "Joshua T.", ""], ["Burns", "Randal", ""]]}, {"id": "1907.03437", "submitter": "Po-Chun Kuo", "authors": "Tzu-Wei Chao and Hao Chung and Po-Chun Kuo", "title": "Fair Byzantine Agreements for Blockchains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Byzantine general problem is the core problem of the consensus algorithm, and\nmany protocols are proposed recently to improve the decentralization level, the\nperformance and the security of the blockchain. There are two challenging\nissues when the blockchain is operating in practice. First, the outcomes of the\nconsensus algorithm are usually related to the incentive model, so whether each\nparticipant's value has an equal probability of being chosen becomes essential.\nHowever, the issues of fairness are not captured in the traditional security\ndefinition of Byzantine agreement. Second, the blockchain should be resistant\nto network failures, such as cloud services shut down or malicious attack,\nwhile remains the high performance most of the time.\n  This paper has two main contributions. First, we propose a novel notion\ncalled fair validity for Byzantine agreement. Intuitively, fair validity\nlower-bounds the expected numbers that honest nodes' values being decided if\nthe protocol is executed many times. However, we also show that any Byzantine\nagreement could not achieve fair validity in an asynchronous network, so we\nfocus on synchronous protocols. This leads to our second contribution: we\npropose a fair, responsive and partition-resilient Byzantine agreement protocol\ntolerating up to 1/3 corruptions. Fairness means that our protocol achieves\nfair validity. Responsiveness means that the termination time only depends on\nthe actual network delay instead of depending on any pre-determined time bound.\nPartition-resilience means that the safety still holds even if the network is\npartitioned, and the termination will hold if the partition is resolved.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 07:43:24 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Chao", "Tzu-Wei", ""], ["Chung", "Hao", ""], ["Kuo", "Po-Chun", ""]]}, {"id": "1907.03565", "submitter": "Ami Paz", "authors": "Armando Casta\\~neda, Pierre Fraigniaud, Ami Paz, Sergio Rajsbaum,\n  Matthieu Roy, and Corentin Travers", "title": "A Topological Perspective on Distributed Network Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  More than two decades ago, combinatorial topology was shown to be useful for\nanalyzing distributed fault-tolerant algorithms in shared memory systems and in\nmessage passing systems. In this work, we show that combinatorial topology can\nalso be useful for analyzing distributed algorithms in failure-free networks of\narbitrary structure. To illustrate this, we analyze consensus, set-agreement,\nand approximate agreement in networks, and derive lower bounds for these\nproblems under classical computational settings, such as the LOCAL model and\ndynamic networks.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 12:45:38 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 15:01:26 GMT"}, {"version": "v3", "created": "Thu, 1 Oct 2020 20:49:36 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Casta\u00f1eda", "Armando", ""], ["Fraigniaud", "Pierre", ""], ["Paz", "Ami", ""], ["Rajsbaum", "Sergio", ""], ["Roy", "Matthieu", ""], ["Travers", "Corentin", ""]]}, {"id": "1907.03592", "submitter": "Sejuti Banik", "authors": "Sejuti Banik, Irvin Steve Cardenas, Jong Hoon Kim", "title": "IoT Platforms for 5G Network and Practical Considerations: A Survey", "comments": "14 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fifth generation (5G) mobile network will enable the Internet of Things\n(IoT) to take a large leap into the age of future computing. As a result of\nextended connectivity, high speed, reduced latency services being provided by\n5G, IoT has experienced and will continue to undergo a remarkable transition in\nevery field of daily life. Furthermore, fog computing will revolutionize the\nIoT platforms by decentralizing the operations by the cloud and ensuring\nsustainability with big data, mobility and reduced processing lag. 5G is\nubiquitous, reliable, scalable and economic in nature. The features will not\nonly globalize IoT in a broader spectrum, but also make common people interact\nsmartly and efficiently with the environment in real time. In this study, a\ncombined survey is presented on different IoT applications coupled with cloud\nplatforms. Moreover, the capabilities of IoT in the influence of 5G are\nexplored as well as how the IoT platform and services will adopt through 5G are\nenvisaged. Additionally, some open issues triggered by 5G have been introduced\nto harness the maximum benefit out of this network. Finally, a platform is\nproposed to implement in the telepresence project based on the investigation\nand findings.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 11:09:08 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Banik", "Sejuti", ""], ["Cardenas", "Irvin Steve", ""], ["Kim", "Jong Hoon", ""]]}, {"id": "1907.03626", "submitter": "Wei Dai", "authors": "Wei Dai and Daniel Berleant", "title": "Benchmarking Contemporary Deep Learning Hardware and Frameworks:A Survey\n  of Qualitative Metrics", "comments": "8 pages, 4 figures, 2 tables", "journal-ref": null, "doi": "10.1109/CogMI48466.2019.00029", "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper surveys benchmarking principles, machine learning devices\nincluding GPUs, FPGAs, and ASICs, and deep learning software frameworks. It\nalso reviews these technologies with respect to benchmarking from the\nperspectives of a 6-metric approach to frameworks and an 11-metric approach to\nhardware platforms. Because MLPerf is a benchmark organization working with\nindustry and academia, and offering deep learning benchmarks that evaluate\ntraining and inference on deep learning hardware devices, the survey also\nmentions MLPerf benchmark results, benchmark metrics, datasets, deep learning\nframeworks and algorithms. We summarize seven benchmarking principles,\ndifferential characteristics of mainstream AI devices, and qualitative\ncomparison of deep learning hardware and frameworks.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 14:09:06 GMT"}, {"version": "v2", "created": "Tue, 9 Jul 2019 19:17:33 GMT"}, {"version": "v3", "created": "Sat, 19 Oct 2019 18:30:38 GMT"}, {"version": "v4", "created": "Sat, 23 Nov 2019 21:29:27 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Dai", "Wei", ""], ["Berleant", "Daniel", ""]]}, {"id": "1907.03655", "submitter": "Quan Nguyen Hoang", "authors": "Quan Nguyen, Andre Cronje, Michael Kong, Alex Kampa, George Samman", "title": "StakeDag: Stake-based Consensus For Scalable Trustless Systems", "comments": "arXiv admin note: text overlap with arXiv:1905.04867", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trustless systems, such as those blockchain enpowered, provide trust in the\nsystem regardless of the trust of its participants, who may be honest or\nmalicious. Proof-of-stake (PoS) protocols and DAG-based approaches have emerged\nas a better alternative than the proof of work (PoW) for consensus. This paper\nintroduces a new model, so-called \\emph{\\stakedag}, which aims for PoS\nconsensus in a DAG-based trustless system. We address a general model of\ntrustless system in which participants are distinguished by their stake or\ntrust: users and validators. Users are normal participants with a no assumed\ntrust and validators are high profile participants with an established trust.\nWe then propose a new family of stake-based consensus protocols $\\mathfrak{S}$,\noperating on the DAG as in the Lachesis protocol~\\cite{lachesis01}.\nSpecifically, we propose a stake-based protocol $S_\\phi$ that leverages\nparticipants' stake as validating weights to achieve more secure distributed\nsystems with practical Byzantine fault tolerance (pBFT) in leaderless\nasynchronous Directed Acyclic Graph (DAG). We then present a general model of\nstaking for asynchronous DAG-based distributed systems.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 04:32:56 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Nguyen", "Quan", ""], ["Cronje", "Andre", ""], ["Kong", "Michael", ""], ["Kampa", "Alex", ""], ["Samman", "George", ""]]}, {"id": "1907.03688", "submitter": "Derek Weitzel", "authors": "Derek Weitzel", "title": "Enabling Microsoft OneDrive Integration with HTCondor", "comments": "Humans in the Loop: Enabling and Facilitating Research on Cloud\n  Computing Workshop at PEARC 19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accessing data from distributed computing is essential in many workflows, but\ncan be complicated for users of cyberinfrastructure. They must perform multiple\nsteps to make data available to distributed computing using unfamiliar tools.\nFurther, most research on data distribution has focused on the efficiency of\nproviding data to computing resources rather than considering the ease of use\nfor distributing data. Creating an easy to use data distribution method can\nreduce the time researchers spend learning cyberinfrastructure and increase its\nusefulness.\n  Microsoft OneDrive is a online storage solution providing both file storage\nand sharing. OneDrive provides many different clients to access data stored in\nthe service. It provides many features that users of cyberinfrastructure could\nfind useful such as automatic synchronization with desktop clients.\n  A barrier to using services such as OneDrive is the credential management\nnecessary to access the service. Recent innovations in HTCondor have allowed\nthe management of OAuth credentials to be handled by the scheduler on the\nuser's behalf. The user no longer has to copy credentials along with the job,\nHTCondor will handle the acquisition, renewal, and secure transfer of\ncredentials on the user's behalf.\n  In this paper, I will focus on providing an easy to use data distribution\nmethod utilizing Microsoft OneDrive. Measuring ease of use is difficult,\ntherefore I will will describe the features and advantages of using OneDrive.\nAdditionally, I will compare it to measurements of data distribution methods\ncurrently used on a national cyberinfastructure, the Open Science Grid.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 15:51:30 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Weitzel", "Derek", ""]]}, {"id": "1907.03730", "submitter": "Alejandro Ranchal-Pedrosa", "authors": "Alejandro Ranchal-Pedrosa, Vincent Gramoli", "title": "Platypus: a Partially Synchronous Offchain Protocol for Blockchains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Offchain protocols aim at bypassing the scalability and privacy limitations\nof classic blockchains by allowing a subset of participants to execute multiple\ntransactions outside the blockchain. While existing solutions like payment\nnetworks and factories depend on a complex routing protocol, other solutions\nsimply require participants to build a \\emph{childchain}, a secondary\nblockchain where their transactions are privately executed. Unfortunately, all\nchildchain solutions assume either synchrony or a trusted execution\nenvironment.\n  In this paper, we present Platypus a childchain that requires neither\nsynchrony nor a trusted execution environment. Relieving the need for a trusted\nexecution environment allows Platypus to ensure privacy without trusting a\ncentral authority, like Intel, that manufactures dedicated hardware chipset,\nlike SGX. Relieving the need for synchrony means that no attacker can steal\ncoins by leveraging clock drifts or message delays to lure timelocks.\n  In order to prove our algorithm correct, we formalize the chilchain problem\nas a Byzantine variant of the classic Atomic Commit problem, where closing a\nchildchain is equivalent to committing the whole set of payments previously\nrecorded on the childchain ``atomically'' on the main chain. Platypus is\nresilience optimal and we explain how to generalize it to crosschain payments.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 17:22:47 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Ranchal-Pedrosa", "Alejandro", ""], ["Gramoli", "Vincent", ""]]}, {"id": "1907.03797", "submitter": "Fabian Kuhn", "authors": "Fabian Kuhn", "title": "Faster Deterministic Distributed Coloring Through Recursive List\n  Coloring", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide novel deterministic distributed vertex coloring algorithms. As our\nmain result, we give a deterministic distributed algorithm to compute a\n$(\\Delta+1)$-coloring of an $n$-node graph with maximum degree $\\Delta$ in\n$2^{O(\\sqrt{\\log\\Delta})}\\cdot\\log n$ rounds. This improves on the best\npreviously known time complexity for a large range of values of $\\Delta$. For\ngraphs with arboricity $a$, we obtain a deterministic distributed algorithm to\ncompute a $(2+o(1))a$-coloring in time $2^{O(\\sqrt{\\log a})}\\cdot\\log^2 n$.\nFurther, for graphs with bounded neighborhood independence, we show that a\n$(\\Delta+1)$-coloring can be computed more efficiently in time\n$2^{O(\\sqrt{\\log\\Delta})} + O(\\log^* n)$. This in particular implies that also\na $(2\\Delta-1)$-edge coloring can be computed deterministically in\n$2^{O(\\sqrt{\\log\\Delta})} + O(\\log^* n)$ rounds, which improves the best known\ntime bound for small values of $\\Delta$. All results even hold for the list\ncoloring variants of the problems. As a consequence, we also obtain an improved\ndeterministic $2^{O(\\sqrt{\\log\\Delta})}\\cdot\\log^3 n$-round algorithm for\n$\\Delta$-coloring non-complete graphs with maximum degree $\\Delta\\geq 3$. Most\nof our algorithms only require messages of $O(\\log n)$ bits (including the\n$(\\Delta+1)$-vertex coloring algorithms).\n  Our main technical contribution is a recursive deterministic distributed list\ncoloring algorithm to solve list coloring problems with lists of size\n$\\Delta^{1+o(1)}$. Given some list coloring problem and an orientation of the\nedges, we show how to recursively divide the global color space into smaller\nsubspaces, assign one of the subspaces to each node of the graph, and compute a\nnew edge orientation such that for each node, the list size to out-degree ratio\ndegrades at most by a constant factor on each recursion level.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 18:22:43 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Kuhn", "Fabian", ""]]}, {"id": "1907.03909", "submitter": "Mohammad Mohammadi Amiri Mr.", "authors": "Mohammad Mohammadi Amiri, Tolga M. Duman, Deniz Gunduz", "title": "Collaborative Machine Learning at the Wireless Edge with Blind\n  Transmitters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study wireless collaborative machine learning (ML), where mobile edge\ndevices, each with its own dataset, carry out distributed stochastic gradient\ndescent (DSGD) over-the-air with the help of a wireless access point acting as\nthe parameter server (PS). At each iteration of the DSGD algorithm wireless\ndevices compute gradient estimates with their local datasets, and send them to\nthe PS over a wireless fading multiple access channel (MAC). Motivated by the\nadditive nature of the wireless MAC, we propose an analog DSGD scheme, in which\nthe devices transmit scaled versions of their gradient estimates in an uncoded\nfashion. We assume that the channel state information (CSI) is available only\nat the PS. We instead allow the PS to employ multiple antennas to alleviate the\ndestructive fading effect, which cannot be cancelled by the transmitters due to\nthe lack of CSI. Theoretical analysis indicates that, with the proposed DSGD\nscheme, increasing the number of PS antennas mitigates the fading effect, and,\nin the limit, the effects of fading and noise disappear, and the PS receives\naligned signals used to update the model parameter. The theoretical results are\nthen corroborated with the experimental ones.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 23:28:40 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Amiri", "Mohammad Mohammadi", ""], ["Duman", "Tolga M.", ""], ["Gunduz", "Deniz", ""]]}, {"id": "1907.04080", "submitter": "Alexey Lastovetsky", "authors": "Hamidreza Khaleghzadeh, Muhammad Fahad, Arsalan Shahid, Ravi Reddy\n  Manumachu, Alexey Lastovetsky", "title": "Bi-objective Optimisation of Data-parallel Applications on Heterogeneous\n  Platforms for Performance and Energy via Workload Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance and energy are the two most important objectives for optimisation\non modern parallel platforms. Latest research demonstrated the importance of\nworkload distribution as a decision variable in the bi-objective optimisation\nfor performance and energy on homogeneous multicore clusters. We show in this\nwork that bi-objective optimisation for performance and energy on heterogeneous\nprocessors results in a large number of Pareto-optimal optimal solutions\n(workload distributions) even in the simple case of linear performance and\nenergy profiles. We then study performance and energy profiles of real-life\ndata-parallel applications and find that their shapes are non-linear, complex\nand non-smooth. We, therefore, propose an efficient and exact global\noptimisation algorithm, which takes as an input most general discrete\nperformance and dynamic energy profiles of the heterogeneous processors and\nsolves the bi-objective optimisation problem. The algorithm is also used as a\nbuilding block to solve the bi-objective optimisation problem for performance\nand total energy. We also propose a novel methodology to build discrete dynamic\nenergy profiles of individual computing devices, which are input to the\nalgorithm. The methodology is based purely on system-level measurements and\naddresses the fundamental challenge of accurate component-level energy\nmodelling of a hybrid data-parallel application running on a heterogeneous\nplatform integrating CPUs and accelerators. We experimentally validate the\nproposed method using two data-parallel applications, matrix multiplication and\n2D fast Fourier transform (2D-FFT).\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 11:15:30 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Khaleghzadeh", "Hamidreza", ""], ["Fahad", "Muhammad", ""], ["Shahid", "Arsalan", ""], ["Manumachu", "Ravi Reddy", ""], ["Lastovetsky", "Alexey", ""]]}, {"id": "1907.04156", "submitter": "Mehmet Aydar", "authors": "Mehmet Aydar, Salih Cemil Cetin, Serkan Ayvaz, Betul Aygun", "title": "Private key encryption and recovery in blockchain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The disruptive technology of blockchain can deliver secure solutions without\nthe need for a central authority. In blockchain protocols, assets that belong\nto a participant are controlled through the private key of an asymmetric key\npair that is owned by the participant. Although, this lets blockchain network\nparticipants to have sovereignty on their assets, it comes with the\nresponsibility of managing their own keys. Currently, there exists two major\nbottlenecks in managing keys; $a)$ users don't have an efficient and secure way\nto store their keys, $b)$ no efficient recovery mechanism exists in case the\nkeys are lost. In this study, we propose secure methods to efficiently store\nand recover keys. For the first, we introduce an efficient encryption mechanism\nto securely encrypt and decrypt the private key using the owner's biometric\nsignature. For the later, we introduce an efficient recovery mechanism using\nbiometrics and secret sharing scheme. By applying the proposed key encryption\nand recovery mechanism, asset owners are able to securely store their keys on\ntheir devices and recover the keys in case they are lost.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 13:35:36 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 14:12:03 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Aydar", "Mehmet", ""], ["Cetin", "Salih Cemil", ""], ["Ayvaz", "Serkan", ""], ["Aygun", "Betul", ""]]}, {"id": "1907.04217", "submitter": "Jeremy Kepner", "authors": "Jeremy Kepner, Vijay Gadepally, Lauren Milechin, Siddharth Samsi,\n  William Arcand, David Bestor, William Bergeron, Chansup Byun, Matthew\n  Hubbell, Michael Houle, Michael Jones, Anne Klein, Peter Michaleas, Julie\n  Mullen, Andrew Prout, Antonio Rosa, Charles Yee, Albert Reuther", "title": "Streaming 1.9 Billion Hypersparse Network Updates per Second with D4M", "comments": "6 pages; 6 figures; accepted to IEEE High Performance Extreme\n  Computing (HPEC) Conference 2019. arXiv admin note: text overlap with\n  arXiv:1807.05308, arXiv:1902.00846", "journal-ref": null, "doi": "10.1109/HPEC.2019.8916508", "report-no": null, "categories": "cs.DC cs.DB cs.DS cs.IR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Dynamic Distributed Dimensional Data Model (D4M) library implements\nassociative arrays in a variety of languages (Python, Julia, and Matlab/Octave)\nand provides a lightweight in-memory database implementation of hypersparse\narrays that are ideal for analyzing many types of network data. D4M relies on\nassociative arrays which combine properties of spreadsheets, databases,\nmatrices, graphs, and networks, while providing rigorous mathematical\nguarantees, such as linearity. Streaming updates of D4M associative arrays put\nenormous pressure on the memory hierarchy. This work describes the design and\nperformance optimization of an implementation of hierarchical associative\narrays that reduces memory pressure and dramatically increases the update rate\ninto an associative array. The parameters of hierarchical associative arrays\nrely on controlling the number of entries in each level in the hierarchy before\nan update is cascaded. The parameters are easily tunable to achieve optimal\nperformance for a variety of applications. Hierarchical arrays achieve over\n40,000 updates per second in a single instance. Scaling to 34,000 instances of\nhierarchical D4M associative arrays on 1,100 server nodes on the MIT SuperCloud\nachieved a sustained update rate of 1,900,000,000 updates per second. This\ncapability allows the MIT SuperCloud to analyze extremely large streaming\nnetwork data sets.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 20:55:04 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Kepner", "Jeremy", ""], ["Gadepally", "Vijay", ""], ["Milechin", "Lauren", ""], ["Samsi", "Siddharth", ""], ["Arcand", "William", ""], ["Bestor", "David", ""], ["Bergeron", "William", ""], ["Byun", "Chansup", ""], ["Hubbell", "Matthew", ""], ["Houle", "Michael", ""], ["Jones", "Michael", ""], ["Klein", "Anne", ""], ["Michaleas", "Peter", ""], ["Mullen", "Julie", ""], ["Prout", "Andrew", ""], ["Rosa", "Antonio", ""], ["Yee", "Charles", ""], ["Reuther", "Albert", ""]]}, {"id": "1907.04691", "submitter": "Mohammadreza Chamanbaz Dr.", "authors": "Mohammadreza Chamanbaz, Giuseppe Notarstefano, Francesco Sasso, Roland\n  Bouffanais", "title": "Randomized Constraints Consensus for Distributed Robust Mixed-Integer\n  Programming", "comments": "Submitted for publication. arXiv admin note: text overlap with\n  arXiv:1706.00488", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a network of processors aiming at cooperatively\nsolving mixed-integer convex programs subject to uncertainty. Each node only\nknows a common cost function and its local uncertain constraint set. We propose\na randomized, distributed algorithm working under asynchronous, unreliable and\ndirected communication. The algorithm is based on a local computation and\ncommunication paradigm. At each communication round, nodes perform two updates:\n(i) a verification in which they check---in a randomized fashion---the robust\nfeasibility of a candidate optimal point, and (ii) an optimization step in\nwhich they exchange their candidate basis (the minimal set of constraints\ndefining a solution) with neighbors and locally solve an optimization problem.\nAs main result, we show that processors can stop the algorithm after a finite\nnumber of communication rounds (either because verification has been successful\nfor a sufficient number of rounds or because a given threshold has been\nreached), so that candidate optimal solutions are consensual. The common\nsolution is proven to be---with high confidence---feasible and hence optimal\nfor the entire set of uncertainty except a subset having an arbitrary small\nprobability measure. We show the effectiveness of the proposed distributed\nalgorithm using two examples: a random, uncertain mixed-integer linear program\nand a distributed localization in wireless sensor networks. The distributed\nalgorithm is implemented on a multi-core platform in which the nodes\ncommunicate asynchronously.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 08:16:09 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Chamanbaz", "Mohammadreza", ""], ["Notarstefano", "Giuseppe", ""], ["Sasso", "Francesco", ""], ["Bouffanais", "Roland", ""]]}, {"id": "1907.04827", "submitter": "Lalith Suresh", "authors": "Mihai Budiu, Parikshit Gopalan, Lalith Suresh, Udi Wieder, Han\n  Kruiger, Marcos K. Aguilera", "title": "Hillview: A trillion-cell spreadsheet for big data", "comments": null, "journal-ref": null, "doi": "10.14778/3342263.3342279", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hillview is a distributed spreadsheet for browsing very large datasets that\ncannot be handled by a single machine. As a spreadsheet, Hillview provides a\nhigh degree of interactivity that permits data analysts to explore information\nquickly along many dimensions while switching visualizations on a whim. To\nprovide the required responsiveness, Hillview introduces visualization\nsketches, or vizketches, as a simple idea to produce compact data\nvisualizations. Vizketches combine algorithmic techniques for data\nsummarization with computer graphics principles for efficient rendering. While\nsimple, vizketches are effective at scaling the spreadsheet by parallelizing\ncomputation, reducing communication, providing progressive visualizations, and\noffering precise accuracy guarantees. Using Hillview running on eight servers,\nwe can navigate and visualize datasets of tens of billions of rows and\ntrillions of cells, much beyond the published capabilities of competing\nsystems.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 17:13:40 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Budiu", "Mihai", ""], ["Gopalan", "Parikshit", ""], ["Suresh", "Lalith", ""], ["Wieder", "Udi", ""], ["Kruiger", "Han", ""], ["Aguilera", "Marcos K.", ""]]}, {"id": "1907.05013", "submitter": "Toshio Endo", "authors": "Yuki Ito, Haruki Imai, Tung Le Duc, Yasushi Negishi, Kiyokuni\n  Kawachiya, Ryo Matsumiya, Toshio Endo", "title": "Profiling based Out-of-core Hybrid Method for Large Neural Networks", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GPUs are widely used to accelerate deep learning with NNs (NNs). On the other\nhand, since GPU memory capacity is limited, it is difficult to implement\nefficient programs that compute large NNs on GPU. To compute NNs exceeding GPU\nmemory capacity, data-swapping method and recomputing method have been proposed\nin existing work. However, in these methods, performance overhead occurs due to\ndata movement or increase of computation. In order to reduce the overhead, it\nis important to consider characteristics of each layer such as sizes and cost\nfor recomputation. Based on this direction, we proposed Profiling based\nout-of-core Hybrid method (PoocH). PoocH determines target layers of swapping\nor recomputing based on runtime profiling. We implemented PoocH by extending a\ndeep learning framework, Chainer, and we evaluated its performance. With PoocH,\nwe successfully computed an NN requiring 50 GB memory on a single GPU with 16\nGB memory. Compared with in-core cases, performance degradation was 38 \\% on\nx86 machine and 28 \\% on POWER9 machine.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 06:31:38 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Ito", "Yuki", ""], ["Imai", "Haruki", ""], ["Duc", "Tung Le", ""], ["Negishi", "Yasushi", ""], ["Kawachiya", "Kiyokuni", ""], ["Matsumiya", "Ryo", ""], ["Endo", "Toshio", ""]]}, {"id": "1907.05016", "submitter": "Jing Li", "authors": "Jing Li and Dongning Guo", "title": "On Analysis of the Bitcoin and Prism Backbone Protocols", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bitcoin is a peer-to-peer payment system proposed by Nakamoto in 2008.\nProperties of the bitcoin backbone protocol have been investigated in some\ndepth: the blockchain growth property quantifies the number of blocks added to\nthe blockchain during any time intervals; the blockchain quality property\nensures the honest miners always contribute at least a certain fraction of the\nblockchain; the common prefix property ensures if a block is deep enough, it\nwill eventually be adopted by all honest miners with high probability.\nFollowing the spirit of decoupling various functionalities of the blockchain,\nthe Prism protocol is proposed to dramatically improve the throughput while\nmaintaining the same level of security. Prior analyses of the bitcoin and Prism\nbackbone protocols assume the lifespan of blockchain is finite. This paper\npresents a streamlined and strengthened analysis without the finite horizon\nassumption. Specifically, the results include a blockchain growth property, a\nblockchain quality property, and a common prefix property of the bitcoin\nbackbone protocol, as well as the liveness and persistence of the Prism\nbackbone protocol regardless of whether the blockchains have a infinite\nlifespan. We also express the properties of bitcoin and Prism backbone\nprotocols in explicit expressions rather than order optimal results, which lead\nto tighter bounds and practical references for public transaction ledger\nprotocol design.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 06:35:05 GMT"}, {"version": "v2", "created": "Sun, 20 Oct 2019 01:10:42 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Li", "Jing", ""], ["Guo", "Dongning", ""]]}, {"id": "1907.05312", "submitter": "Saurabh Jha", "authors": "Saurabh Jha, Archit Patke, Jim Brandt, Ann Gentile, Mike Showerman,\n  Eric Roman, Zbigniew T. Kalbarczyk, William T. Kramer, Ravishankar K. Iyer", "title": "A Study of Network Congestion in Two Supercomputing High-Speed\n  Interconnects", "comments": "Accepted for HOTI2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network congestion in high-speed interconnects is a major source of\napplication run time performance variation. Recent years have witnessed a surge\nof interest from both academia and industry in the development of novel\napproaches for congestion control at the network level and in application\nplacement, mapping, and scheduling at the system-level. However, these studies\nare based on proxy applications and benchmarks that are not representative of\nfield-congestion characteristics of high-speed interconnects. To address this\ngap, we present (a) an end-to-end framework for monitoring and analysis to\nsupport long-term field-congestion characterization studies, and (b) an\nempirical study of network congestion in petascale systems across two different\ninterconnect technologies: (i) Cray Gemini, which uses a 3-D torus topology,\nand (ii) Cray Aries, which uses the DragonFly topology.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 15:41:10 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Jha", "Saurabh", ""], ["Patke", "Archit", ""], ["Brandt", "Jim", ""], ["Gentile", "Ann", ""], ["Showerman", "Mike", ""], ["Roman", "Eric", ""], ["Kalbarczyk", "Zbigniew T.", ""], ["Kramer", "William T.", ""], ["Iyer", "Ravishankar K.", ""]]}, {"id": "1907.05314", "submitter": "Antoine Durand", "authors": "Antoine Durand, Emmanuelle Anceaume, Romaric Ludinard", "title": "StakeCube: Combining Sharding and Proof-of-Stake to build Fork-free\n  Secure Permissionless Distributed Ledgers", "comments": "Preprint, 16 pages, to appear in Proceedings of The 7th Edition of\n  The International Conference on NETworked sYStems (NETYS2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Our work focuses on the design of a scalable permissionless blockchain in the\nproof-of-stake setting. In particular, we use a distributed hash table as a\nbuilding block to set up randomized shards, and then leverage the sharded\narchitecture to validate blocks in an efficient manner. We combine verifiable\nByzantine agreements run by shards of stakeholders and a block validation\nprotocol to guarantee that forks occur with negligible probability. We impose\ninduced churn to make shards robust to eclipse attacks, and we rely on the UTXO\ncoin model to guarantee that any stakeholder action is securely verifiable by\nanyone. Our protocol works against adaptive adversary, and makes no synchrony\nassumption beyond what is required for the byzantine agreement.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 15:42:07 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Durand", "Antoine", ""], ["Anceaume", "Emmanuelle", ""], ["Ludinard", "Romaric", ""]]}, {"id": "1907.05391", "submitter": "Slobodan Mitrovi\\'c", "authors": "Jakub {\\L}\\k{a}cki, Slobodan Mitrovi\\'c, Krzysztof Onak, Piotr\n  Sankowski", "title": "Walking Randomly, Massively, and Efficiently", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a set of techniques that allow for efficiently generating many\nindependent random walks in the Massive Parallel Computation (MPC) model with\nspace per machine strongly sublinear in the number of vertices. In this\nspace-per-machine regime, many natural approaches to graph problems struggle to\novercome the $\\Theta(\\log n)$ MPC round complexity barrier. Our techniques\nenable breaking this barrier for PageRank---one of the most important\napplications of random walks---even in more challenging directed graphs, and\nfor approximate bipartiteness and expansion testing.\n  In the undirected case, we start our random walks from the stationary\ndistribution, which implies that we approximately know the empirical\ndistribution of their next steps. This allows for preparing continuations of\nrandom walks in advance and applying a doubling approach. As a result we can\ngenerate multiple random walks of length $l$ in $\\Theta(\\log l)$ rounds on MPC.\nMoreover, we show that under the popular 1-vs.-2-Cycles conjecture, this round\ncomplexity is asymptotically tight.\n  For directed graphs, our approach stems from our treatment of the PageRank\nMarkov chain. We first compute the PageRank for the undirected version of the\ninput graph and then slowly transition towards the directed case, considering\nconvex combinations of the transition matrices in the process.\n  For PageRank, we achieve the following round complexities for damping factor\nequal to $1 - \\epsilon$:\n  * in $O(\\log \\log n + \\log 1 / \\epsilon)$ rounds for undirected graphs (with\n$\\tilde O(m / \\epsilon^2)$ total space),\n  * in $\\tilde O(\\log^2 \\log n + \\log^2 1/\\epsilon)$ rounds for directed graphs\n(with $\\tilde O((m+n^{1+o(1)}) / poly\\, \\epsilon)$ total space).\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 17:13:26 GMT"}, {"version": "v2", "created": "Sun, 21 Jul 2019 09:30:04 GMT"}, {"version": "v3", "created": "Mon, 28 Oct 2019 09:50:10 GMT"}, {"version": "v4", "created": "Wed, 6 Nov 2019 02:27:31 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["\u0141\u0105cki", "Jakub", ""], ["Mitrovi\u0107", "Slobodan", ""], ["Onak", "Krzysztof", ""], ["Sankowski", "Piotr", ""]]}, {"id": "1907.05420", "submitter": "Juraj Kardos", "authors": "Juraj Kardo\\v{s}, Drosos Kourounis, Olaf Schenk", "title": "Structure Exploiting Interior Point Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interior point methods are among the most popular techniques for large scale\nnonlinear optimization, owing to their intrinsic ability of scaling to\narbitrary large problem sizes. Their efficiency has attracted in recent years a\nlot of attention due to increasing demand for large scale optimization in\nindustry and engineering. A parallel interior point method is discussed that\nexploits the intrinsic structure of large-scale nonlinear optimization problems\nso that the solution process can employ massively parallel high-performance\ncomputing infastructures. Since the overall performance of interior point\nmethods relies heavily on scalable sparse linear algebra solvers, particular\nemphasis is given to the underlying algorithms for the distributed solution of\nthe associated sparse linear systems obtained at each iteration from the\nlinearization of the optimality conditions. The interior point algorithm is\nimplemented in a object-oriented parallel IPM solver and applied for the\nsolution of large scale optimal control problems solved in a daily basis for\nthe secure transmission and distribution of electricity in modern power grids.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 00:19:11 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Kardo\u0161", "Juraj", ""], ["Kourounis", "Drosos", ""], ["Schenk", "Olaf", ""]]}, {"id": "1907.05523", "submitter": "Karl Palmskog", "authors": "Musab A. Alturki, Jing Chen, Victor Luchangco, Brandon Moore, Karl\n  Palmskog, Lucas Pe\\~na, Grigore Ro\\c{s}u", "title": "Towards a Verified Model of the Algorand Consensus Protocol in Coq", "comments": "6 pages; updated to correspond to version in FMBC 2019 proceedings", "journal-ref": "Formal Methods. FM 2019 International Workshops. Lecture Notes in\n  Computer Science, vol 12232, pp. 362-367", "doi": "10.1007/978-3-030-54994-7_27", "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Algorand blockchain is a secure and decentralized public ledger based on\npure proof of stake rather than proof of work. At its core it is a novel\nconsensus protocol with exactly one block certified in each round: that is, the\nprotocol guarantees that the blockchain does not fork. In this paper, we report\non our effort to model and formally verify the Algorand consensus protocol in\nthe Coq proof assistant. Similar to previous consensus protocol verification\nefforts, we model the protocol as a state transition system and reason over\nreachable global states. However, in contrast to previous work, our model\nexplicitly incorporates timing issues (e.g., timeouts and network delays) and\nadversarial actions, reflecting a more realistic environment faced by a public\nblockchain. Thus far, we have proved asynchronous safety of the protocol: two\ndifferent blocks cannot be certified in the same round, even when the adversary\nhas complete control of message delivery in the network. We believe that our\nmodel is sufficiently general and other relevant properties of the protocol\nsuch as liveness can be proved for the same model.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 23:39:38 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2020 14:52:10 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Alturki", "Musab A.", ""], ["Chen", "Jing", ""], ["Luchangco", "Victor", ""], ["Moore", "Brandon", ""], ["Palmskog", "Karl", ""], ["Pe\u00f1a", "Lucas", ""], ["Ro\u015fu", "Grigore", ""]]}, {"id": "1907.05560", "submitter": "Vahid Noormofidi", "authors": "Vahid Noormofidi", "title": "Simulating Nonlinear Neutrino Oscillations on Next-Generation Many-Core\n  Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work an astrophysical simulation code, XFLAT, is developed to study\nneutrino oscillations in supernovae. XFLAT is designed to utilize multiple\nlevels of parallelism through MPI, OpenMP, and SIMD instructions\n(vectorization). It can run on both the CPU and the Xeon Phi co-processor, the\nlatter of which is based on the Intel Many Integrated Core Architecture (MIC).\nThe performance of XFLAT on configurations and scenarios has been analyzed. In\naddition, the impact of I/O and the multi-node configuration on the Xeon\nPhi-equipped heterogeneous supercomputers such as Stampede at the Texas\nAdvanced Computing Center (TACC) was investigated.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 03:21:54 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Noormofidi", "Vahid", ""]]}, {"id": "1907.05636", "submitter": "Mark Burgess", "authors": "Mark Burgess", "title": "From Observability to Significance in Distributed Information Systems", "comments": "Some typos fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.DC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To understand and explain process behaviour we need to be able to see it, and\ndecide its significance, i.e. be able to tell a story about its behaviours.\nThis paper describes a few of the modelling challenges that underlie monitoring\nand observation of processes in IT, by human or by software. The topic of the\nobservability of systems has been elevated recently in connection with computer\nmonitoring and tracing of processes for debugging and forensics. It raises the\nissue of well-known principles of measurement, in bounded contexts, but these\nissues have been left implicit in the Computer Science literature. This paper\naims to remedy this omission, by laying out a simple promise theoretic model,\nsummarizing a long standing trail of work on the observation of distributed\nsystems, based on elementary distinguishability of observations, and classical\ncausality, with history. Three distinct views of a system are sought, across a\nnumber of scales, that described how information is transmitted (and lost) as\nit moves around the system, aggregated into journals and logs.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 09:11:59 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 12:46:04 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Burgess", "Mark", ""]]}, {"id": "1907.05701", "submitter": "Wei Zhang", "authors": "Wei Zhang, Xiaodong Cui, Ulrich Finkler, George Saon, Abdullah Kayi,\n  Alper Buyuktosunoglu, Brian Kingsbury, David Kung, Michael Picheny", "title": "A Highly Efficient Distributed Deep Learning System For Automatic Speech\n  Recognition", "comments": null, "journal-ref": "INTERSPEECH 2019", "doi": null, "report-no": null, "categories": "eess.AS cs.DC cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern Automatic Speech Recognition (ASR) systems rely on distributed deep\nlearning to for quick training completion. To enable efficient distributed\ntraining, it is imperative that the training algorithms can converge with a\nlarge mini-batch size. In this work, we discovered that Asynchronous\nDecentralized Parallel Stochastic Gradient Descent (ADPSGD) can work with much\nlarger batch size than commonly used Synchronous SGD (SSGD) algorithm. On\ncommonly used public SWB-300 and SWB-2000 ASR datasets, ADPSGD can converge\nwith a batch size 3X as large as the one used in SSGD, thus enable training at\na much larger scale. Further, we proposed a Hierarchical-ADPSGD (H-ADPSGD)\nsystem in which learners on the same computing node construct a super learner\nvia a fast allreduce implementation, and super learners deploy ADPSGD algorithm\namong themselves. On a 64 Nvidia V100 GPU cluster connected via a 100Gb/s\nEthernet network, our system is able to train SWB-2000 to reach a 7.6% WER on\nthe Hub5-2000 Switchboard (SWB) test-set and a 13.2% WER on the Call-home (CH)\ntest-set in 5.2 hours. To the best of our knowledge, this is the fastest ASR\ntraining system that attains this level of model accuracy for SWB-2000 task to\nbe ever reported in the literature.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 14:32:59 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Zhang", "Wei", ""], ["Cui", "Xiaodong", ""], ["Finkler", "Ulrich", ""], ["Saon", "George", ""], ["Kayi", "Abdullah", ""], ["Buyuktosunoglu", "Alper", ""], ["Kingsbury", "Brian", ""], ["Kung", "David", ""], ["Picheny", "Michael", ""]]}, {"id": "1907.05767", "submitter": "Amirreza Hashemi", "authors": "Amirreza Hashemi, Mohsen Lahooti, Ebrahim Shirani", "title": "Equal bi-Vectorized (EBV) method to high performance on GPU", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to importance of reducing of time solution in numerical codes, we propose\nan algorithm for parallel LU decomposition solver for dense and sparse matrices\non GPU. This algorithm is based on first bi-vectorizing a triangular matrices\nof decomposed coefficient matrix and then equalizing vectors. So we improve\nperformance of LU decomposition on equal contributed scheme on threads. This\nalgorithm also is convenient for other parallelism method and multi devices.\nSeveral test cases show advantage of this method over other familiar method.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 14:35:32 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Hashemi", "Amirreza", ""], ["Lahooti", "Mohsen", ""], ["Shirani", "Ebrahim", ""]]}, {"id": "1907.05853", "submitter": "Issam Damaj", "authors": "Issam Damaj (American University of Kuwait)", "title": "A Unified Analysis Approach for Hardware and Software Implementations", "comments": "5 pages, 2 figures. arXiv admin note: text overlap with\n  arXiv:1904.01000", "journal-ref": "The 59th IEEE International Midwest Symposium on Circuits and\n  Systems. Abu Dhabi. UAE. 16-19 October, (2016) 577-580", "doi": "10.1109/MWSCAS.2016.7870083", "report-no": null, "categories": "cs.DC cs.CR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart gadgets are being embedded almost in every aspect of our lives. From\nsmart cities to smart watches, modern industries are increasingly supporting\nthe Internet-of-Things (IoT). SysMART aims at making supermarkets smart,\nproductive, and with a touch of modern lifestyle. While similar implementations\nto improve the shopping experience exists, they tend mainly to replace the\nshopping activity at the store with online shopping. Although online shopping\nreduces time and effort, it deprives customers from enjoying the experience.\nSysMART relies on cutting-edge devices and technology to simplify and reduce\nthe time required during grocery shopping inside the supermarket. In addition,\nthe system monitors and maintains perishable products in good condition\nsuitable for human consumption. SysMART is built using state-of-the-art\ntechnologies that support rapid prototyping and precision data acquisition. The\nselected development environment is LabVIEW with its world-class interfacing\nlibraries. The paper comprises a detailed system description, development\nstrategy, interface design, software engineering, and a thorough analysis and\nevaluation.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 19:31:32 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Damaj", "Issam", "", "American University of Kuwait"]]}, {"id": "1907.05917", "submitter": "Steven W. D. Chien", "authors": "Steven W. D. Chien, Ivy B. Peng, Stefano Markidis", "title": "Posit NPB: Assessing the Precision Improvement in HPC Scientific\n  Applications", "comments": "Accepted for publication in PPAM 2019 conference", "journal-ref": null, "doi": "10.1007/978-3-030-43229-4_26", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Floating-point operations can significantly impact the accuracy and\nperformance of scientific applications on large-scale parallel systems.\nRecently, an emerging floating-point format called Posit has attracted\nattention as an alternative to the standard IEEE floating-point formats because\nit could enable higher precision than IEEE formats using the same number of\nbits. In this work, we first explored the feasibility of Posit encoding in\nrepresentative HPC applications by providing a 32-bit Posit NAS Parallel\nBenchmark (NPB) suite. Then, we evaluate the accuracy improvement in different\nHPC kernels compared to the IEEE 754 format. Our results indicate that using\nPosit encoding achieves optimized precision, ranging from 0.6 to 1.4 decimal\ndigit, for all tested kernels and proxy-applications. Also, we quantified the\noverhead of the current software implementation of Posit encoding as 4x-19x\nthat of IEEE 754 hardware implementation. Our study highlights the potential of\nhardware implementations of Posit to benefit a broad range of HPC applications.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 18:39:38 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Chien", "Steven W. D.", ""], ["Peng", "Ivy B.", ""], ["Markidis", "Stefano", ""]]}, {"id": "1907.05984", "submitter": "Burak Bartan", "authors": "Burak Bartan, Mert Pilanci", "title": "Distributed Black-Box Optimization via Error Correcting Codes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel distributed derivative-free optimization framework that\nis resilient to stragglers. The proposed method employs coded search directions\nat which the objective function is evaluated, and a decoding step to find the\nnext iterate. Our framework can be seen as an extension of evolution strategies\nand structured exploration methods where structured search directions were\nutilized. As an application, we consider black-box adversarial attacks on deep\nconvolutional neural networks. Our numerical experiments demonstrate a\nsignificant improvement in the computation times.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 00:36:17 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Bartan", "Burak", ""], ["Pilanci", "Mert", ""]]}, {"id": "1907.06068", "submitter": "David Doty", "authors": "Janna Burman, Ho-Lin Chen, Hsueh-Ping Chen, David Doty, Thomas Nowak,\n  Eric Severson, Chuan Xu", "title": "Time-optimal self-stabilizing leader election in population protocols", "comments": "updated with improved protocols: simultaneously time- and\n  state-optimal O(n) silent protocol, and new bounded-state O(log(n)) time,\n  bounded-state protocol", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the standard population protocol model, where (a priori)\nindistinguishable and anonymous agents interact in pairs according to uniformly\nrandom scheduling. The self-stabilizing leader election problem requires the\nprotocol to converge on a single leader agent from any possible initial\nconfiguration. We initiate the study of time complexity of population protocols\nsolving this problem in its original setting: with probability 1, in a complete\ncommunication graph. The only previously known protocol by Cai, Izumi, and Wada\n[Theor. Comput. Syst. 50] runs in expected parallel time $\\Theta(n^2)$ and has\nthe optimal number of $n$ states in a population of $n$ agents. The existing\nprotocol has the additional property that it becomes silent, i.e., the agents'\nstates eventually stop changing.\n  Observing that any silent protocol solving self-stabilizing leader election\nrequires $\\Omega(n)$ expected parallel time, we introduce a silent protocol\nthat uses optimal $O(n)$ parallel time and states. Without any silence\nconstraints, we show that it is possible to solve self-stabilizing leader\nelection in asymptotically optimal expected parallel time of $O(\\log n)$, but\nusing at least exponential states (a quasi-polynomial number of bits). All of\nour protocols (and also that of Cai et al.) work by solving the more difficult\nranking problem: assigning agents the ranks $1,\\ldots,n$.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 12:35:19 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 08:30:16 GMT"}, {"version": "v3", "created": "Mon, 15 Feb 2021 21:48:58 GMT"}, {"version": "v4", "created": "Thu, 27 May 2021 17:07:40 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Burman", "Janna", ""], ["Chen", "Ho-Lin", ""], ["Chen", "Hsueh-Ping", ""], ["Doty", "David", ""], ["Nowak", "Thomas", ""], ["Severson", "Eric", ""], ["Xu", "Chuan", ""]]}, {"id": "1907.06094", "submitter": "William Pourmajidi", "authors": "William Pourmajidi, Andriy Miranskyy, John Steinbacher, Tony Erwin,\n  David Godwin", "title": "Dogfooding: use IBM Cloud services to monitor IBM Cloud infrastructure", "comments": null, "journal-ref": "Proceedings of the 29th Annual International Conference on\n  Computer Science and Software Engineering (CASCON'19), 2019, pp. 344-353", "doi": null, "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stability and performance of Cloud platforms are essential as they\ndirectly impact customers' satisfaction. Cloud service providers use Cloud\nmonitoring tools to ensure that rendered services match the quality of service\nrequirements indicated in established contracts such as service-level\nagreements. Given the enormous number of resources that need to be monitored,\nhighly scalable and capable monitoring tools are designed and implemented by\nCloud service providers such as Amazon, Google, IBM, and Microsoft. Cloud\nmonitoring tools monitor millions of virtual and physical resources and\ncontinuously generate logs for each one of them. Considering that logs magnify\nany technical issue, they can be used for disaster detection, prevention, and\nrecovery. However, logs are useless if they are not assessed and analyzed\npromptly. Thus, we argue that the scale of Cloud-generated logs makes it\nimpossible for DevOps teams to analyze them effectively. This implies that one\nneeds to automate the process of monitoring and analysis (e.g., using machine\nlearning and artificial intelligence). If the automation will witness an\nanomaly in the logs --- it will alert DevOps staff. The automatic anomaly\ndetectors require a reliable and scalable platform for gathering, filtering,\nand transforming the logs, executing the detector models, and sending out the\nalerts to the DevOps staff. In this work, we report on implementing a prototype\nof such a platform based on the 7-layered architecture pattern, which leverages\nmicro-service principles to distribute tasks among highly scalable,\nresources-efficient modules. The modules interact with each other via an\ninstance of the Publish-Subscribe architectural pattern. The platform is\ndeployed on the IBM Cloud service infrastructure and is used to detect\nanomalies in logs emitted by the IBM Cloud services, hence the dogfooding.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 15:04:03 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Pourmajidi", "William", ""], ["Miranskyy", "Andriy", ""], ["Steinbacher", "John", ""], ["Erwin", "Tony", ""], ["Godwin", "David", ""]]}, {"id": "1907.06110", "submitter": "Amin Mosayyebzadeh", "authors": "Amin Mosayyebzadeh, Apoorve Mohan, Sahil Tikale, Mania Abdi, Nabil\n  Schear, Charles Munson, Trammell Hudson, Larry Rudolph, Gene Cooperman, Peter\n  Desnoyers, Orran Krieger", "title": "Supporting Security Sensitive Tenants in a Bare-Metal Cloud", "comments": "16 Pages, 2019 USENIX Annual Technical Conference (ATC'19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bolted is a new architecture for bare-metal clouds that enables tenants to\ncontrol tradeoffs between security, price, and performance. Security-sensitive\ntenants can minimize their trust in the public cloud provider and achieve\nsimilar levels of security and control that they can obtain in their own\nprivate data centers. At the same time, Bolted neither imposes overhead on\ntenants that are security insensitive nor compromises the flexibility or\noperational efficiency of the provider. Our prototype exploits a novel\nprovisioning system and specialized firmware to enable elasticity similar to\nvirtualized clouds. Experimentally we quantify the cost of different levels of\nsecurity for a variety of workloads and demonstrate the value of giving control\nto the tenant.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 17:41:34 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Mosayyebzadeh", "Amin", ""], ["Mohan", "Apoorve", ""], ["Tikale", "Sahil", ""], ["Abdi", "Mania", ""], ["Schear", "Nabil", ""], ["Munson", "Charles", ""], ["Hudson", "Trammell", ""], ["Rudolph", "Larry", ""], ["Cooperman", "Gene", ""], ["Desnoyers", "Peter", ""], ["Krieger", "Orran", ""]]}, {"id": "1907.06154", "submitter": "Mohamed Wahib", "authors": "Peng Chen, Mohamed Wahib, Shinichiro Takizawa, Ryousei Takano, Satoshi\n  Matsuoka", "title": "A Versatile Software Systolic Execution Model for GPU Memory-Bound\n  Kernels", "comments": "ACM/IEEE Proceedings of the International Conference for High\n  Performance Computing, Networking, Storage and Analysis (SC'19)", "journal-ref": null, "doi": "10.1145/3295500.3356162", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a versatile high-performance execution model, inspired by\nsystolic arrays, for memory-bound regular kernels running on CUDA-enabled GPUs.\nWe formulate a systolic model that shifts partial sums by CUDA warp primitives\nfor the computation. We also employ register files as a cache resource in order\nto operate the entire model efficiently. We demonstrate the effectiveness and\nversatility of the proposed model for a wide variety of stencil kernels that\nappear commonly in HPC, and also convolution kernels (increasingly important in\ndeep learning workloads). Our algorithm outperforms the top reported\nstate-of-the-art stencil implementations, including implementations with\nsophisticated temporal and spatial blocking techniques, on the two latest\nNvidia architectures: Tesla V100 and P100. For 2D convolution of general filter\nsizes and shapes, our algorithm is on average 2.5x faster than Nvidia's NPP on\nV100 and P100 GPUs.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2019 01:48:53 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 05:37:51 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Chen", "Peng", ""], ["Wahib", "Mohamed", ""], ["Takizawa", "Shinichiro", ""], ["Takano", "Ryousei", ""], ["Matsuoka", "Satoshi", ""]]}, {"id": "1907.06183", "submitter": "Alexey Shigarov", "authors": "Igor Bychkov, Julia Dubenskaya, Elena Korosteleva, Alexandr Kryukov,\n  Andrey Mikhailov, Minh-Duc Nguyen, Alexey Shigarov", "title": "Metadata Extraction from Raw Astroparticle Data of TAIGA Experiment", "comments": "9 pages, 3 figures, 3rd International Workshop on Data Life Cycle in\n  Physics", "journal-ref": "Bychkov I. et al. Metadata Extraction from Raw Astroparticle Data\n  of TAIGA Experiment. Proc. 3rd Int. Workshop on Data Life Cycle in Physics.\n  2019. pp. 26-34", "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Today, the operating TAIGA (Tunka Advanced Instrument for cosmic rays and\nGamma Astronomy) experiment continuously produces and accumulates a large\nvolume of raw astroparticle data. To be available for the scientific community\nthese data should be well-described and formally characterized. The use of\nmetadata makes it possible to search for and to aggregate digital objects (e.g.\nevents and runs) by time and equipment through a unified interface to access\nthem. The important part of the metadata is hidden and scattered in\nfolder/files names and package headers. Such metadata should be extracted from\nbinary files, transformed to a unified form of digital objects, and loaded into\nthe catalog. To address this challenge we developed a concept of the metadata\nextractor that can be extended by facility-specific extraction modules. It is\ndesigned to automatically collect descriptive metadata from raw data files of\nall TAIGA formats.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2019 08:06:00 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Bychkov", "Igor", ""], ["Dubenskaya", "Julia", ""], ["Korosteleva", "Elena", ""], ["Kryukov", "Alexandr", ""], ["Mikhailov", "Andrey", ""], ["Nguyen", "Minh-Duc", ""], ["Shigarov", "Alexey", ""]]}, {"id": "1907.06250", "submitter": "Boris Novikov", "authors": "Artem Trofimov, Igor E. Kuralenok, Nikita Marshalkin, Boris Novikov", "title": "Delivery, consistency, and determinism: rethinking guarantees in\n  distributed stream processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consistency requirements for state-of-the-art stream processing systems are\ndefined in terms of delivery guarantees. Exactly-once is the strongest one and\nthe most desirable for end-user. However, there are several issues regarding\nthis concept. Commonly used techniques that enforce exactly-once produce\nsignificant performance overhead. Besides, the notion of exactly-once is not\nformally defined and does not capture all properties that provide stream\nprocessing systems supporting this guarantee. In this paper, we introduce a\nformal framework that allows us to define streaming guarantees more regularly.\nWe demonstrate that the properties of delivery, consistency, and determinism\nare tightly connected within distributed stream processing. We also show that\nhaving lightweight determinism, it is possible to provide exactly-once with\nalmost no performance overhead. Experiments show that the proposed approach can\nsignificantly outperform alternative industrial solutions.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2019 17:24:39 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Trofimov", "Artem", ""], ["Kuralenok", "Igor E.", ""], ["Marshalkin", "Nikita", ""], ["Novikov", "Boris", ""]]}, {"id": "1907.06466", "submitter": "Rafael Pereira Pires", "authors": "Stefan Contiu, S\\'ebastien Vaucher, Rafael Pires, Marcelo Pasin,\n  Pascal Felber and Laurent R\\'eveill\\`ere", "title": "Anonymous and confidential file sharing over untrusted clouds", "comments": null, "journal-ref": "2019 IEEE 38th Symposium on Reliable Distributed Systems (SRDS),\n  Lyon, France, 2019", "doi": "10.1109/SRDS47363.2019.00013", "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using public cloud services for storing and sharing confidential data\nrequires end users to cryptographically protect both the data and the access to\nthe data. In some cases, the identity of end users needs to remain confidential\nagainst the cloud provider and fellow users accessing the data. As such, the\nunderlying cryptographic access control mechanism needs to ensure the anonymity\nof both data producers and consumers. We introduce A-SKY, a cryptographic\naccess control extension capable of providing confidentiality and anonymity\nguarantees, all while efficiently scaling to large organizations. A-SKY\nleverages trusted execution environments (TEEs) to address the impracticality\nof anonymous broadcast encryption (ANOBE) schemes, achieving faster execution\ntimes and shorter ciphertexts. The innovative design of A-SKY limits the usage\nof the TEE to the narrow set of data producing operations, and thus optimizes\nthe dominant data consumption actions by not requiring a TEE. Furthermore, we\npropose a scalable implementation for A-SKY leveraging micro-services that\npreserves strong security guarantees while being able to efficiently manage\nrealistic large user bases. Results highlight that the A-SKY cryptographic\nscheme is 3 orders of magnitude better than state of the art ANOBE, and an\nend-to-end system encapsulating A-SKY can elastically scale to support groups\nof 10 000 users while maintaining processing costs below 1 second.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 12:34:52 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2019 12:26:51 GMT"}, {"version": "v3", "created": "Mon, 6 Apr 2020 15:04:57 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Contiu", "Stefan", ""], ["Vaucher", "S\u00e9bastien", ""], ["Pires", "Rafael", ""], ["Pasin", "Marcelo", ""], ["Felber", "Pascal", ""], ["R\u00e9veill\u00e8re", "Laurent", ""]]}, {"id": "1907.06487", "submitter": "Georg Hager", "authors": "Christie L. Alappat, Georg Hager, Olaf Schenk, Jonas Thies, Achim\n  Basermann, Alan R. Bishop, Holger Fehske, Gerhard Wellein", "title": "A Recursive Algebraic Coloring Technique for Hardware-Efficient\n  Symmetric Sparse Matrix-Vector Multiplication", "comments": "40 pages, 23 figures", "journal-ref": null, "doi": "10.1145/3399732", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The symmetric sparse matrix-vector multiplication (SymmSpMV) is an important\nbuilding block for many numerical linear algebra kernel operations or graph\ntraversal applications. Parallelizing SymmSpMV on today's multicore platforms\nwith up to 100 cores is difficult due to the need to manage conflicting updates\non the result vector. Coloring approaches can be used to solve this problem\nwithout data duplication, but existing coloring algorithms do not take load\nbalancing and deep memory hierarchies into account, hampering scalability and\nfull-chip performance. In this work, we propose the recursive algebraic\ncoloring engine (RACE), a novel coloring algorithm and open-source library\nimplementation, which eliminates the shortcomings of previous coloring methods\nin terms of hardware efficiency and parallelization overhead. We describe the\nlevel construction, distance-k coloring, and load balancing steps in RACE, use\nit to parallelize SymmSpMV, and compare its performance on 31 sparse matrices\nwith other state-of-the-art coloring techniques and Intel MKL on two modern\nmulticore processors. RACE outperforms all other approaches substantially and\nbehaves in accordance with the Roofline model. Outliers are discussed and\nanalyzed in detail. While we focus on SymmSpMV in this paper, our algorithm and\nsoftware is applicable to any sparse matrix operation with data dependencies\nthat can be resolved by distance-k coloring.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 13:15:35 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Alappat", "Christie L.", ""], ["Hager", "Georg", ""], ["Schenk", "Olaf", ""], ["Thies", "Jonas", ""], ["Basermann", "Achim", ""], ["Bishop", "Alan R.", ""], ["Fehske", "Holger", ""], ["Wellein", "Gerhard", ""]]}, {"id": "1907.06723", "submitter": "Gustavo Machado", "authors": "Gustavo V. Machado, \\'Italo Cunha, Adriano C. M. Pereira, Leonardo B.\n  Oliveira", "title": "DOD-ETL: Distributed On-Demand ETL for Near Real-Time Business\n  Intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The competitive dynamics of the globalized market demand information on the\ninternal and external reality of corporations. Information is a precious asset\nand is responsible for establishing key advantages to enable companies to\nmaintain their leadership. However, reliable, rich information is no longer the\nonly goal. The time frame to extract information from data determines its\nusefulness. This work proposes DOD-ETL, a tool that addresses, in an innovative\nmanner, the main bottleneck in Business Intelligence solutions, the Extract\nTransform Load process (ETL), providing it in near real-time. DODETL achieves\nthis by combining an on-demand data stream pipeline with a distributed,\nparallel and technology-independent architecture with in-memory caching and\nefficient data partitioning. We compared DOD-ETL with other Stream Processing\nframeworks used to perform near real-time ETL and found DOD-ETL executes\nworkloads up to 10 times faster. We have deployed it in a large steelworks as a\nreplacement for its previous ETL solution, enabling near real-time reports\npreviously unavailable.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 20:06:03 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Machado", "Gustavo V.", ""], ["Cunha", "\u00cdtalo", ""], ["Pereira", "Adriano C. M.", ""], ["Oliveira", "Leonardo B.", ""]]}, {"id": "1907.06768", "submitter": "Mohammad Hasanzadeh Mofrad", "authors": "Mohammad Hasanzadeh Mofrad, Rami Melhem, Mohammad Hammoud", "title": "Partitioning Graphs for the Cloud using Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose Revolver, a parallel graph partitioning algorithm\ncapable of partitioning large-scale graphs on a single shared-memory machine.\nRevolver employs an asynchronous processing framework, which leverages\nreinforcement learning and label propagation to adaptively partition a graph.\nIn addition, it adopts a vertex-centric view of the graph where each vertex is\nassigned an autonomous agent responsible for selecting a suitable partition for\nit, distributing thereby the computation across all vertices. The intuition\nbehind using a vertex-centric view is that it naturally fits the graph\npartitioning problem, which entails that a graph can be partitioned using local\ninformation provided by each vertex's neighborhood. We fully implemented and\ncomprehensively tested Revolver using nine real-world graphs. Our results show\nthat Revolver is scalable and can outperform three popular and state-of-the-art\ngraph partitioners via producing comparable localized partitions, yet without\nsacrificing the load balance across partitions.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 21:50:56 GMT"}, {"version": "v2", "created": "Wed, 17 Jul 2019 16:16:28 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Mofrad", "Mohammad Hasanzadeh", ""], ["Melhem", "Rami", ""], ["Hammoud", "Mohammad", ""]]}, {"id": "1907.06855", "submitter": "Saber Salehkaleybar", "authors": "Hamidreza Bandealinaeini, Saber Salehkaleybar", "title": "Broadcast Distributed Voting Algorithm in Population Protocols", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of multi-choice majority voting in a network of $n$\nagents where each agent initially selects a choice from a set of $K$ possible\nchoices. The agents try to infer the choice in majority merely by performing\nlocal interactions. Population protocols provide a framework for designing\npairwise interactions between agents in order to perform tasks in a coordinated\nmanner. In this paper, we propose ``Broadcasting Population Protocol\" model as\na counterpart model of conventional population protocols for the networks that\neach agent can send a message to all its neighbors simultaneously. We design\ntwo distributed algorithms for solving the multi-choice majority voting problem\nin the model of broadcasting population protocols. We prove the correctness of\nthese algorithms and analyze their performance in terms of time and message\ncomplexities. Experiments show that the proposed algorithm improves both time\nand message complexities significantly with respect to previous algorithms\nproposed in conventional population protocols and they can be utilized in\nnetworks where messages can be transmitted to a subset of agents simultaneously\nsuch as wireless networks.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 06:07:53 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Bandealinaeini", "Hamidreza", ""], ["Salehkaleybar", "Saber", ""]]}, {"id": "1907.06863", "submitter": "Alexander Kryukov", "authors": "Alexander Kryukov, Minh-Duc Nguyen, Igor Bychkov, Andrey Mikhailov,\n  Alexey Shigarov, and Julia Dubenskaya", "title": "Distributed data storage for modern astroparticle physics experiments", "comments": null, "journal-ref": "Proc. of the 3-d Int. Workshop DLC-2019, CEUR-WS Proceedings,\n  Vol-2406, pp.78-83", "doi": null, "report-no": null, "categories": "cs.DC astro-ph.HE astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The German-Russian Astroparticle Data Life Cycle Initiative is an\ninternational project launched in 2018. The Initiative aims to develop\ntechnologies that provide a unified approach to data management, as well as to\ndemonstrate their applicability on the example of two large astrophysical\nexperiments - KASCADE and TAIGA. One of the key points of the project is the\ndevelopment of a distributed storage, which, on the one hand, will allow data\nof several experiments to be combined into a single repository with unified\ninterface, and on the other hand, will provide data to all participants of\nexperimental groups for multi-messenger analysis. Our approach to storage\ndesign is based on the single write-multiple read (SWMR) model for accessing\nraw or centrally processed data for further analysis. The main feature of the\ndistributed storage is the ability to extract data either as a collection of\nfiles or as aggregated events from different sources. In the last case the\nstorage provides users with a special service that aggregates data from\ndifferent storages into a single sample. Thanks to this feature,\nmulti-messenger methods used for more sophisticated data exploration can be\napplied. Users can use both Web-interface and Application Programming Interface\n(API) for accessing the storage. In this paper we describe the architecture of\na distributed data storage for astroparticle physics and discuss the current\nstatus of our work.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 06:54:58 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Kryukov", "Alexander", ""], ["Nguyen", "Minh-Duc", ""], ["Bychkov", "Igor", ""], ["Mikhailov", "Andrey", ""], ["Shigarov", "Alexey", ""], ["Dubenskaya", "Julia", ""]]}, {"id": "1907.07010", "submitter": "Bryan Ford", "authors": "Bryan Ford", "title": "Threshold Logical Clocks for Asynchronous Distributed Coordination and\n  Consensus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consensus protocols for asynchronous networks are usually complex and\ninefficient, leading practical systems to rely on synchronous protocols. This\npaper attempts to simplify asynchronous consensus by building atop a novel\nthreshold logical clock abstraction, which enables upper layers to operate as\nif on a synchronous network. This approach yields an asynchronous consensus\nprotocol for fail-stop nodes that may be simpler and more robust than Paxos and\nits leader-based variants, requiring no common coins and achieving consensus in\na constant expected number of rounds. The same approach can be strengthened\nagainst Byzantine failures by building on well-established techniques such as\ntamper-evident logging and gossip, accountable state machines, threshold\nsignatures and witness cosigning, and verifiable secret sharing. This\ncombination of existing abstractions and threshold logical clocks yields a\nmodular, cleanly-layered approach to building practical and efficient Byzantine\nconsensus, distributed key generation, time, timestamping, and randomness\nbeacons, and other critical services.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 13:57:46 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Ford", "Bryan", ""]]}, {"id": "1907.07078", "submitter": "Amitabh Trehan", "authors": "Walter Hussak and Amitabh Trehan", "title": "On The Termination of a Flooding Process", "comments": "A summary to appear as a Brief Announcement at ACM PODC'19. Full\n  version under submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flooding is among the simplest and most fundamental of all distributed\nnetwork algorithms. A node begins the process by sending a message to all its\nneighbours and the neighbours, in the next round forward the message to all the\nneighbours they did not receive the message from and so on. We assume that the\nnodes do not keep a record of the flooding event. We call this amnesiac\nflooding (AF). Since the node forgets, if the message is received again in\nsubsequent rounds, it will be forwarded again raising the possibility that the\nmessage may be circulated infinitely even on a finite graph. As far as we know,\nthe question of termination for such a flooding process has not been settled -\nrather, non-termination is implicitly assumed.\n  In this paper, we show that synchronous AF always terminates on any arbitrary\nfinite graph and derive exact termination times which differ sharply in\nbipartite and non-bipartite graphs. Let $G$ be a finite connected graph. We\nshow that synchronous AF from a single source node terminates on $G$ in $e$\nrounds, where $e$ is the eccentricity of the source node, if and only if $G$ is\nbipartite. For non-bipartite $G$, synchronous AF from a single source\nterminates in $j$ rounds where $e < j \\leq e+d+1$ and $d$ is the diameter of\n$G$. This limits termination time to at most $d$ and at most $2d + 1$ for\nbipartite and non-bipartite graphs respectively. If communication/broadcast to\nall nodes is the motivation, our results show that AF is asymptotically time\noptimal and obviates the need for construction and maintenance of spanning\nstructures like spanning trees. The clear separation in the termination times\nof bipartite and non-bipartite graphs also suggests mechanisms for distributed\ndiscovery of the topology/distances in arbitrary graphs.\n  For comparison, we show that, in asynchronous networks, an adaptive adversary\ncan force AF to be non-terminating.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 15:45:12 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Hussak", "Walter", ""], ["Trehan", "Amitabh", ""]]}, {"id": "1907.07110", "submitter": "Ali Tehrani", "authors": "Ali Tehrani, Mohammed Khaleel, Reza Akbari, Ali Jannesari", "title": "DeepRace: Finding Data Race Bugs via Deep Learning", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the proliferation of multi-core hardware, parallel programs have become\nubiquitous. These programs have their own type of bugs known as concurrency\nbugs and among them, data race bugs have been mostly in the focus of\nresearchers over the past decades. In fact, detecting data races is a very\nchallenging and important task. There have been several research paths in this\narea with many sophisticated tools designed and utilized that focus on\ndetecting data race at the file level. In this paper, we propose DeepRace, a\nnovel approach toward detecting data races in the source code. We build a deep\nneural network model to find data races instead of creating a data race\ndetector manually. Our model uses a one-layer convolutional neural network\n(CNN) with different window size to find data races method. Then we adopt the\nclass activation map function with global average pooling to extract the\nweights of the last convolutional layer and backpropagate it with the input\nsource code to extract the line of codes with a data race. Thus, the DeepRace\nmodel can detect the data race bugs on a file and line of code level. In\naddition, we noticed that DeepRace successfully detects several buggy lines of\ncode at different locations of the file. We tested the model with OpenMP and\nPOSIX source code datasets which consist of more than 5000 and 8000 source code\nfiles respectively. We were able to successfully classify buggy source code\nfiles and achieve accuracies ranging from 81% and 86%. We also measured the\nperformance of detecting and visualizing the data race at the line of code\nlevels and our model achieved promising results. We only had a small number of\nfalse positives and false, ranging from 1 to 10. Furthermore, we used the\nintersection of union to measure the accuracy of the buggy lines of code, our\nmodel achieved promising results of 66 percent.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 07:45:01 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Tehrani", "Ali", ""], ["Khaleel", "Mohammed", ""], ["Akbari", "Reza", ""], ["Jannesari", "Ali", ""]]}, {"id": "1907.07149", "submitter": "Emilio Cruciani", "authors": "Luca Becchetti, Emilio Cruciani, Francesco Pasquale, Sara Rizzo", "title": "Step-by-Step Community Detection in Volume-Regular Graphs", "comments": "Preliminary version appeared in Proceedings of ISAAC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral techniques have proved amongst the most effective approaches to\ngraph clustering. However, in general they require explicit computation of the\nmain eigenvectors of a suitable matrix (usually the Laplacian matrix of the\ngraph). Recent work (e.g., Becchetti et al., SODA 2017) suggests that observing\nthe temporal evolution of the power method applied to an initial random vector\nmay, at least in some cases, provide enough information on the space spanned by\nthe first two eigenvectors, so as to allow recovery of a hidden partition\nwithout explicit eigenvector computations. While the results of Becchetti et\nal. apply to perfectly balanced partitions and/or graphs that exhibit very\nstrong forms of regularity, we extend their approach to graphs containing a\nhidden $k$ partition and characterized by a milder form of volume-regularity.\nWe show that the class of $k$-volume-regular graphs is the largest class of\nundirected (possibly weighted) graphs whose transition matrix admits $k$\n\"stepwise\" eigenvectors (i.e., vectors that are constant over each set of the\nhidden partition). To obtain this result, we highlight a connection between\nvolume regularity and lumpability of Markov chains. Moreover, we prove that if\nthe stepwise eigenvectors are those associated to the first $k$ eigenvalues and\nthe gap between the $k$-th and the ($k$+1)-th eigenvalues is sufficiently\nlarge, the averaging dynamics of Becchetti et al. recovers the underlying\ncommunity structure of the graph in logarithmic time, with high probability.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 17:15:42 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 10:47:30 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Becchetti", "Luca", ""], ["Cruciani", "Emilio", ""], ["Pasquale", "Francesco", ""], ["Rizzo", "Sara", ""]]}, {"id": "1907.07248", "submitter": "Mirco Richter", "authors": "Mirco Richter", "title": "Crisis: Probabilistically Self Organizing Total Order in Unstructured\n  P2P Networks", "comments": "First draf. 31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A framework for asynchronous, signature free, fully local and\nprobabilistically converging total order algorithms is developed, that may\nsurvive in high entropy, unstructured Peer-to-Peer networks with near optimal\ncommunication efficiency. Regarding the natural boundaries of the CAP-theorem,\nCrisis chooses different compromises for consistency and availability,\ndepending on the severity of the attack.\n  The family is parameterized by a few constants and external functions called\nvoting-weight, incentivation \\& punishement, difficulty oracle and\nquorum-selector. These functions are necessary to fine tune the dynamics and\nvery different long term behavior might appear, depending on any actual choice.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 18:49:18 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Richter", "Mirco", ""]]}, {"id": "1907.07279", "submitter": "Davide Brunelli PhD", "authors": "Daniele Facinelli, Matteo Larcher, Davide Brunelli, Daniele Fontanelli", "title": "Cooperative UAVs Gas Monitoring using Distributed Consensus", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": "10.1109/COMPSAC.2019.00072", "report-no": null, "categories": "cs.RO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of target detection and localisation in a\nlimited area using multiple coordinated agents. The swarm of Unmanned Aerial\nVehicles (UAVs) determines the position of the dispersion of stack effluents to\na gas plume in a certain production area as fast as possible, that makes the\nproblem challenging to model and solve, because of the time variability of the\ntarget. Three different exploration algorithms are designed and compared.\nBesides the exploration strategies, the paper reports a solution for quick\nconvergence towards the actual stack position once detected by one member of\nthe team. Both the navigation and localisation algorithms are fully distributed\nand based on the consensus theory. Simulations on realistic case studies are\nreported.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 22:23:10 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Facinelli", "Daniele", ""], ["Larcher", "Matteo", ""], ["Brunelli", "Davide", ""], ["Fontanelli", "Daniele", ""]]}, {"id": "1907.07346", "submitter": "Hanlin Tang", "authors": "Hanlin Tang, Xiangru Lian, Shuang Qiu, Lei Yuan, Ce Zhang, Tong Zhang,\n  Ji Liu", "title": "$\\texttt{DeepSqueeze}$: Decentralization Meets Error-Compensated\n  Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communication is a key bottleneck in distributed training. Recently, an\n\\emph{error-compensated} compression technology was particularly designed for\nthe \\emph{centralized} learning and receives huge successes, by showing\nsignificant advantages over state-of-the-art compression based methods in\nsaving the communication cost. Since the \\emph{decentralized} training has been\nwitnessed to be superior to the traditional \\emph{centralized} training in the\ncommunication restricted scenario, therefore a natural question to ask is \"how\nto apply the error-compensated technology to the decentralized learning to\nfurther reduce the communication cost.\" However, a trivial extension of\ncompression based centralized training algorithms does not exist for the\ndecentralized scenario. key difference between centralized and decentralized\ntraining makes this extension extremely non-trivial. In this paper, we propose\nan elegant algorithmic design to employ error-compensated stochastic gradient\ndescent for the decentralized scenario, named $\\texttt{DeepSqueeze}$. Both the\ntheoretical analysis and the empirical study are provided to show the proposed\n$\\texttt{DeepSqueeze}$ algorithm outperforms the existing compression based\ndecentralized learning algorithms. To the best of our knowledge, this is the\nfirst time to apply the error-compensated compression to the decentralized\nlearning.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 05:59:31 GMT"}, {"version": "v2", "created": "Sat, 3 Aug 2019 15:20:38 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Tang", "Hanlin", ""], ["Lian", "Xiangru", ""], ["Qiu", "Shuang", ""], ["Yuan", "Lei", ""], ["Zhang", "Ce", ""], ["Zhang", "Tong", ""], ["Liu", "Ji", ""]]}, {"id": "1907.07433", "submitter": "Ilya Afanasyev", "authors": "Ilya Afanasyev, Alexander Kolotov, Ruslan Rezin, Konstantin Danilov,\n  Manuel Mazzara, Subham Chakraborty, Alexey Kashevnik, Andrey Chechulin,\n  Aleksandr Kapitonov, Vladimir Jotsov, Andon Topalov, Nikola Shakev, Sevil\n  Ahmed", "title": "Towards Blockchain-based Multi-Agent Robotic Systems: Analysis,\n  Classification and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decentralization, immutability and transparency make of Blockchain one of the\nmost innovative technology of recent years. This paper presents an overview of\nsolutions based on Blockchain technology for multi-agent robotic systems, and\nprovide an analysis and classification of this emerging field. The reasons for\nimplementing Blockchain in a multi-robot network may be to increase the\ninteraction efficiency between agents by providing more trusted information\nexchange, reaching a consensus in trustless conditions, assessing robot\nproductivity or detecting performance problems, identifying intruders,\nallocating plans and tasks, deploying distributed solutions and joint missions.\nBlockchain-based applications are discussed to demonstrate how distributed\nledger can be used to extend the number of research platforms and libraries for\nmulti-agent robotic systems.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 10:38:56 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Afanasyev", "Ilya", ""], ["Kolotov", "Alexander", ""], ["Rezin", "Ruslan", ""], ["Danilov", "Konstantin", ""], ["Mazzara", "Manuel", ""], ["Chakraborty", "Subham", ""], ["Kashevnik", "Alexey", ""], ["Chechulin", "Andrey", ""], ["Kapitonov", "Aleksandr", ""], ["Jotsov", "Vladimir", ""], ["Topalov", "Andon", ""], ["Shakev", "Nikola", ""], ["Ahmed", "Sevil", ""]]}, {"id": "1907.07627", "submitter": "Amin Mosayyebzadeh", "authors": "Amin Mosayyebzadeh, Gerardo Ravago, Apoorve Mohan, Ali Raza, Sahil\n  Tikale, Nabil Schear, Trammell Hudson, Jason Hennessey, Naved Ansari, Kyle\n  Hogan, Charles Munson, Larry Rudolph, Gene Cooperman, Peter Desnoyers, Orran\n  Krieger", "title": "A Secure Cloud with Minimal Provider Trust", "comments": "7 Pages, 10th USENIX Workshop on Hot Topics in Cloud Computing\n  (HotCloud '18). arXiv admin note: text overlap with arXiv:1907.06110", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bolted is a new architecture for a bare metal cloud with the goal of\nproviding security-sensitive customers of a cloud the same level of security\nand control that they can obtain in their own private data centers. It allows\ntenants to elastically allocate secure resources within a cloud while being\nprotected from other previous, current, and future tenants of the cloud. The\nprovisioning of a new server to a tenant isolates a bare metal server, only\nallowing it to communicate with other tenant's servers once its critical\nfirmware and software have been attested to the tenant. Tenants, rather than\nthe provider, control the tradeoffs between security, price, and performance. A\nprototype demonstrates scalable end-to-end security with small overhead\ncompared to a less secure alternative.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 17:51:05 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Mosayyebzadeh", "Amin", ""], ["Ravago", "Gerardo", ""], ["Mohan", "Apoorve", ""], ["Raza", "Ali", ""], ["Tikale", "Sahil", ""], ["Schear", "Nabil", ""], ["Hudson", "Trammell", ""], ["Hennessey", "Jason", ""], ["Ansari", "Naved", ""], ["Hogan", "Kyle", ""], ["Munson", "Charles", ""], ["Rudolph", "Larry", ""], ["Cooperman", "Gene", ""], ["Desnoyers", "Peter", ""], ["Krieger", "Orran", ""]]}, {"id": "1907.07735", "submitter": "Yaochen Hu", "authors": "Yaochen Hu, Peng Liu, Linglong Kong, Di Niu", "title": "Learning Privately over Distributed Features: An ADMM Sharing Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed machine learning has been widely studied in order to handle\nexploding amount of data. In this paper, we study an important yet less visited\ndistributed learning problem where features are inherently distributed or\nvertically partitioned among multiple parties, and sharing of raw data or model\nparameters among parties is prohibited due to privacy concerns. We propose an\nADMM sharing framework to approach risk minimization over distributed features,\nwhere each party only needs to share a single value for each sample in the\ntraining process, thus minimizing the data leakage risk. We establish\nconvergence and iteration complexity results for the proposed parallel ADMM\nalgorithm under non-convex loss. We further introduce a novel differentially\nprivate ADMM sharing algorithm and bound the privacy guarantee with carefully\ndesigned noise perturbation. The experiments based on a prototype system shows\nthat the proposed ADMM algorithms converge efficiently in a robust fashion,\ndemonstrating advantage over gradient based methods especially for data set\nwith high dimensional feature spaces.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 20:02:05 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Hu", "Yaochen", ""], ["Liu", "Peng", ""], ["Kong", "Linglong", ""], ["Niu", "Di", ""]]}, {"id": "1907.07760", "submitter": "Georgios Mylonas", "authors": "Georgios Mylonas, Dimitrios Amaxilatis, Stelios Tsampas, Lidia Pocero,\n  Joakim Gunneriusson", "title": "A Methodology for Saving Energy in Educational Buildings Using an IoT\n  Infrastructure", "comments": "To appear in the 10th International Conference on Information,\n  Intelligence, Systems and Applications (IISA 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A considerable part of recent research in smart cities and IoT has focused on\nachieving energy savings in buildings and supporting aspects related to\nsustainability. In this context, the educational community is one of the most\nimportant ones to consider, since school buildings constitute a large part of\nnon-residential buildings, while also educating students on sustainability\nmatters is an investment for the future. In this work, we discuss a methodology\nfor achieving energy savings in schools based on the utilization of data\nproduced by an IoT infrastructure installed inside school buildings and related\neducational scenarios. We present the steps comprising this methodology in\ndetail, along with a set of tangible results achieved within the GAIA project.\nWe also showcase how an IoT infrastructure can support activities in an\neducational setting and produce concrete outcomes, with typical levels of 20%\nenergy savings.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 08:12:34 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Mylonas", "Georgios", ""], ["Amaxilatis", "Dimitrios", ""], ["Tsampas", "Stelios", ""], ["Pocero", "Lidia", ""], ["Gunneriusson", "Joakim", ""]]}, {"id": "1907.07778", "submitter": "Eisa Alanazi", "authors": "Abdulaziz Alashaikh, Eisa Alanazi and Ala Al-Fuqaha", "title": "A Survey on the Use of Preferences for Virtual Machine Placement in\n  Cloud Data Centers", "comments": "40 pages, 5 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the rapid development of virtualization techniques, cloud data centers\nallow for cost effective, flexible, and customizable deployments of\napplications on virtualized infrastructure. Virtual machine (VM) placement aims\nto assign each virtual machine to a server in the cloud environment. VM\nPlacement is of paramount importance to the design of cloud data centers.\nTypically, VM placement involves complex relations and multiple design factors\nas well as local policies that govern the assignment decisions. It also\ninvolves different constituents including cloud administrators and customers\nthat might have disparate preferences while opting for a placement solution.\nThus, it is often valuable to not only return an optimized solution to the VM\nplacement problem but also a solution that reflects the given preferences of\nthe constituents. In this paper, we provide a detailed review on the role of\npreferences in the recent literature on VM placement. We further discuss key\nchallenges and identify possible research opportunities to better incorporate\npreferences within the context of VM placement.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 21:22:27 GMT"}, {"version": "v2", "created": "Sat, 20 Jul 2019 11:23:33 GMT"}, {"version": "v3", "created": "Thu, 16 Jan 2020 23:11:29 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Alashaikh", "Abdulaziz", ""], ["Alanazi", "Eisa", ""], ["Al-Fuqaha", "Ala", ""]]}, {"id": "1907.07929", "submitter": "Tiziano De Matteis", "authors": "Tiziano De Matteis, Johannes de Fine Licht and Torsten Hoefler", "title": "FBLAS: Streaming Linear Algebra on FPGA", "comments": null, "journal-ref": null, "doi": "10.1109/SC41405.2020.00063", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial computing architectures pose an attractive alternative to mitigate\ncontrol and data movement overheads typical of load-store architectures. In\npractice, these devices are rarely considered in the HPC community due to the\nsteep learning curve, low productivity and lack of available libraries for\nfundamental operations. High-level synthesis (HLS) tools are facilitating\nhardware programming, but optimizing for these architectures requires factoring\nin new transformations and resources/performance trade-offs. We present FBLAS,\nan open-source HLS implementation of BLAS for FPGAs, that enables reusability,\nportability and easy integration with existing software and hardware codes.\nFBLAS' implementation allows scaling hardware modules to exploit on-chip\nresources, and module interfaces are designed to natively support streaming\non-chip communications, allowing them to be composed to reduce off-chip\ncommunication. With FBLAS, we set a precedent for FPGA library design, and\ncontribute to the toolbox of customizable hardware components necessary for HPC\ncodes to start productively targeting reconfigurable platforms.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 08:59:58 GMT"}, {"version": "v2", "created": "Fri, 19 Jul 2019 09:46:31 GMT"}, {"version": "v3", "created": "Fri, 30 Aug 2019 08:14:55 GMT"}, {"version": "v4", "created": "Mon, 31 Aug 2020 13:49:12 GMT"}, {"version": "v5", "created": "Tue, 1 Sep 2020 09:00:07 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["De Matteis", "Tiziano", ""], ["Licht", "Johannes de Fine", ""], ["Hoefler", "Torsten", ""]]}, {"id": "1907.07944", "submitter": "Colette Johnen Dr.", "authors": "Ajoy K. Datta, St\\'ephane Devismes, Colette Johnen, Lawrence L.\n  Larmore", "title": "Analysis of a Memory-Efficient Self-Stabilizing BFS Spanning Tree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present results on the last topic we collaborate with our late friend,\nProfessor Ajoy Kumar Datta (1958-2019).\n  In this work, we shed new light on a self-stabilizing wave algorithm proposed\nby Colette Johnen in 1997. This algorithm constructs a BFS spanning tree in any\nconnected rooted network. Nowadays, it is still the best existing\nself-stabilizing BFS spanning tree construction in terms of memory requirement,\n{\\em i.e.}, it only requires $\\Theta(1)$ bits per edge. However, it has been\nproven assuming a weakly fair daemon. Moreover, its stabilization time was\nunknown.\n  Here, we study the slightly modified version of this algorithm, still keeping\nthe same memory requirement. We prove the self-stabilization of this variant\nunder the distributed unfair daemon and show a stabilization time in $O(D.n^2)$\nrounds, where $D$ is the network diameter and $n$ the number of processes.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 09:24:26 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Datta", "Ajoy K.", ""], ["Devismes", "St\u00e9phane", ""], ["Johnen", "Colette", ""], ["Larmore", "Lawrence L.", ""]]}, {"id": "1907.07975", "submitter": "Ana Mikatadze", "authors": "Beka Dalakishvili, Ana Mikatadze", "title": "Powershare Mechanics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN cs.DC q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes the governance framework of a gamified social network for\ncharity crowdfunding fueled by public computing. It introduces optimal scarce\nresource allocation model, technological configuration of the FIRE consensus\nprotocol, and multi-layer incentivization structure that maximizes value\ncreation within the network.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 10:39:02 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Dalakishvili", "Beka", ""], ["Mikatadze", "Ana", ""]]}, {"id": "1907.08057", "submitter": "Jie Xu", "authors": "Jie Xu", "title": "READ: a three-communicating-stage distributed super points detections\n  algorithm", "comments": "18 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A super point is a host that interacts with a far larger number of\ncounterparts in the network over a period of time. Super point detection plays\nan important role in network research and application. With the increase of\nnetwork scale, distributed super point detection has become a hot research\ntopic. Compared with single-node super point detection algorithm, the\ndifficulty of super point detection in multi-node distributed environment is\nhow to reduce communication overhead. Therefore, this paper proposes a\nthree-stage communication distributed super point detection algorithm: Rough\nEstimator based Asynchronous Distributed super point detection algorithm\n(READ). READ uses a lightweight estimator, the Rough Estimator (RE), which is\nfast in computation and takes less memory to generate candidate super point. At\nthe same time, the Linear Estimator (LE) is used to accurately estimate the\ncardinality of each candidate super point, so as to detect the super point\ncorrectly. In READ, each node scans IP address pairs asynchronously. When\nreaching the time window boundary, READ starts three-stage communication to\ndetect the super point. In this paper, we proof that the accuracy of READ in\ndistributed environment is no less than that in the single node environment.\nFour groups of 10 Gb/s and 40 Gb/s real-world high-speed network traffic are\nused to test READ. The experimental results show that READ not only has higher\naccuracy in distributed environment, but also has less than 5% of communication\nburden compared with existing algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 14:03:39 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Xu", "Jie", ""]]}, {"id": "1907.08160", "submitter": "Will Rosenbaum", "authors": "Will Rosenbaum, Jukka Suomela", "title": "Seeing Far vs. Seeing Wide: Volume Complexity of Local Graph Problems", "comments": "various improvement; expanded discussion of MPC model and simulation;\n  more open questions and directions for further research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a graph problem that is locally checkable but not locally solvable:\ngiven a solution we can check that it is feasible by verifying all\nconstant-radius neighborhoods, but to find a solution each node needs to\nexplore the input graph at least up to distance $\\Omega(\\log n)$ in order to\nproduce its output. We consider the complexity of such problems from the\nperspective of volume: how large a subgraph does a node need to see in order to\nproduce its output. We study locally checkable graph problems on bounded-degree\ngraphs. We give a number of constructions that exhibit tradeoffs between\ndeterministic distance, randomized distance, deterministic volume, and\nrandomized volume:\n  - If the deterministic distance is linear, it is also known that randomized\ndistance is near-linear. In contrast, we show that there are problems with\nlinear deterministic volume but only logarithmic randomized volume.\n  - We prove a volume hierarchy theorem for randomized complexity: among\nproblems with linear deterministic volume complexity, there are infinitely many\ndistinct randomized volume complexity classes between $\\Omega(\\log n)$ and\n$O(n)$. This hierarchy persists even when restricting to problems whose\nrandomized and deterministic distance complexities are $\\Theta(\\log n)$.\n  - Similar hierarchies exist for polynomial distance complexities: for any $k,\n\\ell \\in N$ with $k \\leq \\ell$, there are problems whose randomized and\ndeterministic distance complexities are $\\Theta(n^{1/\\ell})$, randomized volume\ncomplexities are $\\Theta(n^{1/k})$, and whose deterministic volume complexities\nare $\\Theta(n)$.\n  Additionally, we consider connections between our volume model and massively\nparallel computation (MPC). We give a general simulation argument that any\nvolume-efficient algorithm can be transformed into a space-efficient MPC\nalgorithm.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 17:02:39 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2020 19:17:00 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Rosenbaum", "Will", ""], ["Suomela", "Jukka", ""]]}, {"id": "1907.08278", "submitter": "Bin Cheng", "authors": "Bin Cheng and Jonathan F\\\"urst and Gurkan Solmaz and Takuya Sanada", "title": "Fog Function: Serverless Fog Computing for Data Intensive IoT Services", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fog computing can support IoT services with fast response time and low\nbandwidth usage by moving computation from the cloud to edge devices. However,\nexisting fog computing frameworks have limited flexibility to support dynamic\nservice composition with a data-oriented approach. Function-as-a-Service (FaaS)\nis a promising programming model for fog computing to enhance flexibility, but\nthe current event- or topic-based design of function triggering and the\nseparation of data management and function execution result in inefficiency for\ndata-intensive IoT services. To achieve both flexibility and efficiency, we\npropose a data-centric programming model called Fog Function and also introduce\nits underlying orchestration mechanism that leverages three types of contexts:\ndata context, system context, and usage context. Moreover, we showcase a\nconcrete use case for smart parking where Fog Function allows service\ndevelopers to easily model their service logic with reduced learning efforts\ncompared to a static service topology. Our performance evaluation results show\nthat the Fog Function can be scaled to hundreds of fog nodes. Fog Function can\nimprove system efficiency by saving 95% of the internal data traffic over cloud\nfunction and it can reduce service latency by 30% over edge function.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 20:54:49 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Cheng", "Bin", ""], ["F\u00fcrst", "Jonathan", ""], ["Solmaz", "Gurkan", ""], ["Sanada", "Takuya", ""]]}, {"id": "1907.08302", "submitter": "Guenter Hesse", "authors": "Guenter Hesse, Christoph Matthies, Kelvin Glass, Johannes Huegle,\n  Matthias Uflacker", "title": "Quantitative Impact Evaluation of an Abstraction Layer for Data Stream\n  Processing Systems", "comments": null, "journal-ref": "2019 International Conference on Distributed Computing Systems\n  (ICDCS), pp. 1381-1392", "doi": "10.1109/ICDCS.2019.00137", "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the demand to process ever-growing data volumes, a variety of new data\nstream processing frameworks have been developed. Moving an implementation from\none such system to another, e.g., for performance reasons, requires adapting\nexisting applications to new interfaces. Apache Beam addresses these high\nsubstitution costs by providing an abstraction layer that enables executing\nprograms on any of the supported streaming frameworks. In this paper, we\npresent a novel benchmark architecture for comparing the performance impact of\nusing Apache Beam on three streaming frameworks: Apache Spark Streaming, Apache\nFlink, and Apache Apex. We find significant performance penalties when using\nApache Beam for application development in the surveyed systems. Overall, usage\nof Apache Beam for the examined streaming applications caused a high variance\nof query execution times with a slowdown of up to a factor of 58 compared to\nqueries developed without the abstraction layer. All developed benchmark\nartifacts are publicly available to ensure reproducible results.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 21:51:29 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Hesse", "Guenter", ""], ["Matthies", "Christoph", ""], ["Glass", "Kelvin", ""], ["Huegle", "Johannes", ""], ["Uflacker", "Matthias", ""]]}, {"id": "1907.08367", "submitter": "Haris Javaid", "authors": "Haris Javaid, Chengchen Hu and Gordon Brebner", "title": "Optimizing Validation Phase of Hyperledger Fabric", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain technologies are on the rise, and Hyperledger Fabric is one of the\nmost popular permissioned blockchain platforms. In this paper, we re-architect\nthe validation phase of Fabric based on our analysis from fine-grained\nbreakdown of the validation phase's latency. Our optimized validation phase\nuses a chaincode cache during validation of transactions, initiates state\ndatabase reads in parallel with validation of transactions, and writes to the\nledger and databases in parallel. Our experiments reveal performance\nimprovements of 2x for CouchDB and 1.3x for LevelDB. Notably, our optimizations\ncan be adopted in a future release of Hyperledger Fabric.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 04:04:20 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Javaid", "Haris", ""], ["Hu", "Chengchen", ""], ["Brebner", "Gordon", ""]]}, {"id": "1907.08526", "submitter": "Saeed Soori", "authors": "Saeed Soori, Bugra Can, Mert Gurbuzbalaba, Maryam Mehri Dehnavi", "title": "ASYNC: A Cloud Engine with Asynchrony and History for Distributed\n  Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ASYNC is a framework that supports the implementation of asynchrony and\nhistory for optimization methods on distributed computing platforms. The\npopularity of asynchronous optimization methods has increased in distributed\nmachine learning. However, their applicability and practical experimentation on\ndistributed systems are limited because current bulk-processing cloud engines\ndo not provide a robust support for asynchrony and history. With introducing\nthree main modules and bookkeeping system-specific and application parameters,\nASYNC provides practitioners with a framework to implement asynchronous machine\nlearning methods. To demonstrate ease-of-implementation in ASYNC, the\nsynchronous and asynchronous variants of two well-known optimization methods,\nstochastic gradient descent and SAGA, are demonstrated in ASYNC.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 14:26:56 GMT"}, {"version": "v2", "created": "Sat, 27 Jul 2019 18:14:10 GMT"}, {"version": "v3", "created": "Tue, 15 Oct 2019 15:36:53 GMT"}, {"version": "v4", "created": "Fri, 21 Feb 2020 03:19:14 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Soori", "Saeed", ""], ["Can", "Bugra", ""], ["Gurbuzbalaba", "Mert", ""], ["Dehnavi", "Maryam Mehri", ""]]}, {"id": "1907.08607", "submitter": "Jessica Shi", "authors": "Jessica Shi and Julian Shun", "title": "Parallel Algorithms for Butterfly Computations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Butterflies are the smallest non-trivial subgraph in bipartite graphs, and\ntherefore having efficient computations for analyzing them is crucial to\nimproving the quality of certain applications on bipartite graphs. In this\npaper, we design a framework called ParButterfly that contains new parallel\nalgorithms for the following problems on processing butterflies: global\ncounting, per-vertex counting, per-edge counting, tip decomposition (vertex\npeeling), and wing decomposition (edge peeling). The main component of these\nalgorithms is aggregating wedges incident on subsets of vertices, and our\nframework supports different methods for wedge aggregation, including sorting,\nhashing, histogramming, and batching. In addition, ParButterfly supports\ndifferent ways of ranking the vertices to speed up counting, including side\nordering, approximate and exact degree ordering, and approximate and exact\ncomplement coreness ordering. For counting, ParButterfly also supports both\nexact computation as well as approximate computation via graph sparsification.\nWe prove strong theoretical guarantees on the work and span of the algorithms\nin ParButterfly.\n  We perform a comprehensive evaluation of all of the algorithms in\nParButterfly on a collection of real-world bipartite graphs using a 48-core\nmachine. Our counting algorithms obtain significant parallel speedup,\noutperforming the fastest sequential algorithms by up to 13.6x with a\nself-relative speedup of up to 38.5x. Compared to general subgraph counting\nsolutions, we are orders of magnitude faster. Our peeling algorithms achieve\nself-relative speedups of up to 10.7x and outperform the fastest sequential\nbaseline by up to several orders of magnitude.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 17:58:55 GMT"}, {"version": "v2", "created": "Sun, 22 Dec 2019 16:19:41 GMT"}, {"version": "v3", "created": "Wed, 17 Jun 2020 01:59:52 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Shi", "Jessica", ""], ["Shun", "Julian", ""]]}, {"id": "1907.08818", "submitter": "Shahrzad Kianidehkordi", "authors": "Shahrzad Kiani, Nuwan Ferdinand and Stark C. Draper", "title": "Hierarchical Coded Matrix Multiplication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Slow working nodes, known as stragglers, can greatly reduce the speed of\ndistributed computation. Coded matrix multiplication is a recently introduced\ntechnique that enables straggler-resistant distributed multiplication of large\nmatrices. A key property is that the finishing time depends only on the work\ncompleted by a set of the fastest workers, while the work done by the slowest\nworkers is ignored completely. This paper is motivated by the observation that\nin real-world commercial cloud computing systems such as Amazon's Elastic\nCompute Cloud (EC2) the distinction between fast and slow nodes is often a soft\none. Thus, if we could also exploit the work completed by stragglers we may\nrealize substantial performance gains. To realize such gains, in this paper we\nuse the idea of hierarchical coding (Ferdinand and Draper, IEEE Int. Symp. Inf.\nTheory, 2018). We decompose the overall matrix multiplication task into a\nhierarchy of heterogeneously sized subtasks. The duty to complete each subtask\nis shared amongst all workers and each subtask is (generally) of a different\ncomplexity. The motivation for the hierarchical decomposition is the\nrecognition that more workers will finish the first subtask than the second (or\nthird, forth, etc.). Connecting to error correction coding, earlier subtasks\ncan therefore be designed to be of a higher rate than later subtasks. Through\nthis hierarchical design our scheme exploits the work completed by stragglers,\nrather than ignoring it, even if that amount is much less than that completed\nby the fastest workers. We numerically show that our method realizes a 60%\nimprovement in the expected finishing time for a widely studied statistical\nmodel of the speed of computation and, on Amazon EC2, the gain is 35%.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2019 14:45:49 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Kiani", "Shahrzad", ""], ["Ferdinand", "Nuwan", ""], ["Draper", "Stark C.", ""]]}, {"id": "1907.08969", "submitter": "Sandeep Kumar", "authors": "Sandeep Kumar, Ketan Rajawat, and Daniel P. Palomar", "title": "Distributed Inexact Successive Convex Approximation ADMM: Analysis-Part\n  I", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this two-part work, we propose an algorithmic framework for solving\nnon-convex problems whose objective function is the sum of a number of smooth\ncomponent functions plus a convex (possibly non-smooth) or/and smooth (possibly\nnon-convex) regularization function. The proposed algorithm incorporates ideas\nfrom several existing approaches such as alternate direction method of\nmultipliers (ADMM), successive convex approximation (SCA), distributed and\nasynchronous algorithms, and inexact gradient methods. Different from a number\nof existing approaches, however, the proposed framework is flexible enough to\nincorporate a class of non-convex objective functions, allow distributed\noperation with and without a fusion center, and include variance reduced\nmethods as special cases. Remarkably, the proposed algorithms are robust to\nuncertainties arising from random, deterministic, and adversarial sources. The\npart I of the paper develops two variants of the algorithm under very mild\nassumptions and establishes first-order convergence rate guarantees. The proof\ndeveloped here allows for generic errors and delays, paving the way for\ndifferent variance-reduced, asynchronous, and stochastic implementations,\noutlined and evaluated in part II.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2019 12:46:05 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Kumar", "Sandeep", ""], ["Rajawat", "Ketan", ""], ["Palomar", "Daniel P.", ""]]}, {"id": "1907.08985", "submitter": "Weiwen Jiang", "authors": "Weiwen Jiang, Edwin H.-M. Sha, Xinyi Zhang, Lei Yang, Qingfeng Zhuge,\n  Yiyu Shi, Jingtong Hu", "title": "Achieving Super-Linear Speedup across Multi-FPGA for Real-Time DNN\n  Inference", "comments": "Accepted by ESWEEK CODES+ISSS 2019, nominated as best paper. Appear\n  in ACM TECS", "journal-ref": "ACM Transactions on Embedded Computing Systems (TECS)October 2019\n  Article No.: 67", "doi": "10.1145/3358192", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time Deep Neural Network (DNN) inference with low-latency requirement\nhas become increasingly important for numerous applications in both cloud\ncomputing (e.g., Apple's Siri) and edge computing (e.g., Google/Waymo's\ndriverless car). FPGA-based DNN accelerators have demonstrated both superior\nflexibility and performance; in addition, for real-time inference with low\nbatch size, FPGA is expected to achieve further performance improvement.\nHowever, the performance gain from the single-FPGA design is obstructed by the\nlimited on-chip resource. In this paper, we employ multiple FPGAs to\ncooperatively run DNNs with the objective of achieving super-linear speed-up\nagainst single-FPGA design. In implementing such systems, we found two barriers\nthat hinder us from achieving the design goal: (1) the lack of a clear\npartition scheme for each DNN layer to fully exploit parallelism, and (2) the\ninsufficient bandwidth between the off-chip memory and the accelerator due to\nthe growing size of DNNs. To tackle these issues, we propose a general\nframework, \"Super-LIP\", which can support different kinds of DNNs. In this\npaper, we take Convolutional Neural Network (CNN) as a vehicle to illustrate\nSuper-LIP. We first formulate an accurate system-level model to support the\nexploration of best partition schemes. Then, we develop a novel design\nmethodology to effectively alleviate the heavy loads on memory bandwidth by\nmoving traffic from memory bus to inter-FPGA links. We implement Super-LIP\nbased on ZCU102 FPGA boards. Results demonstrate that Super-LIP with 2 FPGAs\ncan achieve 3.48x speedup, compared to the state-of-the-art single-FPGA design.\nWhat is more, as the number of FPGAs scales up, the system latency can be\nfurther reduced while maintaining high energy efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2019 15:16:12 GMT"}, {"version": "v2", "created": "Sat, 8 Feb 2020 22:59:03 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Jiang", "Weiwen", ""], ["Sha", "Edwin H. -M.", ""], ["Zhang", "Xinyi", ""], ["Yang", "Lei", ""], ["Zhuge", "Qingfeng", ""], ["Shi", "Yiyu", ""], ["Hu", "Jingtong", ""]]}, {"id": "1907.09112", "submitter": "EPTCS", "authors": "Roman Kuznets (TU Wien), Laurent Prosperi (ENS Paris-Saclay), Ulrich\n  Schmid (TU Wien), Krisztina Fruzsa (TU Wien)", "title": "Causality and Epistemic Reasoning in Byzantine Multi-Agent Systems", "comments": "In Proceedings TARK 2019, arXiv:1907.08335", "journal-ref": "EPTCS 297, 2019, pp. 293-312", "doi": "10.4204/EPTCS.297.19", "report-no": null, "categories": "cs.MA cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causality is an important concept both for proving impossibility results and\nfor synthesizing efficient protocols in distributed computing. For asynchronous\nagents communicating over unreliable channels, causality is well studied and\nunderstood. This understanding, however, relies heavily on the assumption that\nagents themselves are correct and reliable. We provide the first epistemic\nanalysis of causality in the presence of byzantine agents, i.e., agents that\ncan deviate from their protocol and, thus, cannot be relied upon. Using our new\nframework for epistemic reasoning in fault-tolerant multi-agent systems, we\ndetermine the byzantine analog of the causal cone and describe a communication\nstructure, which we call a multipede, necessary for verifying preconditions for\nactions in this setting.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 03:18:05 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Kuznets", "Roman", "", "TU Wien"], ["Prosperi", "Laurent", "", "ENS Paris-Saclay"], ["Schmid", "Ulrich", "", "TU Wien"], ["Fruzsa", "Krisztina", "", "TU Wien"]]}, {"id": "1907.09272", "submitter": "Max Isacson", "authors": "Max Isacson, Mattias Ellert, Richard Brenner", "title": "Extending the ARC Information Providers to report information on GPU\n  resources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC hep-ex", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  General-purpose Computing on Graphics Processing Units (GPGPU) has been\nintroduced to many areas of scientific research such as bioinformatics,\ncryptography, computer vision, and deep learning. However, computing models in\nthe High-energy Physics (HEP) community are still mainly centred around\ntraditional CPU resources. Tasks such as track fitting, particle\nreconstruction, and Monte Carlo simulation could benefit greatly from a\nhigh-throughput GPGPU computing model, streamlining bottlenecks in analysis\nturnover. This technical note describes the basis of an implementation of an\nintegrated GPU discovery mechanism in GRID middleware to facilitate GPGPU.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 12:32:14 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Isacson", "Max", ""], ["Ellert", "Mattias", ""], ["Brenner", "Richard", ""]]}, {"id": "1907.09289", "submitter": "Christopher Ehmke", "authors": "Christopher Ehmke, Florian Blum, Volker Gruhn", "title": "Properties of Decentralized Consensus Technology -- Why not every\n  Blockchain is a Blockchain", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.35506.45765", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Research in the field of blockchain technology and applications is increasing\nat a fast pace. Although the Bitcoin whitepaper by Nakamoto is already ten\nyears old, the field can still be seen as immature and at an early stage.\nCurrent research in this area is lacking a commonly shared knowledge and\nconsensus about terms used to describe the technology and its properties. At\nthe same time this research is challenging fundamental aspects of the Bitcoin\ncore concept. It has to be questioned whether all of these new approaches still\nadequately could be described as blockchain technology. We propose to use the\nterm Decentralized Consensus Technology as a general category instead.\nDecentralized Consensus Technology consists of decentralized ledger and\nnon-ledger technologies. Blockchain technology in turn is only one of multiple\nimplementations of the Decentralized Ledger Technology. Furthermore, we\nidentified three main characteristics of Decentralized Consensus Technology:\ndecentralization, trustlessness and ability to eventually reach consensus.\nDepending on the use case of the specific implementation the following\nadditional properties have to be considered: privacy, participation incentive,\nirreversibility and immutability, operation purpose, confirmation time,\ntransaction costs, ability to externalize transactions and computations and\nscalability possibilities.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 12:58:24 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Ehmke", "Christopher", ""], ["Blum", "Florian", ""], ["Gruhn", "Volker", ""]]}, {"id": "1907.09356", "submitter": "Anastasiia Koloskova", "authors": "Anastasia Koloskova, Tao Lin, Sebastian U. Stich, Martin Jaggi", "title": "Decentralized Deep Learning with Arbitrary Communication Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decentralized training of deep learning models is a key element for enabling\ndata privacy and on-device learning over networks, as well as for efficient\nscaling to large compute clusters. As current approaches suffer from limited\nbandwidth of the network, we propose the use of communication compression in\nthe decentralized training context. We show that Choco-SGD $-$ recently\nintroduced and analyzed for strongly-convex objectives only $-$ converges under\narbitrary high compression ratio on general non-convex functions at the rate\n$O\\bigl(1/\\sqrt{nT}\\bigr)$ where $T$ denotes the number of iterations and $n$\nthe number of workers. The algorithm achieves linear speedup in the number of\nworkers and supports higher compression than previous state-of-the art methods.\nWe demonstrate the practical performance of the algorithm in two key scenarios:\nthe training of deep learning models (i) over distributed user devices,\nconnected by a social network and (ii) in a datacenter (outperforming\nall-reduce time-wise).\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 14:53:02 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 11:06:08 GMT"}, {"version": "v3", "created": "Wed, 11 Nov 2020 17:24:35 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Koloskova", "Anastasia", ""], ["Lin", "Tao", ""], ["Stich", "Sebastian U.", ""], ["Jaggi", "Martin", ""]]}, {"id": "1907.09575", "submitter": "Ancy Tom", "authors": "Ancy Sarah Tom and George Karypis", "title": "A 2D Parallel Triangle Counting Algorithm for Distributed-Memory\n  Architectures", "comments": "10 pages, 3 figures, 48th International Conference on Parallel\n  Processing", "journal-ref": null, "doi": "10.1145/3337821.3337853", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Triangle counting is a fundamental graph analytic operation that is used\nextensively in network science and graph mining. As the size of the graphs that\nneeds to be analyzed continues to grow, there is a requirement in developing\nscalable algorithms for distributed-memory parallel systems. To this end, we\npresent a distributed-memory triangle counting algorithm, which uses a 2D\ncyclic decomposition to balance the computations and reduce the communication\noverheads. The algorithm structures its communication and computational steps\nsuch that it reduces its memory overhead and includes key optimizations that\nleverage the sparsity of the graph and the way the computations are structured.\nExperiments on synthetic and real-world graphs show that our algorithm obtains\nan average relative speedup that range between 3.24 and 7.22 out of 10.56\nacross the datasets using 169 MPI ranks over the performance achieved by 16 MPI\nranks. Moreover, we obtain an average speedup of 10.2 times on comparison with\npreviously developed distributed-memory parallel algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 21:08:39 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Tom", "Ancy Sarah", ""], ["Karypis", "George", ""]]}, {"id": "1907.09769", "submitter": "Mohammad Mohammadi Amiri Dr.", "authors": "Mohammad Mohammadi Amiri and Deniz Gunduz", "title": "Federated Learning over Wireless Fading Channels", "comments": "to appear, IEEE Transactions on Wireless Communications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study federated machine learning at the wireless network edge, where\nlimited power wireless devices, each with its own dataset, build a joint model\nwith the help of a remote parameter server (PS). We consider a\nbandwidth-limited fading multiple access channel (MAC) from the wireless\ndevices to the PS, and propose various techniques to implement distributed\nstochastic gradient descent (DSGD). We first propose a digital DSGD (D-DSGD)\nscheme, in which one device is selected opportunistically for transmission at\neach iteration based on the channel conditions; the scheduled device quantizes\nits gradient estimate to a finite number of bits imposed by the channel\ncondition, and transmits these bits to the PS in a reliable manner. Next,\nmotivated by the additive nature of the wireless MAC, we propose a novel analog\ncommunication scheme, referred to as the compressed analog DSGD (CA-DSGD),\nwhere the devices first sparsify their gradient estimates while accumulating\nerror, and project the resultant sparse vector into a low-dimensional vector\nfor bandwidth reduction. Numerical results show that D-DSGD outperforms other\ndigital approaches in the literature; however, in general the proposed CA-DSGD\nalgorithm converges faster than the D-DSGD scheme and other schemes in the\nliterature, and reaches a higher level of accuracy. We have observed that the\ngap between the analog and digital schemes increases when the datasets of\ndevices are not independent and identically distributed (i.i.d.). Furthermore,\nthe performance of the CA-DSGD scheme is shown to be robust against imperfect\nchannel state information (CSI) at the devices. Overall these results show\nclear advantages for the proposed analog over-the-air DSGD scheme, which\nsuggests that learning and communication algorithms should be designed jointly\nto achieve the best end-to-end performance in machine learning applications at\nthe wireless edge.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 09:00:42 GMT"}, {"version": "v2", "created": "Tue, 11 Feb 2020 04:18:25 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Amiri", "Mohammad Mohammadi", ""], ["Gunduz", "Deniz", ""]]}, {"id": "1907.09871", "submitter": "Xavier Defago", "authors": "Xavier D\\'efago, Adam Heriban, S\\'ebastien Tixeuil, Koichi Wada", "title": "Using Model Checking to Formally Verify Rendezvous Algorithms for Robots\n  with Lights in Euclidean Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper details the first successful attempt at using model-checking\ntechniques to verify the correctness of distributed algorithms for robots\nevolving in a \\emph{continuous} environment. The study focuses on the problem\nof rendezvous of two robots with lights.\n  There exist many different rendezvous algorithms that aim at finding the\nminimal number of colors needed to solve rendezvous in various synchrony models\n(e.g., FSYNC, SSYNC, ASYNC). While these rendezvous algorithms are typically\nvery simple, their analysis and proof of correctness tend to be extremely\ncomplex, tedious, and error-prone as impossibility results are based on subtle\ninteractions between robots activation schedules.\n  The paper presents a generic verification model written for the SPIN\nmodel-checker. In particular, we explain the subtle design decisions that allow\nto keep the search space finite and tractable, as well as prove several\nimportant theorems that support them. As a sanity check, we use the model to\nverify several known rendezvous algorithms in six different models of\nsynchrony. In each case, we find that the results obtained from the\nmodel-checker are consistent with the results known in the literature. The\nmodel-checker outputs a counter-example execution in every case that is known\nto fail.\n  In the course of developing and proving the validity of the model, we\nidentified several fundamental theorems, including the ability for a well\nchosen algorithm and ASYNC scheduler to produce an emerging property of memory\nin a system of oblivious mobile robots, and why it is not a problem for\nluminous rendezvous algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 13:31:39 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["D\u00e9fago", "Xavier", ""], ["Heriban", "Adam", ""], ["Tixeuil", "S\u00e9bastien", ""], ["Wada", "Koichi", ""]]}, {"id": "1907.10134", "submitter": "Shang Wang", "authors": "Shang Wang and Yifan Bai and Gennady Pekhimenko", "title": "BPPSA: Scaling Back-propagation by Parallel Scan Algorithm", "comments": "In Proceedings of MLSys 2020:\n  https://mlsys.org/Conferences/2020/Schedule?showEvent=1407", "journal-ref": "Proceedings of Machine Learning and Systems 2020 (2020) 451-469", "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an era when the performance of a single compute device plateaus, software\nmust be designed to scale on massively parallel systems for better runtime\nperformance. However, in the context of training deep learning models, the\npopular back-propagation (BP) algorithm imposes a strong sequential dependency\nin the process of gradient computation. Under model parallelism, BP takes\n$\\Theta (n)$ steps to complete which hinders its scalability on parallel\nsystems ($n$ represents the number of compute devices into which a model is\npartitioned).\n  In this work, in order to improve the scalability of BP, we reformulate BP\ninto a scan operation which is a primitive that performs an in-order\naggregation on a sequence of values and returns the partial result at each\nstep. We can then scale such reformulation of BP on parallel systems by our\nmodified version of the Blelloch scan algorithm which theoretically takes\n$\\Theta (\\log n)$ steps. We evaluate our approach on a vanilla Recurrent Neural\nNetwork (RNN) training with synthetic datasets and a RNN with Gated Recurrent\nUnits (GRU) training with the IRMAS dataset, and demonstrate up to $2.75\\times$\nspeedup on the overall training time and $108\\times$ speedup on the backward\npass. We also demonstrate that the retraining of pruned networks can be a\npractical use case of our method.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 21:14:56 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 21:46:59 GMT"}, {"version": "v3", "created": "Sat, 7 Mar 2020 05:45:50 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Wang", "Shang", ""], ["Bai", "Yifan", ""], ["Pekhimenko", "Gennady", ""]]}, {"id": "1907.10203", "submitter": "Saurabh Jha", "authors": "Saurabh Jha, Shengkun Cui, Tianyin Xu, Jeremy Enos, Mike Showerman,\n  Mark Dalton, Zbigniew T. Kalbarczyk, William T. Kramer, Ravishankar K. Iyer", "title": "Live Forensics for Distributed Storage Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Kaleidoscope an innovative system that supports live forensics for\napplication performance problems caused by either individual component failures\nor resource contention issues in large-scale distributed storage systems. The\ndesign of Kaleidoscope is driven by our study of I/O failures observed in a\npeta-scale storage system anonymized as PetaStore. Kaleidoscope is built on\nthree key features: 1) using temporal and spatial differential observability\nfor end-to-end performance monitoring of I/O requests, 2) modeling the health\nof storage components as a stochastic process using domain-guided functions\nthat accounts for path redundancy and uncertainty in measurements, and, 3)\nobserving differences in reliability and performance metrics between similar\ntypes of healthy and unhealthy components to attribute the most likely root\ncauses. We deployed Kaleidoscope on PetaStore and our evaluation shows that\nKaleidoscope can run live forensics at 5-minute intervals and pinpoint the root\ncauses of 95.8% of real-world performance issues, with negligible monitoring\noverhead.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 01:52:38 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Jha", "Saurabh", ""], ["Cui", "Shengkun", ""], ["Xu", "Tianyin", ""], ["Enos", "Jeremy", ""], ["Showerman", "Mike", ""], ["Dalton", "Mark", ""], ["Kalbarczyk", "Zbigniew T.", ""], ["Kramer", "William T.", ""], ["Iyer", "Ravishankar K.", ""]]}, {"id": "1907.10278", "submitter": "Ariyam Das", "authors": "Ariyam Das and Carlo Zaniolo", "title": "A Case for Stale Synchronous Distributed Model for Declarative Recursive\n  Computation", "comments": "Paper presented at the 35th International Conference on Logic\n  Programming (ICLP 2019), Las Cruces, New Mexico, USA, 20-25 September 2019,\n  16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DB cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large class of traditional graph and data mining algorithms can be\nconcisely expressed in Datalog, and other Logic-based languages, once\naggregates are allowed in recursion. In fact, for most BigData algorithms, the\ndifficult semantic issues raised by the use of non-monotonic aggregates in\nrecursion are solved by Pre-Mappability (PreM), a property that assures that\nfor a program with aggregates in recursion there is an equivalent\naggregate-stratified program. In this paper we show that, by bringing together\nthe formal abstract semantics of stratified programs with the efficient\noperational one of unstratified programs, PreM can also facilitate and improve\ntheir parallel execution. We prove that PreM-optimized lock-free and\ndecomposable parallel semi-naive evaluations produce the same results as the\nsingle executor programs. Therefore, PreM can be assimilated into the\ndata-parallel computation plans of different distributed systems, irrespective\nof whether these follow bulk synchronous parallel (BSP) or asynchronous\ncomputing models. In addition, we show that non-linear recursive queries can be\nevaluated using a hybrid stale synchronous parallel (SSP) model on distributed\nenvironments. After providing a formal correctness proof for the recursive\nquery evaluation with PreM under this relaxed synchronization model, we present\nexperimental evidence of its benefits. This paper is under consideration for\nacceptance in Theory and Practice of Logic Programming (TPLP).\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 07:35:18 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Das", "Ariyam", ""], ["Zaniolo", "Carlo", ""]]}, {"id": "1907.10308", "submitter": "John Augustine", "authors": "John Augustine, Valerie King, Anisur R. Molla, Gopal Pandurangan, and\n  Jared Saia", "title": "Scalable and Secure Computation Among Strangers: Resource-Competitive\n  Byzantine Protocols", "comments": "24 pages, one figure. The author list has been corrected in the\n  metadata. There are no other changes from version 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated, in part, by the rise of permissionless systems such as Bitcoin\nwhere arbitrary nodes (whose identities are not known apriori) can join and\nleave at will, we extend established research in scalable Byzantine agreement\nto a more practical model where each node (initially) does not know the\nidentity of other nodes. A node can send to new destinations only by sending to\nrandom (or arbitrary) nodes, or responding (if it chooses) to messages received\nfrom those destinations. We assume a synchronous and fully-connected network,\nwith a full-information, but static Byzantine adversary. A general drawback of\nexisting Byzantine protocols is that the communication cost incurred by the\nhonest nodes may not be proportional to those incurred by the Byzantine nodes;\nin fact, they can be significantly higher. Our goal is to design Byzantine\nprotocols for fundamental problems which are {\\em resource competitive}, i.e.,\nthe number of bits sent by honest nodes is not much more than those sent by\nByzantine nodes.\n  We describe a randomized scalable algorithm to solve Byzantine agreement,\nleader election, and committee election in this model. Our algorithm sends an\nexpected $O((T+n)\\log n)$ bits and has latency $O(polylog(n))$, where $n$ is\nthe number of nodes, and $T$ is the minimum of $n^2$ and the number of bits\nsent by adversarially controlled nodes. The algorithm is resilient to\n$(1/4-\\epsilon)n$ Byzantine nodes for any fixed $\\epsilon > 0$, and succeeds\nwith high probability. Our work can be considered as a first application of\nresource-competitive analysis to fundamental Byzantine problems.\n  To complement our algorithm we also show lower bounds for\nresource-competitive Byzantine agreement. We prove that, in general, one cannot\nhope to design Byzantine protocols that have communication cost that is\nsignificantly smaller than the cost of the Byzantine adversary.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 08:59:44 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 05:11:00 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Augustine", "John", ""], ["King", "Valerie", ""], ["Molla", "Anisur R.", ""], ["Pandurangan", "Gopal", ""], ["Saia", "Jared", ""]]}, {"id": "1907.10421", "submitter": "Sumedh Yadav", "authors": "Sumedh Yadav and Mathis Bode", "title": "A graphical heuristic for reduction and partitioning of large datasets\n  for scalable supervised training", "comments": "30 pages, 25 figures, undergoing revision for publication in the\n  Journal of Big Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A scalable graphical method is presented for selecting, and partitioning\ndatasets for the training phase of a classification task. For the heuristic, a\nclustering algorithm is required to get its computation cost in a reasonable\nproportion to the task itself. This step is proceeded by construction of an\ninformation graph of the underlying classification patterns using approximate\nnearest neighbor methods. The presented method constitutes of two approaches,\none for reducing a given training set, and another for partitioning the\nselected/reduced set. The heuristic targets large datasets, since the primary\ngoal is significant reduction in training computation run-time without\ncompromising prediction accuracy. Test results show that both approaches\nsignificantly speed-up the training task when compared against that of\nstate-of-the-art shrinking heuristic available in LIBSVM. Furthermore, the\napproaches closely follow or even outperform in prediction accuracy. A network\ndesign is also presented for the partitioning based distributed training\nformulation. Added speed-up in training run-time is observed when compared to\nthat of serial implementation of the approaches.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 13:05:15 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Yadav", "Sumedh", ""], ["Bode", "Mathis", ""]]}, {"id": "1907.10480", "submitter": "Evgeny Postnikov", "authors": "Evgeny Postnikov (1), Alexander Kryukov (1), Stanislav Polyakov (1),\n  Dmitry Zhurov (2 and 3) ((1) Lomonosov Moscow State University Skobeltsyn\n  Institute of Nuclear Physics (MSU SINP), Moscow, Russia, (2) Applied Physics\n  Institute of Irkutsk State University (API ISU), Irkutsk, Russia, (3) Irkutsk\n  National Research Technical University, Irkutsk, Russia)", "title": "Deep Learning for Energy Estimation and Particle Identification in\n  Gamma-ray Astronomy", "comments": "10 pages, 6 figures. arXiv admin note: text overlap with\n  arXiv:1812.01551", "journal-ref": "Proc. of the 3rd Int. Workshop DLC-2019, CEUR-WS Proceedings,\n  Vol-2406, pp.90-99", "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning techniques, namely convolutional neural networks (CNN), have\npreviously been adapted to select gamma-ray events in the TAIGA experiment,\nhaving achieved a good quality of selection as compared with the conventional\nHillas approach. Another important task for the TAIGA data analysis was also\nsolved with CNN: gamma-ray energy estimation showed some improvement in\ncomparison with the conventional method based on the Hillas analysis.\nFurthermore, our software was completely redeveloped for the graphics\nprocessing unit (GPU), which led to significantly faster calculations in both\nof these tasks. All the results have been obtained with the simulated data of\nTAIGA Monte Carlo software; their experimental confirmation is envisaged for\nthe near future.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 11:27:56 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Postnikov", "Evgeny", "", "2 and 3"], ["Kryukov", "Alexander", "", "2 and 3"], ["Polyakov", "Stanislav", "", "2 and 3"], ["Zhurov", "Dmitry", "", "2 and 3"]]}, {"id": "1907.10484", "submitter": "Muhammad Saad", "authors": "Ashar Ahmad and Muhammad Saad and Aziz Mohaisen", "title": "Secure and Transparent Audit Logs with BlockAudit", "comments": "arXiv admin note: text overlap with arXiv:1811.09944", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audit logs serve as a critical component in enterprise business systems and\nare used for auditing, storing, and tracking changes made to the data. However,\naudit logs are vulnerable to a series of attacks enabling adversaries to tamper\ndata and corresponding audit logs without getting detected. Among them, two\nwell-known attacks are \"the physical access attack,\" which exploits root\nprivileges, and \"the remote vulnerability attack,\" which compromises known\nvulnerabilities in database systems. In this paper, we present BlockAudit: a\nscalable and tamper-proof system that leverages the design properties of audit\nlogs and security guarantees of blockchain to enable secure and trustworthy\naudit logs. Towards that, we construct the design schema of BlockAudit and\noutline its functional and operational procedures. We implement our design on a\ncustom-built Practical Byzantine Fault Tolerance (PBFT) blockchain system and\nevaluate the performance in terms of latency, network size, payload size, and\ntransaction rate. Our results show that conventional audit logs can seamlessly\ntransition into BlockAudit to achieve higher security and defend against the\nknown attacks on audit logs.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2019 00:23:29 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Ahmad", "Ashar", ""], ["Saad", "Muhammad", ""], ["Mohaisen", "Aziz", ""]]}, {"id": "1907.10499", "submitter": "Yannic Maus", "authors": "Yannic Maus", "title": "P-SLOCAL-Completeness of Maximum Independent Set Approximation", "comments": "added author's name. Please note the breakthrough: arXiv:1907.10937", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that the maximum independent set approximation problem with\npolylogarithmic approximation factor is P-SLOCAL-complete. Thus an efficient\nalgorithm for the maximum independent set approximation in the LOCAL model\nimplies efficient algorithms for many problems in the LOCAL model including the\ncomputation of (polylog n, polylog n) network decompositions.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 15:08:07 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 21:53:56 GMT"}, {"version": "v3", "created": "Mon, 23 Dec 2019 09:14:06 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Maus", "Yannic", ""]]}, {"id": "1907.10526", "submitter": "Kai Zhang", "authors": "Kai Zhang, Alireza Entezari", "title": "A Convolutional Forward and Back-Projection Model for Fan-Beam Geometry", "comments": "This paper was submitted to IEEE-TMI, and it's an extension of our\n  ISBI paper (https://ieeexplore.ieee.org/abstract/document/8759285)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Iterative methods for tomographic image reconstruction have great potential\nfor enabling high quality imaging from low-dose projection data. The\ncomputational burden of iterative reconstruction algorithms, however, has been\nan impediment in their adoption in practical CT reconstruction problems. We\npresent an approach for highly efficient and accurate computation of forward\nmodel for image reconstruction in fan-beam geometry in X-ray CT. The efficiency\nof computations makes this approach suitable for large-scale optimization\nalgorithms with on-the-fly, memory-less, computations of the forward and\nback-projection. Our experiments demonstrate the improvements in accuracy as\nwell as efficiency of our model, specifically for first-order box splines\n(i.e., pixel-basis) compared to recently developed methods for this purpose,\nnamely Look-up Table-based Ray Integration (LTRI) and Separable Footprints (SF)\nin 2-D.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 15:39:41 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Zhang", "Kai", ""], ["Entezari", "Alireza", ""]]}, {"id": "1907.10589", "submitter": "Richard Jiang", "authors": "Bing Xu, Tobechukwu Agbele and Richard Jiang", "title": "Biometric Blockchain: A Better Solution for the Security and Trust of\n  Food Logistics", "comments": null, "journal-ref": "2019 3rd International Conference on Artificial Intelligence\n  Applications and Technologies (AIAAT 2019)", "doi": "10.1088/1757-899X/646/1/012009", "report-no": null, "categories": "cs.CR cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain has been emerging as a promising technology that could totally\nchange the landscape of data security in the coming years, particularly for\ndata access over Internet-of-Things and cloud servers. However, blockchain\nitself, though secured by its protocol, does not identify who owns the data and\nwho uses the data. Other than simply encrypting data into keys, in this paper,\nwe proposed a protocol called Biometric Blockchain (BBC) that explicitly\nincorporate the biometric cues of individuals to unambiguously identify the\ncreators and users in a blockchain-based system, particularly to address the\nincreasing needs to secure the food logistics, following the recently widely\nreported incident on wrongly labelled foods that caused the death of a customer\non a flight. The advantage of using BBC in the food logistics is clear: it can\nnot only identify if the data or labels are authentic, but also clearly record\nwho is responsible for the secured data or labels. As a result, such a\nBBC-based solution can great ease the difficulty to control the risks\naccompanying the food logistics, such as faked foods or wrong gradient labels.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2019 23:07:58 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2019 22:18:11 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Xu", "Bing", ""], ["Agbele", "Tobechukwu", ""], ["Jiang", "Richard", ""]]}, {"id": "1907.10595", "submitter": "Amirhossein Reisizadeh", "authors": "Amirhossein Reisizadeh, Hossein Taheri, Aryan Mokhtari, Hamed Hassani,\n  Ramtin Pedarsani", "title": "Robust and Communication-Efficient Collaborative Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a decentralized learning problem, where a set of computing nodes\naim at solving a non-convex optimization problem collaboratively. It is\nwell-known that decentralized optimization schemes face two major system\nbottlenecks: stragglers' delay and communication overhead. In this paper, we\ntackle these bottlenecks by proposing a novel decentralized and gradient-based\noptimization algorithm named as QuanTimed-DSGD. Our algorithm stands on two\nmain ideas: (i) we impose a deadline on the local gradient computations of each\nnode at each iteration of the algorithm, and (ii) the nodes exchange quantized\nversions of their local models. The first idea robustifies to straggling nodes\nand the second alleviates communication efficiency. The key technical\ncontribution of our work is to prove that with non-vanishing noises for\nquantization and stochastic gradients, the proposed method exactly converges to\nthe global optimal for convex loss functions, and finds a first-order\nstationary point in non-convex scenarios. Our numerical evaluations of the\nQuanTimed-DSGD on training benchmark datasets, MNIST and CIFAR-10, demonstrate\nspeedups of up to 3x in run-time, compared to state-of-the-art decentralized\noptimization methods.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 17:55:44 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2019 21:53:56 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Reisizadeh", "Amirhossein", ""], ["Taheri", "Hossein", ""], ["Mokhtari", "Aryan", ""], ["Hassani", "Hamed", ""], ["Pedarsani", "Ramtin", ""]]}, {"id": "1907.10803", "submitter": "Yuichi Sudo", "authors": "Ajoy K. Datta, Lawrence L. Larmore, Toshimitsu Masuzawa, Yuichi Sudo", "title": "A Self-Stabilizing Minimal k-Grouping Algorithm", "comments": "This is a revised version of the conference paper [6], which appears\n  in the proceedings of the 18th International Conference on Distributed\n  Computing and Networking (ICDCN), ACM, 2017. This revised version slightly\n  generalize Theorem 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the minimal k-grouping problem: given a graph G=(V,E) and a\nconstant k, partition G into subgraphs of diameter no greater than k, such that\nthe union of any two subgraphs has diameter greater than k. We give a silent\nself-stabilizing asynchronous distributed algorithm for this problem in the\ncomposite atomicity model of computation, assuming the network has unique\nprocess identifiers. Our algorithm works under the weakly-fair daemon. The time\ncomplexity (i.e., the number of rounds to reach a legitimate configuration) of\nour algorithm is O(nD/k) where n is the number of processes in the network and\n\\diam is the diameter of the network. The space complexity of each process is\nO((n +n_{false})log n) where n_{false} is the number of false identifiers,\ni.e., identifiers that do not match the identifier of any process, but which\nare stored in the local memory of at least one process at the initial\nconfiguration. Our algorithm guarantees that the number of groups is at most\n$2n/k+1$ after convergence. We also give a novel composition technique to\nconcatenate a silent algorithm repeatedly, which we call loop composition.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 02:23:58 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Datta", "Ajoy K.", ""], ["Larmore", "Lawrence L.", ""], ["Masuzawa", "Toshimitsu", ""], ["Sudo", "Yuichi", ""]]}, {"id": "1907.10890", "submitter": "Blesson Varghese", "authors": "Jonathan McChesney and Nan Wang and Ashish Tanwer and Eyal de Lara and\n  Blesson Varghese", "title": "DeFog: Fog Computing Benchmarks", "comments": "Accepted to the ACM/IEEE Symposium on Edge Computing, 2019,\n  Washington DC, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fog computing envisions that deploying services of an application across\nresources in the cloud and those located at the edge of the network may improve\nthe overall performance of the application when compared to running the\napplication on the cloud. However, there are currently no benchmarks that can\ndirectly compare the performance of the application across the cloud-only,\nedge-only and cloud-edge deployment platform to obtain any insight on\nperformance improvement. This paper proposes DeFog, a first Fog benchmarking\nsuite to: (i) alleviate the burden of Fog benchmarking by using a standard\nmethodology, and (ii) facilitate the understanding of the target platform by\ncollecting a catalogue of relevant metrics for a set of benchmarks. The current\nportfolio of DeFog benchmarks comprises six relevant applications conducive to\nusing the edge. Experimental studies are carried out on multiple target\nplatforms to demonstrate the use of DeFog for collecting metrics related to\napplication latencies (communication and computation), for understanding the\nimpact of stress and concurrent users on application latencies, and for\nunderstanding the performance of deploying different combination of services of\nan application across the cloud and edge. DeFog is available for public\ndownload (https://github.com/qub-blesson/DeFog).\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 08:09:27 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["McChesney", "Jonathan", ""], ["Wang", "Nan", ""], ["Tanwer", "Ashish", ""], ["de Lara", "Eyal", ""], ["Varghese", "Blesson", ""]]}, {"id": "1907.10895", "submitter": "Shaked Matar", "authors": "Michael Elkin and Shaked Matar", "title": "Fast Deterministic Constructions of Linear-Size Spanners and Skeletons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the distributed setting, the only existing constructions of \\textit{sparse\nskeletons}, (i.e., subgraphs with $O(n)$ edges) either use randomization or\nlarge messages, or require $\\Omega(D)$ time, where $D$ is the hop-diameter of\nthe input graph $G$. We devise the first deterministic distributed algorithm in\nthe CONGEST model (i.e., uses small messages) for constructing linear-size\nskeletons in time $2^{O(\\sqrt{{\\log n}\\cdot{\\log{\\log n}}})}$. We can also\ncompute a linear-size spanner with stretch $polylog(n)$ in low deterministic\npolynomial time, i.e., $O(n^\\rho)$ for an arbitrarily small constant $\\rho >0$,\nin the CONGEST model. Yet another algorithm that we devise runs in $O({\\log\nn})^{\\kappa-1}$ time, for a parameter $\\kappa=1,2,\\dots,$ and constructs an\n$O({\\log n})^{\\kappa-1}$ spanner with $O(n^{1+1/\\kappa})$ edges. All our\ndistributed algorithms are lightweight from the computational perspective,\ni.e., none of them employs any heavy computations.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 08:21:54 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Elkin", "Michael", ""], ["Matar", "Shaked", ""]]}, {"id": "1907.10904", "submitter": "Siqi Wang", "authors": "Siqi Wang", "title": "Collaborative Heterogeneous Computing on MPSoCs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis (extended abstract) presents the software development efforts\ntoward efficient exploitation of heterogeneity through intricate mapping of\ncomputational kernels, collaborative execution of multiple processing elements\nand application-specific techniques. The goal is to embrace the heterogeneity\nto unleash the full potential of the heterogeneous MPSoCs towards\nhigh-performance energy-efficient mobile computing.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 08:57:56 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Wang", "Siqi", ""]]}, {"id": "1907.10937", "submitter": "Mohsen Ghaffari", "authors": "V\\'aclav Rozho\\v{n} and Mohsen Ghaffari", "title": "Polylogarithmic-Time Deterministic Network Decomposition and Distributed\n  Derandomization", "comments": "Extended version of an article that appears at the Symposium on\n  Theory of Computing (STOC) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple polylogarithmic-time deterministic distributed algorithm\nfor network decomposition. This improves on a celebrated $2^{O(\\sqrt{\\log\nn})}$-time algorithm of Panconesi and Srinivasan [STOC'92] and settles a\ncentral and long-standing question in distributed graph algorithms. It also\nleads to the first polylogarithmic-time deterministic distributed algorithms\nfor numerous other problems, hence resolving several well-known and decades-old\nopen problems, including Linial's question about the deterministic complexity\nof maximal independent set [FOCS'87; SICOMP'92]---which had been called the\nmost outstanding problem in the area.\n  The main implication is a more general distributed derandomization theorem:\nPut together with the results of Ghaffari, Kuhn, and Maus [STOC'17] and\nGhaffari, Harris, and Kuhn [FOCS'18], our network decomposition implies that\n$$\\mathsf{P}\\textit{-}\\mathsf{RLOCAL} = \\mathsf{P}\\textit{-}\\mathsf{LOCAL}.$$\nThat is, for any problem whose solution can be checked deterministically in\npolylogarithmic-time, any polylogarithmic-time randomized algorithm can be\nderandomized to a polylogarithmic-time deterministic algorithm. Informally, for\nthe standard first-order interpretation of efficiency as polylogarithmic-time,\ndistributed algorithms do not need randomness for efficiency.\n  By known connections, our result leads also to substantially faster\nrandomized distributed algorithms for a number of well-studied problems\nincluding $(\\Delta+1)$-coloring, maximal independent set, and Lov\\'{a}sz Local\nLemma, as well as massively parallel algorithms for $(\\Delta+1)$-coloring.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 10:01:49 GMT"}, {"version": "v2", "created": "Sun, 10 May 2020 18:24:18 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Rozho\u0148", "V\u00e1clav", ""], ["Ghaffari", "Mohsen", ""]]}, {"id": "1907.10971", "submitter": "Artur Sterz", "authors": "Artur Sterz, Lars Baumg\\\"artner, Jonas h\\\"ochst, Patrick Lampe, Bernd\n  Freisleben", "title": "OPPLOAD: Offloading Computational Workflows in Opportunistic Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computation offloading is often used in mobile cloud, edge, and/or fog\ncomputing to cope with resource limitations of mobile devices in terms of\ncomputational power, storage, and energy. Computation offloading is\nparticularly challenging in situations where network connectivity is\nintermittent or error-prone. In this paper, we present OPPLOAD, a novel\nframework for offloading computational workflows in opportunistic networks. The\nindividual tasks forming a workflow can be assigned to particular remote\nexecution platforms (workers) either preselected ahead of time or decided just\nin time where a matching worker will automatically be assigned for the next\ntask. Tasks are only assigned to capable workers that announce their\ncapabilities. Furthermore, tasks of a workflow can be executed on multiple\nworkers that are automatically selected to balance the load. Our Python\nimplementation of OPPLOAD is publicly available as open source software. The\nresults of our experimental evaluation demonstrate the feasibility of our\napproach.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 11:11:28 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Sterz", "Artur", ""], ["Baumg\u00e4rtner", "Lars", ""], ["h\u00f6chst", "Jonas", ""], ["Lampe", "Patrick", ""], ["Freisleben", "Bernd", ""]]}, {"id": "1907.11057", "submitter": "Andreas Veneris", "authors": "Andreas Veneris, Andreas Park", "title": "Special Drawing Rights in a New Decentralized Century", "comments": "4 pages, IMF Georgetown", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN cs.CY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unfulfilled expectations from macro-economic initiatives during the Great\nRecession and the massive shift into globalization echo today with political\nupheaval, anti-establishment propaganda, and looming trade/currency wars that\nthreaten domestic and international value chains. Once stable entities like the\nEU now look fragile and political instability in the US presents unprecedented\nchallenges to an International Monetary System (IMS) that predominantly relies\non the USD and EUR as reserve currencies. In this environment, it is critical\nfor an international organization mandated to ensure stability to plan and act\nahead. This paper argues that Decentralized Ledger-based technology (DLT) is\nkey for the International Monetary Fund (IMF) to mitigate some of those risks,\npromote stability and safeguard world prosperity. Over the last two years, DLT\nhas made headline news globally and created a worldwide excitement not seen\nsince the internet entered the mainstream. The rapid adoption and open-to-all\nphilosophy of DLT has already redefined global socioeconomics, promises to\nshake up the world of commerce/finance and challenges the workings of central\ngovernments/regulators. This paper examines DLT core premises and proposes a\ntwo-step approach for the IMF to expand Special Drawing Rights (SDR) into that\nsphere so as to become the originally envisioned numeraire and reserve currency\nfor cross-border transactions in this new decentralized century.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 00:52:04 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Veneris", "Andreas", ""], ["Park", "Andreas", ""]]}, {"id": "1907.11329", "submitter": "Ran Cohen", "authors": "Ran Cohen and Iftach Haitner and Nikolaos Makriyannis and Matan Orland\n  and Alex Samorodnitsky", "title": "On the Round Complexity of Randomized Byzantine Agreement", "comments": "DISC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove lower bounds on the round complexity of randomized Byzantine\nagreement (BA) protocols, bounding the halting probability of such protocols\nafter one and two rounds. In particular, we prove that:\n  (1) BA protocols resilient against $n/3$ [resp., $n/4$] corruptions terminate\n(under attack) at the end of the first round with probability at most $o(1)$\n[resp., $1/2+ o(1)$].\n  (2) BA protocols resilient against $n/4$ corruptions terminate at the end of\nthe second round with probability at most $1-\\Theta(1)$.\n  (3) For a large class of protocols (including all BA protocols used in\npractice) and under a plausible combinatorial conjecture, BA protocols\nresilient against $n/3$ [resp., $n/4$] corruptions terminate at the end of the\nsecond round with probability at most $o(1)$ [resp., $1/2 + o(1)$].\n  The above bounds hold even when the parties use a trusted setup phase, e.g.,\na public-key infrastructure (PKI).\n  The third bound essentially matches the recent protocol of Micali (ITCS'17)\nthat tolerates up to $n/3$ corruptions and terminates at the end of the third\nround with constant probability.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 22:49:43 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 21:12:10 GMT"}, {"version": "v3", "created": "Tue, 17 Dec 2019 04:12:21 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Cohen", "Ran", ""], ["Haitner", "Iftach", ""], ["Makriyannis", "Nikolaos", ""], ["Orland", "Matan", ""], ["Samorodnitsky", "Alex", ""]]}, {"id": "1907.11367", "submitter": "Sunny Sanyal", "authors": "Sunny Sanyal", "title": "Data Aggregation Techniques for Internet of Things", "comments": "This is the master's thesis of Mr. Sunny Sanyal, who graduated from\n  Chongqing University of Posts and Telecommunications, Chongqing, China. This\n  thesis document has received the Excellent Master's thesis Award 2019\n  (includes all departments) from the University. All the chapters in this\n  thesis are published in various venues", "journal-ref": null, "doi": null, "report-no": "D-10617-308-2019", "categories": "cs.NI cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this dissertation is to design efficient data aggregation\nframeworks for massive IoT networks in different scenarios to support the\nproper functioning of IoT analytics layer. This dissertation includes modern\nalgorithmic frameworks such as non convex optimization, machine learning,\nstochastic matrix perturbation theory and federated filtering along with modern\ncomputing infrastructure such as fog computing and cloud computing. The\ndevelopment of such an ambitious design involves many open challenges, this\nproposal envisions three major open challenges for IoT data aggregation: first,\nsevere resource constraints of IoT nodes due to limited power and computational\nability, second, the highly uncertain (unreliable) raw IoT data is not fit for\ndecisionmaking and third, network latency and privacy issue for critical\napplications. This dissertation presents three independent novel approaches for\ndistinct scenarios to solve one or more aforementioned open challenges. The\nfirst approach focuses on energy efficient routing; discusses a clustering\nprotocol based on device to device communication for both stationary and mobile\nIoT nodes. The second approach focuses on processing uncertain raw IoT data;\npresents an IoT data aggregation scheme to improve the quality of raw IoT data.\nFinally, the third approach focuses on power loss due to communication overhead\nand privacy issues for medical IoT devices (IoMT); describes a prediction based\ndata aggregation framework for massive IoMT devices.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 18:21:58 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Sanyal", "Sunny", ""]]}, {"id": "1907.11445", "submitter": "William Buchanan Prof", "authors": "Pierre Chevalier, Bartlomiej Kaminski, Fraser Hutchison, Qi Ma,\n  Spandan Sharma, Andreas Fackler and William J Buchanan", "title": "Protocol for Asynchronous, Reliable, Secure and Efficient Consensus\n  (PARSEC) Version 2.0", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we present an open source, fully asynchronous, leaderless\nalgorithm for reaching consensus in the presence of Byzantine faults in an\nasynchronous network. We prove the algorithm's correctness provided that less\nthan a third of participating nodes are faulty. We also present a way of\napplying the algorithm to a network with dynamic membership, i.e. a network in\nwhich nodes can join and leave at will. The core contribution of this paper is\nan optimal model in the definition of an asynchronous BFT protocol, and which\nis resilient to 1/3 byzantine nodes. This model matches an agreement with\nprobability one (unlike some probabilistic methods), and where a common coin is\nused as a source of randomization so that it respects the FLP impossibility\nresult.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 09:14:54 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Chevalier", "Pierre", ""], ["Kaminski", "Bartlomiej", ""], ["Hutchison", "Fraser", ""], ["Ma", "Qi", ""], ["Sharma", "Spandan", ""], ["Fackler", "Andreas", ""], ["Buchanan", "William J", ""]]}, {"id": "1907.11465", "submitter": "Pedro Garcia Lopez", "authors": "Pedro Garc\\'ia-L\\'opez, Marc S\\'anchez-Artigas, Simon Shillaker, Peter\n  Pietzuch, David Breitgand, Gil Vernik, Pierre Sutra, Tristan Tarrant, Ana\n  Juan Ferrer", "title": "ServerMix: Tradeoffs and Challenges of Serverless Data Analytics", "comments": "15 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Serverless computing has become very popular today since it largely\nsimplifies cloud programming. Developers do not need to longer worry about\nprovisioning or operating servers, and they pay only for the compute resources\nused when their code is run. This new cloud paradigm suits well for many\napplications, and researchers have already begun investigating the feasibility\nof serverless computing for data analytics. Unfortunately, today's serverless\ncomputing presents important limitations that make it really difficult to\nsupport all sorts of analytics workloads. This paper first starts by analyzing\nthree fundamental trade-offs of today's serverless computing model and their\nrelationship with data analytics. It studies how by relaxing disaggregation,\nisolation, and simple scheduling, it is possible to increase the overall\ncomputing performance, but at the expense of essential aspects of the model\nsuch as elasticity, security, or sub-second activations, respectively. The\nconsequence of these trade-offs is that analytics applications may well end up\nembracing hybrid systems composed of serverless and serverful components, which\nwe call Servermix in this paper. We will review the existing related work to\nshow that most applications can be actually categorized as Servermix. Finally,\nthis paper will introduce the major challenges of the CloudButton research\nproject to manage these trade-offs.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 10:02:49 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Garc\u00eda-L\u00f3pez", "Pedro", ""], ["S\u00e1nchez-Artigas", "Marc", ""], ["Shillaker", "Simon", ""], ["Pietzuch", "Peter", ""], ["Breitgand", "David", ""], ["Vernik", "Gil", ""], ["Sutra", "Pierre", ""], ["Tarrant", "Tristan", ""], ["Ferrer", "Ana Juan", ""]]}, {"id": "1907.11580", "submitter": "Phu Lai", "authors": "Phu Lai, Qiang He, Guangming Cui, Xiaoyu Xia, Mohamed Abdelrazek,\n  Feifei Chen, John Hosking, John Grundy, Yun Yang", "title": "Edge User Allocation with Dynamic Quality of Service", "comments": "This manuscript has been accepted for publication at the 17th\n  International Conference on Service-Oriented Computing and may be published\n  in the book series Lecture Notes in Computer Science. All copyrights reserved\n  to Springer Nature Switzerland AG, Gewerbestrasse 11, 6330 Cham, Switzerland", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In edge computing, edge servers are placed in close proximity to end-users.\nApp vendors can deploy their services on edge servers to reduce network latency\nexperienced by their app users. The edge user allocation (EUA) problem\nchallenges service providers with the objective to maximize the number of\nallocated app users with hired computing resources on edge servers while\nensuring their fixed quality of service (QoS), e.g., the amount of computing\nresources allocated to an app user. In this paper, we take a step forward to\nconsider dynamic QoS levels for app users, which generalizes but further\ncomplicates the EUA problem, turning it into a dynamic QoS EUA problem. This\nenables flexible levels of quality of experience (QoE) for app users. We\npropose an optimal approach for finding a solution that maximizes app users'\noverall QoE. We also propose a heuristic approach for quickly finding\nsub-optimal solutions to large-scale instances of the dynamic QoS EUA problem.\nExperiments are conducted on a real-world dataset to demonstrate the\neffectiveness and efficiency of our approaches against a baseline approach and\nthe state of the art.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 13:59:44 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Lai", "Phu", ""], ["He", "Qiang", ""], ["Cui", "Guangming", ""], ["Xia", "Xiaoyu", ""], ["Abdelrazek", "Mohamed", ""], ["Chen", "Feifei", ""], ["Hosking", "John", ""], ["Grundy", "John", ""], ["Yang", "Yun", ""]]}, {"id": "1907.11612", "submitter": "Ido Hakimi", "authors": "Ido Hakimi, Saar Barkai, Moshe Gabel, Assaf Schuster", "title": "Taming Momentum in a Distributed Asynchronous Environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although distributed computing can significantly reduce the training time of\ndeep neural networks, scaling the training process while maintaining high\nefficiency and final accuracy is challenging. Distributed asynchronous training\nenjoys near-linear speedup, but asynchrony causes gradient staleness - the main\ndifficulty in scaling stochastic gradient descent to large clusters. Momentum,\nwhich is often used to accelerate convergence and escape local minima,\nexacerbates the gradient staleness, thereby hindering convergence. We propose\nDANA: a novel technique for asynchronous distributed SGD with momentum that\nmitigates gradient staleness by computing the gradient on an estimated future\nposition of the model's parameters. Thereby, we show for the first time that\nmomentum can be fully incorporated in asynchronous training with almost no\nramifications to final accuracy. Our evaluation on the CIFAR and ImageNet\ndatasets shows that DANA outperforms existing methods, in both final accuracy\nand convergence speed while scaling up to a total batch size of 16K on 64\nasynchronous workers.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 15:07:49 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 15:23:20 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2020 06:09:35 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Hakimi", "Ido", ""], ["Barkai", "Saar", ""], ["Gabel", "Moshe", ""], ["Schuster", "Assaf", ""]]}, {"id": "1907.11621", "submitter": "Guillaume Pierre", "authors": "Arif Ahmed and HamidReza Arkian and Davaadorj Battulga and Ali J. Fahs\n  and Mozhdeh Farhadi and Dimitrios Giouroukis and Adrien Gougeon and Felipe\n  Oliveira Gutierrez and Guillaume Pierre and Paulo R. Souza Jr and Mulugeta\n  Ayalew Tamiru and Li Wu", "title": "Fog Computing Applications: Taxonomy and Requirements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fog computing was designed to support the specific needs of latency-critical\napplications such as augmented reality, and IoT applications which produce\nmassive volumes of data that are impractical to send to faraway cloud data\ncenters for analysis. However this also created new opportunities for a wider\nrange of applications which in turn impose their own requirements on future fog\ncomputing platforms. This article presents a study of a representative set of\n30 fog computing applications and the requirements that a general-purpose fog\ncomputing platform should support.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 15:20:57 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Ahmed", "Arif", ""], ["Arkian", "HamidReza", ""], ["Battulga", "Davaadorj", ""], ["Fahs", "Ali J.", ""], ["Farhadi", "Mozhdeh", ""], ["Giouroukis", "Dimitrios", ""], ["Gougeon", "Adrien", ""], ["Gutierrez", "Felipe Oliveira", ""], ["Pierre", "Guillaume", ""], ["Souza", "Paulo R.", "Jr"], ["Tamiru", "Mulugeta Ayalew", ""], ["Wu", "Li", ""]]}, {"id": "1907.11623", "submitter": "Mirco Mannucci Ph.D.", "authors": "Mirco A. Mannucci, Deborah Tylor", "title": "Node Alertness-Detecting changes in rapidly evolving graphs", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we describe a new approach for detecting changes in rapidly\nevolving large-scale graphs. The key notion involved is local alertness: nodes\nmonitor change within their neighborhoods at each time step. Here we propose a\nfinancial local alertness application for cointegrated stock pairs\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 12:11:24 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Mannucci", "Mirco A.", ""], ["Tylor", "Deborah", ""]]}, {"id": "1907.11678", "submitter": "Yongmin Hu", "authors": "Yongmin Hu, Hailong Yang, Zhongzhi Luan and Depei Qian", "title": "Massively Scaling Seismic Processing on Sunway TaihuLight Supercomputer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common Midpoint (CMP) and Common Reflection Surface (CRS) are widely used\nmethods for improving the signal-to-noise ratio in the field of seismic\nprocessing. These methods are computationally intensive and require high\nperformance computing. This paper optimizes these methods on the Sunway\nmany-core architecture and implements large-scale seismic processing on the\nSunway Taihulight supercomputer. We propose the following three optimization\ntechniques: 1) we propose a software cache method to reduce the overhead of\nmemory accesses, and share data among CPEs via the register communication; 2)\nwe re-design the semblance calculation procedure to further reduce the overhead\nof memory accesses; 3) we propose a vectorization method to improve the\nperformance when processing the small volume of data within short loops. The\nexperimental results show that our implementations of CMP and CRS methods on\nSunway achieve 3.50x and 3.01x speedup on average compared to\nthe-state-of-the-art implementations on CPU. In addition, our implementation is\ncapable to run on more than one million cores of Sunway TaihuLight with good\nscalability.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 17:01:18 GMT"}, {"version": "v2", "created": "Sun, 4 Aug 2019 12:14:09 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Hu", "Yongmin", ""], ["Yang", "Hailong", ""], ["Luan", "Zhongzhi", ""], ["Qian", "Depei", ""]]}, {"id": "1907.11717", "submitter": "Muhammad Bilal", "authors": "Muhammad Bilal and Sangheon Pack", "title": "Secure Distribution of Protected Content in Information-Centric\n  Networking", "comments": "15 pages, 8 figures, This article is an enhancement version of\n  journal article published in IEEE Systems Journal, DOI:\n  10.1109/JSYST.2019.2931813. arXiv admin note: text overlap with\n  arXiv:1808.03289", "journal-ref": "in IEEE Systems Journal, vol. 14, no. 2, pp. 1921-1932, June 2020", "doi": "10.1109/JSYST.2019.2931813", "report-no": null, "categories": "cs.NI cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The benefits of the ubiquitous caching in ICN are profound, such features\nmake ICN promising for content distribution, but it also introduces a challenge\nto content protection against the unauthorized access. The protection of a\ncontent against unauthorized access requires consumer authentication and\ninvolves the conventional end-to-end encryption. However, in\ninformation-centric networking (ICN), such end-to-end encryption makes the\ncontent caching ineffective since encrypted contents stored in a cache are\nuseless for any consumers except those who know the encryption key. For\neffective caching of encrypted contents in ICN, we propose a secure\ndistribution of protected content (SDPC) scheme, which ensures that only\nauthenticated consumers can access the content. SDPC is lightweight and allows\nconsumers to verify the originality of the published content by using a\nsymmetric key encryption. Moreover, SDPC naming scheme provides protection\nagainst privacy leakage. The security of SDPC was proved with the BAN logic and\nScyther tool verification, and simulation results show that SDPC can reduce the\ncontent download delay.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 13:48:41 GMT"}, {"version": "v2", "created": "Sun, 8 Dec 2019 07:58:31 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Bilal", "Muhammad", ""], ["Pack", "Sangheon", ""]]}, {"id": "1907.11804", "submitter": "Kartikeya Bhardwaj", "authors": "Kartikeya Bhardwaj, Chingyi Lin, Anderson Sartor, Radu Marculescu", "title": "Memory- and Communication-Aware Model Compression for Distributed Deep\n  Learning Inference on IoT", "comments": "This preprint is for personal use only. The official article will\n  appear as part of the ESWEEK-TECS special issue and will be presented in the\n  International Conference on Hardware/Software Codesign and System Synthesis\n  (CODES+ISSS), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model compression has emerged as an important area of research for deploying\ndeep learning models on Internet-of-Things (IoT). However, for extremely\nmemory-constrained scenarios, even the compressed models cannot fit within the\nmemory of a single device and, as a result, must be distributed across multiple\ndevices. This leads to a distributed inference paradigm in which memory and\ncommunication costs represent a major bottleneck. Yet, existing model\ncompression techniques are not communication-aware. Therefore, we propose\nNetwork of Neural Networks (NoNN), a new distributed IoT learning paradigm that\ncompresses a large pretrained 'teacher' deep network into several disjoint and\nhighly-compressed 'student' modules, without loss of accuracy. Moreover, we\npropose a network science-based knowledge partitioning algorithm for the\nteacher model, and then train individual students on the resulting disjoint\npartitions. Extensive experimentation on five image classification datasets,\nfor user-defined memory/performance budgets, show that NoNN achieves higher\naccuracy than several baselines and similar accuracy as the teacher model,\nwhile using minimal communication among students. Finally, as a case study, we\ndeploy the proposed model for CIFAR-10 dataset on edge devices and demonstrate\nsignificant improvements in memory footprint (up to 24x), performance (up to\n12x), and energy per node (up to 14x) compared to the large teacher model. We\nfurther show that for distributed inference on multiple edge devices, our\nproposed NoNN model results in up to 33x reduction in total latency w.r.t. a\nstate-of-the-art model compression baseline.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 22:17:42 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Bhardwaj", "Kartikeya", ""], ["Lin", "Chingyi", ""], ["Sartor", "Anderson", ""], ["Marculescu", "Radu", ""]]}, {"id": "1907.11961", "submitter": "John Coulter", "authors": "Jason Wells and J. Eric Coulter", "title": "Research Computing at a Business University", "comments": "5 pages, 5 figures, presented at PEARC19 in Chicago, IL", "journal-ref": null, "doi": "10.1145/3332186.3333161", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research Computing demands are exploding beyond traditional disciplines due\nto the proliferation of data in all walks of life. At Bentley University\n(\"Bentley\"), a business university in the Boston area, this expansion has been\nmost readily seen in our Accounting, Economics, Mathematics, and Natural\nSciences departments. The result has been a small effort to build a research\ncomputing capability at this small New England university. This poster will\nserve as an overview of the steps taken to build such an effort at a business\nuniversity, the revelations we have had along the way, and our plans for the\nfuture.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 18:31:04 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Wells", "Jason", ""], ["Coulter", "J. Eric", ""]]}, {"id": "1907.11997", "submitter": "Yahya Hassanzadeh Nazarabadi", "authors": "Yahya Hassanzadeh-Nazarabadi, Alptekin K\\\"up\\c{c}\\\"u, \\\"Oznur\n  \\\"Ozkasap", "title": "Decentralized utility- and locality-aware replication for heterogeneous\n  DHT-based P2P cloud storage systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a Distributed Hash Table (DHT), Skip Graph routing overlays are exploited\nin several peer-to-peer (P2P) services, including P2P cloud storage. The fully\ndecentralized replication algorithms that are applicable to the Skip\nGraph-based P2P cloud storage fail on improving the performance of the system\nwith respect to both the availability of replicas as well as their response\ntime. Additionally, they presume the system as homogeneous with respect to the\nnodes' latency distribution, availability behavior, bandwidth, or storage. In\nthis paper, we propose Pyramid, which is the first fully decentralized utility-\nand locality-aware replication approach for Skip Graph-based P2P cloud storage\nsystems. Pyramid considers the nodes as heterogeneous with respect to their\nlatency distribution, availability behavior, bandwidth, and storage. Pyramid is\nutility-aware as it maximizes the average available bandwidth of replicas per\ntime slot (e.g., per hour). Additionally, Pyramid is locality-aware as it\nminimizes the average latency between nodes and their closest replica. Our\nsimulation results show that compared to the state-of-the-art solutions that\neither perform good in utility-awareness, or in locality-awareness, our\nproposed Pyramid improves both the utility- and locality-awareness of replicas\nwith a gain of about 1.2 and 1.1 times at the same time, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 01:26:53 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Hassanzadeh-Nazarabadi", "Yahya", ""], ["K\u00fcp\u00e7\u00fc", "Alptekin", ""], ["\u00d6zkasap", "\u00d6znur", ""]]}, {"id": "1907.12011", "submitter": "Parikshit Saikia", "authors": "Parikshit Saikia and Sushanta Karmakar", "title": "Distributed Approximation Algorithms for Steiner Tree in the\n  $\\mathcal{CONGESTED}$ $\\mathcal{CLIQUE}$", "comments": "22 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The \\emph{Steiner tree} problem is one of the fundamental and classical\nproblems in combinatorial optimization. In this paper, we study this problem in\nthe $\\mathcal{CONGESTED}$ $\\mathcal{CLIQUE}$ model of distributed computing and\npresent two deterministic distributed approximation algorithms for the same.\nThe first algorithm computes a Steiner tree in $\\tilde{O}(n^{1/3})$ rounds and\n$\\tilde{O}(n^{7/3})$ messages for a given connected undirected weighted graph\nof $n$ nodes. Note here that $\\tilde{O}(\\cdot)$ notation hides polylogarithmic\nfactors in $n$. The second one computes a Steiner tree in $O(S + \\log\\log n)$\nrounds and $O(S (n - t)^2 + n^2)$ messages, where $S$ and $t$ are the\n\\emph{shortest path diameter} and the number of \\emph{terminal} nodes\nrespectively in the given input graph. Both the algorithms admit an\napproximation factor of $2(1 - 1/\\ell)$, where $\\ell$ is the number of terminal\nleaf nodes in the optimal Steiner tree. For graphs with $S = \\omega(n^{1/3}\n\\log n)$, the first algorithm exhibits better performance than the second one\nin terms of the round complexity. On the other hand, for graphs with $S =\n\\tilde{o}(n^{1/3})$, the second algorithm outperforms the first one in terms of\nthe round complexity. In fact when $S = O(\\log\\log n)$ then the second\nalgorithm admits a round complexity of $O(\\log\\log n)$ and message complexity\nof $\\tilde{O}(n^2)$. To the best of our knowledge, this is the first work to\nstudy the Steiner tree problem in the $\\mathcal{CONGESTED}$ $\\mathcal{CLIQUE}$\nmodel.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 04:41:37 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2019 04:37:24 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Saikia", "Parikshit", ""], ["Karmakar", "Sushanta", ""]]}, {"id": "1907.12028", "submitter": "Sumathi Sivasubramaniam", "authors": "John Augustine and Sumathi Sivasubramaniam", "title": "Spartan: Sparse Robust Addressable Networks", "comments": "25 pages, 10 figures. A preliminary version of this paper appeared in\n  the Proceedings of the IEEE International Parallel and Distributed Processing\n  Symposium (IPDPS) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Peer-to-Peer (P2P) network is a dynamic collection of nodes that connect\nwith each other via virtual overlay links built upon an underlying network\n(usually, the Internet). P2P networks are highly dynamic and can experience\nvery heavy churn, i.e., a large number of nodes join/leave the network\ncontinuously. Thus, building and maintaining a stable overlay network is an\nimportant problem that has been studied extensively for two decades.\n  In this paper, we present our \\Pe overlay network called Sparse Robust\nAddressable Network (Spartan). Spartan can be quickly and efficiently built in\na fully distributed fashion within $O(\\log n)$ rounds. Furthermore, the Spartan\noverlay structure can be maintained, again, in a fully distributed manner\ndespite adversarially controlled churn (i.e., nodes joining and leaving) and\nsignificant variation in the number of nodes. Moreover, new nodes can join a\ncommittee within $O(1)$ rounds and leaving nodes can leave without any notice.\n  The number of nodes in the network lies in $[n, fn]$ for any fixed $f\\ge 1$.\nUp to $\\epsilon n$ nodes (for some small but fixed $\\epsilon > 0$) can be\nadversarially added/deleted within {\\em any} period of $P$ rounds for some $P\n\\in O(\\log \\log n)$. Despite such uncertainty in the network, Spartan maintains\n$\\Theta(n/\\log n)$ committees that are stable and addressable collections of\n$\\Theta(\\log n)$ nodes each for $O(polylog(n))$ rounds with high probability.\n  Spartan's committees are also capable of performing sustained computation and\npassing messages between each other. Thus, any protocol designed for static\nnetworks can be simulated on Spartan with minimal overhead. This makes Spartan\nan ideal platform for developing applications. We experimentally show that\nSpartan will remain robust as long as each committee, on average, contains 24\nnodes for networks of size up to $10240$.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 06:58:43 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 07:54:30 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Augustine", "John", ""], ["Sivasubramaniam", "Sumathi", ""]]}, {"id": "1907.12042", "submitter": "Bin Liu", "authors": "Linfu Yang, Bin Liu", "title": "Temporal Data Fusion at the Edge", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an enabler technique, data fusion has gained great attention in the\ncontext of Internet of things (IoT). In traditional settings, data fusion is\ndone at the cloud server. So the data to be fused should be transferred from\nthe sensor nodes to the cloud server before data fusion. Such an application\nmode of data fusion inherits disturbing concerns from the cloud computing\nframework, e.g., privacy-leaking, large latency between data capture and\ncomputation, excessive ingress bandwidth consumption. We take into account how\nto do temporal data fusion at the edge to bypass the above issues. We present a\nGaussian process based temporal data fusion (GPTDF) method targeted for the\nproblem of sequential online prediction at the edge. The GPTDF method fits the\nedge computing framework and thus inherits desirable properties from edge\ncomputing, such as privacy-preserving, low latency between data capture and\ncomputation, and tiny bandwidth consumption. Through a real-data experiment\nusing archived traffic datasets from the Caltrans Performance Measurement\nSystem (PeMS), we demonstrate that the application of GPTDF can provide more\ntimely and accurate real-time predictions at the network edge.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 08:12:03 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 02:58:59 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Yang", "Linfu", ""], ["Liu", "Bin", ""]]}, {"id": "1907.12152", "submitter": "Ali Mashreghi", "authors": "Ali Mashreghi, Valerie King", "title": "Faster asynchronous MST and low diameter tree construction with\n  sublinear communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building a spanning tree, minimum spanning tree (MST), and BFS tree in a\ndistributed network are fundamental problems which are still not fully\nunderstood in terms of time and communication cost. x The first work to succeed\nin computing a spanning tree with communication sublinear in the number of\nedges in an asynchronous CONGEST network appeared in DISC 2018. That algorithm\nwhich constructs an MST is sequential in the worst case; its running time is\nproportional to the total number of messages sent. Our paper matches its\nmessage complexity but brings the running time down to linear in $n$. Our\ntechniques can also be used to provide an asynchronous algorithm with sublinear\ncommunication to construct a tree in which the distance from a source to each\nnode is within an additive term of $\\sqrt{n}$ of its actual distance.\n  We can convert any asynchronous MST algorithm with time $T(n, m)$ and message\ncomplexity of $M(n, m)$ to an algorithm with time $O(n^{1 - 2\\epsilon} + T(n,\nn^{3/2 + \\epsilon}))$ and message complexity of $\\tilde{O}(n^{3/2 + \\epsilon} +\nM(n, n^{3/2+\\epsilon}))$, for $\\epsilon \\in [0, 1/4]$. Picking $\\epsilon = 0$\nand using Awerbuch's algorithm \\cite{awerbuch1987optimal}, this results in an\nMST algorithm with time $O(n)$ and message complexity $\\tilde{O}(n^{3/2})$.\nHowever, if there were an asynchronous MST algorithm that takes time sublinear\nin $n$ and requires messages linear in $m$, by picking $\\epsilon > 0$ we could\nachieve sublinear time (in $n$) and sublinear communication (in $m$),\nsimultaneously. To the best of our knowledge, there is no such algorithm.\n  All the algorithms presented here are Monte Carlo and succeed with high\nprobability, in the KT1 CONGEST asynchronous model.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 23:00:54 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Mashreghi", "Ali", ""], ["King", "Valerie", ""]]}, {"id": "1907.12182", "submitter": "Zhenlong Li Dr.", "authors": "Zhenlong Li", "title": "Geospatial Big Data Handling with High Performance Computing: Current\n  Approaches and Future Directions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geospatial big data plays a major role in the era of big data, as most data\ntoday are inherently spatial, collected with ubiquitous location-aware sensors.\nEfficiently collecting, managing, storing, and analyzing geospatial data\nstreams enables development of new decision-support systems and provides\nunprecedented opportunities for business, science, and engineering. However,\nhandling the \"Vs\" (volume, variety, velocity, veracity, and value) of big data\nis a challenging task. This is especially true for geospatial big data, since\nthe massive datasets must be analyzed in the context of space and time. High\nperformance computing (HPC) provides an essential solution to geospatial big\ndata challenges. This chapter first summarizes four key aspects for handling\ngeospatial big data with HPC and then briefly reviews existing HPC-related\nplatforms and tools for geospatial big data processing. Lastly, future research\ndirections in using HPC for geospatial big data handling are discussed.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 02:37:43 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Li", "Zhenlong", ""]]}, {"id": "1907.12205", "submitter": "Shashank Rajput", "authors": "Shashank Rajput, Hongyi Wang, Zachary Charles and Dimitris\n  Papailiopoulos", "title": "DETOX: A Redundancy-based Framework for Faster and More Robust Gradient\n  Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To improve the resilience of distributed training to worst-case, or Byzantine\nnode failures, several recent approaches have replaced gradient averaging with\nrobust aggregation methods. Such techniques can have high computational costs,\noften quadratic in the number of compute nodes, and only have limited\nrobustness guarantees. Other methods have instead used redundancy to guarantee\nrobustness, but can only tolerate limited number of Byzantine failures. In this\nwork, we present DETOX, a Byzantine-resilient distributed training framework\nthat combines algorithmic redundancy with robust aggregation. DETOX operates in\ntwo steps, a filtering step that uses limited redundancy to significantly\nreduce the effect of Byzantine nodes, and a hierarchical aggregation step that\ncan be used in tandem with any state-of-the-art robust aggregation method. We\nshow theoretically that this leads to a substantial increase in robustness, and\nhas a per iteration runtime that can be nearly linear in the number of compute\nnodes. We provide extensive experiments over real distributed setups across a\nvariety of large-scale machine learning tasks, showing that DETOX leads to\norders of magnitude accuracy and speedup improvements over many\nstate-of-the-art Byzantine-resilient approaches.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 04:02:35 GMT"}, {"version": "v2", "created": "Sun, 8 Mar 2020 03:02:12 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Rajput", "Shashank", ""], ["Wang", "Hongyi", ""], ["Charles", "Zachary", ""], ["Papailiopoulos", "Dimitris", ""]]}, {"id": "1907.12240", "submitter": "Sina Niedermaier", "authors": "Sina Niedermaier, Falko Koetter, Andreas Freymann and Stefan Wagner", "title": "On Observability and Monitoring of Distributed Systems: An Industry\n  Interview Study", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-33702-5_3", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Business success of companies heavily depends on the availability and\nperformance of their client applications. Due to modern development paradigms\nsuch as DevOps and microservice architectural styles, applications are\ndecoupled into services with complex interactions and dependencies. Although\nthese paradigms enable individual development cycles with reduced delivery\ntimes, they cause several challenges to manage the services in distributed\nsystems. One major challenge is to observe and monitor such distributed\nsystems. This paper provides a qualitative study to understand the challenges\nand good practices in the field of observability and monitoring of distributed\nsystems. In 28 semi-structured interviews with software professionals we\ndiscovered increasing complexity and dynamics in that field. Especially\nobservability becomes an essential prerequisite to ensure stable services and\nfurther development of client applications. However, the participants mentioned\na discrepancy in the awareness regarding the importance of the topic, both from\nthe management as well as from the developer perspective. Besides technical\nchallenges, we identified a strong need for an organizational concept including\nstrategy, roles and responsibilities. Our results support practitioners in\ndeveloping and implementing systematic observability and monitoring for\ndistributed systems.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 07:22:54 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Niedermaier", "Sina", ""], ["Koetter", "Falko", ""], ["Freymann", "Andreas", ""], ["Wagner", "Stefan", ""]]}, {"id": "1907.12275", "submitter": "Wouter Klijn", "authors": "Wouter Klijn, Sandra Diaz-Pier, Abigail Morrison, Alexander Peyser", "title": "Staged deployment of interactive multi-application HPC workflows", "comments": "7 pages, 3 figures, The 2019 International Conference on High\n  Performance Computing & Simulation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Running scientific workflows on a supercomputer can be a daunting task for a\nscientific domain specialist. Workflow management solutions (WMS) are a\nstandard method for reducing the complexity of application deployment on high\nperformance computing (HPC) infrastructure. We introduce the design for a\nmiddleware system that extends and combines the functionality from existing\nsolutions in order to create a high-level, staged user-centric\noperation/deployment model. This design addresses the requirements of several\nuse cases in the life sciences, with a focus on neuroscience. In this\nmanuscript we focus on two use cases: 1) three coupled neuronal simulators (for\nthree different space/time scales) with in-transit visualization and 2) a\nclosed-loop workflow optimized by machine learning, coupling a robot with a\nneural network simulation. We provide a detailed overview of the\napplication-integrated monitoring in relationship with the HPC job. We present\nhere a novel usage model for large scale interactive multi-application\nworkflows running on HPC systems which aims at reducing the complexity of\ndeployment and execution, thus enabling new science.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 08:41:16 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Klijn", "Wouter", ""], ["Diaz-Pier", "Sandra", ""], ["Morrison", "Abigail", ""], ["Peyser", "Alexander", ""]]}, {"id": "1907.12443", "submitter": "Hoa Vu", "authors": "Hsin-Hao Su, Hoa T. Vu", "title": "Distributed Dense Subgraph Detection and Low Outdegree Orientation", "comments": "To appear in DISC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The densest subgraph problem, introduced in the 80s by Picard and Queyranne\nas well as Goldberg, is a classic problem in combinatorial optimization with a\nwide range of applications. The lowest outdegree orientation problem is known\nto be its dual problem. We study both the problem of finding dense subgraphs\nand the problem of computing a low outdegree orientation in the distributed\nsettings.\n  Suppose $G=(V,E)$ is the underlying network as well as the input graph. Let\n$D$ denote the density of the maximum density subgraph of $G$. Our main results\nare as follows.\n  Given a value $\\tilde{D} \\leq D$ and $0 < \\epsilon < 1$, we show that a\nsubgraph with density at least $(1-\\epsilon)\\tilde{D}$ can be identified\ndeterministically in $O((\\log n) / \\epsilon)$ rounds in the LOCAL model. We\nalso present a lower bound showing that our result for the LOCAL model is tight\nup to an $O(\\log n)$ factor.\n  In the CONGEST model, we show that such a subgraph can be identified in\n$O((\\log^3 n) / \\epsilon^3)$ rounds with high probability. Our techniques also\nlead to an $O(diameter + (\\log^4 n)/\\epsilon^4)$-round algorithm that yields a\n$1-\\epsilon$ approximation to the densest subgraph. This improves upon the\nprevious $O(diameter /\\epsilon \\cdot \\log n)$-round algorithm by Das Sarma et\nal. [DISC 2012] that only yields a $1/2-\\epsilon$ approximation.\n  Given an integer $\\tilde{D} \\geq D$ and $\\Omega(1/\\tilde{D}) < \\epsilon <\n1/4$, we give a deterministic, $\\tilde{O}((\\log^2 n) /\\epsilon^2)$-round\nalgorithm in the CONGEST model that computes an orientation where the outdegree\nof every vertex is upper bounded by $(1+\\epsilon)\\tilde{D}$. Previously, the\nbest deterministic algorithm and randomized algorithm by Harris [FOCS 2019] run\nin $\\tilde{O}((\\log^6 n)/ \\epsilon^4)$ rounds and $\\tilde{O}((\\log^3 n)\n/\\epsilon^3)$ rounds respectively and only work in the LOCAL model.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 14:04:17 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 04:39:21 GMT"}, {"version": "v3", "created": "Wed, 22 Jul 2020 09:46:39 GMT"}, {"version": "v4", "created": "Fri, 7 Aug 2020 11:59:32 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Su", "Hsin-Hao", ""], ["Vu", "Hoa T.", ""]]}, {"id": "1907.12500", "submitter": "Bong Jun Choi", "authors": "Duin Baek, Jing Chen, Bong Jun Choi", "title": "Small Profits and Quick Returns: An Incentive Mechanism Design for\n  IoT-based Crowdsourcing under Continuous Platform Competition", "comments": "13 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing can be applied to the Internet-of-Things (IoT) systems to\nprovide more scalable and efficient services to support various tasks. As the\ndriving force of crowdsourcing is the interaction among participants, various\nincentive mechanisms have been proposed to attract and retain a sufficient\nnumber of participants to provide a sustainable crowdsourcing service. However,\nthere exist some gaps between the modeled entities or markets in the existing\nworks and those in reality: 1) \\textit{dichotomous} task valuation and workers'\npunctuality, and 2) crowdsourcing service market \\textit{monopolized} by a\nplatform. To bridge those gaps of such impractical assumption, we model\nworkers' heterogeneous punctuality behavior and task depreciation over time.\nBased on those models, we propose an Expected Social Welfare Maximizing (ESWM)\nmechanism that aims to maximize the expected social welfare by attracting and\nretaining more participants in the long-term, i.e., multiple rounds of\ncrowdsourcing. In the evaluation, we modeled the continuous competition between\nthe ESWM and one of the existing works in both short-term and long-term\nscenarios. Simulation results show that the ESWM mechanism achieves higher\nexpected social welfare and platform utility than the benchmark by attracting\nand retaining more participants. Moreover, we prove that the ESWM mechanism\nachieves the desirable economic properties: individual rationality,\nbudget-balance, computational efficiency, and truthfulness.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 16:03:32 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Baek", "Duin", ""], ["Chen", "Jing", ""], ["Choi", "Bong Jun", ""]]}, {"id": "1907.12505", "submitter": "Joberto Martins Prof. Dr.", "authors": "Pedro F. Moraes and Joberto S. B. Martins", "title": "A Pub/Sub SDN-Integrated Framework for IoT Traffic Orchestration", "comments": "9 pages, 10 figuras", "journal-ref": "ICFNDS 2019 - 3rd International Conference on Future Networks and\n  Distributed Systems, July 01-02, 2019, Paris, FR, ACM, New York, NY, USA, 9\n  pages", "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The Internet of Things (IoT) is advancing and the adoption of\ninternet-connected devices in everyday use is constantly growing. This increase\nnot only affects the traffic from other sources in the network, but also the\ncommunication quality requirements, like Quality of Service (QoS), for the IoT\ndevices and applications. With the rise of dynamic network management and\ndynamic network programming technologies like Software-Defined Networking\n(SDN), traffic management and communication quality requirements can be\ntailored to fit niche use cases and characteristics. We propose a\npublish/subscribe QoS-aware framework (PSIoT-SDN) that orchestrates IoT traffic\nand mediates the allocation of network resources between IoT data aggregators\nand pub/sub consumers. The PSIoT framework allows edge-level QoS control using\nthe features of publish/ subscribe orchestrator at IoT aggregators and, in\naddition, allows network-level QoS control by incorporating SDN features\ncoupled with a bandwidth allocation model for networkwide IoT traffic\nmanagement. The integration of the framework with SDN allows it to dynamically\nreact to bandwidth sharing enabled by the SDN controller, resulting in better\nbandwidth distribution and higher link utilization for IoT traffic.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2019 23:35:44 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Moraes", "Pedro F.", ""], ["Martins", "Joberto S. B.", ""]]}, {"id": "1907.12656", "submitter": "Qiao Kang", "authors": "Qiao Kang, Sunwoo Lee, Kai-yuan Hou, Robert Ross, Ankit Agrawal, Alok\n  Choudhary, Wei-keng Liao", "title": "Improving MPI Collective I/O Performance With Intra-node Request\n  Aggregation", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": "10.1109/TPDS.2020.3000458", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-phase I/O is a well-known strategy for implementing collective MPI-IO\nfunctions. It redistributes I/O requests among the calling processes into a\nform that minimizes the file access costs. As modern parallel computers\ncontinue to grow into the exascale era, the communication cost of such request\nredistribution can quickly overwhelm collective I/O performance. This effect\nhas been observed from parallel jobs that run on multiple compute nodes with a\nhigh count of MPI processes on each node. To reduce the communication cost, we\npresent a new design for collective I/O by adding an extra communication layer\nthat performs request aggregation among processes within the same compute\nnodes. This approach can significantly reduce inter-node communication\ncongestion when redistributing the I/O requests. We evaluate the performance\nand compare with the original two-phase I/O on a Cray XC40 parallel computer\nwith Intel KNL processors. Using I/O patterns from two large-scale production\napplications and an I/O benchmark, we show the performance improvement of up to\n29 times when running 16384 MPI processes on 256 compute nodes.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 21:15:48 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Kang", "Qiao", ""], ["Lee", "Sunwoo", ""], ["Hou", "Kai-yuan", ""], ["Ross", "Robert", ""], ["Agrawal", "Ankit", ""], ["Choudhary", "Alok", ""], ["Liao", "Wei-keng", ""]]}, {"id": "1907.12857", "submitter": "Piotr Krysta", "authors": "Dariusz R. Kowalski and Piotr Krysta", "title": "Deterministic coloring algorithms in the LOCAL model", "comments": "Version date: 10 July 2019; some typos corrected; added explanation\n  p. 5; paper submitted to ACM-SIAM SODA 2020; 13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of bi-chromatic coloring of hypergraphs in the LOCAL\ndistributed model of computation. This problem can easily be solved by a\nrandomized local algorithm with no communication. However, it is not known how\nto solve it deterministically with only a polylogarithmic number of\ncommunication rounds. In this paper we indeed design such a deterministic\nalgorithm that solves this problem with polylogarithmic number of communication\nrounds. This is an almost exponential improvement on the previously known\ndeterministic local algorithms for this problem. Because the bi-chromatic\ncoloring of hypergraphs problem is known to be complete in the class of all\nlocally checkable graph problems, our result implies deterministic local\nalgorithms with polylogarithmic number of communication rounds for all such\nproblems for which an efficient randomized algorithm exists. This solves one of\nthe fundamental open problems in the area of local distributed graph\nalgorithms. By reductions due to Ghaffari, Kuhn and Maus [STOC 2017] this\nimplies such polylogarithmically efficient deterministic local algorithms for\nmany graph problems.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 12:28:52 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 02:40:14 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Kowalski", "Dariusz R.", ""], ["Krysta", "Piotr", ""]]}, {"id": "1907.12874", "submitter": "Boris Krasnopolsky Dr.", "authors": "Boris Krasnopolsky", "title": "Revisiting Performance of BiCGStab Methods for Solving Systems with\n  Multiple Right-Hand Sides", "comments": null, "journal-ref": null, "doi": "10.1016/j.camwa.2019.11.025", "report-no": null, "categories": "math.NA cs.DC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper discusses the efficiency of the classical BiCGStab method and\nseveral of its modifications for solving systems with multiple right-hand side\nvectors. These iterative methods are widely used for solving systems with large\nsparse matrices. The paper presents execution time analytical model for the\ntime to solve the systems. The BiCGStab method and several modifications\nincluding the Reordered BiCGStab and Pipelined BiCGStab methods are analyzed\nand the range of applicability for each method providing the best execution\ntime is highlighted. The results of the analytical model are validated by the\nnumerical experiments and compared with results of other authors. The presented\nresults demonstrate an increasing role of the vector operations when performing\nsimulations with multiple right-hand side vectors. The proposed merging of\nvector operations allows to reduce the memory traffic and improve performance\nof the calculations by about 30%.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 13:09:25 GMT"}, {"version": "v2", "created": "Sat, 14 Sep 2019 23:45:00 GMT"}, {"version": "v3", "created": "Thu, 28 Nov 2019 23:13:03 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Krasnopolsky", "Boris", ""]]}, {"id": "1907.12901", "submitter": "Hao Zheng", "authors": "Yuting Cao, Hao Zheng, Sandip Ray", "title": "A Communication-Centric Observability Selection for Post-Silicon\n  System-on-Chip Integration Debug", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstruction of how components communicate with each other during system\nexecution is crucial for debugging system-on-chip designs. However, limited\nobservability is the major obstacle to the efficient and accurate\nreconstruction in the post-silicon validation stage. This paper addresses that\nproblem by proposing several communication event selection methods guided by\nsystem-level communication protocols. Such methods are optimized for on-chip\ncommunication event tracing infrastructure to enhance observability. The\neffectiveness of these methods are demonstrated with experiments on a\nnon-trivial multicore SoC prototype. The results show that with the proposed\nmethod, more comprehensive information on system internal execution can be\ninferred from traces under limited observability.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 22:23:44 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Cao", "Yuting", ""], ["Zheng", "Hao", ""], ["Ray", "Sandip", ""]]}, {"id": "1907.12916", "submitter": "Subrata Mitra", "authors": "Subrata Mitra, Shanka Subhra Mondal, Nikhil Sheoran, Neeraj Dhake,\n  Ravinder Nehra, Ramanuja Simha", "title": "DeepPlace: Learning to Place Applications in Multi-Tenant Clusters", "comments": "APSys 2019", "journal-ref": null, "doi": "10.1145/3343737.3343741", "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large multi-tenant production clusters often have to handle a variety of jobs\nand applications with a variety of complex resource usage characteristics. It\nis non-trivial and non-optimal to manually create placement rules for\nscheduling that would decide which applications should co-locate. In this\npaper, we present DeepPlace, a scheduler that learns to exploits various\ntemporal resource usage patterns of applications using Deep Reinforcement\nLearning (Deep RL) to reduce resource competition across jobs running in the\nsame machine while at the same time optimizing for overall cluster utilization.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 16:23:30 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Mitra", "Subrata", ""], ["Mondal", "Shanka Subhra", ""], ["Sheoran", "Nikhil", ""], ["Dhake", "Neeraj", ""], ["Nehra", "Ravinder", ""], ["Simha", "Ramanuja", ""]]}, {"id": "1907.12921", "submitter": "Dr. B  Thirumala Rao", "authors": "K.Kavitha, B. Thirumala Rao", "title": "Evaluation of Distance Measures for Feature based Image Registration\n  using AlexNet", "comments": null, "journal-ref": null, "doi": "10.14569/IJACSA.2018.091034", "report-no": null, "categories": "cs.CV cs.DC eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image registration is a classic problem of computer vision with several\napplications across areas like defence, remote sensing, medicine etc. Feature\nbased image registration methods traditionally used hand-crafted feature\nextraction algorithms, which detect key points in an image and describe them\nusing a region around the point. Such features are matched using a threshold\neither on distances or ratio of distances computed between the feature\ndescriptors. Evolution of deep learning, in particular convolution neural\nnetworks, has enabled researchers to address several problems of vision such as\nrecognition, tracking, localization etc. Outputs of convolution layers or fully\nconnected layers of CNN which has been trained for applications like visual\nrecognition are proved to be effective when used as features in other\napplications such as retrieval. In this work, a deep CNN, AlexNet, is used in\nthe place of handcrafted features for feature extraction in the first stage of\nimage registration. However, there is a need to identify a suitable distance\nmeasure and a matching method for effective results. Several distance metrics\nhave been evaluated in the framework of nearest neighbour and nearest neighbour\nratio matching methods using benchmark dataset. Evaluation is done by comparing\nmatching and registration performance using metrics computed from ground truth.\n  Keywords: Distance measures; deep learning; feature detection; feature\ndescriptor; image matching\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2019 09:36:50 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Kavitha", "K.", ""], ["Rao", "B. Thirumala", ""]]}, {"id": "1907.12931", "submitter": "Md Vasimuddin", "authors": "Vasimuddin Md, Sanchit Misra, Heng Li, and Srinivas Aluru", "title": "Efficient Architecture-Aware Acceleration of BWA-MEM for Multicore\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE cs.PF q-bio.GN", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Innovations in Next-Generation Sequencing are enabling generation of DNA\nsequence data at ever faster rates and at very low cost. Large sequencing\ncenters typically employ hundreds of such systems. Such high-throughput and\nlow-cost generation of data underscores the need for commensurate acceleration\nin downstream computational analysis of the sequencing data. A fundamental step\nin downstream analysis is mapping of the reads to a long reference DNA\nsequence, such as a reference human genome. Sequence mapping is a\ncompute-intensive step that accounts for more than 30% of the overall time of\nthe GATK workflow. BWA-MEM is one of the most widely used tools for sequence\nmapping and has tens of thousands of users.\n  In this work, we focus on accelerating BWA-MEM through an efficient\narchitecture aware implementation, while maintaining identical output. The\nvolume of data requires distributed computing environment, usually deploying\nmulticore processors. Since the application can be easily parallelized for\ndistributed memory systems, we focus on performance improvements on a single\nsocket multicore processor. BWA-MEM run time is dominated by three kernels,\ncollectively responsible for more than 85% of the overall compute time. We\nimproved the performance of these kernels by 1) improving cache reuse, 2)\nsimplifying the algorithms, 3) replacing small fragmented memory allocations\nwith a few large contiguous ones, 4) software prefetching, and 5) SIMD\nutilization wherever applicable - and massive reorganization of the source code\nenabling these improvements.\n  As a result, we achieved nearly 2x, 183x, and 8x speedups on the three\nkernels, respectively, resulting in up to 3.5x and 2.4x speedups on end-to-end\ncompute time over the original BWA-MEM on single thread and single socket of\nIntel Xeon Skylake processor. To the best of our knowledge, this is the highest\nreported speedup over BWA-MEM.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 13:18:53 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Md", "Vasimuddin", ""], ["Misra", "Sanchit", ""], ["Li", "Heng", ""], ["Aluru", "Srinivas", ""]]}, {"id": "1907.12947", "submitter": "Saugata Ghose", "authors": "Saugata Ghose, Amirali Boroumand, Jeremie S. Kim, Juan G\\'omez-Luna,\n  Onur Mutlu", "title": "A Workload and Programming Ease Driven Perspective of\n  Processing-in-Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern and emerging applications must process increasingly large volumes\nof data. Unfortunately, prevalent computing paradigms are not designed to\nefficiently handle such large-scale data: the energy and performance costs to\nmove this data between the memory subsystem and the CPU now dominate the total\ncosts of computation. This forces system architects and designers to\nfundamentally rethink how to design computers. Processing-in-memory (PIM) is a\ncomputing paradigm that avoids most data movement costs by bringing computation\nto the data. New opportunities in modern memory systems are enabling\narchitectures that can perform varying degrees of processing inside the memory\nsubsystem. However, there are many practical system-level issues that must be\ntackled to construct PIM architectures, including enabling workloads and\nprogrammers to easily take advantage of PIM. This article examines three key\ndomains of work towards the practical construction and widespread adoption of\nPIM architectures. First, we describe our work on systematically identifying\nopportunities for PIM in real applications, and quantify potential gains for\npopular emerging applications (e.g., machine learning, data analytics, genome\nanalysis). Second, we aim to solve several key issues on programming these\napplications for PIM architectures. Third, we describe challenges that remain\nfor the widespread adoption of PIM.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 20:58:42 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Ghose", "Saugata", ""], ["Boroumand", "Amirali", ""], ["Kim", "Jeremie S.", ""], ["G\u00f3mez-Luna", "Juan", ""], ["Mutlu", "Onur", ""]]}, {"id": "1907.12986", "submitter": "Feras Al-Hawari", "authors": "Feras Al-Hawari, Elias Manolakos", "title": "Runtime QoS service for application-driven adaptation in network\n  computing", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A distributed application executing on a Network of Workstations (NOW) needs\nto be resource state aware to possibly adapt itself accordingly in order to\nkeep satisfying the desired Quality of Service (QoS) demands throughout its\nlifespan. We implemented a QoS service to enable application-driven adaptation\nfor performance and fault tolerance at runtime. The service is associated with\nlightweight middleware that monitors the state and load of all application\nentities (e.g., machines, tasks, and logical network links). Moreover, it makes\nits services available to an application task via an anonymous and simple to\nuse QoS API. We present a Manager-Worker application that uses our fault\ntolerance QoS API to adapt for Worker faults in order to avoid application\ndeadlock at runtime. Moreover, we show how a dynamic application-level\nscheduler can easily utilize the QoS API to find efficient schedules.\nFurthermore, we quantified the overhead of the QoS middleware in various\nscenarios to demonstrate that it has minor impact on the performance of the\napplication it is servicing.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 14:44:24 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Al-Hawari", "Feras", ""], ["Manolakos", "Elias", ""]]}, {"id": "1907.13030", "submitter": "Mathieu Dugr\\'e", "authors": "Mathieu Dugr\\'e, Val\\'erie Hayot-Sasson, Tristan Glatard", "title": "A performance comparison of Dask and Apache Spark for data-intensive\n  neuroimaging pipelines", "comments": "10 pages, 15 figures, 1 tables. To appear in the proceeding of the\n  14th WORKS Workshop on Topics in Workflows in Support of Large-Scale Science,\n  17 November 2019, Denver, CO, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the past few years, neuroimaging has entered the Big Data era due to the\njoint increase in image resolution, data sharing, and study sizes. However, no\nparticular Big Data engines have emerged in this field, and several\nalternatives remain available. We compare two popular Big Data engines with\nPython APIs, Apache Spark and Dask, for their runtime performance in processing\nneuroimaging pipelines. Our evaluation uses two synthetic pipelines processing\nthe 81GB BigBrain image, and a real pipeline processing anatomical data from\nmore than 1,000 subjects. We benchmark these pipelines using various\ncombinations of task durations, data sizes, and numbers of workers, deployed on\nan 8-node (8 cores ea.) compute cluster in Compute Canada's Arbutus cloud. We\nevaluate PySpark's RDD API against Dask's Bag, Delayed and Futures. Results\nshow that despite slight differences between Spark and Dask, both engines\nperform comparably. However, Dask pipelines risk being limited by Python's GIL\ndepending on task type and cluster configuration. In all cases, the major\nlimiting factor was data transfer. While either engine is suitable for\nneuroimaging pipelines, more effort needs to be placed in reducing data\ntransfer time.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 15:42:32 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 19:38:32 GMT"}, {"version": "v3", "created": "Sat, 5 Oct 2019 20:24:36 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Dugr\u00e9", "Mathieu", ""], ["Hayot-Sasson", "Val\u00e9rie", ""], ["Glatard", "Tristan", ""]]}, {"id": "1907.13077", "submitter": "Markus Levonyak", "authors": "Carlos Pachajoa, Markus Levonyak, Wilfried N. Gansterer, Jesper\n  Larsson Tr\\\"aff", "title": "How to Make the Preconditioned Conjugate Gradient Method Resilient\n  Against Multiple Node Failures", "comments": "10 pages, 4 figures, 3 tables", "journal-ref": "Proceedings of the 48th International Conference on Parallel\n  Processing (2019) 67:1-67:10", "doi": "10.1145/3337821.3337849", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study algorithmic approaches for recovering from the failure of several\ncompute nodes in the parallel preconditioned conjugate gradient (PCG) solver on\nlarge-scale parallel computers. In particular, we analyze and extend an exact\nstate reconstruction (ESR) approach, which is based on a method proposed by\nChen (2011). In the ESR approach, the solver keeps redundant information from\nprevious search directions, so that the solver state can be fully reconstructed\nif a node fails unexpectedly. ESR does not require checkpointing or external\nstorage for saving dynamic solver data and has low overhead compared to the\nfailure-free situation.\n  In this paper, we improve the fault tolerance of the PCG algorithm based on\nthe ESR approach. In particular, we support recovery from simultaneous or\noverlapping failures of several nodes for general sparsity patterns of the\nsystem matrix, which cannot be handled by Chen's method. For this purpose, we\nrefine the strategy for how to store redundant information across nodes. We\nanalyze and implement our new method and perform numerical experiments with\nlarge sparse matrices from real-world applications on 128 nodes of the Vienna\nScientific Cluster (VSC). For recovering from three simultaneous node failures\nwe observe average runtime overheads between only 2.8% and 55.0%. The overhead\nof the improved resilience depends on the sparsity pattern of the system\nmatrix.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 17:01:33 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Pachajoa", "Carlos", ""], ["Levonyak", "Markus", ""], ["Gansterer", "Wilfried N.", ""], ["Tr\u00e4ff", "Jesper Larsson", ""]]}, {"id": "1907.13119", "submitter": "Francisco Maturana", "authors": "Francisco Maturana, K. V. Rashmi", "title": "Convertible Codes: Efficient Conversion of Coded Data in Distributed\n  Storage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale distributed storage systems typically use erasure codes to\nprovide durability of data in the face of failures. A set of $k$ blocks to be\nstored is encoded using an $[n, k]$ code to generate $n$ blocks that are then\nstored on different storage nodes. The redundancy configuration is chosen based\non the failure rates of storage devices, and is typically kept constant.\nHowever, a recent work by Kadekodi et al. shows that the failure rate of\nstorage devices vary significantly over time, and that adapting the redundancy\nconfiguration in response to such variations provides significant benefits.\nConverting the redundancy configuration of already encoded data by re-encoding\nrequires significant overhead on resources such as accesses, device IO, network\nbandwidth, and compute cycles.\n  In this work, we first present a framework to formalize the notion of code\nconversion: the process of converting data encoded with an $[n^I, k^I]$ code\ninto data encoded with an $[n^F, k^F]$ code while maintaining desired\ndecodability properties, such as the maximum-distance-separable (MDS) property.\nWe then introduce convertible codes, a new class of codes that allow for code\nconversions in a resource-efficient manner. For an important parameter regime\n(which we call the merge regime) along with the linearity and MDS decodability\nconstraint, we prove tight bounds on the number of nodes accessed during code\nconversion. In particular, our achievability result is an explicit construction\nof MDS convertible codes that are optimal for all parameter values in the merge\nregime albeit with a high field size. We then present explicit low-field-size\nconstructions of optimal MDS convertible codes for a broad range of parameters\nin the merge regime. Our results thus show that it is indeed possible to\nachieve code conversions with significantly lesser resources as compared to\nre-encoding.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 17:59:35 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Maturana", "Francisco", ""], ["Rashmi", "K. V.", ""]]}, {"id": "1907.13208", "submitter": "Donghui Yan", "authors": "Donghui Yan, Ying Xu", "title": "Learning over inherently distributed data", "comments": "26 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent decades have seen a surge of interests in distributed computing.\nExisting work focus primarily on either distributed computing platforms, data\nquery tools, or, algorithms to divide big data and conquer at individual\nmachines etc. It is, however, increasingly often that the data of interest are\ninherently distributed, i.e., data are stored at multiple distributed sites due\nto diverse collection channels, business operations etc. We propose to enable\nlearning and inference in such a setting via a general framework based on the\ndistortion minimizing local transformations. This framework only requires a\nsmall amount of local signatures to be shared among distributed sites,\neliminating the need of having to transmitting big data. Computation can be\ndone very efficiently via parallel local computation. The error incurred due to\ndistributed computing vanishes when increasing the size of local signatures. As\nthe shared data need not be in their original form, data privacy may also be\npreserved. Experiments on linear (logistic) regression and Random Forests have\nshown promise of this approach. This framework is expected to apply to a\ngeneral class of tools in learning and inference with the continuity property.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 20:11:19 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Yan", "Donghui", ""], ["Xu", "Ying", ""]]}, {"id": "1907.13218", "submitter": "Ghareeb Falazi", "authors": "Ghareeb Falazi, Vikas Khinchi, Uwe Breitenb\\\"ucher, Frank Leymann", "title": "Transactional Properties of Permissioned Blockchains", "comments": "12 pages. This is a pre-print of an article published in SICS\n  Software-Intensive Cyber-Physical Systems. The final authenticated version is\n  available online at: https://doi.org/10.1007/s00450-019-00411-y", "journal-ref": null, "doi": "10.1007/s00450-019-00411-y", "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional distributed transaction processing (TP) systems, such as\nreplicated databases, faced difficulties in getting wide adoption for scenarios\nof enterprise integration due to the level of mutual trust required.\nIronically, public blockchains, which promised to solve the problem of mutual\ntrust in collaborative processes, suffer from issues like scalability,\nprobabilistic transaction finality, and lack of data confidentiality. To tackle\nthese issues, permissioned blockchains were introduced as an alternative\napproach combining the positives of the two worlds and avoiding their\ndrawbacks. However, no sufficient analysis has been done to emphasize their\nactual capabilities regarding TP. In this paper, we identify a suitable\ncollection of TP criteria to analyze permissioned blockchains and apply them to\na prominent set of these systems. Finally, we compare the derived properties\nand provide general conclusions.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 20:53:31 GMT"}, {"version": "v2", "created": "Wed, 28 Aug 2019 11:22:20 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Falazi", "Ghareeb", ""], ["Khinchi", "Vikas", ""], ["Breitenb\u00fccher", "Uwe", ""], ["Leymann", "Frank", ""]]}, {"id": "1907.13232", "submitter": "Mikhail Nesterenko", "authors": "Shishir Rai, Kendric Hood, Mikhail Nesterenko, and Gokarna Sharma", "title": "Blockguard: Adaptive Blockchain Security", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of varying the security of blockchain transactions\naccording to their importance. This adaptive security is achieved by using\nvariable size consensus committees. To improve performance, such committees\nfunction concurrently. We present two algorithms that allow adaptive security\nby forming concurrent variable size consensus committees on demand. One is\nbased on a single joint blockchain, the other is based on separate sharded\nblockchains. For in-committee consensus, our algorithms may use various\navailable byzantine-robust fault tolerant algorithms (BFT). We implement\nsynchronous BFT, asynchronous BFT and proof-of-work consensus. We thoroughly\nevaluate the performance of our adaptive security algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 21:32:43 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Rai", "Shishir", ""], ["Hood", "Kendric", ""], ["Nesterenko", "Mikhail", ""], ["Sharma", "Gokarna", ""]]}, {"id": "1907.13257", "submitter": "Saptadeep Pal", "authors": "Saptadeep Pal and Eiman Ebrahimi and Arslan Zulfiqar and Yaosheng Fu\n  and Victor Zhang and Szymon Migacz and David Nellans and Puneet Gupta", "title": "Optimizing Multi-GPU Parallelization Strategies for Deep Learning\n  Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deploying deep learning (DL) models across multiple compute devices to train\nlarge and complex models continues to grow in importance because of the demand\nfor faster and more frequent training. Data parallelism (DP) is the most widely\nused parallelization strategy, but as the number of devices in data parallel\ntraining grows, so does the communication overhead between devices.\nAdditionally, a larger aggregate batch size per step leads to statistical\nefficiency loss, i.e., a larger number of epochs are required to converge to a\ndesired accuracy. These factors affect overall training time and beyond a\ncertain number of devices, the speedup from leveraging DP begins to scale\npoorly. In addition to DP, each training step can be accelerated by exploiting\nmodel parallelism (MP). This work explores hybrid parallelization, where each\ndata parallel worker is comprised of more than one device, across which the\nmodel dataflow graph (DFG) is split using MP. We show that at scale, hybrid\ntraining will be more effective at minimizing end-to-end training time than\nexploiting DP alone. We project that for Inception-V3, GNMT, and BigLSTM, the\nhybrid strategy provides an end-to-end training speedup of at least 26.5%, 8%,\nand 22% respectively compared to what DP alone can achieve at scale.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 23:20:50 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Pal", "Saptadeep", ""], ["Ebrahimi", "Eiman", ""], ["Zulfiqar", "Arslan", ""], ["Fu", "Yaosheng", ""], ["Zhang", "Victor", ""], ["Migacz", "Szymon", ""], ["Nellans", "David", ""], ["Gupta", "Puneet", ""]]}, {"id": "1907.13264", "submitter": "Shaikh Arifuzzaman", "authors": "Janak Dahal, Elias Ioup, Shaikh Arifuzzaman, Mahdi Abdelguerfi", "title": "Distributed Streaming Analytics on Large-scale Oceanographic Data using\n  Apache Spark", "comments": "Preprint, 12 pages, Big Data and Scalable Computing (BDSC) research\n  group, Computer Science, University of New Orleans", "journal-ref": null, "doi": null, "report-no": "BDSC-19-01-01", "categories": "cs.DC cs.DB cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Real-world data from diverse domains require real-time scalable analysis.\nLarge-scale data processing frameworks or engines such as Hadoop fall short\nwhen results are needed on-the-fly. Apache Spark's streaming library is\nincreasingly becoming a popular choice as it can stream and analyze a\nsignificant amount of data. In this paper, we analyze large-scale geo-temporal\ndata collected from the USGODAE (United States Global Ocean Data Assimilation\nExperiment) data catalog, and showcase and assess the ability of Spark stream\nprocessing. We measure the latency of streaming and monitor scalability by\nadding and removing nodes in the middle of a streaming job. We also verify the\nfault tolerance by stopping nodes in the middle of a job and making sure that\nthe job is rescheduled and completed on other nodes. We design a full-stack\napplication that automates data collection, data processing and visualizing the\nresults. We also use Google Maps API to visualize results by color coding the\nworld map with values from various analytics.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 00:00:53 GMT"}, {"version": "v2", "created": "Thu, 1 Aug 2019 01:18:03 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Dahal", "Janak", ""], ["Ioup", "Elias", ""], ["Arifuzzaman", "Shaikh", ""], ["Abdelguerfi", "Mahdi", ""]]}, {"id": "1907.13306", "submitter": "Bing Lin", "authors": "Bin Lin, Yinhao Huang, Jianshan Zhang, Junqin Hu, Xing Chen, Jun Li", "title": "Cost-Driven Offloading for DNN-based Applications over Cloud, Edge and\n  End Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC eess.SP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Currently, deep neural networks (DNNs) have achieved a great success in\nvarious applications. Traditional deployment for DNNs in the cloud may incur a\nprohibitively serious delay in transferring input data from the end devices to\nthe cloud. To address this problem, the hybrid computing environments,\nconsisting of the cloud, edge and end devices, are adopted to offload DNN\nlayers by combining the larger layers (more amount of data) in the cloud and\nthe smaller layers (less amount of data) at the edge and end devices. A key\nissue in hybrid computing environments is how to minimize the system cost while\naccomplishing the offloaded layers with their deadline constraints. In this\npaper, a self-adaptive discrete particle swarm optimization (PSO) algorithm\nusing the genetic algorithm (GA) operators was proposed to reduce the system\ncost caused by data transmission and layer execution. This approach considers\nthe characteristics of DNNs partitioning and layers offloading over the cloud,\nedge and end devices. The mutation operator and crossover operator of GA were\nadopted to avert the premature convergence of PSO, which distinctly reduces the\nsystem cost through enhanced population diversity of PSO. The proposed\noffloading strategy is compared with benchmark solutions, and the results show\nthat our strategy can effectively reduce the cost of offloading for DNN-based\napplications over the cloud, edge and end devices relative to the benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 04:54:47 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Lin", "Bin", ""], ["Huang", "Yinhao", ""], ["Zhang", "Jianshan", ""], ["Hu", "Junqin", ""], ["Chen", "Xing", ""], ["Li", "Jun", ""]]}, {"id": "1907.13444", "submitter": "Vladimir Voloshinov", "authors": "Alexander Sokolov, Vladimir Voloshinov", "title": "Balanced Identification as an Intersection of Optimization and\n  Distributed Computing", "comments": "19 pages, 8 figures, 1 table, 32 references. Due to delay in\n  publication (through no our fault) we uploaded revised text for reviewers of\n  subsequent papers on the subject", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technology of formal quantitative estimation of the conformity of the\nmathematical models to the available dataset is presented. Main purpose of the\ntechnology is to make easier the model selection decision-making process for\nthe researcher. The technology is a combination of approaches from the areas of\ndata analysis, optimization and distributed computing including:\ncross-validation and regularization methods, algebraic modelling in\noptimization and methods of optimization, automatic discretization of\ndifferential and integral equation, optimization REST-services. The technology\nis illustrated by a demo case study. General mathematical formulation of the\nmethod is presented. It is followed by description of the main aspects of\nalgorithmic and software implementation. Success story list of the presented\napproach already is rather long. Nevertheless, domain of applicability and\nimportant unresolved issues are discussed.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 12:23:33 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 11:29:24 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Sokolov", "Alexander", ""], ["Voloshinov", "Vladimir", ""]]}, {"id": "1907.13531", "submitter": "Patrik Keller", "authors": "Patrik Keller and Rainer B\\\"ohme", "title": "HotPoW: Finality from Proof-of-Work Quorums", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental conflict of many proof-of-work systems is that they want to\nachieve inclusiveness and security at the same time. We analyze and resolve\nthis conflict with a theory of proof-of-work quorums, which enables a new\nbridge between Byzantine and Nakamoto consensus. The theory yields stochastic\nuniqueness of quorums as a function of a security parameter. We employ the\ntheory in HotPoW, a scalable permissionless distributed log protocol that\nsupports finality based on the pipelined three-phase commit previously\npresented for HotStuff. We evaluate HotPoW and variants with adversarial\nmodifications by simulation. Results show that the protocol can tolerate\nnetwork latency, churn, and targeted attacks on consistency and liveness with a\nsmall storage overhead compared to plain Nakamoto consensus and less complexity\nthan protocols that rely on sidechains for finality.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 14:40:16 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2019 12:15:59 GMT"}, {"version": "v3", "created": "Fri, 21 Feb 2020 14:15:30 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Keller", "Patrik", ""], ["B\u00f6hme", "Rainer", ""]]}, {"id": "1907.13600", "submitter": "Marcelo Ponce", "authors": "Marcelo Ponce, Ramses van Zon, Scott Northrup, Daniel Gruner, Joseph\n  Chen, Fatih Ertinaz, Alexey Fedoseev, Leslie Groer, Fei Mao, Bruno C. Mundim,\n  Mike Nolta, Jaime Pinto, Marco Saldarriaga, Vladimir Slavnic, Erik Spence,\n  Ching-Hsing Yu, W. Richard Peltier", "title": "Deploying a Top-100 Supercomputer for Large Parallel Workloads: the\n  Niagara Supercomputer", "comments": "PEARC'19: \"Practice and Experience in Advanced Research Computing\",\n  July 28-August 1, 2019, Chicago, IL, USA", "journal-ref": null, "doi": "10.1145/3332186.3332195", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Niagara is currently the fastest supercomputer accessible to academics in\nCanada. It was deployed at the beginning of 2018 and has been serving the\nresearch community ever since. This homogeneous 60,000-core cluster, owned by\nthe University of Toronto and operated by SciNet, was intended to enable large\nparallel jobs and has a measured performance of 3.02 petaflops, debuting at #53\nin the June 2018 TOP500 list. It was designed to optimize throughput of a range\nof scientific codes running at scale, energy efficiency, and network and\nstorage performance and capacity. It replaced two systems that SciNet operated\nfor over 8 years, the Tightly Coupled System (TCS) and the General Purpose\nCluster (GPC). In this paper we describe the transition process from these two\nsystems, the procurement and deployment processes, as well as the unique\nfeatures that make Niagara a one-of-a-kind machine in Canada.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 17:06:15 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Ponce", "Marcelo", ""], ["van Zon", "Ramses", ""], ["Northrup", "Scott", ""], ["Gruner", "Daniel", ""], ["Chen", "Joseph", ""], ["Ertinaz", "Fatih", ""], ["Fedoseev", "Alexey", ""], ["Groer", "Leslie", ""], ["Mao", "Fei", ""], ["Mundim", "Bruno C.", ""], ["Nolta", "Mike", ""], ["Pinto", "Jaime", ""], ["Saldarriaga", "Marco", ""], ["Slavnic", "Vladimir", ""], ["Spence", "Erik", ""], ["Yu", "Ching-Hsing", ""], ["Peltier", "W. Richard", ""]]}]