[{"id": "1103.0086", "submitter": "Xin Liu", "authors": "Xin Liu and Gilles Tredan and Anwitaman Datta", "title": "A generic trust framework for large-scale open systems using machine\n  learning", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many large scale distributed systems and on the web, agents need to\ninteract with other unknown agents to carry out some tasks or transactions. The\nability to reason about and assess the potential risks in carrying out such\ntransactions is essential for providing a safe and reliable environment. A\ntraditional approach to reason about the trustworthiness of a transaction is to\ndetermine the trustworthiness of the specific agent involved, derived from the\nhistory of its behavior. As a departure from such traditional trust models, we\npropose a generic, machine learning approach based trust framework where an\nagent uses its own previous transactions (with other agents) to build a\nknowledge base, and utilize this to assess the trustworthiness of a transaction\nbased on associated features, which are capable of distinguishing successful\ntransactions from unsuccessful ones. These features are harnessed using\nappropriate machine learning algorithms to extract relationships between the\npotential transaction and previous transactions. The trace driven experiments\nusing real auction dataset show that this approach provides good accuracy and\nis highly efficient compared to other trust mechanisms, especially when\nhistorical information of the specific agent is rare, incomplete or inaccurate.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2011 06:03:15 GMT"}], "update_date": "2011-03-02", "authors_parsed": [["Liu", "Xin", ""], ["Tredan", "Gilles", ""], ["Datta", "Anwitaman", ""]]}, {"id": "1103.0759", "submitter": "Fangfei Zhou", "authors": "Fangfei Zhou, Manish Goel, Peter Desnoyers, Ravi Sundaram", "title": "Scheduler Vulnerabilities and Attacks in Cloud Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In hardware virtualization a hypervisor provides multiple Virtual Machines\n(VMs) on a single physical system, each executing a separate operating system\ninstance. The hypervisor schedules execution of these VMs much as the scheduler\nin an operating system does, balancing factors such as fairness and I/O\nperformance. As in an operating system, the scheduler may be vulnerable to\nmalicious behavior on the part of users seeking to deny service to others or\nmaximize their own resource usage.\n  Recently, publically available cloud computing services such as Amazon EC2\nhave used virtualization to provide customers with virtual machines running on\nthe provider's hardware, typically charging by wall clock time rather than\nresources consumed. Under this business model, manipulation of the scheduler\nmay allow theft of service at the expense of other customers, rather than\nmerely reallocating resources within the same administrative domain.\n  We describe a flaw in the Xen scheduler allowing virtual machines to consume\nalmost all CPU time, in preference to other users, and demonstrate kernel-based\nand user-space versions of the attack. We show results demonstrating the\nvulnerability in the lab, consuming as much as 98% of CPU time regardless of\nfair share, as well as on Amazon EC2, where Xen modifications protect other\nusers but still allow theft of service. In case of EC2, following the\nresponsible disclosure model, we have reported this vulnerability to Amazon;\nthey have since implemented a fix that we have tested and verified (See\nAppendix B). We provide a novel analysis of the necessary conditions for such\nattacks, and describe scheduler modifications to eliminate the vulnerability.\n  We present experimental results demonstrating the effectiveness of these\ndefenses while imposing negligible overhead.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2011 19:09:47 GMT"}], "update_date": "2011-03-04", "authors_parsed": [["Zhou", "Fangfei", ""], ["Goel", "Manish", ""], ["Desnoyers", "Peter", ""], ["Sundaram", "Ravi", ""]]}, {"id": "1103.1026", "submitter": "Xu Chen", "authors": "Xu Chen and Jianwei Huang", "title": "Evolutionary Game and Learning for Dynamic Spectrum Access", "comments": "This paper has been withdrawn by the author due to title update! A\n  new version with the updated title \"Evolutionarily Stable Spectrum Access\"\n  will be available at Arxiv", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient dynamic spectrum access mechanism is crucial for improving the\nspectrum utilization. In this paper, we consider the dynamic spectrum access\nmechanism design with both complete and incomplete network information. When\nthe network information is available, we propose an evolutionary spectrum\naccess mechanism. We use the replicator dynamics to study the dynamics of\nchannel selections, and show that the mechanism achieves an equilibrium that is\nan evolutionarily stable strategy and is also max-min fair. With incomplete\nnetwork information, we propose a distributed reinforcement learning mechanism\nfor dynamic spectrum access. Each secondary user applies the maximum likelihood\nestimation method to estimate its expected payoff based on the local\nobservations, and learns to adjust its mixed strategy for channel selections\nadaptively over time. We study the convergence of the learning mechanism based\non the theory of stochastic approximation, and show that it globally converges\nto an approximate Nash equilibrium. Numerical results show that the proposed\nevolutionary spectrum access and distributed reinforcement learning mechanisms\nachieve up to 82% and 70% performance improvement than a random access\nmechanism, respectively, and are robust to random perturbations of channel\nselections.\n", "versions": [{"version": "v1", "created": "Sat, 5 Mar 2011 09:07:53 GMT"}, {"version": "v2", "created": "Wed, 11 Apr 2012 20:57:15 GMT"}], "update_date": "2012-04-13", "authors_parsed": [["Chen", "Xu", ""], ["Huang", "Jianwei", ""]]}, {"id": "1103.1207", "submitter": "Deepti Sharma", "authors": "Ms. Deepti Sharma and Ms. Archana B.Saxena", "title": "Framework to Solve Load Balancing Problem in Heterogeneous Web Servers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For popular websites most important concern is to handle incoming load\ndynamically among web servers, so that they can respond to their client without\nany wait or failure. Different websites use different strategies to distribute\nload among web servers but most of the schemes concentrate on only one factor\nthat is number of requests, but none of the schemes consider the point that\ndifferent type of requests will require different level of processing efforts\nto answer, status record of all the web servers that are associated with one\ndomain name and mechanism to handle a situation when one of the servers is not\nworking. Therefore, there is a fundamental need to develop strategy for dynamic\nload allocation on web side. In this paper, an effort has been made to\nintroduce a cluster based frame work to solve load distribution problem. This\nframework aims to distribute load among clusters on the basis of their\noperational capabilities. Moreover, the experimental results are shown with the\nhelp of example, algorithm and analysis of the algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2011 07:40:22 GMT"}], "update_date": "2011-03-08", "authors_parsed": [["Sharma", "Ms. Deepti", ""], ["Saxena", "Ms. Archana B.", ""]]}, {"id": "1103.1302", "submitter": "Srivatsan Ravi Mr", "authors": "Petr Kuznetsov and Srivatsan Ravi", "title": "On the Cost of Concurrency in Transactional Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The crux of software transactional memory (STM) is to combine an easy-to-use\nprogramming interface with an efficient utilization of the concurrent-computing\nabilities provided by modern machines. But does this combination come with an\ninherent cost? We evaluate the cost of concurrency by measuring the amount of\nexpensive synchronization that must be employed in an STM implementation that\nensures positive concurrency, i.e., allows for concurrent transaction\nprocessing in some executions. We focus on two popular progress conditions that\nprovide positive concurrency: progressiveness and permissiveness. We show that\nin permissive STMs, providing a very high degree of concurrency, a transaction\nperforms a linear number of expensive synchronization patterns with respect to\nits read-set size. In contrast, progressive STMs provide a very small degree of\nconcurrency but, as we demonstrate, can be implemented using at most one\nexpensive synchronization pattern per transaction. However, we show that even\nin progressive STMs, a transaction has to \"protect\" (e.g., by using locks or\nstrong synchronization primitives) a linear amount of data with respect to its\nwrite-set size. Our results suggest that looking for high degrees of\nconcurrency in STM implementations may bring a considerable synchronization\ncost.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2011 15:37:44 GMT"}, {"version": "v2", "created": "Thu, 10 Mar 2011 15:17:22 GMT"}, {"version": "v3", "created": "Wed, 20 Apr 2011 14:27:31 GMT"}, {"version": "v4", "created": "Fri, 6 May 2011 10:07:23 GMT"}, {"version": "v5", "created": "Thu, 19 May 2011 13:36:52 GMT"}, {"version": "v6", "created": "Fri, 20 May 2011 09:46:03 GMT"}, {"version": "v7", "created": "Wed, 22 Jun 2011 11:17:57 GMT"}, {"version": "v8", "created": "Fri, 29 Jul 2011 12:45:34 GMT"}, {"version": "v9", "created": "Thu, 27 Jun 2013 18:32:44 GMT"}], "update_date": "2013-06-28", "authors_parsed": [["Kuznetsov", "Petr", ""], ["Ravi", "Srivatsan", ""]]}, {"id": "1103.2068", "submitter": "Tamara Kolda", "authors": "Justin D. Basilico and M. Arthur Munson and Tamara G. Kolda and Kevin\n  R. Dixon and W. Philip Kegelmeyer", "title": "COMET: A Recipe for Learning and Using Large Ensembles on Massive Data", "comments": null, "journal-ref": "ICDM 2011: Proceedings of the 2011 IEEE International Conference\n  on Data Mining, pp. 41-50, 2011", "doi": "10.1109/ICDM.2011.39", "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  COMET is a single-pass MapReduce algorithm for learning on large-scale data.\nIt builds multiple random forest ensembles on distributed blocks of data and\nmerges them into a mega-ensemble. This approach is appropriate when learning\nfrom massive-scale data that is too large to fit on a single machine. To get\nthe best accuracy, IVoting should be used instead of bagging to generate the\ntraining subset for each decision tree in the random forest. Experiments with\ntwo large datasets (5GB and 50GB compressed) show that COMET compares favorably\n(in both accuracy and training time) to learning on a subsample of data using a\nserial algorithm. Finally, we propose a new Gaussian approach for lazy ensemble\nevaluation which dynamically decides how many ensemble members to evaluate per\ndata point; this can reduce evaluation cost by 100X or more.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2011 16:15:42 GMT"}, {"version": "v2", "created": "Thu, 8 Sep 2011 16:20:45 GMT"}], "update_date": "2013-03-06", "authors_parsed": [["Basilico", "Justin D.", ""], ["Munson", "M. Arthur", ""], ["Kolda", "Tamara G.", ""], ["Dixon", "Kevin R.", ""], ["Kegelmeyer", "W. Philip", ""]]}, {"id": "1103.2215", "submitter": "Xin Liu", "authors": "Xin Liu and Anwitaman Datta and Krzysztof Rzadca", "title": "Trust beyond reputation: A computational trust model based on\n  stereotypes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models of computational trust support users in taking decisions. They are\ncommonly used to guide users' judgements in online auction sites; or to\ndetermine quality of contributions in Web 2.0 sites. However, most existing\nsystems require historical information about the past behavior of the specific\nagent being judged. In contrast, in real life, to anticipate and to predict a\nstranger's actions in absence of the knowledge of such behavioral history, we\noften use our \"instinct\"- essentially stereotypes developed from our past\ninteractions with other \"similar\" persons. In this paper, we propose\nStereoTrust, a computational trust model inspired by stereotypes as used in\nreal-life. A stereotype contains certain features of agents and an expected\noutcome of the transaction. When facing a stranger, an agent derives its trust\nby aggregating stereotypes matching the stranger's profile. Since stereotypes\nare formed locally, recommendations stem from the trustor's own personal\nexperiences and perspective. Historical behavioral information, when available,\ncan be used to refine the analysis. According to our experiments using\nEpinions.com dataset, StereoTrust compares favorably with existing trust models\nthat use different kinds of information and more complete historical\ninformation.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2011 08:15:07 GMT"}, {"version": "v2", "created": "Thu, 5 May 2011 03:50:46 GMT"}, {"version": "v3", "created": "Sun, 15 Jul 2012 14:07:02 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Liu", "Xin", ""], ["Datta", "Anwitaman", ""], ["Rzadca", "Krzysztof", ""]]}, {"id": "1103.2289", "submitter": "Venkatesh Saligrama", "authors": "Venkatesh Saligrama and Murat Alanyali", "title": "A Token Based Algorithm to Distributed Computation in Sensor Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.IT cs.SY math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider distributed algorithms for data aggregation and function\ncomputation in sensor networks. The algorithms perform pairwise computations\nalong edges of an underlying communication graph. A token is associated with\neach sensor node, which acts as a transmission permit. Nodes with active tokens\nhave transmission permits; they generate messages at a constant rate and send\neach message to a randomly selected neighbor. By using different strategies to\ncontrol the transmission permits we can obtain tradeoffs between message and\ntime complexity. Gossip corresponds to the case when all nodes have permits all\nthe time. We study algorithms where permits are revoked after transmission and\nrestored upon reception. Examples of such algorithms include Simple-Random\nWalk(SRW), Coalescent-Random-Walk(CRW) and Controlled Flooding(CFLD) and their\nhybrid variants. SRW has a single node permit, which is passed on in the\nnetwork. CRW, initially initially has a permit for each node but these permits\nare revoked gradually. The final result for SRW and CRW resides at a single(or\nfew) random node(s) making a direct comparison with GOSSIP difficult. A hybrid\ntwo-phase algorithm switching from CRW to CFLD at a suitable pre-determined\ntime can be employed to achieve consensus. We show that such hybrid variants\nachieve significant gains in both message and time complexity. The per-node\nmessage complexity for n-node graphs, such as 2D mesh, torii, and Random\ngeometric graphs, scales as $O(polylog(n))$ and the corresponding time\ncomplexity scales as O(n). The reduced per-node message complexity leads to\nreduced energy utilization in sensor networks.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2011 15:08:12 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Saligrama", "Venkatesh", ""], ["Alanyali", "Murat", ""]]}, {"id": "1103.2408", "submitter": "Sandeep Tata", "authors": "Jun Rao (LinkedIn), Eugene J. Shekita (IBM Research), Sandeep Tata\n  (IBM Research)", "title": "Using Paxos to Build a Scalable, Consistent, and Highly Available\n  Datastore", "comments": "VLDB2011", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 4, No. 4, pp.\n  243-254 (2011)", "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spinnaker is an experimental datastore that is designed to run on a large\ncluster of commodity servers in a single datacenter. It features key-based\nrange partitioning, 3-way replication, and a transactional get-put API with the\noption to choose either strong or timeline consistency on reads. This paper\ndescribes Spinnaker's Paxos-based replication protocol. The use of Paxos\nensures that a data partition in Spinnaker will be available for reads and\nwrites as long a majority of its replicas are alive. Unlike traditional\nmaster-slave replication, this is true regardless of the failure sequence that\noccurs. We show that Paxos replication can be competitive with alternatives\nthat provide weaker consistency guarantees. Compared to an eventually\nconsistent datastore, we show that Spinnaker can be as fast or even faster on\nreads and only 5% to 10% slower on writes.\n", "versions": [{"version": "v1", "created": "Sat, 12 Mar 2011 01:06:32 GMT"}], "update_date": "2011-03-15", "authors_parsed": [["Rao", "Jun", "", "LinkedIn"], ["Shekita", "Eugene J.", "", "IBM Research"], ["Tata", "Sandeep", "", "IBM Research"]]}, {"id": "1103.2590", "submitter": "Rajkumar Buyya", "authors": "Yi Wei, Karthik Sukumar, Christian Vecchiola, Dileban Karunamoorthy\n  and Rajkumar Buyya", "title": "Aneka Cloud Application Platform and Its Integration with Windows Azure", "comments": "30 pages, 24 figures", "journal-ref": "Cloud Computing: Methodology, Systems, and Applications, L. Wang,\n  Rajiv Ranjan, Jinjun Chen, and Boualem Benatallah (eds), ISBN: 9781439856413,\n  CRC Press, Boca Raton, FL, USA, 2011", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aneka is an Application Platform-as-a-Service (Aneka PaaS) for Cloud\nComputing. It acts as a framework for building customized applications and\ndeploying them on either public or private Clouds. One of the key features of\nAneka is its support for provisioning resources on different public Cloud\nproviders such as Amazon EC2, Windows Azure and GoGrid. In this chapter, we\nwill present Aneka platform and its integration with one of the public Cloud\ninfrastructures, Windows Azure, which enables the usage of Windows Azure\nCompute Service as a resource provider of Aneka PaaS. The integration of the\ntwo platforms will allow users to leverage the power of Windows Azure Platform\nfor Aneka Cloud Computing, employing a large number of compute instances to run\ntheir applications in parallel. Furthermore, customers of the Windows Azure\nplatform can benefit from the integration with Aneka PaaS by embracing the\nadvanced features of Aneka in terms of multiple programming models, scheduling\nand management services, application execution services, accounting and pricing\nservices and dynamic provisioning services. Finally, in addition to the Windows\nAzure Platform we will illustrate in this chapter the integration of Aneka PaaS\nwith other public Cloud platforms such as Amazon EC2 and GoGrid, and virtual\nmachine management platforms such as Xen Server. The new support of\nprovisioning resources on Windows Azure once again proves the adaptability,\nextensibility and flexibility of Aneka.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2011 06:38:11 GMT"}], "update_date": "2011-03-15", "authors_parsed": [["Wei", "Yi", ""], ["Sukumar", "Karthik", ""], ["Vecchiola", "Christian", ""], ["Karunamoorthy", "Dileban", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "1103.2626", "submitter": "Kobbi Nissim", "authors": "Amos Beimel, Kobbi Nissim, Eran Omri", "title": "Distributed Private Data Analysis: On Simultaneously Solving How and\n  What", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the combination of two directions in the field of privacy\nconcerning computations over distributed private inputs - secure function\nevaluation (SFE) and differential privacy. While in both the goal is to\nprivately evaluate some function of the individual inputs, the privacy\nrequirements are significantly different. The general feasibility results for\nSFE suggest a natural paradigm for implementing differentially private analyses\ndistributively: First choose what to compute, i.e., a differentially private\nanalysis; Then decide how to compute it, i.e., construct an SFE protocol for\nthis analysis.\n  We initiate an examination whether there are advantages to a paradigm where\nboth decisions are made simultaneously. In particular, we investigate under\nwhich accuracy requirements it is beneficial to adapt this paradigm for\ncomputing a collection of functions including binary sum, gap threshold, and\napproximate median queries. Our results imply that when computing the binary\nsum of $n$ distributed inputs then:\n  * When we require that the error is $o(\\sqrt{n})$ and the number of rounds is\nconstant, there is no benefit in the new paradigm.\n  * When we allow an error of $O(\\sqrt{n})$, the new paradigm yields more\nefficient protocols when we consider protocols that compute symmetric\nfunctions.\n  Our results also yield new separations between the local and global models of\ncomputations for private data analysis.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2011 11:04:45 GMT"}], "update_date": "2011-03-15", "authors_parsed": [["Beimel", "Amos", ""], ["Nissim", "Kobbi", ""], ["Omri", "Eran", ""]]}, {"id": "1103.2635", "submitter": "Lawrence Cayton", "authors": "Lawrence Cayton", "title": "Accelerating Nearest Neighbor Search on Manycore Systems", "comments": null, "journal-ref": "In Proceedings of the 2012 IEEE 26th International Parallel and\n  Distributed Processing Symposium (IPDPS '12). IEEE Computer Society,\n  Washington, DC, USA, 402-413", "doi": "10.1109/IPDPS.2012.45", "report-no": null, "categories": "cs.DB cs.CG cs.DC cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop methods for accelerating metric similarity search that are\neffective on modern hardware. Our algorithms factor into easily parallelizable\ncomponents, making them simple to deploy and efficient on multicore CPUs and\nGPUs. Despite the simple structure of our algorithms, their search performance\nis provably sublinear in the size of the database, with a factor dependent only\non its intrinsic dimensionality. We demonstrate that our methods provide\nsubstantial speedups on a range of datasets and hardware platforms. In\nparticular, we present results on a 48-core server machine, on graphics\nhardware, and on a multicore desktop.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2011 11:39:23 GMT"}, {"version": "v2", "created": "Wed, 30 Mar 2011 18:26:44 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Cayton", "Lawrence", ""]]}, {"id": "1103.2662", "submitter": "Lluis Pamies-Juarez", "authors": "Lluis Pamies-Juarez and Ernst Biersack", "title": "Cost Analysis of Redundancy Schemes for Distributed Storage Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed storage infrastructures require the use of data redundancy to\nachieve high data reliability. Unfortunately, the use of redundancy introduces\nstorage and communication overheads, which can either reduce the overall\nstorage capacity of the system or increase its costs. To mitigate the storage\nand communication overhead, different redundancy schemes have been proposed.\nHowever, due to the great variety of underlaying storage infrastructures and\nthe different application needs, optimizing these redundancy schemes for each\nstorage infrastructure is cumbersome. The lack of rules to determine the\noptimal level of redundancy for each storage configuration leads developers in\nindustry to often choose simpler redundancy schemes, which are usually not the\noptimal ones. In this paper we analyze the cost of different redundancy schemes\nand derive a set of rules to determine which redundancy scheme minimizes the\nstorage and the communication costs for a given system configuration.\nAdditionally, we use simulation to show that theoretically-optimal schemes may\nnot be viable in a realistic setting where nodes can go off-line and repairs\nmay be delayed. In these cases, we identify which are the trade-offs between\nthe storage and communication overheads of the redundancy scheme and its data\nreliability.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2011 13:38:33 GMT"}, {"version": "v2", "created": "Fri, 15 Apr 2011 10:24:54 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Pamies-Juarez", "Lluis", ""], ["Biersack", "Ernst", ""]]}, {"id": "1103.3105", "submitter": "Bingsheng He", "authors": "Bingsheng He (Nanyang Technological University), Jeffrey Xu Yu\n  (Chinese University of Hong Kong)", "title": "High-Throughput Transaction Executions on Graphics Processors", "comments": "VLDB2011", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 4, No. 5, pp.\n  314-325 (2011)", "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  OLTP (On-Line Transaction Processing) is an important business system sector\nin various traditional and emerging online services. Due to the increasing\nnumber of users, OLTP systems require high throughput for executing tens of\nthousands of transactions in a short time period. Encouraged by the recent\nsuccess of GPGPU (General-Purpose computation on Graphics Processors), we\npropose GPUTx, an OLTP engine performing high-throughput transaction executions\non the GPU for in-memory databases. Compared with existing GPGPU studies\nusually optimizing a single task, transaction executions require handling many\nsmall tasks concurrently. Specifically, we propose the bulk execution model to\ngroup multiple transactions into a bulk and to execute the bulk on the GPU as a\nsingle task. The transactions within the bulk are executed concurrently on the\nGPU. We study three basic execution strategies (one with locks and the other\ntwo lock-free), and optimize them with the GPU features including the hardware\nsupport of atomic operations, the massive thread parallelism and the SPMD\n(Single Program Multiple Data) execution. We evaluate GPUTx on a recent NVIDIA\nGPU in comparison with its counterpart on a quad-core CPU. Our experimental\nresults show that optimizations on GPUTx significantly improve the throughput,\nand the optimized GPUTx achieves 4-10 times higher throughput than its\nCPU-based counterpart on public transaction processing benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2011 05:52:51 GMT"}], "update_date": "2011-03-17", "authors_parsed": [["He", "Bingsheng", "", "Nanyang Technological University"], ["Yu", "Jeffrey Xu", "", "Chinese University of Hong Kong"]]}, {"id": "1103.3378", "submitter": "Manjula V", "authors": "V.Manjula, Dr.C.Chellappan", "title": "Replication Attack Mitigations for Static and Mobile WSN", "comments": "12 pages", "journal-ref": "International Journal of Network Security & Its Applications (\n  IJNSA ),March 2011, Volume 3. Number 2", "doi": "10.5121/ijnsa.2011.3210", "report-no": null, "categories": "cs.CR cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Security is important for many sensor network applications. Wireless Sensor\nNetworks (WSN) are often deployed in hostile environments as static or mobile,\nwhere an adversary can physically capture some of the nodes. once a node is\ncaptured, adversary collects all the credentials like keys and identity etc.\nthe attacker can re-program it and replicate the node in order to eavesdrop the\ntransmitted messages or compromise the functionality of the network. Identity\ntheft leads to two types attack: clone and sybil. In particularly a harmful\nattack against sensor networks where one or more node(s) illegitimately claims\nan identity as replicas is known as the node replication attack. The\nreplication attack can be exceedingly injurious to many important functions of\nthe sensor network such as routing, resource allocation, misbehavior detection,\netc. This paper analyzes the threat posed by the replication attack and several\nnovel techniques to detect and defend against the replication attack, and\nanalyzes their effectiveness in both static and mobile WSN.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2011 10:57:01 GMT"}], "update_date": "2011-03-21", "authors_parsed": [["Manjula", "V.", ""], ["Chellappan", "Dr. C.", ""]]}, {"id": "1103.3515", "submitter": "Swan Dubois", "authors": "Swan Dubois (LIP6, INRIA Rocquencourt), Toshimitsu Masuzawa\n  (Department of Information and Computer sciences Osaka University),\n  S\\'ebastien Tixeuil (LIP6)", "title": "Self-Stabilization, Byzantine Containment, and Maximizable Metrics:\n  Necessary Conditions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-stabilization is a versatile approach to fault-tolerance since it\npermits a distributed system to recover from any transient fault that\narbitrarily corrupts the contents of all memories in the system. Byzantine\ntolerance is an attractive feature of distributed systems that permits to cope\nwith arbitrary malicious behaviors. We consider the well known problem of\nconstructing a maximum metric tree in this context. Combining these two\nproperties leads to some impossibility results. In this paper, we provide two\nnecessary conditions to construct maximum metric tree in presence of transients\nand (permanent) Byzantine faults.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2011 20:37:17 GMT"}], "update_date": "2011-03-21", "authors_parsed": [["Dubois", "Swan", "", "LIP6, INRIA Rocquencourt"], ["Masuzawa", "Toshimitsu", "", "Department of Information and Computer sciences Osaka University"], ["Tixeuil", "S\u00e9bastien", "", "LIP6"]]}, {"id": "1103.3671", "submitter": "Peter Robinson", "authors": "Martin Biely, Peter Robinson, Ulrich Schmid", "title": "Easy Impossibility Proofs for k-Set Agreement in Message Passing Systems", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": "Research Report 2/2011, Technische Universitaet Wien", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite of being quite similar agreement problems, consensus and general\nk-set agreement require surprisingly different techniques for proving the\nimpossibility in asynchronous systems with crash failures: Rather than\nrelatively simple bivalence arguments as in the impossibility proof for\nconsensus (= 1-set agreement) in the presence of a single crash failure, known\nproofs for the impossibility of k-set agreement in systems with at least k>1\ncrash failures use algebraic topology or a variant of Sperner's Lemma. In this\npaper, we present a generic theorem for proving the impossibility of k-set\nagreement in various message passing settings, which is based on a simple\nreduction to the consensus impossibility in a certain subsystem. We demonstrate\nthe broad applicability of our result by exploring the\npossibility/impossibility border of k-set agreement in several message-passing\nsystem models: (i) asynchronous systems with crash failures, (ii) partially\nsynchronous processes with (initial) crash failures, and (iii) asynchronous\nsystems augmented with failure detectors. In (i) and (ii), the impossibility\npart is just an instantiation of our main theorem, whereas the possibility of\nachieving k-set agreement in (ii) follows by generalizing the consensus\nalgorithm for initial crashes by Fisher, Lynch and Patterson. In (iii),\napplying our technique yields the exact border for the parameter k where k-set\nagreement is solvable with the failure detector class (Sigma_k,Omega_k), for\n(1<= k<= n-1), of Bonnet and Raynal. Considering that Sigma_k was shown to be\nnecessary for solving k-set agreement, this result yields new insights on the\nquest for the weakest failure detector.\n", "versions": [{"version": "v1", "created": "Fri, 18 Mar 2011 17:31:41 GMT"}, {"version": "v2", "created": "Mon, 21 Mar 2011 02:35:03 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Biely", "Martin", ""], ["Robinson", "Peter", ""], ["Schmid", "Ulrich", ""]]}, {"id": "1103.3737", "submitter": "Zhiying Wang", "authors": "Itzhak Tamo and Zhiying Wang and Jehoshua Bruck", "title": "MDS Array Codes with Optimal Rebuilding", "comments": "14 pages, 4 figures, a short version submitted to ISIT 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MDS array codes are widely used in storage systems to protect data against\nerasures. We address the \\emph{rebuilding ratio} problem, namely, in the case\nof erasures, what is the the fraction of the remaining information that needs\nto be accessed in order to rebuild \\emph{exactly} the lost information? It is\nclear that when the number of erasures equals the maximum number of erasures\nthat an MDS code can correct then the rebuilding ratio is 1 (access all the\nremaining information). However, the interesting (and more practical) case is\nwhen the number of erasures is smaller than the erasure correcting capability\nof the code. For example, consider an MDS code that can correct two erasures:\nWhat is the smallest amount of information that one needs to access in order to\ncorrect a single erasure? Previous work showed that the rebuilding ratio is\nbounded between 1/2 and 3/4, however, the exact value was left as an open\nproblem. In this paper, we solve this open problem and prove that for the case\nof a single erasure with a 2-erasure correcting code, the rebuilding ratio is\n1/2. In general, we construct a new family of $r$-erasure correcting MDS array\ncodes that has optimal rebuilding ratio of $\\frac{1}{r}$ in the case of a\nsingle erasure. Our array codes have efficient encoding and decoding algorithms\n(for the case $r=2$ they use a finite field of size 3) and an optimal update\nproperty.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2011 00:41:12 GMT"}], "update_date": "2011-03-22", "authors_parsed": [["Tamo", "Itzhak", ""], ["Wang", "Zhiying", ""], ["Bruck", "Jehoshua", ""]]}, {"id": "1103.4071", "submitter": "Richard Cole", "authors": "Richard Cole, Vijaya Ramachandran", "title": "Efficient Resource Oblivious Algorithms for Multicores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the design of efficient algorithms for a multicore computing\nenvironment with a global shared memory and p cores, each having a cache of\nsize M, and with data organized in blocks of size B. We characterize the class\nof `Hierarchical Balanced Parallel (HBP)' multithreaded computations for\nmulticores. HBP computations are similar to the hierarchical divide & conquer\nalgorithms considered in recent work, but have some additional features that\nguarantee good performance even when accounting for the cache misses due to\nfalse sharing. Most of our HBP algorithms are derived from known\ncache-oblivious algorithms with high parallelism, however we incorporate new\ntechniques that reduce the effect of false-sharing.\n  Our approach to addressing false sharing costs (or more generally, block\nmisses) is to ensure that any task that can be stolen shares O(1) blocks with\nother tasks. We use a gapping technique for computations that have larger than\nO(1) block sharing. We also incorporate the property of limited access writes\nanalyzed in a companion paper, and we bound the cost of accessing shared blocks\non the execution stacks of tasks.\n  We present the Priority Work Stealing (PWS) scheduler, and we establish that,\ngiven a sufficiently `tall' cache, PWS deterministically schedules several\nhighly parallel HBP algorithms, including those for scans, matrix computations\nand FFT, with cache misses bounded by the sequential complexity, when\naccounting for both traditional cache misses and for false sharing. We also\npresent a list ranking algorithm with almost optimal bounds. PWS schedules\nwithout using cache or block size information, and uses knowledge of processors\nonly to the extent of determining the available locations from which tasks may\nbe stolen; thus it schedules resource-obliviously.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2011 16:39:54 GMT"}], "update_date": "2011-03-22", "authors_parsed": [["Cole", "Richard", ""], ["Ramachandran", "Vijaya", ""]]}, {"id": "1103.4142", "submitter": "Richard Cole", "authors": "Richard Cole, Vijaya Ramachandran", "title": "Analysis of Randomized Work Stealing with False Sharing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper analyzes the cache miss cost of algorithms when scheduled using\nrandomized work stealing (RWS) in a parallel environment, taking into account\nthe effects of false sharing.\n  First, prior analyses (due to Acar et al.) are extended to incorporate false\nsharing. However, to control the possible delays due to false sharing, some\nrestrictions on the algorithms seem necessary. Accordingly, the class of\nHierarchical Tree algorithms is introduced and their performance analyzed.\n  In addition, the paper analyzes the performance of a subclass of the\nHierarchical Tree Algorithms, called HBP algorithms, when scheduled using RWS;\nimproved complexity bounds are obtained for this subclass. This class was\nintroduced in a companion paper with efficient resource oblivious computation\nin mind.\n  Finally, we note that in a scenario in which there is no false sharing the\nresults in this paper match prior bounds for cache misses but with reduced\nassumptions, and in particular with no need for a bounding concave function for\nthe cost of cache misses as in prior work by Frigo and Strumpen. This allows\nnon-trivial cache miss bounds in this case to be obtained for a larger class of\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2011 20:25:29 GMT"}], "update_date": "2011-03-23", "authors_parsed": [["Cole", "Richard", ""], ["Ramachandran", "Vijaya", ""]]}, {"id": "1103.4195", "submitter": "Sewoong Oh", "authors": "Satish Babu Korada and Andrea Montanari and Sewoong Oh", "title": "Gossip PCA", "comments": "13 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eigenvectors of data matrices play an important role in many computational\nproblems, ranging from signal processing to machine learning and control. For\ninstance, algorithms that compute positions of the nodes of a wireless network\non the basis of pairwise distance measurements require a few leading\neigenvectors of the distances matrix. While eigenvector calculation is a\nstandard topic in numerical linear algebra, it becomes challenging under severe\ncommunication or computation constraints, or in absence of central scheduling.\nIn this paper we investigate the possibility of computing the leading\neigenvectors of a large data matrix through gossip algorithms.\n  The proposed algorithm amounts to iteratively multiplying a vector by\nindependent random sparsification of the original matrix and averaging the\nresulting normalized vectors. This can be viewed as a generalization of gossip\nalgorithms for consensus, but the resulting dynamics is significantly more\nintricate. Our analysis is based on controlling the convergence to stationarity\nof the associated Kesten-Furstenberg Markov chain.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2011 03:05:47 GMT"}], "update_date": "2011-03-23", "authors_parsed": [["Korada", "Satish Babu", ""], ["Montanari", "Andrea", ""], ["Oh", "Sewoong", ""]]}, {"id": "1103.4690", "submitter": "Wojciech Golab", "authors": "Wojciech Golab and Lisa Higham and Philipp Woelfel", "title": "Linearizable Implementations Do Not Suffice for Randomized Distributed\n  Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linearizability is the gold standard among algorithm designers for deducing\nthe correctness of a distributed algorithm using implemented shared objects\nfrom the correctness of the corresponding algorithm using atomic versions of\nthe same objects. We show that linearizability does not suffice for this\npurpose when processes can exploit randomization, and we discuss the existence\nof alternative correctness conditions.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2011 07:54:19 GMT"}, {"version": "v2", "created": "Tue, 20 Sep 2011 04:47:26 GMT"}], "update_date": "2011-09-21", "authors_parsed": [["Golab", "Wojciech", ""], ["Higham", "Lisa", ""], ["Woelfel", "Philipp", ""]]}, {"id": "1103.4881", "submitter": "Antonio Wendell de Oliveira Rodrigues", "authors": "Wendell Rodrigues, Fr\\'ed\\'eric Guyomarc'h and Jean-Luc Dekeyser", "title": "Programming Massively Parallel Architectures using MARTE: a Case Study", "comments": "2nd Workshop on Model Based Engineering for Embedded Systems Design\n  on DATE 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, several industrial applications are being ported to parallel\narchitectures. These applications take advantage of the potential parallelism\nprovided by multiple core processors. Many-core processors, especially the\nGPUs(Graphics Processing Unit), have led the race of floating-point performance\nsince 2003. While the performance improvement of general- purpose\nmicroprocessors has slowed significantly, the GPUs have continued to improve\nrelentlessly. As of 2009, the ratio between many-core GPUs and multicore CPUs\nfor peak floating-point calculation throughput is about 10 times. However, as\nparallel programming requires a non-trivial distribution of tasks and data,\ndevelopers find it hard to implement their applications effectively. Aiming to\nimprove the use of many-core processors, this work presents an case-study using\nUML and MARTE profile to specify and generate OpenCL code for intensive signal\nprocessing applications. Benchmark results show us the viability of the use of\nMDE approaches to generate GPU applications.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2011 22:19:25 GMT"}], "update_date": "2011-03-28", "authors_parsed": [["Rodrigues", "Wendell", ""], ["Guyomarc'h", "Fr\u00e9d\u00e9ric", ""], ["Dekeyser", "Jean-Luc", ""]]}, {"id": "1103.4905", "submitter": "Surendra Rahamatkar Mr", "authors": "Surendra Rahamatkar and Dr. Ajay Agarwal", "title": "A Reference Based, Tree Structured Time Synchronization Approach and its\n  Analysis in WSN", "comments": "12 pages", "journal-ref": "International Journal of Ad hoc, Sensor & Ubiquitous Computing\n  (IJASUC)Vol.2, No.1, March 2011", "doi": "10.5121/ijasuc.2011.2103", "report-no": null, "categories": "cs.DC cs.NI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Time synchronization for wireless sensor networks (WSNs) has been studied in\nrecent years as a fundamental and significant research issue. Many applications\nbased on these WSNs assume local clocks at each sensor node that need to be\nsynchronized to a common notion of time. Time synchronization in a WSN is\ncritical for accurate time stamping of events and fine-tuned coordination among\nthe sensor nodes to reduce power consumption. This paper proposes a\nbidirectional, reference based, tree structured time synchronization service\nfor WSNs along with network evaluation phase. This offers a push mechanism for\n(i) accurate and (ii) low overhead for global time synchronization. Analysis\nstudy of proposed approach shows that it is lightweight as the number of\nrequired broadcasting messages is constant in one broadcasting domain.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2011 04:51:39 GMT"}], "update_date": "2011-03-28", "authors_parsed": [["Rahamatkar", "Surendra", ""], ["Agarwal", "Dr. Ajay", ""]]}, {"id": "1103.5102", "submitter": "Michael Goodrich", "authors": "Michael T. Goodrich", "title": "Data-Oblivious External-Memory Algorithms for the Compaction, Selection,\n  and Sorting of Outsourced Data", "comments": "Full version of a paper appearing in 2011 ACM Symp. on Parallelism in\n  Algorithms and Architectures (SPAA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present data-oblivious algorithms in the external-memory model for\ncompaction, selection, and sorting. Motivation for such problems comes from\nclients who use outsourced data storage services and wish to mask their data\naccess patterns. We show that compaction and selection can be done\ndata-obliviously using $O(N/B)$ I/Os, and sorting can be done, with a high\nprobability of success, using $O((N/B)\\log_{M/B} (N/B))$ I/Os. Our methods use\na number of new algorithmic techniques, including data-oblivious uses of\ninvertible Bloom lookup tables, a butterfly-like compression network,\nrandomized data thinning, and \"shuffle-and-deal\" data perturbation. In\naddition, since data-oblivious sorting is the bottleneck in the \"inner loop\" in\nexisting oblivious RAM simulations, our sorting result improves the amortized\ntime overhead to do oblivious RAM simulation by a logarithmic factor in the\nexternal-memory model.\n", "versions": [{"version": "v1", "created": "Sat, 26 Mar 2011 03:18:03 GMT"}], "update_date": "2011-03-29", "authors_parsed": [["Goodrich", "Michael T.", ""]]}, {"id": "1103.5736", "submitter": "Vlad Slavici", "authors": "Vlad Slavici, Daniel Kunkle, Gene Cooperman, Stephen Linton", "title": "Finding the Minimal DFA of Very Large Finite State Automata with an\n  Application to Token Passing Networks", "comments": "14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.CO math.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite state automata (FSA) are ubiquitous in computer science. Two of the\nmost important algorithms for FSA processing are the conversion of a\nnon-deterministic finite automaton (NFA) to a deterministic finite automaton\n(DFA), and then the production of the unique minimal DFA for the original NFA.\nWe exhibit a parallel disk-based algorithm that uses a cluster of 29 commodity\ncomputers to produce an intermediate DFA with almost two billion states and\nthen continues by producing the corresponding unique minimal DFA with less than\n800,000 states. The largest previous such computation in the literature was\ncarried out on a 512-processor CM-5 supercomputer in 1996. That computation\nproduced an intermediate DFA with 525,000 states and an unreported number of\nstates for the corresponding minimal DFA. The work is used to provide strong\nexperimental evidence satisfying a conjecture on a series of token passing\nnetworks. The conjecture concerns stack sortable permutations for a finite\nstack and a 3-buffer. The origins of this problem lie in the work on restricted\npermutations begun by Knuth and Tarjan in the late 1960s. The parallel\ndisk-based computation is also compared with both a single-threaded and\nmulti-threaded RAM-based implementation using a 16-core 128 GB large shared\nmemory computer.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2011 19:33:54 GMT"}], "update_date": "2011-03-30", "authors_parsed": [["Slavici", "Vlad", ""], ["Kunkle", "Daniel", ""], ["Cooperman", "Gene", ""], ["Linton", "Stephen", ""]]}, {"id": "1103.5743", "submitter": "M. Shahriar Hossain", "authors": "M. Shahriar Hossain, M. Muztaba Fuad, Debzani Deb, Kazi Muhammad\n  Najmul Hasan Khan, Md. Mahbubul Alam Joarder", "title": "Load Balancing in a Networked Environment through Homogenization", "comments": "International Conference on Cybernetics and Information Technologies,\n  Orlando, USA, July 21-25, 2004, Pages 99-104", "journal-ref": "International Conference on Cybernetics and Information\n  Technologies, Systems and Applications Orlando, USA, July 21-25, CITSA 2004,\n  Pages: 99-104", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed processing across a networked environment suffers from\nunpredictable behavior of speedup due to heterogeneous nature of the hardware\nand software in the remote machines. It is challenging to get a better\nperformance from a distributed system by distributing task in an intelligent\nmanner such that the heterogeneous nature of the system do not have any effect\non the speedup ratio. This paper introduces homogenization, a technique that\ndistributes and balances the workload in such a manner that the user gets the\nhighest speedup possible from a distributed environment. Along with providing\nbetter performance, homogenization is totally transparent to the user and\nrequires no interaction with the system.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2011 19:56:06 GMT"}], "update_date": "2011-03-31", "authors_parsed": [["Hossain", "M. Shahriar", ""], ["Fuad", "M. Muztaba", ""], ["Deb", "Debzani", ""], ["Khan", "Kazi Muhammad Najmul Hasan", ""], ["Joarder", "Md. Mahbubul Alam", ""]]}, {"id": "1103.5760", "submitter": "M. Shahriar Hossain", "authors": "M. Shahriar Hossain, Kazi Muhammad Najmul Hasan Khan, M. Muztaba Fuad,\n  Debzani Deb", "title": "Triangular Dynamic Architecture for Distributed Computing in a LAN\n  Environment", "comments": "Published", "journal-ref": "6th International Conference on Computer and Information\n  Technology, Dhaka, Bangladesh, 2003, Pages 481-486", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A computationally intensive large job, granulized to concurrent pieces and\noperating in a dynamic environment should reduce the total processing time.\nHowever, distributing jobs across a networked environment is a tedious and\ndifficult task. Job distribution in a Local Area Network based on Triangular\nDynamic Architecture (TDA) is a mechanism that establishes a dynamic\nenvironment for job distribution, load balancing and distributed processing\nwith minimum interaction from the user. This paper introduces TDA and discusses\nits architecture and shows the benefits gained by utilizing such architecture\nin a distributed computing environment.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2011 20:05:50 GMT"}], "update_date": "2011-03-31", "authors_parsed": [["Hossain", "M. Shahriar", ""], ["Khan", "Kazi Muhammad Najmul Hasan", ""], ["Fuad", "M. Muztaba", ""], ["Deb", "Debzani", ""]]}, {"id": "1103.5764", "submitter": "M. Shahriar Hossain", "authors": "M. Shahriar Hossain, M. Muztaba Fuad, Md. Mahbubul Alam Joarder", "title": "Agent Based Processing of Global Evaluation Function", "comments": null, "journal-ref": "Journal of Electronics and Computer Science, Jahangirnagar\n  University Press, Bangladesh, Vol.6, June 2005, Pages 35-45", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Load balancing across a networked environment is a monotonous job. Moreover,\nif the job to be distributed is a constraint satisfying one, the distribution\nof load demands core intelligence. This paper proposes parallel processing\nthrough Global Evaluation Function by means of randomly initialized agents for\nsolving Constraint Satisfaction Problems. A potential issue about the number of\nagents in a machine under the invocation of distribution is discussed here for\nsecuring the maximum benefit from Global Evaluation and parallel processing.\nThe proposed system is compared with typical solution that shows an exclusive\noutcome supporting the nobility of parallel implementation of Global Evaluation\nFunction with certain number of agents in each invoked machine.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2011 20:15:48 GMT"}], "update_date": "2011-03-31", "authors_parsed": [["Hossain", "M. Shahriar", ""], ["Fuad", "M. Muztaba", ""], ["Joarder", "Md. Mahbubul Alam", ""]]}, {"id": "1103.5794", "submitter": "Maryam Helmi", "authors": "Maryam Helmi, Lisa Higham, Eduardo Pacheco, and Philipp Woelfel", "title": "The Space Complexity of Long-lived and One-Shot Timestamp\n  Implementations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with the problem of implementing an unbounded\ntimestamp object from multi-writer atomic registers, in an asynchronous\ndistributed system of n processors with distinct identifiers where timestamps\nare taken from an arbitrary universe. Ellen, Fatourou and Ruppert (2008) showed\nthat sqrt{n}/2-O(1) registers are required for any obstruction-free\nimplementation of long-lived timestamp systems from atomic registers (meaning\nprocessors can repeatedly get timestamps). We improve this existing lower bound\nin two ways. First we establish a lower bound of n/6 - O(1) registers for the\nobstruction-free long-lived timestamp problem. Previous such linear lower\nbounds were only known for constrained versions of the timestamp problem. This\nbound is asymptotically tight; Ellen, Fatourou and Ruppert (2008) constructed a\nwait-free algorithm that uses n-1 registers. Second we show that sqrt{n} - O(1)\nregisters are required for any obstruction-free implementation of one-shot\ntimestamp systems(meaning each processor can get a timestamp at most once). We\nshow that this bound is also asymptotically tight by providing a wait-free\none-shot timestamp system that uses fewer than 2 sqrt{n} registers, thus\nestablishing a space complexity gap between one-shot and long-lived timestamp\nsystems.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2011 23:43:21 GMT"}, {"version": "v2", "created": "Wed, 21 Dec 2011 20:35:05 GMT"}], "update_date": "2011-12-22", "authors_parsed": [["Helmi", "Maryam", ""], ["Higham", "Lisa", ""], ["Pacheco", "Eduardo", ""], ["Woelfel", "Philipp", ""]]}, {"id": "1103.6087", "submitter": "Jianfeng Zhan", "authors": "Xu Liu, Jianfeng Zhan, Kunlin Zhan, Weisong Shi, Lin Yuan, Dan Meng,\n  and Lei Wang", "title": "Automatic Performance Debugging of SPMD-style Parallel Programs", "comments": "16 pages, 23 figures. Accepted by Journal of Parallel and Distributed\n  Computing (JPDC)", "journal-ref": "Journal of Parallel and Distributed Computing (JPDC), March, 2011", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The simple program and multiple data (SPMD) programming model is widely used\nfor both high performance computing and Cloud computing. In this paper, we\ndesign and implement an innovative system, AutoAnalyzer, that automates the\nprocess of debugging performance problems of SPMD-style parallel programs,\nincluding data collection, performance behavior analysis, locating bottlenecks,\nand uncovering their root causes. AutoAnalyzer is unique in terms of two\nfeatures: first, without any apriori knowledge, it automatically locates\nbottlenecks and uncovers their root causes for performance optimization;\nsecond, it is lightweight in terms of the size of performance data to be\ncollected and analyzed. Our contributions are three-fold: first, we propose two\neffective clustering algorithms to investigate the existence of performance\nbottlenecks that cause process behavior dissimilarity or code region behavior\ndisparity, respectively; meanwhile, we present two searching algorithms to\nlocate bottlenecks; second, on a basis of the rough set theory, we propose an\ninnovative approach to automatically uncovering root causes of bottlenecks;\nthird, on the cluster systems with two different configurations, we use two\nproduction applications, written in Fortran 77, and one open source\ncode-MPIBZIP2 (http://compression.ca/mpibzip2/), written in C++, to verify the\neffectiveness and correctness of our methods. For three applications, we also\npropose an experimental approach to investigating the effects of different\nmetrics on locating bottlenecks.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2011 05:38:39 GMT"}], "update_date": "2011-04-01", "authors_parsed": [["Liu", "Xu", ""], ["Zhan", "Jianfeng", ""], ["Zhan", "Kunlin", ""], ["Shi", "Weisong", ""], ["Yuan", "Lin", ""], ["Meng", "Dan", ""], ["Wang", "Lei", ""]]}, {"id": "1103.6114", "submitter": "Alexander Jaffe", "authors": "Alexander Jaffe, Thomas Moscibroda, Laura Effinger-Dean, Luis Ceze,\n  Karin Strauss", "title": "The Impact of Memory Models on Software Reliability in Multiprocessors", "comments": "15 pages, 2 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The memory consistency model is a fundamental system property characterizing\na multiprocessor. The relative merits of strict versus relaxed memory models\nhave been widely debated in terms of their impact on performance, hardware\ncomplexity and programmability. This paper adds a new dimension to this\ndiscussion: the impact of memory models on software reliability. By allowing\nsome instructions to reorder, weak memory models may expand the window between\ncritical memory operations. This can increase the chance of an undesirable\nthread-interleaving, thus allowing an otherwise-unlikely concurrency bug to\nmanifest. To explore this phenomenon, we define and study a probabilistic model\nof shared-memory parallel programs that takes into account such reordering. We\nuse this model to formally derive bounds on the \\emph{vulnerability} to\nconcurrency bugs of different memory models. Our results show that for 2 (or a\nsmall constant number of) concurrent threads, weaker memory models do indeed\nhave a higher likelihood of allowing bugs. On the other hand, we show that as\nthe number of parallel threads increases, the gap between the different memory\nmodels becomes proportionally insignificant. This suggests the\ncounter-intuitive rule that \\emph{as the number of parallel threads in the\nsystem increases, the importance of using a strict memory model diminishes};\nwhich potentially has major implications on the choice of memory consistency\nmodels in future multi-core systems.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2011 07:43:58 GMT"}, {"version": "v2", "created": "Wed, 6 Apr 2011 19:14:51 GMT"}], "update_date": "2011-04-07", "authors_parsed": [["Jaffe", "Alexander", ""], ["Moscibroda", "Thomas", ""], ["Effinger-Dean", "Laura", ""], ["Ceze", "Luis", ""], ["Strauss", "Karin", ""]]}]