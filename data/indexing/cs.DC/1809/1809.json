[{"id": "1809.00233", "submitter": "Ahmet Sayar", "authors": "Serife Acikalin, Suleyman Eken, Ahmet Sayar", "title": "Sleep Stage Classification: Scalability Evaluations of Distributed\n  Approaches", "comments": "Proceedings of The Third International Conference on Data Mining,\n  Internet Computing, and Big Data, Konya, Turkey 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Processing and analyzing of massive clinical data are resource intensive and\ntime consuming with traditional analytic tools. Electroencephalogram (EEG) is\none of the major technologies in detecting and diagnosing various brain\ndisorders, and produces huge volume big data to process. In this study, we\npropose a big data framework to diagnose sleep disorders by classifying the\nsleep stages from EEG signals. The framework is developed with open source\nSparkMlib Libraries. We also tested and evaluated the proposed framework by\nmeasuring the scalabilities of well-known classification algorithms on\nphysionet sleep records.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2018 18:39:57 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Acikalin", "Serife", ""], ["Eken", "Suleyman", ""], ["Sayar", "Ahmet", ""]]}, {"id": "1809.00235", "submitter": "Ahmet Sayar", "authors": "Suleyman Eken, Eray Aydin, Ahmet Sayar", "title": "Vectorization of Large Amounts of Raster Satellite Images in a\n  Distributed Architecture Using HIPI", "comments": "In Turkish, Proceedings of International Artificial Intelligence and\n  Data Processing Symposium (IDAP) 2017", "journal-ref": null, "doi": "10.1109/IDAP.2017.8090237", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vectorization process focus on grouping pixels of a raster image into raw\nline segments, and forming lines, polylines or poligons. To vectorize massive\nraster images regarding resource and performane problems, weuse a distributed\nHIPI image processing interface based on MapReduce approach. Apache Hadoop is\nplaced at the core of the framework. To realize such a system, we first define\nmapper function, and then its input and output formats. In this paper, mappers\nconvert raster mosaics into vector counterparts. Reduc functions are not needed\nfor vectorization. Vector representations of raster images is expected to give\nbetter performance in distributed computations by reducing the negative effects\nof bandwidth problem and horizontal scalability analysis is done.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2018 18:46:57 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Eken", "Suleyman", ""], ["Aydin", "Eray", ""], ["Sayar", "Ahmet", ""]]}, {"id": "1809.00273", "submitter": "Soumyottam Chatterjee", "authors": "Soumyottam Chatterjee and Gopal Pandurangan and Peter Robinson", "title": "The Complexity of Leader Election: A Chasm at Diameter Two", "comments": "A preliminary version of this work appeared in ICDCN 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper focuses on studying the message complexity of implicit leader\nelection in synchronous distributed networks of diameter two. Kutten et al.\\\n[JACM 2015] showed a fundamental lower bound of $\\Omega(m)$ ($m$ is the number\nof edges in the network) on the message complexity of (implicit) leader\nelection that applied also to Monte Carlo randomized algorithms with constant\nsuccess probability; this lower bound applies for graphs that have diameter at\nleast three. On the other hand, for complete graphs (i.e., graphs with diameter\none), Kutten et al.\\ [TCS 2015] established a tight bound of\n$\\tilde{\\Theta}(\\sqrt{n})$ on the message complexity of randomized leader\nelection ($n$ is the number of nodes in the network). For graphs of diameter\ntwo, the complexity was not known.\n  In this paper, we settle this complexity by showing a tight bound of\n$\\tilde{\\Theta}(n)$ on the message complexity of leader election in\ndiameter-two networks. Together with the two previous results of Kutten et al.,\nour results fully characterize the message complexity of leader election\nvis-\\`a-vis the graph diameter.\n", "versions": [{"version": "v1", "created": "Sun, 2 Sep 2018 00:30:27 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Chatterjee", "Soumyottam", ""], ["Pandurangan", "Gopal", ""], ["Robinson", "Peter", ""]]}, {"id": "1809.00554", "submitter": "Fedor Muratov", "authors": "Fedor Muratov, Andrei Lebedev, Nikolai Iushkevich, Bulat Nasrulin,\n  Makoto Takemiya", "title": "YAC: BFT Consensus Algorithm for Blockchain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consensus in decentralized systems that asynchronously receive events and\nwhich are subject to Byzantine faults is a common problem with many real-life\napplications. Advances in decentralized systems, such as distributed ledger\n(i.e., blockchain) technology, has only increased the importance of finding\nperformant and secure solutions to consensus of state machine replication in\ndecentralized systems.\n  YAC is a practical decentralized consensus algorithm, that solves the\nproblems of inefficient message passing and strong leaders that occur in\nclassical Byzantine fault tolerant consensus algorithms. The algorithm is open\nsource and currently is used to provide Byzantine fault tolerant consensus for\nthe Hyperledger Iroha blockchain project. We provide proofs of safety and\nliveness, as well as empirical results showing that our algorithm can scale to\ndozens of validating peers.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 11:20:42 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Muratov", "Fedor", ""], ["Lebedev", "Andrei", ""], ["Iushkevich", "Nikolai", ""], ["Nasrulin", "Bulat", ""], ["Takemiya", "Makoto", ""]]}, {"id": "1809.00710", "submitter": "Cesar A. Uribe", "authors": "C\\'esar A. Uribe and Soomin Lee and Alexander Gasnikov and Angelia\n  Nedi\\'c", "title": "A Dual Approach for Optimal Algorithms in Distributed Optimization over\n  Networks", "comments": "This work is an extended version of the manuscript: Optimal\n  Algorithms for Distributed Optimization arXiv:1712.00232", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.MA cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study dual-based algorithms for distributed convex optimization problems\nover networks, where the objective is to minimize a sum $\\sum_{i=1}^{m}f_i(z)$\nof functions over in a network. We provide complexity bounds for four different\ncases, namely: each function $f_i$ is strongly convex and smooth, each function\nis either strongly convex or smooth, and when it is convex but neither strongly\nconvex nor smooth. Our approach is based on the dual of an appropriately\nformulated primal problem, which includes a graph that models the communication\nrestrictions. We propose distributed algorithms that achieve the same optimal\nrates as their centralized counterparts (up to constant and logarithmic\nfactors), with an additional optimal cost related to the spectral properties of\nthe network. Initially, we focus on functions for which we can explicitly\nminimize its Legendre-Fenchel conjugate, i.e., admissible or dual friendly\nfunctions. Then, we study distributed optimization algorithms for non-dual\nfriendly functions, as well as a method to improve the dependency on the\nparameters of the functions involved. Numerical analysis of the proposed\nalgorithms is also provided.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 20:13:25 GMT"}, {"version": "v2", "created": "Sat, 21 Sep 2019 21:27:17 GMT"}, {"version": "v3", "created": "Mon, 16 Mar 2020 13:55:20 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Uribe", "C\u00e9sar A.", ""], ["Lee", "Soomin", ""], ["Gasnikov", "Alexander", ""], ["Nedi\u0107", "Angelia", ""]]}, {"id": "1809.00828", "submitter": "John Njuguna Jomo", "authors": "John N. Jomo and Frits de Prenter and Mohamed Elhaddad and Davide\n  D'Angella and Clemens V. Verhoosel and Stefan Kollmannsberger and Jan S.\n  Kirschke and Vera N\\\"ubel and Harald van Brummelen and Ernst Rank", "title": "Robust and parallel scalable iterative solutions for large-scale finite\n  cell analyses", "comments": "32 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC cs.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The finite cell method is a highly flexible discretization technique for\nnumerical analysis on domains with complex geometries. By using a non-boundary\nconforming computational domain that can be easily meshed, automatized\ncomputations on a wide range of geometrical models can be performed.\nApplication of the finite cell method, and other immersed methods, to large\nreal-life and industrial problems is often limited due to the conditioning\nproblems associated with these methods. These conditioning problems have caused\nresearchers to resort to direct solution methods, which signifi- cantly limit\nthe maximum size of solvable systems. Iterative solvers are better suited for\nlarge-scale computations than their direct counterparts due to their lower\nmemory requirements and suitability for parallel computing. These benefits can,\nhowever, only be exploited when systems are properly conditioned. In this\ncontribution we present an Additive-Schwarz type preconditioner that enables\nefficient and parallel scalable iterative solutions of large-scale multi-level\nhp-refined finite cell analyses.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 08:06:23 GMT"}, {"version": "v2", "created": "Wed, 5 Sep 2018 17:48:31 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Jomo", "John N.", ""], ["de Prenter", "Frits", ""], ["Elhaddad", "Mohamed", ""], ["D'Angella", "Davide", ""], ["Verhoosel", "Clemens V.", ""], ["Kollmannsberger", "Stefan", ""], ["Kirschke", "Jan S.", ""], ["N\u00fcbel", "Vera", ""], ["van Brummelen", "Harald", ""], ["Rank", "Ernst", ""]]}, {"id": "1809.00896", "submitter": "Muktikanta Sa", "authors": "Bapi Chatterjee, Sathya Peri, Muktikanta Sa, Nandini Singhal", "title": "A Simple and Practical Concurrent Non-blocking Unbounded Graph with\n  Reachability Queries", "comments": "10 pages, 5 figs, submitted to ICDCN-2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph algorithms applied in many applications, including social networks,\ncommunication networks, VLSI design, graphics, and several others, require\ndynamic modifications -- addition and removal of vertices and/or edges -- in\nthe graph. This paper presents a novel concurrent non-blocking algorithm to\nimplement a dynamic unbounded directed graph in a shared-memory machine. The\naddition and removal operations of vertices and edges are lock-free. For a\nfinite sized graph, the lookup operations are wait-free. Most significant\ncomponent of the presented algorithm is the reachability query in a concurrent\ngraph. The reachability queries in our algorithm are obstruction-free and thus\nimpose minimal additional synchronization cost over other operations. We prove\nthat each of the data structure operations are linearizable. We extensively\nevaluate a sample C/C++ implementation of the algorithm through a number of\nmicro-benchmarks. The experimental results show that the proposed algorithm\nscales well with the number of threads and on an average provides 5 to 7x\nperformance improvement over a concurrent graph implementation using\ncoarse-grained locking.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 11:35:27 GMT"}, {"version": "v2", "created": "Mon, 14 Jan 2019 21:51:40 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Chatterjee", "Bapi", ""], ["Peri", "Sathya", ""], ["Sa", "Muktikanta", ""], ["Singhal", "Nandini", ""]]}, {"id": "1809.00939", "submitter": "Ziliang Lai", "authors": "Ziliang Lai, Chris Liu, Eric Lo, Ben Kao, and Siu-Ming Yiu", "title": "Decentralized Search on Decentralized Web", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decentralized Web, or DWeb, is envisioned as a promising future of the Web.\nBeing decentralized, there are no dedicated web servers in DWeb; Devices that\nretrieve web contents also serve their cached data to peer devices with\nstraight privacy-preserving mechanisms. The fact that contents in DWeb are\ndistributed, replicated, and decentralized lead to a number of key advantages\nover the conventional web. These include better resiliency against network\npartitioning and distributed-denial-of-service attacks (DDoS), and better\nbrowsing experiences in terms of shorter latency and higher throughput.\nMoreover, DWeb provides tamper-proof contents because each content piece is\nuniquely identified by a cryptographic hash. DWeb also clicks well with future\nInternet architectures, such as Named Data Networking (NDN).Search engines have\nbeen an inseparable element of the Web. Contemporary (\"Web 2.0\") search\nengines, however, provide centralized services. They are thus subject to DDoS\nattacks, insider threat, and ethical issues like search bias and censorship. As\nthe web moves from being centralized to being decentralized, search engines\nought to follow. We propose QueenBee, a decentralized search engine for DWeb.\nQueenBee is so named because worker bees and honeycomb are a common metaphor\nfor distributed architectures, with the queen being the one that holds the\ncolony together. QueenBee aims to revolutionize the search engine business\nmodel by offering incentives to both content providers and peers that\nparticipate in QueenBee's page indexing and ranking operations.\n", "versions": [{"version": "v1", "created": "Sat, 18 Aug 2018 15:32:26 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Lai", "Ziliang", ""], ["Liu", "Chris", ""], ["Lo", "Eric", ""], ["Kao", "Ben", ""], ["Yiu", "Siu-Ming", ""]]}, {"id": "1809.01029", "submitter": "Kyle Niemeyer", "authors": "Nicholas J. Curtis, Kyle E. Niemeyer, and Chih-Jen Sung", "title": "Using SIMD and SIMT vectorization to evaluate sparse chemical kinetic\n  Jacobian matrices and thermochemical source terms", "comments": "53 pages, 13 figures", "journal-ref": "Combust. Flame 198 (2018) 186-204", "doi": "10.1016/j.combustflame.2018.09.008", "report-no": null, "categories": "physics.comp-ph cs.DC physics.chem-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately predicting key combustion phenomena in reactive-flow simulations,\ne.g., lean blow-out, extinction/ignition limits and pollutant formation,\nnecessitates the use of detailed chemical kinetics. The large size and high\nlevels of numerical stiffness typically present in chemical kinetic models\nrelevant to transportation/power-generation applications make the efficient\nevaluation/factorization of the chemical kinetic Jacobian and thermochemical\nsource-terms critical to the performance of reactive-flow codes. Here we\ninvestigate the performance of vectorized evaluation of\nconstant-pressure/volume thermochemical source-term and sparse/dense chemical\nkinetic Jacobians using single-instruction, multiple-data (SIMD) and\nsingle-instruction, multiple thread (SIMT) paradigms. These are implemented in\npyJac, an open-source, reproducible code generation platform. A new formulation\nof the chemical kinetic governing equations was derived and verified, resulting\nin Jacobian sparsities of 28.6-92.0% for the tested models. Speedups of\n3.40-4.08x were found for shallow-vectorized OpenCL source-rate evaluation\ncompared with a parallel OpenMP code on an avx2 central processing unit (CPU),\nincreasing to 6.63-9.44x and 3.03-4.23x for sparse and dense chemical kinetic\nJacobian evaluation, respectively. Furthermore, the effect of data-ordering was\ninvestigated and a storage pattern specifically formulated for vectorized\nevaluation was proposed; as well, the effect of the constant pressure/volume\nassumptions and varying vector widths were studied on source-term evaluation\nperformance. Speedups reached up to 17.60x and 45.13x for dense and sparse\nevaluation on the GPU, and up to 55.11x and 245.63x on the CPU over a\nfirst-order finite-difference Jacobian approach. Further, dense Jacobian\nevaluation was up to 19.56x and 2.84x times faster than a previous version of\npyJac on a CPU and GPU, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 14:54:14 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Curtis", "Nicholas J.", ""], ["Niemeyer", "Kyle E.", ""], ["Sung", "Chih-Jen", ""]]}, {"id": "1809.01106", "submitter": "Ying Sun", "authors": "Gesualdo Scutari and Ying Sun", "title": "Distributed Nonconvex Constrained Optimization over Time-Varying\n  Digraphs", "comments": "Submitted June 3, 2017, revised June 5, 2108. Part of this work has\n  been presented at the 2016 Asilomar Conference on System, Signal and\n  Computers and the 2017 IEEE ICASSP Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers nonconvex distributed constrained optimization over\nnetworks, modeled as directed (possibly time-varying) graphs. We introduce the\nfirst algorithmic framework for the minimization of the sum of a smooth\nnonconvex (nonseparable) function--the agent's sum-utility--plus a\nDifference-of-Convex (DC) function (with nonsmooth convex part). This general\nformulation arises in many applications, from statistical machine learning to\nengineering. The proposed distributed method combines successive convex\napproximation techniques with a judiciously designed perturbed push-sum\nconsensus mechanism that aims to track locally the gradient of the (smooth part\nof the) sum-utility. Sublinear convergence rate is proved when a fixed\nstep-size (possibly different among the agents) is employed whereas asymptotic\nconvergence to stationary solutions is proved using a diminishing step-size.\nNumerical results show that our algorithms compare favorably with current\nschemes on both convex and nonconvex problems.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 17:19:18 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Scutari", "Gesualdo", ""], ["Sun", "Ying", ""]]}, {"id": "1809.01275", "submitter": "Xiaohan Wei", "authors": "Xiaohan Wei, Hao Yu, Qing Ling, Michael J. Neely", "title": "Solving Non-smooth Constrained Programs with Lower Complexity than\n  $\\mathcal{O}(1/\\varepsilon)$: A Primal-Dual Homotopy Smoothing Approach", "comments": "in NIPS 2018, Montreal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new primal-dual homotopy smoothing algorithm for a linearly\nconstrained convex program, where neither the primal nor the dual function has\nto be smooth or strongly convex. The best known iteration complexity solving\nsuch a non-smooth problem is $\\mathcal{O}(\\varepsilon^{-1})$. In this paper, we\nshow that by leveraging a local error bound condition on the dual function, the\nproposed algorithm can achieve a better primal convergence time of\n$\\mathcal{O}\\left(\\varepsilon^{-2/(2+\\beta)}\\log_2(\\varepsilon^{-1})\\right)$,\nwhere $\\beta\\in(0,1]$ is a local error bound parameter. As an example\napplication of the general algorithm, we show that the distributed geometric\nmedian problem, which can be formulated as a constrained convex program, has\nits dual function non-smooth but satisfying the aforementioned local error\nbound condition with $\\beta=1/2$, therefore enjoying a convergence time of\n$\\mathcal{O}\\left(\\varepsilon^{-4/5}\\log_2(\\varepsilon^{-1})\\right)$. This\nresult improves upon the $\\mathcal{O}(\\varepsilon^{-1})$ convergence time bound\nachieved by existing distributed optimization algorithms. Simulation\nexperiments also demonstrate the performance of our proposed algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 00:26:58 GMT"}, {"version": "v2", "created": "Thu, 6 Sep 2018 14:17:12 GMT"}, {"version": "v3", "created": "Wed, 24 Oct 2018 06:00:48 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Wei", "Xiaohan", ""], ["Yu", "Hao", ""], ["Ling", "Qing", ""], ["Neely", "Michael J.", ""]]}, {"id": "1809.01326", "submitter": "Archit Somani", "authors": "Parwat Singh Anjana, Sweta Kumari, Sathya Peri, Sachin Rathor, and\n  Archit Somani", "title": "An Efficient Framework for Optimistic Concurrent Execution of Smart\n  Contracts", "comments": "16 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain platforms such as Ethereum and several others execute complex\ntransactions in blocks through user-defined scripts known as smart contracts.\nTo append a correct block into blockchain, miners execute these transactions of\nsmart contracts sequentially. Later the validators serially re-execute the\nsmart contract transactions of the block to validate it. If validation is\nsuccessful then the block is added to the blockchain using a consensus protocol\nand miner gets the incentive. In the current era of multi-core processors, by\nemploying the serial execution of the transactions, the miners and validators\nfail to utilize the cores properly and as a result, have poor throughput. By\nentitling concurrency to smart contracts execution, we can achieve better\nefficiency and higher throughput. In this proposal, we develop a novel and\nefficient technique to execute the smart contract transactions concurrently by\nminer using optimistic Software Transactional Memory systems (STMs). The miner\nproposes a block which consists of the set of transactions, conflict graph,\nhash of previous block and final state of each shared data-objects. Later, we\npropose concurrent validator which re-executes the same smart contracts\nconcurrently and deterministically with the help of conflict graph given by\nminer and verifies the final states. On successful validation, proposed block\nappended into the blockchain and miner gets the incentive. We consider a\nbenchmarks from solidity documentation. We execute the smart contract\ntransactions concurrently using Basic Time stamp Ordering (BTO) and\nMulti-Version Time stamp Ordering (MVTO) protocols as optimistic STMs. BTO and\nMVTO miner achieves 3.6x and 3.7x average speedups over serial miner\nrespectively. BTO and MVTO validator outperform average 40.8x and 47.1x than\nserial validator respectively.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 04:59:41 GMT"}, {"version": "v2", "created": "Sun, 4 Nov 2018 03:24:35 GMT"}, {"version": "v3", "created": "Thu, 6 Dec 2018 19:38:32 GMT"}, {"version": "v4", "created": "Mon, 14 Jan 2019 10:02:25 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Anjana", "Parwat Singh", ""], ["Kumari", "Sweta", ""], ["Peri", "Sathya", ""], ["Rathor", "Sachin", ""], ["Somani", "Archit", ""]]}, {"id": "1809.01334", "submitter": "Carsten Burstedde", "authors": "Carsten Burstedde", "title": "Distributed-Memory Forest-of-Octrees Raycasting", "comments": "19 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an MPI-parallel algorithm for the in-situ visualization of\ncomputational data that is built around a distributed linear forest-of-octrees\ndata structure. Such octrees are frequently used in element-based numerical\nsimulations; they store the leaves of the tree that are local in the curent\nparallel partition.\n  We proceed in three stages. First, we prune all elements whose bounding box\nis not visible by a parallel top-down traversal, and repartition the remaining\nones for load-balancing. Second, we intersect each element with every ray\npassing its box to derive color and opacity values for the ray segment. To\nreduce data, we aggregate the segments up the octree in a strictly distributed\nfashion in cycles of coarsening and repartition. Third, we composite all\nremaining ray segments to a tiled partition of the image and write it to disk\nusing parallel I/O.\n  The scalability of the method derives from three concepts: By exploiting the\nspace filling curve encoding of the octrees and by relying on recently\ndeveloped tree algorithms for top-down partition traversal, we are able to\ndetermine sender/receiver pairs without handshaking and/or collective\ncommunication. Furthermore, by partnering the linear traversal of tree leaves\nwith the group action of the attenuation/emission ODE along each segment, we\navoid back-to-front sorting of elements throughout. Lastly, the method is\nproblem adaptive with respect to the refinement and partition of the elements\nand to the accuracy of ODE integration.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 05:50:01 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Burstedde", "Carsten", ""]]}, {"id": "1809.01362", "submitter": "Luanzheng Guo", "authors": "Luanzheng Guo, Dong Li, Ignacio Laguna and Martin Schulz", "title": "FlipTracker: Understanding Natural Error Resilience in HPC Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": "LLNL-CONF-748619", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As high-performance computing systems scale in size and computational power,\nthe danger of silent errors, i.e., errors that can bypass hardware detection\nmechanisms and impact application state, grows dramatically. Consequently,\napplications running on HPC systems need to exhibit resilience to such errors.\nPrevious work has found that, for certain codes, this resilience can come for\nfree, i.e., some applications are naturally resilient, but few studies have\nshown the code patterns---combinations or sequences of computations---that make\nan application naturally resilient. In this paper, we present FlipTracker, a\nframework designed to extract these patterns using fine-grained tracking of\nerror propagation and resilience properties, and we use it to present a set of\ncomputation patterns that are responsible for making representative HPC\napplications naturally resilient to errors. This not only enables a deeper\nunderstanding of resilience properties of these codes, but also can guide\nfuture application designs towards patterns with natural resilience.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 07:43:27 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Guo", "Luanzheng", ""], ["Li", "Dong", ""], ["Laguna", "Ignacio", ""], ["Schulz", "Martin", ""]]}, {"id": "1809.01398", "submitter": "Chen Yuan", "authors": "Chen Yuan, Guangyi Liu, Renchang Dai, Zhiwei Wang", "title": "Power Flow Analysis Using Graph based Combination of Iterative Methods\n  and Vertex Contraction Approach", "comments": "8 pages, 8 figures, 2018 International Conference on Power System\n  Technology (POWERCON 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.DS cs.PF math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared with relational database (RDB), graph database (GDB) is a more\nintuitive expression of the real world. Each node in the GDB is a both storage\nand logic unit. Since it is connected to its neighboring nodes through edges,\nand its neighboring information could be easily obtained in one-step graph\ntraversal. It is able to conduct local computation independently and all nodes\ncan do their local work in parallel. Then the whole system can be maximally\nanalyzed and assessed in parallel to largely improve the computation\nperformance without sacrificing the precision of final results. This paper\nfirstly introduces graph database, power system graph modeling and potential\ngraph computing applications in power systems. Two iterative methods based on\ngraph database and PageRank are presented and their convergence are discussed.\nVertex contraction is proposed to improve the performance by eliminating\nzero-impedance branch. A combination of the two iterative methods is proposed\nto make use of their advantages. Testing results based on a provincial 1425-bus\nsystem demonstrate that the proposed comprehensive approach is a good candidate\nfor power flow analysis.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 09:17:56 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Yuan", "Chen", ""], ["Liu", "Guangyi", ""], ["Dai", "Renchang", ""], ["Wang", "Zhiwei", ""]]}, {"id": "1809.01415", "submitter": "Chen Yuan", "authors": "Chen Yuan, Yi Lu, Kewen Liu, Guangyi Liu, Renchang Dai, Zhiwei Wang", "title": "Exploration of Bi-Level PageRank Algorithm for Power Flow Analysis Using\n  Graph Database", "comments": "7 pages, 6 figures, 3 tables, 2018 IEEE International Congress on Big\n  Data. arXiv admin note: text overlap with arXiv:1809.01398", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared with traditional relational database, graph database, GDB, is a\nnatural expression of most real-world systems. Each node in the GDB is not only\na storage unit, but also a logic operation unit to implement local computation\nin parallel. This paper firstly explores the feasibility of power system\nmodeling using GDB. Then a brief introduction of the PageRank algorithm and the\nfeasibility analysis of its application in GDB are presented. Then the proposed\nGDB based bilevel PageRank algorithm is developed from PageRank algorithm and\nGauss Seidel methodology realize high performance parallel computation. MP\n10790 case, and its extensions, MP 107900 and MP 1079000, are tested to verify\nthe proposed method and investigate its parallelism in GDB. Besides, a\nprovincial system, FJ case which include 1425 buses and 1922 branches, is also\nincluded in the case study to further prove the proposed algorithm\neffectiveness in real world.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 09:59:18 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Yuan", "Chen", ""], ["Lu", "Yi", ""], ["Liu", "Kewen", ""], ["Liu", "Guangyi", ""], ["Dai", "Renchang", ""], ["Wang", "Zhiwei", ""]]}, {"id": "1809.01516", "submitter": "Dmytro Sytnyk", "authors": "Dmytro Sytnyk", "title": "Parallel numerical method for nonlocal-in-time Schr\\\"odinger equation", "comments": "Extended version of the original publication, several mistakes and\n  typos were fixed, 1 figure", "journal-ref": null, "doi": "10.1166/jcsmd.2017.1140", "report-no": null, "categories": "math.NA cs.DC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new parallel numerical method for solving the non-stationary\nSchr\\\"odinger equation with linear nonlocal condition and time-dependent\npotential which does not commute with the stationary part of the Hamiltonian.\nThe given problem is discretized in-time using a polynomial-based collocation\nscheme. We establish the conditions on the existence of solution to the\ndiscretized problem, estimate the accuracy of the discretized solution and\npropose the method how this solution can be approximately found in an efficient\nparallel manner.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 13:57:27 GMT"}, {"version": "v2", "created": "Thu, 20 Sep 2018 13:31:42 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Sytnyk", "Dmytro", ""]]}, {"id": "1809.02318", "submitter": "Samvit Jain", "authors": "Samvit Jain, Ganesh Ananthanarayanan, Junchen Jiang, Yuanchao Shu,\n  Joseph E. Gonzalez", "title": "Scaling Video Analytics Systems to Large Camera Deployments", "comments": "HotMobile 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driven by advances in computer vision and the falling costs of camera\nhardware, organizations are deploying video cameras en masse for the spatial\nmonitoring of their physical premises. Scaling video analytics to massive\ncamera deployments, however, presents a new and mounting challenge, as compute\ncost grows proportionally to the number of camera feeds. This paper is driven\nby a simple question: can we scale video analytics in such a way that cost\ngrows sublinearly, or even remains constant, as we deploy more cameras, while\ninference accuracy remains stable, or even improves. We believe the answer is\nyes. Our key observation is that video feeds from wide-area camera deployments\ndemonstrate significant content correlations (e.g. to other geographically\nproximate feeds), both in space and over time. These spatio-temporal\ncorrelations can be harnessed to dramatically reduce the size of the inference\nsearch space, decreasing both workload and false positive rates in multi-camera\nvideo analytics. By discussing use-cases and technical challenges, we propose a\nroadmap for scaling video analytics to large camera networks, and outline a\nplan for its realization.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 05:54:12 GMT"}, {"version": "v2", "created": "Thu, 4 Oct 2018 01:04:07 GMT"}, {"version": "v3", "created": "Sat, 3 Nov 2018 23:41:41 GMT"}, {"version": "v4", "created": "Fri, 5 Jul 2019 20:38:15 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Jain", "Samvit", ""], ["Ananthanarayanan", "Ganesh", ""], ["Jiang", "Junchen", ""], ["Shu", "Yuanchao", ""], ["Gonzalez", "Joseph E.", ""]]}, {"id": "1809.02436", "submitter": "Thorsten G\\\"otte", "authors": "Thorsten G\\\"otte, Christian Scheideler, Alexander Setzer", "title": "On Underlay-Aware Self-Stabilizing Overlay Networks", "comments": "A conference version of this paper was accepted at the 20th\n  International Symposium on Stabilization, Safety, and Security of Distributed\n  Systems (SSS 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a self-stabilizing protocol for an overlay network that constructs\nthe Minimum Spanning Tree (MST) for an underlay that is modeled by a weighted\ntree. The weight of an overlay edge between two nodes is the weighted length of\ntheir shortest path in the tree. We rigorously prove that our protocol works\ncorrectly under asynchronous and non-FIFO message delivery. Further, the\nprotocol stabilizes after $\\mathcal{O}(N^2)$ asynchronous rounds where $N$ is\nthe number of nodes in the overlay.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 12:26:37 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["G\u00f6tte", "Thorsten", ""], ["Scheideler", "Christian", ""], ["Setzer", "Alexander", ""]]}, {"id": "1809.02666", "submitter": "Fande Kong", "authors": "Fande Kong, Roy H. Stogner, Derek R. Gaston, John W. Peterson, Cody J.\n  Permann, Andrew E. Slaughter, Richard C. Martineau", "title": "A general-purpose hierarchical mesh partitioning method with node\n  balancing strategies for large-scale numerical simulations", "comments": "9 pages. Accepted by 2018 IEEE/ACM 9th Workshop on Latest Advances in\n  Scalable Algorithms for Large-Scale Systems (scalA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale parallel numerical simulations are essential for a wide range of\nengineering problems that involve complex, coupled physical processes\ninteracting across a broad range of spatial and temporal scales. The data\nstructures involved in such simulations (meshes, sparse matrices, etc.) are\nfrequently represented as graphs, and these graphs must be optimally\npartitioned across the available computational resources in order for the\nunderlying calculations to scale efficiently. Partitions which minimize the\nnumber of graph edges that are cut (edge-cuts) while simultaneously maintaining\na balance in the amount of work (i.e. graph nodes) assigned to each processor\ncore are desirable, and the performance of most existing partitioning software\nbegins to degrade in this metric for partitions with more than than $O(10^3)$\nprocessor cores. In this work, we consider a general-purpose hierarchical\npartitioner which takes into account the existence of multiple processor cores\nand shared memory in a compute node while partitioning a graph into an\narbitrary number of subgraphs. We demonstrate that our algorithms significantly\nimprove the preconditioning efficiency and overall performance of realistic\nnumerical simulations running on up to 32,768 processor cores with nearly\n$10^9$ unknowns.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 20:40:26 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 16:00:08 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Kong", "Fande", ""], ["Stogner", "Roy H.", ""], ["Gaston", "Derek R.", ""], ["Peterson", "John W.", ""], ["Permann", "Cody J.", ""], ["Slaughter", "Andrew E.", ""], ["Martineau", "Richard C.", ""]]}, {"id": "1809.02688", "submitter": "Sebastian Perez-Salazar", "authors": "Sebastian Perez-Salazar, Ishai Menache, Mohit Singh, Alejandro\n  Toriello", "title": "Dynamic Resource Allocation in the Cloud with Near-Optimal Efficiency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing has motivated renewed interest in resource allocation\nproblems with new consumption models. A common goal is to share a resource,\nsuch as CPU or I/O bandwidth, among distinct users with different demand\npatterns as well as different quality of service requirements. To ensure these\nservice requirements, cloud offerings often come with a service level agreement\n(SLA) between the provider and the users. An SLA specifies the amount of a\nresource a user is entitled to utilize. In many cloud settings, providers would\nlike to operate resources at high utilization while simultaneously respecting\nindividual SLAs. There is typically a tradeoff between these two objectives;\nfor example, utilization can be increased by shifting away resources from idle\nusers to \"scavenger\" workload, but with the risk of the former then becoming\nactive again. We study this fundamental tradeoff by formulating a resource\nallocation model that captures basic properties of cloud computing systems,\nincluding SLAs, highly limited feedback about the state of the system, and\nvariable and unpredictable input sequences. Our main result is a simple and\npractical algorithm that achieves near-optimal performance on the above two\nobjectives. First, we guarantee nearly optimal utilization of the resource even\nif compared to the omniscient offline dynamic optimum. Second, we\nsimultaneously satisfy all individual SLAs up to a small error. The main\nalgorithmic tool is a multiplicative weight update algorithm, and a primal-dual\nargument to obtain its guarantees. We also provide numerical validation on real\ndata to demonstrate the performance of our algorithm in practical applications.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 21:47:13 GMT"}, {"version": "v2", "created": "Thu, 15 Aug 2019 14:33:23 GMT"}, {"version": "v3", "created": "Tue, 26 Jan 2021 04:45:23 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Perez-Salazar", "Sebastian", ""], ["Menache", "Ishai", ""], ["Singh", "Mohit", ""], ["Toriello", "Alejandro", ""]]}, {"id": "1809.02697", "submitter": "Yida Wang", "authors": "Yizhi Liu, Yao Wang, Ruofei Yu, Mu Li, Vin Sharma, Yida Wang", "title": "Optimizing CNN Model Inference on CPUs", "comments": "15 pages, 4 figures, published at USENIX ATC '19", "journal-ref": "USENIX Annual Technical Conference 2019 1025-1040", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The popularity of Convolutional Neural Network (CNN) models and the ubiquity\nof CPUs imply that better performance of CNN model inference on CPUs can\ndeliver significant gain to a large number of users. To improve the performance\nof CNN inference on CPUs, current approaches like MXNet and Intel OpenVINO\nusually treat the model as a graph and use the high-performance libraries such\nas Intel MKL-DNN to implement the operations of the graph. While achieving\nreasonable performance on individual operations from the off-the-shelf\nlibraries, this solution makes it inflexible to conduct optimizations at the\ngraph level, as the local operation-level optimizations are predefined.\nTherefore, it is restrictive and misses the opportunity to optimize the\nend-to-end inference pipeline as a whole. This paper presents \\emph{NeoCPU}, a\ncomprehensive approach of CNN model inference on CPUs that employs a full-stack\nand systematic scheme of optimizations. \\emph{NeoCPU} optimizes the operations\nas templates without relying on third-parties libraries, which enables further\nimprovement of the performance via operation- and graph-level joint\noptimization. Experiments show that \\emph{NeoCPU} achieves up to 3.45$\\times$\nlower latency for CNN model inference than the current state-of-the-art\nimplementations on various kinds of popular CPUs.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 22:09:23 GMT"}, {"version": "v2", "created": "Fri, 11 Jan 2019 04:24:57 GMT"}, {"version": "v3", "created": "Fri, 5 Jul 2019 19:25:27 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Liu", "Yizhi", ""], ["Wang", "Yao", ""], ["Yu", "Ruofei", ""], ["Li", "Mu", ""], ["Sharma", "Vin", ""], ["Wang", "Yida", ""]]}, {"id": "1809.02839", "submitter": "Chi-Chung Chen", "authors": "Chi-Chung Chen, Chia-Lin Yang, Hsiang-Yun Cheng", "title": "Efficient and Robust Parallel DNN Training through Model Parallelism on\n  Multi-GPU Platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The training process of Deep Neural Network (DNN) is compute-intensive, often\ntaking days to weeks to train a DNN model. Therefore, parallel execution of DNN\ntraining on GPUs is a widely adopted approach to speed up the process nowadays.\nDue to the implementation simplicity, data parallelism is currently the most\ncommonly used parallelization method. Nonetheless, data parallelism suffers\nfrom excessive inter-GPU communication overhead due to frequent weight\nsynchronization among GPUs. Another approach is pipelined model parallelism,\nwhich partitions a DNN model among GPUs, and processes multiple mini-batches\nconcurrently. This approach can significantly reduce inter-GPU communication\ncost compared to data parallelism. However, pipelined model parallelism faces\nthe weight staleness issue; that is, gradients are computed with stale weights,\nleading to training instability and accuracy loss. In this paper, we present a\npipelined model parallel execution method that enables high GPU utilization\nwhile maintaining robust training accuracy via a novel weight prediction\ntechnique, SpecTrain. Experimental results show that our proposal achieves up\nto 8.91x speedup compared to data parallelism on a 4-GPU platform while\nmaintaining comparable model accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2018 17:21:58 GMT"}, {"version": "v2", "created": "Tue, 16 Oct 2018 04:04:34 GMT"}, {"version": "v3", "created": "Sun, 28 Apr 2019 15:36:28 GMT"}, {"version": "v4", "created": "Mon, 28 Oct 2019 10:13:24 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Chen", "Chi-Chung", ""], ["Yang", "Chia-Lin", ""], ["Cheng", "Hsiang-Yun", ""]]}, {"id": "1809.03095", "submitter": "EPTCS", "authors": "\\'Eric Goubault (\\'Ecole Polytechnique), J\\'er\\'emy Ledent (\\'Ecole\n  Polytechnique), Sergio Rajsbaum (UNAM)", "title": "A Simplicial Complex Model for Dynamic Epistemic Logic to study\n  Distributed Task Computability", "comments": "In Proceedings GandALF 2018, arXiv:1809.02416", "journal-ref": "EPTCS 277, 2018, pp. 73-87", "doi": "10.4204/EPTCS.277.6", "report-no": null, "categories": "cs.LO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The usual epistemic model S5n for a multi-agent system is based on a Kripke\nframe, which is a graph whose edges are labeled with agents that do not\ndistinguish between two states. We propose to uncover the higher dimensional\ninformation implicit in this structure, by considering a dual, simplicial\ncomplex model. We use dynamic epistemic logic (DEL) to study how an epistemic\nsimplicial complex model changes after a set of agents communicate with each\nother. We concentrate on an action model that represents the so called\nimmediate snapshot communication patterns of asynchronous agents, because it is\ncentral to distributed computability (but our setting works for other\ncommunication patterns). There are topological invariants preserved from the\ninitial epistemic complex to the one after the action model is applied, which\ndetermine the knowledge that the agents gain after communication. Finally, we\ndescribe how a distributed task specification can be modeled as a DEL action\nmodel, and show that the topological invariants determine whether the task is\nsolvable. We thus provide a bridge between DEL and the topological theory of\ndistributed computability, which studies task solvability in a shared memory or\nmessage passing architecture.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 02:30:43 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Goubault", "\u00c9ric", "", "\u00c9cole Polytechnique"], ["Ledent", "J\u00e9r\u00e9my", "", "\u00c9cole\n  Polytechnique"], ["Rajsbaum", "Sergio", "", "UNAM"]]}, {"id": "1809.03099", "submitter": "EPTCS", "authors": "A.R. Balasubramanian (Chennai Mathematical Institute)", "title": "Parameterized Verification of Coverability in Well-Structured Broadcast\n  Networks", "comments": "In Proceedings GandALF 2018, arXiv:1809.02416", "journal-ref": "EPTCS 277, 2018, pp. 133-146", "doi": "10.4204/EPTCS.277.10", "report-no": null, "categories": "cs.LO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameterized verification of coverability in broadcast networks with finite\nstate processes has been studied for different types of models and topologies.\nIn this paper, we attempt to develop a theory of broadcast networks in which\nthe processes can be well-structured transition systems. The resulting\nformalism is called well-structured broadcast networks. We give an algorithm to\ndecide coverability of well-structured broadcast networks when reconfiguration\nof links between nodes is allowed. Further, for various types of communication\ntopologies, we also prove the decidability of coverability in the static case\nas well. We do this by showing that for these types of static communication\ntopologies, the broadcast network itself is a well-structured transition\nsystem, hence proving the decidability of coverability in the broadcast\nnetwork.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 02:32:08 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Balasubramanian", "A. R.", "", "Chennai Mathematical Institute"]]}, {"id": "1809.03110", "submitter": "Supreeth Shastri", "authors": "Supreeth Shastri and David Irwin", "title": "Cloud Index Tracking: Enabling Predictable Costs in Cloud Spot Markets", "comments": "ACM Symposium on Cloud Computing 2018", "journal-ref": null, "doi": "10.1145/3267809.3267821", "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud spot markets rent VMs for a variable price that is typically much lower\nthan the price of on-demand VMs, which makes them attractive for a wide range\nof large-scale applications. However, applications that run on spot VMs suffer\nfrom cost uncertainty, since spot prices fluctuate, in part, based on supply,\ndemand, or both. The difficulty in predicting spot prices affects users and\napplications: the former cannot effectively plan their IT expenditures, while\nthe latter cannot infer the availability and performance of spot VMs, which are\na function of their variable price. To address the problem, we use properties\nof cloud infrastructure and workloads to show that prices become more stable\nand predictable as they are aggregated together. We leverage this observation\nto define an aggregate index price for spot VMs that serves as a reference for\nwhat users should expect to pay. We show that, even when the spot prices for\nindividual VMs are volatile, the index price remains stable and predictable. We\nthen introduce cloud index tracking: a migration policy that tracks the index\nprice to ensure applications running on spot VMs incur a predictable cost by\nmigrating to a new spot VM if the current VM's price significantly deviates\nfrom the index price.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 02:53:39 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Shastri", "Supreeth", ""], ["Irwin", "David", ""]]}, {"id": "1809.03143", "submitter": "Swapnil Dhamal", "authors": "Swapnil Dhamal, Walid Ben-Ameur, Tijani Chahed, Eitan Altman, Albert\n  Sunny, Sudheer Poojary", "title": "A Stochastic Game Framework for Analyzing Computational Investment\n  Strategies in Distributed Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a stochastic game framework with dynamic set of players, for\nmodeling and analyzing their computational investment strategies in distributed\ncomputing. Players obtain a certain reward for solving the problem or for\nproviding their computational resources, while incur a certain cost based on\nthe invested time and computational power. We first study a scenario where the\nreward is offered for solving the problem, such as in blockchain mining. We\nshow that, in Markov perfect equilibrium, players with cost parameters\nexceeding a certain threshold, do not invest; while those with cost parameters\nless than this threshold, invest maximal power. Here, players need not know the\nsystem state. We then consider a scenario where the reward is offered for\ncontributing to the computational power of a common central entity, such as in\nvolunteer computing. Here, in Markov perfect equilibrium, only players with\ncost parameters in a relatively low range in a given state, invest. For the\ncase where players are homogeneous, they invest proportionally to the 'reward\nto cost' ratio. For both the scenarios, we study the effects of players'\narrival and departure rates on their utilities using simulations and provide\nadditional insights.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 05:43:12 GMT"}, {"version": "v2", "created": "Sun, 18 Nov 2018 07:18:43 GMT"}, {"version": "v3", "created": "Sat, 16 Nov 2019 16:43:21 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Dhamal", "Swapnil", ""], ["Ben-Ameur", "Walid", ""], ["Chahed", "Tijani", ""], ["Altman", "Eitan", ""], ["Sunny", "Albert", ""], ["Poojary", "Sudheer", ""]]}, {"id": "1809.03165", "submitter": "Linshan Jiang", "authors": "Rui Tan, Linshan Jiang, Arvind Easwaran, Jothi Prasanna Shanmuga\n  Sundaram", "title": "Resilience Bounds of Sensing-Based Network Clock Synchronization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies exploited external periodic synchronous signals to synchronize\na pair of network nodes to address a threat of delaying the communications\nbetween the nodes. However, the sensing-based synchronization may yield faults\ndue to nonmalicious signal and sensor noises. This paper considers a system of\nN nodes that will fuse their peer-to-peer synchronization results to correct\nthe faults. Our analysis gives the lower bound of the number of faults that the\nsystem can tolerate when N is up to 12. If the number of faults is no greater\nthan the lower bound, the faults can be identified and corrected. We also prove\nthat the system cannot tolerate more than N-2 faults. Our results can guide the\ndesign of resilient sensing-based clock synchronization systems.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 07:50:50 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Tan", "Rui", ""], ["Jiang", "Linshan", ""], ["Easwaran", "Arvind", ""], ["Sundaram", "Jothi Prasanna Shanmuga", ""]]}, {"id": "1809.03188", "submitter": "Florina Ciorba", "authors": "Florina M. Ciorba, Christian Iwainsky, and Patrick Buder", "title": "OpenMP Loop Scheduling Revisited: Making a Case for More Schedules", "comments": "18 pages, 6 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In light of continued advances in loop scheduling, this work revisits the\nOpenMP loop scheduling by outlining the current state of the art in loop\nscheduling and presenting evidence that the existing OpenMP schedules are\ninsufficient for all combinations of applications, systems, and their\ncharacteristics. A review of the state of the art shows that due to the\nspecifics of the parallel applications, the variety of computing platforms, and\nthe numerous performance degradation factors, no single loop scheduling\ntechnique can be a 'one-fits-all' solution to effectively optimize the\nperformance of all parallel applications in all situations. The impact of\nirregularity in computational workloads and hardware systems, including\noperating system noise, on the performance of parallel applications, results in\nperformance loss and has often been neglected in loop scheduling research, in\nparticular, the context of OpenMP schedules. Existing dynamic loop\nself-scheduling techniques, such as trapezoid self-scheduling, factoring, and\nweighted factoring, offer an unexplored potential to alleviate this degradation\nin OpenMP due to the fact that they explicitly target the minimization of load\nimbalance and scheduling overhead. Through theoretical and experimental\nevaluation, this work shows that these loop self-scheduling methods provide a\nbenefit in the context of OpenMP. In conclusion, OpenMP must include more\nschedules to offer a broader performance coverage of applications executing on\nan increasing variety of heterogeneous shared memory computing platforms.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 08:55:12 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Ciorba", "Florina M.", ""], ["Iwainsky", "Christian", ""], ["Buder", "Patrick", ""]]}, {"id": "1809.03421", "submitter": "Pandurang Kamat", "authors": "Arati Baliga, I Subhod, Pandurang Kamat and Siddhartha Chatterjee", "title": "Performance Evaluation of the Quorum Blockchain Platform", "comments": "8 pages including references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Quorum is a permissioned blockchain platform built from the Ethereum codebase\nwith adaptations to make it a permissioned consortium platform. It is one of\nthe key contenders in the permissioned ledger space. Quorum supports\nconfidentiality and privacy of smart contracts and transactions, and crash and\nByzantine fault tolerant consensus algorithms. In this paper, we characterize\nthe performance features of Quorum. We study the throughput and latency\ncharacteristics of Quorum with different workloads and consensus algorithms\nthat it supports. Through a suite of micro-benchmarks, we explore how certain\ntransaction and smart contract parameters can affect transaction latencies.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 12:32:54 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Baliga", "Arati", ""], ["Subhod", "I", ""], ["Kamat", "Pandurang", ""], ["Chatterjee", "Siddhartha", ""]]}, {"id": "1809.03428", "submitter": "Ji Wang", "authors": "Ji Wang and Jianguo Zhang and Weidong Bao and Xiaomin Zhu and Bokai\n  Cao and Philip S. Yu", "title": "Not Just Privacy: Improving Performance of Private Deep Learning in\n  Mobile Cloud", "comments": "Conference version accepted by KDD'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing demand for on-device deep learning services calls for a highly\nefficient manner to deploy deep neural networks (DNNs) on mobile devices with\nlimited capacity. The cloud-based solution is a promising approach to enabling\ndeep learning applications on mobile devices where the large portions of a DNN\nare offloaded to the cloud. However, revealing data to the cloud leads to\npotential privacy risk. To benefit from the cloud data center without the\nprivacy risk, we design, evaluate, and implement a cloud-based framework ARDEN\nwhich partitions the DNN across mobile devices and cloud data centers. A simple\ndata transformation is performed on the mobile device, while the\nresource-hungry training and the complex inference rely on the cloud data\ncenter. To protect the sensitive information, a lightweight privacy-preserving\nmechanism consisting of arbitrary data nullification and random noise addition\nis introduced, which provides strong privacy guarantee. A rigorous privacy\nbudget analysis is given. Nonetheless, the private perturbation to the original\ndata inevitably has a negative impact on the performance of further inference\non the cloud side. To mitigate this influence, we propose a noisy training\nmethod to enhance the cloud-side network robustness to perturbed data. Through\nthe sophisticated design, ARDEN can not only preserve privacy but also improve\nthe inference performance. To validate the proposed ARDEN, a series of\nexperiments based on three image datasets and a real mobile application are\nconducted. The experimental results demonstrate the effectiveness of ARDEN.\nFinally, we implement ARDEN on a demo system to verify its practicality.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 16:09:58 GMT"}, {"version": "v2", "created": "Wed, 19 Sep 2018 02:50:41 GMT"}, {"version": "v3", "created": "Sat, 5 Jan 2019 11:21:17 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Wang", "Ji", ""], ["Zhang", "Jianguo", ""], ["Bao", "Weidong", ""], ["Zhu", "Xiaomin", ""], ["Cao", "Bokai", ""], ["Yu", "Philip S.", ""]]}, {"id": "1809.03559", "submitter": "Ji Wang", "authors": "Ji Wang and Bokai Cao and Philip S. Yu and Lichao Sun and Weidong Bao\n  and Xiaomin Zhu", "title": "Deep Learning Towards Mobile Applications", "comments": "Conference version accepted by ICDCS'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed an explosive growth of mobile devices. Mobile\ndevices are permeating every aspect of our daily lives. With the increasing\nusage of mobile devices and intelligent applications, there is a soaring demand\nfor mobile applications with machine learning services. Inspired by the\ntremendous success achieved by deep learning in many machine learning tasks, it\nbecomes a natural trend to push deep learning towards mobile applications.\nHowever, there exist many challenges to realize deep learning in mobile\napplications, including the contradiction between the miniature nature of\nmobile devices and the resource requirement of deep neural networks, the\nprivacy and security concerns about individuals' data, and so on. To resolve\nthese challenges, during the past few years, great leaps have been made in this\narea. In this paper, we provide an overview of the current challenges and\nrepresentative achievements about pushing deep learning on mobile devices from\nthree aspects: training with mobile data, efficient inference on mobile\ndevices, and applications of mobile deep learning. The former two aspects cover\nthe primary tasks of deep learning. Then, we go through our two recent\napplications that apply the data collected by mobile devices to inferring mood\ndisturbance and user identification. Finally, we conclude this paper with the\ndiscussion of the future of this area.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 19:28:57 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Wang", "Ji", ""], ["Cao", "Bokai", ""], ["Yu", "Philip S.", ""], ["Sun", "Lichao", ""], ["Bao", "Weidong", ""], ["Zhu", "Xiaomin", ""]]}, {"id": "1809.04070", "submitter": "Xuan Yang", "authors": "Xuan Yang, Mingyu Gao, Qiaoyi Liu, Jeff Ou Setter, Jing Pu, Ankita\n  Nayak, Steven Emberton Bell, Kaidi Cao, Heonjae Ha, Priyanka Raina, Christos\n  Kozyrakis, Mark Horowitz", "title": "Interstellar: Using Halide's Scheduling Language to Analyze DNN\n  Accelerators", "comments": "Published as a conference paper at ASPLOS 2020", "journal-ref": "Proceedings of the Twenty-Fifth International Conference on\n  Architectural Support for Programming Languages and Operating Systems, March,\n  2020, Pages 369-383", "doi": "10.1145/3373376.3378514", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that DNN accelerator micro-architectures and their program mappings\nrepresent specific choices of loop order and hardware parallelism for computing\nthe seven nested loops of DNNs, which enables us to create a formal taxonomy of\nall existing dense DNN accelerators. Surprisingly, the loop transformations\nneeded to create these hardware variants can be precisely and concisely\nrepresented by Halide's scheduling language. By modifying the Halide compiler\nto generate hardware, we create a system that can fairly compare these prior\naccelerators. As long as proper loop blocking schemes are used, and the\nhardware can support mapping replicated loops, many different hardware\ndataflows yield similar energy efficiency with good performance. This is\nbecause the loop blocking can ensure that most data references stay on-chip\nwith good locality and the processing units have high resource utilization. How\nresources are allocated, especially in the memory system, has a large impact on\nenergy and performance. By optimizing hardware resource allocation while\nkeeping throughput constant, we achieve up to 4.2X energy improvement for\nConvolutional Neural Networks (CNNs), 1.6X and 1.8X improvement for Long\nShort-Term Memories (LSTMs) and multi-layer perceptrons (MLPs), respectively.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 23:39:45 GMT"}, {"version": "v2", "created": "Sun, 26 Apr 2020 15:00:48 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Yang", "Xuan", ""], ["Gao", "Mingyu", ""], ["Liu", "Qiaoyi", ""], ["Setter", "Jeff Ou", ""], ["Pu", "Jing", ""], ["Nayak", "Ankita", ""], ["Bell", "Steven Emberton", ""], ["Cao", "Kaidi", ""], ["Ha", "Heonjae", ""], ["Raina", "Priyanka", ""], ["Kozyrakis", "Christos", ""], ["Horowitz", "Mark", ""]]}, {"id": "1809.04195", "submitter": "Javier Turek", "authors": "Michael J. Anderson, Jonathan I. Tamir, Javier S. Turek, Marcus T.\n  Alley, Theodore L. Willke, Shreyas S. Vasanawala, Michael Lustig", "title": "Clinically Deployed Distributed Magnetic Resonance Imaging\n  Reconstruction: Application to Pediatric Knee Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic resonance imaging is capable of producing volumetric images without\nionizing radiation. Nonetheless, long acquisitions lead to prohibitively long\nexams. Compressed sensing (CS) can enable faster scanning via sub-sampling with\nreduced artifacts. However, CS requires significantly higher reconstruction\ncomputation, limiting current clinical applications to 2D/3D or\nlimited-resolution dynamic imaging. Here we analyze the practical limitations\nto T2 Shuffling, a four-dimensional CS-based acquisition, which provides sharp\n3D-isotropic-resolution and multi-contrast images in a single scan. Our\nimprovements to the pipeline on a single machine provide a 3x overall\nreconstruction speedup, which allowed us to add algorithmic changes improving\nimage quality. Using four machines, we achieved additional 2.1x improvement\nthrough distributed parallelization. Our solution reduced the reconstruction\ntime in the hospital to 90 seconds on a 4-node cluster, enabling its use\nclinically. To understand the implications of scaling this application, we\nsimulated running our reconstructions with a multiple scanner setup typical in\nhospitals.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 23:21:48 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Anderson", "Michael J.", ""], ["Tamir", "Jonathan I.", ""], ["Turek", "Javier S.", ""], ["Alley", "Marcus T.", ""], ["Willke", "Theodore L.", ""], ["Vasanawala", "Shreyas S.", ""], ["Lustig", "Michael", ""]]}, {"id": "1809.04339", "submitter": "Robert Kelly", "authors": "Robert Kelly, Barak A. Pearlmutter, Phil Maguire", "title": "Concurrent Robin Hood Hashing", "comments": "16 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we examine the issues involved in adding concurrency to the\nRobin Hood hash table algorithm. We present a non-blocking obstruction-free\nK-CAS Robin Hood algorithm which requires only a single word compare-and-swap\nprimitive, thus making it highly portable. The implementation maintains the\nattractive properties of the original Robin Hood structure, such as a low\nexpected probe length, capability to operate effectively under a high load\nfactor and good cache locality, all of which are essential for high performance\non modern computer architectures. We compare our data-structures to various\nother lock-free and concurrent algorithms, as well as a simple hardware\ntransactional variant, and show that our implementation performs better across\na number of contexts.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 10:06:41 GMT"}, {"version": "v2", "created": "Wed, 14 Nov 2018 16:01:27 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Kelly", "Robert", ""], ["Pearlmutter", "Barak A.", ""], ["Maguire", "Phil", ""]]}, {"id": "1809.04561", "submitter": "Siddhartha Jayanti", "authors": "Prasad Jayanti and Siddhartha Jayanti", "title": "Constant Amortized RMR Complexity Deterministic Abortable Mutual\n  Exclusion Algorithm for CC and DSM Models", "comments": "19 pages with appendix and references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The abortable mutual exclusion problem was introduced by Scott and Scherer to\nmeet a need that arises in database and real time systems, where processes\nsometimes have to abandon their attempt to acquire a mutual exclusion lock to\ninitiate recovery from a potential deadlock or to avoid overshooting a\ndeadline. Algorithms of O(1) RMR complexity have been known for the standard\nmutual exclusion problem for both the Cache-Coherent (CC) and Distributed\nShared Memory (DSM) models of multiprocessors, but whether O(1) RMR complexity\nis also achievable for abortable mutual exclusion has remained open for the 18\nyears that this problem has been investigated.\n  Jayanti gives a Theta(log n) worst case RMR complexity solution for both\nmodels, where n is the maximum number of processes that execute the algorithm\nconcurrently. Giakouppis and Woelfel's algorithm, presented at PODC last year,\nis an O(1) amortized complexity algorithm, but it works only for the CC model,\nuses randomization, does not satisfy Starvation Freedom, and the O(1) amortized\nbound holds only in expectation and is proven for the a weak (oblivious)\nadversary model.\n  We design an algorithm that is free of these limitations: our algorithm is\ndeterministic, supports fast aborts (a process completes an abort in O(1)\nsteps), has a small space complexity of O(n), requires hardware support for\nonly the Fetch&Store instruction, satisfies a novely defined First Come First\nServed for abortable locks, and most importantly, has O(1) amortized RMR\ncomplexity for both the CC and DSM models. Our algorithm is short and practical\nwith fewer than a dozen lines of code, and is accompanied by a rigorous proof\nof mutual exclusion through invariants and of starvation-freedom and complexity\nanalysis through distance and potential functions. Thus, modulo amortization,\nour result answers affirmatively the long standing open question described\nabove.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 16:53:41 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Jayanti", "Prasad", ""], ["Jayanti", "Siddhartha", ""]]}, {"id": "1809.04733", "submitter": "Yueyue Chen", "authors": "Yueyue Chen, Deke Guo, Ming Xu, Guoming Tang, Tongqing Zhou, Bangbang\n  Ren", "title": "PPtaxi: Non-stop Package Delivery via Multi-hop Ridesharing", "comments": "14pages,10figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  City-wide package delivery becomes popular due to the dramatic rise of online\nshopping. It places a tremendous burden on the traditional logistics industry,\nwhich relies on dedicated couriers and is labor-intensive. Leveraging the\nridesharing systems is a promising alternative, yet existing solutions are\nlimited to one-hop ridesharing or need consignment warehouses as relays. In\nthis paper, we propose a new package delivery scheme which takes advantage of\nmulti-hop ridesharing and is entirely consignment free. Specifically, a package\nis assigned to a taxi which is guided to deliver the package all along to its\ndestination while transporting successive passengers. We tackle it with a\ntwo-phase solution, named \\textbf{PPtaxi}. In the first phase, we use the\nMultivariate Gauss distribution and Bayesian inference to predict the passenger\norders. In the second phase, both the computation efficiency and solution\neffectiveness are considered to plan package delivery routes. We evaluate\n\\textbf{PPtaxi} with a real-world dataset from an online taxi-taking platform\nand compare it with multiple benchmarks. The results show that the successful\ndelivery rate of packages with our solution can reach $95\\%$ on average during\nthe daytime, and is at most $46.9\\%$ higher than those of the benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 01:33:03 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Chen", "Yueyue", ""], ["Guo", "Deke", ""], ["Xu", "Ming", ""], ["Tang", "Guoming", ""], ["Zhou", "Tongqing", ""], ["Ren", "Bangbang", ""]]}, {"id": "1809.04923", "submitter": "Till Knollmann", "authors": "Till Knollmann and Christian Scheideler", "title": "A Self-Stabilizing Hashed Patricia Trie", "comments": "A conference version of this paper was accepted at the 20th\n  International Symposium on Stabilization, Safety, and Security of Distributed\n  Systems (SSS 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While a lot of research in distributed computing has covered solutions for\nself-stabilizing computing and topologies, there is far less work on\nself-stabilization for distributed data structures. Considering crashing peers\nin peer-to-peer networks, it should not be taken for granted that a distributed\ndata structure remains intact. In this work, we present a self-stabilizing\nprotocol for a distributed data structure called the hashed Patricia Trie\n(Kniesburges and Scheideler WALCOM'11) that enables efficient prefix search on\na set of keys. The data structure has a wide area of applications including\nstring matching problems while offering low overhead and efficient operations\nwhen embedded on top of a distributed hash table. Especially, longest prefix\nmatching for $x$ can be done in $\\mathcal{O}(\\log |x|)$ hash table read\naccesses. We show how to maintain the structure in a self-stabilizing way. Our\nprotocol assures low overhead in a legal state and a total (asymptotically\noptimal) memory demand of $\\Theta(d)$ bits, where $d$ is the number of bits\nneeded for storing all keys.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 12:55:05 GMT"}, {"version": "v2", "created": "Fri, 14 Sep 2018 15:21:06 GMT"}, {"version": "v3", "created": "Tue, 9 Oct 2018 11:17:27 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Knollmann", "Till", ""], ["Scheideler", "Christian", ""]]}, {"id": "1809.05013", "submitter": "Alexander Setzer", "authors": "Christian Scheideler and Alexander Setzer", "title": "Relays: A New Approach for the Finite Departure Problem in Overlay\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental problem for overlay networks is to safely exclude leaving\nnodes, i.e., the nodes requesting to leave the overlay network are excluded\nfrom it without affecting its connectivity. To rigorously study self-tabilizing\nsolutions to this problem, the Finite Departure Problem (FDP) has been proposed\n[12]. In the FDP we are given a network of processes in an arbitrary state, and\nthe goal is to eventually arrive at (and stay in) a state in which all leaving\nprocesses irrevocably decided to leave the system while for all\nweakly-connected components in the initial overlay network, all staying\nprocesses in that component will still form a weakly connected component. In\nthe standard interconnection model, the FDP is known to be unsolvable by local\ncontrol protocols, so oracles have been investigated that allow the problem to\nbe solved [12]. To avoid the use of oracles, we introduce a new interconnection\nmodel based on relays. Despite the relay model appearing to be rather\nrestrictive, we show that it is universal, i.e., it is possible to transform\nany weakly-connected topology into any other weakly-connected topology, which\nis important for being a useful interconnection model for overlay networks.\nApart from this, our model allows processes to grant and revoke access rights,\nwhich is why we believe it to be of interest beyond the scope of this paper. We\nshow how to implement the relay layer in a self-stabilizing way and identify\nproperties protocols need to satisfy so that the relay layer can recover while\nserving protocol requests.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 15:35:07 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Scheideler", "Christian", ""], ["Setzer", "Alexander", ""]]}, {"id": "1809.05018", "submitter": "E. Wes Bethel", "authors": "Brenton Lessley and Talita Perciano and Colleen Heinemann and David\n  Camp and Hank Childs and E. Wes Bethel", "title": "DPP-PMRF: Rethinking Optimization for a Probabilistic Graphical Model\n  Using Data-Parallel Primitives", "comments": "LDAV 2018, October 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new parallel algorithm for probabilistic graphical model\noptimization. The algorithm relies on data-parallel primitives (DPPs), which\nprovide portable performance over hardware architecture. We evaluate results on\nCPUs and GPUs for an image segmentation problem. Compared to a serial baseline,\nwe observe runtime speedups of up to 13X (CPU) and 44X (GPU). We also compare\nour performance to a reference, OpenMP-based algorithm, and find speedups of up\nto 7X (CPU).\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 15:41:57 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Lessley", "Brenton", ""], ["Perciano", "Talita", ""], ["Heinemann", "Colleen", ""], ["Camp", "David", ""], ["Childs", "Hank", ""], ["Bethel", "E. Wes", ""]]}, {"id": "1809.05197", "submitter": "Dan Iorga", "authors": "Dan Iorga and Tyler Sorensen and Alastair F. Donaldson", "title": "Do Your Cores Play Nicely? A Portable Framework for Multi-core\n  Interference Tuning and Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-core architectures can be leveraged to allow independent processes to\nrun in parallel. However, due to resources shared across cores, such as caches,\ndistinct processes may interfere with one another, e.g. affecting execution\ntime. Analysing the extent of this interference is difficult due to: (1) the\ndiversity of modern architectures, which may contain different implementations\nof shared resources, and (2) the complex nature of modern processors, in which\ninterference might arise due to subtle interactions. To address this, we\npropose a black-box auto-tuning approach that searches for processes that are\neffective at causing slowdowns for a program when executed in parallel. Such\nslowdowns provide lower bounds on worst-case execution time; an important\nmetric in systems with real-time constraints.\n  Our approach considers a set of parameterised \"enemy\" processes and \"victim\"\nprograms, each targeting a shared resource. The autotuner searches for enemy\nprocess parameters that are effective at causing slowdowns in the victim\nprograms. The idea is that victim programs behave as a proxy for shared\nresource usage of arbitrary programs. We evaluate our approach on: 5 different\nchips; 3 resources (cache, memory bus, and main memory); and consider several\nsearch strategies and slowdown metrics. Using enemy processes tuned per chip,\nwe evaluate the slowdowns on the autobench and coremark benchmark suites and\nshow that our method is able to achieve slowdowns in 98% of benchmark/chip\ncombinations and provide similar results to manually written enemy processes.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 22:32:34 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Iorga", "Dan", ""], ["Sorensen", "Tyler", ""], ["Donaldson", "Alastair F.", ""]]}, {"id": "1809.05239", "submitter": "Xu Chen", "authors": "Tao Ouyang and Zhi Zhou and Xu Chen", "title": "Follow Me at the Edge: Mobility-Aware Dynamic Service Placement for\n  Mobile Edge Computing", "comments": "The paper is accepted by IEEE Journal on Selected Areas in\n  Communications, Aug. 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI cs.DC cs.MM cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile edge computing is a new computing paradigm, which pushes cloud\ncomputing capabilities away from the centralized cloud to the network edge.\nHowever, with the sinking of computing capabilities, the new challenge incurred\nby user mobility arises: since end-users typically move erratically, the\nservices should be dynamically migrated among multiple edges to maintain the\nservice performance, i.e., user-perceived latency. Tackling this problem is\nnon-trivial since frequent service migration would greatly increase the\noperational cost. To address this challenge in terms of the performance-cost\ntrade-off, in this paper we study the mobile edge service performance\noptimization problem under long-term cost budget constraint. To address user\nmobility which is typically unpredictable, we apply Lyapunov optimization to\ndecompose the long-term optimization problem into a series of real-time\noptimization problems which do not require a priori knowledge such as user\nmobility. As the decomposed problem is NP-hard, we first design an\napproximation algorithm based on Markov approximation to seek a near-optimal\nsolution. To make our solution scalable and amenable to future 5G application\nscenario with large-scale user devices, we further propose a distributed\napproximation scheme with greatly reduced time complexity, based on the\ntechnique of best response update. Rigorous theoretical analysis and extensive\nevaluations demonstrate the efficacy of the proposed centralized and\ndistributed schemes.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 03:07:40 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Ouyang", "Tao", ""], ["Zhou", "Zhi", ""], ["Chen", "Xu", ""]]}, {"id": "1809.05495", "submitter": "F\\'elix Cuadrado", "authors": "Luis M. Vaquero, Felix Cuadrado", "title": "Auto-tuning Distributed Stream Processing Systems using Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine tuning distributed systems is considered to be a craftsmanship, relying\non intuition and experience. This becomes even more challenging when the\nsystems need to react in near real time, as streaming engines have to do to\nmaintain pre-agreed service quality metrics. In this article, we present an\nautomated approach that builds on a combination of supervised and reinforcement\nlearning methods to recommend the most appropriate lever configurations based\non previous load. With this, streaming engines can be automatically tuned\nwithout requiring a human to determine the right way and proper time to deploy\nthem. This opens the door to new configurations that are not being applied\ntoday since the complexity of managing these systems has surpassed the\nabilities of human experts. We show how reinforcement learning systems can find\nsubstantially better configurations in less time than their human counterparts\nand adapt to changing workloads.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 16:36:05 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Vaquero", "Luis M.", ""], ["Cuadrado", "Felix", ""]]}, {"id": "1809.05528", "submitter": "Zakaria El Mrabet", "authors": "Zakaria El Mrabet, Hamid El Ghazi, Tayeb Sadiki, Hassan El Ghazi", "title": "A New Secure Network Architecture to Increase Security Among Virtual\n  Machines in Cloud Computing", "comments": null, "journal-ref": null, "doi": "10.1007/978-981-287-990-5_9", "report-no": null, "categories": "cs.CR cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing is a new model of computing which provides scalability,\nflexibility and on-demand service. Virtualization is one of the main components\nof the cloud, but unfortunately, this technology suffers from many security\nvulnerabilities. The main purpose of this paper is to present a new secure\narchitecture of Virtual Network machines in order to increase security among\nvirtual machines in a virtualized environment (Xen as a case study). First, we\nexpose the different network modes based on Xen Hypervisor, and then we analyze\nvulnerabilities and security issues within this kind of environment. Finally,\nwe present in details new secure architecture and demonstrate how it can face\nthe main security network attacks.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 17:49:46 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Mrabet", "Zakaria El", ""], ["Ghazi", "Hamid El", ""], ["Sadiki", "Tayeb", ""], ["Ghazi", "Hassan El", ""]]}, {"id": "1809.05574", "submitter": "Muhammad Hilman", "authors": "Muhammad H. Hilman, Maria A. Rodriguez, Rajkumar Buyya", "title": "Multiple Workflows Scheduling in Multi-tenant Distributed Systems: A\n  Taxonomy and Future Directions", "comments": "Several changes has been done based on reviewers' comments after\n  first round review. This is a pre-print for paper (currently under second\n  round review) submitted to ACM Computing Surveys", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The workflow is a general notion representing the automated processes along\nwith the flow of data. The automation ensures the processes being executed in\nthe order. Therefore, this feature attracts users from various background to\nbuild the workflow. However, the computational requirements are enormous and\ninvesting for a dedicated infrastructure for these workflows is not always\nfeasible. To cater to the broader needs, multi-tenant platforms for executing\nworkflows were began to be built. In this paper, we identify the problems and\nchallenges in the multiple workflows scheduling that adhere to the platforms.\nWe present a detailed taxonomy from the existing solutions on scheduling and\nresource provisioning aspects followed by the survey of relevant works in this\narea. We open up the problems and challenges to shove up the research on\nmultiple workflows scheduling in multi-tenant distributed systems.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 20:33:34 GMT"}, {"version": "v2", "created": "Wed, 22 May 2019 03:58:26 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Hilman", "Muhammad H.", ""], ["Rodriguez", "Maria A.", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "1809.05657", "submitter": "Hyun Dok Cho", "authors": "Hyun Dok Cho (1), Okwan Kwon (1), Samuel P. Midkiff (2) ((1) NVIDIA\n  Corporation, (2) Purdue University)", "title": "HDArray: Parallel Array Interface for Distributed Heterogeneous Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneous clusters with nodes containing one or more accelerators, such\nas GPUs, have become common. While MPI provides inter-address space\ncommunication, and OpenCL provides a process with access to heterogeneous\ncomputational resources, programmers are forced to write hybrid programs that\nmanage the interaction of both of these systems. This paper describes an array\nprogramming interface that provides users with automatic and manual\ndistributions of data and work. Using work distribution and kernel def and use\ninformation, communication among processes and devices in a process is\nperformed automatically. By providing a unified programming model to the user,\nprogram development is simplified.\n", "versions": [{"version": "v1", "created": "Sat, 15 Sep 2018 06:15:50 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 06:01:26 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Cho", "Hyun Dok", ""], ["Kwon", "Okwan", ""], ["Midkiff", "Samuel P.", ""]]}, {"id": "1809.05838", "submitter": "Dra\\v{z}en Lu\\v{c}anin PhD", "authors": "Dra\\v{z}en Lu\\v{c}anin and Ivona Brandic", "title": "Pervasive Cloud Controller for Geotemporal Inputs", "comments": null, "journal-ref": "IEEE Transactions on Cloud Computing, vol. 4, no. 2, pp. 180-195,\n  April-June 2016", "doi": "10.1109/TCC.2015.2464794", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid cloud computing growth has turned data center energy consumption\ninto a global problem. At the same time, modern cloud providers operate\nmultiple geographically-distributed data centers. Distributed data center\ninfrastructure changes the rules of cloud control, as energy costs depend on\ncurrent regional electricity prices and temperatures. Furthermore, to account\nfor emerging technologies surrounding the cloud ecosystem, a maintainable\ncontrol solution needs to be forward-compatible. Existing cloud controllers are\nfocused on VM consolidation methods suitable only for a single data center or\nconsider migration just in case of workload peaks, not accounting for all the\naspects of geographically distributed data centers. In this paper, we propose a\npervasive cloud controller for dynamic resource reallocation adapting to\nvolatile time- and location-dependent factors, while considering the QoS impact\nof too frequent migrations and the data quality limits of time series\nforecasting methods. The controller is designed with extensible decision\nsupport components. We evaluate it in a simulation using historical traces of\nelectricity prices and temperatures. By optimising for these additional\nfactors, we estimate 28.6% energy cost savings compared to baseline dynamic VM\nconsolidation. We provide a range of guidelines for cloud providers, showing\nthe environment conditions necessary to achieve significant cost savings and we\nvalidate the controller's extensibility.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 08:50:50 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Lu\u010danin", "Dra\u017een", ""], ["Brandic", "Ivona", ""]]}, {"id": "1809.05840", "submitter": "Dra\\v{z}en Lu\\v{c}anin PhD", "authors": "Dra\\v{z}en Lu\\v{c}anin, Ilia Pietri, Ivona Brandic and Rizos\n  Sakellariou", "title": "A Cloud Controller for Performance-Based Pricing", "comments": "8th IEEE International Conference on Cloud Computing (CLOUD 2015), 27\n  June - 2 July, 2015, New York, USA", "journal-ref": null, "doi": "10.1109/CLOUD.2015.30", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New dynamic cloud pricing options are emerging with cloud providers offering\nresources as a wide range of CPU frequencies and matching prices that can be\nswitched at runtime. On the other hand, cloud providers are facing the problem\nof growing operational energy costs. This raises a trade-off problem between\nenergy savings and revenue loss when performing actions such as CPU frequency\nscaling. Although existing cloud con- trollers for managing cloud resources\ndeploy frequency scaling, they only consider fixed virtual machine (VM)\npricing. In this paper we propose a performance-based pricing model adapted for\nVMs with different CPU-boundedness properties. We present a cloud controller\nthat scales CPU frequencies to achieve energy cost savings that exceed service\nrevenue losses. We evaluate the approach in a simulation based on real VM\nworkload, electricity price and temperature traces, estimating energy cost\nsavings up to 32% in certain scenarios.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 08:57:14 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Lu\u010danin", "Dra\u017een", ""], ["Pietri", "Ilia", ""], ["Brandic", "Ivona", ""], ["Sakellariou", "Rizos", ""]]}, {"id": "1809.05842", "submitter": "Dra\\v{z}en Lu\\v{c}anin PhD", "authors": "Dra\\v{z}en Lu\\v{c}anin, Ilia Pietri, Simon Holmbacka, Ivona Brandic,\n  Johan Lilius and Rizos Sakellariou", "title": "Performance-Based Pricing in Multi-Core Geo-Distributed Cloud Computing", "comments": "IEEE Transactions on Cloud Computing, November 2016", "journal-ref": null, "doi": "10.1109/TCC.2016.2628368", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New pricing policies are emerging where cloud providers charge resource\nprovisioning based on the allocated CPU frequencies. As a result, resources are\noffered to users as combinations of different performance levels and prices\nwhich can be configured at runtime. With such new pricing schemes and the\nincreasing energy costs in data centres, balancing energy savings with\nperformance and revenue losses is a challenging problem for cloud providers.\nCPU frequency scaling can be used to reduce power dissipation, but also impacts\nVM performance and therefore revenue. In this paper, we firstly propose a\nnon-linear power model that estimates power dissipation of a multi-core PM and\nsecondly a pricing model that adjusts the pricing based on the VM's\nCPU-boundedness characteristics. Finally, we present a cloud controller that\nuses these models to allocate VMs and scale CPU frequencies of the PMs to\nachieve energy cost savings that exceed service revenue losses. We evaluate the\nproposed approach using simulations with realistic VM workloads, electricity\nprice and temperature traces and estimate energy savings of up to 14.57%.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 09:18:05 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Lu\u010danin", "Dra\u017een", ""], ["Pietri", "Ilia", ""], ["Holmbacka", "Simon", ""], ["Brandic", "Ivona", ""], ["Lilius", "Johan", ""], ["Sakellariou", "Rizos", ""]]}, {"id": "1809.05853", "submitter": "Dra\\v{z}en Lu\\v{c}anin PhD", "authors": "Dra\\v{z}en Lu\\v{c}anin", "title": "Energy Efficient Cloud Control and Pricing in Geographically Distributed\n  Data Centers", "comments": "Dissertation, Vienna Univ Technology (Apr 2016)", "journal-ref": null, "doi": null, "report-no": "urn:nbn:at:at-ubtuw:1-106532", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is estimated that data centers constitute 1.5% of global electricity\nusage. At the same time, to serve increasing user requirements, modern cloud\nproviders are operating multiple geographically distributed data centers.\nDistributed data center infrastructure changes the rules of cloud control, as\nenergy costs depend on current regional electricity prices and temperatures\nthat we call geotemporal inputs. Furthermore, pricing policies at which cloud\nproviders can offer computational resources depend on the quality of service\n(QoS). With such pricing schemes and the increasing energy costs in data\ncentres, balancing energy savings with performance and revenue losses is a\nchallenging problem. Existing cloud control methods are suitable only for a\nsingle data center or do not consider all the available cloud control actions\nthat can reduce energy costs in geographically distributed data centers. In\nthis thesis, we propose a pervasive cloud control approach consisting of\nmultiple methods for dynamic resource reallocation and hardware configuration\nadapted to volatile geotemporal inputs. The proposed methods consider the QoS\nimpact of cloud control actions and the data quality limits of time series\nforecasting methods. We offer a cloud controller design that supports future\nextensions when new decision support components need to be added. We also\npropose novel pricing schemes which account for the computational resource\navailability and costs that arise from our cloud control approach to enable\nboth flexible, energy-aware and high performance cloud computing. We evaluate\nour methods empirically and in a number of simulations using historical traces\nof electricity prices, temperatures, workloads and other data. Our results show\nthat significant energy cost savings are possible without harming the QoS or\nservice revenue in geographically distributed cloud computing.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 10:52:43 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Lu\u010danin", "Dra\u017een", ""]]}, {"id": "1809.05921", "submitter": "Ankit Agrawal", "authors": "Ankit Agrawal, Renato Mancuso, Rodolfo Pellizzoni and Gerhard Fohler", "title": "Analysis of Dynamic Memory Bandwidth Regulation in Multi-core Real-Time\n  Systems", "comments": "Accepted for publication in the IEEE Real-Time Systems Symposium\n  (RTSS) 2018 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the primary sources of unpredictability in modern multi-core embedded\nsystems is contention over shared memory resources, such as caches,\ninterconnects, and DRAM. Despite significant achievements in the design and\nanalysis of multi-core systems, there is a need for a theoretical framework\nthat can be used to reason on the worst-case behavior of real-time workload\nwhen both processors and memory resources are subject to scheduling decisions.\n  In this paper, we focus our attention on dynamic allocation of main memory\nbandwidth. In particular, we study how to determine the worst-case response\ntime of tasks spanning through a sequence of time intervals, each with a\ndifferent bandwidth-to-core assignment. We show that the response time\ncomputation can be reduced to a maximization problem over assignment of memory\nrequests to different time intervals, and we provide an efficient way to solve\nsuch problem. As a case study, we then demonstrate how our proposed analysis\ncan be used to improve the schedulability of Integrated Modular Avionics\nsystems in the presence of memory-intensive workload.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 18:02:25 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Agrawal", "Ankit", ""], ["Mancuso", "Renato", ""], ["Pellizzoni", "Rodolfo", ""], ["Fohler", "Gerhard", ""]]}, {"id": "1809.05956", "submitter": "Athanasia Panousopoulou", "authors": "A. Panousopoulou, S. Farrens, K. Fotiadou, A. Woiselle, G.\n  Tsagkatakis, J-L. Starck, P. Tsakalides", "title": "A Distributed Learning Architecture for Scientific Imaging Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Current trends in scientific imaging are challenged by the emerging need of\nintegrating sophisticated machine learning with Big Data analytics platforms.\nThis work proposes an in-memory distributed learning architecture for enabling\nsophisticated learning and optimization techniques on scientific imaging\nproblems, which are characterized by the combination of variant information\nfrom different origins. We apply the resulting, Spark-compliant, architecture\non two emerging use cases from the scientific imaging domain, namely: (a) the\nspace variant deconvolution of galaxy imaging surveys (astrophysics), (b) the\nsuper-resolution based on coupled dictionary training (remote sensing). We\nconduct evaluation studies considering relevant datasets, and the results\nreport at least 60\\% improvement in time response against the conventional\ncomputing solutions. Ultimately, the offered discussion provides useful\npractical insights on the impact of key Spark tuning parameters on the speedup\nachieved, and the memory/disk footprint.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 21:01:17 GMT"}, {"version": "v2", "created": "Thu, 27 Sep 2018 21:19:47 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Panousopoulou", "A.", ""], ["Farrens", "S.", ""], ["Fotiadou", "K.", ""], ["Woiselle", "A.", ""], ["Tsagkatakis", "G.", ""], ["Starck", "J-L.", ""], ["Tsakalides", "P.", ""]]}, {"id": "1809.06100", "submitter": "Manuel Stein", "authors": "Manuel Stein", "title": "The Serverless Scheduling Problem and NOAH", "comments": "in revision after submission to HotCloud'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The serverless scheduling problem poses a new challenge to Cloud service\nplatform providers because it is rather a job scheduling problem than a\ntraditional resource allocation or request load balancing problem.\nTraditionally, elastic cloud applications use managed virtual resource\nallocation and employ request load balancers to orchestrate the deployment.\nWith serverless, the provider needs to solve both the load balancing and the\nallocation.\n  This work reviews the current Apache OpenWhisk serverless event load\nbalancing and a noncooperative game-theoretic load balancing approach for\nresponse time minimization in distributed systems. It is shown by simulation\nthat neither performs well under high system utilization which inspired a\nnoncooperative online allocation heuristic that allows tuning the trade-off\nbetween for response time and resource cost of each serverless function.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 09:51:24 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Stein", "Manuel", ""]]}, {"id": "1809.06440", "submitter": "Chang-Shen Lee", "authors": "Chang-Shen Lee, Nicol\\`o Michelusi, Gesualdo Scutari", "title": "Limited Rate Distributed Weight-Balancing and Average Consensus Over\n  Digraphs", "comments": "Part of this work will be presented at the 57th IEEE Conference on\n  Decision and Control", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed quantized weight-balancing and average consensus over fixed\ndigraphs are considered. A digraph with non-negative weights associated to its\nedges is weight-balanced if, for each node, the sum of the weights of its\nout-going edges is equal to that of its incoming edges. This paper proposes and\nanalyzes the first distributed algorithm that solves the weight-balancing\nproblem using only finite rate and simplex communications among nodes\n(compliant to the directed nature of the graph edges). Asymptotic convergence\nof the scheme is proved and a convergence rate analysis is provided. Building\non this result, a novel distributed algorithm is proposed that solves the\naverage consensus problem over digraphs, using, at each iteration, finite rate\nsimplex communications between adjacent nodes -- some bits for the\nweight-balancing problem, other for the average consensus. Convergence of the\nproposed quantized consensus algorithm to the average of the real (i.e.,\nunquantized) agent's initial values is proved, both almost surely and in $r$th\nmean for all positive integer $r$. Finally, numerical results validate our\ntheoretical findings.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 20:53:25 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Lee", "Chang-Shen", ""], ["Michelusi", "Nicol\u00f2", ""], ["Scutari", "Gesualdo", ""]]}, {"id": "1809.06529", "submitter": "Mohsen Amini Salehi", "authors": "Xiangbo Li, Mohsen Amini Salehi, Yamini Joshi, Mahmoud Darwich, Brad\n  Landreneau, Magdy Bayoumi", "title": "Performance Analysis and Modeling of Video Transcoding Using\n  Heterogeneous Cloud Services", "comments": "15 pages", "journal-ref": "IEEE Transactions on Parallel and Distributed Systems (TPDS), Sep.\n  2018", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  High-quality video streaming, either in form of Video-On-Demand (VOD) or live\nstreaming, usually requires converting (ie, transcoding) video streams to match\nthe characteristics of viewers' devices (eg, in terms of spatial resolution or\nsupported formats). Considering the computational cost of the transcoding\noperation and the surge in video streaming demands, Streaming Service Providers\n(SSPs) are becoming reliant on cloud services to guarantee Quality of Service\n(QoS) of streaming for their viewers. Cloud providers offer heterogeneous\ncomputational services in form of different types of Virtual Machines (VMs)\nwith diverse prices. Effective utilization of cloud services for video\ntranscoding requires detailed performance analysis of different video\ntranscoding operations on the heterogeneous cloud VMs. In this research, for\nthe first time, we provide a thorough analysis of the performance of the video\nstream transcoding on heterogeneous cloud VMs. Providing such analysis is\ncrucial for efficient prediction of transcoding time on heterogeneous VMs and\nfor the functionality of any scheduling methods tailored for video transcoding.\nBased upon the findings of this analysis and by considering the cost difference\nof heterogeneous cloud VMs, in this research, we also provide a model to\nquantify the degree of suitability of each cloud VM type for various\ntranscoding tasks. The provided model can supply resource (VM) provisioning\nmethods with accurate performance and cost trade-offs to efficiently utilize\ncloud services for video streaming.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 04:47:43 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Li", "Xiangbo", ""], ["Salehi", "Mohsen Amini", ""], ["Joshi", "Yamini", ""], ["Darwich", "Mahmoud", ""], ["Landreneau", "Brad", ""], ["Bayoumi", "Magdy", ""]]}, {"id": "1809.06536", "submitter": "Mohsen Amini Salehi", "authors": "Chavit Denninnart, Mohsen Amini Salehi, Adel Nadjaran Toosi, and\n  Xiangbo Li", "title": "Leveraging Computational Reuse for Cost- and QoS-Efficient Task\n  Scheduling in Clouds", "comments": "8 pages", "journal-ref": "In the 16th International Conference on Service-Oriented\n  Computing, 2018", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cloud-based computing systems could get oversubscribed due to budget\nconstraints of cloud users which causes violation of Quality of Experience(QoE)\nmetrics such as tasks' deadlines. We investigate an approach to achieve\nrobustness against uncertain task arrival and oversubscription through smart\nreuse of computation while similar tasks are waiting for execution. Our\nmotivation in this study is a cloud-based video streaming engine that processes\nvideo streaming tasks in an on-demand manner. We propose a mechanism to\nidentify various types of \"mergeable\" tasks and determine when it is\nappropriate to aggregate tasks without affecting QoS of other tasks. Experiment\nshows that our mechanism can improve robustness of the system and also saves\nthe overall time of using cloud services by more than 14%.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 05:18:40 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Denninnart", "Chavit", ""], ["Salehi", "Mohsen Amini", ""], ["Toosi", "Adel Nadjaran", ""], ["Li", "Xiangbo", ""]]}, {"id": "1809.06845", "submitter": "G\\'abor Etele G\\'evay", "authors": "G\\'abor E. G\\'evay, Tilmann Rabl, Sebastian Bre\\ss, Lor\\'and\n  Madai-Tahy, Volker Markl", "title": "Labyrinth: Compiling Imperative Control Flow to Parallel Dataflows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel dataflow systems have become a standard technology for large-scale\ndata analytics. Complex data analysis programs in areas such as machine\nlearning and graph analytics often involve control flow, i.e., iterations and\nbranching. Therefore, systems for advanced analytics should include control\nflow constructs that are efficient and easy to use. A natural approach is to\nprovide imperative control flow constructs similar to those of mainstream\nprogramming languages: while-loops, if-statements, and mutable variables, whose\nvalues can change between iteration steps.\n  However, current parallel dataflow systems execute programs written using\nimperative control flow constructs by launching a separate dataflow job after\nevery control flow decision (e.g., for every step of a loop). The performance\nof this approach is suboptimal, because (a) launching a dataflow job incurs\nscheduling overhead; and (b) it prevents certain optimizations across iteration\nsteps.\n  In this paper, we introduce Labyrinth, a method to compile programs written\nusing imperative control flow constructs to a single dataflow job, which\nexecutes the whole program, including all iteration steps. This way, we achieve\nboth efficiency and ease of use. We also conduct an experimental evaluation,\nwhich shows that Labyrinth has orders of magnitude smaller per-iteration-step\noverhead than launching new dataflow jobs, and also allows for significant\noptimizations across iteration steps.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 17:54:07 GMT"}, {"version": "v2", "created": "Thu, 20 Sep 2018 16:48:56 GMT"}, {"version": "v3", "created": "Mon, 15 Oct 2018 13:22:57 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["G\u00e9vay", "G\u00e1bor E.", ""], ["Rabl", "Tilmann", ""], ["Bre\u00df", "Sebastian", ""], ["Madai-Tahy", "Lor\u00e1nd", ""], ["Markl", "Volker", ""]]}, {"id": "1809.07014", "submitter": "Evangelos Georganas", "authors": "Evangelos Georganas, Rob Egan, Steven Hofmeyr, Eugene Goltsman, Bill\n  Arndt, Andrew Tritt, Aydin Buluc, Leonid Oliker, Katherine Yelick", "title": "Extreme Scale De Novo Metagenome Assembly", "comments": "Accepted to SC18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metagenome assembly is the process of transforming a set of short,\noverlapping, and potentially erroneous DNA segments from environmental samples\ninto the accurate representation of the underlying microbiomes's genomes.\nState-of-the-art tools require big shared memory machines and cannot handle\ncontemporary metagenome datasets that exceed Terabytes in size. In this paper,\nwe introduce the MetaHipMer pipeline, a high-quality and high-performance\nmetagenome assembler that employs an iterative de Bruijn graph approach.\nMetaHipMer leverages a specialized scaffolding algorithm that produces long\nscaffolds and accommodates the idiosyncrasies of metagenomes. MetaHipMer is\nend-to-end parallelized using the Unified Parallel C language and therefore can\nrun seamlessly on shared and distributed-memory systems. Experimental results\nshow that MetaHipMer matches or outperforms the state-of-the-art tools in terms\nof accuracy. Moreover, MetaHipMer scales efficiently to large concurrencies and\nis able to assemble previously intractable grand challenge metagenomes. We\ndemonstrate the unprecedented capability of MetaHipMer by computing the first\nfull assembly of the Twitchell Wetlands dataset, consisting of 7.5 billion\nreads - size 2.6 TBytes.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 04:53:39 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Georganas", "Evangelos", ""], ["Egan", "Rob", ""], ["Hofmeyr", "Steven", ""], ["Goltsman", "Eugene", ""], ["Arndt", "Bill", ""], ["Tritt", "Andrew", ""], ["Buluc", "Aydin", ""], ["Oliker", "Leonid", ""], ["Yelick", "Katherine", ""]]}, {"id": "1809.07267", "submitter": "Chris Maynard", "authors": "S.V. Adams, R.W. Ford, M. Hambley, J.M. Hobson, I. Kavcic, C.M.\n  Maynard, T. Melvin, E.H Mueller, S. Mullerworth, A.R. Porter, M. Rezny, B.J.\n  Shipway, R. Wong", "title": "LFRic: Meeting the challenges of scalability and performance portability\n  in Weather and Climate models", "comments": "41 pages, 10 figures. EASC2018", "journal-ref": "Journal of Parallel and Distributed Computing, 132 (2019), 383 --\n  396", "doi": "10.1016/j.jpdc.2019.02.007", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes LFRic: the new weather and climate modelling system\nbeing developed by the UK Met Office to replace the existing Unified Model in\npreparation for exascale computing in the 2020s. LFRic uses the GungHo\ndynamical core and runs on a semi-structured cubed-sphere mesh. The design of\nthe supporting infrastructure follows object orientated principles to\nfacilitate modularity and the use of external libraries where possible. In\nparticular, a `separation of concerns' between the science code and parallel\ncode is imposed to promote performance portability. An application called\nPSyclone, developed at the STFC Hartree centre, can generate the parallel code\nenabling deployment of a single source science code onto different machine\narchitectures. This paper provides an overview of the scientific requirement,\nthe design of the software infrastructure, and examples of PSyclone usage.\nPreliminary performance results show strong scaling and an indication that\nhybrid MPI/OpenMP performs better than pure MPI.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 16:05:07 GMT"}, {"version": "v2", "created": "Fri, 12 Jul 2019 09:32:47 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Adams", "S. V.", ""], ["Ford", "R. W.", ""], ["Hambley", "M.", ""], ["Hobson", "J. M.", ""], ["Kavcic", "I.", ""], ["Maynard", "C. M.", ""], ["Melvin", "T.", ""], ["Mueller", "E. H", ""], ["Mullerworth", "S.", ""], ["Porter", "A. R.", ""], ["Rezny", "M.", ""], ["Shipway", "B. J.", ""], ["Wong", "R.", ""]]}, {"id": "1809.07444", "submitter": "Matthias Springer", "authors": "Matthias Springer", "title": "DynaSOAr: Accelerating Single-Method Multiple-Objects Applications on\n  GPUs", "comments": "ACM Student Research Competition, Grand Finals Submission, Graduate\n  Category", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object-oriented programming (OOP) has long been regarded as too inefficient\nfor SIMD high-performance computing, despite the fact that many important HPC\napplications have an inherent object structure. We discovered a broad subset of\nOOP that can be implemented efficiently on massively parallel SIMD\naccelerators. We call it Single-Method Multiple-Objects (SMMO), because\nparallelism is expressed by running a method on all objects of a type.\n  To make fast GPU programming available to domain experts who are less\nexperienced in GPU programming, we developed DynaSOAr, a CUDA framework for\nSMMO applications. DynaSOAr improves the usage of allocated memory with an SOA\ndata layout and achieves low memory fragmentation through efficient management\nof free and allocated memory blocks with lock-free, hierarchical bitmaps.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 01:39:05 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 03:59:43 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Springer", "Matthias", ""]]}, {"id": "1809.07599", "submitter": "Jean-Baptiste Cordonnier", "authors": "Sebastian U. Stich, Jean-Baptiste Cordonnier and Martin Jaggi", "title": "Sparsified SGD with Memory", "comments": "to appear at NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Huge scale machine learning problems are nowadays tackled by distributed\noptimization algorithms, i.e. algorithms that leverage the compute power of\nmany devices for training. The communication overhead is a key bottleneck that\nhinders perfect scalability. Various recent works proposed to use quantization\nor sparsification techniques to reduce the amount of data that needs to be\ncommunicated, for instance by only sending the most significant entries of the\nstochastic gradient (top-k sparsification). Whilst such schemes showed very\npromising performance in practice, they have eluded theoretical analysis so\nfar.\n  In this work we analyze Stochastic Gradient Descent (SGD) with\nk-sparsification or compression (for instance top-k or random-k) and show that\nthis scheme converges at the same rate as vanilla SGD when equipped with error\ncompensation (keeping track of accumulated errors in memory). That is,\ncommunication can be reduced by a factor of the dimension of the problem\n(sometimes even more) whilst still converging at the same rate. We present\nnumerical experiments to illustrate the theoretical findings and the better\nscalability for distributed applications.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 13:02:14 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 21:13:10 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Stich", "Sebastian U.", ""], ["Cordonnier", "Jean-Baptiste", ""], ["Jaggi", "Martin", ""]]}, {"id": "1809.07611", "submitter": "Pawe{\\l} Ro\\'sciszewski", "authors": "Pawe{\\l} Ro\\'sciszewski", "title": "Optimization of hybrid parallel application execution in heterogeneous\n  high performance computing systems considering execution time and power\n  consumption", "comments": "127 pages, 25 figures. PhD thesis, Gda\\'nsk University of Technology\n  (2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many important computational problems require utilization of high performance\ncomputing (HPC) systems that consist of multi-level structures combining higher\nand higher numbers of devices with various characteristics. Utilizing full\npower of such systems requires programming parallel applications that are\nhybrid in two meanings: they can utilize parallelism on multiple levels at the\nsame time and combine together programming interfaces specific for various\ntypes of computing devices.\n  The main goal of parallel processing is increasing the processing\nperformance, and therefore decreasing the application execution time. The\ninternational HPC community is targeting development of \"Exascale\"\nsupercomputers (able to sustain $10^{18}$ floating point operations per second)\nby the year 2020. One of the main obstacles to achieving this goal is power\nconsumption of the computing systems that exceeds the energy supply limits. New\nprogramming models and algorithms that consider this criterion are one of the\nkey areas where significant progress is necessary in order to achieve the goal.\n  The goal of the dissertation is to extract a general model of hybrid parallel\napplication execution in heterogeneous HPC systems that is a synthesis of\nexisting specific approaches and developing an optimization methodology for\nsuch execution aiming for minimization of the contradicting objectives of\napplication execution time and power consumption of the utilized computing\nhardware. Both meanings of the application hybridity result in multiplicity of\nexecution parameters of nontrivial interdependences and influence on the\nconsidered optimization criteria. Mapping of the application processes on\ncomputing devices has also a significant impact on these criteria.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 13:29:14 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Ro\u015bciszewski", "Pawe\u0142", ""]]}, {"id": "1809.07683", "submitter": "Peng Wei", "authors": "Jason Cong, Peng Wei, Cody Hao Yu, Peng Zhang", "title": "AutoAccel: Automated Accelerator Generation and Optimization with\n  Composable, Parallel and Pipeline Architecture", "comments": null, "journal-ref": null, "doi": "10.1145/3195970.3195999", "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CPU-FPGA heterogeneous architectures are attracting ever-increasing attention\nin an attempt to advance computational capabilities and energy efficiency in\ntoday's datacenters. These architectures provide programmers with the ability\nto reprogram the FPGAs for flexible acceleration of many workloads.\nNonetheless, this advantage is often overshadowed by the poor programmability\nof FPGAs whose programming is conventionally a RTL design practice. Although\nrecent advances in high-level synthesis (HLS) significantly improve the FPGA\nprogrammability, it still leaves programmers facing the challenge of\nidentifying the optimal design configuration in a tremendous design space.\n  This paper aims to address this challenge and pave the path from software\nprograms towards high-quality FPGA accelerators. Specifically, we first propose\nthe composable, parallel and pipeline (CPP) microarchitecture as a template of\naccelerator designs. Such a well-defined template is able to support efficient\naccelerator designs for a broad class of computation kernels, and more\nimportantly, drastically reduce the design space. Also, we introduce an\nanalytical model to capture the performance and resource trade-offs among\ndifferent design configurations of the CPP microarchitecture, which lays the\nfoundation for fast design space exploration. On top of the CPP\nmicroarchitecture and its analytical model, we develop the AutoAccel framework\nto make the entire accelerator generation automated. AutoAccel accepts a\nsoftware program as an input and performs a series of code transformations\nbased on the result of the analytical-model-based design space exploration to\nconstruct the desired CPP microarchitecture. Our experiments show that the\nAutoAccel-generated accelerators outperform their corresponding software\nimplementations by an average of 72x for a broad class of computation kernels.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 06:35:27 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Cong", "Jason", ""], ["Wei", "Peng", ""], ["Yu", "Cody Hao", ""], ["Zhang", "Peng", ""]]}, {"id": "1809.07684", "submitter": "Margo Seltzer", "authors": "Peter Kraft, Amos Waterland, Daniel Y Fu, Anitha Gollamudi, Shai\n  Szulanski, Margo Seltzer", "title": "Automatic Parallelization of Sequential Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior work on Automatically Scalable Computation (ASC) suggests that it is\npossible to parallelize sequential computation by building a model of\nwhole-program execution, using that model to predict future computations, and\nthen speculatively executing those future computations. Although that prior\nwork demonstrated scaling, it did not demonstrate speedup, because it ran\nentirely in emulation. We took this as a challenge to construct a hardware\nprototype that embodies the ideas of ASC, but works on a broader range of\nprograms and runs natively on hardware. The resulting system is similar in\nspirit to the original work, but differs in practically every respect.\n  We present an implementation of the ASC architecture that runs natively on\nx86 hardware and achieves near-linear speedup up to 44-cores (the size of our\ntest platform) for several classes of programs, such as computational kernels,\nmap-style programs, and matrix operations. We observe that programs are either\ncompletely predictable, achieving near-perfect predictive accuracy, or totally\nunpredictable, and therefore not amenable to scaling via ASC-like techniques.\nWe also find that in most cases, speedup is limited only by implementation\ndetails: the overhead of our dependency tracking infrastructure and the\nmanipulation of large state spaces. We are able to automatically parallelize\nprograms with linked data structures that are not amenable to other forms of\nautomatic parallelization.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jul 2018 12:44:58 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Kraft", "Peter", ""], ["Waterland", "Amos", ""], ["Fu", "Daniel Y", ""], ["Gollamudi", "Anitha", ""], ["Szulanski", "Shai", ""], ["Seltzer", "Margo", ""]]}, {"id": "1809.07687", "submitter": "Michal Zasadzinski", "authors": "Micha{\\l} Zasadzi\\'nski, Marc Sol\\'e, Alvaro Brandon, Victor\n  Munt\\'es-Mulero, David Carrera", "title": "Next Stop \"NoOps\": Enabling Cross-System Diagnostics Through Graph-based\n  Composition of Logs and Metrics", "comments": "Peer-reviewed, accepted as a regular paper to IEEE Cluster 2018. To\n  be published through proceedings in September 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performing diagnostics in IT systems is an increasingly complicated task, and\nit is not doable in satisfactory time by even the most skillful operators.\nSystems and their architecture change very rapidly in response to business and\nuser demand. Many organizations see value in the maintenance and management\nmodel of NoOps that stands for No Operations. One of the implementations of\nthis model is a system that is maintained automatically without any human\nintervention. The path to NoOps involves not only precise and fast diagnostics\nbut also reusing as much knowledge as possible after the system is reconfigured\nor changed. The biggest challenge is to leverage knowledge on one IT system and\nreuse this knowledge for diagnostics of another, different system. We propose a\nframework of weighted graphs which can transfer knowledge, and perform\nhigh-quality diagnostics of IT systems. We encode all possible data in a graph\nrepresentation of a system state and automatically calculate weights of these\ngraphs. Then, thanks to the evaluation of similarity between graphs, we\ntransfer knowledge about failures from one system to another and use it for\ndiagnostics. We successfully evaluate the proposed approach on Spark, Hadoop,\nKafka and Cassandra systems.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 13:27:30 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Zasadzi\u0144ski", "Micha\u0142", ""], ["Sol\u00e9", "Marc", ""], ["Brandon", "Alvaro", ""], ["Munt\u00e9s-Mulero", "Victor", ""], ["Carrera", "David", ""]]}, {"id": "1809.07689", "submitter": "Meiling Han", "authors": "Meiling Han, Nan Guan, Jinghao Sun, Qingqiang He, Qingxu Deng, and\n  Weichen Liu", "title": "Response Time Bounds for Typed DAG Parallel Tasks on Heterogeneous\n  Multi-cores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneous multi-cores utilize the strength of different architectures for\nexecuting particular types of workload, and usually offer higher performance\nand energy efficiency. In this paper, we study the worst-case response time\n(WCRT) analysis of \\emph{typed} scheduling of parallel DAG tasks on\nheterogeneous multi-cores, where the workload of each vertex in the DAG is only\nallowed to execute on a particular type of cores. The only known WCRT bound for\nthis problem is grossly pessimistic and suffers the\n\\emph{non-self-sustainability} problem. In this paper, we propose two new WCRT\nbounds. The first new bound has the same time complexity as the existing bound,\nbut is more precise and solves its \\emph{non-self-sustainability} problem. The\nsecond new bound explores more detailed task graph structure information to\ngreatly improve the precision, but is computationally more expensive. We prove\nthat the problem of computing the second bound is strongly NP-hard if the\nnumber of types in the system is a variable, and develop an efficient algorithm\nwhich has polynomial time complexity if the number of types is a constant.\nExperiments with randomly generated workload show that our proposed new methods\nare significantly more precise than the existing bound while having good\nscalability.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 06:28:53 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Han", "Meiling", ""], ["Guan", "Nan", ""], ["Sun", "Jinghao", ""], ["He", "Qingqiang", ""], ["Deng", "Qingxu", ""], ["Liu", "Weichen", ""]]}, {"id": "1809.07693", "submitter": "Gregory Kiar", "authors": "Gregory Kiar, Shawn T Brown, Tristan Glatard, Alan C Evans", "title": "A Serverless Tool for Platform Agnostic Computational Experiment\n  Management", "comments": "12 pages, 3 figures, 1 tool", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuroscience has been carried into the domain of big data and high\nperformance computing (HPC) on the backs of initiatives in data collection and\nan increasingly compute-intensive tools. While managing HPC experiments\nrequires considerable technical acumen, platforms and standards have been\ndeveloped to ease this burden on scientists. While web-portals make resources\nwidely accessible, data organizations such as the Brain Imaging Data Structure\nand tool description languages such as Boutiques provide researchers with a\nfoothold to tackle these problems using their own datasets, pipelines, and\nenvironments. While these standards lower the barrier to adoption of HPC and\ncloud systems for neuroscience applications, they still require the\nconsolidation of disparate domain-specific knowledge. We present Clowdr, a\nlightweight tool to launch experiments on HPC systems and clouds, record rich\nexecution records, and enable the accessible sharing of experimental summaries\nand results. Clowdr uniquely sits between web platforms and bare-metal\napplications for experiment management by preserving the flexibility of\ndo-it-yourself solutions while providing a low barrier for developing,\ndeploying and disseminating neuroscientific analysis.\n", "versions": [{"version": "v1", "created": "Sun, 2 Sep 2018 13:51:15 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Kiar", "Gregory", ""], ["Brown", "Shawn T", ""], ["Glatard", "Tristan", ""], ["Evans", "Alan C", ""]]}, {"id": "1809.07696", "submitter": "Jeffrey Young", "authors": "Jeffrey S. Young, Eric Hein, Srinivas Eswar, Patrick Lavin, Jiajia Li,\n  Jason Riedy, Richard Vuduc, Thomas M. Conte", "title": "A Microbenchmark Characterization of the Emu Chick", "comments": null, "journal-ref": "Parallel Computing, 2019, ISSN 0167-8191", "doi": "10.1016/j.parco.2019.04.012", "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Emu Chick is a prototype system designed around the concept of migratory\nmemory-side processing. Rather than transferring large amounts of data across\npower-hungry, high-latency interconnects, the Emu Chick moves lightweight\nthread contexts to near-memory cores before the beginning of each memory read.\nThe current prototype hardware uses FPGAs to implement cache-less \"Gossamer\ncores for doing computational work and a stationary core to run basic operating\nsystem functions and migrate threads between nodes. In this multi-node\ncharacterization of the Emu Chick, we extend an earlier single-node\ninvestigation (Hein, et al. AsHES 2018) of the the memory bandwidth\ncharacteristics of the system through benchmarks like STREAM, pointer chasing,\nand sparse matrix-vector multiplication. We compare the Emu Chick hardware to\narchitectural simulation and an Intel Xeon-based platform. Our results\ndemonstrate that for many basic operations the Emu Chick can use available\nmemory bandwidth more efficiently than a more traditional, cache-based\narchitecture although bandwidth usage suffers for computationally intensive\nworkloads like SpMV. Moreover, the Emu Chick provides stable, predictable\nperformance with up to 65% of the peak bandwidth utilization on a random-access\npointer chasing benchmark with weak locality.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 15:52:18 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 16:37:16 GMT"}, {"version": "v3", "created": "Fri, 31 May 2019 14:17:25 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Young", "Jeffrey S.", ""], ["Hein", "Eric", ""], ["Eswar", "Srinivas", ""], ["Lavin", "Patrick", ""], ["Li", "Jiajia", ""], ["Riedy", "Jason", ""], ["Vuduc", "Richard", ""], ["Conte", "Thomas M.", ""]]}, {"id": "1809.07701", "submitter": "Pier Luigi Ventre", "authors": "Pier Luigi Ventre, Paolo Lungaroni, Giuseppe Siracusano, Claudio Pisa,\n  Florian Schmidt, Francesco Lombardo and Stefano Salsano", "title": "On the Fly Orchestration of Unikernels: Tuning and Performance\n  Evaluation of Virtual Infrastructure Managers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI cs.OS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network operators are facing significant challenges meeting the demand for\nmore bandwidth, agile infrastructures, innovative services, while keeping costs\nlow. Network Functions Virtualization (NFV) and Cloud Computing are emerging as\nkey trends of 5G network architectures, providing flexibility, fast\ninstantiation times, support of Commercial Off The Shelf hardware and\nsignificant cost savings. NFV leverages Cloud Computing principles to move the\ndata-plane network functions from expensive, closed and proprietary hardware to\nthe so-called Virtual Network Functions (VNFs). In this paper we deal with the\nmanagement of virtual computing resources (Unikernels) for the execution of\nVNFs. This functionality is performed by the Virtual Infrastructure Manager\n(VIM) in the NFV MANagement and Orchestration (MANO) reference architecture. We\ndiscuss the instantiation process of virtual resources and propose a generic\nreference model, starting from the analysis of three open source VIMs, namely\nOpenStack, Nomad and OpenVIM. We improve the aforementioned VIMs introducing\nthe support for special-purpose Unikernels and aiming at reducing the duration\nof the instantiation process. We evaluate some performance aspects of the VIMs,\nconsidering both stock and tuned versions. The VIM extensions and performance\nevaluation tools are available under a liberal open source licence.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 12:17:01 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Ventre", "Pier Luigi", ""], ["Lungaroni", "Paolo", ""], ["Siracusano", "Giuseppe", ""], ["Pisa", "Claudio", ""], ["Schmidt", "Florian", ""], ["Lombardo", "Francesco", ""], ["Salsano", "Stefano", ""]]}, {"id": "1809.07702", "submitter": "Kun Cheng", "authors": "Kun Cheng, Weiyue Liu, Qi Shen and Shengkai Liao", "title": "Design and Implementation of High-throughput PCIe with DMA Architecture\n  between FPGA and PowerPC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We designed and implemented a direct memory access (DMA) architecture of\nPCI-Express(PCIe) between Xilinx Field Program Gate Array(FPGA) and Freescale\nPowerPC. The DMA architecture based on FPGA is compatible with the Xilinx PCIe\ncore while the DMA architecture based on POWERPC is compatible with VxBus of\nVxWorks. The solutions provide a high-performance and low-occupancy alternative\nto commercial. In order to maximize the PCIe throughput while minimizing the\nFPGA resources utilization, the DMA engine adopts a novel strategy where the\nDMA register list is stored both inside the FPGA during initialization phase\nand inside the central memory of the host CPU. The FPGA design package is\ncomplemented with simple register access to control the DMA engine by a VxWorks\ndriver. The design is compatible with Xilinx FPGA Kintex Ultrascale Family, and\noperates with the Xilinx PCIe endpoint Generation 1 with lane configurations\nx8. A data throughput of more than 666 MBytes/s(memory write with data from\nFPGA to PowerPC) has been achieved with the single PCIe Gen1 x8 lanes endpoint\nof this design, PowerPC and FPGA can send memory write request to each other.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 13:22:53 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Cheng", "Kun", ""], ["Liu", "Weiyue", ""], ["Shen", "Qi", ""], ["Liao", "Shengkai", ""]]}, {"id": "1809.07794", "submitter": "Shaun D'Souza", "authors": "Shaun C. D'Souza", "title": "Evolving system bottlenecks in the as a service cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The web ecosystem is rapidly evolving with changing business and functional\nmodels. Cloud platforms are available in a SaaS, PaaS and IaaS model designed\naround commoditized Linux based servers. 10 billion users will be online and\naccessing the web and its various content. The industry has seen a convergence\naround IP based technology. Additionally, Linux based designs allow for a\nsystem wide profiling of application characteristics. The customer is an OEM\nwho provides Linux based servers for telecom solutions. The end customer will\ndevelop business applications on the server. Customers are interested in a\nlatency profiling mechanism which helps them to understand how the application\nbehaves at run time. The latency profiler is supposed to find the code path\nwhich makes an application block on I/O, and other synchronization primitives.\nThis will allow the customer to understand the performance bottleneck and tune\nthe system and application parameters.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 12:51:42 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["D'Souza", "Shaun C.", ""]]}, {"id": "1809.08013", "submitter": "Danilo Guerrera", "authors": "Danilo Guerrera, Rub\\'en M. Cabez\\'on, Jean-Guillaume Piccinali,\n  Aur\\'elien Cavelan, Florina M. Ciorba, David Imbert, Lucio Mayer, Darren Reed", "title": "Towards a Mini-App for Smoothed Particle Hydrodynamics at Exascale", "comments": "18 pages, 4 figures, 5 tables, 2018 IEEE International Conference on\n  Cluster Computing proceedings for WRAp18", "journal-ref": null, "doi": "10.1109/CLUSTER.2018.00077", "report-no": null, "categories": "physics.comp-ph cs.CE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The smoothed particle hydrodynamics (SPH) technique is a purely Lagrangian\nmethod, used in numerical simulations of fluids in astrophysics and\ncomputational fluid dynamics, among many other fields. SPH simulations with\ndetailed physics represent computationally-demanding calculations. The\nparallelization of SPH codes is not trivial due to the absence of a structured\ngrid. Additionally, the performance of the SPH codes can be, in general,\nadversely impacted by several factors, such as multiple time-stepping,\nlong-range interactions, and/or boundary conditions. This work presents\ninsights into the current performance and functionalities of three SPH codes:\nSPHYNX, ChaNGa, and SPH-flow. These codes are the starting point of an\ninterdisciplinary co-design project, SPH-EXA, for the development of an\nExascale-ready SPH mini-app. To gain such insights, a rotating square patch\ntest was implemented as a common test simulation for the three SPH codes and\nanalyzed on two modern HPC systems. Furthermore, to stress the differences with\nthe codes stemming from the astrophysics community (SPHYNX and ChaNGa), an\nadditional test case, the Evrard collapse, has also been carried out. This work\nextrapolates the common basic SPH features in the three codes for the purpose\nof consolidating them into a pure-SPH, Exascale-ready, optimized, mini-app.\nMoreover, the outcome of this serves as direct feedback to the parent codes, to\nimprove their performance and overall scalability.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 09:59:16 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Guerrera", "Danilo", ""], ["Cabez\u00f3n", "Rub\u00e9n M.", ""], ["Piccinali", "Jean-Guillaume", ""], ["Cavelan", "Aur\u00e9lien", ""], ["Ciorba", "Florina M.", ""], ["Imbert", "David", ""], ["Mayer", "Lucio", ""], ["Reed", "Darren", ""]]}, {"id": "1809.08140", "submitter": "Louis Esperet", "authors": "\\'Etienne Bamas and Louis Esperet", "title": "Distributed coloring of graphs with an optimal number of colors", "comments": "19 pages, 2 figures - full version of a paper accepted to STACS 2019\n  (with improved presentation and results compared to v2)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies sufficient conditions to obtain efficient distributed\nalgorithms coloring graphs optimally (i.e.\\ with the minimum number of colors)\nin the LOCAL model of computation. Most of the work on distributed vertex\ncoloring so far has focused on coloring graphs of maximum degree $\\Delta$ with\nat most $\\Delta+1$ colors (or $\\Delta$ colors when some simple obstructions are\nforbidden). When $\\Delta$ is sufficiently large and $c\\ge \\Delta-k_\\Delta+1$,\nfor some integer $k_\\Delta\\approx \\sqrt{\\Delta}-2$, we give a distributed\nalgorithm that given a $c$-colorable graph $G$ of maximum degree $\\Delta$,\nfinds a $c$-coloring of $G$ in $\\min\\{O((\\log\\Delta)^{1/12}\\log n), 2^{O(\\log\n\\Delta+\\sqrt{\\log \\log n})}\\}$ rounds, with high probability. The lower bound\n$\\Delta-k_\\Delta+1$ is best possible in the sense that for infinitely many\nvalues of $\\Delta$, we prove that when $\\chi(G)\\le \\Delta -k_\\Delta$, finding\nan optimal coloring of $G$ requires $\\Omega(n)$ rounds. Our proof is a light\nadaptation of a remarkable result of Molloy and Reed, who proved that for\n$\\Delta$ large enough, for any $c\\ge \\Delta - k_\\Delta$ deciding whether\n$\\chi(G)\\le c$ is in {\\textsf{P}}, while Embden-Weinert \\emph{et al.}\\ proved\nthat for $c\\le \\Delta-k_\\Delta-1$, the same problem is {\\textsf{NP}}-complete.\nNote that the sequential and distributed thresholds differ by one. We also show\nthat for any sufficiently large $\\Delta$, and $\\Omega(\\log \\Delta)\\le k \\le\n\\Delta/100$, every graph of maximum degree $\\Delta$ and clique number at most\n$\\Delta-k$ can be efficiently colored with at most $\\Delta-\\varepsilon k$\ncolors, for some absolute constant $\\varepsilon >0$, with a randomized\nalgorithm running in $O(\\log n/\\log \\log n)$ rounds with high probability.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 14:16:46 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2018 12:35:28 GMT"}, {"version": "v3", "created": "Thu, 24 Jan 2019 09:08:20 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Bamas", "\u00c9tienne", ""], ["Esperet", "Louis", ""]]}, {"id": "1809.08233", "submitter": "Darko Andro\\v{c}ec", "authors": "Darko Andro\\v{c}ec", "title": "Using JSON-LD to Compose Different IoT and Cloud Services", "comments": "4 pages, implementation of IoT and cloud interoperability approach", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Internet of things and cloud computing are in the widespread use today, and\noften work together to accomplish complex business task and use cases. This\npaper propose the framework and its practical implementation to compose\ndifferent things as services and cloud services. The ontology based approach\nand JSON-LD was used to semantically annotate both types of services, and\nenable the mechanism to semi-automatically compose these services. The use case\nand proof-of-concept application that use the proposed theoretical approach is\nalso described in this work.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 09:31:39 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Andro\u010dec", "Darko", ""]]}, {"id": "1809.08438", "submitter": "Ravi Kiran Raman", "authors": "Ravi Kiran Raman, Roman Vaculin, Michael Hind, Sekou L. Remy,\n  Eleftheria K. Pissadaki, Nelson Kibichii Bore, Roozbeh Daneshvar, Biplav\n  Srivastava, Kush R. Varshney", "title": "Trusted Multi-Party Computation and Verifiable Simulations: A Scalable\n  Blockchain Approach", "comments": "16 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT cs.SY math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale computational experiments, often running over weeks and over\nlarge datasets, are used extensively in fields such as epidemiology,\nmeteorology, computational biology, and healthcare to understand phenomena, and\ndesign high-stakes policies affecting everyday health and economy. For\ninstance, the OpenMalaria framework is a computationally-intensive simulation\nused by various non-governmental and governmental agencies to understand\nmalarial disease spread and effectiveness of intervention strategies, and\nsubsequently design healthcare policies. Given that such shared results form\nthe basis of inferences drawn, technological solutions designed, and day-to-day\npolicies drafted, it is essential that the computations are validated and\ntrusted. In particular, in a multi-agent environment involving several\nindependent computing agents, a notion of trust in results generated by peers\nis critical in facilitating transparency, accountability, and collaboration.\nUsing a novel combination of distributed validation of atomic computation\nblocks and a blockchain-based immutable audits mechanism, this work proposes a\nuniversal framework for distributed trust in computations. In particular we\naddress the scalaibility problem by reducing the storage and communication\ncosts using a lossy compression scheme. This framework guarantees not only\nverifiability of final results, but also the validity of local computations,\nand its cost-benefit tradeoffs are studied using a synthetic example of\ntraining a neural network.\n", "versions": [{"version": "v1", "created": "Sat, 22 Sep 2018 14:03:06 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Raman", "Ravi Kiran", ""], ["Vaculin", "Roman", ""], ["Hind", "Michael", ""], ["Remy", "Sekou L.", ""], ["Pissadaki", "Eleftheria K.", ""], ["Bore", "Nelson Kibichii", ""], ["Daneshvar", "Roozbeh", ""], ["Srivastava", "Biplav", ""], ["Varshney", "Kush R.", ""]]}, {"id": "1809.08526", "submitter": "Petr Novotny", "authors": "Petr Novotny, Bong Jun Ko, Alexander L. Wolf", "title": "Harvesting Time-Series Data from Service-Based Systems Hosted in MANETs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are concerned with reliably harvesting data collected from service-based\nsystems hosted on a mobile ad hoc network (MANET). More specifically, we are\nconcerned with time-bounded and time-sensitive time-series monitoring data\ndescribing the state of the network and system. The data are harvested in order\nto perform an analysis, usually one that requires a global view of the data\ntaken from distributed sites. For example, network- and application-state data\nare typically analysed in order to make operational and maintenance decisions.\nMANETs are a challenging environment in which to harvest monitoring data, due\nto the inherently unstable and unpredictable connectivity between nodes, and\nthe overhead of transferring data in a wireless medium. These limitations must\nbe overcome to support time-series analysis of perishable and time-critical\ndata. We present an epidemic, delay tolerant, and intelligent method to\nefficiently and effectively transfer time-series data between the mobile nodes\nof MANETs. The method establishes a network-wide synchronization overlay to\ntransfer increments of the data over intermediate nodes in periodic cycles. The\ndata are then accessible from local stores at the nodes. We implemented the\nmethod in Java~EE and present evaluation on a run-time dependence discovery\nmethod for Web Service applications hosted on MANETs, and comparison to other\nfour methods demonstrating that our method performs significantly better in\nboth data availability and network overhead.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2018 03:58:24 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Novotny", "Petr", ""], ["Ko", "Bong Jun", ""], ["Wolf", "Alexander L.", ""]]}, {"id": "1809.08529", "submitter": "Petr Novotny", "authors": "Petr Novotny, Qi Zhang, Richard Hull, Salman Baset, Jim Laredo, Roman\n  Vaculin, Daniel L. Ford, Donna N. Dillenberger", "title": "Permissioned Blockchain Technologies for Academic Publishing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Academic publishing is continuously evolving with the gradual adoption of new\ntechnologies. Blockchain is a new technology that promises to change how\nindividuals and organizations interact across various boundaries. The adoption\nof blockchains is beginning to transform diverse industries such as finance,\nsupply chain, international trade, as well as energy and resource management\nand many others. Through trust, data immutability, decentralized distribution\nof data, and facilitation of collaboration without the need for centralized\nmanagement and authority, blockchains have the potential to transform the\nacademic publishing domain and to address some of the current problems such as\nproductivity and reputation management, predatory publishing, transparent\npeer-review processes and many others. In this paper, we outline the\ntechnologies available in the domain of permissioned blockchains with focus on\nHyperledger Fabric and discuss how they can be leveraged in the domain of\nacademic publishing.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2018 04:20:32 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Novotny", "Petr", ""], ["Zhang", "Qi", ""], ["Hull", "Richard", ""], ["Baset", "Salman", ""], ["Laredo", "Jim", ""], ["Vaculin", "Roman", ""], ["Ford", "Daniel L.", ""], ["Dillenberger", "Donna N.", ""]]}, {"id": "1809.08628", "submitter": "Suryanarayana Murthy Durbhakula", "authors": "Murthy Durbhakula", "title": "OS Scheduling Algorithms for Memory Intensive Workloads in Multi-socket\n  Multi-core servers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Major chip manufacturers have all introduced multicore microprocessors.\nMulti-socket systems built from these processors are routinely used for running\nvarious server applications. Depending on the application that is run on the\nsystem, remote memory accesses can impact overall performance. This paper\npresents a new operating system (OS) scheduling optimization to reduce the\nimpact of such remote memory accesses. By observing the pattern of local and\nremote DRAM accesses for every thread in each scheduling quantum and applying\ndifferent algorithms, we come up with a new schedule of threads for the next\nquantum. This new schedule potentially cuts down remote DRAM accesses for the\nnext scheduling quantum and improves overall performance. We present three such\nnew algorithms of varying complexity followed by an algorithm which is an\nadaptation of Hungarian algorithm. We used three different synthetic workloads\nto evaluate the algorithm. We also performed sensitivity analysis with respect\nto varying DRAM latency. We show that these algorithms can cut down DRAM access\nlatency by up to 55% depending on the algorithm used. The benefit gained from\nthe algorithms is dependent upon their complexity. In general higher the\ncomplexity higher is the benefit. Hungarian algorithm results in an optimal\nsolution. We find that two out of four algorithms provide a good trade-off\nbetween performance and complexity for the workloads we studied.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2018 16:48:07 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Durbhakula", "Murthy", ""]]}, {"id": "1809.08694", "submitter": "Amir Daneshmand", "authors": "Amir Daneshmand and Gesualdo Scutari and Vyacheslav Kungurtsev", "title": "Second-order Guarantees of Distributed Gradient Algorithms", "comments": "Final version, to appear on SIAM J. on Optimization", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider distributed smooth nonconvex unconstrained optimization over\nnetworks, modeled as a connected graph. We examine the behavior of distributed\ngradient-based algorithms near strict saddle points. Specifically, we establish\nthat (i) the renowned Distributed Gradient Descent (DGD) algorithm likely\nconverges to a neighborhood of a Second-order Stationary (SoS) solution; and\n(ii) the more recent class of distributed algorithms based on gradient\ntracking--implementable also over digraphs--likely converges to exact SoS\nsolutions, thus avoiding (strict) saddle-points. Furthermore, new convergence\nrate results to first-order critical points is established for the latter class\nof algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2018 23:06:39 GMT"}, {"version": "v2", "created": "Sat, 29 Sep 2018 17:54:39 GMT"}, {"version": "v3", "created": "Tue, 9 Oct 2018 16:52:17 GMT"}, {"version": "v4", "created": "Mon, 3 Feb 2020 15:59:34 GMT"}, {"version": "v5", "created": "Mon, 25 May 2020 16:53:35 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Daneshmand", "Amir", ""], ["Scutari", "Gesualdo", ""], ["Kungurtsev", "Vyacheslav", ""]]}, {"id": "1809.08709", "submitter": "Laurent Lessard", "authors": "Akhil Sundararajan, Bryan Van Scoy, Laurent Lessard", "title": "A Canonical Form for First-Order Distributed Optimization Algorithms", "comments": null, "journal-ref": "American Control Conference, pp. 4075-4080, Jul 2019", "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the distributed optimization problem in which a network of agents\naims to minimize the average of local functions. To solve this problem, several\nalgorithms have recently been proposed where agents perform various\ncombinations of communication with neighbors, local gradient computations, and\nupdates to local state variables. In this paper, we present a canonical form\nthat characterizes any first-order distributed algorithm that can be\nimplemented using a single round of communication and gradient computation per\niteration, and where each agent stores up to two state variables. The canonical\nform features a minimal set of parameters that are both unique and expressive\nenough to capture any distributed algorithm in this class. The generic nature\nof our canonical form enables the systematic analysis and design of distributed\noptimization algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 00:41:05 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2019 15:36:40 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Sundararajan", "Akhil", ""], ["Van Scoy", "Bryan", ""], ["Lessard", "Laurent", ""]]}, {"id": "1809.09206", "submitter": "Millad Ghane", "authors": "Millad Ghane, Jeff Larkin, Larry Shi, Sunita Chandrasekaran, and\n  Margaret S. Cheung", "title": "Power and Energy-efficiency Roofline Model for GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy consumption has been a great deal of concern in recent years and\ndevelopers need to take energy-efficiency into account when they design\nalgorithms. Their design needs to be energy-efficient and low-power while it\ntries to achieve attainable performance provided by underlying hardware.\nHowever, different optimization techniques have different effects on power and\nenergy-efficiency and a visual model would assist in the selection process.\n  In this paper, we extended the roofline model and provided a visual\nrepresentation of optimization strategies for power consumption. Our model is\ncomposed of various ceilings regarding each strategy we included in our models.\nOne roofline model for computational performance and one for memory performance\nis introduced. We assembled our models based on some optimization strategies\nfor two widespread GPUs from NVIDIA: Geforce GTX 970 and Tesla K80.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 20:19:14 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Ghane", "Millad", ""], ["Larkin", "Jeff", ""], ["Shi", "Larry", ""], ["Chandrasekaran", "Sunita", ""], ["Cheung", "Margaret S.", ""]]}, {"id": "1809.09395", "submitter": "Teng Ma", "authors": "Teng Ma (1), Mingxing Zhang (1 and 3), Kang Chen (1), Xuehai Qian (2),\n  Yongwei Wu (1) ((1) Tsinghua University, (2) University of Southern\n  California, (3) Sangfor Inc)", "title": "A Case for Asymmetric Non-Volatile Memory Architecture", "comments": "18 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The byte-addressable Non-Volatile Memory (NVM) is a promising technology\nsince it simultaneously provides DRAM-like performance, disk-like capacity, and\npersistency. The current NVM deployment is symmetric, where NVM devices are\ndirectly attached to servers. Due to the higher density, NVM provides larger\ncapacity and can be shared among servers. Unfortunately, in the symmetric\nsetting, the availability of NVM devices is affected by the specific machine it\nis attached to. High availability can be realized by replicating data to NVM on\na remote machine. However, it requires full replication of data structure in\nlocal memory, limiting the size of the working set. This paper rethinks NVM\ndeployment and makes a case for the asymmetric NVM architecture, which\ndecouples servers from persistent data storage. In the proposed AsymNVM\narchitecture, NVM devices (back-end nodes) can be shared by multiple servers\n(front-end nodes) and provide recoverable persistent data structures. The\nasymmetric architecture is made possible by RDMA, and follows the recent\nindustry trend of resource disaggregation. We build AsymNVM framework based on\nAsymNVM architecture that implements: 1) high performance persistent data\nstructure update; 2) NVM data management; 3) concurrency control; and 4)\ncrash-consistency and replication. The central idea is to use operation logs to\nreduce the stall due to RDMA writes and enable efficient batching and caching\nin front-end nodes. To evaluation performance, we construct eight widely used\ndata structures and two applications based on AsymNVM framework, and use traces\nof industry workloads. In a cluster with ten machines, the results show that\nAsymNVM achieves comparable performance to the best possible symmetric\narchitecture while avoiding all the drawbacks with disaggregation. Compared to\nthe baseline AsymNVM, speedup brought by the proposed optimizations is 6~22x.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 10:18:08 GMT"}, {"version": "v2", "created": "Wed, 30 Jan 2019 17:20:47 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Ma", "Teng", "", "1 and 3"], ["Zhang", "Mingxing", "", "1 and 3"], ["Chen", "Kang", ""], ["Qian", "Xuehai", ""], ["Wu", "Yongwei", ""]]}, {"id": "1809.09550", "submitter": "Lillian Tsai", "authors": "Lillian Tsai, Eddie Kohler, M. Frans Kaashoek, and Nickolai Zeldovich", "title": "A Revised and Verified Proof of the Scalable Commutativity Rule", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explains a flaw in the published proof of the Scalable\nCommutativity Rule (SCR), presents a revised and formally verified proof of the\nSCR in the Coq proof assistant, and discusses the insights and open questions\nraised from our experience proving the SCR.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2018 11:52:39 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Tsai", "Lillian", ""], ["Kohler", "Eddie", ""], ["Kaashoek", "M. Frans", ""], ["Zeldovich", "Nickolai", ""]]}, {"id": "1809.09858", "submitter": "Yackolley Amoussou-Guenou", "authors": "Yackolley Amoussou-Guenou (DILS, NPA), Antonella del Pozzo (DILS),\n  Maria Potop-Butucaru (NPA, LINCS), Sara Tucci-Piergiovanni (DILS)", "title": "Dissecting Tendermint", "comments": null, "journal-ref": "Networked Systems - 7th International Conference, NETYS 2019, Jun\n  2019, Marrakech, Morocco", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we analyze Tendermint proposed in [7], one of the most popular\nblockchains based on PBFT Consensus. The current paper dissects Tendermint\nunder various system communication models and Byzantine adversaries. Our\nmethodology consists in identifying the algorithmic principles of Tendermint\nnecessary for a specific combination of communication model-adversary. This\nmethodology allowed to identify bugs [3] in preliminary versions of the\nprotocol ([19], [7]) and to prove its correctness under the most adversarial\nconditions: an eventually synchronous communication model and asymmetric\nByzantine faults.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 09:09:34 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 09:46:31 GMT"}, {"version": "v3", "created": "Tue, 30 Jul 2019 06:43:11 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Amoussou-Guenou", "Yackolley", "", "DILS, NPA"], ["del Pozzo", "Antonella", "", "DILS"], ["Potop-Butucaru", "Maria", "", "NPA, LINCS"], ["Tucci-Piergiovanni", "Sara", "", "DILS"]]}, {"id": "1809.09930", "submitter": "Ben Karsin", "authors": "Michael Gowanlock and Ben Karsin", "title": "GPU Accelerated Similarity Self-Join for Multi-Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The self-join finds all objects in a dataset that are within a search\ndistance, epsilon, of each other; therefore, the self-join is a building block\nof many algorithms. We advance a GPU-accelerated self-join algorithm targeted\ntowards high dimensional data. The massive parallelism afforded by the GPU and\nhigh aggregate memory bandwidth makes the architecture well-suited for\ndata-intensive workloads. We leverage a grid-based, GPU-tailored index to\nperform range queries. We propose the following optimizations: (i) a trade-off\nbetween candidate set filtering and index search overhead by exploiting\nproperties of the index; (ii) reordering the data based on variance in each\ndimension to improve the filtering power of the index; and (iii) a pruning\nmethod for reducing the number of expensive distance calculations. Across most\nscenarios on real-world and synthetic datasets, our algorithm outperforms the\nparallel state-of-the-art approach. Exascale systems are converging on\nheterogeneous distributed-memory architectures. We show that an entity\npartitioning method can be utilized to achieve a balanced workload, and thus\ngood scalability for multi-GPU or distributed-memory self-joins.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 12:04:51 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Gowanlock", "Michael", ""], ["Karsin", "Ben", ""]]}, {"id": "1809.09972", "submitter": "Luiz Fernando Bittencourt", "authors": "Luiz F. Bittencourt, Roger Immich, Rizos Sakellariou, Nelson L. S. da\n  Fonseca, Edmundo R. M. Madeira, Marilia Curado, Leandro Villas, Luiz da\n  Silva, Craig Lee, Omer Rana", "title": "The Internet of Things, Fog and Cloud Continuum: Integration and\n  Challenges", "comments": "Preprint version - to be published in Elsevier's Internet of Things\n  journal", "journal-ref": null, "doi": "10.1016/j.iot.2018.09.005", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet of Things needs for computing power and storage are expected to\nremain on the rise in the next decade. Consequently, the amount of data\ngenerated by devices at the edge of the network will also grow. While cloud\ncomputing has been an established and effective way of acquiring computation\nand storage as a service to many applications, it may not be suitable to handle\nthe myriad of data from IoT devices and fulfill largely heterogeneous\napplication requirements. Fog computing has been developed to lie between IoT\nand the cloud, providing a hierarchy of computing power that can collect,\naggregate, and process data from/to IoT devices. Combining fog and cloud may\nreduce data transfers and communication bottlenecks to the cloud and also\ncontribute to reduced latencies, as fog computing resources exist closer to the\nedge. This paper examines this IoT-Fog-Cloud ecosystem and provides a\nliterature review from different facets of it: how it can be organized, how\nmanagement is being addressed, and how applications can benefit from it.\nLastly, we present challenging issues yet to be addressed in IoT-Fog-Cloud\ninfrastructures.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 13:31:22 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Bittencourt", "Luiz F.", ""], ["Immich", "Roger", ""], ["Sakellariou", "Rizos", ""], ["da Fonseca", "Nelson L. S.", ""], ["Madeira", "Edmundo R. M.", ""], ["Curado", "Marilia", ""], ["Villas", "Leandro", ""], ["da Silva", "Luiz", ""], ["Lee", "Craig", ""], ["Rana", "Omer", ""]]}, {"id": "1809.10023", "submitter": "Jalal Abdulbaqi", "authors": "Jalal Abdulbaqi", "title": "Programming at Exascale: Challenges and Innovations", "comments": "17 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supercomputers become faster as hardware and software technologies continue\nto evolve. Current supercomputers are capable of 1015 floating point operations\nper second (FLOPS) that called Petascale system. The High Performance Computer\n(HPC) community is Looking forward to the system with capability of 1018\n(FLOPS) that is called Exascale. Having a system to thousand times faster than\nthe previous one produces challenges to the high performance computer (HPC)\ncommunity. These challenges require innovation in software and hardware. In\nthis paper, the challenges posed for programming at Exascale systems are\nreviewed and the developments in the main programming models and systems are\nsurveyed.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 23:27:01 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Abdulbaqi", "Jalal", ""]]}, {"id": "1809.10046", "submitter": "Calvin Newport", "authors": "Seth Gilbert and James Maguire and Calvin Newport", "title": "On Bioelectric Algorithms: A Novel Application of Theoretical Computer\n  Science to Core Problems in Developmental Biology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cellular bioelectricity describes the biological phenomenon in which cells in\nliving tissue generate and maintain patterns of voltage gradients induced by\ndiffering concentrations of charged ions. A growing body of research suggests\nthat bioelectric patterns represent an ancient system that plays a key role in\nguiding many important developmental processes including tissue regeneration,\ntumor suppression, and embryogenesis. Understanding the relationship between\nhigh-level bioelectric patterns and low-level biochemical processes might also\nenable powerful new forms of synthetic biology. A key open question in this\narea is understanding how a collection of cells, interacting with each other\nand the extracellular environment only through simple ligand bindings and ion\nfluxes, can compute non-trivial patterns and perform non-trivial information\nprocessing tasks. The standard approach to this question is to model a given\nbioelectrical network as a system of differential equations and then explore\nits behavior using simulation techniques. In this paper, we propose applying a\ncomputational approach. In more detail, we present the cellular bioelectric\nmodel (CBM), a new computational model that captures the primary capabilities\nand constraints of bioelectric interactions between cells and their\nenvironment. We use this model to investigate several important topics in\ncellular bioelectricity, including symmetry breaking and information\nprocessing. Among other results, we describe and analyze a basic bioelectric\nstrategy the efficiently stabilizes arbitrary cell networks into maximal\nindependent sets (a structure known to play a role in the nervous system\ndevelopment of flys), and prove cells in our model are Turing complete in their\nability to process information encoded in their initial voltage potential.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 15:02:40 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Gilbert", "Seth", ""], ["Maguire", "James", ""], ["Newport", "Calvin", ""]]}, {"id": "1809.10170", "submitter": "Jiyuan Zhang", "authors": "Jiyuan Zhang, Franz Franchetti, Tze Meng Low", "title": "High Performance Zero-Memory Overhead Direct Convolutions", "comments": "the 35th International Conference on Machine Learning(ICML 2018),\n  camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computation of convolution layers in deep neural networks typically rely\non high performance routines that trade space for time by using additional\nmemory (either for packing purposes or required as part of the algorithm) to\nimprove performance. The problems with such an approach are two-fold. First,\nthese routines incur additional memory overhead which reduces the overall size\nof the network that can fit on embedded devices with limited memory capacity.\nSecond, these high performance routines were not optimized for performing\nconvolution, which means that the performance obtained is usually less than\nconventionally expected. In this paper, we demonstrate that direct convolution,\nwhen implemented correctly, eliminates all memory overhead, and yields\nperformance that is between 10% to 400% times better than existing high\nperformance implementations of convolution layers on conventional and embedded\nCPU architectures. We also show that a high performance direct convolution\nexhibits better scaling performance, i.e. suffers less performance drop, when\nincreasing the number of threads.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 00:48:12 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Zhang", "Jiyuan", ""], ["Franchetti", "Franz", ""], ["Low", "Tze Meng", ""]]}, {"id": "1809.10361", "submitter": "Chien-Sheng Yang", "authors": "Songze Li, Mingchao Yu, Chien-Sheng Yang, A. Salman Avestimehr,\n  Sreeram Kannan and Pramod Viswanath", "title": "PolyShard: Coded Sharding Achieves Linearly Scaling Efficiency and\n  Security Simultaneously", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's blockchain designs suffer from a trilemma claiming that no blockchain\nsystem can simultaneously achieve decentralization, security, and performance\nscalability. For current blockchain systems, as more nodes join the network,\nthe efficiency of the system (computation, communication, and storage) stays\nconstant at best. A leading idea for enabling blockchains to scale efficiency\nis the notion of sharding: different subsets of nodes handle different portions\nof the blockchain, thereby reducing the load for each individual node. However,\nexisting sharding proposals achieve efficiency scaling by compromising on trust\n- corrupting the nodes in a given shard will lead to the permanent loss of the\ncorresponding portion of data. In this paper, we settle the trilemma by\ndemonstrating a new protocol for coded storage and computation in blockchains.\nIn particular, we propose PolyShard: ``polynomially coded sharding'' scheme\nthat achieves information-theoretic upper bounds on the efficiency of the\nstorage, system throughput, as well as on trust, thus enabling a truly scalable\nsystem. We provide simulation results that numerically demonstrate the\nperformance improvement over state of the arts, and the scalability of the\nPolyShard system. Finally, we discuss potential enhancements, and highlight\npractical considerations in building such a system.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 06:15:40 GMT"}, {"version": "v2", "created": "Fri, 24 Jan 2020 21:46:02 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Li", "Songze", ""], ["Yu", "Mingchao", ""], ["Yang", "Chien-Sheng", ""], ["Avestimehr", "A. Salman", ""], ["Kannan", "Sreeram", ""], ["Viswanath", "Pramod", ""]]}, {"id": "1809.10505", "submitter": "Nikola Konstantinov", "authors": "Dan Alistarh, Torsten Hoefler, Mikael Johansson, Sarit Khirirat,\n  Nikola Konstantinov, C\\'edric Renggli", "title": "The Convergence of Sparsified Gradient Methods", "comments": "NIPS 2018 - Advances in Neural Information Processing Systems;\n  Authors in alphabetic order", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed training of massive machine learning models, in particular deep\nneural networks, via Stochastic Gradient Descent (SGD) is becoming commonplace.\nSeveral families of communication-reduction methods, such as quantization,\nlarge-batch methods, and gradient sparsification, have been proposed. To date,\ngradient sparsification methods - where each node sorts gradients by magnitude,\nand only communicates a subset of the components, accumulating the rest locally\n- are known to yield some of the largest practical gains. Such methods can\nreduce the amount of communication per step by up to three orders of magnitude,\nwhile preserving model accuracy. Yet, this family of methods currently has no\ntheoretical justification.\n  This is the question we address in this paper. We prove that, under analytic\nassumptions, sparsifying gradients by magnitude with local error correction\nprovides convergence guarantees, for both convex and non-convex smooth\nobjectives, for data-parallel SGD. The main insight is that sparsification\nmethods implicitly maintain bounds on the maximum impact of stale updates,\nthanks to selection by magnitude. Our analysis and empirical validation also\nreveal that these methods do require analytical conditions to converge well,\njustifying existing heuristics.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 13:23:35 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Alistarh", "Dan", ""], ["Hoefler", "Torsten", ""], ["Johansson", "Mikael", ""], ["Khirirat", "Sarit", ""], ["Konstantinov", "Nikola", ""], ["Renggli", "C\u00e9dric", ""]]}, {"id": "1809.10525", "submitter": "Vladimir Voloshinov", "authors": "Sergey A. Smirnov and Vladimir V. Voloshinov", "title": "Packing of Circles on Square Flat Torus as Global Optimization of Mixed\n  Integer Nonlinear problem", "comments": "9 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article demonstrates rather general approach to problems of discrete\ngeometry: treat them as global optimization problems to be solved by one of\ngeneral purpose solver implementing branch-and-bound algorithm (B&B). This\napproach may be used for various types of problems, i.e. Tammes problems,\nThomson problems, search of minimal potential energy of micro-clusters, etc.\nHere we consider a problem of densest packing of equal circles in special\ngeometrical object, so called square flat torus $\\mathbb{R}^2/\\mathbb{Z}^2$\nwith the induced metric. It is formulated as Mixed-Integer Nonlinear Problem\nwith linear and non-convex quadratic constraints.\n  The open-source B&B-solver SCIP, http://scip.zib.de, and its parallel\nimplementation ParaSCIP, http://ug.zib.de, had been used in computing\nexperiments to find \"very good\" approximations of optimal arrangements. The\nmain result is a confirmation of the conjecture on optimal packing for N=9 that\nwas published in 2012 by O. Musin and A. Nikitenko. To do that, ParaSCIP took\nabout 2000 CPU*hours (16 hours x 128 CPUs) of cluster HPC4/HPC5, National\nResearch Centre \"Kurchatov Institute\", http://ckp.nrcki.ru\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 14:07:10 GMT"}, {"version": "v2", "created": "Mon, 17 Dec 2018 17:52:45 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Smirnov", "Sergey A.", ""], ["Voloshinov", "Vladimir V.", ""]]}, {"id": "1809.10559", "submitter": "Natacha Crooks", "authors": "Natacha Crooks and Matthew Burke and Ethan Cecchetti and Sitar Harel\n  and Rachit Agarwal and Lorenzo Alvisi", "title": "Obladi: Oblivious Serializable Transactions in the Cloud", "comments": "21 pages, conference and appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the design and implementation of Obladi, the first system\nto provide ACID transactions while also hiding access patterns. Obladi uses as\nits building block oblivious RAM, but turns the demands of supporting\ntransactions into a performance opportunity. By executing transactions within\nepochs and delaying commit decisions until an epoch ends, Obladi reduces the\namortized bandwidth costs of oblivious storage and increases overall system\nthroughput. These performance gains, combined with new oblivious mechanisms for\nconcurrency control and recovery, allow Obladi to execute OLTP workloads with\nreasonable throughput: it comes within 5x to 12x of a non-oblivious baseline on\nthe TPC-C, SmallBank, and FreeHealth applications. Latency overheads, however,\nare higher (70x on TPC-C).\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 14:57:53 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Crooks", "Natacha", ""], ["Burke", "Matthew", ""], ["Cecchetti", "Ethan", ""], ["Harel", "Sitar", ""], ["Agarwal", "Rachit", ""], ["Alvisi", "Lorenzo", ""]]}, {"id": "1809.10596", "submitter": "David Koops", "authors": "David Koops", "title": "Predicting the confirmation time of Bitcoin transactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the probabilistic distribution of the confirmation time of Bitcoin\ntransactions, conditional on the current memory pool (i.e., the queue of\ntransactions awaiting confirmation). The results of this paper are particularly\ninteresting for users that want to make a Bitcoin transaction during\n`heavy-traffic situations', when the transaction demand exceeds the block\ncapacity. In such situations, Bitcoin users tend to bid up the transaction\nfees, in order to gain priority over other users that pay a lower fee. We argue\nthat the time until a Bitcoin transaction is confirmed can be modelled as a\nparticular stochastic fluid queueing process (to be precise: a\nCram\\'er-Lundberg process). We approximate the queueing process in two\ndifferent ways. The first approach leads to a lower bound on the confirmation\nprobability, which becomes increasingly tight as traffic decreases. The second\napproach relies on a diffusion approximation with a continuity correction,\nwhich becomes increasingly accurate as traffic intensifies. The accuracy of the\napproximations under different traffic loads are evaluated in a simulation\nstudy.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 15:58:12 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Koops", "David", ""]]}, {"id": "1809.10624", "submitter": "Niyazi Sorkunlu", "authors": "Niyazi Sorkunlu, Duc Thanh Anh Luong and Varun Chandola", "title": "dynamicMF: A Matrix Factorization Approach to Monitor Resource Usage in\n  High Performance Computing Systems", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  High performance computing (HPC) facilities consist of a large number of\ninterconnected computing units (or nodes) that execute highly complex\nscientific simulations to support scientific research. Monitoring such\nfacilities, in real-time, is essential to ensure that the system operates at\npeak efficiency. Such systems are typically monitored using a variety of\nmeasurement and log data which capture the state of the various components\nwithin the system at regular intervals of time. As modern HPC systems grow in\ncapacity and complexity, the data produced by current resource monitoring tools\nis at a scale that it is no longer feasible to be visually monitored by\nanalysts. We propose a method that transforms the multi-dimensional output of\nresource monitoring tools to a low dimensional representation that facilitates\nthe understanding of the behavior of a High Performance Computing (HPC) system.\nThe proposed method automatically extracts the low-dimensional signal in the\ndata which can be used to track the system efficiency and identify performance\nanomalies. The method models the resource usage data as a three dimensional\ntensor (capturing resource usage of all compute nodes for difference resources\nover time). A dynamic matrix factorization algorithm, called dynamicMF, is\nproposed to extract a low-dimensional temporal signal for each node, which is\nsubsequently fed into an anomaly detector. Results on resource usage data\ncollected from the Lonestar 4 system at the Texas Advanced Computing Center\nshow that the identified anomalies are correlated with actual anomalous events\nreported in the system log messages.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 15:08:50 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Sorkunlu", "Niyazi", ""], ["Luong", "Duc Thanh Anh", ""], ["Chandola", "Varun", ""]]}, {"id": "1809.10778", "submitter": "Victor Eijkhout", "authors": "Victor Eijkhout", "title": "Performance of MPI sends of non-contiguous data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an experimental investigation of the performance of MPI derived\ndatatypes. For messages up to the megabyte range most schemes perform\ncomparably to each other and to manual copying into a regular send buffer.\nHowever, for large messages the internal buffering of MPI causes differences in\nefficiency. The optimal scheme is a combination of packing and derived types.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 21:47:41 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Eijkhout", "Victor", ""]]}, {"id": "1809.10799", "submitter": "Zhao Zhang", "authors": "Zhao Zhang and Lei Huang and Uri Manor and Linjing Fang and Gabriele\n  Merlo and Craig Michoski and John Cazes and Niall Gaffney", "title": "FanStore: Enabling Efficient and Scalable I/O for Distributed Deep\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emerging Deep Learning (DL) applications introduce heavy I/O workloads on\ncomputer clusters. The inherent long lasting, repeated, and random file access\npattern can easily saturate the metadata and data service and negatively impact\nother users. In this paper, we present FanStore, a transient runtime file\nsystem that optimizes DL I/O on existing hardware/software stacks. FanStore\ndistributes datasets to the local storage of compute nodes, and maintains a\nglobal namespace. With the techniques of system call interception, distributed\nmetadata management, and generic data compression, FanStore provides a\nPOSIX-compliant interface with native hardware throughput in an efficient and\nscalable manner. Users do not have to make intrusive code changes to use\nFanStore and take advantage of the optimized I/O. Our experiments with\nbenchmarks and real applications show that FanStore can scale DL training to\n512 compute nodes with over 90\\% scaling efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 23:33:11 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Zhang", "Zhao", ""], ["Huang", "Lei", ""], ["Manor", "Uri", ""], ["Fang", "Linjing", ""], ["Merlo", "Gabriele", ""], ["Michoski", "Craig", ""], ["Cazes", "John", ""], ["Gaffney", "Niall", ""]]}, {"id": "1809.10895", "submitter": "Laurent Orgogozo", "authors": "L. Orgogozo (GET), N. Renon (CALMIP), C. Soulaine (IMFT), F. H\\'enon\n  (IMFT), S. K. Tomer (CESBIO), D. Labat (GET), O.S. Pokrovsky (GET), M.\n  Sekhar, R. Ababou (IMFT), M. Quintard (IMFT)", "title": "An open source massively parallel solver for Richards equation:\n  Mechanistic modelling of water fluxes at the watershed scale", "comments": null, "journal-ref": "Computer Physics Communications, Elsevier, 2014, 185 (12),\n  pp.3358-3371", "doi": "10.1016/j.cpc.2014.08.004", "report-no": null, "categories": "cs.CE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a massively parallel open source solver for Richards\nequation, named the RichardsFOAM solver. This solver has been developed in the\nframework of the open source generalist computational fluid dynamics tool box\nOpenFOAM (R) and is capable to deal with large scale problems in both space and\ntime. The source code for RichardsFOAM may be downloaded from the CPC program\nlibrary website.It exhibits good parallel performances (up to $\\sim$90%\nparallel efficiency with 1024 processors both in strong and weak scaling), and\nthe conditions required for obtaining such performances are analysed and\ndiscussed. These performances enable the mechanistic modelling of water fluxes\nat the scale of experimental watersheds (up to few square kilometres of surface\narea), and on time scales of decades to a century. Such a solver can be useful\nin various applications, such as environmental engineering for long term\ntransport of pollutants in soils, water engineering for assessing the impact of\nland settlement on water resources, or in the study of weathering processes on\nthe watersheds.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 07:36:52 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Orgogozo", "L.", "", "GET"], ["Renon", "N.", "", "CALMIP"], ["Soulaine", "C.", "", "IMFT"], ["H\u00e9non", "F.", "", "IMFT"], ["Tomer", "S. K.", "", "CESBIO"], ["Labat", "D.", "", "GET"], ["Pokrovsky", "O. S.", "", "GET"], ["Sekhar", "M.", "", "IMFT"], ["Ababou", "R.", "", "IMFT"], ["Quintard", "M.", "", "IMFT"]]}, {"id": "1809.10911", "submitter": "Andrei Panu", "authors": "Cristina Georgiana Calancea, Lenuta Alboaie, Andrei Panu", "title": "A SwarmESB Based Architecture for an European Healthcare Insurance\n  System in Compliance with GDPR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the everlasting development of technology and society, data privacy has\nproven to grow into a pressing issue. The bureaucratic state system seems to\nexpand the number of personal documents required for any kind of request.\nTherefore, it becomes obvious that the number of people having access to\ninformation that should be private is on the rise as well. This paper offers an\nalternative cloud integration solution centered on user data privacy, its main\npurpose being to help software services providers and public institutions to\ncomply with the General Data Protection Regulation. Throughout this proposal we\ndescribe how data confidentiality can be achieved by transitioning complex\nhuman procedures into a coordinated and decoupled swarm system, whose core lies\nwithin the \"Privacy by Design\" principles.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 08:28:35 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Calancea", "Cristina Georgiana", ""], ["Alboaie", "Lenuta", ""], ["Panu", "Andrei", ""]]}, {"id": "1809.10915", "submitter": "Andrei Panu", "authors": "Ioana Stanescu, Lenuta Alboaie, Andrei Panu", "title": "Blockchain and Smart-contracts Modeled in a SwarmESB Ecosystem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain has emerged as a trusted and secure distributed ledger for\ntransactions while also being decentralised, distributed and its legitimacy not\nguaranteed by a trusted authority. Since the appearance of Bitcoin, Blockchain\nhas known many implementations based on P2P architectures. This paper presents\nhow the blockchain and smart contracts technologies can be integrated into the\nSwarmESB ecosystem. SwarmESB is a framework that helps building distributed\napplications, which benefit from privacy and scalability features. Our proposal\nwill present the flexibility in building not only microservices based\napplications, but also decen-tralised applications employing blockchain and\nsmart-contracts by modeling a sample Dapp.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 08:35:56 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Stanescu", "Ioana", ""], ["Alboaie", "Lenuta", ""], ["Panu", "Andrei", ""]]}, {"id": "1809.10937", "submitter": "Tom\\'as F. Pena", "authors": "O. G. Lorenzo, M. L. Beco\\~na, T. F. Pena, J. C. Cabaleiro, J. A.\n  Lorenzo, and F. F. Rivera", "title": "New Thread Migration Strategies for NUMA Systems", "comments": "Unpublished work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multicore systems present on-board memory hierarchies and communication\nnetworks that influence performance when executing shared memory parallel\ncodes. Characterising this influence is complex, and understanding the effect\nof particular hardware configurations on different codes is of paramount\nimportance. In previous works, monitoring information extracted from hardware\ncounters at runtime has been used to characterise the behaviour of each thread\nin the parallel code in terms of the number of floating point operations per\nsecond, operational intensity, and latency of memory access. We propose to use\nthis information to guide thread migration strategies that improve execution\nefficiency by increasing locality and affinity. Different configurations of NAS\nParallel OpenMP benchmarks on multicores were used to validate the benefits of\nthe proposed thread migration strategies. Our proposed strategies produce up to\n70% improvement in scenarios where locality and affinity are low, there being a\nsmall degradation in performance for codes with high locality and affinity.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 09:56:05 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Lorenzo", "O. G.", ""], ["Beco\u00f1a", "M. L.", ""], ["Pena", "T. F.", ""], ["Cabaleiro", "J. C.", ""], ["Lorenzo", "J. A.", ""], ["Rivera", "F. F.", ""]]}, {"id": "1809.11060", "submitter": "Peter Robinson", "authors": "Martin Biely and Peter Robinson", "title": "On the Hardness of the Strongly Dependent Decision Problem", "comments": "Appeared in ICDCN 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present necessary and sufficient conditions for solving the strongly\ndependent decision (SDD) problem in various distributed systems. Our main\ncontribution is a novel characterization of the SDD problem based on point-set\ntopology. For partially synchronous systems, we show that any algorithm that\nsolves the SDD problem induces a set of executions that is closed with respect\nto the point-set topology. We also show that the SDD problem is not solvable in\nthe asynchronous system augmented with any arbitrarily strong failure\ndetectors.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 14:35:00 GMT"}, {"version": "v2", "created": "Sun, 6 Jan 2019 05:14:55 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Biely", "Martin", ""], ["Robinson", "Peter", ""]]}]