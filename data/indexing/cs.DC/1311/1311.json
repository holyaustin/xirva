[{"id": "1311.0058", "submitter": "R. J. Sobie", "authors": "Ian Gable, Michael Chester, Patrick Armstrong, Frank Berghaus, Andre\n  Charbonneau, Colin Leavett-Brown, Michael Paterson, Robert Prior, Randall\n  Sobie, Ryan Taylor", "title": "Dynamic web cache publishing for IaaS clouds using Shoal", "comments": "Conference paper at the 2013 Computing in HEP (CHEP) Conference,\n  Amsterdam", "journal-ref": null, "doi": "10.1088/1742-6596/513/3/032035", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have developed a highly scalable application, called Shoal, for tracking\nand utilizing a distributed set of HTTP web caches. Squid servers advertise\ntheir existence to the Shoal server via AMQP messaging by running Shoal Agent.\nThe Shoal server provides a simple REST interface that allows clients to\ndetermine their closest Squid cache. Our goal is to dynamically instantiate\nSquid caches on IaaS clouds in response to client demand. Shoal provides the\nVMs on IaaS clouds with the location of the nearest dynamically instantiated\nSquid Cache. In this paper, we describe the design and performance of Shoal.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2013 22:43:01 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Gable", "Ian", ""], ["Chester", "Michael", ""], ["Armstrong", "Patrick", ""], ["Berghaus", "Frank", ""], ["Charbonneau", "Andre", ""], ["Leavett-Brown", "Colin", ""], ["Paterson", "Michael", ""], ["Prior", "Robert", ""], ["Sobie", "Randall", ""], ["Taylor", "Ryan", ""]]}, {"id": "1311.0269", "submitter": "Peter Elmer", "authors": "David Abdurachmanov, Peter Elmer, Giulio Eulisse, Shahzad Muzaffar", "title": "Initial explorations of ARM processors for scientific computing", "comments": "Submitted to proceedings of the 15th International Workshop on\n  Advanced Computing and Analysis Techniques in Physics Research (ACAT2013),\n  Beijing. arXiv admin note: text overlap with arXiv:1311.1001", "journal-ref": null, "doi": "10.1088/1742-6596/523/1/012009", "report-no": null, "categories": "physics.comp-ph cs.DC cs.NA hep-ex", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power efficiency is becoming an ever more important metric for both high\nperformance and high throughput computing. Over the course of next decade it is\nexpected that flops/watt will be a major driver for the evolution of computer\narchitecture. Servers with large numbers of ARM processors, already ubiquitous\nin mobile computing, are a promising alternative to traditional x86-64\ncomputing. We present the results of our initial investigations into the use of\nARM processors for scientific computing applications. In particular we report\nthe results from our work with a current generation ARMv7 development board to\nexplore ARM-specific issues regarding the software development environment,\noperating system, performance benchmarks and issues for porting High Energy\nPhysics software.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2013 19:33:35 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2014 09:23:39 GMT"}], "update_date": "2014-10-24", "authors_parsed": [["Abdurachmanov", "David", ""], ["Elmer", "Peter", ""], ["Eulisse", "Giulio", ""], ["Muzaffar", "Shahzad", ""]]}, {"id": "1311.0272", "submitter": "Peter Elmer", "authors": "Kapil Arya, Gene Cooperman, Andrea Dotti, Peter Elmer", "title": "Use of checkpoint-restart for complex HEP software on traditional\n  architectures and Intel MIC", "comments": "Submitted to proceedings of the 15th International Workshop on\n  Advanced Computing and Analysis Techniques in Physics Research (ACAT2013),\n  Beijing", "journal-ref": null, "doi": "10.1088/1742-6596/523/1/012015", "report-no": null, "categories": "physics.comp-ph cs.DC cs.NA hep-ex", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process checkpoint-restart is a technology with great potential for use in\nHEP workflows. Use cases include debugging, reducing the startup time of\napplications both in offline batch jobs and the High Level Trigger, permitting\njob preemption in environments where spare CPU cycles are being used\nopportunistically and efficient scheduling of a mix of multicore and\nsingle-threaded jobs. We report on tests of checkpoint-restart technology using\nCMS software, Geant4-MT (multi-threaded Geant4), and the DMTCP (Distributed\nMultithreaded Checkpointing) package. We analyze both single- and\nmulti-threaded applications and test on both standard Intel x86 architectures\nand on Intel MIC. The tests with multi-threaded applications on Intel MIC are\nused to consider scalability and performance. These are considered an indicator\nof what the future may hold for many-core computing.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2013 19:40:39 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2014 08:37:58 GMT"}], "update_date": "2014-10-24", "authors_parsed": [["Arya", "Kapil", ""], ["Cooperman", "Gene", ""], ["Dotti", "Andrea", ""], ["Elmer", "Peter", ""]]}, {"id": "1311.0378", "submitter": "George Teodoro", "authors": "George Teodoro and Tahsin Kurc and Jun Kong and Lee Cooper and Joel\n  Saltz", "title": "Comparative Performance Analysis of Intel Xeon Phi, GPU, and CPU", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  We investigate and characterize the performance of an important class of\noperations on GPUs and Many Integrated Core (MIC) architectures. Our work is\nmotivated by applications that analyze low-dimensional spatial datasets\ncaptured by high resolution sensors, such as image datasets obtained from whole\nslide tissue specimens using microscopy image scanners. We identify the data\naccess and computation patterns of operations in object segmentation and\nfeature computation categories. We systematically implement and evaluate the\nperformance of these core operations on modern CPUs, GPUs, and MIC systems for\na microscopy image analysis application. Our results show that (1) the data\naccess pattern and parallelization strategy employed by the operations strongly\naffect their performance. While the performance on a MIC of operations that\nperform regular data access is comparable or sometimes better than that on a\nGPU; (2) GPUs are significantly more efficient than MICs for operations and\nalgorithms that irregularly access data. This is a result of the low\nperformance of the latter when it comes to random data access; (3) adequate\ncoordinated execution on MICs and CPUs using a performance aware task\nscheduling strategy improves about 1.29x over a first-come-first-served\nstrategy. The example application attained an efficiency of 84% in an execution\nwith of 192 nodes (3072 CPU cores and 192 MICs).\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2013 14:00:40 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Teodoro", "George", ""], ["Kurc", "Tahsin", ""], ["Kong", "Jun", ""], ["Cooper", "Lee", ""], ["Saltz", "Joel", ""]]}, {"id": "1311.0380", "submitter": "Peter Wittich", "authors": "S. Amerio, D. Bastieri, M. Corvo, A. Gianelle, W. Ketchum, T. Liu, A.\n  Lonardo, D. Lucchesi, S. Poprocki, R. Rivera, L. Tosoratto, P. Vicini, P.\n  Wittich", "title": "Many-core applications to online track reconstruction in HEP experiments", "comments": "Proceedings for the 20th International Conference on Computing in\n  High Energy and Nuclear Physics (CHEP); missing acks added", "journal-ref": null, "doi": "10.1088/1742-6596/513/1/012002", "report-no": null, "categories": "physics.ins-det cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interest in parallel architectures applied to real time selections is growing\nin High Energy Physics (HEP) experiments. In this paper we describe performance\nmeasurements of Graphic Processing Units (GPUs) and Intel Many Integrated Core\narchitecture (MIC) when applied to a typical HEP online task: the selection of\nevents based on the trajectories of charged particles. We use as benchmark a\nscaled-up version of the algorithm used at CDF experiment at Tevatron for\nonline track reconstruction - the SVT algorithm - as a realistic test-case for\nlow-latency trigger systems using new computing architectures for LHC\nexperiment. We examine the complexity/performance trade-off in porting existing\nserial algorithms to many-core devices. Measurements of both data processing\nand data transfer latency are shown, considering different I/O strategies\nto/from the parallel devices.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2013 14:37:09 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2013 20:58:15 GMT"}], "update_date": "2014-11-26", "authors_parsed": [["Amerio", "S.", ""], ["Bastieri", "D.", ""], ["Corvo", "M.", ""], ["Gianelle", "A.", ""], ["Ketchum", "W.", ""], ["Liu", "T.", ""], ["Lonardo", "A.", ""], ["Lucchesi", "D.", ""], ["Poprocki", "S.", ""], ["Rivera", "R.", ""], ["Tosoratto", "L.", ""], ["Vicini", "P.", ""], ["Wittich", "P.", ""]]}, {"id": "1311.0402", "submitter": "Yu-Hang Tang", "authors": "Yu-Hang Tang and George Em Karniadakis", "title": "Accelerating Dissipative Particle Dynamics Simulations on GPUs:\n  Algorithms, Numerics and Applications", "comments": null, "journal-ref": "Computer Physics Communications, 2014, 185(11), 2809 - 2822", "doi": "10.1016/j.cpc.2014.06.015", "report-no": null, "categories": "cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a scalable dissipative particle dynamics simulation code, fully\nimplemented on the Graphics Processing Units (GPUs) using a hybrid CUDA/MPI\nprogramming model, which achieves 10-30 times speedup on a single GPU over 16\nCPU cores and almost linear weak scaling across a thousand nodes. A unified\nframework is developed within which the efficient generation of the neighbor\nlist and maintaining particle data locality are addressed. Our algorithm\ngenerates strictly ordered neighbor lists in parallel, while the construction\nis deterministic and makes no use of atomic operations or sorting. Such\nneighbor list leads to optimal data loading efficiency when combined with a\ntwo-level particle reordering scheme. A faster in situ generation scheme for\nGaussian random numbers is proposed using precomputed binary signatures. We\ndesigned custom transcendental functions that are fast and accurate for\nevaluating the pairwise interaction. The correctness and accuracy of the code\nis verified through a set of test cases simulating Poiseuille flow and\nspontaneous vesicle formation. Computer benchmarks demonstrate the speedup of\nour implementation over the CPU implementation as well as strong and weak\nscalability. A large-scale simulation of spontaneous vesicle formation\nconsisting of 128 million particles was conducted to further illustrate the\npracticality of our code in real-world applications.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2013 18:51:53 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Tang", "Yu-Hang", ""], ["Karniadakis", "George Em", ""]]}, {"id": "1311.0590", "submitter": "Hwancheol Jeong", "authors": "Hwancheol Jeong, Weonjong Lee, Jeonghwan Pak, Kwang-jong Choi,\n  Sang-Hyun Park, Jun-sik Yoo, Joo Hwan Kim, Joungjin Lee, and Young Woo Lee", "title": "Performance of Kepler GTX Titan GPUs and Xeon Phi System", "comments": "7 pages, 6 figures, 3 tables, Contribution to proceedings of the 31st\n  International Symposium on Lattice Field Theory (Lattice 2013), July 29 -\n  August 3, 2013", "journal-ref": "PoS (LATTICE 2013) 423", "doi": null, "report-no": null, "categories": "physics.comp-ph cs.DC hep-lat", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NVIDIA's new architecture, Kepler improves GPU's performance significantly\nwith the new streaming multiprocessor SMX. Along with the performance, NVIDIA\nhas also introduced many new technologies such as direct parallelism, hyper-Q\nand GPU Direct with RDMA. Apart from other usual GPUs, NVIDIA also released\nanother Kepler 'GeForce' GPU named GTX Titan. GeForce GTX Titan is not only\ngood for gaming but also good for high performance computing with CUDA.\nNevertheless, it is remarkably cheaper than Kepler Tesla GPUs. We investigate\nthe performance of GTX Titan and find out how to optimize a CUDA code\nappropriately for it. Meanwhile, Intel has launched its new many integrated\ncore (MIC) system, Xeon Phi. A Xeon Phi coprocessor could provide similar\nperformance with NVIDIA Kepler GPUs theoretically but, in reality, it turns out\nthat its performance is significantly inferior to GTX Titan.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2013 06:37:45 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Jeong", "Hwancheol", ""], ["Lee", "Weonjong", ""], ["Pak", "Jeonghwan", ""], ["Choi", "Kwang-jong", ""], ["Park", "Sang-Hyun", ""], ["Yoo", "Jun-sik", ""], ["Kim", "Joo Hwan", ""], ["Lee", "Joungjin", ""], ["Lee", "Young Woo", ""]]}, {"id": "1311.0636", "submitter": "Dhruv Mahajan", "authors": "Dhruv Mahajan, S. Sathiya Keerthi, S. Sundararajan, Leon Bottou", "title": "A Parallel SGD method with Strong Convergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel parallel stochastic gradient descent (SGD) method\nthat is obtained by applying parallel sets of SGD iterations (each set\noperating on one node using the data residing in it) for finding the direction\nin each iteration of a batch descent method. The method has strong convergence\nproperties. Experiments on datasets with high dimensional feature spaces show\nthe value of this method.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2013 10:31:11 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Mahajan", "Dhruv", ""], ["Keerthi", "S. Sathiya", ""], ["Sundararajan", "S.", ""], ["Bottou", "Leon", ""]]}, {"id": "1311.0728", "submitter": "Alaa Elnashar", "authors": "Alaa I. El-Nashar and Nakamura Masaki", "title": "To parallelize or not to parallelize, bugs issue", "comments": null, "journal-ref": "International Journal of Intelligent Computing and Information\n  Science, Egypt, IJICIS, Vol.10, No. 2, JULY 2010", "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Program correctness is one of the most difficult challenges in parallel\nprogramming. Message Passing Interface MPI is widely used in writing parallel\napplications. Since MPI is not a compiled language, the programmer will be\nenfaced with several programming bugs.This paper presents the most common\nprogramming bugs arise in MPI programs to help the programmer to compromise\nbetween the advantage of parallelism and the extra effort needed to detect and\nfix such bugs. An algebraic specification of an MPI-like programming language,\ncalled Simple MPI (SMPI), to be used in writing MPI programs specification has\nalso been proposed. In addition, both nondeterminacy and deadlocks arise in\nSMPI programs have been verified using Maud system.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2013 15:19:47 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["El-Nashar", "Alaa I.", ""], ["Masaki", "Nakamura", ""]]}, {"id": "1311.0731", "submitter": "Alaa Elnashar", "authors": "Alaa I. Elnashar", "title": "To parallelize or not to parallelize, control and data flow issue", "comments": null, "journal-ref": "International Journal of Intelligent Computing and Information\n  Science IJICS, Egypt, v. 9, no. 2, July 2009", "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New trends towards multiple core processors imply using standard programming\nmodels to develop efficient, reliable and portable programs for distributed\nmemory multiprocessors and workstation PC clusters. Message passing using MPI\nis widely used to write efficient, reliable and portable applications. Control\nand data flow analysis concepts, techniques and tools are needed to understand\nand analyze MPI programs. If our point of interest is the program control and\ndata flow analysis, to decide to parallelize or not to parallelize our\napplications, there is a question to be answered, \" Can the existing concepts,\ntechniques and tools used to analyze sequential programs also be used to\nanalyze parallel ones written in MPI?\". In this paper we'll try to answer this\nquestion.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2013 15:24:47 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Elnashar", "Alaa I.", ""]]}, {"id": "1311.0798", "submitter": "Stephane Rovedakis", "authors": "L\\'elia Blin (LIP6), Shlomi Dolev, Maria Gradinariu Potop-Butucaru\n  (LIP6), Stephane Rovedakis (CEDRIC)", "title": "Fast Self-Stabilizing Minimum Spanning Tree Construction Using Compact\n  Nearest Common Ancestor Labeling Scheme", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel self-stabilizing algorithm for minimum spanning tree (MST)\nconstruction. The space complexity of our solution is $O(\\log^2n)$ bits and it\nconverges in $O(n^2)$ rounds. Thus, this algorithm improves the convergence\ntime of previously known self-stabilizing asynchronous MST algorithms by a\nmultiplicative factor $\\Theta(n)$, to the price of increasing the best known\nspace complexity by a factor $O(\\log n)$. The main ingredient used in our\nalgorithm is the design, for the first time in self-stabilizing settings, of a\nlabeling scheme for computing the nearest common ancestor with only\n$O(\\log^2n)$ bits.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2013 18:17:47 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Blin", "L\u00e9lia", "", "LIP6"], ["Dolev", "Shlomi", "", "LIP6"], ["Potop-Butucaru", "Maria Gradinariu", "", "LIP6"], ["Rovedakis", "Stephane", "", "CEDRIC"]]}, {"id": "1311.0864", "submitter": "Alaa Elnashar", "authors": "Alaa Ismail Elnashar, Sultan Aljahdali and Mosaid Al Sadhan", "title": "Two automated techniques for analyzing and debugging Mpi-based programs", "comments": "arXiv admin note: substantial text overlap with arXiv:1311.0731", "journal-ref": null, "doi": null, "report-no": "978-1-880843-83-3/ISCA CAINE/November 2011", "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Message Passing Interface (MPI) is the most commonly used paradigm in writing\nparallel programs since it can be employed not only within a single processing\nnode but also across several connected ones. Data flow analysis concepts,\ntechniques and tools are needed to understand and analyze MPI-based programs to\ndetect bugs arise in these programs. In this paper we propose two automated\ntechniques to analyze and debug MPI-based programs source codes.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2013 15:32:51 GMT"}], "update_date": "2013-11-06", "authors_parsed": [["Elnashar", "Alaa Ismail", ""], ["Aljahdali", "Sultan", ""], ["Sadhan", "Mosaid Al", ""]]}, {"id": "1311.1001", "submitter": "Peter Elmer", "authors": "David Abdurachmanov, Kapil Arya, Josh Bendavid, Tommaso Boccali, Gene\n  Cooperman, Andrea Dotti, Peter Elmer, Giulio Eulisse, Francesco Giacomini,\n  Christopher D. Jones, Matteo Manzali, Shahzad Muzaffar", "title": "Explorations of the viability of ARM and Xeon Phi for physics processing", "comments": "Submitted to proceedings of the 20th International Conference on\n  Computing in High Energy and Nuclear Physics (CHEP13), Amsterdam", "journal-ref": null, "doi": "10.1088/1742-6596/513/5/052008", "report-no": null, "categories": "physics.comp-ph cs.DC hep-ex", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report on our investigations into the viability of the ARM processor and\nthe Intel Xeon Phi co-processor for scientific computing. We describe our\nexperience porting software to these processors and running benchmarks using\nreal physics applications to explore the potential of these processors for\nproduction physics processing.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2013 09:54:14 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2014 18:38:34 GMT"}], "update_date": "2014-10-24", "authors_parsed": [["Abdurachmanov", "David", ""], ["Arya", "Kapil", ""], ["Bendavid", "Josh", ""], ["Boccali", "Tommaso", ""], ["Cooperman", "Gene", ""], ["Dotti", "Andrea", ""], ["Elmer", "Peter", ""], ["Eulisse", "Giulio", ""], ["Giacomini", "Francesco", ""], ["Jones", "Christopher D.", ""], ["Manzali", "Matteo", ""], ["Muzaffar", "Shahzad", ""]]}, {"id": "1311.1006", "submitter": "Marcus Holm", "authors": "Marcus Holm and Stefan Engblom and Anders Goude and Sverker Holmgren", "title": "Dynamic autotuning of adaptive fast multipole methods on hybrid\n  multicore CPU & GPU systems", "comments": null, "journal-ref": "SIAM J. Sci. Comput. 36(4):C376-C399 (2014)", "doi": "10.1137/130943595", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss an implementation of adaptive fast multipole methods targeting\nhybrid multicore CPU- and GPU-systems. From previous experiences with the\ncomputational profile of our version of the fast multipole algorithm, suitable\nparts are off-loaded to the GPU, while the remaining parts are threaded and\nexecuted concurrently by the CPU. The parameters defining the algorithm affects\nthe performance and by measuring this effect we are able to dynamically balance\nthe algorithm towards optimal performance. Our setup uses the dynamic nature of\nthe computations and is therefore of general character.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2013 10:31:57 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2013 13:24:00 GMT"}, {"version": "v3", "created": "Mon, 17 Mar 2014 14:34:27 GMT"}], "update_date": "2014-09-03", "authors_parsed": [["Holm", "Marcus", ""], ["Engblom", "Stefan", ""], ["Goude", "Anders", ""], ["Holmgren", "Sverker", ""]]}, {"id": "1311.1084", "submitter": "Luca Sanguinetti", "authors": "Massimo Monti, Luca Sanguinetti, Christian Tschudin, Marco Luise", "title": "A Chemistry-Inspired Framework for Achieving Consensus in Wireless\n  Sensor Networks", "comments": "12 pages, 10 figures, submitted to IEEE Sensors Journal", "journal-ref": null, "doi": "10.1109/JSEN.2013.2281208", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to show how simple interaction mechanisms, inspired\nby chemical systems, can provide the basic tools to design and analyze a\nmathematical model for achieving consensus in wireless sensor networks,\ncharacterized by balanced directed graphs. The convergence and stability of the\nmodel are first proven by using new mathematical tools, which are borrowed\ndirectly from chemical theory, and then validated by means of simulation\nresults, for different network topologies and number of sensors. The underlying\nchemical theory is also used to derive simple interaction rules that may\naccount for practical issues, such as the estimation of the number of neighbors\nand the robustness against perturbations. Finally, the proposed chemical\nsolution is validated under real-world conditions by means of a four-node\nhardware implementation where the exchange of information among nodes takes\nplace in a distributed manner (with no need for any admission control and\nsynchronism procedure), simply relying on the transmission of a pulse whose\nrate is proportional to the state of each sensor.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2013 08:51:45 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Monti", "Massimo", ""], ["Sanguinetti", "Luca", ""], ["Tschudin", "Christian", ""], ["Luise", "Marco", ""]]}, {"id": "1311.1714", "submitter": "Christian Schulz", "authors": "Peter Sanders and Christian Schulz", "title": "KaHIP v3.00 -- Karlsruhe High Quality Partitioning -- User Guide", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper severs as a user guide to the graph partitioning framework KaHIP\n(Karlsruhe High Quality Partitioning). We give a rough overview of the\ntechniques used within the framework and describe the user interface as well as\nthe file formats used. Moreover, we provide a short description of the current\nlibrary functions provided within the framework. Since version 3.00 we support\nmultilevel partitioning, memetic algorithms, distributed and shared-memory\nparallel algorithms, node separator and ordering algorithms, edge partitioning\nalgorithms as well as ILP solvers.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2013 15:38:09 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2014 12:26:23 GMT"}, {"version": "v3", "created": "Fri, 14 Feb 2014 10:46:48 GMT"}, {"version": "v4", "created": "Mon, 19 May 2014 09:46:47 GMT"}, {"version": "v5", "created": "Fri, 21 Nov 2014 12:52:51 GMT"}, {"version": "v6", "created": "Wed, 1 Mar 2017 12:16:21 GMT"}, {"version": "v7", "created": "Fri, 4 Jan 2019 11:18:22 GMT"}, {"version": "v8", "created": "Mon, 3 Aug 2020 10:27:46 GMT"}, {"version": "v9", "created": "Wed, 5 Aug 2020 08:01:08 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Sanders", "Peter", ""], ["Schulz", "Christian", ""]]}, {"id": "1311.1741", "submitter": "Roberto Ammendola", "authors": "Roberto Ammendola, Andrea Biagioni, Ottorino Frezza, Francesca Lo\n  Cicero, Pier Stanislao Paolucci, Alessandro Lonardo, Davide Rossetti,\n  Francesco Simula, Laura Tosoratto, Piero Vicini", "title": "Architectural improvements and 28 nm FPGA implementation of the APEnet+\n  3D Torus network for hybrid HPC systems", "comments": "Proceedings for the 20th International Conference on Computing in\n  High Energy and Nuclear Physics (CHEP)", "journal-ref": null, "doi": "10.1088/1742-6596/513/5/052002", "report-no": null, "categories": "cs.AR cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern Graphics Processing Units (GPUs) are now considered accelerators for\ngeneral purpose computation. A tight interaction between the GPU and the\ninterconnection network is the strategy to express the full potential on\ncapability computing of a multi-GPU system on large HPC clusters; that is the\nreason why an efficient and scalable interconnect is a key technology to\nfinally deliver GPUs for scientific HPC. In this paper we show the latest\narchitectural and performance improvement of the APEnet+ network fabric, a\nFPGA-based PCIe board with 6 fully bidirectional off-board links with 34 Gbps\nof raw bandwidth per direction, and X8 Gen2 bandwidth towards the host PC. The\nboard implements a Remote Direct Memory Access (RDMA) protocol that leverages\nupon peer-to-peer (P2P) capabilities of Fermi- and Kepler-class NVIDIA GPUs to\nobtain real zero-copy, low-latency GPU-to-GPU transfers. Finally, we report on\nthe development activities for 2013 focusing on the adoption of the latest\ngeneration 28 nm FPGAs and the preliminary tests performed on this new\nplatform.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2013 17:00:02 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2013 19:48:36 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Ammendola", "Roberto", ""], ["Biagioni", "Andrea", ""], ["Frezza", "Ottorino", ""], ["Cicero", "Francesca Lo", ""], ["Paolucci", "Pier Stanislao", ""], ["Lonardo", "Alessandro", ""], ["Rossetti", "Davide", ""], ["Simula", "Francesco", ""], ["Tosoratto", "Laura", ""], ["Vicini", "Piero", ""]]}, {"id": "1311.1753", "submitter": "Rolf Andreassen", "authors": "R. Andreassen, B. T. Meadows, M. de Silva, M. D. Sokoloff, and K.\n  Tomko", "title": "GooFit: A library for massively parallelising maximum-likelihood fits", "comments": "Presented at the CHEP 2013 conference", "journal-ref": null, "doi": "10.1088/1742-6596/513/5/052003", "report-no": null, "categories": "cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fitting complicated models to large datasets is a bottleneck of many\nanalyses. We present GooFit, a library and tool for constructing\narbitrarily-complex probability density functions (PDFs) to be evaluated on\nnVidia GPUs or on multicore CPUs using OpenMP. The massive parallelisation of\ndividing up event calculations between hundreds of processors can achieve\nspeedups of factors 200-300 in real-world problems.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2013 17:18:42 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Andreassen", "R.", ""], ["Meadows", "B. T.", ""], ["de Silva", "M.", ""], ["Sokoloff", "M. D.", ""], ["Tomko", "K.", ""]]}, {"id": "1311.1884", "submitter": "Saurabh Jha", "authors": "Vijay Menon, Saurabh Jha", "title": "A Parallel Simulated Annealing Approach for the Mirrored Traveling\n  Tournament Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Traveling Tournament Problem (TTP) is a benchmark problem in sports\nscheduling and has been extensively studied in recent years. The Mirrored\nTraveling Tournament Problem (mTTP) is variation of the TTP that represents\ncertain types of sports scheduling problems where the main objective is to\nminimize the total distance traveled by all the participating teams. In this\npaper we test a parallel simulated annealing approach for solving the mTTP\nusing OpenMP on shared memory systems and we found that this approach is\nsuperior especially with respect to the number of solution instances that are\nprobed per second. We also see that there is significant speed up of 1.5x -\n2.2x in terms of number of solutions explored per unit time.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2013 06:16:43 GMT"}], "update_date": "2013-11-12", "authors_parsed": [["Menon", "Vijay", ""], ["Jha", "Saurabh", ""]]}, {"id": "1311.1907", "submitter": "Vibha  Rajput", "authors": "Vibha Rajput, Alok Katiyar", "title": "Proactive bottleneck performance analysis in parallel computing using\n  openMP", "comments": "8 Pages,6 figure", "journal-ref": "IJASCSE, Volume 2, Issue 5, 2013", "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The aim of parallel computing is to increase an application performance by\nexecuting the application on multiple processors. OpenMP is an API that\nsupports multi platform shared memory programming model and shared-memory\nprograms are typically executed by multiple threads. The use of multi threading\ncan enhance the performance of application but its excessive use can degrade\nthe performance. This paper describes a novel approach to avoid bottlenecks in\napplication and provide some techniques to improve performance in OpenMP\napplication. This paper analyzes bottleneck performance as bottleneck inhibits\nperformance. Performance of multi threaded applications is limited by a variety\nof bottlenecks, e.g. critical sections, barriers and so on. This paper provides\nsome tips how to avoid performance bottleneck problems. This paper focuses on\nhow to reduce overheads and overall execution time to get better performance of\napplication.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2013 09:17:36 GMT"}], "update_date": "2013-11-12", "authors_parsed": [["Rajput", "Vibha", ""], ["Katiyar", "Alok", ""]]}, {"id": "1311.2019", "submitter": "Crist\\'obal Camarero", "authors": "Crist\\'obal Camarero, Carmen Mart\\'inez and Ram\\'on Beivide", "title": "Symmetric Interconnection Networks from Cubic Crystal Lattices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Torus networks of moderate degree have been widely used in the supercomputer\nindustry. Tori are superb when used for executing applications that require\nnear-neighbor communications. Nevertheless, they are not so good when dealing\nwith global communications. Hence, typical 3D implementations have evolved to\n5D networks, among other reasons, to reduce network distances. Most of these\nbig systems are mixed-radix tori which are not the best option for minimizing\ndistances and efficiently using network resources. This paper is focused on\nimproving the topological properties of these networks.\n  By using integral matrices to deal with Cayley graphs over Abelian groups, we\nhave been able to propose and analyze a family of high-dimensional grid-based\ninterconnection networks. As they are built over $n$-dimensional grids that\ninduce a regular tiling of the space, these topologies have been denoted\n\\textsl{lattice graphs}. We will focus on cubic crystal lattices for modeling\nsymmetric 3D networks. Other higher dimensional networks can be composed over\nthese graphs, as illustrated in this research. Easy network partitioning can\nalso take advantage of this network composition operation. Minimal routing\nalgorithms are also provided for these new topologies. Finally, some practical\nissues such as implementability and preliminary performance evaluations have\nbeen addressed.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2013 16:57:34 GMT"}], "update_date": "2013-11-11", "authors_parsed": [["Camarero", "Crist\u00f3bal", ""], ["Mart\u00ednez", "Carmen", ""], ["Beivide", "Ram\u00f3n", ""]]}, {"id": "1311.2208", "submitter": "Ken Bloom", "authors": "Kenneth Bloom and Richard Gerber", "title": "Report of the Snowmass 2013 Computing Frontier Working Group on\n  Distributed Computing and Facility Infrastructures", "comments": "19 pages, 4 figures; ; to be included in proceedings of Community\n  Summer Study (\"Snowmass\") 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.DC hep-ex hep-lat hep-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the report of the Snowmass 2013 Computing Frontier Working Group on\nDistributed Computing and Facility Infrastructures.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2013 20:33:50 GMT"}], "update_date": "2013-11-13", "authors_parsed": [["Bloom", "Kenneth", ""], ["Gerber", "Richard", ""]]}, {"id": "1311.2426", "submitter": "Jakob Blomer", "authors": "J. Blomer, D. Berzano, P. Buncic, I. Charalampidis, G. Ganis, G.\n  Lestaris, R. Meusel, V. Nicolaou", "title": "Micro-CernVM: Slashing the Cost of Building and Deploying Virtual\n  Machines", "comments": "Conference paper at the 2013 Computing in High Energy Physics (CHEP)\n  Conference, Amsterdam", "journal-ref": null, "doi": "10.1088/1742-6596/513/3/032009", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The traditional virtual machine building and and deployment process is\ncentered around the virtual machine hard disk image. The packages comprising\nthe VM operating system are carefully selected, hard disk images are built for\na variety of different hypervisors, and images have to be distributed and\ndecompressed in order to instantiate a virtual machine. Within the HEP\ncommunity, the CernVM File System has been established in order to decouple the\ndistribution from the experiment software from the building and distribution of\nthe VM hard disk images.\n  We show how to get rid of such pre-built hard disk images altogether. Due to\nthe high requirements on POSIX compliance imposed by HEP application software,\nCernVM-FS can also be used to host and boot a Linux operating system. This\nallows the use of a tiny bootable CD image that comprises only a Linux kernel\nwhile the rest of the operating system is provided on demand by CernVM-FS. This\napproach speeds up the initial instantiation time and reduces virtual machine\nimage sizes by an order of magnitude. Furthermore, security updates can be\ndistributed instantaneously through CernVM-FS. By leveraging the fact that\nCernVM-FS is a versioning file system, a historic analysis environment can be\neasily re-spawned by selecting the corresponding CernVM-FS file system\nsnapshot.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2013 12:22:07 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Blomer", "J.", ""], ["Berzano", "D.", ""], ["Buncic", "P.", ""], ["Charalampidis", "I.", ""], ["Ganis", "G.", ""], ["Lestaris", "G.", ""], ["Meusel", "R.", ""], ["Nicolaou", "V.", ""]]}, {"id": "1311.2444", "submitter": "Gesualdo Scutari", "authors": "Francisco Facchinei and Simone Sagratella and Gesualdo Scutari", "title": "Flexible Parallel Algorithms for Big Data Optimization", "comments": "submitted to IEEE ICASSP 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a decomposition framework for the parallel optimization of the sum\nof a differentiable function and a (block) separable nonsmooth, convex one. The\nlatter term is typically used to enforce structure in the solution as, for\nexample, in Lasso problems. Our framework is very flexible and includes both\nfully parallel Jacobi schemes and Gauss-Seidel (Southwell-type) ones, as well\nas virtually all possibilities in between (e.g., gradient- or Newton-type\nmethods) with only a subset of variables updated at each iteration. Our\ntheoretical convergence results improve on existing ones, and numerical results\nshow that the new method compares favorably to existing algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2013 14:01:37 GMT"}], "update_date": "2013-11-12", "authors_parsed": [["Facchinei", "Francisco", ""], ["Sagratella", "Simone", ""], ["Scutari", "Gesualdo", ""]]}, {"id": "1311.2663", "submitter": "Shandian Zhe", "authors": "Shandian Zhe and Yuan Qi and Youngja Park and Ian Molloy and Suresh\n  Chari", "title": "DinTucker: Scaling up Gaussian process models on multidimensional arrays\n  with billions of elements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infinite Tucker Decomposition (InfTucker) and random function prior models,\nas nonparametric Bayesian models on infinite exchangeable arrays, are more\npowerful models than widely-used multilinear factorization methods including\nTucker and PARAFAC decomposition, (partly) due to their capability of modeling\nnonlinear relationships between array elements. Despite their great predictive\nperformance and sound theoretical foundations, they cannot handle massive data\ndue to a prohibitively high training time. To overcome this limitation, we\npresent Distributed Infinite Tucker (DINTUCKER), a large-scale nonlinear tensor\ndecomposition algorithm on MAPREDUCE. While maintaining the predictive accuracy\nof InfTucker, it is scalable on massive data. DINTUCKER is based on a new\nhierarchical Bayesian model that enables local training of InfTucker on\nsubarrays and information integration from all local training results. We use\ndistributed stochastic gradient descent, coupled with variational inference, to\ntrain this model. We apply DINTUCKER to multidimensional arrays with billions\nof elements from applications in the \"Read the Web\" project (Carlson et al.,\n2010) and in information security and compare it with the state-of-the-art\nlarge-scale tensor decomposition method, GigaTensor. On both datasets,\nDINTUCKER achieves significantly higher prediction accuracy with less\ncomputational time.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2013 02:36:03 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2013 23:50:57 GMT"}, {"version": "v3", "created": "Sun, 15 Dec 2013 13:56:18 GMT"}, {"version": "v4", "created": "Thu, 23 Jan 2014 05:49:44 GMT"}, {"version": "v5", "created": "Sat, 1 Feb 2014 14:35:04 GMT"}], "update_date": "2014-02-04", "authors_parsed": [["Zhe", "Shandian", ""], ["Qi", "Yuan", ""], ["Park", "Youngja", ""], ["Molloy", "Ian", ""], ["Chari", "Suresh", ""]]}, {"id": "1311.2839", "submitter": "He Sun", "authors": "Zeyu Guo and He Sun", "title": "Gossip vs. Markov Chains, and Randomness-Efficient Rumor Spreading", "comments": "41 pages, 1 figure. arXiv admin note: substantial text overlap with\n  arXiv:1304.1359", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study gossip algorithms for the rumor spreading problem which asks one\nnode to deliver a rumor to all nodes in an unknown network. We present the\nfirst protocol for any expander graph $G$ with $n$ nodes such that, the\nprotocol informs every node in $O(\\log n)$ rounds with high probability, and\nuses $\\tilde{O}(\\log n)$ random bits in total. The runtime of our protocol is\ntight, and the randomness requirement of $\\tilde{O}(\\log n)$ random bits almost\nmatches the lower bound of $\\Omega(\\log n)$ random bits for dense graphs. We\nfurther show that, for many graph families, polylogarithmic number of random\nbits in total suffice to spread the rumor in $O(\\mathrm{poly}\\log n)$ rounds.\nThese results together give us an almost complete understanding of the\nrandomness requirement of this fundamental gossip process.\n  Our analysis relies on unexpectedly tight connections among gossip processes,\nMarkov chains, and branching programs. First, we establish a connection between\nrumor spreading processes and Markov chains, which is used to approximate the\nrumor spreading time by the mixing time of Markov chains. Second, we show a\nreduction from rumor spreading processes to branching programs, and this\nreduction provides a general framework to derandomize gossip processes. In\naddition to designing rumor spreading protocols, these novel techniques may\nhave applications in studying parallel and multiple random walks, and\nrandomness complexity of distributed algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2013 17:09:25 GMT"}], "update_date": "2013-11-14", "authors_parsed": [["Guo", "Zeyu", ""], ["Sun", "He", ""]]}, {"id": "1311.2851", "submitter": "Nihar Shah", "authors": "Nihar B. Shah, Kangwook Lee, Kannan Ramchandran", "title": "When Do Redundant Requests Reduce Latency ?", "comments": "Extended version of paper presented at Allerton Conference 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several systems possess the flexibility to serve requests in more than one\nway. For instance, a distributed storage system storing multiple replicas of\nthe data can serve a request from any of the multiple servers that store the\nrequested data, or a computational task may be performed in a compute-cluster\nby any one of multiple processors. In such systems, the latency of serving the\nrequests may potentially be reduced by sending \"redundant requests\": a request\nmay be sent to more servers than needed, and it is deemed served when the\nrequisite number of servers complete service. Such a mechanism trades off the\npossibility of faster execution of at least one copy of the request with the\nincrease in the delay due to an increased load on the system. Due to this\ntradeoff, it is unclear when redundant requests may actually help. Several\nrecent works empirically evaluate the latency performance of redundant requests\nin diverse settings.\n  This work aims at an analytical study of the latency performance of redundant\nrequests, with the primary goals of characterizing under what scenarios sending\nredundant requests will help (and under what scenarios they will not help), as\nwell as designing optimal redundant-requesting policies. We first present a\nmodel that captures the key features of such systems. We show that when service\ntimes are i.i.d. memoryless or \"heavier\", and when the additional copies of\nalready-completed jobs can be removed instantly, redundant requests reduce the\naverage latency. On the other hand, when service times are \"lighter\" or when\nservice times are memoryless and removal of jobs is not instantaneous, then not\nhaving any redundancy in the requests is optimal under high loads. Our results\nhold for arbitrary arrival processes.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2013 02:43:11 GMT"}], "update_date": "2013-11-13", "authors_parsed": [["Shah", "Nihar B.", ""], ["Lee", "Kangwook", ""], ["Ramchandran", "Kannan", ""]]}, {"id": "1311.2927", "submitter": "Archana Kale", "authors": "Archana Kale, Amitkumar Patil, Supratim Biswas", "title": "Parallelization of Loops with Variable Distance Data Dependences", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extent of parallelization of a loop is largely determined by the\ndependences between its statements. While dependence free loops are fully\nparallelizable, those with loop carried dependences are not. Dependence\ndistance is a measure of absolute difference between a pair of dependent\niterations. Loops with constant distance data dependence, because of uniform\ndistance between the dependent iterations, lead to easy partitioning of the\niteration space and hence they have been successfully dealt with.\n  Parallelization of loops with variable distance data dependences is a\nconsiderably difficult problem. It is our belief that partitioning the\niteration space in such loops cannot be done without examining solutions of the\ncorresponding Linear Diophantine Equations. Focus of this work is to study\nvariable distance data dependences and examine the relation between dependent\niterations. Our analysis based on parametric solution leads to a mathematical\nformulation capturing dependence between iterations. Our approach shows the\nexistence of reasonable exploitable parallelism in variable distance data\ndependences loops with multiple LDEs.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2013 05:21:54 GMT"}], "update_date": "2013-11-14", "authors_parsed": [["Kale", "Archana", ""], ["Patil", "Amitkumar", ""], ["Biswas", "Supratim", ""]]}, {"id": "1311.3062", "submitter": "Tobias Langner", "authors": "Yuval Emek, Tobias Langner, Jara Uitto, Roger Wattenhofer", "title": "Ants: Mobile Finite State Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the Ants Nearby Treasure Search (ANTS) problem introduced by\nFeinerman, Korman, Lotker, and Sereni (PODC 2012), where $n$ mobile agents,\ninitially placed at the origin of an infinite grid, collaboratively search for\nan adversarially hidden treasure. In this paper, the model of Feinerman et al.\nis adapted such that the agents are controlled by a (randomized) finite state\nmachine: they possess a constant-size memory and are able to communicate with\neach other through constant-size messages. Despite the restriction to\nconstant-size memory, we show that their collaborative performance remains the\nsame by presenting a distributed algorithm that matches a lower bound\nestablished by Feinerman et al. on the run-time of any ANTS algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2013 09:54:35 GMT"}], "update_date": "2013-11-14", "authors_parsed": [["Emek", "Yuval", ""], ["Langner", "Tobias", ""], ["Uitto", "Jara", ""], ["Wattenhofer", "Roger", ""]]}, {"id": "1311.3070", "submitter": "Brijender Kahanwal Dr.", "authors": "Dr. Brijender Kahanwal and Dr. T. P. Singh", "title": "The Distributed Computing Paradigms: P2P, Grid, Cluster, Cloud, and\n  Jungle", "comments": "5 pages, 7 figures, journal", "journal-ref": "International Journal of Latest Research in Science and\n  Technology, Vol. 1, No. 2, pp. 183-187, 2012", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distributed computing is done on many systems to solve a large scale\nproblem. The growing of high-speed broadband networks in developed and\ndeveloping countries, the continual increase in computing power, and the rapid\ngrowth of the Internet have changed the way. In it the society manages\ninformation and information services. Historically, the state of computing has\ngone through a series of platform and environmental changes. Distributed\ncomputing holds great assurance for using computer systems effectively. As a\nresult, supercomputer sites and data centers have changed from providing high\nperformance floating point computing capabilities to concurrently servicing\nhuge number of requests from billions of users. The distributed computing\nsystem uses multiple computers to solve large-scale problems over the Internet.\nIt becomes data-intensive and network-centric. The applications of distributed\ncomputing have become increasingly wide-spread. In distributed computing, the\nmain stress is on the large scale resource sharing and always goes for the best\nperformance. In this article, we have reviewed the work done in the area of\ndistributed computing paradigms. The main stress is on the evolving area of\ncloud computing.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2013 10:26:58 GMT"}], "update_date": "2013-11-14", "authors_parsed": [["Kahanwal", "Dr. Brijender", ""], ["Singh", "Dr. T. P.", ""]]}, {"id": "1311.3144", "submitter": "Christian Schulz", "authors": "Aydin Buluc, Henning Meyerhenke, Ilya Safro, Peter Sanders, Christian\n  Schulz", "title": "Recent Advances in Graph Partitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We survey recent trends in practical algorithms for balanced graph\npartitioning together with applications and future research directions.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2013 14:33:09 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2013 15:40:29 GMT"}, {"version": "v3", "created": "Tue, 3 Feb 2015 18:37:10 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Buluc", "Aydin", ""], ["Meyerhenke", "Henning", ""], ["Safro", "Ilya", ""], ["Sanders", "Peter", ""], ["Schulz", "Christian", ""]]}, {"id": "1311.3195", "submitter": "EPTCS", "authors": "Kim G. Larsen (Aalborg University), Axel Legay (INRIA Rennes), Ulrik\n  Nyman (Aalborg University)", "title": "Proceedings 1st Workshop on Advances in Systems of Systems", "comments": null, "journal-ref": "EPTCS 133, 2013", "doi": "10.4204/EPTCS.133", "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the proceedings of the first workshop on Advances in\nSystems of Systems (AISOS'13), held in Roma, Italy, March 16. System-of-Systems\ndescribes the large scale integration of many independent self-contained\nsystems to satisfy global needs or multi-system requests. Examples are smart\ngrid, intelligent buildings, smart cities, transport systems, etc. There is a\nneed for new modeling formalisms, analysis methods and tools to help make\ntrade-off decisions during design and evolution avoiding leading to sub-optimal\ndesign and rework during integration and in service. The workshop should focus\non the modeling and analysis of System of Systems. AISOS'13 aims to gather\npeople from different communities in order to encourage exchange of methods and\nviews.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2013 16:32:26 GMT"}], "update_date": "2013-11-14", "authors_parsed": [["Larsen", "Kim G.", "", "Aalborg University"], ["Legay", "Axel", "", "INRIA Rennes"], ["Nyman", "Ulrik", "", "Aalborg University"]]}, {"id": "1311.3200", "submitter": "Dan Alistarh", "authors": "Dan Alistarh and Keren Censor-Hillel and Nir Shavit", "title": "Are Lock-Free Concurrent Algorithms Practically Wait-Free?", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lock-free concurrent algorithms guarantee that some concurrent operation will\nalways make progress in a finite number of steps. Yet programmers prefer to\ntreat concurrent code as if it were wait-free, guaranteeing that all operations\nalways make progress. Unfortunately, designing wait-free algorithms is\ngenerally a very complex task, and the resulting algorithms are not always\nefficient. While obtaining efficient wait-free algorithms has been a long-time\ngoal for the theory community, most non-blocking commercial code is only\nlock-free.\n  This paper suggests a simple solution to this problem. We show that, for a\nlarge class of lock- free algorithms, under scheduling conditions which\napproximate those found in commercial hardware architectures, lock-free\nalgorithms behave as if they are wait-free. In other words, programmers can\nkeep on designing simple lock-free algorithms instead of complex wait-free\nones, and in practice, they will get wait-free progress.\n  Our main contribution is a new way of analyzing a general class of lock-free\nalgorithms under a stochastic scheduler. Our analysis relates the individual\nperformance of processes with the global performance of the system using Markov\nchain lifting between a complex per-process chain and a simpler system progress\nchain. We show that lock-free algorithms are not only wait-free with\nprobability 1, but that in fact a general subset of lock-free algorithms can be\nclosely bounded in terms of the average number of steps required until an\noperation completes.\n  To the best of our knowledge, this is the first attempt to analyze progress\nconditions, typically stated in relation to a worst case adversary, in a\nstochastic model capturing their expected asymptotic behavior.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2013 16:39:40 GMT"}, {"version": "v2", "created": "Fri, 15 Nov 2013 14:09:39 GMT"}], "update_date": "2013-11-18", "authors_parsed": [["Alistarh", "Dan", ""], ["Censor-Hillel", "Keren", ""], ["Shavit", "Nir", ""]]}, {"id": "1311.3322", "submitter": "Thanh Do", "authors": "Thanh Do and Haryadi S. Gunawi", "title": "Impact of Limpware on HDFS: A Probabilistic Estimation", "comments": "9 pages, 6 figures, detailed probability calculation for SOCC 13\n  limplock paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of cloud computing, thousands of machines are connected and\nmanaged collectively. This era is confronted with a new challenge: performance\nvariability, primarily caused by large-scale management issues such as hardware\nfailures, software bugs, and configuration mistakes. In our previous work we\nhighlighted one overlooked cause: limpware - hardware whose performance\ndegrades significantly compared to its specification. We showed that limpware\ncan cause severe impact in current scale-out systems. In this report, we\nquantify how often these scenarios happen in Hadoop Distributed File System.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2013 22:05:58 GMT"}], "update_date": "2013-11-15", "authors_parsed": [["Do", "Thanh", ""], ["Gunawi", "Haryadi S.", ""]]}, {"id": "1311.3348", "submitter": "Richard Ewelle Ewelle", "authors": "Richard Ewelle Ewelle, Abdelkader Goua\\\"ich, Yannick Francillette,\n  Ghulam Mahdi", "title": "Network Traffic Adaptation For Cloud Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the arrival of cloud technology, game accessibility and ubiquity have a\nbright future; Games can be hosted in a centralize server and accessed through\nthe Internet by a thin client on a wide variety of devices with modest\ncapabilities: cloud gaming. However, current cloud gaming systems have very\nstrong requirements in terms of network resources, thus reducing the\naccessibility and ubiquity of cloud games, because devices with little\nbandwidth and people located in area with limited and unstable network\nconnectivity, cannot take advantage of these cloud services. In this paper we\npresent an adaptation technique inspired by the level of detail (LoD) approach\nin 3D graphics. It delivers multiple platform accessibility and network\nadaptability, while improving user's quality of experience (QoE) by reducing\nthe impact of poor and unstable network parameters (delay, packet loss, jitter)\non game interactivity. We validate our approach using a prototype game in a\ncontrolled environment and characterize the user QoE in a pilot experiment. The\nresults show that the proposed framework provides a significant QoE\nenhancement.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2013 00:17:27 GMT"}], "update_date": "2013-11-15", "authors_parsed": [["Ewelle", "Richard Ewelle", ""], ["Goua\u00efch", "Abdelkader", ""], ["Francillette", "Yannick", ""], ["Mahdi", "Ghulam", ""]]}, {"id": "1311.3425", "submitter": "Amos Korman", "authors": "Ofer Feinerman, Bernhard Haeupler, Amos Korman", "title": "Breathe before Speaking: Efficient Information Dissemination Despite\n  Noisy, Limited and Anonymous Communication", "comments": null, "journal-ref": null, "doi": "10.1145/2611462.2611469", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed computing models typically assume reliable communication between\nprocessors. While such assumptions often hold for engineered networks, e.g.,\ndue to underlying error correction protocols, their relevance to biological\nsystems, wherein messages are often distorted before reaching their\ndestination, is quite limited. In this study we take a first step towards\nreducing this gap by rigorously analyzing a model of communication in large\nanonymous populations composed of simple agents which interact through short\nand highly unreliable messages.\n  We focus on the broadcast problem and the majority-consensus problem. Both\nare fundamental information dissemination problems in distributed computing, in\nwhich the goal of agents is to converge to some prescribed desired opinion. We\ninitiate the study of these problems in the presence of communication noise.\nOur model for communication is extremely weak and follows the push gossip\ncommunication paradigm: In each round each agent that wishes to send\ninformation delivers a message to a random anonymous agent. This communication\nis further restricted to contain only one bit (essentially representing an\nopinion). Lastly, the system is assumed to be so noisy that the bit in each\nmessage sent is flipped independently with probability $1/2-\\epsilon$, for some\nsmall $\\epsilon >0$.\n  Even in this severely restricted, stochastic and noisy setting we give\nnatural protocols that solve the noisy broadcast and the noisy\nmajority-consensus problems efficiently. Our protocols run in $O(\\log n /\n\\epsilon^2)$ rounds and use $O(n \\log n / \\epsilon^2)$ messages/bits in total,\nwhere $n$ is the number of agents. These bounds are asymptotically optimal and,\nin fact, are as fast and message efficient as if each agent would have been\nsimultaneously informed directly by an agent that knows the prescribed desired\nopinion.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2013 09:10:15 GMT"}, {"version": "v2", "created": "Sun, 4 May 2014 00:18:04 GMT"}, {"version": "v3", "created": "Fri, 26 Jun 2015 10:12:15 GMT"}], "update_date": "2015-06-29", "authors_parsed": [["Feinerman", "Ofer", ""], ["Haeupler", "Bernhard", ""], ["Korman", "Amos", ""]]}, {"id": "1311.3833", "submitter": "Francisco Prieto-Castrillo Dr.", "authors": "Francisco Prieto-Castrillo, Antonio Astillero, Mar\\'ia\n  Bot\\'on-Fern\\'andez", "title": "Distributed Computing on Complex Networks", "comments": "15 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.AO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work considers the problem of finding analytical expressions for the\nexpected values of dis- tributed computing performance metrics when the\nunderlying communication network has a complex structure. Through active\nprobing tests a real distributed computing environment is analysed. From the\nresulting network, ensembles of synthetic graphs with additional structure are\nused in Monte Carlo simulations to both validate analytical expressions and\nexplore the performance metrics under different conditions. Computing paradigms\nwith different hierarchical structures in computing ser- vices are gauged,\nfully decentralised (i.e., peer-to-peer) environments providing the best\nperformance. Moreover, it is found that by implementing more intelligent\ncomputing services configurations (e.g., betweenness centrality based mappings)\nand task allocations strategies, significant improvements in the parallel\nefficiency can be achieved. We qualitatively reproduce results from previous\nworks and provide closed-form solutions for the expected performance metrics\nlinking topological, application structure and allocation parameters when job\ndependencies and a complex network structure are considered.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2013 12:49:58 GMT"}], "update_date": "2013-11-18", "authors_parsed": [["Prieto-Castrillo", "Francisco", ""], ["Astillero", "Antonio", ""], ["Bot\u00f3n-Fern\u00e1ndez", "Mar\u00eda", ""]]}, {"id": "1311.3987", "submitter": "Seyed-Mehdi-Reza Beheshti", "authors": "Seyed-Mehdi-Reza Beheshti and Srikumar Venugopal and Seung Hwan Ryu\n  and Boualem Benatallah and Wei Wang", "title": "Big Data and Cross-Document Coreference Resolution: Current State and\n  Future Opportunities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information Extraction (IE) is the task of automatically extracting\nstructured information from unstructured/semi-structured machine-readable\ndocuments. Among various IE tasks, extracting actionable intelligence from\never-increasing amount of data depends critically upon Cross-Document\nCoreference Resolution (CDCR) - the task of identifying entity mentions across\nmultiple documents that refer to the same underlying entity. Recently, document\ndatasets of the order of peta-/tera-bytes has raised many challenges for\nperforming effective CDCR such as scaling to large numbers of mentions and\nlimited representational power. The problem of analysing such datasets is\ncalled \"big data\". The aim of this paper is to provide readers with an\nunderstanding of the central concepts, subtasks, and the current\nstate-of-the-art in CDCR process. We provide assessment of existing\ntools/techniques for CDCR subtasks and highlight big data challenges in each of\nthem to help readers identify important and outstanding issues for further\ninvestigation. Finally, we provide concluding remarks and discuss possible\ndirections for future work.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2013 06:10:15 GMT"}], "update_date": "2013-11-19", "authors_parsed": [["Beheshti", "Seyed-Mehdi-Reza", ""], ["Venugopal", "Srikumar", ""], ["Ryu", "Seung Hwan", ""], ["Benatallah", "Boualem", ""], ["Wang", "Wei", ""]]}, {"id": "1311.4007", "submitter": "Alessandro Lonardo", "authors": "R. Ammendola, A. Biagioni, O. Frezza, G. Lamanna, A. Lonardo, F. Lo\n  Cicero, P. S. Paolucci, F. Pantaleo, D. Rossetti, F. Simula, M. Sozzi, L.\n  Tosoratto, P. Vicini", "title": "NaNet: a flexible and configurable low-latency NIC for real-time trigger\n  systems based on GPUs", "comments": "Proceedings for the TWEPP 2013 - Topical Workshop on Electronics for\n  Particle Physics workshop", "journal-ref": null, "doi": "10.1088/1748-0221/9/02/C02023", "report-no": null, "categories": "physics.ins-det cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NaNet is an FPGA-based PCIe X8 Gen2 NIC supporting 1/10 GbE links and the\ncustom 34 Gbps APElink channel. The design has GPUDirect RDMA capabilities and\nfeatures a network stack protocol offloading module, making it suitable for\nbuilding low-latency, real-time GPU-based computing systems. We provide a\ndetailed description of the NaNet hardware modular architecture. Benchmarks for\nlatency and bandwidth for GbE and APElink channels are presented, followed by a\nperformance analysis on the case study of the GPU-based low level trigger for\nthe RICH detector in the NA62 CERN experiment, using either the NaNet GbE and\nAPElink channels. Finally, we give an outline of project future activities.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2013 00:48:40 GMT"}, {"version": "v2", "created": "Thu, 9 Jan 2014 17:48:26 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Ammendola", "R.", ""], ["Biagioni", "A.", ""], ["Frezza", "O.", ""], ["Lamanna", "G.", ""], ["Lonardo", "A.", ""], ["Cicero", "F. Lo", ""], ["Paolucci", "P. S.", ""], ["Pantaleo", "F.", ""], ["Rossetti", "D.", ""], ["Simula", "F.", ""], ["Sozzi", "M.", ""], ["Tosoratto", "L.", ""], ["Vicini", "P.", ""]]}, {"id": "1311.4112", "submitter": "Guoru Ding", "authors": "Guoru Ding, Long Wang, Qihui Wu", "title": "Big Data Analytics in Future Internet of Things", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current research on Internet of Things (IoT) mainly focuses on how to enable\ngeneral objects to see, hear, and smell the physical world for themselves, and\nmake them connected to share the observations. In this paper, we argue that\nonly connected is not enough, beyond that, general objects should have the\ncapability to learn, think, and understand both the physical world by\nthemselves. On the other hand, the future IoT will be highly populated by large\nnumbers of heterogeneous networked embedded devices, which are generating\nmassive or big data in an explosive fashion. Although there is a consensus\namong almost everyone on the great importance of big data analytics in IoT, to\ndate, limited results, especially the mathematical foundations, are obtained.\nThese practical needs impels us to propose a systematic tutorial on the\ndevelopment of effective algorithms for big data analytics in future IoT, which\nare grouped into four classes: 1) heterogeneous data processing, 2) nonlinear\ndata processing, 3) high-dimensional data processing, and 4) distributed and\nparallel data processing. We envision that the presented research is offered as\na mere baby step in a potentially fruitful research direction. We hope that\nthis article, with interdisciplinary perspectives, will stimulate more\ninterests in research and development of practical and effective algorithms for\nspecific IoT applications, to enable smart resource allocation, automatic\nnetwork operation, and intelligent service provisioning.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2013 03:03:36 GMT"}], "update_date": "2013-11-19", "authors_parsed": [["Ding", "Guoru", ""], ["Wang", "Long", ""], ["Wu", "Qihui", ""]]}, {"id": "1311.4150", "submitter": "Jian-Feng Yan", "authors": "Jian-Feng Yan, Jia Zeng, Zhi-Qiang Liu, Yang Gao", "title": "Towards Big Topic Modeling", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To solve the big topic modeling problem, we need to reduce both time and\nspace complexities of batch latent Dirichlet allocation (LDA) algorithms.\nAlthough parallel LDA algorithms on the multi-processor architecture have low\ntime and space complexities, their communication costs among processors often\nscale linearly with the vocabulary size and the number of topics, leading to a\nserious scalability problem. To reduce the communication complexity among\nprocessors for a better scalability, we propose a novel communication-efficient\nparallel topic modeling architecture based on power law, which consumes orders\nof magnitude less communication time when the number of topics is large. We\ncombine the proposed communication-efficient parallel architecture with the\nonline belief propagation (OBP) algorithm referred to as POBP for big topic\nmodeling tasks. Extensive empirical results confirm that POBP has the following\nadvantages to solve the big topic modeling problem: 1) high accuracy, 2)\ncommunication-efficient, 3) fast speed, and 4) constant memory usage when\ncompared with recent state-of-the-art parallel LDA algorithms on the\nmulti-processor architecture.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2013 11:52:42 GMT"}], "update_date": "2013-11-19", "authors_parsed": [["Yan", "Jian-Feng", ""], ["Zeng", "Jia", ""], ["Liu", "Zhi-Qiang", ""], ["Gao", "Yang", ""]]}, {"id": "1311.4305", "submitter": "Tomofumi Yuki", "authors": "Tomofumi Yuki and Paul Feautrier and Sanjay Rajopadhye and Vijay\n  Saraswat", "title": "Checking Race Freedom of Clocked X10 Programs", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of many approaches to better take advantage of parallelism, which has now\nbecome mainstream, is the introduction of parallel programming languages.\nHowever, parallelism is by nature non-deterministic, and not all parallel bugs\ncan be avoided by language design. This paper proposes a method for\nguaranteeing absence of data races in the polyhedral subset of clocked X10\nprograms.\n  Clocks in X10 are similar to barriers, but are more dynamic; the subset of\nprocesses that participate in the synchronization can dynamically change at\nruntime. We construct the happens-before relation for clocked X10 programs, and\nshow that the problem of race detection is undecidable. However, in many\npractical cases, modern tools are able to find solutions or disprove their\nexistence. We present a set of benchmarks for which the analysis is possible\nand has an acceptable running time.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2013 09:36:31 GMT"}], "update_date": "2013-11-19", "authors_parsed": [["Yuki", "Tomofumi", ""], ["Feautrier", "Paul", ""], ["Rajopadhye", "Sanjay", ""], ["Saraswat", "Vijay", ""]]}, {"id": "1311.4502", "submitter": "Christian Lavault", "authors": "Christian Lavault (LIPN)", "title": "Multiplicate inverse forms of terminating hypergeometric series", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": "Equipe CALIN. LIPN CNRS UMR 7030", "categories": "math.CO cs.DC cs.DM math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multiplicate form of Gould--Hsu's inverse series relations enables to\ninvestigate the dual relations of the Chu-Vandermonde-Gau{\\ss}'s, the\nPfaff-Saalsch\\\"utz's summation theorems and the binomial convolution formula\ndue to Hagen and Rothe. Several identitity and reciprocal relations are thus\nestablished for terminating hypergeometric series. By virtue of the duplicate\ninversions, we establish several dual formulae of Chu-Vandermonde-Gau{\\ss}'s\nand Pfaff-Saalsch\\\"utz's summation theorems in Section (3)\\cite{ChuVanGauss}\nand (4)\\cite{PfaffSaalsch}, respectively. Finally, the last section is devoted\nto deriving several identities and reciprocal relations for terminating\nbalanced hypergeometric series from Hagen-Rothe's convolution identity in\naccordance with the duplicate, triplicate and multiplicate inversions.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2013 19:26:55 GMT"}], "update_date": "2013-11-19", "authors_parsed": [["Lavault", "Christian", "", "LIPN"]]}, {"id": "1311.4521", "submitter": "Bartosz Kostrzewa", "authors": "A. Deuzeman, K. Jansen, B. Kostrzewa, C. Urbach", "title": "Experiences with OpenMP in tmLQCD", "comments": "presented at the 31st International Symposium on Lattice Field Theory\n  (Lattice 2013), 29 July - 3 August 2013, Mainz, Germany", "journal-ref": "PoS(LATTICE 2013)416", "doi": null, "report-no": "HU-EP-13/60, DESY 13-217, SFB/CPP-13-93", "categories": "hep-lat cs.DC physics.comp-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  An overview is given of the lessons learned from the introduction of\nmulti-threading using OpenMP in tmLQCD. In particular, programming style,\nperformance measurements, cache misses, scaling, thread distribution for hybrid\ncodes, race conditions, the overlapping of communication and computation and\nthe measurement and reduction of certain overheads are discussed. Performance\nmeasurements and sampling profiles are given for different implementations of\nthe hopping matrix computational kernel.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2013 20:18:24 GMT"}], "update_date": "2013-11-19", "authors_parsed": [["Deuzeman", "A.", ""], ["Jansen", "K.", ""], ["Kostrzewa", "B.", ""], ["Urbach", "C.", ""]]}, {"id": "1311.4527", "submitter": "Jose Bento", "authors": "Jose Bento, Nate Derbinsky, Javier Alonso-Mora, Jonathan Yedidia", "title": "A message-passing algorithm for multi-agent trajectory planning", "comments": "In Advances in Neural Information Processing Systems (NIPS), 2013.\n  Demo video available at http://www.youtube.com/watch?v=yuGCkVT8Bew", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC cs.MA cs.RO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a novel approach for computing collision-free \\emph{global}\ntrajectories for $p$ agents with specified initial and final configurations,\nbased on an improved version of the alternating direction method of multipliers\n(ADMM). Compared with existing methods, our approach is naturally\nparallelizable and allows for incorporating different cost functionals with\nonly minor adjustments. We apply our method to classical challenging instances\nand observe that its computational requirements scale well with $p$ for several\ncost functionals. We also show that a specialization of our algorithm can be\nused for {\\em local} motion planning by solving the problem of joint\noptimization in velocity space.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2013 20:38:57 GMT"}], "update_date": "2013-11-19", "authors_parsed": [["Bento", "Jose", ""], ["Derbinsky", "Nate", ""], ["Alonso-Mora", "Javier", ""], ["Yedidia", "Jonathan", ""]]}, {"id": "1311.4533", "submitter": "Kirana Kumara P", "authors": "Kirana Kumara P", "title": "A Study of Speed of the Boundary Element Method as applied to the\n  Realtime Computational Simulation of Biological Organs", "comments": "preprint, draft, 2 tables, 47 references, 7 files, Codes that can\n  solve three dimensional linear elastostatic problems using constant boundary\n  elements (of triangular shape) while ignoring body forces are provided as\n  supplementary files; codes are distributed under the MIT License in three\n  versions: i) MATLAB version ii) Fortran 90 version (sequential code) iii)\n  Fortran 90 version (parallel code)", "journal-ref": "Electronic Journal of Boundary Elements, Vol. 12, No. 2, pp. 1-25\n  (2014)", "doi": null, "report-no": null, "categories": "cs.CE cs.DC cs.MS physics.comp-ph physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, possibility of simulating biological organs in realtime using\nthe Boundary Element Method (BEM) is investigated. Biological organs are\nassumed to follow linear elastostatic material behavior, and constant boundary\nelement is the element type used. First, a Graphics Processing Unit (GPU) is\nused to speed up the BEM computations to achieve the realtime performance.\nNext, instead of the GPU, a computer cluster is used. Results indicate that BEM\nis fast enough to provide for realtime graphics if biological organs are\nassumed to follow linear elastostatic material behavior. Although the present\nwork does not conduct any simulation using nonlinear material models, results\nfrom using the linear elastostatic material model imply that it would be\ndifficult to obtain realtime performance if highly nonlinear material models\nthat properly characterize biological organs are used. Although the use of BEM\nfor the simulation of biological organs is not new, the results presented in\nthe present study are not found elsewhere in the literature.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2013 20:54:26 GMT"}, {"version": "v2", "created": "Mon, 6 Jan 2014 04:29:47 GMT"}, {"version": "v3", "created": "Tue, 14 Jan 2014 18:28:10 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["P", "Kirana Kumara", ""]]}, {"id": "1311.4588", "submitter": "Daniel Ruprecht", "authors": "Johannes Steiner, Daniel Ruprecht, Robert Speck, Rolf Krause", "title": "Convergence of Parareal for the Navier-Stokes equations depending on the\n  Reynolds number", "comments": null, "journal-ref": "Lecture Notes in Computational Science and Engineering 103,\n  Springer International Publishing, pages 195 - 202, 2015", "doi": "10.1007/978-3-319-10705-9__19", "report-no": null, "categories": "math.NA cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents first a linear stability analysis for the time-parallel\nParareal method, using an IMEX Euler as coarse and a Runge-Kutta-3 method as\nfine propagator, confirming that dominant imaginary eigenvalues negatively\naffect Parareal's convergence. This suggests that when Parareal is applied to\nthe nonlinear Navier-Stokes equations, problems for small viscosities could\narise. Numerical results for a driven cavity benchmark are presented,\nconfirming that Parareal's convergence can indeed deteriorate as viscosity\ndecreases and the flow becomes increasingly dominated by convection. The effect\nis found to strongly depend on the spatial resolution.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2013 23:17:20 GMT"}, {"version": "v2", "created": "Wed, 15 Oct 2014 09:15:19 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Steiner", "Johannes", ""], ["Ruprecht", "Daniel", ""], ["Speck", "Robert", ""], ["Krause", "Rolf", ""]]}, {"id": "1311.4627", "submitter": "Andrey Vladimirov", "authors": "Troy A. Porter, Andrey E. Vladimirov", "title": "Calculation of Stochastic Heating and Emissivity of Cosmic Dust Grains\n  with Optimization for the Intel Many Integrated Core Architecture", "comments": "23 pages. Submitted to Computer Physics Communications. Comments to a\n  subsequent revision will indicate resources from which the source code of the\n  HEATCODE library can be freely obtained", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM astro-ph.GA cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cosmic dust particles effectively attenuate starlight. Their absorption of\nstarlight produces emission spectra from the near- to far-infrared, which\ndepends on the sizes and properties of the dust grains, and spectrum of the\nheating radiation field. The near- to mid-infrared is dominated by the\nemissions by very small grains. Modeling the absorption of starlight by these\nparticles is, however, computationally expensive and a significant bottleneck\nfor self-consistent radiation transport codes treating the heating of dust by\nstars. In this paper, we summarize the formalism for computing the stochastic\nemissivity of cosmic dust, which was developed in earlier works, and present a\nnew library HEATCODE implementing this formalism for the calculation for\narbitrary grain properties and heating radiation fields. Our library is highly\noptimized for general-purpose processors with multiple cores and vector\ninstructions, with hierarchical memory cache structure. The HEATCODE library\nalso efficiently runs on co-processor cards implementing the Intel Many\nIntegrated Core (Intel MIC) architecture. We discuss in detail the optimization\nsteps that we took in order to optimize for the Intel MIC architecture, which\nalso significantly benefited the performance of the code on general-purpose\nprocessors, and provide code samples and performance benchmarks for each step.\nThe HEATCODE library performance on a single Intel Xeon Phi coprocessor (Intel\nMIC architecture) is approximately 2 times a general-purpose two-socket\nmulticore processor system with approximately the same nominal power\nconsumption. The library supports heterogeneous calculations employing host\nprocessors simultaneously with multiple coprocessors, and can be easily\nincorporated into existing radiation transport codes.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2013 06:11:07 GMT"}], "update_date": "2013-11-20", "authors_parsed": [["Porter", "Troy A.", ""], ["Vladimirov", "Andrey E.", ""]]}, {"id": "1311.4780", "submitter": "Willie Neiswanger", "authors": "Willie Neiswanger, Chong Wang, Eric Xing", "title": "Asymptotically Exact, Embarrassingly Parallel MCMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communication costs, resulting from synchronization requirements during\nlearning, can greatly slow down many parallel machine learning algorithms. In\nthis paper, we present a parallel Markov chain Monte Carlo (MCMC) algorithm in\nwhich subsets of data are processed independently, with very little\ncommunication. First, we arbitrarily partition data onto multiple machines.\nThen, on each machine, any classical MCMC method (e.g., Gibbs sampling) may be\nused to draw samples from a posterior distribution given the data subset.\nFinally, the samples from each machine are combined to form samples from the\nfull posterior. This embarrassingly parallel algorithm allows each machine to\nact independently on a subset of the data (without communication) until the\nfinal combination stage. We prove that our algorithm generates asymptotically\nexact samples and empirically demonstrate its ability to parallelize burn-in\nand sampling in several models.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2013 15:23:04 GMT"}, {"version": "v2", "created": "Fri, 21 Mar 2014 04:25:50 GMT"}], "update_date": "2014-03-24", "authors_parsed": [["Neiswanger", "Willie", ""], ["Wang", "Chong", ""], ["Xing", "Eric", ""]]}, {"id": "1311.4805", "submitter": "James Cruise", "authors": "James Cruise and Ayalvadi Ganesh", "title": "Probabilistic consensus via polling and majority rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider lightweight decentralised algorithms for achieving\nconsensus in distributed systems. Each member of a distributed group has a\nprivate value from a fixed set consisting of, say, two elements, and the goal\nis for all members to reach consensus on the majority value. We explore\nvariants of the voter model applied to this problem. In the voter model, each\nnode polls a randomly chosen group member and adopts its value. The process is\nrepeated until consensus is reached. We generalize this so that each member\npolls a (deterministic or random) number of other group members and changes\nopinion only if a suitably defined super-majority has a different opinion. We\nshow that this modification greatly speeds up the convergence of the algorithm,\nas well as substantially reducing the probability of it reaching consensus on\nthe incorrect value.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2013 17:04:32 GMT"}], "update_date": "2013-11-20", "authors_parsed": [["Cruise", "James", ""], ["Ganesh", "Ayalvadi", ""]]}, {"id": "1311.4952", "submitter": "Deepanwita  Das", "authors": "Deepanwita Das and Srabani Mukhopadhyaya", "title": "Distributed Painting by a Swarm of Robots with Unlimited Sensing\n  Capabilities and Its Simulation", "comments": "15 pages, 10 figures", "journal-ref": "International Journal of Information Processing, Volume 7, Issue\n  3, page:1-15, 2013. ISSN: 0973-8215", "doi": null, "report-no": null, "categories": "cs.DC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a distributed painting algorithm for painting a priori\nknown rectangular region by swarm of autonomous mobile robots. We assume that\nthe region is obstacle free and of rectangular in shape. The basic approach is\nto divide the region into some cells, and to let each robot to paint one of\nthese cells. Assignment of different cells to the robots is done by ranking the\nrobots according to their relative positions. In this algorithm, the robots\nfollow the basic Wait-Observe-Compute-Move model together with the synchronous\ntiming model. This paper also presents a simulation of the proposed algorithm.\nThe simulation is performed using the Player/Stage Robotic Simulator on Ubuntu\n10.04 (Lucid Lynx) platform.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2013 04:17:33 GMT"}], "update_date": "2013-11-21", "authors_parsed": [["Das", "Deepanwita", ""], ["Mukhopadhyaya", "Srabani", ""]]}, {"id": "1311.5304", "submitter": "Wasuwee Sodsong", "authors": "Wasuwee Sodsong, Jingun Hong, Seongwook Chung, Yeongkyu Lim, Shin-Dug\n  Kim, Bernd Burgstaller", "title": "Dynamic Partitioning-based JPEG Decompression on Heterogeneous Multicore\n  Architectures", "comments": "Abstract shortened to respect the arXiv limit of 1920 characters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the emergence of social networks and improvements in computational\nphotography, billions of JPEG images are shared and viewed on a daily basis.\nDesktops, tablets and smartphones constitute the vast majority of hardware\nplatforms used for displaying JPEG images. Despite the fact that these\nplatforms are heterogeneous multicores, no approach exists yet that is capable\nof joining forces of a system's CPU and GPU for JPEG decoding. In this paper we\nintroduce a novel JPEG decoding scheme for heterogeneous architectures\nconsisting of a CPU and an OpenCL-programmable GPU. We employ an offline\nprofiling step to determine the performance of a system's CPU and GPU with\nrespect to JPEG decoding. For a given JPEG image, our performance model uses\n(1) the CPU and GPU performance characteristics, (2) the image entropy and (3)\nthe width and height of the image to balance the JPEG decoding workload on the\nunderlying hardware. Our run-time partitioning and scheduling scheme exploits\ntask, data and pipeline parallelism by scheduling the non-parallelizable\nentropy decoding task on the CPU, whereas inverse cosine transformations\n(IDCTs), color conversions and upsampling are conducted on both the CPU and the\nGPU. Our kernels have been optimized for GPU memory hierarchies. We have\nimplemented the proposed method in the context of the libjpeg-turbo library,\nwhich is an industrial-strength JPEG encoding and decoding engine.\nLibjpeg-turbo's hand-optimized SIMD routines for ARM and x86 constitute a\ncompetitive yardstick for the comparison to the proposed approach.\nRetro-fitting our method with libjpeg-turbo provided insights on the\nsoftware-engineering aspects of re-engineering legacy code for heterogeneous\nmulticores.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2013 03:49:00 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2013 14:42:42 GMT"}, {"version": "v3", "created": "Mon, 12 May 2014 09:07:11 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Sodsong", "Wasuwee", ""], ["Hong", "Jingun", ""], ["Chung", "Seongwook", ""], ["Lim", "Yeongkyu", ""], ["Kim", "Shin-Dug", ""], ["Burgstaller", "Bernd", ""]]}, {"id": "1311.5317", "submitter": "Mohsen Ghaffari", "authors": "Keren Censor-Hillel, Mohsen Ghaffari, Fabian Kuhn", "title": "Distributed Connectivity Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present time-efficient distributed algorithms for decomposing graphs with\nlarge edge or vertex connectivity into multiple spanning or dominating trees,\nrespectively. As their primary applications, these decompositions allow us to\nachieve information flow with size close to the connectivity by parallelizing\nit along the trees. More specifically, our distributed decomposition algorithms\nare as follows:\n  (I) A decomposition of each undirected graph with vertex-connectivity $k$\ninto (fractionally) vertex-disjoint weighted dominating trees with total weight\n$\\Omega(\\frac{k}{\\log n})$, in $\\widetilde{O}(D+\\sqrt{n})$ rounds.\n  (II) A decomposition of each undirected graph with edge-connectivity\n$\\lambda$ into (fractionally) edge-disjoint weighted spanning trees with total\nweight $\\lceil\\frac{\\lambda-1}{2}\\rceil(1-\\varepsilon)$, in\n$\\widetilde{O}(D+\\sqrt{n\\lambda})$ rounds.\n  We also show round complexity lower bounds of\n$\\tilde{\\Omega}(D+\\sqrt{\\frac{n}{k}})$ and\n$\\tilde{\\Omega}(D+\\sqrt{\\frac{n}{\\lambda}})$ for the above two decompositions,\nusing techniques of [Das Sarma et al., STOC'11]. Moreover, our\nvertex-connectivity decomposition extends to centralized algorithms and\nimproves the time complexity of [Censor-Hillel et al., SODA'14] from $O(n^3)$\nto near-optimal $\\tilde{O}(m)$.\n  As corollaries, we also get distributed oblivious routing broadcast with\n$O(1)$-competitive edge-congestion and $O(\\log n)$-competitive\nvertex-congestion. Furthermore, the vertex connectivity decomposition leads to\nnear-time-optimal $O(\\log n)$-approximation of vertex connectivity: centralized\n$\\widetilde{O}(m)$ and distributed $\\tilde{O}(D+\\sqrt{n})$. The former moves\ntoward the 1974 conjecture of Aho, Hopcroft, and Ullman postulating an $O(m)$\ncentralized exact algorithm while the latter is the first distributed vertex\nconnectivity approximation.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2013 06:24:20 GMT"}], "update_date": "2013-11-22", "authors_parsed": [["Censor-Hillel", "Keren", ""], ["Ghaffari", "Mohsen", ""], ["Kuhn", "Fabian", ""]]}, {"id": "1311.5685", "submitter": "Blesson Varghese", "authors": "Blesson Varghese and Andrew Rau-Chaplin", "title": "Data Challenges in High-Performance Risk Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Risk Analytics is important to quantify, manage and analyse risks from the\nmanufacturing to the financial setting. In this paper, the data challenges in\nthe three stages of the high-performance risk analytics pipeline, namely risk\nmodelling, portfolio risk management and dynamic financial analysis is\npresented.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2013 09:22:03 GMT"}], "update_date": "2013-11-25", "authors_parsed": [["Varghese", "Blesson", ""], ["Rau-Chaplin", "Andrew", ""]]}, {"id": "1311.5686", "submitter": "Blesson Varghese", "authors": "Zhimin Yao, Blesson Varghese and Andrew Rau-Chaplin", "title": "High Performance Risk Aggregation: Addressing the Data Processing\n  Challenge the Hadoop MapReduce Way", "comments": "ScienceCloud 2013 at HPDC 2013, New York, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo simulations employed for the analysis of portfolios of\ncatastrophic risk process large volumes of data. Often times these simulations\nare not performed in real-time scenarios as they are slow and consume large\ndata. Such simulations can benefit from a framework that exploits parallelism\nfor addressing the computational challenge and facilitates a distributed file\nsystem for addressing the data challenge. To this end, the Apache Hadoop\nframework is chosen for the simulation reported in this paper so that the\ncomputational challenge can be tackled using the MapReduce model and the data\nchallenge can be addressed using the Hadoop Distributed File System. A parallel\nalgorithm for the analysis of aggregate risk is proposed and implemented using\nthe MapReduce model in this paper. An evaluation of the performance of the\nalgorithm indicates that the Hadoop MapReduce model offers a framework for\nprocessing large data in aggregate risk analysis. A simulation of aggregate\nrisk employing 100,000 trials with 1000 catastrophic events per trial on a\ntypical exposure set and contract structure is performed on multiple worker\nnodes in less than 6 minutes. The result indicates the scope and feasibility of\nMapReduce for tackling the computational and data challenge in the analysis of\naggregate risk for real-time use.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2013 09:35:40 GMT"}], "update_date": "2013-11-25", "authors_parsed": [["Yao", "Zhimin", ""], ["Varghese", "Blesson", ""], ["Rau-Chaplin", "Andrew", ""]]}, {"id": "1311.5740", "submitter": "Joris Borgdorff", "authors": "Joris Borgdorff, Mariusz Mamonski, Bartosz Bosak, Krzysztof Kurowski,\n  Mohamed Ben Belgacem, Bastien Chopard, Derek Groen, Peter V. Coveney, Alfons\n  G. Hoekstra", "title": "Distributed Multiscale Computing with MUSCLE 2, the Multiscale Coupling\n  Library and Environment", "comments": "18 pages, 22 figures, submitted to journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Multiscale Coupling Library and Environment: MUSCLE 2. This\nmultiscale component-based execution environment has a simple to use Java, C++,\nC, Python and Fortran API, compatible with MPI, OpenMP and threading codes. We\ndemonstrate its local and distributed computing capabilities and compare its\nperformance to MUSCLE 1, file copy, MPI, MPWide, and GridFTP. The local\nthroughput of MPI is about two times higher, so very tightly coupled code\nshould use MPI as a single submodel of MUSCLE 2; the distributed performance of\nGridFTP is lower, especially for small messages. We test the performance of a\ncanal system model with MUSCLE 2, where it introduces an overhead as small as\n5% compared to MPI.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2013 13:02:15 GMT"}], "update_date": "2013-11-25", "authors_parsed": [["Borgdorff", "Joris", ""], ["Mamonski", "Mariusz", ""], ["Bosak", "Bartosz", ""], ["Kurowski", "Krzysztof", ""], ["Belgacem", "Mohamed Ben", ""], ["Chopard", "Bastien", ""], ["Groen", "Derek", ""], ["Coveney", "Peter V.", ""], ["Hoekstra", "Alfons G.", ""]]}, {"id": "1311.5806", "submitter": "Ravi Mazumdar Dr", "authors": "Arpan Mukhopadhyay and Ravi R. Mazumdar", "title": "Analysis of Load Balancing in Large Heterogeneous Processor Sharing\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF math.OC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze randomized dynamic load balancing schemes for multi-server\nprocessor sharing systems when the number of servers in the system is large and\nthe servers have heterogeneous service rates. In particular, we focus on the\nclassical power-of-two load balancing scheme and a variant of it in which a\nnewly arrived job is assigned to the server having the least instantaneous\nLagrange shadow cost among two randomly chosen servers. The instantaneous\nLagrange shadow cost at a server is given by the ratio of the number of\nunfinished jobs at the server to the capacity of the server. Two different\napproaches of analysis are presented for each scheme. For exponential job\nlength distribution, the analysis is done using the mean field approach and for\nmore general job length distributions the analysis is carried out assuming an\nasymptotic independence property. Analytical expressions to compute mean\nsojourn time of jobs are found for both schemes. Asymptotic insensitivity of\nthe schemes to the type of job length distribution is established. Numerical\nresults are presented to validate the theoretical results and to show that,\nunlike the homogeneous scenario, the power-of-two type schemes considered in\nthis paper may not always result in better behaviour in terms of the mean\nsojourn time of jobs.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2013 16:44:35 GMT"}, {"version": "v2", "created": "Mon, 9 Feb 2015 22:07:16 GMT"}], "update_date": "2015-02-11", "authors_parsed": [["Mukhopadhyay", "Arpan", ""], ["Mazumdar", "Ravi R.", ""]]}, {"id": "1311.5904", "submitter": "J. C. D\\'iaz-V\\'elez", "authors": "M. G. Aartsen, R. Abbasi, M. Ackermann, J. Adams, J. A. Aguilar, M.\n  Ahlers, D. Altmann, C. Arguelles, J. Auffenberg, X. Bai, M. Baker, S. W.\n  Barwick, V. Baum, R. Bay, J. J. Beatty, J. Becker Tjus, K.-H. Becker, S.\n  BenZvi, P. Berghaus, D. Berley, E. Bernardini, A. Bernhard, D. Z. Besson, G.\n  Binder, D. Bindig, M. Bissok, E. Blaufuss, J. Blumenthal, D. J. Boersma, C.\n  Bohm, D. Bose, S. B\\\"oser, O. Botner, L. Brayeur, H.-P. Bretz, A. M. Brown,\n  R. Bruijn, J. Casey, M. Casier, D. Chirkin, A. Christov, B. Christy, K.\n  Clark, L. Classen, F. Clevermann, S. Coenders, S. Cohen, D. F. Cowen, A. H.\n  Cruz Silva, M. Danninger, J. Daughhetee, J. C. Davis, M. Day, C. De Clercq,\n  S. De Ridder, P. Desiati, K. D. de Vries, M. de With, T. DeYoung, J. C.\n  D\\'iaz-V\\'elez, M. Dunkman, R. Eagan, B. Eberhardt, B. Eichmann, J. Eisch, S.\n  Euler, P. A. Evenson, O. Fadiran, A. R. Fazely, A. Fedynitch, J. Feintzeig,\n  T. Feusels, K. Filimonov, C. Finley, T. Fischer-Wasels, S. Flis, A.\n  Franckowiak, K. Frantzen, T. Fuchs, T. K. Gaisser, J. Gallagher, L. Gerhardt,\n  L. Gladstone, T. Gl\\\"usenkamp, A. Goldschmidt, G. Golup, J. G. Gonzalez, J.\n  A. Goodman, D. G\\'ora, D. T. Grandmont, D. Grant, P. Gretskov, J. C. Groh, A.\n  Gro{\\ss}, C. Ha, A. Haj Ismail, P. Hallen, A. Hallgren, F. Halzen, K. Hanson,\n  D. Hebecker, D. Heereman, D. Heinen, K. Helbing, R. Hellauer, S. Hickford, G.\n  C. Hill, K. D. Hoffman, R. Hoffmann, A. Homeier, K. Hoshina, F. Huang, W.\n  Huelsnitz, P. O. Hulth, K. Hultqvist, S. Hussain, A. Ishihara, E. Jacobi, J.\n  Jacobsen, K. Jagielski, G. S. Japaridze, K. Jero, O. Jlelati, B. Kaminsky, A.\n  Kappes, T. Karg, A. Karle, M. Kauer, J. L. Kelley, J. Kiryluk, J. Kl\\\"as, S.\n  R. Klein, J.-H. K\\\"ohne, G. Kohnen, H. Kolanoski, L. K\\\"opke, C. Kopper, S.\n  Kopper, D. J. Koskinen, M. Kowalski, M. Krasberg, A. Kriesten, K. Krings, G.\n  Kroll, J. Kunnen, N. Kurahashi, T. Kuwabara, M. Labare, H. Landsman, M. J.\n  Larson, M. Lesiak-Bzdak, M. Leuermann, J. Leute, J. L\\\"unemann, O. Mac\\'ias,\n  J. Madsen, G. Maggi, R. Maruyama, K. Mase, H. S. Matis, F. McNally, K.\n  Meagher, M. Merck, G. Merino, T. Meures, S. Miarecki, E. Middell, N. Milke,\n  J. Miller, L. Mohrmann, T. Montaruli, R. Morse, R. Nahnhauer, U. Naumann, H.\n  Niederhausen, S. C. Nowicki, D. R. Nygren, A. Obertacke, S. Odrowski, A.\n  Olivas, A. Omairat, A. O'Murchadha, L. Paul, J. A. Pepper, C. P\\'erez de los\n  Heros, C. Pfendner, D. Pieloth, E. Pinat, J. Posselt, P. B. Price, G. T.\n  Przybylski, M. Quinnan, L. R \\\"adel, I. Rae, M. Rameez, K. Rawlins, P. Redl,\n  R. Reimann, E. Resconi, W. Rhode, M. Ribordy, M. Richman, B. Riedel, J. P.\n  Rodrigues, C. Rott, T. Ruhe, B. Ruzybayev, D. Ryckbosch, S. M. Saba, H.-G.\n  Sander, M. Santander, S. Sarkar, K. Schatto, F. Scheriau, T. Schmidt, M.\n  Schmitz, S. Schoenen, S. Sch\\\"oneberg, A. Sch\\\"onwald, A. Schukraft, L.\n  Schulte, D. Schultz, O. Schulz, D. Seckel, Y. Sestayo, S. Seunarine, R.\n  Shanidze, C. Sheremata, M. W. E. Smith, D. Soldin, G. M. Spiczak, C.\n  Spiering, M. Stamatikos, T. Stanev, N. A. Stanisha, A. Stasik, T.\n  Stezelberger, R. G. Stokstad, A. St\\\"o{\\ss}l, E. A. Strahler, R. Str\\\"om, N.\n  L. Strotjohann, G. W. Sullivan, H. Taavola, I. Taboada, A. Tamburro, A. Tepe,\n  S. Ter-Antonyan, G. Te\\v{s}i\\'c, S. Tilav, P. A. Toale, M. N. Tobin, S.\n  Toscano, M. Tselengidou, E. Unger, M. Usner, S. Vallecorsa, N. van\n  Eijndhoven, A. Van Overloop, J. van Santen, M. Vehring, M. Voge, M. Vraeghe,\n  C. Walck, T. Waldenmaier, M. Wallraff, Ch. Weaver, M. Wellons, C. Wendt, S.\n  Westerhoff, N. Whitehorn, K. Wiebe, C. H. Wiebusch, D. R. Williams, H.\n  Wissing, M. Wolf, T. R. Wood, K. Woschnagg, D. L. Xu, X. W. Xu, J. P. Yanez,\n  G. Yodh, S. Yoshida, P. Zarzhitsky, J. Ziemann, S. Zierke, M. Zoll", "title": "The IceProd Framework: Distributed Data Processing for the IceCube\n  Neutrino Observatory", "comments": null, "journal-ref": "Journal of Parallel & Distributed Computing 75:198,2015", "doi": "10.1016/j.jpdc.2014.08.001", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  IceCube is a one-gigaton instrument located at the geographic South Pole,\ndesigned to detect cosmic neutrinos, iden- tify the particle nature of dark\nmatter, and study high-energy neutrinos themselves. Simulation of the IceCube\ndetector and processing of data require a significant amount of computational\nresources. IceProd is a distributed management system based on Python, XML-RPC\nand GridFTP. It is driven by a central database in order to coordinate and\nadmin- ister production of simulations and processing of data produced by the\nIceCube detector. IceProd runs as a separate layer on top of other middleware\nand can take advantage of a variety of computing resources, including grids and\nbatch systems such as CREAM, Condor, and PBS. This is accomplished by a set of\ndedicated daemons that process job submission in a coordinated fashion through\nthe use of middleware plugins that serve to abstract the details of job\nsubmission and job management from the framework.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2013 21:16:58 GMT"}, {"version": "v2", "created": "Tue, 22 Jul 2014 22:31:16 GMT"}, {"version": "v3", "created": "Fri, 22 Aug 2014 21:31:55 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Aartsen", "M. G.", ""], ["Abbasi", "R.", ""], ["Ackermann", "M.", ""], ["Adams", "J.", ""], ["Aguilar", "J. A.", ""], ["Ahlers", "M.", ""], ["Altmann", "D.", ""], ["Arguelles", "C.", ""], ["Auffenberg", "J.", ""], ["Bai", "X.", ""], ["Baker", "M.", ""], ["Barwick", "S. W.", ""], ["Baum", "V.", ""], ["Bay", "R.", ""], ["Beatty", "J. J.", ""], ["Tjus", "J. Becker", ""], ["Becker", "K. -H.", ""], ["BenZvi", "S.", ""], ["Berghaus", "P.", ""], ["Berley", "D.", ""], ["Bernardini", "E.", ""], ["Bernhard", "A.", ""], ["Besson", "D. Z.", ""], ["Binder", "G.", ""], ["Bindig", "D.", ""], ["Bissok", "M.", ""], ["Blaufuss", "E.", ""], ["Blumenthal", "J.", ""], ["Boersma", "D. J.", ""], ["Bohm", "C.", ""], ["Bose", "D.", ""], ["B\u00f6ser", "S.", ""], ["Botner", "O.", ""], ["Brayeur", "L.", ""], ["Bretz", "H. -P.", ""], ["Brown", "A. M.", ""], ["Bruijn", "R.", ""], ["Casey", "J.", ""], ["Casier", "M.", ""], ["Chirkin", "D.", ""], ["Christov", "A.", ""], ["Christy", "B.", ""], ["Clark", "K.", ""], ["Classen", "L.", ""], ["Clevermann", "F.", ""], ["Coenders", "S.", ""], ["Cohen", "S.", ""], ["Cowen", "D. F.", ""], ["Silva", "A. H. Cruz", ""], ["Danninger", "M.", ""], ["Daughhetee", "J.", ""], ["Davis", "J. C.", ""], ["Day", "M.", ""], ["De Clercq", "C.", ""], ["De Ridder", "S.", ""], ["Desiati", "P.", ""], ["de Vries", "K. D.", ""], ["de With", "M.", ""], ["DeYoung", "T.", ""], ["D\u00edaz-V\u00e9lez", "J. C.", ""], ["Dunkman", "M.", ""], ["Eagan", "R.", ""], ["Eberhardt", "B.", ""], ["Eichmann", "B.", ""], ["Eisch", "J.", ""], ["Euler", "S.", ""], ["Evenson", "P. A.", ""], ["Fadiran", "O.", ""], ["Fazely", "A. R.", ""], ["Fedynitch", "A.", ""], ["Feintzeig", "J.", ""], ["Feusels", "T.", ""], ["Filimonov", "K.", ""], ["Finley", "C.", ""], ["Fischer-Wasels", "T.", ""], ["Flis", "S.", ""], ["Franckowiak", "A.", ""], ["Frantzen", "K.", ""], ["Fuchs", "T.", ""], ["Gaisser", "T. K.", ""], ["Gallagher", "J.", ""], ["Gerhardt", "L.", ""], ["Gladstone", "L.", ""], ["Gl\u00fcsenkamp", "T.", ""], ["Goldschmidt", "A.", ""], ["Golup", "G.", ""], ["Gonzalez", "J. G.", ""], ["Goodman", "J. A.", ""], ["G\u00f3ra", "D.", ""], ["Grandmont", "D. T.", ""], ["Grant", "D.", ""], ["Gretskov", "P.", ""], ["Groh", "J. C.", ""], ["Gro\u00df", "A.", ""], ["Ha", "C.", ""], ["Ismail", "A. Haj", ""], ["Hallen", "P.", ""], ["Hallgren", "A.", ""], ["Halzen", "F.", ""], ["Hanson", "K.", ""], ["Hebecker", "D.", ""], ["Heereman", "D.", ""], ["Heinen", "D.", ""], ["Helbing", "K.", ""], ["Hellauer", "R.", ""], ["Hickford", "S.", ""], ["Hill", "G. C.", ""], ["Hoffman", "K. D.", ""], ["Hoffmann", "R.", ""], ["Homeier", "A.", ""], ["Hoshina", "K.", ""], ["Huang", "F.", ""], ["Huelsnitz", "W.", ""], ["Hulth", "P. O.", ""], ["Hultqvist", "K.", ""], ["Hussain", "S.", ""], ["Ishihara", "A.", ""], ["Jacobi", "E.", ""], ["Jacobsen", "J.", ""], ["Jagielski", "K.", ""], ["Japaridze", "G. S.", ""], ["Jero", "K.", ""], ["Jlelati", "O.", ""], ["Kaminsky", "B.", ""], ["Kappes", "A.", ""], ["Karg", "T.", ""], ["Karle", "A.", ""], ["Kauer", "M.", ""], ["Kelley", "J. L.", ""], ["Kiryluk", "J.", ""], ["Kl\u00e4s", "J.", ""], ["Klein", "S. R.", ""], ["K\u00f6hne", "J. -H.", ""], ["Kohnen", "G.", ""], ["Kolanoski", "H.", ""], ["K\u00f6pke", "L.", ""], ["Kopper", "C.", ""], ["Kopper", "S.", ""], ["Koskinen", "D. J.", ""], ["Kowalski", "M.", ""], ["Krasberg", "M.", ""], ["Kriesten", "A.", ""], ["Krings", "K.", ""], ["Kroll", "G.", ""], ["Kunnen", "J.", ""], ["Kurahashi", "N.", ""], ["Kuwabara", "T.", ""], ["Labare", "M.", ""], ["Landsman", "H.", ""], ["Larson", "M. J.", ""], ["Lesiak-Bzdak", "M.", ""], ["Leuermann", "M.", ""], ["Leute", "J.", ""], ["L\u00fcnemann", "J.", ""], ["Mac\u00edas", "O.", ""], ["Madsen", "J.", ""], ["Maggi", "G.", ""], ["Maruyama", "R.", ""], ["Mase", "K.", ""], ["Matis", "H. S.", ""], ["McNally", "F.", ""], ["Meagher", "K.", ""], ["Merck", "M.", ""], ["Merino", "G.", ""], ["Meures", "T.", ""], ["Miarecki", "S.", ""], ["Middell", "E.", ""], ["Milke", "N.", ""], ["Miller", "J.", ""], ["Mohrmann", "L.", ""], ["Montaruli", "T.", ""], ["Morse", "R.", ""], ["Nahnhauer", "R.", ""], ["Naumann", "U.", ""], ["Niederhausen", "H.", ""], ["Nowicki", "S. C.", ""], ["Nygren", "D. R.", ""], ["Obertacke", "A.", ""], ["Odrowski", "S.", ""], ["Olivas", "A.", ""], ["Omairat", "A.", ""], ["O'Murchadha", "A.", ""], ["Paul", "L.", ""], ["Pepper", "J. A.", ""], ["Heros", "C. P\u00e9rez de los", ""], ["Pfendner", "C.", ""], ["Pieloth", "D.", ""], ["Pinat", "E.", ""], ["Posselt", "J.", ""], ["Price", "P. B.", ""], ["Przybylski", "G. T.", ""], ["Quinnan", "M.", ""], ["\u00e4del", "L. R", ""], ["Rae", "I.", ""], ["Rameez", "M.", ""], ["Rawlins", "K.", ""], ["Redl", "P.", ""], ["Reimann", "R.", ""], ["Resconi", "E.", ""], ["Rhode", "W.", ""], ["Ribordy", "M.", ""], ["Richman", "M.", ""], ["Riedel", "B.", ""], ["Rodrigues", "J. P.", ""], ["Rott", "C.", ""], ["Ruhe", "T.", ""], ["Ruzybayev", "B.", ""], ["Ryckbosch", "D.", ""], ["Saba", "S. M.", ""], ["Sander", "H. -G.", ""], ["Santander", "M.", ""], ["Sarkar", "S.", ""], ["Schatto", "K.", ""], ["Scheriau", "F.", ""], ["Schmidt", "T.", ""], ["Schmitz", "M.", ""], ["Schoenen", "S.", ""], ["Sch\u00f6neberg", "S.", ""], ["Sch\u00f6nwald", "A.", ""], ["Schukraft", "A.", ""], ["Schulte", "L.", ""], ["Schultz", "D.", ""], ["Schulz", "O.", ""], ["Seckel", "D.", ""], ["Sestayo", "Y.", ""], ["Seunarine", "S.", ""], ["Shanidze", "R.", ""], ["Sheremata", "C.", ""], ["Smith", "M. W. E.", ""], ["Soldin", "D.", ""], ["Spiczak", "G. M.", ""], ["Spiering", "C.", ""], ["Stamatikos", "M.", ""], ["Stanev", "T.", ""], ["Stanisha", "N. A.", ""], ["Stasik", "A.", ""], ["Stezelberger", "T.", ""], ["Stokstad", "R. G.", ""], ["St\u00f6\u00dfl", "A.", ""], ["Strahler", "E. A.", ""], ["Str\u00f6m", "R.", ""], ["Strotjohann", "N. L.", ""], ["Sullivan", "G. W.", ""], ["Taavola", "H.", ""], ["Taboada", "I.", ""], ["Tamburro", "A.", ""], ["Tepe", "A.", ""], ["Ter-Antonyan", "S.", ""], ["Te\u0161i\u0107", "G.", ""], ["Tilav", "S.", ""], ["Toale", "P. A.", ""], ["Tobin", "M. N.", ""], ["Toscano", "S.", ""], ["Tselengidou", "M.", ""], ["Unger", "E.", ""], ["Usner", "M.", ""], ["Vallecorsa", "S.", ""], ["van Eijndhoven", "N.", ""], ["Van Overloop", "A.", ""], ["van Santen", "J.", ""], ["Vehring", "M.", ""], ["Voge", "M.", ""], ["Vraeghe", "M.", ""], ["Walck", "C.", ""], ["Waldenmaier", "T.", ""], ["Wallraff", "M.", ""], ["Weaver", "Ch.", ""], ["Wellons", "M.", ""], ["Wendt", "C.", ""], ["Westerhoff", "S.", ""], ["Whitehorn", "N.", ""], ["Wiebe", "K.", ""], ["Wiebusch", "C. H.", ""], ["Williams", "D. R.", ""], ["Wissing", "H.", ""], ["Wolf", "M.", ""], ["Wood", "T. R.", ""], ["Woschnagg", "K.", ""], ["Xu", "D. L.", ""], ["Xu", "X. W.", ""], ["Yanez", "J. P.", ""], ["Yodh", "G.", ""], ["Yoshida", "S.", ""], ["Zarzhitsky", "P.", ""], ["Ziemann", "J.", ""], ["Zierke", "S.", ""], ["Zoll", "M.", ""]]}, {"id": "1311.5949", "submitter": "Yogesh Simmhan", "authors": "Yogesh Simmhan, Alok Kumbhare, Charith Wickramaarachchi, Soonil\n  Nagarkar, Santosh Ravi, Cauligi Raghavendra, Viktor Prasanna", "title": "GoFFish: A Sub-Graph Centric Framework for Large-Scale Graph Analytics", "comments": "Under review by a conference, 2014", "journal-ref": "Proceedings of the European Conference on Parallel Processing\n  (Euro-Par 2014), Lecture Notes in Computer Science, vol. 8632, pp. 451-462", "doi": "10.1007/978-3-319-09873-9_38", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale graph processing is a major research area for Big Data\nexploration. Vertex centric programming models like Pregel are gaining traction\ndue to their simple abstraction that allows for scalable execution on\ndistributed systems naturally. However, there are limitations to this approach\nwhich cause vertex centric algorithms to under-perform due to poor compute to\ncommunication overhead ratio and slow convergence of iterative superstep. In\nthis paper we introduce GoFFish a scalable sub-graph centric framework\nco-designed with a distributed persistent graph storage for large scale graph\nanalytics on commodity clusters. We introduce a sub-graph centric programming\nabstraction that combines the scalability of a vertex centric approach with the\nflexibility of shared memory sub-graph computation. We map Connected\nComponents, SSSP and PageRank algorithms to this model to illustrate its\nflexibility. Further, we empirically analyze GoFFish using several real world\ngraphs and demonstrate its significant performance improvement, orders of\nmagnitude in some cases, compared to Apache Giraph, the leading open source\nvertex centric implementation.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2013 02:53:58 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Simmhan", "Yogesh", ""], ["Kumbhare", "Alok", ""], ["Wickramaarachchi", "Charith", ""], ["Nagarkar", "Soonil", ""], ["Ravi", "Santosh", ""], ["Raghavendra", "Cauligi", ""], ["Prasanna", "Viktor", ""]]}, {"id": "1311.6146", "submitter": "Mohammad  Mozumdar", "authors": "Qunzhi Zhou, Yogesh Simmhan and Viktor Prasanna", "title": "On Using Complex Event Processing for Dynamic Demand Response\n  Optimization in Microgrid", "comments": "Proceedings of Green Energy and Systems Conference 2013, November 25,\n  Long Beach, CA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Demand-side load reduction is a key benefit of Smart Grids. However, existing\ndemand response optimization (DR) programs fail to effectively leverage the\nnear-realtime information available from smart meters and Building Area\nNetworks to respond dynamically to changing energy use profiles. We investigate\nthe use of semantic Complex Event Processing (CEP) patterns to model and detect\ndynamic situations in a campus microgrid to facilitate adaptive DR. Our focus\nis on demand-side management rather than supply-side constraints. Continuous\ndata from information sources like smart meters and building sensors are\nabstracted as event streams. Event patterns for situations that assist with DR\nare detected from them. Specifically, we offer a taxonomy of event patterns\nthat can guide operators to define situations of interest and we illustrate its\nefficacy for DR by applying these patterns on realtime events in the USC Campus\nmicrogrid using our CEP framework\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2013 17:04:42 GMT"}], "update_date": "2013-11-26", "authors_parsed": [["Zhou", "Qunzhi", ""], ["Simmhan", "Yogesh", ""], ["Prasanna", "Viktor", ""]]}, {"id": "1311.6183", "submitter": "Parisa Jalili Marandi", "authors": "Parisa Jalili Marandi, Carlos Eduardo Bezerra, Fernando Pedone", "title": "Rethinking State-Machine Replication for Parallelism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-machine replication, a fundamental approach to designing fault-tolerant\nservices, requires commands to be executed in the same order by all replicas.\nMoreover, command execution must be deterministic: each replica must produce\nthe same output upon executing the same sequence of commands. These\nrequirements usually result in single-threaded replicas, which hinders service\nperformance. This paper introduces Parallel State-Machine Replication (P-SMR),\na new approach to parallelism in state-machine replication. P-SMR scales better\nthan previous proposals since no component plays a centralizing role in the\nexecution of independent commands---those that can be executed concurrently, as\ndefined by the service. The paper introduces P-SMR, describes a \"commodified\narchitecture\" to implement it, and compares its performance to other proposals\nusing a key-value store and a networked file system.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2013 22:45:59 GMT"}], "update_date": "2013-11-26", "authors_parsed": [["Marandi", "Parisa Jalili", ""], ["Bezerra", "Carlos Eduardo", ""], ["Pedone", "Fernando", ""]]}, {"id": "1311.6209", "submitter": "Peter Robinson", "authors": "Hartmut Klauck, Danupon Nanongkai, Gopal Pandurangan, Peter Robinson", "title": "Distributed Computation of Large-scale Graph Problems", "comments": "In Proceedings of SODA 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the increasing need for fast distributed processing of\nlarge-scale graphs such as the Web graph and various social networks, we study\na message-passing distributed computing model for graph processing and present\nlower bounds and algorithms for several graph problems. This work is inspired\nby recent large-scale graph processing systems (e.g., Pregel and Giraph) which\nare designed based on the message-passing model of distributed computing.\n  Our model consists of a point-to-point communication network of $k$ machines\ninterconnected by bandwidth-restricted links. Communicating data between the\nmachines is the costly operation (as opposed to local computation). The network\nis used to process an arbitrary $n$-node input graph (typically $n \\gg k > 1$)\nthat is randomly partitioned among the $k$ machines (a common implementation in\nmany real world systems). Our goal is to study fundamental complexity bounds\nfor solving graph problems in this model.\n  We present techniques for obtaining lower bounds on the distributed time\ncomplexity. Our lower bounds develop and use new bounds in random-partition\ncommunication complexity. We first show a lower bound of $\\Omega(n/k)$ rounds\nfor computing a spanning tree (ST) of the input graph. This result also implies\nthe same bound for other fundamental problems such as computing a minimum\nspanning tree (MST). We also show an $\\Omega(n/k^2)$ lower bound for\nconnectivity, ST verification and other related problems.\n  We give algorithms for various fundamental graph problems in our model. We\nshow that problems such as PageRank, MST, connectivity, and graph covering can\nbe solved in $\\tilde{O}(n/k)$ time, whereas for shortest paths, we present\nalgorithms that run in $\\tilde{O}(n/\\sqrt{k})$ time (for $(1+\\epsilon)$-factor\napprox.) and in $\\tilde{O}(n/k)$ time (for $O(\\log n)$-factor approx.)\nrespectively.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2013 05:14:48 GMT"}, {"version": "v2", "created": "Wed, 3 Sep 2014 16:44:54 GMT"}, {"version": "v3", "created": "Sat, 11 Oct 2014 18:13:20 GMT"}, {"version": "v4", "created": "Thu, 6 Nov 2014 05:23:40 GMT"}, {"version": "v5", "created": "Mon, 2 Feb 2015 08:28:47 GMT"}, {"version": "v6", "created": "Wed, 16 Sep 2015 17:18:40 GMT"}], "update_date": "2015-09-17", "authors_parsed": [["Klauck", "Hartmut", ""], ["Nanongkai", "Danupon", ""], ["Pandurangan", "Gopal", ""], ["Robinson", "Peter", ""]]}, {"id": "1311.6505", "submitter": "James Elliott", "authors": "James Elliott, Mark Hoemmen, Frank Mueller", "title": "Evaluating the Impact of SDC on the GMRES Iterative Solver", "comments": null, "journal-ref": null, "doi": "10.1109/IPDPS.2014.123", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing parallelism and transistor density, along with increasingly\ntighter energy and peak power constraints, may force exposure of occasionally\nincorrect computation or storage to application codes. Silent data corruption\n(SDC) will likely be infrequent, yet one SDC suffices to make numerical\nalgorithms like iterative linear solvers cease progress towards the correct\nanswer. Thus, we focus on resilience of the iterative linear solver GMRES to a\nsingle transient SDC. We derive inexpensive checks to detect the effects of an\nSDC in GMRES that work for a more general SDC model than presuming a bit flip.\nOur experiments show that when GMRES is used as the inner solver of an\ninner-outer iteration, it can \"run through\" SDC of almost any magnitude in the\ncomputationally intensive orthogonalization phase. That is, it gets the right\nanswer using faulty data without any required roll back. Those SDCs which it\ncannot run through, get caught by our detection scheme.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2013 22:19:39 GMT"}], "update_date": "2016-02-26", "authors_parsed": [["Elliott", "James", ""], ["Hoemmen", "Mark", ""], ["Mueller", "Frank", ""]]}, {"id": "1311.6902", "submitter": "Yannai A. Gonczarowski", "authors": "Armando Casta\\~neda, Yannai A. Gonczarowski, and Yoram Moses", "title": "Good, Better, Best! - Unbeatable Protocols for Consensus and Set\n  Consensus", "comments": null, "journal-ref": null, "doi": null, "report-no": "Hebrew University of Jerusalem Center for the Study of Rationality\n  discussion paper 653", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the very first consensus protocols for the synchronous model were\ndesigned to match the worst-case lower bound, deciding in exactly t+1 rounds in\nall runs, it was soon realized that they could be strictly improved upon by\nearly stopping protocols. These dominate the first ones, by always deciding in\nat most t+1 rounds, but often much faster. A protocol is unbeatable if it can't\nbe strictly dominated. Unbeatability is often a much more suitable notion of\noptimality for distributed protocols than worst-case performance. Using a\nknowledge-based analysis, this paper studies unbeatability for both consensus\nand k-set consensus. We present unbeatable solutions to non-uniform consensus\nand k-set consensus, and uniform consensus in synchronous message-passing\ncontexts with crash failures.\n  The k-set consensus problem is much more technically challenging than\nconsensus, and its analysis has triggered the development of the topological\napproach to distributed computing. Worst-case lower bounds for this problem\nhave required either techniques based on algebraic topology, or reduction-based\nproofs. Our proof of unbeatability is purely combinatorial, and is a direct,\nalbeit nontrivial, generalization of the one for consensus. We also present an\nalternative topological unbeatability proof that allows to understand the\nconnection between the connectivity of protocol complexes and the decision time\nof processes.\n  For the synchronous model, only solutions to the uniform variant of k-set\nconsensus have been offered. Based on our unbeatable protocols for uniform\nconsensus and for non-uniform k-set consensus, we present a uniform k-set\nconsensus protocol that strictly dominates all known solutions to this problem\nin the synchronous model.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2013 09:13:43 GMT"}], "update_date": "2013-11-28", "authors_parsed": [["Casta\u00f1eda", "Armando", ""], ["Gonczarowski", "Yannai A.", ""], ["Moses", "Yoram", ""]]}, {"id": "1311.7011", "submitter": "Yasset Perez-Riverol PhD", "authors": "Yasset Perez-Riverol and Roberto Vera Alvarez", "title": "A UML-based Approach to Design Parallel and Distributed Applications", "comments": "5 Figures, Work presented in two conferences and related with the\n  design of a Parallel Program for Conformational Search in small molecules\n  (PMID: 23030613)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC cs.DS", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Parallel and distributed application design is a major area of interest in\nthe domain of high performance scientific and industrial computing. Over the\nyears, various approaches have been proposed to aid parallel program developers\nto modeling their applications. In this paper it will be used some concepts\nfrom agile development methodologies and Unified Modeling Language (UML) to\nmodeling parallel and distributed applications. The UML-based approach of this\npaper describes through different artifacts and graphs the main flows of events\nin the development of parallel and high performance applications. Here, we\npresented three work flows to describe and to model our parallel program,\nDomain Model, Design and Modeling and Test. All these phases of the development\nsoftware allow to programmers convert the requirements of the problem in a good\nand efficient solution.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2013 15:41:22 GMT"}], "update_date": "2013-11-28", "authors_parsed": [["Perez-Riverol", "Yasset", ""], ["Alvarez", "Roberto Vera", ""]]}, {"id": "1311.7210", "submitter": "Debajyoti Mukhopadhyay Prof.", "authors": "Debajyoti Mukhopadhyay, Falguni J. Chathly, Nagesh N. Jadhav", "title": "QoS Based Framework for Effective Web Services in Cloud Computing", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enhancements in technology always follow Consumer requirements. Consumer\nrequires best of service with least possible mismatch and on time. Numerous\napplications available today are based on Web Services and Cloud Computing.\nRecently, there exist many Web Services with similar functional\ncharacteristics. Choosing a right Service from group of similar Web Service is\na complicated task for Service Consumer. In that case, Service Consumer can\ndiscover the required Web Service using non functional attributes of the Web\nServices such as QoS. Proposed layered architecture and Web Service Cloud\ni.e.WS Cloud computing Framework synthesizes the Non functional attributes that\nincludes reliability, availability, response time, latency etc. The Service\nConsumer is projected to provide the QoS requirements as part of Service\ndiscovery query. This framework will discover and filter the Web Services form\nthe cloud and rank them according to Service Consumer preferences to facilitate\nService on time.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2013 05:16:09 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Mukhopadhyay", "Debajyoti", ""], ["Chathly", "Falguni J.", ""], ["Jadhav", "Nagesh N.", ""]]}, {"id": "1311.7283", "submitter": "Dmitry N. Kozlov", "authors": "Dmitry N. Kozlov", "title": "Topology of the view complex", "comments": "accepted for publication in Homotopy, Homology Appl", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider a family of simplicial complexes, which we call the\nview complexes. Our choice of objects of study is motivated by theoretical\ndistributed computing, since the view complex is a key simplicial construction\nused for protocol complexes in the snapshot computational model. We show that\nthe view complex $\\view$ can be collapsed to the well-known complex\n$\\chi(\\Delta^n)$, called standard chromatic subdivision of a simplex, and that\n$\\chi(\\Delta^n)$ is itself collapsible. Furthermore, we show that the collapses\ncan be performed simultaneously in entire orbits of the natural symmetric group\naction. Our results yield a purely combinatorial and constructive understanding\nof the topology of view complexes, at the same time as they enhance our\nknowledge about the standard chromatic subdivision of a simplex.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2013 11:43:44 GMT"}, {"version": "v2", "created": "Thu, 26 Jun 2014 13:12:55 GMT"}, {"version": "v3", "created": "Fri, 5 Dec 2014 13:24:44 GMT"}], "update_date": "2014-12-08", "authors_parsed": [["Kozlov", "Dmitry N.", ""]]}, {"id": "1311.7289", "submitter": "Dmitry N. Kozlov", "authors": "Dmitry N. Kozlov", "title": "Weak symmetry breaking and abstract simplex paths", "comments": "revised version, 30 pages To appear in Mathematical Structures in\n  Computer Science", "journal-ref": "Math. Struct. Comp. Sci. 25 (2015) 1432-1462", "doi": "10.1017/S0960129514000085", "report-no": null, "categories": "cs.DC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by questions in theoretical distributed computing, we develop the\ncombinatorial theory of abstract simplex path subdivisions. Our main\napplication is a short and structural proof of the theorem of Castaneda and\nRajsbaum. This theorem in turn implies the solvability of the weak symmetry\nbreaking task in the immediate snapshot wait-free model in the case when the\nnumber of processes is not a power of a prime number.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2013 11:56:30 GMT"}, {"version": "v2", "created": "Wed, 5 Mar 2014 11:27:51 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Kozlov", "Dmitry N.", ""]]}, {"id": "1311.7422", "submitter": "Liang Wang", "authors": "Liang Wang, Jussi Kangasharju", "title": "LiteLab: Efficient Large-scale Network Experiments", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale network experiments is a challenging problem. Simulations,\nemulations, and real-world testbeds all have their advantages and\ndisadvantages. In this paper we present LiteLab, a light-weight platform\nspecialized for large-scale networking experiments. We cover in detail its\ndesign, key features, and architecture. We also perform an extensive evaluation\nof LiteLab's performance and accuracy and show that it is able to both simulate\nnetwork parameters with high accuracy, and also able to scale up to very large\nnetworks. LiteLab is flexible, easy to deploy, and allows researchers to\nperform large-scale network experiments with a short development cycle. We have\nused LiteLab for many different kinds of network experiments and are planning\nto make it available for others to use as well.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2013 20:56:09 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Wang", "Liang", ""], ["Kangasharju", "Jussi", ""]]}, {"id": "1311.7435", "submitter": "Liang Wang Ph.D Candidate", "authors": "Liang Wang, Jussi Kangasharju", "title": "Experimenting with BitTorrent on a Cluster: A Good or a Bad Idea?", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluation of large-scale network systems and applications is usually done in\none of three ways: simulations, real deployment on Internet, or on an emulated\nnetwork testbed such as a cluster. Simulations can study very large systems but\noften abstract out many practical details, whereas real world tests are often\nquite small, on the order of a few hundred nodes at most, but have very\nrealistic conditions. Clusters and other dedicated testbeds offer a middle\nground between the two: large systems with real application code. They also\ntypically allow configuring the testbed to enable repeatable experiments. In\nthis paper we explore how to run large BitTorrent experiments in a cluster\nsetup. We have chosen BitTorrent because the source code is available and it\nhas been a popular target for research. Our contribution is twofold. First, we\nshow how to tweak and configure the BitTorrent client to allow for a maximum\nnumber of clients to be run on a single machine, without running into any\nphysical limits of the machine. Second, our results show that the behavior of\nBitTorrent can be very sensitive to the configuration and we revisit some\nexisting BitTorrent research and consider the implications of our findings on\npreviously published results. As we show in this paper, BitTorrent can change\nits behavior in subtle ways which are sometimes ignored in published works.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2013 22:34:11 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Wang", "Liang", ""], ["Kangasharju", "Jussi", ""]]}, {"id": "1311.7635", "submitter": "Konrad Kulakowski", "authors": "Konrad Ku{\\l}akowski", "title": "Concurrent bisimulation algorithm", "comments": "22 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The coarsest bisimulation-finding problem plays an important role in the\nformal analysis of concurrent systems. For example, solving this problem allows\nthe behavior of different processes to be compared or specifications to be\nverified. Hence, in this paper an efficient concurrent bisimulation algorithm\nis presented. It is based on the sequential Paige and Tarjan algorithm and the\nconcept of the state signatures. The original solution follows Hopcroft's\nprinciple \"process the smaller half\". The presented algorithm uses its\ngeneralized version \"process all but the largest one\" better suited for\nconcurrent and parallel applications. The running time achieved is comparable\nwith the best known sequential and concurrent solutions. At the end of the\nwork, the results of tests carried out are presented. The question of the lower\nbound for the running time of the optimal algorithm is also discussed.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2013 19:14:21 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2014 23:36:48 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Ku\u0142akowski", "Konrad", ""]]}, {"id": "1311.7676", "submitter": "Song Gao", "authors": "Song Gao, Linna Li, Wenwen Li, Krzysztof Janowicz, Yue Zhang", "title": "Constructing Gazetteers from Volunteered Big Geo-Data Based on Hadoop", "comments": "45 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional gazetteers are built and maintained by authoritative mapping\nagencies. In the age of Big Data, it is possible to construct gazetteers in a\ndata-driven approach by mining rich volunteered geographic information (VGI)\nfrom the Web. In this research, we build a scalable distributed platform and a\nhigh-performance geoprocessing workflow based on the Hadoop ecosystem to\nharvest crowd-sourced gazetteer entries. Using experiments based on geotagged\ndatasets in Flickr, we find that the MapReduce-based workflow running on the\nspatially enabled Hadoop cluster can reduce the processing time compared with\ntraditional desktop-based operations by an order of magnitude. We demonstrate\nhow to use such a novel spatial-computing infrastructure to facilitate\ngazetteer research. In addition, we introduce a provenance-based trust model\nfor quality assurance. This work offers new insights on enriching future\ngazetteers with the use of Hadoop clusters, and makes contributions in\nconnecting GIS to the cloud computing environment for the next frontier of Big\nGeo-Data analytics.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2013 19:52:42 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2014 07:11:22 GMT"}], "update_date": "2014-02-10", "authors_parsed": [["Gao", "Song", ""], ["Li", "Linna", ""], ["Li", "Wenwen", ""], ["Janowicz", "Krzysztof", ""], ["Zhang", "Yue", ""]]}]