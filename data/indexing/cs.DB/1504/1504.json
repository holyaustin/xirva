[{"id": "1504.00150", "submitter": "Peng Feifei", "authors": "Feifei Peng and Haiming Chen", "title": "Discovering Restricted Regular Expressions with Interleaving", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering a concise schema from given XML documents is an important problem\nin XML applications. In this paper, we focus on the problem of learning an\nunordered schema from a given set of XML examples, which is actually a problem\nof learning a restricted regular expression with interleaving using positive\nexample strings. Schemas with interleaving could present meaningful knowledge\nthat cannot be disclosed by previous inference techniques. Moreover, inference\nof the minimal schema with interleaving is challenging. The problem of finding\na minimal schema with interleaving is shown to be NP-hard. Therefore, we\ndevelop an approximation algorithm and a heuristic solution to tackle the\nproblem using techniques different from known inference algorithms. We do\nexperiments on real-world data sets to demonstrate the effectiveness of our\napproaches. Our heuristic algorithm is shown to produce results that are very\nclose to optimal.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2015 08:55:12 GMT"}], "update_date": "2015-04-02", "authors_parsed": [["Peng", "Feifei", ""], ["Chen", "Haiming", ""]]}, {"id": "1504.00229", "submitter": "Niv Dayan", "authors": "Niv Dayan, Luc Bouganim, Philippe Bonnet", "title": "Modelling and Managing SSD Write-amplification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How stable is the performance of your flash-based Solid State Drives (SSDs)?\nThis question is central for database designers and administrators, cloud\nservice providers, and SSD constructors. The answer depends on\nwrite-amplification, i.e., garbage collection overhead. More specifically, the\nanswer depends on how write-amplification evolves in time.\n  How then can one model and manage write-amplification, especially when\napplication workloads change? This is the focus of this paper. Managing\nwrite-amplification boils down to managing the surplus physical space, called\nover-provisioned space. Modern SSDs essentially separate the physical space\ninto several partitions, based on the update frequency of the pages they\ncontain, and divide the over-provisioned space among the groups so as to\nminimize write-amplification. We introduce Wolf, a block manager that allocates\nover-provisioned space to SSD partitions using a near-optimal closed-form\nexpression, based on the sizes and update frequencies of groups of pages. Our\nevaluation shows that Wolf is robust to workloads change, with an improvement\nfactor of 2 with respect to the state-of-the-art. We also show that Wolf\nperforms comparably and even slightly better than the state of the art with\nstable workloads (over 20% improvement with a TPC-C workload).\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2015 13:41:30 GMT"}], "update_date": "2015-04-02", "authors_parsed": [["Dayan", "Niv", ""], ["Bouganim", "Luc", ""], ["Bonnet", "Philippe", ""]]}, {"id": "1504.00331", "submitter": "Eldon Carman Jr", "authors": "E. Preston Carman Jr. (1), Till Westmann (2), Vinayak R. Borkar (3),\n  Michael J. Carey (3) and Vassilis J. Tsotras (1) ((1) UC Riverside, (2)\n  Oracle Labs, (3) UC Irvine)", "title": "Apache VXQuery: A Scalable XQuery Implementation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The wide use of XML for document management and data exchange has created the\nneed to query large repositories of XML data. To efficiently query such large\ndata collections and take advantage of parallelism, we have implemented Apache\nVXQuery, an open-source scalable XQuery processor. The system builds upon two\nother open-source frameworks -- Hyracks, a parallel execution engine, and\nAlgebricks, a language agnostic compiler toolbox. Apache VXQuery extends these\ntwo frameworks and provides an implementation of the XQuery specifics (data\nmodel, data-model dependent functions and optimizations, and a parser). We\ndescribe the architecture of Apache VXQuery, its integration with Hyracks and\nAlgebricks, and the XQuery optimization rules applied to the query plan to\nimprove path expression efficiency and to enable query parallelism. An\nexperimental evaluation using a real 500GB dataset with various selection,\naggregation and join XML queries shows that Apache VXQuery performs well both\nin terms of scale-up and speed-up. Our experiments show that it is about 3x\nfaster than Saxon (an open-source and commercial XQuery processor) on a 4-core,\nsingle node implementation, and around 2.5x faster than Apache MRQL (a\nMapReduce-based parallel query processor) on an eight (4-core) node cluster.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2015 18:27:23 GMT"}], "update_date": "2015-04-02", "authors_parsed": [["Carman", "E. Preston", "Jr."], ["Westmann", "Till", ""], ["Borkar", "Vinayak R.", ""], ["Carey", "Michael J.", ""], ["Tsotras", "Vassilis J.", ""]]}, {"id": "1504.00457", "submitter": "Patrick Arnold", "authors": "Christian Wartner, Patrick Arnold and Erhard Rahm", "title": "Semi-automatic identification of counterfeit offers in online shopping\n  platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Product counterfeiting is a serious problem causing the industry estimated\nlosses of billions of dollars every year. With the increasing spread of\ne-commerce, the number of counterfeit products sold online increased\nsubstantially. We propose the adoption of a semi-automatic workflow to identify\nlikely counterfeit offers in online platforms and to present these offers to a\ndomain expert for manual verification. The workflow includes steps to generate\nsearch queries for relevant product offers, to match and cluster similar\nproduct offers, and to assess the counterfeit suspiciousness based on different\ncriteria. The goal is to support the periodic identification of many\ncounterfeit offers with a limited amount of manual effort. We explain how the\nproposed approach can be realized. We also present a preliminary evaluation of\nits most important steps on a case study using the eBay platform.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2015 07:14:13 GMT"}], "update_date": "2015-04-03", "authors_parsed": [["Wartner", "Christian", ""], ["Arnold", "Patrick", ""], ["Rahm", "Erhard", ""]]}, {"id": "1504.01048", "submitter": "Carsten Binnig", "authors": "Carsten Binnig, Andrew Crotty, Alex Galakatos, Tim Kraska, Erfan\n  Zamanian", "title": "The End of Slow Networks: It's Time for a Redesign", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Next generation high-performance RDMA-capable networks will require a\nfundamental rethinking of the design and architecture of modern distributed\nDBMSs. These systems are commonly designed and optimized under the assumption\nthat the network is the bottleneck: the network is slow and \"thin\", and thus\nneeds to be avoided as much as possible. Yet this assumption no longer holds\ntrue. With InfiniBand FDR 4x, the bandwidth available to transfer data across\nnetwork is in the same ballpark as the bandwidth of one memory channel, and it\nincreases even further with the most recent EDR standard. Moreover, with the\nincreasing advances of RDMA, the latency improves similarly fast. In this\npaper, we first argue that the \"old\" distributed database design is not capable\nof taking full advantage of the network. Second, we propose architectural\nredesigns for OLTP, OLAP and advanced analytical frameworks to take better\nadvantage of the improved bandwidth, latency and RDMA capabilities. Finally,\nfor each of the workload categories, we show that remarkable performance\nimprovements can be achieved.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2015 20:30:00 GMT"}, {"version": "v2", "created": "Sat, 19 Dec 2015 15:59:49 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Binnig", "Carsten", ""], ["Crotty", "Andrew", ""], ["Galakatos", "Alex", ""], ["Kraska", "Tim", ""], ["Zamanian", "Erfan", ""]]}, {"id": "1504.01117", "submitter": "Krzysztof Choromanski", "authors": "Krzysztof Choromanski, Afshin Rostamizadeh, Umar Syed", "title": "An $\\tilde{O}(\\frac{1}{\\sqrt{T}})$-error online algorithm for retrieving\n  heavily perturbated statistical databases in the low-dimensional querying\n  mode", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the first $\\tilde{O}(\\frac{1}{\\sqrt{T}})$-error online algorithm for\nreconstructing noisy statistical databases, where $T$ is the number of (online)\nsample queries received. The algorithm, which requires only $O(\\log T)$ memory,\naims to learn a hidden database-vector $w^{*} \\in \\mathbb{R}^{D}$ in order to\naccurately answer a stream of queries regarding the hidden database, which\narrive in an online fashion from some unknown distribution $\\mathcal{D}$. We\nassume the distribution $\\mathcal{D}$ is defined on the neighborhood of a\nlow-dimensional manifold. The presented algorithm runs in $O(dD)$-time per\nquery, where $d$ is the dimensionality of the query-space. Contrary to the\nclassical setting, there is no separate training set that is used by the\nalgorithm to learn the database --- the stream on which the algorithm will be\nevaluated must also be used to learn the database-vector. The algorithm only\nhas access to a binary oracle $\\mathcal{O}$ that answers whether a particular\nlinear function of the database-vector plus random noise is larger than a\nthreshold, which is specified by the algorithm. We note that we allow for a\nsignificant $O(D)$ amount of noise to be added while other works focused on the\nlow noise $o(\\sqrt{D})$-setting. For a stream of $T$ queries our algorithm\nachieves an average error $\\tilde{O}(\\frac{1}{\\sqrt{T}})$ by filtering out\nrandom noise, adapting threshold values given to the oracle based on its\nprevious answers and, as a consequence, recovering with high precision a\nprojection of a database-vector $w^{*}$ onto the manifold defining the\nquery-space.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2015 12:55:08 GMT"}], "update_date": "2015-04-07", "authors_parsed": [["Choromanski", "Krzysztof", ""], ["Rostamizadeh", "Afshin", ""], ["Syed", "Umar", ""]]}, {"id": "1504.01563", "submitter": "Alan Freihof Tygel", "authors": "Alan Freihof Tygel, Judie Attard, Fabrizio Orlandi, Maria Luiza\n  Machado Campos, S\\\"oren Auer", "title": "\"How much?\" Is Not Enough - An Analysis of Open Budget Initiatives", "comments": "10 pages. Submited as a full paper to WebSci2015 conference on 20th\n  March 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  A worldwide movement towards the publication of Open Government Data is\ntaking place, and budget data is one of the key elements pushing this trend.\nIts importance is mostly related to transparency, but publishing budget data,\ncombined with other actions, can also improve democratic participation, allow\ncomparative analysis of governments and boost data-driven business. However,\nthe lack of standards and common evaluation criteria still hinders the\ndevelopment of appropriate tools and the materialization of the appointed\nbenefits. In this paper, we present a model to analyse government initiatives\nto publish budget data. We identify the main features of these initiatives with\na double objective: (i) to drive a structured analysis, relating some\ndimensions to their possible impacts, and (ii) to derive characterization\nattributes to compare initiatives based on each dimension. We define use\nperspectives and analyse some initiatives using this model. We conclude that,\nin order to favour use perspectives, special attention must be given to user\nfeedback, semantics standards and linking possibilities.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 11:56:05 GMT"}], "update_date": "2015-04-08", "authors_parsed": [["Tygel", "Alan Freihof", ""], ["Attard", "Judie", ""], ["Orlandi", "Fabrizio", ""], ["Campos", "Maria Luiza Machado", ""], ["Auer", "S\u00f6ren", ""]]}, {"id": "1504.01666", "submitter": "Niv Dayan", "authors": "Niv Dayan, Philippe Bonnet", "title": "Garbage Collection Techniques for Flash-Resident Page-Mapping FTLs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Storage devices based on flash memory have replaced hard disk drives (HDDs)\ndue to their superior performance, increasing density, and lower power\nconsumption. Unfortunately, flash memory is subject to challenging\nidiosyncrasies like erase-before-write and limited block lifetime. These\nconstraints are handled by a flash translation layer (FTL), which performs\nout-of-place updates, wear-leveling and garbage-collection behind the scene,\nwhile offering the application a virtualization of the physical address space.\n  A class of relevant FTLs employ a flash-resident page-associative mapping\ntable from logical to physical addresses, with a smaller RAM-resident cache for\nfrequently mapped entries. In this paper, we address the problem of performing\ngarbage-collection under such FTLs. We observe two problems. Firstly,\nmaintaining the metadata needed to perform garbage-collection under these\nschemes is problematic, because at write-time we do not necessarily know the\nphysical address of the before-image. Secondly, the size of this metadata must\nremain small, because it makes RAM unavailable for caching frequently accessed\nentries. We propose two complementary techniques, called Lazy Gecko and\nLogarithmic Gecko, which address these issues. Lazy Gecko works well when RAM\nis plentiful enough to store the GC metadata. Logarithmic Gecko works well when\nRAM isn't plentiful and efficiently stores the GC metadata in flash. Thus,\nthese techniques are applicable to a wide range of flash devices with varying\namounts of embedded RAM.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 16:58:31 GMT"}], "update_date": "2015-04-08", "authors_parsed": [["Dayan", "Niv", ""], ["Bonnet", "Philippe", ""]]}, {"id": "1504.01891", "submitter": "Marios Meimaris", "authors": "Marios Meimaris, George Papastefanatos, Stratis Viglas, Yannis\n  Stavrakas, Christos Pateritsas and Ioannis Anagnostopoulos", "title": "A Query Language for Multi-version Data Web Archives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Data Web refers to the vast and rapidly increasing quantity of\nscientific, corporate, government and crowd-sourced data published in the form\nof Linked Open Data, which encourages the uniform representation of\nheterogeneous data items on the web and the creation of links between them. The\ngrowing availability of open linked datasets has brought forth significant new\nchallenges regarding their proper preservation and the management of evolving\ninformation within them. In this paper, we focus on the evolution and\npreservation challenges related to publishing and preserving evolving linked\ndata across time. We discuss the main problems regarding their proper modelling\nand querying and provide a conceptual model and a query language for modelling\nand retrieving evolving data along with changes affecting them. We present in\ndetails the syntax of the query language and demonstrate its functionality over\na real-world use case of evolving linked dataset from the biological domain.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 09:53:52 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2015 14:38:17 GMT"}, {"version": "v3", "created": "Thu, 12 May 2016 16:00:10 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Meimaris", "Marios", ""], ["Papastefanatos", "George", ""], ["Viglas", "Stratis", ""], ["Stavrakas", "Yannis", ""], ["Pateritsas", "Christos", ""], ["Anagnostopoulos", "Ioannis", ""]]}, {"id": "1504.02255", "submitter": "Aleksey Buzmakov A", "authors": "Aleksey Buzmakov, Elias Egho, Nicolas Jay, Sergei O. Kuznetsov, Amedeo\n  Napoli, Chedy Ra\\\"issi", "title": "On mining complex sequential data by means of FCA and pattern structures", "comments": "An accepted publication in International Journal of General Systems.\n  The paper is created in the wake of the conference on Concept Lattice and\n  their Applications (CLA'2013). 27 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays data sets are available in very complex and heterogeneous ways.\nMining of such data collections is essential to support many real-world\napplications ranging from healthcare to marketing. In this work, we focus on\nthe analysis of \"complex\" sequential data by means of interesting sequential\npatterns. We approach the problem using the elegant mathematical framework of\nFormal Concept Analysis (FCA) and its extension based on \"pattern structures\".\nPattern structures are used for mining complex data (such as sequences or\ngraphs) and are based on a subsumption operation, which in our case is defined\nwith respect to the partial order on sequences. We show how pattern structures\nalong with projections (i.e., a data reduction of sequential structures), are\nable to enumerate more meaningful patterns and increase the computing\nefficiency of the approach. Finally, we show the applicability of the presented\nmethod for discovering and analyzing interesting patient patterns from a French\nhealthcare data set on cancer. The quantitative and qualitative results (with\nannotations and analysis from a physician) are reported in this use case which\nis the main motivation for this work.\n  Keywords: data mining; formal concept analysis; pattern structures;\nprojections; sequences; sequential data.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 10:57:53 GMT"}], "update_date": "2015-04-10", "authors_parsed": [["Buzmakov", "Aleksey", ""], ["Egho", "Elias", ""], ["Jay", "Nicolas", ""], ["Kuznetsov", "Sergei O.", ""], ["Napoli", "Amedeo", ""], ["Ra\u00efssi", "Chedy", ""]]}, {"id": "1504.02424", "submitter": "Vasileios Kapinas", "authors": "Panagiotis D. Diamantoulakis and Vasileios M. Kapinas and George K.\n  Karagiannidis", "title": "Big Data Analytics for Dynamic Energy Management in Smart Grids", "comments": "Published in ELSEVIER Big Data Research", "journal-ref": "Big Data Research, vol. 2, no. 3, pp. 94-101, Sep. 2015", "doi": "10.1016/j.bdr.2015.03.003", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The smart electricity grid enables a two-way flow of power and data between\nsuppliers and consumers in order to facilitate the power flow optimization in\nterms of economic efficiency, reliability and sustainability. This\ninfrastructure permits the consumers and the micro-energy producers to take a\nmore active role in the electricity market and the dynamic energy management\n(DEM). The most important challenge in a smart grid (SG) is how to take\nadvantage of the users' participation in order to reduce the cost of power.\nHowever, effective DEM depends critically on load and renewable production\nforecasting. This calls for intelligent methods and solutions for the real-time\nexploitation of the large volumes of data generated by a vast amount of smart\nmeters. Hence, robust data analytics, high performance computing, efficient\ndata network management, and cloud computing techniques are critical towards\nthe optimized operation of SGs. This research aims to highlight the big data\nissues and challenges faced by the DEM employed in SG networks. It also\nprovides a brief description of the most commonly used data processing methods\nin the literature, and proposes a promising direction for future research in\nthe field.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 19:04:35 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2015 17:47:34 GMT"}, {"version": "v3", "created": "Thu, 30 Jul 2015 16:22:40 GMT"}], "update_date": "2015-07-31", "authors_parsed": [["Diamantoulakis", "Panagiotis D.", ""], ["Kapinas", "Vasileios M.", ""], ["Karagiannidis", "George K.", ""]]}, {"id": "1504.02523", "submitter": "G\\\"une\\c{s} Alu\\c{c}", "authors": "G\\\"une\\c{s} Alu\\c{c}, M. Tamer \\\"Ozsu, Khuzaima Daudjee", "title": "Clustering RDF Databases Using Tunable-LSH", "comments": "Fixed typos, updated related work section", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Resource Description Framework (RDF) is a W3C standard for representing\ngraph-structured data, and SPARQL is the standard query language for RDF.\nRecent advances in Information Extraction, Linked Data Management and the\nSemantic Web have led to a rapid increase in both the volume and the variety of\nRDF data that are publicly available. As businesses start to capitalize on RDF\ndata, RDF data management systems are being exposed to workloads that are far\nmore diverse and dynamic than what they were designed to handle. Consequently,\nthere is a growing need for developing workload-adaptive and self-tuning RDF\ndata management systems. To realize this vision, we introduce a fast and\nefficient method for dynamically clustering records in an RDF data management\nsystem. Specifically, we assume nothing about the workload upfront, but as\nSPARQL queries are executed, we keep track of records that are co-accessed by\nthe queries in the workload and physically cluster them. To decide dynamically\n(hence, in constant-time) where a record needs to be placed in the storage\nsystem, we develop a new locality-sensitive hashing (LSH) scheme, Tunable-LSH.\nUsing Tunable-LSH, records that are co-accessed across similar sets of queries\ncan be hashed to the same or nearby physical pages in the storage system. What\nsets Tunable-LSH apart from existing LSH schemes is that it can auto-tune to\nachieve the aforementioned clustering objective with high accuracy even when\nthe workloads change. Experimental evaluation of Tunable-LSH in our prototype\nRDF data management system, chameleon-db, as well as in a standalone hashtable\nshows significant end-to-end improvements over existing solutions.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2015 00:34:59 GMT"}, {"version": "v2", "created": "Sun, 19 Apr 2015 01:04:28 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Alu\u00e7", "G\u00fcne\u015f", ""], ["\u00d6zsu", "M. Tamer", ""], ["Daudjee", "Khuzaima", ""]]}, {"id": "1504.02616", "submitter": "EPTCS", "authors": "Luc Moreau (University of Southampton)", "title": "Aggregation by Provenance Types: A Technique for Summarising Provenance\n  Graphs", "comments": "In Proceedings GaM 2015, arXiv:1504.02448", "journal-ref": "EPTCS 181, 2015, pp. 129-144", "doi": "10.4204/EPTCS.181.9", "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As users become confronted with a deluge of provenance data, dedicated\ntechniques are required to make sense of this kind of information. We present\nAggregation by Provenance Types, a provenance graph analysis that is capable of\ngenerating provenance graph summaries. It proceeds by converting provenance\npaths up to some length k to attributes, referred to as provenance types, and\nby grouping nodes that have the same provenance types. The summary also\nincludes numeric values representing the frequency of nodes and edges in the\noriginal graph. A quantitative evaluation and a complexity analysis show that\nthis technique is tractable; with small values of k, it can produce useful\nsummaries and can help detect outliers. We illustrate how the generated\nsummaries can further be used for conformance checking and visualization.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2015 09:41:37 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Moreau", "Luc", "", "University of Southampton"]]}, {"id": "1504.02875", "submitter": "Kira Adaricheva V", "authors": "Kira Adaricheva and J.B.Nation", "title": "Discovery of the $D$-basis in binary tables based on hypergraph\n  dualization", "comments": "14 pages, one table and one figure", "journal-ref": "Theoretical Computer Science, v.658 Part B, 307-315 (2017)", "doi": "10.1016/j.tcs.2015.11.031", "report-no": null, "categories": "cs.DB cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovery of (strong) association rules, or implications, is an important\ntask in data management, and it finds application in artificial intelligence,\ndata mining and the semantic web. We introduce a novel approach for the\ndiscovery of a specific set of implications, called the $D$-basis, that\nprovides a representation for a reduced binary table, based on the structure of\nits Galois lattice. At the core of the method are the $D$-relation defined in\nthe lattice theory framework, and the hypergraph dualization algorithm that\nallows us to effectively produce the set of transversals for a given Sperner\nhypergraph. The latter algorithm, first developed by specialists from Rutgers\nCenter for Operations Research, has already found numerous applications in\nsolving optimization problems in data base theory, artificial intelligence and\ngame theory. One application of the method is for analysis of gene expression\ndata related to a particular phenotypic variable, and some initial testing is\ndone for the data provided by the University of Hawaii Cancer Center.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2015 13:53:39 GMT"}, {"version": "v2", "created": "Sun, 31 Jan 2016 08:10:46 GMT"}], "update_date": "2017-01-27", "authors_parsed": [["Adaricheva", "Kira", ""], ["Nation", "J. B.", ""]]}, {"id": "1504.02957", "submitter": "Amel Grissa Prof.", "authors": "Fadoua Hassen and Amel Grissa Touzi", "title": "Intelligent Implementation Processor Design for Oracle Distributed\n  Databases System", "comments": null, "journal-ref": "International Conference on Control, Engineering & Information\n  Technology (CEIT 2014), pp. 278-296, Tunisie, 2014", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the increasing need for modeling and implementing Distributed\nDatabases (DDB), distributed database management systems are still quite far\nfrom helping the designer to directly implement its BDD. Indeed, the\nfundamental principle of implementation of a DDB is to make the database appear\nas a centralized database, providing series of transparencies, something that\nis not provided directly by the current DDBMS. We focus in this work on Oracle\nDBMS which, despite its market dominance, offers only a few logical mechanisms\nto implement distribution. To remedy this problem, we propose a new\narchitecture of DDBMS Oracle. The idea is based on extending it by an\nintelligent layer that provides: 1) creation of different types of\nfragmentation through a GUI for defining different sites geographically\ndispersed 2) allocation and replication of DB. The system must automatically\ngenerate SQL scripts for each site of the original configuration.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2015 11:23:26 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Hassen", "Fadoua", ""], ["Touzi", "Amel Grissa", ""]]}, {"id": "1504.03247", "submitter": "Foto Afrati", "authors": "Foto N. Afrati, Jeffrey D. Ullman, Angelos Vasilakopoulos", "title": "Handling Skew in Multiway Joins in Parallel Processing", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Handling skew is one of the major challenges in query processing. In\ndistributed computational environments such as MapReduce, uneven distribution\nof the data to the servers is not desired. One of the dominant measures that we\nwant to optimize in distributed environments is communication cost. In a\nMapReduce job this is the amount of data that is transferred from the mappers\nto the reducers. In this paper we will introduce a novel technique for handling\nskew when we want to compute a multiway join in one MapReduce round with\nminimum communication cost. This technique is actually an adaptation of the\nShares algorithm [Afrati et. al, TKDE 2011].\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2015 16:29:10 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Afrati", "Foto N.", ""], ["Ullman", "Jeffrey D.", ""], ["Vasilakopoulos", "Angelos", ""]]}, {"id": "1504.03386", "submitter": "Leopoldo Bertossi", "authors": "Mostafa Milani and Leopoldo Bertossi", "title": "Tractable Query Answering and Optimization for Extensions of\n  Weakly-Sticky Datalog+-", "comments": "To appear in Proc. Alberto Mendelzon WS on Foundations of Data\n  Management (AMW15)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a semantic class, weakly-chase-sticky (WChS), and a syntactic\nsubclass, jointly-weakly-sticky (JWS), of Datalog+- programs. Both extend that\nof weakly-sticky (WS) programs, which appear in our applications to data\nquality. For WChS programs we propose a practical, polynomial-time query\nanswering algorithm (QAA). We establish that the two classes are closed under\nmagic-sets rewritings. As a consequence, QAA can be applied to the optimized\nprograms. QAA takes as inputs the program (including the query) and semantic\ninformation about the \"finiteness\" of predicate positions. For the syntactic\nsubclasses JWS and WS of WChS, this additional information is computable.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2015 23:01:58 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Milani", "Mostafa", ""], ["Bertossi", "Leopoldo", ""]]}, {"id": "1504.03440", "submitter": "Georgios Kellaris", "authors": "Georgios Kellaris, Stavros Papadopoulos, and Dimitris Papadias", "title": "Engineering Methods for Differentially Private Histograms: Efficiency\n  Beyond Utility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Publishing histograms with $\\epsilon$-differential privacy has been studied\nextensively in the literature. Existing schemes aim at maximizing the utility\nof the published data, while previous experimental evaluations analyze the\nprivacy/utility trade-off. In this paper we provide the first experimental\nevaluation of differentially private methods that goes beyond utility,\nemphasizing also on another important aspect, namely efficiency. Towards this\nend, we first observe that all existing schemes are comprised of a small set of\ncommon blocks. We then optimize and choose the best implementation for each\nblock, determine the combinations of blocks that capture the entire literature,\nand propose novel block combinations. We qualitatively assess the quality of\nthe schemes based on the skyline of efficiency and utility, i.e., based on\nwhether a method is dominated on both aspects or not. Using exhaustive\nexperiments on four real datasets with different characteristics, we conclude\nthat there are always trade-offs in terms of utility and efficiency. We\ndemonstrate that the schemes derived from our novel block combinations provide\nthe best trade-offs for time critical applications. Our work can serve as a\nguide to help practitioners engineer a differentially private histogram scheme\ndepending on their application requirements.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 07:29:25 GMT"}, {"version": "v2", "created": "Thu, 20 Apr 2017 19:27:53 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Kellaris", "Georgios", ""], ["Papadopoulos", "Stavros", ""], ["Papadias", "Dimitris", ""]]}, {"id": "1504.03770", "submitter": "Xuhui Li", "authors": "Xuhui Li, Mengchi Liu, Xiaoying Wu and Shanfeng Zhu", "title": "Design Issues of JPQ: a Pattern-based Query Language for Document\n  Databases", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document databases are becoming popular, but how to present complex document\nquery to obtain useful information from the document remains an important topic\nto study. In this paper, we describe the design issues of a pattern-based\ndocument database query language named JPQ. JPQ uses various expressive\npatterns to extract and construct document fragments following a JSON-like\ndocument data model. It adopts tree-like extraction patterns with a coherent\npattern composition mechanism to extract data elements from hierarchically\nstructured documents and maintain the logical relationships among the elements.\nBased on these relationships, JPQ deploys a deductive mechanism to\ndeclaratively specify the data transformation requests and considers also data\nfiltering on hierarchical data structure. We use various examples to show the\nfeatures of the language and to demonstrate its expressiveness and\ndeclarativeness in presenting complex document queries.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2015 02:35:11 GMT"}], "update_date": "2015-04-16", "authors_parsed": [["Li", "Xuhui", ""], ["Liu", "Mengchi", ""], ["Wu", "Xiaoying", ""], ["Zhu", "Shanfeng", ""]]}, {"id": "1504.04031", "submitter": "Minyar Sassi", "authors": "Olfa Arfaoui and Minyar Sassi Hidri", "title": "Mining Semi-structured Data", "comments": null, "journal-ref": "The 5th International Conference on Web and Information\n  Technologies (ICWIT), pp. 51-60, 2013", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for discovering knowledge from XML documents according to both\nstructure and content features has become challenging, due to the increase in\napplication contexts for which handling both structure and content information\nin XML data is essential. So, the challenge is to find an hierarchical\nstructure which ensure a combination of data levels and their representative\nstructures. In this work, we will be based on the Formal Concept Analysis-based\nviews to index and query both content and structure. We evaluate given\nstructure in a querying process which allows the searching of user query\nanswers.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2015 20:06:33 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Arfaoui", "Olfa", ""], ["Hidri", "Minyar Sassi", ""]]}, {"id": "1504.04044", "submitter": "Mahmoud Abo Khamis", "authors": "Mahmoud Abo Khamis, Hung Q. Ngo, Atri Rudra", "title": "FAQ: Questions Asked Frequently", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define and study the Functional Aggregate Query (FAQ) problem, which\nencompasses many frequently asked questions in constraint satisfaction,\ndatabases, matrix operations, probabilistic graphical models and logic. This is\nour main conceptual contribution.\n  We then present a simple algorithm called \"InsideOut\" to solve this general\nproblem. InsideOut is a variation of the traditional dynamic programming\napproach for constraint programming based on variable elimination. Our\nvariation adds a couple of simple twists to basic variable elimination in order\nto deal with the generality of FAQ, to take full advantage of Grohe and Marx's\nfractional edge cover framework, and of the analysis of recent worst-case\noptimal relational join algorithms.\n  As is the case with constraint programming and graphical model inference, to\nmake InsideOut run efficiently we need to solve an optimization problem to\ncompute an appropriate 'variable ordering'. The main technical contribution of\nthis work is a precise characterization of when a variable ordering is\n'semantically equivalent' to the variable ordering given by the input FAQ\nexpression. Then, we design an approximation algorithm to find an equivalent\nvariable ordering that has the best 'fractional FAQ-width'. Our results imply a\nhost of known and a few new results in graphical model inference, matrix\noperations, relational joins, and logic.\n  We also briefly explain how recent algorithms on beyond worst-case analysis\nfor joins and those for solving SAT and #SAT can be viewed as variable\nelimination to solve FAQ over compactly represented input functions.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2015 20:31:00 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2015 23:38:27 GMT"}, {"version": "v3", "created": "Mon, 11 May 2015 17:49:01 GMT"}, {"version": "v4", "created": "Sun, 9 Aug 2015 04:05:49 GMT"}, {"version": "v5", "created": "Fri, 15 Apr 2016 09:55:45 GMT"}, {"version": "v6", "created": "Mon, 6 Feb 2017 01:14:58 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Khamis", "Mahmoud Abo", ""], ["Ngo", "Hung Q.", ""], ["Rudra", "Atri", ""]]}, {"id": "1504.04750", "submitter": "Dilek K\\\"u\\c{c}\\\"uk", "authors": "Dilek K\\\"u\\c{c}\\\"uk and Tolga \\.Inan and Burak Boyrazo\\u{g}lu and\n  Serkan Buhan and \\\"Ozg\\\"ul Salor and I\\c{s}{\\i}k \\c{C}ad{\\i}rc{\\i} and\n  Muammer Ermi\\c{s}", "title": "PQStream: A Data Stream Architecture for Electrical Power Quality", "comments": "Appears in Proceedings of International Workshop on Knowledge\n  Discovery from Ubiquitous Data Streams of ECML/PKDD, 2007", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a data stream architecture is presented for electrical power\nquality (PQ) which is called PQStream. PQStream is developed to process and\nmanage time-evolving data coming from the country-wide mobile measurements of\nelectrical PQ parameters of the Turkish Electricity Transmission System. It is\na full-fledged system with a data measurement module which carries out\nprocessing of continuous PQ data, a stream database which stores the output of\nthe measurement module, and finally a Graphical User Interface for\nretrospective analysis of the PQ data stored in the stream database. The\npresented model is deployed and is available to PQ experts, academicians and\nresearchers of the area. As further studies, data mining methods such as\nclassification and clustering algorithms will be applied in order to deduce\nuseful PQ information from this database of PQ data.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2015 18:25:18 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["K\u00fc\u00e7\u00fck", "Dilek", ""], ["\u0130nan", "Tolga", ""], ["Boyrazo\u011flu", "Burak", ""], ["Buhan", "Serkan", ""], ["Salor", "\u00d6zg\u00fcl", ""], ["\u00c7ad\u0131rc\u0131", "I\u015f\u0131k", ""], ["Ermi\u015f", "Muammer", ""]]}, {"id": "1504.05707", "submitter": "Daniel Ritter", "authors": "Daniel Ritter", "title": "Towards More Data-Aware Application Integration (extended version)", "comments": "18 Pages, extended version of the contribution to British\n  International Conference on Databases (BICOD), 2015, Edinburgh, Scotland", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although most business application data is stored in relational databases,\nprogramming languages and wire formats in integration middleware systems are\nnot table-centric. Due to costly format conversions, data-shipments and faster\ncomputation, the trend is to \"push-down\" the integration operations closer to\nthe storage representation.\n  We address the alternative case of defining declarative, table-centric\nintegration semantics within standard integration systems. For that, we replace\nthe current operator implementations for the well-known Enterprise Integration\nPatterns by equivalent \"in-memory\" table processing, and show a practical\nrealization in a conventional integration system for a non-reliable,\n\"data-intensive\" messaging example. The results of the runtime analysis show\nthat table-centric processing is promising already in standard, \"single-record\"\nmessage routing and transformations, and can potentially excel the message\nthroughput for \"multi-record\" table messages.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 09:40:01 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Ritter", "Daniel", ""]]}, {"id": "1504.06451", "submitter": "Marios Meimaris", "authors": "Marios Meimaris, George Papastefanatos, Christos Pateritsas, Theodora\n  Galani and Yannis Stavrakas", "title": "A Framework for Managing Evolving Information Resources on the Data Web", "comments": "arXiv admin note: text overlap with arXiv:1504.01891", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The web of data has brought forth the need to preserve and sustain evolving\ninformation within linked datasets; however, a basic requirement of data\npreservation is the maintenance of the datasets' structural characteristics as\nwell. As open data are often found using different and/or heterogeneous data\nmodels and schemata from one source to another, there is a need to reconcile\nthese mismatches and provide common denominations of interpretation on a\nmultitude of levels, in order to be able to preserve and manage the evolution\nof the generated resources. In this paper, we present a linked data approach\nfor the preservation and archiving of open heterogeneous datasets that evolve\nthrough time, at both the structural and the semantic layer. We first propose a\nset of re-quirements for modelling evolving linked datasets. We then proceed on\nconcep-tualizing a modelling framework for evolving entities and place these in\na 2x2 model space that consists of the semantic and the temporal dimensions.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2015 10:02:01 GMT"}, {"version": "v2", "created": "Tue, 5 May 2015 14:43:54 GMT"}], "update_date": "2015-05-06", "authors_parsed": [["Meimaris", "Marios", ""], ["Papastefanatos", "George", ""], ["Pateritsas", "Christos", ""], ["Galani", "Theodora", ""], ["Stavrakas", "Yannis", ""]]}, {"id": "1504.06867", "submitter": "Rafal Scherer", "authors": "Rafal Grycuk, Marcin Gabryel, Rafal Scherer, Sviatoslav Voloshynovskiy", "title": "Multi-layer Architecture For Storing Visual Data Based on WCF and\n  Microsoft SQL Server Database", "comments": "Accepted for the 14th International Conference on Artificial\n  Intelligence and Soft Computing, ICAISC, June 14-18, 2015, Zakopane, Poland", "journal-ref": null, "doi": "10.1007/978-3-319-19324-3_64", "report-no": null, "categories": "cs.DB cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper we present a novel architecture for storing visual data.\nEffective storing, browsing and searching collections of images is one of the\nmost important challenges of computer science. The design of architecture for\nstoring such data requires a set of tools and frameworks such as SQL database\nmanagement systems and service-oriented frameworks. The proposed solution is\nbased on a multi-layer architecture, which allows to replace any component\nwithout recompilation of other components. The approach contains five\ncomponents, i.e. Model, Base Engine, Concrete Engine, CBIR service and\nPresentation. They were based on two well-known design patterns: Dependency\nInjection and Inverse of Control. For experimental purposes we implemented the\nSURF local interest point detector as a feature extractor and $K$-means\nclustering as indexer. The presented architecture is intended for content-based\nretrieval systems simulation purposes as well as for real-world CBIR tasks.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2015 19:11:37 GMT"}], "update_date": "2015-06-29", "authors_parsed": [["Grycuk", "Rafal", ""], ["Gabryel", "Marcin", ""], ["Scherer", "Rafal", ""], ["Voloshynovskiy", "Sviatoslav", ""]]}, {"id": "1504.06920", "submitter": "Swapnil Kharche Mr", "authors": "Swapnil Kharche, Jagdish patil, Kanchan Gohad, Bharti Ambetkar", "title": "Preventing SQL Injection attack using pattern matching algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SQL injection attacks, a class of injection flaw in which specially crafted\ninput strings leads to illegal queries to databases, are one of the topmost\nthreats to web applications. A Number of research prototypes and commercial\nproducts that maintain the queries structure in web applications have been\ndeveloped. But these techniques either fail to address the full scope of the\nproblem or have limitations. Based on our observation that the injected string\nin a SQL injection attack is interpreted differently on different\ndatabases.Injection attack is a method that can inject any kind of malicious\nstring or anomaly string on the original string. Pattern matching is a\ntechnique that can be used to identify or detect any anomaly packet from a\nsequential action. Most of the pattern based techniques are used static\nanalysis and patterns are generated from the attacked statements. In this\npaper, we proposed a detection and prevention technique for preventing SQL\nInjection Attack using AhoCorasick pattern matching algorithm. In this paper,\nwe proposed an overview of the architecture. In the initial stage evaluation,\nwe consider some sample of standard attack patterns and it shows that the\nproposed algorithm is works well against the SQL Injection Attack.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 03:42:47 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Kharche", "Swapnil", ""], ["patil", "Jagdish", ""], ["Gohad", "Kanchan", ""], ["Ambetkar", "Bharti", ""]]}, {"id": "1504.07018", "submitter": "Vandit  Agarwal", "authors": "Vandit Agarwal, Mandhani Kushal and Dr. Preetham Kumar", "title": "An Improvised Frequent Pattern Tree Based Association Rule Mining\n  Technique with Mining Frequent Item Sets Algorithm and a Modified Header\n  Table", "comments": "15 pages, 8 tables, 7 figures, journal paper", "journal-ref": "International Journal of Data Mining & Knowledge Management\n  Process (IJDKP), March 2015, Volume 5, Number 2", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In todays world there is a wide availability of huge amount of data and thus\nthere is a need for turning this data into useful information which is referred\nto as knowledge. This demand for knowledge discovery process has led to the\ndevelopment of many algorithms used to determine the association rules. One of\nthe major problems faced by these algorithms is generation of candidate sets.\nThe FP Tree algorithm is one of the most preferred algorithms for association\nrule mining because it gives association rules without generating candidate\nsets. But in the process of doing so, it generates many CP trees which\ndecreases its efficiency. In this research paper, an improvised FP tree\nalgorithm with a modified header table, along with a spare table and the MFI\nalgorithm for association rule mining is proposed. This algorithm generates\nfrequent item sets without using candidate sets and CP trees.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 10:45:18 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Agarwal", "Vandit", ""], ["Kushal", "Mandhani", ""], ["Kumar", "Dr. Preetham", ""]]}, {"id": "1504.07597", "submitter": "Nicolas Turenne", "authors": "Nicolas Turenne", "title": "Duplicate Detection with Efficient Language Models for Automatic\n  Bibliographic Heterogeneous Data Integration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method to detect duplicates used to merge different\nbibliographic record corpora with the help of lexical and social information.\nAs we show, a trivial key is not available to delete useless documents. Merging\nheteregeneous document databases to get a maximum of information can be of\ninterest. In our case we try to build a document corpus about the TOR molecule\nso as to extract relationships with other gene components from PubMed and\nWebOfScience document databases. Our approach makes key fingerprints based on\nn-grams. We made two documents gold standards using this corpus to make an\nevaluation. Comparison with other well-known methods in deduplication gives\nbest scores of recall (95\\%) and precision (100\\%).\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 11:53:01 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Turenne", "Nicolas", ""]]}, {"id": "1504.07758", "submitter": "Jeremy Debattista", "authors": "Jeremy Debattista, Christoph Lange, S\\\"oren Auer", "title": "Luzzu Quality Metric Language -- A DSL for Linked Data Quality\n  Assessment", "comments": "arXiv admin note: text overlap with arXiv:1412.3750", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The steadily growing number of linked open datasets brought about a number of\nreservations amongst data consumers with regard to the datasets' quality.\nQuality assessment requires significant effort and consideration, including the\ndefinition of data quality metrics and a process to assess datasets based on\nthese definitions. Luzzu is a quality assessment framework for linked data that\nallows domain-specific metrics to be plugged in. LQML offers notations,\nabstractions and expressive power, focusing on the representation of quality\nmetrics. It provides expressive power for defining sophisticated quality\nmetrics. Its integration with Luzzu enables their efficient processing and\nexecution and thus the comprehensive assessment of extremely large datasets in\na streaming way. We also describe a novel ontology that enables the reuse,\nsharing and querying of such definitions. Finally, we evaluate the proposed DSL\nagainst the cognitive dimensions of notation framework.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 08:17:20 GMT"}], "update_date": "2015-04-30", "authors_parsed": [["Debattista", "Jeremy", ""], ["Lange", "Christoph", ""], ["Auer", "S\u00f6ren", ""]]}, {"id": "1504.08262", "submitter": "Nikolay Yakovets", "authors": "Nikolay Yakovets, Parke Godfrey, Jarek Gryz", "title": "Towards Query Optimization for SPARQL Property Paths", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extension of SPARQL in version 1.1 with property paths offers a type of\nregular path query for RDF graph databases. Such queries are difficult to\noptimize and evaluate efficiently, however. We have embarked on a project,\nWaveguide, to build a cost-based optimizer for SPARQL queries with property\npaths. Waveguide builds a query plan - a waveguide plan (WGP) - which guides\nthe query evaluation. There are numerous choices in the construction of a plan,\nand a number of optimization methods, meaning the space of plans for a query\ncan be quite large. Execution costs of plans for the same query can vary by\norders of magnitude. We illustrate the types of optimizations this approach\naffords and the performance gains that can be obtained. A WGP's costs can be\nestimated, which opens the way to cost-based optimization.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 15:04:31 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["Yakovets", "Nikolay", ""], ["Godfrey", "Parke", ""], ["Gryz", "Jarek", ""]]}]