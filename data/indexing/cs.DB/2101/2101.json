[{"id": "2101.00170", "submitter": "Daniel Szelogowski", "authors": "Daniel Szelogowski", "title": "Visualization Techniques with Data Cubes: Utilizing Concurrency for\n  Complex Data", "comments": "11 pages, 4 figures Update: Revised format to align closer to IEEE\n  standards", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With web and mobile platforms becoming more prominent devices utilized in\ndata analysis, there are currently few systems which are not without flaw. In\norder to increase the performance of these systems and decrease errors of data\noversimplification, we seek to understand how other programming languages can\nbe used across these platforms which provide data and type safety, as well as\nutilizing concurrency to perform complex data manipulation tasks.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 05:41:44 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 22:38:37 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Szelogowski", "Daniel", ""]]}, {"id": "2101.00171", "submitter": "Daniel Szelogowski", "authors": "Daniel Szelogowski", "title": "Optimizing Data Cube Visualization for Web Applications: Performance and\n  User-Friendly Data Aggregation", "comments": "12 pages, 2 figures, 3 tables Update: Revised format to align closer\n  to IEEE standards", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Current open source applications which allow for cross-platform data\nvisualization of OLAP cubes feature issues of high overhead and inconsistency\ndue to data oversimplification. To improve upon this issue, there is a need to\ncut down the number of pipelines that the data must travel between for these\naggregation operations and create a single, unified application which performs\nefficiently without sacrificing data, and allows for ease of usability and\nextension.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 05:42:38 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 22:34:35 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Szelogowski", "Daniel", ""]]}, {"id": "2101.00172", "submitter": "Daniel Szelogowski", "authors": "Daniel Szelogowski", "title": "Chunk List: Concurrent Data Structures", "comments": "20 pages, 3 figures A full implementation can be found at\n  https://github.com/danielathome19/Chunk-List Update: Revised format to align\n  closer to IEEE standards", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.DC cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Chunking data is obviously no new concept; however, I had never found any\ndata structures that used chunking as the basis of their implementation. I\nfigured that by using chunking alongside concurrency, I could create an\nextremely fast run-time in regards to particular methods as searching and/or\nsorting. By using chunking and concurrency to my advantage, I came up with the\nchunk list - a dynamic list-based data structure that would separate large\namounts of data into specifically sized chunks, each of which should be able to\nbe searched at the exact same time by searching each chunk on a separate\nthread. As a result of implementing this concept into its own class, I was able\nto create something that almost consistently gives around 20x-300x faster\nresults than a regular ArrayList. However, should speed be a particular issue\neven after implementation, users can modify the size of the chunks and\nbenchmark the speed of using smaller or larger chunks, depending on the amount\nof data being stored.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 05:45:56 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 22:32:25 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Szelogowski", "Daniel", ""]]}, {"id": "2101.00314", "submitter": "Otmar Ertl", "authors": "Otmar Ertl", "title": "SetSketch: Filling the Gap between MinHash and HyperLogLog", "comments": "extended version of major revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MinHash and HyperLogLog are sketching algorithms that have become\nindispensable for set summaries in big data applications. While HyperLogLog\nallows counting different elements with very little space, MinHash is suitable\nfor the fast comparison of sets as it allows estimating the Jaccard similarity\nand other joint quantities. This work presents a new data structure called\nSetSketch that is able to continuously fill the gap between both use cases. Its\ncommutative and idempotent insert operation and its mergeable state make it\nsuitable for distributed environments. Fast, robust, and easy-to-implement\nestimators for cardinality and joint quantities, as well as the ability to use\nSetSketch for similarity search, enable versatile applications. The presented\njoint estimator can also be applied to other data structures such as MinHash,\nHyperLogLog, or HyperMinHash, where it even performs better than the\ncorresponding state-of-the-art estimators in many cases.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 20:14:33 GMT"}, {"version": "v2", "created": "Sun, 16 May 2021 07:23:31 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Ertl", "Otmar", ""]]}, {"id": "2101.00361", "submitter": "Olga Poppe", "authors": "Olga Poppe, Chuan Lei, Lei Ma, Allison Rozet, Elke A. Rundensteiner", "title": "To Share, or not to Share Online Event Trend Aggregation Over Bursty\n  Event Streams", "comments": "Technical report for the paper in SIGMOD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex event processing (CEP) systems continuously evaluate large workloads\nof pattern queries under tight time constraints. Event trend aggregation\nqueries with Kleene patterns are commonly used to retrieve summarized insights\nabout the recent trends in event streams. State-of-art methods are limited\neither due to repetitive computations or unnecessary trend construction.\nExisting shared approaches are guided by statically selected and hence rigid\nsharing plans that are often sub-optimal under stream fluctuations. In this\nwork, we propose a novel framework Hamlet that is the first to overcome these\nlimitations. Hamlet introduces two key innovations. First, Hamlet adaptively\ndecides whether to share or not to share computations depending on the current\nstream properties at run time to harvest the maximum sharing benefit. Second,\nHamlet is equipped with a highly efficient shared trend aggregation strategy\nthat avoids trend construction. Our experimental study on both real and\nsynthetic data sets demonstrates that Hamlet consistently reduces query latency\nby up to five orders of magnitude compared to the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 03:21:32 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 06:18:38 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Poppe", "Olga", ""], ["Lei", "Chuan", ""], ["Ma", "Lei", ""], ["Rozet", "Allison", ""], ["Rundensteiner", "Elke A.", ""]]}, {"id": "2101.00808", "submitter": "Yaliang Li", "authors": "Yaliang Li, Daoyuan Chen, Bolin Ding, Kai Zeng, Jingren Zhou", "title": "A Pluggable Learned Index Method via Sampling and Gap Insertion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Database indexes facilitate data retrieval and benefit broad applications in\nreal-world systems. Recently, a new family of index, named learned index, is\nproposed to learn hidden yet useful data distribution and incorporate such\ninformation into the learning of indexes, which leads to promising performance\nimprovements. However, the \"learning\" process of learned indexes is still\nunder-explored. In this paper, we propose a formal machine learning based\nframework to quantify the index learning objective, and study two general and\npluggable techniques to enhance the learning efficiency and learning\neffectiveness for learned indexes. With the guidance of the formal learning\nobjective, we can efficiently learn index by incorporating the proposed\nsampling technique, and learn precise index with enhanced generalization\nability brought by the proposed result-driven gap insertion technique.\n  We conduct extensive experiments on real-world datasets and compare several\nindexing methods from the perspective of the index learning objective. The\nresults show the ability of the proposed framework to help to design suitable\nindexes for different scenarios. Further, we demonstrate the effectiveness of\nthe proposed sampling technique, which achieves up to 78x construction speedup\nwhile maintaining non-degraded indexing performance. Finally, we show the gap\ninsertion technique can enhance both the static and dynamic indexing\nperformances of existing learned index methods with up to 1.59x query speedup.\nWe will release our codes and processed data for further study, which can\nenable more exploration of learned indexes from both the perspectives of\nmachine learning and database.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 07:17:23 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Li", "Yaliang", ""], ["Chen", "Daoyuan", ""], ["Ding", "Bolin", ""], ["Zeng", "Kai", ""], ["Zhou", "Jingren", ""]]}, {"id": "2101.00810", "submitter": "Aman Abidi", "authors": "Aman Abidi, Lu Chen, Rui Zhou, Chengfei Liu", "title": "Searching Personalized $k$-wing in Large and Dynamic Bipartite Graphs", "comments": "13 pages, 10 figures and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are extensive studies focusing on the application scenario that all the\nbipartite cohesive subgraphs need to be discovered in a bipartite graph.\nHowever, we observe that, for some applications, one is interested in finding\nbipartite cohesive subgraphs containing a specific vertex. In this paper, we\nstudy a new query dependent bipartite cohesive subgraph search problem based on\n$k$-wing model, named as the personalized $k$-wing search problem. We introduce\na $k$-wing equivalence relationship to summarize the edges of a bipartite graph\n$G$ into groups. Therefore, all the edges of $G$ are segregated into different\ngroups, i.e. $k$-wing equivalence class, forming an efficient and wing number\nconserving index called EquiWing. Further, we propose a more compact version of\nEquiWing, EquiWing-Comp, which is achieved by integrating our proposed\n$k$-butterfly loose approach and discovered hierarchy properties. These indices\nare used to expedite the personalized $k$-wing search with a non-repetitive\naccess to $G$, which leads to linear algorithms for searching the personalized\n$k$-wing. Moreover, we conduct a thorough study on the maintenance of the\nproposed indices for evolving bipartite graphs. We discover novel properties\nthat help us localize the scope of the maintenance at a low cost. By exploiting\nthe discoveries, we propose novel algorithms for maintaining the two indices,\nwhich substantially reduces the cost of maintenance. We perform extensive\nexperimental studies in real, large-scale graphs to validate the efficiency and\neffectiveness of EquiWing and EquiWing-Comp compared to the baseline.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 07:19:52 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 02:13:03 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Abidi", "Aman", ""], ["Chen", "Lu", ""], ["Zhou", "Rui", ""], ["Liu", "Chengfei", ""]]}, {"id": "2101.01159", "submitter": "Joseph Hellerstein", "authors": "Alvin Cheung, Natacha Crooks, Joseph M. Hellerstein and Matthew Milano", "title": "New Directions in Cloud Programming", "comments": null, "journal-ref": "CIDR 2021", "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.OS cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nearly twenty years after the launch of AWS, it remains difficult for most\ndevelopers to harness the enormous potential of the cloud. In this paper we lay\nout an agenda for a new generation of cloud programming research aimed at\nbringing research ideas to programmers in an evolutionary fashion. Key to our\napproach is a separation of distributed programs into a PACT of four facets:\nProgram semantics, Availablity, Consistency and Targets of optimization. We\npropose to migrate developers gradually to PACT programming by lifting familiar\ncode into our more declarative level of abstraction. We then propose a\nmulti-stage compiler that emits human-readable code at each stage that can be\nhand-tuned by developers seeking more control. Our agenda raises numerous\nresearch challenges across multiple areas including language design, query\noptimization, transactions, distributed consistency, compilers and program\nsynthesis.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 18:42:54 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Cheung", "Alvin", ""], ["Crooks", "Natacha", ""], ["Hellerstein", "Joseph M.", ""], ["Milano", "Matthew", ""]]}, {"id": "2101.01292", "submitter": "Maximilian Schleich", "authors": "Maximilian Schleich, Zixuan Geng, Yihong Zhang, Dan Suciu", "title": "GeCo: Quality Counterfactual Explanations in Real Time", "comments": "16 pages, 12 figures, 3 tables, 3 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning is increasingly applied in high-stakes decision making that\ndirectly affect people's lives, and this leads to an increased demand for\nsystems to explain their decisions. Explanations often take the form of\ncounterfactuals, which consists of conveying to the end user what she/he needs\nto change in order to improve the outcome. Computing counterfactual\nexplanations is challenging, because of the inherent tension between a rich\nsemantics of the domain, and the need for real time response. In this paper we\npresent GeCo, the first system that can compute plausible and feasible\ncounterfactual explanations in real time. At its core, GeCo relies on a genetic\nalgorithm, which is customized to favor searching counterfactual explanations\nwith the smallest number of changes. To achieve real-time performance, we\nintroduce two novel optimizations: $\\Delta$-representation of candidate\ncounterfactuals, and partial evaluation of the classifier. We compare\nempirically GeCo against five other systems described in the literature, and\nshow that it is the only system that can achieve both high quality explanations\nand real time answers.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 00:23:58 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 04:23:30 GMT"}, {"version": "v3", "created": "Wed, 19 May 2021 00:02:49 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Schleich", "Maximilian", ""], ["Geng", "Zixuan", ""], ["Zhang", "Yihong", ""], ["Suciu", "Dan", ""]]}, {"id": "2101.01363", "submitter": "Xiaoou Ding", "authors": "Xiaoou Ding, Hongzhi Wang, Chen Wang, Zijue Li, Zheng Liang", "title": "Exploring Data and Knowledge combined Anomaly Explanation of\n  Multivariate Industrial Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The demand for high-performance anomaly detection techniques of IoT data\nbecomes urgent, especially in industry field. The anomaly identification and\nexplanation in time series data is one essential task in IoT data mining. Since\nthat the existing anomaly detection techniques focus on the identification of\nanomalies, the explanation of anomalies is not well-solved. We address the\nanomaly explanation problem for multivariate IoT data and propose a 3-step\nself-contained method in this paper. We formalize and utilize the domain\nknowledge in our method, and identify the anomalies by the violation of\nconstraints. We propose set-cover-based anomaly explanation algorithms to\ndiscover the anomaly events reflected by violation features, and further\ndevelop knowledge update algorithms to improve the original knowledge set.\nExperimental results on real datasets from large-scale IoT systems verify that\nour method computes high-quality explanation solutions of anomalies. Our work\nprovides a guide to navigate the explicable anomaly detection in both IoT fault\ndiagnosis and temporal data cleaning.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 06:19:39 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Ding", "Xiaoou", ""], ["Wang", "Hongzhi", ""], ["Wang", "Chen", ""], ["Li", "Zijue", ""], ["Liang", "Zheng", ""]]}, {"id": "2101.01507", "submitter": "Hai Lan", "authors": "Hai Lan, Zhifeng Bao, Yuwei Peng", "title": "A Survey on Advancing the DBMS Query Optimizer: Cardinality Estimation,\n  Cost Model, and Plan Enumeration", "comments": "This paper was accepted by Data Science and Engineering (DSEJ) in\n  Dec, 2020", "journal-ref": null, "doi": "10.1007/s41019-020-00149-7", "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query optimizer is at the heart of the database systems. Cost-based optimizer\nstudied in this paper is adopted in almost all current database systems. A\ncost-based optimizer introduces a plan enumeration algorithm to find a\n(sub)plan, and then uses a cost model to obtain the cost of that plan, and\nselects the plan with the lowest cost. In the cost model, cardinality, the\nnumber of tuples through an operator, plays a crucial role. Due to the\ninaccuracy in cardinality estimation, errors in cost model, and the huge plan\nspace, the optimizer cannot find the optimal execution plan for a complex query\nin a reasonable time. In this paper, we first deeply study the causes behind\nthe limitations above. Next, we review the techniques used to improve the\nquality of the three key components in the cost-based optimizer, cardinality\nestimation, cost model, and plan enumeration. We also provide our insights on\nthe future directions for each of the above aspects.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 13:47:45 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Lan", "Hai", ""], ["Bao", "Zhifeng", ""], ["Peng", "Yuwei", ""]]}, {"id": "2101.01852", "submitter": "Xikui Wang", "authors": "Xikui Wang, Michael J. Carey, Vassilis J. Tsotras", "title": "Bridging BAD Islands: Declarative Data Sharing at Scale", "comments": "10 pages, 34 figures, to appear on IEEE Big Data - Workshop on\n  Scalable Cloud Data Management", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many Big Data applications today, information needs to be actively shared\nbetween systems managed by different organizations. To enable sharing Big Data\nat scale, developers would have to create dedicated server programs and glue\ntogether multiple Big Data systems for scalability. Developing and managing\nsuch glued data sharing services requires a significant amount of work from\ndevelopers. In our prior work, we developed a Big Active Data (BAD) system for\nenabling Big Data subscriptions and analytics with millions of subscribers.\nBased on that, we introduce a new mechanism for enabling the sharing of Big\nData at scale declaratively so that developers can easily create and provide\ndata sharing services using declarative statements and can benefit from an\nunderlying scalable infrastructure. We show our implementation on top of the\nBAD system, explain the data sharing data flow among multiple systems, and\npresent a prototype system with experimental results.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 03:22:29 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Wang", "Xikui", ""], ["Carey", "Michael J.", ""], ["Tsotras", "Vassilis J.", ""]]}, {"id": "2101.01898", "submitter": "Mingxi Wu", "authors": "Mingxi Wu, Xi Chen", "title": "Connecting The Dots To Combat Collective Fraud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern fraudsters write malicious programs to coordinate a group of accounts\nto commit collective fraud for illegal profits in online platforms. These\nprograms have access to a set of finite resources - a set of IPs, devices, and\naccounts etc. and sometime manipulate fake accounts to collaboratively attack\nthe target system. Inspired by these observations, we share our experience in\nbuilding two real-time risk control systems to detect collective fraud. We show\nthat with TigerGraph, a powerful graph database, and its innovative query\nlanguage - GSQL, data scientists and fraud experts can conveniently implement\nand deploy an end-to-end risk control system as a graph database application.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 07:28:23 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Wu", "Mingxi", ""], ["Chen", "Xi", ""]]}, {"id": "2101.01945", "submitter": "Markus Schmid", "authors": "Katrin Casel and Markus L. Schmid", "title": "Fine-Grained Complexity of Regular Path Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DB cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A regular path query (RPQ) is a regular expression q that returns all node\npairs (u, v) from a graph database that are connected by an arbitrary path\nlabelled with a word from L(q). The obvious algorithmic approach to\nRPQ-evaluation (called PG-approach), i.e., constructing the product graph\nbetween an NFA for q and the graph database, is appealing due to its simplicity\nand also leads to efficient algorithms. However, it is unclear whether the\nPG-approach is optimal. We address this question by thoroughly investigating\nwhich upper complexity bounds can be achieved by the PG-approach, and we\ncomplement these with conditional lower bounds (in the sense of the\nfine-grained complexity framework). A special focus is put on enumeration and\ndelay bounds, as well as the data complexity perspective. A main insight is\nthat we can achieve optimal (or near optimal) algorithms with the PG-approach,\nbut the delay for enumeration is rather high (linear in the database). We\nexplore three successful approaches towards enumeration with sub-linear delay:\nsuper-linear preprocessing, approximations of the solution sets, and restricted\nclasses of RPQs.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 10:07:16 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Casel", "Katrin", ""], ["Schmid", "Markus L.", ""]]}, {"id": "2101.02174", "submitter": "Reza Karegar", "authors": "Reza Karegar, Parke Godfrey, Lukasz Golab, Mehdi Kargar, Divesh\n  Srivastava, Jaroslaw Szlichta", "title": "Efficient Discovery of Approximate Order Dependencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Order dependencies (ODs) capture relationships between ordered domains of\nattributes. Approximate ODs (AODs) capture such relationships even when there\nexist exceptions in the data. During automated discovery of ODs, validation is\nthe process of verifying whether an OD holds. We present an algorithm for\nvalidating approximate ODs with significantly improved runtime performance over\nexisting methods for AODs, and prove that it is correct and has optimal\nruntime. By replacing the validation step in a leading algorithm for\napproximate OD discovery with ours, we achieve orders-of-magnitude improvements\nin performance.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 18:22:52 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Karegar", "Reza", ""], ["Godfrey", "Parke", ""], ["Golab", "Lukasz", ""], ["Kargar", "Mehdi", ""], ["Srivastava", "Divesh", ""], ["Szlichta", "Jaroslaw", ""]]}, {"id": "2101.02466", "submitter": "Miika Hannula", "authors": "Miika Hannula, Juha Kontinen, Sebastian Link", "title": "On the Interaction of Functional and Inclusion Dependencies with\n  Independence Atoms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infamously, the finite and unrestricted implication problems for the classes\nof i) functional and inclusion dependencies together, and ii) embedded\nmultivalued dependencies alone are each undecidable. Famously, the restriction\nof i) to functional and unary inclusion dependencies in combination with the\nrestriction of ii) to multivalued dependencies yield implication problems that\nare still different in the finite and unrestricted case, but each are finitely\naxiomatizable and decidable in low-degree polynomial time. An important\nembedded tractable fragment of embedded multivalued dependencies are\nindependence atoms that stipulate independence between two attribute sets. We\nestablish a series of results for implication problems over subclasses of the\ncombined class of functional and inclusion dependencies as well as independence\natoms. One of our main results is that both finite and unrestricted implication\nproblems for the combined class of independence atoms, unary functional and\nunary inclusion dependencies are axiomatizable and decidable in low-degree\npolynomial time.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 10:13:02 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Hannula", "Miika", ""], ["Kontinen", "Juha", ""], ["Link", "Sebastian", ""]]}, {"id": "2101.02472", "submitter": "Miika Hannula", "authors": "Miika Hannula, Xinyi Li, Sebastian Link", "title": "Controlling Entity Integrity with Key Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Codd's rule of entity integrity stipulates that every table has a primary\nkey. Hence, the attributes of the primary key carry unique and complete value\ncombinations. In practice, data cannot always meet such requirements. Previous\nwork proposed the superior notion of key sets for controlling entity integrity.\nWe establish a linear-time algorithm for validating whether a given key set\nholds on a given data set, and demonstrate its efficiency on real-world data.\nWe establish a binary axiomatization for the associated implication problem,\nand prove its coNP-completeness. However, the implication of unary by arbitrary\nkey sets has better properties. The fragment enjoys a unary axiomatization and\nis decidable in quadratic time. Hence, we can minimize overheads before\nvalidating key sets. While perfect models do not always exist in general, we\nshow how to compute them for any instance of our fragment. This provides\ncomputational support towards the acquisition of key sets.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 10:36:51 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Hannula", "Miika", ""], ["Li", "Xinyi", ""], ["Link", "Sebastian", ""]]}, {"id": "2101.02502", "submitter": "Miika Hannula", "authors": "Miika Hannula, Bor-Kuan Song, Sebastian Link", "title": "An Algorithm for the Discovery of Independence from Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For years, independence has been considered as an important concept in many\ndisciplines. Nevertheless, we present the first research that investigates the\ndiscovery problem of independence in data. In its arguably simplest form,\nindependence is a statement between two sets of columns expressing that for\nevery two rows in a table there is also a row in the table that coincides with\nthe first row on the first set of columns and with the second row on the second\nset of columns. We show that the problem of deciding whether there is an\nindependence statement that holds on a given table is not only NP-complete but\n$W[3]$-complete in its arguably most natural parameter, namely its arity. We\nestablish the first algorithm to discover all independence statement that hold\non a given table. We illustrate in experiments with benchmark data that our\nalgorithm performs well within the limits established by our hardness results.\nIn practice, it is often useful to determine the ratio with which independence\nstatements hold on a given table. For that purpose, we show that our treatment\nof independence and the design of our algorithm enables us to extend our\nfindings to approximate independence. In our final experiments, we provide some\ninsight into the trade-off between run time and the approximation ratio.\nNaturally, the smaller the ratio, the more approximate independence statements\nhold, and the more time it takes to discover all of them. While this research\nestablishes first insight into the computational properties of discovering\nindependence from data, we hope to initiate research into more sophisticated\nnotions of independence, including embedded multivalued dependencies, as well\nas their context-specific and probabilistic variants.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 11:43:01 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Hannula", "Miika", ""], ["Song", "Bor-Kuan", ""], ["Link", "Sebastian", ""]]}, {"id": "2101.02591", "submitter": "William Godoy", "authors": "William F Godoy, Peter F Peterson, Steven E Hahn, Jay J Billings", "title": "Efficient Data Management in Neutron Scattering Data Reduction Workflows\n  at ORNL", "comments": "7 pages, 4 figures, International Workshop on Big Data Reduction held\n  with 2020 IEEE International Conference on Big Data", "journal-ref": null, "doi": "10.1109/BigData50022.2020.9377836", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Oak Ridge National Laboratory (ORNL) experimental neutron science facilities\nproduce 1.2\\,TB a day of raw event-based data that is stored using the standard\nmetadata-rich NeXus schema built on top of the HDF5 file format. Performance of\nseveral data reduction workflows is largely determined by the amount of time\nspent on the loading and processing algorithms in Mantid, an open-source data\nanalysis framework used across several neutron sciences facilities around the\nworld. The present work introduces new data management algorithms to address\nidentified input output (I/O) bottlenecks on Mantid. First, we introduce an\nin-memory binary-tree metadata index that resemble NeXus data access patterns\nto provide a scalable search and extraction mechanism. Second, data\nencapsulation in Mantid algorithms is optimally redesigned to reduce the total\ncompute and memory runtime footprint associated with metadata I/O\nreconstruction tasks. Results from this work show speed ups in wall-clock time\non ORNL data reduction workflows, ranging from 11\\% to 30\\% depending on the\ncomplexity of the targeted instrument-specific data. Nevertheless, we highlight\nthe need for more research to address reduction challenges as experimental data\nvolumes increase.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 20:58:51 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Godoy", "William F", ""], ["Peterson", "Peter F", ""], ["Hahn", "Steven E", ""], ["Billings", "Jay J", ""]]}, {"id": "2101.02627", "submitter": "Majid Rafiei", "authors": "Majid Rafiei and Wil M.P. van der Aalst", "title": "Privacy-Preserving Data Publishing in Process Mining", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-58638-6_8", "report-no": null, "categories": "cs.CR cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Process mining aims to provide insights into the actual processes based on\nevent data. These data are often recorded by information systems and are widely\navailable. However, they often contain sensitive private information that\nshould be analyzed responsibly. Therefore, privacy issues in process mining are\nrecently receiving more attention. Privacy preservation techniques obviously\nneed to modify the original data, yet, at the same time, they are supposed to\npreserve the data utility. Privacy-preserving transformations of the data may\nlead to incorrect or misleading analysis results. Hence, new infrastructures\nneed to be designed for publishing the privacy-aware event data whose aim is to\nprovide metadata regarding the privacy-related transformations on event data\nwithout revealing details of privacy preservation techniques or the protected\ninformation. In this paper, we provide formal definitions for the main\nanonymization operations, used by privacy models in process mining. These are\nused to create an infrastructure for recording the privacy metadata. We\nadvocate the proposed privacy metadata in practice by designing a privacy\nextension for the XES standard and a general data structure for event data\nwhich are not in the form of standard event logs.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 15:03:28 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Rafiei", "Majid", ""], ["van der Aalst", "Wil M. P.", ""]]}, {"id": "2101.02914", "submitter": "Meifan Zhang", "authors": "Meifan Zhang and Hongzhi Wang", "title": "Approximate Query Processing for Group-By Queries based on Conditional\n  Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Group-By query is an important kind of query, which is common and widely\nused in data warehouses, data analytics, and data visualization. Approximate\nquery processing is an effective way to increase the querying efficiency on big\ndata. The answer to a group-by query involves multiple values, which makes it\ndifficult to provide sufficiently accurate estimations for all the groups.\nStratified sampling improves the accuracy compared with the uniform sampling,\nbut the samples chosen for some special queries cannot work for other queries.\nOnline sampling chooses samples for the given query at query time, but it\nrequires a long latency. Thus, it is a challenge to achieve both accuracy and\nefficiency at the same time. Facing such challenge, in this work, we propose a\nsample generation framework based on a conditional generative model. The sample\ngeneration framework can generate any number of samples for the given query\nwithout accessing the data. The proposed framework based on the lightweight\nmodel can be combined with stratified sampling and online aggregation to\nimprove the estimation accuracy for group-by queries. The experimental results\nshow that our proposed methods are both efficient and accurate.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 08:49:21 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Zhang", "Meifan", ""], ["Wang", "Hongzhi", ""]]}, {"id": "2101.02969", "submitter": "Hui Luo", "authors": "Hui Luo, Jingbo Zhou, Zhifeng Bao, Shuangli Li, J. Shane Culpepper,\n  Haochao Ying, Hao Liu, Hui Xiong", "title": "Spatial Object Recommendation with Hints: When Spatial Granularity\n  Matters", "comments": null, "journal-ref": "SIGIR Conference (2020) 781-790", "doi": "10.1145/3397271.3401090", "report-no": null, "categories": "cs.IR cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing spatial object recommendation algorithms generally treat objects\nidentically when ranking them. However, spatial objects often cover different\nlevels of spatial granularity and thereby are heterogeneous. For example, one\nuser may prefer to be recommended a region (say Manhattan), while another user\nmight prefer a venue (say a restaurant). Even for the same user, preferences\ncan change at different stages of data exploration. In this paper, we study how\nto support top-k spatial object recommendations at varying levels of spatial\ngranularity, enabling spatial objects at varying granularity, such as a city,\nsuburb, or building, as a Point of Interest (POI). To solve this problem, we\npropose the use of a POI tree, which captures spatial containment relationships\nbetween POIs. We design a novel multi-task learning model called MPR (short for\nMulti-level POI Recommendation), where each task aims to return the top-k POIs\nat a certain spatial granularity level. Each task consists of two subtasks: (i)\nattribute-based representation learning; (ii) interaction-based representation\nlearning. The first subtask learns the feature representations for both users\nand POIs, capturing attributes directly from their profiles. The second subtask\nincorporates user-POI interactions into the model. Additionally, MPR can\nprovide insights into why certain recommendations are being made to a user\nbased on three types of hints: user-aspect, POI-aspect, and interaction-aspect.\nWe empirically validate our approach using two real-life datasets, and show\npromising performance improvements over several state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 11:39:51 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Luo", "Hui", ""], ["Zhou", "Jingbo", ""], ["Bao", "Zhifeng", ""], ["Li", "Shuangli", ""], ["Culpepper", "J. Shane", ""], ["Ying", "Haochao", ""], ["Liu", "Hao", ""], ["Xiong", "Hui", ""]]}, {"id": "2101.03020", "submitter": "Camille Chapdelaine", "authors": "Cyril Cappi, Camille Chapdelaine, Laurent Gardes, Eric Jenn, Baptiste\n  Lefevre, Sylvaine Picard, Thomas Soumarmon", "title": "Dataset Definition Standard (DDS)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document gives a set of recommendations to build and manipulate the\ndatasets used to develop and/or validate machine learning models such as deep\nneural networks. This document is one of the 3 documents defined in [1] to\nensure the quality of datasets. This is a work in progress as good practices\nevolve along with our understanding of machine learning. The document is\ndivided into three main parts. Section 2 addresses the data collection\nactivity. Section 3 gives recommendations about the annotation process.\nFinally, Section 4 gives recommendations concerning the breakdown between\ntrain, validation, and test datasets. In each part, we first define the desired\nproperties at stake, then we explain the objectives targeted to meet the\nproperties, finally we state the recommendations to reach these objectives.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 10:11:03 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Cappi", "Cyril", ""], ["Chapdelaine", "Camille", ""], ["Gardes", "Laurent", ""], ["Jenn", "Eric", ""], ["Lefevre", "Baptiste", ""], ["Picard", "Sylvaine", ""], ["Soumarmon", "Thomas", ""]]}, {"id": "2101.03058", "submitter": "Marcin Przyby{\\l}ko", "authors": "Cristina Feier, Carsten Lutz, and Marcin Przyby{\\l}ko", "title": "Answer Counting under Guarded TGDs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the complexity of answer counting for ontology-mediated queries and\nfor querying under constraints, considering conjunctive queries and unions\nthereof (UCQs) as the query language and guarded TGDs as the ontology and\nconstraint language, respectively. Our main result is a classification\naccording to whether answer counting is fixed-parameter tractable (FPT),\nW[1]-equivalent, #W[1]-equivalent, #W[2]-hard, or #A[2]-equivalent, lifting a\nrecent classification for UCQs without ontologies and constraints due to Dell\net al. The classification pertains to various structural measures, namely\ntreewidth, contract treewidth, starsize, and linked matching number.\n  Our results rest on the assumption that the arity of relation symbols is\nbounded by a constant and, in the case of ontology-mediated querying, that all\nsymbols from the ontology and query can occur in the data (so-called full data\nschema).\n  We also study the meta-problems for the mentioned structural measures, that\nis, to decide whether a given ontology-mediated query or constraint-query\nspecification is equivalent to one for which the structural measure is bounded.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 15:23:51 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Feier", "Cristina", ""], ["Lutz", "Carsten", ""], ["Przyby\u0142ko", "Marcin", ""]]}, {"id": "2101.03298", "submitter": "Bolin Ding", "authors": "Shuyuan Yan, Bolin Ding, Wei Guo, Jingren Zhou, Zhewei Wei, Xiaowei\n  Jiang, and Sheng Xu", "title": "FlashP: An Analytical Pipeline for Real-time Forecasting of Time-Series\n  Relational Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Interactive response time is important in analytical pipelines for users to\nexplore a sufficient number of possibilities and make informed business\ndecisions. We consider a forecasting pipeline with large volumes of\nhigh-dimensional time series data. Real-time forecasting can be conducted in\ntwo steps. First, we specify the part of data to be focused on and the measure\nto be predicted by slicing, dicing, and aggregating the data. Second, a\nforecasting model is trained on the aggregated results to predict the trend of\nthe specified measure. While there are a number of forecasting models\navailable, the first step is the performance bottleneck. A natural idea is to\nutilize sampling to obtain approximate aggregations in real time as the input\nto train the forecasting model. Our scalable real-time forecasting system\nFlashP (Flash Prediction) is built based on this idea, with two major\nchallenges to be resolved in this paper: first, we need to figure out how\napproximate aggregations affect the fitting of forecasting models, and\nforecasting results; and second, accordingly, what sampling algorithms we\nshould use to obtain these approximate aggregations and how large the samples\nare. We introduce a new sampling scheme, called GSW sampling, and analyze error\nbounds for estimating aggregations using GSW samples. We introduce how to\nconstruct compact GSW samples with the existence of multiple measures to be\nanalyzed. We conduct experiments to evaluate our solution and compare it with\nalternatives on real data.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 06:23:13 GMT"}, {"version": "v2", "created": "Sat, 16 Jan 2021 00:46:04 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Yan", "Shuyuan", ""], ["Ding", "Bolin", ""], ["Guo", "Wei", ""], ["Zhou", "Jingren", ""], ["Wei", "Zhewei", ""], ["Jiang", "Xiaowei", ""], ["Xu", "Sheng", ""]]}, {"id": "2101.03712", "submitter": "Shaleen Deep", "authors": "Shaleen Deep, Xiao Hu, Paraschos Koutris", "title": "Enumeration Algorithms for Conjunctive Queries with Projection", "comments": "To appear in proceedings of ICDT 2021. Comments are welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate the enumeration of query results for an important subset of\nCQs with projections, namely star and path queries. The task is to design data\nstructures and algorithms that allow for efficient enumeration with delay\nguarantees after a preprocessing phase. Our main contribution is a series of\nresults based on the idea of interleaving precomputed output with further join\nprocessing to maintain delay guarantees, which maybe of independent interest.\nIn particular, for star queries, we design combinatorial algorithms that\nprovide instance-specific delay guarantees in linear preprocessing time. These\nalgorithms improve upon the currently best known results. Further, we show how\nexisting results can be improved upon by using fast matrix multiplication. We\nalso present new results involving tradeoff between preprocessing time and\ndelay guarantees for enumeration of path queries that contain projections. CQs\nwith projection where the join attribute is projected away is equivalent to\nboolean matrix multiplication. Our results can therefore also be interpreted as\nsparse, output-sensitive matrix multiplication with delay guarantees.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 05:49:49 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 06:28:39 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Deep", "Shaleen", ""], ["Hu", "Xiao", ""], ["Koutris", "Paraschos", ""]]}, {"id": "2101.04102", "submitter": "Wilmer Ricciotti", "authors": "Wilmer Ricciotti and James Cheney", "title": "Query Lifting: Language-integrated query for heterogeneous nested\n  collections", "comments": "Full version of ESOP 2021 conference paper", "journal-ref": null, "doi": "10.1007/978-3-030-72019-3_21", "report-no": null, "categories": "cs.PL cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Language-integrated query based on comprehension syntax is a powerful\ntechnique for safe database programming, and provides a basis for advanced\ntechniques such as query shredding or query flattening that allow efficient\nprogramming with complex nested collections. However, the foundations of these\ntechniques are lacking: although SQL, the most widely-used database query\nlanguage, supports heterogeneous queries that mix set and multiset semantics,\nthese important capabilities are not supported by known correctness results or\nimplementations that assume homogeneous collections. In this paper we study\nlanguage-integrated query for a heterogeneous query language\n$NRC_\\lambda(Set,Bag)$ that combines set and multiset constructs. We show how\nto normalize and translate queries to SQL, and develop a novel approach to\nquerying heterogeneous nested collections, based on the insight that ``local''\nquery subexpressions that calculate nested subcollections can be ``lifted'' to\nthe top level analogously to lambda-lifting for local function definitions.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 18:48:26 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 14:55:34 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Ricciotti", "Wilmer", ""], ["Cheney", "James", ""]]}, {"id": "2101.04226", "submitter": "Arif Usta", "authors": "Arif Usta, Akifhan Karakayali and \\\"Ozg\\\"ur Ulusoy", "title": "DBTagger: Multi-Task Learning for Keyword Mapping in NLIDBs Using\n  Bi-Directional Recurrent Neural Networks", "comments": "To appear in VLDB 2021", "journal-ref": null, "doi": "10.14778/3446095.3446103", "report-no": null, "categories": "cs.DB cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Translating Natural Language Queries (NLQs) to Structured Query Language\n(SQL) in interfaces deployed in relational databases is a challenging task,\nwhich has been widely studied in database community recently. Conventional rule\nbased systems utilize series of solutions as a pipeline to deal with each step\nof this task, namely stop word filtering, tokenization, stemming/lemmatization,\nparsing, tagging, and translation. Recent works have mostly focused on the\ntranslation step overlooking the earlier steps by using ad-hoc solutions. In\nthe pipeline, one of the most critical and challenging problems is keyword\nmapping; constructing a mapping between tokens in the query and relational\ndatabase elements (tables, attributes, values, etc.). We define the keyword\nmapping problem as a sequence tagging problem, and propose a novel deep\nlearning based supervised approach that utilizes POS tags of NLQs. Our proposed\napproach, called \\textit{DBTagger} (DataBase Tagger), is an end-to-end and\nschema independent solution, which makes it practical for various relational\ndatabases. We evaluate our approach on eight different datasets, and report new\nstate-of-the-art accuracy results, $92.4\\%$ on the average. Our results also\nindicate that DBTagger is faster than its counterparts up to $10000$ times and\nscalable for bigger databases.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 22:54:39 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Usta", "Arif", ""], ["Karakayali", "Akifhan", ""], ["Ulusoy", "\u00d6zg\u00fcr", ""]]}, {"id": "2101.04432", "submitter": "Tanja Auge", "authors": "Tanja Auge and Nic Scharlau and Andreas Heuer", "title": "Privacy Aspects of Provenance Queries", "comments": "Accepted at ProvenanceWeek 2020 (\n  https://iitdbgroup.github.io/ProvenanceWeek2020/ )", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given a query result of a big database, why-provenance can be used to\ncalculate the necessary part of this database, consisting of so-called\nwitnesses. If this database consists of personal data, privacy protection has\nto prevent the publication of these witnesses. This implies a natural conflict\nof interest between publishing original data (provenance) and protecting these\ndata (privacy).\n  In this paper, privacy goes beyond the concept of personal data protection.\nThe paper gives an extended definition of privacy as intellectual property\nprotection. If the provenance information is not sufficient to reconstruct a\nquery result, additional data such as witnesses or provenance polynomials have\nto be published to guarantee traceability. Nevertheless, publishing this\nprovenance information might be a problem if (significantly) more tuples than\nnecessary can be derived from the original database. At this point, it is\nalready possible to violate privacy policies, provided that quasi identifiers\nare included in this provenance information. With this poster, we point out\nfundamental problems and discuss first proposals for solutions.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 12:05:30 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Auge", "Tanja", ""], ["Scharlau", "Nic", ""], ["Heuer", "Andreas", ""]]}, {"id": "2101.04964", "submitter": "Parimarjan Negi", "authors": "Parimarjan Negi, Ryan Marcus, Andreas Kipf, Hongzi Mao, Nesime Tatbul,\n  Tim Kraska, Mohammad Alizadeh", "title": "Flow-Loss: Learning Cardinality Estimates That Matter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous approaches to learned cardinality estimation have focused on\nimproving average estimation error, but not all estimates matter equally. Since\nlearned models inevitably make mistakes, the goal should be to improve the\nestimates that make the biggest difference to an optimizer. We introduce a new\nloss function, Flow-Loss, that explicitly optimizes for better query plans by\napproximating the optimizer's cost model and dynamic programming search\nalgorithm with analytical functions. At the heart of Flow-Loss is a reduction\nof query optimization to a flow routing problem on a certain plan graph in\nwhich paths correspond to different query plans. To evaluate our approach, we\nintroduce the Cardinality Estimation Benchmark, which contains the ground truth\ncardinalities for sub-plans of over 16K queries from 21 templates with up to 15\njoins. We show that across different architectures and databases, a model\ntrained with Flow-Loss improves the cost of plans (using the PostgreSQL cost\nmodel) and query runtimes despite having worse estimation accuracy than a model\ntrained with Q-Error. When the test set queries closely match the training\nqueries, both models improve performance significantly over PostgreSQL and are\nclose to the optimal performance (using true cardinalities). However, the\nQ-Error trained model degrades significantly when evaluated on queries that are\nslightly different (e.g., similar but not identical query templates), while the\nFlow-Loss trained model generalizes better to such situations. For example, the\nFlow-Loss model achieves up to 1.5x better runtimes on unseen templates\ncompared to the Q-Error model, despite leveraging the same model architecture\nand training data.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 09:50:32 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Negi", "Parimarjan", ""], ["Marcus", "Ryan", ""], ["Kipf", "Andreas", ""], ["Mao", "Hongzi", ""], ["Tatbul", "Nesime", ""], ["Kraska", "Tim", ""], ["Alizadeh", "Mohammad", ""]]}, {"id": "2101.05037", "submitter": "Maximilian Ernst Tschuchnig", "authors": "Maximilian Ernst Tschuchnig and Dejan Radovanovic and Eduard Hirsch\n  and Anna-Maria Oberluggauer and Georg Sch\\\"afer", "title": "Immutable and Democratic Data in permissionless Peer-to-Peer Systems", "comments": null, "journal-ref": null, "doi": "10.1109/SDS.2019.8768645", "report-no": null, "categories": "cs.DB cs.CR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Conventional data storage methods like SQL and NoSQL offer a huge amount of\npossibilities with one major disadvantage, having to use a centralized\nauthority. This authority may be in the form of a centralized or decentralized\nmaster server or a permissioned peer-to-peer setting. This paper looks at\ndifferent technologies on how to persist data without using a central\nauthority, mainly looking at permissionless peer-to-peer networks, primarily\nDistributed Ledger Technologies (DLTs) and a combination of DLTs with\nconventional databases. Afterwards it is shown how a system like this might be\nimplemented in two prototypes which are then evaluated against conventional\ndatabases.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 12:56:25 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Tschuchnig", "Maximilian Ernst", ""], ["Radovanovic", "Dejan", ""], ["Hirsch", "Eduard", ""], ["Oberluggauer", "Anna-Maria", ""], ["Sch\u00e4fer", "Georg", ""]]}, {"id": "2101.05308", "submitter": "Adel Ardalan", "authors": "Adel Ardalan, Derek Paulsen, Amanpreet Singh Saini, Walter Cai, AnHai\n  Doan", "title": "Toward Data Cleaning with a Target Accuracy: A Case Study for Value\n  Normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications need to clean data with a target accuracy. As far as we\nknow, this problem has not been studied in depth. In this paper we take the\nfirst step toward solving it. We focus on value normalization (VN), the problem\nof replacing all string that refer to the same entity with a unique string. VN\nis ubiquitous, and we often want to do VN with 100% accuracy. This is typically\ndone today in industry by automatically clustering the strings then asking a\nuser to verify and clean the clusters, until reaching 100% accuracy. This\nsolution has significant limitations. It does not tell the users how to verify\nand clean the clusters. This part also often takes a lot of time, e.g., days.\nFurther, there is no effective way for multiple users to collaboratively verify\nand clean. In this paper we address these challenges. Overall, our work\nadvances the state of the art in data cleaning by introducing a novel cleaning\nproblem and describing a promising solution template.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 19:12:05 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Ardalan", "Adel", ""], ["Paulsen", "Derek", ""], ["Saini", "Amanpreet Singh", ""], ["Cai", "Walter", ""], ["Doan", "AnHai", ""]]}, {"id": "2101.06126", "submitter": "Daniel Obraczka", "authors": "Daniel Obraczka, Jonathan Schuchart, Erhard Rahm", "title": "EAGER: Embedding-Assisted Entity Resolution for Knowledge Graphs", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Entity Resolution (ER) is a constitutional part for integrating different\nknowledge graphs in order to identify entities referring to the same real-world\nobject. A promising approach is the use of graph embeddings for ER in order to\ndetermine the similarity of entities based on the similarity of their graph\nneighborhood. The similarity computations for such embeddings translates to\ncalculating the distance between them in the embedding space which is\ncomparatively simple. However, previous work has shown that the use of graph\nembeddings alone is not sufficient to achieve high ER quality. We therefore\npropose a more comprehensive ER approach for knowledge graphs called EAGER\n(Embedding-Assisted Knowledge Graph Entity Resolution) to flexibly utilize both\nthe similarity of graph embeddings and attribute values within a supervised\nmachine learning approach. We evaluate our approach on 23 benchmark datasets\nwith differently sized and structured knowledge graphs and use hypothesis tests\nto ensure statistical significance of our results. Furthermore we compare our\napproach with state-of-the-art ER solutions, where our approach yields\ncompetitive results for table-oriented ER problems and shallow knowledge graphs\nbut much better results for deeper knowledge graphs.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 14:12:10 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Obraczka", "Daniel", ""], ["Schuchart", "Jonathan", ""], ["Rahm", "Erhard", ""]]}, {"id": "2101.06240", "submitter": "Polly Fahey", "authors": "Isolde Adler and Polly Fahey", "title": "Towards Approximate Query Enumeration with Sublinear Preprocessing Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims at providing extremely efficient algorithms for approximate\nquery enumeration on sparse databases, that come with performance and accuracy\nguarantees. We introduce a new model for approximate query enumeration on\nclasses of relational databases of bounded degree. We first prove that on\ndatabases of bounded degree any local first-order definable query can be\nenumerated approximately with constant delay after a constant time\npreprocessing phase. We extend this, showing that on databases of bounded\ntree-width and bounded degree, every query that is expressible in first-order\nlogic can be enumerated approximately with constant delay after a sublinear\n(more precisely, polylogarithmic) time preprocessing phase.\n  Durand and Grandjean (ACM Transactions on Computational Logic 2007) proved\nthat exact enumeration of first-order queries on databases of bounded degree\ncan be done with constant delay after a linear time preprocessing phase. Hence\nwe achieve a significant speed-up in the preprocessing phase. Since sublinear\nrunning time does not allow reading the whole input database even once,\nsacrificing some accuracy is inevitable for our speed-up. Nevertheless, our\nenumeration algorithms come with guarantees: With high probability, (1) only\ntuples are enumerated that are answers to the query or `close' to being answers\nto the query, and (2) if the proportion of tuples that are answers to the query\nis sufficiently large, then all answers will be enumerated. Here the notion of\n`closeness' is a tuple edit distance in the input database. For local\nfirst-order queries, only actual answers are enumerated, strengthening (1).\nMoreover, both the `closeness' and the proportion required in (2) are\ncontrollable.\n  We combine methods from property testing of bounded degree graphs with logic\nand query enumeration, which we believe can inspire further research.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 17:55:22 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Adler", "Isolde", ""], ["Fahey", "Polly", ""]]}, {"id": "2101.06637", "submitter": "Rabia Azzi", "authors": "Rabia Azzi and Gayo Diallo", "title": "AMALGAM: A Matching Approach to fairfy tabuLar data with knowledGe grAph\n  Model", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we present AMALGAM, a matching approach to fairify tabular data\nwith the use of a knowledge graph. The ultimate goal is to provide fast and\nefficient approach to annotate tabular data with entities from a background\nknowledge. The approach combines lookup and filtering services combined with\ntext pre-processing techniques. Experiments conducted in the context of the\n2020 Semantic Web Challenge on Tabular Data to Knowledge Graph Matching with\nboth Column Type Annotation and Cell Type Annotation tasks showed promising\nresults.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 10:17:06 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Azzi", "Rabia", ""], ["Diallo", "Gayo", ""]]}, {"id": "2101.06758", "submitter": "Massimo Cafaro", "authors": "Massimo Cafaro, Catiuscia Melle, Italo Epicoco, Marco Pulimeno", "title": "Data stream fusion for accurate quantile tracking and analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  UDDSKETCH is a recent algorithm for accurate tracking of quantiles in data\nstreams, derived from the DDSKETCH algorithm. UDDSKETCH provides accuracy\nguarantees covering the full range of quantiles independently of the input\ndistribution and greatly improves the accuracy with regard to DDSKETCH. In this\npaper we show how to compress and fuse data streams (or datasets) by using\nUDDSKETCH data summaries that are fused into a new summary related to the union\nof the streams (or datasets) processed by the input summaries whilst preserving\nboth the error and size guarantees provided by UDDSKETCH. This property of\nsketches, known as mergeability, enables parallel and distributed processing.\nWe prove that UDDSKETCH is fully mergeable and introduce a parallel version of\nUDDSKETCH suitable for message-passing based architectures. We formally prove\nits correctness and compare it to a parallel version of DDSKETCH, showing\nthrough extensive experimental results that our parallel algorithm almost\nalways outperforms the parallel DDSKETCH algorithm with regard to the overall\naccuracy in determining the quantiles.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 19:30:00 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Cafaro", "Massimo", ""], ["Melle", "Catiuscia", ""], ["Epicoco", "Italo", ""], ["Pulimeno", "Marco", ""]]}, {"id": "2101.06761", "submitter": "Peng Gao", "authors": "Peng Gao, Fei Shao, Xiaoyuan Liu, Xusheng Xiao, Haoyuan Liu, Zheng\n  Qin, Fengyuan Xu, Prateek Mittal, Sanjeev R. Kulkarni, Dawn Song", "title": "A System for Efficiently Hunting for Cyber Threats in Computer Systems\n  Using Threat Intelligence", "comments": "Accepted paper at ICDE 2021 demonstrations track. arXiv admin note:\n  substantial text overlap with arXiv:2010.13637", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Log-based cyber threat hunting has emerged as an important solution to\ncounter sophisticated cyber attacks. However, existing approaches require\nnon-trivial efforts of manual query construction and have overlooked the rich\nexternal knowledge about threat behaviors provided by open-source Cyber Threat\nIntelligence (OSCTI). To bridge the gap, we build ThreatRaptor, a system that\nfacilitates cyber threat hunting in computer systems using OSCTI. Built upon\nmature system auditing frameworks, ThreatRaptor provides (1) an unsupervised,\nlight-weight, and accurate NLP pipeline that extracts structured threat\nbehaviors from unstructured OSCTI text, (2) a concise and expressive\ndomain-specific query language, TBQL, to hunt for malicious system activities,\n(3) a query synthesis mechanism that automatically synthesizes a TBQL query\nfrom the extracted threat behaviors, and (4) an efficient query execution\nengine to search the big system audit logging data.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 19:44:09 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 06:39:48 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Gao", "Peng", ""], ["Shao", "Fei", ""], ["Liu", "Xiaoyuan", ""], ["Xiao", "Xusheng", ""], ["Liu", "Haoyuan", ""], ["Qin", "Zheng", ""], ["Xu", "Fengyuan", ""], ["Mittal", "Prateek", ""], ["Kulkarni", "Sanjeev R.", ""], ["Song", "Dawn", ""]]}, {"id": "2101.06801", "submitter": "Hemant Saxena", "authors": "Hemant Saxena, Lukasz Golab, Stratos Idreos, Ihab F. Ilyas", "title": "Real-Time LSM-Trees for HTAP Workloads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time data analytics systems such as SAP HANA, MemSQL, and IBM Wildfire\nemploy hybrid data layouts, in which data are stored in different formats\nthroughout their lifecycle. Recent data are stored in a row-oriented format to\nserve OLTP workloads and support high data rates, while older data are\ntransformed to a column-oriented format for OLAP access patterns. We observe\nthat a Log-Structured Merge (LSM) Tree is a natural fit for a lifecycle-aware\nstorage engine due to its high write throughput and level-oriented structure,\nin which records propagate from one level to the next over time. To build a\nlifecycle-aware storage engine using an LSM-Tree, we make a crucial\nmodification to allow different data layouts in different levels, ranging from\npurely row-oriented to purely column-oriented, leading to a Real-Time LSM-Tree.\nWe give a cost model and an algorithm to design a Real-Time LSM-Tree that is\nsuitable for a given workload, followed by an experimental evaluation of LASER\n- a prototype implementation of our idea built on top of the RocksDB key-value\nstore. In our evaluation, LASER is almost 5x faster than Postgres (a pure\nrow-store) and two orders of magnitude faster than MonetDB (a pure\ncolumn-store) for real-time data analytics workloads.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 23:15:32 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Saxena", "Hemant", ""], ["Golab", "Lukasz", ""], ["Idreos", "Stratos", ""], ["Ilyas", "Ihab F.", ""]]}, {"id": "2101.07026", "submitter": "Masatoshi Hanai", "authors": "Masatoshi Hanai, Nikos Tziritas, Toyotaro Suzumura, Wentong Cai,\n  Georgios Theodoropoulos", "title": "Time-Efficient and High-Quality Graph Partitioning for Graph Dynamic\n  Scaling", "comments": "21 pages, 15 figures. Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.DM cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dynamic scaling of distributed computations plays an important role in\nthe utilization of elastic computational resources, such as the cloud. It\nenables the provisioning and de-provisioning of resources to match dynamic\nresource availability and demands. In the case of distributed graph processing,\nchanging the number of the graph partitions while maintaining high partitioning\nquality imposes serious computational overheads as typically a time-consuming\ngraph partitioning algorithm needs to execute each time repartitioning is\nrequired. In this paper, we propose a dynamic scaling method that can\nefficiently change the number of graph partitions while keeping its quality\nhigh. Our idea is based on two techniques: preprocessing and very fast edge\npartitioning, called graph edge ordering and chunk-based edge partitioning,\nrespectively. The former converts the graph data into an ordered edge list in\nsuch a way that edges with high locality are closer to each other. The latter\nimmediately divides the ordered edge list into an arbitrary number of\nhigh-quality partitions. The evaluation with the real-world billion-scale\ngraphs demonstrates that our proposed approach significantly reduces the\nrepartitioning time, while the partitioning quality it achieves is on par with\nthat of the best existing static method.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 12:06:00 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Hanai", "Masatoshi", ""], ["Tziritas", "Nikos", ""], ["Suzumura", "Toyotaro", ""], ["Cai", "Wentong", ""], ["Theodoropoulos", "Georgios", ""]]}, {"id": "2101.07136", "submitter": "Philipp D. Rohde", "authors": "M\\'onica Figuera and Philipp D. Rohde and Maria-Esther Vidal", "title": "Trav-SHACL: Efficiently Validating Networks of SHACL Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graphs have emerged as expressive data structures for Web data.\nKnowledge graph potential and the demand for ecosystems to facilitate their\ncreation, curation, and understanding, is testified in diverse domains, e.g.,\nbiomedicine. The Shapes Constraint Language (SHACL) is the W3C recommendation\nlanguage for integrity constraints over RDF knowledge graphs. Enabling quality\nassements of knowledge graphs, SHACL is rapidly gaining attention in real-world\nscenarios. SHACL models integrity constraints as a network of shapes, where a\nshape contains the constraints to be fullfiled by the same entities. The\nvalidation of a SHACL shape schema can face the issue of tractability during\nvalidation. To facilitate full adoption, efficient computational methods are\nrequired. We present Trav-SHACL, a SHACL engine capable of planning the\ntraversal and execution of a shape schema in a way that invalid entities are\ndetected early and needless validations are minimized. Trav-SHACL reorders the\nshapes in a shape schema for efficient validation and rewrites target and\nconstraint queries for the fast detection of invalid entities. Trav-SHACL is\nempirically evaluated on 27 testbeds executed against knowledge graphs of up to\n34M triples. Our experimental results suggest that Trav-SHACL exhibits high\nperformance gradually and reduces validation time by a factor of up to 28.93\ncompared to the state of the art.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 15:57:14 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Figuera", "M\u00f3nica", ""], ["Rohde", "Philipp D.", ""], ["Vidal", "Maria-Esther", ""]]}, {"id": "2101.07361", "submitter": "Maliha Islam", "authors": "Maliha Tashfia Islam, Anna Fariha, Alexandra Meliou", "title": "Through the Data Management Lens: Experimental Analysis and Evaluation\n  of Fair Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification, a heavily-studied data-driven machine learning task, drives\nan increasing number of prediction systems involving critical human decisions\nsuch as loan approval and criminal risk assessment. However, classifiers often\ndemonstrate discriminatory behavior, especially when presented with biased\ndata. Consequently, fairness in classification has emerged as a high-priority\nresearch area. Data management research is showing an increasing presence and\ninterest in topics related to data and algorithmic fairness, including the\ntopic of fair classification. The interdisciplinary efforts in fair\nclassification, with machine learning research having the largest presence,\nhave resulted in a large number of fairness notions and a wide range of\napproaches that have not been systematically evaluated and compared. In this\npaper, we contribute a broad analysis of 13 fair classification approaches and\nadditional variants, over their correctness, fairness, efficiency, scalability,\nand stability, using a variety of metrics and real-world datasets. Our analysis\nhighlights novel insights on the impact of different metrics and high-level\napproach characteristics on different aspects of performance. We also discuss\ngeneral principles for choosing approaches suitable for different practical\nsettings, and identify areas where data-management-centric solutions are likely\nto have the most impact.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 22:55:40 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Islam", "Maliha Tashfia", ""], ["Fariha", "Anna", ""], ["Meliou", "Alexandra", ""]]}, {"id": "2101.07622", "submitter": "Chang Sun", "authors": "Chang Sun", "title": "Knowledge Graph for Microdata of Statistics Netherlands", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Statistics Netherlands (CBS) hosted a huge amount of data not only on the\nstatistical level but also on the individual level. With the development of\ndata science technologies, more and more researchers request to conduct their\nresearch by using high-quality individual data from CBS (called CBS Microdata)\nor combining them with other data sources. Making great use of these data for\nresearch and scientific purposes can tremendously benefit the whole society.\nHowever, CBS Microdata has been collected and maintained in different ways by\ndifferent departments in and out of CBS. The representation, quality, metadata\nof datasets are not sufficiently harmonized. The project converts the\ndescriptions of all CBS microdata sets into one knowledge graph with\ncomprehensive metadata in Dutch and English using text mining and semantic web\ntechnologies. Researchers can easily query the metadata, explore the relations\namong multiple datasets, and find the needed variables. For example, if a\nresearcher searches a dataset about \"Age at Death\" in the Health and Well-being\ncategory, all information related to this dataset will appear including\nkeywords and variable names. \"Age at Death\" dataset has a keyword - \"Death\".\nThis keyword will lead to other datasets such as \"Date of Death\". \"Cause of\nDeath\", \"Production statistics Health and welfare\" from Population, Business\ncategories, and Health and well-being categories. This will tremendously save\ntime and costs for the data requester but also data maintainers.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 13:54:57 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Sun", "Chang", ""]]}, {"id": "2101.07690", "submitter": "Peng Jiang", "authors": "Peng Jiang, Rujia Wang, Bo Wu", "title": "Efficient Mining of Frequent Subgraphs with Two-Vertex Exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Frequent Subgraph Mining (FSM) is the key task in many graph mining and\nmachine learning applications. Numerous systems have been proposed for FSM in\nthe past decade. Although these systems show good performance for small\npatterns (with no more than four vertices), we found that they have difficulty\nin mining larger patterns. In this work, we propose a novel two-vertex\nexploration strategy to accelerate the mining process. Compared with the\nsingle-vertex exploration adopted by previous systems, our two-vertex\nexploration avoids the large memory consumption issue and significantly reduces\nthe memory access overhead. We further enhance the performance through an\nindex-based quick pattern technique that reduces the overhead of isomorphism\nchecks, and a subgraph sampling technique that mitigates the issue of subgraph\nexplosion. The experimental results show that our system achieves significant\nspeedups against the state-of-the-art graph pattern mining systems and supports\nlarger pattern mining tasks that none of the existing systems can handle.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 15:35:24 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 21:08:42 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Jiang", "Peng", ""], ["Wang", "Rujia", ""], ["Wu", "Bo", ""]]}, {"id": "2101.07731", "submitter": "Daniel Shen", "authors": "Daniel Shen, Min Chi", "title": "TC-DTW: Accelerating Multivariate Dynamic Time Warping Through Triangle\n  Inequality and Point Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": "North Carolina State University TR-2021-2", "categories": "cs.LG cs.AI cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dynamic time warping (DTW) plays an important role in analytics on time\nseries. Despite the large body of research on speeding up univariate DTW, the\nmethod for multivariate DTW has not been improved much in the last two decades.\nThe most popular algorithm used today is still the one developed seventeen\nyears ago. This paper presents a solution that, as far as we know, for the\nfirst time consistently outperforms the classic multivariate DTW algorithm\nacross dataset sizes, series lengths, data dimensions, temporal window sizes,\nand machines. The new solution, named TC-DTW, introduces Triangle Inequality\nand Point Clustering into the algorithm design on lower bound calculations for\nmultivariate DTW. In experiments on DTW-based nearest neighbor finding, the new\nsolution avoids as much as 98% (60% average) DTW distance calculations and\nyields as much as 25X (7.5X average) speedups.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 16:38:28 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 02:55:15 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Shen", "Daniel", ""], ["Chi", "Min", ""]]}, {"id": "2101.07769", "submitter": "Peng Gao", "authors": "Peng Gao, Xiaoyuan Liu, Edward Choi, Bhavna Soman, Chinmaya Mishra,\n  Kate Farris, Dawn Song", "title": "A System for Automated Open-Source Threat Intelligence Gathering and\n  Management", "comments": "Accepted paper at SIGMOD 2021 demonstrations track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.CL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To remain aware of the fast-evolving cyber threat landscape, open-source\nCyber Threat Intelligence (OSCTI) has received growing attention from the\ncommunity. Commonly, knowledge about threats is presented in a vast number of\nOSCTI reports. Despite the pressing need for high-quality OSCTI, existing OSCTI\ngathering and management platforms, however, have primarily focused on\nisolated, low-level Indicators of Compromise. On the other hand, higher-level\nconcepts (e.g., adversary tactics, techniques, and procedures) and their\nrelationships have been overlooked, which contain essential knowledge about\nthreat behaviors that is critical to uncovering the complete threat scenario.\nTo bridge the gap, we propose SecurityKG, a system for automated OSCTI\ngathering and management. SecurityKG collects OSCTI reports from various\nsources, uses a combination of AI and NLP techniques to extract high-fidelity\nknowledge about threat behaviors, and constructs a security knowledge graph.\nSecurityKG also provides a UI that supports various types of interactivity to\nfacilitate knowledge graph exploration.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 18:31:35 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 20:50:51 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Gao", "Peng", ""], ["Liu", "Xiaoyuan", ""], ["Choi", "Edward", ""], ["Soman", "Bhavna", ""], ["Mishra", "Chinmaya", ""], ["Farris", "Kate", ""], ["Song", "Dawn", ""]]}, {"id": "2101.08167", "submitter": "Khaled Zaouk", "authors": "Khaled Zaouk, Fei Song, Chenghao Lyu and Yanlei Diao", "title": "Neural-based Modeling for Performance Tuning of Spark Data Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud data analytics has become an integral part of enterprise business\noperations for data-driven insight discovery. Performance modeling of cloud\ndata analytics is crucial for performance tuning and other critical operations\nin the cloud. Traditional modeling techniques fail to adapt to the high degree\nof diversity in workloads and system behaviors in this domain. In this paper,\nwe bring recent Deep Learning techniques to bear on the process of automated\nperformance modeling of cloud data analytics, with a focus on Spark data\nanalytics as representative workloads. At the core of our work is the notion of\nlearning workload embeddings (with a set of desired properties) to represent\nfundamental computational characteristics of different jobs, which enable\nperformance prediction when used together with job configurations that control\nresource allocation and other system knobs. Our work provides an in-depth study\nof different modeling choices that suit our requirements. Results of extensive\nexperiments reveal the strengths and limitations of different modeling methods,\nas well as superior performance of our best performing method over a\nstate-of-the-art modeling tool for cloud analytics.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 14:58:55 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Zaouk", "Khaled", ""], ["Song", "Fei", ""], ["Lyu", "Chenghao", ""], ["Diao", "Yanlei", ""]]}, {"id": "2101.08358", "submitter": "Jason Mohoney", "authors": "Jason Mohoney, Roger Waleffe, Yiheng Xu, Theodoros Rekatsinas,\n  Shivaram Venkataraman", "title": "Marius: Learning Massive Graph Embeddings on a Single Machine", "comments": "Accepted into OSDI '21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework for computing the embeddings of large-scale graphs\non a single machine. A graph embedding is a fixed length vector representation\nfor each node (and/or edge-type) in a graph and has emerged as the de-facto\napproach to apply modern machine learning on graphs. We identify that current\nsystems for learning the embeddings of large-scale graphs are bottlenecked by\ndata movement, which results in poor resource utilization and inefficient\ntraining. These limitations require state-of-the-art systems to distribute\ntraining across multiple machines. We propose Marius, a system for efficient\ntraining of graph embeddings that leverages partition caching and buffer-aware\ndata orderings to minimize disk access and interleaves data movement with\ncomputation to maximize utilization. We compare Marius against two\nstate-of-the-art industrial systems on a diverse array of benchmarks. We\ndemonstrate that Marius achieves the same level of accuracy but is up to one\norder of magnitude faster. We also show that Marius can scale training to\ndatasets an order of magnitude beyond a single machine's GPU and CPU memory\ncapacity, enabling training of configurations with more than a billion edges\nand 550 GB of total parameters on a single machine with 16 GB of GPU memory and\n64 GB of CPU memory. Marius is open-sourced at www.marius-project.org.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 23:17:31 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 00:22:46 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Mohoney", "Jason", ""], ["Waleffe", "Roger", ""], ["Xu", "Yiheng", ""], ["Rekatsinas", "Theodoros", ""], ["Venkataraman", "Shivaram", ""]]}, {"id": "2101.08779", "submitter": "Ruilong Li", "authors": "Ruilong Li, Shan Yang, David A. Ross, Angjoo Kanazawa", "title": "Learn to Dance with AIST++: Music Conditioned 3D Dance Generation", "comments": "Project page: https://google.github.io/aichoreographer/; Dataset\n  page: https://google.github.io/aistplusplus_dataset/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB cs.GR cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a transformer-based learning framework for 3D dance\ngeneration conditioned on music. We carefully design our network architecture\nand empirically study the keys for obtaining qualitatively pleasing results.\nThe critical components include a deep cross-modal transformer, which well\nlearns the correlation between the music and dance motion; and the\nfull-attention with future-N supervision mechanism which is essential in\nproducing long-range non-freezing motion. In addition, we propose a new dataset\nof paired 3D motion and music called AIST++, which we reconstruct from the AIST\nmulti-view dance videos. This dataset contains 1.1M frames of 3D dance motion\nin 1408 sequences, covering 10 genres of dance choreographies and accompanied\nwith multi-view camera parameters. To our knowledge it is the largest dataset\nof this kind. Rich experiments on AIST++ demonstrate our method produces much\nbetter results than the state-of-the-art methods both qualitatively and\nquantitatively.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 18:59:22 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 05:23:59 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Li", "Ruilong", ""], ["Yang", "Shan", ""], ["Ross", "David A.", ""], ["Kanazawa", "Angjoo", ""]]}, {"id": "2101.08784", "submitter": "Claudio Silvestri", "authors": "Giacomo Chiarot, Claudio Silvestri", "title": "Time series compression: a survey", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The presence of smart objects is increasingly widespread and their ecosystem,\nalso known as Internet of Things, is relevant in many different application\nscenarios. The huge amount of temporally annotated data produced by these smart\ndevices demand for efficient techniques for transfer and storage of time series\ndata. Compression techniques play an important role toward this goal and,\ndespite the fact that standard compression methods could be used with some\nbenefit, there exist several ones that specifically address the case of time\nseries by exploiting their peculiarities to achieve a more effective\ncompression and a more accurate decompression in the case of lossy compression\ntechniques. This paper provides a state-of-the-art survey of the principal time\nseries compression techniques, proposing a taxonomy to classify them\nconsidering their overall approach and their characteristics. Furthermore, we\nanalyze the performances of the selected algorithms by discussing and comparing\nthe experimental results that where provided in the original articles. The goal\nof this paper is to provide a comprehensive and homogeneous reconstruction of\nthe state-of-the-art which is currently fragmented across many papers that use\ndifferent notations and where the proposed methods are not organized according\nto a classification.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 14:01:25 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Chiarot", "Giacomo", ""], ["Silvestri", "Claudio", ""]]}, {"id": "2101.08819", "submitter": "Mohammad Javad Amiri", "authors": "Mohammad Javad Amiri, Ziliang Lai, Liana Patel, Boon Thau Loo, Eric\n  Lo, Wenchao Zhou", "title": "Saguaro: Efficient Processing of Transactions in Wide Area Networks\n  using a Hierarchical Permissioned Blockchain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.NI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The next frontier for the Internet leading by innovations in mobile\ncomputing, in particular, 5G, together with blockchains' transparency,\nimmutability, provenance, and authenticity, indicates the potentials of running\na new generation of applications on the mobile internet. A 5G-enabled\nblockchain system is structured as a hierarchy and needs to deal with different\nchallenges such as maintaining blockchain ledger at different spatial domains\nand various levels of the networks, efficient processing of cross-domain\ntransactions, establishing consensus among heterogeneous nodes, and supporting\ndelay-tolerant mobile transactions. In this paper, we present Saguaro, a\nhierarchical permissioned blockchain designed specifically for Internet-scale\nmobile networks. Saguaro benefits from the hierarchical structure of mobile\nnetwork infrastructure to address the aforementioned challenges. Our extensive\nexperimental results demonstrate the high potential of Saguaro being the first\n5G-enabled permissioned blockchain system in the community.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 19:16:22 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Amiri", "Mohammad Javad", ""], ["Lai", "Ziliang", ""], ["Patel", "Liana", ""], ["Loo", "Boon Thau", ""], ["Lo", "Eric", ""], ["Zhou", "Wenchao", ""]]}, {"id": "2101.08929", "submitter": "Bolong Zheng", "authors": "Bolong Zheng, Lianggui Weng, Xi Zhao, Kai Zeng, Xiaofang Zhou,\n  Christian S. Jensen", "title": "REPOSE: Distributed Top-k Trajectory Similarity Search with Local\n  Reference Point Tries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trajectory similarity computation is a fundamental component in a variety of\nreal-world applications, such as ridesharing, road planning, and transportation\noptimization. Recent advances in mobile devices have enabled an unprecedented\nincrease in the amount of available trajectory data such that efficient query\nprocessing can no longer be supported by a single machine. As a result, means\nof performing distributed in-memory trajectory similarity search are called\nfor. However, existing distributed proposals suffer from either computing\nresource waste or are unable to support the range of similarity measures that\nare being used. We propose a distributed in-memory management framework called\nREPOSE for processing top-k trajectory similarity queries on Spark. We develop\na reference point trie (RP-Trie) index to organize trajectory data for local\nsearch. In addition, we design a novel heterogeneous global partitioning\nstrategy to eliminate load imbalance in distributed settings. We report on\nextensive experiments with real-world data that offer insight into the\nperformance of the solution, and show that the solution is capable of\noutperforming the state-of-the-art proposals.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 03:27:21 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 09:49:11 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Zheng", "Bolong", ""], ["Weng", "Lianggui", ""], ["Zhao", "Xi", ""], ["Zeng", "Kai", ""], ["Zhou", "Xiaofang", ""], ["Jensen", "Christian S.", ""]]}, {"id": "2101.09094", "submitter": "Kangfei Zhao", "authors": "Kangfei Zhao, Jeffrey Xu Yu, Yu Rong, Ming Liao, Junzhou Huang", "title": "Towards Expectation-Maximization by SQL in RDBMS", "comments": "12 pages", "journal-ref": "Long version of DASFAA 2021", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrating machine learning techniques into RDBMSs is an important task\nsince there are many real applications that require modeling (e.g., business\nintelligence, strategic analysis) as well as querying data in RDBMSs. In this\npaper, we provide an SQL solution that has the potential to support different\nmachine learning modelings. As an example, we study how to support unsupervised\nprobabilistic modeling, that has a wide range of applications in clustering,\ndensity estimation and data summarization, and focus on\nExpectation-Maximization (EM) algorithms, which is a general technique for\nfinding maximum likelihood estimators. To train a model by EM, it needs to\nupdate the model parameters by an E-step and an M-step in a while-loop\niteratively until it converges to a level controled by some threshold or\nrepeats a certain number of iterations. To support EM in RDBMSs, we show our\nanswers to the matrix/vectors representations in RDBMSs, the relational algebra\noperations to support the linear algebra operations required by EM, parameters\nupdate by relational algebra, and the support of a while-loop. It is important\nto note that the SQL'99 recursion cannot be used to handle such a while-loop\nsince the M-step is non-monotonic. In addition, assume that a model has been\ntrained by an EM algorithm, we further design an automatic in-database model\nmaintenance mechanism to maintain the model when the underlying training data\nchanges.We have conducted experimental studies and will report our findings in\nthis paper.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 13:17:24 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Zhao", "Kangfei", ""], ["Yu", "Jeffrey Xu", ""], ["Rong", "Yu", ""], ["Liao", "Ming", ""], ["Huang", "Junzhou", ""]]}, {"id": "2101.09441", "submitter": "Qiuyi Lyu", "authors": "Qiuyi Lyu, Yuchen Li, Bingsheng He, Bin Gong", "title": "DBL: Efficient Reachability Queries on Dynamic Graphs (Complete Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reachability query is a fundamental problem on graphs, which has been\nextensively studied in academia and industry. Since graphs are subject to\nfrequent updates in many applications, it is essential to support efficient\ngraph updates while offering good performance in reachability queries. Existing\nsolutions compress the original graph with the Directed Acyclic Graph (DAG) and\npropose efficient query processing and index update techniques. However, they\nfocus on optimizing the scenarios where the Strong Connected Components(SCCs)\nremain unchanged and have overlooked the prohibitively high cost of the DAG\nmaintenance when SCCs are updated. In this paper, we propose DBL, an efficient\nDAG-free index to support the reachability query on dynamic graphs with\ninsertion-only updates. DBL builds on two complementary indexes: Dynamic\nLandmark (DL) label and Bidirectional Leaf (BL) label. The former leverages\nlandmark nodes to quickly determine reachable pairs whereas the latter prunes\nunreachable pairs by indexing the leaf nodes in the graph. We evaluate DBL\nagainst the state-of-the-art approaches on dynamic reachability index with\nextensive experiments on real-world datasets. The results have demonstrated\nthat DBL achieves orders of magnitude speedup in terms of index update, while\nstill producing competitive query efficiency.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 07:22:38 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 05:18:22 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Lyu", "Qiuyi", ""], ["Li", "Yuchen", ""], ["He", "Bingsheng", ""], ["Gong", "Bin", ""]]}, {"id": "2101.09668", "submitter": "Fangda Guo", "authors": "Fangda Guo, Ye Yuan, Guoren Wang, Xiangguo Zhao and Hao Sun", "title": "Multi-attributed Community Search in Road-social Networks", "comments": null, "journal-ref": null, "doi": "10.1109/ICDE51399.2021.00017", "report-no": null, "categories": "cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a location-based social network, how to find the communities that are\nhighly relevant to query users and have top overall scores in multiple\nattributes according to user preferences? Typically, in the face of such a\nproblem setting, we can model the network as a multi-attributed road-social\nnetwork, in which each user is linked with location information and $d$\n($\\geq\\! 1$) numerical attributes. In practice, user preferences (i.e.,\nweights) are usually inherently uncertain and can only be estimated with\nbounded accuracy, because a human user is not able to designate exact values\nwith absolute precision. Inspired by this, we introduce a normative community\nmodel suitable for multi-criteria decision making, called multi-attributed\ncommunity (MAC), based on the concepts of $k$-core and a novel dominance\nrelationship specific to preferences. Given uncertain user preferences, namely,\nan approximate representation of weights, the MAC search reports the exact\ncommunities for each of the possible weight settings. We devise an elegant\nindex structure to maintain the dominance relationships, based on which two\nalgorithms are developed to efficiently compute the top-$j$ MACs. The\nefficiency and scalability of our algorithms and the effectiveness of MAC model\nare demonstrated by extensive experiments on both real-world and synthetic\nroad-social networks.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 07:47:11 GMT"}, {"version": "v2", "created": "Sun, 21 Feb 2021 05:36:57 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Guo", "Fangda", ""], ["Yuan", "Ye", ""], ["Wang", "Guoren", ""], ["Zhao", "Xiangguo", ""], ["Sun", "Hao", ""]]}, {"id": "2101.10231", "submitter": "David Daly", "authors": "David Daly", "title": "Creating a Virtuous Cycle in Performance Testing at MongoDB", "comments": "Author's copy and preprint. Accepted for publication at ICPE2021. 9\n  pages, 5 figures", "journal-ref": null, "doi": "10.1145/3427921.3450234", "report-no": null, "categories": "cs.SE cs.DB cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is important to detect changes in software performance during development\nin order to avoid performance decreasing release to release or dealing with\ncostly delays at release time. Performance testing is part of the development\nprocess at MongoDB, and integrated into our continuous integration system. We\ndescribe a set of changes to that performance testing environment designed to\nimprove testing effectiveness. These changes help improve coverage, provide\nfaster and more accurate signaling for performance changes, and help us better\nunderstand the state of performance. In addition to each component performing\nbetter, we believe that we have created and exploited a virtuous cycle:\nperformance test improvements drive impact, which drives more use, which drives\nfurther impact and investment in improvements. Overall, MongoDB is getting\nfaster and we avoid shipping major performance regressions to our customers\nbecause of this infrastructure.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 16:47:05 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2021 02:14:37 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Daly", "David", ""]]}, {"id": "2101.10457", "submitter": "Ali Hadian", "authors": "Ali Hadian, Thomas Heinis", "title": "Shift-Table: A Low-latency Learned Index for Range Queries using Model\n  Correction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indexing large-scale databases in main memory is still challenging today.\nLearned index structures -- in which the core components of classical indexes\nare replaced with machine learning models -- have recently been suggested to\nsignificantly improve performance for read-only range queries. However, a\nrecent benchmark study shows that learned indexes only achieve limited\nperformance improvements for real-world data on modern hardware. More\nspecifically, a learned model cannot learn the micro-level details and\nfluctuations of data distributions thus resulting in poor accuracy; or it can\nfit to the data distribution at the cost of training a big model whose\nparameters cannot fit into cache. As a consequence, querying a learned index on\nreal-world data takes a substantial number of memory lookups, thereby degrading\nperformance. In this paper, we adopt a different approach for modeling a data\ndistribution that complements the model fitting approach of learned indexes. We\npropose Shift-Table, an algorithmic layer that captures the micro-level data\ndistribution and resolves the local biases of a learned model at the cost of at\nmost one memory lookup. Our suggested model combines the low latency of lookup\ntables with learned indexes and enables low-latency processing of range\nqueries. Using Shift-Table, we achieve a speedup of 1.5X to 2X on real-world\ndatasets compared to trained and tuned learned indexes.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 22:17:36 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Hadian", "Ali", ""], ["Heinis", "Thomas", ""]]}, {"id": "2101.10699", "submitter": "Chao Li", "authors": "Qinwei Lin, Chao Li, Xifeng Zhao and Xianhai Chen", "title": "Measuring Decentralization in Bitcoin and Ethereum using Multiple\n  Metrics and Granularities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decentralization has been widely acknowledged as a core virtue of\nblockchains. However, in the past, there have been few measurement studies on\nmeasuring and comparing the actual level of decentralization between existing\nblockchains using multiple metrics and granularities. This paper presents a new\ncomparison study of the degree of decentralization in Bitcoin and Ethereum, the\ntwo most prominent blockchains, with various decentralization metrics and\ndifferent granularities within the time dimension. Specifically, we measure the\ndegree of decentralization in the two blockchains during 2019 by computing the\ndistribution of mining power with three metrics (Gini coefficient, Shannon\nentropy, and Nakamoto coefficient) as well as three granularities (days, weeks,\nand months). Our measurement results with different metrics and granularities\nreveal the same trend that, compared with each other, the degree of\ndecentralization in Bitcoin is higher, while the degree of decentralization in\nEthereum is more stable. To obtain the cross-interval information missed in the\nfixed window based measurements, we propose the sliding window based\nmeasurement approach. The corresponding results demonstrate that the use of\nsliding windows could reveal additional cross-interval information overlooked\nby the fixed window based measurements, thus enhancing the effectiveness of\nmeasuring decentralization in terms of continuous trends and abnormal\nsituations. We believe that the methodologies and findings in this paper can\nfacilitate future studies of decentralization in blockchains.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 10:45:06 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 03:56:56 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Lin", "Qinwei", ""], ["Li", "Chao", ""], ["Zhao", "Xifeng", ""], ["Chen", "Xianhai", ""]]}, {"id": "2101.10905", "submitter": "Francesco Silvestri", "authors": "Martin Aum\\\"uller, Sariel Har-Peled, Sepideh Mahabadi, Rasmus Pagh,\n  Francesco Silvestri", "title": "Sampling a Near Neighbor in High Dimensions -- Who is the Fairest of\n  Them All?", "comments": "arXiv admin note: text overlap with arXiv:1906.02640", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity search is a fundamental algorithmic primitive, widely used in many\ncomputer science disciplines. Given a set of points $S$ and a radius parameter\n$r>0$, the $r$-near neighbor ($r$-NN) problem asks for a data structure that,\ngiven any query point $q$, returns a point $p$ within distance at most $r$ from\n$q$. In this paper, we study the $r$-NN problem in the light of individual\nfairness and providing equal opportunities: all points that are within distance\n$r$ from the query should have the same probability to be returned. In the\nlow-dimensional case, this problem was first studied by Hu, Qiao, and Tao (PODS\n2014). Locality sensitive hashing (LSH), the theoretically strongest approach\nto similarity search in high dimensions, does not provide such a fairness\nguarantee. In this work, we show that LSH based algorithms can be made fair,\nwithout a significant loss in efficiency. We propose several efficient data\nstructures for the exact and approximate variants of the fair NN problem. Our\napproach works more generally for sampling uniformly from a sub-collection of\nsets of a given collection and can be used in a few other applications. We also\ndevelop a data structure for fair similarity search under inner product that\nrequires nearly-linear space and exploits locality sensitive filters. The paper\nconcludes with an experimental evaluation that highlights the inherent\nunfairness of NN data structures and shows the performance of our algorithms on\nreal-world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 16:13:07 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Aum\u00fcller", "Martin", ""], ["Har-Peled", "Sariel", ""], ["Mahabadi", "Sepideh", ""], ["Pagh", "Rasmus", ""], ["Silvestri", "Francesco", ""]]}, {"id": "2101.11259", "submitter": "Donatella Firmani", "authors": "Valter Crescenzi, Andrea De Angelis, Donatella Firmani, Maurizio\n  Mazzei, Paolo Merialdo, Federico Piai, Divesh Srivastava", "title": "Alaska: A Flexible Benchmark for Data Integration Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data integration is a long-standing interest of the data management community\nand has many disparate applications, including business, science and\ngovernment. We have recently witnessed impressive results in specific data\nintegration tasks, such as Entity Resolution, thanks to the increasing\navailability of benchmarks. A limitation of such benchmarks is that they\ntypically come with their own task definition and it can be difficult to\nleverage them for complex integration pipelines. As a result, evaluating\nend-to-end pipelines for the entire data integration process is still an\nelusive goal. In this work, we present Alaska, the first benchmark based on\nreal-world dataset to support seamlessly multiple tasks (and their variants) of\nthe data integration pipeline. The dataset consists of ~70k heterogeneous\nproduct specifications from 71 e-commerce websites with thousands of different\nproduct attributes. Our benchmark comes with profiling meta-data, a set of\npre-defined use cases with diverse characteristics, and an extensive manually\ncurated ground truth. We demonstrate the flexibility of our benchmark by\nfocusing on several variants of two crucial data integration tasks, Schema\nMatching and Entity Resolution. Our experiments show that our benchmark enables\nthe evaluation of a variety of methods that previously were difficult to\ncompare, and can foster the design of more holistic data integration solutions.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 08:32:42 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 14:54:45 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Crescenzi", "Valter", ""], ["De Angelis", "Andrea", ""], ["Firmani", "Donatella", ""], ["Mazzei", "Maurizio", ""], ["Merialdo", "Paolo", ""], ["Piai", "Federico", ""], ["Srivastava", "Divesh", ""]]}, {"id": "2101.11727", "submitter": "Cristina Feier", "authors": "Cristina Feier", "title": "Characterising Fixed Parameter Tractability of Query Evaluation over\n  Guarded TGDs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.CC cs.DB cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the parameterized complexity of evaluating Ontology Mediated Queries\n(OMQs) based on Guarded TGDs (GTGDs) and Unions of Conjunctive Queries (UCQs),\nin the case where relational symbols have unrestricted arity and where the\nparameter is the size of the OMQ. We establish exact criteria for\nfixed-parameter tractability (fpt) evaluation of recursively enumerable classes\nof such OMQs (under the widely held Exponential Time Hypothesis). One of the\nmain technical tools introduced in the paper is an fpt-reduction from deciding\nparameterized uniform CSPs to parameterized OMQ evaluation. The reduction\npreserves measures which are known to be essential for classifying recursively\nenumerable classes of parameterized uniform CSPs: submodular width (according\nto the well known result of Marx for unrestricted-arity schemas) and treewidth\n(according to the well known result of Grohe for bounded-arity schemas). As\nsuch, it can be employed to obtain hardness results for evaluation of\nrecursively enumerable classes of parameterized OMQs both in the unrestricted\nand in the bounded arity case. Previously, in the case of bounded arity\nschemas, this has been tackled using a technique requiring full introspection\ninto the construction employed by Grohe.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 22:32:16 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 14:08:08 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Feier", "Cristina", ""]]}, {"id": "2101.12158", "submitter": "Nikolaos Tziavelis", "authors": "Nikolaos Tziavelis, Wolfgang Gatterbauer, Mirek Riedewald", "title": "Beyond Equi-joins: Ranking, Enumeration and Factorization", "comments": "18 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study theta-joins in general and join predicates with conjunctions and\ndisjunctions of inequalities in particular, focusing on ranked enumeration\nwhere the answers are returned incrementally in an order dictated by a given\nranking function. Our approach achieves strong time and space complexity\nproperties: with $n$ denoting the number of tuples in the database, we\nguarantee for acyclic full join queries with inequality conditions that for\nevery value of $k$, the $k$ top-ranked answers are returned in $O(n\n\\operatorname{polylog} n + k \\log k)$ time. This is within a polylogarithmic\nfactor of the best known complexity for equi-joins and even of\n$\\mathcal{O}(n+k)$, the time it takes to look at the input and return $k$\nanswers in any order. Our guarantees extend to join queries with selections and\nmany types of projections, such as the so-called free-connex queries.\nRemarkably, they hold even when the entire output is of size $n^\\ell$ for a\njoin of $\\ell$ relations. The key ingredient is a novel $\\mathcal{O}(n\n\\operatorname{polylog} n)$-size factorized representation of the query output,\nwhich is constructed on-the-fly for a given query and database. In addition to\nproviding the first non-trivial theoretical guarantees beyond equi-joins, we\nshow in an experimental study that our ranked-enumeration approach is also\nmemory-efficient and fast in practice, beating the running time of\nstate-of-the-art database systems by orders of magnitude.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 18:12:26 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 19:37:00 GMT"}, {"version": "v3", "created": "Wed, 16 Jun 2021 10:00:48 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Tziavelis", "Nikolaos", ""], ["Gatterbauer", "Wolfgang", ""], ["Riedewald", "Mirek", ""]]}, {"id": "2101.12289", "submitter": "Martin Grohe", "authors": "Martin Grohe and Benjamin Lucien Kaminski and Joost-Pieter Katoen and\n  Peter Lindner", "title": "Probabilistic Data with Continuous Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical models of real world data typically involve continuous\nprobability distributions such as normal, Laplace, or exponential\ndistributions. Such distributions are supported by many probabilistic modelling\nformalisms, including probabilistic database systems. Yet, the traditional\ntheoretical framework of probabilistic databases focusses entirely on finite\nprobabilistic databases.\n  Only recently, we set out to develop the mathematical theory of infinite\nprobabilistic databases. The present paper is an exposition of two recent\npapers which are cornerstones of this theory. In (Grohe, Lindner; ICDT 2020) we\npropose a very general framework for probabilistic databases, possibly\ninvolving continuous probability distributions, and show that queries have a\nwell-defined semantics in this framework. In (Grohe, Kaminski, Katoen, Lindner;\nPODS 2020) we extend the declarative probabilistic programming language\nGenerative Datalog, proposed by (B\\'ar\\'any et al.~2017) for discrete\nprobability distributions, to continuous probability distributions and show\nthat such programs yield generative models of continuous probabilistic\ndatabases.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 21:39:22 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 16:47:37 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Grohe", "Martin", ""], ["Kaminski", "Benjamin Lucien", ""], ["Katoen", "Joost-Pieter", ""], ["Lindner", "Peter", ""]]}, {"id": "2101.12305", "submitter": "Anil Pacaci", "authors": "Anil Pacaci, Angela Bonifati, M. Tamer \\\"Ozsu", "title": "Evaluating Complex Queries on Streaming Graphs", "comments": "18 pages; typos fixed, experimental setup and analysis updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of evaluating persistent queries over\nstreaming graphs in a principled fashion. These queries need to be evaluated\nover unbounded and very high speed graph streams. We define a streaming graph\nmodel and a streaming graph query model incorporating navigational queries,\nsubgraph queries and paths as first-class citizens. To support this\nfull-fledged query model we develop a streaming graph algebra that describes\nthe precise semantics of persistent graph queries with their complex\nconstructs. We present transformation rules and describe query formulation and\nplan generation for persistent graph queries over streaming graphs. Our\nimplementation of a streaming graph query processor based on the dataflow\ncomputational model shows the feasibility of our approach and allows us to\ngauge the high performance gains obtained for query processing over streaming\ngraphs.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 22:34:20 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 01:00:06 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Pacaci", "Anil", ""], ["Bonifati", "Angela", ""], ["\u00d6zsu", "M. Tamer", ""]]}, {"id": "2101.12334", "submitter": "Aida Sheshbolouki", "authors": "Aida Sheshbolouki and M. Tamer \\\"Ozsu", "title": "sGrapp: Butterfly Approximation in Streaming Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the fundamental problem of butterfly (i.e. (2,2)-bicliques) counting\nin bipartite streaming graphs. Similar to triangles in unipartite graphs,\nenumerating butterflies is crucial in understanding the structure of bipartite\ngraphs. This benefits many applications where studying the cohesion in a graph\nshaped data is of particular interest. Examples include investigating the\nstructure of computational graphs or input graphs to the algorithms, as well as\ndynamic phenomena and analytic tasks over complex real graphs. Butterfly\ncounting is computationally expensive, and known techniques do not scale to\nlarge graphs; the problem is even harder in streaming graphs. In this paper,\nfollowing a data-driven methodology, we first conduct an empirical analysis to\nuncover temporal organizing principles of butterflies in real streaming graphs\nand then we introduce an approximate adaptive window-based algorithm, sGrapp,\nfor counting butterflies as well as its optimized version sGrapp-x. sGrapp is\ndesigned to operate efficiently and effectively over any graph stream with any\ntemporal behavior. Experimental studies of sGrapp and sGrapp-x show superior\nperformance in terms of both accuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 00:58:12 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 21:39:53 GMT"}, {"version": "v3", "created": "Wed, 3 Feb 2021 16:50:55 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Sheshbolouki", "Aida", ""], ["\u00d6zsu", "M. Tamer", ""]]}, {"id": "2101.12417", "submitter": "Daichi Amagata", "authors": "Shohei Tsuruoka, Daichi Amagata, Shunya Nishio, Takahiro Hara", "title": "Distributed Spatial-Keyword kNN Monitoring for Location-aware Pub/Sub", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent applications employ publish/subscribe (Pub/Sub) systems so that\npublishers can easily receive attentions of customers and subscribers can\nmonitor useful information generated by publishers. Due to the prevalence of\nsmart devices and social networking services, a large number of objects that\ncontain both spatial and keyword information have been generated continuously,\nand the number of subscribers also continues to increase. This poses a\nchallenge to Pub/Sub systems: they need to continuously extract useful\ninformation from massive objects for each subscriber in real time. In this\npaper, we address the problem of k nearest neighbor monitoring on a\nspatial-keyword data stream for a large number of subscriptions. To scale well\nto massive objects and subscriptions, we propose a distributed solution, namely\nDkM-SKS. Given m workers, DkM-SKS divides a set of subscriptions into m\ndisjoint subsets based on a cost model so that each worker has almost the same\nkNN-update cost, to maintain load balancing. DkM-SKS allows an arbitrary\napproach to updating kNN of each subscription, so with a suitable in-memory\nindex, DkM-SKS can accelerate update efficiency by pruning irrelevant\nsubscriptions for a given new object. We conduct experiments on real datasets,\nand the results demonstrate the efficiency and scalability of DkM-SKS.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 06:07:00 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 01:35:40 GMT"}, {"version": "v3", "created": "Wed, 10 Feb 2021 02:40:24 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Tsuruoka", "Shohei", ""], ["Amagata", "Daichi", ""], ["Nishio", "Shunya", ""], ["Hara", "Takahiro", ""]]}, {"id": "2101.12602", "submitter": "Shun Zhang", "authors": "Zhang Shun, Duan Benfei, Chen Zhili, Zhong Hong", "title": "On the differential privacy of dynamic location obfuscation with\n  personalized error bounds", "comments": "5 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geo-indistinguishability and expected inference error are two complementary\nnotions for location privacy. The joint guarantee of differential privacy\n(indistinguishability) and distortion privacy (inference error) limits the\ninformation leakage. In this paper, we analyze the differential privacy of PIVE\ndynamic location obfuscation mechanism proposed by Yu, Liu and Pu (ISOC Network\nand Distributed System Security Symposium, 2017) and show that PIVE fails to\noffer differential privacy guarantees on adaptive protection location set as\nclaimed. Specifically, we demonstrate that different protection location sets\ncould intersect with one another due to the defined search algorithm and then\ndifferent locations in the same protection location set could have different\nprotection diameters. As a result, we can show that the proof of differential\nprivacy for PIVE is incorrect. We also make some detailed discussions on\nfeasible privacy frameworks with achieving personalized error bounds.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 14:31:18 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Shun", "Zhang", ""], ["Benfei", "Duan", ""], ["Zhili", "Chen", ""], ["Hong", "Zhong", ""]]}, {"id": "2101.12631", "submitter": "Mengzhao Wang", "authors": "Mengzhao Wang and Xiaoliang Xu and Qiang Yue and Yuxiang Wang", "title": "A Comprehensive Survey and Experimental Comparison of Graph-Based\n  Approximate Nearest Neighbor Search", "comments": "28 pages, 21 figures, 24 tables, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate nearest neighbor search (ANNS) constitutes an important operation\nin a multitude of applications, including recommendation systems, information\nretrieval, and pattern recognition. In the past decade, graph-based ANNS\nalgorithms have been the leading paradigm in this domain, with dozens of\ngraph-based ANNS algorithms proposed. Such algorithms aim to provide effective,\nefficient solutions for retrieving the nearest neighbors for a given query.\nNevertheless, these efforts focus on developing and optimizing algorithms with\ndifferent approaches, so there is a real need for a comprehensive survey about\nthe approaches' relative performance, strengths, and pitfalls. Thus here we\nprovide a thorough comparative analysis and experimental evaluation of 13\nrepresentative graph-based ANNS algorithms via a new taxonomy and fine-grained\npipeline. We compared each algorithm in a uniform test environment on eight\nreal-world datasets and 12 synthetic datasets with varying sizes and\ncharacteristics. Our study yields novel discoveries, offerings several useful\nprinciples to improve algorithms, thus designing an optimized method that\noutperforms the state-of-the-art algorithms. This effort also helped us\npinpoint algorithms' working portions, along with rule-of-thumb recommendations\nabout promising research directions and suitable algorithms for practitioners\nin different fields.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 15:12:35 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 08:54:16 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Wang", "Mengzhao", ""], ["Xu", "Xiaoliang", ""], ["Yue", "Qiang", ""], ["Wang", "Yuxiang", ""]]}]