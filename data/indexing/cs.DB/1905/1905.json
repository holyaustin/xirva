[{"id": "1905.00687", "submitter": "Fares Zaidi", "authors": "Fares Zaidi (RI2C - LITIS), Laurent Amanton (RI2C - LITIS), Eric\n  Sanlaville (RI2C - LITIS)", "title": "Towards a Novel Cooperative Logistics Information System Framework", "comments": null, "journal-ref": "7th International Conference on Information Systems, Logistics and\n  Supply Chain (ILS2018), Jul 2018, Lyon, France", "doi": null, "report-no": null, "categories": "cs.DB cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supply Chains and Logistics have a growing importance in global economy.\nSupply Chain Information Systems over the world are heterogeneous and each one\ncan both produce and receive massive amounts of structured and unstructured\ndata in real-time, which are usually generated by information systems,\nconnected objects or manually by humans. This heterogeneity is due to Logistics\nInformation Systems components and processes that are developed by different\nmodelling methods and running on many platforms; hence, decision making process\nis difficult in such multi-actor environment. In this paper we identify some\ncurrent challenges and integration issues between separately designed Logistics\nInformation Systems (LIS), and we propose a Distributed Cooperative Logistics\nPlatform (DCLP) framework based on NoSQL, which facilitates real-time\ncooperation between stakeholders and improves decision making process in a\nmulti-actor environment. We included also a case study of Hospital Supply Chain\n(HSC), and a brief discussion on perspectives and future scope of work.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 12:03:52 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Zaidi", "Fares", "", "RI2C - LITIS"], ["Amanton", "Laurent", "", "RI2C - LITIS"], ["Sanlaville", "Eric", "", "RI2C - LITIS"]]}, {"id": "1905.00700", "submitter": "Wenhui Yu", "authors": "Wenhui Yu, Jinfei Liu, Jian Pei, Li Xiong, Xu Chen, Zheng Qin", "title": "Efficient Contour Computation of Group-based Skyline", "comments": "17 pages, submitted to Transactions on Knowledge and Data Engineering\n  (TKDE)", "journal-ref": null, "doi": "10.1109/TKDE.2019.2905239", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skyline, aiming at finding a Pareto optimal subset of points in a\nmulti-dimensional dataset, has gained great interest due to its extensive use\nfor multi-criteria analysis and decision making. The skyline consists of all\npoints that are not dominated by any other points. It is a candidate set of the\noptimal solution, which depends on a specific evaluation criterion for optimum.\nHowever, conventional skyline queries, which return individual points, are\ninadequate in group querying case since optimal combinations are required. To\naddress this gap, we study the skyline computation in the group level and\npropose efficient methods to find the Group-based skyline (G-skyline). For\ncomputing the front $l$ skyline layers, we lay out an efficient approach that\ndoes the search concurrently on each dimension and investigates each point in\nthe subspace. After that, we present a novel structure to construct the\nG-skyline with a queue of combinations of the first-layer points. We further\ndemonstrate that the G-skyline is a complete candidate set of top-$l$\nsolutions, which is the main superiority over previous group-based skyline\ndefinitions. However, as G-skyline is complete, it contains a large number of\ngroups which can make it impractical. To represent the \"contour\" of the\nG-skyline, we define the Representative G-skyline (RG-skyline). Then, we\npropose a Group-based clustering (G-clustering) algorithm to find out\nRG-skyline groups. Experimental results show that our algorithms are several\norders of magnitude faster than the previous work.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 12:37:56 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Yu", "Wenhui", ""], ["Liu", "Jinfei", ""], ["Pei", "Jian", ""], ["Xiong", "Li", ""], ["Chen", "Xu", ""], ["Qin", "Zheng", ""]]}, {"id": "1905.00774", "submitter": "Anthony Kleerekoper", "authors": "Anthony Kleerekoper, Javier Navaridas, Mikel Lujan", "title": "Can the Optimizer Cost be Used to Predict Query Execution Times?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the execution time of queries is an important problem with\napplications in scheduling, service level agreements and error detection.\nDuring query planning, a cost is associated with the chosen execution plan and\nused to rank competing plans. It would be convenient to use that cost to\npredict execution time, but it has been claimed in the literature that this is\nnot possible. In this paper, we thoroughly investigate this claim considering\nboth linear and non-linear models. We find that the accuracy using more complex\nmodels with only the optimizer cost is comparable to the reported accuracy in\nthe literature. The most accurate method in the literature is nearest-neighbour\nregression which does not produce a model. The published results used a large\nfeature set to identify nearest neighbours. We show that it is possible to\nachieve the same level of accuracy using only the cost to identify nearest\nneighbours. Using a smaller feature set brings the advantages of reduced\noverhead in terms of both storage space for the training data and the time to\nproduce a prediction.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 14:35:47 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Kleerekoper", "Anthony", ""], ["Navaridas", "Javier", ""], ["Lujan", "Mikel", ""]]}, {"id": "1905.00983", "submitter": "Vatche Ishakian", "authors": "Phuong Nguyen and Vatche Ishakian and Vinod Muthusamy and Aleksander\n  Slominski", "title": "SUMMARIZED: Efficient Framework for Analyzing Multidimensional Process\n  Traces under Edit-distance Constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domains such as scientific workflows and business processes exhibit data\nmodels with complex relationships between objects. This relationship is\ntypically represented as sequences, where each data item is annotated with\nmulti-dimensional attributes. There is a need to analyze this data for\noperational insights. For example, in business processes, users are interested\nin clustering process traces into smaller subsets to discover less complex\nprocess models. This requires expensive computation of similarity metrics\nbetween sequence-based data. Related work on dimension reduction and embedding\nmethods do not take into account the multi-dimensional attributes of data, and\ndo not address the interpretability of data in the embedding space (i.e., by\nfavoring vector-based representation). In this work, we introduce Summarized, a\nframework for efficient analysis on sequence-based multi-dimensional data using\nintuitive and user-controlled summarizations. We introduce summarization\nschemes that provide tunable trade-offs between the quality and efficiency of\nanalysis tasks and derive an error model for summary-based similarity under an\nedit-distance constraint. Evaluations using real-world datasets show the\neffectives of our framework.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 22:04:10 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Nguyen", "Phuong", ""], ["Ishakian", "Vatche", ""], ["Muthusamy", "Vinod", ""], ["Slominski", "Aleksander", ""]]}, {"id": "1905.01042", "submitter": "Ben Fulcher", "authors": "Ben D. Fulcher, Carl H. Lubba, Sarab S. Sethi, Nick S. Jones", "title": "CompEngine: a self-organizing, living library of time-series data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern biomedical applications often involve time-series data, from\nhigh-throughput phenotyping of model organisms, through to individual disease\ndiagnosis and treatment using biomedical data streams. Data and tools for\ntime-series analysis are developed and applied across the sciences and in\nindustry, but meaningful cross-disciplinary interactions are limited by the\nchallenge of identifying fruitful connections. Here we introduce the web\nplatform, CompEngine, a self-organizing, living library of time-series data\nthat lowers the barrier to forming meaningful interdisciplinary connections\nbetween time series. Using a canonical feature-based representation, CompEngine\nplaces all time series in a common space, regardless of their origin, allowing\nusers to upload their data and immediately explore interdisciplinary\nconnections to other data with similar properties, and be alerted when similar\ndata is uploaded in the future. In contrast to conventional databases, which\nare organized by assigned metadata, CompEngine incentivizes data sharing by\nautomatically connecting experimental and theoretical scientists across\ndisciplines based on the empirical structure of their data. CompEngine's\ngrowing library of interdisciplinary time-series data also facilitates\ncomprehensively characterization of algorithm performance across diverse types\nof data, and can be used to empirically motivate the development of new\ntime-series analysis algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 06:41:21 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Fulcher", "Ben D.", ""], ["Lubba", "Carl H.", ""], ["Sethi", "Sarab S.", ""], ["Jones", "Nick S.", ""]]}, {"id": "1905.01135", "submitter": "Dominik Durner", "authors": "Dominik Durner, Viktor Leis, Thomas Neumann", "title": "On the Impact of Memory Allocation on High-Performance Query Processing", "comments": null, "journal-ref": "DaMoN 2019", "doi": "10.1145/3329785.3329918", "report-no": null, "categories": "cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Somewhat surprisingly, the behavior of analytical query engines is crucially\naffected by the dynamic memory allocator used. Memory allocators highly\ninfluence performance, scalability, memory efficiency and memory fairness to\nother processes. In this work, we provide the first comprehensive experimental\nanalysis on the impact of memory allocation for high-performance query engines.\nWe test five state-of-the-art dynamic memory allocators and discuss their\nstrengths and weaknesses within our DBMS. The right allocator can increase the\nperformance of TPC-DS (SF 100) by 2.7x on a 4-socket Intel Xeon server.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 12:10:02 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Durner", "Dominik", ""], ["Leis", "Viktor", ""], ["Neumann", "Thomas", ""]]}, {"id": "1905.01294", "submitter": "Jeremy Kepner", "authors": "Pieter Cailliau, Tim Davis, Vijay Gadepally, Jeremy Kepner, Roi\n  Lipman, Jeffrey Lovitz, Keren Ouaknine", "title": "RedisGraph GraphBLAS Enabled Graph Database", "comments": "Accepted to IEEE IPDPS 2019 GrAPL workshop", "journal-ref": null, "doi": "10.1109/IPDPSW.2019.00054", "report-no": null, "categories": "cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RedisGraph is a Redis module developed by Redis Labs to add graph database\nfunctionality to the Redis database. RedisGraph represents connected data as\nadjacency matrices. By representing the data as sparse matrices and employing\nthe power of GraphBLAS (a highly optimized library for sparse matrix\noperations), RedisGraph delivers a fast and efficient way to store, manage and\nprocess graphs. Initial benchmarks indicate that RedisGraph is significantly\nfaster than comparable graph databases.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 23:39:42 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Cailliau", "Pieter", ""], ["Davis", "Tim", ""], ["Gadepally", "Vijay", ""], ["Kepner", "Jeremy", ""], ["Lipman", "Roi", ""], ["Lovitz", "Jeffrey", ""], ["Ouaknine", "Keren", ""]]}, {"id": "1905.01306", "submitter": "Nataliya Shakhovska Prof", "authors": "Nataliya Shakhovska, Uyrii Bolubash, Oleh Veres", "title": "Big Data Model \"Entity and Features\"", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article deals with the problem which led to Big Data. Big Data\ninformation technology is the set of methods and means of processing different\ntypes of structured and unstructured dynamic large amounts of data for their\nanalysis and use of decision support. Features of NoSQL databases and\ncategories are described. The developed Big Data Model \"Entity and Features\"\nallows determining the distance between the sources of data on the availability\nof information about a particular entity. The information structure of Big Data\nhas been devised. It became a basis for further research and for concentrating\non a problem of development of diverse data without their preliminary\nintegration.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 16:13:52 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Shakhovska", "Nataliya", ""], ["Bolubash", "Uyrii", ""], ["Veres", "Oleh", ""]]}, {"id": "1905.01307", "submitter": "Nataliya Shakhovska Prof", "authors": "Nataliya Shakhovska, Yurii Bolubash", "title": "Dataspace architecture and manage its components class projection", "comments": "10 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big Data technology is described. Big data is a popular term used to describe\nthe exponential growth and availability of data, both structured and\nunstructured. There is constructed dataspace architecture. Dataspace has\nfocused solely - and passionately - on providing unparalleled expertise in\nbusiness intelligence and data warehousing strategy and implementation.\nDataspaces are an abstraction in data management that aims to overcome some of\nthe problems encountered in data integration system. In our case it is block\nvector for heterogeneous data representation. Traditionally, data integration\nand data exchange systems have aimed to offer many of the purported services of\ndataspace systems. Dataspaces can be viewed as a next step in the evolution of\ndata integration architectures, but are distinct from current data integration\nsystems in the following way. Data integration systems require semantic\nintegration before any services can be provided. Hence, although there is not a\nsingle schema to which all the data conforms and the data resides in a\nmultitude of host systems, the data integration system knows the precise\nrelationships between the terms used in each schema. As a result, significant\nup-front effort is required in order to set up a data integration system. For\nrealization of data integration from different sources we used SQL Server\nIntegration Services, SSIS. For developing the portal as an architectural\npattern there is used pattern Model-View-Controller (MVC). There is specifics\ndebug operation data space as a complex system. The query translator in\nBackus/Naur Form is give.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 16:18:19 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Shakhovska", "Nataliya", ""], ["Bolubash", "Yurii", ""]]}, {"id": "1905.01349", "submitter": "Anastasios Gounaris", "authors": "Nikodimos Nikolaidis and Anastasios Gounaris", "title": "Adaptive filter ordering in Spark", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report describes a technical methodology to render the Apache Spark\nexecution engine adaptive. It presents the engineering solutions, which\nspecifically target to adaptively reorder predicates in data streams with\nevolving statistics. The system extension developed is available as an\nopen-source prototype. Indicative experimental results show its overhead and\nsensitivity to tuning parameters.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 19:36:55 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Nikolaidis", "Nikodimos", ""], ["Gounaris", "Anastasios", ""]]}, {"id": "1905.01351", "submitter": "Luke Rodriguez", "authors": "Luke Rodriguez, Bill Howe", "title": "In Defense of Synthetic Data", "comments": "Discussion paper at FATES on the Web 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthetic datasets have long been thought of as second-rate, to be used only\nwhen \"real\" data collected directly from the real world is unavailable. But\nthis perspective assumes that raw data is clean, unbiased, and trustworthy,\nwhich it rarely is. Moreover, the benefits of synthetic data for privacy and\nfor bias correction are becoming increasingly important in any domain that\nworks with people. Curated synthetic datasets - synthetic data derived from\nminimal perturbations of real data - enable early stage product development and\ncollaboration, protect privacy, afford reproducibility, increase dataset\ndiversity in research, and protect disadvantaged groups from problematic\ninferences on the original data that reflects systematic discrimination. Rather\nthan representing a departure from the true state of the world, in this paper\nwe argue that properly generated synthetic data is a step towards responsible\nand equitable research and development of machine learning systems.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 19:53:11 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Rodriguez", "Luke", ""], ["Howe", "Bill", ""]]}, {"id": "1905.01425", "submitter": "Zhihan Guo", "authors": "Zhihan Guo and Theodoros Rekatsinas", "title": "Learning Functional Dependencies with Sparse Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of discovering functional dependencies (FD) from a noisy\ndataset. We focus on FDs that correspond to statistical dependencies in a\ndataset and draw connections between FD discovery and structure learning in\nprobabilistic graphical models. We show that discovering FDs from a noisy\ndataset is equivalent to learning the structure of a graphical model over\nbinary random variables, where each random variable corresponds to a functional\nof the dataset attributes. We build upon this observation to introduce AutoFD a\nconceptually simple framework in which learning functional dependencies\ncorresponds to solving a sparse regression problem. We show that our methods\ncan recover true functional dependencies across a diverse array of real-world\nand synthetic datasets, even in the presence of noisy or missing data. We find\nthat AutoFD scales to large data instances with millions of tuples and hundreds\nof attributes while it yields an average F1 improvement of 2 times against\nstate-of-the-art FD discovery methods.\n", "versions": [{"version": "v1", "created": "Sat, 4 May 2019 03:59:05 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Guo", "Zhihan", ""], ["Rekatsinas", "Theodoros", ""]]}, {"id": "1905.02010", "submitter": "Jaroslaw Szlichta", "authors": "Parke Godfrey, Lukasz Golab, Mehdi Kargar, Divesh Srivastava, Jaroslaw\n  Szlichta", "title": "Errata Note: Discovering Order Dependencies through Order Compatibility", "comments": "5", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of extensions to the classical notion of functional dependencies\nhave been proposed to express and enforce application semantics. One of these\nextensions is that of order dependencies (ODs), which express rules involving\norder. The article entitled \"Discovering Order Dependencies through Order\nCompatibility\" by Consonni et al., published in the EDBT conference proceedings\nin March 2019, investigates the OD discovery problem. They claim to prove that\ntheir OD discovery algorithm, OCDDISCOVER, is complete, as well as being\nsignificantly more efficient in practice than the state-of-the-art. They\nfurther claim that the implementation of the existing FASTOD algorithm\n(ours)-we shared our code base with the authors-which they benchmark against is\nflawed, as OCDDISCOVER and FASTOD report different sets of ODs over the same\ndata sets.\n  In this rebuttal, we show that their claim of completeness is, in fact, not\ntrue. Built upon their incorrect claim, OCDDISCOVER's pruning rules are overly\naggressive, and prune parts of the search space that contain legitimate ODs.\nThis is the reason their approach appears to be \"faster\" in practice. Finally,\nwe show that Consonni et al. misinterpret our set-based canonical form for ODs,\nleading to an incorrect claim that our FASTOD implementation has an error.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 13:02:54 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Godfrey", "Parke", ""], ["Golab", "Lukasz", ""], ["Kargar", "Mehdi", ""], ["Srivastava", "Divesh", ""], ["Szlichta", "Jaroslaw", ""]]}, {"id": "1905.02051", "submitter": "James Cheney", "authors": "Stefan Fehrenbach and James Cheney", "title": "Language-integrated provenance by trace analysis", "comments": "DBPL 2019", "journal-ref": null, "doi": "10.1145/3315507.3330198", "report-no": null, "categories": "cs.PL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language-integrated provenance builds on language-integrated query techniques\nto make provenance information explaining query results readily available to\nprogrammers. In previous work we have explored language-integrated approaches\nto provenance in Links and Haskell. However, implementing a new form of\nprovenance in a language-integrated way is still a major challenge. We propose\na self-tracing transformation and trace analysis features that, together with\nexisting techniques for type-directed generic programming, make it possible to\ndefine different forms of provenance as user code. We present our design as an\nextension to a core language for Links called LinksT, give examples showing its\ncapabilities, and outline its metatheory and key correctness properties.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 13:56:46 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Fehrenbach", "Stefan", ""], ["Cheney", "James", ""]]}, {"id": "1905.02069", "submitter": "James Cheney", "authors": "Wilmer Ricciotti and James Cheney", "title": "Mixing set and bag semantics", "comments": "DBPL 2019 -- short paper", "journal-ref": null, "doi": "10.1145/3315507.3330202", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conservativity theorem for nested relational calculus implies that query\nexpressions can freely use nesting and unnesting, yet as long as the query\nresult type is a flat relation, these capabilities do not lead to an increase\nin expressiveness over flat relational queries. Moreover, Wong showed how such\nqueries can be translated to SQL via a constructive rewriting algorithm. While\nthis result holds for queries over either set or multiset semantics, to the\nbest of our knowledge, the questions of conservativity and normalization have\nnot been studied for queries that mix set and bag collections, or provide\nduplicate-elimination operations such as SQL's\n$\\mathtt{SELECT}~\\mathtt{DISTINCT}$. In this paper we formalize the problem,\nand present partial progress: specifically, we introduce a calculus with both\nset and multiset collection types, along with natural mappings from sets to\nbags and vice versa, present a set of valid rewrite rules for normalizing such\nqueries, and give an inductive characterization of a set of queries whose\nnormal forms can be translated to SQL. We also consider examples that do not\nappear straightforward to translate to SQL, illustrating that the relative\nexpressiveness of flat and nested queries with mixed set and multiset semantics\nremains an open question.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 14:47:33 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Ricciotti", "Wilmer", ""], ["Cheney", "James", ""]]}, {"id": "1905.02157", "submitter": "Dongfang Zhao", "authors": "Xinying Wang and Abdullah Al-Mamun and Feng Yan and Mohammad Sadoghi\n  and Dongfang Zhao", "title": "BlockLite: A Lightweight Emulator for Public Blockchains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain is an enabler of many emerging decentralized applications in areas\nof cryptocurrency, Internet of Things, smart healthcare, among many others.\nAlthough various open-source blockchain frameworks are available, the\ninfrastructure is complex enough and difficult for many users to modify or test\nout new research ideas. To make it worse, many advantages of blockchain systems\ncan be demonstrated only at large scales, e.g., thousands of nodes, which are\nnot always available to researchers. This demo paper presents a lightweight\nsingle-node emulator of blockchain systems, namely \\mbox{BlockLite}, designed\nto be executing real proof-of-work workload along with peer-to-peer network\ncommunications and hash-based immutability. BlockLite employs a preprocessing\napproach to avoid the per-node computation overhead at runtime and thus scales\nto thousands of nodes. Moreover, BlockLite offers an easy-to-use programming\ninterface allowing for a Lego-like customization to the system, e.g. new ad-hoc\nconsensus protocols.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 17:17:44 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Wang", "Xinying", ""], ["Al-Mamun", "Abdullah", ""], ["Yan", "Feng", ""], ["Sadoghi", "Mohammad", ""], ["Zhao", "Dongfang", ""]]}, {"id": "1905.02304", "submitter": "Justin Chen", "authors": "Justin Chen, Edward Gan, Kexin Rong, Sahaana Suri, Peter Bailis", "title": "CrossTrainer: Practical Domain Adaptation with Loss Reweighting", "comments": null, "journal-ref": null, "doi": "10.1145/3329486.3329491", "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation provides a powerful set of model training techniques given\ndomain-specific training data and supplemental data with unknown relevance. The\ntechniques are useful when users need to develop models with data from varying\nsources, of varying quality, or from different time ranges. We build\nCrossTrainer, a system for practical domain adaptation. CrossTrainer utilizes\nloss reweighting, which provides consistently high model accuracy across a\nvariety of datasets in our empirical analysis. However, loss reweighting is\nsensitive to the choice of a weight hyperparameter that is expensive to tune.\nWe develop optimizations leveraging unique properties of loss reweighting that\nallow CrossTrainer to output accurate models while improving training time\ncompared to naive hyperparameter search.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 00:48:48 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Chen", "Justin", ""], ["Gan", "Edward", ""], ["Rong", "Kexin", ""], ["Suri", "Sahaana", ""], ["Bailis", "Peter", ""]]}, {"id": "1905.02828", "submitter": "Akhil Dixit", "authors": "Akhil A. Dixit, Phokion G. Kolaitis", "title": "A SAT-based System for Consistent Query Answering", "comments": "25 pages including appendix, to appear in the 22nd International\n  Conference on Theory and Applications of Satisfiability Testing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An inconsistent database is a database that violates one or more integrity\nconstraints, such as functional dependencies. Consistent Query Answering is a\nrigorous and principled approach to the semantics of queries posed against\ninconsistent databases. The consistent answers to a query on an inconsistent\ndatabase is the intersection of the answers to the query on every repair, i.e.,\non every consistent database that differs from the given inconsistent one in a\nminimal way. Computing the consistent answers of a fixed conjunctive query on a\ngiven inconsistent database can be a coNP-hard problem, even though every fixed\nconjunctive query is efficiently computable on a given consistent database.\n  We designed, implemented, and evaluated CAvSAT, a SAT-based system for\nconsistent query answering. CAvSAT leverages a set of natural reductions from\nthe complement of consistent query answering to SAT and to Weighted MaxSAT. The\nsystem is capable of handling unions of conjunctive queries and arbitrary\ndenial constraints, which include functional dependencies as a special case. We\nreport results from experiments evaluating CAvSAT on both synthetic and\nreal-world databases. These results provide evidence that a SAT-based approach\ncan give rise to a comprehensive and scalable system for consistent query\nanswering.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 22:31:49 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Dixit", "Akhil A.", ""], ["Kolaitis", "Phokion G.", ""]]}, {"id": "1905.02847", "submitter": "Victor Zakhary", "authors": "Victor Zakhary, Divyakant Agrawal, Amr El Abbadi", "title": "Atomic Commitment Across Blockchains", "comments": null, "journal-ref": null, "doi": "10.14778/3397230.3397231", "report-no": null, "categories": "cs.DB cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent adoption of blockchain technologies and open permissionless\nnetworks suggest the importance of peer-to-peer atomic cross-chain transaction\nprotocols. Users should be able to atomically exchange tokens and assets\nwithout depending on centralized intermediaries such as exchanges. Recent\npeer-to-peer atomic cross-chain swap protocols use hashlocks and timelocks to\nensure that participants comply to the protocol. However, an expired timelock\ncould lead to a violation of the all-or-nothing atomicity property. An honest\nparticipant who fails to execute a smart contract on time due to a crash\nfailure or network delays at her site might end up losing her assets. Although\na crashed participant is the only participant who ends up worse off, current\nproposals are unsuitable for atomic cross-chain transactions in asynchronous\nenvironments where crash failures and network delays are the norm. In this\npaper, we present AC3WN, the first decentralized all-or-nothing atomic\ncross-chain commitment protocol. The redeem and refund events of the smart\ncontracts that exchange assets are modeled as conflicting events. An open\npermissionless network of witnesses is used to guarantee that conflicting\nevents could never simultaneously occur and either all smart contracts in an\natomic cross-chain transaction are redeemed or all of them are refunded.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 00:02:12 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2019 18:53:12 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zakhary", "Victor", ""], ["Agrawal", "Divyakant", ""], ["Abbadi", "Amr El", ""]]}, {"id": "1905.02896", "submitter": "Yu Luo", "authors": "Yu Luo, Beth Plale", "title": "Pilot evaluation of Collection API with PID Kernel Information", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As digital data become increasingly available for research, there is a\ngrowing awareness of the value of domain agnostic Persistent Identifiers (PIDs)\nfor data. A PID is a globally unique reference to a digital object, which in\nour case is data. In an ecosystem of connected digital objects, a PID will\nreference a digital object, and the digital object will be a simple entity, a\ncollection of homogeneous objects, or a set of heterogeneous objects.\n  In this paper, we study two recent recommendations from the Research Data\nAlliance (RDA) that both address pieces of an ecosystem of connected digital\nobjects. The recommendations address Persistent ID records and representations\nof collections of data. We evaluate different approaches in where to locate key\ninformation about a data collection between these two component solutions.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 04:11:41 GMT"}, {"version": "v2", "created": "Wed, 3 Jul 2019 20:19:13 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Luo", "Yu", ""], ["Plale", "Beth", ""]]}, {"id": "1905.03061", "submitter": "Nataliya Shakhovska Prof", "authors": "Shakhovska Nataliya, Veres Oleh, Hirnyak Mariia", "title": "Generalized formal model of big data", "comments": null, "journal-ref": "ECONTECHMOD. AN INTERNATIONAL QUARTERLY JOURNAL - 2016, Vol. 05,\n  No. 2, 33-38", "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article dwells on the basic characteristic features of the Big Data\ntechnologies. It is analyzed the existing definition of the \"big data\" term.\nThe article proposes and describes the elements of the generalized formal model\nof big data. It is analyzed the peculiarities of the application of the\nproposed model components. It described the fundamental differences between Big\nData technology and business analytics. Big Data is supported by the\ndistributed file system Google File System technology, Cassandra, HBase, Lustre\nand ZFS, by the MapReduce and Hadoop programming constructs and many other\nsolutions. According to the experts, such as McKinsey Institute, the\nmanufacturing, healthcare, trade, administration and control of individual\nmovements undergo the transformations under the influence of the Big Data.\n", "versions": [{"version": "v1", "created": "Sat, 4 May 2019 03:01:20 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Nataliya", "Shakhovska", ""], ["Oleh", "Veres", ""], ["Mariia", "Hirnyak", ""]]}, {"id": "1905.03695", "submitter": "Kwang Woo Nam", "authors": "Wei Ding, KwangSoo Yang and Kwang Woo Nam", "title": "Measuring similarity between geo-tagged videos using largest common view", "comments": "2 pages", "journal-ref": "IET electronics letters, vol.55, no. 8, pp.450-452, 2019", "doi": "10.1049/el.2018.7499", "report-no": null, "categories": "cs.CV cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel problem for discovering the similar trajectories\nbased on the field of view (FoV) of the video data. The problem is important\nfor many societal applications such as grouping moving objects, classifying\ngeo-images, and identifying the interesting trajectory patterns. Prior work\nconsider only either spatial locations or spatial relationship between two\nline-segments. However, these approaches show a limitation to find the similar\nmoving objects with common views. In this paper, we propose new algorithm that\ncan group both spatial locations and points of view to identify similar\ntrajectories. We also propose novel methods that reduce the computational cost\nfor the proposed work. Experimental results using real-world datasets\ndemonstrates that the proposed approach outperforms prior work and reduces the\ncomputational cost.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 14:46:06 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Ding", "Wei", ""], ["Yang", "KwangSoo", ""], ["Nam", "Kwang Woo", ""]]}, {"id": "1905.04037", "submitter": "Jerome Darmont", "authors": "Pegdwend\\'e Sawadogo (ERIC), Tokio Kibata, J\\'er\\^ome Darmont (ERIC)", "title": "Metadata Management for Textual Documents in Data Lakes", "comments": null, "journal-ref": "21st International Conference on Enterprise Information Systems\n  (ICEIS 2019), May 2019, Heraklion, Greece. pp.72-83", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data lakes have emerged as an alternative to data warehouses for the storage,\nexploration and analysis of big data. In a data lake, data are stored in a raw\nstate and bear no explicit schema. Thence, an efficient metadata system is\nessential to avoid the data lake turning to a so-called data swamp. Existing\nworks about managing data lake metadata mostly focus on structured and\nsemi-structured data, with little research on unstructured data. Thus, we\npropose in this paper a methodological approach to build and manage a metadata\nsystem that is specific to textual documents in data lakes. First, we make an\ninventory of usual and meaningful metadata to extract. Then, we apply some\nspecific techniques from the text mining and information retrieval domains to\nextract, store and reuse these metadata within the COREL research project, in\norder to validate our proposals.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 09:46:01 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Sawadogo", "Pegdwend\u00e9", "", "ERIC"], ["Kibata", "Tokio", "", "ERIC"], ["Darmont", "J\u00e9r\u00f4me", "", "ERIC"]]}, {"id": "1905.04278", "submitter": "Zongheng Yang", "authors": "Zongheng Yang, Eric Liang, Amog Kamsetty, Chenggang Wu, Yan Duan, Xi\n  Chen, Pieter Abbeel, Joseph M. Hellerstein, Sanjay Krishnan, Ion Stoica", "title": "Deep Unsupervised Cardinality Estimation", "comments": "VLDB 2020. Updates since version 1: new title and new/revised content", "journal-ref": "Proceedings of the VLDB Endowment (PLVDB), Vol. 13, No. 3, pp.\n  279-292 (2019)", "doi": "10.14778/3368289.3368294", "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardinality estimation has long been grounded in statistical tools for\ndensity estimation. To capture the rich multivariate distributions of\nrelational tables, we propose the use of a new type of high-capacity\nstatistical model: deep autoregressive models. However, direct application of\nthese models leads to a limited estimator that is prohibitively expensive to\nevaluate for range or wildcard predicates. To produce a truly usable estimator,\nwe develop a Monte Carlo integration scheme on top of autoregressive models\nthat can efficiently handle range queries with dozens of dimensions or more.\n  Like classical synopses, our estimator summarizes the data without\nsupervision. Unlike previous solutions, we approximate the joint data\ndistribution without any independence assumptions. Evaluated on real-world\ndatasets and compared against real systems and dominant families of techniques,\nour estimator achieves single-digit multiplicative error at tail, an up to\n90$\\times$ accuracy improvement over the second best method, and is space- and\nruntime-efficient.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 17:36:00 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 18:36:07 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Yang", "Zongheng", ""], ["Liang", "Eric", ""], ["Kamsetty", "Amog", ""], ["Wu", "Chenggang", ""], ["Duan", "Yan", ""], ["Chen", "Xi", ""], ["Abbeel", "Pieter", ""], ["Hellerstein", "Joseph M.", ""], ["Krishnan", "Sanjay", ""], ["Stoica", "Ion", ""]]}, {"id": "1905.04505", "submitter": "Suhansanu Kumar", "authors": "Suhansanu Kumar, Heting Gao, Changyu Wang, Hari Sundaram, Kevin\n  Chen-Chuan Chang", "title": "Mining Hidden Populations through Attributed Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Researchers often query online social platforms through their application\nprogramming interfaces (API) to find target populations such as people with\nmental illness~\\cite{De-Choudhury2017} and jazz\nmusicians~\\cite{heckathorn2001finding}. Entities of such target population\nsatisfy a property that is typically identified using an oracle (human or a\npre-trained classifier). When the property of the target entities is not\ndirectly queryable via the API, we refer to the property as `hidden' and the\npopulation as a hidden population. Finding individuals who belong to these\npopulations on social networks is hard because they are non-queryable, and the\nsampler has to explore from a combinatorial query space within a finite budget\nlimit. By exploiting the correlation between queryable attributes and the\npopulation of interest and by hierarchically ordering the query space, we\npropose a Decision tree-based Thompson sampler (\\texttt{DT-TMP}) that\nefficiently discovers the right combination of attributes to query. Our\nproposed sampler outperforms the state-of-the-art samplers in online\nexperiments, for example by 54\\% on Twitter. When the number of matching\nentities to a query is known in offline experiments, \\texttt{DT-TMP} performs\nexceedingly well by a factor of 0.9-1.5$\\times$ over the baseline samplers. In\nthe future, we wish to explore the option of finding hidden populations by\nformulating more complex queries.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2019 11:36:37 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Kumar", "Suhansanu", ""], ["Gao", "Heting", ""], ["Wang", "Changyu", ""], ["Sundaram", "Hari", ""], ["Chang", "Kevin Chen-Chuan", ""]]}, {"id": "1905.04611", "submitter": "Tatsuki Sekino", "authors": "Tatsuki Sekino", "title": "Data description and retrieval using periods represented by uncertain\n  time intervals", "comments": "9 pages, 8 figures, 1 table; Journal of Information Processing (JIP)\n  28(2), in press", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time periods are frequently used to specify time in metadata and retrieval.\nHowever, it is not easy to describe and retrieve information about periods,\nbecause the temporal ranges represented by periods are often ambiguous. This is\nbecause these temporal ranges do not have fixed beginning and end points. To\nsolve this problem, basic logics to describe and process uncertain time\nintervals were developed in this study. An uncertain time interval is\nrepresented as a set of time intervals that indicate states when the uncertain\ntime interval is determined. Based on this concept, a logic to retrieve\nuncertain time intervals satisfying a given condition was established, and it\nwas revealed that retrieval results belong to three states: reliable,\nimpossible, and possible matches. Additionally, to describe data about\nuncertain periods, an ontology (the HuTime Ontology) was constructed based on\nthe logic. This ontology is characterized by the fact that uncertain time\nintervals can be defined recursively. It is expected that more data about time\nperiods will be created and released using the result of this study.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2019 23:48:03 GMT"}, {"version": "v2", "created": "Sun, 29 Dec 2019 23:59:17 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Sekino", "Tatsuki", ""]]}, {"id": "1905.04616", "submitter": "\\c{C}a\\u{g}atay Demiralp", "authors": "Kevin Hu and Neil Gaikwad and Michiel Bakker and Madelon Hulsebos and\n  Emanuel Zgraggen and C\\'esar Hidalgo and Tim Kraska and Guoliang Li and\n  Arvind Satyanarayan and \\c{C}a\\u{g}atay Demiralp", "title": "VizNet: Towards A Large-Scale Visualization Learning and Benchmarking\n  Repository", "comments": "CHI'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers currently rely on ad hoc datasets to train automated\nvisualization tools and evaluate the effectiveness of visualization designs.\nThese exemplars often lack the characteristics of real-world datasets, and\ntheir one-off nature makes it difficult to compare different techniques. In\nthis paper, we present VizNet: a large-scale corpus of over 31 million datasets\ncompiled from open data repositories and online visualization galleries. On\naverage, these datasets comprise 17 records over 3 dimensions and across the\ncorpus, we find 51% of the dimensions record categorical data, 44%\nquantitative, and only 5% temporal. VizNet provides the necessary common\nbaseline for comparing visualization design techniques, and developing\nbenchmark models and algorithms for automating visual analysis. To demonstrate\nVizNet's utility as a platform for conducting online crowdsourced experiments\nat scale, we replicate a prior study assessing the influence of user task and\ndata distribution on visual encoding effectiveness, and extend it by\nconsidering an additional task: outlier detection. To contend with running such\nstudies at scale, we demonstrate how a metric of perceptual effectiveness can\nbe learned from experimental results, and show its predictive power across test\ndatasets.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2019 00:47:28 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Hu", "Kevin", ""], ["Gaikwad", "Neil", ""], ["Bakker", "Michiel", ""], ["Hulsebos", "Madelon", ""], ["Zgraggen", "Emanuel", ""], ["Hidalgo", "C\u00e9sar", ""], ["Kraska", "Tim", ""], ["Li", "Guoliang", ""], ["Satyanarayan", "Arvind", ""], ["Demiralp", "\u00c7a\u011fatay", ""]]}, {"id": "1905.04638", "submitter": "\\c{C}a\\u{g}atay Demiralp", "authors": "Wenbo Tao and Xiaoyu Liu and \\c{C}a\\u{g}atay Demiralp and Remco Chang\n  and Michael Stonebraker", "title": "Kyrix: Interactive Visual Data Exploration at Scale", "comments": "CIDR'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scalable interactive visual data exploration is crucial in many domains due\nto increasingly large datasets generated at rapid rates. Details-on-demand\nprovides a useful interaction paradigm for exploring large datasets, where\nusers start at an overview, find regions of interest, zoom in to see detailed\nviews, zoom out and then repeat. This paradigm is the primary user interaction\nmode of widely-used systems such as Google Maps, Aperture Tiles and ForeCache.\nThese earlier systems, however, are highly customized with hardcoded visual\nrepresentations and optimizations. A more general framework is needed to\nfacilitate the development of visual data exploration systems at scale. In this\npaper, we present Kyrix, an end-to-end system for developing scalable\ndetails-on-demand data exploration applications. Kyrix provides developers with\na declarative model for easy specification of general visualizations. Behind\nthe scenes, Kyrix utilizes a suite of performance optimization techniques to\nachieve a response time within 500ms for various user interactions. We also\nreport results from a performance study which shows that a novel dynamic\nfetching scheme adopted by Kyrix outperforms tile-based fetching used in\nearlier systems.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2019 03:06:40 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Tao", "Wenbo", ""], ["Liu", "Xiaoyu", ""], ["Demiralp", "\u00c7a\u011fatay", ""], ["Chang", "Remco", ""], ["Stonebraker", "Michael", ""]]}, {"id": "1905.04767", "submitter": "Ilia Petrov", "authors": "Tobias Vincon, Andreas Koch, Ilia Petrov", "title": "Moving Processing to Data: On the Influence of Processing in Memory on\n  Data Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Near-Data Processing refers to an architectural hardware and software\nparadigm, based on the co-location of storage and compute units. Ideally, it\nwill allow to execute application-defined data- or compute-intensive operations\nin-situ, i.e. within (or close to) the physical data storage. Thus, Near-Data\nProcessing seeks to minimize expensive data movement, improving performance,\nscalability, and resource-efficiency. Processing-in-Memory is a sub-class of\nNear-Data processing that targets data processing directly within memory (DRAM)\nchips. The effective use of Near-Data Processing mandates new architectures,\nalgorithms, interfaces, and development toolchains.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2019 18:27:44 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Vincon", "Tobias", ""], ["Koch", "Andreas", ""], ["Petrov", "Ilia", ""]]}, {"id": "1905.04795", "submitter": "Mustafa Bal", "authors": "Mustafa Bal and Caitlin Ner", "title": "NFTracer: A Non-Fungible Token Tracking Proof-of-Concept Using\n  Hyperledger Fabric", "comments": "9 pages, 3 figures, 5 algorithms, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various start-up developers and academic researchers have investigated the\nusage of blockchain as a data storage medium due to the advantages offered by\nits tamper-proof and decentralized nature. However, there have not been many\nattempts to provide a standard platform for virtually storing the states of\nunique tangible entities and their subsequent modifications. In this paper, we\npropose NFTracer, a non-fungible token tracking proof-of-concept based on\nHyperledger Composer and Hyperledger Fabric Blockchain. To achieve the\ncapabilities of our platform, we use NFTracer to build an artwork auction and a\nreal estate auction, which vary in technical complexity and demonstrate the\nadvantages of being able to track entities and their resulting modifications in\na decentralized manner. We also present its accompanying modular architecture\nand system components, and discuss possible future works on NFTracer.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2019 21:39:03 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Bal", "Mustafa", ""], ["Ner", "Caitlin", ""]]}, {"id": "1905.05384", "submitter": "Abhishek Santra", "authors": "Soumyava Das, Abhishek Santra, Jay Bodra, Sharma Chakravarthy", "title": "Query Processing on Large Graphs: Approaches To Scalability and Response\n  Time Trade Offs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of social networks and the web, the graph sizes have grown\ntoo large to fit in main memory precipitating the need for alternative\napproaches for an efficient, scalable evaluation of queries on graphs of any\nsize.\n  Here, we use the divide and conquer approach by partitioning a graph and\nprocess queries over partitions to obtain all or specified number of answers.\nThis entails correctly computing answers that span multiple partitions or even\nneed the same partition more than once. Given a set of partitions, there are\nmany approaches to evaluate a query: i) One Partition At a Time approach, ii)\nTraditional use of Multiple Processors, and iii) using the Map/Reduce\nMulti-Processor approach. Approach (i), detailed in this paper, has established\nscalability through independent processing of partitions. The other two\napproaches address response time in addition to scalability. For approach (i),\nnecessary minimal book keeping has been identified and its correctness\nestablished in this paper. Query answering on partitioned graphs also requires\nanalyzing partitioning schemes for their impact on query processing and\ndetermining the number and the sequence in which partitions need to be loaded\nto reduce the response time to process queries. We correlate query properties\nand partition characteristics to reduce query processing time in terms of the\nresources available.\n  We also identify a set of quantitative metrics and use them to formulate\nheuristics to determine the order of loading partitions for efficient query\nprocessing. For approach (i), experiments on large graphs (synthetic,\nreal-world) using different partitioning schemes analyze the proposed\nheuristics on a variety of query types. The other two approaches are fleshed\nout and analyzed. An existing graph querying system has been extended to\nevaluate queries on partitioned graphs. Finally all approaches are contrasted.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 04:17:46 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Das", "Soumyava", ""], ["Santra", "Abhishek", ""], ["Bodra", "Jay", ""], ["Chakravarthy", "Sharma", ""]]}, {"id": "1905.05981", "submitter": "Jin Wang", "authors": "Jiacheng Wu, Yong Zhang, Jin Wang, Chunbin Lin, Yingjia Fu, Chunxiao\n  Xing", "title": "Improving Distributed Similarity Join in Metric Space with Error-bounded\n  Sampling", "comments": "full version of a submission to PVLDB 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given two sets of objects, metric similarity join finds all similar pairs of\nobjects according to a particular distance function in metric space. There is\nan increasing demand to provide a scalable similarity join framework which can\nsupport efficient query and analytical services in the era of Big Data. The\nexisting distributed metric similarity join algorithms adopt random sampling\ntechniques to produce pivots and utilize holistic partitioning methods based on\nthe generated pivots to partition data, which results in data skew problem\nsince both the generated pivots and the partition strategies have no quality\nguarantees.\n  To address the limitation, we propose SP-Join, an end-to-end framework to\nsupport distributed similarity join in metric space based on the MapReduce\nparadigm, which (i) employs an estimation-based stratified sampling method to\nproduce pivots with quality guarantees for any sample size, and (ii) devises an\neffective cost model as the guideline to split the whole datasets into\npartition in map and reduce phases according to the sampled pivots. Although\nobtaining an optimal set of partitions is proven to be NP-Hard, SP-Join adopts\nefficient partitioning strategies based on such a cost model to achieve an even\npartition with explainable quality. We implement our framework upon Apache\nSpark platform and conduct extensive experiments on four real-world datasets.\nThe results show that our method significantly outperforms state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 07:07:28 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Wu", "Jiacheng", ""], ["Zhang", "Yong", ""], ["Wang", "Jin", ""], ["Lin", "Chunbin", ""], ["Fu", "Yingjia", ""], ["Xing", "Chunxiao", ""]]}, {"id": "1905.06167", "submitter": "George Papadakis", "authors": "George Papadakis, Dimitrios Skoutas, Emmanouil Thanos, Themis Palpanas", "title": "A Survey of Blocking and Filtering Techniques for Entity Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficiency techniques are an integral part of Entity Resolution, since its\ninfancy. In this survey, we organized the bulk of works in the field into\nBlocking, Filtering and hybrid techniques, facilitating their understanding and\nuse. We also provided an in-dept coverage of each category, further classifying\nthe corresponding works into novel sub-categories. Lately, the efficiency\ntechniques have received more attention, due to the rise of Big Data. This\nincludes large volumes of semi-structured data, which pose challenges not only\nto the scalability of efficiency techniques, but also to their core\nassumptions: the requirement of Blocking for schema knowledge and of Filtering\nfor high similarity thresholds. The former led to the introduction of\nschema-agnostic Blocking in conjunction with Block Processing techniques, while\nthe latter led to more relaxed criteria of similarity. Our survey covers these\nnew fields in detail, putting in context all relevant works.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 13:28:56 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 09:53:29 GMT"}, {"version": "v3", "created": "Mon, 23 Mar 2020 09:43:52 GMT"}, {"version": "v4", "created": "Fri, 21 Aug 2020 15:40:39 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Papadakis", "George", ""], ["Skoutas", "Dimitrios", ""], ["Thanos", "Emmanouil", ""], ["Palpanas", "Themis", ""]]}, {"id": "1905.06170", "submitter": "George Papadakis", "authors": "Vasilis Efthymiou, George Papadakis, Kostas Stefanidis, Vassilis\n  Christophides", "title": "MinoanER: Schema-Agnostic, Non-Iterative, Massively Parallel Resolution\n  of Web Entities", "comments": "Presented at EDBT 20019", "journal-ref": null, "doi": "10.5441/002/edbt.2019.33", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Entity Resolution (ER) aims to identify different descriptions in various\nKnowledge Bases (KBs) that refer to the same entity. ER is challenged by the\nVariety, Volume and Veracity of entity descriptions published in the Web of\nData. To address them, we propose the MinoanER framework that simultaneously\nfulfills full automation, support of highly heterogeneous entities, and massive\nparallelization of the ER process. MinoanER leverages a token-based similarity\nof entities to define a new metric that derives the similarity of neighboring\nentities from the most important relations, as they are indicated only by\nstatistics. A composite blocking method is employed to capture different\nsources of matching evidence from the content, neighbors, or names of entities.\nThe search space of candidate pairs for comparison is compactly abstracted by a\nnovel disjunctive blocking graph and processed by a non-iterative, massively\nparallel matching algorithm that consists of four generic, schema-agnostic\nmatching rules that are quite robust with respect to their internal\nconfiguration. We demonstrate that the effectiveness of MinoanER is comparable\nto existing ER tools over real KBs exhibiting low Variety, but it outperforms\nthem significantly when matching KBs with high Variety.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 13:31:47 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Efthymiou", "Vasilis", ""], ["Papadakis", "George", ""], ["Stefanidis", "Kostas", ""], ["Christophides", "Vassilis", ""]]}, {"id": "1905.06209", "submitter": "William Cohen", "authors": "William W. Cohen and Matthew Siegler and Alex Hofer", "title": "Neural Query Language: A Knowledge Base Query Language for Tensorflow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large knowledge bases (KBs) are useful for many AI tasks, but are difficult\nto integrate into modern gradient-based learning systems. Here we describe a\nframework for accessing soft symbolic database using only differentiable\noperators. For example, this framework makes it easy to conveniently write\nneural models that adjust confidences associated with facts in a soft KB;\nincorporate prior knowledge in the form of hand-coded KB access rules; or learn\nto instantiate query templates using information extracted from text. NQL can\nwork well with KBs with millions of tuples and hundreds of thousands of\nentities on a single GPU.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 14:26:24 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Cohen", "William W.", ""], ["Siegler", "Matthew", ""], ["Hofer", "Alex", ""]]}, {"id": "1905.06256", "submitter": "Pengfei Li", "authors": "Pengfei Li, Yu Hua, Pengfei Zuo, Jingnan Jia", "title": "A Scalable Learned Index Scheme in Storage Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Index structures are important for efficient data access, which have been\nwidely used to improve the performance in many in-memory systems. Due to high\nin-memory overheads, traditional index structures become difficult to process\nthe explosive growth of data, let alone providing low latency and high\nthroughput performance with limited system resources. The promising learned\nindexes leverage deep-learning models to complement existing index structures\nand obtain significant memory savings. However, the learned indexes fail to\nbecome scalable due to the heavy inter-model dependency and expensive\nretraining. To address these problems, we propose a scalable learned index\nscheme to construct different linear regression models according to the data\ndistribution. Moreover, the used models are independent so as to reduce the\ncomplexity of retraining and become easy to partition and store the data into\ndifferent pages, blocks or distributed systems. Our experimental results show\nthat compared with state-of-the-art schemes, AIDEL improves the insertion\nperformance by about 2$\\times$ and provides comparable lookup performance,\nwhile efficiently supporting scalability.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 08:14:19 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Li", "Pengfei", ""], ["Hua", "Yu", ""], ["Zuo", "Pengfei", ""], ["Jia", "Jingnan", ""]]}, {"id": "1905.06361", "submitter": "M. Emre Gursoy", "authors": "Mehmet Emre Gursoy, Acar Tamersoy, Stacey Truex, Wenqi Wei, Ling Liu", "title": "Secure and Utility-Aware Data Collection with Condensed Local\n  Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local Differential Privacy (LDP) is popularly used in practice for\nprivacy-preserving data collection. Although existing LDP protocols offer high\nutility for large user populations (100,000 or more users), they perform poorly\nin scenarios with small user populations (such as those in the cybersecurity\ndomain) and lack perturbation mechanisms that are effective for both ordinal\nand non-ordinal item sequences while protecting sequence length and content\nsimultaneously. In this paper, we address the small user population problem by\nintroducing the concept of Condensed Local Differential Privacy (CLDP) as a\nspecialization of LDP, and develop a suite of CLDP protocols that offer\ndesirable statistical utility while preserving privacy. Our protocols support\ndifferent types of client data, ranging from ordinal data types in finite\nmetric spaces (numeric malware infection statistics), to non-ordinal items (OS\nversions, transaction categories), and to sequences of ordinal and non-ordinal\nitems. Extensive experiments are conducted on multiple datasets, including\ndatasets that are an order of magnitude smaller than those used in existing\napproaches, which show that proposed CLDP protocols yield high utility.\nFurthermore, case studies with Symantec datasets demonstrate that our protocols\naccurately support key cybersecurity-focused tasks of detecting ransomware\noutbreaks, identifying targeted and vulnerable OSs, and inspecting suspicious\nactivities on infected machines.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 18:06:58 GMT"}, {"version": "v2", "created": "Sun, 20 Oct 2019 05:51:37 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Gursoy", "Mehmet Emre", ""], ["Tamersoy", "Acar", ""], ["Truex", "Stacey", ""], ["Wei", "Wenqi", ""], ["Liu", "Ling", ""]]}, {"id": "1905.06385", "submitter": "George Papadakis", "authors": "Giovanni Simonini, George Papadakis, Themis Palpanas, and Sonia\n  Bergamaschi", "title": "Schema-agnostic Progressive Entity Resolution (extended version)", "comments": null, "journal-ref": "IEEE Trans. Knowl. Data Eng. 31(6): 1208-1221 (2019)", "doi": "10.1109/TKDE.2018.2852763", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity Resolution (ER) is the task of finding entity profiles that correspond\nto the same real-world entity. Progressive ER aims to efficiently resolve large\ndatasets when limited time and/or computational resources are available. In\npractice, its goal is to provide the best possible partial solution by\napproximating the optimal comparison order of the entity profiles. So far,\nProgressive ER has only been examined in the context of structured (relational)\ndata sources, as the existing methods rely on schema knowledge to save\nunnecessary comparisons: they restrict their search space to similar entities\nwith the help of schema-based blocking keys (i.e., signatures that represent\nthe entity profiles). As a result, these solutions are not applicable in Big\nData integration applications, which involve large and heterogeneous datasets,\nsuch as relational and RDF databases, JSON files, Web corpus etc. To cover this\ngap, we propose a family of schema-agnostic Progressive ER methods, which do\nnot require schema information, thus applying to heterogeneous data sources of\nany schema variety. First, we introduce two naive schema-agnostic methods,\nshowing that straightforward solutions exhibit a poor performance that does not\nscale well to large volumes of data. Then, we propose four different advanced\nmethods. Through an extensive experimental evaluation over 7 real-world,\nestablished datasets, we show that all the advanced methods outperform to a\nsignificant extent both the na\\\"ive and the state-of-the-art schema-based ones.\nWe also investigate the relative performance of the advanced methods, providing\nguidelines on the method selection.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 18:51:59 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Simonini", "Giovanni", ""], ["Papadakis", "George", ""], ["Palpanas", "Themis", ""], ["Bergamaschi", "Sonia", ""]]}, {"id": "1905.06397", "submitter": "George Papadakis", "authors": "Vassilis Christophides, Vasilis Efthymiou, Themis Palpanas, George\n  Papadakis, Kostas Stefanidis", "title": "End-to-End Entity Resolution for Big Data: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most important tasks for improving data quality and the\nreliability of data analytics results is Entity Resolution (ER). ER aims to\nidentify different descriptions that refer to the same real-world entity, and\nremains a challenging problem. While previous works have studied specific\naspects of ER (and mostly in traditional settings), in this survey, we provide\nfor the first time an end-to-end view of modern ER workflows, and of the novel\naspects of entity indexing and matching methods in order to cope with more than\none of the Big Data characteristics simultaneously. We present the basic\nconcepts, processing steps and execution strategies that have been proposed by\ndifferent communities, i.e., database, semantic Web and machine learning, in\norder to cope with the loose structuredness, extreme diversity, high speed and\nlarge scale of entity descriptions used by real-world applications. Finally, we\nprovide a synthetic discussion of the existing approaches, and conclude with a\ndetailed presentation of open research directions.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 19:15:11 GMT"}, {"version": "v2", "created": "Sat, 2 Nov 2019 02:55:01 GMT"}, {"version": "v3", "created": "Wed, 19 Aug 2020 16:38:49 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Christophides", "Vassilis", ""], ["Efthymiou", "Vasilis", ""], ["Palpanas", "Themis", ""], ["Papadakis", "George", ""], ["Stefanidis", "Kostas", ""]]}, {"id": "1905.06425", "submitter": "Jennifer Ortiz", "authors": "Jennifer Ortiz, Magdalena Balazinska, Johannes Gehrke and S. Sathiya\n  Keerthi", "title": "An Empirical Analysis of Deep Learning for Cardinality Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We implement and evaluate deep learning for cardinality estimation by\nstudying the accuracy, space and time trade-offs across several architectures.\nWe find that simple deep learning models can learn cardinality estimations\nacross a variety of datasets (reducing the error by 72% - 98% on average\ncompared to PostgreSQL). In addition, we empirically evaluate the impact of\ninjecting cardinality estimates produced by deep learning models into the\nPostgreSQL optimizer. In many cases, the estimates from these models lead to\nbetter query plans across all datasets, reducing the runtimes by up to 49% on\nselect-project-join workloads. As promising as these models are, we also\ndiscuss and address some of the challenges of using them in practice.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 20:30:44 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 00:00:02 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Ortiz", "Jennifer", ""], ["Balazinska", "Magdalena", ""], ["Gehrke", "Johannes", ""], ["Keerthi", "S. Sathiya", ""]]}, {"id": "1905.06480", "submitter": "Rafael S. Gon\\c{c}alves", "authors": "Rafael S. Gon\\c{c}alves, Martin J. O'Connor, Marcos Mart\\'inez-Romero,\n  Attila L. Egyedi, Debra Willrett, John Graybeal and Mark A. Musen", "title": "The CEDAR Workbench: An Ontology-Assisted Environment for Authoring\n  Metadata that Describe Scientific Experiments", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-68204-4_10", "report-no": null, "categories": "cs.DB cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Center for Expanded Data Annotation and Retrieval (CEDAR) aims to\nrevolutionize the way that metadata describing scientific experiments are\nauthored. The software we have developed--the CEDAR Workbench--is a suite of\nWeb-based tools and REST APIs that allows users to construct metadata\ntemplates, to fill in templates to generate high-quality metadata, and to share\nand manage these resources. The CEDAR Workbench provides a versatile,\nREST-based environment for authoring metadata that are enriched with terms from\nontologies. The metadata are available as JSON, JSON-LD, or RDF for easy\nintegration in scientific applications and reusability on the Web. Users can\nleverage our APIs for validating and submitting metadata to external\nrepositories. The CEDAR Workbench is freely available and open-source.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 00:19:49 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Gon\u00e7alves", "Rafael S.", ""], ["O'Connor", "Martin J.", ""], ["Mart\u00ednez-Romero", "Marcos", ""], ["Egyedi", "Attila L.", ""], ["Willrett", "Debra", ""], ["Graybeal", "John", ""], ["Musen", "Mark A.", ""]]}, {"id": "1905.06520", "submitter": "Dumitrel Loghin", "authors": "Dumitrel Loghin, Gang Chen, Tien Tuan Anh Dinh, Beng Chin Ooi, Yong\n  Meng Teo", "title": "Blockchain Goes Green? An Analysis of Blockchain on Low-Power Nodes", "comments": "17 pages, 13 pages paper, 4 pages appendix, 20 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.ET cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motivated by the massive energy usage of blockchain, on the one hand, and by\nsignificant performance improvements in low-power, wimpy systems, on the other\nhand, we perform an in-depth time-energy analysis of blockchain systems on\nlow-power nodes in comparison to high-performance nodes. We use three low-power\nsystems to represent a wide range of the performance-power spectrum, while\ncovering both x86/64 and ARM architectures. We show that low-end wimpy nodes\nare struggling to run full-fledged blockchains mainly due to their small and\nlow-bandwidth memory. On the other hand, wimpy systems with balanced\nperformance-to-power ratio achieve reasonable performance while saving\nsignificant amounts of energy. For example, Jetson TX2 nodes achieve around 80%\nand 30% of the throughput of Parity and Hyperledger, respectively, while using\n18x and 23x less energy compared to traditional brawny servers with Intel Xeon\nCPU.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 04:21:04 GMT"}, {"version": "v2", "created": "Mon, 17 Jun 2019 06:21:18 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Loghin", "Dumitrel", ""], ["Chen", "Gang", ""], ["Dinh", "Tien Tuan Anh", ""], ["Ooi", "Beng Chin", ""], ["Teo", "Yong Meng", ""]]}, {"id": "1905.06720", "submitter": "Yang Shi", "authors": "Yang Shi, Yuyin Liu, Hanghang Tong, Jingrui He, Gang Yan, and Nan Cao", "title": "Visual Analytics of Anomalous User Behaviors: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.DB cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing accessibility of data provides substantial opportunities for\nunderstanding user behaviors. Unearthing anomalies in user behaviors is of\nparticular importance as it helps signal harmful incidents such as network\nintrusions, terrorist activities, and financial frauds. Many visual analytics\nmethods have been proposed to help understand user behavior-related data in\nvarious application domains. In this work, we survey the state of art in visual\nanalytics of anomalous user behaviors and classify them into four categories\nincluding social interaction, travel, network communication, and transaction.\nWe further examine the research works in each category in terms of data types,\nanomaly detection techniques, and visualization techniques, and interaction\nmethods. Finally, we discuss the findings and potential research directions.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 03:58:31 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 08:37:27 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Shi", "Yang", ""], ["Liu", "Yuyin", ""], ["Tong", "Hanghang", ""], ["He", "Jingrui", ""], ["Yan", "Gang", ""], ["Cao", "Nan", ""]]}, {"id": "1905.06760", "submitter": "Lucas Lersch", "authors": "Lucas Lersch, Wolfgang Lehner, Ismail Oukid", "title": "Persistent Buffer Management with Optimistic Consistency", "comments": null, "journal-ref": "Proceedings of the 15th International Workshop on Data Management\n  on New Hardware, Article No. 14 (2019) 1-3", "doi": "10.1145/3329785.3329931", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the best way to leverage non-volatile memory (NVM) on modern database\nsystems is still an open problem. The answer is far from trivial since the\nclear boundary between memory and storage present in most systems seems to be\nincompatible with the intrinsic memory-storage duality of NVM. Rather than\ntreating NVM either solely as memory or solely as storage, in this work we\npropose how NVM can be simultaneously used as both in the context of modern\ndatabase systems. We design a persistent buffer pool on NVM, enabling pages to\nbe directly read/written by the CPU (like memory) while recovering corrupted\npages after a failure (like storage). The main benefits of our approach are an\neasy integration in the existing database architectures, reduced costs (by\nreplacing DRAM with NVM), and faster peak-performance recovery.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 14:02:29 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Lersch", "Lucas", ""], ["Lehner", "Wolfgang", ""], ["Oukid", "Ismail", ""]]}, {"id": "1905.06900", "submitter": "Nicolas Le Scouarnec", "authors": "Fabien Andr\\'e, Anne-Marie Kermarrec, Nicolas Le Scouarnec", "title": "Derived Codebooks for High-Accuracy Nearest Neighbor Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional Nearest Neighbor (NN) search is central in multimedia search\nsystems. Product Quantization (PQ) is a widespread NN search technique which\nhas a high performance and good scalability. PQ compresses high-dimensional\nvectors into compact codes thanks to a combination of quantizers. Large\ndatabases can, therefore, be stored entirely in RAM, enabling fast responses to\nNN queries. In almost all cases, PQ uses 8-bit quantizers as they offer low\nresponse times. In this paper, we advocate the use of 16-bit quantizers.\nCompared to 8-bit quantizers, 16-bit quantizers boost accuracy but they\nincrease response time by a factor of 3 to 10. We propose a novel approach that\nallows 16-bit quantizers to offer the same response time as 8-bit quantizers,\nwhile still providing a boost of accuracy. Our approach builds on two key\nideas: (i) the construction of derived codebooks that allow a fast and\napproximate distance evaluation, and (ii) a two-pass NN search procedure which\nbuilds a candidate set using the derived codebooks, and then refines it using\n16-bit quantizers. On 1 billion SIFT vectors, with an inverted index, our\napproach offers a Recall@100 of 0.85 in 5.2 ms. By contrast, 16-bit quantizers\nalone offer a Recall@100 of 0.85 in 39 ms, and 8-bit quantizers a Recall@100 of\n0.82 in 3.8 ms.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 16:47:30 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Andr\u00e9", "Fabien", ""], ["Kermarrec", "Anne-Marie", ""], ["Scouarnec", "Nicolas Le", ""]]}, {"id": "1905.06946", "submitter": "Chao Yan", "authors": "Chao Yan, Haifeng Xu, Yevgeniy Vorobeychik, Bo Li, Daniel Fabbri,\n  Bradley Malin", "title": "To Warn or Not to Warn: Online Signaling in Audit Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Routine operational use of sensitive data is often governed by law and\nregulation. For instance, in the medical domain, there are various statues at\nthe state and federal level that dictate who is permitted to work with\npatients' records and under what conditions. To screen for potential privacy\nbreaches, logging systems are usually deployed to trigger alerts whenever\nsuspicious access is detected. However, such mechanisms are often inefficient\nbecause 1) the vast majority of triggered alerts are false positives, 2) small\nbudgets make it unlikely that a real attack will be detected, and 3) attackers\ncan behave strategically, such that traditional auditing mechanisms cannot\neasily catch them. To improve efficiency, information systems may invoke\nsignaling, so that whenever a suspicious access request occurs, the system can,\nin real time, warn the user that the access may be audited. Then, at the close\nof a finite period, a selected subset of suspicious accesses are audited. This\ngives rise to an online problem in which one needs to determine 1) whether a\nwarning should be triggered and 2) the likelihood that the data request event\nwill be audited. In this paper, we formalize this auditing problem as a\nSignaling Audit Game (SAG), in which we model the interactions between an\nauditor and an attacker in the context of signaling and the usability cost is\nrepresented as a factor of the auditor's payoff. We study the properties of its\nStackelberg equilibria and develop a scalable approach to compute its solution.\nWe show that a strategic presentation of warnings adds value in that SAGs\nrealize significantly higher utility for the auditor than systems without\nsignaling. We illustrate the value of the proposed auditing model and the\nconsistency of its advantages over existing baseline methods.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 05:08:51 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2019 20:03:23 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Yan", "Chao", ""], ["Xu", "Haifeng", ""], ["Vorobeychik", "Yevgeniy", ""], ["Li", "Bo", ""], ["Fabbri", "Daniel", ""], ["Malin", "Bradley", ""]]}, {"id": "1905.07113", "submitter": "Ye Zhu", "authors": "Ye Zhu", "title": "High Throughput Push Based Storage Manager", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The storage manager, as a key component of the database system, is\nresponsible for organizing, reading, and delivering data to the execution\nengine for processing. According to the data serving mechanism, existing\nstorage managers are either pull-based, incurring high latency, or push-based,\nleading to a high number of I/O requests when the CPU is busy. To improve these\nshortcomings, this thesis proposes a push-based prefetching strategy in a\ncolumn-wise storage manager. The proposed strategy implements an efficient\ncache layer to store shared data among queries to reduce the number of I/O\nrequests. The capacity of the cache is maintained by a time access-aware\neviction mechanism. Our strategy enables the storage manager to coordinate\nmultiple queries by merging their requests and dynamically generate an optimal\nread order that maximizes the overall I/O throughput. We evaluated our storage\nmanager both over a disk-based redundant array of independent disks (RAID) and\nan NVM Express (NVMe) solid-state drive (SSD). With the high read performance\nof the SSD, we successfully minimized the total read time and number of I/O\naccesses.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 04:50:53 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Zhu", "Ye", ""]]}, {"id": "1905.07167", "submitter": "Patrick Valduriez", "authors": "Renan Souza (UFRJ), V\\'itor Silva (UFRJ), Jos\\'e Camata (UFIJ), Alvaro\n  Coutinho (UFRJ), Patrick Valduriez (ZENITH), Marta Mattoso (UFRJ)", "title": "Keeping Track of User Steering Actions in Dynamic Workflows", "comments": null, "journal-ref": "Future Generation Computer Systems, Elsevier, 2019, 99, pp.624-643", "doi": "10.1016/j.future.2019.05.011", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In long-lasting scientific workflow executions in HPC machines, computational\nscientists (the users in this work) often need to fine-tune several workflow\nparameters. These tunings are done through user steering actions that may\nsignificantly improve performance (e.g., reduce execution time) or improve the\noverall results. However, in executions that last for weeks, users can lose\ntrack of what has been adapted if the tunings are not properly registered. In\nthis work, we build on provenance data management to address the problem of\ntracking online parameter fine-tuning in dynamic workflows steered by users. We\npropose a lightweight solution to capture and manage provenance of the steering\nactions online with negligible overhead. The resulting provenance database\nrelates tuning data with data for domain, dataflow provenance, execution, and\nperformance, and is available for analysis at runtime. We show how users may\nget a detailed view of the execution, providing insights to determine when and\nhow to tune. We discuss the applicability of our solution in different domains\nand validate its ability to allow for online capture and analyses of parameter\nfine-tunings in a real workflow in the Oil and Gas industry. In this\nexperiment, the user could determine which tuned parameters influenced\nsimulation accuracy and performance. The observed overhead for keeping track of\nuser steering actions at runtime is less than 1% of total execution time.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 08:48:08 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Souza", "Renan", "", "UFRJ"], ["Silva", "V\u00edtor", "", "UFRJ"], ["Camata", "Jos\u00e9", "", "UFIJ"], ["Coutinho", "Alvaro", "", "UFRJ"], ["Valduriez", "Patrick", "", "ZENITH"], ["Mattoso", "Marta", "", "UFRJ"]]}, {"id": "1905.07169", "submitter": "Shuaifeng Pang", "authors": "Shuaifeng Pang, Xiaodong Qi, Zhao Zhang, Cheqing Jin, Aoying Zhou", "title": "Concurrency Protocol Aiming at High Performance of Execution and Replay\n  for Smart Contracts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the emergence of the programmable smart contract makes blockchain\nsystems easily embrace a wider range of industrial areas, how to execute smart\ncontracts efficiently becomes a big challenge nowadays. Due to the existence of\nByzantine nodes, the mechanism of executing smart contracts is quite different\nfrom that in database systems, so that existing successful concurrency control\nprotocols in database systems cannot be employed directly. Moreover, even\nthough smart contract execution follows a two-phase style, i.e, the miner node\nexecutes a batch of smart contracts in the first phase and the validators\nreplay them in the second phase, existing parallel solutions only focus on the\noptimization in the first phase, but not including the second phase.\n  In this paper, we propose a novel efficient concurrency control scheme which\nis the first one to do optimization in both phases. Specifically, (i) in the\nfirst phase, we give a variant of OCC (Optimistic Concurrency Control) protocol\nbased on {\\em batching} feature to improve the concurrent execution efficiency\nfor the miner and produce a schedule log with high parallelism for validators.\nAlso, a graph partition algorithm is devised to divide the original schedule\nlog into small pieces and further reduce the communication cost; and (ii) in\nthe second phase, we give a deterministic OCC protocol to replay all smart\ncontracts efficiently on multi-core validators where all cores can replay smart\ncontracts independently. Theoretical analysis and extensive experimental\nresults illustrate that the proposed scheme outperforms state-of-art solutions\nsignificantly.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 08:58:57 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Pang", "Shuaifeng", ""], ["Qi", "Xiaodong", ""], ["Zhang", "Zhao", ""], ["Jin", "Cheqing", ""], ["Zhou", "Aoying", ""]]}, {"id": "1905.07408", "submitter": "EPTCS", "authors": "Giovanni de Felice (University of Oxford), Konstantinos Meichanetzidis\n  (University of Oxford, Cambridge Quantum Computing Ltd.), Alexis Toumi\n  (University of Oxford)", "title": "Functorial Question Answering", "comments": "In Proceedings ACT 2019, arXiv:2009.06334", "journal-ref": "EPTCS 323, 2020, pp. 84-94", "doi": "10.4204/EPTCS.323.6", "report-no": null, "categories": "cs.CL cs.DB cs.LO math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributional compositional (DisCo) models are functors that compute the\nmeaning of a sentence from the meaning of its words. We show that DisCo models\nin the category of sets and relations correspond precisely to relational\ndatabases. As a consequence, we get complexity-theoretic reductions from\nsemantics and entailment of a fragment of natural language to evaluation and\ncontainment of conjunctive queries, respectively. Finally, we define question\nanswering as an NP-complete problem.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 15:23:39 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2019 14:35:12 GMT"}, {"version": "v3", "created": "Tue, 15 Sep 2020 02:14:18 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["de Felice", "Giovanni", "", "University of Oxford"], ["Meichanetzidis", "Konstantinos", "", "University of Oxford, Cambridge Quantum Computing Ltd."], ["Toumi", "Alexis", "", "University of Oxford"]]}, {"id": "1905.07663", "submitter": "Anuj Singh", "authors": "Anuj Singh", "title": "Regions In a Linked Dataset For Change Detection", "comments": "It is a doctoral consortium paper, which was accepted in ISWC 2018\n  but was not published there as the author was not able to attend the\n  conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linked Datasets (LDs) are constantly evolving and the applications using a\nLinked Dataset (LD) may face several issues such as outdated data or broken\ninterlinks due to evolution of the dataset. To overcome these issues, the\ndetection of changes in LDs during their evolution has proven crucial. As LDs\nevolve frequently, the change detection during the evolution should also be\ndone at frequent intervals. However, due to limitation of available\ncomputational resources such as capacity to fetch data from LD and time to\ndetect changes, the frequent change detection may not be possible with existing\nchange detection techniques. This research proposes to explore the notion of\nprioritization of regions (subsets) in LDs for change detection with the aim of\nachieving optimal accuracy and efficient use of available computational\nresources. This will facilitate the detection of changes in an evolving LD at\nfrequent intervals and will allow the applications to update their data closest\nto real-time data.\n", "versions": [{"version": "v1", "created": "Sun, 19 May 2019 00:01:05 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Singh", "Anuj", ""]]}, {"id": "1905.08021", "submitter": "Oren Kalinsky", "authors": "Oren Kalinsky, Benny Kimelfeld, Yoav Etsion", "title": "The TrieJax Architecture: Accelerating Graph Operations Through\n  Relational Joins", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph pattern matching (e.g., finding all cycles and cliques) has become an\nimportant component in many critical domains such as social networks, biology,\nand cyber-security. This development motivated research to develop faster\nalgorithms that target graph pattern matching. In recent years, the database\ncommunity has shown that mapping graph pattern matching problems to a new class\nof relational join algorithms provides an efficient framework for computing\nthese problems.\n  In this paper, we argue that this new class of relational join algorithms is\nhighly amenable to specialized hardware acceleration thanks to two fundamental\nproperties: improved locality and inherent concurrency. The improved locality\nis a result of the provably bound number of intermediate results these\nalgorithms generate, which results in smaller working sets. In addition, their\ninherent concurrency can be leveraged for effective hardware acceleration and\nhiding memory latency. We demonstrate the hardware amenability of this new\nclass of algorithms by introducing TrieJax, a hardware accelerator for graph\npattern matching. The TrieJax design leverages the improved locality and high\nconcurrency properties to dramatically accelerate graph pattern matching, and\ncan be tightly integrated into existing manycore processors.\n  We evaluate TrieJax on a set standard graph pattern matching queries and\ndatasets. Our evaluation shows that TrieJax outperforms recently proposed\nhardware accelerators for graph and database processing that do not employ the\nnew class of algorithms by 7-63x on average (up to 539x), while consuming\n15-179x less energy (up to 1750x). systems that do incorporate modern\nrelational join algorithms by 9-20x on average (up to 45x), while consuming\n59-110x less energy (up to 372x).\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 10:59:26 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Kalinsky", "Oren", ""], ["Kimelfeld", "Benny", ""], ["Etsion", "Yoav", ""]]}, {"id": "1905.08320", "submitter": "Tianhao Wang", "authors": "Tianhao Wang, Milan Lopuha\\\"a-Zwakenberg, Zitao Li, Boris Skoric,\n  Ninghui Li", "title": "Locally Differentially Private Frequency Estimation with Consistency", "comments": "NDSS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local Differential Privacy (LDP) protects user privacy from the data\ncollector. LDP protocols have been increasingly deployed in the industry. A\nbasic building block is frequency oracle (FO) protocols, which estimate\nfrequencies of values. While several FO protocols have been proposed, the\ndesign goal does not lead to optimal results for answering many queries. In\nthis paper, we show that adding post-processing steps to FO protocols by\nexploiting the knowledge that all individual frequencies should be non-negative\nand they sum up to one can lead to significantly better accuracy for a wide\nrange of tasks, including frequencies of individual values, frequencies of the\nmost frequent values, and frequencies of subsets of values. We consider 10\ndifferent methods that exploit this knowledge differently. We establish\ntheoretical relationships between some of them and conducted extensive\nexperimental evaluations to understand which methods should be used for\ndifferent query tasks.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 19:49:52 GMT"}, {"version": "v2", "created": "Wed, 29 Jan 2020 21:56:31 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Wang", "Tianhao", ""], ["Lopuha\u00e4-Zwakenberg", "Milan", ""], ["Li", "Zitao", ""], ["Skoric", "Boris", ""], ["Li", "Ninghui", ""]]}, {"id": "1905.08337", "submitter": "Subhasis Dasgupta", "authors": "Subhasis Dasgupta, Aditya Bagchi and Amarnath Gupta", "title": "Ingesting High-Velocity Streaming Graphs from Social Media Sources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many data science applications like social network analysis use graphs as\ntheir primary form of data. However, acquiring graph-structured data from\nsocial media presents some interesting challenges. The first challenge is the\nhigh data velocity and bursty nature of the social media data. The second\nchallenge is that the complex nature of the data makes the ingestion process\nexpensive. If we want to store the streaming graph data in a graph database, we\nface a third challenge -- the database is very often unable to sustain the\ningestion of high-velocity, high-burst data. We have developed an adaptive\nbuffering mechanism and a graph compression technique that effectively\nmitigates the problem. A novel aspect of our method is that the adaptive\nbuffering algorithm uses the data rate, the data content as well as the CPU\nresources of the database machine to determine an optimal data ingestion\nmechanism. We further show that an ingestion-time graph-compression strategy\nimproves the efficiency of the data ingestion into the database. We have\nverified the efficacy of our ingestion optimization strategy through extensive\nexperiments.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 20:29:44 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Dasgupta", "Subhasis", ""], ["Bagchi", "Aditya", ""], ["Gupta", "Amarnath", ""]]}, {"id": "1905.08847", "submitter": "Ovidiu Vaduvescu", "authors": "Ovidiu Vaduvescu, Lucian Curelaru, Marcel Popescu", "title": "Mega-Archive and the EURONEAR Tools for Datamining World Astronomical\n  Images", "comments": "Paper submitted to Astronomy and Computing (25 Mar 2019)", "journal-ref": "Astronomy and Computing, 2019", "doi": "10.1016/j.ascom.2019.100356", "report-no": "Accepted, available online 12 Dec 2019", "categories": "astro-ph.IM cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The world astronomical image archives represent huge opportunities to\ntime-domain astronomy sciences and other hot topics such as space defense, and\nastronomical observatories should improve this wealth and make it more\naccessible in the big data era. In 2010 we introduced the Mega-Archive database\nand the Mega-Precovery server for data mining images containing Solar system\nbodies, with focus on near Earth asteroids (NEAs). This paper presents the\nimprovements and introduces some new related data mining tools developed during\nthe last five years. Currently, the Mega-Archive has indexed 15 million images\navailable from six major collections (CADC, ESO, ING, LCOGT, NVO and SMOKA) and\nother instrument archives and surveys. This meta-data index collection is daily\nupdated (since 2014) by a crawler which performs automated query of five major\ncollections. Since 2016, these data mining tools run to the new dedicated\nEURONEAR server, and the database migrated to SQL engine which supports robust\nand fast queries. To constrain the area to search moving or fixed objects in\nimages taken by large mosaic cameras, we built the graphical tools FindCCD and\nFindCCD for Fixed Objects which overlay the targets across one of seven mosaic\ncameras (Subaru-SuprimeCam, VST-OmegaCam, INT-WFC, VISTA-VIRCAM, CFHT-MegaCam,\nBlanco-DECam and Subaru-HSC), also plotting the uncertainty ellipse for poorly\nobserved NEAs. In 2017 we improved Mega-Precovery, which offers now two options\nfor calculus of the ephemerides and three options for the input (objects\ndefined by designation, orbit or observations). Additionally, we developed\nMega-Archive for Fixed Objects (MASFO) and Mega-Archive Search for Double Stars\n(MASDS). We believe that the huge potential of science imaging archives is\nstill insufficiently exploited.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 19:56:55 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Vaduvescu", "Ovidiu", ""], ["Curelaru", "Lucian", ""], ["Popescu", "Marcel", ""]]}, {"id": "1905.08898", "submitter": "Umar Farooq Minhas", "authors": "Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do, Yinan\n  Li, Hantian Zhang, Badrish Chandramouli, Johannes Gehrke, Donald Kossmann,\n  David Lomet, Tim Kraska", "title": "ALEX: An Updatable Adaptive Learned Index", "comments": null, "journal-ref": null, "doi": "10.1145/3318464.3389711", "report-no": "MSR-TR-2020-12", "categories": "cs.DB cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on \"learned indexes\" has changed the way we look at the\ndecades-old field of DBMS indexing. The key idea is that indexes can be thought\nof as \"models\" that predict the position of a key in a dataset. Indexes can,\nthus, be learned. The original work by Kraska et al. shows that a learned index\nbeats a B+Tree by a factor of up to three in search time and by an order of\nmagnitude in memory footprint. However, it is limited to static, read-only\nworkloads.\n  In this paper, we present a new learned index called ALEX which addresses\npractical issues that arise when implementing learned indexes for workloads\nthat contain a mix of point lookups, short range queries, inserts, updates, and\ndeletes. ALEX effectively combines the core insights from learned indexes with\nproven storage and indexing techniques to achieve high performance and low\nmemory footprint. On read-only workloads, ALEX beats the learned index from\nKraska et al. by up to 2.2X on performance with up to 15X smaller index size.\nAcross the spectrum of read-write workloads, ALEX beats B+Trees by up to 4.1X\nwhile never performing worse, with up to 2000X smaller index size. We believe\nALEX presents a key step towards making learned indexes practical for a broader\nclass of database workloads with dynamic updates.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 23:22:01 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 03:12:24 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Ding", "Jialin", ""], ["Minhas", "Umar Farooq", ""], ["Yu", "Jia", ""], ["Wang", "Chi", ""], ["Do", "Jaeyoung", ""], ["Li", "Yinan", ""], ["Zhang", "Hantian", ""], ["Chandramouli", "Badrish", ""], ["Gehrke", "Johannes", ""], ["Kossmann", "Donald", ""], ["Lomet", "David", ""], ["Kraska", "Tim", ""]]}, {"id": "1905.09251", "submitter": "Murali Mani", "authors": "Murali Mani, Naveenkumar Singaraj and Zhenyan Liu", "title": "IPAW 2020 Preprint: Efficient Computation of Provenance for Query Result\n  Exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users typically interact with a database by asking queries and examining the\nresults. We refer to the user examining the query results and asking follow-up\nquestions as query result exploration. Our work builds on two decades of\nprovenance research useful for query result exploration. Three approaches for\ncomputing provenance have been described in the literature: lazy, eager, and\nhybrid. We investigate lazy and eager approaches that utilize constraints that\nwe have identified in the context of query result exploration, as well as novel\nhybrid approaches. For the TPC-H benchmark, these constraints are applicable to\n19 out of the 22 queries, and result in a better performance for all queries\nthat have a join. Furthermore, the performance benefits from our approaches are\nsignificant, sometimes several orders of magnitude.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 17:14:20 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2020 19:21:55 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Mani", "Murali", ""], ["Singaraj", "Naveenkumar", ""], ["Liu", "Zhenyan", ""]]}, {"id": "1905.09359", "submitter": "Mohammad Javad Amiri", "authors": "Victor Zakhary, Mohammad Javad Amiri, Sujaya Maiyya, Divyakant\n  Agrawal, Amr El Abbadi", "title": "Towards Global Asset Management in Blockchain Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Permissionless blockchains (e.g., Bitcoin, Ethereum, etc) have shown a wide\nsuccess in implementing global scale peer-to-peer cryptocurrency systems. In\nsuch blockchains, new currency units are generated through the mining process\nand are used in addition to transaction fees to incentivize miners to maintain\nthe blockchain. Although it is clear how currency units are generated and\ntransacted on, it is unclear how to use the infrastructure of permissionless\nblockchains to manage other assets than the blockchain's currency units (e.g.,\ncars, houses, etc). In this paper, we propose a global asset management system\nby unifying permissioned and permissionless blockchains. A governmental\npermissioned blockchain authenticates the registration of end-user assets\nthrough smart contract deployments on a permissionless blockchain. Afterwards,\nend-users can transact on their assets through smart contract function calls\n(e.g., sell a car, rent a room in a house, etc). In return, end-users get paid\nin currency units of the same blockchain or other blockchains through atomic\ncross-chain transactions and governmental offices receive taxes on these\ntransactions in cryptocurrency units.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 20:44:36 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Zakhary", "Victor", ""], ["Amiri", "Mohammad Javad", ""], ["Maiyya", "Sujaya", ""], ["Agrawal", "Divyakant", ""], ["Abbadi", "Amr El", ""]]}, {"id": "1905.09624", "submitter": "Timo Bingmann", "authors": "Timo Bingmann, Phelim Bradley, Florian Gauger, and Zamin Iqbal", "title": "COBS: a Compact Bit-Sliced Signature Index", "comments": "To appear in 26th International Symposium on String Processing and\n  Information Retrieval (SPIRE'19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present COBS, a COmpact Bit-sliced Signature index, which is a cross-over\nbetween an inverted index and Bloom filters. Our target application is to index\n$k$-mers of DNA samples or $q$-grams from text documents and process\napproximate pattern matching queries on the corpus with a user-chosen coverage\nthreshold. Query results may contain a number of false positives which\ndecreases exponentially with the query length. We compare COBS to seven other\nindex software packages on 100000 microbial DNA samples. COBS' compact but\nsimple data structure outperforms the other indexes in construction time and\nquery performance with Mantis by Pandey et al. in second place. However, unlike\nMantis and other previous work, COBS does not need the complete index in RAM\nand is thus designed to scale to larger document sets.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 12:49:58 GMT"}, {"version": "v2", "created": "Fri, 26 Jul 2019 13:00:54 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Bingmann", "Timo", ""], ["Bradley", "Phelim", ""], ["Gauger", "Florian", ""], ["Iqbal", "Zamin", ""]]}, {"id": "1905.09848", "submitter": "Stijn Vansummeren", "authors": "Muhammad Idris and Mart\\'in Ugarte and Stijn Vansummeren and Hannes\n  Voigt and Wolfgang Lehner", "title": "Conjunctive Queries with Theta Joins Under Updates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern application domains such as Composite Event Recognition (CER) and\nreal-time Analytics require the ability to dynamically refresh query results\nunder high update rates. Traditional approaches to this problem are based\neither on the materialization of subresults (to avoid their recomputation) or\non the recomputation of subresults (to avoid the space overhead of\nmaterialization). Both techniques have recently been shown suboptimal: instead\nof materializing results and subresults, one can maintain a data structure that\nsupports efficient maintenance under updates and can quickly enumerate the full\nquery output, as well as the changes produced under single updates.\nUnfortunately, these data structures have been developed only for\naggregate-join queries composed of equi-joins, limiting their applicability in\ndomains such as CER where temporal joins are commonplace. In this paper, we\npresent a new approach for dynamically evaluating queries with multi-way\ntheta-joins under updates that is effective in avoiding both materialization\nand recomputation of results, while supporting a wide range of applications. To\ndo this we generalize Dynamic Yannakakis, an algorithm for dynamically\nprocessing acyclic equi-join queries. In tandem, and of independent interest,\nwe generalize the notions of acyclicity and free-connexity to arbitrary\ntheta-joins and show how to compute corresponding join trees. We instantiate\nour framework to the case where theta-joins are only composed of equalities and\ninequalities and experimentally compare our algorithm to state of the art CER\nsystems as well as incremental view maintenance engines. Our approach performs\nconsistently better than the competitor systems with up to two orders of\nmagnitude improvements in both time and memory consumption.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 18:08:10 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Idris", "Muhammad", ""], ["Ugarte", "Mart\u00edn", ""], ["Vansummeren", "Stijn", ""], ["Voigt", "Hannes", ""], ["Lehner", "Wolfgang", ""]]}, {"id": "1905.10336", "submitter": "Rekha Singhal Dr.", "authors": "Rekha Singhal, Nathan Zhang, Luigi Nardi, Muhammad Shahbaz, Kunle\n  Olukotun", "title": "Polystore++: Accelerated Polystore System for Heterogeneous Workloads", "comments": "11 pages, Accepted in ICDCS 2019", "journal-ref": "ICDCS 2019", "doi": null, "report-no": null, "categories": "cs.AR cs.DB cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern real-time business analytic consist of heterogeneous workloads (e.g,\ndatabase queries, graph processing, and machine learning). These analytic\napplications need programming environments that can capture all aspects of the\nconstituent workloads (including data models they work on and movement of data\nacross processing engines). Polystore systems suit such applications; however,\nthese systems currently execute on CPUs and the slowdown of Moore's Law means\nthey cannot meet the performance and efficiency requirements of modern\nworkloads. We envision Polystore++, an architecture to accelerate existing\npolystore systems using hardware accelerators (e.g, FPGAs, CGRAs, and GPUs).\nPolystore++ systems can achieve high performance at low power by identifying\nand offloading components of a polystore system that are amenable to\nacceleration using specialized hardware. Building a Polystore++ system is\nchallenging and introduces new research problems motivated by the use of\nhardware accelerators (e.g, optimizing and mapping query plans across\nheterogeneous computing units and exploiting hardware pipelining and\nparallelism to improve performance). In this paper, we discuss these challenges\nin detail and list possible approaches to address these problems.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 17:01:36 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Singhal", "Rekha", ""], ["Zhang", "Nathan", ""], ["Nardi", "Luigi", ""], ["Shahbaz", "Muhammad", ""], ["Olukotun", "Kunle", ""]]}, {"id": "1905.10482", "submitter": "Subhasis Dasgupta", "authors": "Junan Guo, Subhasis Dasgupta and Amarnath Gupta", "title": "Multi-Model Investigative Exploration of Social Media Data with\n  boutique: A Case Study in Public Health", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.HC cs.IR cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present our experience with a data science problem in Public Health, where\nresearchers use social media (Twitter) to determine whether the public shows\nawareness of HIV prevention measures offered by Public Health campaigns. To\nhelp the researcher, we develop an investigative exploration system called\nboutique that allows a user to perform a multi-step visualization and\nexploration of data through a dashboard interface. Unique features of boutique\nincludes its ability to handle heterogeneous types of data provided by a\npolystore, and its ability to use computation as part of the investigative\nexploration process. In this paper, we present the design of the boutique\nmiddleware and walk through an investigation process for a real-life problem.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 23:34:48 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Guo", "Junan", ""], ["Dasgupta", "Subhasis", ""], ["Gupta", "Amarnath", ""]]}, {"id": "1905.10688", "submitter": "\\c{C}a\\u{g}atay Demiralp", "authors": "Madelon Hulsebos and Kevin Hu and Michiel Bakker and Emanuel Zgraggen\n  and Arvind Satyanarayan and Tim Kraska and \\c{C}a\\u{g}atay Demiralp and\n  C\\'esar Hidalgo", "title": "Sherlock: A Deep Learning Approach to Semantic Data Type Detection", "comments": "KDD'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correctly detecting the semantic type of data columns is crucial for data\nscience tasks such as automated data cleaning, schema matching, and data\ndiscovery. Existing data preparation and analysis systems rely on dictionary\nlookups and regular expression matching to detect semantic types. However,\nthese matching-based approaches often are not robust to dirty data and only\ndetect a limited number of types. We introduce Sherlock, a multi-input deep\nneural network for detecting semantic types. We train Sherlock on $686,765$\ndata columns retrieved from the VizNet corpus by matching $78$ semantic types\nfrom DBpedia to column headers. We characterize each matched column with\n$1,588$ features describing the statistical properties, character\ndistributions, word embeddings, and paragraph vectors of column values.\nSherlock achieves a support-weighted F$_1$ score of $0.89$, exceeding that of\nmachine learning baselines, dictionary and regular expression benchmarks, and\nthe consensus of crowdsourced annotations.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 22:36:05 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Hulsebos", "Madelon", ""], ["Hu", "Kevin", ""], ["Bakker", "Michiel", ""], ["Zgraggen", "Emanuel", ""], ["Satyanarayan", "Arvind", ""], ["Kraska", "Tim", ""], ["Demiralp", "\u00c7a\u011fatay", ""], ["Hidalgo", "C\u00e9sar", ""]]}, {"id": "1905.10989", "submitter": "Simon Razniewski", "authors": "Julien Romero, Simon Razniewski, Koninika Pal, Jeff Z. Pan, Archit\n  Sakhadeo, Gerhard Weikum", "title": "Commonsense Properties from Query Logs and Question Answering Forums", "comments": "Updated appendix reporting on Quasimodo v4.3 (2/2021)", "journal-ref": "CIKM 2019", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commonsense knowledge about object properties, human behavior and general\nconcepts is crucial for robust AI applications. However, automatic acquisition\nof this knowledge is challenging because of sparseness and bias in online\nsources. This paper presents Quasimodo, a methodology and tool suite for\ndistilling commonsense properties from non-standard web sources. We devise\nnovel ways of tapping into search-engine query logs and QA forums, and\ncombining the resulting candidate assertions with statistical cues from\nencyclopedias, books and image tags in a corroboration step. Unlike prior work\non commonsense knowledge bases, Quasimodo focuses on salient properties that\nare typically associated with certain objects or concepts. Extensive\nevaluations, including extrinsic use-case studies, show that Quasimodo provides\nbetter coverage than state-of-the-art baselines with comparable quality.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 06:12:56 GMT"}, {"version": "v2", "created": "Fri, 31 May 2019 07:01:43 GMT"}, {"version": "v3", "created": "Mon, 2 Sep 2019 08:52:21 GMT"}, {"version": "v4", "created": "Wed, 10 Feb 2021 21:40:55 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Romero", "Julien", ""], ["Razniewski", "Simon", ""], ["Pal", "Koninika", ""], ["Pan", "Jeff Z.", ""], ["Sakhadeo", "Archit", ""], ["Weikum", "Gerhard", ""]]}, {"id": "1905.11499", "submitter": "Dongjun Lee", "authors": "Dongjun Lee, Jaesik Yoon, Jongyun Song, Sanggil Lee, Sungroh Yoon", "title": "One-Shot Learning for Text-to-SQL Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Most deep learning approaches for text-to-SQL generation are limited to the\nWikiSQL dataset, which only supports very simple queries. Recently,\ntemplate-based and sequence-to-sequence approaches were proposed to support\ncomplex queries, which contain join queries, nested queries, and other types.\nHowever, Finegan-Dollak et al. (2018) demonstrated that both the approaches\nlack the ability to generate SQL of unseen templates. In this paper, we propose\na template-based one-shot learning model for the text-to-SQL generation so that\nthe model can generate SQL of an untrained template based on a single example.\nFirst, we classify the SQL template using the Matching Network that is\naugmented by our novel architecture Candidate Search Network. Then, we fill the\nvariable slots in the predicted template using the Pointer Network. We show\nthat our model outperforms state-of-the-art approaches for various text-to-SQL\ndatasets in two aspects: 1) the SQL generation accuracy for the trained\ntemplates, and 2) the adaptability to the unseen SQL templates based on a\nsingle example without any additional training.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 06:29:29 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Lee", "Dongjun", ""], ["Yoon", "Jaesik", ""], ["Song", "Jongyun", ""], ["Lee", "Sanggil", ""], ["Yoon", "Sungroh", ""]]}, {"id": "1905.11695", "submitter": "Xavier Ouvrard", "authors": "Xavier Ouvrard and Jean-Marie Le Goff and St\\'ephane Marchand-Maillet", "title": "The HyperBagGraph DataEdron: An Enriched Browsing Experience of\n  Multimedia Datasets", "comments": "Extension of the hypergraph framework shortly presented in\n  arXiv:1809.00164 (possible small overlaps); use the theoretical framework of\n  hb-graphs presented in arXiv:1809.00190", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional verbatim browsers give back information in a linear way according\nto a ranking performed by a search engine that may not be optimal for the\nsurfer. The latter may need to assess the pertinence of the information\nretrieved, particularly when s$\\cdot$he wants to explore other facets of a\nmulti-facetted information space. For instance, in a multimedia dataset\ndifferent facets such as keywords, authors, publication category, organisations\nand figures can be of interest. The facet simultaneous visualisation can help\nto gain insights on the information retrieved and call for further searches.\nFacets are co-occurence networks, modeled by HyperBag-Graphs -- families of\nmultisets -- and are in fact linked not only to the publication itself, but to\nany chosen reference. These references allow to navigate inside the dataset and\nperform visual queries. We explore here the case of scientific publications\nbased on Arxiv searches.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 09:14:08 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Ouvrard", "Xavier", ""], ["Goff", "Jean-Marie Le", ""], ["Marchand-Maillet", "St\u00e9phane", ""]]}, {"id": "1905.11948", "submitter": "Jaroslaw Szlichta", "authors": "Pei Li, Michael Bohlen, Jaroslaw Szlichta, Divesh Srivastava", "title": "ABC of Order Dependencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We enhance constrained-based data quality with approximate band conditional\norder dependencies (abcODs). Band ODs model the semantics of attributes that\nare monotonically related with small variations without there being an\nintrinsic violation of semantics. The class of abcODs generalizes band ODs to\nmake them more relevant to real-world applications by relaxing them to hold\napproximately (abODs) with some exceptions and conditionally (bcODs) on subsets\nof the data. We study the problem of automatic dependency discovery over a\nhierarchy of abcODs. First, we propose a more efficient algorithm to discover\nabODs than in recent prior work. The algorithm is based on a new optimization\nto compute a longest monotonic band (longest subsequence of tuples that satisfy\na band OD) through dynamic programming by decreasing the runtime from O(n^2) to\nO(n \\log n) time. We then illustrate that while the discovery of bcODs is\nrelatively straightforward, there exist codependencies between approximation\nand conditioning that make the problem of abcOD discovery challenging. The\nnaive solution is prohibitively expensive as it considers all possible\nsegmentations of tuples resulting in exponential time complexity. To reduce the\nsearch space, we devise a dynamic programming algorithm for abcOD discovery\nthat determines the optimal solution in O(n^3 \\log n) complexity. To further\noptimize the performance, we adapt the algorithm to cheaply identify\nconsecutive tuples that are guaranteed to belong to the same band--this\nimproves the performance significantly in practice without losing optimality.\nWhile unidirectional abcODs are most common in practice, for generality we\nextend our algorithms with both ascending and descending orders to discover\nbidirectional abcODs. Finally, we perform a thorough experimental evaluation of\nour techniques over real-world and synthetic datasets.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 17:09:12 GMT"}, {"version": "v2", "created": "Sun, 8 Sep 2019 13:46:36 GMT"}, {"version": "v3", "created": "Wed, 11 Sep 2019 02:35:50 GMT"}, {"version": "v4", "created": "Fri, 28 Feb 2020 16:16:45 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Li", "Pei", ""], ["Bohlen", "Michael", ""], ["Szlichta", "Jaroslaw", ""], ["Srivastava", "Divesh", ""]]}, {"id": "1905.12133", "submitter": "Edmon Begoli", "authors": "Edmon Begoli, Tyler Akidau, Fabian Hueske, Julian Hyde, Kathryn\n  Knight, Kenneth Knowles", "title": "One SQL to Rule Them All", "comments": null, "journal-ref": null, "doi": "10.1145/3299869.3314040", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time data analysis and management are increasingly critical for today`s\nbusinesses. SQL is the de facto lingua franca for these endeavors, yet support\nfor robust streaming analysis and management with SQL remains limited. Many\napproaches restrict semantics to a reduced subset of features and/or require a\nsuite of non-standard constructs. Additionally, use of event timestamps to\nprovide native support for analyzing events according to when they actually\noccurred is not pervasive, and often comes with important limitations. We\npresent a three-part proposal for integrating robust streaming into the SQL\nstandard, namely: (1) time-varying relations as a foundation for classical\ntables as well as streaming data, (2) event time semantics, (3) a limited set\nof optional keyword extensions to control the materialization of time-varying\nquery results. Motivated and illustrated using examples and lessons learned\nfrom implementations in Apache Calcite, Apache Flink, and Apache Beam, we show\nhow with these minimal additions it is possible to utilize the complete suite\nof standard SQL semantics to perform robust stream processing.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 23:26:08 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Begoli", "Edmon", ""], ["Akidau", "Tyler", ""], ["Hueske", "Fabian", ""], ["Hyde", "Julian", ""], ["Knight", "Kathryn", ""], ["Knowles", "Kenneth", ""]]}, {"id": "1905.12411", "submitter": "Vuong M. Ngo", "authors": "Vuong M. Ngo and Nhien-An Le-Khac and M-Tahar Kechadi", "title": "Designing and Implementing Data Warehouse for Agricultural Big Data", "comments": "Business intelligent, data warehouse, constellation schema, Big Data,\n  precision agriculture", "journal-ref": "BigData 2019", "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, precision agriculture that uses modern information and\ncommunication technologies is becoming very popular. Raw and semi-processed\nagricultural data are usually collected through various sources, such as:\nInternet of Thing (IoT), sensors, satellites, weather stations, robots, farm\nequipment, farmers and agribusinesses, etc. Besides, agricultural datasets are\nvery large, complex, unstructured, heterogeneous, non-standardized, and\ninconsistent. Hence, the agricultural data mining is considered as Big Data\napplication in terms of volume, variety, velocity and veracity. It is a key\nfoundation to establishing a crop intelligence platform, which will enable\nresource efficient agronomy decision making and recommendations. In this paper,\nwe designed and implemented a continental level agricultural data warehouse by\ncombining Hive, MongoDB and Cassandra. Our data warehouse capabilities: (1)\nflexible schema; (2) data integration from real agricultural multi datasets;\n(3) data science and business intelligent support; (4) high performance; (5)\nhigh storage; (6) security; (7) governance and monitoring; (8) replication and\nrecovery; (9) consistency, availability and partition tolerant; (10)\ndistributed and cloud deployment. We also evaluate the performance of our data\nwarehouse.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 13:18:03 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Ngo", "Vuong M.", ""], ["Le-Khac", "Nhien-An", ""], ["Kechadi", "M-Tahar", ""]]}, {"id": "1905.12744", "submitter": "David Pujol", "authors": "Satya Kuppam, Ryan Mckenna, David Pujol, Michael Hay, Ashwin\n  Machanavajjhala, Gerome Miklau", "title": "Fair Decision Making using Privacy-Protected Data", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data collected about individuals is regularly used to make decisions that\nimpact those same individuals. We consider settings where sensitive personal\ndata is used to decide who will receive resources or benefits. While it is well\nknown that there is a tradeoff between protecting privacy and the accuracy of\ndecisions, we initiate a first-of-its-kind study into the impact of formally\nprivate mechanisms (based on differential privacy) on fair and equitable\ndecision-making. We empirically investigate novel tradeoffs on two real-world\ndecisions made using U.S. Census data (allocation of federal funds and\nassignment of voting rights benefits) as well as a classic apportionment\nproblem. Our results show that if decisions are made using an\n$\\epsilon$-differentially private version of the data, under strict privacy\nconstraints (smaller $\\epsilon$), the noise added to achieve privacy may\ndisproportionately impact some groups over others. We propose novel measures of\nfairness in the context of randomized differentially private algorithms and\nidentify a range of causes of outcome disparities.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 21:32:23 GMT"}, {"version": "v2", "created": "Fri, 24 Jan 2020 21:41:53 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Kuppam", "Satya", ""], ["Mckenna", "Ryan", ""], ["Pujol", "David", ""], ["Hay", "Michael", ""], ["Machanavajjhala", "Ashwin", ""], ["Miklau", "Gerome", ""]]}, {"id": "1905.13011", "submitter": "Pratyush Mahapatra", "authors": "Pratyush Mahapatra, Mark D. Hill, Michael M. Swift", "title": "Don't Persist All : Efficient Persistent Data Structures", "comments": "10 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data structures used in software development have inbuilt redundancy to\nimprove software reliability and to speed up performance. Examples include a\nDoubly Linked List which allows a faster deletion due to the presence of the\nprevious pointer. With the introduction of Persistent Memory, storing the\nredundant data fields into persistent memory adds a significant write overhead,\nand reduces performance. In this work, we focus on three data structures -\nDoubly Linked List, B+Tree and Hashmap, and showcase alternate partly\npersistent implementations where we only store a limited set of data fields to\npersistent memory. After a crash/restart, we use the persistent data fields to\nrecreate the data structures along with the redundant data fields. We compare\nour implementation with the base implementation and show that we achieve\nspeedups around 5-20% for some data structures, and up to 165% for a\nflush-dominated data structure.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 17:56:55 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Mahapatra", "Pratyush", ""], ["Hill", "Mark D.", ""], ["Swift", "Michael M.", ""]]}, {"id": "1905.13363", "submitter": "Yasith Jayawardana", "authors": "Yasith Jayawardana, Sampath Jayarathna", "title": "DFS: A Dataset File System for Data Discovering Users", "comments": null, "journal-ref": null, "doi": "10.1109/JCDL.2019.00068", "report-no": null, "categories": "cs.DL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many research questions can be answered quickly and efficiently using data\nalready collected for previous research. This practice is called secondary data\nanalysis (SDA), and has gained popularity due to lower costs and improved\nresearch efficiency. In this paper we propose DFS, a file system to standardize\nthe metadata representation of datasets, and DDU, a scalable architecture based\non DFS for semi-automated metadata generation and data recommendation on the\ncloud. We discuss how DFS and DDU lays groundwork for automatic dataset\naggregation, how it integrates with existing data wrangling and machine\nlearning tools, and explores their implications on datasets stored in digital\nlibraries.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 00:23:26 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Jayawardana", "Yasith", ""], ["Jayarathna", "Sampath", ""]]}, {"id": "1905.13376", "submitter": "Rekha Singhal Dr.", "authors": "Kunle Olukotun, Raghu Prabhakar, Rekha Singhal, Jeffrey D.Ullman, and\n  Yaqi Zhang", "title": "Efficient Multiway Hash Join on Reconfigurable Hardware", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the algorithms for performing multiway joins using a new type of\ncoarse grain reconfigurable hardware accelerator~-- ``Plasticine''~-- that,\ncompared with other accelerators, emphasizes high compute capability and high\non-chip communication bandwidth. Joining three or more relations in a single\nstep, i.e. multiway join, is efficient when the join of any two relations\nyields too large an intermediate relation. We show at least 200X speedup for a\nsequence of binary hash joins execution on Plasticine over CPU. We further show\nthat in some realistic cases, a Plasticine-like accelerator can make 3-way\njoins more efficient than a cascade of binary hash joins on the same hardware,\nby a factor of up to 45X.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 01:48:58 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Olukotun", "Kunle", ""], ["Prabhakar", "Raghu", ""], ["Singhal", "Rekha", ""], ["Ullman", "Jeffrey D.", ""], ["Zhang", "Yaqi", ""]]}, {"id": "1905.13415", "submitter": "Elias Stehle", "authors": "Elias Stehle and Hans-Arno Jacobsen", "title": "ParPaRaw: Massively Parallel Parsing of Delimiter-Separated Raw Data", "comments": null, "journal-ref": "PVLDB, 13(5): 616-628, 2020", "doi": "10.14778/3377369.3377372", "report-no": null, "categories": "cs.DB cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parsing is essential for a wide range of use cases, such as stream\nprocessing, bulk loading, and in-situ querying of raw data. Yet, the\ncompute-intense step often constitutes a major bottleneck in the data ingestion\npipeline, since parsing of inputs that require more involved parsing rules is\nchallenging to parallelise. This work proposes a massively parallel algorithm\nfor parsing delimiter-separated data formats on GPUs. Other than the\nstate-of-the-art, the proposed approach does not require an initial sequential\npass over the input to determine a thread's parsing context. That is, how a\nthread, beginning somewhere in the middle of the input, should interpret a\ncertain symbol (e.g., whether to interpret a comma as a delimiter or as part of\na larger string enclosed in double-quotes). Instead of tailoring the approach\nto a single format, we are able to perform a massively parallel FSM simulation,\nwhich is more flexible and powerful, supporting more expressive parsing rules\nwith general applicability. Achieving a parsing rate of as much as 14.2 GB/s,\nour experimental evaluation on a GPU with 3584 cores shows that the presented\napproach is able to scale to thousands of cores and beyond. With an end-to-end\nstreaming approach, we are able to exploit the full-duplex capabilities of the\nPCIe bus and hide latency from data transfers. Considering the end-to-end\nperformance, the algorithm parses 4.8 GB in as little as 0.44 seconds,\nincluding data transfers.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 05:04:39 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 04:52:33 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Stehle", "Elias", ""], ["Jacobsen", "Hans-Arno", ""]]}]