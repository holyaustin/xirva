[{"id": "1511.00367", "submitter": "Lu Qin", "authors": "Dong Wen, Lu Qin, Ying Zhang, Xuemin Lin, and Jeffrey Xu Yu", "title": "I/O Efficient Core Graph Decomposition at Web Scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Core decomposition is a fundamental graph problem with a large number of\napplications. Most existing approaches for core decomposition assume that the\ngraph is kept in memory of a machine. Nevertheless, many real-world graphs are\nbig and may not reside in memory. In the literature, there is only one work for\nI/O efficient core decomposition that avoids loading the whole graph in memory.\nHowever, this approach is not scalable to handle big graphs because it cannot\nbound the memory size and may load most parts of the graph in memory. In\naddition, this approach can hardly handle graph updates. In this paper, we\nstudy I/O efficient core decomposition following a semi-external model, which\nonly allows node information to be loaded in memory. This model works well in\nmany web-scale graphs. We propose a semi-external algorithm and two optimized\nalgorithms for I/O efficient core decomposition using very simple structures\nand data access model. To handle dynamic graph updates, we show that our\nalgorithm can be naturally extended to handle edge deletion. We also propose an\nI/O efficient core maintenance algorithm to handle edge insertion, and an\nimproved algorithm to further reduce I/O and CPU cost by investigating some new\ngraph properties. We conduct extensive experiments on 12 real large graphs. Our\noptimal algorithm significantly outperform the existing I/O efficient algorithm\nin terms of both processing time and memory consumption. In many\nmemory-resident graphs, our algorithms for both core decomposition and\nmaintenance can even outperform the in-memory algorithm due to the simple\nstructures and data access model used. Our algorithms are very scalable to\nhandle web-scale graphs. As an example, we are the first to handle a web graph\nwith 978.5 million nodes and 42.6 billion edges using less than 4.2 GB memory.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 03:18:41 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Wen", "Dong", ""], ["Qin", "Lu", ""], ["Zhang", "Ying", ""], ["Lin", "Xuemin", ""], ["Yu", "Jeffrey Xu", ""]]}, {"id": "1511.00384", "submitter": "Arthur Ryman", "authors": "Arthur Ryman", "title": "Z Specification for the W3C Editor's Draft Core SHACL Semantics", "comments": "57 pages, Invited Expert contribution to the W3C RDF Data Shapes\n  Working Group", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article provides a formalization of the W3C Draft Core SHACL Semantics\nspecification using Z notation. This formalization exercise has identified a\nnumber of quality issues in the draft. It has also established that the\nrecursive definitions in the draft are well-founded. Further formal validation\nof the draft will require the use of an executable specification technology.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 05:31:42 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Ryman", "Arthur", ""]]}, {"id": "1511.00628", "submitter": "Mohamad Dolatshah", "authors": "Mohamad Dolatshah, Ali Hadian and Behrouz Minaei-Bidgoli", "title": "Ball*-tree: Efficient spatial indexing for constrained nearest-neighbor\n  search in metric spaces", "comments": "15 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emerging location-based systems and data analysis frameworks requires\nefficient management of spatial data for approximate and exact search. Exact\nsimilarity search can be done using space partitioning data structures, such as\nKd-tree, R*-tree, and Ball-tree. In this paper, we focus on Ball-tree, an\nefficient search tree that is specific for spatial queries which use euclidean\ndistance. Each node of a Ball-tree defines a ball, i.e. a hypersphere that\ncontains a subset of the points to be searched.\n  In this paper, we propose Ball*-tree, an improved Ball-tree that is more\nefficient for spatial queries. Ball*-tree enjoys a modified space partitioning\nalgorithm that considers the distribution of the data points in order to find\nan efficient splitting hyperplane. Also, we propose a new algorithm for KNN\nqueries with restricted range using Ball*-tree, which performs better than both\nKNN and range search for such queries. Results show that Ball*-tree performs\n39%-57% faster than the original Ball-tree algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 18:54:49 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Dolatshah", "Mohamad", ""], ["Hadian", "Ali", ""], ["Minaei-Bidgoli", "Behrouz", ""]]}, {"id": "1511.00725", "submitter": "Wajdi Dhifli", "authors": "Wajdi Dhifli, Abdoulaye Banir\\'e Diallo", "title": "Toward an Efficient Multi-class Classification in an Open Universe", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification is a fundamental task in machine learning and data mining.\nExisting classification methods are designed to classify unknown instances\nwithin a set of previously known training classes. Such a classification takes\nthe form of a prediction within a closed-set of classes. However, a more\nrealistic scenario that fits real-world applications is to consider the\npossibility of encountering instances that do not belong to any of the training\nclasses, $i.e.$, an open-set classification. In such situation, existing\nclosed-set classifiers will assign a training label to these instances\nresulting in a misclassification. In this paper, we introduce Galaxy-X, a novel\nmulti-class classification approach for open-set recognition problems. For each\nclass of the training set, Galaxy-X creates a minimum bounding hyper-sphere\nthat encompasses the distribution of the class by enclosing all of its\ninstances. In such manner, our method is able to distinguish instances\nresembling previously seen classes from those that are of unknown ones. To\nadequately evaluate open-set classification, we introduce a novel evaluation\nprocedure. Experimental results on benchmark datasets show the efficiency of\nour approach in classifying novel instances from known as well as unknown\nclasses.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 22:04:00 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2016 02:27:55 GMT"}, {"version": "v3", "created": "Thu, 1 Mar 2018 17:22:56 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Dhifli", "Wajdi", ""], ["Diallo", "Abdoulaye Banir\u00e9", ""]]}, {"id": "1511.00938", "submitter": "Luc Segoufin", "authors": "Nadime Francis (ENS Cachan and INRIA), Luc Segoufin (INRIA & ENS\n  Cachan), Cristina Sirangelo (ENS Cachan, CNRS and INRIA)", "title": "Datalog Rewritings of Regular Path Queries using Views", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 11, Issue 4 (December\n  22, 2015) lmcs:1615", "doi": "10.2168/LMCS-11(4:14)2015", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider query answering using views on graph databases, i.e. databases\nstructured as edge-labeled graphs. We mainly consider views and queries\nspecified by Regular Path Queries (RPQ). These are queries selecting pairs of\nnodes in a graph database that are connected via a path whose sequence of edge\nlabels belongs to some regular language. We say that a view V determines a\nquery Q if for all graph databases D, the view image V(D) always contains\nenough information to answer Q on D. In other words, there is a well defined\nfunction from V(D) to Q(D). Our main result shows that when this function is\nmonotone, there exists a rewriting of Q as a Datalog query over the view\ninstance V(D). In particular the rewriting query can be evaluated in time\npolynomial in the size of V(D). Moreover this implies that it is decidable\nwhether an RPQ query can be rewritten in Datalog using RPQ views.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 15:00:40 GMT"}, {"version": "v2", "created": "Sun, 20 Dec 2015 17:03:53 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Francis", "Nadime", "", "ENS Cachan and INRIA"], ["Segoufin", "Luc", "", "INRIA & ENS\n  Cachan"], ["Sirangelo", "Cristina", "", "ENS Cachan, CNRS and INRIA"]]}, {"id": "1511.01281", "submitter": "Fabrice Rossi", "authors": "Mohamed Khalil El Mahrsi (LTCI, SAMM), Romain Guigour\\`es (SAMM),\n  Fabrice Rossi (SAMM), Marc Boull\\'e", "title": "Co-Clustering Network-Constrained Trajectory Data", "comments": null, "journal-ref": "Advances in Knowledge Discovery and Management, 615, Springer\n  International Publishing, pp.19-32, 2015, Studies in Computational\n  Intelligence, 978-3-319-23750-3", "doi": "10.1007/978-3-319-23751-0_2", "report-no": null, "categories": "stat.ML cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, clustering moving object trajectories kept gaining interest from\nboth the data mining and machine learning communities. This problem, however,\nwas studied mainly and extensively in the setting where moving objects can move\nfreely on the euclidean space. In this paper, we study the problem of\nclustering trajectories of vehicles whose movement is restricted by the\nunderlying road network. We model relations between these trajectories and road\nsegments as a bipartite graph and we try to cluster its vertices. We\ndemonstrate our approaches on synthetic data and show how it could be useful in\ninferring knowledge about the flow dynamics and the behavior of the drivers\nusing the road network.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 10:47:29 GMT"}], "update_date": "2015-11-05", "authors_parsed": [["Mahrsi", "Mohamed Khalil El", "", "LTCI, SAMM"], ["Guigour\u00e8s", "Romain", "", "SAMM"], ["Rossi", "Fabrice", "", "SAMM"], ["Boull\u00e9", "Marc", ""]]}, {"id": "1511.01768", "submitter": "Immanuel Trummer Mr.", "authors": "Immanuel Trummer and Christoph Koch", "title": "Parallelizing Query Optimization on Shared-Nothing Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data processing systems offer an ever increasing degree of parallelism on the\nlevels of cores, CPUs, and processing nodes. Query optimization must exploit\nhigh degrees of parallelism in order not to gradually become the bottleneck of\nquery evaluation. We show how to parallelize query optimization at a massive\nscale.\n  We present algorithms for parallel query optimization in left-deep and bushy\nplan spaces. At optimization start, we divide the plan space for a given query\ninto partitions of equal size that are explored in parallel by worker nodes. At\nthe end of optimization, each worker returns the optimal plan in its partition\nto the master which determines the globally optimal plan from the\npartition-optimal plans. No synchronization or data exchange is required during\nthe actual optimization phase. The amount of data sent over the network, at the\nstart and at the end of optimization, as well as the complexity of serial steps\nwithin our algorithms increase only linearly in the number of workers and in\nthe query size. The time and space complexity of optimization within one\npartition decreases uniformly in the number of workers. We parallelize single-\nand multi-objective query optimization over a cluster with 100 nodes in our\nexperiments, using more than 250 concurrent worker threads (Spark executors).\nDespite high network latency and task assignment overheads, parallelization\nyields speedups of up to one order of magnitude for large queries whose\noptimization takes minutes on a single node.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 14:59:27 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Trummer", "Immanuel", ""], ["Koch", "Christoph", ""]]}, {"id": "1511.01782", "submitter": "Immanuel Trummer Mr.", "authors": "Immanuel Trummer and Christoph Koch", "title": "Probably Approximately Optimal Query Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating query predicates on data samples is the only way to estimate their\nselectivity in certain scenarios. Finding a guaranteed optimal query plan is\nnot a reasonable optimization goal in those cases as it might require an\ninfinite number of samples. We therefore introduce probably approximately\noptimal query optimization (PAO) where the goal is to find a query plan whose\ncost is near-optimal with a certain probability. We will justify why PAO is a\nsuitable formalism to model scenarios in which predicate sampling and\noptimization need to be interleaved.\n  We present the first algorithm for PAO. Our algorithm is non-intrusive and\nuses standard query optimizers and sampling components as sub-functions. It is\ngeneric and can be applied to a wide range of scenarios. Our algorithm is\niterative and calculates in each iteration a query plan together with a region\nin the selectivity space where the plan has near-optimal cost. It determines\nthe confidence that the true selectivity values fall within the aforementioned\nregion and chooses the next samples to take based on the current state if the\nconfidence does not reach the threshold specified as problem input. We devise\ndifferent algorithm variants and analyze their complexity. We experimentally\ncompare them in terms of the number of optimizer invocations, samples, and\niterations over many different query classes.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 15:39:00 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Trummer", "Immanuel", ""], ["Koch", "Christoph", ""]]}, {"id": "1511.02071", "submitter": "Immanuel Trummer Mr.", "authors": "Immanuel Trummer and Christoph Koch", "title": "Solving the Join Ordering Problem via Mixed Integer Linear Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We transform join ordering into a mixed integer linear program (MILP). This\nallows to address query optimization by mature MILP solver implementations that\nhave evolved over decades and steadily improved their performance. They offer\nfeatures such as anytime optimization and parallel search that are highly\nrelevant for query optimization.\n  We present a MILP formulation for searching left-deep query plans. We use\nsets of binary variables to represent join operands and intermediate results,\noperator implementation choices or the presence of interesting orders. Linear\nconstraints restrict value assignments to the ones representing valid query\nplans. We approximate the cost of scan and join operations via linear\nfunctions, allowing to increase approximation precision up to arbitrary\ndegrees. Our experimental results are encouraging: we are able to find optimal\nplans for joins between 60 tables; a query size that is beyond the capabilities\nof prior exhaustive query optimization methods.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2015 13:28:45 GMT"}], "update_date": "2015-11-09", "authors_parsed": [["Trummer", "Immanuel", ""], ["Koch", "Christoph", ""]]}, {"id": "1511.03036", "submitter": "Hong Sun", "authors": "Hong Sun, Kristof Depraetere, Jos De Roo, Giovanni Mels, Boris De\n  Vloed, Marc Twagirumukiza, Dirk Colaert", "title": "Semantic processing of EHR data for clinical research", "comments": "Accepted for publication in Journal of Biomedical Informatics, 2015,\n  preprint version", "journal-ref": null, "doi": "10.1016/j.jbi.2015.10.009", "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing need to semantically process and integrate clinical data\nfrom different sources for clinical research. This paper presents an approach\nto integrate EHRs from heterogeneous resources and generate integrated data in\ndifferent data formats or semantics to support various clinical research\napplications. The proposed approach builds semantic data virtualization layers\non top of data sources, which generate data in the requested semantics or\nformats on demand. This approach avoids upfront dumping to and synchronizing of\nthe data with various representations. Data from different EHR systems are\nfirst mapped to RDF data with source semantics, and then converted to\nrepresentations with harmonized domain semantics where domain ontologies and\nterminologies are used to improve reusability. It is also possible to further\nconvert data to application semantics and store the converted results in\nclinical research databases, e.g. i2b2, OMOP, to support different clinical\nresearch settings. Semantic conversions between different representations are\nexplicitly expressed using N3 rules and executed by an N3 Reasoner (EYE), which\ncan also generate proofs of the conversion processes. The solution presented in\nthis paper has been applied to real-world applications that process large scale\nEHR data.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 09:47:14 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Sun", "Hong", ""], ["Depraetere", "Kristof", ""], ["De Roo", "Jos", ""], ["Mels", "Giovanni", ""], ["De Vloed", "Boris", ""], ["Twagirumukiza", "Marc", ""], ["Colaert", "Dirk", ""]]}, {"id": "1511.03086", "submitter": "Oliver Schulte", "authors": "Jan Motl and Oliver Schulte", "title": "The CTU Prague Relational Learning Repository", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of the CTU Prague Relational Learning Repository is to support\nmachine learning research with multi-relational data. The repository currently\ncontains 50 SQL databases hosted on a public MySQL server located at\nrelational.fit.cvut.cz. A searchable meta-database provides metadata (e.g., the\nnumber of tables in the database, the number of rows and columns in the tables,\nthe number of foreign key constraints between tables).\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 12:30:42 GMT"}], "update_date": "2015-11-11", "authors_parsed": [["Motl", "Jan", ""], ["Schulte", "Oliver", ""]]}, {"id": "1511.03576", "submitter": "Mohammad Khabbaz", "authors": "Mohammad Khabbaz", "title": "DataGrinder: Fast, Accurate, Fully non-Parametric Classification\n  Approach Using 2D Convex Hulls", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CG cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been a long time, since data mining technologies have made their ways\nto the field of data management. Classification is one of the most important\ndata mining tasks for label prediction, categorization of objects into groups,\nadvertisement and data management. In this paper, we focus on the standard\nclassification problem which is predicting unknown labels in Euclidean space.\nMost efforts in Machine Learning communities are devoted to methods that use\nprobabilistic algorithms which are heavy on Calculus and Linear Algebra. Most\nof these techniques have scalability issues for big data, and are hardly\nparallelizable if they are to maintain their high accuracies in their standard\nform. Sampling is a new direction for improving scalability, using many small\nparallel classifiers. In this paper, rather than conventional sampling methods,\nwe focus on a discrete classification algorithm with O(n) expected running\ntime. Our approach performs a similar task as sampling methods. However, we use\ncolumn-wise sampling of data, rather than the row-wise sampling used in the\nliterature. In either case, our algorithm is completely deterministic. Our\nalgorithm, proposes a way of combining 2D convex hulls in order to achieve high\nclassification accuracy as well as scalability in the same time. First, we\nthoroughly describe and prove our O(n) algorithm for finding the convex hull of\na point set in 2D. Then, we show with experiments our classifier model built\nbased on this idea is very competitive compared with existing sophisticated\nclassification algorithms included in commercial statistical applications such\nas MATLAB.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 17:06:35 GMT"}], "update_date": "2015-11-12", "authors_parsed": [["Khabbaz", "Mohammad", ""]]}, {"id": "1511.03935", "submitter": "Milinda Pathirage", "authors": "Milinda Pathirage, Beth Plale", "title": "Fast Data Management with Distributed Streaming SQL", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To stay competitive in today's data driven economy, enterprises large and\nsmall are turning to stream processing platforms to process high volume, high\nvelocity, and diverse streams of data (fast data) as they arrive. Low-level\nprogramming models provided by the popular systems of today suffer from lack of\nresponsiveness to change: enhancements require code changes with attendant\nlarge turn-around times. Even though distributed SQL query engines have been\navailable for Big Data, we still lack support for SQL-based stream querying\ncapabilities in distributed stream processing systems. In this white paper, we\nidentify a set of requirements and propose a standard SQL based streaming query\nmodel for management of what has been referred to as Fast Data.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 15:54:49 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Pathirage", "Milinda", ""], ["Plale", "Beth", ""]]}, {"id": "1511.04583", "submitter": "Yanhong Annie Liu", "authors": "Yanhong A. Liu, Jon Brandvein, Scott D. Stoller, Bo Lin", "title": "Demand-Driven Incremental Object Queries", "comments": null, "journal-ref": "PPDP 2016: Proceedings of the 18th International Symposium on\n  Principles and Practice of Declarative Programming, September 2016, Pages\n  228-241. ACM Press", "doi": "10.1145/2967973.2968610", "report-no": null, "categories": "cs.PL cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object queries are essential in information seeking and decision making in\nvast areas of applications. However, a query may involve complex conditions on\nobjects and sets, which can be arbitrarily nested and aliased. The objects and\nsets involved as well as the demand---the given parameter values of\ninterest---can change arbitrarily. How to implement object queries efficiently\nunder all possible updates, and furthermore to provide complexity guarantees?\n  This paper describes an automatic method. The method allows powerful queries\nto be written completely declaratively. It transforms demand as well as all\nobjects and sets into relations. Most importantly, it defines invariants for\nnot only the query results, but also all auxiliary values about the objects and\nsets involved, including those for propagating demand, and incrementally\nmaintains all of them. Implementation and experiments with problems from a\nvariety of application areas, including distributed algorithms and\nprobabilistic queries, confirm the analyzed complexities, trade-offs, and\nsignificant improvements over prior work.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2015 17:27:33 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2015 03:34:09 GMT"}, {"version": "v3", "created": "Fri, 15 Jul 2016 18:06:30 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Liu", "Yanhong A.", ""], ["Brandvein", "Jon", ""], ["Stoller", "Scott D.", ""], ["Lin", "Bo", ""]]}, {"id": "1511.04750", "submitter": "Nikos Bikakis", "authors": "Nikos Bikakis, George Papastefanatos, Melina Skourla, Timos Sellis", "title": "A Hierarchical Aggregation Framework for Efficient Multilevel Visual\n  Exploration and Analysis", "comments": "Semantic Web Journal 2016 (to appear)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data exploration and visualization systems are of great importance in the Big\nData era, in which the volume and heterogeneity of available information make\nit difficult for humans to manually explore and analyse data. Most traditional\nsystems operate in an offline way, limited to accessing preprocessed (static)\nsets of data. They also restrict themselves to dealing with small dataset\nsizes, which can be easily handled with conventional techniques. However, the\nBig Data era has realized the availability of a great amount and variety of big\ndatasets that are dynamic in nature; most of them offer API or query endpoints\nfor online access, or the data is received in a stream fashion. Therefore,\nmodern systems must address the challenge of on-the-fly scalable visualizations\nover large dynamic sets of data, offering efficient exploration techniques, as\nwell as mechanisms for information abstraction and summarization. In this work,\nwe present a generic model for personalized multilevel exploration and analysis\nover large dynamic sets of numeric and temporal data. Our model is built on top\nof a lightweight tree-based structure which can be efficiently constructed\non-the-fly for a given set of data. This tree structure aggregates input\nobjects into a hierarchical multiscale model. Considering different exploration\nscenarios over large datasets, the proposed model enables efficient multilevel\nexploration, offering incremental construction and prefetching via user\ninteraction, and dynamic adaptation of the hierarchies based on user\npreferences. A thorough theoretical analysis is presented, illustrating the\nefficiency of the proposed model. The proposed model is realized in a web-based\nprototype tool, called SynopsViz that offers multilevel visual exploration and\nanalysis over Linked Data datasets.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2015 18:23:27 GMT"}, {"version": "v2", "created": "Sun, 6 Dec 2015 12:51:23 GMT"}, {"version": "v3", "created": "Fri, 22 Jan 2016 18:08:18 GMT"}, {"version": "v4", "created": "Fri, 19 Feb 2016 14:33:45 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Bikakis", "Nikos", ""], ["Papastefanatos", "George", ""], ["Skourla", "Melina", ""], ["Sellis", "Timos", ""]]}, {"id": "1511.05010", "submitter": "Carlos Baquero", "authors": "Marek Zawirski, Carlos Baquero, Annette Bieniusa, Nuno Pregui\\c{c}a,\n  Marc Shapiro", "title": "Eventually Consistent Register Revisited", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to converge in the presence of concurrent updates, modern eventually\nconsistent replication systems rely on causality information and operation\nsemantics. It is relatively easy to use semantics of high-level operations on\nreplicated data structures, such as sets, lists, etc. However, it is difficult\nto exploit semantics of operations on registers, which store opaque data. In\nexisting register designs, concurrent writes are resolved either by the\napplication, or by arbitrating them according to their timestamps. The former\nis complex and may require user intervention, whereas the latter causes\narbitrary updates to be lost. In this work, we identify a register construction\nthat generalizes existing ones by combining runtime causality ordering, to\nidentify concurrent writes, with static data semantics, to resolve them. We\npropose a simple conflict resolution template based on an\napplication-predefined order on the domain of values. It eliminates or reduces\nthe number of conflicts that need to be resolved by the user or by an explicit\napplication logic. We illustrate some variants of our approach with use cases,\nand how it generalizes existing designs.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 15:59:10 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Zawirski", "Marek", ""], ["Baquero", "Carlos", ""], ["Bieniusa", "Annette", ""], ["Pregui\u00e7a", "Nuno", ""], ["Shapiro", "Marc", ""]]}, {"id": "1511.05911", "submitter": "Bo Zong", "authors": "Bo Zong, Xusheng Xiao, Zhichun Li, Zhenyu Wu, Zhiyun Qian, Xifeng Yan,\n  Ambuj K. Singh, Guofei Jiang", "title": "Behavior Query Discovery in System-Generated Temporal Graphs", "comments": "The full version of the paper \"Behavior Query Discovery in\n  System-Generated Temporal Graphs\", to appear in VLDB'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Computer system monitoring generates huge amounts of logs that record the\ninteraction of system entities. How to query such data to better understand\nsystem behaviors and identify potential system risks and malicious behaviors\nbecomes a challenging task for system administrators due to the dynamics and\nheterogeneity of the data. System monitoring data are essentially heterogeneous\ntemporal graphs with nodes being system entities and edges being their\ninteractions over time. Given the complexity of such graphs, it becomes\ntime-consuming for system administrators to manually formulate useful queries\nin order to examine abnormal activities, attacks, and vulnerabilities in\ncomputer systems.\n  In this work, we investigate how to query temporal graphs and treat query\nformulation as a discriminative temporal graph pattern mining problem. We\nintroduce TGMiner to mine discriminative patterns from system logs, and these\npatterns can be taken as templates for building more complex queries. TGMiner\nleverages temporal information in graphs to prune graph patterns that share\nsimilar growth trend without compromising pattern quality. Experimental results\non real system data show that TGMiner is 6-32 times faster than baseline\nmethods. The discovered patterns were verified by system experts; they achieved\nhigh precision (97%) and recall (91%).\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 19:03:41 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2015 16:39:41 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Zong", "Bo", ""], ["Xiao", "Xusheng", ""], ["Li", "Zhichun", ""], ["Wu", "Zhenyu", ""], ["Qian", "Zhiyun", ""], ["Yan", "Xifeng", ""], ["Singh", "Ambuj K.", ""], ["Jiang", "Guofei", ""]]}, {"id": "1511.06033", "submitter": "Athanasios N. Nikolakopoulos", "authors": "Athanasios N. Nikolakopoulos, Vassilis Kalantzis, Efstratios\n  Gallopoulos and John D. Garofalakis", "title": "EigenRec: Generalizing PureSVD for Effective and Efficient Top-N\n  Recommendations", "comments": "23 pages. Journal version of the conference paper \"Factored Proximity\n  Models for Top-N Recommendation\"", "journal-ref": null, "doi": "10.1007/s10115-018-1197-7", "report-no": null, "categories": "cs.IR cs.DB cs.DC cs.NA cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce EigenRec; a versatile and efficient Latent-Factor framework for\nTop-N Recommendations that includes the well-known PureSVD algorithm as a\nspecial case. EigenRec builds a low dimensional model of an inter-item\nproximity matrix that combines a similarity component, with a scaling operator,\ndesigned to control the influence of the prior item popularity on the final\nmodel. Seeing PureSVD within our framework provides intuition about its inner\nworkings, exposes its inherent limitations, and also, paves the path towards\npainlessly improving its recommendation performance. A comprehensive set of\nexperiments on the MovieLens and the Yahoo datasets based on widely applied\nperformance metrics, indicate that EigenRec outperforms several\nstate-of-the-art algorithms, in terms of Standard and Long-Tail recommendation\naccuracy, exhibiting low susceptibility to sparsity, even in its most extreme\nmanifestations -- the Cold-Start problems. At the same time EigenRec has an\nattractive computational profile and it can apply readily in large-scale\nrecommendation settings.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 00:34:51 GMT"}, {"version": "v2", "created": "Wed, 31 May 2017 16:00:46 GMT"}, {"version": "v3", "created": "Tue, 5 Dec 2017 12:56:14 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Nikolakopoulos", "Athanasios N.", ""], ["Kalantzis", "Vassilis", ""], ["Gallopoulos", "Efstratios", ""], ["Garofalakis", "John D.", ""]]}, {"id": "1511.07846", "submitter": "Leonidas Fegaras", "authors": "Leonidas Fegaras", "title": "Incremental Query Processing on Big Data Streams", "comments": "Extended version of a paper submitted to a journal", "journal-ref": null, "doi": "10.1109/TKDE.2016.2601103", "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses online query processing for large-scale, incremental\ndata analysis on a distributed stream processing engine (DSPE). Our goal is to\nconvert any SQL-like query to an incremental DSPE program automatically. In\ncontrast to other approaches, we derive incremental programs that return\naccurate results, not approximate answers. This is accomplished by retaining a\nminimal state during the query evaluation lifetime and by using incremental\nevaluation techniques to return an accurate snapshot answer at each time\ninterval that depends on the current state and the latest batches of data. Our\nmethods can handle many forms of queries on nested data collections, including\niterative and nested queries, group-by with aggregation, and equi-joins.\nFinally, we report on a prototype implementation of our framework, called MRQL\nStreaming, running on top of Spark and we experimentally validate the\neffectiveness of our methods.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 19:55:09 GMT"}, {"version": "v2", "created": "Sun, 17 Jan 2016 22:59:08 GMT"}, {"version": "v3", "created": "Sun, 6 Mar 2016 19:21:25 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Fegaras", "Leonidas", ""]]}, {"id": "1511.08386", "submitter": "Radu Ciucanu", "authors": "Guillaume Bagan, Angela Bonifati, Radu Ciucanu, George H. L. Fletcher,\n  Aur\\'elien Lemay, Nicky Advokaat", "title": "gMark: Schema-Driven Generation of Graphs and Queries", "comments": "Accepted in November 2016. URL:\n  http://ieeexplore.ieee.org/document/7762945/. in IEEE Transactions on\n  Knowledge and Data Engineering 2017", "journal-ref": null, "doi": "10.1109/TKDE.2016.2633993", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive graph data sets are pervasive in contemporary application domains.\nHence, graph database systems are becoming increasingly important. In the\nexperimental study of these systems, it is vital that the research community\nhas shared solutions for the generation of database instances and query\nworkloads having predictable and controllable properties. In this paper, we\npresent the design and engineering principles of gMark, a domain- and query\nlanguage-independent graph instance and query workload generator. A core\ncontribution of gMark is its ability to target and control the diversity of\nproperties of both the generated instances and the generated workloads coupled\nto these instances. Further novelties include support for regular path queries,\na fundamental graph query paradigm, and schema-driven selectivity estimation of\nqueries, a key feature in controlling workload chokepoints. We illustrate the\nflexibility and practical usability of gMark by showcasing the framework's\ncapabilities in generating high quality graphs and workloads, and its ability\nto encode user-defined schemas across a variety of application domains.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 13:36:25 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2015 00:21:08 GMT"}, {"version": "v3", "created": "Sat, 6 Feb 2016 20:24:14 GMT"}, {"version": "v4", "created": "Wed, 22 Jun 2016 15:46:06 GMT"}, {"version": "v5", "created": "Fri, 7 Oct 2016 09:48:39 GMT"}, {"version": "v6", "created": "Tue, 6 Dec 2016 19:50:06 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Bagan", "Guillaume", ""], ["Bonifati", "Angela", ""], ["Ciucanu", "Radu", ""], ["Fletcher", "George H. L.", ""], ["Lemay", "Aur\u00e9lien", ""], ["Advokaat", "Nicky", ""]]}, {"id": "1511.08723", "submitter": "Antoine Amarilli", "authors": "Antoine Amarilli and Pierre Bourhis and Pierre Senellart", "title": "Provenance Circuits for Trees and Treelike Instances (Extended Version)", "comments": "48 pages. Presented at ICALP'15", "journal-ref": null, "doi": "10.1007/978-3-662-47666-6_5", "report-no": null, "categories": "cs.DB cs.LO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Query evaluation in monadic second-order logic (MSO) is tractable on trees\nand treelike instances, even though it is hard for arbitrary instances. This\ntractability result has been extended to several tasks related to query\nevaluation, such as counting query results [3] or performing query evaluation\non probabilistic trees [10]. These are two examples of the more general problem\nof computing augmented query output, that is referred to as provenance. This\narticle presents a provenance framework for trees and treelike instances, by\ndescribing a linear-time construction of a circuit provenance representation\nfor MSO queries. We show how this provenance can be connected to the usual\ndefinitions of semiring provenance on relational instances [20], even though we\ncompute it in an unusual way, using tree automata; we do so via intrinsic\ndefinitions of provenance for general semirings, independent of the operational\ndetails of query evaluation. We show applications of this provenance to capture\nexisting counting and probabilistic results on trees and treelike instances,\nand give novel consequences for probability evaluation.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2015 16:11:56 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Amarilli", "Antoine", ""], ["Bourhis", "Pierre", ""], ["Senellart", "Pierre", ""]]}, {"id": "1511.08851", "submitter": "Makoto Hamana", "authors": "Makoto Hamana, Kazutaka Matsuda and Kazuyuki Asada", "title": "The Algebra of Recursive Graph Transformation Language UnCAL: Complete\n  Axiomatisation and Iteration Categorical Semantics", "comments": "53 pages, to appear in MSCS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to provide mathematical foundations of a graph\ntransformation language, called UnCAL, using categorical semantics of type\ntheory and fixed points. About twenty years ago, Buneman et al. developed a\ngraph database query language UnQL on the top of a functional meta-language\nUnCAL for describing and manipulating graphs. Recently, the functional\nprogramming community has shown renewed interest in UnCAL, because it provides\nan efficient graph transformation language which is useful for various\napplications, such as bidirectional computation.\n  In order to make UnCAL more flexible and fruitful for further extensions and\napplications, in this paper, we give a more conceptual understanding of UnCAL\nusing categorical semantics. Our general interest of this paper is to clarify\nwhat is the algebra of UnCAL. Thus, we give an equational axiomatisation and\ncategorical semantics of UnCAL, both of which are new. We show that the\naxiomatisation is complete for the original bisimulation semantics of UnCAL.\nMoreover, we provide a clean characterisation of the computation mechanism of\nUnCAL called \"structural recursion on graphs\" using our categorical semantics.\nWe show a concrete model of UnCAL given by the lambdaG-calculus, which shows an\ninteresting connection to lazy functional programming.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2015 23:26:05 GMT"}, {"version": "v2", "created": "Sun, 11 Sep 2016 21:24:18 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Hamana", "Makoto", ""], ["Matsuda", "Kazutaka", ""], ["Asada", "Kazuyuki", ""]]}, {"id": "1511.08915", "submitter": "Markus Kr\\\"otzsch", "authors": "Jacopo Urbani, Ceriel Jacobs, Markus Kr\\\"otzsch", "title": "Column-Oriented Datalog Materialization for Large Knowledge Graphs\n  (Extended Technical Report)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The evaluation of Datalog rules over large Knowledge Graphs (KGs) is\nessential for many applications. In this paper, we present a new method of\nmaterializing Datalog inferences, which combines a column-based memory layout\nwith novel optimization methods that avoid redundant inferences at runtime. The\npro-active caching of certain subqueries further increases efficiency. Our\nempirical evaluation shows that this approach can often match or even surpass\nthe performance of state-of-the-art systems, especially under restricted\nresources.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2015 17:16:55 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2016 16:12:55 GMT"}], "update_date": "2016-02-12", "authors_parsed": [["Urbani", "Jacopo", ""], ["Jacobs", "Ceriel", ""], ["Kr\u00f6tzsch", "Markus", ""]]}, {"id": "1511.09059", "submitter": "Richard McClatchey", "authors": "Jetendr Shamdasani, Richard McClatchey, Andrew Branson and Zsolt\n  Kovacs", "title": "Analysis Traceability and Provenance for HEP", "comments": "8 pagesd, 4 figures. Presented at 21st Int Conf on Computing in High\n  Energy and Nuclear Physics (CHEP15). Okinawa, Japan. April 2015", "journal-ref": null, "doi": "10.1088/1742-6596/664/3/032028", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the use of the CRISTAL software in the N4U project.\nCRISTAL was used to create a set of provenance aware analysis tools for the\nNeuroscience domain. This paper advocates that the approach taken in N4U to\nbuild the analysis suite is sufficiently generic to be able to be applied to\nthe HEP domain. A mapping to the PROV model for provenance interoperability is\nalso presented and how this can be applied to the HEP domain for the\ninteroperability of HEP analyses.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2015 18:46:06 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Shamdasani", "Jetendr", ""], ["McClatchey", "Richard", ""], ["Branson", "Andrew", ""], ["Kovacs", "Zsolt", ""]]}, {"id": "1511.09061", "submitter": "Richard McClatchey", "authors": "Khawar Hasham, Kamran Munir and Richard McClatchey", "title": "Using Cloud-Aware Provenance to Reproduce Scientific Workflow Execution\n  on Cloud", "comments": "10 pages, 5 figures, 1 table. Proc of the 5th International\n  Conference on Cloud Computing and Services Science (CLOSER) Lisbon MAy 2015.\n  arXiv admin note: substantial text overlap with arXiv:1502.01539", "journal-ref": null, "doi": "10.5220/0005452800490059", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Provenance has been thought of a mechanism to verify a workflow and to\nprovide workflow reproducibility. This provenance of scientific workflows has\nbeen effectively carried out in Grid based scientific workflow systems.\nHowever, recent adoption of Cloud-based scientific workflows present an\nopportunity to investigate the suitability of existing approaches or propose\nnew approaches to collect provenance information from the Cloud and to utilize\nit for workflow repeatability in the Cloud infrastructure. This paper presents\na novel approach that can assist in mitigating this challenge. This approach\ncan collect Cloud infrastructure information from an outside Cloud client along\nwith workflow provenance and can establish a mapping between them. This mapping\nis later used to re-provision resources on the Cloud for workflow execution.\nThe reproducibility of the workflow execution is performed by: (a) capturing\nthe Cloud infrastructure information (virtual machine configuration) along with\nthe workflow provenance, (b) re-provisioning the similar resources on the Cloud\nand re-executing the workflow on them and (c) by comparing the outputs of\nworkflows. The evaluation of the prototype suggests that the proposed approach\nis feasible and can be investigated further. Moreover, there is no reference\nreproducibility model exists in literature that can provide guidelines to\nachieve this goal in Cloud. This paper also attempts to present a model that is\nused in the proposed design to achieve workflow reproducibility in the Cloud\nenvironment.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2015 18:54:58 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Hasham", "Khawar", ""], ["Munir", "Kamran", ""], ["McClatchey", "Richard", ""]]}, {"id": "1511.09065", "submitter": "Richard McClatchey", "authors": "Richard McClatchey, Jetendr Shamdasani, Andrew Branson, Kamran Munir\n  and Zsolt Kovacs", "title": "Traceability and Provenance in Big Data Medical Systems", "comments": "6 pages, 3 diagrams. Proc of the 28th Int Symposium on Computer-Based\n  Medical Systems (CBMS 2015) Sao Carlos, Brazil. June 2015. arXiv admin note:\n  text overlap with arXiv:1502.01545", "journal-ref": null, "doi": "10.1109/CBMS.2015.10", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Providing an appropriate level of accessibility to and tracking of data or\nprocess elements in large volumes of medical data, is an essential requirement\nin the Big Data era. Researchers require systems that provide traceability of\ninformation through provenance data capture and management to support their\nclinical analyses. We present an approach that has been adopted in the neuGRID\nand N4U projects, which aimed to provide detailed traceability to support\nresearch analysis processes in the study of biomarkers for Alzheimers disease,\nbut is generically applicable across medical systems. To facilitate the\norchestration of complex, large-scale analyses in these projects we have\nadapted CRISTAL, a workflow and provenance tracking solution. The use of\nCRISTAL has provided a rich environment for neuroscientists to track and manage\nthe evolution of data and workflow usage over time in neuGRID and N4U.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2015 19:05:31 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["McClatchey", "Richard", ""], ["Shamdasani", "Jetendr", ""], ["Branson", "Andrew", ""], ["Munir", "Kamran", ""], ["Kovacs", "Zsolt", ""]]}, {"id": "1511.09066", "submitter": "Richard McClatchey", "authors": "Kamran Munir, Khawar Hasham Ahmad and Richard McClatchey", "title": "Development of a Large-scale Neuroimages and Clinical Variables Data\n  Atlas in the neuGRID4You (N4U) project", "comments": "35 pages, 15 figures, Journal of Biomedical Informatics, 2015", "journal-ref": null, "doi": "10.1016/j.jbi.2015.08.004", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exceptional growth in the availability of large-scale clinical imaging\ndatasets has led to the development of computational infrastructures offering\nscientists access to image repositories and associated clinical variables data.\nThe EU FP7 neuGRID and its follow on neuGRID4You (N4U) project is a leading\ne-Infrastructure where neuroscientists can find core services and resources for\nbrain image analysis. The core component of this e-Infrastructure is the N4U\nVirtual Laboratory, which offers an easy access for neuroscientists to a wide\nrange of datasets and algorithms, pipelines, computational resources, services,\nand associated support services. The foundation of this virtual laboratory is a\nmassive data store plus information services called the Data Atlas that stores\ndatasets, clinical study data, data dictionaries, algorithm/pipeline\ndefinitions, and provides interfaces for parameterised querying so that\nneuroscientists can perform analyses on required datasets. This paper presents\nthe overall design and development of the Data Atlas, its associated datasets\nand indexing and a set of retrieval services that originated from the\ndevelopment of the N4U Virtual Laboratory in the EU FP7 N4U project in the\nlight of user requirements.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2015 19:14:53 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Munir", "Kamran", ""], ["Ahmad", "Khawar Hasham", ""], ["McClatchey", "Richard", ""]]}, {"id": "1511.09116", "submitter": "Bridger Hahn", "authors": "Nolan Donoghue, Bridger Hahn, Helen Xu, Thomas Kroeger, David Zage and\n  Rob Johnson", "title": "Tracking Network Events with Write Optimized Data Structures: The Design\n  and Implementation of TWIAD: The Write-Optimized IP Address Database", "comments": "7 pages, 2 figures, 6 tables. Submitted and accepted to BADGERS 2015\n  at RAID 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Access to network traffic records is an integral part of recognizing and\naddressing network security breaches. Even with the increasing sophistication\nof network attacks, basic network events such as connections between two IP\naddresses play an important role in any network defense. Given the duration of\ncurrent attacks, long-term data archival is critical but typically very little\nof the data is ever accessed. Previous work has provided tools and identified\nthe need to trace connections. However, traditional databases raise performance\nconcerns as they are optimized for querying rather than ingestion.\n  The study of write-optimized data structures (WODS) is a new and growing\nfield that provides a novel approach to traditional storage structures (e.g.,\nB-trees). WODS trade minor degradations in query performance for significant\ngains in the ability to quickly insert more data elements, typically on the\norder of 10 to 100 times more inserts per second. These efficient,\nout-of-memory data structures can play a critical role in enabling robust,\nlong-term tracking of network events.\n  In this paper, we present TWIAD, the Write-optimized IP Address Database.\nTWIAD uses a write-optimized B-tree known as a B {\\epsilon} tree to track all\nIP address connections in a network traffic stream. Our initial implementation\nfocuses on utilizing lower cost hardware, demonstrating that basic long-term\ntracking can be done without advanced equipment. We tested TWIAD on a modest\ndesktop system and showed a sustained ingestion rate of about 20,000 inserts\nper second.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 00:23:34 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Donoghue", "Nolan", ""], ["Hahn", "Bridger", ""], ["Xu", "Helen", ""], ["Kroeger", "Thomas", ""], ["Zage", "David", ""], ["Johnson", "Rob", ""]]}]