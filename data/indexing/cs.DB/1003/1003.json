[{"id": "1003.1010", "submitter": "Anca Muscholl", "authors": "Blaise Genest (INRIA - Irisa), Anca Muscholl (LaBRI), Zhilin Wu\n  (LaBRI)", "title": "Verifying Recursive Active Documents with Positive Data Tree Rewriting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.OH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a data tree-rewriting framework for modeling evolving\ndocuments. The framework is close to Guarded Active XML, a platform used for\nhandling XML repositories evolving through web services. We focus on automatic\nverification of properties of evolving documents that can contain data from an\ninfinite domain. We establish the boundaries of decidability, and show that\nverification of a {\\em positive} fragment that can handle recursive service\ncalls is decidable. We also consider bounded model-checking in our data\ntree-rewriting framework and show that it is $\\nexptime$-complete.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2010 10:31:01 GMT"}], "update_date": "2010-03-05", "authors_parsed": [["Genest", "Blaise", "", "INRIA - Irisa"], ["Muscholl", "Anca", "", "LaBRI"], ["Wu", "Zhilin", "", "LaBRI"]]}, {"id": "1003.1018", "submitter": "Matjaz Perc", "authors": "Matjaz Perc", "title": "Zipf's law and log-normal distributions in measures of scientific output\n  across fields and institutions: 40 years of Slovenia's research as an example", "comments": "8 pages, 3 figures; accepted for publication in Journal of\n  Informetrics [supplementary material available at\n  http://www.matjazperc.com/sicris/stats.html]", "journal-ref": "Journal of Informetrics 4 (2010) 358-364", "doi": "10.1016/j.joi.2010.03.001", "report-no": null, "categories": "physics.data-an cs.DB stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Slovenia's Current Research Information System (SICRIS) currently hosts\n86,443 publications with citation data from 8,359 researchers working on the\nwhole plethora of social and natural sciences from 1970 till present. Using\nthese data, we show that the citation distributions derived from individual\npublications have Zipfian properties in that they can be fitted by a power law\n$P(x) \\sim x^{-\\alpha}$, with $\\alpha$ between 2.4 and 3.1 depending on the\ninstitution and field of research. Distributions of indexes that quantify the\nsuccess of researchers rather than individual publications, on the other hand,\ncannot be associated with a power law. We find that for Egghe's g-index and\nHirsch's h-index the log-normal form $P(x) \\sim \\exp[-a\\ln x -b(\\ln x)^2]$\napplies best, with $a$ and $b$ depending moderately on the underlying set of\nresearchers. In special cases, particularly for institutions with a strongly\nhierarchical constitution and research fields with high self-citation rates,\nexponential distributions can be observed as well. Both indexes yield\ndistributions with equivalent statistical properties, which is a strong\nindicator for their consistency and logical connectedness. At the same time,\ndifferences in the assessment of citation histories of individual researchers\nstrengthen their importance for properly evaluating the quality and impact of\nscientific output.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2010 11:48:28 GMT"}], "update_date": "2010-06-08", "authors_parsed": [["Perc", "Matjaz", ""]]}, {"id": "1003.1179", "submitter": "Diego Calvanese", "authors": "Diego Calvanese, Giuseppe De Giacomo, Maurizio Lenzerini, Moshe Y.\n  Vardi", "title": "View Synthesis from Schema Mappings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In data management, and in particular in data integration, data exchange,\nquery optimization, and data privacy, the notion of view plays a central role.\nIn several contexts, such as data integration, data mashups, and data\nwarehousing, the need arises of designing views starting from a set of known\ncorrespondences between queries over different schemas. In this paper we deal\nwith the issue of automating such a design process. We call this novel problem\n\"view synthesis from schema mappings\": given a set of schema mappings, each\nrelating a query over a source schema to a query over a target schema,\nautomatically synthesize for each source a view over the target schema in such\na way that for each mapping, the query over the source is a rewriting of the\nquery over the target wrt the synthesized views. We study view synthesis from\nschema mappings both in the relational setting, where queries and views are\n(unions of) conjunctive queries, and in the semistructured data setting, where\nqueries and views are (two-way) regular path queries, as well as unions of\nconjunctions thereof. We provide techniques and complexity upper bounds for\neach of these cases.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2010 03:24:13 GMT"}], "update_date": "2010-03-08", "authors_parsed": [["Calvanese", "Diego", ""], ["De Giacomo", "Giuseppe", ""], ["Lenzerini", "Maurizio", ""], ["Vardi", "Moshe Y.", ""]]}, {"id": "1003.1251", "submitter": "Arnab Bhattacharya", "authors": "Viswanath Gunturi, Shashi Shekhar, Arnab Bhattacharya", "title": "Minimum Spanning Tree on Spatio-Temporal Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a spatio-temporal network (ST network) where edge properties vary with\ntime, a time-sub-interval minimum spanning tree (TSMST) is a collection of\nminimum spanning trees of the ST network, where each tree is associated with a\ntime interval. During this time interval, the total cost of tree is least among\nall the spanning trees. The TSMST problem aims to identify a collection of\ndistinct minimum spanning trees and their respective time-sub-intervals under\nthe constraint that the edge weight functions are piecewise linear. This is an\nimportant problem in ST network application domains such as wireless sensor\nnetworks (e.g., energy efficient routing). Computing TSMST is challenging\nbecause the ranking of candidate spanning trees is non-stationary over a given\ntime interval. Existing methods such as dynamic graph algorithms and kinetic\ndata structures assume separable edge weight functions. In contrast, we propose\nnovel algorithms to find TSMST for large ST networks by accounting for both\nseparable and non-separable piecewise linear edge weight functions. The\nalgorithms are based on the ordering of edges in edge-order-intervals and\nintersection points of edge weight functions.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2010 12:39:18 GMT"}, {"version": "v2", "created": "Fri, 21 May 2010 12:34:40 GMT"}], "update_date": "2010-05-24", "authors_parsed": [["Gunturi", "Viswanath", ""], ["Shekhar", "Shashi", ""], ["Bhattacharya", "Arnab", ""]]}, {"id": "1003.1500", "submitter": "Rdv Ijcsis", "authors": "M .V.Vijaya Saradhi, B. R. Sastry, P.Satish", "title": "Hierarchical Approach for Online Mining--Emphasis towards Software\n  Metrics", "comments": "Pages IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS, Vol. 7 No. 2, February 2010, USA. ISSN 1947\n  5500, http://sites.google.com/site/ijcsis/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Several multi-pass algorithms have been proposed for Association Rule Mining\nfrom static repositories. However, such algorithms are incapable of online\nprocessing of transaction streams. In this paper we introduce an efficient\nsingle-pass algorithm for mining association rules, given a hierarchical\nclassification amongest items. Processing efficiency is achieved by utilizing\ntwo optimizations, hierarchy aware counting and transaction reduction, which\nbecome possible in the context of hierarchical classification. This paper\nconsiders the problem of integrating constraints that are Boolean expression\nover the presence or absence of items into the association discovery algorithm.\nThis paper present three integrated algorithms for mining association rules\nwith item constraints and discuss their tradeoffs. It is concluded that the\nvariation of complexity depends on the measure of DIT (Depth of Inheritance\nTree) and NOC (Number of Children) in the context of Hierarchical\nClassification.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2010 17:44:18 GMT"}], "update_date": "2010-04-28", "authors_parsed": [["Saradhi", "M . V. Vijaya", ""], ["Sastry", "B. R.", ""], ["Satish", "P.", ""]]}, {"id": "1003.1816", "submitter": "Rdv Ijcsis", "authors": "Sabyasachi Pattanaik, Partha Pratim Ghosh", "title": "Role of Data Mining in E-Payment systems", "comments": "Pages IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS, Vol. 7 No. 2, February 2010, USA. ISSN 1947\n  5500, http://sites.google.com/site/ijcsis/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Data Mining deals extracting hidden knowledge, unexpected pattern and new\nrules from large database. Various customized data mining tools have been\ndeveloped for domain specific applications such as Biomedicine, DNA analysis\nand telecommunication. Trends in data mining include further efforts towards\nthe exploration of new application areas and methods for handling complex data\ntypes, algorithm scalability, constraint based data mining and visualization\nmethods. In this paper we will present domain specific Secure Multiparty\ncomputation technique and applications. Data mining has matured as a field of\nbasic and applied research in computer science in general. In this paper, we\nsurvey some of the recent approaches and architectures where data mining has\nbeen applied in the fields of e-payment systems. In this paper we limit our\ndiscussion to data mining in the context of e-payment systems. We also mention\na few directions for further work in this domain, based on the survey.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2010 07:31:37 GMT"}], "update_date": "2010-03-11", "authors_parsed": [["Pattanaik", "Sabyasachi", ""], ["Ghosh", "Partha Pratim", ""]]}, {"id": "1003.2586", "submitter": "Francesca A. Lisi", "authors": "Francesca A. Lisi", "title": "Inductive Logic Programming in Databases: from Datalog to DL+log", "comments": "30 pages, 3 figures, 2 tables.", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address an issue that has been brought to the attention of\nthe database community with the advent of the Semantic Web, i.e. the issue of\nhow ontologies (and semantics conveyed by them) can help solving typical\ndatabase problems, through a better understanding of KR aspects related to\ndatabases. In particular, we investigate this issue from the ILP perspective by\nconsidering two database problems, (i) the definition of views and (ii) the\ndefinition of constraints, for a database whose schema is represented also by\nmeans of an ontology. Both can be reformulated as ILP problems and can benefit\nfrom the expressive and deductive power of the KR framework DL+log. We\nillustrate the application scenarios by means of examples. Keywords: Inductive\nLogic Programming, Relational Databases, Ontologies, Description Logics, Hybrid\nKnowledge Representation and Reasoning Systems. Note: To appear in Theory and\nPractice of Logic Programming (TPLP).\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2010 17:40:43 GMT"}], "update_date": "2010-03-15", "authors_parsed": [["Lisi", "Francesca A.", ""]]}, {"id": "1003.2682", "submitter": "David Spivak", "authors": "David I. Spivak", "title": "Table manipulation in simplicial databases", "comments": "8 pages.", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In \\cite{Spi}, we developed a category of databases in which the schema of a\ndatabase is represented as a simplicial set. Each simplex corresponds to a\ntable in the database. There, our main concern was to find a categorical\nformulation of databases; the simplicial nature of the schemas was to some\ndegree unexpected and unexploited.\n  In the present note, we show how to use this geometric formulation\neffectively on a computer. If we think of each simplex as a polygonal tile, we\ncan imagine assembling custom databases by mixing and matching tiles. Queries\non this database can be performed by drawing paths through the resulting tile\nformations, selecting records at the start-point of this path and retrieving\ncorresponding records at its end-point.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2010 06:22:07 GMT"}], "update_date": "2010-03-16", "authors_parsed": [["Spivak", "David I.", ""]]}, {"id": "1003.3082", "submitter": "Achmad Benny Mutiara", "authors": "L.Y. Banowosari, I.W.S. Wicaksana, and A.B. Mutiara", "title": "Agreement Maintenance Based on Schema and Ontology Change in P2P\n  Environment", "comments": "6 pages, the *11th Annual International Seminar on Global Meltdown or\n  Recession: India vis-\\`a-vis the rest of the world, January 4-5, 2010 at New\n  Delhi.", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concern about developing a semantic agreement maintenance\nmethod based on semantic distance by calculating the change of local schema or\nontology. This approach is important in dynamic and autonomous environment, in\nwhich the current approach assumed that agreement or mapping in static\nenvironment. The contribution of this research is to develop a framework based\non semantic agreement maintenance approach for P2P environment. This framework\nbased on two level hybrid P2P model architecture, which consist of two peer\ntype: (1) super peer that use to register and manage the other peers, and (2)\nsimple peer, as a simple peer, it exports and shares its contents with others.\nThis research develop a model to maintain the semantic agreement in P2P\nenvironment, so the current approach which does not have the mechanism to know\nthe change, since it assumed that ontology and local schema are in the static\ncondition, and it is different in dynamic condition. The main issues are how to\ncalculate the change of local schema or common ontology and the calculation\nresult is used to determine which algorithm in maintaining the agreement. The\nexperiment on the job matching domain in Indonesia have been done to show how\nfar the performance of the approach. From the experiment, the main result are\n(i) the more change so the F-measure value tend to be decreased, (ii) there is\nno significant different in F-measure value for various modification type (add,\ndelete, rename), and (iii) the correct choice of algorithm would improve the\nF-measure value.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2010 05:28:48 GMT"}], "update_date": "2010-03-17", "authors_parsed": [["Banowosari", "L. Y.", ""], ["Wicaksana", "I. W. S.", ""], ["Mutiara", "A. B.", ""]]}, {"id": "1003.3139", "submitter": "Andrea Cal\\`i", "authors": "Andrea Cali and Davide Martinenghi", "title": "Querying Incomplete Data over Extended ER Schemata", "comments": "40 pages, 1 figure.", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since Chen's Entity-Relationship (ER) model, conceptual modeling has been\nplaying a fundamental role in relational data design. In this paper we consider\nan extended ER (EER) model enriched with cardinality constraints, disjointness\nassertions, and is-a relations among both entities and relationships. In this\nsetting, we consider the case of incomplete data, which is likely to occur, for\ninstance, when data from different sources are integrated. In such a context,\nwe address the problem of providing correct answers to conjunctive queries by\nreasoning on the schema. Based on previous results about decidability of the\nproblem, we provide a query answering algorithm that performs rewriting of the\ninitial query into a recursive Datalog query encoding the information about the\nschema. We finally show extensions to more general settings. This paper will\nappear in the special issue of Theory and Practice of Logic Programming (TPLP)\ntitled Logic Programming in Databases: From Datalog to Semantic-Web Rules.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2010 12:55:21 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2010 19:37:01 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Cali", "Andrea", ""], ["Martinenghi", "Davide", ""]]}, {"id": "1003.3370", "submitter": "Yeb Havinga", "authors": "Yeb Havinga, Willem Dijkstra, Ander de Keijzer", "title": "Adding HL7 version 3 data types to PostgreSQL", "comments": "12 pages, 9 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The HL7 standard is widely used to exchange medical information\nelectronically. As a part of the standard, HL7 defines scalar communication\ndata types like physical quantity, point in time and concept descriptor but\nalso complex types such as interval types, collection types and probabilistic\ntypes. Typical HL7 applications will store their communications in a database,\nresulting in a translation from HL7 concepts and types into database types.\nSince the data types were not designed to be implemented in a relational\ndatabase server, this transition is cumbersome and fraught with programmer\nerror. The purpose of this paper is two fold. First we analyze the HL7 version\n3 data type definitions and define a number of conditions that must be met, for\nthe data type to be suitable for implementation in a relational database. As a\nresult of this analysis we describe a number of possible improvements in the\nHL7 specification. Second we describe an implementation in the PostgreSQL\ndatabase server and show that the database server can effectively execute\nscientific calculations with units of measure, supports a large number of\noperations on time points and intervals, and can perform operations that are\nakin to a medical terminology server. Experiments on synthetic data show that\nthe user defined types perform better than an implementation that uses only\nstandard data types from the database server.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2010 14:01:19 GMT"}], "update_date": "2010-03-18", "authors_parsed": [["Havinga", "Yeb", ""], ["Dijkstra", "Willem", ""], ["de Keijzer", "Ander", ""]]}, {"id": "1003.3536", "submitter": "Bin Jiang", "authors": "Bin Jiang and Xintao Liu", "title": "Computing the Fewest-turn Map Directions based on the Connectivity of\n  Natural Roads", "comments": "12 pages, 5 figures, and 4 tables, language editing, some significant\n  revisions, missing references added", "journal-ref": "International Journal of Geographical Information Science, 25(7),\n  2011, 1069-1082", "doi": null, "report-no": null, "categories": "cs.CG cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduced a novel approach to computing the fewest-turn\nmap directions or routes based on the concept of natural roads. Natural roads\nare joined road segments that perceptually constitute good continuity. This\napproach relies on the connectivity of natural roads rather than that of road\nsegments for computing routes or map directions. Because of this, the derived\nroutes posses the fewest turns. However, what we intend to achieve are the\nroutes that not only possess the fewest turns, but are also as short as\npossible. This kind of map direction is more effective and favorable by people,\nbecause they bear less cognitive burden. Furthermore, the computation of the\nroutes is more efficient, since it is based on the graph encoding the\nconnectivity of roads, which is significantly smaller than the graph of road\nsegments. We made experiments applied to eight urban street networks from North\nAmerica and Europe in order to illustrate the above stated advantages. The\nexperimental results indicate that the fewest-turn routes posses fewer turns\nand shorter distances than the simplest paths and the routes provided by Google\nMaps. For example, the fewest-turn-and-shortest routes are on average 15%\nshorter than the routes suggested by Google Maps, while the number of turns is\njust half as much. This approach is a key technology behind FromToMap.org - a\nweb mapping service using openstreetmap data.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2010 09:49:19 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2010 18:48:52 GMT"}, {"version": "v3", "created": "Fri, 14 May 2010 22:56:14 GMT"}, {"version": "v4", "created": "Sat, 21 Aug 2010 10:40:28 GMT"}], "update_date": "2013-07-17", "authors_parsed": [["Jiang", "Bin", ""], ["Liu", "Xintao", ""]]}, {"id": "1003.4066", "submitter": "William Jackson", "authors": "S. Vidhya, S. Karthikeyan", "title": "A Security Based Data Mining Approach in Data Grid", "comments": null, "journal-ref": "Journal of Computing, Volume 2, Issue 3, March 2010,\n  https://sites.google.com/site/journalofcomputing/", "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grid computing is the next logical step to distributed computing. Main\nobjective of grid computing is an innovative approach to share resources such\nas CPU usage; memory sharing and software sharing. Data Grids provide\ntransparent access to semantically related data resources in a heterogeneous\nsystem. The system incorporates both data mining and grid computing techniques\nwhere Grid application reduces the time for sending results to several clients\nat the same time and Data mining application on computational grids gives fast\nand sophisticated results to users. In this work, grid based data mining\ntechnique is used to do automatic allocation based on probabilistic mining\nfrequent sequence algorithm. It finds frequent sequences for many users at a\ntime with accurate result. It also includes the trust management architecture\nfor trust enhanced security.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2010 05:30:53 GMT"}], "update_date": "2010-03-23", "authors_parsed": [["Vidhya", "S.", ""], ["Karthikeyan", "S.", ""]]}, {"id": "1003.4068", "submitter": "William Jackson", "authors": "Pratima Gautam, K. R. Pardasani", "title": "A Novel Approach For Discovery Multi Level Fuzzy Association Rule Mining", "comments": null, "journal-ref": "Journal of Computing, Volume 2, Issue 3, March 2010,\n  https://sites.google.com/site/journalofcomputing/", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding multilevel association rules in transaction databases is most\ncommonly seen in is widely used in data mining. In this paper, we present a\nmodel of mining multilevel association rules which satisfies the different\nminimum support at each level, we have employed fuzzy set concepts, multi-level\ntaxonomy and different minimum supports to find fuzzy multilevel association\nrules in a given transaction data set. Apriori property is used in model to\nprune the item sets. The proposed model adopts a topdown progressively\ndeepening approach to derive large itemsets. This approach incorporates fuzzy\nboundaries instead of sharp boundary intervals. An example is also given to\ndemonstrate and support that the proposed mining algorithm can derive the\nmultiple-level association rules under different supports in a simple and\neffective manner.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2010 05:38:24 GMT"}], "update_date": "2010-03-23", "authors_parsed": [["Gautam", "Pratima", ""], ["Pardasani", "K. R.", ""]]}, {"id": "1003.4076", "submitter": "William Jackson", "authors": "M. S. Danessh, C. Balasubramanian, K. Duraiswamy", "title": "Similarity Data Item Set Approach: An Encoded Temporal Data Base\n  Technique", "comments": null, "journal-ref": "Journal of Computing, Volume 2, Issue 3, March 2010,\n  https://sites.google.com/site/journalofcomputing/", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data mining has been widely recognized as a powerful tool to explore added\nvalue from large-scale databases. Finding frequent item sets in databases is a\ncrucial in data mining process of extracting association rules. Many algorithms\nwere developed to find the frequent item sets. This paper presents a summary\nand a comparative study of the available FP-growth algorithm variations\nproduced for mining frequent item sets showing their capabilities and\nefficiency in terms of time and memory consumption on association rule mining\nby taking application of specific information into account. It proposes pattern\ngrowth mining paradigm based FP-tree growth algorithm, which employs a tree\nstructure to compress the database. The performance study shows that the anti-\nFP-growth method is efficient and scalable for mining both long and short\nfrequent patterns and is about an order of magnitude faster than the Apriority\nalgorithm and also faster than some recently reported new frequent-pattern\nmining.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2010 06:23:56 GMT"}], "update_date": "2010-03-23", "authors_parsed": [["Danessh", "M. S.", ""], ["Balasubramanian", "C.", ""], ["Duraiswamy", "K.", ""]]}, {"id": "1003.4353", "submitter": "Kim Nguyen", "authors": "Sebastian Maneth and Kim Nguyen", "title": "XPath Whole Query Optimization", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous work reports about SXSI, a fast XPath engine which executes tree\nautomata over compressed XML indexes. Here, reasons are investigated why SXSI\nis so fast. It is shown that tree automata can be used as a general framework\nfor fine grained XML query optimization. We define the \"relevant nodes\" of a\nquery as those nodes that a minimal automaton must touch in order to answer the\nquery. This notion allows to skip many subtrees during execution, and, with the\nhelp of particular tree indexes, even allows to skip internal nodes of the\ntree. We efficiently approximate runs over relevant nodes by means of\non-the-fly removal of alternation and non-determinism of (alternating) tree\nautomata. We also introduce many implementation techniques which allows us to\nefficiently evaluate tree automata, even in the absence of special indexes.\nThrough extensive experiments, we demonstrate the impact of the different\noptimization techniques.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2010 09:11:11 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2010 04:49:44 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Maneth", "Sebastian", ""], ["Nguyen", "Kim", ""]]}, {"id": "1003.4418", "submitter": "Stefan Endrullis", "authors": "Stefan Endrullis, Andreas Thor, Erhard Rahm", "title": "Evaluation of Query Generators for Entity Search Engines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic web applications such as mashups need efficient access to web data\nthat is only accessible via entity search engines (e.g. product or publication\nsearch engines). However, most current mashup systems and applications only\nsupport simple keyword searches for retrieving data from search engines. We\npropose the use of more powerful search strategies building on so-called query\ngenerators. For a given set of entities query generators are able to\nautomatically determine a set of search queries to retrieve these entities from\nan entity search engine. We demonstrate the usefulness of query generators for\non-demand web data integration and evaluate the effectiveness and efficiency of\nquery generators for a challenging real-world integration scenario.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2010 13:55:40 GMT"}], "update_date": "2010-03-24", "authors_parsed": [["Endrullis", "Stefan", ""], ["Thor", "Andreas", ""], ["Rahm", "Erhard", ""]]}, {"id": "1003.4827", "submitter": "Jos\\'e Martinez", "authors": "Jos\\'e Martinez (LINA), Carmelo Malta", "title": "Tuple-based abstract data types: full parallelism", "comments": null, "journal-ref": "International Symposium on Computer and Information Sciences\n  (ISCIS'92), Antalya : Turkey (1992)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commutativity has the same inherent limitations as compatibility. Then, it is\nworth conceiving simple concurrency control techniques. We propose a restricted\nform of commutativity which increases parallelism without incurring a higher\noverhead than compatibility. Advantages of our proposition are: (1)\ncommutativity of operations is determined at compile-time, (2) run-time\nchecking is as efficient as for compatibility, (3) neither commutativity\nrelations, (4) nor inverse operations, need to be specified, and (5) log space\nutilization is reduced.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2010 09:23:18 GMT"}], "update_date": "2010-04-08", "authors_parsed": [["Martinez", "Jos\u00e9", "", "LINA"], ["Malta", "Carmelo", ""]]}, {"id": "1003.4828", "submitter": "Carmelo Malta", "authors": "Carmelo Malta, Jos\\'e Martinez (LINA)", "title": "A framework for designing concurrent and recoverable abstract data types\n  based on commutativity", "comments": null, "journal-ref": "International Symposium on Computer and Information Sciences\n  (ISCIS'91), Side : Turkey (1991)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we try to focus the reader's interest on the problems that\ntransactional systems have to resolve for taking advantage of commutativity in\na serializable and recoverable way. Our framework is, (as others), based on the\nuse of conditional commutativity on abstract date types. We present new\nfeatures that have not been found in the literature hitherto, that both\nincrease concurrency and simplify recovery.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2010 09:33:35 GMT"}], "update_date": "2010-04-08", "authors_parsed": [["Malta", "Carmelo", "", "LINA"], ["Martinez", "Jos\u00e9", "", "LINA"]]}, {"id": "1003.4830", "submitter": "Carmelo Malta", "authors": "Carmelo Malta, Jos\\'e Martinez (LINA)", "title": "Limits of Commutativity on Abstract Data Types", "comments": null, "journal-ref": "5th International Conference on Information Systems and Management\n  of Data (CISMOD'92), Bangalore : India (1992)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present some formal properties of (symmetrical) commutativity, the major\ncriterion used in transactional systems, which allow us to fully understand its\nadvantages and disadvantages. The main result is that commutativity is subject\nto the same limitation as compatibility for arbitrary objects. However,\ncommutativity has also a number of attracting properties, one of which is\nrelated to recovery and, to our knowledge, has not been exploited in the\nliterature. Advantages and disadvantages are illustrated on abstract data types\nof interest. We also show how limits of commutativity have been circumvented,\nwhich gives guidelines for doing so (or not!).\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2010 09:52:39 GMT"}], "update_date": "2010-04-08", "authors_parsed": [["Malta", "Carmelo", "", "LINA"], ["Martinez", "Jos\u00e9", "", "LINA"]]}, {"id": "1003.4836", "submitter": "Carmelo Malta", "authors": "Carmelo Malta, Jos\\'e Martinez (LINA)", "title": "Automating Fine Concurrency Control in Object-Oriented Databases", "comments": null, "journal-ref": "IEEE 9th International Conference on Data Engineering (ICDE'93),\n  Vienn : Austria (1993)", "doi": "10.1109/ICDE.1993.344057", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several propositions were done to provide adapted concurrency control to\nobject-oriented databases. However, most of these proposals miss the fact that\nconsidering solely read and write access modes on instances may lead to less\nparallelism than in relational databases! This paper cope with that issue, and\nadvantages are numerous: (1) commutativity of methods is determined a priori\nand automatically by the compiler, without measurable overhead, (2) run-time\nchecking of commutativity is as efficient as for compatibility, (3) inverse\noperations need not be specified for recovery, (4) this scheme does not\npreclude more sophisticated approaches, and, last but not least, (5) relational\nand object-oriented concurrency control schemes with read and write access\nmodes are subsumed under this proposition.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2010 09:55:48 GMT"}], "update_date": "2010-03-26", "authors_parsed": [["Malta", "Carmelo", "", "LINA"], ["Martinez", "Jos\u00e9", "", "LINA"]]}, {"id": "1003.5056", "submitter": "Sebastien Nedjar", "authors": "Sebastien Nedjar (LIF), Alain Casali (LIF), Rosine Cicchetti (LIF),\n  Lotfi Lakhal (LIF)", "title": "Cubes convexes", "comments": null, "journal-ref": "Revue des Sciences et Technologies de l'Information (S\\'erie\n  Ing\\'enierie des Syst\\`emes d'Information) 11, 6 (2006) 11-31", "doi": "10.3166/isi.11.6.11-31", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In various approaches, data cubes are pre-computed in order to answer\nefficiently OLAP queries. The notion of data cube has been declined in various\nways: iceberg cubes, range cubes or differential cubes. In this paper, we\nintroduce the concept of convex cube which captures all the tuples of a\ndatacube satisfying a constraint combination. It can be represented in a very\ncompact way in order to optimize both computation time and required storage\nspace. The convex cube is not an additional structure appended to the list of\ncube variants but we propose it as a unifying structure that we use to\ncharacterize, in a simple, sound and homogeneous way, the other quoted types of\ncubes. Finally, we introduce the concept of emerging cube which captures the\nsignificant trend inversions. characterizations.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2010 07:46:33 GMT"}], "update_date": "2010-04-08", "authors_parsed": [["Nedjar", "Sebastien", "", "LIF"], ["Casali", "Alain", "", "LIF"], ["Cicchetti", "Rosine", "", "LIF"], ["Lakhal", "Lotfi", "", "LIF"]]}, {"id": "1003.5080", "submitter": "Xiaokui Xiao", "authors": "Xiaokui Xiao, Yufei Tao, Nick Koudas", "title": "Transparent Anonymization: Thwarting Adversaries Who Know the Algorithm", "comments": "To appear in the ACM Transaction on Database Systems (TODS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous generalization techniques have been proposed for privacy preserving\ndata publishing. Most existing techniques, however, implicitly assume that the\nadversary knows little about the anonymization algorithm adopted by the data\npublisher. Consequently, they cannot guard against privacy attacks that exploit\nvarious characteristics of the anonymization mechanism. This paper provides a\npractical solution to the above problem. First, we propose an analytical model\nfor evaluating disclosure risks, when an adversary knows everything in the\nanonymization process, except the sensitive values. Based on this model, we\ndevelop a privacy principle, transparent l-diversity, which ensures privacy\nprotection against such powerful adversaries. We identify three algorithms that\nachieve transparent l-diversity, and verify their effectiveness and efficiency\nthrough extensive experiments with real data.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2010 08:19:02 GMT"}], "update_date": "2010-03-29", "authors_parsed": [["Xiao", "Xiaokui", ""], ["Tao", "Yufei", ""], ["Koudas", "Nick", ""]]}, {"id": "1003.5350", "submitter": "Ian Mackie", "authors": "Daniel J. Dougherty", "title": "An Improved Algorithm for Generating Database Transactions from\n  Relational Algebra Specifications", "comments": null, "journal-ref": "EPTCS 21, 2010, pp. 77-89", "doi": "10.4204/EPTCS.21.7", "report-no": null, "categories": "cs.DB cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alloy is a lightweight modeling formalism based on relational algebra. In\nprior work with Fisler, Giannakopoulos, Krishnamurthi, and Yoo, we have\npresented a tool, Alchemy, that compiles Alloy specifications into\nimplementations that execute against persistent databases. The foundation of\nAlchemy is an algorithm for rewriting relational algebra formulas into code for\ndatabase transactions. In this paper we report on recent progress in improving\nthe robustness and efficiency of this transformation.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2010 08:11:05 GMT"}], "update_date": "2010-03-31", "authors_parsed": [["Dougherty", "Daniel J.", ""]]}]