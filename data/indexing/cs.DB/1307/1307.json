[{"id": "1307.0191", "submitter": "A B M Moniruzzaman Mr.", "authors": "A B M Moniruzzaman and Syed Akhter Hossain", "title": "NoSQL Database: New Era of Databases for Big data Analytics -\n  Classification, Characteristics and Comparison", "comments": "14 pages, 10 figures, 44 references used and with authors biographies", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Digital world is growing very fast and become more complex in the volume\n(terabyte to petabyte), variety (structured and un-structured and hybrid),\nvelocity (high speed in growth) in nature. This refers to as Big Data that is a\nglobal phenomenon. This is typically considered to be a data collection that\nhas grown so large it can not be effectively managed or exploited using\nconventional data management tools: e.g., classic relational database\nmanagement systems (RDBMS) or conventional search engines. To handle this\nproblem, traditional RDBMS are complemented by specifically designed a rich set\nof alternative DBMS; such as - NoSQL, NewSQL and Search-based systems. This\npaper motivation is to provide - classification, characteristics and evaluation\nof NoSQL databases in Big Data Analytics. This report is intended to help\nusers, especially to the organizations to obtain an independent understanding\nof the strengths and weaknesses of various NoSQL database approaches to\nsupporting applications that process huge volumes of data.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2013 09:53:16 GMT"}], "update_date": "2013-07-02", "authors_parsed": [["Moniruzzaman", "A B M", ""], ["Hossain", "Syed Akhter", ""]]}, {"id": "1307.0193", "submitter": "Supriya Nirkhiwale", "authors": "Supriya Nirkhiwale, Alin Dobra, Chris Jermaine", "title": "A Sampling Algebra for Aggregate Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As of 2005, sampling has been incorporated in all major database systems.\nWhile efficient sampling techniques are realizable, determining the accuracy of\nan estimate obtained from the sample is still an unresolved problem. In this\npaper, we present a theoretical framework that allows an elegant treatment of\nthe problem. We base our work on generalized uniform sampling (GUS), a class of\nsampling methods that subsumes a wide variety of sampling techniques. We\nintroduce a key notion of equivalence that allows GUS sampling operators to\ncommute with selection and join, and derivation of confidence intervals. We\nillustrate the theory through extensive examples and give indications on how to\nuse it to provide meaningful estimations in database systems.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2013 10:07:22 GMT"}], "update_date": "2013-07-02", "authors_parsed": [["Nirkhiwale", "Supriya", ""], ["Dobra", "Alin", ""], ["Jermaine", "Chris", ""]]}, {"id": "1307.0320", "submitter": "Wanling Gao", "authors": "Wanling Gao, Yuqing Zhu, Zhen Jia, Chunjie Luo, Lei Wang, Zhiguo Li,\n  Jianfeng Zhan, Yong Qi, Yongqiang He, Shiming Gong, Xiaona Li, Shujie Zhang,\n  and Bizhu Qiu", "title": "BigDataBench: a Big Data Benchmark Suite from Web Search Engines", "comments": "7 pages, 5 figures, The Third Workshop on Architectures and Systems\n  for Big Data(ASBD 2013) in conjunction with The 40th International Symposium\n  on Computer Architecture, May 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents our joint research efforts on big data benchmarking with\nseveral industrial partners. Considering the complexity, diversity, workload\nchurns, and rapid evolution of big data systems, we take an incremental\napproach in big data benchmarking. For the first step, we pay attention to\nsearch engines, which are the most important domain in Internet services in\nterms of the number of page views and daily visitors. However, search engine\nservice providers treat data, applications, and web access logs as business\nconfidentiality, which prevents us from building benchmarks. To overcome those\ndifficulties, with several industry partners, we widely investigated the open\nsource solutions in search engines, and obtained the permission of using\nanonymous Web access logs. Moreover, with two years' great efforts, we created\na sematic search engine named ProfSearch (available from\nhttp://prof.ict.ac.cn). These efforts pave the path for our big data benchmark\nsuite from search engines---BigDataBench, which is released on the web page\n(http://prof.ict.ac.cn/BigDataBench). We report our detailed analysis of search\nengine workloads, and present our benchmarking methodology. An innovative data\ngeneration methodology and tool are proposed to generate scalable volumes of\nbig data from a small seed of real data, preserving semantics and locality of\ndata. Also, we preliminarily report two case studies using BigDataBench for\nboth system and architecture researches.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2013 10:27:48 GMT"}], "update_date": "2013-07-02", "authors_parsed": [["Gao", "Wanling", ""], ["Zhu", "Yuqing", ""], ["Jia", "Zhen", ""], ["Luo", "Chunjie", ""], ["Wang", "Lei", ""], ["Li", "Zhiguo", ""], ["Zhan", "Jianfeng", ""], ["Qi", "Yong", ""], ["He", "Yongqiang", ""], ["Gong", "Shiming", ""], ["Li", "Xiaona", ""], ["Zhang", "Shujie", ""], ["Qiu", "Bizhu", ""]]}, {"id": "1307.0441", "submitter": "Jakub Z\\'avodn\\'y", "authors": "Nurzhan Bakibayev, Tom\\'a\\v{s} Ko\\v{c}isk\\'y, Dan Olteanu, and Jakub\n  Z\\'avodn\\'y", "title": "Aggregation and Ordering in Factorised Databases", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common approach to data analysis involves understanding and manipulating\nsuccinct representations of data. In earlier work, we put forward a succinct\nrepresentation system for relational data called factorised databases and\nreported on the main-memory query engine FDB for select-project-join queries on\nsuch databases.\n  In this paper, we extend FDB to support a larger class of practical queries\nwith aggregates and ordering. This requires novel optimisation and evaluation\ntechniques. We show how factorisation coupled with partial aggregation can\neffectively reduce the number of operations needed for query evaluation. We\nalso show how factorisations of query results can support enumeration of tuples\nin desired orders as efficiently as listing them from the unfactorised, sorted\nresults.\n  We experimentally observe that FDB can outperform off-the-shelf relational\nengines by orders of magnitude.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2013 17:03:44 GMT"}], "update_date": "2013-07-02", "authors_parsed": [["Bakibayev", "Nurzhan", ""], ["Ko\u010disk\u00fd", "Tom\u00e1\u0161", ""], ["Olteanu", "Dan", ""], ["Z\u00e1vodn\u00fd", "Jakub", ""]]}, {"id": "1307.0589", "submitter": "Steven Ness", "authors": "Steven Ness, Helena Symonds, Paul Spong, George Tzanetakis", "title": "The Orchive : Data mining a massive bioacoustic archive", "comments": "ICML 2013 Workshop on Machine Learning for Bioacoustics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Orchive is a large collection of over 20,000 hours of audio recordings\nfrom the OrcaLab research facility located off the northern tip of Vancouver\nIsland. It contains recorded orca vocalizations from the 1980 to the present\ntime and is one of the largest resources of bioacoustic data in the world. We\nhave developed a web-based interface that allows researchers to listen to these\nrecordings, view waveform and spectral representations of the audio, label\nclips with annotations, and view the results of machine learning classifiers\nbased on automatic audio features extraction. In this paper we describe such\nclassifiers that discriminate between background noise, orca calls, and the\nvoice notes that are present in most of the tapes. Furthermore we show\nclassification results for individual calls based on a previously existing orca\ncall catalog. We have also experimentally investigated the scalability of\nclassifiers over the entire Orchive.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2013 04:59:19 GMT"}], "update_date": "2013-07-03", "authors_parsed": [["Ness", "Steven", ""], ["Symonds", "Helena", ""], ["Spong", "Paul", ""], ["Tzanetakis", "George", ""]]}, {"id": "1307.0803", "submitter": "Marinka Zitnik", "authors": "Marinka \\v{Z}itnik and Bla\\v{z} Zupan", "title": "Data Fusion by Matrix Factorization", "comments": "Short preprint, 13 pages, 3 Figures, 3 Tables. Full paper in\n  10.1109/TPAMI.2014.2343973", "journal-ref": "Marinka Zitnik and Blaz Zupan. IEEE Transactions on Pattern\n  Analysis and Machine Intelligence, 37(1):41-53 (2015)", "doi": "10.1109/TPAMI.2014.2343973", "report-no": null, "categories": "cs.LG cs.AI cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For most problems in science and engineering we can obtain data sets that\ndescribe the observed system from various perspectives and record the behavior\nof its individual components. Heterogeneous data sets can be collectively mined\nby data fusion. Fusion can focus on a specific target relation and exploit\ndirectly associated data together with contextual data and data about system's\nconstraints. In the paper we describe a data fusion approach with penalized\nmatrix tri-factorization (DFMF) that simultaneously factorizes data matrices to\nreveal hidden associations. The approach can directly consider any data that\ncan be expressed in a matrix, including those from feature-based\nrepresentations, ontologies, associations and networks. We demonstrate the\nutility of DFMF for gene function prediction task with eleven different data\nsources and for prediction of pharmacologic actions by fusing six data sources.\nOur data fusion algorithm compares favorably to alternative data integration\napproaches and achieves higher accuracy than can be obtained from any single\ndata source alone.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2013 19:35:21 GMT"}, {"version": "v2", "created": "Fri, 6 Feb 2015 16:15:38 GMT"}], "update_date": "2015-02-09", "authors_parsed": [["\u017ditnik", "Marinka", ""], ["Zupan", "Bla\u017e", ""]]}, {"id": "1307.0844", "submitter": "Andrei Todor", "authors": "Andrei Todor, Alin Dobra, Tamer Kahveci, Christopher Dudley", "title": "Making massive probabilistic databases practical", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existence of incomplete and imprecise data has moved the database paradigm\nfrom deterministic to proba- babilistic information. Probabilistic databases\ncontain tuples that may or may not exist with some probability. As a result,\nthe number of possible deterministic database instances that can be observed\nfrom a probabilistic database grows exponentially with the number of\nprobabilistic tuples. In this paper, we consider the problem of answering both\naggregate and non-aggregate queries on massive probabilistic databases. We\nadopt the tuple independence model, in which each tuple is assigned a\nprobability value. We develop a method that exploits Probability Generating\nFunctions (PGF) to answer such queries efficiently. Our method maintains a\npolynomial for each tuple. It incrementally builds a master polynomial that\nexpresses the distribution of the possible result values precisely. We also\ndevelop an approximation method that finds the distribution of the result value\nwith negligible errors. Our experiments suggest that our methods are orders of\nmagnitude faster than the most recent systems that answer such queries,\nincluding MayBMS and SPROUT. In our experiments, we were able to scale up to\nseveral terabytes of data on TPC- H queries, while existing methods could only\nrun for a few gigabytes of data on the same queries.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2013 20:50:15 GMT"}], "update_date": "2013-07-04", "authors_parsed": [["Todor", "Andrei", ""], ["Dobra", "Alin", ""], ["Kahveci", "Tamer", ""], ["Dudley", "Christopher", ""]]}, {"id": "1307.0966", "submitter": "Jordi Soria-Comas", "authors": "Jordi Soria-Comas", "title": "Improving data utility in differential privacy and k-anonymity", "comments": "Ph.D. Thesis defended on June 14, 2013, at the Department of Computer\n  Engineering and Mathematics of Universitat Rovira i Virgili. Advisor: Josep\n  Domingo-Ferrer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on two mainstream privacy models: k-anonymity and differential\nprivacy. Once a privacy model has been selected, the goal is to enforce it\nwhile preserving as much data utility as possible. The main objective of this\nthesis is to improve the data utility in k-anonymous and differentially private\ndata releases. k-Anonymity has several drawbacks. On the disclosure limitation\nside, there is a lack of protection against attribute disclosure and against\ninformed intruders. On the data utility side, dealing with a large number of\nquasi-identifier attributes is problematic. We propose a relaxation of\nk-anonymity that deals with these issues.\n  Differential privacy limits disclosure risk through noise addition. The\nLaplace distribution is commonly used for the random noise. We show that the\nLaplace distribution is not optimal: the same disclosure limitation guarantee\ncan be attained by adding less noise. Optimal univariate and multivariate\nnoises are characterized and constructed.\n  Common mechanisms to attain differential privacy do not take into account the\nusers prior knowledge; they implicitly assume zero initial knowledge about the\nquery response. We propose a mechanism that focuses on limiting the knowledge\ngain over the prior knowledge.\n  Microaggregation-based k-anonymity and differential privacy can be combined\nto produce microdata releases with the strong privacy guarantees of\ndifferential privacy and improved data accuracy.\n  The last contribution delves into the relation between t-closeness and\ndifferential privacy. We see that for a specific distance and under some\nreasonable assumptions on the intruders knowledge, t-closeness leads to\ndifferential privacy.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2013 10:55:30 GMT"}], "update_date": "2013-07-04", "authors_parsed": [["Soria-Comas", "Jordi", ""]]}, {"id": "1307.1370", "submitter": "Latanya Sweeney", "authors": "Latanya Sweeney", "title": "Matching Known Patients to Health Records in Washington State Data", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The State of Washington sells patient-level health data for $50. This\npublicly available dataset has virtually all hospitalizations occurring in the\nState in a given year, including patient demographics, diagnoses, procedures,\nattending physician, hospital, a summary of charges, and how the bill was paid.\nIt does not contain patient names or addresses (only ZIPs). Newspaper stories\nprinted in the State for the same year that contain the word \"hospitalized\"\noften include a patient's name and residential information and explain why the\nperson was hospitalized, such as vehicle accident or assault. News information\nuniquely and exactly matched medical records in the State database for 35 of\nthe 81 cases (or 43 percent) found in 2011, thereby putting names to patient\nrecords. A news reporter verified matches by contacting patients. Employers,\nfinancial organizations and others know the same kind of information as\nreported in news stories making it just as easy for them to identify the\nmedical records of employees, debtors, and others.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2013 15:21:34 GMT"}, {"version": "v2", "created": "Fri, 5 Jul 2013 23:04:48 GMT"}], "update_date": "2013-07-09", "authors_parsed": [["Sweeney", "Latanya", ""]]}, {"id": "1307.1584", "submitter": "Uwe Aickelin", "authors": "Jenna Reps, Jonathan M. Garibaldi, Uwe Aickelin, Daniele Soria, Jack\n  E. Gibson, Richard B. Hubbard", "title": "Comparing Data-mining Algorithms Developed for Longitudinal\n  Observational Databases", "comments": "UKCI 2012, the 12th Annual Workshop on Computational Intelligence,\n  Heriot-Watt University, pp 1-8, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Longitudinal observational databases have become a recent interest in the\npost marketing drug surveillance community due to their ability of presenting a\nnew perspective for detecting negative side effects. Algorithms mining\nlongitudinal observation databases are not restricted by many of the\nlimitations associated with the more conventional methods that have been\ndeveloped for spontaneous reporting system databases. In this paper we\ninvestigate the robustness of four recently developed algorithms that mine\nlongitudinal observational databases by applying them to The Health Improvement\nNetwork (THIN) for six drugs with well document known negative side effects.\nOur results show that none of the existing algorithms was able to consistently\nidentify known adverse drug reactions above events related to the cause of the\ndrug and no algorithm was superior.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2013 11:24:55 GMT"}], "update_date": "2013-07-08", "authors_parsed": [["Reps", "Jenna", ""], ["Garibaldi", "Jonathan M.", ""], ["Aickelin", "Uwe", ""], ["Soria", "Daniele", ""], ["Gibson", "Jack E.", ""], ["Hubbard", "Richard B.", ""]]}, {"id": "1307.1927", "submitter": "Murat A Bayir", "authors": "Murat Ali Bayir, Ismail Hakki Toroslu", "title": "Link Based Session Reconstruction: Finding All Maximal Paths", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new method for the session construction problem,\nwhich is the first main step of the web usage mining process. Through\nexperiments, it is shown that when our new technique is used, it outperforms\nprevious approaches in web usage mining applications such as next-page\nprediction.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2013 22:20:27 GMT"}], "update_date": "2013-07-09", "authors_parsed": [["Bayir", "Murat Ali", ""], ["Toroslu", "Ismail Hakki", ""]]}, {"id": "1307.2015", "submitter": "Christos Tryfonopoulos Dr.", "authors": "Lefteris Zervakis, Christos Tryfonopoulos, Antonios\n  Papadakis-Pesaresi, Manolis Koubarakis, Spiros Skiadopoulos", "title": "Full-text Support for Publish/Subscribe Ontology Systems", "comments": "ESWC 2012 Demo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We envision a publish/subscribe ontology system that is able to index\nmillions of user subscriptions and filter them against ontology data that\narrive in a streaming fashion. In this work, we propose a SPARQL extension\nappropriate for a publish/subscribe setting; our extension builds on the\nnatural semantic graph matching of the language and supports the creation of\nfull-text subscriptions. Subsequently, we propose a main-memory subscription\nindexing algorithm which performs both semantic and full-text matching at low\ncomplexity and minimal filtering time. Thus, when ontology data are published\nmatching subscriptions are identified and notifications are forwarded to users.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2013 10:12:01 GMT"}], "update_date": "2013-07-09", "authors_parsed": [["Zervakis", "Lefteris", ""], ["Tryfonopoulos", "Christos", ""], ["Papadakis-Pesaresi", "Antonios", ""], ["Koubarakis", "Manolis", ""], ["Skiadopoulos", "Spiros", ""]]}, {"id": "1307.2554", "submitter": "El Amin Aoulad Abdelouarit", "authors": "El Amin Aoulad Abdelouarit (LSIT)", "title": "Les index pour les entrep\\^ots de donn\\'ees : comparaison entre index\n  arbre-B et Bitmap", "comments": "14 pages, Proposition et d\\'emonstration exp\\'eriment\\'ee", "journal-ref": "REVIST - Revue Ivoirienne des Sciences et Technologie - ISSN\n  1813-3290 - No 20 - December 2012 - Pages:35-67", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of decision systems and specially data warehouses, the\nvisibility of the data warehouse design before its creation has become\nessential, and that because of data warehouse importance as considered as the\nunique data source giving meaning to the decision. In a decision system the\nproper functioning of a data warehouse resides in the smooth running of the\nmiddleware tools ETC step one hand, and the restitution step through the data\nmining, reporting solutions, dashboards... etc other. The large volume of data\nthat passes through these stages require an optimal design for a highly\nefficient decision system, without disregarding the choice of technologies that\nare introduced for the data warehouse implementation such as: database\nmanagement system, the type of server operating systems, physical server\narchitecture (64-bit, for example) that can be a benefit performance of this\nsystem. The designer of the data warehouse should consider the effectiveness of\ndata query, this depends on the selection of relevant indexes and their\ncombination with the materialized views, note that the index selection is a\nNPcomplete problem, because the number of indexes is exponential in the total\nnumber of attributes in the database, So, it is necessary to provide, while the\ndata warehouse design, the suitable type of index for this data warehouse. This\npaper presents a comparative study between the index B-tree type and type\nBitmap, their advantages and disadvantages, with a real experiment showing that\nits index of type Bitmap more advantageous than the index B-tree type.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2013 19:31:50 GMT"}], "update_date": "2013-10-03", "authors_parsed": [["Abdelouarit", "El Amin Aoulad", "", "LSIT"]]}, {"id": "1307.2603", "submitter": "Olivier Cur\\'e", "authors": "Olivier Cur\\'e and Myriam Lamolle and Chan Le Duc", "title": "Ontology Based Data Integration Over Document and Column Family Oriented\n  NOSQL", "comments": "16 pages, 6 figures, SSWS 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The World Wide Web infrastructure together with its more than 2 billion users\nenables to store information at a rate that has never been achieved before.\nThis is mainly due to the will of storing almost all end-user interactions\nperformed on some web applications. In order to reply to scalability and\navailability constraints, many web companies involved in this process recently\nstarted to design their own data management systems. Many of them are referred\nto as NOSQL databases, standing for 'Not only SQL'. With their wide adoption\nemerges new needs and data integration is one of them. In this paper, we\nconsider that an ontology-based representation of the information stored in a\nset of NOSQL sources is highly needed. The main motivation of this approach is\nthe ability to reason on elements of the ontology and to retrieve information\nin an efficient and distributed manner. Our contributions are the following:\n(1) we analyze a set of schemaless NOSQL databases to generate local\nontologies, (2) we generate a global ontology based on the discovery of\ncorrespondences between the local ontologies and finally (3) we propose a query\ntranslation solution from SPARQL to query languages of the sources. We are\ncurrently implementing our data integration solution on two popular NOSQL\ndatabases: MongoDB as a document database and Cassandra as a column family\nstore.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2013 21:08:27 GMT"}], "update_date": "2013-07-11", "authors_parsed": [["Cur\u00e9", "Olivier", ""], ["Lamolle", "Myriam", ""], ["Duc", "Chan Le", ""]]}, {"id": "1307.2991", "submitter": "Qiwei Lu", "authors": "Qiwei Lu, Wenchao Huang, Yan Xiong and Xudong Gong", "title": "Integrity Verification for Outsourcing Uncertain Frequent Itemset Mining", "comments": "under submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, due to the wide applications of uncertain data (e.g., noisy\ndata), uncertain frequent itemsets (UFI) mining over uncertain databases has\nattracted much attention, which differs from the corresponding deterministic\nproblem from the generalized definition and resolutions. As the most costly\ntask in association rule mining process, it has been shown that outsourcing\nthis task to a service provider (e.g.,the third cloud party) brings several\nbenefits to the data owner such as cost relief and a less commitment to storage\nand computational resources. However, the correctness integrity of mining\nresults can be corrupted if the service provider is with random fault or not\nhonest (e.g., lazy, malicious, etc). Therefore, in this paper, we focus on the\nintegrity and verification issue in UFI mining problem during outsourcing\nprocess, i.e., how the data owner verifies the mining results. Specifically, we\nexplore and extend the existing work on deterministic FI outsourcing\nverification to uncertain scenario. For this purpose, We extend the existing\noutsourcing FI mining work to uncertain area w.r.t. the two popular UFI\ndefinition criteria and the approximate UFI mining methods. Specifically, We\nconstruct and improve the basic/enhanced verification scheme with such\ndifferent UFI definition respectively. After that, we further discuss the\nscenario of existing approximation UFP mining, where we can see that our\ntechnique can provide good probabilistic guarantees about the correctness of\nthe verification. Finally, we present the comparisons and analysis on the\nschemes proposed in this paper.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2013 07:03:06 GMT"}, {"version": "v2", "created": "Fri, 12 Jul 2013 01:47:54 GMT"}], "update_date": "2013-07-15", "authors_parsed": [["Lu", "Qiwei", ""], ["Huang", "Wenchao", ""], ["Xiong", "Yan", ""], ["Gong", "Xudong", ""]]}, {"id": "1307.3046", "submitter": "LeiLa Esheiba", "authors": "Leila Esheiba, Hoda M.O.Mokhtar, Mohamed El-Sharkawi", "title": "Spatio-Temporal Queries for moving objects Data warehousing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade, Moving Object Databases (MODs) have attracted a lot of\nattention from researchers. Several research works were conducted to extend\ntraditional database techniques to accommodate the new requirements imposed by\nthe continuous change in location information of moving objects. Managing,\nquerying, storing, and mining moving objects were the key research directions.\nThis extensive interest in moving objects is a natural consequence of the\nrecent ubiquitous location-aware devices, such as PDAs, mobile phones, etc., as\nwell as the variety of information that can be extracted from such new\ndatabases. In this paper we propose a Spatio-Temporal data warehousing (STDW)\nfor efficiently querying location information of moving objects. The proposed\nschema introduces new measures like direction majority and other\ndirection-based measures that enhance the decision making based on location\ninformation.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2013 10:35:01 GMT"}], "update_date": "2013-07-12", "authors_parsed": [["Esheiba", "Leila", ""], ["Mokhtar", "Hoda M. O.", ""], ["El-Sharkawi", "Mohamed", ""]]}, {"id": "1307.3061", "submitter": "Ahmed Nour Eldeen", "authors": "Dr.Osama E.Sheta and Ahmed Nour Eldeen", "title": "The technology of using a data warehouse to support decision-making in\n  health care", "comments": "12 pages", "journal-ref": null, "doi": "10.5121/ijdms.2013.5305", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the technology of data warehouse in healthcare\ndecision-making and tools for support of these technologies, which is used to\ncancer diseases. The healthcare executive managers and doctors needs\ninformation about and insight into the existing health data, so as to make\ndecision more efficiently without interrupting the daily work of an On-Line\nTransaction Processing (OLTP) system. This is a complex problem during the\nhealthcare decision-making process. To solve this problem, the building a\nhealthcare data warehouse seems to be efficient. First in this paper we explain\nthe concepts of the data warehouse, On-Line Analysis Processing (OLAP).\nChanging the data in the data warehouse into a multidimensional data cube is\nthen shown. Finally, an application example is given to illustrate the use of\nthe healthcare data warehouse specific to cancer diseases developed in this\nstudy. The executive managers and doctors can view data from more than one\nperspective with reduced query time, thus making decisions faster and more\ncomprehensive.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2013 11:30:32 GMT"}], "update_date": "2013-07-16", "authors_parsed": [["Sheta", "Dr. Osama E.", ""], ["Eldeen", "Ahmed Nour", ""]]}, {"id": "1307.3419", "submitter": "Michael Schmidt", "authors": "Michael Schmidt and Georg Lausen", "title": "Pleasantly Consuming Linked Data with RDF Data Descriptions", "comments": "12 pages + Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the intention of RDF is to provide an open, minimally constraining\nway for representing information, there exists an increasing number of\napplications for which guarantees on the structure and values of an RDF data\nset become desirable if not essential. What is missing in this respect are\nmechanisms to tie RDF data to quality guarantees akin to schemata of relational\ndatabases, or DTDs in XML, in particular when translating legacy data coming\nwith a rich set of integrity constraints - like keys or cardinality\nrestrictions - into RDF. Addressing this shortcoming, we present the RDF Data\nDescription language (RDD), which makes it possible to specify instance-level\ndata constraints over RDF. Making such constraints explicit does not only help\nin asserting and maintaining data quality, but also opens up new optimization\nopportunities for query engines and, most importantly, makes query formulation\na lot easier for users and system developers. We present design goals, syntax,\nand a formal, First-order logics based semantics of RDDs and discuss the impact\non consuming Linked Data.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2013 11:30:58 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2013 19:18:36 GMT"}], "update_date": "2013-12-09", "authors_parsed": [["Schmidt", "Michael", ""], ["Lausen", "Georg", ""]]}, {"id": "1307.3448", "submitter": "Ahmed Nour Eldeen", "authors": "Dr.Osama E.Sheta, Ahmed Nour Eldeen", "title": "Evaluating a healthcare data warehouse for cancer diseases", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the evaluation of the architecture of healthcare data\nwarehouse specific to cancer diseases. This data warehouse containing relevant\ncancer medical information and patient data. The data warehouse provides the\nsource for all current and historical health data to help executive manager and\ndoctors to improve the decision making process for cancer patients. The\nevaluation model based on Bill Inmon's definition of data warehouse is proposed\nto evaluate the Cancer data warehouse.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2013 13:16:21 GMT"}], "update_date": "2013-07-15", "authors_parsed": [["Sheta", "Dr. Osama E.", ""], ["Eldeen", "Ahmed Nour", ""]]}, {"id": "1307.4519", "submitter": "Dhammika Pieris", "authors": "Dhammika Pieris", "title": "Extending the ER Model to relational Model novel transformation\n  Algorithm: transforming relationship Types among Subtypes", "comments": "15 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel approach for creating ER conceptual models and an algorithm for\ntransforming them to the relational model has been developed by modifying and\nextending the existing methods. A part of the new algorithm has previously been\npresented. This paper presents the rest of the algorithm. One of the objectives\nof this paper is to use it as a supportive document for ongoing empirical\nevaluations of the new approach being conducted using the cognitive engagement\nmethod and with the participation of different segments of the field as\nrespondents.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2013 07:09:30 GMT"}], "update_date": "2013-07-18", "authors_parsed": [["Pieris", "Dhammika", ""]]}, {"id": "1307.4635", "submitter": "Ricardo Rocha", "authors": "Benoit Desouter and Tom Schrijvers", "title": "Integrating Datalog and Constraint Solving", "comments": "Proceedings of the 13th International Colloquium on Implementation of\n  Constraint LOgic Programming Systems (CICLOPS 2013), Istanbul, Turkey, August\n  25, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LP is a common formalism for the field of databases and CSP, both at the\ntheoretical level and the implementation level in the form of Datalog and CLP.\nIn the past, close correspondences have been made between both fields at the\ntheoretical level. Yet correspondence at the implementation level has been much\nless explored. In this article we work towards relating them at the\nimplementation level. Concretely, we show how to derive the efficient Leapfrog\nTriejoin execution algorithm of Datalog from a generic CP execution scheme.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2013 13:58:38 GMT"}], "update_date": "2013-07-19", "authors_parsed": [["Desouter", "Benoit", ""], ["Schrijvers", "Tom", ""]]}, {"id": "1307.5437", "submitter": "Chanchal Yadav Ms.", "authors": "Chanchal Yadav, Shuliang Wang, Manoj Kumar", "title": "Algorithm and approaches to handle large Data- A Survey", "comments": "5 pages", "journal-ref": "International Journal of computer science and network, vol 2,\n  issue 3, 2013", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data mining environment produces a large amount of data, that need to be\nanalyzed, patterns have to be extracted from that to gain knowledge. In this\nnew era with boom of data both structured and unstructured, in the field of\ngenomics, meteorology, biology, environmental research and many others, it has\nbecome difficult to process, manage and analyze patterns using traditional\ndatabases and architectures. So, a proper architecture should be understood to\ngain knowledge about the Big Data. This paper presents a review of various\nalgorithms from 1994-2013 necessary for handling such large data set. These\nalgorithms define various structures and methods implemented to handle Big\nData, also in the paper are listed various tool that were developed for\nanalyzing them.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2013 16:19:35 GMT"}], "update_date": "2013-07-23", "authors_parsed": [["Yadav", "Chanchal", ""], ["Wang", "Shuliang", ""], ["Kumar", "Manoj", ""]]}, {"id": "1307.5894", "submitter": "Md Mansurul Bhuiyan", "authors": "Mansurul A Bhuiyan and Mohammad Al Hasan", "title": "MIRAGE: An Iterative MapReduce based FrequentSubgraph Mining Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frequent subgraph mining (FSM) is an important task for exploratory data\nanalysis on graph data. Over the years, many algorithms have been proposed to\nsolve this task. These algorithms assume that the data structure of the mining\ntask is small enough to fit in the main memory of a computer. However, as the\nreal-world graph data grows, both in size and quantity, such an assumption does\nnot hold any longer. To overcome this, some graph database-centric methods have\nbeen proposed in recent years for solving FSM; however, a distributed solution\nusing MapReduce paradigm has not been explored extensively. Since, MapReduce is\nbecoming the de- facto paradigm for computation on massive data, an efficient\nFSM algorithm on this paradigm is of huge demand. In this work, we propose a\nfrequent subgraph mining algorithm called MIRAGE which uses an iterative\nMapReduce based framework. MIRAGE is complete as it returns all the frequent\nsubgraphs for a given user-defined support, and it is efficient as it applies\nall the optimizations that the latest FSM algorithms adopt. Our experiments\nwith real life and large synthetic datasets validate the effectiveness of\nMIRAGE for mining frequent subgraphs from large graph datasets. The source code\nof MIRAGE is available from www.cs.iupui.edu/alhasan/software/\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2013 21:26:00 GMT"}], "update_date": "2013-07-24", "authors_parsed": [["Bhuiyan", "Mansurul A", ""], ["Hasan", "Mohammad Al", ""]]}, {"id": "1307.6348", "submitter": "Radu Ciucanu", "authors": "Radu Ciucanu, Slawek Staworko", "title": "Learning Schemas for Unordered XML", "comments": "Proceedings of the 14th International Symposium on Database\n  Programming Languages (DBPL 2013), August 30, 2013, Riva del Garda, Trento,\n  Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider unordered XML, where the relative order among siblings is\nignored, and we investigate the problem of learning schemas from examples given\nby the user. We focus on the schema formalisms proposed in [10]: disjunctive\nmultiplicity schemas (DMS) and its restriction, disjunction-free multiplicity\nschemas (MS). A learning algorithm takes as input a set of XML documents which\nmust satisfy the schema (i.e., positive examples) and a set of XML documents\nwhich must not satisfy the schema (i.e., negative examples), and returns a\nschema consistent with the examples. We investigate a learning framework\ninspired by Gold [18], where a learning algorithm should be sound i.e., always\nreturn a schema consistent with the examples given by the user, and complete\ni.e., able to produce every schema with a sufficiently rich set of examples.\nAdditionally, the algorithm should be efficient i.e., polynomial in the size of\nthe input. We prove that the DMS are learnable from positive examples only, but\nthey are not learnable when we also allow negative examples. Moreover, we show\nthat the MS are learnable in the presence of positive examples only, and also\nin the presence of both positive and negative examples. Furthermore, for the\nlearnable cases, the proposed learning algorithms return minimal schemas\nconsistent with the examples.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2013 09:33:41 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2013 09:01:16 GMT"}], "update_date": "2013-07-26", "authors_parsed": [["Ciucanu", "Radu", ""], ["Staworko", "Slawek", ""]]}, {"id": "1307.6365", "submitter": "Josif Grabocka", "authors": "Josif Grabocka, Martin Wistuba, Lars Schmidt-Thieme", "title": "Time-Series Classification Through Histograms of Symbolic Polynomials", "comments": null, "journal-ref": null, "doi": "10.1109/TKDE.2014.2377746", "report-no": null, "categories": "cs.AI cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-series classification has attracted considerable research attention due\nto the various domains where time-series data are observed, ranging from\nmedicine to econometrics. Traditionally, the focus of time-series\nclassification has been on short time-series data composed of a unique pattern\nwith intraclass pattern distortions and variations, while recently there have\nbeen attempts to focus on longer series composed of various local patterns.\nThis study presents a novel method which can detect local patterns in long\ntime-series via fitting local polynomial functions of arbitrary degrees. The\ncoefficients of the polynomial functions are converted to symbolic words via\nequivolume discretizations of the coefficients' distributions. The symbolic\npolynomial words enable the detection of similar local patterns by assigning\nthe same words to similar polynomials. Moreover, a histogram of the frequencies\nof the words is constructed from each time-series' bag of words. Each row of\nthe histogram enables a new representation for the series and symbolize the\nexistence of local patterns and their frequencies. Experimental evidence\ndemonstrates outstanding results of our method compared to the state-of-art\nbaselines, by exhibiting the best classification accuracies in all the datasets\nand having statistically significant improvements in the absolute majority of\nexperiments.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2013 10:07:50 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2013 03:40:27 GMT"}, {"version": "v3", "created": "Wed, 31 Jul 2013 10:58:02 GMT"}, {"version": "v4", "created": "Mon, 23 Dec 2013 22:26:35 GMT"}], "update_date": "2015-03-12", "authors_parsed": [["Grabocka", "Josif", ""], ["Wistuba", "Martin", ""], ["Schmidt-Thieme", "Lars", ""]]}, {"id": "1307.6574", "submitter": "Abhirup  Chakraborty", "authors": "Abhirup Chakraborty, Ajit Singh", "title": "Parallelizing Windowed Stream Joins in a Shared-Nothing Cluster", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of large number of processing nodes in a parallel and\ndistributed computing environment enables sophisticated real time processing\nover high speed data streams, as required by many emerging applications.\nSliding window stream joins are among the most important operators in a stream\nprocessing system. In this paper, we consider the issue of parallelizing a\nsliding window stream join operator over a shared nothing cluster. We propose a\nframework, based on fixed or predefined communication pattern, to distribute\nthe join processing loads over the shared-nothing cluster. We consider various\noverheads while scaling over a large number of nodes, and propose solution\nmethodologies to cope with the issues. We implement the algorithm over a\ncluster using a message passing system, and present the experimental results\nshowing the effectiveness of the join processing algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2013 20:30:07 GMT"}], "update_date": "2013-07-26", "authors_parsed": [["Chakraborty", "Abhirup", ""], ["Singh", "Ajit", ""]]}, {"id": "1307.7328", "submitter": "Eiad Alhyasat Dr", "authors": "Eiad Basher Alhyasat and Mahmoud Al-Dalahmeh", "title": "Data Warehouse Success and Strategic Oriented Business Intelligence: A\n  Theoretical Framework", "comments": null, "journal-ref": "Journal of Management Research 5(3), 2013", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the proliferation of the data warehouses as supportive decision making\ntools, organizations are increasingly looking forward for a complete data\nwarehouse success model that would manage the enormous amounts of growing data.\nIt is therefore important to measure the success of these massive projects.\nWhile general IS success models have received great deals of attention, few\nresearch has been conducted to assess the success of data warehouses for\nstrategic business intelligence purposes. The framework developed in this study\nconsists of the following nine measures: Vendors and Consultants, Management\nActions, System Quality, Information Quality, Data Warehouse Usage, Perceived\nutility, Individual Decision Making Impact, Organizational Decision Making\nImpact, and Corporate Strategic Goals Attainment.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2013 03:50:16 GMT"}], "update_date": "2013-07-30", "authors_parsed": [["Alhyasat", "Eiad Basher", ""], ["Al-Dalahmeh", "Mahmoud", ""]]}, {"id": "1307.7411", "submitter": "Wajdi Dhifli Wajdi DHIFLI", "authors": "Wajdi Dhifli, Mohamed Moussaoui, Rabie Saidi, Engelbert Mephu Nguifo", "title": "Towards an Efficient Discovery of the Topological Representative\n  Subgraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the emergence of graph databases, the task of frequent subgraph\ndiscovery has been extensively addressed. Although the proposed approaches in\nthe literature have made this task feasible, the number of discovered frequent\nsubgraphs is still very high to be efficiently used in any further exploration.\nFeature selection for graph data is a way to reduce the high number of frequent\nsubgraphs based on exact or approximate structural similarity. However, current\nstructural similarity strategies are not efficient enough in many real-world\napplications, besides, the combinatorial nature of graphs makes it\ncomputationally very costly. In order to select a smaller yet structurally\nirredundant set of subgraphs, we propose a novel approach that mines the top-k\ntopological representative subgraphs among the frequent ones. Our approach\nallows detecting hidden structural similarities that existing approaches are\nunable to detect such as the density or the diameter of the subgraph. In\naddition, it can be easily extended using any user defined structural or\ntopological attributes depending on the sought properties. Empirical studies on\nreal and synthetic graph datasets show that our approach is fast and scalable.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2013 22:17:40 GMT"}, {"version": "v2", "created": "Tue, 13 Aug 2013 21:52:44 GMT"}, {"version": "v3", "created": "Thu, 15 Aug 2013 00:28:30 GMT"}], "update_date": "2013-08-16", "authors_parsed": [["Dhifli", "Wajdi", ""], ["Moussaoui", "Mohamed", ""], ["Saidi", "Rabie", ""], ["Nguifo", "Engelbert Mephu", ""]]}, {"id": "1307.7513", "submitter": "Vasanth Sena pesari", "authors": "P Vasanth Sena", "title": "An Approach Finding Frequent Items In Text Or Transactional Data Base By\n  Using BST To Improve The Efficiency Of Apriori Algorithm", "comments": "1 Algorithm. arXiv admin note: text overlap with arXiv:1009.4982 by\n  other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data mining techniques have been widely used in various applications. Binary\nsearch tree based frequent items is an effective method for automatically\nrecognize the most frequent items, least frequent items and average frequent\nitems. This paper presents a new approach in order to find out frequent items.\nThe word frequent item refers to how many times the item appeared in the given\ninput. This approach is used to find out item sets in any order using familiar\napproach binary search tree. The method adapted here is in order to find out\nfrequent items by comparing and incrementing the counter variable in existing\ntransactional data base or text data. We are also representing different\napproaches in frequent item sets and also propose an algorithmic approach for\nthe problem solving\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2013 09:39:16 GMT"}], "update_date": "2013-07-30", "authors_parsed": [["Sena", "P Vasanth", ""]]}, {"id": "1307.8269", "submitter": "\\'Emilien Antoine", "authors": "Serge Abiteboul, \\'Emilien Antoine, Gerome Miklau, Julia Stoyanovich,\n  Vera Zaychik Moffitt", "title": "Introducing Access Control in Webdamlog", "comments": "Proceedings of the 14th International Symposium on Database\n  Programming Languages (DBPL 2013), August 30, 2013, Riva del Garda, Trento,\n  Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We survey recent work on the specification of an access control mechanism in\na collaborative environment. The work is presented in the context of the\nWebdamLog language, an extension of datalog to a distributed context. We\ndiscuss a fine-grained access control mechanism for intentional data based on\nprovenance as well as a control mechanism for delegation, i.e., for deploying\nrules at remote peers.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2013 10:21:33 GMT"}], "update_date": "2013-08-01", "authors_parsed": [["Abiteboul", "Serge", ""], ["Antoine", "\u00c9milien", ""], ["Miklau", "Gerome", ""], ["Stoyanovich", "Julia", ""], ["Moffitt", "Vera Zaychik", ""]]}]