[{"id": "1403.0017", "submitter": "Zoran Majkic", "authors": "Zoran Majkic", "title": "Intensional RDB Manifesto: a Unifying NewSQL Model for Flexible Big Data", "comments": "29 pages. arXiv admin note: text overlap with arXiv:1103.0967,\n  arXiv:1103.0680", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new family of Intensional RDBs (IRDBs) which\nextends the traditional RDBs with the Big Data and flexible and 'Open schema'\nfeatures, able to preserve the user-defined relational database schemas and all\npreexisting user's applications containing the SQL statements for a deployment\nof such a relational data. The standard RDB data is parsed into an internal\nvector key/value relation, so that we obtain a column representation of data\nused in Big Data applications, covering the key/value and column-based Big Data\napplications as well, into a unifying RDB framework. We define a query\nrewriting algorithm, based on the GAV Data Integration methods, so that each\nuser-defined SQL query is rewritten into a SQL query over this vector relation,\nand hence the user-defined standard RDB schema is maintained as an empty global\nschema for the RDB schema modeling of data and as the SQL interface to stored\nvector relation. Such an IRDB architecture is adequate for the massive\nmigrations from the existing slow RDBMSs into this new family of fast IRDBMSs\nby offering a Big Data and new flexible schema features as well.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2014 21:48:30 GMT"}, {"version": "v2", "created": "Thu, 10 Apr 2014 18:52:29 GMT"}], "update_date": "2014-04-11", "authors_parsed": [["Majkic", "Zoran", ""]]}, {"id": "1403.0230", "submitter": "Richard McClatchey", "authors": "Ashiq Anjum, Peter Bloodsworth, Andrew Branson, Irfan Habib, Richard\n  McClatchey, Tony Solomonides and the neuGRID Consortium", "title": "Research Traceability using Provenance Services for Biomedical Analysis", "comments": "9 pages, 5 figures; Proceedings of the 8th HealthGrid Int. Conference\n  (HG'10). Paris, France. June 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We outline the approach being developed in the neuGRID project to use\nprovenance management techniques for the purposes of capturing and preserving\nthe provenance data that emerges in the specification and execution of\nworkflows in biomedical analyses. In the neuGRID project a provenance service\nhas been designed and implemented that is intended to capture, store, retrieve\nand reconstruct the workflow information needed to facilitate users in\nconducting user analyses. We describe the architecture of the neuGRID\nprovenance service and discuss how the CRISTAL system from CERN is being\nadapted to address the requirements of the project and then consider how a\ngeneralised approach for provenance management could emerge for more generic\napplication to the (Health)Grid community.\n", "versions": [{"version": "v1", "created": "Sun, 2 Mar 2014 16:11:58 GMT"}], "update_date": "2014-03-04", "authors_parsed": [["Anjum", "Ashiq", ""], ["Bloodsworth", "Peter", ""], ["Branson", "Andrew", ""], ["Habib", "Irfan", ""], ["McClatchey", "Richard", ""], ["Solomonides", "Tony", ""], ["Consortium", "the neuGRID", ""]]}, {"id": "1403.0701", "submitter": "Aapo Kyrola", "authors": "Aapo Kyrola and Carlos Guestrin", "title": "GraphChi-DB: Simple Design for a Scalable Graph Database System -- on\n  Just a PC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new data structure, Parallel Adjacency Lists (PAL), for\nefficiently managing graphs with billions of edges on disk. The PAL structure\nis based on the graph storage model of GraphChi (Kyrola et. al., OSDI 2012),\nbut we extend it to enable online database features such as queries and fast\ninsertions. In addition, we extend the model with edge and vertex attributes.\nCompared to previous data structures, PAL can store graphs more compactly while\nallowing fast access to both the incoming and the outgoing edges of a vertex,\nwithout duplicating data. Based on PAL, we design a graph database management\nsystem, GraphChi-DB, which can also execute powerful analytical graph\ncomputation.\n  We evaluate our design experimentally and demonstrate that GraphChi-DB\nachieves state-of-the-art performance on graphs that are much larger than the\navailable memory. GraphChi-DB enables anyone with just a laptop or a PC to work\nwith extremely large graphs.\n", "versions": [{"version": "v1", "created": "Tue, 4 Mar 2014 07:05:06 GMT"}], "update_date": "2014-03-05", "authors_parsed": [["Kyrola", "Aapo", ""], ["Guestrin", "Carlos", ""]]}, {"id": "1403.0779", "submitter": "Minhao Jiang", "authors": "Minhao Jiang, Ada Wai-Chee Fu, Raymond Chi-Wing Wong, Yanyan Xu", "title": "Hop Doubling Label Indexing for Point-to-Point Distance Querying on\n  Scale-Free Networks", "comments": "13 pages. More experiments and discussions are added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of point-to-point distance querying for massive\nscale-free graphs, which is important for numerous applications. Given a\ndirected or undirected graph, we propose to build an index for answering such\nqueries based on a hop-doubling labeling technique. We derive bounds on the\nindex size, the computation costs and I/O costs based on the properties of\nunweighted scale-free graphs. We show that our method is much more efficient\ncompared to the state-of-the-art technique, in terms of both querying time and\nindexing time. Our empirical study shows that our method can handle graphs that\nare orders of magnitude larger than existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 4 Mar 2014 13:09:09 GMT"}, {"version": "v2", "created": "Fri, 2 May 2014 14:09:59 GMT"}], "update_date": "2014-05-05", "authors_parsed": [["Jiang", "Minhao", ""], ["Fu", "Ada Wai-Chee", ""], ["Wong", "Raymond Chi-Wing", ""], ["Xu", "Yanyan", ""]]}, {"id": "1403.0783", "submitter": "Antoine Amarilli", "authors": "Antoine Amarilli, Yael Amsterdamer, Tova Milo", "title": "Uncertainty in Crowd Data Sourcing under Structural Constraints", "comments": "8 pages, vision paper. To appear at UnCrowd 2014", "journal-ref": null, "doi": "10.1007/978-3-662-43984-5_27", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications extracting data from crowdsourcing platforms must deal with the\nuncertainty of crowd answers in two different ways: first, by deriving\nestimates of the correct value from the answers; second, by choosing crowd\nquestions whose answers are expected to minimize this uncertainty relative to\nthe overall data collection goal. Such problems are already challenging when we\nassume that questions are unrelated and answers are independent, but they are\neven more complicated when we assume that the unknown values follow hard\nstructural constraints (such as monotonicity).\n  In this vision paper, we examine how to formally address this issue with an\napproach inspired by [Amsterdamer et al., 2013]. We describe a generalized\nsetting where we model constraints as linear inequalities, and use them to\nguide the choice of crowd questions and the processing of answers. We present\nthe main challenges arising in this setting, and propose directions to solve\nthem.\n", "versions": [{"version": "v1", "created": "Tue, 4 Mar 2014 13:21:39 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Amarilli", "Antoine", ""], ["Amsterdamer", "Yael", ""], ["Milo", "Tova", ""]]}, {"id": "1403.0802", "submitter": "Jianting Zhang", "authors": "Jianting Zhang Simin You", "title": "Large-Scale Geospatial Processing on Multi-Core and Many-Core\n  Processors: Evaluations on CPUs, GPUs and MICs", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geospatial Processing, such as queries based on point-to-polyline shortest\ndistance and point-in-polygon test, are fundamental to many scientific and\nengineering applications, including post-processing large-scale environmental\nand climate model outputs and analyzing traffic and travel patterns from\nmassive GPS collections in transportation engineering and urban studies.\nCommodity parallel hardware, such as multi-core CPUs, many-core GPUs and Intel\nMIC accelerators, provide enormous computing power which can potentially\nachieve significant speedups on existing geospatial processing and open the\nopportunities for new applications. However, the realizable potential for\ngeospatial processing on these new hardware devices is largely unknown due to\nthe complexity in porting serial algorithms to diverse parallel hardware\nplatforms. In this study, we aim at experimenting our data-parallel designs and\nimplementations of point-to-polyline shortest distance computation (P2P) and\npoint-in-polygon topological test (PIP) on different commodity hardware using\nreal large-scale geospatial data, comparing their performance and discussing\nimportant factors that may significantly affect the performance. Our\nexperiments have shown that, while GPUs can be several times faster than\nmulti-core CPUs without utilizing the increasingly available SIMD computing\npower on Vector Processing Units (VPUs) that come with multi-core CPUs and\nMICs, multi-core CPUs and MICs can be several times faster than GPUs when VPUs\nare utilized. By adopting a Domain Specific Language (DSL) approach to\nexploiting the VPU computing power in geospatial processing, we are free from\nprogramming SIMD intrinsic functions directly which makes the new approach more\neffective, portable and scalable. Our designs, implementations and experiments\ncan serve as case studies for parallel geospatial computing on modern commodity\nparallel hardware.\n", "versions": [{"version": "v1", "created": "Tue, 4 Mar 2014 14:46:29 GMT"}], "update_date": "2014-03-05", "authors_parsed": [["You", "Jianting Zhang Simin", ""]]}, {"id": "1403.1180", "submitter": "Nikos Chondros", "authors": "Nikos Chondros, Mema Roussopoulos", "title": "A distributed Integrity Catalog for digital repositories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital repositories, either digital preservation systems or archival\nsystems, periodically check the integrity of stored objects to assure users of\ntheir correctness. To do so, prior solutions calculate integrity metadata and\nrequire the repository to store it alongside the actual data objects. This\nintegrity metadata is essential for regularly verifying the correctness of the\nstored data objects. To safeguard and detect damage to this metadata, prior\nsolutions rely on widely visible media, that is unaffiliated third parties, to\nstore and provide back digests of the metadata to verify it is intact. However,\nthey do not address recovery of the integrity metadata in case of damage or\nattack by an adversary. In essence, they do not preserve this metadata. We\nintroduce IntegrityCatalog, a system that collects all integrity related\nmetadata in a single component, and treats them as first class objects,\nmanaging both their integrity and their preservation. We introduce a\ntreap-based persistent authenticated dictionary managing arbitrary length\nkey/value pairs, which we use to store all integrity metadata, accessible\nsimply by object name. Additionally, IntegrityCatalog is a distributed system\nthat includes a network protocol that manages both corruption detection and\npreservation of this metadata, using administrator-selected network peers with\ntwo possible roles. Verifiers store and offer attestations on digests and have\nminimal storage requirements, while preservers efficiently synchronize a\ncomplete copy of the catalog to assist in recovery in case of a detected\ncatalog compromise on the local system. We describe our prototype\nimplementation of IntegrityCatalog, measure its performance empirically, and\ndemonstrate its effectiveness in real-world situations, with worst measured\nthroughput of approximately 1K insertions per second, and 2K verified search\noperations per second.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 17:52:22 GMT"}, {"version": "v2", "created": "Thu, 25 Sep 2014 09:13:02 GMT"}], "update_date": "2014-09-26", "authors_parsed": [["Chondros", "Nikos", ""], ["Roussopoulos", "Mema", ""]]}, {"id": "1403.2194", "submitter": "Yannis Haralambous", "authors": "Yannis Haralambous and Pedro Quaresma", "title": "Querying Geometric Figures Using a Controlled Language, Ontological\n  Graphs and Dependency Lattices", "comments": "14 pages, 5 figures, accepted at CICM 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.AI cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic geometry systems (DGS) have become basic tools in many areas of\ngeometry as, for example, in education. Geometry Automated Theorem Provers\n(GATP) are an active area of research and are considered as being basic tools\nin future enhanced educational software as well as in a next generation of\nmechanized mathematics assistants. Recently emerged Web repositories of\ngeometric knowledge, like TGTP and Intergeo, are an attempt to make the already\nvast data set of geometric knowledge widely available. Considering the large\namount of geometric information already available, we face the need of a query\nmechanism for descriptions of geometric constructions.\n  In this paper we discuss two approaches for describing geometric figures\n(declarative and procedural), and present algorithms for querying geometric\nfigures in declaratively and procedurally described corpora, by using a DGS or\na dedicated controlled natural language for queries.\n", "versions": [{"version": "v1", "created": "Mon, 10 Mar 2014 09:39:12 GMT"}, {"version": "v2", "created": "Thu, 8 May 2014 07:06:02 GMT"}, {"version": "v3", "created": "Tue, 13 May 2014 23:44:33 GMT"}], "update_date": "2014-05-15", "authors_parsed": [["Haralambous", "Yannis", ""], ["Quaresma", "Pedro", ""]]}, {"id": "1403.2307", "submitter": "Sudip Roy Sudip Roy", "authors": "Sudip Roy, Lucja Kot, Gabriel Bender, Bailu Ding, Hossein Hojjat,\n  Christoph Koch, Nate Foster, Johannes Gehrke", "title": "The Homeostasis Protocol: Avoiding Transaction Coordination Through\n  Program Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Datastores today rely on distribution and replication to achieve improved\nperformance and fault-tolerance. But correctness of many applications depends\non strong consistency properties - something that can impose substantial\noverheads, since it requires coordinating the behavior of multiple nodes. This\npaper describes a new approach to achieving strong consistency in distributed\nsystems while minimizing communication between nodes. The key insight is to\nallow the state of the system to be inconsistent during execution, as long as\nthis inconsistency is bounded and does not affect transaction correctness. In\ncontrast to previous work, our approach uses program analysis to extract\nsemantic information about permissible levels of inconsistency and is fully\nautomated. We then employ a novel homeostasis protocol to allow sites to\noperate independently, without communicating, as long as any inconsistency is\ngoverned by appropriate treaties between the nodes. We discuss mechanisms for\noptimizing treaties based on workload characteristics to minimize\ncommunication, as well as a prototype implementation and experiments that\ndemonstrate the benefits of our approach on common transactional benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 10 Mar 2014 17:12:09 GMT"}, {"version": "v2", "created": "Tue, 20 Jan 2015 02:54:07 GMT"}], "update_date": "2015-01-21", "authors_parsed": [["Roy", "Sudip", ""], ["Kot", "Lucja", ""], ["Bender", "Gabriel", ""], ["Ding", "Bailu", ""], ["Hojjat", "Hossein", ""], ["Koch", "Christoph", ""], ["Foster", "Nate", ""], ["Gehrke", "Johannes", ""]]}, {"id": "1403.2404", "submitter": "Long Cheng", "authors": "Long Cheng, Avinash Malik, Spyros Kotoulas, Tomas E Ward, Georgios\n  Theodoropoulos", "title": "Scalable RDF Data Compression using X10", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Semantic Web comprises enormous volumes of semi-structured data elements.\nFor interoperability, these elements are represented by long strings. Such\nrepresentations are not efficient for the purposes of Semantic Web applications\nthat perform computations over large volumes of information. A typical method\nfor alleviating the impact of this problem is through the use of compression\nmethods that produce more compact representations of the data. The use of\ndictionary encoding for this purpose is particularly prevalent in Semantic Web\ndatabase systems. However, centralized implementations present performance\nbottlenecks, giving rise to the need for scalable, efficient distributed\nencoding schemes. In this paper, we describe an encoding implementation based\non the asynchronous partitioned global address space (APGAS) parallel\nprogramming model. We evaluate performance on a cluster of up to 384 cores and\ndatasets of up to 11 billion triples (1.9 TB). Compared to the state-of-art\nMapReduce algorithm, we demonstrate a speedup of 2.6-7.4x and excellent\nscalability. These results illustrate the strong potential of the APGAS model\nfor efficient implementation of dictionary encoding and contributes to the\nengineering of larger scale Semantic Web applications.\n", "versions": [{"version": "v1", "created": "Mon, 10 Mar 2014 20:48:08 GMT"}], "update_date": "2014-03-12", "authors_parsed": [["Cheng", "Long", ""], ["Malik", "Avinash", ""], ["Kotoulas", "Spyros", ""], ["Ward", "Tomas E", ""], ["Theodoropoulos", "Georgios", ""]]}, {"id": "1403.2763", "submitter": "Saravanan Thirumuruanathan", "authors": "Weimo Liu, Saravanan Thirumuruganathan, Nan Zhang, Gautam Das", "title": "Aggregate Estimation Over Dynamic Hidden Web Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many databases on the web are \"hidden\" behind (i.e., accessible only through)\ntheir restrictive, form-like, search interfaces. Recent studies have shown that\nit is possible to estimate aggregate query answers over such hidden web\ndatabases by issuing a small number of carefully designed search queries\nthrough the restrictive web interface. A problem with these existing work,\nhowever, is that they all assume the underlying database to be static, while\nmost real-world web databases (e.g., Amazon, eBay) are frequently updated. In\nthis paper, we study the novel problem of estimating/tracking aggregates over\ndynamic hidden web databases while adhering to the stringent query-cost\nlimitation they enforce (e.g., at most 1,000 search queries per day).\nTheoretical analysis and extensive real-world experiments demonstrate the\neffectiveness of our proposed algorithms and their superiority over baseline\nsolutions (e.g., the repeated execution of algorithms designed for static web\ndatabases).\n", "versions": [{"version": "v1", "created": "Tue, 11 Mar 2014 21:32:59 GMT"}, {"version": "v2", "created": "Thu, 1 May 2014 17:28:16 GMT"}], "update_date": "2014-05-02", "authors_parsed": [["Liu", "Weimo", ""], ["Thirumuruganathan", "Saravanan", ""], ["Zhang", "Nan", ""], ["Das", "Gautam", ""]]}, {"id": "1403.2848", "submitter": "Suprativ Saha", "authors": "Ananya Bose and Suprativ Saha", "title": "Delineation of Techniques to implement on the enhanced proposed model\n  using data mining for protein sequence classification", "comments": "8 pages, 1 figures", "journal-ref": "International Journal of Database Management Systems ( IJDMS )\n  Vol.6, No.1, February 2014", "doi": "10.5121/ijdms.2014.6105", "report-no": null, "categories": "cs.DB cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In post genomic era with the advent of new technologies a huge amount of\ncomplex molecular data are generated with high throughput. The management of\nthis biological data is definitely a challenging task due to complexity and\nheterogeneity of data for discovering new knowledge. Issues like managing noisy\nand incomplete data are needed to be dealt with. Use of data mining in\nbiological domain has made its inventory success. Discovering new knowledge\nfrom the biological data is a major challenge in data mining technique. The\nnovelty of the proposed model is its combined use of intelligent techniques to\nclassify the protein sequence faster and efficiently. Use of FFT, fuzzy\nclassifier, String weighted algorithm, gram encoding method, neural network\nmodel and rough set classifier in a single model and in an appropriate place\ncan enhance the quality of the classification system.Thus the primary challenge\nis to identify and classify the large protein sequences in a very fast and easy\nbut intellectual way to decrease the time complexity and space complexity.\n", "versions": [{"version": "v1", "created": "Wed, 12 Mar 2014 08:44:08 GMT"}], "update_date": "2014-03-13", "authors_parsed": [["Bose", "Ananya", ""], ["Saha", "Suprativ", ""]]}, {"id": "1403.2958", "submitter": "Deepa  S", "authors": "Deepa S", "title": "An Approach for Normalizing Fuzzy Relational Databases Based on Join\n  Dependency", "comments": "3 pages", "journal-ref": "International Journal of Computer Trends and Technology (IJCTT)\n  V9(1):1-3, March 2014. ISSN:2231-2803. www.ijcttjournal.org. Published by\n  Seventh Sense Research Group", "doi": "10.14445/22312803/IJCTT-V9P101", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Fuzziness in databases is used to denote uncertain or incomplete data.\nRelational Databases stress on the nature of the data to be certain. This\ncertainty based data is used as the basis of the normalization approach\ndesigned for traditional relational databases. But real world data may not\nalways be certain, thereby making it necessary to design an approach for\nnormalization that deals with fuzzy data. This paper focuses on the approach\nfor designing the fifth normal form (5NF) based on join dependencies for fuzzy\ndata. The basis of join dependency for fuzzy relational databases is derived\nfrom the basic relational database concepts. As join dependency implies an\nmultivalued dependency by symmetry the proof of join dependency based\nnormalization is stated from the perspective of multivalued dependency based\nnormalization on fuzzy relational databases.\n", "versions": [{"version": "v1", "created": "Tue, 11 Mar 2014 07:30:43 GMT"}], "update_date": "2014-03-13", "authors_parsed": [["S", "Deepa", ""]]}, {"id": "1403.3304", "submitter": "Hadi Hajari", "authors": "Hadi Hajari and Farshad Hakimpour", "title": "A Spatial Data Model for Moving Object Databases", "comments": "This paper includes 20 pages and is published in IJDMS", "journal-ref": "International Journal of Database Management Systems ( IJDMS )\n  Vol.6, No.1, February 2014. ISSN:0975-5705 (Online); 0975-5985", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Moving Object Databases will have significant role in Geospatial Information\nSystems as they allow users to model continuous movements of entities in the\ndatabases and perform spatio-temporal analysis. For representing and querying\nmoving objects, and algebra with a comprehensive framework of User Defined\nTypes together with a set of functions on those types is needed. Moreover,\nconcerning real world applications, moving objects move along constrained\nenvironments like transportation networks so that an extra algebra for modeling\nnetworks is demanded, too. These algebras can be inserted in any data model if\ntheir designs are based on available standards such as Open Geospatial\nConsortium that provides a common model for existing DBMS's. In this paper, we\nfocus on extending a spatial data model for constrained moving objects. Static\nand moving geometries in our model are based on Open Geospatial Consortium\nstandards. We also extend Structured Query Language for retrieving, querying,\nand manipulating spatio-temporal data related to moving objects as a simple and\nexpressive query language. Finally as a proof of concept, we implement a\ngenerator to generate data for moving objects constrained by a transportation\nnetwork. Such a generator primarily aims at traffic planning applications.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2014 15:40:54 GMT"}], "update_date": "2014-03-14", "authors_parsed": [["Hajari", "Hadi", ""], ["Hakimpour", "Farshad", ""]]}, {"id": "1403.3460", "submitter": "Chi Wang", "authors": "Chi Wang, Xueqing Liu, Yanglei Song, Jiawei Han", "title": "Scalable and Robust Construction of Topical Hierarchies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated generation of high-quality topical hierarchies for a text\ncollection is a dream problem in knowledge engineering with many valuable\napplications. In this paper a scalable and robust algorithm is proposed for\nconstructing a hierarchy of topics from a text collection. We divide and\nconquer the problem using a top-down recursive framework, based on a tensor\northogonal decomposition technique. We solve a critical challenge to perform\nscalable inference for our newly designed hierarchical topic model. Experiments\nwith various real-world datasets illustrate its ability to generate robust,\nhigh-quality hierarchies efficiently. Our method reduces the time of\nconstruction by several orders of magnitude, and its robust feature renders it\npossible for users to interactively revise the hierarchy.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2014 23:22:21 GMT"}], "update_date": "2014-03-17", "authors_parsed": [["Wang", "Chi", ""], ["Liu", "Xueqing", ""], ["Song", "Yanglei", ""], ["Han", "Jiawei", ""]]}, {"id": "1403.3495", "submitter": "Shuliang Wang", "authors": "Shuliang Wang, Yiping Zhao", "title": "Analyzing Large Biological Datasets with an Improved Algorithm for MIC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A computational framework utilizes the traditional similarity measures for\nmining the significant relationships in biological annotations is recently\nproposed by Tatiana V. Karpinets et al. [2]. In this paper, an improved\napproximation algorithm for MIC (maximal information coefficient) named IAMIC\nis suggested to perfect this framework for discovering the hidden regularities\nbetween biological annotations. Further, IAMIC is the enhanced algorithm for\napproximating a novel similarity coefficient MIC with generality and\nequitability, which makes it more appropriate for data exploration. Here it is\nshown that IAMIC is also applicable for identify the associations between\nbiological annotations.\n", "versions": [{"version": "v1", "created": "Fri, 14 Mar 2014 07:26:33 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Wang", "Shuliang", ""], ["Zhao", "Yiping", ""]]}, {"id": "1403.3758", "submitter": "Philippe Besse", "authors": "Philippe Besse (IMT), Aur\\'elien Garivier (IMT), Jean-Michel Loubes\n  (IMT)", "title": "Big Data Analytics - Retour vers le Futur 3; De Statisticien \\`a Data\n  Scientist", "comments": "in French", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DB stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid evolution of information systems managing more and more voluminous\ndata has caused profound paradigm shifts in the job of statistician, becoming\nsuccessively data miner, bioinformatician and now data scientist. Without the\nsake of completeness and after having illustrated these successive mutations,\nthis article briefly introduced the new research issues that quickly rise in\nStatistics, and more generally in Mathematics, in order to integrate the\ncharacteristics: volume, variety and velocity, of big data.\n", "versions": [{"version": "v1", "created": "Sat, 15 Mar 2014 06:10:50 GMT"}, {"version": "v2", "created": "Tue, 18 Mar 2014 05:29:20 GMT"}, {"version": "v3", "created": "Wed, 21 May 2014 09:12:05 GMT"}], "update_date": "2014-05-22", "authors_parsed": [["Besse", "Philippe", "", "IMT"], ["Garivier", "Aur\u00e9lien", "", "IMT"], ["Loubes", "Jean-Michel", "", "IMT"]]}, {"id": "1403.3909", "submitter": "Nesreen Ahmed", "authors": "Nesreen K. Ahmed, Nick Duffield, Jennifer Neville, Ramana Kompella", "title": "Graph Sample and Hold: A Framework for Big-Graph Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sampling is a standard approach in big-graph analytics; the goal is to\nefficiently estimate the graph properties by consulting a sample of the whole\npopulation. A perfect sample is assumed to mirror every property of the whole\npopulation. Unfortunately, such a perfect sample is hard to collect in complex\npopulations such as graphs (e.g. web graphs, social networks etc), where an\nunderlying network connects the units of the population. Therefore, a good\nsample will be representative in the sense that graph properties of interest\ncan be estimated with a known degree of accuracy. While previous work focused\nparticularly on sampling schemes used to estimate certain graph properties\n(e.g. triangle count), much less is known for the case when we need to estimate\nvarious graph properties with the same sampling scheme. In this paper, we\npropose a generic stream sampling framework for big-graph analytics, called\nGraph Sample and Hold (gSH). To begin, the proposed framework samples from\nmassive graphs sequentially in a single pass, one edge at a time, while\nmaintaining a small state. We then show how to produce unbiased estimators for\nvarious graph properties from the sample. Given that the graph analysis\nalgorithms will run on a sample instead of the whole population, the runtime\ncomplexity of these algorithm is kept under control. Moreover, given that the\nestimators of graph properties are unbiased, the approximation error is kept\nunder control. Finally, we show the performance of the proposed framework (gSH)\non various types of graphs, such as social graphs, among others.\n", "versions": [{"version": "v1", "created": "Sun, 16 Mar 2014 12:26:45 GMT"}], "update_date": "2014-03-18", "authors_parsed": [["Ahmed", "Nesreen K.", ""], ["Duffield", "Nick", ""], ["Neville", "Jennifer", ""], ["Kompella", "Ramana", ""]]}, {"id": "1403.3948", "submitter": "Mohammed Al-Maolegi", "authors": "Mohammed Al-Maolegi, Bassam Arkok", "title": "An Improved Apriori Algorithm for Association Rules", "comments": "9 pages, 3 figures, 8 tables, journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  There are several mining algorithms of association rules. One of the most\npopular algorithms is Apriori that is used to extract frequent itemsets from\nlarge database and getting the association rule for discovering the knowledge.\nBased on this algorithm, this paper indicates the limitation of the original\nApriori algorithm of wasting time for scanning the whole database searching on\nthe frequent itemsets, and presents an improvement on Apriori by reducing that\nwasted time depending on scanning only some transactions. The paper shows by\nexperimental results with several groups of transactions, and with several\nvalues of minimum support that applied on the original Apriori and our\nimplemented improved Apriori that our improved Apriori reduces the time\nconsumed by 67.38% in comparison with the original Apriori, and makes the\nApriori algorithm more efficient and less time consuming.\n", "versions": [{"version": "v1", "created": "Sun, 16 Mar 2014 18:50:54 GMT"}], "update_date": "2014-03-18", "authors_parsed": [["Al-Maolegi", "Mohammed", ""], ["Arkok", "Bassam", ""]]}, {"id": "1403.5006", "submitter": "Ning Yan", "authors": "Ning Yan, Sona Hasani, Abolfazl Asudeh, Chengkai Li", "title": "Generating Preview Tables for Entity Graphs", "comments": "This is the camera-ready version of a SIGMOD16 paper. There might be\n  tiny differences in layout, spacing and linebreaking, compared with the\n  version in the SIGMOD16 proceedings, since we must submit TeX files and use\n  arXiv to compile the files", "journal-ref": null, "doi": "10.1145/2882903.2915221", "report-no": null, "categories": "cs.DB cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Users are tapping into massive, heterogeneous entity graphs for many\napplications. It is challenging to select entity graphs for a particular need,\ngiven abundant datasets from many sources and the oftentimes scarce information\nfor them. We propose methods to produce preview tables for compact presentation\nof important entity types and relationships in entity graphs. The preview\ntables assist users in attaining a quick and rough preview of the data. They\ncan be shown in a limited display space for a user to browse and explore,\nbefore she decides to spend time and resources to fetch and investigate the\ncomplete dataset. We formulate several optimization problems that look for\npreviews with the highest scores according to intuitive goodness measures,\nunder various constraints on preview size and distance between preview tables.\nThe optimization problem under distance constraint is NP-hard. We design a\ndynamic-programming algorithm and an Apriori-style algorithm for finding\noptimal previews. Results from experiments, comparison with related work and\nuser studies demonstrated the scoring measures' accuracy and the discovery\nalgorithms' efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 20 Mar 2014 00:21:37 GMT"}, {"version": "v2", "created": "Wed, 4 May 2016 04:40:31 GMT"}], "update_date": "2016-05-05", "authors_parsed": [["Yan", "Ning", ""], ["Hasani", "Sona", ""], ["Asudeh", "Abolfazl", ""], ["Li", "Chengkai", ""]]}, {"id": "1403.5199", "submitter": "Rada Chirkova", "authors": "Rada Chirkova and Ting Yu", "title": "Obtaining Information about Queries behind Views and Dependencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problems of finding and determining certain query answers and\nof determining containment between queries; each problem is formulated in\npresence of materialized views and dependencies under the closed-world\nassumption. We show a tight relationship between the problems in this setting.\nFurther, we introduce algorithms for solving each problem for those inputs\nwhere all the queries and views are conjunctive, and the dependencies are\nembedded weakly acyclic. We also determine the complexity of each problem under\nthe security-relevant complexity measure introduced by Zhang and Mendelzon in\n2005. The problems studied in this paper are fundamental in ensuring correct\nspecification of database access-control policies, in particular in case of\nfine-grained access control. Our approaches can also be applied in the areas of\ninference control, secure data publishing, and database auditing.\n", "versions": [{"version": "v1", "created": "Thu, 20 Mar 2014 16:46:37 GMT"}], "update_date": "2014-03-21", "authors_parsed": [["Chirkova", "Rada", ""], ["Yu", "Ting", ""]]}, {"id": "1403.5381", "submitter": "Silu Huang", "authors": "Silu Huang, Ada Wai-Chee Fu", "title": "({\\alpha}, k)-Minimal Sorting and Skew Join in MPI and MapReduce", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As computer clusters are found to be highly effective for handling massive\ndatasets, the design of efficient parallel algorithms for such a computing\nmodel is of great interest. We consider ({\\alpha}, k)-minimal algorithms for\nsuch a purpose, where {\\alpha} is the number of rounds in the algorithm, and k\nis a bound on the deviation from perfect workload balance. We focus on new\n({\\alpha}, k)-minimal algorithms for sorting and skew equijoin operations for\ncomputer clusters. To the best of our knowledge the proposed sorting and skew\njoin algorithms achieve the best workload balancing guarantee when compared to\nprevious works. Our empirical study shows that they are close to optimal in\nworkload balancing. In particular, our proposed sorting algorithm is around 25%\nmore efficient than the state-of-the-art Terasort algorithm and achieves\nsignificantly more even workload distribution by over 50%.\n", "versions": [{"version": "v1", "created": "Fri, 21 Mar 2014 07:10:16 GMT"}], "update_date": "2014-03-24", "authors_parsed": [["Huang", "Silu", ""], ["Fu", "Ada Wai-Chee", ""]]}, {"id": "1403.5645", "submitter": "Todd Veldhuizen", "authors": "Todd L. Veldhuizen", "title": "Transaction Repair: Full Serializability Without Locks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transaction Repair is a method for lock-free, scalable transaction processing\nthat achieves full serializability. It demonstrates parallel speedup even in\ninimical scenarios where all pairs of transactions have significant read-write\nconflicts. In the transaction repair approach, each transaction runs in\ncomplete isolation in a branch of the database; when conflicts occur, we detect\nand repair them. These repairs are performed efficiently in parallel, and the\nnet effect is that of serial processing. Within transactions, we use no locks.\nThis frees users from the complications and performance hazards of locks, and\nfrom the anomalies of sub-SERIALIZABLE isolation levels. Our approach builds on\nan incrementalized variant of leapfrog triejoin, a worst-case optimal algorithm\nfor $\\exists_1$ formulae, and on well-established techniques from programming\nlanguages: declarative languages, purely functional data structures,\nincremental computation, and fixpoint equations.\n", "versions": [{"version": "v1", "created": "Sat, 22 Mar 2014 11:04:03 GMT"}], "update_date": "2014-03-25", "authors_parsed": [["Veldhuizen", "Todd L.", ""]]}, {"id": "1403.5715", "submitter": "Scott Stoller", "authors": "Zhongyuan Xu and Scott D. Stoller", "title": "Mining Attribute-Based Access Control Policies from Logs", "comments": "arXiv admin note: substantial text overlap with arXiv:1306.2401", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attribute-based access control (ABAC) provides a high level of flexibility\nthat promotes security and information sharing. ABAC policy mining algorithms\nhave potential to significantly reduce the cost of migration to ABAC, by\npartially automating the development of an ABAC policy from information about\nthe existing access-control policy and attribute data. This paper presents an\nalgorithm for mining ABAC policies from operation logs and attribute data. To\nthe best of our knowledge, it is the first algorithm for this problem.\n", "versions": [{"version": "v1", "created": "Sun, 23 Mar 2014 03:09:28 GMT"}, {"version": "v2", "created": "Thu, 27 Mar 2014 17:36:15 GMT"}, {"version": "v3", "created": "Sat, 31 May 2014 03:31:38 GMT"}, {"version": "v4", "created": "Fri, 25 Jul 2014 19:53:25 GMT"}, {"version": "v5", "created": "Tue, 13 Feb 2018 03:21:49 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Xu", "Zhongyuan", ""], ["Stoller", "Scott D.", ""]]}, {"id": "1403.5946", "submitter": "Jack Kelly", "authors": "Jack Kelly and William Knottenbelt", "title": "Metadata for Energy Disaggregation", "comments": "To appear in The 2nd IEEE International Workshop on Consumer Devices\n  and Systems (CDS 2014) in V\\\"aster{\\aa}s, Sweden", "journal-ref": null, "doi": "10.1109/COMPSACW.2014.97", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy disaggregation is the process of estimating the energy consumed by\nindividual electrical appliances given only a time series of the whole-home\npower demand. Energy disaggregation researchers require datasets of the power\ndemand from individual appliances and the whole-home power demand. Multiple\nsuch datasets have been released over the last few years but provide metadata\nin a disparate array of formats including CSV files and plain-text README\nfiles. At best, the lack of a standard metadata schema makes it unnecessarily\ntime-consuming to write software to process multiple datasets and, at worse,\nthe lack of a standard means that crucial information is simply absent from\nsome datasets. We propose a metadata schema for representing appliances,\nmeters, buildings, datasets, prior knowledge about appliances and appliance\nmodels. The schema is relational and provides a simple but powerful inheritance\nmechanism.\n", "versions": [{"version": "v1", "created": "Mon, 24 Mar 2014 13:29:04 GMT"}, {"version": "v2", "created": "Wed, 2 Apr 2014 14:50:39 GMT"}, {"version": "v3", "created": "Mon, 19 May 2014 22:15:00 GMT"}], "update_date": "2015-09-23", "authors_parsed": [["Kelly", "Jack", ""], ["Knottenbelt", "William", ""]]}, {"id": "1403.6089", "submitter": "Zoran Majkic", "authors": "Zoran Majkic", "title": "Intensional RDB for Big Data Interoperability", "comments": "30 pages, 3 figures. arXiv admin note: substantial text overlap with\n  arXiv:1103.0967, arXiv:1403.0017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new family of Intensional RDBs (IRDBs), introduced in [1], extends the\ntraditional RDBs with the Big Data and flexible and 'Open schema' features,\nable to preserve the user-defined relational database schemas and all\npreexisting user's applications containing the SQL statements for a deployment\nof such a relational data. The standard RDB data is parsed into an internal\nvector key/value relation, so that we obtain a column representation of data\nused in Big Data applications, covering the key/value and column-based Big Data\napplications as well, into a unifying RDB framework. Such an IRDB architecture\nis adequate for the massive migrations from the existing slow RDBMSs into this\nnew family of fast IRDBMSs by offering a Big Data and new flexible schema\nfeatures as well. Here we present the interoperability features of the IRDBs by\npermitting the queries also over the internal vector relations created by\nparsing of each federated database in a given Multidatabase system. We show\nthat the SchemaLog with the second-order syntax and ad hoc Logic Programming\nand its querying fragment can be embedded into the standard SQL IRDBMSs, so\nthat we obtain a full interoperabilty features of IRDBs by using only the\nstandard relational SQL for querying both data and meta-data.\n", "versions": [{"version": "v1", "created": "Mon, 24 Mar 2014 19:15:37 GMT"}, {"version": "v2", "created": "Thu, 10 Apr 2014 19:00:09 GMT"}], "update_date": "2014-04-11", "authors_parsed": [["Majkic", "Zoran", ""]]}, {"id": "1403.6968", "submitter": "Milos Nikolic", "authors": "Milos Nikolic, Mohammed ElSeidy, Christoph Koch", "title": "LINVIEW: Incremental View Maintenance for Complex Analytical Queries", "comments": "14 pages, SIGMOD", "journal-ref": null, "doi": "10.1145/2588555.2610519", "report-no": null, "categories": "cs.DB cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many analytics tasks and machine learning problems can be naturally expressed\nby iterative linear algebra programs. In this paper, we study the incremental\nview maintenance problem for such complex analytical queries. We develop a\nframework, called LINVIEW, for capturing deltas of linear algebra programs and\nunderstanding their computational cost. Linear algebra operations tend to cause\nan avalanche effect where even very local changes to the input matrices spread\nout and infect all of the intermediate results and the final view, causing\nincremental view maintenance to lose its performance benefit over\nre-evaluation. We develop techniques based on matrix factorizations to contain\nsuch epidemics of change. As a consequence, our techniques make incremental\nview maintenance of linear algebra practical and usually substantially cheaper\nthan re-evaluation. We show, both analytically and experimentally, the\nusefulness of these techniques when applied to standard analytics tasks. Our\nevaluation demonstrates the efficiency of LINVIEW in generating parallel\nincremental programs that outperform re-evaluation techniques by more than an\norder of magnitude.\n", "versions": [{"version": "v1", "created": "Thu, 27 Mar 2014 10:22:32 GMT"}, {"version": "v2", "created": "Fri, 9 May 2014 10:54:26 GMT"}], "update_date": "2014-05-12", "authors_parsed": [["Nikolic", "Milos", ""], ["ElSeidy", "Mohammed", ""], ["Koch", "Christoph", ""]]}, {"id": "1403.6985", "submitter": "Kostyantyn Demchuk", "authors": "Kostyantyn Demchuk and Douglas J. Leith", "title": "A Fast Minimal Infrequent Itemset Mining Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel fast algorithm for finding quasi identifiers in large datasets is\npresented. Performance measurements on a broad range of datasets demonstrate\nsubstantial reductions in run-time relative to the state of the art and the\nscalability of the algorithm to realistically-sized datasets up to several\nmillion records.\n", "versions": [{"version": "v1", "created": "Thu, 27 Mar 2014 11:54:27 GMT"}, {"version": "v2", "created": "Fri, 11 Apr 2014 15:53:20 GMT"}, {"version": "v3", "created": "Thu, 16 Oct 2014 14:56:52 GMT"}], "update_date": "2014-10-17", "authors_parsed": [["Demchuk", "Kostyantyn", ""], ["Leith", "Douglas J.", ""]]}, {"id": "1403.7248", "submitter": "Axel Polleres", "authors": "Albin Ahmeti and Diego Calvanese and Axel Polleres", "title": "Updating RDFS ABoxes and TBoxes in SPARQL", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Updates in RDF stores have recently been standardised in the SPARQL 1.1\nUpdate specification. However, computing answers entailed by ontologies in\ntriple stores is usually treated orthogonal to updates. Even the W3C's recent\nSPARQL 1.1 Update language and SPARQL 1.1 Entailment Regimes specifications\nexplicitly exclude a standard behaviour how SPARQL endpoints should treat\nentailment regimes other than simple entailment in the context of updates. In\nthis paper, we take a first step to close this gap. We define a fragment of\nSPARQL basic graph patterns corresponding to (the RDFS fragment of) DL-Lite and\nthe corresponding SPARQL update language, dealing with updates both of ABox and\nof TBox statements. We discuss possible semantics along with potential\nstrategies for implementing them. We treat both, (i) materialised RDF stores,\nwhich store all entailed triples explicitly, and (ii) reduced RDF Stores, that\nis, redundancy-free RDF stores that do not store any RDF triples (corresponding\nto DL-Lite ABox statements) entailed by others already.\n", "versions": [{"version": "v1", "created": "Thu, 27 Mar 2014 23:43:38 GMT"}], "update_date": "2014-03-31", "authors_parsed": [["Ahmeti", "Albin", ""], ["Calvanese", "Diego", ""], ["Polleres", "Axel", ""]]}, {"id": "1403.7550", "submitter": "Ce Zhang", "authors": "Ce Zhang and Christopher R\\'e", "title": "DimmWitted: A Study of Main-Memory Statistical Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We perform the first study of the tradeoff space of access methods and\nreplication to support statistical analytics using first-order methods executed\nin the main memory of a Non-Uniform Memory Access (NUMA) machine. Statistical\nanalytics systems differ from conventional SQL-analytics in the amount and\ntypes of memory incoherence they can tolerate. Our goal is to understand\ntradeoffs in accessing the data in row- or column-order and at what granularity\none should share the model and data for a statistical task. We study this new\ntradeoff space, and discover there are tradeoffs between hardware and\nstatistical efficiency. We argue that our tradeoff study may provide valuable\ninformation for designers of analytics engines: for each system we consider,\nour prototype engine can run at least one popular task at least 100x faster. We\nconduct our study across five architectures using popular models including\nSVMs, logistic regression, Gibbs sampling, and neural networks.\n", "versions": [{"version": "v1", "created": "Fri, 28 Mar 2014 21:48:00 GMT"}, {"version": "v2", "created": "Thu, 1 May 2014 06:14:29 GMT"}, {"version": "v3", "created": "Mon, 7 Jul 2014 17:20:20 GMT"}], "update_date": "2014-07-08", "authors_parsed": [["Zhang", "Ce", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1403.7729", "submitter": "Minos Garofalakis", "authors": "Minos Garofalakis and Yannis Ioannidis", "title": "Multi-Resource Parallel Query Scheduling and Optimization", "comments": "50 pages; Conference version of the paper has appeared in the\n  Proceedings of the 23rd International Conference on Very Large Databases\n  (VLDB'1997), Athens, Greece, August 1997", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scheduling query execution plans is a particularly complex problem in\nshared-nothing parallel systems, where each site consists of a collection of\nlocal time-shared (e.g., CPU(s) or disk(s)) and space-shared (e.g., memory)\nresources and communicates with remote sites by message-passing. Earlier work\non parallel query scheduling employs either (a) one-dimensional models of\nparallel task scheduling, effectively ignoring the potential benefits of\nresource sharing, or (b) models of globally accessible resource units, which\nare appropriate only for shared-memory architectures, since they cannot capture\nthe affinity of system resources to sites. In this paper, we develop a general\napproach capturing the full complexity of scheduling distributed,\nmulti-dimensional resource units for all forms of parallelism within and across\nqueries and operators. We present a level-based list scheduling heuristic\nalgorithm for independent query tasks (i.e., physical operator pipelines) that\nis provably near-optimal for given degrees of partitioned parallelism (with a\nworst-case performance ratio that depends on the number of time-shared and\nspace-shared resources per site and the granularity of the clones). We also\npropose extensions to handle blocking constraints in logical operator (e.g.,\nhash-join) pipelines and bushy query plans as well as on-line task arrivals\n(e.g., in a dynamic or multi-query execution environment). Experiments with our\nscheduling algorithms implemented on top of a detailed simulation model verify\ntheir effectiveness compared to existing approaches in a realistic setting.\nBased on our analytical and experimental results, we revisit the open problem\nof designing efficient cost models for parallel query optimization and propose\na solution that captures all the important parameters of parallel execution.\n", "versions": [{"version": "v1", "created": "Sun, 30 Mar 2014 10:15:06 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["Garofalakis", "Minos", ""], ["Ioannidis", "Yannis", ""]]}, {"id": "1403.7928", "submitter": "Jakub Urban", "authors": "J. Urban, J. Pipek, M. Hron, F. Janky, R. Pap\\v{r}ok, M. Peterka, A.\n  S. Duarte", "title": "Integrated Data Acquisition, Storage, Retrieval and Processing Using the\n  COMPASS DataBase (CDB)", "comments": null, "journal-ref": null, "doi": "10.1016/j.fusengdes.2014.03.032", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a complex data handling system for the COMPASS tokamak, operated\nby IPP ASCR Prague, Czech Republic [1]. The system, called CDB (Compass\nDataBase), integrates different data sources as an assortment of data\nacquisition hardware and software from different vendors is used. Based on\nwidely available open source technologies wherever possible, CDB is vendor and\nplatform independent and it can be easily scaled and distributed. The data is\ndirectly stored and retrieved using a standard NAS (Network Attached Storage),\nhence independent of the particular technology; the description of the data\n(the metadata) is recorded in a relational database. Database structure is\ngeneral and enables the inclusion of multi-dimensional data signals in multiple\nrevisions (no data is overwritten). This design is inherently distributed as\nthe work is off-loaded to the clients. Both NAS and database can be implemented\nand optimized for fast local access as well as secure remote access. CDB is\nimplemented in Python language; bindings for Java, C/C++, IDL and Matlab are\nprovided. Independent data acquisitions systems as well as nodes managed by\nFireSignal [2] are all integrated using CDB. An automated data post-processing\nserver is a part of CDB. Based on dependency rules, the server executes, in\nparallel if possible, prescribed post-processing tasks.\n", "versions": [{"version": "v1", "created": "Mon, 31 Mar 2014 09:38:37 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["Urban", "J.", ""], ["Pipek", "J.", ""], ["Hron", "M.", ""], ["Janky", "F.", ""], ["Pap\u0159ok", "R.", ""], ["Peterka", "M.", ""], ["Duarte", "A. S.", ""]]}, {"id": "1403.8144", "submitter": "Ping Li", "authors": "Ping Li, Michael Mitzenmacher, Anshumali Shrivastava", "title": "Coding for Random Projections and Approximate Near Neighbor Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical note compares two coding (quantization) schemes for random\nprojections in the context of sub-linear time approximate near neighbor search.\nThe first scheme is based on uniform quantization while the second scheme\nutilizes a uniform quantization plus a uniformly random offset (which has been\npopular in practice). The prior work compared the two schemes in the context of\nsimilarity estimation and training linear classifiers, with the conclusion that\nthe step of random offset is not necessary and may hurt the performance\n(depending on the similarity level). The task of near neighbor search is\nrelated to similarity estimation with importance distinctions and requires own\nstudy. In this paper, we demonstrate that in the context of near neighbor\nsearch, the step of random offset is not needed either and may hurt the\nperformance (sometimes significantly so, depending on the similarity and other\nparameters).\n", "versions": [{"version": "v1", "created": "Mon, 31 Mar 2014 19:43:53 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["Li", "Ping", ""], ["Mitzenmacher", "Michael", ""], ["Shrivastava", "Anshumali", ""]]}]