[{"id": "2102.00177", "submitter": "Chao Li", "authors": "Chao Li, Balaji Palanisamy, Runhua Xu, Jinlai Xu, and Jingzhe Wang", "title": "SteemOps: Extracting and Analyzing Key Operations in Steemit\n  Blockchain-based Social Media Platform", "comments": "Accepted by ACM CODASPY'21. arXiv admin note: text overlap with\n  arXiv:1904.07310", "journal-ref": null, "doi": "10.1145/3422337.3447845", "report-no": null, "categories": "cs.CR cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advancements in distributed ledger technologies are driving the rise of\nblockchain-based social media platforms such as Steemit, where users interact\nwith each other in similar ways as conventional social networks. These\nplatforms are autonomously managed by users using decentralized consensus\nprotocols in a cryptocurrency ecosystem. The deep integration of social\nnetworks and blockchains in these platforms provides potential for numerous\ncross-domain research studies that are of interest to both the research\ncommunities. However, it is challenging to process and analyze large volumes of\nraw Steemit data as it requires specialized skills in both software engineering\nand blockchain systems and involves substantial efforts in extracting and\nfiltering various types of operations. To tackle this challenge, we collect\nover 38 million blocks generated in Steemit during a 45 month time period from\n2016/03 to 2019/11 and extract ten key types of operations performed by the\nusers. The results generate SteemOps, a new dataset that organizes more than\n900 million operations from Steemit into three sub-datasets namely (i)\nsocial-network operation dataset (SOD), (ii) witness-election operation dataset\n(WOD) and (iii) value-transfer operation dataset (VOD). We describe the dataset\nschema and its usage in detail and outline possible future research studies\nusing SteemOps. SteemOps is designed to facilitate future research aimed at\nproviding deeper insights on emerging blockchain-based social media platforms.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 07:18:39 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 05:51:39 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Li", "Chao", ""], ["Palanisamy", "Balaji", ""], ["Xu", "Runhua", ""], ["Xu", "Jinlai", ""], ["Wang", "Jingzhe", ""]]}, {"id": "2102.00262", "submitter": "Sabah Al-Fedaghi Dr.", "authors": "Sabah Al-Fedaghi", "title": "Conceptual Temporal Modeling Applied to Databases", "comments": "11 pages, 15 figures", "journal-ref": "(IJACSA) International Journal of Advanced Computer Science and\n  Applications, Vol. 12, No. 1, 2021", "doi": null, "report-no": null, "categories": "cs.SE cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a different approach to developing a concept of time for\nspecifying temporality in the conceptual modeling of software and database\nsystems. In the database field, various proposals and products address temporal\ndata. The difficulty with most of the current approaches to modeling\ntemporality is that they represent and record time as just another type of data\n(e.g., values of a bank balance or amounts of money), instead of appreciating\nthat time and its values are unique, in comparison to typical data attributes.\nTime is an engulfing phenomenon that lifts a system s entire model from\nstaticity to dynamism and beyond. In this paper, we propose a conceptualization\nof temporality involving the construction of a multilevel modeling method that\nprogresses from static representation to system compositions that form regions\nof dynamism. Then, a chronology of events is used to define the system s\nbehavior. Lastly, the events are viewed as data sources with which to build a\ntemporal model. A case-study model of a temporal banking-management system\ndatabase that extends UML and the object-constraint language is re-modeled\nusing thinging machine (TM) modeling. The resultant TM diagrammatic\nspecification delivers a new approach to temporality that can be extended to be\na holistic monitoring system for historic data and events.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 16:27:22 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Al-Fedaghi", "Sabah", ""]]}, {"id": "2102.00654", "submitter": "Shun Zhang", "authors": "Shun Zhang, Benfei Duan, Zhili Chen, Tianjiao Ni, and Hong Zhong", "title": "Regionalized location obfuscation mechanism with personalized privacy\n  levels", "comments": "12 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global Positioning Systems are now a standard module in mobile devices, and\ntheir ubiquity is fueling the rapid growth of location-based services (LBSs).\nThis poses the risk of location privacy disclosure. Effective location privacy\npreservation is foremost for various mobile applications. Recently two strong\nprivacy notions, geo-indistinguishability and expected inference error, are\nproposed based on statistical quantification. They are shown to be\ncomplementary for limiting the leakage of location information. In this paper,\nwe argue that personalization means regionalization for\ngeo-indistinguishability, and we propose a regionalized location obfuscation\nmechanism with personalized utility sensitivities. This substantially corrects\nthe differential privacy problem of PIVE framework proposed by Yu, Liu and Pu\non ISOC Network and Distributed System Security Symposium (NDSS) in 2017. Since\nPIVE fails to provide differential privacy guarantees on adaptive protection\nlocation set (PLS) as pointed in our previous work, we develop DPIVE with two\nphases. In Phase I, we determine disjoint sets by partitioning all possible\npositions such that different locations in the same set share the common PLS.\nIn Phase II, we construct a probability distribution matrix by exponential\nmechanism in which the rows corresponding to the same PLS have their own\nsensitivity of utility (diameter of PLS). Moreover, we improve DPIVE with\nrefined location partition and fine-grained personalization, in which each\nlocation has its own privacy level on two privacy control knobs, minimum\ninference error and differential privacy parameter. Experiments with two public\ndatasets demonstrate that our mechanisms have the superior performance\ntypically on skewed locations.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 06:05:10 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 05:12:13 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Zhang", "Shun", ""], ["Duan", "Benfei", ""], ["Chen", "Zhili", ""], ["Ni", "Tianjiao", ""], ["Zhong", "Hong", ""]]}, {"id": "2102.01048", "submitter": "Vasiliki Kalavri", "authors": "John Liagouris, Vasiliki Kalavri, Muhammad Faisal, Mayank Varia", "title": "Secrecy: Secure collaborative analytics on secret-shared data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We study the problem of composing and optimizing relational query plans under\nsecure multi-party computation (MPC). MPC enables mutually distrusting parties\nto jointly compute arbitrary functions over private data, while preserving data\nprivacy from each other and from external entities.\n  In this paper, we propose a relational MPC framework based on replicated\nsecret sharing. We define a set of oblivious operators, explain the secure\nprimitives they rely on, and provide an analysis of their costs in terms of\noperations and inter-party communication. We show how these operators can be\ncomposed to form end-to-end oblivious queries, and we introduce logical and\nphysical optimizations that dramatically reduce the space and communication\nrequirements during query execution, in some cases from quadratic to linear\nwith respect to the cardinality of the input.\n  We provide an efficient implementation of our framework, called Secrecy, and\nevaluate it using real queries from several MPC application areas. Our results\ndemonstrate that the optimizations we propose can result in up to 1000x lower\nexecution times compared to baseline approaches, enabling Secrecy to outperform\nstate-of-the-art frameworks and compute MPC queries on millions of input rows\nwith a single thread per party.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 18:37:20 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Liagouris", "John", ""], ["Kalavri", "Vasiliki", ""], ["Faisal", "Muhammad", ""], ["Varia", "Mayank", ""]]}, {"id": "2102.01244", "submitter": "Xiaoyang Gu", "authors": "Xie Lu, Xiaoguang Wang, Xiaoyang Gu", "title": "New Recruiter and Jobs: The Largest Enterprise Data Migration at\n  LinkedIn", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In August 2019, we introduced to our members and customers the idea of moving\nLinkedIn's two core talent products -- Jobs and Recruiter -- onto a single\nplatform to help talent professionals be even more productive. This single\nplatform is called the New Recruiter & Jobs. A critical and difficult part of\nthis effort is migrating their existing data from the legacy database to the\nnew database and ensure there is no data discrepancy and no down time. In this\narticle, we will discuss the general architecture for a successful data\nmigration and the thought process we followed. Then we expand these ideas to\nour circumstances and explain in more detail about our specific challenges and\nsolutions. In the Ramp Process section, we explain the inherent difficulties in\nsatisfying our success criteria and describe how we overcome these difficulties\nand fulfill the success criteria practically.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 01:01:22 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 04:30:31 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Lu", "Xie", ""], ["Wang", "Xiaoguang", ""], ["Gu", "Xiaoyang", ""]]}, {"id": "2102.01411", "submitter": "Henderik Alex Proper", "authors": "Henderik Alex Proper", "title": "Interactive Query Formulation using Point to Point Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": "Asymetrix Research Report 94-1", "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Effective information disclosure in the context of databases with a large\nconceptual schema is known to be a non-trivial problem. In particular the\nformulation of ad-hoc queries is a major problem in such contexts. Existing\napproaches for tackling this problem include graphical query interfaces, query\nby navigation, and query by construction. In this article we propose the point\nto point query mechanism that can be combined with the existing mechanism into\nan unprecedented computer supported query formulation mechanism. In a point to\npoint query a path through the information structure is build. This path can\nthen be used to formulate more complex queries. A point to point query is\ntypically useful when users know some object types which are relevant for their\ninformation need, but do not (yet) know how they are related in the conceptual\nschema. Part of the point to point query mechanism is therefore the selection\nof the most appropriate path between object types (points) in the conceptual\nschema. This article both discusses some of the pragmatic issues involved in\nthe point to point query mechanism, and the theoretical issues involved in\nfinding the relevant paths between selected object types.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 10:03:10 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 08:07:17 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Proper", "Henderik Alex", ""]]}, {"id": "2102.01832", "submitter": "Ricardo Di Pasquale", "authors": "Ricardo Di Pasquale and Javier Marenco", "title": "Optimization meets Big Data: A survey", "comments": "8 pages, 3 figures, IEEE CEC DSO 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DB cs.SE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper reviews recent advances in big data optimization, providing the\nstate-of-art of this emerging field. The main focus in this review are\noptimization techniques being applied in big data analysis environments.\nInteger linear programming, coordinate descent methods, alternating direction\nmethod of multipliers, simulation optimization and metaheuristics like\nevolutionary and genetic algorithms, particle swarm optimization, differential\nevolution, fireworks, bat, firefly and cuckoo search algorithms implementations\nare reviewed and discussed. The relation between big data optimization and\nsoftware engineering topics like information work-flow styles, software\narchitectures, and software framework is discussed. Comparative analysis in\nplatforms being used in big data optimization environments are highlighted in\norder to bring a state-or-art of possible architectures and topologies.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 01:44:39 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Di Pasquale", "Ricardo", ""], ["Marenco", "Javier", ""]]}, {"id": "2102.02246", "submitter": "J\\'er\\^ome Darmont", "authors": "Ciprian-Octavian Truic\\u{a} and Elena-Simona Apostol and J\\'er\\^ome\n  Darmont and Torben Bach Pedersen", "title": "The Forgotten Document-Oriented Database Management Systems: An Overview\n  and Benchmark of Native XML DODBMSes in Comparison with JSON DODBMSes", "comments": "28 pages, 6 figures, 7 tables", "journal-ref": "Big Data Research, Vol. 25, July 2021", "doi": "10.1016/j.bdr.2021.100205", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the current context of Big Data, a multitude of new NoSQL solutions for\nstoring, managing, and extracting information and patterns from semi-structured\ndata have been proposed and implemented. These solutions were developed to\nrelieve the issue of rigid data structures present in relational databases, by\nintroducing semi-structured and flexible schema design. As current data\ngenerated by different sources and devices, especially from IoT sensors and\nactuators, use either XML or JSON format, depending on the application,\ndatabase technologies that store and query semi-structured data in XML format\nare needed. Thus, Native XML Databases, which were initially designed to\nmanipulate XML data using standardized querying languages, i.e., XQuery and\nXPath, were rebranded as NoSQL Document-Oriented Databases Systems. Currently,\nthe majority of these solutions have been replaced with the more modern JSON\nbased Database Management Systems. However, we believe that XML-based solutions\ncan still deliver performance in executing complex queries on heterogeneous\ncollections. Unfortunately nowadays, research lacks a clear comparison of the\nscalability and performance for database technologies that store and query\ndocuments in XML versus the more modern JSON format. Moreover, to the best of\nour knowledge, there are no Big Data-compliant benchmarks for such database\ntechnologies. In this paper, we present a comparison for selected\nDocument-Oriented Database Systems that either use the XML format to encode\ndocuments, i.e., BaseX, eXist-db, and Sedna, or the JSON format, i.e., MongoDB,\nCouchDB, and Couchbase. To underline the performance differences we also\npropose a benchmark that uses a heterogeneous complex schema on a large DBLP\ncorpus.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 19:27:36 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Truic\u0103", "Ciprian-Octavian", ""], ["Apostol", "Elena-Simona", ""], ["Darmont", "J\u00e9r\u00f4me", ""], ["Pedersen", "Torben Bach", ""]]}, {"id": "2102.02307", "submitter": "Peiran Yao", "authors": "Peiran Yao and Denilson Barbosa", "title": "Typing Errors in Factual Knowledge Graphs: Severity and Possible Ways\n  Out", "comments": "9 pages, 3 figures Camera-ready for WWW2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Factual knowledge graphs (KGs) such as DBpedia and Wikidata have served as\npart of various downstream tasks and are also widely adopted by artificial\nintelligence research communities as benchmark datasets. However, we found\nthese KGs to be surprisingly noisy. In this study, we question the quality of\nthese KGs, where the typing error rate is estimated to be 27% for\ncoarse-grained types on average, and even 73% for certain fine-grained types.\nIn pursuit of solutions, we propose an active typing error detection algorithm\nthat maximizes the utilization of both gold and noisy labels. We also\ncomprehensively discuss and compare unsupervised, semi-supervised, and\nsupervised paradigms to deal with typing errors in factual KGs. The outcomes of\nthis study provide guidelines for researchers to use noisy factual KGs. To help\npractitioners deploy the techniques and conduct further research, we published\nour code and data.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 21:47:37 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Yao", "Peiran", ""], ["Barbosa", "Denilson", ""]]}, {"id": "2102.02440", "submitter": "Florin Rusu", "authors": "Yesdaulet Izenov, Asoke Datta, Florin Rusu, Jun Hyung Shin", "title": "Online Sketch-based Query Optimization", "comments": "Extended version of paper \"COMPASS: Online Sketch-based Query\n  Optimization for In-Memory Databases\" accepted at SIGMOD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cost-based query optimization remains a critical task in relational databases\neven after decades of research and industrial development. Query optimizers\nrely on a large range of statistical synopses -- including attribute-level\nhistograms and table-level samples -- for accurate cardinality estimation. As\nthe complexity of selection predicates and the number of join predicates\nincrease, two problems arise. First, statistics cannot be incrementally\ncomposed to effectively estimate the cost of the sub-plans generated in plan\nenumeration. Second, small errors are propagated exponentially through join\noperators, which can lead to severely sub-optimal plans. In this paper, we\nintroduce COMPASS, a novel query optimization paradigm for in-memory databases\nbased on a single type of statistics -- Fast-AGMS sketches. In COMPASS, query\noptimization and execution are intertwined. Selection predicates and sketch\nupdates are pushed-down and evaluated online during query optimization. This\nallows Fast-AGMS sketches to be computed only over the relevant tuples -- which\nenhances cardinality estimation accuracy. Plan enumeration is performed over\nthe query join graph by incrementally composing attribute-level sketches -- not\nby building a separate sketch for every sub-plan. We prototype COMPASS in MapD\n-- an open-source parallel database -- and perform extensive experiments over\nthe complete JOB benchmark. The results prove that COMPASS generates better\nexecution plans -- both in terms of cardinality and runtime -- compared to four\nother database systems. Overall, COMPASS achieves a speedup ranging from 1.35X\nto 11.28X in cumulative query execution time over the considered competitors.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 06:43:36 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Izenov", "Yesdaulet", ""], ["Datta", "Asoke", ""], ["Rusu", "Florin", ""], ["Shin", "Jun Hyung", ""]]}, {"id": "2102.02705", "submitter": "Rajesh Bordawekar", "authors": "Rajesh Bordawekar and Bulent Abali and Ming-Hung Chen", "title": "EFloat: Entropy-coded Floating Point Format for Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the EFloat floating-point number format with 4 to 6 additional\nbits of precision and a wider exponent range than the existing floating point\n(FP) formats of any width including FP32, BFloat16, IEEE-Half precision,\nDLFloat, TensorFloat, and 8-bit floats. In a large class of deep learning\nmodels we observe that FP exponent values tend to cluster around few unique\nvalues which presents entropy encoding opportunities. The EFloat format encodes\nfrequent exponent values and signs with Huffman codes to minimize the average\nexponent field width. Saved bits then become available to the mantissa\nincreasing the EFloat numeric precision on average by 4 to 6 bits compared to\nother FP formats of equal width. The proposed encoding concept may be\nbeneficial to low-precision formats including 8-bit floats. Training deep\nlearning models with low precision arithmetic is challenging. EFloat, with its\nincreased precision may provide an opportunity for those tasks as well. We\ncurrently use the EFloat format for compressing and saving memory used in large\nNLP deep learning models. A potential hardware implementation for improving\nPCIe and memory bandwidth limitations of AI accelerators is also discussed.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 15:58:01 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Bordawekar", "Rajesh", ""], ["Abali", "Bulent", ""], ["Chen", "Ming-Hung", ""]]}, {"id": "2102.02753", "submitter": "Efthymia Tsamoura", "authors": "Efthymia Tsamoura, David Carral, Enrico Malizia, Jacopo Urbani", "title": "Materializing Knowledge Bases via Trigger Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The chase is a well-established family of algorithms used to materialize\nKnowledge Bases (KBs), like Knowledge Graphs (KGs), to tackle important tasks\nlike query answering under dependencies or data cleaning. A general problem of\nchase algorithms is that they might perform redundant computations. To counter\nthis problem, we introduce the notion of Trigger Graphs (TGs), which guide the\nexecution of the rules avoiding redundant computations. We present the results\nof an extensive theoretical and empirical study that seeks to answer when and\nhow TGs can be computed and what are the benefits of TGs when applied over\nreal-world KBs. Our results include introducing algorithms that compute\n(minimal) TGs. We implemented our approach in a new engine, and our experiments\nshow that it can be significantly more efficient than the chase enabling us to\nmaterialize KBs with 17B facts in less than 40 min on commodity machines.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 17:31:25 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Tsamoura", "Efthymia", ""], ["Carral", "David", ""], ["Malizia", "Enrico", ""], ["Urbani", "Jacopo", ""]]}, {"id": "2102.03251", "submitter": "Jinseok Kim", "authors": "Jinseok Kim", "title": "A fast and integrative algorithm for clustering performance evaluation\n  in author name disambiguation", "comments": "20 pages", "journal-ref": "Scientometrics, 120(2), 661-681 (2019)", "doi": "10.1007/s11192-019-03143-7", "report-no": null, "categories": "cs.DL cs.DB cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Author name disambiguation results are often evaluated by measures such as\nCluster-F, K-metric, Pairwise-F, Splitting & Lumping Error, and B-cubed.\nAlthough these measures have distinctive evaluation schemes, this paper shows\nthat they can be calculated in a single framework by a set of common steps that\ncompare truth and predicted clusters through two hash tables recording\ninformation about name instances with their predicted cluster indices and\nfrequencies of those indices per truth cluster. This integrative calculation\nreduces greatly calculation runtime, which is scalable to a clustering task\ninvolving millions of name instances within a few seconds. During the\nintegration process, B-cubed and K-metric are shown to produce the same\nprecision and recall scores. In this framework, especially, name instance pairs\nfor Pairwise-F are counted using a heuristic, surpassing a state-of-the-art\nalgorithm in speedy calculation. Details of the integrative calculation are\ndescribed with examples and pseudo-code to assist scholars to implement each\nmeasure easily and validate the correctness of implementation. The integrative\ncalculation will help scholars compare similarities and differences of multiple\nmeasures before they select ones that characterize best the clustering\nperformances of their disambiguation methods.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 15:54:49 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Kim", "Jinseok", ""]]}, {"id": "2102.03269", "submitter": "Lars Heling", "authors": "Lars Heling and Maribel Acosta", "title": "A Framework for Federated SPARQL Query Processing over Heterogeneous\n  Linked Data Fragments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Linked Data Fragments (LDFs) refer to Web interfaces that allow for accessing\nand querying Knowledge Graphs on the Web. These interfaces, such as SPARQL\nendpoints or Triple Pattern Fragment servers, differ in the SPARQL expressions\nthey can evaluate and the metadata they provide. Client-side query processing\napproaches have been proposed and are tailored to evaluate queries over\nindividual interfaces. Moreover, federated query processing has focused on\nfederations with a single type of LDF interface, typically SPARQL endpoints. In\nthis work, we address the challenges of SPARQL query processing over\nfederations with heterogeneous LDF interfaces. To this end, we formalize the\nconcept of federations of Linked Data Fragment and propose a framework for\nfederated querying over heterogeneous federations with different LDF\ninterfaces. The framework comprises query decomposition, query planning, and\nphysical operators adapted to the particularities of different LDF interfaces.\nFurther, we propose an approach for each component of our framework and\nevaluate them in an experimental study on the well-known FedBench benchmark.\nThe results show a substantial improvement in performance achieved by devising\nthese interface-aware approaches exploiting the capabilities of heterogeneous\ninterfaces in federations.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 16:20:12 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 08:54:03 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Heling", "Lars", ""], ["Acosta", "Maribel", ""]]}, {"id": "2102.03578", "submitter": "Yau Pun Chen", "authors": "Phoomraphee Luenam, Yau Pun Chen, Raymond Chi-Wing Wong", "title": "Approximating Happiness Maximizing Set Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A Happiness Maximizing Set (HMS) is a useful concept in which a smaller\nsubset of a database is selected while mostly preserving the best scores along\nevery possible utility function. In this paper, we study the Average Happiness\nMaximizing Sets (AHMS) and $k$-Happiness Maximizing Sets ($k$-HMS) problems.\nSpecifically, AHMS maximizes the average of this ratio within a distribution of\nutility functions. Meanwhile, $k$-HMS selects $r$ records from the database\nsuch that the minimum happiness ratio between the $k$-th best score in the\ndatabase and the best score in the selected records for any possible utility\nfunction is maximized. AHMS and $k$-HMS seek the same optimal solutions as the\nmore established Average Regret Minimizing Sets (ARMS) and $k$-Regret\nMinimizing Sets ($k$-RMS) problems, respectively, but the use of the happiness\nmetric allows for the derivation of stronger theoretical results and more\nnatural approximation schemes.\n  We provide approximation algorithms for AHMS with better approximation ratios\nand time complexities than known algorithms for ARMS. Next, we show that the\nproblem of approximating $k$-HMS within any finite factor is NP-Hard when the\ndimensionality of the database is unconstrained and extend the result to an\ninapproximability proof of $k$-RMS. Finally, we provide dataset reduction\nschemes which can be used to reduce the runtime of existing heuristic based\nalgorithms, as well as to derive polynomial-time approximation schemes for both\n$k$-HMS when dimensionality is fixed. We further provide experimental\nvalidation showing that our AHMS algorithm achieves the same happiness as the\nexisting Greedy Shrink FAM algorithm while running faster by over 2 orders of\nmagnitude on even small datasets while our reduction scheme was able to reduce\nruntimes of existing $k$-RMS solvers by up to 92\\%.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 13:00:01 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 14:08:52 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Luenam", "Phoomraphee", ""], ["Chen", "Yau Pun", ""], ["Wong", "Raymond Chi-Wing", ""]]}, {"id": "2102.04141", "submitter": "Angelos-Christos Anadiotis", "authors": "Angelos-Christos Anadiotis, Oana Balalau, Theo Bouganim, Francesco\n  Chimienti, Helena Galhardas, Mhd Yamen Haddad, Stephane Horel, Ioana\n  Manolescu, Youssr Youssef", "title": "Empowering Investigative Journalism with Graph-based Heterogeneous Data\n  Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Investigative Journalism (IJ, in short) is staple of modern, democratic\nsocieties. IJ often necessitates working with large, dynamic sets of\nheterogeneous, schema-less data sources, which can be structured,\nsemi-structured, or textual, limiting the applicability of classical data\nintegration approaches. In prior work, we have developed ConnectionLens, a\nsystem capable of integrating such sources into a single heterogeneous graph,\nleveraging Information Extraction (IE) techniques; users can then query the\ngraph by means of keywords, and explore query results and their neighborhood\nusing an interactive GUI. Our keyword search problem is complicated by the\ngraph heterogeneity, and by the lack of a result score function that would\nallow to prune some of the search space. In this work, we describe an actual IJ\napplication studying conflicts of interest in the biomedical domain, and we\nshow how ConnectionLens supports it. Then, we present novel techniques\naddressing the scalability challenges raised by this application: one allows to\nreduce the significant IE costs while building the graph, while the other is a\nnovel, parallel, in-memory keyword search engine, which achieves orders of\nmagnitude speed-up over our previous engine. Our experimental study on the\nreal-world IJ application data confirms the benefits of our contributions.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 11:39:51 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Anadiotis", "Angelos-Christos", ""], ["Balalau", "Oana", ""], ["Bouganim", "Theo", ""], ["Chimienti", "Francesco", ""], ["Galhardas", "Helena", ""], ["Haddad", "Mhd Yamen", ""], ["Horel", "Stephane", ""], ["Manolescu", "Ioana", ""], ["Youssef", "Youssr", ""]]}, {"id": "2102.05216", "submitter": "Sara Bunian", "authors": "Sara Bunian, Kai Li, Chaima Jemmali, Casper Harteveld, Yun Fu, Magy\n  Seif El-Nasr", "title": "VINS: Visual Search for Mobile User Interface Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Searching for relative mobile user interface (UI) design examples can aid\ninterface designers in gaining inspiration and comparing design alternatives.\nHowever, finding such design examples is challenging, especially as current\nsearch systems rely on only text-based queries and do not consider the UI\nstructure and content into account. This paper introduces VINS, a visual search\nframework, that takes as input a UI image (wireframe, high-fidelity) and\nretrieves visually similar design examples. We first survey interface designers\nto better understand their example finding process. We then develop a\nlarge-scale UI dataset that provides an accurate specification of the\ninterface's view hierarchy (i.e., all the UI components and their specific\nlocation). By utilizing this dataset, we propose an object-detection based\nimage retrieval framework that models the UI context and hierarchical\nstructure. The framework achieves a mean Average Precision of 76.39\\% for the\nUI detection and high performance in querying similar UI designs.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 01:46:33 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Bunian", "Sara", ""], ["Li", "Kai", ""], ["Jemmali", "Chaima", ""], ["Harteveld", "Casper", ""], ["Fu", "Yun", ""], ["El-Nasr", "Magy Seif", ""]]}, {"id": "2102.05238", "submitter": "Shantanu Sharma", "authors": "Peeyush Gupta, Sharad Mehrotra, Shantanu Sharma, Nalini\n  Venkatasubramanian, Guoxi Wang", "title": "Concealer: SGX-based Secure, Volume Hiding, and Verifiable Processing of\n  Spatial Time-Series Datasets", "comments": "A preliminary version of this paper has been accepted in the 24th\n  International Conference on Extending Database Technology (EDBT) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a system, entitled Concealer that allows sharing\ntime-varying spatial data (e.g., as produced by sensors) in encrypted form to\nan untrusted third-party service provider to provide location-based\napplications (involving aggregation queries over selected regions over time\nwindows) to users. Concealer exploits carefully selected encryption techniques\nto use indexes supported by database systems and combines ways to add fake\ntuples in order to realize an efficient system that protects against leakage\nbased on output-size. Thus, the design of Concealer overcomes two limitations\nof existing symmetric searchable encryption (SSE) techniques: (i) it avoids the\nneed of specialized data structures that limit usability/practicality of SSE in\nlarge scale deployments, and (ii) it avoids information leakages based on the\noutput-size, which may leak data distributions. Experimental results validate\nthe efficiency of the proposed algorithms over a spatial time-series dataset\n(collected from a smart space) and TPC-H datasets, each of 136 Million rows,\nthe size of which prior approaches have not scaled to.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 03:28:25 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Gupta", "Peeyush", ""], ["Mehrotra", "Sharad", ""], ["Sharma", "Shantanu", ""], ["Venkatasubramanian", "Nalini", ""], ["Wang", "Guoxi", ""]]}, {"id": "2102.05308", "submitter": "Brandon Lockhart BSc", "authors": "Brandon Lockhart, Jinglin Peng, Weiyuan Wu, Jiannan Wang, Eugene Wu", "title": "Explaining Inference Queries with Bayesian Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obtaining an explanation for an SQL query result can enrich the analysis\nexperience, reveal data errors, and provide deeper insight into the data.\nInference query explanation seeks to explain unexpected aggregate query results\non inference data; such queries are challenging to explain because an\nexplanation may need to be derived from the source, training, or inference data\nin an ML pipeline. In this paper, we model an objective function as a black-box\nfunction and propose BOExplain, a novel framework for explaining inference\nqueries using Bayesian optimization (BO). An explanation is a predicate\ndefining the input tuples that should be removed so that the query result of\ninterest is significantly affected. BO - a technique for finding the global\noptimum of a black-box function - is used to find the best predicate. We\ndevelop two new techniques (individual contribution encoding and warm start) to\nhandle categorical variables. We perform experiments showing that the\npredicates found by BOExplain have a higher degree of explanation compared to\nthose found by the state-of-the-art query explanation engines. We also show\nthat BOExplain is effective at deriving explanations for inference queries from\nsource and training data on a variety of real-world datasets. BOExplain is\nopen-sourced as a Python package at https://github.com/sfu-db/BOExplain.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 08:08:32 GMT"}, {"version": "v2", "created": "Sat, 19 Jun 2021 16:49:38 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Lockhart", "Brandon", ""], ["Peng", "Jinglin", ""], ["Wu", "Weiyuan", ""], ["Wang", "Jiannan", ""], ["Wu", "Eugene", ""]]}, {"id": "2102.05473", "submitter": "Erich Gr\\\"adel", "authors": "Erich Gr\\\"adel and Lovro Mrkonji\\'c", "title": "Elementary equivalence versus isomorphism in semiring semantics", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.DB cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the first-order axiomatisability of finite semiring interpretations\nor, equivalently, the question whether elementary equivalence and isomorphism\ncoincide for valuations of atomic facts over a finite universe into a\ncommutative semiring. Contrary to the classical case of Boolean semantics,\nwhere every finite structure can obviously be axiomatised up to isomorphism by\na first-order sentence, the situation in semiring semantics is rather\ndifferent, and strongly depends on the underlying semiring. We prove that for a\nnumber of important semirings, including min-max semirings, and the semirings\nof positive Boolean expressions, there exist finite semiring interpretations\nthat are elementarily equivalent but not isomorphic. The same is true for the\npolynomial semirings that are universal for the classes of absorptive,\nidempotent, and fully idempotent semirings, respectively. On the other side, we\nprove that for other, practically relevant, semirings such as the Viterby\nsemiring, the tropical semiring, the natural semiring and the universal\npolynomial semiring N[X], all finite semiring interpretations are first-order\naxiomatisable (and thus elementary equivalence implies isomorphism), although\nsome of the axiomatisations that we exhibit use an infinite set of axioms.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 15:00:01 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Gr\u00e4del", "Erich", ""], ["Mrkonji\u0107", "Lovro", ""]]}, {"id": "2102.05716", "submitter": "Aline Bessa", "authors": "Fernando Chirigati, R\\'emi Rampin, A\\'ecio Santos, Aline Bessa, and\n  Juliana Freire", "title": "Auctus: A Dataset Search Engine for Data Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning models are increasingly being adopted in many applications.\nThe quality of these models critically depends on the input data on which they\nare trained, and by augmenting their input data with external data, we have the\nopportunity to create better models. However, the massive number of datasets\navailable on the Web makes it challenging to find data suitable for\naugmentation. In this demo, we present our ongoing efforts to develop a dataset\nsearch engine tailored for data augmentation. Our prototype, named Auctus,\nautomatically discovers datasets on the Web and, different from existing\ndataset search engines, infers consistent metadata for indexing and supports\njoin and union search queries. Auctus is already being used in a real\ndeployment environment to improve the performance of ML models. The\ndemonstration will include various real-world data augmentation examples and\nvisitors will be able to interact with the system.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 19:49:10 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Chirigati", "Fernando", ""], ["Rampin", "R\u00e9mi", ""], ["Santos", "A\u00e9cio", ""], ["Bessa", "Aline", ""], ["Freire", "Juliana", ""]]}, {"id": "2102.06139", "submitter": "Milos Jovanovik", "authors": "Milos Jovanovik, Timo Homburg, Mirko Spasi\\'c", "title": "A GeoSPARQL Compliance Benchmark", "comments": null, "journal-ref": "ISPRS International Journal of Geo-Information. 2021; 10(7):487", "doi": "10.3390/ijgi10070487", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a series of tests that check for the compliance of RDF\ntriplestores with the GeoSPARQL standard. The purpose of the benchmark is to\ntest how many of the requirements outlined in the standard a tested system\nsupports and to push triplestores forward in achieving a full GeoSPARQL\ncompliance. This topic is of concern because the support of GeoSPARQL varies\ngreatly between different triplestore implementations, and such support is of\ngreat importance for the domain of geospatial RDF data. Additionally, we\npresent a comprehensive comparison of triplestores, providing an insight into\ntheir current GeoSPARQL support.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 17:28:52 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Jovanovik", "Milos", ""], ["Homburg", "Timo", ""], ["Spasi\u0107", "Mirko", ""]]}, {"id": "2102.06219", "submitter": "Ralf Ramsauer", "authors": "Wolfgang Mauerer, Ralf Ramsauer, Edson R. F. Lucas, Stefanie\n  Scherzinger", "title": "Silentium! Run-Analyse-Eradicate the Noise out of the DB/OS Stack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When multiple tenants compete for resources, database performance tends to\nsuffer. Yet there are scenarios where guaranteed sub-millisecond latencies are\ncrucial, such as in real-time data processing, IoT devices, or when operating\nin safety-critical environments. In this paper, we study how to make query\nlatencies deterministic in the face of noise (whether caused by other tenants\nor unrelated operating system tasks). We perform controlled experiments with an\nin-memory database engine in a multi-tenant setting, where we successively\neradicate noisy interference from within the system software stack, to the\npoint where the engine runs close to bare-metal on the underlying hardware.\n  We show that we can achieve query latencies comparable to the database engine\nrunning as the sole tenant, but without noticeably impacting the workload of\ncompeting tenants. We discuss these results in the context of ongoing efforts\nto build custom operating systems for database workloads, and point out that\nfor certain use cases, the margin for improvement is rather narrow. In fact,\nfor scenarios like ours, existing operating systems might just be good enough,\nprovided that they are expertly configured. We then critically discuss these\nfindings in the light of a broader family of database systems (e.g., including\ndisk-based), and how to extend the approach of this paper accordingly.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 19:00:40 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 19:54:26 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Mauerer", "Wolfgang", ""], ["Ramsauer", "Ralf", ""], ["Lucas", "Edson R. F.", ""], ["Scherzinger", "Stefanie", ""]]}, {"id": "2102.06557", "submitter": "Steffen Kl\\\"abe", "authors": "Steffen Kl\\\"abe, Kai-Uwe Sattler, Stephan Baumann", "title": "Updatable Materialization of Approximate Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern big data applications integrate data from various sources. As a\nresult, these datasets may not satisfy perfect constraints, leading to sparse\nschema information and non-optimal query performance. The existing approach of\nPatchIndexes enable the definition of approximate constraints and improve query\nperformance by exploiting the materialized constraint information. As real\nworld data warehouse workloads are often not limited to read-only queries, we\nenhance the PatchIndex structure towards an update-conscious design in this\npaper. Therefore, we present a sharded bitmap as the underlying data structure\nwhich offers efficient update operations, and describe approaches to maintain\napproximate constraints under updates, avoiding index recomputations and full\ntable scans. In our evaluation, we prove that PatchIndexes significantly impact\nquery performance while achieving lightweight update support.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 14:43:49 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Kl\u00e4be", "Steffen", ""], ["Sattler", "Kai-Uwe", ""], ["Baumann", "Stephan", ""]]}, {"id": "2102.06563", "submitter": "Matthew Damigos", "authors": "Foto N. Afrati and Matthew Damigos", "title": "Querying collections of tree-structured records in the presence of\n  within-record referential constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a tree-structured data model used in many\ncommercial databases like Dremel, F1, JSON stores. We define identity and\nreferential constraints within each tree-structured record. The query language\nis a variant of SQL and flattening is used as an evaluation mechanism. We\ninvestigate querying in the presence of these constraints, and point out the\nchallenges that arise from taking them into account during query evaluation.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 15:00:51 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 13:40:50 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Afrati", "Foto N.", ""], ["Damigos", "Matthew", ""]]}, {"id": "2102.06789", "submitter": "Songnian Zhang", "authors": "Songnian Zhang, Suprio Ray, Rongxing Lu, Yandong Zheng", "title": "Spatial Interpolation-based Learned Index for Range and kNN Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A corpus of recent work has revealed that the learned index can improve query\nperformance while reducing the storage overhead. It potentially offers an\nopportunity to address the spatial query processing challenges caused by the\nsurge in location-based services. Although several learned indexes have been\nproposed to process spatial data, the main idea behind these approaches is to\nutilize the existing one-dimensional learned models, which requires either\nconverting the spatial data into one-dimensional data or applying the learned\nmodel on individual dimensions separately. As a result, these approaches cannot\nfully utilize or take advantage of the information regarding the spatial\ndistribution of the original spatial data. To this end, in this paper, we\nexploit it by using the spatial (multi-dimensional) interpolation function as\nthe learned model, which can be directly employed on the spatial data.\nSpecifically, we design an efficient SPatial inteRpolation functIon based Grid\nindex (SPRIG) to process the range and kNN queries. Detailed experiments are\nconducted on real-world datasets, and the results indicate that our proposed\nlearned index can significantly improve the performance in comparison with the\ntraditional spatial indexes and a state-of-the-art multi-dimensional learned\nindex.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 21:49:12 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Zhang", "Songnian", ""], ["Ray", "Suprio", ""], ["Lu", "Rongxing", ""], ["Zheng", "Yandong", ""]]}, {"id": "2102.07731", "submitter": "Johannes Sedlmeir", "authors": "Tobias Guggenberger and Johannes Sedlmeir and Gilbert Fridgen and\n  Andr\\'e Luckow", "title": "An In-Depth Investigation of Performance Characteristics of Hyperledger\n  Fabric", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.CR cs.DB cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Private permissioned blockchains, such as Hyperledger Fabric, are widely\ndeployed across the industry to facilitate cross-organizational processes and\npromise improved performance compared to their public counterparts. However,\nthe lack of empirical and theoretical results prevent precise prediction of the\nreal-world performance. We address this gap by conducting an in-depth\nperformance analysis of Hyperledger Fabric. The paper presents a detailed\ncompilation of various performance characteristics using an enhanced version of\nthe Distributed Ledger Performance Scan. Researchers and practitioners alike\ncan use the results as guidelines to better configure and implement their\nblockchains and utilize the DLPS framework to conduct their measurements.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 18:30:43 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Guggenberger", "Tobias", ""], ["Sedlmeir", "Johannes", ""], ["Fridgen", "Gilbert", ""], ["Luckow", "Andr\u00e9", ""]]}, {"id": "2102.07750", "submitter": "Cedric Renggli", "authors": "Cedric Renggli, Luka Rimanic, Nezihe Merve G\\\"urel, Bojan Karla\\v{s},\n  Wentao Wu, Ce Zhang", "title": "A Data Quality-Driven View of MLOps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing machine learning models can be seen as a process similar to the\none established for traditional software development. A key difference between\nthe two lies in the strong dependency between the quality of a machine learning\nmodel and the quality of the data used to train or perform evaluations. In this\nwork, we demonstrate how different aspects of data quality propagate through\nvarious stages of machine learning development. By performing a joint analysis\nof the impact of well-known data quality dimensions and the downstream machine\nlearning process, we show that different components of a typical MLOps pipeline\ncan be efficiently designed, providing both a technical and theoretical\nperspective.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 18:46:08 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Renggli", "Cedric", ""], ["Rimanic", "Luka", ""], ["G\u00fcrel", "Nezihe Merve", ""], ["Karla\u0161", "Bojan", ""], ["Wu", "Wentao", ""], ["Zhang", "Ce", ""]]}, {"id": "2102.08081", "submitter": "Guanli Liu", "authors": "Guanli Liu and Lars Kulik and Xingjun Ma and Jianzhong Qi", "title": "A Lazy Approach for Efficient Index Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learned indices using neural networks have been shown to outperform\ntraditional indices such as B-trees in both query time and memory. However,\nlearning the distribution of a large dataset can be expensive, and updating\nlearned indices is difficult, thus hindering their usage in practical\napplications. In this paper, we address the efficiency and update issues of\nlearned indices through agile model reuse. We pre-train learned indices over a\nset of synthetic (rather than real) datasets and propose a novel approach to\nreuse these pre-trained models for a new (real) dataset. The synthetic datasets\nare created to cover a large range of different distributions. Given a new\ndataset DT, we select the learned index of a synthetic dataset similar to DT,\nto index DT. We show a bound over the indexing error when a pre-trained index\nis selected. We further show how our techniques can handle data updates and\nbound the resultant indexing errors. Experimental results on synthetic and real\ndatasets confirm the effectiveness and efficiency of our proposed lazy (model\nreuse) approach.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 11:07:48 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 03:33:03 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Liu", "Guanli", ""], ["Kulik", "Lars", ""], ["Ma", "Xingjun", ""], ["Qi", "Jianzhong", ""]]}, {"id": "2102.08228", "submitter": "James Cheney", "authors": "James Cheney, Adriane Chapman, Joy Davidson, and Alistair Forbes", "title": "Data provenance, curation and quality in metrology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data metrology -- the assessment of the quality of data -- particularly in\nscientific and industrial settings, has emerged as an important requirement for\nthe UK National Physical Laboratory (NPL) and other national metrology\ninstitutes. Data provenance and data curation are key components for emerging\nunderstanding of data metrology. However, to date provenance research has had\nlimited visibility to or uptake in metrology. In this work, we summarize a\nscoping study carried out with NPL staff and industrial participants to\nunderstand their current and future needs for provenance, curation and data\nquality. We then survey provenance technology and standards that are relevant\nto metrology. We analyse the gaps between requirements and the current state of\nthe art.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 15:44:27 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Cheney", "James", ""], ["Chapman", "Adriane", ""], ["Davidson", "Joy", ""], ["Forbes", "Alistair", ""]]}, {"id": "2102.08466", "submitter": "Dongjin Lee", "authors": "Dongjin Lee and Kijung Shin", "title": "Robust Factorization of Real-world Tensor Streams with Patterns, Missing\n  Values, and Outliers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider multiple seasonal time series being collected in real-time, in the\nform of a tensor stream. Real-world tensor streams often include missing\nentries (e.g., due to network disconnection) and at the same time unexpected\noutliers (e.g., due to system errors). Given such a real-world tensor stream,\nhow can we estimate missing entries and predict future evolution accurately in\nreal-time? In this work, we answer this question by introducing SOFIA, a robust\nfactorization method for real-world tensor streams. In a nutshell, SOFIA\nsmoothly and tightly integrates tensor factorization, outlier removal, and\ntemporal-pattern detection, which naturally reinforce each other. Moreover,\nSOFIA integrates them in linear time, in an online manner, despite the presence\nof missing entries. We experimentally show that SOFIA is (a) robust and\naccurate: yielding up to 76% lower imputation error and 71% lower forecasting\nerror; (b) fast: up to 935X faster than the second-most accurate competitor;\nand (c) scalable: scaling linearly with the number of new entries per time\nstep.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 22:01:25 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Lee", "Dongjin", ""], ["Shin", "Kijung", ""]]}, {"id": "2102.08481", "submitter": "Jiashen Cao", "authors": "Jiashen Cao and Ramyad Hadidi and Joy Arulraj and Hyesoon Kim", "title": "THIA: Accelerating Video Analytics using Early Inference and\n  Fine-Grained Query Planning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To efficiently process visual data at scale, researchers have proposed two\ntechniques for lowering the computational overhead associated with the\nunderlying deep learning models. The first approach consists of leveraging a\nspecialized, lightweight model to directly answer the query. The second\napproach focuses on filtering irrelevant frames using a lightweight model and\nprocessing the filtered frames using a heavyweight model. These techniques\nsuffer from two limitations. With the first approach, the specialized model is\nunable to provide accurate results for hard-to-detect events. With the second\napproach, the system is unable to accelerate queries focusing on frequently\noccurring events as the filter is unable to eliminate a significant fraction of\nframes in the video.\n  In this paper, we present THIA, a video analytics system for tackling these\nlimitations. The design of THIA is centered around three techniques. First,\ninstead of using a cascade of models, it uses a single object detection model\nwith multiple exit points for short-circuiting the inference. This early\ninference technique allows it to support a range of throughput-accuracy\ntradeoffs. Second, it adopts a fine-grained approach to planning and processes\ndifferent chunks of the video using different exit points to meet the user's\nrequirements. Lastly, it uses a lightweight technique for directly estimating\nthe exit point for a chunk to lower the optimization time. We empirically show\nthat these techniques enable THIA to outperform two state-of-the-art video\nanalytics systems by up to 6.5X, while providing accurate results even on\nqueries focusing on hard-to-detect events.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 22:40:08 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Cao", "Jiashen", ""], ["Hadidi", "Ramyad", ""], ["Arulraj", "Joy", ""], ["Kim", "Hyesoon", ""]]}, {"id": "2102.08870", "submitter": "Panagiotis Tampakis", "authors": "Andreas Tritsarolis, Eva Chondrodima, Panagiotis Tampakis and Aggelos\n  Pikrakis", "title": "Online Co-movement Pattern Prediction in Mobility Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive analytics over mobility data are of great importance since they\ncan assist an analyst to predict events, such as collisions, encounters,\ntraffic jams, etc. A typical example of such analytics is future location\nprediction, where the goal is to predict the future location of a moving\nobject,given a look-ahead time. What is even more challenging is being able to\naccurately predict collective behavioural patterns of movement, such as\nco-movement patterns. In this paper, we provide an accurate solution to the\nproblem of Online Prediction of Co-movement Patterns. In more detail, we split\nthe original problem into two sub-problems, namely Future Location Prediction\nand Evolving Cluster Detection. Furthermore, in order to be able to calculate\nthe accuracy of our solution, we propose a co-movement pattern similarity\nmeasure, which facilitates us to match the predicted clusters with the actual\nones. Finally, the accuracy of our solution is demonstrated experimentally over\na real dataset from the maritime domain.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 17:01:32 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Tritsarolis", "Andreas", ""], ["Chondrodima", "Eva", ""], ["Tampakis", "Panagiotis", ""], ["Pikrakis", "Aggelos", ""]]}, {"id": "2102.08942", "submitter": "Omid Jafari", "authors": "Omid Jafari, Preeti Maurya, Parth Nagarkar, Khandker Mushfiqul Islam,\n  and Chidambaram Crushev", "title": "A Survey on Locality Sensitive Hashing Algorithms and their Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding nearest neighbors in high-dimensional spaces is a fundamental\noperation in many diverse application domains. Locality Sensitive Hashing (LSH)\nis one of the most popular techniques for finding approximate nearest neighbor\nsearches in high-dimensional spaces. The main benefits of LSH are its\nsub-linear query performance and theoretical guarantees on the query accuracy.\nIn this survey paper, we provide a review of state-of-the-art LSH and\nDistributed LSH techniques. Most importantly, unlike any other prior survey, we\npresent how Locality Sensitive Hashing is utilized in different application\ndomains.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 18:56:03 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Jafari", "Omid", ""], ["Maurya", "Preeti", ""], ["Nagarkar", "Parth", ""], ["Islam", "Khandker Mushfiqul", ""], ["Crushev", "Chidambaram", ""]]}, {"id": "2102.09295", "submitter": "Suvam Kumar Das", "authors": "Alex Watson, Suvam Kumar Das and Suprio Ray", "title": "A Unified System for Data Analytics and In Situ Query Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In today's world data is being generated at a high rate due to which it has\nbecome inevitable to analyze and quickly get results from this data. Most of\nthe relational databases primarily support SQL querying with a limited support\nfor complex data analysis. Due to this reason, data scientists have no other\noption, but to use a different system for complex data analysis. Due to this,\ndata science frameworks are in huge demand. But to use such a framework, all\nthe data needs to be loaded into it. This requires significant data movement\nacross multiple systems, which can be expensive.\n  We believe that it has become the need of the hour to come up with a single\nsystem which can perform both data analysis tasks and SQL querying. This will\nsave the data scientists from the expensive data transfer operation across\nsystems. In our work, we present DaskDB, a system built over the Python's Dask\nframework, which is a scalable data science system having support for both data\nanalytics and in situ SQL query processing over heterogeneous data sources.\nDaskDB supports invoking any Python APIs as User-Defined Functions (UDF) over\nSQL queries. So, it can be easily integrated with most existing Python data\nscience applications, without modifying the existing code. Since joining two\nrelations is a very vital but expensive operation, so a novel distributed\nlearned index is also introduced to improve the join performance. Our\nexperimental evaluation demonstrates that DaskDB significantly outperforms\nexisting systems.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 12:16:20 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 14:55:35 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Watson", "Alex", ""], ["Das", "Suvam Kumar", ""], ["Ray", "Suprio", ""]]}, {"id": "2102.10133", "submitter": "Leny Vinceslas", "authors": "Leny Vinceslas, Hirsh Pithadia, Safak Dogan, Srikumar Sundareshwar,\n  Ahmet M. Kondoz", "title": "Abstracting data in distributed ledger systems for higher level\n  analytics and visualizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By design, distributed ledger technologies persist low-level data which makes\nconducting complex business analysis of the recorded operations challenging.\nExisting blockchain visualization and analytics tools such as block explorers\ntend to rely on this low-level data and complex interfacing to provide enriched\nlevel of analytics. The ability to derive richer analytics could be improved\nthrough the availability of a higher level abstraction of the data. This\narticle proposes an abstraction layer architecture that enables the design of\nhigh-level analytics of distributed ledger systems and the decentralized\napplications that run on top. Based on the analysis of existing initiatives and\nidentification of the relevant user requirements, this work aims to establish\nkey insights and specifications to improve the auditability and intuitiveness\nof distributed ledger systems by leveraging the development of future user\ninterfaces. To illustrate the benefits offered by the proposed abstraction\nlayer architecture, a regulated sector use case is explored.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 19:34:12 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Vinceslas", "Leny", ""], ["Pithadia", "Hirsh", ""], ["Dogan", "Safak", ""], ["Sundareshwar", "Srikumar", ""], ["Kondoz", "Ahmet M.", ""]]}, {"id": "2102.10185", "submitter": "Zhihan Guo", "authors": "Zhihan Guo, Xinyu Zeng, Ziwei Ren, Xiangyao Yu", "title": "Cornus: One-Phase Commit for Cloud Databases with Storage Disaggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Two-phase commit (2PC) has been widely used in distributed databases to\nensure atomicity for distributed transactions. However, 2PC suffers from two\nlimitations. First, 2PC incurs long latency as it requires two logging\noperations on the critical path. Second, when a coordinator fails, a\nparticipant may be blocked waiting for the coordinator's decision, leading to\nindefinitely long latency and low throughput. We make a key observation that\nmodern cloud databases feature a storage disaggregation architecture, which\nallows a transaction's final decision to not rely on the central coordinator.\nWe propose Cornus, a one-phase commit (1PC) protocol specifically designed for\nthis architecture. Cornus can solve the two problems mentioned above by\nleveraging the fact that all compute nodes are able to access and modify the\nlog data on any storage node. We present Cornus in detail, formally prove its\ncorrectness, develop certain optimization techniques, and evaluate against 2PC\non YCSB and TPC-C workloads. The results show that Cornus can achieve 1.5x\nspeedup in latency.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 22:28:02 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Guo", "Zhihan", ""], ["Zeng", "Xinyu", ""], ["Ren", "Ziwei", ""], ["Yu", "Xiangyao", ""]]}, {"id": "2102.10588", "submitter": "Damjan Gjurovski", "authors": "Angjela Davitkova, Damjan Gjurovski, Sebastian Michel", "title": "LMKG: Learned Models for Cardinality Estimation in Knowledge Graphs", "comments": "12 pages, 11 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate cardinality estimates are a key ingredient to achieve optimal query\nplans. For RDF engines, specifically under common knowledge graph processing\nworkloads, the lack of schema, correlated predicates, and various types of\nqueries involving multiple joins, render cardinality estimation a particularly\nchallenging task. In this paper, we develop a framework, termed LMKG, that\nadopts deep learning approaches for effectively estimating the cardinality of\nqueries over RDF graphs. We employ both supervised (i.e., deep neural networks)\nand unsupervised (i.e., autoregressive models) approaches that adapt to the\nsubgraph patterns and produce more accurate cardinality estimates. To feed the\nunderlying data to the models, we put forward a novel encoding that represents\nthe queries as subgraph patterns. Through extensive experiments on both\nreal-world and synthetic datasets, we evaluate our models and show that they\noverall outperform the state-of-the-art approaches in terms of accuracy and\nexecution time.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 11:45:22 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Davitkova", "Angjela", ""], ["Gjurovski", "Damjan", ""], ["Michel", "Sebastian", ""]]}, {"id": "2102.10768", "submitter": "Xin Jin", "authors": "Xin Jin, Zhengyi Yang, Xuemin Lin, Shiyu Yang, Lu Qin, You Peng", "title": "FAST: FPGA-based Subgraph Matching on Massive Graphs", "comments": "14 pages, ICDE 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subgraph matching is a basic operation widely used in many applications.\nHowever, due to its NP-hardness and the explosive growth of graph data, it is\nchallenging to compute subgraph matching, especially in large graphs. In this\npaper, we aim at scaling up subgraph matching on a single machine using FPGAs.\nSpecifically, we propose a CPU-FPGA co-designed framework. On the CPU side, we\nfirst develop a novel auxiliary data structure called candidate search tree\n(CST) which serves as a complete search space of subgraph matching. CST can be\npartitioned and fully loaded into FPGAs' on-chip memory. Then, a workload\nestimation technique is proposed to balance the load between the CPU and FPGA.\nOn the FPGA side, we design and implement the first FPGA-based subgraph\nmatching algorithm, called FAST. To take full advantage of the pipeline\nmechanism on FPGAs, task parallelism optimization and task generator separation\nstrategy are proposed for FAST, achieving massive parallelism. Moreover, we\ncarefully develop a BRAM-only matching process to fully utilize FPGA's on-chip\nmemory, which avoids the expensive intermediate data transfer between FPGA's\nBRAM and DRAM. Comprehensive experiments show that FAST achieves up to 462.0x\nand 150.0x speedup compared with the state-of-the-art algorithm DAF and CECI,\nrespectively. In addition, FAST is the only algorithm that can handle the\nbillion-scale graph using one machine in our experiments.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 04:35:41 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 03:20:04 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Jin", "Xin", ""], ["Yang", "Zhengyi", ""], ["Lin", "Xuemin", ""], ["Yang", "Shiyu", ""], ["Qin", "Lu", ""], ["Peng", "You", ""]]}, {"id": "2102.10802", "submitter": "Praneeth Vepakomma", "authors": "Praneeth Vepakomma, Julia Balla, Ramesh Raskar", "title": "Differentially Private Supervised Manifold Learning with Applications\n  like Private Image Retrieval", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential Privacy offers strong guarantees such as immutable privacy under\npost processing. Thus it is often looked to as a solution to learning on\nscattered and isolated data. This work focuses on supervised manifold learning,\na paradigm that can generate fine-tuned manifolds for a target use case. Our\ncontributions are two fold. 1) We present a novel differentially private method\n\\textit{PrivateMail} for supervised manifold learning, the first of its kind to\nour knowledge. 2) We provide a novel private geometric embedding scheme for our\nexperimental use case. We experiment on private \"content based image retrieval\"\n- embedding and querying the nearest neighbors of images in a private manner -\nand show extensive privacy-utility tradeoff results, as well as the\ncomputational efficiency and practicality of our methods.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 06:58:46 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Vepakomma", "Praneeth", ""], ["Balla", "Julia", ""], ["Raskar", "Ramesh", ""]]}, {"id": "2102.11389", "submitter": "Michael Cochez", "authors": "Ruud van Bakel, Teodor Aleksiev, Daniel Daza, Dimitrios Alivanistos,\n  Michael Cochez", "title": "Approximate Knowledge Graph Query Answering: From Ranking to Binary\n  Classification", "comments": "To be published in Lecture Notes in Artificial Intelligence\n  (Springer)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large, heterogeneous datasets are characterized by missing or even erroneous\ninformation. This is more evident when they are the product of community effort\nor automatic fact extraction methods from external sources, such as text. A\nspecial case of the aforementioned phenomenon can be seen in knowledge graphs,\nwhere this mostly appears in the form of missing or incorrect edges and nodes.\n  Structured querying on such incomplete graphs will result in incomplete sets\nof answers, even if the correct entities exist in the graph, since one or more\nedges needed to match the pattern are missing. To overcome this problem,\nseveral algorithms for approximate structured query answering have been\nproposed. Inspired by modern Information Retrieval metrics, these algorithms\nproduce a ranking of all entities in the graph, and their performance is\nfurther evaluated based on how high in this ranking the correct answers appear.\n  In this work we take a critical look at this way of evaluation. We argue that\nperforming a ranking-based evaluation is not sufficient to assess methods for\ncomplex query answering. To solve this, we introduce Message Passing Query\nBoxes (MPQB), which takes binary classification metrics back into use and shows\nthe effect this has on the recently proposed query embedding method MPQE.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 22:28:08 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["van Bakel", "Ruud", ""], ["Aleksiev", "Teodor", ""], ["Daza", "Daniel", ""], ["Alivanistos", "Dimitrios", ""], ["Cochez", "Michael", ""]]}, {"id": "2102.11517", "submitter": "Taehyung Kwon", "authors": "Taehyung Kwon, Inkyu Park, Dongjin Lee, and Kijung Shin", "title": "SliceNStitch: Continuous CP Decomposition of Sparse Tensor Streams", "comments": "Updated Figures 4, 5, 6, 7, and 8 after fixing a bug in preprocessing\n  the Divvy dataset. To appear at the 37th IEEE International Conference on\n  Data Engineering (ICDE '21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider traffic data (i.e., triplets in the form of\nsource-destination-timestamp) that grow over time. Tensors (i.e.,\nmulti-dimensional arrays) with a time mode are widely used for modeling and\nanalyzing such multi-aspect data streams. In such tensors, however, new entries\nare added only once per period, which is often an hour, a day, or even a year.\nThis discreteness of tensors has limited their usage for real-time\napplications, where new data should be analyzed instantly as it arrives. How\ncan we analyze time-evolving multi-aspect sparse data 'continuously' using\ntensors where time is'discrete'? We propose SLICENSTITCH for continuous\nCANDECOMP/PARAFAC (CP) decomposition, which has numerous time-critical\napplications, including anomaly detection, recommender systems, and stock\nmarket prediction. SLICENSTITCH changes the starting point of each period\nadaptively, based on the current time, and updates factor matrices (i.e.,\noutputs of CP decomposition) instantly as new data arrives. We show,\ntheoretically and experimentally, that SLICENSTITCH is (1) 'Any time': updating\nfactor matrices immediately without having to wait until the current time\nperiod ends, (2) Fast: with constant-time updates up to 464x faster than online\nmethods, and (3) Accurate: with fitness comparable (specifically, 72 ~ 100%) to\noffline methods.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 06:28:42 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 06:46:44 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Kwon", "Taehyung", ""], ["Park", "Inkyu", ""], ["Lee", "Dongjin", ""], ["Shin", "Kijung", ""]]}, {"id": "2102.11527", "submitter": "Fernando Gualo MSc", "authors": "Fernando Gualo, Mois\\'es Rodr\\'iguez, Javier Verdugo, Ismael\n  Caballero, Mario Piattini", "title": "Data Quality Certification using ISO/IEC 25012: Industrial Experiences", "comments": "35 pages, 10 tables, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The most successful organizations in the world are data-driven businesses.\nData is at the core of the business of many organizations as one of the most\nimportant assets, since the decisions they make cannot be better than the data\non which they are based. Due to this reason, organizations need to be able to\ntrust their data. One important activity that helps to achieve data reliability\nis the evaluation and certification of the quality level of organizational data\nrepositories. This paper describes the results of the application of a data\nquality evaluation and certification process to the repositories of three\nEuropean organizations belonging to different sectors. We present findings from\nthe point of view of both the data quality evaluation team and the\norganizations that underwent the evaluation process. In this respect, several\nbenefits have been explicitly recognised by the involved organizations after\nachieving the data quality certification for their repositories (e.g.,\nlong-term organizational sustainability, better internal knowledge of data, and\na more efficient management of data quality). As a result of this experience,\nwe have also identified a set of best practices aimed to enhance the data\nquality evaluation process.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 07:28:25 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Gualo", "Fernando", ""], ["Rodr\u00edguez", "Mois\u00e9s", ""], ["Verdugo", "Javier", ""], ["Caballero", "Ismael", ""], ["Piattini", "Mario", ""]]}, {"id": "2102.11793", "submitter": "Cong Ding", "authors": "Cong Ding, Dixin Tang, Xi Liang, Aaron J. Elmore, Sanjay Krishnan", "title": "CIAO: An Optimization Framework for Client-Assisted Data Loading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data loading has been one of the most common performance bottlenecks for many\nbig data applications, especially when they are running on inefficient\nhuman-readable formats, such as JSON or CSV. Parsing, validating, integrity\nchecking and data structure maintenance are all computationally expensive steps\nin loading these formats. Regardless of these costs, many records may be\nfiltered later during query evaluation due to highly selective predicates --\nresulting in wasted computation. Meanwhile, the computing power of client ends\nis typically not exploited. Here, we explore investing limited cycles of\nclients on prefiltering to accelerate data loading and enable data skipping for\nquery execution. In this paper, we present CIAO, a tunable system to enable\nclient cooperation with the server to enable efficient partial loading and data\nskipping for a given workload. We proposed an efficient algorithm that would\nselect a near-optimal predicate set to push down within a given budget.\nMoreover, CIAO will address the trade-off between client cost and server\nsavings by setting different budgets for different clients. We implemented CIAO\nand evaluated its performance on three real-world datasets. Our experimental\nresults show that the system substantially accelerates data loading by up to\n21x and query execution by up to 23x and improves end-to-end performance by up\nto 19x within a budget of 1.0 microseconds latency per record on clients.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 16:57:31 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Ding", "Cong", ""], ["Tang", "Dixin", ""], ["Liang", "Xi", ""], ["Elmore", "Aaron J.", ""], ["Krishnan", "Sanjay", ""]]}, {"id": "2102.11796", "submitter": "Su Feng", "authors": "Su Feng, Aaron Huber, Boris Glavic, Oliver Kennedy", "title": "Efficient Uncertainty Tracking for Complex Queries with Attribute-level\n  Bounds (extended version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Certain answers are a principled method for coping with the uncertainty that\narises in many practical data management tasks. Unfortunately, this method is\nexpensive and may exclude useful (if uncertain) answers. Prior work introduced\nUncertainty Annotated Databases (UA-DBs), which combine an under- and\nover-approximation of certain answers. UA-DBs combine the reliability of\ncertain answers based on incomplete K-relations with the performance of\nclassical deterministic database systems. However, UA-DBs only support a\nlimited class of queries and do not support attribute-level uncertainty which\ncan lead to inaccurate under-approximations of certain answers. In this paper,\nwe introduce attribute-annotated uncertain databases (AU-DBs) which extend the\nUA-DB model with attribute-level annotations that record bounds on the values\nof an attribute across all possible worlds. This enables more precise\napproximations of incomplete databases. Furthermore, we extend UA-DBs to encode\nan compact over-approximation of possible answers which is necessary to support\nnon-monotone queries including aggregation and set difference. We prove that\nquery processing over AU-DBs preserves the bounds of certain and possible\nanswers and investigate algorithms for compacting intermediate results to\nretain efficiency. Through an compact encoding of possible answers, our\napproach also provides a solid foundation for handling missing data. Using\noptimizations that trade accuracy for performance, our approach scales to\ncomplex queries and large datasets, and produces accurate results. Furthermore,\nit significantly outperforms alternative methods for uncertain data management.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 17:02:15 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Feng", "Su", ""], ["Huber", "Aaron", ""], ["Glavic", "Boris", ""], ["Kennedy", "Oliver", ""]]}, {"id": "2102.12072", "submitter": "Junyang Gao", "authors": "Junyang Gao, Stavros Sintos, Pankaj K. Agarwal, Jun Yang", "title": "Durable Top-K Instant-Stamped Temporal Records with User-Specified\n  Scoring Functions", "comments": "in ICDE 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A way of finding interesting or exceptional records from instant-stamped\ntemporal data is to consider their \"durability,\" or, intuitively speaking, how\nwell they compare with other records that arrived earlier or later, and how\nlong they retain their supremacy. For example, people are naturally fascinated\nby claims with long durability, such as: \"On January 22, 2006, Kobe Bryant\ndropped 81 points against Toronto Raptors. Since then, this scoring record has\nyet to be broken.\" In general, given a sequence of instant-stamped records,\nsuppose that we can rank them by a user-specified scoring function $f$, which\nmay consider multiple attributes of a record to compute a single score for\nranking. This paper studies \"durable top-$k$ queries\", which find records whose\nscores were within top-$k$ among those records within a \"durability window\" of\ngiven length, e.g., a 10-year window starting/ending at the timestamp of the\nrecord. The parameter $k$, the length of the durability window, and parameters\nof the scoring function (which capture user preference) can all be given at the\nquery time. We illustrate why this problem formulation yields more meaningful\nanswers in some practical situations than other similar types of queries\nconsidered previously. We propose new algorithms for solving this problem, and\nprovide a comprehensive theoretical analysis on the complexities of the problem\nitself and of our algorithms. Our algorithms vastly outperform various\nbaselines (by up to two orders of magnitude on real and synthetic datasets).\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 05:06:15 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 22:46:11 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Gao", "Junyang", ""], ["Sintos", "Stavros", ""], ["Agarwal", "Pankaj K.", ""], ["Yang", "Jun", ""]]}, {"id": "2102.12531", "submitter": "Ran Ben Basat", "authors": "Ran Ben Basat, Gil Einziger, Michael Mitzenmacher, Shay Vargaftik", "title": "SALSA: Self-Adjusting Lean Streaming Analytics", "comments": "An extended version of the conference paper that will appear in IEEE\n  ICDE 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counters are the fundamental building block of many data sketching schemes,\nwhich hash items to a small number of counters and account for collisions to\nprovide good approximations for frequencies and other measures. Most existing\nmethods rely on fixed-size counters, which may be wasteful in terms of space,\nas counters must be large enough to eliminate any risk of overflow. Instead,\nsome solutions use small, fixed-size counters that may overflow into secondary\nstructures.\n  This paper takes a different approach. We propose a simple and general method\ncalled SALSA for dynamic re-sizing of counters and show its effectiveness.\nSALSA starts with small counters, and overflowing counters simply merge with\ntheir neighbors. SALSA can thereby allow more counters for a given space,\nexpanding them as necessary to represent large numbers. Our evaluation\ndemonstrates that, at the cost of a small overhead for its merging logic, SALSA\nsignificantly improves the accuracy of popular schemes (such as Count-Min\nSketch and Count Sketch) over a variety of tasks. Our code is released as\nopen-source [1].\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 19:51:24 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Basat", "Ran Ben", ""], ["Einziger", "Gil", ""], ["Mitzenmacher", "Michael", ""], ["Vargaftik", "Shay", ""]]}, {"id": "2102.12922", "submitter": "Asaf Cidon", "authors": "Yu Jian Wu, Hongyi Wang, Yuhong Zhong, Asaf Cidon, Ryan Stutsman, Amy\n  Tai, Junfeng Yang", "title": "BPF for storage: an exokernel-inspired approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The overhead of the kernel storage path accounts for half of the access\nlatency for new NVMe storage devices. We explore using BPF to reduce this\noverhead, by injecting user-defined functions deep in the kernel's I/O\nprocessing stack. When issuing a series of dependent I/O requests, this\napproach can increase IOPS by over 2.5$\\times$ and cut latency by half, by\nbypassing kernel layers and avoiding user-kernel boundary crossings. However,\nwe must avoid losing important properties when bypassing the file system and\nblock layer such as the safety guarantees of the file system and translation\nbetween physical blocks addresses and file offsets. We sketch potential\nsolutions to these problems, inspired by exokernel file systems from the late\n90s, whose time, we believe, has finally come!\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 15:22:38 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Wu", "Yu Jian", ""], ["Wang", "Hongyi", ""], ["Zhong", "Yuhong", ""], ["Cidon", "Asaf", ""], ["Stutsman", "Ryan", ""], ["Tai", "Amy", ""], ["Yang", "Junfeng", ""]]}, {"id": "2102.12970", "submitter": "Mouna Labiadh", "authors": "Mouna Labiadh (SOC, LIRIS, CETHIL), Christian Obrecht (CETHIL),\n  Catarina Ferreira da Silva (ISCTE-IUL), Parisa Ghodous (SOC, LIRIS)", "title": "A microservice-based framework for exploring data selection in\n  cross-building knowledge transfer", "comments": "Service Oriented Computing and Applications, Springer, 2020", "journal-ref": null, "doi": "10.1007/s11761-020-00306-w", "report-no": null, "categories": "cs.LG cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised deep learning has achieved remarkable success in various\napplications. Successful machine learning application however depends on the\navailability of sufficiently large amount of data. In the absence of data from\nthe target domain, representative data collection from multiple sources is\noften needed. However, a model trained on existing multi-source data might\ngeneralize poorly on the unseen target domain. This problem is referred to as\ndomain shift. In this paper, we explore the suitability of multi-source\ntraining data selection to tackle the domain shift challenge in the context of\ndomain generalization. We also propose a microservice-oriented methodology for\nsupporting this solution. We perform our experimental study on the use case of\nbuilding energy consumption prediction. Experimental results suggest that\nminimal building description is capable of improving cross-building\ngeneralization performances when used to select energy consumption data.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 10:15:06 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Labiadh", "Mouna", "", "SOC, LIRIS, CETHIL"], ["Obrecht", "Christian", "", "CETHIL"], ["da Silva", "Catarina Ferreira", "", "ISCTE-IUL"], ["Ghodous", "Parisa", "", "SOC, LIRIS"]]}, {"id": "2102.13027", "submitter": "Waqas Ali Mr", "authors": "Waqas Ali, Muhammad Saleem, Bin Yao, Aidan Hogan, Axel-Cyrille Ngonga\n  Ngomo", "title": "A Survey of RDF Stores & SPARQL Engines for Querying Knowledge Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent years have seen the growing adoption of non-relational data models for\nrepresenting diverse, incomplete data. Among these, the RDF graph-based data\nmodel has seen ever-broadening adoption, particularly on the Web. This adoption\nhas prompted the standardization of the SPARQL query language for RDF, as well\nas the development of a variety of local and distributed engines for processing\nqueries over RDF graphs. These engines implement a diverse range of specialized\ntechniques for storage, indexing, and query processing. A number of benchmarks,\nbased on both synthetic and real-world data, have also emerged to allow for\ncontrasting the performance of different query engines, often at large scale.\nThis survey paper draws together these developments, providing a comprehensive\nreview of the techniques, engines and benchmarks for querying RDF knowledge\ngraphs.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 17:35:12 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 21:22:19 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Ali", "Waqas", ""], ["Saleem", "Muhammad", ""], ["Yao", "Bin", ""], ["Hogan", "Aidan", ""], ["Ngomo", "Axel-Cyrille Ngonga", ""]]}, {"id": "2102.13370", "submitter": "Hao Zhang", "authors": "Hao Zhang, Miao Qiao, Jeffrey Xu Yu, Hong Cheng", "title": "Fast Distributed Complex Join Processing", "comments": "Long Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the problem of co-optimize communication,\npre-computing, and computation cost in one-round multi-way join evaluation. We\npropose a multi-way join approach ADJ (Adaptive Distributed Join) for complex\njoin which finds one optimal query plan to process by exploring cost-effective\npartial results in terms of the trade-off between pre-computing, communication,\nand computation.We analyze the input relations for a given join query and find\none optimal over a set of query plans in some specific form, with high-quality\ncost estimation by sampling. Our extensive experiments confirm that ADJ\noutperforms the existing multi-way join methods by up to orders of magnitude.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 09:41:04 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Zhang", "Hao", ""], ["Qiao", "Miao", ""], ["Yu", "Jeffrey Xu", ""], ["Cheng", "Hong", ""]]}]