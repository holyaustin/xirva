[{"id": "1012.0178", "submitter": "Stefano Balietti", "authors": "Dirk Helbing and Stefano Balietti", "title": "From Social Data Mining to Forecasting Socio-Economic Crisis", "comments": "65 pages, 1 figure, Visioneer White Paper, see\n  http://www.visioneer.ethz.ch", "journal-ref": "The European Physical Journal - Special Topics Volume 195, Number\n  1, 3-68, DOI: 10.1140/epjst/e2011-01401-8", "doi": "10.1140/epjst/e2011-01401-8", "report-no": null, "categories": "cs.CY cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Socio-economic data mining has a great potential in terms of gaining a better\nunderstanding of problems that our economy and society are facing, such as\nfinancial instability, shortages of resources, or conflicts. Without\nlarge-scale data mining, progress in these areas seems hard or impossible.\nTherefore, a suitable, distributed data mining infrastructure and research\ncenters should be built in Europe. It also appears appropriate to build a\nnetwork of Crisis Observatories. They can be imagined as laboratories devoted\nto the gathering and processing of enormous volumes of data on both natural\nsystems such as the Earth and its ecosystem, as well as on human\ntechno-socio-economic systems, so as to gain early warnings of impending\nevents. Reality mining provides the chance to adapt more quickly and more\naccurately to changing situations. Further opportunities arise by individually\ncustomized services, which however should be provided in a privacy-respecting\nway. This requires the development of novel ICT (such as a self- organizing\nWeb), but most likely new legal regulations and suitable institutions as well.\nAs long as such regulations are lacking on a world-wide scale, it is in the\npublic interest that scientists explore what can be done with the huge data\navailable. Big data do have the potential to change or even threaten democratic\nsocieties. The same applies to sudden and large-scale failures of ICT systems.\nTherefore, dealing with data must be done with a large degree of responsibility\nand care. Self-interests of individuals, companies or institutions have limits,\nwhere the public interest is affected, and public interest is not a sufficient\njustification to violate human rights of individuals. Privacy is a high good,\nas confidentiality is, and damaging it would have serious side effects for\nsociety.\n", "versions": [{"version": "v1", "created": "Wed, 1 Dec 2010 12:48:38 GMT"}, {"version": "v2", "created": "Thu, 2 Dec 2010 12:32:34 GMT"}, {"version": "v3", "created": "Mon, 24 Jan 2011 17:06:59 GMT"}, {"version": "v4", "created": "Thu, 24 Feb 2011 16:03:20 GMT"}, {"version": "v5", "created": "Tue, 26 Jul 2011 09:03:41 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Helbing", "Dirk", ""], ["Balietti", "Stefano", ""]]}, {"id": "1012.0335", "submitter": "Vittorio Perduca", "authors": "Sudeepa Roy, Vittorio Perduca and Val Tannen", "title": "Faster Query Answering in Probabilistic Databases using Read-Once\n  Functions", "comments": "Accepted in ICDT 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A boolean expression is in read-once form if each of its variables appears\nexactly once. When the variables denote independent events in a probability\nspace, the probability of the event denoted by the whole expression in\nread-once form can be computed in polynomial time (whereas the general problem\nfor arbitrary expressions is #P-complete). Known approaches to checking\nread-once property seem to require putting these expressions in disjunctive\nnormal form. In this paper, we tell a better story for a large subclass of\nboolean event expressions: those that are generated by conjunctive queries\nwithout self-joins and on tuple-independent probabilistic databases. We first\nshow that given a tuple-independent representation and the provenance graph of\nan SPJ query plan without self-joins, we can, without using the DNF of a result\nevent expression, efficiently compute its co-occurrence graph. From this, the\nread-once form can already, if it exists, be computed efficiently using\nexisting techniques. Our second and key contribution is a complete, efficient,\nand simple to implement algorithm for computing the read-once forms (whenever\nthey exist) directly, using a new concept, that of co-table graph, which can be\nsignificantly smaller than the co-occurrence graph.\n", "versions": [{"version": "v1", "created": "Wed, 1 Dec 2010 22:05:14 GMT"}, {"version": "v2", "created": "Sat, 4 Dec 2010 19:39:58 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Roy", "Sudeepa", ""], ["Perduca", "Vittorio", ""], ["Tannen", "Val", ""]]}, {"id": "1012.0663", "submitter": "Amin Milani Fard", "authors": "Amin Milani Fard, and Ke Wang", "title": "An Effective Clustering Approach to Web Query Log Anonymization", "comments": "9 pages", "journal-ref": "Proc. 2010 International Conference on Security and Cryptography\n  (SECRYPT'10), July 2010, Athens, Greece", "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web query log data contain information useful to research; however, release\nof such data can re-identify the search engine users issuing the queries. These\nprivacy concerns go far beyond removing explicitly identifying information such\nas name and address, since non-identifying personal data can be combined with\npublicly available information to pinpoint to an individual. In this work we\nmodel web query logs as unstructured transaction data and present a novel\ntransaction anonymization technique based on clustering and generalization\ntechniques to achieve the k-anonymity privacy. We conduct extensive experiments\non the AOL query log data. Our results show that this method results in a\nhigher data utility compared to the state of-the-art transaction anonymization\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 3 Dec 2010 08:58:20 GMT"}], "update_date": "2010-12-06", "authors_parsed": [["Fard", "Amin Milani", ""], ["Wang", "Ke", ""]]}, {"id": "1012.1565", "submitter": "Wided Oueslati wided", "authors": "wided oueslati, jalel akaichi", "title": "A Survey on Data Warehouse Evolution", "comments": "14 pages", "journal-ref": "International Journal of Database Management Systems ( IJDMS ),\n  Vol.2, No.4, November 2010 pages 11-24", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The data warehouse (DW) technology was developed to integrate heterogeneous\ninformation sources for analysis purposes. Information sources are more and\nmore autonomous and they often change their content due to perpetual\ntransactions (data changes) and may change their structure due to continual\nusers' requirements evolving (schema changes). Handling properly all type of\nchanges is a must. In fact, the DW which is considered as the core component of\nthe modern decision support systems has to be update according to different\ntype of evolution of information sources to reflect the real world subject to\nanalysis. The goal of this paper is to propose an overview and a comparative\nstudy between different works related to the DW evolution problem.\n", "versions": [{"version": "v1", "created": "Tue, 7 Dec 2010 17:47:31 GMT"}, {"version": "v2", "created": "Mon, 20 Dec 2010 20:17:01 GMT"}], "update_date": "2010-12-21", "authors_parsed": [["oueslati", "wided", ""], ["akaichi", "jalel", ""]]}, {"id": "1012.1621", "submitter": "Adrian Paschke", "authors": "Abdelaali Briache, Kamar Marrakchi, Amine Kerzazi, Ismael\n  Navas-Delgado, Jose F Aldana Montes, Badr D. Rossi Hassani, Khalid Lairini", "title": "YeastMed: an XML-Based System for Biological Data Integration of Yeast", "comments": "in Adrian Paschke, Albert Burger, Andrea Splendiani, M. Scott\n  Marshall, Paolo Romano: Proceedings of the 3rd International Workshop on\n  Semantic Web Applications and Tools for the Life Sciences, Berlin,Germany,\n  December 8-10, 2010", "journal-ref": null, "doi": null, "report-no": "SWAT4LS 2010", "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key goal of bioinformatics is to create database systems and software\nplatforms capable of storing and analysing large sets of biological data.\nHundreds of biological databases are now available and provide access to huge\namount of biological data. SGD, Yeastract, CYGD-MIPS, BioGrid and PhosphoGrid\nare five of the most visited databases by the yeast community. These sources\nprovide complementary data on biological entities. Biologists are brought\nsystematically to query these data sources in order to analyse the results of\ntheir experiments. Because of the heterogeneity of these sources, querying them\nseparately and then manually combining the returned result is a complex and\nlaborious task. To provide transparent and simultaneous access to these\nsources, we have developed a mediator-based system called YeastMed. In this\npaper, we present YeastMed focusing on its architecture.\n", "versions": [{"version": "v1", "created": "Tue, 7 Dec 2010 21:53:31 GMT"}], "update_date": "2010-12-09", "authors_parsed": [["Briache", "Abdelaali", ""], ["Marrakchi", "Kamar", ""], ["Kerzazi", "Amine", ""], ["Navas-Delgado", "Ismael", ""], ["Montes", "Jose F Aldana", ""], ["Hassani", "Badr D. Rossi", ""], ["Lairini", "Khalid", ""]]}, {"id": "1012.1632", "submitter": "Adrian Paschke", "authors": "Vladimir Mironov, Nirmala Seethappan, Ward Blonde, Erick Antezana,\n  Bjorn Lindi, Martin Kuiper", "title": "Benchmarking triple stores with biological data", "comments": "in Adrian Paschke, Albert Burger, Andrea Splendiani, M. Scott\n  Marshall, Paolo Romano: Proceedings of the 3rd International Workshop on\n  Semantic Web Applications and Tools for the Life Sciences, Berlin,Germany,\n  December 8-10, 2010", "journal-ref": null, "doi": null, "report-no": "SWAT4LS 2010", "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have compared the performance of five non-commercial triple stores,\nVirtuoso-open source, Jena SDB, Jena TDB, SWIFT-OWLIM and 4Store. We examined\nthree performance aspects: the query execution time, scalability and run-to-run\nreproducibility. The queries we chose addressed different ontological or\nbiological topics, and we obtained evidence that individual store performance\nwas quite query specific. We identified three groups of queries displaying\nsimilar behavior across the different stores: 1) relatively short response\ntime, 2) moderate response time and 3) relatively long response time. OWLIM\nproved to be a winner in the first group, 4Store in the second and Virtuoso in\nthe third. Our benchmarking showed Virtuoso to be a very balanced performer -\nits response time was better than average for all the 24 queries; it showed a\nvery good scalability and a reasonable run-to-run reproducibility.\n", "versions": [{"version": "v1", "created": "Tue, 7 Dec 2010 22:24:15 GMT"}], "update_date": "2010-12-09", "authors_parsed": [["Mironov", "Vladimir", ""], ["Seethappan", "Nirmala", ""], ["Blonde", "Ward", ""], ["Antezana", "Erick", ""], ["Lindi", "Bjorn", ""], ["Kuiper", "Martin", ""]]}, {"id": "1012.1645", "submitter": "Adrian Paschke", "authors": "Alexandru Todor, Adrian Paschke, Stephan Heineke", "title": "ChemCloud: Chemical e-Science Information Cloud", "comments": "in Adrian Paschke, Albert Burger, Andrea Splendiani, M. Scott\n  Marshall, Paolo Romano: Proceedings of the 3rd International Workshop on\n  Semantic Web Applications and Tools for the Life Sciences, Berlin,Germany,\n  December 8-10, 2010", "journal-ref": null, "doi": null, "report-no": "SWAT4LS 2010", "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our Chemical e-Science Information Cloud (ChemCloud) - a Semantic Web based\neScience infrastructure - integrates and automates a multitude of databases,\ntools and services in the domain of chemistry, pharmacy and bio-chemistry\navailable at the Fachinformationszentrum Chemie (FIZ Chemie), at the Freie\nUniversitaet Berlin (FUB), and on the public Web. Based on the approach of the\nW3C Linked Open Data initiative and the W3C Semantic Web technologies for\nontologies and rules it semantically links and integrates knowledge from our\nW3C HCLS knowledge base hosted at the FUB, our multi-domain knowledge base\nDBpedia (Deutschland) implemented at FUB, which is extracted from Wikipedia\n(De) providing a public semantic resource for chemistry, and our\nwell-established databases at FIZ Chemie such as ChemInform for organic\nreaction data, InfoTherm the leading source for thermophysical data, Chemisches\nZentralblatt, the complete chemistry knowledge from 1830 to 1969, and\nChemgaPedia the largest and most frequented e-Learning platform for Chemistry\nand related sciences in German language.\n", "versions": [{"version": "v1", "created": "Tue, 7 Dec 2010 23:52:54 GMT"}], "update_date": "2010-12-09", "authors_parsed": [["Todor", "Alexandru", ""], ["Paschke", "Adrian", ""], ["Heineke", "Stephan", ""]]}, {"id": "1012.1650", "submitter": "Adrian Paschke", "authors": "Samuel Croset, Christoph Grabm\\\"uller, Chen Li, Silvestras\n  Kavaliauskas and Dietrich Rebholz-Schuhmann", "title": "The CALBC RDF Triple Store: retrieval over large literature content", "comments": "in Adrian Paschke, Albert Burger, Andrea Splendiani, M. Scott\n  Marshall, Paolo Romano: Proceedings of the 3rd International Workshop on\n  Semantic Web Applications and Tools for the Life Sciences, Berlin,Germany,\n  December 8-10, 2010", "journal-ref": null, "doi": null, "report-no": "SWAT4LS 2010", "categories": "cs.DL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integration of the scientific literature into a biomedical research\ninfrastructure requires the processing of the literature, identification of the\ncontained named entities (NEs) and concepts, and to represent the content in a\nstandardised way. The CALBC project partners (PPs) have produced a large-scale\nannotated biomedical corpus with four different semantic groups through the\nharmonisation of annotations from automatic text mining solutions (Silver\nStandard Corpus, SSC). The four semantic groups were chemical entities and\ndrugs (CHED), genes and proteins (PRGE), diseases and disorders (DISO) and\nspecies (SPE). The content of the SSC has been fully integrated into RDF Triple\nStore (4,568,678 triples) and has been aligned with content from the GeneAtlas\n(182,840 triples), UniProtKb (12,552,239 triples for human) and the lexical\nresource LexEBI (BioLexicon). RDF Triple Store enables querying the scientific\nliterature and bioinformatics resources at the same time for evidence of\ngenetic causes, such as drug targets and disease involvement.\n", "versions": [{"version": "v1", "created": "Wed, 8 Dec 2010 00:19:06 GMT"}], "update_date": "2010-12-09", "authors_parsed": [["Croset", "Samuel", ""], ["Grabm\u00fcller", "Christoph", ""], ["Li", "Chen", ""], ["Kavaliauskas", "Silvestras", ""], ["Rebholz-Schuhmann", "Dietrich", ""]]}, {"id": "1012.1660", "submitter": "Adrian Paschke", "authors": "Jerven Bolleman, Alain Gateau, Sebastien Gehant, Nicole Redaschi", "title": "Provenance and evidence in UniProtKB", "comments": "in Adrian Paschke, Albert Burger, Andrea Splendiani, M. Scott\n  Marshall, Paolo Romano: Proceedings of the 3rd International Workshop on\n  Semantic Web Applications and Tools for the Life Sciences, Berlin,Germany,\n  December 8-10, 2010", "journal-ref": null, "doi": null, "report-no": "SWAT4LS 2010", "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The primary mission of UniProt is to support biological research by\nmaintaining a stable, comprehensive, fully classified, richly and accurately\nannotated protein sequence knowledgebase, with extensive cross-references to\nexternal resources, that is freely available to the scientific community. To\nenable users of the knowledgebase to accurately assess the reliability of the\ninformation contained in this resource, the evidence for and provenance of the\ninformation must be recorded. This paper discusses the user requirements for\nthis kind of metadata and the manner in which UniProtKB records it.\n", "versions": [{"version": "v1", "created": "Wed, 8 Dec 2010 00:51:37 GMT"}], "update_date": "2010-12-09", "authors_parsed": [["Bolleman", "Jerven", ""], ["Gateau", "Alain", ""], ["Gehant", "Sebastien", ""], ["Redaschi", "Nicole", ""]]}, {"id": "1012.1898", "submitter": "Adrian Paschke", "authors": "Doug Howe, Christian Pich", "title": "Ontology Usage at ZFIN", "comments": "in Adrian Paschke, Albert Burger, Andrea Splendiani, M. Scott\n  Marshall, Paolo Romano: Proceedings of the 3rd International Workshop on\n  Semantic Web Applications and Tools for the Life Sciences, Berlin,Germany,\n  December 8-10, 2010", "journal-ref": null, "doi": null, "report-no": "SWAT4LS 2010", "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Zebrafish Model Organism Database (ZFIN) provides a Web resource of\nzebrafish genomic, genetic, developmental, and phenotypic data. Four different\nontologies are currently used to annotate data to the most specific term\navailable facilitating a better comparison between inter-species data. In\naddition, ontologies are used to help users find and cluster data more quickly\nwithout the need of knowing the exact technical name for a term.\n", "versions": [{"version": "v1", "created": "Thu, 9 Dec 2010 00:03:31 GMT"}], "update_date": "2010-12-10", "authors_parsed": [["Howe", "Doug", ""], ["Pich", "Christian", ""]]}, {"id": "1012.2299", "submitter": "Wlodzimierz Drabent", "authors": "Wlodzimierz Drabent", "title": "A Simple Correctness Proof for Magic Transformation", "comments": "Submitted to \"Theory and Practice of Logic Programming\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a simple and concise proof of correctness of the magic\ntransformation. We believe it may provide a useful example of formal reasoning\nabout logic programs.\n  The correctness property concerns the declarative semantics. The proof,\nhowever, refers to the operational semantics (LD-resolution) of the source\nprograms. Its conciseness is due to applying a suitable proof method.\n", "versions": [{"version": "v1", "created": "Fri, 10 Dec 2010 15:50:36 GMT"}], "update_date": "2010-12-13", "authors_parsed": [["Drabent", "Wlodzimierz", ""]]}, {"id": "1012.2633", "submitter": "Vishal Gupta Mr", "authors": "Vishal Gupta and Ashutosh Saxena", "title": "Personalized Data Set for Analysis", "comments": "8 pages, 3 Diagrams, 1 Table, International Journal of Database\n  Management Systems (IJDMS)", "journal-ref": "International Journal of Database Management Systems ( IJDMS ),\n  Vol.2, No.4, November 2010", "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data Management portfolio within an organization has seen an upsurge in\ninitiatives for compliance, security, repurposing and storage within and\noutside the organization. When such initiatives are being put to practice care\nmust be taken while granting access to data repositories for analysis and\nmining activities. Also, initiatives such as Master Data Management, cloud\ncomputing and self service business intelligence have raised concerns in the\narena of regulatory compliance and data privacy, especially when a large data\nset of an organization are being outsourced for testing, consolidation and data\nmanagement. Here, an approach is presented where a new service layer is\nintroduced, by data governance group, in the architecture for data management\nand can be used for preserving privacy of sensitive information.\n", "versions": [{"version": "v1", "created": "Mon, 13 Dec 2010 05:28:01 GMT"}], "update_date": "2010-12-14", "authors_parsed": [["Gupta", "Vishal", ""], ["Saxena", "Ashutosh", ""]]}, {"id": "1012.2648", "submitter": "Georg Gottlob", "authors": "S. Abiteboul, G. Gottlob, M. Manna", "title": "Distributed XML Design", "comments": "\"56 pages, 4 figures\"", "journal-ref": "Proceedings of PODS '09 (2009) 247-258", "doi": "10.1145/1559795.1559833", "report-no": null, "categories": "cs.DB cs.CC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A distributed XML document is an XML document that spans several machines. We\nassume that a distribution design of the document tree is given, consisting of\nan XML kernel-document T[f1,...,fn] where some leaves are \"docking points\" for\nexternal resources providing XML subtrees (f1,...,fn, standing, e.g., for Web\nservices or peers at remote locations). The top-down design problem consists\nin, given a type (a schema document that may vary from a DTD to a tree\nautomaton) for the distributed document, \"propagating\" locally this type into a\ncollection of types, that we call typing, while preserving desirable\nproperties. We also consider the bottom-up design which consists in, given a\ntype for each external resource, exhibiting a global type that is enforced by\nthe local types, again with natural desirable properties. In the article, we\nlay out the fundamentals of a theory of distributed XML design, analyze\nproblems concerning typing issues in this setting, and study their complexity.\n", "versions": [{"version": "v1", "created": "Mon, 13 Dec 2010 07:45:30 GMT"}], "update_date": "2010-12-15", "authors_parsed": [["Abiteboul", "S.", ""], ["Gottlob", "G.", ""], ["Manna", "M.", ""]]}, {"id": "1012.2858", "submitter": "Jan Van den Bussche", "authors": "Tom Ameloot, Frank Neven, Jan Van den Bussche", "title": "Relational transducers for declarative networking", "comments": null, "journal-ref": "30th ACM Symposium on Principles of Database Systems, 2011", "doi": "10.1145/1989284.1989321", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by a recent conjecture concerning the expressiveness of declarative\nnetworking, we propose a formal computation model for \"eventually consistent\"\ndistributed querying, based on relational transducers. A tight link has been\nconjectured between coordination-freeness of computations, and monotonicity of\nthe queries expressed by such computations. Indeed, we propose a formal\ndefinition of coordination-freeness and confirm that the class of monotone\nqueries is captured by coordination-free transducer networks.\nCoordination-freeness is a semantic property, but the syntactic class that we\ndefine of \"oblivious\" transducers also captures the same class of monotone\nqueries. Transducer networks that are not coordination-free are much more\npowerful.\n", "versions": [{"version": "v1", "created": "Mon, 13 Dec 2010 20:48:12 GMT"}], "update_date": "2011-06-29", "authors_parsed": [["Ameloot", "Tom", ""], ["Neven", "Frank", ""], ["Bussche", "Jan Van den", ""]]}, {"id": "1012.3189", "submitter": "Jian Li", "authors": "Jian Li, Amol Deshpande", "title": "Maximizing Expected Utility for Stochastic Combinatorial Optimization\n  Problems", "comments": "31 pages, Preliminary version appears in the Proceeding of the 52nd\n  Annual IEEE Symposium on Foundations of Computer Science (FOCS 2011), This\n  version contains several new results ( results (2) and (3) in the abstract)", "journal-ref": null, "doi": "10.1109/FOCS.2011.33", "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the stochastic versions of a broad class of combinatorial problems\nwhere the weights of the elements in the input dataset are uncertain. The class\nof problems that we study includes shortest paths, minimum weight spanning\ntrees, and minimum weight matchings, and other combinatorial problems like\nknapsack. We observe that the expected value is inadequate in capturing\ndifferent types of {\\em risk-averse} or {\\em risk-prone} behaviors, and instead\nwe consider a more general objective which is to maximize the {\\em expected\nutility} of the solution for some given utility function, rather than the\nexpected weight (expected weight becomes a special case). Under the assumption\nthat there is a pseudopolynomial time algorithm for the {\\em exact} version of\nthe problem (This is true for the problems mentioned above), we can obtain the\nfollowing approximation results for several important classes of utility\nfunctions: (1) If the utility function $\\uti$ is continuous, upper-bounded by a\nconstant and $\\lim_{x\\rightarrow+\\infty}\\uti(x)=0$, we show that we can obtain\na polynomial time approximation algorithm with an {\\em additive error}\n$\\epsilon$ for any constant $\\epsilon>0$. (2) If the utility function $\\uti$ is\na concave increasing function, we can obtain a polynomial time approximation\nscheme (PTAS). (3) If the utility function $\\uti$ is increasing and has a\nbounded derivative, we can obtain a polynomial time approximation scheme. Our\nresults recover or generalize several prior results on stochastic shortest\npath, stochastic spanning tree, and stochastic knapsack. Our algorithm for\nutility maximization makes use of the separability of exponential utility and a\ntechnique to decompose a general utility function into exponential utility\nfunctions, which may be useful in other stochastic optimization problems.\n", "versions": [{"version": "v1", "created": "Tue, 14 Dec 2010 22:34:32 GMT"}, {"version": "v2", "created": "Fri, 15 Apr 2011 01:10:47 GMT"}, {"version": "v3", "created": "Sun, 14 Aug 2011 02:53:22 GMT"}, {"version": "v4", "created": "Fri, 19 Aug 2011 10:32:48 GMT"}, {"version": "v5", "created": "Tue, 19 Mar 2013 09:11:30 GMT"}, {"version": "v6", "created": "Sat, 19 Dec 2015 10:13:56 GMT"}, {"version": "v7", "created": "Wed, 10 Aug 2016 09:02:50 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Li", "Jian", ""], ["Deshpande", "Amol", ""]]}, {"id": "1012.3311", "submitter": "Frederic Magniez", "authors": "Christian Konrad and Frederic Magniez", "title": "Validating XML Documents in the Streaming Model with External Memory", "comments": "Change title. Remove a statement on a lower bound (now Conjecture 2\n  in Annexe B) since the proof was incomplete", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of validating XML documents of size $N$ against general\nDTDs in the context of streaming algorithms. The starting point of this work is\na well-known space lower bound. There are XML documents and DTDs for which\n$p$-pass streaming algorithms require $\\Omega(N/p)$ space.\n  We show that when allowing access to external memory, there is a\ndeterministic streaming algorithm that solves this problem with memory space\n$O(\\log^2 N)$, a constant number of auxiliary read/write streams, and $O(\\log\nN)$ total number of passes on the XML document and auxiliary streams.\n  An important intermediate step of this algorithm is the computation of the\nFirst-Child-Next-Sibling (FCNS) encoding of the initial XML document in a\nstreaming fashion. We study this problem independently, and we also provide\nmemory efficient streaming algorithms for decoding an XML document given in its\nFCNS encoding.\n  Furthermore, validating XML documents encoding binary trees in the usual\nstreaming model without external memory can be done with sublinear memory.\nThere is a one-pass algorithm using $O(\\sqrt{N \\log N})$ space, and a\nbidirectional two-pass algorithm using $O(\\log^2 N)$ space performing this\ntask.\n", "versions": [{"version": "v1", "created": "Wed, 15 Dec 2010 12:43:26 GMT"}, {"version": "v2", "created": "Sun, 19 Dec 2010 21:14:40 GMT"}, {"version": "v3", "created": "Thu, 18 Aug 2011 08:31:35 GMT"}], "update_date": "2011-08-19", "authors_parsed": [["Konrad", "Christian", ""], ["Magniez", "Frederic", ""]]}, {"id": "1012.3320", "submitter": "Wolfgang Gatterbauer", "authors": "Wolfgang Gatterbauer, Dan Suciu", "title": "Data Conflict Resolution Using Trust Mappings", "comments": "20 pages, 19 figures", "journal-ref": "Full version of SIGMOD 2010 conference paper, pp. 219-230", "doi": "10.1145/1807167.1807193", "report-no": "University of Washington CSE Technical Report 09-11-01", "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In massively collaborative projects such as scientific or community\ndatabases, users often need to agree or disagree on the content of individual\ndata items. On the other hand, trust relationships often exist between users,\nallowing them to accept or reject other users' beliefs by default. As those\ntrust relationships become complex, however, it becomes difficult to define and\ncompute a consistent snapshot of the conflicting information. Previous\nsolutions to a related problem, the update reconciliation problem, are\ndependent on the order in which the updates are processed and, therefore, do\nnot guarantee a globally consistent snapshot. This paper proposes the first\nprincipled solution to the automatic conflict resolution problem in a community\ndatabase. Our semantics is based on the certain tuples of all stable models of\na logic program. While evaluating stable models in general is well known to be\nhard, even for very simple logic programs, we show that the conflict resolution\nproblem admits a PTIME solution. To the best of our knowledge, ours is the\nfirst PTIME algorithm that allows conflict resolution in a principled way. We\nfurther discuss extensions to negative beliefs and prove that some of these\nextensions are hard. This work is done in the context of the BeliefDB project\nat the University of Washington, which focuses on the efficient management of\nconflicts in community databases.\n", "versions": [{"version": "v1", "created": "Wed, 15 Dec 2010 13:08:38 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Gatterbauer", "Wolfgang", ""], ["Suciu", "Dan", ""]]}, {"id": "1012.3502", "submitter": "Wolfgang Gatterbauer", "authors": "Wolfgang Gatterbauer", "title": "Rules of Thumb for Information Acquisition from Large and Redundant Data", "comments": "40 pages, 17 figures; for details see the project page:\n  http://uniquerecall.com", "journal-ref": "Full version of upcoming ECIR 2011 conference paper", "doi": null, "report-no": null, "categories": "cs.IR cs.DB physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an abstract model of information acquisition from redundant data.\nWe assume a random sampling process from data which provide information with\nbias and are interested in the fraction of information we expect to learn as\nfunction of (i) the sampled fraction (recall) and (ii) varying bias of\ninformation (redundancy distributions). We develop two rules of thumb with\nvarying robustness. We first show that, when information bias follows a Zipf\ndistribution, the 80-20 rule or Pareto principle does surprisingly not hold,\nand we rather expect to learn less than 40% of the information when randomly\nsampling 20% of the overall data. We then analytically prove that for large\ndata sets, randomized sampling from power-law distributions leads to \"truncated\ndistributions\" with the same power-law exponent. This second rule is very\nrobust and also holds for distributions that deviate substantially from a\nstrict power law. We further give one particular family of powerlaw functions\nthat remain completely invariant under sampling. Finally, we validate our model\nwith two large Web data sets: link distributions to domains and tag\ndistributions on delicious.com.\n", "versions": [{"version": "v1", "created": "Thu, 16 Dec 2010 02:36:13 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Gatterbauer", "Wolfgang", ""]]}, {"id": "1012.4074", "submitter": "Woong-Kee Loh Prof.", "authors": "Woong-Kee Loh, Yang-Sae Moon, and Wookey Lee", "title": "A fast divide-and-conquer algorithm for indexing human genome sequences", "comments": "8 pages, 9 figures", "journal-ref": null, "doi": "10.1587/transinf.E94.D.1369", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the release of human genome sequences, one of the most important\nresearch issues is about indexing the genome sequences, and the suffix tree is\nmost widely adopted for that purpose. The traditional suffix tree construction\nalgorithms have severe performance degradation due to the memory bottleneck\nproblem. The recent disk-based algorithms also have limited performance\nimprovement due to random disk accesses. Moreover, they do not fully utilize\nthe recent CPUs with multiple cores. In this paper, we propose a fast algorithm\nbased on 'divide-and-conquer' strategy for indexing the human genome sequences.\nOur algorithm almost eliminates random disk accesses by accessing the disk in\nthe unit of contiguous chunks. In addition, our algorithm fully utilizes the\nmulti-core CPUs by dividing the genome sequences into multiple partitions and\nthen assigning each partition to a different core for parallel processing.\nExperimental results show that our algorithm outperforms the previous fastest\nDIGEST algorithm by up to 3.5 times.\n", "versions": [{"version": "v1", "created": "Sat, 18 Dec 2010 09:46:03 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Loh", "Woong-Kee", ""], ["Moon", "Yang-Sae", ""], ["Lee", "Wookey", ""]]}, {"id": "1012.4250", "submitter": "Catuscia Palamidessi", "authors": "M\\'ario S. Alvim (INRIA Saclay - Ile de France), Konstantinos\n  Chatzikokolakis (TUE), Pierpaolo Degano (DI), Catuscia Palamidessi (INRIA\n  Saclay - Ile de France)", "title": "Differential Privacy versus Quantitative Information Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.DB math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy is a notion of privacy that has become very popular in\nthe database community. Roughly, the idea is that a randomized query mechanism\nprovides sufficient privacy protection if the ratio between the probabilities\nof two different entries to originate a certain answer is bound by e^\\epsilon.\nIn the fields of anonymity and information flow there is a similar concern for\ncontrolling information leakage, i.e. limiting the possibility of inferring the\nsecret information from the observables. In recent years, researchers have\nproposed to quantify the leakage in terms of the information-theoretic notion\nof mutual information. There are two main approaches that fall in this\ncategory: One based on Shannon entropy, and one based on R\\'enyi's min entropy.\nThe latter has connection with the so-called Bayes risk, which expresses the\nprobability of guessing the secret. In this paper, we show how to model the\nquery system in terms of an information-theoretic channel, and we compare the\nnotion of differential privacy with that of mutual information. We show that\nthe notion of differential privacy is strictly stronger, in the sense that it\nimplies a bound on the mutual information, but not viceversa.\n", "versions": [{"version": "v1", "created": "Mon, 20 Dec 2010 07:55:35 GMT"}], "update_date": "2010-12-22", "authors_parsed": [["Alvim", "M\u00e1rio S.", "", "INRIA Saclay - Ile de France"], ["Chatzikokolakis", "Konstantinos", "", "TUE"], ["Degano", "Pierpaolo", "", "DI"], ["Palamidessi", "Catuscia", "", "INRIA\n  Saclay - Ile de France"]]}, {"id": "1012.4855", "submitter": "Salvatore Raunich", "authors": "Salvatore Raunich, Erhard Rahm", "title": "Target-driven merging of Taxonomies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of ontologies and taxonomies in many domains increasingly\ndemands the integration of multiple such ontologies. The goal of ontology\nintegration is to merge two or more given ontologies in order to provide a\nunified view on the input ontologies while maintaining all information coming\nfrom them. We propose a new taxonomy merging algorithm that, given as input two\ntaxonomies and an equivalence matching between them, can generate an integrated\ntaxonomy in a fully automatic manner. The approach is target-driven, i.e. we\nmerge a source taxonomy into the target taxonomy and preserve the structure of\nthe target ontology as much as possible. We also discuss how to extend the\nmerge algorithm providing auxiliary information, like additional relationships\nbetween source and target concepts, in order to semantically improve the final\nresult. The algorithm was implemented in a working prototype and evaluated\nusing synthetic and real-world scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 22 Dec 2010 01:43:14 GMT"}], "update_date": "2010-12-23", "authors_parsed": [["Raunich", "Salvatore", ""], ["Rahm", "Erhard", ""]]}, {"id": "1012.4889", "submitter": "Mert Sa\\u{g}lam", "authors": "Hossein Jowhari, Mert Sa\\u{g}lam, G\\'abor Tardos", "title": "Tight Bounds for Lp Samplers, Finding Duplicates in Streams, and Related\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present near-optimal space bounds for Lp-samplers. Given a\nstream of updates (additions and subtraction) to the coordinates of an\nunderlying vector x \\in R^n, a perfect Lp sampler outputs the i-th coordinate\nwith probability |x_i|^p/||x||_p^p. In SODA 2010, Monemizadeh and Woodruff\nshowed polylog space upper bounds for approximate Lp-samplers and demonstrated\nvarious applications of them. Very recently, Andoni, Krauthgamer and Onak\nimproved the upper bounds and gave a O(\\epsilon^{-p} log^3 n) space \\epsilon\nrelative error and constant failure rate Lp-sampler for p \\in [1,2]. In this\nwork, we give another such algorithm requiring only O(\\epsilon^{-p} log^2 n)\nspace for p \\in (1,2). For p \\in (0,1), our space bound is O(\\epsilon^{-1}\nlog^2 n), while for the $p=1$ case we have an O(log(1/\\epsilon)\\epsilon^{-1}\nlog^2 n) space algorithm. We also give a O(log^2 n) bits zero relative error\nL0-sampler, improving the O(log^3 n) bits algorithm due to Frahling, Indyk and\nSohler.\n  As an application of our samplers, we give better upper bounds for the\nproblem of finding duplicates in data streams. In case the length of the stream\nis longer than the alphabet size, L1 sampling gives us an O(log^2 n) space\nalgorithm, thus improving the previous O(log^3 n) bound due to Gopalan and\nRadhakrishnan.\n  In the second part of our work, we prove an Omega(log^2 n) lower bound for\nsampling from 0, \\pm 1 vectors (in this special case, the parameter p is not\nrelevant for Lp sampling). This matches the space of our sampling algorithms\nfor constant \\epsilon > 0. We also prove tight space lower bounds for the\nfinding duplicates and heavy hitters problems. We obtain these lower bounds\nusing reductions from the communication complexity problem augmented indexing.\n", "versions": [{"version": "v1", "created": "Wed, 22 Dec 2010 06:55:58 GMT"}], "update_date": "2010-12-23", "authors_parsed": [["Jowhari", "Hossein", ""], ["Sa\u011flam", "Mert", ""], ["Tardos", "G\u00e1bor", ""]]}, {"id": "1012.5506", "submitter": "Adrian Paschke", "authors": "Alejandra Gonzalez-Beltran, Ben Tagger, and Anthony Finkelstein", "title": "Ontology-based Queries over Cancer Data", "comments": "in Adrian Paschke, Albert Burger, Andrea Splendiani, M. Scott\n  Marshall, Paolo Romano: Proceedings of the 3rd International Workshop on\n  Semantic Web Applications and Tools for the Life Sciences, Berlin,Germany,\n  December 8-10, 2010", "journal-ref": null, "doi": null, "report-no": "SWAT4LS 2010", "categories": "cs.AI cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ever-increasing amount of data in biomedical research, and in cancer\nresearch in particular, needs to be managed to support efficient data access,\nexchange and integration. Existing software infrastructures, such caGrid,\nsupport access to distributed information annotated with a domain ontology.\nHowever, caGrid's current querying functionality depends on the structure of\nindividual data resources without exploiting the semantic annotations. In this\npaper, we present the design and development of an ontology-based querying\nfunctionality that consists of: the generation of OWL2 ontologies from the\nunderlying data resources metadata and a query rewriting and translation\nprocess based on reasoning, which converts a query at the domain ontology level\ninto queries at the software infrastructure level. We present a detailed\nanalysis of our approach as well as an extensive performance evaluation. While\nthe implementation and evaluation was performed for the caGrid infrastructure,\nthe approach could be applicable to other model and metadata-driven\nenvironments for data sharing.\n", "versions": [{"version": "v1", "created": "Sun, 26 Dec 2010 10:49:52 GMT"}], "update_date": "2010-12-30", "authors_parsed": [["Gonzalez-Beltran", "Alejandra", ""], ["Tagger", "Ben", ""], ["Finkelstein", "Anthony", ""]]}, {"id": "1012.5546", "submitter": "Amine Farhat Mr.", "authors": "Mohamed Salah Gouider and Amine Farhat", "title": "Mining Multi-Level Frequent Itemsets under Constraints", "comments": "20 pages", "journal-ref": "Internatinal Journal of Database Theory and Application, Vol. 3,\n  No. 4, PP. 15-35, December, 2010", "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining association rules is a task of data mining, which extracts knowledge\nin the form of significant implication relation of useful items (objects) from\na database. Mining multilevel association rules uses concept hierarchies, also\ncalled taxonomies and defined as relations of type 'is-a' between objects, to\nextract rules that items belong to different levels of abstraction. These rules\nare more useful, more refined and more interpretable by the user. Several\nalgorithms have been proposed in the literature to discover the multilevel\nassociation rules. In this article, we are interested in the problem of\ndiscovering multi-level frequent itemsets under constraints, involving the user\nin the research process. We proposed a technique for modeling and\ninterpretation of constraints in a context of use of concept hierarchies. Three\napproaches for discovering multi-level frequent itemsets under constraints were\nproposed and discussed: Basic approach, \"Test and Generate\" approach and\nPruning based Approach.\n", "versions": [{"version": "v1", "created": "Sun, 26 Dec 2010 22:23:00 GMT"}], "update_date": "2010-12-30", "authors_parsed": [["Gouider", "Mohamed Salah", ""], ["Farhat", "Amine", ""]]}, {"id": "1012.5696", "submitter": "Sebastian Maneth", "authors": "Sebastian Maneth and Tom Sebastian", "title": "Fast and Tiny Structural Self-Indexes for XML", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  XML document markup is highly repetitive and therefore well compressible\nusing dictionary-based methods such as DAGs or grammars. In the context of\nselectivity estimation, grammar-compressed trees were used before as synopsis\nfor structural XPath queries. Here a fully-fledged index over such grammars is\npresented. The index allows to execute arbitrary tree algorithms with a\nslow-down that is comparable to the space improvement. More interestingly,\ncertain algorithms execute much faster over the index (because no decompression\noccurs). E.g., for structural XPath count queries, evaluating over the index is\nfaster than previous XPath implementations, often by two orders of magnitude.\nThe index also allows to serialize XML results (including texts) faster than\nprevious systems, by a factor of ca. 2-3. This is due to efficient copy\nhandling of grammar repetitions, and because materialization is totally\navoided. In order to compare with twig join implementations, we implemented a\nmaterializer which writes out pre-order numbers of result nodes, and show its\ncompetitiveness.\n", "versions": [{"version": "v1", "created": "Tue, 28 Dec 2010 04:48:21 GMT"}], "update_date": "2010-12-30", "authors_parsed": [["Maneth", "Sebastian", ""], ["Sebastian", "Tom", ""]]}, {"id": "1012.6009", "submitter": "Rahmat Widia Sembiring", "authors": "Rahmat Widia Sembiring, Jasni Mohamad Zain", "title": "Cluster Evaluation of Density Based Subspace Clustering", "comments": "6 pages, 15 figures", "journal-ref": "Journal of Computing, Volume 2, Issue 11, November 2010, ISSN\n  2151-9617", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering real world data often faced with curse of dimensionality, where\nreal world data often consist of many dimensions. Multidimensional data\nclustering evaluation can be done through a density-based approach. Density\napproaches based on the paradigm introduced by DBSCAN clustering. In this\napproach, density of each object neighbours with MinPoints will be calculated.\nCluster change will occur in accordance with changes in density of each object\nneighbours. The neighbours of each object typically determined using a distance\nfunction, for example the Euclidean distance. In this paper SUBCLU, FIRES and\nINSCY methods will be applied to clustering 6x1595 dimension synthetic\ndatasets. IO Entropy, F1 Measure, coverage, accurate and time consumption used\nas evaluation performance parameters. Evaluation results showed SUBCLU method\nrequires considerable time to process subspace clustering; however, its value\ncoverage is better. Meanwhile INSCY method is better for accuracy comparing\nwith two other methods, although consequence time calculation was longer.\n", "versions": [{"version": "v1", "created": "Wed, 29 Dec 2010 19:34:11 GMT"}], "update_date": "2010-12-30", "authors_parsed": [["Sembiring", "Rahmat Widia", ""], ["Zain", "Jasni Mohamad", ""]]}]