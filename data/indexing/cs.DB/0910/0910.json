[{"id": "0910.0097", "submitter": "Alexandre Vaniachine", "authors": "A. Vaniachine (for the ATLAS Collaboration)", "title": "Scalable Database Access Technologies for ATLAS Distributed Computing", "comments": "6 pages, 7 figures. To be published in the proceedings of DPF-2009,\n  Detroit, MI, July 2009, eConf C090726", "journal-ref": null, "doi": null, "report-no": "ANL-HEP-CP-09-085", "categories": "physics.ins-det cs.DB cs.DC hep-ex", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  ATLAS event data processing requires access to non-event data (detector\nconditions, calibrations, etc.) stored in relational databases. The\ndatabase-resident data are crucial for the event data reconstruction processing\nsteps and often required for user analysis. A main focus of ATLAS database\noperations is on the worldwide distribution of the Conditions DB data, which\nare necessary for every ATLAS data processing job. Since Conditions DB access\nis critical for operations with real data, we have developed the system where a\ndifferent technology can be used as a redundant backup. Redundant database\noperations infrastructure fully satisfies the requirements of ATLAS\nreprocessing, which has been proven on a scale of one billion database queries\nduring two reprocessing campaigns of 0.5 PB of single-beam and cosmics data on\nthe Grid. To collect experience and provide input for a best choice of\ntechnologies, several promising options for efficient database access in user\nanalysis were evaluated successfully. We present ATLAS experience with scalable\ndatabase access technologies and describe our approach for prevention of\ndatabase access bottlenecks in a Grid computing environment.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2009 07:20:54 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2009 13:55:12 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Vaniachine", "A.", "", "for the ATLAS Collaboration"]]}, {"id": "0910.0112", "submitter": "Rasmus Pagh", "authors": "Andrea Campagna and Rasmus Pagh", "title": "Finding Associations and Computing Similarity via Biased Pair Sampling", "comments": "This is an extended version of a paper that appeared at the IEEE\n  International Conference on Data Mining, 2009. The conference version is (c)\n  2009 IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This version is ***superseded*** by a full version that can be found at\nhttp://www.itu.dk/people/pagh/papers/mining-jour.pdf, which contains stronger\ntheoretical results and fixes a mistake in the reporting of experiments.\n  Abstract: Sampling-based methods have previously been proposed for the\nproblem of finding interesting associations in data, even for low-support\nitems. While these methods do not guarantee precise results, they can be vastly\nmore efficient than approaches that rely on exact counting. However, for many\nsimilarity measures no such methods have been known. In this paper we show how\na wide variety of measures can be supported by a simple biased sampling method.\nThe method also extends to find high-confidence association rules. We\ndemonstrate theoretically that our method is superior to exact methods when the\nthreshold for \"interesting similarity/confidence\" is above the average pairwise\nsimilarity/confidence, and the average support is not too low. Our method is\nparticularly good when transactions contain many items. We confirm in\nexperiments on standard association mining benchmarks that this gives a\nsignificant speedup on real data sets (sometimes much larger than the\ntheoretical guarantees). Reductions in computation time of over an order of\nmagnitude, and significant savings in space, are observed.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2009 09:02:54 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2010 09:32:14 GMT"}], "update_date": "2010-02-17", "authors_parsed": [["Campagna", "Andrea", ""], ["Pagh", "Rasmus", ""]]}, {"id": "0910.0983", "submitter": "Tomas  Skopal", "authors": "Tomas Skopal, Jakub Lokoc", "title": "On Metric Skyline Processing by PM-tree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DL cs.MM cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of similarity search in multimedia databases is usually accomplished\nby range or k nearest neighbor queries. However, the expressing power of these\n\"single-example\" queries fails when the user's delicate query intent is not\navailable as a single example. Recently, the well-known skyline operator was\nreused in metric similarity search as a \"multi-example\" query type. When\napplied on a multi-dimensional database (i.e., on a multi-attribute table), the\ntraditional skyline operator selects all database objects that are not\ndominated by other objects. The metric skyline query adopts the skyline\noperator such that the multiple attributes are represented by distances\n(similarities) to multiple query examples. Hence, we can view the metric\nskyline as a set of representative database objects which are as similar to all\nthe examples as possible and, simultaneously, are semantically distinct. In\nthis paper we propose a technique of processing the metric skyline query by use\nof PM-tree, while we show that our technique significantly outperforms the\noriginal M-tree based implementation in both time and space costs. In\nexperiments we also evaluate the partial metric skyline processing, where only\na controlled number of skyline objects is retrieved.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2009 12:09:52 GMT"}], "update_date": "2009-10-07", "authors_parsed": [["Skopal", "Tomas", ""], ["Lokoc", "Jakub", ""]]}, {"id": "0910.1495", "submitter": "Ping Li", "authors": "Ping Li", "title": "Estimating Entropy of Data Streams Using Compressed Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Shannon entropy is a widely used summary statistic, for example, network\ntraffic measurement, anomaly detection, neural computations, spike trains, etc.\nThis study focuses on estimating Shannon entropy of data streams. It is known\nthat Shannon entropy can be approximated by Reenyi entropy or Tsallis entropy,\nwhich are both functions of the p-th frequency moments and approach Shannon\nentropy as p->1.\n  Compressed Counting (CC) is a new method for approximating the p-th frequency\nmoments of data streams. Our contributions include:\n  1) We prove that Renyi entropy is (much) better than Tsallis entropy for\napproximating Shannon entropy.\n  2) We propose the optimal quantile estimator for CC, which considerably\nimproves the previous estimators.\n  3) Our experiments demonstrate that CC is indeed highly effective\napproximating the moments and entropies. We also demonstrate the crucial\nimportance of utilizing the variance-bias trade-off.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2009 12:54:36 GMT"}], "update_date": "2009-10-09", "authors_parsed": [["Li", "Ping", ""]]}, {"id": "0910.2405", "submitter": "Maya Ramanath", "authors": "Maya Ramanath, Kondreddi Sarath Kumar, Georgiana Ifrim", "title": "Generating Concise and Readable Summaries of XML Documents", "comments": null, "journal-ref": null, "doi": null, "report-no": "MPI-I-2009-5-002", "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  XML has become the de-facto standard for data representation and exchange,\nresulting in large scale repositories and warehouses of XML data. In order for\nusers to understand and explore these large collections, a summarized, bird's\neye view of the available data is a necessity. In this paper, we are interested\nin semantic XML document summaries which present the \"important\" information\navailable in an XML document to the user. In the best case, such a summary is a\nconcise replacement for the original document itself. At the other extreme, it\nshould at least help the user make an informed choice as to the relevance of\nthe document to his needs. In this paper, we address the two main issues which\narise in producing such meaningful and concise summaries: i) which tags or text\nunits are important and should be included in the summary, ii) how to generate\nsummaries of different sizes.%for different memory budgets. We conduct user\nstudies with different real-life datasets and show that our methods are useful\nand effective in practice.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2009 14:19:01 GMT"}], "update_date": "2009-10-14", "authors_parsed": [["Ramanath", "Maya", ""], ["Kumar", "Kondreddi Sarath", ""], ["Ifrim", "Georgiana", ""]]}, {"id": "0910.3148", "submitter": "Riccardo Dondi", "authors": "Stefano Beretta, Paola Bonizzoni, Gianluca Della Vedova, Riccardo\n  Dondi and Yuri Pirola", "title": "Parameterized Complexity of the k-anonymity Problem", "comments": "22 pages, 2 figures", "journal-ref": "J. of Combinatorial Optimization 26.1 (2013) 19-43", "doi": "10.1007/s10878-011-9428-9", "report-no": null, "categories": "cs.DS cs.DB cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of publishing personal data without giving up privacy is becoming\nincreasingly important. An interesting formalization that has been recently\nproposed is the $k$-anonymity. This approach requires that the rows of a table\nare partitioned in clusters of size at least $k$ and that all the rows in a\ncluster become the same tuple, after the suppression of some entries. The\nnatural optimization problem, where the goal is to minimize the number of\nsuppressed entries, is known to be APX-hard even when the records values are\nover a binary alphabet and $k=3$, and when the records have length at most 8\nand $k=4$ . In this paper we study how the complexity of the problem is\ninfluenced by different parameters. In this paper we follow this direction of\nresearch, first showing that the problem is W[1]-hard when parameterized by the\nsize of the solution (and the value $k$). Then we exhibit a fixed parameter\nalgorithm, when the problem is parameterized by the size of the alphabet and\nthe number of columns. Finally, we investigate the computational (and\napproximation) complexity of the $k$-anonymity problem, when restricting the\ninstance to records having length bounded by 3 and $k=3$. We show that such a\nrestriction is APX-hard.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2009 16:01:33 GMT"}, {"version": "v2", "created": "Mon, 17 May 2010 08:06:06 GMT"}], "update_date": "2013-11-20", "authors_parsed": [["Beretta", "Stefano", ""], ["Bonizzoni", "Paola", ""], ["Della Vedova", "Gianluca", ""], ["Dondi", "Riccardo", ""], ["Pirola", "Yuri", ""]]}, {"id": "0910.3349", "submitter": "Ping Li", "authors": "Ping Li, Arnd Christian Konig", "title": "b-Bit Minwise Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper establishes the theoretical framework of b-bit minwise hashing.\nThe original minwise hashing method has become a standard technique for\nestimating set similarity (e.g., resemblance) with applications in information\nretrieval, data management, social networks and computational advertising.\n  By only storing the lowest $b$ bits of each (minwise) hashed value (e.g., b=1\nor 2), one can gain substantial advantages in terms of computational efficiency\nand storage space. We prove the basic theoretical results and provide an\nunbiased estimator of the resemblance for any b. We demonstrate that, even in\nthe least favorable scenario, using b=1 may reduce the storage space at least\nby a factor of 21.3 (or 10.7) compared to using b=64 (or b=32), if one is\ninterested in resemblance > 0.5.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2009 03:39:56 GMT"}], "update_date": "2009-10-20", "authors_parsed": [["Li", "Ping", ""], ["Konig", "Arnd Christian", ""]]}, {"id": "0910.3372", "submitter": "Marcelo Arenas", "authors": "Marcelo Arenas, Jorge Perez, Juan Reutter and Cristian Riveros", "title": "Composition and Inversion of Schema Mappings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent years, a lot of attention has been paid to the development of\nsolid foundations for the composition and inversion of schema mappings. In this\npaper, we review the proposals for the semantics of these crucial operators.\nFor each of these proposals, we concentrate on the three following problems:\nthe definition of the semantics of the operator, the language needed to express\nthe operator, and the algorithmic issues associated to the problem of computing\nthe operator. It should be pointed out that we primarily consider the\nformalization of schema mappings introduced in the work on data exchange. In\nparticular, when studying the problem of computing the composition and inverse\nof a schema mapping, we will be mostly interested in computing these operators\nfor mappings specified by source-to-target tuple-generating dependencies.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2009 13:46:36 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2009 17:48:48 GMT"}, {"version": "v3", "created": "Mon, 21 Dec 2009 18:22:06 GMT"}, {"version": "v4", "created": "Thu, 4 Mar 2010 18:36:16 GMT"}], "update_date": "2010-03-04", "authors_parsed": [["Arenas", "Marcelo", ""], ["Perez", "Jorge", ""], ["Reutter", "Juan", ""], ["Riveros", "Cristian", ""]]}]