[{"id": "1312.0001", "submitter": "Debajyoti Mukhopadhyay Prof.", "authors": "Ayan Chakraborty, Shiladitya Munshi, Debajyoti Mukhopadhyay", "title": "A Proposal for the Characterization of Multi-Dimensional\n  Inter-relationships of RDF Graphs Based on Set Theoretic Approach", "comments": "8 pages, 5 figures. arXiv admin note: substantial text overlap with\n  arXiv:1311.7200", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a Set Theoretic approach has been reported for analyzing\ninter-relationship between any numbers of RDF Graphs. An RDF Graph represents\ntriples in Resource Description Format of semantic web. So the identification\nand characterization of criteria for inter-relationship of RDF Graphs shows a\nnew road in semantic search. Using set theoretic approach, a sound framing\ncriteria can be designed that examine whether two RDF Graphs are related and if\nyes, how these relationships could be described with formal set theory. Along\nwith this, by introducing RDF Schema, the inter-relationship status is refined\ninto n-dimensional induced relationships.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2013 04:11:36 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Chakraborty", "Ayan", ""], ["Munshi", "Shiladitya", ""], ["Mukhopadhyay", "Debajyoti", ""]]}, {"id": "1312.0032", "submitter": "Gerardo Simari", "authors": "Thomas Lukasiewicz, Maria Vanina Martinez, Cristian Molinaro, Livia\n  Predoiu, and Gerardo I. Simari", "title": "Top-k Query Answering in Datalog+/- Ontologies under Subjective Reports\n  (Technical Report)", "comments": "arXiv admin note: text overlap with arXiv:1106.3767 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of preferences in query answering, both in traditional databases and\nin ontology-based data access, has recently received much attention, due to its\nmany real-world applications. In this paper, we tackle the problem of top-k\nquery answering in Datalog+/- ontologies subject to the querying user's\npreferences and a collection of (subjective) reports of other users. Here, each\nreport consists of scores for a list of features, its author's preferences\namong the features, as well as other information. Theses pieces of information\nof every report are then combined, along with the querying user's preferences\nand his/her trust into each report, to rank the query results. We present two\nalternative such rankings, along with algorithms for top-k (atomic) query\nanswering under these rankings. We also show that, under suitable assumptions,\nthese algorithms run in polynomial time in the data complexity. We finally\npresent more general reports, which are associated with sets of atoms rather\nthan single atoms.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2013 22:06:09 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Lukasiewicz", "Thomas", ""], ["Martinez", "Maria Vanina", ""], ["Molinaro", "Cristian", ""], ["Predoiu", "Livia", ""], ["Simari", "Gerardo I.", ""]]}, {"id": "1312.0042", "submitter": "Bojian Xu", "authors": "Bojian Xu", "title": "Boosting the Basic Counting on Distributed Streams", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the classic basic counting problem in the distributed streaming\nmodel that was studied by Gibbons and Tirthapura (GT). In the solution for\nmaintaining an $(\\epsilon,\\delta)$-estimate, as what GT's method does, we make\nthe following new contributions: (1) For a bit stream of size $n$, where each\nbit has a probability at least $\\gamma$ to be 1, we exponentially reduced the\naverage total processing time from GT's $\\Theta(n \\log(1/\\delta))$ to\n$O((1/(\\gamma\\epsilon^2))(\\log^2 n) \\log(1/\\delta))$, thus providing the first\nsublinear-time streaming algorithm for this problem. (2) In addition to an\noverall much faster processing speed, our method provides a new tradeoff that a\nlower accuracy demand (a larger value for $\\epsilon$) promises a faster\nprocessing speed, whereas GT's processing speed is $\\Theta(n \\log(1/\\delta))$\nin any case and for any $\\epsilon$. (3) The worst-case total time cost of our\nmethod matches GT's $\\Theta(n\\log(1/\\delta))$, which is necessary but rarely\noccurs in our method. (4) The space usage overhead in our method is a lower\norder term compared with GT's space usage and occurs only $O(\\log n)$ times\nduring the stream processing and is too negligible to be detected by the\noperating system in practice. We further validate these solid theoretical\nresults with experiments on both real-world and synthetic data, showing that\nour method is faster than GT's by a factor of several to several thousands\ndepending on the stream size and accuracy demands, without any detectable space\nusage overhead. Our method is based on a faster sampling technique that we\ndesign for boosting GT's method and we believe this technique can be of other\ninterest.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2013 23:56:52 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Xu", "Bojian", ""]]}, {"id": "1312.0156", "submitter": "Verena Kantere", "authors": "Verena Kantere", "title": "Datom: Towards modular data management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent technology breakthroughs have enabled data collection of unprecedented\nscale, rate, variety and complexity that has led to an explosion in data\nmanagement requirements. Existing theories and techniques are not adequate to\nfulfil these requirements. We endeavour to rethink the way data management\nresearch is being conducted and we propose to work towards modular data\nmanagement that will allow for unification of the expression of data management\nproblems and systematization of their solution. The core of such an approach is\nthe novel notion of a datom, i.e. a data management atom, which encapsulates\ngeneric data management provision. The datom is the foundation for comparison,\ncustomization and re-usage of data management problems and solutions. The\nproposed approach can signal a revolution in data management research and a\nlong anticipated evolution in data management engineering.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2013 21:33:49 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Kantere", "Verena", ""]]}, {"id": "1312.0189", "submitter": "Hasan Jamil", "authors": "Hasan M. Jamil", "title": "Empowering Evolving Social Network Users with Privacy Rights", "comments": "8 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Considerable concerns exist over privacy on social networks, and huge debates\npersist about how to extend the artifacts users need to effectively protect\ntheir rights to privacy. While many interesting ideas have been proposed, no\nsingle approach appears to be comprehensive enough to be the front runner. In\nthis paper, we propose a comprehensive and novel reference conceptual model for\nprivacy in constantly evolving social networks and establish its novelty by\nbriefly contrasting it with contemporary research. We also present the contours\nof a possible query language that we can develop with desirable features in\nlight of the reference model, and refer to a new query language, {\\em PiQL},\ndeveloped on the basis of this model that aims to support user driven privacy\npolicy authoring and enforcement. The strength of our model is that such\nextensions are now possible by developing appropriate linguistic constructs as\npart of query languages such as SQL, as demonstrated in PiQL.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2013 08:14:39 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Jamil", "Hasan M.", ""]]}, {"id": "1312.0285", "submitter": "Lukasz Golab", "authors": "Lukasz Golab and Marios Hadjieleftheriou and Howard Karloff and Barna\n  Saha", "title": "Distributed Data Placement via Graph Partitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the widespread use of shared-nothing clusters of servers, there has been\na proliferation of distributed object stores that offer high availability,\nreliability and enhanced performance for MapReduce-style workloads. However,\nrelational workloads cannot always be evaluated efficiently using MapReduce\nwithout extensive data migrations, which cause network congestion and reduced\nquery throughput. We study the problem of computing data placement strategies\nthat minimize the data communication costs incurred by typical relational query\nworkloads in a distributed setting.\n  Our main contribution is a reduction of the data placement problem to the\nwell-studied problem of {\\sc Graph Partitioning}, which is NP-Hard but for\nwhich efficient approximation algorithms exist. The novelty and significance of\nthis result lie in representing the communication cost exactly and using\nstandard graphs instead of hypergraphs, which were used in prior work on data\nplacement that optimized for different objectives (not communication cost).\n  We study several practical extensions of the problem: with load balancing,\nwith replication, with materialized views, and with complex query plans\nconsisting of sequences of intermediate operations that may be computed on\ndifferent servers. We provide integer linear programs (IPs) that may be used\nwith any IP solver to find an optimal data placement. For the no-replication\ncase, we use publicly available graph partitioning libraries (e.g., METIS) to\nefficiently compute nearly-optimal solutions. For the versions with\nreplication, we introduce two heuristics that utilize the {\\sc Graph\nPartitioning} solution of the no-replication case. Using the TPC-DS workload,\nit may take an IP solver weeks to compute an optimal data placement, whereas\nour reduction produces nearly-optimal solutions in seconds.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2013 23:16:42 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Golab", "Lukasz", ""], ["Hadjieleftheriou", "Marios", ""], ["Karloff", "Howard", ""], ["Saha", "Barna", ""]]}, {"id": "1312.0742", "submitter": "Leandro Pacheco", "authors": "Leandro Pacheco, Daniele Sciascia, Fernando Pedone", "title": "Parallel Deferred Update Replication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deferred update replication (DUR) is an established approach to implementing\nhighly efficient and available storage. While the throughput of read-only\ntransactions scales linearly with the number of deployed replicas in DUR, the\nthroughput of update transactions experiences limited improvements as replicas\nare added. This paper presents Parallel Deferred Update Replication (P-DUR), a\nvariation of classical DUR that scales both read-only and update transactions\nwith the number of cores available in a replica. In addition to introducing the\nnew approach, we describe its full implementation and compare its performance\nto classical DUR and to Berkeley DB, a well-known standalone database.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2013 09:17:22 GMT"}], "update_date": "2013-12-04", "authors_parsed": [["Pacheco", "Leandro", ""], ["Sciascia", "Daniele", ""], ["Pedone", "Fernando", ""]]}, {"id": "1312.1860", "submitter": "Minyar Sassi", "authors": "Olfa Arfaoui, Minyar Sassi-Hidri", "title": "Flexible queries in XML native databases", "comments": "5 Pages, 1 Figure", "journal-ref": "International Conference on Control, Engineering & Information\n  Technology (CEIT), Proceedings Engineering & Technology, Vol. 4, pp. 100-104,\n  2013", "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To date, most of the XML native databases (DB) flexible querying systems are\nbased on exploiting the tree structure of their semi structured data (SSD).\nHowever, it becomes important to test the efficiency of Formal Concept Analysis\n(FCA) formalism for this type of data since it has been proved a great\nperformance in the field of information retrieval (IR). So, the IR in XML\ndatabases based on FCA is mainly based on the use of the lattice structure.\nEach concept of this lattice can be interpreted as a pair (response, query). In\nthis work, we provide a new flexible modeling of XML DB based on fuzzy FCA as a\nfirst step towards flexible querying of SSD.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2013 13:42:07 GMT"}], "update_date": "2013-12-09", "authors_parsed": [["Arfaoui", "Olfa", ""], ["Sassi-Hidri", "Minyar", ""]]}, {"id": "1312.2063", "submitter": "Amir Ingber", "authors": "Amir Ingber and Tsachy Weissman", "title": "The Minimal Compression Rate for Similarity Identification", "comments": "45 pages, 6 figures. Submitted to IEEE Transactions on Information\n  Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DB cs.IR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, data compression deals with the problem of concisely\nrepresenting a data source, e.g. a sequence of letters, for the purpose of\neventual reproduction (either exact or approximate). In this work we are\ninterested in the case where the goal is to answer similarity queries about the\ncompressed sequence, i.e. to identify whether or not the original sequence is\nsimilar to a given query sequence. We study the fundamental tradeoff between\nthe compression rate and the reliability of the queries performed on compressed\ndata. For i.i.d. sequences, we characterize the minimal compression rate that\nallows query answers, that are reliable in the sense of having a vanishing\nfalse-positive probability, when false negatives are not allowed. The result is\npartially based on a previous work by Ahlswede et al., and the inherently\ntypical subset lemma plays a key role in the converse proof. We then\ncharacterize the compression rate achievable by schemes that use lossy source\ncodes as a building block, and show that such schemes are, in general,\nsuboptimal. Finally, we tackle the problem of evaluating the minimal\ncompression rate, by converting the problem to a sequence of convex programs\nthat can be solved efficiently.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2013 06:40:55 GMT"}], "update_date": "2013-12-10", "authors_parsed": [["Ingber", "Amir", ""], ["Weissman", "Tsachy", ""]]}, {"id": "1312.2065", "submitter": "Hanumanth Sastry Sistla", "authors": "S.Hanumanth Sastry and Prof.M.S.Prasada Babu", "title": "Implementation of CRISP Methodology for ERP Systems", "comments": "International Journal of Computer Science Engineering (IJCSE).\n  http://www.ijcse.net/issue.php?file=vol02issue5 Volume 2 Issue 4 September\n  2013. arXiv admin note: text overlap with arXiv:1211.5723 by other authors\n  without attribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ERP systems contain huge amounts of data related to the actual execution of\nbusiness processes. These systems have a particular way of recording activities\nwhich results in an unclear display of business processes in event logs.\nSeveral works have been conducted on ERP systems, most of them focusing on the\ndevelopment of new algorithms for the automatic discovery of business\nprocesses. We focused on addressing issues like, how can organizations with ERP\nsystems apply process mining for analyzing their business processes in order to\nimprove them. The data handling aspect of ERP systems contrasts with those of\nBPMS or workflow based systems, whose systematical storage of events\nfacilitates the application of process mining techniques. CRISP-DM has emerged\nas the de facto standard for developing data mining and knowledge discovery\nprojects. Successful data mining requires three families of analytical\ncapabilities namely reporting, classification and forecasting. A data miner\nuses more than one analytical method to get the best results. The objective of\nthis paper is to improve the usability and understandability of process mining\ntechniques, by implementing CRISP-DM methodology for their application in ERP\ncontexts, detailed in terms of specific implementation tools and step by step\ncoordination. Our study confirms that data discovery from ERP system improves\nstrategic and operational decision making.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2013 07:29:56 GMT"}], "update_date": "2013-12-10", "authors_parsed": [["Sastry", "S. Hanumanth", ""], ["Babu", "Prof. M. S. Prasada", ""]]}, {"id": "1312.2069", "submitter": "Ali Azimi", "authors": "Ali Azimi, Azar Kaffashpour", "title": "Applying the Apriori algorithm for investigating the relationships\n  between demographic characteristics of Iranian top 100 enterprises and the\n  strcture of their commercial website", "comments": "19 pages, 3 figures, 2 tables, 21 references, 2 appendix", "journal-ref": "International Journal of Data Mining & Knowledge Management\n  Process (IJDKP) Vol.3, No.6, November 2013", "doi": "10.5121/ijdkp.2013.3602", "report-no": null, "categories": "cs.DB cs.CY", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  This study was conducted with the main aim to investigate the relationships\nbetween demographic characteristics of companies and the facilities required\nfor their commercial websites. The research samples are the top 100 Iranian\ncompanies as ranked by the Iranian Industrial Management Institute; the method\napplied is datamining, using Association Rules throught the Apriori algorithms.\nTo collect the data, an aithor-modified check list has been utilized, coverig\nthe three areas of faclities within commercial websites, i.e. fundamental,\ninformation-providing, and service-delivering facilities. having extracted the\nassociation rules between the mentioned two sets of variables, 68 rules with a\nconfidence rate of 90% and above were obtained, and based on their significance\nwere classified into two groups of must-have and should-have requirements; a\nrecommended package of facilities is hitherto offered to other companies which\nintend to enter e-commerce through their commerical websites with regards to\neach company's unique demographic characteristics.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2013 08:08:12 GMT"}], "update_date": "2013-12-10", "authors_parsed": [["Azimi", "Ali", ""], ["Kaffashpour", "Azar", ""]]}, {"id": "1312.2140", "submitter": "Peyman Mohammadi", "authors": "Peyman Mohammadi, Abdolreza Hatamlou and Mohammad Masdari", "title": "A Comparative Study on Remote Tracking of Parkinsons Disease Progression\n  Using Data Mining Methods", "comments": "13 Pages, 4 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, applications of data mining methods are become more popular\nin many fields of medical diagnosis and evaluations. The data mining methods\nare appropriate tools for discovering and extracting of available knowledge in\nmedical databases. In this study, we divided 11 data mining algorithms into\nfive groups which are applied to a data set of patients clinical variables data\nwith Parkinsons Disease (PD) to study the disease progression. The data set\nincludes 22 properties of 42 people that all of our algorithms are applied to\nthis data set. The Decision Table with 0.9985 correlation coefficients has the\nbest accuracy and Decision Stump with 0.7919 correlation coefficients has the\nlowest accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2013 20:27:20 GMT"}], "update_date": "2013-12-10", "authors_parsed": [["Mohammadi", "Peyman", ""], ["Hatamlou", "Abdolreza", ""], ["Masdari", "Mohammad", ""]]}, {"id": "1312.2353", "submitter": "Davide Martinenghi", "authors": "Davide Martinenghi", "title": "On the difference between checking integrity constraints before or after\n  updates", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrity checking is a crucial issue, as databases change their instance all\nthe time and therefore need to be checked continuously and rapidly. Decades of\nresearch have produced a plethora of methods for checking integrity constraints\nof a database in an incremental manner. However, not much has been said about\nwhen to check integrity. In this paper, we study the differences and\nsimilarities between checking integrity before an update (a.k.a. pre-test) or\nafter (a.k.a. post-test) in order to assess the respective convenience and\nproperties.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2013 09:34:05 GMT"}], "update_date": "2013-12-10", "authors_parsed": [["Martinenghi", "Davide", ""]]}, {"id": "1312.2355", "submitter": "Davide Martinenghi", "authors": "Davide Martinenghi", "title": "On the dependency on the size of the data when chasing under conceptual\n  dependencies", "comments": "22 pages, 2 figures (one of which with 5 subfigures). arXiv admin\n  note: substantial text overlap with arXiv:1003.3139", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conceptual dependencies (CDs) are particular kinds of key dependencies (KDs)\nand inclusion dependencies (IDs) that precisely characterize relational\nschemata modeled according to the main features of the Entity-Relationship (ER)\nmodel. An instance for such a schema may be inconsistent (data violate the\ndependencies) and incomplete (data constitute a piece of correct information,\nbut not necessarily all the relevant information). While undecidable under\ngeneral KDs and IDs, query answering under incomplete data is known to be\ndecidable for CDs. The known techniques are based on the chase -- a special\ninstance, organized in levels of depth, that is a representative of all the\ninstances that satisfy the dependencies and that include the initial instance.\nAlthough the chase generally has infinite size, query answering can be\naddressed by posing the query (or a rewriting thereof) on a finite, initial\npart of the chase. Contrary to previous claims, we show that the maximum level\nof such an initial part cannot be bounded by a constant that does not depend on\nthe size of the initial instance.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2013 09:37:16 GMT"}], "update_date": "2013-12-10", "authors_parsed": [["Martinenghi", "Davide", ""]]}, {"id": "1312.2378", "submitter": "Ramachandra Rao Kurada Mr.", "authors": "Ramachandra Rao Kurada", "title": "Unsupervised classification of uncertain data objects in spatial\n  databases using computational geometry and indexing techniques", "comments": "9 pages", "journal-ref": "International Journal of Engineering Research and Applications\n  (IJERA), Vol. 2, Issue 2, Mar-Apr 2012, pp.806-814, ISSN: 2248-9622", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised classification called clustering is a process of organizing\nobjects into groups whose members are similar in some way. Clustering of\nuncertain data objects is a challenge in spatial data bases. In this paper we\nuse Probability Density Functions (PDF) to represent these uncertain data\nobjects, and apply Uncertain K-Means algorithm to generate the clusters. This\nclustering algorithm uses the Expected Distance (ED) to compute the distance\nbetween objects and cluster representatives. To further improve the performance\nof UK-Means we propose a novel technique called Voronoi Diagrams from\nComputational Geometry to prune the number of computations of ED. This\ntechnique works efficiently but results pruning overheads. In order to reduce\nthese in pruning overhead we introduce R*-tree indexing over these uncertain\ndata objects, so that it reduces the computational cost and pruning overheads.\nOur novel approach of integrating UK-Means with voronoi diagrams and R* Tree\napplied over uncertain data objects generates imposing outcome when compared\nwith the accessible methods.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2013 10:49:08 GMT"}], "update_date": "2013-12-10", "authors_parsed": [["Kurada", "Ramachandra Rao", ""]]}, {"id": "1312.2669", "submitter": "Vishwanath Hulipalled  R H", "authors": "R H Vishwanath, T V Samartha, K C Srikantaiah, K R Venugopal and L M\n  Patnaik", "title": "DRSP : Dimension Reduction For Similarity Matching And Pruning Of Time\n  Series Data Streams", "comments": "20 pages,8 figures, 6 Tables", "journal-ref": "International Journal of Data Mining & Knowledge Management\n  Process (IJDKP) Vol.3, No.6,pp.107-126, November 2013", "doi": "10.5121/ijdkp.2013.3607", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Similarity matching and join of time series data streams has gained a lot of\nrelevance in today's world that has large streaming data. This process finds\nwide scale application in the areas of location tracking, sensor networks,\nobject positioning and monitoring to name a few. However, as the size of the\ndata stream increases, the cost involved to retain all the data in order to aid\nthe process of similarity matching also increases. We develop a novel framework\nto addresses the following objectives. Firstly, Dimension reduction is\nperformed in the preprocessing stage, where large stream data is segmented and\nreduced into a compact representation such that it retains all the crucial\ninformation by a technique called Multi-level Segment Means (MSM). This reduces\nthe space complexity associated with the storage of large time-series data\nstreams. Secondly, it incorporates effective Similarity Matching technique to\nanalyze if the new data objects are symmetric to the existing data stream. And\nfinally, the Pruning Technique that filters out the pseudo data object pairs\nand join only the relevant pairs. The computational cost for MSM is O(l*ni) and\nthe cost for pruning is O(DRF*wsize*d), where DRF is the Dimension Reduction\nFactor. We have performed exhaustive experimental trials to show that the\nproposed framework is both efficient and competent in comparison with earlier\nworks.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2013 05:14:08 GMT"}], "update_date": "2013-12-11", "authors_parsed": [["Vishwanath", "R H", ""], ["Samartha", "T V", ""], ["Srikantaiah", "K C", ""], ["Venugopal", "K R", ""], ["Patnaik", "L M", ""]]}, {"id": "1312.2678", "submitter": "Hanumanth Sastry Sistla", "authors": "S.Hanumanth Sastry and Prof.M.S.Prasada Babu", "title": "Analysis & Prediction of Sales Data in SAP-ERP System using Clustering\n  Algorithms", "comments": "AIRCC-IJCSITY Journal Publication.\n  http://airccse.org/journal/ijcsity/Paper.html", "journal-ref": "International Journal of Computational Science and Information\n  Technology (IJCSITY) Vol.1, No.4, November 2013", "doi": "10.5121/ijcsity.2013.1407", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is an important data mining technique where we will be interested\nin maximizing intracluster distance and also minimizing intercluster distance.\nWe have utilized clustering techniques for detecting deviation in product sales\nand also to identify and compare sales over a particular period of time.\nClustering is suited to group items that seem to fall naturally together, when\nthere is no specified class for any new item. We have utilizedannual sales data\nof a steel major to analyze Sales Volume & Value with respect to dependent\nattributes like products, customers and quantities sold. The demand for steel\nproducts is cyclical and depends on many factors like customer profile,\nprice,Discounts and tax issues. In this paper, we have analyzed sales data with\nclustering algorithms like K-Means&EMwhichrevealed many interesting\npatternsuseful for improving sales revenue and achieving higher sales volume.\nOur study confirms that partition methods like K-Means & EM algorithms are\nbetter suited to analyze our sales data in comparison to Density based methods\nlike DBSCAN & OPTICS or Hierarchical methods like COBWEB.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2013 05:58:43 GMT"}], "update_date": "2013-12-11", "authors_parsed": [["Sastry", "S. Hanumanth", ""], ["Babu", "Prof. M. S. Prasada", ""]]}, {"id": "1312.2738", "submitter": "Bojian Xu", "authors": "Atalay Mert \\.Ileri and M. O\\u{g}uzhan K\\\"ulekci and Bojian Xu", "title": "Shortest Unique Substring Query Revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the problem of finding shortest unique substring (SUS) proposed\nrecently by [6]. We propose an optimal $O(n)$ time and space algorithm that can\nfind an SUS for every location of a string of size $n$. Our algorithm\nsignificantly improves the $O(n^2)$ time complexity needed by [6]. We also\nsupport finding all the SUSes covering every location, whereas the solution in\n[6] can find only one SUS for every location. Further, our solution is simpler\nand easier to implement and can also be more space efficient in practice, since\nwe only use the inverse suffix array and longest common prefix array of the\nstring, while the algorithm in [6] uses the suffix tree of the string and other\nauxiliary data structures. Our theoretical results are validated by an\nempirical study that shows our algorithm is much faster and more space-saving\nthan the one in [6].\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2013 10:06:13 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2013 01:37:52 GMT"}, {"version": "v3", "created": "Fri, 10 Jan 2014 23:10:22 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["\u0130leri", "Atalay Mert", ""], ["K\u00fclekci", "M. O\u011fuzhan", ""], ["Xu", "Bojian", ""]]}, {"id": "1312.2919", "submitter": "Daniel Zinn", "authors": "Daniel Zinn and Todd J Green and Bertram Lud\\\"ascher", "title": "Win-Move is Coordination-Free (Sometimes)", "comments": "Proceedings of the 15th International Conference on Database Theory.\n  Pages 99-113. March 26-30, 2012, Berlin, Germany", "journal-ref": null, "doi": "10.1145/2274576.2274588", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent paper by Hellerstein [15], a tight relationship was conjectured\nbetween the number of strata of a Datalog${}^\\neg$ program and the number of\n\"coordination stages\" required for its distributed computation. Indeed, Ameloot\net al. [9] showed that a query can be computed by a coordination-free\nrelational transducer network iff it is monotone, thus answering in the\naffirmative a variant of Hellerstein's CALM conjecture, based on a particular\ndefinition of coordination-free computation. In this paper, we present three\nadditional models for declarative networking. In these variants, relational\ntransducers have limited access to the way data is distributed. This variation\nallows transducer networks to compute more queries in a coordination-free\nmanner: e.g., a transducer can check whether a ground atom $A$ over the input\nschema is in the \"scope\" of the local node, and then send either $A$ or $\\neg\nA$ to other nodes.\n  We show the surprising result that the query given by the well-founded\nsemantics of the unstratifiable win-move program is coordination-free in some\nof the models we consider. We also show that the original transducer network\nmodel [9] and our variants form a strict hierarchy of classes of\ncoordination-free queries. Finally, we identify different syntactic fragments\nof Datalog${}^{\\neg\\neg}_{\\forall}$, called semi-monotone programs, which can\nbe used as declarative network programming languages, whose distributed\ncomputation is guaranteed to be eventually consistent and coordination-free.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2013 19:27:41 GMT"}], "update_date": "2013-12-11", "authors_parsed": [["Zinn", "Daniel", ""], ["Green", "Todd J", ""], ["Lud\u00e4scher", "Bertram", ""]]}, {"id": "1312.2990", "submitter": "Angelos Vasilakopoulos", "authors": "Foto N. Afrati, Dimitris Fotakis, Angelos Vasilakopoulos", "title": "Efficient Lineage for SUM Aggregate Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AI systems typically make decisions and find patterns in data based on the\ncomputation of aggregate and specifically sum functions, expressed as queries,\non data's attributes. This computation can become costly or even inefficient\nwhen these queries concern the whole or big parts of the data and especially\nwhen we are dealing with big data. New types of intelligent analytics require\nalso the explanation of why something happened. In this paper we present a\nrandomised algorithm that constructs a small summary of the data, called\nAggregate Lineage, which can approximate well and explain all sums with large\nvalues in time that depends only on its size. The size of Aggregate Lineage is\npractically independent on the size of the original data. Our algorithm does\nnot assume any knowledge on the set of sum queries to be approximated.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2013 22:48:02 GMT"}, {"version": "v2", "created": "Mon, 9 Jun 2014 21:56:49 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["Afrati", "Foto N.", ""], ["Fotakis", "Dimitris", ""], ["Vasilakopoulos", "Angelos", ""]]}, {"id": "1312.3248", "submitter": "Antoine Amarilli", "authors": "Antoine Amarilli, Yael Amsterdamer, Tova Milo", "title": "On the Complexity of Mining Itemsets from the Crowd Using Taxonomies", "comments": "18 pages, 2 figures. To be published to ICDT'13. Added missing\n  acknowledgement", "journal-ref": null, "doi": "10.5441/002/icdt.2014.06", "report-no": null, "categories": "cs.DB cs.CC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of frequent itemset mining in domains where data is not\nrecorded in a conventional database but only exists in human knowledge. We\nprovide examples of such scenarios, and present a crowdsourcing model for them.\nThe model uses the crowd as an oracle to find out whether an itemset is\nfrequent or not, and relies on a known taxonomy of the item domain to guide the\nsearch for frequent itemsets. In the spirit of data mining with oracles, we\nanalyze the complexity of this problem in terms of (i) crowd complexity, that\nmeasures the number of crowd questions required to identify the frequent\nitemsets; and (ii) computational complexity, that measures the computational\neffort required to choose the questions. We provide lower and upper complexity\nbounds in terms of the size and structure of the input taxonomy, as well as the\nsize of a concise description of the output itemsets. We also provide\nconstructive algorithms that achieve the upper bounds, and consider more\nefficient variants for practical situations.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2013 17:15:39 GMT"}, {"version": "v2", "created": "Mon, 16 Dec 2013 11:50:11 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Amarilli", "Antoine", ""], ["Amsterdamer", "Yael", ""], ["Milo", "Tova", ""]]}, {"id": "1312.3913", "submitter": "Xi He", "authors": "Xi He and Ashwin Machanavajjhala and Bolin Ding", "title": "Blowfish Privacy: Tuning Privacy-Utility Trade-offs using Policies", "comments": "Full version of the paper at SIGMOD'14 Snowbird, Utah USA", "journal-ref": null, "doi": "10.1145/2588555.2588581", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy definitions provide ways for trading-off the privacy of individuals\nin a statistical database for the utility of downstream analysis of the data.\nIn this paper, we present Blowfish, a class of privacy definitions inspired by\nthe Pufferfish framework, that provides a rich interface for this trade-off. In\nparticular, we allow data publishers to extend differential privacy using a\npolicy, which specifies (a) secrets, or information that must be kept secret,\nand (b) constraints that may be known about the data. While the secret\nspecification allows increased utility by lessening protection for certain\nindividual properties, the constraint specification provides added protection\nagainst an adversary who knows correlations in the data (arising from\nconstraints). We formalize policies and present novel algorithms that can\nhandle general specifications of sensitive information and certain count\nconstraints. We show that there are reasonable policies under which our privacy\nmechanisms for k-means clustering, histograms and range queries introduce\nsignificantly lesser noise than their differentially private counterparts. We\nquantify the privacy-utility trade-offs for various policies analytically and\nempirically on real datasets.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2013 19:23:12 GMT"}, {"version": "v2", "created": "Sat, 28 Dec 2013 06:49:22 GMT"}, {"version": "v3", "created": "Tue, 11 Feb 2014 15:55:15 GMT"}, {"version": "v4", "created": "Tue, 8 Apr 2014 16:13:26 GMT"}, {"version": "v5", "created": "Mon, 23 Jun 2014 05:09:12 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["He", "Xi", ""], ["Machanavajjhala", "Ashwin", ""], ["Ding", "Bolin", ""]]}, {"id": "1312.4012", "submitter": "Arvind Arasu", "authors": "Arvind Arasu and Raghav Kaushik", "title": "Oblivious Query Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by cloud security concerns, there is an increasing interest in\ndatabase systems that can store and support queries over encrypted data. A\ncommon architecture for such systems is to use a trusted component such as a\ncryptographic co-processor for query processing that is used to securely\ndecrypt data and perform computations in plaintext. The trusted component has\nlimited memory, so most of the (input and intermediate) data is kept encrypted\nin an untrusted storage and moved to the trusted component on ``demand.''\n  In this setting, even with strong encryption, the data access pattern from\nuntrusted storage has the potential to reveal sensitive information; indeed,\nall existing systems that use a trusted component for query processing over\nencrypted data have this vulnerability. In this paper, we undertake the first\nformal study of secure query processing, where an adversary having full\nknowledge of the query (text) and observing the query execution learns nothing\nabout the underlying database other than the result size of the query on the\ndatabase. We introduce a simpler notion, oblivious query processing, and show\nformally that a query admits secure query processing iff it admits oblivious\nquery processing. We present oblivious query processing algorithms for a rich\nclass of database queries involving selections, joins, grouping and\naggregation. For queries not handled by our algorithms, we provide some initial\nevidence that designing oblivious (and therefore secure) algorithms would be\nhard via reductions from two simple, well-studied problems that are generally\nbelieved to be hard. Our study of oblivious query processing also reveals\ninteresting connections to database join theory.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2013 07:14:09 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["Arasu", "Arvind", ""], ["Kaushik", "Raghav", ""]]}, {"id": "1312.4125", "submitter": "Sudeepa Roy", "authors": "Paul Beame, Jerry Li, Sudeepa Roy, Dan Suciu", "title": "Model Counting of Query Expressions: Limitations of Propositional\n  Methods", "comments": "To appear in International Conference on Database Theory (ICDT) 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query evaluation in tuple-independent probabilistic databases is the problem\nof computing the probability of an answer to a query given independent\nprobabilities of the individual tuples in a database instance. There are two\nmain approaches to this problem: (1) in `grounded inference' one first obtains\nthe lineage for the query and database instance as a Boolean formula, then\nperforms weighted model counting on the lineage (i.e., computes the probability\nof the lineage given probabilities of its independent Boolean variables); (2)\nin methods known as `lifted inference' or `extensional query evaluation', one\nexploits the high-level structure of the query as a first-order formula.\nAlthough it is widely believed that lifted inference is strictly more powerful\nthan grounded inference on the lineage alone, no formal separation has\npreviously been shown for query evaluation. In this paper we show such a formal\nseparation for the first time.\n  We exhibit a class of queries for which model counting can be done in\npolynomial time using extensional query evaluation, whereas the algorithms used\nin state-of-the-art exact model counters on their lineages provably require\nexponential time. Our lower bounds on the running times of these exact model\ncounters follow from new exponential size lower bounds on the kinds of d-DNNF\nrepresentations of the lineages that these model counters (either explicitly or\nimplicitly) produce. Though some of these queries have been studied before, no\nnon-trivial lower bounds on the sizes of these representations for these\nqueries were previously known.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2013 09:06:32 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["Beame", "Paul", ""], ["Li", "Jerry", ""], ["Roy", "Sudeepa", ""], ["Suciu", "Dan", ""]]}, {"id": "1312.4283", "submitter": "Siddharth Barman", "authors": "Yeye He, Siddharth Barman and Jeffrey F. Naughton", "title": "On Load Shedding in Complex Event Processing", "comments": "The conference version of this work to appear in the International\n  Conference on Database Theory (ICDT), 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex Event Processing (CEP) is a stream processing model that focuses on\ndetecting event patterns in continuous event streams. While the CEP model has\ngained popularity in the research communities and commercial technologies, the\nproblem of gracefully degrading performance under heavy load in the presence of\nresource constraints, or load shedding, has been largely overlooked. CEP is\nsimilar to \"classical\" stream data management, but addresses a substantially\ndifferent class of queries. This unfortunately renders the load shedding\nalgorithms developed for stream data processing inapplicable. In this paper we\nstudy CEP load shedding under various resource constraints. We formalize broad\nclasses of CEP load-shedding scenarios as different optimization problems. We\ndemonstrate an array of complexity results that reveal the hardness of these\nproblems and construct shedding algorithms with performance guarantees. Our\nresults shed some light on the difficulty of developing load-shedding\nalgorithms that maximize utility.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2013 10:03:03 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["He", "Yeye", ""], ["Barman", "Siddharth", ""], ["Naughton", "Jeffrey F.", ""]]}, {"id": "1312.4477", "submitter": "Ghazi Al-Naymat", "authors": "Ghazi Al-Naymat", "title": "GCG: Mining Maximal Complete Graph Patterns from Large Spatial Data", "comments": "11", "journal-ref": "International Conference on Computer Systems and Applications\n  (AICCSA), pp.1,8. Fes, Morocco.27-30 May 2013", "doi": "10.1109/AICCSA.2013.6616417", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Recent research on pattern discovery has progressed from mining frequent\npatterns and sequences to mining structured patterns, such as trees and graphs.\nGraphs as general data structure can model complex relations among data with\nwide applications in web exploration and social networks. However, the process\nof mining large graph patterns is a challenge due to the existence of large\nnumber of subgraphs. In this paper, we aim to mine only frequent complete graph\npatterns. A graph g in a database is complete if every pair of distinct\nvertices is connected by a unique edge. Grid Complete Graph (GCG) is a mining\nalgorithm developed to explore interesting pruning techniques to extract\nmaximal complete graphs from large spatial dataset existing in Sloan Digital\nSky Survey (SDSS) data. Using a divide and conquer strategy, GCG shows high\nefficiency especially in the presence of large number of patterns. In this\npaper, we describe GCG that can mine not only simple co-location spatial\npatterns but also complex ones. To the best of our knowledge, this is the first\nalgorithm used to exploit the extraction of maximal complete graphs in the\nprocess of mining complex co-location patterns in large spatial dataset.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2013 15:00:50 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["Al-Naymat", "Ghazi", ""]]}, {"id": "1312.4678", "submitter": "Djamal Belazzougui", "authors": "Ibrahim Chegrane and Djamal Belazzougui", "title": "Simple, compact and robust approximate string dictionary", "comments": "Accepted to a journal (19 pages, 2 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with practical implementations of approximate string\ndictionaries that allow edit errors. In this problem, we have as input a\ndictionary $D$ of $d$ strings of total length $n$ over an alphabet of size\n$\\sigma$. Given a bound $k$ and a pattern $x$ of length $m$, a query has to\nreturn all the strings of the dictionary which are at edit distance at most $k$\nfrom $x$, where the edit distance between two strings $x$ and $y$ is defined as\nthe minimum-cost sequence of edit operations that transform $x$ into $y$. The\ncost of a sequence of operations is defined as the sum of the costs of the\noperations involved in the sequence. In this paper, we assume that each of\nthese operations has unit cost and consider only three operations: deletion of\none character, insertion of one character and substitution of a character by\nanother. We present a practical implementation of the data structure we\nrecently proposed and which works only for one error. We extend the scheme to\n$2\\leq k<m$. Our implementation has many desirable properties: it has a very\nfast and space-efficient building algorithm. The dictionary data structure is\ncompact and has fast and robust query time. Finally our data structure is\nsimple to implement as it only uses basic techniques from the literature,\nmainly hashing (linear probing and hash signatures) and succinct data\nstructures (bitvectors supporting rank queries).\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2013 07:54:49 GMT"}, {"version": "v2", "created": "Sat, 23 Aug 2014 01:50:57 GMT"}], "update_date": "2014-08-26", "authors_parsed": [["Chegrane", "Ibrahim", ""], ["Belazzougui", "Djamal", ""]]}, {"id": "1312.4800", "submitter": "Thabet Slimani", "authors": "Thabet Slimani", "title": "New Approach to Optimize the Time of Association Rules Extraction", "comments": "10 pages, 6 figures", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 10,\n  Issue 5, No 1, September 2013", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The knowledge discovery algorithms have become ineffective at the abundance\nof data and the need for fast algorithms or optimizing methods is required. To\naddress this limitation, the objective of this work is to adapt a new method\nfor optimizing the time of association rules extractions from large databases.\nIndeed, given a relational database (one relation) represented as a set of\ntuples, also called set of attributes, we transform the original database as a\nbinary table (Bitmap table) containing binary numbers. Then, we use this Bitmap\ntable to construct a data structure called Peano Tree stored as a binary file\non which we apply a new algorithm called BF-ARM (extension of the well known\nApriori algorithm). Since the database is loaded into a binary file, our\nproposed algorithm will traverse this file, and the processes of association\nrules extractions will be based on the file stored on disk. The BF-ARM\nalgorithm is implemented and compared with Apriori, Apriori+ and RS-Rules+\nalgorithms. The evaluation process is based on three benchmarks (Mushroom, Car\nEvaluation and Adult). Our preliminary experimental results showed that our\nalgorithm produces association rules with a minimum time compared to other\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2013 14:30:38 GMT"}], "update_date": "2013-12-18", "authors_parsed": [["Slimani", "Thabet", ""]]}, {"id": "1312.4833", "submitter": "EPTCS", "authors": "Ryo Iwase (Osaka University), Yasunori Ishihara (Osaka University),\n  Toru Fujiwara (Osaka University)", "title": "Toward Security Verification against Inference Attacks on Data Trees", "comments": "In Proceedings TTATT 2013, arXiv:1311.5058", "journal-ref": "EPTCS 134, 2013, pp. 49-59", "doi": "10.4204/EPTCS.134.6", "report-no": null, "categories": "cs.CR cs.DB cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our ongoing work on security verification against\ninference attacks on data trees. We focus on infinite secrecy against inference\nattacks, which means that attackers cannot narrow down the candidates for the\nvalue of the sensitive information to finite by available information to the\nattackers. Our purpose is to propose a model under which infinite secrecy is\ndecidable. To be specific, we first propose tree transducers which are\nexpressive enough to represent practical queries. Then, in order to represent\nattackers' knowledge, we propose data tree types such that type inference and\ninverse type inference on those tree transducers are possible with respect to\ndata tree types, and infiniteness of data tree types is decidable.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2013 21:01:12 GMT"}], "update_date": "2013-12-18", "authors_parsed": [["Iwase", "Ryo", "", "Osaka University"], ["Ishihara", "Yasunori", "", "Osaka University"], ["Fujiwara", "Toru", "", "Osaka University"]]}, {"id": "1312.5148", "submitter": "Xiaolu Lu", "authors": "Xiaolu Lu, Dongxu Li, Xiang Li, Ling Feng", "title": "Object Selection under Team Context", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context-aware database has drawn increasing attention from both industry and\nacademia recently by taking users' current situation and environment into\nconsideration. However, most of the literature focus on individual context,\noverlooking the team users. In this paper, we investigate how to integrate team\ncontext into database query process to help the users' get top-ranked database\ntuples and make the team more competitive. We introduce naive and optimized\nquery algorithm to select the suitable records and show that they output the\nsame results while the latter is more computational efficient. Extensive\nempirical studies are conducted to evaluate the query approaches and\ndemonstrate their effectiveness and efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2013 14:17:09 GMT"}, {"version": "v2", "created": "Mon, 14 Apr 2014 05:08:00 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Lu", "Xiaolu", ""], ["Li", "Dongxu", ""], ["Li", "Xiang", ""], ["Feng", "Ling", ""]]}, {"id": "1312.5912", "submitter": "Andrea Cal\\`i PhD", "authors": "Andrea Cal\\`i and Riccardo Torlone", "title": "Containment of Schema Mappings for Data Exchange (Preliminary Report)", "comments": "11 pages, no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In data exchange, data are materialised from a source schema to a target\nschema, according to suitable source-to-target constraints. Constraints are\nalso expressed on the target schema to represent the domain of interest. A\nschema mapping is the union of the source-to-target and of the target\nconstraints.\n  In this paper, we address the problem of containment of schema mappings for\ndata exchange, which has been recently proposed in this framework as a step\ntowards the optimization of data exchange settings. We refer to a natural\nnotion of containment that relies on the behaviour of schema mappings with\nrespect to conjunctive query answering, in the presence of so-called LAV TGDs\nas target constraints. Our contribution is a practical technique for testing\nthe containment based on the existence of a homomorphism between special\n\"dummy\" instances, which can be easily built from schema mappings.\n  We argue that containment of schema mappings is decidable for most practical\ncases, and we set the basis for further investigations in the topic. This paper\nextends our preliminary results.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 12:13:11 GMT"}, {"version": "v2", "created": "Tue, 31 Dec 2013 09:51:14 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Cal\u00ec", "Andrea", ""], ["Torlone", "Riccardo", ""]]}, {"id": "1312.5914", "submitter": "Andrea Cal\\`i PhD", "authors": "Andrea Cal\\`i, Marco Console, Riccardo Frosini", "title": "Deep Separability of Ontological Constraints", "comments": "14 pages, no figures. arXiv admin note: text overlap with\n  arXiv:1112.0343 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When data schemata are enriched with expressive constraints that aim at\nrepresenting the domain of interest, in order to answer queries one needs to\nconsider the logical theory consisting of both the data and the constraints.\nQuery answering in such a context is called ontological query answering.\nCommonly adopted database constraints in this field are tuple-generating\ndependencies (TGDs) and equality-generating dependencies (EGDs). It is well\nknown that their interaction leads to intractability or undecidability of query\nanswering even in the case of simple subclasses. Several conditions have been\nfound to guarantee separability, that is lack of interaction, between TGDs and\nEGDs. Separability makes EGDs (mostly) irrelevant for query answering and\ntherefore often guarantees tractability, as long as the theory is satisfiable.\nIn this paper we review the two notions of separability found in the\nliterature, as well as several syntactic conditions that are sufficient to\nprove them. We then shed light on the issue of satisfiability checking, showing\nthat under a sufficient condition called deep separability it can be done by\nconsidering the TGDs only.\n  We show that, fortunately, in the case of TGDs and EGDs, separability implies\ndeep separability. This result generalizes several analogous ones, proved ad\nhoc for particular classes of constraints. Applications include the class of\nsticky TGDs and EGDs, for which we provide a syntactic separability condition\nwhich extends the analogous one for linear TGDs; preliminary experiments show\nthe feasibility of query answering in this case.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 12:18:29 GMT"}, {"version": "v2", "created": "Tue, 31 Dec 2013 09:42:10 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Cal\u00ec", "Andrea", ""], ["Console", "Marco", ""], ["Frosini", "Riccardo", ""]]}, {"id": "1312.6293", "submitter": "Jerome Darmont", "authors": "Jaume Ferrarons (ERIC), Mulu Adhana (ERIC), Carlos Colmenares (ERIC),\n  Sandra Pietrowska (ERIC), Fadila Bentayeb (ERIC), J\\'er\\^ome Darmont (ERIC)", "title": "PRIMEBALL: a Parallel Processing Framework Benchmark for Big Data\n  Applications in the Cloud", "comments": "5th TPC Technology Conference on Performance Evaluation and\n  Benchmarking (VLDB/TPCTC 13), Riva del Garda : Italy (2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we draw the specifications of a novel benchmark for comparing\nparallel processing frameworks in the context of big data applications hosted\nin the cloud. We aim at filling several gaps in already existing cloud data\nprocessing benchmarks, which lack a real-life context for their processes, thus\nlosing relevance when trying to assess performance for real applications.\nHence, we propose a fictitious news site hosted in the cloud that is to be\nmanaged by the framework under analysis, together with several objective use\ncase scenarios and measures for evaluating system performance. The main\nstrengths of our benchmark are parallelization capabilities supporting cloud\nfeatures and big data properties.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2013 19:15:22 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Ferrarons", "Jaume", "", "ERIC"], ["Adhana", "Mulu", "", "ERIC"], ["Colmenares", "Carlos", "", "ERIC"], ["Pietrowska", "Sandra", "", "ERIC"], ["Bentayeb", "Fadila", "", "ERIC"], ["Darmont", "J\u00e9r\u00f4me", "", "ERIC"]]}, {"id": "1312.6675", "submitter": "Martin Atzmueller", "authors": "Martin Atzmueller", "title": "Data Mining on Social Interaction Networks", "comments": "minor rev/corrections; enlarged figures, typos, commas, updated the\n  \"in press\"/\"to appear\" entries in references", "journal-ref": "Journal of Data Mining & Digital Humanities, 2014 (June 24, 2014)\n  jdmdh:11", "doi": "10.46298/jdmdh.7", "report-no": null, "categories": "cs.SI cs.DB physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media and social networks have already woven themselves into the very\nfabric of everyday life. This results in a dramatic increase of social data\ncapturing various relations between the users and their associated artifacts,\nboth in online networks and the real world using ubiquitous devices. In this\nwork, we consider social interaction networks from a data mining perspective -\nalso with a special focus on real-world face-to-face contact networks: We\ncombine data mining and social network analysis techniques for examining the\nnetworks in order to improve our understanding of the data, the modeled\nbehavior, and its underlying emergent processes. Furthermore, we adapt, extend\nand apply known predictive data mining algorithms on social interaction\nnetworks. Additionally, we present novel methods for descriptive data mining\nfor uncovering and extracting relations and patterns for hypothesis generation\nand exploration, in order to provide characteristic information about the data\nand networks. The presented approaches and methods aim at extracting valuable\nknowledge for enhancing the understanding of the respective data, and for\nsupporting the users of the respective systems. We consider data from several\nsocial systems, like the social bookmarking system BibSonomy, the social\nresource sharing system flickr, and ubiquitous social systems: Specifically, we\nfocus on data from the social conference guidance system Conferator and the\nsocial group interaction system MyGroup. This work first gives a short\nintroduction into social interaction networks, before we describe several\nanalysis results in the context of online social networks and real-world\nface-to-face contact networks. Next, we present predictive data mining methods,\ni.e., for localization, recommendation and link prediction. After that, we\npresent novel descriptive data mining methods for mining communities and\npatterns.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2013 20:54:34 GMT"}, {"version": "v2", "created": "Sat, 15 Mar 2014 23:14:29 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Atzmueller", "Martin", ""]]}, {"id": "1312.7373", "submitter": "Leopoldo Bertossi", "authors": "Mostafa Milani, Leopoldo Bertossi, Sina Ariyan", "title": "Extending Contexts with Ontologies for Multidimensional Data Quality\n  Assessment", "comments": "To appear in Proc. 5th International Workshop on Data Engineering\n  meets the Semantic Web (DESWeb). In conjunction with ICDE 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data quality and data cleaning are context dependent activities. Starting\nfrom this observation, in previous work a context model for the assessment of\nthe quality of a database instance was proposed. In that framework, the context\ntakes the form of a possibly virtual database or data integration system into\nwhich a database instance under quality assessment is mapped, for additional\nanalysis and processing, enabling quality assessment. In this work we extend\ncontexts with dimensions, and by doing so, we make possible a multidimensional\nassessment of data quality assessment. Multidimensional contexts are\nrepresented as ontologies written in Datalog+-. We use this language for\nrepresenting dimensional constraints, and dimensional rules, and also for doing\nquery answering based on dimensional navigation, which becomes an important\nauxiliary activity in the assessment of data. We show ideas and mechanisms by\nmeans of examples.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2013 23:59:54 GMT"}, {"version": "v2", "created": "Sat, 4 Jan 2014 05:47:36 GMT"}, {"version": "v3", "created": "Mon, 20 Jan 2014 22:54:20 GMT"}], "update_date": "2014-01-22", "authors_parsed": [["Milani", "Mostafa", ""], ["Bertossi", "Leopoldo", ""], ["Ariyan", "Sina", ""]]}, {"id": "1312.7542", "submitter": "Daniel Ritter", "authors": "Daniel Ritter", "title": "Towards Connected Enterprises: The Business Network System", "comments": "10 pages, 15. GI-Fachtagung Datenbanksysteme f\\\"ur Business,\n  Technologie und Web (BTW): Data Management in the Cloud (DMC), Magdeburg,\n  2013. arXiv admin note: text overlap with arXiv:1312.7436", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discovery, representation and reconstruction of Business Networks (BN)\nfrom Network Mining (NM) raw data is a difficult problem for enterprises. This\nis due to huge amounts of complex business processes within and across\nenterprise boundaries, heterogeneous technology stacks, and fragmented data. To\nremain competitive, visibility into the enterprise and partner networks on\ndifferent, interrelated abstraction levels is desirable. We present a novel\ndata discovery, mining and network inference system, called Business Network\nSystem (BNS), that reconstructs the BN--integration and business process\nnetworks--from raw data, hidden in the enterprises' landscapes. BNS provides a\nnew, declarative foundation for gathering information, defining a network\nmodel, inferring the network and check its conformance to the real-world\n\"as-is\" network. The paper covers both the foundation and the key features of\nBNS, including its underlying technologies, its overall system architecture,\nand its most interesting capabilities.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2013 14:38:24 GMT"}], "update_date": "2013-12-31", "authors_parsed": [["Ritter", "Daniel", ""]]}]