[{"id": "1508.00192", "submitter": "Ling Chen", "authors": "Ling Chen, Ting Yu, Rada Chirkova", "title": "WaveCluster with Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  WaveCluster is an important family of grid-based clustering algorithms that\nare capable of finding clusters of arbitrary shapes. In this paper, we\ninvestigate techniques to perform WaveCluster while ensuring differential\nprivacy. Our goal is to develop a general technique for achieving differential\nprivacy on WaveCluster that accommodates different wavelet transforms. We show\nthat straightforward techniques based on synthetic data generation and\nintroduction of random noise when quantizing the data, though generally\npreserving the distribution of data, often introduce too much noise to preserve\nuseful clusters. We then propose two optimized techniques, PrivTHR and\nPrivTHREM, which can significantly reduce data distortion during two key steps\nof WaveCluster: the quantization step and the significant grid identification\nstep. We conduct extensive experiments based on four datasets that are\nparticularly interesting in the context of clustering, and show that PrivTHR\nand PrivTHREM achieve high utility when privacy budgets are properly allocated.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2015 04:41:51 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Chen", "Ling", ""], ["Yu", "Ting", ""], ["Chirkova", "Rada", ""]]}, {"id": "1508.00703", "submitter": "Naman Goel", "authors": "Naman Goel, Divyakant Agrawal, Sanjay Chawla, Ahmed Elmagarmid", "title": "Parameter Database : Data-centric Synchronization for Scalable Machine\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": "QCRI-TR-2015-003", "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new data-centric synchronization framework for carrying out of\nmachine learning (ML) tasks in a distributed environment. Our framework\nexploits the iterative nature of ML algorithms and relaxes the application\nagnostic bulk synchronization parallel (BSP) paradigm that has previously been\nused for distributed machine learning. Data-centric synchronization complements\nfunction-centric synchronization based on using stale updates to increase the\nthroughput of distributed ML computations. Experiments to validate our\nframework suggest that we can attain substantial improvement over BSP while\nguaranteeing sequential correctness of ML tasks.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2015 08:42:41 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Goel", "Naman", ""], ["Agrawal", "Divyakant", ""], ["Chawla", "Sanjay", ""], ["Elmagarmid", "Ahmed", ""]]}, {"id": "1508.01023", "submitter": "Bokai Cao", "authors": "Bokai Cao, Xiangnan Kong, Philip S. Yu", "title": "A review of heterogeneous data mining for brain disorders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE cs.DB q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With rapid advances in neuroimaging techniques, the research on brain\ndisorder identification has become an emerging area in the data mining\ncommunity. Brain disorder data poses many unique challenges for data mining\nresearch. For example, the raw data generated by neuroimaging experiments is in\ntensor representations, with typical characteristics of high dimensionality,\nstructural complexity and nonlinear separability. Furthermore, brain\nconnectivity networks can be constructed from the tensor data, embedding subtle\ninteractions between brain regions. Other clinical measures are usually\navailable reflecting the disease status from different perspectives. It is\nexpected that integrating complementary information in the tensor data and the\nbrain network data, and incorporating other clinical parameters will be\npotentially transformative for investigating disease mechanisms and for\ninforming therapeutic interventions. Many research efforts have been devoted to\nthis area. They have achieved great success in various applications, such as\ntensor-based modeling, subgraph pattern mining, multi-view feature analysis. In\nthis paper, we review some recent data mining methods that are used for\nanalyzing brain disorders.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 09:57:32 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Cao", "Bokai", ""], ["Kong", "Xiangnan", ""], ["Yu", "Philip S.", ""]]}, {"id": "1508.01083", "submitter": "Paolo Nesi", "authors": "Pierfrancesco Bellini, Paolo Nesi, Nadia Rauch", "title": "Ontology Bulding vs Data Harvesting and Cleaning for Smart-city Services", "comments": "DMS 2014, Distributed Multimedia Systems", "journal-ref": "Journal of Visual Languages & Computing, Vo. 25, n.6, december\n  2014", "doi": "10.1016/j.jvlc.2014.10.023", "report-no": null, "categories": "cs.DB cs.AI cs.CY", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Presently, a very large number of public and private data sets are available\naround the local governments. In most cases, they are not semantically\ninteroperable and a huge human effort is needed to create integrated ontologies\nand knowledge base for smart city. Smart City ontology is not yet standardized,\nand a lot of research work is needed to identify models that can easily support\nthe data reconciliation, the management of the complexity and reasoning. In\nthis paper, a system for data ingestion and reconciliation smart cities related\naspects as road graph, services available on the roads, traffic sensors etc.,\nis proposed. The system allows managing a big volume of data coming from a\nvariety of sources considering both static and dynamic data. These data are\nmapped to smart-city ontology and stored into an RDF-Store where they are\navailable for applications via SPARQL queries to provide new services to the\nusers. The paper presents the process adopted to produce the ontology and the\nknowledge base and the mechanisms adopted for the verification, reconciliation\nand validation. Some examples about the possible usage of the coherent\nknowledge base produced are also offered and are accessible from the RDF-Store.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 14:17:10 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Bellini", "Pierfrancesco", ""], ["Nesi", "Paolo", ""], ["Rauch", "Nadia", ""]]}, {"id": "1508.01086", "submitter": "Paolo Nesi", "authors": "Pierfrancesco Bellini, Monica Benigni, Riccardo Billero, Paolo Nesi,\n  Nadia Rauch", "title": "Km4City Ontology Building vs Data Harvesting and Cleaning for Smart-city\n  Services", "comments": null, "journal-ref": null, "doi": "10.1016/j.jvlc.2014.10.023", "report-no": null, "categories": "cs.DB cs.AI cs.CY", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Presently, a very large number of public and private data sets are available\nfrom local governments. In most cases, they are not semantically interoperable\nand a huge human effort would be needed to create integrated ontologies and\nknowledge base for smart city. Smart City ontology is not yet standardized, and\na lot of research work is needed to identify models that can easily support the\ndata reconciliation, the management of the complexity, to allow the data\nreasoning. In this paper, a system for data ingestion and reconciliation of\nsmart cities related aspects as road graph, services available on the roads,\ntraffic sensors etc., is proposed. The system allows managing a big data volume\nof data coming from a variety of sources considering both static and dynamic\ndata. These data are mapped to a smart-city ontology, called KM4City (Knowledge\nModel for City), and stored into an RDF-Store where they are available for\napplications via SPARQL queries to provide new services to the users via\nspecific applications of public administration and enterprises. The paper\npresents the process adopted to produce the ontology and the big data\narchitecture for the knowledge base feeding on the basis of open and private\ndata, and the mechanisms adopted for the data verification, reconciliation and\nvalidation. Some examples about the possible usage of the coherent big data\nknowledge base produced are also offered and are accessible from the RDF-Store\nand related services. The article also presented the work performed about\nreconciliation algorithms and their comparative assessment and selection.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 14:24:23 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Bellini", "Pierfrancesco", ""], ["Benigni", "Monica", ""], ["Billero", "Riccardo", ""], ["Nesi", "Paolo", ""], ["Rauch", "Nadia", ""]]}, {"id": "1508.01171", "submitter": "Shantanu Sharma", "authors": "Foto Afrati, Shlomi Dolev, Shantanu Sharma, Jeffrey D. Ullman", "title": "Meta-MapReduce: A Technique for Reducing Communication in MapReduce\n  Computations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MapReduce has proven to be one of the most useful paradigms in the revolution\nof distributed computing, where cloud services and cluster computing become the\nstandard venue for computing. The federation of cloud and big data activities\nis the next challenge where MapReduce should be modified to avoid (big) data\nmigration across remote (cloud) sites. This is exactly our scope of research,\nwhere only the very essential data for obtaining the result is transmitted,\nreducing communication, processing and preserving data privacy as much as\npossible. In this work, we propose an algorithmic technique for MapReduce\nalgorithms, called Meta-MapReduce, that decreases the communication cost by\nallowing us to process and move metadata to clouds and from the map phase to\nreduce phase. In Meta-MapReduce, the reduce phase fetches only the required\ndata at required iterations, which in turn, assists in preserving the data\nprivacy.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 18:54:32 GMT"}, {"version": "v2", "created": "Thu, 28 Jul 2016 16:48:26 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Afrati", "Foto", ""], ["Dolev", "Shlomi", ""], ["Sharma", "Shantanu", ""], ["Ullman", "Jeffrey D.", ""]]}, {"id": "1508.01239", "submitter": "Manas Joglekar", "authors": "Manas Joglekar, Christopher Re", "title": "It's all a matter of degree: Using degree information to optimize\n  multiway joins", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We optimize multiway equijoins on relational tables using degree information.\nWe give a new bound that uses degree information to more tightly bound the\nmaximum output size of a query. On real data, our bound on the number of\ntriangles in a social network can be up to $95$ times tighter than existing\nworst case bounds. We show that using only a constant amount of degree\ninformation, we are able to obtain join algorithms with a running time that has\na smaller exponent than existing algorithms--{\\em for any database instance}.\nWe also show that this degree information can be obtained in nearly linear\ntime, which yields asymptotically faster algorithms in the serial setting and\nlower communication algorithms in the MapReduce setting.\n  In the serial setting, the data complexity of join processing can be\nexpressed as a function $O(\\IN^x + \\OUT)$ in terms of input size $\\IN$ and\noutput size $\\OUT$ in which $x$ depends on the query. An upper bound for $x$ is\ngiven by fractional hypertreewidth. We are interested in situations in which we\ncan get algorithms for which $x$ is strictly smaller than the fractional\nhypertreewidth. We say that a join can be processed in subquadratic time if $x\n< 2$. Building on the AYZ algorithm for processing cycle joins in quadratic\ntime, for a restricted class of joins which we call $1$-series-parallel graphs,\nwe obtain a complete decision procedure for identifying subquadratic\nsolvability (subject to the $3$-SUM problem requiring quadratic time). Our\n$3$-SUM based quadratic lower bound is tight, making it the only known tight\nbound for joins that does not require any assumption about the matrix\nmultiplication exponent $\\omega$. We also give a MapReduce algorithm that meets\nour improved communication bound and handles essentially optimal parallelism.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 22:17:53 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2015 22:55:28 GMT"}, {"version": "v3", "created": "Fri, 14 Aug 2015 00:58:02 GMT"}, {"version": "v4", "created": "Mon, 17 Aug 2015 00:41:37 GMT"}, {"version": "v5", "created": "Sun, 30 Aug 2015 02:20:13 GMT"}, {"version": "v6", "created": "Sun, 25 Oct 2015 21:05:45 GMT"}, {"version": "v7", "created": "Tue, 22 Dec 2015 00:12:19 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["Joglekar", "Manas", ""], ["Re", "Christopher", ""]]}, {"id": "1508.01306", "submitter": "Michael Minock", "authors": "Michael Minock and Nils Everling", "title": "Replication and Generalization of PRECISE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report describes an initial replication study of the PRECISE system and\ndevelops a clearer, more formal description of the approach. Based on our\nevaluation, we conclude that the PRECISE results do not fully replicate.\nHowever the formalization developed here suggests a road map to further enhance\nand extend the approach pioneered by PRECISE.\n  After a long, productive discussion with Ana-Maria Popescu (one of the\nauthors of PRECISE) we got more clarity on the PRECISE approach and how the\nlexicon was authored for the GEO evaluation. Based on this we built a more\ndirect implementation over a repaired formalism. Although our new evaluation is\nnot yet complete, it is clear that the system is performing much better now. We\nwill continue developing our ideas and implementation and generate a future\nreport/publication that more accurately evaluates PRECISE like approaches.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2015 07:56:59 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2015 09:41:34 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Minock", "Michael", ""], ["Everling", "Nils", ""]]}, {"id": "1508.01340", "submitter": "Marc Boull\\'e", "authors": "Marc Boull\\'e", "title": "Universal Approximation of Edge Density in Large Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel way to summarize the structure of large\ngraphs, based on non-parametric estimation of edge density in directed\nmultigraphs. Following coclustering approach, we use a clustering of the\nvertices, with a piecewise constant estimation of the density of the edges\nacross the clusters, and address the problem of automatically and reliably\ninferring the number of clusters, which is the granularity of the coclustering.\nWe use a model selection technique with data-dependent prior and obtain an\nexact evaluation criterion for the posterior probability of edge density\nestimation models. We demonstrate, both theoretically and empirically, that our\ndata-dependent modeling technique is consistent, resilient to noise, valid non\nasymptotically and asymptotically behaves as an universal approximator of the\ntrue edge density in directed multigraphs. We evaluate our method using\nartificial graphs and present its practical interest on real world graphs. The\nmethod is both robust and scalable. It is able to extract insightful patterns\nin the unsupervised learning setting and to provide state of the art accuracy\nwhen used as a preparation step for supervised learning.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2015 09:40:28 GMT"}], "update_date": "2015-08-07", "authors_parsed": [["Boull\u00e9", "Marc", ""]]}, {"id": "1508.01447", "submitter": "Iyad AlAgha", "authors": "Iyad AlAgha", "title": "Using Linguistic Analysis to Translate Arabic Natural Language Queries\n  to SPARQL", "comments": "Journal Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The logic-based machine-understandable framework of the Semantic Web often\nchallenges naive users when they try to query ontology-based knowledge bases.\nExisting research efforts have approached this problem by introducing Natural\nLanguage (NL) interfaces to ontologies. These NL interfaces have the ability to\nconstruct SPARQL queries based on NL user queries. However, most efforts were\nrestricted to queries expressed in English, and they often benefited from the\nadvancement of English NLP tools. However, little research has been done to\nsupport querying the Arabic content on the Semantic Web by using NL queries.\nThis paper presents a domain-independent approach to translate Arabic NL\nqueries to SPARQL by leveraging linguistic analysis. Based on a special\nconsideration on Noun Phrases (NPs), our approach uses a language parser to\nextract NPs and the relations from Arabic parse trees and match them to the\nunderlying ontology. It then utilizes knowledge in the ontology to group NPs\ninto triple-based representations. A SPARQL query is finally generated by\nextracting targets and modifiers, and interpreting them into SPARQL. The\ninterpretation of advanced semantic features including negation, conjunctive\nand disjunctive modifiers is also supported. The approach was evaluated by\nusing two datasets consisting of OWL test data and queries, and the obtained\nresults have confirmed its feasibility to translate Arabic NL queries to\nSPARQL.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2015 16:10:21 GMT"}], "update_date": "2015-08-07", "authors_parsed": [["AlAgha", "Iyad", ""]]}, {"id": "1508.01951", "submitter": "Besmira Nushi", "authors": "Besmira Nushi, Adish Singla, Anja Gruenheid, Erfan Zamanian, Andreas\n  Krause, Donald Kossmann", "title": "Crowd Access Path Optimization: Diversity Matters", "comments": "10 pages, 3rd AAAI Conference on Human Computation and Crowdsourcing\n  (HCOMP 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quality assurance is one the most important challenges in crowdsourcing.\nAssigning tasks to several workers to increase quality through redundant\nanswers can be expensive if asking homogeneous sources. This limitation has\nbeen overlooked by current crowdsourcing platforms resulting therefore in\ncostly solutions. In order to achieve desirable cost-quality tradeoffs it is\nessential to apply efficient crowd access optimization techniques. Our work\nargues that optimization needs to be aware of diversity and correlation of\ninformation within groups of individuals so that crowdsourcing redundancy can\nbe adequately planned beforehand. Based on this intuitive idea, we introduce\nthe Access Path Model (APM), a novel crowd model that leverages the notion of\naccess paths as an alternative way of retrieving information. APM aggregates\nanswers ensuring high quality and meaningful confidence. Moreover, we devise a\ngreedy optimization algorithm for this model that finds a provably good\napproximate plan to access the crowd. We evaluate our approach on three\ncrowdsourced datasets that illustrate various aspects of the problem. Our\nresults show that the Access Path Model combined with greedy optimization is\ncost-efficient and practical to overcome common difficulties in large-scale\ncrowdsourcing like data sparsity and anonymity.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2015 20:36:54 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2015 07:21:57 GMT"}], "update_date": "2015-08-12", "authors_parsed": [["Nushi", "Besmira", ""], ["Singla", "Adish", ""], ["Gruenheid", "Anja", ""], ["Zamanian", "Erfan", ""], ["Krause", "Andreas", ""], ["Kossmann", "Donald", ""]]}, {"id": "1508.02050", "submitter": "Tahani Almanie", "authors": "Tahani Almanie, Rsha Mirza and Elizabeth Lor", "title": "Crime Prediction Based On Crime Types And Using Spatial And Temporal\n  Criminal Hotspots", "comments": "19 pages, 18 figures, 7 tables", "journal-ref": "International Journal of Data Mining & Knowledge Management\n  Process (IJDKP) Vol.5, No.4, July 2015", "doi": "10.5121/ijdkp.2015.5401", "report-no": null, "categories": "cs.AI cs.CY cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on finding spatial and temporal criminal hotspots. It\nanalyses two different real-world crimes datasets for Denver, CO and Los\nAngeles, CA and provides a comparison between the two datasets through a\nstatistical analysis supported by several graphs. Then, it clarifies how we\nconducted Apriori algorithm to produce interesting frequent patterns for\ncriminal hotspots. In addition, the paper shows how we used Decision Tree\nclassifier and Naive Bayesian classifier in order to predict potential crime\ntypes. To further analyse crimes datasets, the paper introduces an analysis\nstudy by combining our findings of Denver crimes dataset with its demographics\ninformation in order to capture the factors that might affect the safety of\nneighborhoods. The results of this solution could be used to raise awareness\nregarding the dangerous locations and to help agencies to predict future crimes\nin a specific location within a particular time.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2015 17:15:56 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Almanie", "Tahani", ""], ["Mirza", "Rsha", ""], ["Lor", "Elizabeth", ""]]}, {"id": "1508.02061", "submitter": "Akhil Jabbar Meerja", "authors": "M.A.Jabbar, B.L Deekshatulu, Priti Chandra", "title": "Classification of Heart Disease Using K- Nearest Neighbor and Genetic\n  Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Data mining techniques have been widely used to mine knowledgeable\ninformation from medical data bases. In data mining classification is a\nsupervised learning that can be used to design models describing important data\nclasses, where class attribute is involved in the construction of the\nclassifier. Nearest neighbor (KNN) is very simple, most popular, highly\nefficient and effective algorithm for pattern recognition.KNN is a straight\nforward classifier, where samples are classified based on the class of their\nnearest neighbor. Medical data bases are high volume in nature. If the data set\ncontains redundant and irrelevant attributes, classification may produce less\naccurate result. Heart disease is the leading cause of death in INDIA. In\nAndhra Pradesh heart disease was the leading cause of mortality accounting for\n32%of all deaths, a rate as high as Canada (35%) and USA.Hence there is a need\nto define a decision support system that helps clinicians decide to take\nprecautionary steps. In this paper we propose a new algorithm which combines\nKNN with genetic algorithm for effective classification. Genetic algorithms\nperform global search in complex large and multimodal landscapes and provide\noptimal solution. Experimental results shows that our algorithm enhance the\naccuracy in diagnosis of heart disease.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 16:44:00 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Jabbar", "M. A.", ""], ["Deekshatulu", "B. L", ""], ["Chandra", "Priti", ""]]}, {"id": "1508.02428", "submitter": "Zhensong Qian", "authors": "Oliver Schulte and Zhensong Qian", "title": "FactorBase: SQL for Learning A Multi-Relational Graphical Model", "comments": "14 pages, 10 figures, 10 tables, Published on 2015 IEEE International\n  Conference on Data Science and Advanced Analytics (IEEE DSAA'2015), Oct\n  19-21, 2015, Paris, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe FactorBase, a new SQL-based framework that leverages a relational\ndatabase management system to support multi-relational model discovery. A\nmulti-relational statistical model provides an integrated analysis of the\nheterogeneous and interdependent data resources in the database. We adopt the\nBayesStore design philosophy: statistical models are stored and managed as\nfirst-class citizens inside a database. Whereas previous systems like\nBayesStore support multi-relational inference, FactorBase supports\nmulti-relational learning. A case study on six benchmark databases evaluates\nhow our system supports a challenging machine learning application, namely\nlearning a first-order Bayesian network model for an entire database. Model\nlearning in this setting has to examine a large number of potential statistical\nassociations across data tables. Our implementation shows how the SQL\nconstructs in FactorBase facilitate the fast, modular, and reliable development\nof highly scalable model learning systems.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2015 21:15:43 GMT"}], "update_date": "2015-08-12", "authors_parsed": [["Schulte", "Oliver", ""], ["Qian", "Zhensong", ""]]}, {"id": "1508.03116", "submitter": "Christan Grant", "authors": "Christan Grant, Daisy Zhe Wang, Michael L. Wick", "title": "Query-Driven Sampling for Collective Entity Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic databases play a preeminent role in the processing and\nmanagement of uncertain data. Recently, many database research efforts have\nintegrated probabilistic models into databases to support tasks such as\ninformation extraction and labeling. Many of these efforts are based on batch\noriented inference which inhibits a realtime workflow. One important task is\nentity resolution (ER). ER is the process of determining records (mentions) in\na database that correspond to the same real-world entity. Traditional pairwise\nER methods can lead to inconsistencies and low accuracy due to localized\ndecisions. Leading ER systems solve this problem by collectively resolving all\nrecords using a probabilistic graphical model and Markov chain Monte Carlo\n(MCMC) inference. However, for large datasets this is an extremely expensive\nprocess. One key observation is that, such exhaustive ER process incurs a huge\nup-front cost, which is wasteful in practice because most users are interested\nin only a small subset of entities. In this paper, we advocate pay-as-you-go\nentity resolution by developing a number of query-driven collective ER\ntechniques. We introduce two classes of SQL queries that involve ER operators\n--- selection-driven ER and join-driven ER. We implement novel variations of\nthe MCMC Metropolis Hastings algorithm to generate biased samples and\nselectivity-based scheduling algorithms to support the two classes of ER\nqueries. Finally, we show that query-driven ER algorithms can converge and\nreturn results within minutes over a database populated with the extraction\nfrom a newswire dataset containing 71 million mentions.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2015 04:23:58 GMT"}], "update_date": "2015-08-14", "authors_parsed": [["Grant", "Christan", ""], ["Wang", "Daisy Zhe", ""], ["Wick", "Michael L.", ""]]}, {"id": "1508.03763", "submitter": "Yodsawalai Chodpathumwan", "authors": "Yodsawalai Chodpathumwan, Arash Termehchy, Stephen A. Ramsey, Aayam\n  Shresta, Amy Glen, and Zheng Liu", "title": "Structural Generalizability: The Case of Similarity Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph similarity search algorithms usually leverage the structural properties\nof a database. Hence, these algorithms are effective only on some structural\nvariations of the data and are ineffective on other forms, which makes them\nhard to use. Ideally, one would like to design a data analytics algorithm that\nis structurally robust, i.e., it returns essentially the same accurate results\nover all possible structural variations of a dataset. We propose a novel\napproach to create a structurally robust similarity search algorithm over graph\ndatabases. We leverage the classic insight in the database literature that\nschematic variations are caused by having constraints in the database. We then\npresent RelSim algorithm which is provably structurally robust under these\nvariations. Our empirical studies show that our proposed algorithms are\nstructurally robust while being efficient and as effective as or more effective\nthan the state-of-the-art similarity search algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2015 19:07:47 GMT"}, {"version": "v2", "created": "Thu, 25 Aug 2016 08:38:06 GMT"}, {"version": "v3", "created": "Sat, 17 Dec 2016 15:34:48 GMT"}, {"version": "v4", "created": "Wed, 31 Mar 2021 17:31:06 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Chodpathumwan", "Yodsawalai", ""], ["Termehchy", "Arash", ""], ["Ramsey", "Stephen A.", ""], ["Shresta", "Aayam", ""], ["Glen", "Amy", ""], ["Liu", "Zheng", ""]]}, {"id": "1508.03843", "submitter": "Marko A. Rodriguez", "authors": "Marko A. Rodriguez", "title": "The Gremlin Graph Traversal Machine and Language", "comments": "To appear in the Proceedings of the 2015 ACM Database Programming\n  Languages Conference", "journal-ref": "ACM Proceedings of the 15th Symposium on Database Programming\n  Languages, pages 1-10, 2015", "doi": "10.1145/2815072.2815073", "report-no": null, "categories": "cs.DB cs.DM", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Gremlin is a graph traversal machine and language designed, developed, and\ndistributed by the Apache TinkerPop project. Gremlin, as a graph traversal\nmachine, is composed of three interacting components: a graph $G$, a traversal\n$\\Psi$, and a set of traversers $T$. The traversers move about the graph\naccording to the instructions specified in the traversal, where the result of\nthe computation is the ultimate locations of all halted traversers. A Gremlin\nmachine can be executed over any supporting graph computing system such as an\nOLTP graph database and/or an OLAP graph processor. Gremlin, as a graph\ntraversal language, is a functional language implemented in the user's native\nprogramming language and is used to define the $\\Psi$ of a Gremlin machine.\nThis article provides a mathematical description of Gremlin and details its\nautomaton and functional properties. These properties enable Gremlin to\nnaturally support imperative and declarative querying, host language\nagnosticism, user-defined domain specific languages, an extensible\ncompiler/optimizer, single- and multi-machine execution models, hybrid depth-\nand breadth-first evaluation, as well as the existence of a Universal Gremlin\nMachine and its respective entailments.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2015 16:30:27 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Rodriguez", "Marko A.", ""]]}, {"id": "1508.03846", "submitter": "Jose Picado", "authors": "Jose Picado, Arash Termehchy, Alan Fern, Parisa Ataei", "title": "Schema Independent Relational Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning novel concepts and relations from relational databases is an\nimportant problem with many applications in database systems and machine\nlearning. Relational learning algorithms learn the definition of a new relation\nin terms of existing relations in the database. Nevertheless, the same data set\nmay be represented under different schemas for various reasons, such as\nefficiency, data quality, and usability. Unfortunately, the output of current\nrelational learning algorithms tends to vary quite substantially over the\nchoice of schema, both in terms of learning accuracy and efficiency. This\nvariation complicates their off-the-shelf application. In this paper, we\nintroduce and formalize the property of schema independence of relational\nlearning algorithms, and study both the theoretical and empirical dependence of\nexisting algorithms on the common class of (de) composition schema\ntransformations. We study both sample-based learning algorithms, which learn\nfrom sets of labeled examples, and query-based algorithms, which learn by\nasking queries to an oracle. We prove that current relational learning\nalgorithms are generally not schema independent. For query-based learning\nalgorithms we show that the (de) composition transformations influence their\nquery complexity. We propose Castor, a sample-based relational learning\nalgorithm that achieves schema independence by leveraging data dependencies. We\nsupport the theoretical results with an empirical study that demonstrates the\nschema dependence/independence of several algorithms on existing benchmark and\nreal-world datasets under (de) compositions.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2015 16:57:20 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 20:35:32 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Picado", "Jose", ""], ["Termehchy", "Arash", ""], ["Fern", "Alan", ""], ["Ataei", "Parisa", ""]]}, {"id": "1508.04066", "submitter": "Tim Weninger PhD", "authors": "Tim Weninger, Rodrigo Palacios, Valter Crescenzi, Thomas Gottron,\n  Paolo Merialdo", "title": "Web Content Extraction - a Meta-Analysis of its Past and Thoughts on its\n  Future", "comments": "Accepted for publication in SIGKDD Explorations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a meta-analysis of several Web content extraction\nalgorithms, and make recommendations for the future of content extraction on\nthe Web. First, we find that nearly all Web content extractors do not consider\na very large, and growing, portion of modern Web pages. Second, it is well\nunderstood that wrapper induction extractors tend to break as the Web changes;\nheuristic/feature engineering extractors were thought to be immune to a Web\nsite's evolution, but we find that this is not the case: heuristic content\nextractor performance also tends to degrade over time due to the evolution of\nWeb site forms and practices. We conclude with recommendations for future work\nthat address these and other findings.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2015 15:52:23 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2015 14:13:14 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Weninger", "Tim", ""], ["Palacios", "Rodrigo", ""], ["Crescenzi", "Valter", ""], ["Gottron", "Thomas", ""], ["Merialdo", "Paolo", ""]]}, {"id": "1508.04159", "submitter": "Andr\\'e Dietrich", "authors": "Andr\\'e Dietrich, Sebastian Zug, Luigi Nardi, J\\\"org Kaiser", "title": "Reasoning in complex environments with the SelectScript declarative\n  language", "comments": "15 pages, 7 figures, 6th International Workshop on Domain-Specific\n  Languages and models for ROBotic systems (DSLRob-15)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI cs.DB cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SelectScript is an extendable, adaptable, and declarative domain-specific\nlanguage aimed at information retrieval from simulation environments and\nrobotic world models in an SQL-like manner. In this work we have extended the\nlanguage in two directions. First, we have implemented hierarchical queries;\nsecond, we improve efficiency enabling manual design space exploration on\ndifferent \"search\" strategies. We demonstrate the applicability of such\nextensions in two application problems; the basic language concepts are\nexplained by solving the classical problem of the Towers of Hanoi and then a\ncommon path planning problem in a complex 3D environment is implemented.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2015 21:26:39 GMT"}, {"version": "v2", "created": "Sun, 4 Oct 2015 15:53:29 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Dietrich", "Andr\u00e9", ""], ["Zug", "Sebastian", ""], ["Nardi", "Luigi", ""], ["Kaiser", "J\u00f6rg", ""]]}, {"id": "1508.04957", "submitter": "Aggeliki Dimitriou", "authors": "Aggeliki Dimitriou, Ananya Dass, Dimitri Theodoratos", "title": "Cohesiveness Relationships to Empower Keyword Search on Tree Data on the\n  Web", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keyword search is the most popular querying technique on semistructured data.\nKeyword queries are simple and con- venient. However, as a consequence of their\nimprecision, the quality of their answers is poor and the existing algorithms\ndo not scale satisfactorily.\n  In this paper, we introduce the novel concept of cohesive keyword queries for\ntree data. Intuitively, a cohesiveness relationship on keywords indicates that\nthey should form a cohesive whole in a query result. Cohesive keyword queries\nallow term nesting and keyword repetition. Although more expressive, they are\nas simple as flat keyword queries. We provide formal semantics for cohesive\nkeyword queries rank- ing query results on the proximity of the keyword\ninstances. We design a stack based algorithm which efficiently evaluates\ncohesive keyword queries. Our experiments demonstrate that our approach\noutperforms in quality previous filtering semantics and our algorithm scales\nsmoothly on queries of even 20 keywords on large datasets.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 11:17:04 GMT"}], "update_date": "2015-08-21", "authors_parsed": [["Dimitriou", "Aggeliki", ""], ["Dass", "Ananya", ""], ["Theodoratos", "Dimitri", ""]]}, {"id": "1508.05347", "submitter": "Vasilis Syrgkanis", "authors": "Vasilis Syrgkanis, Johannes Gehrke", "title": "Pricing Queries Approximately Optimally", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data as a commodity has always been purchased and sold. Recently, web\nservices that are data marketplaces have emerged that match data buyers with\ndata sellers. So far there are no guidelines how to price queries against a\ndatabase. We consider the recently proposed query-based pricing framework of\nKoutris et al and ask the question of computing optimal input prices in this\nframework by formulating a buyer utility model.\n  We establish the interesting and deep equivalence between arbitrage-freeness\nin the query-pricing framework and envy-freeness in pricing theory for\nappropriately chosen buyer valuations. Given the approximation hardness results\nfrom envy-free pricing we then develop logarithmic approximation pricing\nalgorithms exploiting the max flow interpretation of the arbitrage-free pricing\nfor the restricted query language proposed by Koutris et al. We propose a novel\npolynomial-time logarithmic approximation pricing scheme and show that our new\nscheme performs better than the existing envy-free pricing algorithms\ninstance-by-instance. We also present a faster pricing algorithm that is always\ngreater than the existing solutions, but worse than our previous scheme. We\nexperimentally show how our pricing algorithms perform with respect to the\nexisting envy-free pricing algorithms and to the optimal exponentially\ncomputable solution, and our experiments show that our approximation algorithms\nconsistently arrive at about 99% of the optimal.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2015 18:19:28 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2015 10:18:35 GMT"}, {"version": "v3", "created": "Tue, 25 Aug 2015 17:36:06 GMT"}], "update_date": "2015-08-26", "authors_parsed": [["Syrgkanis", "Vasilis", ""], ["Gehrke", "Johannes", ""]]}, {"id": "1508.05710", "submitter": "Zixuan Zhuang", "authors": "Zixuan Zhuang", "title": "An Experimental Study of Distributed Quantile Estimation", "comments": "M.S. Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantiles are very important statistics information used to describe the\ndistribution of datasets. Given the quantiles of a dataset, we can easily know\nthe distribution of the dataset, which is a fundamental problem in data\nanalysis. However, quite often, computing quantiles directly is inappropriate\ndue to the memory limitations. Further, in many settings such as data streaming\nand sensor network model, even the data size is unpredictable. Although the\nquantiles computation has been widely studied, it was mostly in the sequential\nsetting. In this paper, we study several quantile computation algorithms in the\ndistributed setting and compare them in terms of space usage, running time, and\naccuracy. Moreover, we provide detailed experimental comparisons between\nseveral popular algorithms. Our work focuses on the approximate quantile\nalgorithms which provide error bounds. Approximate quantiles have received more\nattentions than exact ones since they are often faster, can be more easily\nadapted to the distributed setting while giving sufficiently good statistical\ninformation on the data sets.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2015 07:49:38 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Zhuang", "Zixuan", ""]]}, {"id": "1508.06013", "submitter": "Leopoldo Bertossi", "authors": "Zeinab Bahmani, Leopoldo Bertossi and Nikolaos Vasiloglou", "title": "ERBlox: Combining Matching Dependencies with Machine Learning for Entity\n  Resolution", "comments": "To appear in Proc. SUM, 2015", "journal-ref": "Proc. SUM'15, 2015, Springer LNAI 9310, pp. 399-414", "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity resolution (ER), an important and common data cleaning problem, is\nabout detecting data duplicate representations for the same external entities,\nand merging them into single representations. Relatively recently, declarative\nrules called matching dependencies (MDs) have been proposed for specifying\nsimilarity conditions under which attribute values in database records are\nmerged. In this work we show the process and the benefits of integrating three\ncomponents of ER: (a) Classifiers for duplicate/non-duplicate record pairs\nbuilt using machine learning (ML) techniques, (b) MDs for supporting both the\nblocking phase of ML and the merge itself; and (c) The use of the declarative\nlanguage LogiQL -an extended form of Datalog supported by the LogicBlox\nplatform- for data processing, and the specification and enforcement of MDs.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2015 02:35:58 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Bahmani", "Zeinab", ""], ["Bertossi", "Leopoldo", ""], ["Vasiloglou", "Nikolaos", ""]]}, {"id": "1508.06098", "submitter": "Patricia Serrano-Alvarado Mme", "authors": "Georges Nassopoulos, Patricia Serrano-Alvarado, Pascal Molli, Emmanuel\n  Desmontils", "title": "Tracking Federated Queries in the Linked Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Federated query engines allow data consumers to execute queries over the\nfederation of Linked Data (LD). However, as federated queries are decomposed\ninto potentially thousands of subqueries distributed among SPARQL endpoints,\ndata providers do not know federated queries, they only know subqueries they\nprocess. Consequently, unlike warehousing approaches, LD data providers have no\naccess to secondary data. In this paper, we propose FETA (FEderated query\nTrAcking), a query tracking algorithm that infers Basic Graph Patterns (BGPs)\nprocessed by a federation from a shared log maintained by data providers.\nConcurrent execution of thousand subqueries generated by multiple federated\nquery engines makes the query tracking process challenging and uncertain.\nExperiments with Anapsid show that FETA is able to extract BGPs which, even in\na worst case scenario, contain BGPs of original queries.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2015 10:15:53 GMT"}], "update_date": "2015-08-26", "authors_parsed": [["Nassopoulos", "Georges", ""], ["Serrano-Alvarado", "Patricia", ""], ["Molli", "Pascal", ""], ["Desmontils", "Emmanuel", ""]]}, {"id": "1508.06976", "submitter": "Byung Suk Lee", "authors": "Saurav Acharya, Byung Suk Lee and Paul Hines", "title": "Real-time Top-K Predictive Query Processing over Event Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of predicting the k events that are most\nlikely to occur next, over historical real-time event streams. Existing\napproaches to causal prediction queries have a number of limitations. First,\nthey exhaustively search over an acyclic causal network to find the most likely\nk effect events; however, data from real event streams frequently reflect\ncyclic causality. Second, they contain conservative assumptions intended to\nexclude all possible non-causal links in the causal network; it leads to the\nomission of many less-frequent but important causal links. We overcome these\nlimitations by proposing a novel event precedence model and a run-time causal\ninference mechanism. The event precedence model constructs a first order\nabsorbing Markov chain incrementally over event streams, where an edge between\ntwo events signifies a temporal precedence relationship between them, which is\na necessary condition for causality. Then, the run-time causal inference\nmechanism learns causal relationships dynamically during query processing. This\nis done by removing some of the temporal precedence relationships that do not\nexhibit causality in the presence of other events in the event precedence\nmodel. This paper presents two query processing algorithms -- one performs\nexhaustive search on the model and the other performs a more efficient reduced\nsearch with early termination. Experiments using two real datasets (cascading\nblackouts in power systems and web page views) verify the effectiveness of the\nprobabilistic top-k prediction queries and the efficiency of the algorithms.\nSpecifically, the reduced search algorithm reduced runtime, relative to\nexhaustive search, by 25-80% (depending on the application) with only a small\nreduction in accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 15:02:09 GMT"}], "update_date": "2015-08-28", "authors_parsed": [["Acharya", "Saurav", ""], ["Lee", "Byung Suk", ""], ["Hines", "Paul", ""]]}, {"id": "1508.07306", "submitter": "Ashwin Machanavajjhala", "authors": "Yan Chen and Ashwin Machanavajjhala", "title": "On the Privacy Properties of Variants on the Sparse Vector Technique", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sparse vector technique is a powerful differentially private primitive\nthat allows an analyst to check whether queries in a stream are greater or\nlesser than a threshold. This technique has a unique property -- the algorithm\nworks by adding noise with a finite variance to the queries and the threshold,\nand guarantees privacy that only degrades with (a) the maximum sensitivity of\nany one query in stream, and (b) the number of positive answers output by the\nalgorithm. Recent work has developed variants of this algorithm, which we call\n{\\em generalized private threshold testing}, and are claimed to have privacy\nguarantees that do not depend on the number of positive or negative answers\noutput by the algorithm. These algorithms result in a significant improvement\nin utility over the sparse vector technique for a given privacy budget, and\nhave found applications in frequent itemset mining, feature selection in\nmachine learning and generating synthetic data.\n  In this paper we critically analyze the privacy properties of generalized\nprivate threshold testing. We show that generalized private threshold testing\ndoes not satisfy \\epsilon-differential privacy for any finite \\epsilon. We\nidentify a subtle error in the privacy analysis of this technique in prior\nwork. Moreover, we show an adversary can use generalized private threshold\ntesting to recover counts from the datasets (especially small counts) exactly\nwith high accuracy, and thus can result in individuals being reidentified. We\ndemonstrate our attacks empirically on real datasets.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2015 18:42:56 GMT"}], "update_date": "2015-08-31", "authors_parsed": [["Chen", "Yan", ""], ["Machanavajjhala", "Ashwin", ""]]}, {"id": "1508.07371", "submitter": "Vijay Gadepally", "authors": "Vijay Gadepally, Jeremy Kepner, William Arcand, David Bestor, Bill\n  Bergeron, Chansup Byun, Lauren Edwards, Matthew Hubbell, Peter Michaleas,\n  Julie Mullen, Andrew Prout, Antonio Rosa, Charles Yee, Albert Reuther", "title": "D4M: Bringing Associative Arrays to Database Engines", "comments": null, "journal-ref": null, "doi": "10.1109/HPEC.2015.7322472", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to collect and analyze large amounts of data is a growing problem\nwithin the scientific community. The growing gap between data and users calls\nfor innovative tools that address the challenges faced by big data volume,\nvelocity and variety. Numerous tools exist that allow users to store, query and\nindex these massive quantities of data. Each storage or database engine comes\nwith the promise of dealing with complex data. Scientists and engineers who\nwish to use these systems often quickly find that there is no single technology\nthat offers a panacea to the complexity of information. When using multiple\ntechnologies, however, there is significant trouble in designing the movement\nof information between storage and database engines to support an end-to-end\napplication along with a steep learning curve associated with learning the\nnuances of each underlying technology. In this article, we present the Dynamic\nDistributed Dimensional Data Model (D4M) as a potential tool to unify database\nand storage engine operations. Previous articles on D4M have showcased the\nability of D4M to interact with the popular NoSQL Accumulo database. Recently\nhowever, D4M now operates on a variety of backend storage or database engines\nwhile providing a federated look to the end user through the use of associative\narrays. In order to showcase how new databases may be supported by D4M, we\ndescribe the process of building the D4M-SciDB connector and present\nperformance of this connection.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2015 22:46:56 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Gadepally", "Vijay", ""], ["Kepner", "Jeremy", ""], ["Arcand", "William", ""], ["Bestor", "David", ""], ["Bergeron", "Bill", ""], ["Byun", "Chansup", ""], ["Edwards", "Lauren", ""], ["Hubbell", "Matthew", ""], ["Michaleas", "Peter", ""], ["Mullen", "Julie", ""], ["Prout", "Andrew", ""], ["Rosa", "Antonio", ""], ["Yee", "Charles", ""], ["Reuther", "Albert", ""]]}, {"id": "1508.07372", "submitter": "Vijay Gadepally", "authors": "Vijay Gadepally, Jake Bolewski, Dan Hook, Dylan Hutchison, Ben Miller,\n  Jeremy Kepner", "title": "Graphulo: Linear Algebra Graph Kernels for NoSQL Databases", "comments": "10 pages", "journal-ref": null, "doi": "10.1109/IPDPSW.2015.19", "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data and the Internet of Things era continue to challenge computational\nsystems. Several technology solutions such as NoSQL databases have been\ndeveloped to deal with this challenge. In order to generate meaningful results\nfrom large datasets, analysts often use a graph representation which provides\nan intuitive way to work with the data. Graph vertices can represent users and\nevents, and edges can represent the relationship between vertices. Graph\nalgorithms are used to extract meaningful information from these very large\ngraphs. At MIT, the Graphulo initiative is an effort to perform graph\nalgorithms directly in NoSQL databases such as Apache Accumulo or SciDB, which\nhave an inherently sparse data storage scheme. Sparse matrix operations have a\nhistory of efficient implementations and the Graph Basic Linear Algebra\nSubprogram (GraphBLAS) community has developed a set of key kernels that can be\nused to develop efficient linear algebra operations. However, in order to use\nthe GraphBLAS kernels, it is important that common graph algorithms be recast\nusing the linear algebra building blocks. In this article, we look at common\nclasses of graph algorithms and recast them into linear algebra operations\nusing the GraphBLAS building blocks.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2015 23:03:10 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2015 03:23:10 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Gadepally", "Vijay", ""], ["Bolewski", "Jake", ""], ["Hook", "Dan", ""], ["Hutchison", "Dylan", ""], ["Miller", "Ben", ""], ["Kepner", "Jeremy", ""]]}, {"id": "1508.07455", "submitter": "Anastasios Gounaris", "authors": "Anastasios Gounaris", "title": "Towards Automated Performance Optimization of BPMN Business Processes", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Business Process Model and Notation (BPMN) provides a standard for the design\nof business processes. It focuses on bridging the gap between the analysis and\nthe technical perspectives, and aims to deliver process automation. The aim of\nthis technical report is to complement this effort by transferring knowledge\nfrom the related field of data-centric workflows aiming to provide automated\nperformance optimization of the business process execution. Automated\noptimization lifts a burden from BPMN designers and increases workflow\nflexibility and resilience. As a key step towards this goal, the contribution\nof this work is to provide a methodology to map BPMNv2.0 models to annotated\ndirected acyclic graphs, which emphasize the volume of the tokens exchanged and\nare amenable to existing automated optimization algorithms. In addition,\nconcrete examples of mappings are given, while the optimization opportunities\nthat are opened are explained, thus providing insights into the potential\nperformance benefits and we discuss technical research issues.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2015 14:31:55 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Gounaris", "Anastasios", ""]]}, {"id": "1508.07532", "submitter": "Rohan Puttagunta", "authors": "Manas Joglekar, Rohan Puttagunta, Christopher R\\'e", "title": "Aggregations over Generalized Hypertree Decompositions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a class of aggregate-join queries with multiple aggregation\noperators evaluated over annotated relations. We show that straightforward\nextensions of standard multiway join algorithms and generalized hypertree\ndecompositions (GHDs) provide best-known runtime guarantees. In contrast, prior\nwork uses bespoke algorithms and data structures and does not match these\nguarantees. Our extensions to the standard techniques are a pair of simple\ntests that (1) determine if two orderings of aggregation operators are\nequivalent and (2) determine if a GHD is compatible with a given ordering.\nThese tests provide a means to find an optimal GHD that, when provided to\nstandard join algorithms, will correctly answer a given aggregate-join query.\nThe second class of our contributions is a pair of complete characterizations\nof (1) the set of orderings equivalent to a given ordering and (2) the set of\nGHDs compatible with some equivalent ordering. We show by example that previous\napproaches are incomplete. The key technical consequence of our\ncharacterizations is a decomposition of a compatible GHD into a set of\n(smaller) {\\em unconstrained} GHDs, i.e. into a set of GHDs of sub-queries\nwithout aggregations. Since this decomposition is comprised of unconstrained\nGHDs, we are able to connect to the wide literature on GHDs for join query\nprocessing, thereby obtaining improved runtime bounds, MapReduce variants, and\nan efficient method to find approximately optimal GHDs.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2015 06:24:59 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2015 17:04:01 GMT"}, {"version": "v3", "created": "Tue, 27 Oct 2015 02:25:10 GMT"}, {"version": "v4", "created": "Thu, 10 Dec 2015 00:58:58 GMT"}], "update_date": "2015-12-11", "authors_parsed": [["Joglekar", "Manas", ""], ["Puttagunta", "Rohan", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1508.07845", "submitter": "Peng Peng", "authors": "Peng Peng, Lei Zou, Lei Chen, Dongyan Zhao", "title": "Query Workload-based RDF Graph Fragmentation and Allocation", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the volume of the RDF data becomes increasingly large, it is essential for\nus to design a distributed database system to manage it. For distributed RDF\ndata design, it is quite common to partition the RDF data into some parts,\ncalled fragments, which are then distributed. Thus, the distribution design\nconsists of two steps: fragmentation and allocation. In this paper, we propose\na method to explore the intrinsic similarities among the structures of queries\nin a workload for fragmentation and allocation, which aims to reduce the number\nof crossing matches and the communication cost during SPARQL query processing.\nSpecifically, we mine and select some frequent access patterns to reflect the\ncharacteristics of the workload. Here, although we prove that selecting the\noptimal set of frequent access patterns is NP-hard, we propose a heuristic\nalgorithm which guarantees both the data integrity and the approximation ratio.\nBased on the selected frequent access patterns, we propose two fragmentation\nstrategies, vertical and horizontal fragmentation strategies, to divide RDF\ngraphs while meeting different kinds of query processing objectives. Vertical\nfragmentation is for better throughput and horizontal fragmentation is for\nbetter performance. After fragmentation, we discuss how to allocate these\nfragments to various sites. Finally, we discuss how to process a query based on\nthe results of fragmentation and allocation. Extensive experiments confirm the\nsuperior performance of our proposed solutions.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2015 14:23:38 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2015 02:29:51 GMT"}, {"version": "v3", "created": "Thu, 10 Dec 2015 16:10:04 GMT"}, {"version": "v4", "created": "Sat, 20 Feb 2016 08:16:59 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Peng", "Peng", ""], ["Zou", "Lei", ""], ["Chen", "Lei", ""], ["Zhao", "Dongyan", ""]]}]