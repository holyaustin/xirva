[{"id": "1404.0046", "submitter": "Immanuel Trummer Mr.", "authors": "Immanuel Trummer, Christoph Koch", "title": "Approximation Schemes for Many-Objective Query Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of multi-objective query optimization (MOQO) is to find query plans\nthat realize a good compromise between conflicting objectives such as\nminimizing execution time and minimizing monetary fees in a Cloud scenario. A\npreviously proposed exhaustive MOQO algorithm needs hours to optimize even\nsimple TPC-H queries. This is why we propose several approximation schemes for\nMOQO that generate guaranteed near-optimal plans in seconds where exhaustive\noptimization takes hours.\n  We integrated all MOQO algorithms into the Postgres optimizer and present\nexperimental results for TPC-H queries; we extended the Postgres cost model and\noptimize for up to nine conflicting objectives in our experiments. The proposed\nalgorithms are based on a formal analysis of typical cost functions that occur\nin the context of MOQO. We identify properties that hold for a broad range of\nobjectives and can be exploited for the design of future MOQO algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 31 Mar 2014 21:39:29 GMT"}], "update_date": "2014-04-02", "authors_parsed": [["Trummer", "Immanuel", ""], ["Koch", "Christoph", ""]]}, {"id": "1404.0606", "submitter": "Andr\\'e Frochaux", "authors": "Andr\\'e Frochaux and Martin Grohe and Nicole Schweikardt", "title": "Monadic Datalog Containment on Trees", "comments": "This article is the full version of an article published in the\n  proccedings of the 8th Alberto Mendelzon Workshop (AMW 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the query containment problem for monadic datalog on finite\nunranked labeled trees can be solved in 2-fold exponential time when (a)\nconsidering unordered trees using the axes child and descendant, and when (b)\nconsidering ordered trees using the axes firstchild, nextsibling, child, and\ndescendant. When omitting the descendant-axis, we obtain that in both cases the\nproblem is EXPTIME-complete.\n", "versions": [{"version": "v1", "created": "Wed, 2 Apr 2014 16:34:17 GMT"}], "update_date": "2014-04-03", "authors_parsed": [["Frochaux", "Andr\u00e9", ""], ["Grohe", "Martin", ""], ["Schweikardt", "Nicole", ""]]}, {"id": "1404.0696", "submitter": "Spyros Sioutas SS", "authors": "S. Sioutas, E. Sakkopoulos, A. Panaretos, D. Tsoumakos, P.\n  Gerolymatos, G. Tzimas and Y. Manolopoulos", "title": "D-P2P-Sim+:A Novel Distributed Framework for P2P Protocols Performance\n  Testing", "comments": "51 pages, 37 figures, submitted to JSS (Elsevier)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent IoT (Internet of Things) and Web 2.0 technologies, a critical\nproblem arises with respect to storing and processing the large amount of\ncollected data. In this paper we develop and evaluate distributed\ninfrastructures for storing and processing large amount of such data. We\npresent a distributed framework that supports customized deployment of a\nvariety of indexing engines over million-node overlays. The proposed framework\nprovides the appropriate integrated set of tools that allows applications\nprocessing large amount of data, to evaluate and test the performance of\nvarious application protocols for very large scale deployments (multi million\nnodes - billions of keys). The key aim is to provide the appropriate\nenvironment that contributes in taking decisions regarding the choice of the\nprotocol in storage P2P systems for a variety of big data applications. Using\nlightweight and efficient collection mechanisms, our system enables real-time\nregistration of multiple measures, integrating support for real-life parameters\nsuch as node failure models and recovery strategies. Experiments have been\nperformed at the PlanetLab network and at a typical research laboratory in\norder to verify scalability and show maximum re-usability of our setup.\nD-P2P-Sim+ framework is publicly available at\nhttp://code.google.com/p/d-p2p-sim/downloads/list.\n", "versions": [{"version": "v1", "created": "Wed, 2 Apr 2014 20:31:12 GMT"}], "update_date": "2014-04-04", "authors_parsed": [["Sioutas", "S.", ""], ["Sakkopoulos", "E.", ""], ["Panaretos", "A.", ""], ["Tsoumakos", "D.", ""], ["Gerolymatos", "P.", ""], ["Tzimas", "G.", ""], ["Manolopoulos", "Y.", ""]]}, {"id": "1404.0703", "submitter": "Mahmoud Abo Khamis", "authors": "Mahmoud Abo Khamis, Hung Q. Ngo, Christopher R\\'e, Atri Rudra", "title": "Joins via Geometric Resolutions: Worst-case and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple geometric framework for the relational join. Using this\nframework, we design an algorithm that achieves the fractional hypertree-width\nbound, which generalizes classical and recent worst-case algorithmic results on\ncomputing joins. In addition, we use our framework and the same algorithm to\nshow a series of what are colloquially known as beyond worst-case results. The\nframework allows us to prove results for data stored in Btrees,\nmultidimensional data structures, and even multiple indices per table. A key\nidea in our framework is formalizing the inference one does with an index as a\ntype of geometric resolution; transforming the algorithmic problem of computing\njoins to a geometric problem. Our notion of geometric resolution can be viewed\nas a geometric analog of logical resolution. In addition to the geometry and\nlogic connections, our algorithm can also be thought of as backtracking search\nwith memoization.\n", "versions": [{"version": "v1", "created": "Wed, 2 Apr 2014 20:50:22 GMT"}, {"version": "v2", "created": "Fri, 4 Apr 2014 16:39:13 GMT"}, {"version": "v3", "created": "Thu, 10 Apr 2014 19:41:03 GMT"}, {"version": "v4", "created": "Mon, 13 Oct 2014 00:06:10 GMT"}, {"version": "v5", "created": "Thu, 5 Feb 2015 03:11:16 GMT"}, {"version": "v6", "created": "Tue, 22 Dec 2015 20:48:18 GMT"}, {"version": "v7", "created": "Fri, 23 Dec 2016 19:05:41 GMT"}], "update_date": "2016-12-26", "authors_parsed": [["Khamis", "Mahmoud Abo", ""], ["Ngo", "Hung Q.", ""], ["R\u00e9", "Christopher", ""], ["Rudra", "Atri", ""]]}, {"id": "1404.0900", "submitter": "Xiaokui Xiao", "authors": "Youze Tang, Xiaokui Xiao, Yanchen Shi", "title": "Influence Maximization: Near-Optimal Time Complexity Meets Practical\n  Efficiency", "comments": "Revised Sections 1, 2.3, and 5 to remove incorrect claims about\n  reference [3]. Updated experiments accordingly. A shorter version of the\n  paper will appear in SIGMOD 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a social network G and a constant k, the influence maximization problem\nasks for k nodes in G that (directly and indirectly) influence the largest\nnumber of nodes under a pre-defined diffusion model. This problem finds\nimportant applications in viral marketing, and has been extensively studied in\nthe literature. Existing algorithms for influence maximization, however, either\ntrade approximation guarantees for practical efficiency, or vice versa. In\nparticular, among the algorithms that achieve constant factor approximations\nunder the prominent independent cascade (IC) model or linear threshold (LT)\nmodel, none can handle a million-node graph without incurring prohibitive\noverheads.\n  This paper presents TIM, an algorithm that aims to bridge the theory and\npractice in influence maximization. On the theory side, we show that TIM runs\nin O((k+\\ell) (n+m) \\log n / \\epsilon^2) expected time and returns a\n(1-1/e-\\epsilon)-approximate solution with at least 1 - n^{-\\ell} probability.\nThe time complexity of TIM is near-optimal under the IC model, as it is only a\n\\log n factor larger than the \\Omega(m + n) lower-bound established in previous\nwork (for fixed k, \\ell, and \\epsilon). Moreover, TIM supports the triggering\nmodel, which is a general diffusion model that includes both IC and LT as\nspecial cases. On the practice side, TIM incorporates novel heuristics that\nsignificantly improve its empirical efficiency without compromising its\nasymptotic performance. We experimentally evaluate TIM with the largest\ndatasets ever tested in the literature, and show that it outperforms the\nstate-of-the-art solutions (with approximation guarantees) by up to four orders\nof magnitude in terms of running time. In particular, when k = 50, \\epsilon =\n0.2, and \\ell = 1, TIM requires less than one hour on a commodity machine to\nprocess a network with 41.6 million nodes and 1.4 billion edges.\n", "versions": [{"version": "v1", "created": "Thu, 3 Apr 2014 13:23:10 GMT"}, {"version": "v2", "created": "Wed, 30 Apr 2014 03:40:36 GMT"}], "update_date": "2014-05-02", "authors_parsed": [["Tang", "Youze", ""], ["Xiao", "Xiaokui", ""], ["Shi", "Yanchen", ""]]}, {"id": "1404.1270", "submitter": "Iovka Boneva", "authors": "Iovka Boneva, Jose Emilio Labra Gayo, Eric G. Prud'hommeau", "title": "Semantics and Validation of Shapes Schemas for RDF", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a formal semantics and proof of soundness for shapes schemas, an\nexpressive schema language for RDF graphs that is the foundation of Shape\nExpressions Language 2.0. It can be used to describe the vocabulary and the\nstructure of an RDF graph, and to constrain the admissible properties and\nvalues for nodes in that graph. The language defines a typing mechanism called\nshapes against which nodes of the graph can be checked. It includes an\nalgebraic grouping operator, a choice operator and cardinality constraints for\nthe number of allowed occurrences of a property. Shapes can be combined using\nBoolean operators, and can use possibly recursive references to other shapes.\n  We describe the syntax of the language and define its semantics. The\nsemantics is proven to be well-defined for schemas that satisfy a reasonable\nsyntactic restriction, namely stratified use of negation and recursion. We\npresent two algorithms for the validation of an RDF graph against a shapes\nschema. The first algorithm is a direct implementation of the semantics,\nwhereas the second is a non-trivial improvement. We also briefly give\nimplementation guidelines.\n", "versions": [{"version": "v1", "created": "Fri, 4 Apr 2014 14:39:48 GMT"}, {"version": "v2", "created": "Thu, 7 Aug 2014 17:48:41 GMT"}, {"version": "v3", "created": "Tue, 19 Sep 2017 12:36:41 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Boneva", "Iovka", ""], ["Gayo", "Jose Emilio Labra", ""], ["Prud'hommeau", "Eric G.", ""]]}, {"id": "1404.2034", "submitter": "Felix Martin Schuhknecht", "authors": "Victor Alvarez, Felix Martin Schuhknecht, Jens Dittrich, Stefan\n  Richter", "title": "Main Memory Adaptive Indexing for Multi-core Systems", "comments": "26 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive indexing is a concept that considers index creation in databases as\na by-product of query processing; as opposed to traditional full index creation\nwhere the indexing effort is performed up front before answering any queries.\nAdaptive indexing has received a considerable amount of attention, and several\nalgorithms have been proposed over the past few years; including a recent\nexperimental study comparing a large number of existing methods. Until now,\nhowever, most adaptive indexing algorithms have been designed single-threaded,\nyet with multi-core systems already well established, the idea of designing\nparallel algorithms for adaptive indexing is very natural. In this regard only\none parallel algorithm for adaptive indexing has recently appeared in the\nliterature: The parallel version of standard cracking. In this paper we\ndescribe three alternative parallel algorithms for adaptive indexing, including\na second variant of a parallel standard cracking algorithm. Additionally, we\ndescribe a hybrid parallel sorting algorithm, and a NUMA-aware method based on\nsorting. We then thoroughly compare all these algorithms experimentally; along\na variant of a recently published parallel version of radix sort. Parallel\nsorting algorithms serve as a realistic baseline for multi-threaded adaptive\nindexing techniques. In total we experimentally compare seven parallel\nalgorithms. Additionally, we extensively profile all considered algorithms. The\ninitial set of experiments considered in this paper indicates that our parallel\nalgorithms significantly improve over previously known ones. Our results\nsuggest that, although adaptive indexing algorithms are a good design choice in\nsingle-threaded environments, the rules change considerably in the parallel\ncase. That is, in future highly-parallel environments, sorting algorithms could\nbe serious alternatives to adaptive indexing.\n", "versions": [{"version": "v1", "created": "Tue, 8 Apr 2014 08:11:32 GMT"}], "update_date": "2014-04-09", "authors_parsed": [["Alvarez", "Victor", ""], ["Schuhknecht", "Felix Martin", ""], ["Dittrich", "Jens", ""], ["Richter", "Stefan", ""]]}, {"id": "1404.2160", "submitter": "Timur Mirzoev", "authors": "Timur Mirzoev, Craig Brockman", "title": "SAP HANA and its performance benefits", "comments": "i-managers Journal on Information Technology, Vol. 2, No. 1 December\n  2012 February 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In-memory computing has changed the landscape of database technology. Within\nthe database and technology field, advancements occur over the course of time\nthat has had the capacity to transform some fundamental tenants of the\ntechnology and how it is applied. The concept of Database Management Systems\n(DBMS) was realized in industry during the 1960s, allowing users and developers\nto use a navigational model to access the data stored by the computers of that\nday as they grew in speed and capability. This manuscript is specifically\nexamines the SAPHigh Performance Analytics Appliance(HANA) approach, which is\none of the commonly used technologies today. Additionally, this manuscript\nprovides the analysis of the first two of the four common main usecases to\nutilize SAP HANA's in-memory computing database technology. The performance\nbenefits are important factors for DB calculations.Some of the benefits are\nquantified and the demonstrated by the defined sets of data.\n", "versions": [{"version": "v1", "created": "Tue, 8 Apr 2014 14:46:03 GMT"}], "update_date": "2014-04-09", "authors_parsed": [["Mirzoev", "Timur", ""], ["Brockman", "Craig", ""]]}, {"id": "1404.3131", "submitter": "Antoine Amarilli", "authors": "Antoine Amarilli", "title": "The Possibility Problem for Probabilistic XML (Extended Version)", "comments": "20 pages, 1 table, 2 figures. This is the complete version (including\n  proofs) of work initially submitted as an extended abstract (without proofs)\n  at the AMW 2014 workshop and subsequently submitted (with proofs) at the BDA\n  2014 conference (no formal proceedings). This version integrates the feedback\n  from both rounds of reviews", "journal-ref": null, "doi": "10.3166/isi.20.5.53-75", "report-no": null, "categories": "cs.DB cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the possibility problem of determining if a document is a\npossible world of a probabilistic document, in the setting of probabilistic\nXML. This basic question is a special case of query answering or tree automata\nevaluation, but it has specific practical uses, such as checking whether an\nuser-provided probabilistic document outcome is possible or sufficiently\nlikely. In this paper, we study the complexity of the possibility problem for\nprobabilistic XML models of varying expressiveness. We show that the decision\nproblem is often tractable in the absence of long-distance dependencies, but\nthat its computation variant is intractable on unordered documents. We also\nintroduce an explicit matches variant to generalize practical situations where\nnode labels are unambiguous; this ensures tractability of the possibility\nproblem, even under long-distance dependencies, provided event conjunctions are\ndisallowed. Our results entirely classify the tractability boundary over all\nconsidered problem variants.\n", "versions": [{"version": "v1", "created": "Fri, 11 Apr 2014 15:07:19 GMT"}, {"version": "v2", "created": "Mon, 14 Apr 2014 15:08:59 GMT"}, {"version": "v3", "created": "Fri, 16 May 2014 09:47:13 GMT"}, {"version": "v4", "created": "Tue, 22 Jul 2014 10:55:57 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Amarilli", "Antoine", ""]]}, {"id": "1404.3461", "submitter": "Xiaolu Lu", "authors": "Xiaolu Lu, Dongxu Li, Xiang Li, Ling Feng", "title": "A 2D based Partition Strategy for Solving Ranking under Team Context\n  (RTP)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a 2D based partition method for solving the problem\nof Ranking under Team Context(RTC) on datasets without a priori. We first map\nthe data into 2D space using its minimum and maximum value among all\ndimensions. Then we construct window queries with consideration of current team\ncontext. Besides, during the query mapping procedure, we can pre-prune some\ntuples which are not top ranked ones. This pre-classified step will defer\nprocessing those tuples and can save cost while providing solutions for the\nproblem. Experiments show that our algorithm performs well especially on large\ndatasets with correctness.\n", "versions": [{"version": "v1", "created": "Mon, 14 Apr 2014 05:20:48 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Lu", "Xiaolu", ""], ["Li", "Dongxu", ""], ["Li", "Xiang", ""], ["Feng", "Ling", ""]]}, {"id": "1404.3722", "submitter": "Samuel Haney", "authors": "Samuel Haney, Ashwin Machanavajjhala, Bolin Ding", "title": "Design of Policy-Aware Differentially Private Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of designing error optimal differentially private algorithms is\nwell studied. Recent work applying differential privacy to real world settings\nhave used variants of differential privacy that appropriately modify the notion\nof neighboring databases. The problem of designing error optimal algorithms for\nsuch variants of differential privacy is open. In this paper, we show a novel\ntransformational equivalence result that can turn the problem of query\nanswering under differential privacy with a modified notion of neighbors to one\nof query answering under standard differential privacy, for a large class of\nneighbor definitions.\n  We utilize the Blowfish privacy framework that generalizes differential\nprivacy. Blowfish uses a {\\em policy graph} to instantiate different notions of\nneighboring databases. We show that the error incurred when answering a\nworkload $\\mathbf{W}$ on a database $\\mathbf{x}$ under a Blowfish policy graph\n$G$ is identical to the error required to answer a transformed workload\n$f_G(\\mathbf{W})$ on database $g_G(\\mathbf{x})$ under standard differential\nprivacy, where $f_G$ and $g_G$ are linear transformations based on $G$. Using\nthis result, we develop error efficient algorithms for releasing histograms and\nmultidimensional range queries under different Blowfish policies. We believe\nthe tools we develop will be useful for finding mechanisms to answer many other\nclasses of queries with low error under other policy graphs.\n", "versions": [{"version": "v1", "created": "Mon, 14 Apr 2014 19:13:54 GMT"}, {"version": "v2", "created": "Wed, 5 Nov 2014 18:21:35 GMT"}, {"version": "v3", "created": "Fri, 20 Nov 2015 14:44:26 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Haney", "Samuel", ""], ["Machanavajjhala", "Ashwin", ""], ["Ding", "Bolin", ""]]}, {"id": "1404.4286", "submitter": "Maryam Nazaridoust", "authors": "Behrouz Minaei Bidgoli and Maryam Nazaridoust", "title": "Case study: Data Mining of Associate Degree Accepted Candidates by\n  Modular Method", "comments": "8 pages, 8 figures, 2 tabales. http://www.SciRP.org/journal/cn", "journal-ref": "Communications and Network, 2012, 4, 189-268", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since about 10 years ago, University of Applied Science and Technology (UAST)\nin Iran has admitted students in discontinuous associate degree by modular\nmethod, so that almost 100,000 students are accepted every year. Although the\nfirst aim of holding such courses was to improve scientific and skill level of\nemployees, over time a considerable group of unemployed people have been\ninterested to participate in these courses. According to this fact, in this\npaper, we mine and analyze a sample data of accepted candidates in modular 2008\nand 2009 courses by using unsupervised and supervised learning paradigms. In\nthe first step, by using unsupervised paradigm, we grouped (clustered) set of\nmodular accepted candidates based on their student status and labeled data sets\nby three classes so that each class somehow shows educational and student\nstatus of modular accepted candidates. In the second step, by using supervised\nand unsupervised algorithms, we generated predicting models in 2008 data sets.\nThen, by making a comparison between performances of generated models, we\nselected predicting model of association rules through which some rules were\nextracted. Finally, this model is executed for Test set which includes accepted\ncandidates of next course then by evaluation of results, the percentage of\ncorrectness and confidentiality of obtained results can be viewed.\n", "versions": [{"version": "v1", "created": "Wed, 16 Apr 2014 15:31:38 GMT"}], "update_date": "2014-04-17", "authors_parsed": [["Bidgoli", "Behrouz Minaei", ""], ["Nazaridoust", "Maryam", ""]]}, {"id": "1404.4468", "submitter": "Miika Hannula", "authors": "Miika Hannula, Juha Kontinen, Sebastian Link", "title": "On Independence Atoms and Keys", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uniqueness and independence are two fundamental properties of data. Their\nenforcement in database systems can lead to higher quality data, faster data\nservice response time, better data-driven decision making and knowledge\ndiscovery from data. The applications can be effectively unlocked by providing\nefficient solutions to the underlying implication problems of keys and\nindependence atoms. Indeed, for the sole class of keys and the sole class of\nindependence atoms the associated finite and general implication problems\ncoincide and enjoy simple axiomatizations. However, the situation changes\ndrastically when keys and independence atoms are combined. We show that the\nfinite and the general implication problems are already different for keys and\nunary independence atoms. Furthermore, we establish a finite axiomatization for\nthe general implication problem, and show that the finite implication problem\ndoes not enjoy a k-ary axiomatization for any k.\n", "versions": [{"version": "v1", "created": "Thu, 17 Apr 2014 10:03:24 GMT"}], "update_date": "2014-04-18", "authors_parsed": [["Hannula", "Miika", ""], ["Kontinen", "Juha", ""], ["Link", "Sebastian", ""]]}, {"id": "1404.4821", "submitter": "Sergey Kovalchuk", "authors": "Sergey V. Kovalchuk, Artem V. Zakharchuk, Jiaqi Liao, Sergey V.\n  Ivanov, Alexander V. Boukhanovsky", "title": "A Technology for BigData Analysis Task Description using Domain-Specific\n  Languages", "comments": "To appear in Proceedings of the International Conference on\n  Computational Science (ICCS) 2014", "journal-ref": null, "doi": "10.1016/j.procs.2014.05.044", "report-no": null, "categories": "cs.DC cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article presents a technology for dynamic knowledge-based building of\nDomain-Specific Languages (DSL) to describe data-intensive scientific discovery\ntasks using BigData technology. The proposed technology supports high level\nabstract definition of analytic and simulation parts of the task as well as\nintegration into the composite scientific solutions. Automatic translation of\nthe abstract task definition enables seamless integration of various data\nsources within single solution.\n", "versions": [{"version": "v1", "created": "Fri, 18 Apr 2014 15:44:29 GMT"}], "update_date": "2016-03-21", "authors_parsed": [["Kovalchuk", "Sergey V.", ""], ["Zakharchuk", "Artem V.", ""], ["Liao", "Jiaqi", ""], ["Ivanov", "Sergey V.", ""], ["Boukhanovsky", "Alexander V.", ""]]}, {"id": "1404.4963", "submitter": "Daniel Lemire", "authors": "Antonio Badia and Daniel Lemire", "title": "Functional dependencies with null markers", "comments": "accepted at the Computer Journal (April 2014)", "journal-ref": "Computer Journal 58 (5), 2015", "doi": "10.1093/comjnl/bxu039", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Functional dependencies are an integral part of database design. However,\nthey are only defined when we exclude null markers. Yet we commonly use null\nmarkers in practice. To bridge this gap between theory and practice,\nresearchers have proposed definitions of functional dependencies over relations\nwith null markers. Though sound, these definitions lack some qualities that we\nfind desirable. For example, some fail to satisfy Armstrong's axioms---while\nthese axioms are part of the foundation of common database methodologies. We\npropose a set of properties that any extension of functional dependencies over\nrelations with null markers should possess. We then propose two new extensions\nhaving these properties. These extensions attempt to allow null markers where\nthey make sense to practitioners.\n  They both support Armstrong's axioms and provide realizable null markers: at\nany time, some or all of the null markers can be replaced by actual values\nwithout causing an anomaly. Our proposals may improve database designs.\n", "versions": [{"version": "v1", "created": "Sat, 19 Apr 2014 15:46:01 GMT"}, {"version": "v2", "created": "Thu, 15 May 2014 14:54:17 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Badia", "Antonio", ""], ["Lemire", "Daniel", ""]]}, {"id": "1404.5585", "submitter": "Matthew Skala", "authors": "Matthew Skala", "title": "A Structural Query System for Han Characters", "comments": "28 pages, 5 figures, for submission to ACM Transactions on Asian\n  Language Information Processing", "journal-ref": "International Journal of Asian Language Processing 23(2) (2015)\n  127-159", "doi": null, "report-no": null, "categories": "cs.CL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The IDSgrep structural query system for Han character dictionaries is\npresented. This system includes a data model and syntax for describing the\nspatial structure of Han characters using Extended Ideographic Description\nSequences (EIDSes) based on the Unicode IDS syntax; a language for querying\nEIDS databases, designed to suit the needs of font developers and foreign\nlanguage learners; a bit vector index inspired by Bloom filters for faster\nquery operations; a freely available implementation; and format translation\nfrom popular third-party IDS and XML character databases. Experimental results\nare included, with a comparison to other software used for similar\napplications.\n", "versions": [{"version": "v1", "created": "Tue, 22 Apr 2014 18:26:09 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Skala", "Matthew", ""]]}, {"id": "1404.5686", "submitter": "Liu Yue", "authors": "Yue Liu, Songlin Hu, Tilmann Rabl, Wantao Liu, Hans-Arno Jacobsen,\n  Kaifeng Wu, Jian Chen, Jintao Li", "title": "DGFIndex for Smart Grid: Enhancing Hive with a Cost-Effective\n  Multidimensional Range Index", "comments": "12 pages, VLDB 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Smart Grid applications, as the number of deployed electric smart meters\nincreases, massive amounts of valuable meter data is generated and collected\nevery day. To enable reliable data collection and make business decisions fast,\nhigh throughput storage and high-performance analysis of massive meter data\nbecome crucial for grid companies. Considering the advantage of high\nefficiency, fault tolerance, and price-performance of Hadoop and Hive systems,\nthey are frequently deployed as underlying platform for big data processing.\nHowever, in real business use cases, these data analysis applications typically\ninvolve multidimensional range queries (MDRQ) as well as batch reading and\nstatistics on the meter data. While Hive is high-performance at complex data\nbatch reading and analysis, it lacks efficient indexing techniques for MDRQ.\n  In this paper, we propose DGFIndex, an index structure for Hive that\nefficiently supports MDRQ for massive meter data. DGFIndex divides the data\nspace into cubes using the grid file technique. Unlike the existing indexes in\nHive, which stores all combinations of multiple dimensions, DGFIndex only\nstores the information of cubes. This leads to smaller index size and faster\nquery processing. Furthermore, with pre-computing user-defined aggregations of\neach cube, DGFIndex only needs to access the boundary region for aggregation\nquery. Our comprehensive experiments show that DGFIndex can save significant\ndisk space in comparison with the existing indexes in Hive and the query\nperformance with DGFIndex is 2-50 times faster than existing indexes in Hive\nand HadoopDB for aggregation query, 2-5 times faster than both for\nnon-aggregation query, 2-75 times faster than scanning the whole table in\ndifferent query selectivity.\n", "versions": [{"version": "v1", "created": "Wed, 23 Apr 2014 02:44:26 GMT"}, {"version": "v2", "created": "Tue, 10 Jun 2014 02:32:27 GMT"}, {"version": "v3", "created": "Wed, 9 Jul 2014 09:02:23 GMT"}], "update_date": "2014-07-10", "authors_parsed": [["Liu", "Yue", ""], ["Hu", "Songlin", ""], ["Rabl", "Tilmann", ""], ["Liu", "Wantao", ""], ["Jacobsen", "Hans-Arno", ""], ["Wu", "Kaifeng", ""], ["Chen", "Jian", ""], ["Li", "Jintao", ""]]}, {"id": "1404.6570", "submitter": "Jayanta Mondal", "authors": "Jayanta Mondal and Amol Deshpande", "title": "EAGr: Supporting Continuous Ego-centric Aggregate Queries over Large\n  Dynamic Graphs", "comments": "18 pages, 1 table, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present EAGr, a system for supporting large numbers of\ncontinuous neighborhood-based (\"ego-centric\") aggregate queries over large,\nhighly dynamic, and rapidly evolving graphs. Examples of such queries include\ncomputation of personalized, tailored trends in social networks, anomaly/event\ndetection in financial transaction networks, local search and alerts in\nspatio-temporal networks, to name a few. Key challenges in supporting such\ncontinuous queries include high update rates typically seen in these\nsituations, large numbers of queries that need to be executed simultaneously,\nand stringent low latency requirements. We propose a flexible, general, and\nextensible in-memory framework for executing different types of ego-centric\naggregate queries over large dynamic graphs with low latencies. Our framework\nis built around the notion of an aggregation overlay graph, a pre-compiled data\nstructure that encodes the computations to be performed when an update/query is\nreceived. The overlay graph enables sharing of partial aggregates across\nmultiple ego-centric queries (corresponding to the nodes in the graph), and\nalso allows partial pre-computation of the aggregates to minimize the query\nlatencies. We present several highly scalable techniques for constructing an\noverlay graph given an aggregation function, and also design incremental\nalgorithms for handling structural changes to the underlying graph. We also\npresent an optimal, polynomial-time algorithm for making the pre-computation\ndecisions given an overlay graph, and evaluate an approach to incrementally\nadapt those decisions as the workload changes. Although our approach is\nnaturally parallelizable, we focus on a single-machine deployment and show that\nour techniques can easily handle graphs of size up to 320 million nodes and\nedges, and achieve update/query throughputs of over 500K/s using a single,\npowerful machine.\n", "versions": [{"version": "v1", "created": "Fri, 25 Apr 2014 22:12:40 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Mondal", "Jayanta", ""], ["Deshpande", "Amol", ""]]}, {"id": "1404.6664", "submitter": "Marc Burdon", "authors": "Marc Burdon", "title": "How to extract data from proprietary software database systems using\n  TCP/IP?", "comments": "7 pages, 5 figures, corrected Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document is an white paper about how to connect reverse engineering and\nprograming skills to extract data from a proprietary implementation of a\ndatabase system to build EML-Tools for data format conversion into raw data.\nThis article shows how to access data of a source software system without any\ninterface for data conversion. We discuss how raw data can be transfered into\nstructural format by using XML or any other custom designed software solution.\nFor demonstration purposes only, we will use a CRM system called Harmony(r) by\nHarmony(r) Software AG, the programing language Python and methods of computer\nsecurity, which are used to get quick access to the raw data.\n  All trademarks are property of their owners, as Harmony(r) is of Harmony\nSoftware AG.\n", "versions": [{"version": "v1", "created": "Sat, 26 Apr 2014 17:11:22 GMT"}, {"version": "v2", "created": "Sun, 11 May 2014 20:01:56 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Burdon", "Marc", ""]]}, {"id": "1404.6857", "submitter": "Leopoldo Bertossi", "authors": "Babak Salimi and Leopoldo Bertossi", "title": "Causality in Databases: The Diagnosis and Repair Connections", "comments": "Proc. 15th International Workshop on Non-Monotonic Reasoning (NMR\n  2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we establish and investigate the connections between causality\nfor query answers in databases, database repairs wrt. denial constraints, and\nconsistency-based diagnosis. The first two are relatively new problems in\ndatabases, and the third one is an established subject in knowledge\nrepresentation. We show how to obtain database repairs from causes and the\nother way around. The vast body of research on database repairs can be applied\nto the newer problem of determining actual causes for query answers. By\nformulating a causality problem as a diagnosis problem, we manage to\ncharacterize causes in terms of a system's diagnoses.\n", "versions": [{"version": "v1", "created": "Mon, 28 Apr 2014 02:58:31 GMT"}, {"version": "v2", "created": "Sat, 28 Jun 2014 23:32:35 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Salimi", "Babak", ""], ["Bertossi", "Leopoldo", ""]]}, {"id": "1404.6878", "submitter": "Wantao Liu", "authors": "Songlin Hu, Wantao Liu, Tilmann Rabl, Shuo Huang, Ying Liang, Zheng\n  Xiao, Hans-Arno Jacobsen, Xubin Pei, Jiye Wang", "title": "DualTable: A Hybrid Storage Model for Update Optimization in Hive", "comments": "accepted by industry session of ICDE2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hive is the most mature and prevalent data warehouse tool providing SQL-like\ninterface in the Hadoop ecosystem. It is successfully used in many Internet\ncompanies and shows its value for big data processing in traditional\nindustries. However, enterprise big data processing systems as in Smart Grid\napplications usually require complicated business logics and involve many data\nmanipulation operations like updates and deletes. Hive cannot offer sufficient\nsupport for these while preserving high query performance. Hive using the\nHadoop Distributed File System (HDFS) for storage cannot implement data\nmanipulation efficiently and Hive on HBase suffers from poor query performance\neven though it can support faster data manipulation.There is a project based on\nHive issue Hive-5317 to support update operations, but it has not been finished\nin Hive's latest version. Since this ACID compliant extension adopts same data\nstorage format on HDFS, the update performance problem is not solved.\n  In this paper, we propose a hybrid storage model called DualTable, which\ncombines the efficient streaming reads of HDFS and the random write capability\nof HBase. Hive on DualTable provides better data manipulation support and\npreserves query performance at the same time. Experiments on a TPC-H data set\nand on a real smart grid data set show that Hive on DualTable is up to 10 times\nfaster than Hive when executing update and delete operations.\n", "versions": [{"version": "v1", "created": "Mon, 28 Apr 2014 06:43:36 GMT"}, {"version": "v2", "created": "Mon, 1 Dec 2014 05:45:45 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Hu", "Songlin", ""], ["Liu", "Wantao", ""], ["Rabl", "Tilmann", ""], ["Huang", "Shuo", ""], ["Liang", "Ying", ""], ["Xiao", "Zheng", ""], ["Jacobsen", "Hans-Arno", ""], ["Pei", "Xubin", ""], ["Wang", "Jiye", ""]]}, {"id": "1404.7078", "submitter": "James Cheney", "authors": "James Cheney and Sam Lindley and Philip Wadler", "title": "Query shredding: Efficient relational evaluation of queries over nested\n  multisets (extended version)", "comments": "Extended version of SIGMOD 2014 conference paper", "journal-ref": null, "doi": "10.1145/2588555.2612186", "report-no": null, "categories": "cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nested relational query languages have been explored extensively, and\nunderlie industrial language-integrated query systems such as Microsoft's LINQ.\nHowever, relational databases do not natively support nested collections in\nquery results. This can lead to major performance problems: if programmers\nwrite queries that yield nested results, then such systems typically either\nfail or generate a large number of queries. We present a new approach to query\nshredding, which converts a query returning nested data to a fixed number of\nSQL queries. Our approach, in contrast to prior work, handles multiset\nsemantics, and generates an idiomatic SQL:1999 query directly from a normal\nform for nested queries. We provide a detailed description of our translation\nand present experiments showing that it offers comparable or better performance\nthan a recent alternative approach on a range of examples.\n", "versions": [{"version": "v1", "created": "Mon, 28 Apr 2014 18:13:49 GMT"}, {"version": "v2", "created": "Fri, 2 May 2014 12:03:44 GMT"}], "update_date": "2014-05-05", "authors_parsed": [["Cheney", "James", ""], ["Lindley", "Sam", ""], ["Wadler", "Philip", ""]]}, {"id": "1404.7571", "submitter": "Mina Ghashami", "authors": "Mina Ghashami, Jeff M. Phillips and Feifei Li", "title": "Continuous Matrix Approximation on Distributed Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking and approximating data matrices in streaming fashion is a\nfundamental challenge. The problem requires more care and attention when data\ncomes from multiple distributed sites, each receiving a stream of data. This\npaper considers the problem of \"tracking approximations to a matrix\" in the\ndistributed streaming model. In this model, there are m distributed sites each\nobserving a distinct stream of data (where each element is a row of a\ndistributed matrix) and has a communication channel with a coordinator, and the\ngoal is to track an eps-approximation to the norm of the matrix along any\ndirection. To that end, we present novel algorithms to address the matrix\napproximation problem. Our algorithms maintain a smaller matrix B, as an\napproximation to a distributed streaming matrix A, such that for any unit\nvector x: | ||A x||^2 - ||B x||^2 | <= eps ||A||_F^2. Our algorithms work in\nstreaming fashion and incur small communication, which is critical for\ndistributed computation. Our best method is deterministic and uses only\nO((m/eps) log(beta N)) communication, where N is the size of stream (at the\ntime of the query) and beta is an upper-bound on the squared norm of any row of\nthe matrix. In addition to proving all algorithmic properties theoretically,\nextensive experiments with real large datasets demonstrate the efficiency of\nthese protocols.\n", "versions": [{"version": "v1", "created": "Wed, 30 Apr 2014 01:57:40 GMT"}], "update_date": "2014-05-01", "authors_parsed": [["Ghashami", "Mina", ""], ["Phillips", "Jeff M.", ""], ["Li", "Feifei", ""]]}]