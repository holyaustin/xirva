[{"id": "0909.0685", "submitter": "Chris Giannella", "authors": "Joel W. Branch, Chris Giannella, Boleslaw Szymanski, Ran Wolff, Hillol\n  Kargupta", "title": "In-Network Outlier Detection in Wireless Sensor Networks", "comments": "Extended version of a paper appearing in the Int'l Conference on\n  Distributed Computing Systems 2006", "journal-ref": "Knowledge and Information Systems 34(1) January, 2013, pp. 23-54", "doi": "10.1007/s10115-011-0474-5", "report-no": null, "categories": "cs.DB cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To address the problem of unsupervised outlier detection in wireless sensor\nnetworks, we develop an approach that (1) is flexible with respect to the\noutlier definition, (2) computes the result in-network to reduce both bandwidth\nand energy usage,(3) only uses single hop communication thus permitting very\nsimple node failure detection and message reliability assurance mechanisms\n(e.g., carrier-sense), and (4) seamlessly accommodates dynamic updates to data.\nWe examine performance using simulation with real sensor data streams. Our\nresults demonstrate that our approach is accurate and imposes a reasonable\ncommunication load and level of power consumption.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2009 15:26:38 GMT"}], "update_date": "2013-05-15", "authors_parsed": [["Branch", "Joel W.", ""], ["Giannella", "Chris", ""], ["Szymanski", "Boleslaw", ""], ["Wolff", "Ran", ""], ["Kargupta", "Hillol", ""]]}, {"id": "0909.1127", "submitter": "Raymond Chi-Wing Wong", "authors": "Raymond Chi-Wing Wong, Ada Wai-Chee Fu, Ke Wang, Yabo Xu, Jian Pei,\n  Philip S. Yu", "title": "Anonymization with Worst-Case Distribution-Based Background Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background knowledge is an important factor in privacy preserving data\npublishing. Distribution-based background knowledge is one of the well studied\nbackground knowledge. However, to the best of our knowledge, there is no\nexisting work considering the distribution-based background knowledge in the\nworst case scenario, by which we mean that the adversary has accurate knowledge\nabout the distribution of sensitive values according to some tuple attributes.\nConsidering this worst case scenario is essential because we cannot overlook\nany breaching possibility. In this paper, we propose an algorithm to anonymize\ndataset in order to protect individual privacy by considering this background\nknowledge. We prove that the anonymized datasets generated by our proposed\nalgorithm protects individual privacy. Our empirical studies show that our\nmethod preserves high utility for the published data at the same time.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2009 01:44:36 GMT"}], "update_date": "2009-09-08", "authors_parsed": [["Wong", "Raymond Chi-Wing", ""], ["Fu", "Ada Wai-Chee", ""], ["Wang", "Ke", ""], ["Xu", "Yabo", ""], ["Pei", "Jian", ""], ["Yu", "Philip S.", ""]]}, {"id": "0909.1346", "submitter": "Daniel Lemire", "authors": "Daniel Lemire and Owen Kaser", "title": "Reordering Columns for Smaller Indexes", "comments": "to appear in Information Sciences", "journal-ref": "Daniel Lemire and Owen Kaser, Reordering Columns for Smaller\n  Indexes, Information Sciences 181 (12), 2011", "doi": "10.1016/j.ins.2011.02.002", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Column-oriented indexes-such as projection or bitmap indexes-are compressed\nby run-length encoding to reduce storage and increase speed. Sorting the tables\nimproves compression. On realistic data sets, permuting the columns in the\nright order before sorting can reduce the number of runs by a factor of two or\nmore. Unfortunately, determining the best column order is NP-hard. For many\ncases, we prove that the number of runs in table columns is minimized if we\nsort columns by increasing cardinality. Experimentally, sorting based on\nHilbert space-filling curves is poor at minimizing the number of runs.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2009 21:34:52 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2009 03:38:20 GMT"}, {"version": "v3", "created": "Wed, 6 Jan 2010 01:28:29 GMT"}, {"version": "v4", "created": "Mon, 22 Mar 2010 19:04:11 GMT"}, {"version": "v5", "created": "Mon, 6 Sep 2010 13:00:38 GMT"}, {"version": "v6", "created": "Thu, 6 Jan 2011 23:57:37 GMT"}, {"version": "v7", "created": "Wed, 9 Feb 2011 21:38:42 GMT"}, {"version": "v8", "created": "Tue, 22 Feb 2011 16:00:56 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Lemire", "Daniel", ""], ["Kaser", "Owen", ""]]}, {"id": "0909.1758", "submitter": "Nicolas Bruno", "authors": "Nicolas Bruno (Microsoft)", "title": "Teaching an Old Elephant New Tricks", "comments": "CIDR 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.PF", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In recent years, column stores (or C-stores for short) have emerged as a\nnovel approach to deal with read-mostly data warehousing applications.\nExperimental evidence suggests that, for certain types of queries, the new\nfeatures of C-stores result in orders of magnitude improvement over traditional\nrelational engines. At the same time, some C-store proponents argue that\nC-stores are fundamentally different from traditional engines, and therefore\ntheir benefits cannot be incorporated into a relational engine short of a\ncomplete rewrite. In this paper we challenge this claim and show that many of\nthe benefits of C-stores can indeed be simulated in traditional engines with no\nchanges whatsoever. We then identify some limitations of our ?pure-simulation?\napproach for the case of more complex queries. Finally, we predict that\ntraditional relational engines will eventually leverage most of the benefits of\nC-stores natively, as is currently happening in other domains such as XML data.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2009 18:00:21 GMT"}], "update_date": "2009-09-15", "authors_parsed": [["Bruno", "Nicolas", "", "Microsoft"]]}, {"id": "0909.1759", "submitter": "Boon Thau Loo", "authors": "William Marczak (University of Pennsylvania), David Zook (LogicBlox),\n  Wenchao Zhou (University of Pennsylvania), Molham Aref (LogicBlox), Boon Thau\n  Loo (U. Pennsylvania)", "title": "Declarative Reconfigurable Trust Management", "comments": "CIDR 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In recent years, there has been a proliferation of declarative logic-based\ntrust management languages and systems proposed to ease the description,\nconfiguration, and enforcement of security policies. These systems have\ndifferent tradeoffs in expressiveness and complexity, depending on the security\nconstructs (e.g. authentication, delegation, secrecy, etc.) that are supported,\nand the assumed trust level and scale of the execution environment. In this\npaper, we present LBTrust, a unified declarative system for reconfigurable\ntrust management, where various security constructs can be customized and\ncomposed in a declarative fashion. We present an initial proof-of-concept\nimplementation of LBTrust using LogicBlox, an emerging commercial Datalog-based\nplatform for enterprise software systems. The LogicBlox language enhances\nDatalog in a variety of ways, including constraints and meta-programming, as\nwell as support for programmer defined constraints which on the meta-model\nitself ? meta-constraints ? which act to restrict the set of allowable\nprograms. LBTrust utilizes LogicBlox?s meta-programming and meta-constraints to\nenable customizable cryptographic, partitioning and distribution strategies\nbased on the execution environment. We present uses cases of LBTrust based on\nthree trust management systems (Binder, D1LP, and Secure Network Datalog), and\nprovide a preliminary evaluation of a Binder-based trust management system.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2009 18:05:31 GMT"}], "update_date": "2009-09-15", "authors_parsed": [["Marczak", "William", "", "University of Pennsylvania"], ["Zook", "David", "", "LogicBlox"], ["Zhou", "Wenchao", "", "University of Pennsylvania"], ["Aref", "Molham", "", "LogicBlox"], ["Loo", "Boon Thau", "", "U. Pennsylvania"]]}, {"id": "0909.1760", "submitter": "Xiaodan Wang", "authors": "Xiaodan Wang (Johns Hopkins University), Randal Burns (Johns Hopkins),\n  Tanu Malik (Purdue University)", "title": "LifeRaft: Data-Driven, Batch Processing for the Exploration of\n  Scientific Databases", "comments": "CIDR 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Workloads that comb through vast amounts of data are gaining importance in\nthe sciences. These workloads consist of \"needle in a haystack\" queries that\nare long running and data intensive so that query throughput limits\nperformance. To maximize throughput for data-intensive queries, we put forth\nLifeRaft: a query processing system that batches queries with overlapping data\nrequirements. Rather than scheduling queries in arrival order, LifeRaft\nexecutes queries concurrently against an ordering of the data that maximizes\ndata sharing among queries. This decreases I/O and increases cache utility.\nHowever, such batch processing can increase query response time by starving\ninteractive workloads. LifeRaft addresses starvation using techniques inspired\nby head scheduling in disk drives. Depending upon the workload saturation and\nqueuing times, the system adaptively and incrementally trades-off processing\nqueries in arrival order and data-driven batch processing. Evaluating LifeRaft\nin the SkyQuery federation of astronomy databases reveals a two-fold\nimprovement in query throughput.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2009 18:05:37 GMT"}], "update_date": "2009-09-15", "authors_parsed": [["Wang", "Xiaodan", "", "Johns Hopkins University"], ["Burns", "Randal", "", "Johns Hopkins"], ["Malik", "Tanu", "", "Purdue University"]]}, {"id": "0909.1763", "submitter": "Ragib Hasan", "authors": "Ragib Hasan (University of Illinois), Radu Sion (Stony Brook\n  University), Marianne Winslett (University of Illinois)", "title": "Remembrance: The Unbearable Sentience of Being Digital", "comments": "CIDR 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.OS", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We introduce a world vision in which data is endowed with memory. In this\ndata-centric systems paradigm, data items can be enabled to retain all or some\nof their previous values. We call this ability \"remembrance\" and posit that it\nempowers significant leaps in the security, availability, and general\noperational dimensions of systems. With the explosion in cheap, fast memories\nand storage, large-scale remembrance will soon become practical. Here, we\nintroduce and explore the advantages of such a paradigm and the challenges in\nmaking it a reality.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2009 18:09:06 GMT"}], "update_date": "2009-09-15", "authors_parsed": [["Hasan", "Ragib", "", "University of Illinois"], ["Sion", "Radu", "", "Stony Brook\n  University"], ["Winslett", "Marianne", "", "University of Illinois"]]}, {"id": "0909.1764", "submitter": "Uwe Roehm", "authors": "Uwe Roehm (University of Sydney), Jose Blakeley (Microsoft)", "title": "Data Management for High-Throughput Genomics", "comments": "CIDR 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB q-bio.GN", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Today's sequencing technology allows sequencing an individual genome within a\nfew weeks for a fraction of the costs of the original Human Genome project.\nGenomics labs are faced with dozens of TB of data per week that have to be\nautomatically processed and made available to scientists for further analysis.\nThis paper explores the potential and the limitations of using relational\ndatabase systems as the data processing platform for high-throughput genomics.\nIn particular, we are interested in the storage management for high-throughput\nsequence data and in leveraging SQL and user-defined functions for data\nanalysis inside a database system. We give an overview of a database design for\nhigh-throughput genomics, how we used a SQL Server database in some\nunconventional ways to prototype this scenario, and we will discuss some\ninitial findings about the scalability and performance of such a more\ndatabase-centric approach.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2009 18:09:17 GMT"}], "update_date": "2009-09-15", "authors_parsed": [["Roehm", "Uwe", "", "University of Sydney"], ["Blakeley", "Jose", "", "Microsoft"]]}, {"id": "0909.1765", "submitter": "Arnab Nandi", "authors": "Arnab Nandi (University of Michigan), H V Jagadish (University of\n  Michigan)", "title": "Qunits: queried units in database search", "comments": "CIDR 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Keyword search against structured databases has become a popular topic of\ninvestigation, since many users find structured queries too hard to express,\nand enjoy the freedom of a ``Google-like'' query box into which search terms\ncan be entered. Attempts to address this problem face a fundamental dilemma.\nDatabase querying is based on the logic of predicate evaluation, with a\nprecisely defined answer set for a given query. On the other hand, in an\ninformation retrieval approach, ranked query results have long been accepted as\nfar superior to results based on boolean query evaluation. As a consequence,\nwhen keyword queries are attempted against databases, relatively ad-hoc ranking\nmechanisms are invented (if ranking is used at all), and there is little\nleverage from the large body of IR literature regarding how to rank query\nresults.\n  Our proposal is to create a clear separation between ranking and database\nquerying. This divides the problem into two parts, and allows us to address\nthese separately. The first task is to represent the database, conceptually, as\na collection of independent ``queried units'', or ``qunits'', each of which\nrepresents the desired result for some query against the database. The second\ntask is to evaluate keyword queries against a collection of qunits, which can\nbe treated as independent documents for query purposes, thereby permitting the\nuse of standard IR techniques. We provide insights that encourage the use of\nthis query paradigm, and discuss preliminary investigations into the efficacy\nof a qunits-based framework based on a prototype implementation.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2009 18:09:23 GMT"}], "update_date": "2009-09-15", "authors_parsed": [["Nandi", "Arnab", "", "University of Michigan"], ["Jagadish", "H V", "", "University of\n  Michigan"]]}, {"id": "0909.1766", "submitter": "Yi Zhang", "authors": "Yi Zhang (Duke University), Herodotos Herodotou, Jun Yang (Duke)", "title": "RIOT: I/O-Efficient Numerical Computing without SQL", "comments": "CIDR 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  R is a numerical computing environment that is widely popular for statistical\ndata analysis. Like many such environments, R performs poorly for large\ndatasets whose sizes exceed that of physical memory. We present our vision of\nRIOT (R with I/O Transparency), a system that makes R programs I/O-efficient in\na way transparent to the users. We describe our experience with RIOT-DB, an\ninitial prototype that uses a relational database system as a backend. Despite\nthe overhead and inadequacy of generic database systems in handling array data\nand numerical computation, RIOT-DB significantly outperforms R in many\nlarge-data scenarios, thanks to a suite of high-level, inter-operation\noptimizations that integrate seamlessly into R. While many techniques in RIOT\nare inspired by databases (and, for RIOT-DB, realized by a database system),\nRIOT users are insulated from anything database related. Compared with previous\napproaches that require users to learn new languages and rewrite their programs\nto interface with a database, RIOT will, we believe, be easier to adopt by the\nmajority of the R users.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2009 18:09:27 GMT"}], "update_date": "2009-09-15", "authors_parsed": [["Zhang", "Yi", "", "Duke University"], ["Herodotou", "Herodotos", "", "Duke"], ["Yang", "Jun", "", "Duke"]]}, {"id": "0909.1767", "submitter": "Jignesh Patel", "authors": "Willis Lang (University of Wisconsin-Madiso), Jignesh Patel\n  (Wisconsin)", "title": "Towards Eco-friendly Database Management Systems", "comments": "CIDR 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Database management systems (DBMSs) have largely ignored the task of managing\nthe energy consumed during query processing. Both economical and environmental\nfactors now require that DBMSs pay close attention to energy consumption. In\nthis paper we approach this issue by considering energy consumption as a\nfirst-class performance goal for query processing in a DBMS. We present two\nconcrete techniques that can be used by a DBMS to directly manage the energy\nconsumption. Both techniques trade energy consumption for performance. The\nfirst technique, called PVC, leverages the ability of modern processors to\nexecute at lower processor voltage and frequency. The second technique, called\nQED, uses query aggregation to leverage common components of queries in a\nworkload. Using experiments run on a commercial DBMS and MySQL, we show that\nPVC can reduce the processor energy consumption by 49% of the original\nconsumption while increasing the response time by only 3%. On MySQL, PVC can\nreduce energy consumption by 20% with a response time penalty of only 6%. For\nsimple selection queries with no predicate overlap, we show that QED can be\nused to gracefully trade response time for energy, reducing energy consumption\nby 54% for a 43% increase in average response time. In this paper we also\nhighlight some research issues in the emerging area of energy-efficient data\nprocessing.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2009 18:09:31 GMT"}], "update_date": "2009-09-15", "authors_parsed": [["Lang", "Willis", "", "University of Wisconsin-Madiso"], ["Patel", "Jignesh", "", "Wisconsin"]]}, {"id": "0909.1768", "submitter": "Alan Fekete", "authors": "David Lomet (Microsoft Research), Alan Fekete (University of Sydney),\n  Gerhard Weikum (Max Plank Institute), Mike Zwilling (Microsoft SQL Server)", "title": "Unbundling Transaction Services in the Cloud", "comments": "CIDR 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The traditional architecture for a DBMS engine has the recovery, concurrency\ncontrol and access method code tightly bound together in a storage engine for\nrecords. We propose a different approach, where the storage engine is factored\ninto two layers (each of which might have multiple heterogeneous instances). A\nTransactional Component (TC) works at a logical level only: it knows about\ntransactions and their \"logical\" concurrency control and undo/redo recovery,\nbut it does not know about page layout, B-trees etc. A Data Component (DC)\nknows about the physical storage structure. It supports a record oriented\ninterface that provides atomic operations, but it does not know about\ntransactions. Providing atomic record operations may itself involve DC-local\nconcurrency control and recovery, which can be implemented using system\ntransactions. The interaction of the mechanisms in TC and DC leads to\nmulti-level redo (unlike the repeat history paradigm for redo in integrated\nengines). This refactoring of the system architecture could allow easier\ndeployment of application-specific physical structures and may also be helpful\nto exploit multi-core hardware. Particularly promising is its potential to\nenable flexible transactions in cloud database deployments. We describe the\nnecessary principles for unbundled recovery, and discuss implementation issues.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2009 18:09:36 GMT"}], "update_date": "2009-09-15", "authors_parsed": [["Lomet", "David", "", "Microsoft Research"], ["Fekete", "Alan", "", "University of Sydney"], ["Weikum", "Gerhard", "", "Max Plank Institute"], ["Zwilling", "Mike", "", "Microsoft SQL Server"]]}, {"id": "0909.1769", "submitter": "Zachary Ives", "authors": "Zachary Ives (University Of Pennsylvania), Craig Knoblock (University\n  of Southern California - Information Sciences Institute), Steve Minton (Fetch\n  Technologies), Marie Jacob (University of Pennsylvania), Partha Talukdar\n  (University of Pennsylvania), Rattapoom Tuchinda (University of Southern\n  California - Information Sciences Institute), Jose Luis Ambite (University of\n  Southern California - Information Sciences Institute), Maria Muslea\n  (University of Southern California - Information Sciences Institute), Cenk\n  Gazen (Fetch Technologies)", "title": "Interactive Data Integration through Smart Copy & Paste", "comments": "CIDR 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In many scenarios, such as emergency response or ad hoc collaboration, it is\ncritical to reduce the overhead in integrating data. Ideally, one could perform\nthe entire process interactively under one unified interface: defining\nextractors and wrappers for sources, creating a mediated schema, and adding\nschema mappings ? while seeing how these impact the integrated view of the\ndata, and refining the design accordingly.\n  We propose a novel smart copy and paste (SCP) model and architecture for\nseamlessly combining the design-time and run-time aspects of data integration,\nand we describe an initial prototype, the CopyCat system. In CopyCat, the user\ndoes not need special tools for the different stages of integration: instead,\nthe system watches as the user copies data from applications (including the Web\nbrowser) and pastes them into CopyCat?s spreadsheet-like workspace. CopyCat\ngeneralizes these actions and presents proposed auto-completions, each with an\nexplanation in the form of provenance. The user provides feedback on these\nsuggestions ? through either direct interactions or further copy-and-paste\noperations ? and the system learns from this feedback. This paper provides an\noverview of our prototype system, and identifies key research challenges in\nachieving SCP in its full generality.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2009 18:09:41 GMT"}], "update_date": "2009-09-15", "authors_parsed": [["Ives", "Zachary", "", "University Of Pennsylvania"], ["Knoblock", "Craig", "", "University\n  of Southern California - Information Sciences Institute"], ["Minton", "Steve", "", "Fetch\n  Technologies"], ["Jacob", "Marie", "", "University of Pennsylvania"], ["Talukdar", "Partha", "", "University of Pennsylvania"], ["Tuchinda", "Rattapoom", "", "University of Southern\n  California - Information Sciences Institute"], ["Ambite", "Jose Luis", "", "University of\n  Southern California - Information Sciences Institute"], ["Muslea", "Maria", "", "University of Southern California - Information Sciences Institute"], ["Gazen", "Cenk", "", "Fetch Technologies"]]}, {"id": "0909.1770", "submitter": "Benjamin Sowell", "authors": "Benjamin Sowell (Cornell University), Alan Demers (Cornell\n  University), Johannes Gehrke (Cornell University), Nitin Gupta (Cornell\n  University), Haoyuan Li (Cornell University), Walker White (Cornell\n  University)", "title": "From Declarative Languages to Declarative Processing in Computer Games", "comments": "CIDR 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.MA", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Recent work has shown that we can dramatically improve the performance of\ncomputer games and simulations through declarative processing: Character AI can\nbe written in an imperative scripting language which is then compiled to\nrelational algebra and executed by a special games engine with features similar\nto a main memory database system. In this paper we lay out a challenging\nresearch agenda built on these ideas.\n  We discuss several research ideas for novel language features to support\natomic actions and reactive programming. We also explore challenges for\nmain-memory query processing in games and simulations including adaptive query\nplan selection, support for parallel architectures, debugging simulation\nscripts, and extensions for multi-player games and virtual worlds. We believe\nthat these research challenges will result in a dramatic change in the design\nof game engines over the next decade.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2009 18:09:44 GMT"}], "update_date": "2009-09-15", "authors_parsed": [["Sowell", "Benjamin", "", "Cornell University"], ["Demers", "Alan", "", "Cornell\n  University"], ["Gehrke", "Johannes", "", "Cornell University"], ["Gupta", "Nitin", "", "Cornell\n  University"], ["Li", "Haoyuan", "", "Cornell University"], ["White", "Walker", "", "Cornell\n  University"]]}, {"id": "0909.1771", "submitter": "Ken Smith", "authors": "Ken Smith (MITRE), Michael Morse (MITRE), Peter Mork (MITRE), Maya Li\n  (MITRE), Arnon Rosenthal, David Allen, Len Seligman, Chris Wolf (MITRE)", "title": "The Role of Schema Matching in Large Enterprises", "comments": "CIDR 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  To date, the principal use case for schema matching research has been as a\nprecursor for code generation, i.e., constructing mappings between schema\nelements with the end goal of data transfer. In this paper, we argue that\nschema matching plays valuable roles independent of mapping construction,\nespecially as schemata grow to industrial scales. Specifically, in large\nenterprises human decision makers and planners are often the immediate consumer\nof information derived from schema matchers, instead of schema mapping tools.\nWe list a set of real application areas illustrating this role for schema\nmatching, and then present our experiences tackling a customer problem in one\nof these areas. We describe the matcher used, where the tool was effective,\nwhere it fell short, and our lessons learned about how well current schema\nmatching technology is suited for use in large enterprises. Finally, we suggest\na new agenda for schema matching research based on these experiences.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2009 18:09:48 GMT"}], "update_date": "2009-09-15", "authors_parsed": [["Smith", "Ken", "", "MITRE"], ["Morse", "Michael", "", "MITRE"], ["Mork", "Peter", "", "MITRE"], ["Li", "Maya", "", "MITRE"], ["Rosenthal", "Arnon", "", "MITRE"], ["Allen", "David", "", "MITRE"], ["Seligman", "Len", "", "MITRE"], ["Wolf", "Chris", "", "MITRE"]]}, {"id": "0909.1772", "submitter": "Harumi Kuno", "authors": "Goetz Graefe (HP), Harumi Kuno (Hewlett-Packard Co.), Janet Wiener\n  (Hewlett-Packard, Co.)", "title": "Visualizing the robustness of query execution", "comments": "CIDR 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PF", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In database query processing, actual run-time conditions (e.g., actual\nselectivities and actual available memory) very often differ from compile-time\nexpectations of run-time conditions (e.g., estimated predicate selectivities\nand anticipated memory availability). Robustness of query processing can be\ndefined as the ability to handle unexpected conditions. Robustness of query\nexecution, specifically, can be defined as the ability to process a specific\nplan efficiently in an unexpected condition. We focus on query execution\n(run-time), ignoring query optimization (compile-time), in order to complement\nexisting research and to explore untapped potential for improved robustness in\ndatabase query processing.\n  One of our initial steps has been to devise diagrams or maps that show how\nwell plans perform in the face of varying run-time conditions and how\ngracefully a system's query architecture, operators, and their implementation\ndegrade in the face of adverse conditions. In this paper, we show several kinds\nof diagrams with data from three real systems and report on what we have\nlearned both about these visualization techniques and about the three database\nsystems\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2009 18:09:52 GMT"}], "update_date": "2009-09-15", "authors_parsed": [["Graefe", "Goetz", "", "HP"], ["Kuno", "Harumi", "", "Hewlett-Packard Co."], ["Wiener", "Janet", "", "Hewlett-Packard, Co."]]}, {"id": "0909.1773", "submitter": "Fatma Ozcan", "authors": "Andrey Balmin (IBM Almaden Research Center), Latha Colby (IBM Almaden\n  Research Center), Emiran Curtmola (UC, San Diego), Quanzhong Li (IBM Almaden\n  Research Center), Fatma Ozcan (IBM Almaden Research Center)", "title": "Search Driven Analysis of Heterogenous XML Data", "comments": "CIDR 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Analytical processing on XML repositories is usually enabled by designing\ncomplex data transformations that shred the documents into a common data\nwarehousing schema. This can be very time-consuming and costly, especially if\nthe underlying XML data has a lot of variety in structure, and only a subset of\nattributes constitutes meaningful dimensions and facts. Today, there is no tool\nto explore an XML data set, discover interesting attributes, dimensions and\nfacts, and rapidly prototype an OLAP solution.\n  In this paper, we propose a system, called SEDA that enables users to start\nwith simple keyword-style querying, and interactively refine the query based on\nresult summaries. SEDA then maps query results onto a set of known, or newly\ncreated, facts and dimensions, and derives a star schema and its instantiation\nto be fed into an off-the-shelf OLAP tool, for further analysis.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2009 18:09:55 GMT"}], "update_date": "2009-09-15", "authors_parsed": [["Balmin", "Andrey", "", "IBM Almaden Research Center"], ["Colby", "Latha", "", "IBM Almaden\n  Research Center"], ["Curtmola", "Emiran", "", "UC, San Diego"], ["Li", "Quanzhong", "", "IBM Almaden\n  Research Center"], ["Ozcan", "Fatma", "", "IBM Almaden Research Center"]]}, {"id": "0909.1774", "submitter": "Georgia Koutrika", "authors": "Georgia Koutrika (Stanford University), Benjamin Bercovitz (Stanford\n  University), Robert Ikeda (Stanford University), Filip Kaliszan (Stanford\n  University), Henry Liou (Stanford University), Zahra Mohammadi Zadeh\n  (Stanford University), Hector Garcia-Molina (Stanford University)", "title": "Social Systems: Can we Do More Than Just Poke Friends?", "comments": "CIDR 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CY", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Social sites have become extremely popular among users but have they\nattracted equal attention from the research community? Are they good only for\nsimple tasks, such as tagging and poking friends? Do they present any new or\ninteresting research challenges? In this paper, we describe the insights we\nhave obtained implementing CourseRank, a course evaluation and planning social\nsystem. We argue that more attention should be given to social sites like ours\nand that there are many challenges (though not the traditional DBMS ones) that\nshould be addressed by our community.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2009 18:10:01 GMT"}], "update_date": "2009-09-15", "authors_parsed": [["Koutrika", "Georgia", "", "Stanford University"], ["Bercovitz", "Benjamin", "", "Stanford\n  University"], ["Ikeda", "Robert", "", "Stanford University"], ["Kaliszan", "Filip", "", "Stanford\n  University"], ["Liou", "Henry", "", "Stanford University"], ["Zadeh", "Zahra Mohammadi", "", "Stanford University"], ["Garcia-Molina", "Hector", "", "Stanford University"]]}, {"id": "0909.1775", "submitter": "Michael Armbrust", "authors": "Michael Armbrust (UC Berkeley), Armando Fox (UC Berkeley), David\n  Patterson (UC Berkeley), Nick Lanham (UC Berkeley), Beth Trushkowsky (UC\n  Berkeley), Jesse Trutna (UC Berkeley), Haruki Oh (UC Berkeley)", "title": "SCADS: Scale-Independent Storage for Social Computing Applications", "comments": "CIDR 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Collaborative web applications such as Facebook, Flickr and Yelp present new\nchallenges for storing and querying large amounts of data. As users and\ndevelopers are focused more on performance than single copy consistency or the\nability to perform ad-hoc queries, there exists an opportunity for a\nhighly-scalable system tailored specifically for relaxed consistency and\npre-computed queries. The Web 2.0 development model demands the ability to both\nrapidly deploy new features and automatically scale with the number of users.\nThere have been many successful distributed key-value stores, but so far none\nprovide as rich a query language as SQL. We propose a new architecture, SCADS,\nthat allows the developer to declaratively state application specific\nconsistency requirements, takes advantage of utility computing to provide cost\neffective scale-up and scale-down, and will use machine learning models to\nintrospectively anticipate performance problems and predict the resource\nrequirements of new queries before execution.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2009 18:10:04 GMT"}], "update_date": "2009-09-15", "authors_parsed": [["Armbrust", "Michael", "", "UC Berkeley"], ["Fox", "Armando", "", "UC Berkeley"], ["Patterson", "David", "", "UC Berkeley"], ["Lanham", "Nick", "", "UC Berkeley"], ["Trushkowsky", "Beth", "", "UC\n  Berkeley"], ["Trutna", "Jesse", "", "UC Berkeley"], ["Oh", "Haruki", "", "UC Berkeley"]]}, {"id": "0909.1776", "submitter": "Xin (Luna) Dong", "authors": "Laure Berti-Equille (Universite de Rennes 1), Anish Das Sarma\n  (Stanford University), Xin (Luna) Dong (AT&T Labs-Research), Amelie Marian\n  (Rutgus University), Divesh Srivastava (ATT Labs-Research)", "title": "Sailing the Information Ocean with Awareness of Currents: Discovery and\n  Application of Source Dependence", "comments": "CIDR 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The Web has enabled the availability of a huge amount of useful information,\nbut has also eased the ability to spread false information and rumors across\nmultiple sources, making it hard to distinguish between what is true and what\nis not. Recent examples include the premature Steve Jobs obituary, the second\nbankruptcy of United airlines, the creation of Black Holes by the operation of\nthe Large Hadron Collider, etc. Since it is important to permit the expression\nof dissenting and conflicting opinions, it would be a fallacy to try to ensure\nthat the Web provides only consistent information. However, to help in\nseparating the wheat from the chaff, it is essential to be able to determine\ndependence between sources. Given the huge number of data sources and the vast\nvolume of conflicting data available on the Web, doing so in a scalable manner\nis extremely challenging and has not been addressed by existing work yet.\n  In this paper, we present a set of research problems and propose some\npreliminary solutions on the issues involved in discovering dependence between\nsources. We also discuss how this knowledge can benefit a variety of\ntechnologies, such as data integration and Web 2.0, that help users manage and\naccess the totality of the available information from various sources.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2009 18:10:07 GMT"}], "update_date": "2009-09-15", "authors_parsed": [["Berti-Equille", "Laure", "", "Universite de Rennes 1"], ["Sarma", "Anish Das", "", "Stanford University"], ["Xin", "", "", "Luna"], ["Dong", "", "", "AT&T Labs-Research"], ["Marian", "Amelie", "", "Rutgus University"], ["Srivastava", "Divesh", "", "ATT Labs-Research"]]}, {"id": "0909.1777", "submitter": "Yanlei Diao", "authors": "Yanlei Diao (U. Massachusetts-Amherst), Boduo Li (University of\n  Massachusetts Amherst), Anna Liu (UMass Amherst), Liping Peng (UMass\n  Amherst), Charles Sutton (UC Berkeley), Thanh Tran (UMass Amherst), Michael\n  Zink", "title": "Capturing Data Uncertainty in High-Volume Stream Processing", "comments": "CIDR 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We present the design and development of a data stream system that captures\ndata uncertainty from data collection to query processing to final result\ngeneration. Our system focuses on data that is naturally modeled as continuous\nrandom variables. For such data, our system employs an approach grounded in\nprobability and statistical theory to capture data uncertainty and integrates\nthis approach into high-volume stream processing. The first component of our\nsystem captures uncertainty of raw data streams from sensing devices. Since\nsuch raw streams can be highly noisy and may not carry sufficient information\nfor query processing, our system employs probabilistic models of the data\ngeneration process and stream-speed inference to transform raw data into a\ndesired format with an uncertainty metric. The second component captures\nuncertainty as data propagates through query operators. To efficiently quantify\nresult uncertainty of a query operator, we explore a variety of techniques\nbased on probability and statistical theory to compute the result distribution\nat stream speed. We are currently working with a group of scientists to\nevaluate our system using traces collected from the domains of (and eventually\nin the real systems for) hazardous weather monitoring and object tracking and\nmonitoring.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2009 18:10:11 GMT"}], "update_date": "2009-09-15", "authors_parsed": [["Diao", "Yanlei", "", "U. Massachusetts-Amherst"], ["Li", "Boduo", "", "University of\n  Massachusetts Amherst"], ["Liu", "Anna", "", "UMass Amherst"], ["Peng", "Liping", "", "UMass\n  Amherst"], ["Sutton", "Charles", "", "UC Berkeley"], ["Tran", "Thanh", "", "UMass Amherst"], ["Zink", "Michael", ""]]}, {"id": "0909.1778", "submitter": "Nodira Khoussainova", "authors": "Nodira Khoussainova (University of Washington), Magda Balazinska (U.\n  Washington), Wolfgang Gatterbauer (University of Washington), YongChul Kwon\n  (University of Washington), Dan Suciu (University of Washington)", "title": "A Case for A Collaborative Query Management System", "comments": "CIDR 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Over the past 40 years, database management systems (DBMSs) have evolved to\nprovide a sophisticated variety of data management capabilities. At the same\ntime, tools for managing queries over the data have remained relatively\nprimitive. One reason for this is that queries are typically issued through\napplications. They are thus debugged once and re-used repeatedly. This mode of\ninteraction, however, is changing. As scientists (and others) store and share\nincreasingly large volumes of data in data centers, they need the ability to\nanalyze the data by issuing exploratory queries. In this paper, we argue that,\nin these new settings, data management systems must provide powerful query\nmanagement capabilities, from query browsing to automatic query\nrecommendations. We first discuss the requirements for a collaborative query\nmanagement system. We outline an early system architecture and discuss the many\nresearch challenges associated with building such an engine.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2009 18:10:16 GMT"}], "update_date": "2009-09-15", "authors_parsed": [["Khoussainova", "Nodira", "", "University of Washington"], ["Balazinska", "Magda", "", "U.\n  Washington"], ["Gatterbauer", "Wolfgang", "", "University of Washington"], ["Kwon", "YongChul", "", "University of Washington"], ["Suciu", "Dan", "", "University of Washington"]]}, {"id": "0909.1779", "submitter": "Sam Madden", "authors": "Philippe Cudre-Mauroux (MIT), Eugene Wu (MIT), Sam Madden (MIT)", "title": "The Case for RodentStore, an Adaptive, Declarative Storage System", "comments": "CIDR 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Recent excitement in the database community surrounding new\napplications?analytic, scientific, graph, geospatial, etc.?has led to an\nexplosion in research on database storage systems. New storage systems are\nvital to the database community, as they are at the heart of making database\nsystems perform well in new application domains. Unfortunately, each such\nsystem also represents a substantial engineering effort including a great deal\nof duplication of mechanisms for features such as transactions and caching. In\nthis paper, we make the case for RodentStore, an adaptive and declarative\nstorage system providing a high-level interface for describing the physical\nrepresentation of data. Specifically, RodentStore uses a declarative storage\nalgebra whereby administrators (or database design tools) specify how a logical\nschema should be grouped into collections of rows, columns, and/or arrays, and\nthe order in which those groups should be laid out on disk. We describe the key\noperators and types of our algebra, outline the general architecture of\nRodentStore, which interprets algebraic expressions to generate a physical\nrepresentation of the data, and describe the interface between RodentStore and\nother parts of a database system, such as the query optimizer and executor. We\nprovide a case study of the potential use of RodentStore in representing dense\ngeospatial data collected from a mobile sensor network, showing the ease with\nwhich different storage layouts can be expressed using some of our algebraic\nconstructs and the potential performance gains that a RodentStore-built storage\nsystem can offer.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2009 18:10:20 GMT"}], "update_date": "2009-09-15", "authors_parsed": [["Cudre-Mauroux", "Philippe", "", "MIT"], ["Wu", "Eugene", "", "MIT"], ["Madden", "Sam", "", "MIT"]]}, {"id": "0909.1781", "submitter": "Marcos Vieira", "authors": "Abhishek Mitra (UC Riverside), Marcos Vieira (UCR), Petko Bakalov\n  (ESRI), Walid Najjar (UCR), Vassilis Tsotras (UCR)", "title": "Boosting XML Filtering with a Scalable FPGA-based Architecture", "comments": "CIDR 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The growing amount of XML encoded data exchanged over the Internet increases\nthe importance of XML based publish-subscribe (pub-sub) and content based\nrouting systems. The input in such systems typically consists of a stream of\nXML documents and a set of user subscriptions expressed as XML queries. The\npub-sub system then filters the published documents and passes them to the\nsubscribers. Pub-sub systems are characterized by very high input ratios,\ntherefore the processing time is critical. In this paper we propose a \"pure\nhardware\" based solution, which utilizes XPath query blocks on FPGA to solve\nthe filtering problem. By utilizing the high throughput that an FPGA provides\nfor parallel processing, our approach achieves drastically better throughput\nthan the existing software or mixed (hardware/software) architectures. The\nXPath queries (subscriptions) are translated to regular expressions which are\nthen mapped to FPGA devices. By introducing stacks within the FPGA we are able\nto express and process a wide range of path queries very efficiently, on a\nscalable environment. Moreover, the fact that the parser and the filter\nprocessing are performed on the same FPGA chip, eliminates expensive\ncommunication costs (that a multi-core system would need) thus enabling very\nfast and efficient pipelining. Our experimental evaluation reveals more than\none order of magnitude improvement compared to traditional pub/sub systems.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2009 18:10:30 GMT"}], "update_date": "2009-09-15", "authors_parsed": [["Mitra", "Abhishek", "", "UC Riverside"], ["Vieira", "Marcos", "", "UCR"], ["Bakalov", "Petko", "", "ESRI"], ["Najjar", "Walid", "", "UCR"], ["Tsotras", "Vassilis", "", "UCR"]]}, {"id": "0909.1782", "submitter": "Shel Finkelstein", "authors": "Shel Finkelstein (SAP), Dean Jacobs (SAP), Rainer Brendle (SAP)", "title": "Principles for Inconsistency", "comments": "CIDR 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Data consistency is very desirable because strong semantic properties make it\neasier to write correct programs that perform as users expect. However, there\nare good reasons why consistency may have to be weakened to achieve other\nbusiness goals. In this CIDR 2009 Perspectives paper, we present real-world\nreasons inconsistency may be necessary, offer principles for managing\ninconsistency coherently, and describe implementation approaches we are\ninvestigating for sustainably scalable systems that offer comprehensible user\nexperiences despite inconsistency.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2009 18:10:33 GMT"}], "update_date": "2009-09-15", "authors_parsed": [["Finkelstein", "Shel", "", "SAP"], ["Jacobs", "Dean", "", "SAP"], ["Brendle", "Rainer", "", "SAP"]]}, {"id": "0909.1783", "submitter": "AnHai Doan", "authors": "AnHai Doan (Univ of Wisconsin), Jeff Naughton (Wisconsin), Akanksha\n  Baid (Wisconsin), Xiaoyong Chai (Wisconsin), Fei Chen (Wisconsin), Ting Chen\n  (Wisconsin), Eric Chu (Wisconsin), Pedro DeRose (Wisconsin), Byron Gao\n  (Wisconsin), Chaitanya Gokhale (Wisconsin), Jiansheng Huang (Wisconsin),\n  Warren Shen (Wisconsin), Ba-Quy Vuong (Wisconsin)", "title": "The Case for a Structured Approach to Managing Unstructured Data", "comments": "CIDR 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The challenge of managing unstructured data represents perhaps the largest\ndata management opportunity for our community since managing relational data.\nAnd yet we are risking letting this opportunity go by, ceding the playing field\nto other players, ranging from communities such as AI, KDD, IR, Web, and\nSemantic Web, to industrial players such as Google, Yahoo, and Microsoft. In\nthis essay we explore what we can do to improve upon this situation. Drawing on\nthe lessons learned while managing relational data, we outline a structured\napproach to managing unstructured data. We conclude by discussing the potential\nimplications of this approach to managing other kinds of non-relational data,\nand to the identify of our field.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2009 18:10:36 GMT"}], "update_date": "2009-09-15", "authors_parsed": [["Doan", "AnHai", "", "Univ of Wisconsin"], ["Naughton", "Jeff", "", "Wisconsin"], ["Baid", "Akanksha", "", "Wisconsin"], ["Chai", "Xiaoyong", "", "Wisconsin"], ["Chen", "Fei", "", "Wisconsin"], ["Chen", "Ting", "", "Wisconsin"], ["Chu", "Eric", "", "Wisconsin"], ["DeRose", "Pedro", "", "Wisconsin"], ["Gao", "Byron", "", "Wisconsin"], ["Gokhale", "Chaitanya", "", "Wisconsin"], ["Huang", "Jiansheng", "", "Wisconsin"], ["Shen", "Warren", "", "Wisconsin"], ["Vuong", "Ba-Quy", "", "Wisconsin"]]}, {"id": "0909.1784", "submitter": "Stavros Harizopoulos", "authors": "Stavros Harizopoulos (HP Labs), Mehul Shah, Justin Meza (UCLA),\n  Parthasarathy Ranganathan (HP Labs)", "title": "Energy Efficiency: The New Holy Grail of Data Management Systems\n  Research", "comments": "CIDR 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PF", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Energy costs are quickly rising in large-scale data centers and are soon\nprojected to overtake the cost of hardware. As a result, data center operators\nhave recently started turning into using more energy-friendly hardware. Despite\nthe growing body of research in power management techniques, there has been\nlittle work to date on energy efficiency from a data management software\nperspective.\n  In this paper, we argue that hardware-only approaches are only part of the\nsolution, and that data management software will be key in optimizing for\nenergy efficiency. We discuss the problems arising from growing energy use in\ndata centers and the trends that point to an increasing set of opportunities\nfor software-level optimizations. Using two simple experiments, we illustrate\nthe potential of such optimizations, and, motivated by these examples, we\ndiscuss general approaches for reducing energy waste. Lastly, we point out\nexisting places within database systems that are promising for\nenergy-efficiency optimizations and urge the data management systems community\nto shift focus from performance-oriented research to energy-efficient\ncomputing.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2009 18:10:39 GMT"}], "update_date": "2009-09-15", "authors_parsed": [["Harizopoulos", "Stavros", "", "HP Labs"], ["Shah", "Mehul", "", "UCLA"], ["Meza", "Justin", "", "UCLA"], ["Ranganathan", "Parthasarathy", "", "HP Labs"]]}, {"id": "0909.1785", "submitter": "Jayant Madhavan", "authors": "Jayant Madhavan (Google Inc.), Loredana Afanasiev (Universiteit van\n  Amsterdam), Lyublena Antova (Cornell University), Alon Halevy (Google)", "title": "Harnessing the Deep Web: Present and Future", "comments": "CIDR 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Over the past few years, we have built a system that has exposed large\nvolumes of Deep-Web content to Google.com users. The content that our system\nexposes contributes to more than 1000 search queries per-second and spans over\n50 languages and hundreds of domains. The Deep Web has long been acknowledged\nto be a major source of structured data on the web, and hence accessing\nDeep-Web content has long been a problem of interest in the data management\ncommunity. In this paper, we report on where we believe the Deep Web provides\nvalue and where it does not. We contrast two very different approaches to\nexposing Deep-Web content -- the surfacing approach that we used, and the\nvirtual integration approach that has often been pursued in the data management\nliterature. We emphasize where the values of each of the two approaches lie and\ncaution against potential pitfalls. We outline important areas of future\nresearch and, in particular, emphasize the value that can be derived from\nanalyzing large collections of potentially disparate structured data on the\nweb.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2009 18:10:42 GMT"}], "update_date": "2009-09-15", "authors_parsed": [["Madhavan", "Jayant", "", "Google Inc."], ["Afanasiev", "Loredana", "", "Universiteit van\n  Amsterdam"], ["Antova", "Lyublena", "", "Cornell University"], ["Halevy", "Alon", "", "Google"]]}, {"id": "0909.1786", "submitter": "Alkis Simitsis", "authors": "Alkis Simitsis (HP Labs), Yannis Ioannidis", "title": "DBMSs Should Talk Back Too", "comments": "CIDR 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.HC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Natural language user interfaces to database systems have been studied for\nseveral decades now. They have mainly focused on parsing and interpreting\nnatural language queries to generate them in a formal database language. We\nenvision the reverse functionality, where the system would be able to take the\ninternal result of that translation, say in SQL form, translate it back into\nnatural language, and show it to the initiator of the query for verification.\nLikewise, information extraction has received considerable attention in the\npast ten years or so, identifying structured information in free text so that\nit may then be stored appropriately and queried. Validation of the records\nstored with a backward translation into text would again be very powerful.\nVerification and validation of query and data input of a database system\ncorrespond to just one example of the many important applications that would\nbenefit greatly from having mature techniques for translating such database\nconstructs into free-flowing text. The problem appears to be deceivingly\nsimple, as there are no ambiguities or other complications in interpreting\ninternal database elements, so initially a straightforward translation appears\nadequate. Reality teaches us quite the opposite, however, as the resulting text\nshould be expressive, i.e., accurate in capturing the underlying queries or\ndata, and effective, i.e., allowing fast and unique interpretation of them.\nAchieving both of these qualities is very difficult and raises several\ntechnical challenges that need to be addressed. In this paper, we first expose\nthe reader to several situations and applications that need translation into\nnatural language, thereby, motivating the problem. We then outline, by example,\nthe research problems that need to be solved, separately for data translations\nand query translations.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2009 18:10:46 GMT"}], "update_date": "2009-09-15", "authors_parsed": [["Simitsis", "Alkis", "", "HP Labs"], ["Ioannidis", "Yannis", ""]]}, {"id": "0909.2030", "submitter": "Gregory Valiant", "authors": "Gregory Valiant and Paul Valiant", "title": "Size Bounds for Conjunctive Queries with General Functional Dependencies", "comments": "22 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper extends the work of Gottlob, Lee, and Valiant (PODS 2009)[GLV],\nand considers worst-case bounds for the size of the result Q(D) of a\nconjunctive query Q to a database D given an arbitrary set of functional\ndependencies. The bounds in [GLV] are based on a \"coloring\" of the query\nvariables. In order to extend the previous bounds to the setting of arbitrary\nfunctional dependencies, we leverage tools from information theory to formalize\nthe original intuition that each color used represents some possible entropy of\nthat variable, and bound the maximum possible size increase via a linear\nprogram that seeks to maximize how much more entropy is in the result of the\nquery than the input. This new view allows us to precisely characterize the\nentropy structure of worst-case instances for conjunctive queries with simple\nfunctional dependencies (keys), providing new insights into the results of\n[GLV]. We extend these results to the case of general functional dependencies,\nproviding upper and lower bounds on the worst-case size increase. We identify\nthe fundamental connection between the gap in these bounds and a central open\nquestion in information theory.\n  Finally, we show that, while both the upper and lower bounds are given by\nexponentially large linear programs, one can distinguish in polynomial time\nwhether the result of a query with an arbitrary set of functional dependencies\ncan be any larger than the input database.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2009 20:28:20 GMT"}, {"version": "v2", "created": "Sat, 12 Dec 2009 08:00:21 GMT"}], "update_date": "2009-12-12", "authors_parsed": [["Valiant", "Gregory", ""], ["Valiant", "Paul", ""]]}, {"id": "0909.2058", "submitter": "Sihem Amer-Yahia", "authors": "Sihem Amer-Yahia (Yahoo! Research), Laks Lakshmanan (UBC), Cong Yu\n  (Yahoo! Research)", "title": "SocialScope: Enabling Information Discovery on Social Content Sites", "comments": "CIDR 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.HC cs.IR cs.PL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Recently, many content sites have started encouraging their users to engage\nin social activities such as adding buddies on Yahoo! Travel and sharing\narticles with their friends on New York Times. This has led to the emergence of\n{\\em social content sites}, which is being facilitated by initiatives like\nOpenID (http://www.openid.net/) and OpenSocial (http://www.opensocial.org/).\nThese community standards enable the open access to users' social profiles and\nconnections by individual content sites and are bringing content-oriented sites\nand social networking sites ever closer. The integration of content and social\ninformation raises new challenges for {\\em information management and\ndiscovery} over such sites. We propose a logical architecture, named\n\\kw{SocialScope}, consisting of three layers, for tackling the challenges. The\n{\\em content management} layer is responsible for integrating, maintaining and\nphysically accessing the content and social data. The {\\em information\ndiscovery} layer takes care of analyzing content to derive interesting new\ninformation, and interpreting and processing the user's information need to\nidentify relevant information. Finally, the {\\em information presentation}\nlayer explores the discovered information and helps users better understand it\nin a principled way. We describe the challenges in each layer and propose\nsolutions for some of those challenges. In particular, we propose a uniform\nalgebraic framework, which can be leveraged to uniformly and flexibly specify\nmany of the information discovery and analysis tasks and provide the foundation\nfor the optimization of those tasks.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2009 22:08:17 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Amer-Yahia", "Sihem", "", "Yahoo! Research"], ["Lakshmanan", "Laks", "", "UBC"], ["Yu", "Cong", "", "Yahoo! Research"]]}, {"id": "0909.2062", "submitter": "Rafael Fernandez-Moctezuma", "authors": "Rafael Fern\\'andez-Moctezuma (Portland State University), Kristin\n  Tufte (Portland State University), Jin Li (Portland State University)", "title": "Inter-Operator Feedback in Data Stream Management Systems via\n  Punctuation", "comments": "CIDR 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  High-volume, high-speed data streams may overwhelm the capabilities of stream\nprocessing systems; techniques such as data prioritization, avoidance of\nunnecessary processing and on-demand result production may be necessary to\nreduce processing requirements. However, the dynamic nature of data streams, in\nterms of both rate and content, makes the application of such techniques\nchallenging. Such techniques have been addressed in the context of static and\ncentralized query optimization; however, they have not been fully addressed for\ndata stream management systems. In this work, we present a comprehensive\nframework that supports prioritization, avoidance of unnecessary work, and\non-demand result production over distributed, unreliable, bursty, disordered\ndata sources, typical of many data streams. We propose a form of inter-operator\nfeedback, which flows against the stream direction, to communicate the\ninformation needed to enable execution of these techniques. This feedback\nleverages punctuations to describe the subsets of interest. We identify\npotential sources of feedback information, characterize new types of\npunctuation to support feedback, and describe the roles of producers,\nexploiters, and relayers of feedback that query operators may implement. We\npresent initial experimental observations using the NiagaraST data-stream\nsystem.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2009 22:55:16 GMT"}], "update_date": "2010-02-23", "authors_parsed": [["Fern\u00e1ndez-Moctezuma", "Rafael", "", "Portland State University"], ["Tufte", "Kristin", "", "Portland State University"], ["Li", "Jin", "", "Portland State University"]]}, {"id": "0909.2194", "submitter": "Dominique Tschopp", "authors": "Dominique Tschopp, Suhas Diggavi", "title": "Approximate Nearest Neighbor Search through Comparisons", "comments": "19 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of finding the nearest neighbor (or one of\nthe R-nearest neighbors) of a query object q in a database of n objects. In\ncontrast with most existing approaches, we can only access the ``hidden'' space\nin which the objects live through a similarity oracle. The oracle, given two\nreference objects and a query object, returns the reference object closest to\nthe query object. The oracle attempts to model the behavior of human users,\ncapable of making statements about similarity, but not of assigning meaningful\nnumerical values to distances between objects.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2009 15:32:03 GMT"}], "update_date": "2009-09-14", "authors_parsed": [["Tschopp", "Dominique", ""], ["Diggavi", "Suhas", ""]]}, {"id": "0909.2290", "submitter": "Tiancheng Li", "authors": "Tiancheng Li, Ninghui Li, Jian Zhang, Ian Molloy", "title": "Slicing: A New Approach to Privacy Preserving Data Publishing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several anonymization techniques, such as generalization and bucketization,\nhave been designed for privacy preserving microdata publishing. Recent work has\nshown that generalization loses considerable amount of information, especially\nfor high-dimensional data. Bucketization, on the other hand, does not prevent\nmembership disclosure and does not apply for data that do not have a clear\nseparation between quasi-identifying attributes and sensitive attributes.\n  In this paper, we present a novel technique called slicing, which partitions\nthe data both horizontally and vertically. We show that slicing preserves\nbetter data utility than generalization and can be used for membership\ndisclosure protection. Another important advantage of slicing is that it can\nhandle high-dimensional data. We show how slicing can be used for attribute\ndisclosure protection and develop an efficient algorithm for computing the\nsliced data that obey the l-diversity requirement. Our workload experiments\nconfirm that slicing preserves better utility than generalization and is more\neffective than bucketization in workloads involving the sensitive attribute.\nOur experiments also demonstrate that slicing can be used to prevent membership\ndisclosure.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2009 01:42:55 GMT"}], "update_date": "2009-09-15", "authors_parsed": [["Li", "Tiancheng", ""], ["Li", "Ninghui", ""], ["Zhang", "Jian", ""], ["Molloy", "Ian", ""]]}, {"id": "0909.2623", "submitter": "Reza Akbarinia", "authors": "Reza Akbarinia (INRIA - Lina), Esther Pacitti (INRIA - Lina), Patrick\n  Valduriez (INRIA - Lina)", "title": "Reducing Network Traffic in Unstructured P2P Systems Using Top-k Queries", "comments": null, "journal-ref": "Distributed and Parallel Databases 19, 2-3 (2006) 67-86", "doi": "10.1007/s10619-006-8313-5", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major problem of unstructured P2P systems is their heavy network traffic.\nThis is caused mainly by high numbers of query answers, many of which are\nirrelevant for users. One solution to this problem is to use Top-k queries\nwhereby the user can specify a limited number (k) of the most relevant answers.\nIn this paper, we present FD, a (Fully Distributed) framework for executing\nTop-k queries in unstructured P2P systems, with the objective of reducing\nnetwork traffic. FD consists of a family of algorithms that are simple but\neffec-tive. FD is completely distributed, does not depend on the existence of\ncertain peers, and addresses the volatility of peers during query execution. We\nvali-dated FD through implementation over a 64-node cluster and simulation\nusing the BRITE topology generator and SimJava. Our performance evaluation\nshows that FD can achieve major performance gains in terms of communication and\nresponse time.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2009 19:07:58 GMT"}], "update_date": "2009-09-15", "authors_parsed": [["Akbarinia", "Reza", "", "INRIA - Lina"], ["Pacitti", "Esther", "", "INRIA - Lina"], ["Valduriez", "Patrick", "", "INRIA - Lina"]]}, {"id": "0909.3169", "submitter": "Purushottam Kar", "authors": "Arnab Bhattacharya, Purushottam Kar and Manjish Pal", "title": "On Low Distortion Embeddings of Statistical Distance Measures into Low\n  Dimensional Spaces", "comments": "18 pages, The short version of this paper was accepted for\n  presentation at the 20th International Conference on Database and Expert\n  Systems Applications, DEXA 2009", "journal-ref": "Database and Expert Systems Applications (DEXA) 2009", "doi": "10.1007/978-3-642-03573-9_13", "report-no": null, "categories": "cs.CG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical distance measures have found wide applicability in information\nretrieval tasks that typically involve high dimensional datasets. In order to\nreduce the storage space and ensure efficient performance of queries,\ndimensionality reduction while preserving the inter-point similarity is highly\ndesirable. In this paper, we investigate various statistical distance measures\nfrom the point of view of discovering low distortion embeddings into\nlow-dimensional spaces. More specifically, we consider the Mahalanobis distance\nmeasure, the Bhattacharyya class of divergences and the Kullback-Leibler\ndivergence. We present a dimensionality reduction method based on the\nJohnson-Lindenstrauss Lemma for the Mahalanobis measure that achieves\narbitrarily low distortion. By using the Johnson-Lindenstrauss Lemma again, we\nfurther demonstrate that the Bhattacharyya distance admits dimensionality\nreduction with arbitrarily low additive error. We also examine the question of\nembeddability into metric spaces for these distance measures due to the\navailability of efficient indexing schemes on metric spaces. We provide\nexplicit constructions of point sets under the Bhattacharyya and the\nKullback-Leibler divergences whose embeddings into any metric space incur\narbitrarily large distortions. We show that the lower bound presented for\nBhattacharyya distance is nearly tight by providing an embedding that\napproaches the lower bound for relatively small dimensional datasets.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2009 09:29:48 GMT"}], "update_date": "2010-03-09", "authors_parsed": [["Bhattacharya", "Arnab", ""], ["Kar", "Purushottam", ""], ["Pal", "Manjish", ""]]}, {"id": "0909.3892", "submitter": "Kirk D. Borne", "authors": "Kirk D. Borne (1) ((1) George Mason University)", "title": "Astroinformatics: A 21st Century Approach to Astronomy", "comments": "14 pages total: 1 cover page, 3 pages of co-signers, plus 10 pages,\n  Astro2010 Decadal Survey State of the Profession Position Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DB cs.DL cs.IR physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data volumes from multiple sky surveys have grown from gigabytes into\nterabytes during the past decade, and will grow from terabytes into tens (or\nhundreds) of petabytes in the next decade. This exponential growth of new data\nboth enables and challenges effective astronomical research, requiring new\napproaches. Thus far, astronomy has tended to address these challenges in an\ninformal and ad hoc manner, with the necessary special expertise being assigned\nto e-Science or survey science. However, we see an even wider scope and\ntherefore promote a broader vision of this data-driven revolution in\nastronomical research. For astronomy to effectively cope with and reap the\nmaximum scientific return from existing and future large sky surveys,\nfacilities, and data-producing projects, we need our own information science\nspecialists. We therefore recommend the formal creation, recognition, and\nsupport of a major new discipline, which we call Astroinformatics.\nAstroinformatics includes a set of naturally-related specialties including data\norganization, data description, astronomical classification taxonomies,\nastronomical concept ontologies, data mining, machine learning, visualization,\nand astrostatistics. By virtue of its new stature, we propose that astronomy\nnow needs to integrate Astroinformatics as a formal sub-discipline within\nagency funding plans, university departments, research programs, graduate\ntraining, and undergraduate education. Now is the time for the recognition of\nAstroinformatics as an essential methodology of astronomical research. The\nfuture of astronomy depends on it.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2009 02:21:49 GMT"}], "update_date": "2009-11-04", "authors_parsed": [["Borne", "Kirk D.", "", "George Mason University"]]}, {"id": "0909.3895", "submitter": "Kirk D. Borne", "authors": "Kirk D. Borne (1), Suzanne Jacoby (2), K. Carney (3), A. Connolly (4),\n  T. Eastman (5), M. J. Raddick (6), J. A. Tyson (7), J. Wallin (1) ((1) George\n  Mason University, (2) LSST Corporation, (3) Adler Planetarium, (4) U.\n  Washington, (5) Wyle Information Systems, (6) JHU/SDSS, (7) UC Davis)", "title": "The Revolution in Astronomy Education: Data Science for the Masses", "comments": "12 pages total: 1 cover page, 1 page of co-signers, plus 10 pages,\n  State of the Profession Position Paper submitted to the Astro2010 Decadal\n  Survey (March 2009)", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DB cs.DL cs.IR physics.ed-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As our capacity to study ever-expanding domains of our science has increased\n(including the time domain, non-electromagnetic phenomena, magnetized plasmas,\nand numerous sky surveys in multiple wavebands with broad spatial coverage and\nunprecedented depths), so have the horizons of our understanding of the\nUniverse been similarly expanding. This expansion is coupled to the exponential\ndata deluge from multiple sky surveys, which have grown from gigabytes into\nterabytes during the past decade, and will grow from terabytes into Petabytes\n(even hundreds of Petabytes) in the next decade. With this increased vastness\nof information, there is a growing gap between our awareness of that\ninformation and our understanding of it. Training the next generation in the\nfine art of deriving intelligent understanding from data is needed for the\nsuccess of sciences, communities, projects, agencies, businesses, and\neconomies. This is true for both specialists (scientists) and non-specialists\n(everyone else: the public, educators and students, workforce). Specialists\nmust learn and apply new data science research techniques in order to advance\nour understanding of the Universe. Non-specialists require information literacy\nskills as productive members of the 21st century workforce, integrating\nfoundational skills for lifelong learning in a world increasingly dominated by\ndata. We address the impact of the emerging discipline of data science on\nastronomy education within two contexts: formal education and lifelong\nlearners.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2009 02:55:14 GMT"}], "update_date": "2009-11-04", "authors_parsed": [["Borne", "Kirk D.", ""], ["Jacoby", "Suzanne", ""], ["Carney", "K.", ""], ["Connolly", "A.", ""], ["Eastman", "T.", ""], ["Raddick", "M. J.", ""], ["Tyson", "J. A.", ""], ["Wallin", "J.", ""]]}, {"id": "0909.4196", "submitter": "R Doomun", "authors": "Sabah S. Al-Fedaghi, Bernhard Thalheim", "title": "Personal Information Databases", "comments": "10 pages IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS 2009, ISSN 1947 5500, Impact Factor 0.423,\n  http://sites.google.com/site/ijcsis/", "journal-ref": "Sabah S. Al-Fedaghi, Bernhard Thalheim, International Journal of\n  Computer Science and Information Security, IJCSIS, Vol. 5, No. 1, pp. 11-20,\n  August 2009, USA", "doi": null, "report-no": "ISSN 1947 5500", "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most important aspects of security organization is to establish a\nframework to identify security significant points where policies and procedures\nare declared. The (information) security infrastructure comprises entities,\nprocesses, and technology. All are participants in handling information, which\nis the item that needs to be protected. Privacy and security information\ntechnology is a critical and unmet need in the management of personal\ninformation. This paper proposes concepts and technologies for management of\npersonal information. Two different types of information can be distinguished:\npersonal information and nonpersonal information. Personal information can be\neither personal identifiable information (PII), or nonidentifiable information\n(NII). Security, policy, and technical requirements can be based on this\ndistinction. At the conceptual level, PII is defined and formalized by\npropositions over infons (discrete pieces of information) that specify\ntransformations in PII and NII. PII is categorized into simple infons that\nreflect the proprietor s aspects, relationships with objects, and relationships\nwith other proprietors. The proprietor is the identified person about whom the\ninformation is communicated. The paper proposes a database organization that\nfocuses on the PII spheres of proprietors. At the design level, the paper\ndescribes databases of personal identifiable information built exclusively for\nthis type of information, with their own conceptual scheme, system management,\nand physical structure.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2009 12:55:26 GMT"}], "update_date": "2009-09-24", "authors_parsed": [["Al-Fedaghi", "Sabah S.", ""], ["Thalheim", "Bernhard", ""]]}, {"id": "0909.4409", "submitter": "Mohamed El-Zawawy", "authors": "Mohamed A. El-Zawawy, Mohamed E. El-Sharkawi", "title": "Clustering with Obstacles in Spatial Databases", "comments": "In Proc. 2001 IEEE Int. Symposium of Signal Processing and\n  Information Technology (ISSPIT01), pages 420-425, Cairo, Egypt, Dec. 2001", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering large spatial databases is an important problem, which tries to\nfind the densely populated regions in a spatial area to be used in data mining,\nknowledge discovery, or efficient information retrieval. However most\nalgorithms have ignored the fact that physical obstacles such as rivers, lakes,\nand highways exist in the real world and could thus affect the result of the\nclustering. In this paper, we propose CPO, an efficient clustering technique to\nsolve the problem of clustering in the presence of obstacles. The proposed\nalgorithm divides the spatial area into rectangular cells. Each cell is\nassociated with statistical information used to label the cell as dense or\nnon-dense. It also labels each cell as obstructed (i.e. intersects any\nobstacle) or nonobstructed. For each obstructed cell, the algorithm finds a\nnumber of non-obstructed sub-cells. Then it finds the dense regions of\nnon-obstructed cells or sub-cells by a breadthfirst search as the required\nclusters with a center to each region.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2009 10:52:58 GMT"}], "update_date": "2009-09-25", "authors_parsed": [["El-Zawawy", "Mohamed A.", ""], ["El-Sharkawi", "Mohamed E.", ""]]}, {"id": "0909.4412", "submitter": "Mohamed El-Zawawy", "authors": "Mohamed E. El-Sharkawi, Mohamed A. El-Zawawy", "title": "Algorithm for Spatial Clustering with Obstacles", "comments": "In Proc. 2002 ICICIS Int. Conference on Intelligent Computing and\n  Information Systems (ICICIS02), Cairo, Egypt, June 2002", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an efficient clustering technique to solve the\nproblem of clustering in the presence of obstacles. The proposed algorithm\ndivides the spatial area into rectangular cells. Each cell is associated with\nstatistical information that enables us to label the cell as dense or\nnon-dense. We also label each cell as obstructed (i.e. intersects any obstacle)\nor non-obstructed. Then the algorithm finds the regions (clusters) of\nconnected, dense, non-obstructed cells. Finally, the algorithm finds a center\nfor each such region and returns those centers as centers of the relatively\ndense regions (clusters) in the spatial area.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2009 11:09:15 GMT"}], "update_date": "2009-09-25", "authors_parsed": [["El-Sharkawi", "Mohamed E.", ""], ["El-Zawawy", "Mohamed A.", ""]]}, {"id": "0909.5166", "submitter": "Rdv Ijcsis", "authors": "Neelu Khare, Neeru Adlakha, K. R. Pardasani", "title": "An Algorithm for Mining Multidimensional Fuzzy Association Rules", "comments": "5 pages IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS 2009, ISSN 1947 5500, Impact Factor 0.423,\n  http://sites.google.com/site/ijcsis/", "journal-ref": "International Journal of Computer Science and Information\n  Security, IJCSIS, Vol. 5, No. 1, pp. 72-76, September 2009, USA", "doi": null, "report-no": "ISSN 1947 5500", "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multidimensional association rule mining searches for interesting\nrelationship among the values from different dimensions or attributes in a\nrelational database. In this method the correlation is among set of dimensions\ni.e., the items forming a rule come from different dimensions. Therefore each\ndimension should be partitioned at the fuzzy set level. This paper proposes a\nnew algorithm for generating multidimensional association rules by utilizing\nfuzzy sets. A database consisting of fuzzy transactions, the Apriory property\nis employed to prune the useless candidates, itemsets.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2009 18:44:30 GMT"}], "update_date": "2009-09-29", "authors_parsed": [["Khare", "Neelu", ""], ["Adlakha", "Neeru", ""], ["Pardasani", "K. R.", ""]]}, {"id": "0909.5530", "submitter": "Xiaokui Xiao", "authors": "Xiaokui Xiao, Guozhang Wang, Johannes Gehrke", "title": "Differential Privacy via Wavelet Transforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy preserving data publishing has attracted considerable research\ninterest in recent years. Among the existing solutions, {\\em\n$\\epsilon$-differential privacy} provides one of the strongest privacy\nguarantees. Existing data publishing methods that achieve\n$\\epsilon$-differential privacy, however, offer little data utility. In\nparticular, if the output dataset is used to answer count queries, the noise in\nthe query answers can be proportional to the number of tuples in the data,\nwhich renders the results useless.\n  In this paper, we develop a data publishing technique that ensures\n$\\epsilon$-differential privacy while providing accurate answers for {\\em\nrange-count queries}, i.e., count queries where the predicate on each attribute\nis a range. The core of our solution is a framework that applies {\\em wavelet\ntransforms} on the data before adding noise to it. We present instantiations of\nthe proposed framework for both ordinal and nominal data, and we provide a\ntheoretical analysis on their privacy and utility guarantees. In an extensive\nexperimental study on both real and synthetic data, we show the effectiveness\nand efficiency of our solution.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2009 07:16:38 GMT"}], "update_date": "2009-10-01", "authors_parsed": [["Xiao", "Xiaokui", ""], ["Wang", "Guozhang", ""], ["Gehrke", "Johannes", ""]]}]