[{"id": "1809.00089", "submitter": "Chenggang Wu", "authors": "Chenggang Wu, Vikram Sreekanti, Joseph M. Hellerstein", "title": "Eliminating Boundaries in Cloud Storage with Anna", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe how we extended a distributed key-value store\ncalled Anna into an elastic, multi-tier service for the cloud. In its extended\nform, Anna is designed to overcome the narrow cost-performance limitations\ntypical of current cloud storage systems. We describe three key aspects of\nAnna's new design: multi-master selective replication of hot keys, a vertical\ntiering of storage layers with different cost-performance tradeoffs, and\nhorizontal elasticity of each tier to add and remove nodes in response to load\ndynamics. Anna's policy engine uses these mechanisms to balance service-level\nobjectives around cost, latency and fault tolerance. Experimental results\nexplore the behavior of Anna's mechanisms and policy, exhibiting orders of\nmagnitude efficiency improvements over both commodity cloud KVS services and\nresearch systems.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2018 00:20:05 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Wu", "Chenggang", ""], ["Sreekanti", "Vikram", ""], ["Hellerstein", "Joseph M.", ""]]}, {"id": "1809.00159", "submitter": "Renato Marroqu\\'in", "authors": "Renato Marroqu\\'in, Ingo M\\\"uller, Darko Makreshanski and Gustavo\n  Alonso", "title": "Pay One, Get Hundreds for Free: Reducing Cloud Costs through Shared\n  Query Execution", "comments": null, "journal-ref": "Proceedings of the ACM Symposium on Cloud Computing (SoCC) 2018,\n  pages 439-450", "doi": "10.1145/3267809.3267822", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cloud-based data analysis is nowadays common practice because of the lower\nsystem management overhead as well as the pay-as-you-go pricing model. The\npricing model, however, is not always suitable for query processing as heavy\nuse results in high costs. For example, in query-as-a-service systems, where\nusers are charged per processed byte, collections of queries accessing the same\ndata frequently can become expensive. The problem is compounded by the limited\noptions for the user to optimize query execution when using declarative\ninterfaces such as SQL. In this paper, we show how, without modifying existing\nsystems and without the involvement of the cloud provider, it is possible to\nsignificantly reduce the overhead, and hence the cost, of query-as-a-service\nsystems. Our approach is based on query rewriting so that multiple concurrent\nqueries are combined into a single query. Our experiments show the aggregated\namount of work done by the shared execution is smaller than in a\nquery-at-a-time approach. Since queries are charged per byte processed, the\ncost of executing a group of queries is often the same as executing a single\none of them. As an example, we demonstrate how the shared execution of the\nTPC-H benchmark is up to 100x and 16x cheaper in Amazon Athena and Google\nBigQuery than using a query-at-a-time approach while achieving a higher\nthroughput.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2018 12:15:16 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Marroqu\u00edn", "Renato", ""], ["M\u00fcller", "Ingo", ""], ["Makreshanski", "Darko", ""], ["Alonso", "Gustavo", ""]]}, {"id": "1809.00164", "submitter": "Xavier Ouvrard", "authors": "Xavier Ouvrard, Jean-Marie Le Goff and Stephane Marchand-Maillet", "title": "Hypergraph Modeling and Visualisation of Complex Co-occurence Networks", "comments": "Preprint submitted at ENDM Special Journal 2nd IMA Conference on\n  Theoretical and Computational Discrete Mathematics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding inherent or processed links within a dataset allows to discover\npotential knowledge. The main contribution of this article is to define a\nglobal framework that enables optimal knowledge discovery by visually rendering\nco-occurences (i.e. groups of linked data instances attached to a metadata\nreference) - either inherently present or processed - from a dataset as facets.\nHypergraphs are well suited for modeling co-occurences since they support\nmulti-adicity whereas graphs only support pairwise relationships. This article\nintroduces an efficient navigation between different facets of an information\nspace based on hypergraph modelisation and visualisation.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2018 12:48:28 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Ouvrard", "Xavier", ""], ["Goff", "Jean-Marie Le", ""], ["Marchand-Maillet", "Stephane", ""]]}, {"id": "1809.00405", "submitter": "Ting Xie", "authors": "Ting Xie, Oliver Kennedy, Varun Chandola", "title": "Query Log Compression for Workload Analytics", "comments": "Typos fixed, some irrelevant figures and paragraphs are trimmed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing database access logs is a key part of performance tuning, intrusion\ndetection, benchmark development, and many other database administration tasks.\nUnfortunately, it is common for production databases to deal with millions or\neven more queries each day, so these logs must be summarized before they can be\nused. Designing an appropriate summary encoding requires trading off between\nconciseness and information content. For example: simple workload sampling may\nmiss rare, but high impact queries. In this paper, we present LogR, a lossy log\ncompression scheme suitable use for many automated log analytics tools, as well\nas for human inspection. We formalize and analyze the space/fidelity trade-off\nin the context of a broader family of \"pattern\" and \"pattern mixture\" log\nencodings to which LogR belongs. We show through a series of experiments that\nLogR compressed encodings can be created efficiently, come with provable\ninformation-theoretic bounds on their accuracy, and outperform state-of-art log\nsummarization strategies.\n", "versions": [{"version": "v1", "created": "Sun, 2 Sep 2018 22:41:32 GMT"}, {"version": "v2", "created": "Sat, 29 Sep 2018 04:47:54 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Xie", "Ting", ""], ["Kennedy", "Oliver", ""], ["Chandola", "Varun", ""]]}, {"id": "1809.00458", "submitter": "Yang Yang", "authors": "Yang Yang, Ying Zhang, Wenjie Zhang and Zengfeng Huang", "title": "GB-KMV: An Augmented KMV Sketch for Approximate Containment Similarity\n  Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of approximate containment similarity\nsearch. Given two records Q and X, the containment similarity between Q and X\nwith respect to Q is |Q intersect X|/ |Q|. Given a query record Q and a set of\nrecords S, the containment similarity search finds a set of records from S\nwhose containment similarity regarding Q are not less than the given threshold.\nThis problem has many important applications in commercial and scientific\nfields such as record matching and domain search. Existing solution relies on\nthe asymmetric LSH method by transforming the containment similarity to\nwell-studied Jaccard similarity. In this paper, we use a different framework by\ntransforming the containment similarity to set intersection. We propose a novel\naugmented KMV sketch technique, namely GB-KMV, which is data-dependent and can\nachieve a good trade-off between the sketch size and the accuracy. We provide a\nset of theoretical analysis to underpin the proposed augmented KMV sketch\ntechnique, and show that it outperforms the state-of-the-art technique LSH-E in\nterms of estimation accuracy under practical assumption. Our comprehensive\nexperiments on real-life datasets verify that GB-KMV is superior to LSH-E in\nterms of the space-accuracy trade-off, time-accuracy trade-off, and the sketch\nconstruction time. For instance, with similar estimation accuracy (F-1 score),\nGB-KMV is over 100 times faster than LSH-E on some real-life dataset.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 06:02:21 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Yang", "Yang", ""], ["Zhang", "Ying", ""], ["Zhang", "Wenjie", ""], ["Huang", "Zengfeng", ""]]}, {"id": "1809.00641", "submitter": "Jose Oliveira Prof", "authors": "Jo\\~ao M. Afonso, Gabriel D. Fernandes, Jo\\~ao P. Fernandes, Filipe\n  Oliveira, Bruno M. Ribeiro, Rog\\'erio Pontes, Jos\\'e N. Oliveira, Alberto J.\n  Proen\\c{c}a", "title": "Typed Linear Algebra for Efficient Analytical Querying", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper uses typed linear algebra (LA) to represent data and perform\nanalytical querying in a single, unified framework. The typed approach offers\nstrong type checking (as in modern programming languages) and a diagrammatic\nway of expressing queries (paths in LA diagrams). A kernel of LA operators has\nbeen implemented so that paths extracted from LA diagrams can be executed. The\napproach is validated and evaluated taking TPC-H benchmark queries as\nreference. The performance of the LA-based approach is compared with popular\ndatabase competitors (PostgreSQL and MySQL).\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 16:19:55 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Afonso", "Jo\u00e3o M.", ""], ["Fernandes", "Gabriel D.", ""], ["Fernandes", "Jo\u00e3o P.", ""], ["Oliveira", "Filipe", ""], ["Ribeiro", "Bruno M.", ""], ["Pontes", "Rog\u00e9rio", ""], ["Oliveira", "Jos\u00e9 N.", ""], ["Proen\u00e7a", "Alberto J.", ""]]}, {"id": "1809.00677", "submitter": "Andreas Kipf", "authors": "Andreas Kipf, Thomas Kipf, Bernhard Radke, Viktor Leis, Peter Boncz,\n  Alfons Kemper", "title": "Learned Cardinalities: Estimating Correlated Joins with Deep Learning", "comments": "CIDR 2019. https://github.com/andreaskipf/learnedcardinalities", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new deep learning approach to cardinality estimation. MSCN is a\nmulti-set convolutional network, tailored to representing relational query\nplans, that employs set semantics to capture query features and true\ncardinalities. MSCN builds on sampling-based estimation, addressing its\nweaknesses when no sampled tuples qualify a predicate, and in capturing\njoin-crossing correlations. Our evaluation of MSCN using a real-world dataset\nshows that deep learning significantly enhances the quality of cardinality\nestimation, which is the core problem in query optimization.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 18:05:12 GMT"}, {"version": "v2", "created": "Tue, 18 Dec 2018 11:16:34 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Kipf", "Andreas", ""], ["Kipf", "Thomas", ""], ["Radke", "Bernhard", ""], ["Leis", "Viktor", ""], ["Boncz", "Peter", ""], ["Kemper", "Alfons", ""]]}, {"id": "1809.00792", "submitter": "Srikumar Krishnamoorthy", "authors": "Srikumar Krishnamoorthy", "title": "A comparative study of top-k high utility itemset mining methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High Utility Itemset (HUI) mining problem is one of the important problems in\nthe data mining literature. The problem offers greater flexibility to a\ndecision maker to incorporate her/his notion of utility into the pattern mining\nprocess. The problem, however, requires the decision maker to choose a minimum\nutility threshold value for discovering interesting patterns. This is quite\nchallenging due to the disparate itemset characteristics and their utility\ndistributions. In order to address this issue, Top-K High Utility Itemset\n(THUI) mining problem was introduced in the literature. THUI mining problem is\nprimarily a variant of the HUI mining problem that allows a decision maker to\nspecify the desired number of HUIs rather than the minimum utility threshold\nvalue. Several algorithms have been introduced in the literature to efficiently\nmine top-k HUIs. This paper systematically analyses the top-k HUI mining\nmethods in the literature, describes the methods, and performs a comparative\nanalysis. The data structures, threshold raising strategies, and pruning\nstrategies adopted for efficient top-k HUI mining are also presented and\nanalysed. Furthermore, the paper reviews several extensions of the top-k HUI\nmining problem such as data stream mining, sequential pattern mining and\non-shelf utility mining. The paper is likely to be useful for researchers to\nexamine the key methods in top-k HUI mining, evaluate the gaps in literature,\nexplore new research opportunities and enhance the state-of-the-art in high\nutility pattern mining.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 04:18:52 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Krishnamoorthy", "Srikumar", ""]]}, {"id": "1809.00974", "submitter": "Fabian Gotzens", "authors": "Fabian Gotzens, Heidi Heinrichs, Jonas H\\\"orsch, Fabian Hofmann", "title": "Performing energy modelling exercises in a transparent way the issue of\n  data quality in power plant databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In energy modelling, open data and open source code can help enhance\ntraceability and reproducibility of model exercises which contribute to\nfacilitate controversial debates and improve policy advice. While the\navailability of open power plant databases increased in recent years, they\noften differ considerably from each other and their data quality has not been\nsystematically compared to proprietary sources yet. Here, we introduce the\npython-based \"powerplantmatching\" (PPM), an open source toolset for cleaning,\nstandardizing and combining multiple power plant databases. We apply it once\nonly with open databases and once with an additional proprietary database in\norder to discuss and elaborate the issue of data quality, by analysing\ncapacities, countries, fuel types, geographic coordinates and commissioning\nyears for conventional power plants. We find that a derived dataset purely\nbased on open data is not yet on a par with one in which a proprietary database\nhas been added to the matching, even though the statistical values for capacity\nmatched to a large degree with both datasets. When commissioning years are\nneeded for modelling purposes in the final dataset, the proprietary database\nhelps crucially to increase the quality of the derived dataset.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 08:47:59 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Gotzens", "Fabian", ""], ["Heinrichs", "Heidi", ""], ["H\u00f6rsch", "Jonas", ""], ["Hofmann", "Fabian", ""]]}, {"id": "1809.01398", "submitter": "Chen Yuan", "authors": "Chen Yuan, Guangyi Liu, Renchang Dai, Zhiwei Wang", "title": "Power Flow Analysis Using Graph based Combination of Iterative Methods\n  and Vertex Contraction Approach", "comments": "8 pages, 8 figures, 2018 International Conference on Power System\n  Technology (POWERCON 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.DS cs.PF math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared with relational database (RDB), graph database (GDB) is a more\nintuitive expression of the real world. Each node in the GDB is a both storage\nand logic unit. Since it is connected to its neighboring nodes through edges,\nand its neighboring information could be easily obtained in one-step graph\ntraversal. It is able to conduct local computation independently and all nodes\ncan do their local work in parallel. Then the whole system can be maximally\nanalyzed and assessed in parallel to largely improve the computation\nperformance without sacrificing the precision of final results. This paper\nfirstly introduces graph database, power system graph modeling and potential\ngraph computing applications in power systems. Two iterative methods based on\ngraph database and PageRank are presented and their convergence are discussed.\nVertex contraction is proposed to improve the performance by eliminating\nzero-impedance branch. A combination of the two iterative methods is proposed\nto make use of their advantages. Testing results based on a provincial 1425-bus\nsystem demonstrate that the proposed comprehensive approach is a good candidate\nfor power flow analysis.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 09:17:56 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Yuan", "Chen", ""], ["Liu", "Guangyi", ""], ["Dai", "Renchang", ""], ["Wang", "Zhiwei", ""]]}, {"id": "1809.01415", "submitter": "Chen Yuan", "authors": "Chen Yuan, Yi Lu, Kewen Liu, Guangyi Liu, Renchang Dai, Zhiwei Wang", "title": "Exploration of Bi-Level PageRank Algorithm for Power Flow Analysis Using\n  Graph Database", "comments": "7 pages, 6 figures, 3 tables, 2018 IEEE International Congress on Big\n  Data. arXiv admin note: text overlap with arXiv:1809.01398", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared with traditional relational database, graph database, GDB, is a\nnatural expression of most real-world systems. Each node in the GDB is not only\na storage unit, but also a logic operation unit to implement local computation\nin parallel. This paper firstly explores the feasibility of power system\nmodeling using GDB. Then a brief introduction of the PageRank algorithm and the\nfeasibility analysis of its application in GDB are presented. Then the proposed\nGDB based bilevel PageRank algorithm is developed from PageRank algorithm and\nGauss Seidel methodology realize high performance parallel computation. MP\n10790 case, and its extensions, MP 107900 and MP 1079000, are tested to verify\nthe proposed method and investigate its parallelism in GDB. Besides, a\nprovincial system, FJ case which include 1425 buses and 1922 branches, is also\nincluded in the case study to further prove the proposed algorithm\neffectiveness in real world.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 09:59:18 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Yuan", "Chen", ""], ["Lu", "Yi", ""], ["Liu", "Kewen", ""], ["Liu", "Guangyi", ""], ["Dai", "Renchang", ""], ["Wang", "Zhiwei", ""]]}, {"id": "1809.01622", "submitter": "Marco Antonio Casanova", "authors": "Elisa S. Menendez, Marco A. Casanova, Mohand Boughanem, Luiz Andr\\'e\n  P. Paes Leme", "title": "Ranking RDF Instances in Degree-decoupled RDF Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade, RDF emerged as a new kind of standardized data model, and\na sizable body of knowledge from fields such as Information Retrieval was\nadapted to RDF graphs. One common task in graph databases is to define an\nimportance score for nodes based on centrality measures, such as PageRank and\nHITS. The majority of the strategies highly depend on the degree of the node.\nHowever, in some RDF graphs, called degree-decoupled RDF graphs, the notion of\nimportance is not directly related to the node degree. Therefore, this work\nfirst proposes three novel node importance measures, named InfoRank I, II and\nIII, for degree-decoupled RDF graphs. It then compares the proposed measures\nwith traditional PageRank and other familiar centrality measures, using with an\nIMDb dataset.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 17:02:00 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Menendez", "Elisa S.", ""], ["Casanova", "Marco A.", ""], ["Boughanem", "Mohand", ""], ["Leme", "Luiz Andr\u00e9 P. Paes", ""]]}, {"id": "1809.02345", "submitter": "Marios Meimaris", "authors": "Marios Meimaris, George Papastefanatos", "title": "Hierarchical Characteristic Set Merging for Optimizing SPARQL Queries in\n  Heterogeneous RDF", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Characteristic sets (CS) organize RDF triples based on the set of properties\ncharacterizing their subject nodes. This concept is recently used in indexing\ntechniques, as it can capture the implicit schema of RDF data. While most\nCS-based approaches yield significant improvements in space and query\nperformance, they fail to perform well in the presence of schema heterogeneity,\ni.e., when the number of CSs becomes very large, resulting in a highly\npartitioned data organization. In this paper, we address this problem by\nintroducing a novel technique, for merging CSs based on their hierarchical\nstructure. Our technique employs a lattice to capture the hierarchical\nrelationships between CSs, identifies dense CSs and merges dense CSs with their\nancestors, thus reducing the size of the CSs as well as the links between them.\nWe implemented our algorithm on top of a relational backbone, where each merged\nCS is stored in a relational table, and we performed an extensive experimental\nstudy to evaluate the performance and impact of merging to the storage and\nquerying of RDF datasets, indicating significant improvements.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 08:21:01 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Meimaris", "Marios", ""], ["Papastefanatos", "George", ""]]}, {"id": "1809.02631", "submitter": "Arvind Arasu", "authors": "Panagiotis Antonopoulos, Arvind Arasu, Ken Eguro, Joachim Hammer,\n  Raghav Kaushik, Donald Kossmann, Ravi Ramamurthy, Jakub Szymaszek", "title": "Pushing the Limits of Encrypted Databases with Secure Hardware", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Encrypted databases have been studied for more than 10 years and are quickly\nemerging as a critical technology for the cloud. The current state of the art\nis to use property-preserving encrypting techniques (e.g., deterministic\nencryption) to protect the confidentiality of the data and support query\nprocessing at the same time. Unfortunately, these techniques have many\nlimitations. Recently, trusted computing platforms (e.g., Intel SGX) have\nemerged as an alternative to implement encrypted databases. This paper\ndemonstrates some vulnerabilities and the limitations of this technology, but\nit also shows how to make best use of it in order to improve on\nconfidentiality, functionality, and performance.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 18:31:52 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Antonopoulos", "Panagiotis", ""], ["Arasu", "Arvind", ""], ["Eguro", "Ken", ""], ["Hammer", "Joachim", ""], ["Kaushik", "Raghav", ""], ["Kossmann", "Donald", ""], ["Ramamurthy", "Ravi", ""], ["Szymaszek", "Jakub", ""]]}, {"id": "1809.03261", "submitter": "Aron Szanto", "authors": "Aron Szanto", "title": "The Skiplist-Based LSM Tree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Log-Structured Merge (LSM) Trees provide a tiered data storage and retrieval\nparadigm that is attractive for write-optimized data systems. Maintaining an\nefficient buffer in memory and deferring updates past their initial write-time,\nthe structure provides quick operations over hot data. Because each layer of\nthe structure is logically separate from the others, the structure is also\nconducive to opportunistic and granular optimization. In this paper, we\nintroduce the Skiplist-Based LSM Tree (sLSM), a novel system in which the\nmemory buffer of the LSM is composed of a sequence of skiplists. We develop\ntheoretical and experimental results that demonstrate that the breadth of\ntuning parameters inherent to the sLSM allows it broad flexibility for\nexcellent performance across a wide variety of workloads.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 12:11:29 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Szanto", "Aron", ""]]}, {"id": "1809.03445", "submitter": "Jasper Kyle Catapang", "authors": "Jasper Kyle Catapang", "title": "A collection of database industrial techniques and optimization\n  approaches of database operations", "comments": null, "journal-ref": null, "doi": "10.5281/zenodo.1439511", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Databases play an essential role in our society today. Databases are embedded\nin sectors like corporations, institutions, and government organizations, among\nothers. These databases are used for our video and audio streaming platforms,\nsocial gaming, finances, cloud storage, e-commerce, healthcare, economy, etc.\nIt is therefore imperative that we learn how to properly execute database\noperations and efficiently implement methodologies so that we may optimize the\nperformance of databases.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 16:33:11 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Catapang", "Jasper Kyle", ""]]}, {"id": "1809.03822", "submitter": "Jaroslav Pokorn\\'y", "authors": "Jaroslav Pokorny", "title": "Integration of Relational and Graph Databases Functionally", "comments": "In the Pre-proceedings of the Semantics in Big Data Management\n  workshop, IFIP W.G. 2.6 on Database, Tuesday 18th September 2018 at IFIP\n  World Computer Congress 2018 - Poznan, Poland", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A significant category of NoSQL approaches is known as graph da-tabases. They\nare usually represented by one property graph. We introduce a functional\napproach to modelling relations and property graphs. Single-valued and\nmultivalued functions will be sufficient in this case. Then, a typed\n{\\lambda}-calculus, i.e., the language of lambda terms, will be used as a data\nmanipulation lan-guage. Some integration options at the query language level\nare discussed.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 12:39:17 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Pokorny", "Jaroslav", ""]]}, {"id": "1809.04017", "submitter": "Mengchen Zhang", "authors": "Chen Jason Zhang, Lei Chen, H.V.Jagadish, Mengchen Zhang, and Yongxin\n  Tong", "title": "Reducing Uncertainty of Schema Matching via Crowdsourcing with Accuracy\n  Rates", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Schema matching is a central challenge for data integration systems. Inspired\nby the popularity and the success of crowdsourcing platforms, we explore the\nuse of crowdsourcing to reduce the uncertainty of schema matching. Since\ncrowdsourcing platforms are most effective for simple questions, we assume that\neach Correspondence Correctness Question (CCQ) asks the crowd to decide whether\na given correspondence should exist in the correct matching. Furthermore,\nmembers of a crowd may sometimes return incorrect answers with different\nprobabilities. Accuracy rates of individual crowd workers are probabilities of\nreturning correct answers which can be attributes of CCQs as well as\nevaluations of individual workers. We prove that uncertainty reduction equals\nto entropy of answers minus entropy of crowds and show how to obtain lower and\nupper bounds for it. We propose frameworks and efficient algorithms to\ndynamically manage the CCQs to maximize the uncertainty reduction within a\nlimited budget of questions. We develop two novel approaches, namely `Single\nCCQ' and `Multiple CCQ', which adaptively select, publish and manage questions.\nWe verify the value of our solutions with simulation and real implementation.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 16:42:24 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Zhang", "Chen Jason", ""], ["Chen", "Lei", ""], ["Jagadish", "H. V.", ""], ["Zhang", "Mengchen", ""], ["Tong", "Yongxin", ""]]}, {"id": "1809.04284", "submitter": "Darja Solodovnikova", "authors": "Darja Solodovnikova and Laila Niedrite", "title": "An Approach to Handle Big Data Warehouse Evolution", "comments": "In the Pre-proceedings of the Semantics in Big Data Management\n  workshop, IFIP W.G. 2.6 on Database, Tuesday 18th September 2018 at IFIP\n  World Computer Congress 2018 - Poznan, Poland", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the purposes of Big Data systems is to support analysis of data\ngathered from heterogeneous data sources. Since data warehouses have been used\nfor several decades to achieve the same goal, they could be leveraged also to\nprovide analysis of data stored in Big Data systems. The problem of adapting\ndata warehouse data and schemata to changes in these requirements as well as\ndata sources has been studied by many researchers worldwide. However,\ninnovative methods must be developed also to support evolution of data\nwarehouses that are used to analyze data stored in Big Data systems. In this\npaper, we propose a data warehouse architecture that allows to perform\ndifferent kinds of analytical tasks, including OLAP-like analysis, on big data\nloaded from multiple heterogeneous data sources with different latency and is\ncapable of processing changes in data sources as well as evolving analysis\nrequirements. The operation of the architecture is highly based on the metadata\nthat are outlined in the paper.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 07:32:29 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Solodovnikova", "Darja", ""], ["Niedrite", "Laila", ""]]}, {"id": "1809.04709", "submitter": "Hiba Khalid", "authors": "Hiba Khalid, Esteban Zimanyi, Robert Wrembel", "title": "Evaluation of Semantic Metadata Pair Modelling Using Data Clustering", "comments": "In the Pre-proceedings of the Semantics in Big Data Management\n  workshop, IFIP W.G. 2.6 on Database, Tuesday 18th September 2018 at IFIP\n  World Computer Congress 2018 - Poznan, Poland", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metadata presents a medium for connection, elaboration, examination, and\ncomprehension of relativity between two datasets. Metadata can be enriched to\ncalculate the existence of a connection between different disintegrated\ndatasets. In order to do so, the very first task is to attain a generic\nmetadata representation for domains. This representation narrows down the\nmetadata search space. The metadata search space consists of attributes, tags,\nsemantic content, annotations etc. to perform classification. The existing\ntechnologies limit the metadata bandwidth i.e. the operation set for matching\npurposes is restricted or limited. This research focuses on generating a mapper\nfunction called cognate that can find mathematical relevance based on pairs of\nattributes between disintegrated datasets. Each pair is designed from one of\nthe datasets under consideration using the existing metadata and available\nmeta-tags. After pairs have been generated, samples are constructed using a\ndifferent combination of pairs. The similarity and relevance between two or\nmore pairs are attained by using a data clustering technique to generate large\ngroups from smaller groups based on similarity index. The search space is\ndivided using a domain divider function and smaller search spaces are created\nusing relativity and tagging as the main concept. For this research, the\ninitial datasets have been limited to textual information. Once all disjoint\nmeta-collection have been generated the approximation algorithm calculates the\ncenters of each meta-set. These centers serve the purpose of meta-pointers i.e.\na collection of meta-domain representations. Each pointer can then join a\ncluster based on the content i.e. meta-content. It also facilitates the process\nof possible synonyms across cross-functional domains. This can be examined\nusing meta-pointers and graph pools.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 23:23:54 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Khalid", "Hiba", ""], ["Zimanyi", "Esteban", ""], ["Wrembel", "Robert", ""]]}, {"id": "1809.05054", "submitter": "Tianze Shi", "authors": "Tianze Shi, Kedar Tatwawadi, Kaushik Chakrabarti, Yi Mao, Oleksandr\n  Polozov, Weizhu Chen", "title": "IncSQL: Training Incremental Text-to-SQL Parsers with Non-Deterministic\n  Oracles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a sequence-to-action parsing approach for the natural language to\nSQL task that incrementally fills the slots of a SQL query with feasible\nactions from a pre-defined inventory. To account for the fact that typically\nthere are multiple correct SQL queries with the same or very similar semantics,\nwe draw inspiration from syntactic parsing techniques and propose to train our\nsequence-to-action models with non-deterministic oracles. We evaluate our\nmodels on the WikiSQL dataset and achieve an execution accuracy of 83.7% on the\ntest set, a 2.1% absolute improvement over the models trained with traditional\nstatic oracles assuming a single correct target SQL query. When further\ncombined with the execution-guided decoding strategy, our model sets a new\nstate-of-the-art performance at an execution accuracy of 87.1%.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 16:42:21 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2018 17:55:37 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Shi", "Tianze", ""], ["Tatwawadi", "Kedar", ""], ["Chakrabarti", "Kaushik", ""], ["Mao", "Yi", ""], ["Polozov", "Oleksandr", ""], ["Chen", "Weizhu", ""]]}, {"id": "1809.05234", "submitter": "Camila Ferreira Costa", "authors": "Camila F. Costa and Mario A. Nascimento", "title": "In-Route Task Selection in Crowdsourcing", "comments": "An abridged version of this manuscript has been accepted for\n  publication as a short paper at ACM SIGSPATIAL 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One important problem in crowdsourcing is that of assigning tasks to workers.\nWe consider a scenario where a worker is traveling on a preferred/typical path\n(e.g., from school to home) and there is a set of tasks available to be\nperformed. Furthermore, we assume that: each task yields a positive reward, the\nworker has the skills necessary to perform all available tasks and he/she is\nwilling to possibly deviate from his/her preferred path as long as he/she\ntravels at most a total given distance/time. We call this problem the In-Route\nTask Selection (IRTS) problem and investigate it using the skyline paradigm in\norder to obtain the exact set of non-dominated solutions, i.e., good and\ndiverse solutions yielding different combinations of smaller or larger rewards\nwhile traveling more or less. This is a practically relevant problem as it\nempowers the worker as he/she can decide, in real time, which tasks suit\nhis/her needs and/or availability better. After showing that the IRTS problem\nis NP-hard, we propose an exact (but expensive) solution and a few others\npractical heuristic solutions. While the exact solution is suitable only for\nreasonably small IRTS instances, the heuristic solutions can produce solutions\nwith good values of precision and recall for problems of realistic sizes within\npractical, in fact most often sub-second, query processing time.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 02:55:15 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Costa", "Camila F.", ""], ["Nascimento", "Mario A.", ""]]}, {"id": "1809.05467", "submitter": "Panagiotis Mandros", "authors": "Panagiotis Mandros, Mario Boley, Jilles Vreeken", "title": "Discovering Reliable Dependencies from Data: Hardness and Improved\n  Algorithms", "comments": "Accepted to Proceedings of the IEEE International Conference on Data\n  Mining (ICDM'18)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reliable fraction of information is an attractive score for quantifying\n(functional) dependencies in high-dimensional data. In this paper, we\nsystematically explore the algorithmic implications of using this measure for\noptimization. We show that the problem is NP-hard, which justifies the usage of\nworst-case exponential-time as well as heuristic search methods. We then\nsubstantially improve the practical performance for both optimization styles by\nderiving a novel admissible bounding function that has an unbounded potential\nfor additional pruning over the previously proposed one. Finally, we\nempirically investigate the approximation ratio of the greedy algorithm and\nshow that it produces highly competitive results in a fraction of time needed\nfor complete branch-and-bound style search.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 15:33:06 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Mandros", "Panagiotis", ""], ["Boley", "Mario", ""], ["Vreeken", "Jilles", ""]]}, {"id": "1809.05495", "submitter": "F\\'elix Cuadrado", "authors": "Luis M. Vaquero, Felix Cuadrado", "title": "Auto-tuning Distributed Stream Processing Systems using Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine tuning distributed systems is considered to be a craftsmanship, relying\non intuition and experience. This becomes even more challenging when the\nsystems need to react in near real time, as streaming engines have to do to\nmaintain pre-agreed service quality metrics. In this article, we present an\nautomated approach that builds on a combination of supervised and reinforcement\nlearning methods to recommend the most appropriate lever configurations based\non previous load. With this, streaming engines can be automatically tuned\nwithout requiring a human to determine the right way and proper time to deploy\nthem. This opens the door to new configurations that are not being applied\ntoday since the complexity of managing these systems has surpassed the\nabilities of human experts. We show how reinforcement learning systems can find\nsubstantially better configurations in less time than their human counterparts\nand adapt to changing workloads.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 16:36:05 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Vaquero", "Luis M.", ""], ["Cuadrado", "Felix", ""]]}, {"id": "1809.05951", "submitter": "Emanuel Sallinger", "authors": "Gerald Berger, Georg Gottlob, Andreas Pieris, Emanuel Sallinger", "title": "The Space-Efficient Core of Vadalog", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vadalog is a system for performing complex reasoning tasks such as those\nrequired in advanced knowledge graphs. The logical core of the underlying\nVadalog language is the warded fragment of tuple-generating dependencies\n(TGDs). This formalism ensures tractable reasoning in data complexity, while a\nrecent analysis focusing on a practical implementation led to the reasoning\nalgorithm around which the Vadalog system is built. A fundamental question that\nhas emerged in the context of Vadalog is the following: can we limit the\nrecursion allowed by wardedness in order to obtain a formalism that provides a\nconvenient syntax for expressing useful recursive statements, and at the same\ntime achieves space-efficiency? After analyzing several real-life examples of\nwarded sets of TGDs provided by our industrial partners, as well as recent\nbenchmarks, we observed that recursion is often used in a restricted way: the\nbody of a TGD contains at most one atom whose predicate is mutually recursive\nwith a predicate in the head. We show that this type of recursion, known as\npiece-wise linear in the Datalog literature, is the answer to our main\nquestion. We further show that piece-wise linear recursion alone, without the\nwardedness condition, is not enough as it leads to the undecidability of\nreasoning. We finally study the relative expressiveness of the query languages\nbased on (piece-wise linear) warded sets of TGDs.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 20:33:45 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Berger", "Gerald", ""], ["Gottlob", "Georg", ""], ["Pieris", "Andreas", ""], ["Sallinger", "Emanuel", ""]]}, {"id": "1809.06831", "submitter": "Theodoros Chondrogiannis", "authors": "Theodoros Chondrogiannis, Panagiotis Bouros, Johann Gamper, Ulf Leser,\n  David B. Blumenthal", "title": "Finding k-Dissimilar Paths with Minimum Collective Length", "comments": "Extended version of the SIGSPATIAL'18 paper under the same title", "journal-ref": "26th ACM SIGSPATIAL International Conference on Advances in\n  Geographic Information Systems (ACM SIGSPATIAL GIS 2018), Seattle,\n  Washington, USA, November 6-9, 2018", "doi": "10.1145/3274895.3274903", "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shortest path computation is a fundamental problem in road networks. However,\nin many real-world scenarios, determining solely the shortest path is not\nenough. In this paper, we study the problem of finding k-Dissimilar Paths with\nMinimum Collective Length (kDPwML), which aims at computing a set of paths from\na source s to a target t such that all paths are pairwise dissimilar by at\nleast \\theta and the sum of the path lengths is minimal. We introduce an exact\nalgorithm for the kDPwML problem, which iterates over all possible s-t paths\nwhile employing two pruning techniques to reduce the prohibitively expensive\ncomputational cost. To achieve scalability, we also define the much smaller set\nof the simple single-via paths, and we adapt two algorithms for kDPwML queries\nto iterate over this set. Our experimental analysis on real road networks shows\nthat iterating over all paths is impractical, while iterating over the set of\nsimple single-via paths can lead to scalable solutions with only a small\ntrade-off in the quality of the results.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 17:02:55 GMT"}, {"version": "v2", "created": "Wed, 24 Oct 2018 14:20:34 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Chondrogiannis", "Theodoros", ""], ["Bouros", "Panagiotis", ""], ["Gamper", "Johann", ""], ["Leser", "Ulf", ""], ["Blumenthal", "David B.", ""]]}, {"id": "1809.06859", "submitter": "Dennis Diefenbach", "authors": "Dennis Diefenbach and Jos\\'ee M. Gim\\'enez-Garc\\'ia", "title": "HDTCat: let's make HDT scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  HDT (Header, Dictionary, Triples) is a serialization for RDF. HDT has become\nvery popular in the last years because it allows to store RDF data with a small\ndisk footprint, while remaining at the same time queriable. For this reason HDT\nis often used when scalability becomes an issue. Once RDF data is serialized\ninto HDT, the disk footprint to store it and the memory footprint to query it\nare very low. However, generating HDT files from raw text RDF serializations\n(like N-Triples) is a time-consuming and (especially) memory-consuming task. In\nthis publication we present HDTCat, an algorithm and command line tool to join\ntwo HDT files with low memory footprint. HDTCat can be used in a\ndivide-and-conquer strategy to generate HDT files from huge datasets using a\nlow-memory footprint.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 15:10:52 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Diefenbach", "Dennis", ""], ["Gim\u00e9nez-Garc\u00eda", "Jos\u00e9e M.", ""]]}, {"id": "1809.07473", "submitter": "Xin Huang", "authors": "Xin Huang, Jiaxin Jiang, Byron Choi, Jianliang Xu, Zhiwei Zhang, Yunya\n  Song,", "title": "PP-DBLP: Modeling and Generating Attributed Public-Private Networks with\n  DBLP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many online social networks (e.g., Facebook, Google+, Twitter, and\nInstagram), users prefer to hide her/his partial or all relationships, which\nmakes such private relationships not visible to public users or even friends.\nThis leads to a new graph model called public-private networks, where each user\nhas her/his own perspective of the network including the private connections.\nRecently, public-private network analysis has attracted significant research\ninterest in the literature. A great deal of important graph computing problems\n(e.g., shortest paths, centrality, PageRank, and reachability tree) has been\nstudied. However, due to the limited data sources and privacy concerns,\nproposed approaches are not tested on real-world datasets, but on synthetic\ndatasets by randomly selecting vertices as private ones. Therefore, real-world\ndatasets of public-private networks are essential and urgently needed for such\nalgorithms in the evaluation of efficiency and effectiveness.\n  In this paper, we generate four public-private networks from real-world DBLP\nrecords, called PPDBLP. We take published articles as public information and\nregard ongoing collaborations as the hidden information, which is only known by\nthe authors. Our released datasets of PPDBLP offer the prospects for verifying\nvarious kinds of efficient public-private analysis algorithms in a fair way. In\naddition, motivated by widely existing attributed graphs, we propose an\nadvanced model of attributed public-private graphs where vertices have not only\nprivate edges but also private attributes. We also discuss open problems on\nattributed public-private graphs. Preliminary experimental results on our\ngenerated real-world datasets verify the effectiveness and efficiency of\npublic-private models and state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 04:38:41 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Huang", "Xin", ""], ["Jiang", "Jiaxin", ""], ["Choi", "Byron", ""], ["Xu", "Jianliang", ""], ["Zhang", "Zhiwei", ""], ["Song", "Yunya", ""]]}, {"id": "1809.10054", "submitter": "Lidia Contreras-Ochando", "authors": "Lidia Contreras-Ochando, C\\'esar Ferri, Jos\\'e Hern\\'andez-Orallo,\n  Fernando Mart\\'inez-Plumed, Mar\\'ia Jos\\'e Ram\\'irez-Quintana, Susumu\n  Katayama", "title": "General-purpose Declarative Inductive Programming with Domain-Specific\n  Background Knowledge for Data Wrangling Automation", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given one or two examples, humans are good at understanding how to solve a\nproblem independently of its domain, because they are able to detect what the\nproblem is and to choose the appropriate background knowledge according to the\ncontext. For instance, presented with the string \"8/17/2017\" to be transformed\nto \"17th of August of 2017\", humans will process this in two steps: (1) they\nrecognise that it is a date and (2) they map the date to the 17th of August of\n2017. Inductive Programming (IP) aims at learning declarative (functional or\nlogic) programs from examples. Two key advantages of IP are the use of\nbackground knowledge and the ability to synthesise programs from a few\ninput/output examples (as humans do). In this paper we propose to use IP as a\nmeans for automating repetitive data manipulation tasks, frequently presented\nduring the process of {\\em data wrangling} in many data manipulation problems.\nHere we show that with the use of general-purpose declarative (programming)\nlanguages jointly with generic IP systems and the definition of domain-specific\nknowledge, many specific data wrangling problems from different application\ndomains can be automatically solved from very few examples. We also propose an\nintegrated benchmark for data wrangling, which we share publicly for the\ncommunity.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 15:12:14 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Contreras-Ochando", "Lidia", ""], ["Ferri", "C\u00e9sar", ""], ["Hern\u00e1ndez-Orallo", "Jos\u00e9", ""], ["Mart\u00ednez-Plumed", "Fernando", ""], ["Ram\u00edrez-Quintana", "Mar\u00eda Jos\u00e9", ""], ["Katayama", "Susumu", ""]]}, {"id": "1809.10212", "submitter": "Ryan Marcus", "authors": "Ryan Marcus, Olga Papaemmanouil", "title": "Towards a Hands-Free Query Optimizer through Deep Learning", "comments": "Published in CIDR19", "journal-ref": "9th Biennial Conference on Innovative Data Systems Research, 2019", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query optimization remains one of the most important and well-studied\nproblems in database systems. However, traditional query optimizers are complex\nheuristically-driven systems, requiring large amounts of time to tune for a\nparticular database and requiring even more time to develop and maintain in the\nfirst place. In this vision paper, we argue that a new type of query optimizer,\nbased on deep reinforcement learning, can drastically improve on the\nstate-of-the-art. We identify potential complications for future research that\nintegrates deep learning with query optimization, and we describe three novel\ndeep learning based approaches that can lead the way to end-to-end\nlearning-based query optimizers.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 19:51:03 GMT"}, {"version": "v2", "created": "Tue, 18 Dec 2018 00:00:33 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Marcus", "Ryan", ""], ["Papaemmanouil", "Olga", ""]]}, {"id": "1809.10286", "submitter": "Leopoldo Bertossi", "authors": "Leopoldo Bertossi", "title": "Repair-Based Degrees of Database Inconsistency: Computation and\n  Complexity", "comments": "Some editing made and some new paragraphs added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a generic numerical measure of the inconsistency of a database\nwith respect to a set of integrity constraints. It is based on an abstract\nrepair semantics. In particular, an inconsistency measure associated to\ncardinality-repairs is investigated in detail. More specifically, it is shown\nthat it can be computed via answer-set programs, but sometimes its computation\ncan be intractable in data complexity. However, polynomial-time deterministic\nand randomized approximations are exhibited. The behavior of this measure under\nsmall updates is analyzed, obtaining fixed-parameter tractability results.\nFurthermore, alternative inconsistency measures are proposed and discussed.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 00:57:33 GMT"}, {"version": "v2", "created": "Sun, 23 Dec 2018 02:32:17 GMT"}, {"version": "v3", "created": "Tue, 22 Jan 2019 17:14:34 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Bertossi", "Leopoldo", ""]]}, {"id": "1809.10357", "submitter": "Hiroyuki Kato Dr.", "authors": "Yasuhito Asano, Soichiro Hidaka, Zhenjiang Hu, Yasunori Ishihara,\n  Hiroyuki Kato, Hsiang-Shang Ko, Keisuke Nakano, Makoto Onizuka, Yuya Sasaki,\n  Toshiyuki Shimizu, Van-Dang Tran, Kanae Tsushima, Masatoshi Yoshikawa", "title": "Making View Update Strategies Programmable - Toward Controlling and\n  Sharing Distributed Data -", "comments": "6 pages. arXiv admin note: text overlap with arXiv:1803.06674", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Views are known mechanisms for controlling access of data and for sharing\ndata of different schemas. Despite long and intensive research on views in both\nthe database community and the programming language community, we are facing\ndifficulties to use views in practice. The main reason is that we lack ways to\ndirectly describe view update strategies to deal with the inherent ambiguity of\nview updating. This paper aims to provide a new language-based approach to\ncontrolling and sharing distributed data based on views, and establish a\nsoftware foundation for systematic construction of such data management\nsystems. Our key observation is that a view should be defined through a view\nupdate strategy rather than a view definition. We show that Datalog can be used\nfor specifying view update strategies whose unique view definition can be\nautomatically derived, present a novel P2P-based programmable architecture for\ndistributed data management where updatable views are fully utilized for\ncontrolling and sharing distributed data, and demonstrate its usefulness\nthrough the development of a privacy-preserving ride-sharing alliance system.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 06:07:27 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Asano", "Yasuhito", ""], ["Hidaka", "Soichiro", ""], ["Hu", "Zhenjiang", ""], ["Ishihara", "Yasunori", ""], ["Kato", "Hiroyuki", ""], ["Ko", "Hsiang-Shang", ""], ["Nakano", "Keisuke", ""], ["Onizuka", "Makoto", ""], ["Sasaki", "Yuya", ""], ["Shimizu", "Toshiyuki", ""], ["Tran", "Van-Dang", ""], ["Tsushima", "Kanae", ""], ["Yoshikawa", "Masatoshi", ""]]}, {"id": "1809.10404", "submitter": "Yaliang Li", "authors": "Yang Deng, Yaliang Li, Ying Shen, Nan Du, Wei Fan, Min Yang, Kai Lei", "title": "MedTruth: A Semi-supervised Approach to Discovering Knowledge Condition\n  Information from Multi-Source Medical Data", "comments": "Accepted as CIKM2019 long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Graph (KG) contains entities and the relations between entities.\nDue to its representation ability, KG has been successfully applied to support\nmany medical/healthcare tasks. However, in the medical domain, knowledge holds\nunder certain conditions. For example, symptom \\emph{runny nose} highly\nindicates the existence of disease \\emph{whooping cough} when the patient is a\nbaby rather than the people at other ages. Such conditions for medical\nknowledge are crucial for decision-making in various medical applications,\nwhich is missing in existing medical KGs. In this paper, we aim to discovery\nmedical knowledge conditions from texts to enrich KGs.\n  Electronic Medical Records (EMRs) are systematized collection of clinical\ndata and contain detailed information about patients, thus EMRs can be a good\nresource to discover medical knowledge conditions. Unfortunately, the amount of\navailable EMRs is limited due to reasons such as regularization. Meanwhile, a\nlarge amount of medical question answering (QA) data is available, which can\ngreatly help the studied task. However, the quality of medical QA data is quite\ndiverse, which may degrade the quality of the discovered medical knowledge\nconditions. In the light of these challenges, we propose a new truth discovery\nmethod, MedTruth, for medical knowledge condition discovery, which incorporates\nprior source quality information into the source reliability estimation\nprocedure, and also utilizes the knowledge triple information for trustworthy\ninformation computation. We conduct series of experiments on real-world medical\ndatasets to demonstrate that the proposed method can discover meaningful and\naccurate conditions for medical knowledge by leveraging both EMR and QA data.\nFurther, the proposed method is tested on synthetic datasets to validate its\neffectiveness under various scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 08:35:16 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2019 03:11:00 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Deng", "Yang", ""], ["Li", "Yaliang", ""], ["Shen", "Ying", ""], ["Du", "Nan", ""], ["Fan", "Wei", ""], ["Yang", "Min", ""], ["Lei", "Kai", ""]]}, {"id": "1809.10856", "submitter": "Amarnath Gupta", "authors": "Kai Lin and Subhasis Dasgupta and Amarnath Gupta", "title": "Answering Analytical Queries on Text Data with Temporal Term Histograms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal text, i.e., time-stamped text data are found abundantly in a variety\nof data sources like newspapers, blogs and social media posts. While today's\ndata management systems provide facilities for searching full-text data, they\ndo not provide any simple primitives for performing analytical operations with\ntext. This paper proposes the temporal term histograms (TTH) as an intermediate\nprimitive that can be used for analytical tasks. We propose an algebra, with\noperators and equivalence rules for TTH and present a reference implementation\non a relational database system.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 04:47:54 GMT"}, {"version": "v2", "created": "Mon, 15 Oct 2018 03:15:27 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Lin", "Kai", ""], ["Dasgupta", "Subhasis", ""], ["Gupta", "Amarnath", ""]]}, {"id": "1809.11084", "submitter": "Saravanan Thirumuruganathan", "authors": "Saravanan Thirumuruganathan, Shameem A Puthiya Parambath, Mourad\n  Ouzzani, Nan Tang, Shafiq Joty", "title": "Reuse and Adaptation for Entity Resolution through Transfer Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity resolution (ER) is one of the fundamental problems in data\nintegration, where machine learning (ML) based classifiers often provide the\nstate-of-the-art results. Considerable human effort goes into feature\nengineering and training data creation. In this paper, we investigate a new\nproblem: Given a dataset D_T for ER with limited or no training data, is it\npossible to train a good ML classifier on D_T by reusing and adapting the\ntraining data of dataset D_S from same or related domain? Our major\ncontributions include (1) a distributed representation based approach to encode\neach tuple from diverse datasets into a standard feature space; (2)\nidentification of common scenarios where the reuse of training data can be\nbeneficial; and (3) five algorithms for handling each of the aforementioned\nscenarios. We have performed comprehensive experiments on 12 datasets from 5\ndifferent domains (publications, movies, songs, restaurants, and books). Our\nexperiments show that our algorithms provide significant benefits such as\nproviding superior performance for a fixed training data size.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 15:26:17 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Thirumuruganathan", "Saravanan", ""], ["Parambath", "Shameem A Puthiya", ""], ["Ouzzani", "Mourad", ""], ["Tang", "Nan", ""], ["Joty", "Shafiq", ""]]}, {"id": "1809.11099", "submitter": "Michael F\\\"arber", "authors": "Michael F\\\"arber, Achim Rettinger", "title": "Which Knowledge Graph Is Best for Me?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, DBpedia, Freebase, OpenCyc, Wikidata, and YAGO have been\npublished as noteworthy large, cross-domain, and freely available knowledge\ngraphs. Although extensively in use, these knowledge graphs are hard to compare\nagainst each other in a given setting. Thus, it is a challenge for researchers\nand developers to pick the best knowledge graph for their individual needs. In\nour recent survey, we devised and applied data quality criteria to the\nabove-mentioned knowledge graphs. Furthermore, we proposed a framework for\nfinding the most suitable knowledge graph for a given setting. With this paper\nwe intend to ease the access to our in-depth survey by presenting simplified\nrules that map individual data quality requirements to specific knowledge\ngraphs. However, this paper does not intend to replace our previously\nintroduced decision-support framework. For an informed decision on which KG is\nbest for you we still refer to our in-depth survey.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 15:44:21 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["F\u00e4rber", "Michael", ""], ["Rettinger", "Achim", ""]]}]