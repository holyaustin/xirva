[{"id": "1903.00226", "submitter": "Wim Martens", "authors": "Wim Martens and Matthias Niewerth and Tina Trautner", "title": "A Trichotomy for Regular Trail Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regular path queries (RPQs) are an essential component of graph query\nlanguages. Such queries consider a regular expression r and a directed\nedge-labeled graph G and search for paths in G for which the sequence of labels\nis in the language of r. In order to avoid having to consider infinitely many\npaths, some database engines restrict such paths to be trails, that is, they\nonly consider paths without repeated edges. In this paper we consider the\nevaluation problem for RPQs under trail semantics, in the case where the\nexpression is fixed. We show that, in this setting, there exists a trichotomy.\nMore precisely, the complexity of RPQ evaluation divides the regular languages\ninto the finite languages, the class Ttract (for which the problem is\ntractable), and the rest. Interestingly, the tractable class in the trichotomy\nis larger than for the trichotomy for simple paths, discovered by Bagan,\nBonifati, and Groz [PODS 2013]. In addition to this trichotomy result, we also\nstudy characterizations of the tractable class, its expressivity, the\nrecognition problem, closure properties, and show how the decision problem can\nbe extended to the enumeration problem, which is relevant to practice.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 09:50:53 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Martens", "Wim", ""], ["Niewerth", "Matthias", ""], ["Trautner", "Tina", ""]]}, {"id": "1903.00237", "submitter": "Yi Liu", "authors": "Yi Wu, Yi Liu, Syed Hassan Ahmed, Jialiang Peng, Ahmed A. Abd El-Latif", "title": "Dominant Dataset Selection Algorithms for Electricity Consumption\n  Time-Series Data Analysis Based on Affine Transformation", "comments": "This paper has been accepted by IEEE Internet of Things Journal", "journal-ref": "IEEE Internet of Things Journal, pp. 1-1, 2019", "doi": "10.1109/JIOT.2019.2946753", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The explosive growth of time-series data, the scale of time-series data (TSD)\nsuggests that the scale and capability of many Internet of Things (IoT)-based\napplications has already been exceeded. Moreover, redundancy persists in TSD\ndue to the correlation between information acquired via different sources. In\nthis paper, we propose a cohort of dominant dataset selection algorithms for\nelectricity consumption time-series data with a focus on discriminating the\ndominant dataset that is small dataset but capable of representing the kernel\ninformation carried by time-series data with an arbitrarily small error rate\nless than \". Furthermore, we prove that the selection problem of the minimum\ndominant dataset is an NP-complete problem. The affine transformation model is\nintroduced to define the linear correlation relationship between time-series\ndata objects. Our proposed framework consists of the scanning selection\nalgorithm with O(n3) time complexity and the greedy selection algorithm with\nO(n4) time complexity, which are respectively proposed to select the dominant\ndataset based on the linear correlation distance between time-series data\nobjects. The proposed algorithms are evaluated on the real electricity\nconsumption data Harbin city in China. The experimental results show that the\nproposed algorithms not only reduce the size of the extracted kernel dataset\nbut also ensure the time-series data integrity in terms of accuracy and\nefficiency.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 10:21:42 GMT"}, {"version": "v2", "created": "Wed, 13 Mar 2019 04:08:43 GMT"}, {"version": "v3", "created": "Sun, 31 Mar 2019 13:39:41 GMT"}, {"version": "v4", "created": "Fri, 17 May 2019 16:25:46 GMT"}, {"version": "v5", "created": "Thu, 5 Sep 2019 01:15:39 GMT"}, {"version": "v6", "created": "Mon, 11 Nov 2019 04:43:36 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Wu", "Yi", ""], ["Liu", "Yi", ""], ["Ahmed", "Syed Hassan", ""], ["Peng", "Jialiang", ""], ["El-Latif", "Ahmed A. Abd", ""]]}, {"id": "1903.00424", "submitter": "Yi Lu", "authors": "Yi Lu, Xiangyao Yu, Samuel Madden", "title": "SCAR: Strong Consistency using Asynchronous Replication with Minimal\n  Coordination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data replication is crucial in modern distributed systems as a means to\nprovide high availability. Many techniques have been proposed to utilize\nreplicas to improve a system's performance, often requiring expensive\ncoordination or sacrificing consistency. In this paper, we present SCAR, a new\ndistributed and replicated in-memory database that allows serializable\ntransactions to read from backup replicas with minimal coordination. SCAR works\nby assigning logical timestamps to database records so that a transaction can\nsafely read from a backup replica without coordinating with the primary\nreplica, because the records cannot be changed up to a certain logical time. In\naddition, we propose two optimization techniques, timestamp synchronization and\nparallel locking and validation, to further reduce coordination. We show that\nSCAR outperforms systems with conventional concurrency control algorithms and\nreplication strategies by up to a factor of 2 on three popular benchmarks. We\nalso demonstrate that SCAR achieves higher throughput by running under reduced\nisolation levels and detects concurrency anomalies in real time.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 17:31:13 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Lu", "Yi", ""], ["Yu", "Xiangyao", ""], ["Madden", "Samuel", ""]]}, {"id": "1903.00452", "submitter": "Amirhesam Shahvarani", "authors": "Amirhesam Shahvarani, Hans-Arno Jacobsen", "title": "Parallel Index-based Stream Join on a Multicore CPU", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is increasing interest in using multicore processors to accelerate\nstream processing. For example, indexing sliding window content to enhance the\nperformance of streaming queries is greatly improved by utilizing the\ncomputational capabilities of a multicore processor. However, designing an\neffective concurrency control mechanism that addresses the problem of\nconcurrent indexing in highly dynamic settings remains a challenge. In this\npaper, we introduce an index data structure, called the Partitioned In-memory\nMerge-Tree, to address the challenges that arise when indexing highly dynamic\ndata, which are common in streaming settings. To complement the index, we\ndesign an algorithm to realize a parallel index-based stream join that exploits\nthe computational power of multicore processors. Our experiments using an\nocta-core processor show that our parallel stream join achieves up to 5.5 times\nhigher throughput than a single-threaded approach.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 18:25:04 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Shahvarani", "Amirhesam", ""], ["Jacobsen", "Hans-Arno", ""]]}, {"id": "1903.00731", "submitter": "Dimitrios Liarokapis", "authors": "Dimitrios Liarokapis, Elizabeth ONeil, Patrick ONeil", "title": "HISTEX (HISTory EXerciser) : A tool for testing the implementation of\n  Isolation Levels of Relational Database Management Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a multi-process application called HISTEX (HISTory EXerciser),\nwhich executes input histories in a generic transactional notation on\ncommercial DBMS platforms. HISTEX could be used to discover potential errors in\nthe implementation of Isolation Levels by Relational Database Management\nSystems or cases where a system behaves over restrictively. It can also be used\nfor performance measurements related to database workloads executing on real\ndatabase systems instead of simulated environments. HISTEX has been implemented\nin C by utilizing Embedded SQL. However, many of its ideas could be\nreincarnated in new implementations that could rely on other database\nconnectivity paradigms such as JDBC, JPA etc. We expect that by presenting some\nof the ideas behind its development we could re-invigorate some fresh interest\nand involvement in the research community regarding such tools.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2019 16:32:33 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Liarokapis", "Dimitrios", ""], ["ONeil", "Elizabeth", ""], ["ONeil", "Patrick", ""]]}, {"id": "1903.01363", "submitter": "Sanjay Krishnan", "authors": "Xi Liang, Aaron J. Elmore, Sanjay Krishnan", "title": "Opportunistic View Materialization with Deep Reinforcement Learning", "comments": "14 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Carefully selected materialized views can greatly improve the performance of\nOLAP workloads. We study using deep reinforcement learning to learn adaptive\nview materialization and eviction policies. Our insight is that such selection\npolicies can be effectively trained with an asynchronous RL algorithm, that\nruns paired counter-factual experiments during system idle times to evaluate\nthe incremental value of persisting certain views. Such a strategy obviates the\nneed for accurate cardinality estimation or hand-designed scoring heuristics.\nWe focus on inner-join views and modeling effects in a main-memory, OLAP\nsystem. Our research prototype system, called DQM, is implemented in SparkSQL\nand we experiment on several workloads including the Join Order Benchmark and\nthe TPC-DS workload. Results suggest that: (1) DQM can outperform heuristic\nwhen their assumptions are not satisfied by the workload or there are temporal\neffects like period maintenance, (2) even with the cost of learning, DQM is\nmore adaptive to changes in the workload, and (3) DQM is broadly applicable to\ndifferent workloads and skews.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 16:54:56 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Liang", "Xi", ""], ["Elmore", "Aaron J.", ""], ["Krishnan", "Sanjay", ""]]}, {"id": "1903.01498", "submitter": "Yuliang Li", "authors": "Sara Evensen, Aaron Feng, Alon Halevy, Jinfeng Li, Vivian Li, Yuliang\n  Li, Huining Liu, George Mihaila, John Morales, Natalie Nuno, Ekaterina\n  Pavlovic, Wang-Chiew Tan, Xiaolan Wang", "title": "Voyageur: An Experiential Travel Search Engine", "comments": "Demo paper accepted to the Web Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe Voyageur, which is an application of experiential search to the\ndomain of travel. Unlike traditional search engines for online services,\nexperiential search focuses on the experiential aspects of the service under\nconsideration. In particular, Voyageur needs to handle queries for subjective\naspects of the service (e.g., quiet hotel, friendly staff) and combine these\nwith objective attributes, such as price and location. Voyageur also highlights\ninteresting facts and tips about the services the user is considering to\nprovide them with further insights into their choices.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 19:25:15 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Evensen", "Sara", ""], ["Feng", "Aaron", ""], ["Halevy", "Alon", ""], ["Li", "Jinfeng", ""], ["Li", "Vivian", ""], ["Li", "Yuliang", ""], ["Liu", "Huining", ""], ["Mihaila", "George", ""], ["Morales", "John", ""], ["Nuno", "Natalie", ""], ["Pavlovic", "Ekaterina", ""], ["Tan", "Wang-Chiew", ""], ["Wang", "Xiaolan", ""]]}, {"id": "1903.02076", "submitter": "Amine Mhedhbi", "authors": "Amine Mhedhbi, Semih Salihoglu", "title": "Optimizing Subgraph Queries by Combining Binary and Worst-Case Optimal\n  Joins", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of optimizing subgraph queries using the new worst-case\noptimal join plans. Worst-case optimal plans evaluate queries by matching one\nquery vertex at a time using multiway intersections. The core problem in\noptimizing worst-case optimal plans is to pick an ordering of the query\nvertices to match. We design a cost-based optimizer that (i) picks efficient\nquery vertex orderings for worst-case optimal plans; and (ii) generates hybrid\nplans that mix traditional binary joins with worst-case optimal style multiway\nintersections. Our cost metric combines the cost of binary joins with a new\ncost metric called intersection-cost. The plan space of our optimizer contains\nplans that are not in the plan spaces based on tree decompositions from prior\nwork. In addition to our optimizer, we describe an adaptive technique that\nchanges the orderings of the worst-case optimal sub-plans during query\nexecution. We demonstrate the effectiveness of the plans our optimizer picks\nand adaptive technique through extensive experiments. Our optimizer is\nintegrated into the Graphflow DBMS.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 21:57:11 GMT"}, {"version": "v2", "created": "Sun, 2 Jun 2019 16:40:19 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Mhedhbi", "Amine", ""], ["Salihoglu", "Semih", ""]]}, {"id": "1903.02641", "submitter": "Abhishek Santra", "authors": "Abhishek Santra, Kanthi Sannappa Komar, Sanjukta Bhowmick and Sharma\n  Chakravarthy", "title": "Structure-Preserving Community In A Multilayer Network: Definition,\n  Detection, And Analysis", "comments": "27 pages, Submitted to VLDB 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilayer networks or MLNs (also called multiplexes or network of networks)\nare being used extensively for modeling and analysis of data sets with multiple\nentity and feature types as well as their relationships. As the concept of\ncommunities and hubs are used for these analysis, a structure-preserving\ndefinition for them on MLNs (that retains the original MLN structure and\nnode/edge labels and types) and its efficient detection are critical. There is\nno structure-preserving definition of a community for a MLN as most of the\ncurrent analyses aggregate a MLN to a single graph. Although there is consensus\non community definition for single graphs (and detection packages) and to a\nlesser extent for homogeneous MLNs, it is lacking for heterogeneous MLNs. In\nthis paper, we not only provide a structure-preserving definition for the first\ntime, but also its efficient computation using a decoupling approach, and\ndiscuss its characteristics & significance for analysis. The proposed\ndecoupling approach for efficiency combines communities from individual layers\nto form a serial k-community for connected k layers in a MLN. We propose\nseveral weight metrics for composing layer-wise communities using the bipartite\ngraph match approach based on the analysis semantics. Our proposed approach has\na number of advantages. It: i) leverages extant single graph community\ndetection algorithms, ii) is based on the widely-used maximal flow bipartite\ngraph matching for composing k layers, iii) introduces several weight metrics\nthat are customized for the community concept, and iv) experimentally validates\nthe definition, mapping, and efficiency from a flexible analysis perspective on\nwidely-used IMDb data set.\n  Keywords: Heterogeneous Multilayer Networks; Bipartite Graphs; Community\nDefinition and Detection; Decoupling-Based Composition\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 22:47:47 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Santra", "Abhishek", ""], ["Komar", "Kanthi Sannappa", ""], ["Bhowmick", "Sanjukta", ""], ["Chakravarthy", "Sharma", ""]]}, {"id": "1903.02759", "submitter": "Marc Shapiro", "authors": "Sreeja Nair (DELYS), Gustavo Petri, Marc Shapiro (DELYS)", "title": "Invariant Safety for Distributed Applications", "comments": "Workshop on Principles and Practice of Consistency for Distributed\n  Data (PaPoC), Mar 2019, Dresden, Germany.\n  https://novasys.di.fct.unl.pt/conferences/papoc19/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a proof methodology for verifying the safety of data invariants of\nhighly-available distributed applications that replicate state. The proof is\n(1) modular: one can reason about each individual operation separately, and (2)\nsequential: one can reason about a distributed application as if it were\nsequential. We automate the methodology and illustrate the use of the tool with\na representative example.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 07:40:22 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Nair", "Sreeja", "", "DELYS"], ["Petri", "Gustavo", "", "DELYS"], ["Shapiro", "Marc", "", "DELYS"]]}, {"id": "1903.02949", "submitter": "Hermano Lustosa", "authors": "Hermano Lustosa, Fabio Porto", "title": "SAVIME: A Multidimensional System for the Analysis and Visualization of\n  Simulation Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scientific applications produce a huge amount of data, which imposes serious\nmanagement and analysis challenges. In particular, limitations in current\ndatabase management systems prevent their adoption in simulation applications,\nin which in-situ analysis libraries, in-transit I/O interfaces and scientific\nformat files are preferred over DBMSs. In order to make simulation applications\nbenefit from DBMS support, the author proposes the development of a system\ncalled SAVIME in the context of his PhD thesis. SAVIME is an array database\nsystem designed to manage numerical simulation data. In this document, the\nauthor presents all work conducted so far and the current state of development.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 14:46:04 GMT"}, {"version": "v2", "created": "Fri, 15 Mar 2019 01:33:33 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Lustosa", "Hermano", ""], ["Porto", "Fabio", ""]]}, {"id": "1903.02990", "submitter": "Anthony Tomasic", "authors": "Yangjun Sheng and Anthony Tomasic and Tieying Zhang and Andrew Pavlo", "title": "Scheduling OLTP Transactions via Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current main memory database system architectures are still challenged by\nhigh contention workloads and this challenge will continue to grow as the\nnumber of cores in processors continues to increase. These systems schedule\ntransactions randomly across cores to maximize concurrency and to produce a\nuniform load across cores. Scheduling never considers potential conflicts.\nPerformance could be improved if scheduling balanced between concurrency to\nmaximize throughput and scheduling transactions linearly to avoid conflicts. In\nthis paper, we present the design of several intelligent transaction scheduling\nalgorithms that consider both potential transaction conflicts and concurrency.\nTo incorporate reasoning about transaction conflicts, we develop a supervised\nmachine learning model that estimates the probability of conflict. This model\nis incorporated into several scheduling algorithms. In addition, we integrate\nan unsupervised machine learning algorithm into an intelligent scheduling\nalgorithm. We then empirically measure the performance impact of different\nscheduling algorithms on OLTP and social networking workloads. Our results show\nthat, with appropriate settings, intelligent scheduling can increase throughput\nby 54% and reduce abort rate by 80% on a 20-core machine, relative to random\nscheduling. In summary, the paper provides preliminary evidence that\nintelligent scheduling significantly improves DBMS performance.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 15:27:30 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 12:36:39 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Sheng", "Yangjun", ""], ["Tomasic", "Anthony", ""], ["Zhang", "Tieying", ""], ["Pavlo", "Andrew", ""]]}, {"id": "1903.03229", "submitter": "John Feser", "authors": "John K. Feser, Samuel Madden, Nan Tang, Armando Solar-Lezama", "title": "Deductive Optimization of Relational Data Storage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimizing the physical data storage and retrieval of data are two key\ndatabase management problems. In this paper, we propose a language that can\nexpress a wide range of physical database layouts, going well beyond the row-\nand column-based methods that are widely used in database management systems.\nWe use deductive synthesis to turn a high-level relational representation of a\ndatabase query into a highly optimized low-level implementation which operates\non a specialized layout of the dataset. We build a compiler for this language\nand conduct experiments using a popular database benchmark, which shows that\nthe performance of these specialized queries is competitive with a\nstate-of-the-art in memory compiled database system.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 00:40:21 GMT"}, {"version": "v2", "created": "Sun, 14 Jul 2019 10:30:29 GMT"}, {"version": "v3", "created": "Wed, 5 Feb 2020 22:07:28 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Feser", "John K.", ""], ["Madden", "Samuel", ""], ["Tang", "Nan", ""], ["Solar-Lezama", "Armando", ""]]}, {"id": "1903.03676", "submitter": "Lei Zhang", "authors": "Lei Zhang, Sean Howard, Tom Montpool, Jessica Moore, Krittika Mahajan,\n  Andriy Miranskyy", "title": "RESTORE: Automated Regression Testing for Datasets", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In data mining, the data in various business cases (e.g., sales, marketing,\nand demography) gets refreshed periodically. During the refresh, the old\ndataset is replaced by a new one. Confirming the quality of the new dataset can\nbe challenging because changes are inevitable.\n  How do analysts distinguish reasonable real-world changes vs. errors related\nto data capture or data transformation? While some of the errors are easy to\nspot, the others may be more subtle. In order to detect such types of errors,\nan analyst will typically have to examine the data manually and assess if the\ndata produced are \"believable\". Due to the scale of data, such examination is\ntedious and laborious. Thus, to save the analyst's time, it is important to\ndetect these errors automatically. However, both the literature and the\nindustry are still lacking methods to assess the difference between old and new\nversions of a dataset during the refresh process.\n  In this paper, we present a comprehensive set of tests for the detection of\nabnormalities in a refreshed dataset, based on the information obtained from a\nprevious vintage of the dataset. We implement these tests in automated test\nharness made available as an open-source package, called RESTORE, for R\nlanguage. The harness accepts flat or hierarchical numeric datasets. We also\npresent a validation case study, where we apply our test harness to\nhierarchical demographic datasets. The results of the study and feedback from\ndata scientists using the package suggest that RESTORE enables fast and\nefficient detection of errors in the data as well as decreases the cost of\ntesting.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 21:47:28 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Zhang", "Lei", ""], ["Howard", "Sean", ""], ["Montpool", "Tom", ""], ["Moore", "Jessica", ""], ["Mahajan", "Krittika", ""], ["Miranskyy", "Andriy", ""]]}, {"id": "1903.03683", "submitter": "Julia Stoyanovich", "authors": "Serge Abiteboul, Julia Stoyanovich", "title": "Transparency, Fairness, Data Protection, Neutrality: Data Management\n  Challenges in the Face of New Regulation", "comments": "To appear in the ACM Journal of Data and Information Quality (JDIQ)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The data revolution continues to transform every sector of science, industry\nand government. Due to the incredible impact of data-driven technology on\nsociety, we are becoming increasingly aware of the imperative to use data and\nalgorithms responsibly -- in accordance with laws and ethical norms. In this\narticle we discuss three recent regulatory frameworks: the European Union's\nGeneral Data Protection Regulation (GDPR), the New York City Automated\nDecisions Systems (ADS) Law, and the Net Neutrality principle, that aim to\nprotect the rights of individuals who are impacted by data collection and\nanalysis. These frameworks are prominent examples of a global trend:\nGovernments are starting to recognize the need to regulate data-driven\nalgorithmic technology.\n  Our goal in this paper is to bring these regulatory frameworks to the\nattention of the data management community, and to underscore the technical\nchallenges they raise and which we, as a community, are well-equipped to\naddress. The main take-away of this article is that legal and ethical norms\ncannot be incorporated into data-driven systems as an afterthought. Rather, we\nmust think in terms of responsibility by design, viewing it as a systems\nrequirement.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 22:12:12 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Abiteboul", "Serge", ""], ["Stoyanovich", "Julia", ""]]}, {"id": "1903.03761", "submitter": "Petr Luk\\'a\\v{s}", "authors": "Petr Luk\\'a\\v{s}, Radim Ba\\v{c}a, Michal Kr\\'atk\\'y", "title": "RadegastXDB - Prototype of Native XML Database Management System:\n  Technical Report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A lot of advances in the processing of XML data have been proposed in last\ntwo decades. There were many approaches focused on the efficient processing of\ntwig pattern queries (TPQ). However, including the TPQ into an XQuery compiler\nis not a straightforward task and current XML DBMSs process XQueries without\nany TPQ detection. In this paper, we demonstrate our prototype of a native XML\nDBMS called RadegastXDB that uses a TPQ detection to accelerate structural\nXQueries. Such a detection allows us to utilize state-of-the-art TPQ processing\nalgorithms. Our experiments show that, for the structural queries, these\nalgorithms and state-of-the-art XML indexing techniques make our prototype\nfaster than all of the current XML DBMSs, especially for large data\ncollections. We also show that using the same techniques is also efficient for\nthe processing of queries with value predicates.\n", "versions": [{"version": "v1", "created": "Sat, 9 Mar 2019 08:18:55 GMT"}, {"version": "v2", "created": "Thu, 14 Mar 2019 07:25:34 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Luk\u00e1\u0161", "Petr", ""], ["Ba\u010da", "Radim", ""], ["Kr\u00e1tk\u00fd", "Michal", ""]]}, {"id": "1903.04049", "submitter": "Behrooz Omidvar-Tehrani", "authors": "Placido A. Souza Neto and Francisco B. Silva Junior and Felipe F.\n  Pontes and Behrooz Omidvar-Tehrani", "title": "Exploration of Interesting Dense Regions in Spatial Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nowadays, spatial data are ubiquitous in various fields of science, such as\ntransportation and the social Web. A recent research direction in analyzing\nspatial data is to provide means for \"exploratory analysis\" of such data where\nanalysts are guided towards interesting options in consecutive analysis\niterations. Typically, the guidance component learns analyst's preferences\nusing her explicit feedback, e.g., picking a spatial point or selecting a\nregion of interest. However, it is often the case that analysts forget or don't\nfeel necessary to explicitly express their feedback in what they find\ninteresting. Our approach captures implicit feedback on spatial data. The\napproach consists of observing mouse moves (as a means of analyst's\ninteraction) and also the explicit analyst's interaction with data points in\norder to discover interesting spatial regions with dense mouse hovers. In this\npaper, we define, formalize and explore Interesting Dense Regions (IDRs) which\ncapture preferences of analysts, in order to automatically find interesting\nspatial highlights. Our approach involves a polygon-based abstraction layer for\ncapturing preferences. Using these IDRs, we highlight points to guide analysts\nin the analysis process. We discuss the efficiency and effectiveness of our\napproach through realistic examples and experiments on Airbnb and Yelp\ndatasets.\n", "versions": [{"version": "v1", "created": "Sun, 10 Mar 2019 19:44:43 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Neto", "Placido A. Souza", ""], ["Junior", "Francisco B. Silva", ""], ["Pontes", "Felipe F.", ""], ["Omidvar-Tehrani", "Behrooz", ""]]}, {"id": "1903.04181", "submitter": "Fabien Gandon", "authors": "Fabien Gandon (Laboratoire I3S - SPARKS, WIMMICS, CRISAM), Franck\n  Michel (WIMMICS), Olivier Corby (WIMMICS), Michel Buffa (WIMMICS), Andrea\n  Tettamanzi (WIMMICS), Catherine Faron Zucker (WIMMICS), Elena Cabrio\n  (WIMMICS), Serena Villata (WIMMICS)", "title": "Graph Data on the Web: extend the pivot, don't reinvent the wheel", "comments": "W3C Workshop on Web Standardization for Graph Data - Creating\n  Bridges: RDF, Property Graph and SQL, Mar 2019, Berlin, France. 2019,\n  https://www.w3.org/Data/events/data-ws-2019/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is a collective position paper from the Wimmics research team,\nexpressing our vision of how Web graph data technologies should evolve in the\nfuture in order to ensure a high-level of interoperability between the many\ntypes of applications that produce and consume graph data. Wimmics stands for\nWeb-Instrumented Man-Machine Interactions, Communities, and Semantics. We are a\njoint research team between INRIA Sophia Antipolis-M{\\'e}diterran{\\'e}e and I3S\n(CNRS and Universit{\\'e} C{\\^o}te d'Azur). Our challenge is to bridge formal\nsemantics and social semantics on the web. Our research areas are\ngraph-oriented knowledge representation, reasoning and operationalization to\nmodel and support actors, actions and interactions in web-based epistemic\ncommunities. The application of our research is supporting and fostering\ninteractions in online communities and management of their resources. In this\nposition paper, we emphasize the need to extend the semantic Web standard stack\nto address and fulfill new graph data needs, as well as the importance of\nremaining compatible with existing recommendations, in particular the RDF\nstack, to avoid the painful duplication of models, languages, frameworks, etc.\nThe following sections group motivations for different directions of work and\ncollect reasons for the creation of a working group on RDF 2.0 and other\nrecommendations of the RDF family.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 09:09:06 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Gandon", "Fabien", "", "Laboratoire I3S - SPARKS, WIMMICS, CRISAM"], ["Michel", "Franck", "", "WIMMICS"], ["Corby", "Olivier", "", "WIMMICS"], ["Buffa", "Michel", "", "WIMMICS"], ["Tettamanzi", "Andrea", "", "WIMMICS"], ["Zucker", "Catherine Faron", "", "WIMMICS"], ["Cabrio", "Elena", "", "WIMMICS"], ["Villata", "Serena", "", "WIMMICS"]]}, {"id": "1903.04538", "submitter": "Richard Galvez", "authors": "Richard Galvez, David F. Fouhey, Meng Jin, Alexandre Szenicer,\n  Andr\\'es Mu\\~noz-Jaramillo, Mark C. M. Cheung, Paul J. Wright, Monica G.\n  Bobra, Yang Liu, James Mason, Rajat Thomas", "title": "A Machine Learning Dataset Prepared From the NASA Solar Dynamics\n  Observatory Mission", "comments": "Accepted to The Astrophysical Journal Supplement Series; 11 pages, 8\n  figures", "journal-ref": null, "doi": "10.3847/1538-4365/ab1005", "report-no": null, "categories": "astro-ph.SR cs.AI cs.DB cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we present a curated dataset from the NASA Solar Dynamics\nObservatory (SDO) mission in a format suitable for machine learning research.\nBeginning from level 1 scientific products we have processed various\ninstrumental corrections, downsampled to manageable spatial and temporal\nresolutions, and synchronized observations spatially and temporally. We\nillustrate the use of this dataset with two example applications: forecasting\nfuture EVE irradiance from present EVE irradiance and translating HMI\nobservations into AIA observations. For each application we provide metrics and\nbaselines for future model comparison. We anticipate this curated dataset will\nfacilitate machine learning research in heliophysics and the physical sciences\ngenerally, increasing the scientific return of the SDO mission. This work is a\ndirect result of the 2018 NASA Frontier Development Laboratory Program. Please\nsee the appendix for access to the dataset.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 18:50:48 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Galvez", "Richard", ""], ["Fouhey", "David F.", ""], ["Jin", "Meng", ""], ["Szenicer", "Alexandre", ""], ["Mu\u00f1oz-Jaramillo", "Andr\u00e9s", ""], ["Cheung", "Mark C. M.", ""], ["Wright", "Paul J.", ""], ["Bobra", "Monica G.", ""], ["Liu", "Yang", ""], ["Mason", "James", ""], ["Thomas", "Rajat", ""]]}, {"id": "1903.04552", "submitter": "Nilaksh Das", "authors": "Nilaksh Das, Sanya Chaba, Renzhi Wu, Sakshi Gandhi, Duen Horng Chau,\n  Xu Chu", "title": "GOGGLES: Automatic Image Labeling with Affinity Coding", "comments": "Published at 2020 ACM SIGMOD International Conference on Management\n  of Data", "journal-ref": null, "doi": "10.1145/3318464.3380592", "report-no": null, "categories": "cs.CV cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating large labeled training data is becoming the biggest bottleneck in\nbuilding and deploying supervised machine learning models. Recently, the data\nprogramming paradigm has been proposed to reduce the human cost in labeling\ntraining data. However, data programming relies on designing labeling functions\nwhich still requires significant domain expertise. Also, it is prohibitively\ndifficult to write labeling functions for image datasets as it is hard to\nexpress domain knowledge using raw features for images (pixels).\n  We propose affinity coding, a new domain-agnostic paradigm for automated\ntraining data labeling. The core premise of affinity coding is that the\naffinity scores of instance pairs belonging to the same class on average should\nbe higher than those of pairs belonging to different classes, according to some\naffinity functions. We build the GOGGLES system that implements affinity coding\nfor labeling image datasets by designing a novel set of reusable affinity\nfunctions for images, and propose a novel hierarchical generative model for\nclass inference using a small development set.\n  We compare GOGGLES with existing data programming systems on 5 image labeling\ntasks from diverse domains. GOGGLES achieves labeling accuracies ranging from a\nminimum of 71% to a maximum of 98% without requiring any extensive human\nannotation. In terms of end-to-end performance, GOGGLES outperforms the\nstate-of-the-art data programming system Snuba by 21% and a state-of-the-art\nfew-shot learning technique by 5%, and is only 7% away from the fully\nsupervised upper bound.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 19:19:30 GMT"}, {"version": "v2", "created": "Sun, 1 Mar 2020 02:43:24 GMT"}, {"version": "v3", "created": "Tue, 3 Mar 2020 06:30:24 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Das", "Nilaksh", ""], ["Chaba", "Sanya", ""], ["Wu", "Renzhi", ""], ["Gandhi", "Sakshi", ""], ["Chau", "Duen Horng", ""], ["Chu", "Xu", ""]]}, {"id": "1903.04880", "submitter": "Supreeth Shastri", "authors": "Aashaka Shah, Vinay Banakar, Supreeth Shastri, Melissa Wasserman, and\n  Vijay Chidambaram", "title": "Analyzing the Impact of GDPR on Storage Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently introduced General Data Protection Regulation (GDPR) is forcing\nseveral companies to make significant changes to their systems to achieve\ncompliance. Motivated by the finding that more than 30% of GDPR articles are\nrelated to storage, we investigate the impact of GDPR compliance on storage\nsystems. We illustrate the challenges of retrofitting existing systems into\ncompliance by modifying Redis to be GDPR-compliant. We show that despite\nneeding to introduce a small set of new features, a strict real-time compliance\n(eg logging every user request synchronously) lowers Redis' throughput by 20x.\nOur work reveals how GDPR allows compliance to be a spectrum, and what its\nimplications are for system designers. We discuss the technical challenges that\nneed to be solved before strict compliance can be efficiently achieved.\n", "versions": [{"version": "v1", "created": "Sat, 9 Mar 2019 02:31:56 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 19:10:32 GMT"}, {"version": "v3", "created": "Thu, 16 May 2019 15:53:44 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Shah", "Aashaka", ""], ["Banakar", "Vinay", ""], ["Shastri", "Supreeth", ""], ["Wasserman", "Melissa", ""], ["Chidambaram", "Vijay", ""]]}, {"id": "1903.05008", "submitter": "Raul Castro Fernandez", "authors": "Raul Castro Fernandez, Samuel Madden", "title": "Termite: A System for Tunneling Through Heterogeneous Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data-driven analysis is important in virtually every modern organization.\nYet, most data is underutilized because it remains locked in silos inside of\norganizations; large organizations have thousands of databases, and billions of\nfiles that are not integrated together in a single, queryable repository.\nDespite 40+ years of continuous effort by the database community, data\nintegration still remains an open challenge. In this paper, we advocate a\ndifferent approach: rather than trying to infer a common schema, we aim to find\nanother common representation for diverse, heterogeneous data. Specifically, we\nargue for an embedding (i.e., a vector space) in which all entities, rows,\ncolumns, and paragraphs are represented as points. In the embedding, the\ndistance between points indicates their degree of relatedness. We present\nTermite, a prototype we have built to learn the best embedding from the data.\nBecause the best representation is learned, this allows Termite to avoid much\nof the human effort associated with traditional data integration tasks. On top\nof Termite, we have implemented a Termite-Join operator, which allows people to\nidentify related concepts, even when these are stored in databases with\ndifferent schemas and in unstructured data such as text files, webpages, etc.\nFinally, we show preliminary evaluation results of our prototype via a user\nstudy, and describe a list of future directions we have identified.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 15:51:50 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Fernandez", "Raul Castro", ""], ["Madden", "Samuel", ""]]}, {"id": "1903.05228", "submitter": "Hemant Saxena", "authors": "Hemant Saxena, Lukasz Golab, Ihab F. Ilyas", "title": "Distributed Dependency Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the problem of discovering dependencies from distributed big data.\nExisting (non-distributed) algorithms focus on minimizing computation by\npruning the search space of possible dependencies. However, distributed\nalgorithms must also optimize communication costs, especially in shared-nothing\nsettings, leading to a more complex optimization space. To understand this\nspace, we introduce six primitives shared by existing dependency discovery\nalgorithms, corresponding to data processing steps separated by communication\nbarriers. Through case studies, we show how the primitives allow us to analyze\nthe design space and develop communication-optimized implementations. Finally,\nwe support our analysis with an experimental evaluation on real datasets.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 21:29:08 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Saxena", "Hemant", ""], ["Golab", "Lukasz", ""], ["Ilyas", "Ihab F.", ""]]}, {"id": "1903.05826", "submitter": "Congcong Ge", "authors": "Yunjun Gao, Congcong Ge, Xiaoye Miao, Haobo Wang, Bin Yao, Qing Li", "title": "A Hybrid Data Cleaning Framework using Markov Logic Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increase of dirty data, data cleaning turns into a crux of data\nanalysis. Most of the existing algorithms rely on either qualitative techniques\n(e.g., data rules) or quantitative ones (e.g., statistical methods). In this\npaper, we present a novel hybrid data cleaning framework on top of Markov logic\nnetworks (MLNs), termed as MLNClean, which is capable of cleaning both\nschema-level and instance-level errors. MLNClean mainly consists of two\ncleaning stages, namely, first cleaning multiple data versions separately (each\nof which corresponds to one data rule), and then deriving the final clean data\nbased on multiple data versions. Moreover, we propose a series of\ntechniques/concepts, e.g., the MLN index, the concepts of reliability score and\nfusion score, to facilitate the cleaning process. Extensive experimental\nresults on both real and synthetic datasets demonstrate the superiority of\nMLNClean to the state-of-the-art approach in terms of both accuracy and\nefficiency.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 06:08:46 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Gao", "Yunjun", ""], ["Ge", "Congcong", ""], ["Miao", "Xiaoye", ""], ["Wang", "Haobo", ""], ["Yao", "Bin", ""], ["Li", "Qing", ""]]}, {"id": "1903.05908", "submitter": "Andrea Detti PhD", "authors": "Andrea Detti, Giulio Rossi, Nicola Blefari Melazzi", "title": "Exploiting Information-centric Networking to Federate Spatial Databases", "comments": null, "journal-ref": "IEEE Access, vol. 7, pp. 165248-165261, 2019", "doi": "10.1109/ACCESS.2019.2953043", "report-no": null, "categories": "cs.NI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the methodologies, challenges, and expected advantages\nrelated to the use of the information-centric network (ICN) technology for\nfederating spatial databases. ICN services allow simplifying the design of\nfederation procedures, improving their performance, and providing so-called\ndata-centric security. In this work, we present an architecture that is able to\nfederate spatial databases and evaluate its performance using a real data set\ncoming from OpenStreetMap within a heterogeneous federation formed by MongoDB\nand CouchBase spatial database systems.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 10:41:19 GMT"}, {"version": "v2", "created": "Sat, 23 Nov 2019 19:35:50 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Detti", "Andrea", ""], ["Rossi", "Giulio", ""], ["Melazzi", "Nicola Blefari", ""]]}, {"id": "1903.06453", "submitter": "Guenter Hesse", "authors": "Guenter Hesse, Christoph Matthies, Werner Sinzig, Matthias Uflacker", "title": "Adding Value by Combining Business and Sensor Data: An Industry 4.0 Use\n  Case", "comments": "Accepted at International Conference on Database Systems for Advanced\n  Applications (DASFAA 2019)", "journal-ref": null, "doi": "10.1007/978-3-030-18590-9_80", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Industry 4.0 and the Internet of Things are recent developments that have\nlead to the creation of new kinds of manufacturing data. Linking this new kind\nof sensor data to traditional business information is crucial for enterprises\nto take advantage of the data's full potential. In this paper, we present a\ndemo which allows experiencing this data integration, both vertically between\ntechnical and business contexts and horizontally along the value chain. The\ntool simulates a manufacturing company, continuously producing both business\nand sensor data, and supports issuing ad-hoc queries that answer specific\nquestions related to the business. In order to adapt to different environments,\nusers can configure sensor characteristics to their needs.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 10:46:53 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2019 13:22:59 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Hesse", "Guenter", ""], ["Matthies", "Christoph", ""], ["Sinzig", "Werner", ""], ["Uflacker", "Matthias", ""]]}, {"id": "1903.06565", "submitter": "Ripon Patgiri", "authors": "Ripon Patgiri, Sabuzima Nayak and Samir Kumar Borgohain", "title": "Role of Bloom Filter in Big Data Research: A Survey", "comments": "7 Pages, 3 Figures, 1 Table", "journal-ref": "International Journal of Advanced Computer Science and\n  Applications(IJACSA), Volume 9 Issue 11, 2018", "doi": "10.14569/IJACSA.2018.091193", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big Data is the most popular emerging trends that becomes a blessing for\nhuman kinds and it is the necessity of day-to-day life. For example, Facebook.\nEvery person involves with producing data either directly or indirectly. Thus,\nBig Data is a high volume of data with exponential growth rate that consists of\na variety of data. Big Data touches all fields, including Government sector, IT\nindustry, Business, Economy, Engineering, Bioinformatics, and other basic\nsciences. Thus, Big Data forms a data silo. Most of the data are duplicates and\nunstructured. To deal with such kind of data silo, Bloom Filter is a precious\nresource to filter out the duplicate data. Also, Bloom Filter is inevitable in\na Big Data storage system to optimize the memory consumption. Undoubtedly,\nBloom Filter uses a tiny amount of memory space to filter a very large data\nsize and it stores information of a large set of data. However, functionality\nof the Bloom Filter is limited to membership filter, but it can be adapted in\nvarious applications. Besides, the Bloom Filter is deployed in diverse field,\nand also used in the interdisciplinary research area. Bioinformatics, for\ninstance. In this article, we expose the usefulness of Bloom Filter in Big Data\nresearch.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 14:16:06 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Patgiri", "Ripon", ""], ["Nayak", "Sabuzima", ""], ["Borgohain", "Samir Kumar", ""]]}, {"id": "1903.06570", "submitter": "Ripon Patgiri", "authors": "Ripon Patgiri, Sabuzima Nayak and Samir Kumar Borgohain", "title": "scaleBF: A High Scalable Membership Filter using 3D Bloom Filter", "comments": "6 Pages, 3 Figures, 1 Table", "journal-ref": "International Journal of Advanced Computer Science and\n  Applications(IJACSA), Volume 9 Issue 12, 2018", "doi": "10.14569/IJACSA.2018.091277", "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bloom Filter is extensively deployed data structure in various applications\nand research domain since its inception. Bloom Filter is able to reduce the\nspace consumption in an order of magnitude. Thus, Bloom Filter is used to keep\ninformation of a very large scale data. There are numerous variants of Bloom\nFilters available, however, scalability is a serious dilemma of Bloom Filter\nfor years. To solve this dilemma, there are also diverse variants of Bloom\nFilter. However, the time complexity and space complexity become the key issue\nagain. In this paper, we present a novel Bloom Filter to address the\nscalability issue without compromising the performance, called scaleBF. scaleBF\ndeploys many 3D Bloom Filter to filter the set of items. In this paper, we\ntheoretically compare the contemporary Bloom Filter for scalability and scaleBF\noutperforms in terms of time complexity.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 14:26:59 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Patgiri", "Ripon", ""], ["Nayak", "Sabuzima", ""], ["Borgohain", "Samir Kumar", ""]]}, {"id": "1903.06607", "submitter": "Jimmy Lin", "authors": "Michael Azmy, Peng Shi, Jimmy Lin, and Ihab F. Ilyas", "title": "Matching Entities Across Different Knowledge Graphs with Graph\n  Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the problem of matching entities across different\nknowledge graphs. Given a query entity in one knowledge graph, we wish to find\nthe corresponding real-world entity in another knowledge graph. We formalize\nthis problem and present two large-scale datasets for this task based on\nexiting cross-ontology links between DBpedia and Wikidata, focused on several\nhundred thousand ambiguous entities. Using a classification-based approach, we\nfind that a simple multi-layered perceptron based on representations derived\nfrom RDF2Vec graph embeddings of entities in each knowledge graph is sufficient\nto achieve high accuracy, with only small amounts of training data. The\ncontributions of our work are datasets for examining this problem and strong\nbaselines on which future work can be based.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 15:45:34 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Azmy", "Michael", ""], ["Shi", "Peng", ""], ["Lin", "Jimmy", ""], ["Ilyas", "Ihab F.", ""]]}, {"id": "1903.06619", "submitter": "Seyyedyousef Oleyaeimotlagh", "authors": "Seyyed Yousef Oleyaei-Motlagh and Adan Ernesto Vela", "title": "Inferring demand from partially observed data to address the mismatch\n  between demand and supply of taxis in the presence of rain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing mismatch in supply and demand of taxis is an important effort to\nunderstand passengers' demand. In this paper, we have analyzed the effect of\nrain on the demand for yellow taxis in city-wide as well as in a point of\ninterest in New York City. Because a pickup event is a realized demand, we\nstudied empty travel time, the number of pickups per driver, the average amount\nof income per drive indices to infer demand from taxis data of 2013. Findings\nhighlight that the higher demand exists because of many short-trips during the\nrain. This paper illustrates the change in passengers' demand increased by the\nonset of weather condition.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 16:04:02 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Oleyaei-Motlagh", "Seyyed Yousef", ""], ["Vela", "Adan Ernesto", ""]]}, {"id": "1903.07097", "submitter": "Abdullah Alomar", "authors": "Anish Agarwal, Abdullah Alomar, Devavrat Shah", "title": "tspDB: Time Series Predict DB", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major bottleneck of the current Machine Learning (ML) workflow is the time\nconsuming, error prone engineering required to get data from a datastore or a\ndatabase (DB) to the point an ML algorithm can be applied to it. Hence, we\nexplore the feasibility of directly integrating prediction functionality on top\nof a data store or DB. Such a system ideally: (i) provides an intuitive\nprediction query interface which alleviates the unwieldy data engineering; (ii)\nprovides state-of-the-art statistical accuracy while ensuring incremental model\nupdate, low model training time and low latency for making predictions. As the\nmain contribution we explicitly instantiate a proof-of-concept, tspDB, which\ndirectly integrates with PostgreSQL. We rigorously test tspDB's statistical and\ncomputational performance against the state-of-the-art time series algorithms,\nincluding a Long-Short-Term-Memory (LSTM) neural network and DeepAR (industry\nstandard deep learning library by Amazon). Statistically, on standard time\nseries benchmarks, tspDB outperforms LSTM and DeepAR with 1.1-1.3x higher\nrelative accuracy. Computationally, tspDB is 59-62x and 94-95x faster compared\nto LSTM and DeepAR in terms of median ML model training time and prediction\nquery latency, respectively. Further, compared to PostgreSQL's bulk insert time\nand its SELECT query latency, tspDB is slower only by 1.3x and 2.6x\nrespectively. That is, tspDB is a real-time prediction system in that its model\ntraining / prediction query time is similar to just inserting / reading data\nfrom a DB. As an algorithmic contribution, we introduce an incremental\nmultivariate matrix factorization based time series method, which tspDB is\nbuilt off. We show this method also allows one to produce reliable prediction\nintervals by accurately estimating the time-varying variance of a time series,\nthereby addressing an important problem in time series analysis.\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2019 14:28:50 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2019 00:43:53 GMT"}, {"version": "v3", "created": "Tue, 24 Sep 2019 04:55:58 GMT"}, {"version": "v4", "created": "Wed, 24 Jun 2020 03:12:35 GMT"}, {"version": "v5", "created": "Fri, 10 Jul 2020 20:41:47 GMT"}, {"version": "v6", "created": "Sat, 12 Dec 2020 20:31:12 GMT"}, {"version": "v7", "created": "Sat, 13 Feb 2021 17:58:59 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Agarwal", "Anish", ""], ["Alomar", "Abdullah", ""], ["Shah", "Devavrat", ""]]}, {"id": "1903.07748", "submitter": "Panagiotis Tampakis", "authors": "Panagiotis Tampakis, Christos Doulkeridis, Nikos Pelekis and Yannis\n  Theodoridis", "title": "Distributed Subtrajectory Join on Massive Datasets", "comments": null, "journal-ref": "ACM Transactions on Spatial Algorithms and Systems (TSAS), Volume\n  6, Issue 2, (2020), Article No.: 8", "doi": "10.1145/3373642", "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joining trajectory datasets is a significant operation in mobility data\nanalytics and the cornerstone of various methods that aim to extract knowledge\nout of them. In the era of Big Data, the production of mobility data has become\nmassive and, consequently, performing such an operation in a centralized way is\nnot feasible. In this paper, we address the problem of Distributed\nSubtrajectory Join processing by utilizing the MapReduce programming model.\nCompared to traditional trajectory join queries, this problem is even more\nchallenging since the goal is to retrieve all the \"maximal\" portions of\ntrajectories that are \"similar\". We propose three solutions: (i) a\nwell-designed basic solution, coined DTJb, (ii) a solution that uses a\npreprocessing step that repartitions the data, labeled DTJr, and (iii) a\nsolution that, additionally, employs an indexing scheme, named DTJi. In our\nexperimental study, we utilize a 56GB dataset of real trajectories from the\nmaritime domain, which, to the best of our knowledge, is the largest real\ndataset used for experimentation in the literature of trajectory data\nmanagement. The results show that DTJi performs up to 16x faster compared with\nDTJb, 10x faster than DTJr and 3x faster than the closest related state of the\nart algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 22:39:30 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Tampakis", "Panagiotis", ""], ["Doulkeridis", "Christos", ""], ["Pelekis", "Nikos", ""], ["Theodoridis", "Yannis", ""]]}, {"id": "1903.08132", "submitter": "Vimalkumar Jeyakumar", "authors": "Vimalkumar Jeyakumar, Omid Madani, Ali Parandeh, Ashutosh\n  Kulshreshtha, Weifei Zeng, and Navindra Yadav", "title": "ExplainIt! -- A declarative root-cause analysis engine for time series\n  data (extended version)", "comments": "SIGMOD Industry Track 2019", "journal-ref": null, "doi": "10.1145/3299869.3314048", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ExplainIt!, a declarative, unsupervised root-cause analysis engine\nthat uses time series monitoring data from large complex systems such as data\ncentres. ExplainIt! empowers operators to succinctly specify a large number of\ncausal hypotheses to search for causes of interesting events. ExplainIt! then\nranks these hypotheses, reducing the number of causal dependencies from\nhundreds of thousands to a handful for human understanding. We show how a\ndeclarative language, such as SQL, can be effective in declaratively\nenumerating hypotheses that probe the structure of an unknown probabilistic\ngraphical causal model of the underlying system. Our thesis is that databases\nare in a unique position to enable users to rapidly explore the possible causal\nmechanisms in data collected from diverse sources. We empirically demonstrate\nhow ExplainIt! had helped us resolve over 30 performance issues in a commercial\nproduct since late 2014, of which we discuss a few cases in detail.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 17:42:05 GMT"}, {"version": "v2", "created": "Fri, 22 Mar 2019 17:47:56 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Jeyakumar", "Vimalkumar", ""], ["Madani", "Omid", ""], ["Parandeh", "Ali", ""], ["Kulshreshtha", "Ashutosh", ""], ["Zeng", "Weifei", ""], ["Yadav", "Navindra", ""]]}, {"id": "1903.08334", "submitter": "Sourav Mukherjee", "authors": "Sourav Mukherjee", "title": "Indexes in Microsoft SQL Server", "comments": "16 pages, 3 figures", "journal-ref": null, "doi": "10.6084/m9.figshare.7866176", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indexes are the best apposite choice for quickly retrieving the records. This\nis nothing but cutting down the number of Disk IO. Instead of scanning the\ncomplete table for the results, we can decrease the number of IO's or page\nfetches using index structures such as B-Trees or Hash Indexes to retrieve the\ndata faster. The most convenient way to consider an index is to think like a\ndictionary. It has words and its corresponding definitions against those words.\nThe dictionary will have an index on \"word\" because when we open a dictionary\nand we want to fetch its corresponding word quickly, then find its definition.\nThe dictionary generally contains just a single index - an index ordered by\nword. When we modify any record and change the corresponding value of an\nindexed column in a clustered index, the database might require moving the\nentire row into a separately new position to maintain the rows in the sorted\norder. This action is essentially turned into an update query into a DELETE\nfollowed by an INSERT, and it decreases the performance of the query. The\nclustered index in the table can often be available on the primary key or a\nforeign key column because key values usually do not modify once a record is\ninjected into the database.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 03:55:23 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Mukherjee", "Sourav", ""]]}, {"id": "1903.08431", "submitter": "Alfonso De La Vega", "authors": "Alfonso de la Vega, Diego Garc\\'ia-Saiz, Marta Zorrilla and Pablo\n  S\\'anchez", "title": "How Far are we from Data Mining Democratisation? A Systematic Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context: Data mining techniques have demonstrated to be a powerful technique\nfor discovering insights hidden in data from a domain. However, these\ntechniques demand very specialised skills. People willing to analyse data often\nlack these skills, so they must rely on data scientists, which hinders data\nmining democratisation. Different approaches have appeared in the last years to\naddress this issue.\n  Objective: Analyse the state of the art to know how far are we from an\neffective data mining democratisation, what has already been accomplished, and\nwhat should be done in the upcoming years.\n  Method: We performed a state-of-the-art review following a systematic and\nobjective procedure, which included works both from the academia and the\nindustry. The reviewed works were grouped in four categories. Each category was\nthen evaluated in detail using a well-defined evaluation criteria to identify\nits strengths and weaknesses.\n  Results: Around 700 works were initially considered, from which 43 were\nfinally selected for a more in-depth analysis. Only two out of the four\nidentified categories provide effective solutions to data mining\ndemocratisation. From these two categories, one always requires a minimum\nintervention of a data scientist, whereas the other one does not provide\nsupport for all the stages of the data mining process, and might exhibit\naccuracy problems in some contexts.\n  Conclusion: In all analysed approaches, a data scientist is still required to\nperform some steps of the analysis process. Moreover, automated approaches that\ndo not require data scientists for some steps expose some problems in other\nquality attributes, such as accuracy. Therefore, although existent work shows\nsome promising initial steps, we are still far from data mining\ndemocratisation.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 10:52:53 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["de la Vega", "Alfonso", ""], ["Garc\u00eda-Saiz", "Diego", ""], ["Zorrilla", "Marta", ""], ["S\u00e1nchez", "Pablo", ""]]}, {"id": "1903.08587", "submitter": "Xiangyu Ke", "authors": "Xiangyu Ke, Arijit Khan, Mohammad Al Hasan, Rojin Rezvansangsari", "title": "Reliability Maximization in Uncertain Graphs", "comments": null, "journal-ref": "IEEE Transaction on Knowledge and Data Engineering, 2020", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network reliability measures the probability that a target node is reachable\nfrom a source node in an uncertain graph, i.e., a graph where every edge is\nassociated with a probability of existence. In this paper, we investigate the\nnovel and fundamental problem of adding a small number of edges in the\nuncertain network for maximizing the reliability between a given pair of nodes.\nWe study the NP-hardness and the approximation hardness of our problem, and\ndesign effective, scalable solutions. Furthermore, we consider extended\nversions of our problem (e.g., multiple source and target nodes can be provided\nas input) to support and demonstrate a wider family of queries and\napplications, including sensor network reliability maximization and social\ninfluence maximization. Experimental results validate the effectiveness and\nefficiency of the proposed algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 16:15:24 GMT"}, {"version": "v2", "created": "Thu, 21 Mar 2019 03:31:57 GMT"}, {"version": "v3", "created": "Tue, 3 Dec 2019 09:50:46 GMT"}, {"version": "v4", "created": "Mon, 25 May 2020 12:48:59 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Ke", "Xiangyu", ""], ["Khan", "Arijit", ""], ["Hasan", "Mohammad Al", ""], ["Rezvansangsari", "Rojin", ""]]}, {"id": "1903.08621", "submitter": "Michael Mior", "authors": "Michael J. Mior and Alexander G. Ororbia II", "title": "Column2Vec: Structural Understanding via Distributed Representations of\n  Database Schemas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Column2Vec, a distributed representation of database columns based\non column metadata. Our distributed representation has several applications.\nUsing known names for groups of columns (i.e., a table name), we train a model\nto generate an appropriate name for columns in an unnamed table. We demonstrate\nthe viability of our approach using schema information collected from open\nsource applications on GitHub.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 17:07:11 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Mior", "Michael J.", ""], ["Ororbia", "Alexander G.", "II"]]}, {"id": "1903.09238", "submitter": "Ahmed Metwally", "authors": "Ahmed Metwally and Chun-Heng Huang", "title": "Scalable Similarity Joins of Tokenized Strings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work tackles the problem of fuzzy joining of strings that naturally\ntokenize into meaningful substrings, e.g., full names. Tokenized-string joins\nhave several established applications in the context of data integration and\ncleaning. This work is primarily motivated by fraud detection, where attackers\nslightly modify tokenized strings, e.g., names on accounts, to create numerous\nidentities that she can use to defraud service providers, e.g., Google, and\nLinkedIn. To detect such attacks, all the accounts are pair-wise compared, and\nthe resulting similar accounts are considered suspicious and are further\ninvestigated. Comparing the tokenized-string features of a large number of\naccounts requires an intuitive tokenized-string distance that can detect subtle\nedits introduced by an adversary, and a very scalable algorithm. This is not\nachievable by existing distance measure that are unintuitive, hard to tune, and\nwhose join algorithms are serial and hence unscalable. We define a novel\nintuitive distance measure between tokenized strings, Normalized Setwise\nLevenshtein Distance (NSLD). To the best of our knowledge, NSLD is the first\nmetric proposed for comparing tokenized strings. We propose a scalable\ndistributed framework, Tokenized-String Joiner (TSJ), that adopts existing\nscalable string-join algorithms as building blocks to perform NSLD-joins. We\ncarefully engineer optimizations and approximations that dramatically improve\nthe efficiency of TSJ. The effectiveness of the TSJ framework is evident from\nthe evaluation conducted on tens of millions of tokenized-string names from\nGoogle accounts. The superiority of the tokenized-string-specific TSJ framework\nover the general-purpose metric-spaces joining algorithms has been established.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 21:16:28 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Metwally", "Ahmed", ""], ["Huang", "Chun-Heng", ""]]}, {"id": "1903.09242", "submitter": "Ugo Comignani", "authors": "Angela Bonifati, Ugo Comignani and Efthymia Tsamoura", "title": "Repairing mappings under policy views", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of data exchange involves a source schema, a target schema and a\nset of mappings from transforming the data between the two schemas. We study\nthe problem of data exchange in the presence of privacy restrictions on the\nsource. The privacy restrictions are expressed as a set of policy views\nrepresenting the information that is safe to expose over all instances of the\nsource. We propose a protocol that provides formal privacy guarantees and is\ndata-independent, i.e., if certain criteria are met, then the protocol\nguarantees that the mappings leak no sensitive information independently of the\ndata that lies in the source. We also propose an algorithm for repairing an\ninput mapping w.r.t. a set of policy views, in cases where the input mapping\nleaks sensitive information. The empirical evaluation of our work shows that\nthe proposed algorithm is quite efficient, repairing sets of 300 s-t tgds in an\naverage time of 5s on a commodity machine. To the best of our knowledge, our\nwork is the first one that studies the problems of exchanging data and\nrepairing mappings under such privacy restrictions. Furthermore, our work is\nthe first to provide practical algorithms for a logical privacy-preservation\nparadigm, described as an open research challenge in previous work on this\narea.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 21:28:28 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Bonifati", "Angela", ""], ["Comignani", "Ugo", ""], ["Tsamoura", "Efthymia", ""]]}, {"id": "1903.09246", "submitter": "Xiaolan Wang", "authors": "Xiaolan Wang, Alexandra Meliou", "title": "Explain3D: Explaining Disagreements in Disjoint Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data plays an important role in applications, analytic processes, and many\naspects of human activity. As data grows in size and complexity, we are met\nwith an imperative need for tools that promote understanding and explanations\nover data-related operations. Data management research on explanations has\nfocused on the assumption that data resides in a single dataset, under one\ncommon schema. But the reality of today's data is that it is frequently\nun-integrated, coming from different sources with different schemas. When\ndifferent datasets provide different answers to semantically similar questions,\nunderstanding the reasons for the discrepancies is challenging and cannot be\nhandled by the existing single-dataset solutions.\n  In this paper, we propose Explain3D, a framework for explaining the\ndisagreements across disjoint datasets (3D). Explain3D focuses on identifying\nthe reasons for the differences in the results of two semantically similar\nqueries operating on two datasets with potentially different schemas. Our\nframework leverages the queries to perform a semantic mapping across the\nrelevant parts of their provenance; discrepancies in this mapping point to\ncauses of the queries' differences. Exploiting the queries gives Explain3D an\nedge over traditional schema matching and record linkage techniques, which are\nquery-agnostic. Our work makes the following contributions: (1) We formalize\nthe problem of deriving optimal explanations for the differences of the results\nof semantically similar queries over disjoint datasets. (2) We design a 3-stage\nframework for solving the optimal explanation problem. (3) We develop a\nsmart-partitioning optimizer that improves the efficiency of the framework by\norders of magnitude. (4)~We experiment with real-world and synthetic data to\ndemonstrate that Explain3D can derive precise explanations efficiently.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 21:43:40 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Wang", "Xiaolan", ""], ["Meliou", "Alexandra", ""]]}, {"id": "1903.09270", "submitter": "Marcos Martinez-Romero PhD", "authors": "Marcos Mart\\'inez-Romero, Martin J. O'Connor, Attila L. Egyedi, Debra\n  Willrett, Josef Hardi, John Graybeal, Mark A. Musen", "title": "Using association rule mining and ontologies to generate metadata\n  recommendations from multiple biomedical databases", "comments": null, "journal-ref": null, "doi": "10.1093/database/baz059", "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metadata-the machine-readable descriptions of the data-are increasingly seen\nas crucial for describing the vast array of biomedical datasets that are\ncurrently being deposited in public repositories. While most public\nrepositories have firm requirements that metadata must accompany submitted\ndatasets, the quality of those metadata is generally very poor. A key problem\nis that the typical metadata acquisition process is onerous and time consuming,\nwith little interactive guidance or assistance provided to users. Secondary\nproblems include the lack of validation and sparse use of standardized terms or\nontologies when authoring metadata. There is a pressing need for improvements\nto the metadata acquisition process that will help users to enter metadata\nquickly and accurately. In this paper we outline a recommendation system for\nmetadata that aims to address this challenge. Our approach uses association\nrule mining to uncover hidden associations among metadata values and to\nrepresent them in the form of association rules. These rules are then used to\npresent users with real-time recommendations when authoring metadata. The\nnovelties of our method are that it is able to combine analyses of metadata\nfrom multiple repositories when generating recommendations and can enhance\nthose recommendations by aligning them with ontology terms. We implemented our\napproach as a service integrated into the CEDAR Workbench metadata authoring\nplatform, and evaluated it using metadata from two public biomedical\nrepositories: US-based National Center for Biotechnology Information (NCBI)\nBioSample and European Bioinformatics Institute (EBI) BioSamples. The results\nshow that our approach is able to use analyses of previous entered metadata\ncoupled with ontology-based mappings to present users with accurate\nrecommendations when authoring metadata.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 23:50:21 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Mart\u00ednez-Romero", "Marcos", ""], ["O'Connor", "Martin J.", ""], ["Egyedi", "Attila L.", ""], ["Willrett", "Debra", ""], ["Hardi", "Josef", ""], ["Graybeal", "John", ""], ["Musen", "Mark A.", ""]]}, {"id": "1903.09717", "submitter": "Xiao Hu", "authors": "Xiao Hu and Ke Yi", "title": "Instance and Output Optimal Parallel Algorithms for Acyclic Joins", "comments": null, "journal-ref": null, "doi": "10.1145/3294052.3319698", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massively parallel join algorithms have received much attention in recent\nyears, while most prior work has focused on worst-optimal algorithms. However,\nthe worst-case optimality of these join algorithms relies on hard instances\nhaving very large output sizes, which rarely appear in practice. A stronger\nnotion of optimality is {\\em output-optimal}, which requires an algorithm to be\noptimal within the class of all instances sharing the same input and output\nsize. An even stronger optimality is {\\em instance-optimal}, i.e., the\nalgorithm is optimal on every single instance, but this may not always be\nachievable.\n  In the traditional RAM model of computation, the classical Yannakakis\nalgorithm is instance-optimal on any acyclic join. But in the massively\nparallel computation (MPC) model, the situation becomes much more complicated.\nWe first show that for the class of r-hierarchical joins, instance-optimality\ncan still be achieved in the MPC model. Then, we give a new MPC algorithm for\nan arbitrary acyclic join with load $O ({\\IN \\over p} + {\\sqrt{\\IN \\cdot \\OUT}\n\\over p})$, where $\\IN,\\OUT$ are the input and output sizes of the join, and\n$p$ is the number of servers in the MPC model. This improves the MPC version of\nthe Yannakakis algorithm by an $O (\\sqrt{\\OUT \\over \\IN} )$ factor.\nFurthermore, we show that this is output-optimal when $\\OUT = O(p \\cdot \\IN)$,\nfor every acyclic but non-r-hierarchical join. Finally, we give the first\noutput-sensitive lower bound for the triangle join in the MPC model, showing\nthat it is inherently more difficult than acyclic joins.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 21:58:37 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2019 18:51:24 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Hu", "Xiao", ""], ["Yi", "Ke", ""]]}, {"id": "1903.09999", "submitter": "Saravanan Thirumuruganathan", "authors": "Shohedul Hasan, Saravanan Thirumuruganathan, Jees Augustine, Nick\n  Koudas, Gautam Das", "title": "Multi-Attribute Selectivity Estimation Using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selectivity estimation - the problem of estimating the result size of queries\n- is a fundamental problem in databases. Accurate estimation of query\nselectivity involving multiple correlated attributes is especially challenging.\nPoor cardinality estimates could result in the selection of bad plans by the\nquery optimizer. We investigate the feasibility of using deep learning based\napproaches for both point and range queries and propose two complementary\napproaches. Our first approach considers selectivity as an unsupervised deep\ndensity estimation problem. We successfully introduce techniques from neural\ndensity estimation for this purpose. The key idea is to decompose the joint\ndistribution into a set of tractable conditional probability distributions such\nthat they satisfy the autoregressive property. Our second approach formulates\nselectivity estimation as a supervised deep learning problem that predicts the\nselectivity of a given query. We also introduce and address a number of\npractical challenges arising when adapting deep learning for relational data.\nThese include query/data featurization, incorporating query workload\ninformation in a deep learning framework and the dynamic scenario where both\ndata and workload queries could be updated. Our extensive experiments with a\nspecial emphasis on queries with a large number of predicates and/or small\nresult sizes demonstrates that our proposed techniques provide fast and\naccurate selective estimates with minimal space overhead.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2019 15:21:04 GMT"}, {"version": "v2", "created": "Mon, 17 Jun 2019 20:43:56 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Hasan", "Shohedul", ""], ["Thirumuruganathan", "Saravanan", ""], ["Augustine", "Jees", ""], ["Koudas", "Nick", ""], ["Das", "Gautam", ""]]}, {"id": "1903.10000", "submitter": "Saravanan Thirumuruganathan", "authors": "Saravanan Thirumuruganathan, Shohedul Hasan, Nick Koudas, Gautam Das", "title": "Approximate Query Processing using Deep Generative Models", "comments": "Accepted to ICDE 2020 as \"Approximate Query Processing for Data\n  Exploration using Deep Generative Models\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data is generated at an unprecedented rate surpassing our ability to analyze\nthem. The database community has pioneered many novel techniques for\nApproximate Query Processing (AQP) that could give approximate results in a\nfraction of time needed for computing exact results. In this work, we explore\nthe usage of deep learning (DL) for answering aggregate queries specifically\nfor interactive applications such as data exploration and visualization. We use\ndeep generative models, an unsupervised learning based approach, to learn the\ndata distribution faithfully such that aggregate queries could be answered\napproximately by generating samples from the learned model. The model is often\ncompact - few hundred KBs - so that arbitrary AQP queries could be answered on\nthe client side without contacting the database server. Our other contributions\ninclude identifying model bias and minimizing it through a rejection sampling\nbased approach and an algorithm to build model ensembles for AQP for improved\naccuracy. Our extensive experiments show that our proposed approach can provide\nanswers with high accuracy and low latency.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2019 15:21:19 GMT"}, {"version": "v2", "created": "Mon, 17 Jun 2019 20:38:25 GMT"}, {"version": "v3", "created": "Mon, 18 Nov 2019 20:05:13 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Thirumuruganathan", "Saravanan", ""], ["Hasan", "Shohedul", ""], ["Koudas", "Nick", ""], ["Das", "Gautam", ""]]}, {"id": "1903.10269", "submitter": "S{\\o}ren Kejser Jensen", "authors": "S{\\o}ren Kejser Jensen, Torben Bach Pedersen, Christian Thomsen", "title": "Scalable Model-Based Management of Correlated Dimensional Time Series in\n  ModelarDB+", "comments": "12 Pages, 28 Figures, and 1 Table", "journal-ref": null, "doi": "10.1109/ICDE51399.2021.00123", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To monitor critical infrastructure, high quality sensors sampled at a high\nfrequency are increasingly used. However, as they produce huge amounts of data,\nonly simple aggregates are stored. This removes outliers and fluctuations that\ncould indicate problems. As a remedy, we present a model-based approach for\nmanaging time series with dimensions that exploits correlation in and among\ntime series. Specifically, we propose compressing groups of correlated time\nseries using an extensible set of model types within a user-defined error bound\n(possibly zero). We name this new category of model-based compression methods\nfor time series Multi-Model Group Compression (MMGC). We present the first MMGC\nmethod GOLEMM and extend model types to compress time series groups. We propose\nprimitives for users to effectively define groups for differently sized data\nsets, and based on these, an automated grouping method using only the time\nseries dimensions. We propose algorithms for executing simple and\nmulti-dimensional aggregate queries on models. Last, we implement our methods\nin the Time Series Management System (TSMS) ModelarDB (ModelarDB+). Our\nevaluation shows that compared to widely used formats, ModelarDB+ provides up\nto 13.7 times faster ingestion due to high compression, 113 times better\ncompression due to the adaptivity of GOLEMM, 630 times faster aggregates by\nusing models, and close to linear scalability. It is also extensible and\nsupports online query processing.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 12:31:40 GMT"}, {"version": "v2", "created": "Sat, 5 Dec 2020 13:12:44 GMT"}, {"version": "v3", "created": "Tue, 29 Jun 2021 14:51:10 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Jensen", "S\u00f8ren Kejser", ""], ["Pedersen", "Torben Bach", ""], ["Thomsen", "Christian", ""]]}, {"id": "1903.10579", "submitter": "Kristopher Brown", "authors": "Kristopher Brown, David I. Spivak, Ryan Wisnesky", "title": "Categorical Data Integration for Computational Science", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Categorical Query Language is an open-source query and data integration\nscripting language that can be applied to common challenges in the field of\ncomputational science. We discuss how the structure-preserving nature of CQL\ndata migrations protect those who publicly share data from the\nmisinterpretation of their data. Likewise, this feature of CQL migrations\nallows those who draw from public data sources to be sure only data which meets\ntheir specification will actually be transferred. We argue some open problems\nin the field of data sharing in computational science are addressable by\nworking within this paradigm of functorial data migration. We demonstrate these\ntools by integrating data from the Open Quantum Materials Database with some\nalternative materials databases.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 20:08:22 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Brown", "Kristopher", ""], ["Spivak", "David I.", ""], ["Wisnesky", "Ryan", ""]]}, {"id": "1903.10706", "submitter": "Miika Hannula", "authors": "Miika Hannula and Lauri Hella", "title": "Complexity Thresholds in Inclusion Logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logics with team semantics provide alternative means for logical\ncharacterization of complexity classes. Both dependence and independence logic\nare known to capture non-deterministic polynomial time, and the frontiers of\ntractability in these logics are relatively well understood. Inclusion logic is\nsimilar to these team-based logical formalisms with the exception that it\ncorresponds to deterministic polynomial time in ordered models. In this article\nwe examine connections between syntactical fragments of inclusion logic and\ndifferent complexity classes in terms of two computational problems: maximal\nsubteam membership and the model checking problem for a fixed inclusion logic\nformula. We show that very simple quantifier-free formulae with one or two\ninclusion atoms generate instances of these problems that are complete for\n(non-deterministic) logarithmic space and polynomial time. Furthermore, we\npresent a fragment of inclusion logic that captures non-deterministic\nlogarithmic space in ordered models.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 06:52:34 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Hannula", "Miika", ""], ["Hella", "Lauri", ""]]}, {"id": "1903.10970", "submitter": "Jes\\'us Camacho-Rodr\\'iguez", "authors": "Jes\\'us Camacho-Rodr\\'iguez, Ashutosh Chauhan, Alan Gates, Eugene\n  Koifman, Owen O'Malley, Vineet Garg, Zoltan Haindrich, Sergey Shelukhin,\n  Prasanth Jayachandran, Siddharth Seth, Deepak Jaiswal, Slim Bouguerra,\n  Nishant Bangarwa, Sankar Hariappan, Anishek Agarwal, Jason Dere, Daniel Dai,\n  Thejas Nair, Nita Dembla, Gopal Vijayaraghavan, G\\\"unther Hagleitner", "title": "Apache Hive: From MapReduce to Enterprise-grade Big Data Warehousing", "comments": "SIGMOD'19, 14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Apache Hive is an open-source relational database system for analytic\nbig-data workloads. In this paper we describe the key innovations on the\njourney from batch tool to fully fledged enterprise data warehousing system. We\npresent a hybrid architecture that combines traditional MPP techniques with\nmore recent big data and cloud concepts to achieve the scale and performance\nrequired by today's analytic applications. We explore the system by detailing\nenhancements along four main axis: Transactions, optimizer, runtime, and\nfederation. We then provide experimental results to demonstrate the performance\nof the system for typical workloads and conclude with a look at the community\nroadmap.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 15:53:49 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Camacho-Rodr\u00edguez", "Jes\u00fas", ""], ["Chauhan", "Ashutosh", ""], ["Gates", "Alan", ""], ["Koifman", "Eugene", ""], ["O'Malley", "Owen", ""], ["Garg", "Vineet", ""], ["Haindrich", "Zoltan", ""], ["Shelukhin", "Sergey", ""], ["Jayachandran", "Prasanth", ""], ["Seth", "Siddharth", ""], ["Jaiswal", "Deepak", ""], ["Bouguerra", "Slim", ""], ["Bangarwa", "Nishant", ""], ["Hariappan", "Sankar", ""], ["Agarwal", "Anishek", ""], ["Dere", "Jason", ""], ["Dai", "Daniel", ""], ["Nair", "Thejas", ""], ["Dembla", "Nita", ""], ["Vijayaraghavan", "Gopal", ""], ["Hagleitner", "G\u00fcnther", ""]]}, {"id": "1903.11203", "submitter": "Yingjun Wu", "authors": "Yingjun Wu, Jia Yu, Yuanyuan Tian, Richard Sidle, Ronald Barber", "title": "Designing Succinct Secondary Indexing Mechanism by Exploiting Column\n  Correlations (Extended Version)", "comments": "To appear in SIGMOD 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Database administrators construct secondary indexes on data tables to\naccelerate query processing in relational database management systems (RDBMSs).\nThese indexes are built on top of the most frequently queried columns according\nto the data statistics. Unfortunately, maintaining multiple secondary indexes\nin the same database can be extremely space consuming, causing significant\nperformance degradation due to the potential exhaustion of memory space. In\nthis paper, we demonstrate that there exist many opportunities to exploit\ncolumn correlations for accelerating data access. We propose HERMIT, a succinct\nsecondary indexing mechanism for modern RDBMSs. HERMIT judiciously leverages\nthe rich soft functional dependencies hidden among columns to prune out\nredundant structures for indexed key access. Instead of building a complete\nindex that stores every single entry in the key columns, HERMIT navigates any\nincoming key access queries to an existing index built on the correlated\ncolumns. This is achieved through the Tiered Regression Search Tree (TRS-Tree),\na succinct, ML-enhanced data structure that performs fast curve fitting to\nadaptively and dynamically capture both column correlations and outliers. Our\nextensive experimental study in two different RDBMSs have confirmed that HERMIT\ncan significantly reduce space consumption with limited performance overhead in\nterms of query response time and index maintenance time, especially when\nsupporting complex range queries.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 00:44:43 GMT"}, {"version": "v2", "created": "Mon, 1 Apr 2019 22:33:56 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Wu", "Yingjun", ""], ["Yu", "Jia", ""], ["Tian", "Yuanyuan", ""], ["Sidle", "Richard", ""], ["Barber", "Ronald", ""]]}, {"id": "1903.11749", "submitter": "Wenqing Lin", "authors": "Wenqing Lin", "title": "Distributed Algorithms for Fully Personalized PageRank on Large Graphs", "comments": "Full Research Paper accepted in the proceedings of the 30th World\n  Wide Web Conference (WWW 2019)", "journal-ref": null, "doi": "10.1145/3308558.3313555", "report-no": null, "categories": "cs.SI cs.DB cs.DC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Personalized PageRank (PPR) has enormous applications, such as link\nprediction and recommendation systems for social networks, which often require\nthe fully PPR to be known. Besides, most of real-life graphs are edge-weighted,\ne.g., the interaction between users on the Facebook network. However, it is\ncomputationally difficult to compute the fully PPR, especially on large graphs,\nnot to mention that most existing approaches do not consider the weights of\nedges. In particular, the existing approach cannot handle graphs with billion\nedges on a moderate-size cluster. To address this problem, this paper presents\na novel study on the computation of fully edge-weighted PPR on large graphs\nusing the distributed computing framework. Specifically, we employ the Monte\nCarlo approximation that performs a large number of random walks from each node\nof the graph, and exploits the parallel pipeline framework to reduce the\noverall running time of the fully PPR. Based on that, we develop several\noptimization techniques which (i) alleviate the issue of large nodes that could\nexplode the memory space, (ii) pre-compute short walks for small nodes that\nlargely speedup the computation of random walks, and (iii) optimize the amount\nof random walks to compute in each pipeline that significantly reduces the\noverhead. With extensive experiments on a variety of real-life graph datasets,\nwe demonstrate that our solution is several orders of magnitude faster than the\nstate-of-the-arts, and meanwhile, largely outperforms the baseline algorithms\nin terms of accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 01:14:37 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Lin", "Wenqing", ""]]}, {"id": "1903.12469", "submitter": "Jef Wijsen", "authors": "Jef Wijsen", "title": "Corrigendum to \"Counting Database Repairs that Satisfy Conjunctive\n  Queries with Self-Joins\"", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The helping Lemma 7 in [Maslowski and Wijsen, ICDT, 2014] is false. The lemma\nis used in (and only in) the proof of Theorem 3 of that same paper. In this\ncorrigendum, we provide a new proof for the latter theorem.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 12:28:28 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Wijsen", "Jef", ""]]}, {"id": "1903.12554", "submitter": "Valentina Presutti", "authors": "Tayeb Abderrahmani Ghor, Esha Agrawal, Mehwish Alam, Omar Alqawasmeh,\n  Claudia D'amato, Amina Annane, Amr Azzam, Andrew Berezovskyi, Russa Biswas,\n  Mathias Bonduel, Quentin Brabant, Cristina-iulia Bucur, Elena Camossi,\n  Valentina Anita Carriero, Shruthi Chari, David Chaves Fraga, Fiorela Ciroku,\n  Michael Cochez, Hubert Curien, Vincenzo Cutrona, Rahma Dandan, Danilo Dess,\n  Valerio Di Carlo, Ahmed El Amine Djebri, Marieke Van Erp, Faiq Miftakhul\n  Falakh, Alba Fernndez Izquierdo, Giuseppe Futia, Aldo Gangemi, Simone\n  Gasperoni, Arnaud Grall, Lars Heling, Pierre Henri, Noura Herradi, Subhi\n  Issa, Samaneh Jozashoori, Nyoman Juniarta, Lucie-aime Kaffee, Ilkcan Keles,\n  Prashant Khare, Viktor Kovtun, Valentina Leone, Siying Li, Sven Lieber,\n  Pasquale Lisena, Tatiana Makhalova, Ludovica Marinucci, Thomas Minier,\n  Benjamin Moreau, Alberto Moya Loustaunau, Durgesh Nandini, Sylwia Ozdowska,\n  Amanda Pacini De Moura, Swati Padhee, Guillermo Palma, Pedro Del Pozo Jimnez,\n  Valentina Presutti, Roberto Reda, Ettore Rizza, Henry Rosales-mndez,\n  Sebastian Rudolph, Harald Sack, Luca Sciullo, Humasak Simanjuntak, Carlo\n  Stomeo, Thiviyan Thanapalasingam, Tabea Tietz, Dalia Varanka, Maria-esther\n  Vidal, Michael Wolowyk, Maximilian Zocholl", "title": "Linked Open Data Validity -- A Technical Report from ISWS 2018", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linked Open Data (LOD) is the publicly available RDF data in the Web. Each\nLOD entity is identfied by a URI and accessible via HTTP. LOD encodes\nglobalscale knowledge potentially available to any human as well as artificial\nintelligence that may want to benefit from it as background knowledge for\nsupporting their tasks. LOD has emerged as the backbone of applications in\ndiverse fields such as Natural Language Processing, Information Retrieval,\nComputer Vision, Speech Recognition, and many more. Nevertheless, regardless of\nthe specific tasks that LOD-based tools aim to address, the reuse of such\nknowledge may be challenging for diverse reasons, e.g. semantic heterogeneity,\nprovenance, and data quality. As aptly stated by Heath et al. Linked Data might\nbe outdated, imprecise, or simply wrong\": there arouses a necessity to\ninvestigate the problem of linked data validity. This work reports a\ncollaborative effort performed by nine teams of students, guided by an equal\nnumber of senior researchers, attending the International Semantic Web Research\nSchool (ISWS 2018) towards addressing such investigation from different\nperspectives coupled with different approaches to tackle the issue.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 09:20:05 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Ghor", "Tayeb Abderrahmani", ""], ["Agrawal", "Esha", ""], ["Alam", "Mehwish", ""], ["Alqawasmeh", "Omar", ""], ["D'amato", "Claudia", ""], ["Annane", "Amina", ""], ["Azzam", "Amr", ""], ["Berezovskyi", "Andrew", ""], ["Biswas", "Russa", ""], ["Bonduel", "Mathias", ""], ["Brabant", "Quentin", ""], ["Bucur", "Cristina-iulia", ""], ["Camossi", "Elena", ""], ["Carriero", "Valentina Anita", ""], ["Chari", "Shruthi", ""], ["Fraga", "David Chaves", ""], ["Ciroku", "Fiorela", ""], ["Cochez", "Michael", ""], ["Curien", "Hubert", ""], ["Cutrona", "Vincenzo", ""], ["Dandan", "Rahma", ""], ["Dess", "Danilo", ""], ["Di Carlo", "Valerio", ""], ["Djebri", "Ahmed El Amine", ""], ["Van Erp", "Marieke", ""], ["Falakh", "Faiq Miftakhul", ""], ["Izquierdo", "Alba Fernndez", ""], ["Futia", "Giuseppe", ""], ["Gangemi", "Aldo", ""], ["Gasperoni", "Simone", ""], ["Grall", "Arnaud", ""], ["Heling", "Lars", ""], ["Henri", "Pierre", ""], ["Herradi", "Noura", ""], ["Issa", "Subhi", ""], ["Jozashoori", "Samaneh", ""], ["Juniarta", "Nyoman", ""], ["Kaffee", "Lucie-aime", ""], ["Keles", "Ilkcan", ""], ["Khare", "Prashant", ""], ["Kovtun", "Viktor", ""], ["Leone", "Valentina", ""], ["Li", "Siying", ""], ["Lieber", "Sven", ""], ["Lisena", "Pasquale", ""], ["Makhalova", "Tatiana", ""], ["Marinucci", "Ludovica", ""], ["Minier", "Thomas", ""], ["Moreau", "Benjamin", ""], ["Loustaunau", "Alberto Moya", ""], ["Nandini", "Durgesh", ""], ["Ozdowska", "Sylwia", ""], ["De Moura", "Amanda Pacini", ""], ["Padhee", "Swati", ""], ["Palma", "Guillermo", ""], ["Jimnez", "Pedro Del Pozo", ""], ["Presutti", "Valentina", ""], ["Reda", "Roberto", ""], ["Rizza", "Ettore", ""], ["Rosales-mndez", "Henry", ""], ["Rudolph", "Sebastian", ""], ["Sack", "Harald", ""], ["Sciullo", "Luca", ""], ["Simanjuntak", "Humasak", ""], ["Stomeo", "Carlo", ""], ["Thanapalasingam", "Thiviyan", ""], ["Tietz", "Tabea", ""], ["Varanka", "Dalia", ""], ["Vidal", "Maria-esther", ""], ["Wolowyk", "Michael", ""], ["Zocholl", "Maximilian", ""]]}]