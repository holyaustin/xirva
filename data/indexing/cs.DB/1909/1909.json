[{"id": "1909.00084", "submitter": "Subru Krishnan", "authors": "Ashvin Agrawal, Rony Chatterjee, Carlo Curino, Avrilia Floratou, Neha\n  Gowdal, Matteo Interlandi, Alekh Jindal, Kostantinos Karanasos, Subru\n  Krishnan, Brian Kroth, Jyoti Leeka, Kwanghyun Park, Hiren Patel, Olga Poppe,\n  Fotis Psallidas, Raghu Ramakrishnan, Abhishek Roy, Karla Saur, Rathijit Sen,\n  Markus Weimer, Travis Wright, Yiwen Zhu", "title": "Cloudy with high chance of DBMS: A 10-year prediction for\n  Enterprise-Grade ML", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Machine learning (ML) has proven itself in high-value web applications such\nas search ranking and is emerging as a powerful tool in a much broader range of\nenterprise scenarios including voice recognition and conversational\nunderstanding for customer support, autotuning for videoconferencing,\nintelligent feedback loops in large-scale sysops, manufacturing and autonomous\nvehicle management, complex financial predictions, just to name a few.\nMeanwhile, as the value of data is increasingly recognized and monetized,\nconcerns about securing valuable data and risks to individual privacy have been\ngrowing. Consequently, rigorous data management has emerged as a key\nrequirement in enterprise settings. How will these trends (ML growing\npopularity, and stricter data governance) intersect? What are the unmet\nrequirements for applying ML in enterprise settings? What are the technical\nchallenges for the DB community to solve? In this paper, we present our vision\nof how ML and database systems are likely to come together, and early steps we\ntake towards making this vision a reality.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 22:37:15 GMT"}, {"version": "v2", "created": "Fri, 27 Dec 2019 23:48:38 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Agrawal", "Ashvin", ""], ["Chatterjee", "Rony", ""], ["Curino", "Carlo", ""], ["Floratou", "Avrilia", ""], ["Gowdal", "Neha", ""], ["Interlandi", "Matteo", ""], ["Jindal", "Alekh", ""], ["Karanasos", "Kostantinos", ""], ["Krishnan", "Subru", ""], ["Kroth", "Brian", ""], ["Leeka", "Jyoti", ""], ["Park", "Kwanghyun", ""], ["Patel", "Hiren", ""], ["Poppe", "Olga", ""], ["Psallidas", "Fotis", ""], ["Ramakrishnan", "Raghu", ""], ["Roy", "Abhishek", ""], ["Saur", "Karla", ""], ["Sen", "Rathijit", ""], ["Weimer", "Markus", ""], ["Wright", "Travis", ""], ["Zhu", "Yiwen", ""]]}, {"id": "1909.00271", "submitter": "Maria Luiza Mondelli", "authors": "Maria Luiza Mondelli, A. Townsend Peterson and Luiz M. R. Gadelha Jr", "title": "Exploring Reproducibility and FAIR Principles in Data Science Using\n  Ecological Niche Modeling as a Case Study", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reproducibility is a fundamental requirement of the scientific process since\nit enables outcomes to be replicated and verified. Computational scientific\nexperiments can benefit from improved reproducibility for many reasons,\nincluding validation of results and reuse by other scientists. However,\ndesigning reproducible experiments remains a challenge and hence the need for\ndeveloping methodologies and tools that can support this process. Here, we\npropose a conceptual model for reproducibility to specify its main attributes\nand properties, along with a framework that allows for computational\nexperiments to be findable, accessible, interoperable, and reusable. We present\na case study in ecological niche modeling to demonstrate and evaluate the\nimplementation of this framework.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 19:43:05 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Mondelli", "Maria Luiza", ""], ["Peterson", "A. Townsend", ""], ["Gadelha", "Luiz M. R.", "Jr"]]}, {"id": "1909.00607", "submitter": "Benjamin Hilprecht", "authors": "Benjamin Hilprecht, Andreas Schmidt, Moritz Kulessa, Alejandro Molina,\n  Kristian Kersting and Carsten Binnig", "title": "DeepDB: Learn from Data, not from Queries!", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The typical approach for learned DBMS components is to capture the behavior\nby running a representative set of queries and use the observations to train a\nmachine learning model. This workload-driven approach, however, has two major\ndownsides. First, collecting the training data can be very expensive, since all\nqueries need to be executed on potentially large databases. Second, training\ndata has to be recollected when the workload and the data changes. To overcome\nthese limitations, we take a different route: we propose to learn a pure\ndata-driven model that can be used for different tasks such as query answering\nor cardinality estimation. This data-driven model also supports ad-hoc queries\nand updates of the data without the need of full retraining when the workload\nor data changes. Indeed, one may now expect that this comes at a price of lower\naccuracy since workload-driven models can make use of more information.\nHowever, this is not the case. The results of our empirical evaluation\ndemonstrate that our data-driven approach not only provides better accuracy\nthan state-of-the-art learned components but also generalizes better to unseen\nqueries.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 08:59:17 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Hilprecht", "Benjamin", ""], ["Schmidt", "Andreas", ""], ["Kulessa", "Moritz", ""], ["Molina", "Alejandro", ""], ["Kersting", "Kristian", ""], ["Binnig", "Carsten", ""]]}, {"id": "1909.00743", "submitter": "Marcos Didonet Del Fabro", "authors": "Fabiola Santore, Lucas F. Oliveira, Rafael de Paulo Dias, Henrique V.\n  Ehrenfried, Alessandro Elias, Diego Pasqualin, Luis C. E. de Bona, Marcos\n  Didonet Del Fabro, Marcos Sunye", "title": "Blended Integrated Open Data: dados abertos p\\'ublicos integrados", "comments": "8 pages. in Portuguese", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While several public institutions provide its data openly, the effort\nrequired to access, integrate and query this data is too high, reducing the\namount of possible dataset users. The Blended Integrated Open Data (BIOD)\nproject has as objective to ease the access to public Open Data. It integrates\nand makes available more than 300Gb of data, containing billions of records\nfrom different Open Data Sets, allowing to query over them, and thus to\nretrieve related information from originally disconnected data sets. This paper\npresents the set of open data available, how to access it and how produce new\ncompatible data to improve the existing data set.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 14:45:06 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 12:19:55 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Santore", "Fabiola", ""], ["Oliveira", "Lucas F.", ""], ["Dias", "Rafael de Paulo", ""], ["Ehrenfried", "Henrique V.", ""], ["Elias", "Alessandro", ""], ["Pasqualin", "Diego", ""], ["de Bona", "Luis C. E.", ""], ["Del Fabro", "Marcos Didonet", ""], ["Sunye", "Marcos", ""]]}, {"id": "1909.00841", "submitter": "Mengwei Xu", "authors": "Mengwei Xu, Xiwen Zhang, Yunxin Liu, Gang Huang, Xuanzhe Liu, Felix\n  Xiaozhu Lin", "title": "Approximate Query Service on Autonomous IoT Cameras", "comments": null, "journal-ref": null, "doi": "10.1145/3386901.3388948", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Elf is a runtime for an energy-constrained camera to continuously summarize\nvideo scenes as approximate object counts. Elf's novelty centers on planning\nthe camera's count actions under energy constraint. (1) Elf explores the rich\naction space spanned by the number of sample image frames and the choice of\nper-frame object counters; it unifies errors from both sources into one single\nbounded error. (2) To decide count actions at run time, Elf employs a\nlearning-based planner, jointly optimizing for past and future videos without\ndelaying result materialization. Tested with more than 1,000 hours of videos\nand under realistic energy constraints, Elf continuously generates object\ncounts within only 11% of the true counts on average. Alongside the counts, Elf\npresents narrow errors shown to be bounded and up to 3.4x smaller than\ncompetitive baselines. At a higher level, Elf makes a case for advancing the\ngeographic frontier of video analytics.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 19:45:55 GMT"}, {"version": "v2", "created": "Tue, 24 Dec 2019 03:16:03 GMT"}, {"version": "v3", "created": "Tue, 17 Mar 2020 09:49:00 GMT"}, {"version": "v4", "created": "Tue, 5 May 2020 15:25:04 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Xu", "Mengwei", ""], ["Zhang", "Xiwen", ""], ["Liu", "Yunxin", ""], ["Huang", "Gang", ""], ["Liu", "Xuanzhe", ""], ["Lin", "Felix Xiaozhu", ""]]}, {"id": "1909.00845", "submitter": "Shaleen Deep", "authors": "Shuchi Chawla, Shaleen Deep, Paraschos Koutris, Yifeng Teng", "title": "Revenue Maximization for Query Pricing", "comments": "To appear in PVLDB; version 2 with some cosmetic changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Buying and selling of data online has increased substantially over the last\nfew years. Several frameworks have already been proposed that study query\npricing in theory and practice. The key guiding principle in these works is the\nnotion of {\\em arbitrage-freeness} where the broker can set different prices\nfor different queries made to the dataset, but must ensure that the pricing\nfunction does not provide the buyers with opportunities for arbitrage. However,\nlittle is known about revenue maximization aspect of query pricing. In this\npaper, we study the problem faced by a broker selling access to data with the\ngoal of maximizing her revenue. We show that this problem can be formulated as\na revenue maximization problem with single-minded buyers and unlimited supply,\nfor which several approximation algorithms are known. We perform an extensive\nempirical evaluation of the performance of several pricing algorithms for the\nquery pricing problem on real-world instances. In addition to previously known\napproximation algorithms, we propose several new heuristics and analyze them\nboth theoretically and experimentally. Our experiments show that algorithms\nwith the best theoretical bounds are not necessarily the best empirically. We\nidentify algorithms and heuristics that are both fast and also provide\nconsistently good performance when valuations are drawn from a variety of\ndistributions.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 19:53:25 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 16:48:44 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Chawla", "Shuchi", ""], ["Deep", "Shaleen", ""], ["Koutris", "Paraschos", ""], ["Teng", "Yifeng", ""]]}, {"id": "1909.00882", "submitter": "Kien Nguyen", "authors": "Hien To, Kien Nguyen, Cyrus Shahabi", "title": "Differentially Private Publication of Location Entropy", "comments": "SIGSPATIAL'16, 10 pages", "journal-ref": "Proceedings of the 24th ACM SIGSPATIAL International Conference on\n  Advances in Geographic Information Systems, 2019", "doi": "10.1145/2996913.2996985", "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Location entropy (LE) is a popular metric for measuring the popularity of\nvarious locations (e.g., points-of-interest). Unlike other metrics computed\nfrom only the number of (unique) visits to a location, namely frequency, LE\nalso captures the diversity of the users' visits, and is thus more accurate\nthan other metrics. Current solutions for computing LE require full access to\nthe past visits of users to locations, which poses privacy threats. This paper\ndiscusses, for the first time, the problem of perturbing location entropy for a\nset of locations according to differential privacy. The problem is challenging\nbecause removing a single user from the dataset will impact multiple records of\nthe database; i.e., all the visits made by that user to various locations.\nTowards this end, we first derive non-trivial, tight bounds for both local and\nglobal sensitivity of LE, and show that to satisfy $\\epsilon$-differential\nprivacy, a large amount of noise must be introduced, rendering the published\nresults useless. Hence, we propose a thresholding technique to limit the number\nof users' visits, which significantly reduces the perturbation error but\nintroduces an approximation error. To achieve better utility, we extend the\ntechnique by adopting two weaker notions of privacy: smooth sensitivity\n(slightly weaker) and crowd-blending (strictly weaker). Extensive experiments\non synthetic and real-world datasets show that our proposed techniques preserve\noriginal data distribution without compromising location privacy.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 22:25:51 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["To", "Hien", ""], ["Nguyen", "Kien", ""], ["Shahabi", "Cyrus", ""]]}, {"id": "1909.00955", "submitter": "Kien Nguyen", "authors": "Kien Nguyen, Jingyun Yang, Yijun Lin, Jianfa Lin, Yao-Yi Chiang, Cyrus\n  Shahabi", "title": "Los Angeles Metro Bus Data Analysis Using GPS Trajectory and Schedule\n  Data (Demo Paper)", "comments": "SIGSPATIAL'18, demo paper, 4 pages", "journal-ref": "26th ACM SIGSPATIAL International Conference on Advances in\n  Geographic Information Systems (SIGSPATIAL '18), 2018", "doi": "10.1145/3274895.3274911", "report-no": null, "categories": "cs.DB cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the widespread installation of location-enabled devices on public\ntransportation, public vehicles are generating massive amounts of trajectory\ndata in real time. However, using these trajectory data for meaningful analysis\nrequires careful considerations in storing, managing, processing, and\nvisualizing the data. Using the location data of the Los Angeles Metro bus\nsystem, along with publicly available bus schedule data, we conduct a data\nprocessing and analyses study to measure the performance of the public\ntransportation system in Los Angeles utilizing a number of metrics including\ntravel-time reliability, on-time performance, bus bunching, and travel-time\nestimation. We demonstrate the visualization of the data analysis results\nthrough an interactive web-based application. The developed algorithms and\nsystem provide powerful tools to detect issues and improve the efficiency of\npublic transportation systems.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 05:01:44 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Nguyen", "Kien", ""], ["Yang", "Jingyun", ""], ["Lin", "Yijun", ""], ["Lin", "Jianfa", ""], ["Chiang", "Yao-Yi", ""], ["Shahabi", "Cyrus", ""]]}, {"id": "1909.00985", "submitter": "Daniel Weidner", "authors": "Daniel Weidner and Martin Atzmueller and Dietmar Seipel", "title": "Finding Maximal Non-Redundant Association Rules in Tennis Data", "comments": "Part of DECLARE 19 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of association rules is well--known in data mining. But often\nredundancy and subsumption are not considered, and standard approaches produce\nthousands or even millions of resulting association rules. Without further\ninformation or post--mining approaches, this huge number of rules is typically\nuseless for the domain specialist -- which is an instance of the infamous\npattern explosion problem. In this work, we present a new definition of\nredundancy and subsumption based on the confidence and the support of the rules\nand propose post-- mining to prune a set of association rules. In a case study,\nwe apply our method to association rules mined from spatio--temporal data. The\ndata represent the trajectories of the ball in tennis matches -- more\nprecisely, the points/times the tennis ball hits the ground. The goal is to\nanalyze the strategies of the players and to try to improve their performance\nby looking at the resulting association rules. The proposed approach is\ngeneral, and can also be applied to other spatio--temporal data with a similar\nstructure.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 07:28:23 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Weidner", "Daniel", ""], ["Atzmueller", "Martin", ""], ["Seipel", "Dietmar", ""]]}, {"id": "1909.01032", "submitter": "Samaneh Jozashoori", "authors": "Samaneh Jozashoori, Maria-Esther Vidal", "title": "MapSDI: A Scaled-up Semantic Data Integration Framework for Knowledge\n  Graph Creation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic web technologies have significantly contributed with effective\nsolutions for the problems of data integration and knowledge graph creation.\nHowever, with the rapid growth of big data in diverse domains, different\ninteroperability issues still demand to be addressed, being scalability one of\nthe main challenges. In this paper, we address the problem of knowledge graph\ncreation at scale and provide MapSDI, a mapping rule-based framework for\noptimizing semantic data integration into knowledge graphs. MapSDI allows for\nthe semantic enrichment of large-sized, heterogeneous, and potentially\nlow-quality data efficiently. The input of MapSDI is a set of data sources and\nmapping rules being generated by a mapping language such as RML. First, MapSDI\npre-processes the sources based on semantic information extracted from mapping\nrules, by performing basic database operators; it projects out required\nattributes, eliminates duplicates, and selects relevant entries. All these\noperators are defined based on the knowledge encoded by the mapping rules which\nwill be then used by the semantification engine (or RDFizer) to produce a\nknowledge graph. We have empirically studied the impact of MapSDI on existing\nRDFizers, and observed that knowledge graph creation time can be reduced on\naverage in one order of magnitude. It is also shown, theoretically, that the\nsources and rules transformations provided by MapSDI are data-lossless.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 10:05:44 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Jozashoori", "Samaneh", ""], ["Vidal", "Maria-Esther", ""]]}, {"id": "1909.01091", "submitter": "Projjal Gipta", "authors": "Projjal Gupta", "title": "Usage of Permissioned Blockchain Architecture for Big Data in Electronic\n  Medical Records", "comments": "4 pages, 3 figures, 3 Code Objects", "journal-ref": null, "doi": "10.13140/RG.2.2.26981.35048", "report-no": null, "categories": "cs.CY cs.CR cs.DB cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the advent of blockchain technology, multiple research avenues and\nplatforms for dialogue have opened up. However technology transfer to the pubic\nhas not been implemented, such that regular public can access and make use of\nsecure and decentralized software. Most blockchain solutions till date deal\nwith financial applications or monetary transactions, which may not be helpful\nor be accessible to the general public, especially the lower levels of the\nfinancial society. Medi-Chain is a people-first medical blockchain with a\nusable desktop application and interface which makes use of cutting-edge\nblockchain technology along with BFT consensus protocols to ensure highly\nsecure and private medical data records. This paper aims to bring about a\nchange in how blockchains-as-a-service is perceived and how adoption of new\ntechnology is largely based on usability and ease of adoption.\n", "versions": [{"version": "v1", "created": "Sun, 25 Aug 2019 10:06:57 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Gupta", "Projjal", ""]]}, {"id": "1909.01109", "submitter": "Michael Luggen", "authors": "Michael Luggen, Djellel Difallah, Cristina Sarasua, Gianluca\n  Demartini, and Philippe Cudr\\'e-Mauroux", "title": "Non-Parametric Class Completeness Estimators for Collaborative Knowledge\n  Graphs -- The Case of Wikidata", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-30793-6_26", "report-no": null, "categories": "cs.DB cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Collaborative Knowledge Graph platforms allow humans and automated scripts to\ncollaborate in creating, updating and interlinking entities and facts. To\nensure both the completeness of the data as well as a uniform coverage of the\ndifferent topics, it is crucial to identify underrepresented classes in the\nKnowledge Graph. In this paper, we tackle this problem by developing\nstatistical techniques for class cardinality estimation in collaborative\nKnowledge Graph platforms. Our method is able to estimate the completeness of a\nclass - as defined by a schema or ontology - hence can be used to answer\nquestions such as \"Does the knowledge base have a complete list of all {Beer\nBrands|Volcanos|Video Game Consoles}?\" As a use-case, we focus on Wikidata,\nwhich poses unique challenges in terms of the size of its ontology, the number\nof users actively populating its graph, and its extremely dynamic nature. Our\ntechniques are derived from species estimation and data-management\nmethodologies, and are applied to the case of graphs and collaborative editing.\nIn our empirical evaluation, we observe that i) the number and frequency of\nunique class instances drastically influence the performance of an estimator,\nii) bursts of inserts cause some estimators to overestimate the true size of\nthe class if they are not properly handled, and iii) one can effectively\nmeasure the convergence of a class towards its true size by considering the\nstability of an estimator against the number of available instances.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 12:16:05 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Luggen", "Michael", ""], ["Difallah", "Djellel", ""], ["Sarasua", "Cristina", ""], ["Demartini", "Gianluca", ""], ["Cudr\u00e9-Mauroux", "Philippe", ""]]}, {"id": "1909.01120", "submitter": "Saravanan Thirumuruganathan", "authors": "Riccardo Cappuzzo, Paolo Papotti, Saravanan Thirumuruganathan", "title": "Local Embeddings for Relational Data Integration", "comments": "Accepted to SIGMOD 2020 as Creating Embeddings of Heterogeneous\n  Relational Datasets for Data Integration Tasks. Code can be found at\n  https://gitlab.eurecom.fr/cappuzzo/embdi", "journal-ref": null, "doi": "10.1145/3318464.3389742", "report-no": null, "categories": "cs.DB cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based techniques have been recently used with promising results\nfor data integration problems. Some methods directly use pre-trained embeddings\nthat were trained on a large corpus such as Wikipedia. However, they may not\nalways be an appropriate choice for enterprise datasets with custom vocabulary.\nOther methods adapt techniques from natural language processing to obtain\nembeddings for the enterprise's relational data. However, this approach blindly\ntreats a tuple as a sentence, thus losing a large amount of contextual\ninformation present in the tuple.\n  We propose algorithms for obtaining local embeddings that are effective for\ndata integration tasks on relational databases. We make four major\ncontributions. First, we describe a compact graph-based representation that\nallows the specification of a rich set of relationships inherent in the\nrelational world. Second, we propose how to derive sentences from such a graph\nthat effectively \"describe\" the similarity across elements (tokens, attributes,\nrows) in the two datasets. The embeddings are learned based on such sentences.\nThird, we propose effective optimization to improve the quality of the learned\nembeddings and the performance of integration tasks. Finally, we propose a\ndiverse collection of criteria to evaluate relational embeddings and perform an\nextensive set of experiments validating them against multiple baseline methods.\nOur experiments show that our framework, EmbDI, produces meaningful results for\ndata integration tasks such as schema matching and entity resolution both in\nsupervised and unsupervised settings.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 12:45:02 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2020 08:56:00 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Cappuzzo", "Riccardo", ""], ["Papotti", "Paolo", ""], ["Thirumuruganathan", "Saravanan", ""]]}, {"id": "1909.01216", "submitter": "Alejandro Vaisman Dr.", "authors": "Leticia G\\'omez, Bart Kuijpers, Alejandro Vaisman", "title": "Online Analytical Processsing on Graph Data", "comments": "This is a draft version of the work that will appear in Volume 24(2)\n  of the Intelligent Data Analysis Journal, in early 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online Analytical Processing (OLAP) comprises tools and algorithms that allow\nquerying multidimensional databases. It is based on the multidimensional model,\nwhere data can be seen as a cube such that each cell contains one or more\nmeasures that can be aggregated along dimensions. In a Big Data scenario,\ntraditional data warehousing and OLAP operations are clearly not sufficient to\naddress current data analysis requirements, for example, social network\nanalysis. Furthermore, OLAP operations and models can expand the possibilities\nof graph analysis beyond the traditional graph-based computation. Nevertheless,\nthere is not much work on the problem of taking OLAP analysis to the graph data\nmodel. This paper proposes a formal multidimensional model for graph analysis,\nthat considers the basic graph data, and also background information in the\nform of dimension hierarchies. The graphs in this model are node- and\nedge-labelled directed multi-hypergraphs, called graphoids, which can be\ndefined at several different levels of granularity using the dimensions\nassociated with them. Operations analogous to the ones used in typical OLAP\nover cubes are defined over graphoids. The paper presents a formal definition\nof the graphoid model for OLAP, proves that the typical OLAP operations on\ncubes can be expressed over the graphoid model, and shows that the classic data\ncube model is a particular case of the graphoid data model. Finally, a case\nstudy supports the claim that, for many kinds of OLAP-like analysis on graphs,\nthe graphoid model works better than the typical relational OLAP alternative,\nand for the classic OLAP queries, it remains competitive.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 14:30:33 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["G\u00f3mez", "Leticia", ""], ["Kuijpers", "Bart", ""], ["Vaisman", "Alejandro", ""]]}, {"id": "1909.01421", "submitter": "Niek Tax", "authors": "Niek Tax", "title": "Mining Insights from Weakly-Structured Event Data", "comments": "PhD thesis successfully defended and doctoral degree obtained at\n  Eindhoven University of Technology on 19th of June 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis focuses on process mining on event data where such a normative\nspecification is absent and, as a result, the event data is less structured.\nThe thesis puts special emphasis on one application domain that fits this\ndescription: the analysis of smart home data where sequences of daily\nactivities are recorded. In this thesis we propose a set of techniques to\nanalyze such data, which can be grouped into two categories of techniques. The\nfirst category of methods focuses on preprocessing event logs in order to\nenable process discovery techniques to extract insights into unstructured event\ndata. In this category we have developed the following techniques: - An\nunsupervised approach to refine event labels based on the time at which the\nevent took place, allowing for example to distinguish recorded eating events\ninto breakfast, lunch, and dinner. - An approach to detect and filter from\nevent logs so-called chaotic activities, which are activities that cause\nprocess discovery methods to overgeneralize. - A supervised approach to\nabstract low-level events into more high-level events, where we show that there\nexist situations where process discovery approaches overgeneralize on the\nlow-level event data but are able to find precise models on the high-level\nevent data. The second category focuses on mining local process models, i.e.,\ncollections of process model patterns that each describe some frequent pattern,\nin contrast to the single global process model that is obtained with existing\nprocess discovery techniques. Several techniques are introduced in the area of\nlocal process model mining, including a basic method, fast but approximate\nheuristic methods, and constraint-based techniques.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 19:43:37 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Tax", "Niek", ""]]}, {"id": "1909.01602", "submitter": "Andrea Giovanni Nuzzolese", "authors": "Paolo Ciancarini and Andrea Giovanni Nuzzolese and Valentina Presutti\n  and Daniel Russo", "title": "SQuAP-Ont: an Ontology of Software Quality Relational Factors from\n  Financial Systems", "comments": null, "journal-ref": null, "doi": "10.3233/SW-200372", "report-no": null, "categories": "cs.AI cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Quality, architecture, and process are considered the keystones of software\nengineering. ISO defines them in three separate standards. However, their\ninteraction has been scarcely studied, so far. The SQuAP model (Software\nQuality, Architecture, Process) describes twenty-eight main factors that impact\non software quality in banking systems, and each factor is described as a\nrelation among some characteristics from the three ISO standards. Hence, SQuAP\nmakes such relations emerge rigorously, although informally. In this paper, we\npresent SQuAP-Ont, an OWL ontology designed by following a well-established\nmethodology based on the re-use of Ontology Design Patterns (i.e. ODPs).\nSQuAP-Ont formalises the relations emerging from SQuAP to represent and reason\nvia Linked Data about software engineering in a three-dimensional model\nconsisting of quality, architecture, and process ISO characteristics.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 07:57:54 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Ciancarini", "Paolo", ""], ["Nuzzolese", "Andrea Giovanni", ""], ["Presutti", "Valentina", ""], ["Russo", "Daniel", ""]]}, {"id": "1909.01822", "submitter": "Syed Ahmad Chan Bukhari", "authors": "Hafsa Shareef Dar, M.Ikramullah Lali, Moin Ul Din, Khalid Mahmood\n  Malik, Syed Ahmad Chan Bukhari", "title": "Frameworks for Querying Databases Using Natural Language: A Literature\n  Review", "comments": "5 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Natural Language Interface (NLI) facilitates users to pose queries to\nretrieve information from a database without using any artificial language such\nas the Structured Query Language (SQL). Several applications in various domains\nincluding healthcare, customer support and search engines, require elaborating\nstructured data having information on text. Moreover, many issues have been\nexplored including configuration complexity, processing of intensive\nalgorithms, and popularity of relational databases, due to which translating\nnatural language to database query has become a secondary area of\ninvestigation. The emerging trend of querying systems and speech-enabled\ninterfaces revived natural language to database queries research area., The\nlast survey published on this topic was six years ago in 2013. To best of our\nknowledge, there is no recent study found which discusses the current state of\nthe art translations frameworks for natural language for structured and\nnon-structured query languages. In this paper, we have reviewed 47 frameworks\nfrom 2008 to 2018. Out of 47, 35 were closely relevant to our work. SQL based\nframeworks have been categorized as statistical, symbolic and connectionist\napproaches. Whereas, NoSQL based frameworks have been categorized as semantic\nmatching and pattern matching. These frameworks are then reviewed based on\ntheir supporting language, scheme of their heuristic rule, interoperability\nsupport, dataset scope and their overall performance score. The findings stated\nthat 70% of the work in natural language to database querying has been carried\nout for SQL, and NoSQL share 15%, 10% and 5% of languages like SPAROL, CYPHER\nand GREMLIN respectively. It has also been observed that most of the frameworks\nsupport English language only.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 15:49:35 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Dar", "Hafsa Shareef", ""], ["Lali", "M. Ikramullah", ""], ["Din", "Moin Ul", ""], ["Malik", "Khalid Mahmood", ""], ["Bukhari", "Syed Ahmad Chan", ""]]}, {"id": "1909.01917", "submitter": "Damien Desfontaines", "authors": "Royce J Wilson, Celia Yuxin Zhang, William Lam, Damien Desfontaines,\n  Daniel Simmons-Marengo, Bryant Gipson", "title": "Differentially Private SQL with Bounded User Contribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Differential privacy (DP) provides formal guarantees that the output of a\ndatabase query does not reveal too much information about any individual\npresent in the database. While many differentially private algorithms have been\nproposed in the scientific literature, there are only a few end-to-end\nimplementations of differentially private query engines. Crucially, existing\nsystems assume that each individual is associated with at most one database\nrecord, which is unrealistic in practice. We propose a generic and scalable\nmethod to perform differentially private aggregations on databases, even when\nindividuals can each be associated with arbitrarily many rows. We express this\nmethod as an operator in relational algebra, and implement it in an SQL engine.\nTo validate this system, we test the utility of typical queries on industry\nbenchmarks, and verify its correctness with a stochastic test framework we\ndeveloped. We highlight the promises and pitfalls learned when deploying such a\nsystem in practice, and we publish its core components as open-source software.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 16:14:29 GMT"}, {"version": "v2", "created": "Thu, 5 Sep 2019 09:05:32 GMT"}, {"version": "v3", "created": "Mon, 25 Nov 2019 23:08:41 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Wilson", "Royce J", ""], ["Zhang", "Celia Yuxin", ""], ["Lam", "William", ""], ["Desfontaines", "Damien", ""], ["Simmons-Marengo", "Daniel", ""], ["Gipson", "Bryant", ""]]}, {"id": "1909.02629", "submitter": "Ming-Hung Shih", "authors": "Trong Duc Nguyen, Ming-Hung Shih, Sai Sree Parvathaneni, Bojian Xu,\n  Divesh Srivastava, Srikanta Tirthapura", "title": "Random Sampling for Group-By Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random sampling has been widely used in approximate query processing on large\ndatabases, due to its potential to significantly reduce resource usage and\nresponse times, at the cost of a small approximation error. We consider random\nsampling for answering the ubiquitous class of group-by queries, which first\ngroup data according to one or more attributes, and then aggregate within each\ngroup after filtering through a predicate. The challenge with group-by queries\nis that a sampling method cannot focus on optimizing the quality of a single\nanswer (e.g. the mean of selected data), but must simultaneously optimize the\nquality of a set of answers (one per group).\n  We present CVOPT, a query- and data-driven sampling framework for a set of\ngroup-by queries. To evaluate the quality of a sample, CVOPT defines a metric\nbased on the norm (e.g. $\\ell_2$ or $\\ell_\\infty$) of the coefficients of\nvariation (CVs) of different answers, and constructs a stratified sample that\nprovably optimizes the metric. CVOPT can handle group-by queries on data where\ngroups have vastly different statistical characteristics, such as frequencies,\nmeans, or variances. CVOPT jointly optimizes for multiple aggregations and\nmultiple group-by clauses, and provides a way to prioritize specific groups or\naggregates. It can be tuned to cases when partial information about a query\nworkload is known, such as a data warehouse where queries are scheduled to run\nperiodically.\n  Our experimental results show that CVOPT outperforms current state-of-the-art\non sample quality and estimation accuracy for group-by queries. On a set of\nqueries on two real-world data sets, CVOPT yields relative errors that are 5x\nsmaller than competing approaches, under the same space budget.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 20:52:42 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 16:15:22 GMT"}, {"version": "v3", "created": "Thu, 12 Sep 2019 07:14:41 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Nguyen", "Trong Duc", ""], ["Shih", "Ming-Hung", ""], ["Parvathaneni", "Sai Sree", ""], ["Xu", "Bojian", ""], ["Srivastava", "Divesh", ""], ["Tirthapura", "Srikanta", ""]]}, {"id": "1909.02976", "submitter": "Matthias Boehm", "authors": "Matthias Boehm, Iulian Antonov, Sebastian Baunsgaard, Mark Dokter,\n  Robert Ginthoer, Kevin Innerebner, Florijan Klezin, Stefanie Lindstaedt,\n  Arnab Phani, Benjamin Rath, Berthold Reinwald, Shafaq Siddiqi, Sebastian\n  Benjamin Wrede", "title": "SystemDS: A Declarative Machine Learning System for the End-to-End Data\n  Science Lifecycle", "comments": "CIDR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning (ML) applications become increasingly common in many\ndomains. ML systems to execute these workloads include numerical computing\nframeworks and libraries, ML algorithm libraries, and specialized systems for\ndeep neural networks and distributed ML. These systems focus primarily on\nefficient model training and scoring. However, the data science process is\nexploratory, and deals with underspecified objectives and a wide variety of\nheterogeneous data sources. Therefore, additional tools are employed for data\nengineering and debugging, which requires boundary crossing, unnecessary manual\neffort, and lacks optimization across the lifecycle. In this paper, we\nintroduce SystemDS, an open source ML system for the end-to-end data science\nlifecycle from data integration, cleaning, and preparation, over local,\ndistributed, and federated ML model training, to debugging and serving. To this\nend, we aim to provide a stack of declarative language abstractions for the\ndifferent lifecycle tasks, and users with different expertise. We describe the\noverall system architecture, explain major design decisions (motivated by\nlessons learned from Apache SystemML), and discuss key features and research\ndirections. Finally, we provide preliminary results that show the potential of\nend-to-end lifecycle optimization.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 15:41:09 GMT"}, {"version": "v2", "created": "Wed, 8 Jan 2020 00:10:20 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Boehm", "Matthias", ""], ["Antonov", "Iulian", ""], ["Baunsgaard", "Sebastian", ""], ["Dokter", "Mark", ""], ["Ginthoer", "Robert", ""], ["Innerebner", "Kevin", ""], ["Klezin", "Florijan", ""], ["Lindstaedt", "Stefanie", ""], ["Phani", "Arnab", ""], ["Rath", "Benjamin", ""], ["Reinwald", "Berthold", ""], ["Siddiqi", "Shafaq", ""], ["Wrede", "Sebastian Benjamin", ""]]}, {"id": "1909.03026", "submitter": "Jonas Traub", "authors": "Jonas Traub, Jorge-Arnulfo Quian\\'e-Ruiz, Zoi Kaoudi, Volker Markl\n  (Technische Universit\\\"at Berlin, German Research Center for Artificial\n  Intelligence (DFKI))", "title": "Agora: A Unified Asset Ecosystem Going Beyond Marketplaces and Cloud\n  Services", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.DC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data, algorithms, and compute/storage infrastructure are key assets that\ndrive data science and artificial intelligence applications. As providing all\nthese assets requires a huge investment, data science and artificial\nintelligence technologies are currently dominated by a small number of\nproviders who can afford these investments. This leads to lock-in effects and\nhinders features that require a flexible exchange of assets among users. In\nthis vision paper, we present Agora, a unified asset ecosystem. The Agora\nsystem provides the technical infrastructure that allows for offering and using\ndata and algorithms, as well as physical infrastructure components. Agora is\ndesigned as an open ecosystem of asset marketplaces and provides to a broad\naudience not only data but the entire data value chain (including computational\nresources and human expertise). Agora (i) leverages a fine-grained exchange of\nassets, (ii) allows for combining assets to novel applications, and (iii)\nflexibly executes such applications on available resources. As a result, Agora\novercomes lock-in effects and removes entry barriers for new asset providers.\nIn contrast to existing data management systems, Agora operates in a heavily\ndecentralized and dynamic environment: Data, algorithms, and even compute\nresources are dynamically created, modified, and removed by different\nstakeholders. Agora presents novel research directions for the data management\ncommunity as a whole: It requires to combine our traditional expertise in\nscalable data processing and management with infrastructure provisioning as\nwell as economic and application aspects of data, algorithms, and\ninfrastructure.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 17:22:28 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2019 14:35:22 GMT"}, {"version": "v3", "created": "Sun, 19 Jul 2020 07:23:17 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Traub", "Jonas", "", "Technische Universit\u00e4t Berlin, German Research Center for Artificial\n  Intelligence"], ["Quian\u00e9-Ruiz", "Jorge-Arnulfo", "", "Technische Universit\u00e4t Berlin, German Research Center for Artificial\n  Intelligence"], ["Kaoudi", "Zoi", "", "Technische Universit\u00e4t Berlin, German Research Center for Artificial\n  Intelligence"], ["Markl", "Volker", "", "Technische Universit\u00e4t Berlin, German Research Center for Artificial\n  Intelligence"]]}, {"id": "1909.03291", "submitter": "Torsten Grust", "authors": "Christian Duta, Denis Hirn, Torsten Grust", "title": "Compiling PL/SQL Away", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"PL/SQL functions are slow,\" is common developer wisdom that derives from the\ntension between set-oriented SQL evaluation and statement-by-statement PL/SQL\ninterpretation. We pursue the radical approach of compiling PL/SQL away,\nturning interpreted functions into regular subqueries that can then be\nefficiently evaluated together with their embracing SQL query, avoiding any\nPL/SQL to SQL context switches. Input PL/SQL functions may exhibit arbitrary\ncontrol flow. Iteration, in particular, is compiled into SQL-level recursion.\nRDBMSs across the board reward this compilation effort with significant run\ntime savings that render established developer lore questionable.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 15:42:22 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Duta", "Christian", ""], ["Hirn", "Denis", ""], ["Grust", "Torsten", ""]]}, {"id": "1909.03330", "submitter": "Zhi Yan Dr.", "authors": "Zhi Yan, Li Sun, Tomas Krajnik, and Yassine Ruichek", "title": "EU Long-term Dataset with Multiple Sensors for Autonomous Driving", "comments": "8 pages, 8 figures, accepted at IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of autonomous driving has grown tremendously over the past few\nyears, along with the rapid progress in sensor technology. One of the major\npurposes of using sensors is to provide environment perception for vehicle\nunderstanding, learning and reasoning, and ultimately interacting with the\nenvironment. In this paper, we first introduce a multisensor platform allowing\nvehicle to perceive its surroundings and locate itself in a more efficient and\naccurate way. The platform integrates eleven heterogeneous sensors including\nvarious cameras and lidars, a radar, an IMU (Inertial Measurement Unit), and a\nGPS-RTK (Global Positioning System / Real-Time Kinematic), while exploits a ROS\n(Robot Operating System) based software to process the sensory data. Then, we\npresent a new dataset (https://epan-utbm.github.io/utbm_robocar_dataset/) for\nautonomous driving captured many new research challenges (e.g. highly dynamic\nenvironment), and especially for long-term autonomy (e.g. creating and\nmaintaining maps), collected with our instrumented vehicle, publicly available\nto the community.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 20:23:00 GMT"}, {"version": "v2", "created": "Sat, 29 Feb 2020 22:43:00 GMT"}, {"version": "v3", "created": "Thu, 6 Aug 2020 16:51:08 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Yan", "Zhi", ""], ["Sun", "Li", ""], ["Krajnik", "Tomas", ""], ["Ruichek", "Yassine", ""]]}, {"id": "1909.03443", "submitter": "Shuo Zhang", "authors": "Shuo Zhang and Krisztian Balog", "title": "Auto-completion for Data Cells in Relational Tables", "comments": "In Proceedings of the 28th ACM International Conference on\n  Information and Knowledge Management (CIKM '19), 2019", "journal-ref": null, "doi": "10.1145/3357384.3357932", "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the task of auto-completing data cells in relational tables. Such\ntables describe entities (in rows) with their attributes (in columns). We\npresent the CellAutoComplete framework to tackle several novel aspects of this\nproblem, including: (i) enabling a cell to have multiple, possibly conflicting\nvalues, (ii) supplementing the predicted values with supporting evidence, (iii)\ncombining evidence from multiple sources, and (iv) handling the case where a\ncell should be left empty. Our framework makes use of a large table corpus and\na knowledge base as data sources, and consists of preprocessing, candidate\nvalue finding, and value ranking components. Using a purpose-built test\ncollection, we show that our approach is 40\\% more effective than the best\nbaseline.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 12:17:15 GMT"}, {"version": "v2", "created": "Wed, 5 Feb 2020 18:54:45 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Zhang", "Shuo", ""], ["Balog", "Krisztian", ""]]}, {"id": "1909.04550", "submitter": "Jianshen Liu", "authors": "Jianshen Liu, Philip Kufeldt, Carlos Maltzahn", "title": "MBWU: Benefit Quantification for Data Access Function Offloading", "comments": "16 pages, 11 figures", "journal-ref": "HPC I/O in the Data Center Workshop, 2019", "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The storage industry is considering new kinds of storage devices that support\ndata access function offloading, i.e. the ability to perform data access\nfunctions on the storage device itself as opposed to performing it on a\nseparate compute system to which the storage device is connected. But what is\nthe benefit of offloading to a storage device that is controlled by an embedded\nplatform, very different from a host platform? To quantify the benefit, we need\na measurement methodology that enables apple-to-apple comparisons between\ndifferent platforms. We propose a Media-based Work Unit (MBWU, pronounced\n\"MibeeWu\"), and an MBWU-based measurement methodology to standardize the\nplatform efficiency evaluation so as to quantify the benefit of offloading. To\ndemonstrate the merit of this methodology, we implemented a prototype to\nautomate quantifying the benefit of offloading the key-value data access\nfunction.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 04:15:33 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Liu", "Jianshen", ""], ["Kufeldt", "Philip", ""], ["Maltzahn", "Carlos", ""]]}, {"id": "1909.04844", "submitter": "Jonas Mueller", "authors": "Jonas Mueller, Alex Smola", "title": "Recognizing Variables from their Data via Deep Embeddings of\n  Distributions", "comments": "IEEE International Conference on Data Mining (ICDM), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key obstacle in automated analytics and meta-learning is the inability to\nrecognize when different datasets contain measurements of the same variable.\nBecause provided attribute labels are often uninformative in practice, this\ntask may be more robustly addressed by leveraging the data values themselves\nrather than just relying on their arbitrarily selected variable names. Here, we\npresent a computationally efficient method to identify high-confidence variable\nmatches between a given set of data values and a large repository of previously\nencountered datasets. Our approach enjoys numerous advantages over\ndistributional similarity based techniques because we leverage learned vector\nembeddings of datasets which adaptively account for natural forms of data\nvariation encountered in practice. Based on the neural architecture of deep\nsets, our embeddings can be computed for both numeric and string data. In\ndataset search and schema matching tasks, our methods outperform standard\nstatistical techniques and we find that the learned embeddings generalize well\nto new data sources.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 04:10:48 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Mueller", "Jonas", ""], ["Smola", "Alex", ""]]}, {"id": "1909.04881", "submitter": "Joshua Shinavier", "authors": "Joshua Shinavier, Ryan Wisnesky", "title": "Algebraic Property Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the main challenges of building an enterprise knowledge graph is\nguaranteeing interoperability between a single graph data model and the diverse\nand often changeable ecosystem of non-graph data models, languages, and tools\nwhich surround and support the graph. The simple and developer-friendly\nproperty graph family of data models lends itself to this task, yet the lack of\na formal specification deprives the graph of well-defined semantics. In this\npaper, we observe that algebraic data types are a common foundation of most of\nthe enterprise schema languages we deal with in practice, and are also a\nsuitable basis for a property graph formalism. We introduce this formalism in\nterms of type theory, algebra, and category theory, also providing algorithms\nfor query processing and data migration with guarantees of semantic consistency\nacross supported languages and datasets. These results have clear connections\nto relational database theory, programming language theory, and graph theory,\nproviding starting points for significant future work. Open research challenges\ndescribed in the paper include adding constraints, query and schema languages,\nand logics on top of the basic type system, interfacing with specific graph and\nnon-graph data models, and performing operations which are typically difficult\nor ill-defined on property graphs, such as graph merges.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 07:29:28 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 16:29:02 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Shinavier", "Joshua", ""], ["Wisnesky", "Ryan", ""]]}, {"id": "1909.05028", "submitter": "Ajay Shrestha", "authors": "Ajay Kumar Shrestha, Ralph Deters and Julita Vassileva", "title": "User-Controlled Privacy-Preserving User Profile Data Sharing based on\n  Blockchain", "comments": "10 pages, 11 figures, 4 tables, Future Technologies Conference (FTC)\n  2017, \"for associated proceeding paper, see\n  https://saiconference.com/Downloads/FTC2017/Proceedings/3_Paper_127-User-Controlled_Privacy-Preserving_User_Profile_Data_Sharing.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The tremendous technological advancement in the last few decades has brought\nmany enterprises to collaborate in a better way while making intelligent\ndecisions. The use of Information Technology tools in obtaining data of\npeople's everyday life from various autonomous data sources allowing\nunrestricted access to user data has emerged as an important practical issue\nand has given rise to legal implications. Various innovative models for data\nsharing and management have privacy and centrality issues. To alleviate these\nlimitations, we have incorporated blockchain in user modeling. In this paper,\nwe constructed a decentralized data sharing architecture with MultiChain\nblockchain in the travel domain, which is also applicable to other similar\ndomains including education, health, and sports. Businesses that operate in the\ntourism industries including travel and tour agencies, hotels and resorts,\nshopping malls are connected to the MultiChain and they share their user\nprofile data via stream in the MultiChain. The paper presents the hotel booking\nservice for an imaginary hotel as one of the enterprise nodes, which collects\nuser profile data with proper validation and will allow users to decide which\nof their data to be shared thus ensuring user control over their data and the\npreservation of privacy. The data from the repository is converted into an open\ndata format while sharing via stream in the blockchain so that other enterprise\nnodes, after receiving the data, can easily convert them and store into their\nown repositories. The paper presents an evaluation of the performance of the\nmodel by measuring the latency and memory consumption with three test scenarios\nthat mostly affect the user experience. The node responded quickly in all of\nthese cases.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 17:10:22 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Shrestha", "Ajay Kumar", ""], ["Deters", "Ralph", ""], ["Vassileva", "Julita", ""]]}, {"id": "1909.05372", "submitter": "Christopher R\\'e", "authors": "Christopher R\\'e, Feng Niu, Pallavi Gudipati, Charles Srisuwananukorn", "title": "Overton: A Data System for Monitoring and Improving Machine-Learned\n  Products", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a system called Overton, whose main design goal is to support\nengineers in building, monitoring, and improving production machine learning\nsystems. Key challenges engineers face are monitoring fine-grained quality,\ndiagnosing errors in sophisticated applications, and handling contradictory or\nincomplete supervision data. Overton automates the life cycle of model\nconstruction, deployment, and monitoring by providing a set of novel\nhigh-level, declarative abstractions. Overton's vision is to shift developers\nto these higher-level tasks instead of lower-level machine learning tasks. In\nfact, using Overton, engineers can build deep-learning-based applications\nwithout writing any code in frameworks like TensorFlow. For over a year,\nOverton has been used in production to support multiple applications in both\nnear-real-time applications and back-of-house processing. In that time,\nOverton-based applications have answered billions of queries in multiple\nlanguages and processed trillions of records reducing errors 1.7-2.9 times\nversus production systems.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 03:51:13 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["R\u00e9", "Christopher", ""], ["Niu", "Feng", ""], ["Gudipati", "Pallavi", ""], ["Srisuwananukorn", "Charles", ""]]}, {"id": "1909.05380", "submitter": "Stavros Sintos", "authors": "Stavros Sintos, Pankaj K. Agarwal, Jun Yang", "title": "Selecting Data to Clean for Fact Checking: Minimizing Uncertainty vs.\n  Maximizing Surprise", "comments": null, "journal-ref": null, "doi": "10.14778/3358701.3358708", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the optimization problem of selecting numerical quantities to clean\nin order to fact-check claims based on such data. Oftentimes, such claims are\ntechnically correct, but they can still mislead for two reasons. First, data\nmay contain uncertainty and errors. Second, data can be \"fished\" to advance\nparticular positions. In practice, fact-checkers cannot afford to clean all\ndata and must choose to clean what \"matters the most\" to checking a claim. We\nexplore alternative definitions of what \"matters the most\": one is to ascertain\nclaim qualities (by minimizing uncertainty in these measures), while an\nalternative is just to counter the claim (by maximizing the probability of\nfinding a counterargument). We show whether the two objectives align with each\nother, with important implications on when fact-checkers should exercise care\nin selective data cleaning, to avoid potential bias introduced by their desire\nto counter claims. We develop efficient algorithms for solving the various\nvariants of the optimization problem, showing significant improvements over\nnaive solutions. The problem is particularly challenging because the objectives\nin the fact-checking context are complex, non-linear functions over data. We\nobtain results that generalize to a large class of functions, with potential\napplications beyond fact-checking.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 21:25:40 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Sintos", "Stavros", ""], ["Agarwal", "Pankaj K.", ""], ["Yang", "Jun", ""]]}, {"id": "1909.05859", "submitter": "Simon Gottschalk", "authors": "Simon Gottschalk, Nicolas Tempelmeier, G\\\"unter Kniesel, Vasileios\n  Iosifidis, Besnik Fetahu, Elena Demidova", "title": "Simple-ML: Towards a Framework for Semantic Data Analytics Workflows", "comments": "SEMANTiCS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present the Simple-ML framework that we develop to support\nefficient configuration, robustness and reusability of data analytics workflows\nthrough the adoption of semantic technologies. We present semantic data models\nthat lay the foundation for the framework development and discuss the data\nanalytics workflows based on these models. Furthermore, we present an example\ninstantiation of the Simple-ML data models for a real-world use case in the\nmobility domain.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 16:23:56 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Gottschalk", "Simon", ""], ["Tempelmeier", "Nicolas", ""], ["Kniesel", "G\u00fcnter", ""], ["Iosifidis", "Vasileios", ""], ["Fetahu", "Besnik", ""], ["Demidova", "Elena", ""]]}, {"id": "1909.06039", "submitter": "Neil G. Marchant", "authors": "Neil G. Marchant, Andee Kaplan, Daniel N. Elazar, Benjamin I. P.\n  Rubinstein, Rebecca C. Steorts", "title": "d-blink: Distributed End-to-End Bayesian Entity Resolution", "comments": "32 pages, 6 figures, 5 tables. Includes 22 pages of supplementary\n  material. This revision incorporates a case study on the 2010 U.S. Decennial\n  Census", "journal-ref": null, "doi": "10.1080/10618600.2020.1825451", "report-no": null, "categories": "stat.CO cs.DB cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity resolution (ER; also known as record linkage or de-duplication) is the\nprocess of merging noisy databases, often in the absence of unique identifiers.\nA major advancement in ER methodology has been the application of Bayesian\ngenerative models, which provide a natural framework for inferring latent\nentities with rigorous quantification of uncertainty. Despite these advantages,\nexisting models are severely limited in practice, as standard inference\nalgorithms scale quadratically in the number of records. While scaling can be\nmanaged by fitting the model on separate blocks of the data, such a na\\\"ive\napproach may induce significant error in the posterior. In this paper, we\npropose a principled model for scalable Bayesian ER, called \"distributed\nBayesian linkage\" or d-blink, which jointly performs blocking and ER without\ncompromising posterior correctness. Our approach relies on several key ideas,\nincluding: (i) an auxiliary variable representation that induces a partition of\nthe entities and records into blocks; (ii) a method for constructing\nwell-balanced blocks based on k-d trees; (iii) a distributed\npartially-collapsed Gibbs sampler with improved mixing; and (iv) fast\nalgorithms for performing Gibbs updates. Empirical studies on six data\nsets---including a case study on the 2010 Decennial Census---demonstrate the\nscalability and effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 05:28:37 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 00:58:17 GMT"}, {"version": "v3", "created": "Tue, 22 Sep 2020 13:42:27 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Marchant", "Neil G.", ""], ["Kaplan", "Andee", ""], ["Elazar", "Daniel N.", ""], ["Rubinstein", "Benjamin I. P.", ""], ["Steorts", "Rebecca C.", ""]]}, {"id": "1909.06182", "submitter": "Alex Galakatos", "authors": "Nathaniel Weir, Andrew Crotty, Alex Galakatos, Amir Ilkhechi, Shekar\n  Ramaswamy, Rohin Bhushan, Ugur Cetintemel, Prasetya Utama, Nadja Geisler,\n  Benjamin H\\\"attasch, Steffen Eger, Carsten Binnig", "title": "DBPal: Weak Supervision for Learning a Natural Language Interface to\n  Databases", "comments": "arXiv admin note: text overlap with arXiv:1804.00401", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes DBPal, a new system to translate natural language\nutterances into SQL statements using a neural machine translation model. While\nother recent approaches use neural machine translation to implement a Natural\nLanguage Interface to Databases (NLIDB), existing techniques rely on supervised\nlearning with manually curated training data, which results in substantial\noverhead for supporting each new database schema. In order to avoid this issue,\nDBPal implements a novel training pipeline based on weak supervision that\nsynthesizes all training data from a given database schema. In our evaluation,\nwe show that DBPal can outperform existing rule-based NLIDBs while achieving\ncomparable performance to other NLIDBs that leverage deep neural network models\nwithout relying on manually curated training data for every new database\nschema.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 22:01:58 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Weir", "Nathaniel", ""], ["Crotty", "Andrew", ""], ["Galakatos", "Alex", ""], ["Ilkhechi", "Amir", ""], ["Ramaswamy", "Shekar", ""], ["Bhushan", "Rohin", ""], ["Cetintemel", "Ugur", ""], ["Utama", "Prasetya", ""], ["Geisler", "Nadja", ""], ["H\u00e4ttasch", "Benjamin", ""], ["Eger", "Steffen", ""], ["Binnig", "Carsten", ""]]}, {"id": "1909.06494", "submitter": "Victor Zakhary", "authors": "Victor Zakhary, Divyakant Agrawal, Amr El Abbadi", "title": "Transactional Smart Contracts in Blockchain Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents TXSC, a framework that provides smart contract developers\nwith transaction primitives. These primitives allow developers to write smart\ncontracts without the need to reason about the anomalies that can arise due to\nconcurrent smart contract function executions.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 00:36:13 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Zakhary", "Victor", ""], ["Agrawal", "Divyakant", ""], ["Abbadi", "Amr El", ""]]}, {"id": "1909.06610", "submitter": "Steffen Ehrmann", "authors": "Steffen Ehrmann, Ralf Seppelt, Carsten Meyer", "title": "Harmonise and integrate heterogeneous areal data with the R package\n  arealDB", "comments": "14 pages, 3 supplements, 6 figures, R-package", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Many relevant applications in the environmental and socioeconomic sciences\nuse areal data, such as biodiversity checklists, agricultural statistics, or\nsocioeconomic surveys. For applications that surpass the spatial, temporal or\nthematic scope of any single data source, data must be integrated from several\nheterogeneous sources. Inconsistent concepts, definitions, or messy data tables\nmake this a tedious and error-prone process. To date, a dedicated tool to\naddress these challenges is still lacking. Here, we introduce the R package\narealDB that integrates heterogeneous areal data and associated geometries into\na consistent database, in an easy-to-use workflow. It is useful for harmonising\nlanguage and semantics of variables, relating data to geometries, and\ndocumenting metadata and provenance. We illustrate the functionality by\nintegrating two disparate datasets (Brazil, USA) on the harvested area of\nsoybean. arealDB promises quality-improvements to downstream scientific,\nmonitoring, and management applications but also substantial time-savings to\ndatabase collation efforts.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 15:19:23 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 10:26:19 GMT"}, {"version": "v3", "created": "Wed, 19 Feb 2020 10:40:44 GMT"}, {"version": "v4", "created": "Tue, 14 Jul 2020 15:41:09 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Ehrmann", "Steffen", ""], ["Seppelt", "Ralf", ""], ["Meyer", "Carsten", ""]]}, {"id": "1909.06997", "submitter": "Han Liu", "authors": "Han Liu, Ge Gao, Hehua Zhang, Yu-Shen Liu, Yan Song, Ming Gu", "title": "MVDLite: A Light-weight Model View Definition Representation with Fast\n  Validation for Building Information Model", "comments": "Preprint submitted to Automation In Construction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model View Definition (MVD) is the standard methodology to define the\nexchange requirements and data constraints for Building Information Model\n(BIM). In this paper, MVDLite is proposed as a novel light-weight\nrepresentation for MVD. Compared with mvdXML, MVDLite is more concise and could\nbe used in more flexible scenarios. MVDLite introduces a \"rule chain\" structure\nto combine the subgraph templates and value constants, based on which a fast\nMVD validation algorithm is proposed. It is also compatible with the current\nmvdXML format, and supports bi-directional conversion with mvdXML. A case study\nis provided to show the workflow for developing an enterprise-level MVD based\non MVDLite, and its applications in MVD validation and partial model\nextraction. The outperforming experimental results show that our method is much\nfaster than the state-of-the-art methods on large real-world models.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 05:39:49 GMT"}, {"version": "v2", "created": "Fri, 31 Jan 2020 08:54:35 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Liu", "Han", ""], ["Gao", "Ge", ""], ["Zhang", "Hehua", ""], ["Liu", "Yu-Shen", ""], ["Song", "Yan", ""], ["Gu", "Ming", ""]]}, {"id": "1909.07440", "submitter": "Jeremy Welborn", "authors": "Jeremy Welborn, Michael Schaarschmidt, Eiko Yoneki", "title": "Learning Index Selection with Structured Action Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Configuration spaces for computer systems can be challenging for traditional\nand automatic tuning strategies. Injecting task-specific knowledge into the\ntuner for a task may allow for more efficient exploration of candidate\nconfigurations. We apply this idea to the task of index set selection to\naccelerate database workloads. Index set selection has been amenable to recent\napplications of vanilla deep RL, but real deployments remain out of reach. In\nthis paper, we explore how learning index selection can be enhanced with\ntask-specific inductive biases, specifically by encoding these inductive biases\nin better action structures. Index selection-specific action representations\narise when the problem is reformulated in terms of permutation learning and we\nrely on recent work for learning RL policies on permutations. Through this\napproach, we build an indexing agent that is able to achieve improved indexing\nand validate its behavior with task-specific statistics. Early experiments\nreveal that our agent can find configurations that are up to 40% smaller for\nthe same levels of latency as compared with other approaches and indicate more\nintuitive indexing behavior.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 19:16:41 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Welborn", "Jeremy", ""], ["Schaarschmidt", "Michael", ""], ["Yoneki", "Eiko", ""]]}, {"id": "1909.07918", "submitter": "Elisabet Lobo-Vesga", "authors": "Elisabet Lobo-Vesga (1), Alejandro Russo (1), Marco Gaboardi (2) ((1)\n  Chalmers University of Technology, (2) Boston University)", "title": "A Programming Framework for Differential Privacy with Accuracy\n  Concentration Bounds", "comments": "22 pages, 11 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy offers a formal framework for reasoning about privacy\nand accuracy of computations on private data. It also offers a rich set of\nbuilding blocks for constructing data analyses. When carefully calibrated,\nthese analyses simultaneously guarantee privacy of the individuals contributing\ntheir data, and accuracy of their results for inferring useful properties about\nthe population. The compositional nature of differential privacy has motivated\nthe design and implementation of several programming languages aimed at helping\na data analyst in programming differentially private analyses. However, most of\nthe programming languages for differential privacy proposed so far provide\nsupport for reasoning about privacy but not for reasoning about the accuracy of\ndata analyses. To overcome this limitation, in this work we present DPella, a\nprogramming framework providing data analysts with support for reasoning about\nprivacy, accuracy and their trade-offs. The distinguishing feature of DPella is\na novel component which statically tracks the accuracy of different data\nanalyses. In order to make tighter accuracy estimations, this component\nleverages taint analysis for automatically inferring statistical independence\nof the different noise quantities added for guaranteeing privacy. We show the\nflexibility of our approach by not only implementing classical counting queries\n(e.g., CDFs) but also by analyzing hierarchical counting queries (like those\ndone by Census Bureaus), where accuracy have different constraints per level\nand data analysts should figure out the best manner to calibrate privacy to\nmeet the accuracy requirements.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 20:29:08 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Lobo-Vesga", "Elisabet", ""], ["Russo", "Alejandro", ""], ["Gaboardi", "Marco", ""]]}, {"id": "1909.08006", "submitter": "Yuanjing Shi", "authors": "Yuanjing Shi, Zhaoxing Li", "title": "Leyenda: An Adaptive, Hybrid Sorting Algorithm for Large Scale Data with\n  Limited Memory", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sorting is the one of the fundamental tasks of modern data management\nsystems. With Disk I/O being the most-accused performance bottleneck and more\ncomputation-intensive workloads, it has come to our attention that in\nheterogeneous environment, performance bottleneck may vary among different\ninfrastructure. As a result, sort kernels need to be adaptive to changing\nhardware conditions. In this paper, we propose Leyenda, a hybrid, parallel and\nefficient Radix Most-Significant-Bit (MSB) MergeSort algorithm, with\nutilization of local thread-level CPU cache and efficient disk/memory I/O.\nLeyenda is capable of performing either internal or external sort efficiently,\nbased on different I/O and processing conditions. We benchmarked Leyenda with\nthree different workloads from Sort Benchmark, targeting three unique use\ncases, including internal, partially in-memory and external sort, and we found\nLeyenda to outperform GNU's parallel in-memory quick/merge sort implementations\nby up to three times. Leyenda is also ranked the second best external sort\nalgorithm on ACM 2019 SIGMOD programming contest and forth overall.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 18:10:04 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Shi", "Yuanjing", ""], ["Li", "Zhaoxing", ""]]}, {"id": "1909.08096", "submitter": "Dumitrel Loghin", "authors": "Dumitrel Loghin, Shaofeng Cai, Gang Chen, Tien Tuan Anh Dinh, Feiyi\n  Fan, Qian Lin, Janice Ng, Beng Chin Ooi, Xutao Sun, Quang-Trung Ta, Wei Wang,\n  Xiaokui Xiao, Yang Yang, Meihui Zhang, Zhonghua Zhang", "title": "The Disruptions of 5G on Data-driven Technologies and Applications", "comments": "19 pages, 10 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With 5G on the verge of being adopted as the next mobile network, there is a\nneed to analyze its impact on the landscape of computing and data management.\nIn this paper, we analyze the impact of 5G on both traditional and emerging\ntechnologies and project our view on future research challenges and\nopportunities. With a predicted increase of 10-100x in bandwidth and 5-10x\ndecrease in latency, 5G is expected to be the main enabler for smart cities,\nsmart IoT and efficient healthcare, where machine learning is conducted at the\nedge. In this context, we investigate how 5G can help the development of\nfederated learning. Network slicing, another key feature of 5G, allows running\nmultiple isolated networks on the same physical infrastructure. However,\nsecurity remains the main concern in the context of virtualization,\nmulti-tenancy and high device density. Formal verification of 5G networks can\nbe applied to detect security issues in massive virtualized environments. In\nsummary, 5G will make the world even more densely and closely connected. What\nwe have experienced in 4G connectivity will pale in comparison to the vast\namounts of possibilities engendered by 5G.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 23:27:18 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2019 06:20:25 GMT"}, {"version": "v3", "created": "Mon, 16 Dec 2019 03:29:05 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Loghin", "Dumitrel", ""], ["Cai", "Shaofeng", ""], ["Chen", "Gang", ""], ["Dinh", "Tien Tuan Anh", ""], ["Fan", "Feiyi", ""], ["Lin", "Qian", ""], ["Ng", "Janice", ""], ["Ooi", "Beng Chin", ""], ["Sun", "Xutao", ""], ["Ta", "Quang-Trung", ""], ["Wang", "Wei", ""], ["Xiao", "Xiaokui", ""], ["Yang", "Yang", ""], ["Zhang", "Meihui", ""], ["Zhang", "Zhonghua", ""]]}, {"id": "1909.08246", "submitter": "EPTCS", "authors": "K. Tuncay Tekle (Stony Brook University), Yanhong A. Liu (Stony Brook\n  University)", "title": "Extended Magic for Negation: Efficient Demand-Driven Evaluation of\n  Stratified Datalog with Precise Complexity Guarantees", "comments": "In Proceedings ICLP 2019, arXiv:1909.07646", "journal-ref": "EPTCS 306, 2019, pp. 241-254", "doi": "10.4204/EPTCS.306.28", "report-no": null, "categories": "cs.LO cs.AI cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of Datalog rules, facts, and a query, answers to the query can be\ninferred bottom-up starting from the facts or top-down starting from the query.\nFor efficiency, top-down evaluation is extended with memoization of inferred\nfacts, and bottom-up evaluation is performed after transformations to make\nrules driven by the demand from the query. Prior work has shown their precise\ncomplexity analysis and relationships. However, when Datalog is extended with\neven stratified negation, which has a simple and universally accepted\nsemantics, transformations to make rules demand-driven may result in\nnon-stratified negation, which has had many complex semantics and evaluation\nmethods.\n  This paper presents (1) a simple extension to demand transformation, a\ntransformation to make rules demand-driven for Datalog without negation, to\nsupport stratified negation, and (2) a simple extension to an optimal bottom-up\nevaluation method for Datalog with stratified negation, to handle\nnon-stratified negation in the resulting rules. We show that the method\nprovides precise complexity guarantees. It is also optimal in that only facts\nneeded for top-down evaluation of the query are inferred and each firing of a\nrule to infer such a fact takes worst-case constant time. We extend the precise\nrelationship between top-down evaluation and demand-driven bottom-up evaluation\nto Datalog with stratified negation. Finally, we show experimental results for\nperformance, as well as applications to previously challenging examples.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 07:07:42 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Tekle", "K. Tuncay", "", "Stony Brook University"], ["Liu", "Yanhong A.", "", "Stony Brook\n  University"]]}, {"id": "1909.08249", "submitter": "EPTCS", "authors": "Ariyam Das (University of California, Los Angeles, USA), Youfu Li\n  (University of California, Los Angeles, USA), Jin Wang (University of\n  California, Los Angeles, USA), Mingda Li (University of California, Los\n  Angeles, USA), Carlo Zaniolo (University of California, Los Angeles, USA)", "title": "BigData Applications from Graph Analytics to Machine Learning by\n  Aggregates in Recursion", "comments": "In Proceedings ICLP 2019, arXiv:1909.07646. Paper presented at the\n  35th International Conference on Logic Programming (ICLP 2019), Las Cruces,\n  New Mexico, USA, 20-25 September 2019, 7 pages (short paper - applications\n  track)", "journal-ref": "EPTCS 306, 2019, pp. 273-279", "doi": "10.4204/EPTCS.306.32", "report-no": null, "categories": "cs.LO cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past, the semantic issues raised by the non-monotonic nature of\naggregates often prevented their use in the recursive statements of logic\nprograms and deductive databases. However, the recently introduced notion of\nPre-mappability (PreM) has shown that, in key applications of interest,\naggregates can be used in recursion to optimize the perfect-model semantics of\naggregate-stratified programs. Therefore we can preserve the declarative formal\nsemantics of such programs while achieving a highly efficient operational\nsemantics that is conducive to scalable implementations on parallel and\ndistributed platforms. In this paper, we show that with PreM, a wide spectrum\nof classical algorithms of practical interest, ranging from graph analytics and\ndynamic programming based optimization problems to data mining and machine\nlearning applications can be concisely expressed in declarative languages by\nusing aggregates in recursion. Our examples are also used to show that PreM can\nbe checked using simple techniques and templatized verification strategies. A\nwide range of advanced BigData applications can now be expressed declaratively\nin logic-based languages, including Datalog, Prolog, and even SQL, while\nenabling their execution with superior performance and scalability.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 07:08:44 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Das", "Ariyam", "", "University of California, Los Angeles, USA"], ["Li", "Youfu", "", "University of California, Los Angeles, USA"], ["Wang", "Jin", "", "University of\n  California, Los Angeles, USA"], ["Li", "Mingda", "", "University of California, Los\n  Angeles, USA"], ["Zaniolo", "Carlo", "", "University of California, Los Angeles, USA"]]}, {"id": "1909.08254", "submitter": "EPTCS", "authors": "Nicos Angelopoulos (Department of Computer Science and Electronic\n  Engineering, University of Essex), Jan Wielemaker (Centrum voor Wiskunde en\n  Informatica (CWI), Amsterdam, The Netherlands)", "title": "Advances in Big Data Bio Analytics", "comments": "In Proceedings ICLP 2019, arXiv:1909.07646", "journal-ref": "EPTCS 306, 2019, pp. 309-322", "doi": "10.4204/EPTCS.306.36", "report-no": null, "categories": "cs.LO cs.DB q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Delivering effective data analytics is of crucial importance to the\ninterpretation of the multitude of biological datasets currently generated by\nan ever increasing number of high throughput techniques. Logic programming has\nmuch to offer in this area. Here, we detail advances that highlight two of the\nstrengths of logical formalisms in developing data analytic solutions in\nbiological settings: access to large relational databases and building\nanalytical pipelines collecting graph information from multiple sources. We\npresent significant advances on the bio_db package which serves biological\ndatabases as Prolog facts that can be served either by in-memory loading or via\ndatabase backends. These advances include modularising the underlying\narchitecture and the incorporation of datasets from a second organism (mouse).\nIn addition, we introduce a number of data analytics tools that operate on\nthese datasets and are bundled in the analysis package: bio_analytics. Emphasis\nin both packages is on ease of installation and use. We highlight the general\narchitecture of our components based approach. An experimental graphical user\ninterface via SWISH for local installation is also available. Finally, we\nadvocate that biological data analytics is a fertile area which can drive\nfurther innovation in applied logic programming.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 07:10:12 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Angelopoulos", "Nicos", "", "Department of Computer Science and Electronic\n  Engineering, University of Essex"], ["Wielemaker", "Jan", "", "Centrum voor Wiskunde en\n  Informatica"]]}, {"id": "1909.09377", "submitter": "Jerome Darmont", "authors": "Pegdwend\\'e Sawadogo (ERIC), Etienne Scholly (ERIC), C\\'ecile Favre\n  (ERIC), Eric Ferey, Sabine Loudcher (ERIC), J\\'er\\^ome Darmont (ERIC)", "title": "Metadata Systems for Data Lakes: Models and Features", "comments": null, "journal-ref": "1st International Workshop on BI and Big Data Applications\n  (BBIGAP@ADBIS 2019), Sep 2019, Bled, Slovenia. pp.440-451", "doi": "10.1007/978-3-030-30278-8", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decade, the data lake concept has emerged as an alternative to\ndata warehouses for storing and analyzing big data. A data lake allows storing\ndata without any predefined schema. Therefore, data querying and analysis\ndepend on a metadata system that must be efficient and comprehensive. However,\nmetadata management in data lakes remains a current issue and the criteria for\nevaluating its effectiveness are more or less nonexistent.In this paper, we\nintroduce MEDAL, a generic, graph-based model for metadata management in data\nlakes. We also propose evaluation criteria for data lake metadata systems\nthrough a list of expected features. Eventually, we show that our approach is\nmore comprehensive than existing metadata systems.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 09:06:28 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Sawadogo", "Pegdwend\u00e9", "", "ERIC"], ["Scholly", "Etienne", "", "ERIC"], ["Favre", "C\u00e9cile", "", "ERIC"], ["Ferey", "Eric", "", "ERIC"], ["Loudcher", "Sabine", "", "ERIC"], ["Darmont", "J\u00e9r\u00f4me", "", "ERIC"]]}, {"id": "1909.09638", "submitter": "Sobhan Moosavi", "authors": "Sobhan Moosavi, Mohammad Hossein Samavatian, Srinivasan Parthasarathy,\n  Radu Teodorescu, Rajiv Ramnath", "title": "Accident Risk Prediction based on Heterogeneous Sparse Data: New Dataset\n  and Insights", "comments": "In Proceedings of the 27th ACM SIGSPATIAL, International Conference\n  on Advances in Geographic Information Systems (2019). arXiv admin note:\n  substantial text overlap with arXiv:1906.05409", "journal-ref": null, "doi": "10.1145/3347146.3359078", "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reducing traffic accidents is an important public safety challenge,\ntherefore, accident analysis and prediction has been a topic of much research\nover the past few decades. Using small-scale datasets with limited coverage,\nbeing dependent on extensive set of data, and being not applicable for\nreal-time purposes are the important shortcomings of the existing studies. To\naddress these challenges, we propose a new solution for real-time traffic\naccident prediction using easy-to-obtain, but sparse data. Our solution relies\non a deep-neural-network model (which we have named DAP, for Deep Accident\nPrediction); which utilizes a variety of data attributes such as traffic\nevents, weather data, points-of-interest, and time. DAP incorporates multiple\ncomponents including a recurrent (for time-sensitive data), a fully connected\n(for time-insensitive data), and a trainable embedding component (to capture\nspatial heterogeneity). To fill the data gap, we have - through a comprehensive\nprocess of data collection, integration, and augmentation - created a\nlarge-scale publicly available database of accident information named\nUS-Accidents. By employing the US-Accidents dataset and through an extensive\nset of experiments across several large cities, we have evaluated our proposal\nagainst several baselines. Our analysis and results show significant\nimprovements to predict rare accident events. Further, we have shown the impact\nof traffic information, time, and points-of-interest data for real-time\naccident prediction.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 22:41:41 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Moosavi", "Sobhan", ""], ["Samavatian", "Mohammad Hossein", ""], ["Parthasarathy", "Srinivasan", ""], ["Teodorescu", "Radu", ""], ["Ramnath", "Rajiv", ""]]}, {"id": "1909.09807", "submitter": "Hongzhi Wang", "authors": "Hiba Abu Ahmad, Hongzhi Wang", "title": "Automatic Weighted Matching Rectifying Rule Discovery for Data Repairing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data repairing is a key problem in data cleaning which aims to uncover and\nrectify data errors. Traditional methods depend on data dependencies to check\nthe existence of errors in data, but they fail to rectify the errors. To\novercome this limitation, recent methods define repairing rules on which they\ndepend to detect and fix errors. However, all existing data repairing rules are\nprovided by experts which is an expensive task in time and effort. Besides,\nrule-based data repairing methods need an external verified data source or user\nverifications; otherwise they are incomplete where they can repair only a small\nnumber of errors. In this paper, we define weighted matching rectifying rules\n(WMRRs) based on similarity matching to capture more errors. We propose a novel\nalgorithm to discover WMRRs automatically from dirty data in-hand. We also\ndevelop an automatic algorithm for rules inconsistency resolution.\nAdditionally, based on WMRRs, we propose an automatic data repairing algorithm\n(WMRR-DR) which uncovers a large number of errors and rectifies them\ndependably. We experimentally verify our method on both real-life and synthetic\ndata. The experimental results prove that our method can discover effective\nWMRRs from the dirty data in-hand, and perform dependable and full-automatic\nrepairing based on the discovered WMRRs, with higher accuracy than the existing\ndependable methods.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2019 10:39:07 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Ahmad", "Hiba Abu", ""], ["Wang", "Hongzhi", ""]]}, {"id": "1909.09903", "submitter": "Jeancarlo Campos Le\\~ao", "authors": "Jeancarlo Campos Le\\~ao (1), Alberto H. F. Laender (2), Pedro O. S.\n  Vaz de Melo (2) ((1) Instituto Federal do Norte de Minas, (2) Universidade\n  Federal de Minas Gerais)", "title": "A Multi-Strategy Approach to Overcoming Bias in Community Detection\n  Evaluation", "comments": "12 pages, 6 figures, 3 tables. This paper has been submitted to the\n  34th Brazilian Symposium on Databases, 2019 (SBBD2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community detection is key to understand the structure of complex networks.\nHowever, the lack of appropriate evaluation strategies for this specific task\nmay produce biased and incorrect results that might invalidate further analyses\nor applications based on such networks. In this context, the main contribution\nof this paper is an approach that supports a robust quality evaluation when\ndetecting communities in real-world networks. In our approach, we use multiple\nstrategies that capture distinct aspects of the communities. The conclusion on\nthe quality of these communities is based on the consensus among the strategies\nadopted for the structural evaluation, as well as on the comparison with\ncommunities detected by different methods and with their existing ground\ntruths. In this way, our approach allows one to overcome biases in network\ndata, detection algorithms and evaluation metrics, thus providing more\nconsistent conclusions about the quality of the detected communities.\nExperiments conducted with several real and synthetic networks provided results\nthat show the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2019 21:36:50 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Le\u00e3o", "Jeancarlo Campos", ""], ["Laender", "Alberto H. F.", ""], ["de Melo", "Pedro O. S. Vaz", ""]]}, {"id": "1909.09908", "submitter": "Abhishek Santra", "authors": "Abhishek Santra, Kanthi Sannappa Komar, Sanjukta Bhowmick and Sharma\n  Chakravarthy", "title": "Making a Case for MLNs for Data-Driven Analysis: Modeling, Efficiency,\n  and Versatility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Datasets of real-world applications are characterized by entities of\ndifferent types, which are defined by multiple features and connected via\nvaried types of relationships. A critical challenge for these datasets is\ndeveloping models and computations to support flexible analysis, i.e., the\nability to compute varied types of analysis objectives in an efficient manner.\n  To address this problem, in this paper, we make a case for modeling such\ncomplex data sets as multilayer networks (or MLNs), and argue that MLNs provide\na more informative model than the currently popular simple and attribute\ngraphs. Through analyzing communities and hubs on homogeneous and heterogeneous\nMLNs, we demonstrate the flexibility of the chosen model. We also show that\ncompared to current analysis approaches, a network decoupling-based analysis of\nMLNs is more efficient and also preserves the structure and result semantics.\n  We use three diverse data sets to showcase the effectiveness of modeling them\nas MLNs and analyzing them using the decoupling-based approach. We use both\nhomogeneous and heterogeneous MLNs for modeling and community and hub\ncomputations for analysis. The data sets are from US commercial airlines and\nIMDb, a large international movie data set. Our experimental analysis validate\nmodeling, efficiency of computation, and versatility of the approach.\nCorrectness of results are verified using independently available ground truth.\nFor the data sets used, efficiency improvement is in the range of 64% to 98%.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2019 21:59:24 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Santra", "Abhishek", ""], ["Komar", "Kanthi Sannappa", ""], ["Bhowmick", "Sanjukta", ""], ["Chakravarthy", "Sharma", ""]]}, {"id": "1909.10152", "submitter": "Dumitrel Loghin", "authors": "Beng Chin Ooi and Gang Chen and Dumitrel Loghin and Wei Wang and\n  Meihui Zhang", "title": "5G: Agent for Further Digital Disruptive Transformations", "comments": "Published in the Bulletin of the Technical Committee on Data\n  Engineering (http://sites.computer.org/debull/A19sept/p9.pdf)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DB cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The fifth-generation (5G) mobile communication technologies are on the way to\nbe adopted as the next standard for mobile networking. It is therefore timely\nto analyze the impact of 5G on the landscape of computing, in particular, data\nmanagement and data-driven technologies. With a predicted increase of\n10-100$\\times$ in bandwidth and 5-10$\\times$ decrease in latency, 5G is\nexpected to be the main enabler for edge computing which includes accessing\ncloud-like services, as well as conducting machine learning at the edge. In\nthis paper, we examine the impact of 5G on both traditional and emerging\ntechnologies, and discuss research challenges and opportunities.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 04:36:42 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Ooi", "Beng Chin", ""], ["Chen", "Gang", ""], ["Loghin", "Dumitrel", ""], ["Wang", "Wei", ""], ["Zhang", "Meihui", ""]]}, {"id": "1909.10766", "submitter": "Rasmus Pagh", "authors": "Rasmus Pagh, Johan Sivertsen", "title": "The space complexity of inner product filters", "comments": "To appear at ICDT 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.IR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the problem of filtering candidate pairs in inner product\nsimilarity joins we study the following inner product estimation problem: Given\nparameters $d\\in {\\bf N}$, $\\alpha>\\beta\\geq 0$ and unit vectors $x,y\\in {\\bf\nR}^{d}$ consider the task of distinguishing between the cases $\\langle x,\ny\\rangle\\leq\\beta$ and $\\langle x, y\\rangle\\geq \\alpha$ where $\\langle x,\ny\\rangle = \\sum_{i=1}^d x_i y_i$ is the inner product of vectors $x$ and $y$.\nThe goal is to distinguish these cases based on information on each vector\nencoded independently in a bit string of the shortest length possible. In\ncontrast to much work on compressing vectors using randomized dimensionality\nreduction, we seek to solve the problem deterministically, with no probability\nof error. Inner product estimation can be solved in general via estimating\n$\\langle x, y\\rangle$ with an additive error bounded by $\\varepsilon = \\alpha -\n\\beta$. We show that $d \\log_2 \\left(\\tfrac{\\sqrt{1-\\beta}}{\\varepsilon}\\right)\n\\pm \\Theta(d)$ bits of information about each vector is necessary and\nsufficient. Our upper bound is constructive and improves a known upper bound of\n$d \\log_2(1/\\varepsilon) + O(d)$ by up to a factor of 2 when $\\beta$ is close\nto $1$. The lower bound holds even in a stronger model where one of the vectors\nis known exactly, and an arbitrary estimation function is allowed.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 09:02:28 GMT"}, {"version": "v2", "created": "Sun, 12 Jan 2020 11:45:14 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Pagh", "Rasmus", ""], ["Sivertsen", "Johan", ""]]}, {"id": "1909.11057", "submitter": "Mustafa Atay", "authors": "Ali Alwehaibi and Mustafa Atay", "title": "A Rule-Based Relational XML Access Control Model in the Presence of\n  Authorization Conflicts", "comments": "14th International Conference on Information Technology - New\n  Generations, Las Vegas, NV, April 10-12, 2017, Published by Springer, Cham; 6\n  pages, 5 figures, 2 tables", "journal-ref": "Advances in Intelligent Systems and Computing Information\n  Technology - New Generations, 311-319 (2017)", "doi": "10.1007/978-3-319-54978-1_43", "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is considerable amount of sensitive XML data stored in relational\ndatabases. It is a challenge to enforce node level fine-grained authorization\npolicies for XML data stored in relational databases which typically support\ntable and column level access control. Moreover, it is common to have\nconflicting authorization policies over the hierarchical nested structure of\nXML data. There are a couple of XML access control models for relational XML\ndatabases proposed in the literature. However, to our best knowledge, none of\nthem discussed handling authorization conflicts with conditions in the domain\nof relational XML databases. Therefore, we believe that there is a need to\ndefine and incorporate effective fine-grained XML authorization models with\nconflict handling mechanisms in the presence of conditions into relational XML\ndatabases. We address this issue in this study.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 17:16:40 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Alwehaibi", "Ali", ""], ["Atay", "Mustafa", ""]]}, {"id": "1909.11224", "submitter": "Weilong Ren", "authors": "Weilong Ren, Xiang Lian, Kambiz Ghazinour", "title": "Skyline Queries Over Incomplete Data Streams (Technical Report)", "comments": "26 pages, 20 figures, VLDB Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, efficient and effective processing over massive stream data has\nattracted much attention from the database community, which are useful in many\nreal applications such as sensor data monitoring, network intrusion detection,\nand so on. In practice, due to the malfunction of sensing devices or imperfect\ndata collection techniques, real-world stream data may often contain missing or\nincomplete data attributes. In this paper, we will formalize and tackle a novel\nand important problem, named skyline query over incomplete data stream\n(Sky-iDS), which retrieves skyline objects (in the presence of missing\nattributes) with high confidences from incomplete data stream. In order to\ntackle the Sky-iDS problem, we will design efficient approaches to impute\nmissing attributes of objects from incomplete data stream via differential\ndependency (DD) rules. We will propose effective pruning strategies to reduce\nthe search space of the Sky-iDS problem, devise cost-model-based index\nstructures to facilitate the data imputation and skyline computation at the\nsame time, and integrate our proposed techniques into an efficient Sky-iDS\nquery answering algorithm. Extensive experiments have been conducted to confirm\nthe efficiency and effectiveness of our Sky-iDS processing approach over both\nreal and synthetic data sets.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 23:17:29 GMT"}, {"version": "v2", "created": "Thu, 26 Sep 2019 01:23:16 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Ren", "Weilong", ""], ["Lian", "Xiang", ""], ["Ghazinour", "Kambiz", ""]]}, {"id": "1909.11369", "submitter": "M. Praveen", "authors": "Agnishom Chattopadhyay and M. Praveen", "title": "Query Preserving Watermarking Schemes for Locally Treelike Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Watermarking is a way of embedding information in digital documents. Much\nresearch has been done on techniques for watermarking relational databases and\nXML documents, where the process of embedding information shouldn't distort\nquery outputs too much. Recently, techniques have been proposed to watermark\nsome classes of relational structures preserving first-order and monadic second\norder queries. For relational structures whose Gaifman graphs have bounded\ndegree, watermarking can be done preserving first-order queries.\n  We extend this line of work and study watermarking schemes for other classes\nof structures. We prove that for relational structures whose Gaifman graphs\nbelong to a class of graphs that have locally bounded tree-width and is closed\nunder minors, watermarking schemes exist that preserve first-order queries. We\nuse previously known properties of logical formulas and graphs, and build on\nthem with some technical work to make them work in our context. This\nconstitutes a part of the first steps to understand the extent to which\ntechniques from algorithm design and computational learning theory can be\nadapted for watermarking.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 09:36:39 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Chattopadhyay", "Agnishom", ""], ["Praveen", "M.", ""]]}, {"id": "1909.11567", "submitter": "Marco Pegoraro", "authors": "Marco Pegoraro, Merih Seran Uysal and Wil M.P. van der Aalst", "title": "Discovering Process Models from Uncertain Event Data", "comments": "12 pages, 7 figures, 1 table", "journal-ref": "International Conference on Business Process Management (BPM),\n  Vienna, Austria, 2019, pp. 238-249", "doi": "10.1007/978-3-030-37453-2_20", "report-no": null, "categories": "cs.DB cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern information systems are able to collect event data in the form of\nevent logs. Process mining techniques allow to discover a model from event\ndata, to check the conformance of an event log against a reference model, and\nto perform further process-centric analyses. In this paper, we consider\nuncertain event logs, where data is recorded together with explicit uncertainty\ninformation. We describe a technique to discover a directly-follows graph from\nsuch event data which retains information about the uncertainty in the process.\nWe then present experimental results of performing inductive mining over the\ndirectly-follows graph to obtain models representing the certain and uncertain\npart of the process.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 08:42:53 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 09:29:10 GMT"}, {"version": "v3", "created": "Mon, 9 Mar 2020 15:54:38 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Pegoraro", "Marco", ""], ["Uysal", "Merih Seran", ""], ["van der Aalst", "Wil M. P.", ""]]}, {"id": "1909.11693", "submitter": "Bernardo Anibal Subercaseaux Roa", "authors": "Pablo Barcel\\'o, Nelson Higuera, Jorge P\\'erez and Bernardo\n  Subercaseaux", "title": "On the Expressiveness of LARA: A Unified Language for Linear and\n  Relational Algebra", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the expressive power of the LARA language -- a recently proposed\nunified model for expressing relational and linear algebra operations -- both\nin terms of traditional database query languages and some analytic tasks often\nperformed in machine learning pipelines. We start by showing LARA to be\nexpressive complete with respect to first-order logic with aggregation. Since\nLARA is parameterized by a set of user-defined functions which allow to\ntransform values in tables, the exact expressive power of the language depends\non how these functions are defined. We distinguish two main cases depending on\nthe level of genericity queries are enforced to satisfy. Under strong\ngenericity assumptions the language cannot express matrix convolution, a very\nimportant operation in current machine learning operations. This language is\nalso local, and thus cannot express operations such as matrix inverse that\nexhibit a recursive behavior. For expressing convolution, one can relax the\ngenericity requirement by adding an underlying linear order on the domain.\nThis, however, destroys locality and turns the expressive power of the language\nmuch more difficult to understand. In particular, although under complexity\nassumptions the resulting language can still not express matrix inverse, a\nproof of this fact without such assumptions seems challenging to obtain.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 18:20:52 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Barcel\u00f3", "Pablo", ""], ["Higuera", "Nelson", ""], ["P\u00e9rez", "Jorge", ""], ["Subercaseaux", "Bernardo", ""]]}, {"id": "1909.12102", "submitter": "Kaleb Alway", "authors": "Kaleb Alway, Eric Blais, Semih Salihoglu", "title": "Box Covers and Domain Orderings for Beyond Worst-Case Join Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent beyond worst-case optimal join algorithms Minesweeper and its\ngeneralization Tetris have brought the theory of indexing and join processing\ntogether by developing a geometric framework for joins. These algorithms take\nas input an index $\\mathcal{B}$, referred to as a box cover, that stores output\ngaps that can be inferred from traditional indexes, such as B+ trees or tries,\non the input relations. The performances of these algorithms highly depend on\nthe certificate of $\\mathcal{B}$, which is the smallest subset of gaps in\n$\\mathcal{B}$ whose union covers all of the gaps in the output space of a query\n$Q$. We study how to generate box covers that contain small size certificates\nto guarantee efficient runtimes for these algorithms. First, given a query $Q$\nover a set of relations of size $N$ and a fixed set of domain orderings for the\nattributes, we give a $\\tilde{O}(N)$-time algorithm called GAMB which generates\na box cover for $Q$ that is guaranteed to contain the smallest size certificate\nacross any box cover for $Q$. Second, we show that finding a domain ordering to\nminimize the box cover size and certificate is NP-hard through a reduction from\nthe 2 consecutive block minimization problem on boolean matrices. Our third\ncontribution is a $\\tilde{O}(N)$-time approximation algorithm called ADORA to\ncompute domain orderings, under which one can compute a box cover of size\n$\\tilde{O}(K^r)$, where $K$ is the minimum box cover for $Q$ under any domain\nordering and $r$ is the maximum arity of any relation. This guarantees\ncertificates of size $\\tilde{O}(K^r)$. We combine ADORA and GAMB with Tetris to\nform a new algorithm we call TetrisReordered, which provides several new beyond\nworst-case bounds. On infinite families of queries, TetrisReordered's runtimes\nare unboundedly better than the bounds stated in prior work.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 13:50:05 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 20:50:12 GMT"}, {"version": "v3", "created": "Sun, 10 Jan 2021 23:11:52 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Alway", "Kaleb", ""], ["Blais", "Eric", ""], ["Salihoglu", "Semih", ""]]}, {"id": "1909.12514", "submitter": "Han Liu", "authors": "Han Liu, Xianchao Zhang, Xiaotong Zhang, Qimai Li, Xiao-Ming Wu", "title": "Clustering Uncertain Data via Representative Possible Worlds with\n  Consistency Learning", "comments": "Accepted by IJCAI 2019 Workshops (AI for Internet of Things)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering uncertain data is an essential task in data mining for the\ninternet of things. Possible world based algorithms seem promising for\nclustering uncertain data. However, there are two issues in existing possible\nworld based algorithms: (1) They rely on all the possible worlds and treat them\nequally, but some marginal possible worlds may cause negative effects. (2) They\ndo not well utilize the consistency among possible worlds, since they conduct\nclustering or construct the affinity matrix on each possible world\nindependently. In this paper, we propose a representative possible world based\nconsistent clustering (RPC) algorithm for uncertain data. First, by introducing\nrepresentative loss and using Jensen-Shannon divergence as the distribution\nmeasure, we design a heuristic strategy for the selection of representative\npossible worlds, thus avoiding the negative effects caused by marginal possible\nworlds. Second, we integrate a consistency learning procedure into spectral\nclustering to deal with the representative possible worlds synergistically,\nthus utilizing the consistency to achieve better performance. Experimental\nresults show that our proposed algorithm performs better than the\nstate-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 06:36:47 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Liu", "Han", ""], ["Zhang", "Xianchao", ""], ["Zhang", "Xiaotong", ""], ["Li", "Qimai", ""], ["Wu", "Xiao-Ming", ""]]}, {"id": "1909.12656", "submitter": "Lhouari Nourine", "authors": "Lhouari Nourine and Jean Marc Petit and Simon Vilmin", "title": "Towards declarative comparabilities: application to functional\n  dependencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DB cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whether two values can be considered as equal is more subtle and complex than\nit seems. In practice, only domain experts could tell what does \"equality\"\nmean. Surprisingly, declarative frameworks allowing to specify equality at a\nhighlevel of abstraction are missing. Thus, we introduce a lattice-based\ndeclarative framework to cope with this issue.\n  First, we assign a comparability function to each attribute of the relation\nscheme.\n  This function maps each pair of the attribute's domain to a truth value in a\ntruth lattice.\n  Then, we associate a lattice of comparabilities to every relation, comparing\npairwise its tuples.\n  We define realities being {0,1}-interpretations of this lattice. Realities\nmodel several interpretations of equality. In this setting, we define abstract\nFDs, from which the semantics of classical FDs can be recovered with realities.\n  Eventually, we apply the notions of possible/certain answers in databases to\nFDs and study associated complexity and problems.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 12:45:20 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 16:26:26 GMT"}, {"version": "v3", "created": "Sat, 3 Apr 2021 17:27:35 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Nourine", "Lhouari", ""], ["Petit", "Jean Marc", ""], ["Vilmin", "Simon", ""]]}, {"id": "1909.13670", "submitter": "Se Kwon Lee", "authors": "Se Kwon Lee, Jayashree Mohan, Sanidhya Kashyap, Taesoo Kim, Vijay\n  Chidambaram", "title": "RECIPE : Converting Concurrent DRAM Indexes to Persistent-Memory Indexes", "comments": "3pages: Added one more reference", "journal-ref": null, "doi": "10.1145/3341301.3359635", "report-no": null, "categories": "cs.DC cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Recipe, a principled approach for converting concurrent DRAM\nindexes into crash-consistent indexes for persistent memory (PM). The main\ninsight behind Recipe is that isolation provided by a certain class of\nconcurrent in-memory indexes can be translated with small changes to\ncrash-consistency when the same index is used in PM. We present a set of\nconditions that enable the identification of this class of DRAM indexes, and\nthe actions to be taken to convert each index to be persistent. Based on these\nconditions and conversion actions, we modify five different DRAM indexes based\non B+ trees, tries, radix trees, and hash tables to their crash-consistent PM\ncounterparts. The effort involved in this conversion is minimal, requiring\n30-200 lines of code. We evaluated the converted PM indexes on Intel DC\nPersistent Memory, and found that they outperform state-of-the-art,\nhand-crafted PM indexes in multi-threaded workloads by up-to 5.2x. For example,\nwe built P-CLHT, our PM implementation of the CLHT hash table by modifying only\n30 LOC. When running YCSB workloads, P-CLHT performs up to 2.4x better than\nCacheline-Conscious Extendible Hashing (CCEH), the state-of-the-art PM hash\ntable.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 02:21:18 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 16:59:31 GMT"}, {"version": "v3", "created": "Wed, 6 Nov 2019 04:03:04 GMT"}, {"version": "v4", "created": "Fri, 8 Nov 2019 18:23:08 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Lee", "Se Kwon", ""], ["Mohan", "Jayashree", ""], ["Kashyap", "Sanidhya", ""], ["Kim", "Taesoo", ""], ["Chidambaram", "Vijay", ""]]}, {"id": "1909.13762", "submitter": "Maliheh Heydarpour", "authors": "Maryam Alizadeh, Maliheh Heydarpour Shahrezaei, Farajollah\n  Tahernezhad-Javazm", "title": "Ontology Based Information Integration: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An ontology makes a special vocabulary which describes the domain of interest\nand the meaning of the term on that vocabulary. Based on the precision of the\nspecification, the concept of the ontology contains several data and conceptual\nmodels. The notion of ontology has emerged into wide ranges of applications\nincluding database integration, peer-to-peer systems, e-commerce, semantic web,\netc. It can be considered as a practical tool for conceptualizing things which\nare expressed in computer format. This paper is devoted to ontology matching as\na mean or information integration. Several matching solutions have been\npresented from various areas such as databases, information systems and\nartificial intelligence. All of them take advantages of different attributes of\nontology like, structures, data instances, semantics and labels and its other\nvaluable properties. The solutions have some common techniques and cope with\nsimilar problems, but use different methods for combining and exploiting their\nresults. Information integration is among the first classes of applications at\nwhich matching was considered as a probable solution. Information integration\ncontains many fields including, data integration, schema integration, catalogue\nintegration and semantic integration. We cover these notions in term of\nontology in our proposed paper.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 23:19:51 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Alizadeh", "Maryam", ""], ["Shahrezaei", "Maliheh Heydarpour", ""], ["Tahernezhad-Javazm", "Farajollah", ""]]}]