[{"id": "0912.0071", "submitter": "Anand Sarwate", "authors": "Kamalika Chaudhuri, Claire Monteleoni, Anand D. Sarwate", "title": "Differentially Private Empirical Risk Minimization", "comments": "40 pages, 7 figures, accepted to the Journal of Machine Learning\n  Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy-preserving machine learning algorithms are crucial for the\nincreasingly common setting in which personal data, such as medical or\nfinancial records, are analyzed. We provide general techniques to produce\nprivacy-preserving approximations of classifiers learned via (regularized)\nempirical risk minimization (ERM). These algorithms are private under the\n$\\epsilon$-differential privacy definition due to Dwork et al. (2006). First we\napply the output perturbation ideas of Dwork et al. (2006), to ERM\nclassification. Then we propose a new method, objective perturbation, for\nprivacy-preserving machine learning algorithm design. This method entails\nperturbing the objective function before optimizing over classifiers. If the\nloss and regularizer satisfy certain convexity and differentiability criteria,\nwe prove theoretical results showing that our algorithms preserve privacy, and\nprovide generalization bounds for linear and nonlinear kernels. We further\npresent a privacy-preserving technique for tuning the parameters in general\nmachine learning algorithms, thereby providing end-to-end privacy guarantees\nfor the training process. We apply these results to produce privacy-preserving\nanalogues of regularized logistic regression and support vector machines. We\nobtain encouraging results from evaluating their performance on real\ndemographic and benchmark data sets. Our results show that both theoretically\nand empirically, objective perturbation is superior to the previous\nstate-of-the-art, output perturbation, in managing the inherent tradeoff\nbetween privacy and learning performance.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2009 04:35:44 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2010 19:33:44 GMT"}, {"version": "v3", "created": "Tue, 23 Feb 2010 05:26:22 GMT"}, {"version": "v4", "created": "Wed, 2 Jun 2010 23:16:36 GMT"}, {"version": "v5", "created": "Wed, 16 Feb 2011 22:35:55 GMT"}], "update_date": "2011-02-18", "authors_parsed": [["Chaudhuri", "Kamalika", ""], ["Monteleoni", "Claire", ""], ["Sarwate", "Anand D.", ""]]}, {"id": "0912.0579", "submitter": "Rdv Ijcsis", "authors": "Mohammad Ghulam Ali", "title": "A Multidatabase System as 4-Tiered Client-Server Distributed\n  Heterogeneous Database System", "comments": "5 pages IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS November 2009, ISSN 1947 5500,\n  http://sites.google.com/site/ijcsis/", "journal-ref": "International Journal of Computer Science and Information\n  Security, IJCSIS, Vol. 6, No. 2, pp. 010-014, November 2009, USA", "doi": null, "report-no": "ISSN 19475500", "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe a multidatabase system as 4tiered Client-Server\nDBMS architectures. We discuss their functional components and provide an\noverview of their performance characteristics. The first component of this\nproposed system is a web based interface or Graphical User Interface, which\nresides on top of the Client Application Program, the second component of the\nsystem is a client Application program running in an application server, which\nresides on top of the Global Database Management System, the third component of\nthe system is a Global Database Management System and global schema of the\nmultidatabase system server, which resides on top of the distributed\nheterogeneous local component database system servers, and the fourth component\nis remote heterogeneous local component database system servers. Transaction\nsubmitted from client interface to a multidatabase system server through an\napplication server will be decomposed into a set of sub queries and will be\nexecuted at various remote heterogeneous local component database servers and\nalso in case of information retrieval all sub queries will be composed and will\nget back results to the end users.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2009 05:34:56 GMT"}], "update_date": "2009-12-04", "authors_parsed": [["Ali", "Mohammad Ghulam", ""]]}, {"id": "0912.0603", "submitter": "Rdv Ijcsis", "authors": "Mohammad Ghulam Ali", "title": "Object Oriented Approach for Integration of Heterogeneous Databases in a\n  Multidatabase System and Local Schemas Modifications Propagation", "comments": "6 pages IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS November 2009, ISSN 1947 5500,\n  http://sites.google.com/site/ijcsis/", "journal-ref": "International Journal of Computer Science and Information\n  Security, IJCSIS, Vol. 6, No. 2, pp. 055-060, November 2009, USA", "doi": null, "report-no": "ISSN 1947 5500", "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the challenging problems in the multidatabase systems is to find the\nmost viable solution to the problem of interoperability of distributed\nheterogeneous autonomous local component databases. This has resulted in the\ncreation of a global schema over set of these local component database schemas\nto provide a uniform representation of local schemas. The aim of this paper is\nto use object oriented approach to integrate schemas of distributed\nheterogeneous autonomous local component database schemas into a global schema.\nThe resulting global schema provides a uniform interface and high level of\nlocation transparency for retrieval of data from the local component databases.\nA set of integration operators are defined to integrate local schemas based on\nthe semantic relevance of their classes and to provide a model independent\nrepresentation of virtual classes of the global schema. The schematic\nrepresentation and heterogeneity is also taken into account in the integration\nprocess. Justifications about Object Oriented Modal are also discussed. Bottom\nup local schema modifications propagation in Global schema is also considered\nto maintain Global schema as local schemas are autonomous and evolve over time.\nAn example illustrates the applicability of the integration operator defined.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2009 09:01:39 GMT"}], "update_date": "2009-12-04", "authors_parsed": [["Ali", "Mohammad Ghulam", ""]]}, {"id": "0912.0840", "submitter": "Benjamin Nguyen", "authors": "Benjamin Nguyen, Antoine Vion, Francois-Xavier Dudouet, Loic\n  Saint-Ghislain", "title": "Applying an XML Warehouse to Social Network Analysis, Lessons from the\n  WebStand Project", "comments": "W3C Workshop on the Future of Social Networking", "journal-ref": "W3C Workshop on the Future of Social Networking, electronic\n  proceedings available at http://www.w3.org/2008/09/msnws/ Barcelona, Spain,\n  2009", "doi": null, "report-no": null, "categories": "cs.DB cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present the state of advancement of the French ANR WebStand\nproject. The objective of this project is to construct a customizable XML based\nwarehouse platform to acquire, transform, analyze, store, query and export data\nfrom the web, in particular mailing lists, with the final intension of using\nthis data to perform sociological studies focused on social groups of World\nWide Web, with a specific emphasis on the temporal aspects of this data. We are\ncurrently using this system to analyze the standardization process of the W3C,\nthrough its social network of standard setters.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2009 13:04:44 GMT"}], "update_date": "2009-12-07", "authors_parsed": [["Nguyen", "Benjamin", ""], ["Vion", "Antoine", ""], ["Dudouet", "Francois-Xavier", ""], ["Saint-Ghislain", "Loic", ""]]}, {"id": "0912.1016", "submitter": "Rdv Ijcsis", "authors": "Ayeesha Dsousa, Shalini Bhatia", "title": "Refactoring of a Database", "comments": "9 pages IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS November 2009, ISSN 1947 5500,\n  http://sites.google.com/site/ijcsis/", "journal-ref": "International Journal of Computer Science and Information\n  Security, IJCSIS, Vol. 6, No. 2, pp. 307-315, November 2009, USA", "doi": null, "report-no": "ISSN 1947 5500", "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The technique of database refactoring is all about applying disciplined and\ncontrolled techniques to change an existing database schema. The problem is to\nsuccessfully create a Database Refactoring Framework for databases. This paper\nconcentrates on the feasibility of adapting this concept to work as a generic\ntemplate. To retain the constraints regardless of the modifications to the\nmetadata, the paper proposes a MetaData Manipulation Tool to facilitate change.\nThe tool adopts a Template Design Pattern to make it database independent. The\npaper presents a drawback of using java for constraint extraction and proposes\nan alternative.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2009 13:21:57 GMT"}], "update_date": "2009-12-08", "authors_parsed": [["Dsousa", "Ayeesha", ""], ["Bhatia", "Shalini", ""]]}, {"id": "0912.1110", "submitter": "Boris Verhaegen", "authors": "Serge Boucher, Boris Verhaegen, Esteban Zim\\'anyi", "title": "XML Multidimensional Modelling and Querying", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As XML becomes ubiquitous and XML storage and processing becomes more\nefficient, the range of use cases for these technologies widens daily. One\npromising area is the integration of XML and data warehouses, where an\nXML-native database stores multidimensional data and processes OLAP queries\nwritten in the XQuery interrogation language. This paper explores issues\narising in the implementation of such a data warehouse. We first compare\napproaches for multidimensional data modelling in XML, then describe how\ntypical OLAP queries on these models can be expressed in XQuery. We then show\nhow, regardless of the model, the grouping features of XQuery 1.1 improve\nperformance and readability of these queries. Finally, we evaluate the\nperformance of query evaluation in each modelling choice using the eXist\ndatabase, which we extended with a grouping clause implementation.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2009 15:26:43 GMT"}], "update_date": "2009-12-08", "authors_parsed": [["Boucher", "Serge", ""], ["Verhaegen", "Boris", ""], ["Zim\u00e1nyi", "Esteban", ""]]}, {"id": "0912.2134", "submitter": "Emil Vassev Dr.", "authors": "Emil Vassev", "title": "Enterprise Multi-Branch Database Synchronization with MSMQ", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When we talk about databases there have always been problems concerning data\nsynchronization. The latter is a technique for maintaining consistency among\ndifferent copies of data (often called replicas). In general, there is no\nuniversal solution to this problem and often a particular situation requires a\nparticular approach driven by specific conditions. This paper presents an\napproach tackling the issue of data synchronization in a distributed\nmulti-branch enterprise database. The proposed solution is based on MSMQ\n(Microsoft Message Queue), a mechanism for asynchronous messaging.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2009 00:58:32 GMT"}], "update_date": "2009-12-14", "authors_parsed": [["Vassev", "Emil", ""]]}, {"id": "0912.2282", "submitter": "Kadirvelu SivaKumar", "authors": "Mrs. Neelu Nihalani, Dr. Sanjay Silakari and Dr. Mahesh Motwani", "title": "Design of Intelligent layer for flexible querying in databases", "comments": null, "journal-ref": "IJCSE Volume 1 Issue 2 2009 30-39", "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer-based information technologies have been extensively used to help\nmany organizations, private companies, and academic and education institutions\nmanage their processes and information systems hereby become their nervous\ncentre. The explosion of massive data sets created by businesses, science and\ngovernments necessitates intelligent and more powerful computing paradigms so\nthat users can benefit from this data. Therefore most new-generation database\napplications demand intelligent information management to enhance efficient\ninteractions between database and the users. Database systems support only a\nBoolean query model. A selection query on SQL database returns all those tuples\nthat satisfy the conditions in the query.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2009 17:04:01 GMT"}], "update_date": "2009-12-14", "authors_parsed": [["Nihalani", "Mrs. Neelu", ""], ["Silakari", "Dr. Sanjay", ""], ["Motwani", "Dr. Mahesh", ""]]}, {"id": "0912.2307", "submitter": "Kadirvelu SivaKumar", "authors": "Jayanthi Manicassamy, P. Dhavachelvan", "title": "Rank Based Clustering For Document Retrieval From Biomedical Databases", "comments": null, "journal-ref": "IJCSE Volume 1 Issue 2 2009 111-115", "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Now a day's, search engines are been most widely used for extracting\ninformation's from various resources throughout the world. Where, majority of\nsearches lies in the field of biomedical for retrieving related documents from\nvarious biomedical databases. Currently search engines lacks in document\nclustering and representing relativeness level of documents extracted from the\ndatabases. In order to overcome these pitfalls a text based search engine have\nbeen developed for retrieving documents from Medline and PubMed biomedical\ndatabases. The search engine has incorporated page ranking bases clustering\nconcept which automatically represents relativeness on clustering bases. Apart\nfrom this graph tree construction is made for representing the level of\nrelatedness of the documents that are networked together. This advance\nfunctionality incorporation for biomedical document based search engine found\nto provide better results in reviewing related documents based on relativeness.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2009 18:22:54 GMT"}], "update_date": "2009-12-14", "authors_parsed": [["Manicassamy", "Jayanthi", ""], ["Dhavachelvan", "P.", ""]]}, {"id": "0912.2404", "submitter": "Shaddin Dughmi", "authors": "Ioannis Antonellis, Anish Das Sarma, Shaddin Dughmi", "title": "Succinct Coverage Oracles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we identify a fundamental algorithmic problem that we term\nsuccinct dynamic covering (SDC), arising in many modern-day web applications,\nincluding ad-serving and online recommendation systems in eBay and Netflix.\nRoughly speaking, SDC applies two restrictions to the well-studied Max-Coverage\nproblem: Given an integer k, X={1,2,...,n} and I={S_1, ..., S_m}, S_i a subset\nof X, find a subset J of I, such that |J| <= k and the union of S in J is as\nlarge as possible. The two restrictions applied by SDC are: (1) Dynamic: At\nquery-time, we are given a query Q, a subset of X, and our goal is to find J\nsuch that the intersection of Q with the union of S in J is as large as\npossible; (2) Space-constrained: We don't have enough space to store (and\nprocess) the entire input; specifically, we have o(mn), and maybe as little as\nO((m+n)polylog(mn)) space. The goal of SDC is to maintain a small data\nstructure so as to answer most dynamic queries with high accuracy. We call such\na scheme a Coverage Oracle.\n  We present algorithms and complexity results for coverage oracles. We present\ndeterministic and probabilistic near-tight upper and lower bounds on the\napproximation ratio of SDC as a function of the amount of space available to\nthe oracle. Our lower bound results show that to obtain constant-factor\napproximations we need Omega(mn) space. Fortunately, our upper bounds present\nan explicit tradeoff between space and approximation ratio, allowing us to\ndetermine the amount of space needed to guarantee certain accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2009 07:36:52 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2010 06:12:46 GMT"}], "update_date": "2010-04-09", "authors_parsed": [["Antonellis", "Ioannis", ""], ["Sarma", "Anish Das", ""], ["Dughmi", "Shaddin", ""]]}, {"id": "0912.2548", "submitter": "Grigorios Loukides", "authors": "Grigorios Loukides, Aris Gkoulalas-Divanis and Bradley Malin", "title": "Towards Utility-driven Anonymization of Transactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Publishing person-specific transactions in an anonymous form is increasingly\nrequired by organizations. Recent approaches ensure that potentially\nidentifying information (e.g., a set of diagnosis codes) cannot be used to link\npublished transactions to persons' identities, but all are limited in\napplication because they incorporate coarse privacy requirements (e.g.,\nprotecting a certain set of m diagnosis codes requires protecting all m-sized\nsets), do not integrate utility requirements, and tend to explore a small\nportion of the solution space. In this paper, we propose a more general\nframework for anonymizing transactional data under specific privacy and utility\nrequirements. We model such requirements as constraints, investigate how these\nconstraints can be specified, and propose COAT (COnstraint-based Anonymization\nof Transactions), an algorithm that anonymizes transactions using a flexible\nhierarchy-free generalization scheme to meet the specified constraints.\nExperiments with benchmark datasets verify that COAT significantly outperforms\nthe current state-of-the-art algorithm in terms of data utility, while being\ncomparable in terms of efficiency. The effectiveness of our approach is also\ndemonstrated in a real-world scenario, which requires disseminating a private,\npatient-specific transactional dataset in a way that preserves both privacy and\nutility in intended studies.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2009 23:30:24 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2010 05:26:00 GMT"}], "update_date": "2010-01-26", "authors_parsed": [["Loukides", "Grigorios", ""], ["Gkoulalas-Divanis", "Aris", ""], ["Malin", "Bradley", ""]]}, {"id": "0912.2822", "submitter": "Gerhard Mayer", "authors": "Gerhard Mayer", "title": "Data management in Systems biology II - Outlook towards the semantic web", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB q-bio.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The benefit of using ontologies, defined by the respective data standards, is\nshown. It is presented how ontologies can be used for the semantic enrichment\nof data and how this can contribute to the vision of the semantic web to become\ntrue. The problems existing today on the way to a true semantic web are\npinpointed, different semantic web standards, tools and development frameworks\nare overlooked and an outlook towards artificial intelligence and agents for\nsearching and mining the data in the semantic web are given, paving the way\nfrom data management to information and in the end true knowledge management\nsystems.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2009 08:55:12 GMT"}], "update_date": "2009-12-16", "authors_parsed": [["Mayer", "Gerhard", ""]]}, {"id": "0912.3924", "submitter": "William Jackson", "authors": "M. Ramaswami, R. Bhaskaran", "title": "A Study on Feature Selection Techniques in Educational Data Mining", "comments": null, "journal-ref": "Journal of Computing, Volume 1, Issue 1, pp 7-11, December 2009", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Educational data mining (EDM) is a new growing research area and the essence\nof data mining concepts are used in the educational field for the purpose of\nextracting useful information on the behaviors of students in the learning\nprocess. In this EDM, feature selection is to be made for the generation of\nsubset of candidate variables. As the feature selection influences the\npredictive accuracy of any performance model, it is essential to study\nelaborately the effectiveness of student performance model in connection with\nfeature selection techniques. In this connection, the present study is devoted\nnot only to investigate the most relevant subset features with minimum\ncardinality for achieving high predictive performance by adopting various\nfiltered feature selection techniques in data mining but also to evaluate the\ngoodness of subsets with different cardinalities and the quality of six\nfiltered feature selection algorithms in terms of F-measure value and Receiver\nOperating Characteristics (ROC) value, generated by the NaiveBayes algorithm as\nbase-line classifier method. The comparative study carried out by us on six\nfilter feature section algorithms reveals the best method, as well as optimal\ndimensionality of the feature subset. Benchmarking of filter feature selection\nmethod is subsequently carried out by deploying different classifier models.\nThe result of the present study effectively supports the well known fact of\nincrease in the predictive accuracy with the existence of minimum number of\nfeatures. The expected outcomes show a reduction in computational time and\nconstructional cost in both training and classification phases of the student\nperformance model.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2009 18:51:40 GMT"}], "update_date": "2009-12-22", "authors_parsed": [["Ramaswami", "M.", ""], ["Bhaskaran", "R.", ""]]}, {"id": "0912.4742", "submitter": "Chao Li", "authors": "Chao Li, Michael Hay, Vibhor Rastogi, Gerome Miklau, Andrew McGregor", "title": "Optimizing Histogram Queries under Differential Privacy", "comments": "22 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy is a robust privacy standard that has been successfully\napplied to a range of data analysis tasks. Despite much recent work, optimal\nstrategies for answering a collection of correlated queries are not known.\n  We study the problem of devising a set of strategy queries, to be submitted\nand answered privately, that will support the answers to a given workload of\nqueries. We propose a general framework in which query strategies are formed\nfrom linear combinations of counting queries, and we describe an optimal method\nfor deriving new query answers from the answers to the strategy queries. Using\nthis framework we characterize the error of strategies geometrically, and we\npropose solutions to the problem of finding optimal strategies.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2009 21:22:50 GMT"}, {"version": "v2", "created": "Mon, 6 Sep 2010 13:24:19 GMT"}], "update_date": "2010-09-07", "authors_parsed": [["Li", "Chao", ""], ["Hay", "Michael", ""], ["Rastogi", "Vibhor", ""], ["Miklau", "Gerome", ""], ["McGregor", "Andrew", ""]]}, {"id": "0912.5241", "submitter": "Wolfgang Gatterbauer", "authors": "Wolfgang Gatterbauer, Magdalena Balazinska, Nodira Khoussainova, Dan\n  Suciu", "title": "Believe It or Not: Adding Belief Annotations to Databases", "comments": "17 pages, 10 figures", "journal-ref": "Full version of: VLDB 2009 conference version; PVLDB 2(1):1-12\n  (2009)", "doi": null, "report-no": "University of Washington CSE Technical Report 08-12-01", "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a database model that allows users to annotate data with belief\nstatements. Our motivation comes from scientific database applications where a\ncommunity of users is working together to assemble, revise, and curate a shared\ndata repository. As the community accumulates knowledge and the database\ncontent evolves over time, it may contain conflicting information and members\ncan disagree on the information it should store. For example, Alice may believe\nthat a tuple should be in the database, whereas Bob disagrees. He may also\ninsert the reason why he thinks Alice believes the tuple should be in the\ndatabase, and explain what he thinks the correct tuple should be instead.\n  We propose a formal model for Belief Databases that interprets users'\nannotations as belief statements. These annotations can refer both to the base\ndata and to other annotations. We give a formal semantics based on a fragment\nof multi-agent epistemic logic and define a query language over belief\ndatabases. We then prove a key technical result, stating that every belief\ndatabase can be encoded as a canonical Kripke structure. We use this structure\nto describe a relational representation of belief databases, and give an\nalgorithm for translating queries over the belief database into standard\nrelational queries. Finally, we report early experimental results with our\nprototype implementation on synthetic data.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2009 18:36:21 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Gatterbauer", "Wolfgang", ""], ["Balazinska", "Magdalena", ""], ["Khoussainova", "Nodira", ""], ["Suciu", "Dan", ""]]}, {"id": "0912.5340", "submitter": "Wolfgang Gatterbauer", "authors": "Alexandra Meliou, Wolfgang Gatterbauer, Katherine F. Moore, Dan Suciu", "title": "Why so? or Why no? Functional Causality for Explaining Query Answers", "comments": "18 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": "University of Washington CSE Technical Report 09-12-01", "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose causality as a unified framework to explain query\nanswers and non-answers, thus generalizing and extending several previously\nproposed approaches of provenance and missing query result explanations.\n  We develop our framework starting from the well-studied definition of actual\ncauses by Halpern and Pearl. After identifying some undesirable characteristics\nof the original definition, we propose functional causes as a refined\ndefinition of causality with several desirable properties. These properties\nallow us to apply our notion of causality in a database context and apply it\nuniformly to define the causes of query results and their individual\ncontributions in several ways: (i) we can model both provenance as well as\nnon-answers, (ii) we can define explanations as either data in the input\nrelations or relational operations in a query plan, and (iii) we can give\ngraded degrees of responsibility to individual causes, thus allowing us to rank\ncauses. In particular, our approach allows us to explain contributions to\nrelational aggregate functions and to rank causes according to their respective\nresponsibilities. We give complexity results and describe polynomial algorithms\nfor evaluating causality in tractable cases. Throughout the paper, we\nillustrate the applicability of our framework with several examples.\n  Overall, we develop in this paper the theoretical foundations of causality\ntheory in a database context.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2009 05:56:22 GMT"}], "update_date": "2009-12-31", "authors_parsed": [["Meliou", "Alexandra", ""], ["Gatterbauer", "Wolfgang", ""], ["Moore", "Katherine F.", ""], ["Suciu", "Dan", ""]]}, {"id": "0912.5426", "submitter": "Xiaokui Xiao", "authors": "Xiaokui Xiao, Ke Yi, Yufei Tao", "title": "The Hardness and Approximation Algorithms for L-Diversity", "comments": "EDBT 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existing solutions to privacy preserving publication can be classified\ninto the theoretical and heuristic categories. The former guarantees provably\nlow information loss, whereas the latter incurs gigantic loss in the worst\ncase, but is shown empirically to perform well on many real inputs. While\nnumerous heuristic algorithms have been developed to satisfy advanced privacy\nprinciples such as l-diversity, t-closeness, etc., the theoretical category is\ncurrently limited to k-anonymity which is the earliest principle known to have\nsevere vulnerability to privacy attacks. Motivated by this, we present the\nfirst theoretical study on l-diversity, a popular principle that is widely\nadopted in the literature. First, we show that optimal l-diverse generalization\nis NP-hard even when there are only 3 distinct sensitive values in the\nmicrodata. Then, an (l*d)-approximation algorithm is developed, where d is the\ndimensionality of the underlying dataset. This is the first known algorithm\nwith a non-trivial bound on information loss. Extensive experiments with real\ndatasets validate the effectiveness and efficiency of proposed solution.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2009 08:31:10 GMT"}], "update_date": "2009-12-31", "authors_parsed": [["Xiao", "Xiaokui", ""], ["Yi", "Ke", ""], ["Tao", "Yufei", ""]]}]