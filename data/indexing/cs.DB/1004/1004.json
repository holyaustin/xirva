[{"id": "1004.0048", "submitter": "Sudipto Das", "authors": "Sudipto Das, Omer Egecioglu and Amr El Abbadi", "title": "Anonimos: An LP based Approach for Anonymizing Weighted Social Network\n  Graphs", "comments": "15 pages.", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing popularity of social networks has initiated a fertile research\narea in information extraction and data mining. Anonymization of these social\ngraphs is important to facilitate publishing these data sets for analysis by\nexternal entities. Prior work has concentrated mostly on node identity\nanonymization and structural anonymization. But with the growing interest in\nanalyzing social networks as a weighted network, edge weight anonymization is\nalso gaining importance. We present An\\'onimos, a Linear Programming based\ntechnique for anonymization of edge weights that preserves linear properties of\ngraphs. Such properties form the foundation of many important graph-theoretic\nalgorithms such as shortest paths problem, k-nearest neighbors, minimum cost\nspanning tree, and maximizing information spread. As a proof of concept, we\napply An\\'onimos to the shortest paths problem and its extensions, prove the\ncorrectness, analyze complexity, and experimentally evaluate it using real\nsocial network data sets. Our experiments demonstrate that An\\'onimos\nanonymizes the weights, improves k-anonymity of the weights, and also scrambles\nthe relative ordering of the edges sorted by weights, thereby providing robust\nand effective anonymization of the sensitive edge-weights. Additionally, we\ndemonstrate the composability of different models generated using An\\'onimos, a\nproperty that allows a single anonymized graph to preserve multiple linear\nproperties.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2010 03:39:33 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Das", "Sudipto", ""], ["Egecioglu", "Omer", ""], ["Abbadi", "Amr El", ""]]}, {"id": "1004.1001", "submitter": "Marko A. Rodriguez", "authors": "Marko A. Rodriguez and Peter Neubauer", "title": "The Graph Traversal Pattern", "comments": null, "journal-ref": "chapter in Graph Data Management: Techniques and Applications,\n  eds. S. Sakr, E. Pardede, 2011", "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  A graph is a structure composed of a set of vertices (i.e.nodes, dots)\nconnected to one another by a set of edges (i.e.links, lines). The concept of a\ngraph has been around since the late 19$^\\text{th}$ century, however, only in\nrecent decades has there been a strong resurgence in both theoretical and\napplied graph research in mathematics, physics, and computer science. In\napplied computing, since the late 1960s, the interlinked table structure of the\nrelational database has been the predominant information storage and retrieval\nmodel. With the growth of graph/network-based data and the need to efficiently\nprocess such data, new data management systems have been developed. In contrast\nto the index-intensive, set-theoretic operations of relational databases, graph\ndatabases make use of index-free, local traversals. This article discusses the\ngraph traversal pattern and its use in computing.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2010 05:12:27 GMT"}], "update_date": "2010-12-24", "authors_parsed": [["Rodriguez", "Marko A.", ""], ["Neubauer", "Peter", ""]]}, {"id": "1004.1229", "submitter": "Rdv Ijcsis", "authors": "Dr. P. AnandhaKumar, V. Balamurugan", "title": "Feature-Based Adaptive Tolerance Tree (FATT): An Efficient Indexing\n  Technique for Content-Based Image Retrieval Using Wavelet Transform", "comments": "IEEE Publication format, ISSN 1947 5500,\n  http://sites.google.com/site/ijcsis/", "journal-ref": "IJCSIS, Vol. 7 No. 3, March 2010,", "doi": null, "report-no": null, "categories": "cs.MM cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper introduces a novel indexing and access method, called Feature-\nBased Adaptive Tolerance Tree (FATT), using wavelet transform is proposed to\norganize large image data sets efficiently and to support popular image access\nmechanisms like Content Based Image Retrieval (CBIR).Conventional database\nsystems are designed for managing textual and numerical data and retrieving\nsuch data is often based on simple comparisons of text or numerical values.\nHowever, this method is no longer adequate for images, since the digital\npresentation of images does not convey the reality of images. Retrieval of\nimages become difficult when the database is very large. This paper addresses\nsuch problems and presents a novel indexing technique, Feature Based Adaptive\nTolerance Tree (FATT), which is designed to bring an effective solution\nespecially for indexing large databases. The proposed indexing scheme is then\nused along with a query by image content, in order to achieve the ultimate goal\nfrom the user point of view that is retrieval of all relevant images. FATT\nindexing technique, features of the image is extracted using 2-dimensional\ndiscrete wavelet transform (2DDWT) and index code is generated from the\ndeterminant value of the features. Multiresolution analysis technique using\n2D-DWT can decompose the image into components at different scales, so that the\ncoarest scale components carry the global approximation information while the\nfiner scale components contain the detailed information. Experimental results\nshow that the FATT outperforms M-tree upto 200%, Slim-tree up to 120% and HCT\nupto 89%. FATT indexing technique is adopted to increase the efficiently of\ndata storage and retrieval.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2010 02:56:58 GMT"}], "update_date": "2010-04-09", "authors_parsed": [["AnandhaKumar", "Dr. P.", ""], ["Balamurugan", "V.", ""]]}, {"id": "1004.1249", "submitter": "Karl Schnaitter", "authors": "Karl Schnaitter and Neoklis Polyzotis", "title": "Semi-Automatic Index Tuning: Keeping DBAs in the Loop", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To obtain good system performance, a DBA must choose a set of indices that is\nappropriate for the workload. The system can aid in this challenging task by\nproviding recommendations for the index configuration. We propose a new index\nrecommendation technique, termed semi-automatic tuning, that keeps the DBA \"in\nthe loop\" by generating recommendations that use feedback about the DBA's\npreferences. The technique also works online, which avoids the limitations of\ncommercial tools that require the workload to be known in advance. The\nfoundation of our approach is the Work Function Algorithm, which can solve a\nwide variety of online optimization problems with strong competitive\nguarantees. We present an experimental analysis that validates the benefits of\nsemi-automatic tuning in a wide variety of conditions.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2010 06:10:11 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2010 18:11:34 GMT"}, {"version": "v3", "created": "Fri, 7 May 2010 08:16:59 GMT"}, {"version": "v4", "created": "Mon, 1 Nov 2010 03:58:24 GMT"}, {"version": "v5", "created": "Wed, 1 Jun 2011 07:34:22 GMT"}, {"version": "v6", "created": "Sun, 30 Oct 2011 21:02:23 GMT"}], "update_date": "2015-03-14", "authors_parsed": [["Schnaitter", "Karl", ""], ["Polyzotis", "Neoklis", ""]]}, {"id": "1004.1569", "submitter": "Gokarna Sharma", "authors": "Gokarna Sharma, Costas Busch, Srikanta Tirthapura", "title": "A Streaming Approximation Algorithm for Klee's Measure Problem", "comments": "This paper has been withdrawn by the author due to a small technical\n  error in Algorithm 3 and 4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The efficient estimation of frequency moments of a data stream in one-pass\nusing limited space and time per item is one of the most fundamental problem in\ndata stream processing. An especially important estimation is to find the\nnumber of distinct elements in a data stream, which is generally referred to as\nthe zeroth frequency moment and denoted by $F_0$. In this paper, we consider\nstreams of rectangles defined over a discrete space and the task is to compute\nthe total number of distinct points covered by the rectangles. This is known as\nthe Klee's measure problem in 2 dimensions. We present and analyze a randomized\nstreaming approximation algorithm which gives an $(\\epsilon,\n\\delta)$-approximation of $F_0$ for the total area of Klee's measure problem in\n2 dimensions. Our algorithm achieves the following complexity bounds: (a) the\namortized processing time per rectangle is $O(\\frac{1}{\\epsilon^4}\\log^3\nn\\log\\frac{1}{\\delta})$; (b) the space complexity is\n$O(\\frac{1}{\\epsilon^2}\\log n \\log\\frac{1}{\\delta})$ bits; and (c) the time to\nanswer a query for $F_0$ is $O(\\log\\frac{1}{\\delta})$, respectively. To our\nknowledge, this is the first streaming approximation for the Klee's measure\nproblem that achieves sub-polynomial bounds.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2010 14:59:08 GMT"}, {"version": "v2", "created": "Thu, 28 Oct 2010 18:34:22 GMT"}], "update_date": "2010-10-29", "authors_parsed": [["Sharma", "Gokarna", ""], ["Busch", "Costas", ""], ["Tirthapura", "Srikanta", ""]]}, {"id": "1004.1614", "submitter": "Anish Das Sarma", "authors": "Anish Das Sarma and Alpa Jain and Philip Bohannon", "title": "PROBER: Ad-Hoc Debugging of Extraction and Integration Pipelines", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex information extraction (IE) pipelines assembled by plumbing together\noff-the-shelf operators, specially customized operators, and operators re-used\nfrom other text processing pipelines are becoming an integral component of most\ntext processing frameworks. A critical task faced by the IE pipeline user is to\nrun a post-mortem analysis on the output. Due to the diverse nature of\nextraction operators (often implemented by independent groups), it is time\nconsuming and error-prone to describe operator semantics formally or\noperationally to a provenance system. We introduce the first system that helps\nIE users analyze pipeline semantics and infer provenance interactively while\ndebugging. This allows the effort to be proportional to the need, and to focus\non the portions of the pipeline under the greatest suspicion. We present a\ngeneric debugger for running post-execution analysis of any IE pipeline\nconsisting of arbitrary types of operators. We propose an effective provenance\nmodel for IE pipelines which captures a variety of operator types, ranging from\nthose for which full or no specifications are available. We present a suite of\nalgorithms to effectively build provenance and facilitate debugging. Finally,\nwe present an extensive experimental study on large-scale real-world\nextractions from an index of ~500 million Web documents.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2010 17:33:37 GMT"}], "update_date": "2010-04-12", "authors_parsed": [["Sarma", "Anish Das", ""], ["Jain", "Alpa", ""], ["Bohannon", "Philip", ""]]}, {"id": "1004.1677", "submitter": "Rdv Ijcsis", "authors": "J. Arokia Renjit, K. L. Shunmuganathan", "title": "Mining The Data From Distributed Database Using An Improved Mining\n  Algorithm", "comments": "IEEE Publication format, International Journal of Computer Science\n  and Information Security, IJCSIS, Vol. 7 No. 3, March 2010, USA. ISSN 1947\n  5500, http://sites.google.com/site/ijcsis/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Association rule mining is an active data mining research area and most ARM\nalgorithms cater to a centralized environment. Centralized data mining to\ndiscover useful patterns in distributed databases isn't always feasible because\nmerging data sets from different sites incurs huge network communication costs.\nIn this paper, an Improved algorithm based on good performance level for data\nmining is being proposed. In local sites, it runs the application based on the\nimproved LMatrix algorithm, which is used to calculate local support counts.\nLocal Site also finds a centre site to manage every message exchanged to obtain\nall globally frequent item sets. It also reduces the time of scan of partition\ndatabase by using LMatrix which increases the performance of the algorithm.\nTherefore, the research is to develop a distributed algorithm for\ngeographically distributed data sets that reduces communication costs, superior\nrunning efficiency, and stronger scalability than direct application of a\nsequential algorithm in distributed databases.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2010 03:46:01 GMT"}], "update_date": "2010-04-13", "authors_parsed": [["Renjit", "J. Arokia", ""], ["Shunmuganathan", "K. L.", ""]]}, {"id": "1004.1747", "submitter": "Rdv Ijcsis", "authors": "Samidha Dwivedi Sharma and Dr. R. S. Kasana", "title": "Mobile Database System: Role of Mobility on the Query Processing", "comments": "IEEE Publication format, ISSN 1947 5500,\n  http://sites.google.com/site/ijcsis/", "journal-ref": "IJCSIS, Vol. 7 No. 3, March 2010, 211-216", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The rapidly expanding technology of mobile communication will give mobile\nusers capability of accessing information from anywhere and any time. The\nwireless technology has made it possible to achieve continuous connectivity in\nmobile environment. When the query is specified as continuous, the requesting\nmobile user can obtain continuously changing result. In order to provide\naccurate and timely outcome to requesting mobile user, the locations of moving\nobject has to be closely monitored. The objective of paper is to discuss the\nproblem related to the role of personal and terminal mobility and query\nprocessing in the mobile environment.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2010 22:23:27 GMT"}], "update_date": "2010-04-13", "authors_parsed": [["Sharma", "Samidha Dwivedi", ""], ["Kasana", "Dr. R. S.", ""]]}, {"id": "1004.2155", "submitter": "Ahmad Kamran Malik", "authors": "Ahmad Kamran Malik, Muhammad Abdul Qadir, Nadeem Iftikhar, and\n  Muhammad Usman", "title": "Constraint-based Query Distribution Framework for an Integrated Global\n  Schema", "comments": "The Proceedings of the 13th INMIC 2009), Dec. 14-15, 2009, Islamabad,\n  Pakistan. Pages 1 - 6 Print ISBN: 978-1-4244-4872-2 INSPEC Accession Number:\n  11072575 Date of Current Version : 15 January 2010", "journal-ref": null, "doi": "10.1109/INMIC.2009.5383089", "report-no": null, "categories": "cs.DB cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed heterogeneous data sources need to be queried uniformly using\nglobal schema. Query on global schema is reformulated so that it can be\nexecuted on local data sources. Constraints in global schema and mappings are\nused for source selection, query optimization,and querying partitioned and\nreplicated data sources. The provided system is all XML-based which poses query\nin XML form, transforms, and integrates local results in an XML document.\nContributions include the use of constraints in our existing global schema\nwhich help in source selection and query optimization, and a global query\ndistribution framework for querying distributed heterogeneous data sources.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2010 11:41:35 GMT"}], "update_date": "2010-04-14", "authors_parsed": [["Malik", "Ahmad Kamran", ""], ["Qadir", "Muhammad Abdul", ""], ["Iftikhar", "Nadeem", ""], ["Usman", "Muhammad", ""]]}, {"id": "1004.2372", "submitter": "Stijn Vansummeren", "authors": "Geert Jan Bex, Wouter Gelade, Frank Neven and Stijn Vansummeren", "title": "Learning Deterministic Regular Expressions for the Inference of Schemas\n  from XML Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring an appropriate DTD or XML Schema Definition (XSD) for a given\ncollection of XML documents essentially reduces to learning deterministic\nregular expressions from sets of positive example words. Unfortunately, there\nis no algorithm capable of learning the complete class of deterministic regular\nexpressions from positive examples only, as we will show. The regular\nexpressions occurring in practical DTDs and XSDs, however, are such that every\nalphabet symbol occurs only a small number of times. As such, in practice it\nsuffices to learn the subclass of deterministic regular expressions in which\neach alphabet symbol occurs at most k times, for some small k. We refer to such\nexpressions as k-occurrence regular expressions (k-OREs for short). Motivated\nby this observation, we provide a probabilistic algorithm that learns k-OREs\nfor increasing values of k, and selects the deterministic one that best\ndescribes the sample based on a Minimum Description Length argument. The\neffectiveness of the method is empirically validated both on real world and\nsynthetic data. Furthermore, the method is shown to be conservative over the\nsimpler classes of expressions considered in previous work.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2010 10:58:42 GMT"}], "update_date": "2010-04-15", "authors_parsed": [["Bex", "Geert Jan", ""], ["Gelade", "Wouter", ""], ["Neven", "Frank", ""], ["Vansummeren", "Stijn", ""]]}, {"id": "1004.3272", "submitter": "Vishal Goyal", "authors": "Nattapon Pannurat, Nittaya Kerdprasop, Kittisak Kerdprasop", "title": "Database Reverse Engineering based on Association Rule Mining", "comments": "International Journal of Computer Science Issues online at\n  http://ijcsi.org/articles/Database-Reverse-Engineering-based-on-Association-Rule-Mining.php", "journal-ref": "IJCSI, Volume 7, Issue 2, March 2010", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maintaining a legacy database is a difficult task especially when system\ndocumentation is poor written or even missing. Database reverse engineering is\nan attempt to recover high-level conceptual design from the existing database\ninstances. In this paper, we propose a technique to discover conceptual schema\nusing the association mining technique. The discovered schema corresponds to\nthe normalization at the third normal form, which is a common practice in many\nbusiness organizations. Our algorithm also includes the rule filtering\nheuristic to solve the problem of exponential growth of discovered rules\ninherited with the association mining technique.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2010 18:18:43 GMT"}], "update_date": "2010-04-20", "authors_parsed": [["Pannurat", "Nattapon", ""], ["Kerdprasop", "Nittaya", ""], ["Kerdprasop", "Kittisak", ""]]}, {"id": "1004.3565", "submitter": "Vishal Goyal", "authors": "P.Velvadivu, K.Duraisamy", "title": "An Optimized Weighted Association Rule Mining On Dynamic Content", "comments": "International Journal of Computer Science Issues online at\n  http://ijcsi.org/articles/An-Optimized-Weighted-Association-Rule-Mining-On-Dynamic-Content.php", "journal-ref": "IJCSI, Volume 7, Issue 2, March 2010", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Association rule mining aims to explore large transaction databases for\nassociation rules. Classical Association Rule Mining (ARM) model assumes that\nall items have the same significance without taking their weight into account.\nIt also ignores the difference between the transactions and importance of each\nand every itemsets. But, the Weighted Association Rule Mining (WARM) does not\nwork on databases with only binary attributes. It makes use of the importance\nof each itemset and transaction. WARM requires each item to be given weight to\nreflect their importance to the user. The weights may correspond to special\npromotions on some products, or the profitability of different items. This\nresearch work first focused on a weight assignment based on a directed graph\nwhere nodes denote items and links represent association rules. A generalized\nversion of HITS is applied to the graph to rank the items, where all nodes and\nlinks are allowed to have weights. This research then uses enhanced HITS\nalgorithm by developing an online eigenvector calculation method that can\ncompute the results of mutual reinforcement voting in case of frequent updates.\nFor Example in Share Market Shares price may go down or up. So we need to\ncarefully watch the market and our association rule mining has to produce the\nitems that have undergone frequent changes. These are done by estimating the\nupper bound of perturbation and postponing of the updates whenever possible.\nNext we prove that enhanced algorithm is more efficient than the original HITS\nunder the context of dynamic data.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2010 20:29:12 GMT"}], "update_date": "2010-04-22", "authors_parsed": [["Velvadivu", "P.", ""], ["Duraisamy", "K.", ""]]}, {"id": "1004.3568", "submitter": "Vishal Goyal", "authors": "Vikram Singh, Sapna Nagpal", "title": "Integrating User's Domain Knowledge with Association Rule Mining", "comments": "International Journal of Computer Science Issues online at\n  http://ijcsi.org/articles/Integrating-Users-Domain-Knowledge-with-Association-Rule-Mining.php", "journal-ref": "IJCSI, Volume 7, Issue 2, March 2010", "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a variation of Apriori algorithm that includes the role\nof domain expert to guide and speed up the overall knowledge discovery task.\nUsually, the user is interested in finding relationships between certain\nattributes instead of the whole dataset. Moreover, he can help the mining\nalgorithm to select the target database which in turn takes less time to find\nthe desired association rules. Variants of the standard Apriori and Interactive\nApriori algorithms have been run on artificial datasets. The results show that\nincorporating user's preference in selection of target attribute helps to\nsearch the association rules efficiently both in terms of space and time.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2010 20:37:32 GMT"}], "update_date": "2010-04-22", "authors_parsed": [["Singh", "Vikram", ""], ["Nagpal", "Sapna", ""]]}, {"id": "1004.3811", "submitter": "Jeremiah Blocki", "authors": "Jeremiah Blocki and Ryan Williams", "title": "Resolving the Complexity of Some Data Privacy Problems", "comments": "Full Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formally study two methods for data sanitation that have been used\nextensively in the database community: k-anonymity and l-diversity. We settle\nseveral open problems concerning the difficulty of applying these methods\noptimally, proving both positive and negative results:\n  1. 2-anonymity is in P.\n  2. The problem of partitioning the edges of a triangle-free graph into\n4-stars (degree-three vertices) is NP-hard. This yields an alternative proof\nthat 3-anonymity is NP-hard even when the database attributes are all binary.\n  3. 3-anonymity with only 27 attributes per record is MAX SNP-hard.\n  4. For databases with n rows, k-anonymity is in O(4^n poly(n)) time for all k\n> 1.\n  5. For databases with n rows and l <= log_{2c+2} log n attributes over an\nalphabet of cardinality c = O(1), k-anonymity is in P. Assuming c, l = O(1),\nk-anonymity is in O(n).\n  6. 3-diversity with binary attributes is NP-hard, with one sensitive\nattribute.\n  7. 2-diversity with binary attributes is NP-hard, with three sensitive\nattributes.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2010 21:49:37 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2010 17:42:54 GMT"}], "update_date": "2010-04-26", "authors_parsed": [["Blocki", "Jeremiah", ""], ["Williams", "Ryan", ""]]}, {"id": "1004.4022", "submitter": "Paul Lesov", "authors": "Paul Lesov", "title": "Database Security: A Historical Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The importance of security in database research has greatly increased over\nthe years as most of critical functionality of the business and military\nenterprises became digitized. Database is an integral part of any information\nsystem and they often hold sensitive data. The security of the data depends on\nphysical security, OS security and DBMS security. Database security can be\ncompromised by obtaining sensitive data, changing data or degrading\navailability of the database. Over the last 30 years the information technology\nenvironment have gone through many changes of evolution and the database\nresearch community have tried to stay a step ahead of the upcoming threats to\nthe database security. The database research community has thoughts about these\nissues long before they were address by the implementations. This paper will\nexamine the different topics pertaining to database security and see the\nadaption of the research to the changing environment. Some short term database\nresearch trends will be ascertained at the conclusion.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2010 22:22:02 GMT"}], "update_date": "2010-04-26", "authors_parsed": [["Lesov", "Paul", ""]]}, {"id": "1004.4216", "submitter": "Alan Sexton", "authors": "Alan P. Sexton and Richard Swinbank", "title": "Symmetric M-tree", "comments": "8 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": "CSR-04-2", "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The M-tree is a paged, dynamically balanced metric access method that\nresponds gracefully to the insertion of new objects. To date, no algorithm has\nbeen published for the corresponding Delete operation. We believe this to be\nnon-trivial because of the design of the M-tree's Insert algorithm. We propose\na modification to Insert that overcomes this problem and give the corresponding\nDelete algorithm. The performance of the tree is comparable to the M-tree and\noffers additional benefits in terms of supported operations, which we briefly\ndiscuss.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2010 20:04:57 GMT"}], "update_date": "2010-04-27", "authors_parsed": [["Sexton", "Alan P.", ""], ["Swinbank", "Richard", ""]]}, {"id": "1004.4718", "submitter": "Woong-Kee Loh", "authors": "Woong-Kee Loh, Yang-Sae Moon, and Jun-Gyu Kang", "title": "A Data Cleansing Method for Clustering Large-scale Transaction Databases", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": "10.1587/transinf.E93.D.3120", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we emphasize the need for data cleansing when clustering\nlarge-scale transaction databases and propose a new data cleansing method that\nimproves clustering quality and performance. We evaluate our data cleansing\nmethod through a series of experiments. As a result, the clustering quality and\nperformance were significantly improved by up to 165% and 330%, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2010 05:56:24 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Loh", "Woong-Kee", ""], ["Moon", "Yang-Sae", ""], ["Kang", "Jun-Gyu", ""]]}, {"id": "1004.4729", "submitter": "Venkatesan Chakaravarthy", "authors": "Venkatesan T. Chakaravarthy and Vinayaka Pandit and Yogish Sabharwal", "title": "On the Complexity of the $k$-Anonymization Problem", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of anonymizing tables containing personal information\nbefore releasing them for public use. One of the formulations considered in\nthis context is the $k$-anonymization problem: given a table, suppress a\nminimum number of cells so that in the transformed table, each row is identical\nto atleast $k-1$ other rows. The problem is known to be NP-hard and\nMAXSNP-hard; but in the known reductions, the number of columns in the\nconstructed tables is arbitrarily large. However, in practical settings the\nnumber of columns is much smaller. So, we study the complexity of the practical\nsetting in which the number of columns $m$ is small. We show that the problem\nis NP-hard, even when the number of columns $m$ is a constant ($m=3$). We also\nprove MAXSNP-hardness for this restricted version and derive that the problem\ncannot be approximated within a factor of (6238/6237). Our reduction uses\nalphabets $\\Sigma$ of arbitrarily large size. A natural question is whether the\nproblem remains NP-hard when both $m$ and $|\\Sigma|$ are small. We prove that\nthe $k$-anonymization problem is in $P$ when both $m$ and $|\\Sigma|$ are\nconstants.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2010 07:46:35 GMT"}], "update_date": "2010-04-28", "authors_parsed": [["Chakaravarthy", "Venkatesan T.", ""], ["Pandit", "Vinayaka", ""], ["Sabharwal", "Yogish", ""]]}, {"id": "1004.4824", "submitter": "Matjaz Perc", "authors": "Matjaz Perc", "title": "Growth and structure of Slovenia's scientific collaboration network", "comments": "10 pages, 3 figures; accepted for publication in Journal of\n  Informetrics [related work available at http://arxiv.org/abs/1003.1018 and\n  http://www.matjazperc.com/sicris/stats.html]", "journal-ref": "Journal of Informetrics 4 (2010) 475-482", "doi": "10.1016/j.joi.2010.04.003", "report-no": null, "categories": "physics.soc-ph cond-mat.stat-mech cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the evolution of Slovenia's scientific collaboration network from\n1960 till present with a yearly resolution. For each year the network was\nconstructed from publication records of Slovene scientists, whereby two were\nconnected if, up to the given year inclusive, they have coauthored at least one\npaper together. Starting with no more than 30 scientists with an average of 1.5\ncollaborators in the year 1960, the network to date consists of 7380\nindividuals that, on average, have 10.7 collaborators. We show that, in spite\nof the broad myriad of research fields covered, the networks form \"small\nworlds\" and that indeed the average path between any pair of scientists scales\nlogarithmically with size after the largest component becomes large enough.\nMoreover, we show that the network growth is governed by near-liner\npreferential attachment, giving rise to a log-normal distribution of\ncollaborators per author, and that the average starting year is roughly\ninversely proportional to the number of collaborators eventually acquired.\nUnderstandably, not all that became active early have till now gathered many\ncollaborators. We also give results for the clustering coefficient and the\ndiameter of the network over time, and compare our conclusions with those\nreported previously.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2010 14:53:09 GMT"}], "update_date": "2010-09-09", "authors_parsed": [["Perc", "Matjaz", ""]]}]