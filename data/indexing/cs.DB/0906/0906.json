[{"id": "0906.0252", "submitter": "Jeong Hoon Lee", "authors": "Jeong-Hoon Lee, Kyu-Young Whang, Hyo-Sang Lim, Byung-Suk Lee, and\n  Jun-Seok Heo", "title": "Progressive Processing of Continuous Range Queries in Hierarchical\n  Wireless Sensor Networks", "comments": "41 pages, 20 figures", "journal-ref": null, "doi": "10.1587/transinf.E93.D.1832", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of processing continuous range queries in\na hierarchical wireless sensor network. Contrasted with the traditional\napproach of building networks in a \"flat\" structure using sensor devices of the\nsame capability, the hierarchical approach deploys devices of higher capability\nin a higher tier, i.e., a tier closer to the server. While query processing in\nflat sensor networks has been widely studied, the study on query processing in\nhierarchical sensor networks has been inadequate. In wireless sensor networks,\nthe main costs that should be considered are the energy for sending data and\nthe storage for storing queries. There is a trade-off between these two costs.\nBased on this, we first propose a progressive processing method that\neffectively processes a large number of continuous range queries in\nhierarchical sensor networks. The proposed method uses the query merging\ntechnique proposed by Xiang et al. as the basis and additionally considers the\ntrade-off between the two costs. More specifically, it works toward reducing\nthe storage cost at lower-tier nodes by merging more queries, and toward\nreducing the energy cost at higher-tier nodes by merging fewer queries (thereby\nreducing \"false alarms\"). We then present how to build a hierarchical sensor\nnetwork that is optimal with respect to the weighted sum of the two costs. It\nallows for a cost-based systematic control of the trade-off based on the\nrelative importance between the storage and energy in a given network\nenvironment and application. Experimental results show that the proposed method\nachieves a near-optimal control between the storage and energy and reduces the\ncost by 0.989~84.995 times compared with the cost achieved using the flat\n(i.e., non-hierarchical) setup as in the work by Xiang et al.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2009 11:04:04 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Lee", "Jeong-Hoon", ""], ["Whang", "Kyu-Young", ""], ["Lim", "Hyo-Sang", ""], ["Lee", "Byung-Suk", ""], ["Heo", "Jun-Seok", ""]]}, {"id": "0906.0684", "submitter": "Chris Giannella", "authors": "Chris Giannella", "title": "New Instability Results for High Dimensional Nearest Neighbor Search", "comments": null, "journal-ref": "Information Processing Letters 109(19), 2009.", "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a dataset of n(d) points generated independently from R^d according\nto a common p.d.f. f_d with support(f_d) = [0,1]^d and sup{f_d([0,1]^d)}\ngrowing sub-exponentially in d. We prove that: (i) if n(d) grows\nsub-exponentially in d, then, for any query point q^d in [0,1]^d and any\nepsilon>0, the ratio of the distance between any two dataset points and q^d is\nless that 1+epsilon with probability -->1 as d-->infinity; (ii) if\nn(d)>[4(1+epsilon)]^d for large d, then for all q^d in [0,1]^d (except a small\nsubset) and any epsilon>0, the distance ratio is less than 1+epsilon with\nlimiting probability strictly bounded away from one. Moreover, we provide\npreliminary results along the lines of (i) when f_d=N(mu_d,Sigma_d).\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2009 15:13:12 GMT"}], "update_date": "2009-09-01", "authors_parsed": [["Giannella", "Chris", ""]]}, {"id": "0906.0885", "submitter": "Yongxin Tong", "authors": "Yongxin Tong, Li Zhao, Dan Yu, Shilong Ma, Ke Xu", "title": "Mining Compressed Repetitive Gapped Sequential Patterns Efficiently", "comments": "19 pages, 7 figures", "journal-ref": "The 5th International Conference on Advanced Data Mining and\n  Applications (ADMA2009)", "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining frequent sequential patterns from sequence databases has been a\ncentral research topic in data mining and various efficient mining sequential\npatterns algorithms have been proposed and studied. Recently, in many problem\ndomains (e.g, program execution traces), a novel sequential pattern mining\nresearch, called mining repetitive gapped sequential patterns, has attracted\nthe attention of many researchers, considering not only the repetition of\nsequential pattern in different sequences but also the repetition within a\nsequence is more meaningful than the general sequential pattern mining which\nonly captures occurrences in different sequences. However, the number of\nrepetitive gapped sequential patterns generated by even these closed mining\nalgorithms may be too large to understand for users, especially when support\nthreshold is low. In this paper, we propose and study the problem of\ncompressing repetitive gapped sequential patterns. Inspired by the ideas of\nsummarizing frequent itemsets, RPglobal, we develop an algorithm, CRGSgrow\n(Compressing Repetitive Gapped Sequential pattern grow), including an efficient\npruning strategy, SyncScan, and an efficient representative pattern checking\nscheme, -dominate sequential pattern checking. The CRGSgrow is a two-step\napproach: in the first step, we obtain all closed repetitive sequential\npatterns as the candidate set of representative repetitive sequential patterns,\nand at the same time get the most of representative repetitive sequential\npatterns; in the second step, we only spend a little time in finding the\nremaining the representative patterns from the candidate set. An empirical\nstudy with both real and synthetic data sets clearly shows that the CRGSgrow\nhas good performance.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2009 11:17:34 GMT"}], "update_date": "2009-06-05", "authors_parsed": [["Tong", "Yongxin", ""], ["Zhao", "Li", ""], ["Yu", "Dan", ""], ["Ma", "Shilong", ""], ["Xu", "Ke", ""]]}, {"id": "0906.0910", "submitter": "Daniel Lemire", "authors": "Sylvie Noel, Daniel Lemire", "title": "On the Challenges of Collaborative Data Processing", "comments": "to appear as a chapter in an upcoming book (Collaborative Information\n  Behavior)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last 30 years have seen the creation of a variety of electronic\ncollaboration tools for science and business. Some of the best-known\ncollaboration tools support text editing (e.g., wikis). Wikipedia's success\nshows that large-scale collaboration can produce highly valuable content.\nMeanwhile much structured data is being collected and made publicly available.\nWe have never had access to more powerful databases and statistical packages.\nIs large-scale collaborative data analysis now possible? Using a quantitative\nanalysis of Web 2.0 data visualization sites, we find evidence that at least\nmoderate open collaboration occurs. We then explore some of the limiting\nfactors of collaboration over data.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2009 13:28:51 GMT"}], "update_date": "2009-06-05", "authors_parsed": [["Noel", "Sylvie", ""], ["Lemire", "Daniel", ""]]}, {"id": "0906.2459", "submitter": "Vit Niennattrakul", "authors": "Vit Niennattrakul, Pongsakorn Ruengronghirunya, Chotirat Ann\n  Ratanamahatana", "title": "Exact Indexing for Massive Time Series Databases under Time Warping\n  Distance", "comments": "Submitted to Data Mining and Knowledge Discovery (DMKD). 33 pages, 19\n  figures, and 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among many existing distance measures for time series data, Dynamic Time\nWarping (DTW) distance has been recognized as one of the most accurate and\nsuitable distance measures due to its flexibility in sequence alignment.\nHowever, DTW distance calculation is computationally intensive. Especially in\nvery large time series databases, sequential scan through the entire database\nis definitely impractical, even with random access that exploits some index\nstructures since high dimensionality of time series data incurs extremely high\nI/O cost. More specifically, a sequential structure consumes high CPU but low\nI/O costs, while an index structure requires low CPU but high I/O costs. In\nthis work, we therefore propose a novel indexed sequential structure called\nTWIST (Time Warping in Indexed Sequential sTructure) which benefits from both\nsequential access and index structure. When a query sequence is issued, TWIST\ncalculates lower bounding distances between a group of candidate sequences and\nthe query sequence, and then identifies the data access order in advance, hence\nreducing a great number of both sequential and random accesses. Impressively,\nour indexed sequential structure achieves significant speedup in a querying\nprocess by a few orders of magnitude. In addition, our method shows superiority\nover existing rival methods in terms of query processing time, number of page\naccesses, and storage requirement with no false dismissal guaranteed.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2009 09:07:05 GMT"}], "update_date": "2009-06-16", "authors_parsed": [["Niennattrakul", "Vit", ""], ["Ruengronghirunya", "Pongsakorn", ""], ["Ratanamahatana", "Chotirat Ann", ""]]}, {"id": "0906.3112", "submitter": "Panagiotis Papadakos", "authors": "Panagiotis Papadakos, Yannis Theoharis, Yannis Marketakis, Nikos\n  Armenatzoglou and Yannis Tzitzikas", "title": "Object-Relational Database Representations for Text Indexing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the distinctive features of Information Retrieval systems comparing to\nDatabase Management systems, is that they offer better compression for posting\nlists, resulting in better I/O performance and thus faster query evaluation. In\nthis paper, we introduce database representations of the index that reduce the\nsize (and thus the disk I/Os) of the posting lists. This is not achieved by\nredesigning the DBMS, but by exploiting the non 1NF features that existing\nObject-Relational DBM systems (ORDBMS) already offer. Specifically, four\ndifferent database representations are described and detailed experimental\nresults for one million pages are reported. Three of these representations are\none order of magnitude more space efficient and faster (in query evaluation)\nthan the plain relational representation.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2009 13:32:09 GMT"}], "update_date": "2009-06-18", "authors_parsed": [["Papadakos", "Panagiotis", ""], ["Theoharis", "Yannis", ""], ["Marketakis", "Yannis", ""], ["Armenatzoglou", "Nikos", ""], ["Tzitzikas", "Yannis", ""]]}, {"id": "0906.3585", "submitter": "Arnab Bhattacharya", "authors": "Vishwakarma Singh, Arnab Bhattacharya, Ambuj K. Singh", "title": "Finding Significant Subregions in Large Image Databases", "comments": "16 pages, 48 figures", "journal-ref": "Extending Database Technology (EDBT) 2010", "doi": null, "report-no": null, "categories": "cs.DB cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images have become an important data source in many scientific and commercial\ndomains. Analysis and exploration of image collections often requires the\nretrieval of the best subregions matching a given query. The support of such\ncontent-based retrieval requires not only the formulation of an appropriate\nscoring function for defining relevant subregions but also the design of new\naccess methods that can scale to large databases. In this paper, we propose a\nsolution to this problem of querying significant image subregions. We design a\nscoring scheme to measure the similarity of subregions. Our similarity measure\nextends to any image descriptor. All the images are tiled and each alignment of\nthe query and a database image produces a tile score matrix. We show that the\nproblem of finding the best connected subregion from this matrix is NP-hard and\ndevelop a dynamic programming heuristic. With this heuristic, we develop two\nindex based scalable search strategies, TARS and SPARS, to query patterns in a\nlarge image repository. These strategies are general enough to work with other\nscoring schemes and heuristics. Experimental results on real image datasets\nshow that TARS saves more than 87% query time on small queries, and SPARS saves\nup to 52% query time on large queries as compared to linear search. Qualitative\ntests on synthetic and real datasets achieve precision of more than 80%.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2009 06:57:51 GMT"}], "update_date": "2010-03-09", "authors_parsed": [["Singh", "Vishwakarma", ""], ["Bhattacharya", "Arnab", ""], ["Singh", "Ambuj K.", ""]]}, {"id": "0906.4096", "submitter": "Naveen Ashish", "authors": "Naveen Ashish, Dmitri Kalashnikov, Sharad Mehrotra and Nalini\n  Venkatasubramanian", "title": "An Event Based Approach To Situational Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Many application domains require representing interrelated real-world\nactivities and/or evolving physical phenomena. In the crisis response domain,\nfor instance, one may be interested in representing the state of the unfolding\ncrisis (e.g., forest fire), the progress of the response activities such as\nevacuation and traffic control, and the state of the crisis site(s). Such a\nsituation representation can then be used to support a multitude of\napplications including situation monitoring, analysis, and planning. In this\npaper, we make a case for an event based representation of situations where\nevents are defined to be domain-specific significant occurrences in space and\ntime. We argue that events offer a unifying and powerful abstraction to\nbuilding situational awareness applications. We identify challenges in building\nan Event Management System (EMS) for which traditional data and knowledge\nmanagement systems prove to be limited and suggest possible directions and\ntechnologies to address the challenges.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2009 19:40:12 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2009 20:04:32 GMT"}], "update_date": "2009-09-30", "authors_parsed": [["Ashish", "Naveen", ""], ["Kalashnikov", "Dmitri", ""], ["Mehrotra", "Sharad", ""], ["Venkatasubramanian", "Nalini", ""]]}, {"id": "0906.4172", "submitter": "R Doomun", "authors": "Anjana Pandey, K.R.Pardasani", "title": "Rough Set Model for Discovering Hybrid Association Rules", "comments": "5 pages, International Journal of Computer Science and Information\n  Security", "journal-ref": "IJCSIS June 2009 Issue, Vol. 2", "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the mining of hybrid association rules with rough set approach\nis investigated as the algorithm RSHAR.The RSHAR algorithm is constituted of\ntwo steps mainly. At first, to join the participant tables into a general table\nto generate the rules which is expressing the relationship between two or more\ndomains that belong to several different tables in a database. Then we apply\nthe mapping code on selected dimension, which can be added directly into the\ninformation system as one certain attribute. To find the association rules,\nfrequent itemsets are generated in second step where candidate itemsets are\ngenerated through equivalence classes and also transforming the mapping code in\nto real dimensions. The searching method for candidate itemset is similar to\napriori algorithm. The analysis of the performance of algorithm has been\ncarried out.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2009 06:24:57 GMT"}], "update_date": "2009-06-24", "authors_parsed": [["Pandey", "Anjana", ""], ["Pardasani", "K. R.", ""]]}, {"id": "0906.4228", "submitter": "Michael Meier", "authors": "Michael Meier, Michael Schmidt, Georg Lausen", "title": "On Chase Termination Beyond Stratification", "comments": "Technical Report of VLDB 2009 conference version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the termination problem of the chase algorithm, a central tool in\nvarious database problems such as the constraint implication problem,\nConjunctive Query optimization, rewriting queries using views, data exchange,\nand data integration. The basic idea of the chase is, given a database instance\nand a set of constraints as input, to fix constraint violations in the database\ninstance. It is well-known that, for an arbitrary set of constraints, the chase\ndoes not necessarily terminate (in general, it is even undecidable if it does\nor not). Addressing this issue, we review the limitations of existing\nsufficient termination conditions for the chase and develop new techniques that\nallow us to establish weaker sufficient conditions. In particular, we introduce\ntwo novel termination conditions called safety and inductive restriction, and\nuse them to define the so-called T-hierarchy of termination conditions. We then\nstudy the interrelations of our termination conditions with previous conditions\nand the complexity of checking our conditions. This analysis leads to an\nalgorithm that checks membership in a level of the T-hierarchy and accounts for\nthe complexity of termination conditions. As another contribution, we study the\nproblem of data-dependent chase termination and present sufficient termination\nconditions w.r.t. fixed instances. They might guarantee termination although\nthe chase does not terminate in the general case. As an application of our\ntechniques beyond those already mentioned, we transfer our results into the\nfield of query answering over knowledge bases where the chase on the underlying\ndatabase may not terminate, making existing algorithms applicable to broader\nclasses of constraints.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2009 11:46:43 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2009 14:38:04 GMT"}], "update_date": "2009-09-17", "authors_parsed": [["Meier", "Michael", ""], ["Schmidt", "Michael", ""], ["Lausen", "Georg", ""]]}, {"id": "0906.4327", "submitter": "R Doomun", "authors": "Jigyasa Bisaria, Namita Shrivastava, K.R. Pardasani", "title": "A Rough Sets Partitioning Model for Mining Sequential Patterns with Time\n  Constraint", "comments": "9 pages, International Journal of Computer Science and Information\n  Security, IJCSIS 2009", "journal-ref": "IJCSIS, June 2009 Issue, Vol. 2, No.1", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Now a days, data mining and knowledge discovery methods are applied to a\nvariety of enterprise and engineering disciplines to uncover interesting\npatterns from databases. The study of Sequential patterns is an important data\nmining problem due to its wide applications to real world time dependent\ndatabases. Sequential patterns are inter-event patterns ordered over a\ntime-period associated with specific objects under study. Analysis and\ndiscovery of frequent sequential patterns over a predetermined time-period are\ninteresting data mining results, and can aid in decision support in many\nenterprise applications. The problem of sequential pattern mining poses\ncomputational challenges as a long frequent sequence contains enormous number\nof frequent subsequences. Also useful results depend on the right choice of\nevent window. In this paper, we have studied the problem of sequential pattern\nmining through two perspectives, one the computational aspect of the problem\nand the other is incorporation and adjustability of time constraint. We have\nused Indiscernibility relation from theory of rough sets to partition the\nsearch space of sequential patterns and have proposed a novel algorithm that\nallows previsualization of patterns and allows adjustment of time constraint\nprior to execution of mining task. The algorithm Rough Set Partitioning is at\nleast ten times faster than the naive time constraint based sequential pattern\nmining algorithm GSP. Besides this an additional knowledge of time interval of\nsequential patterns is also determined with the method.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2009 18:29:06 GMT"}], "update_date": "2009-06-24", "authors_parsed": [["Bisaria", "Jigyasa", ""], ["Shrivastava", "Namita", ""], ["Pardasani", "K. R.", ""]]}, {"id": "0906.4560", "submitter": "Edith Cohen", "authors": "Edith Cohen, Haim Kaplan, Subhabrata Sen", "title": "Coordinated Weighted Sampling for Estimating Aggregates Over Multiple\n  Weight Assignments", "comments": "This is an updated full version of the PVLDB 2009 conference version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many data sources are naturally modeled by multiple weight assignments over a\nset of keys: snapshots of an evolving database at multiple points in time,\nmeasurements collected over multiple time periods, requests for resources\nserved at multiple locations, and records with multiple numeric attributes.\nOver such vector-weighted data we are interested in aggregates with respect to\none set of weights, such as weighted sums, and aggregates over multiple sets of\nweights such as the $L_1$ difference.\n  Sample-based summarization is highly effective for data sets that are too\nlarge to be stored or manipulated. The summary facilitates approximate\nprocessing queries that may be specified after the summary was generated.\n  Current designs, however, are geared for data sets where a single {\\em\nscalar} weight is associated with each key.\n  We develop a sampling framework based on {\\em coordinated weighted samples}\nthat is suited for multiple weight assignments and obtain estimators that are\n{\\em orders of magnitude tighter} than previously possible.\n  We demonstrate the power of our methods through an extensive empirical\nevaluation on diverse data sets ranging from IP network to stock quotes data.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2009 20:09:00 GMT"}, {"version": "v2", "created": "Tue, 9 Nov 2010 23:13:19 GMT"}], "update_date": "2010-11-11", "authors_parsed": [["Cohen", "Edith", ""], ["Kaplan", "Haim", ""], ["Sen", "Subhabrata", ""]]}, {"id": "0906.4927", "submitter": "Lijun Chang", "authors": "Lijun Chang, Jeffrey Xu Yu, Lu Qin", "title": "Fast Probabilistic Ranking under x-Relation Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The probabilistic top-k queries based on the interplay of score and\nprobability, under the possible worlds semantic, become an important research\nissue that considers both score and uncertainty on the same basis. In the\nliterature, many different probabilistic top-k queries are proposed. Almost all\nof them need to compute the probability of a tuple t_i to be ranked at the j-th\nposition across the entire set of possible worlds. The cost of such computing\nis the dominant cost and is known as O(kn^2), where n is the size of dataset.\nIn this paper, we propose a new novel algorithm that computes such probability\nin O(kn).\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2009 13:24:57 GMT"}], "update_date": "2009-06-29", "authors_parsed": [["Chang", "Lijun", ""], ["Yu", "Jeffrey Xu", ""], ["Qin", "Lu", ""]]}, {"id": "0906.5040", "submitter": "Chendong Li", "authors": "Chendong Li", "title": "Towards the Patterns of Hard CSPs with Association Rule Mining", "comments": "10 pages, 3 figures, submitted to ICDM'09", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The hardness of finite domain Constraint Satisfaction Problems (CSPs) is a\nvery important research area in Constraint Programming (CP) community. However,\nthis problem has not yet attracted much attention from the researchers in the\nassociation rule mining community. As a popular data mining technique,\nassociation rule mining has an extremely wide application area and it has\nalready been successfully applied to many interdisciplines. In this paper, we\nstudy the association rule mining techniques and propose a cascaded approach to\nextract the interesting patterns of the hard CSPs. As far as we know, this\nproblem is investigated with the data mining techniques for the first time.\nSpecifically, we generate the random CSPs and collect their characteristics by\nsolving all the CSP instances, and then apply the data mining techniques on the\ndata set and further to discover the interesting patterns of the hardness of\nthe randomly generated CSPs\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2009 05:11:48 GMT"}], "update_date": "2009-06-30", "authors_parsed": [["Li", "Chendong", ""]]}, {"id": "0906.5148", "submitter": "Tijl De Bie", "authors": "Tijl De Bie", "title": "Explicit probabilistic models for databases and networks", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work in data mining and related areas has highlighted the importance\nof the statistical assessment of data mining results. Crucial to this endeavour\nis the choice of a non-trivial null model for the data, to which the found\npatterns can be contrasted. The most influential null models proposed so far\nare defined in terms of invariants of the null distribution. Such null models\ncan be used by computation intensive randomization approaches in estimating the\nstatistical significance of data mining results.\n  Here, we introduce a methodology to construct non-trivial probabilistic\nmodels based on the maximum entropy (MaxEnt) principle. We show how MaxEnt\nmodels allow for the natural incorporation of prior information. Furthermore,\nthey satisfy a number of desirable properties of previously introduced\nrandomization approaches. Lastly, they also have the benefit that they can be\nrepresented explicitly. We argue that our approach can be used for a variety of\ndata types. However, for concreteness, we have chosen to demonstrate it in\nparticular for databases and networks.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2009 07:55:41 GMT"}], "update_date": "2009-06-30", "authors_parsed": [["De Bie", "Tijl", ""]]}, {"id": "0906.5485", "submitter": "Markus Ojala", "authors": "Markus Ojala, Gemma C. Garriga, Aristides Gionis, Heikki Mannila", "title": "Query Significance in Databases via Randomizations", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many sorts of structured data are commonly stored in a multi-relational\nformat of interrelated tables. Under this relational model, exploratory data\nanalysis can be done by using relational queries. As an example, in the\nInternet Movie Database (IMDb) a query can be used to check whether the average\nrank of action movies is higher than the average rank of drama movies.\n  We consider the problem of assessing whether the results returned by such a\nquery are statistically significant or just a random artifact of the structure\nin the data. Our approach is based on randomizing the tables occurring in the\nqueries and repeating the original query on the randomized tables. It turns out\nthat there is no unique way of randomizing in multi-relational data. We propose\nseveral randomization techniques, study their properties, and show how to find\nout which queries or hypotheses about our data result in statistically\nsignificant information. We give results on real and generated data and show\nhow the significance of some queries vary between different randomizations.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2009 14:08:19 GMT"}], "update_date": "2009-07-01", "authors_parsed": [["Ojala", "Markus", ""], ["Garriga", "Gemma C.", ""], ["Gionis", "Aristides", ""], ["Mannila", "Heikki", ""]]}, {"id": "0906.5608", "submitter": "Saqib Saeed", "authors": "Saqib Saeed, Christoph Kunz", "title": "Loading Arbitrary Knowledge Bases in Matrix Browser", "comments": "This paper was published in the proceedings of IEEE International\n  Multi Topic Conference (INMIC 2004) Lahore, Pakistan 24th- 26th December 2004", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the work done on Matrix Browser, which is a recently\ndeveloped graphical user interface to explore and navigate complex networked\ninformation spaces. This approach presents a new way of navigating information\nnets in windows explorer like widget. The problem on hand was how to export\narbitrary knowledge bases in Matrix Browser. This was achieved by identifying\nthe relationships present in knowledge bases and then by forming the\nhierarchies from this data and these hierarchies are being exported to matrix\nbrowser. This paper gives solution to this problem and informs about\nimplementation work.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2009 18:42:59 GMT"}], "update_date": "2012-05-14", "authors_parsed": [["Saeed", "Saqib", ""], ["Kunz", "Christoph", ""]]}]