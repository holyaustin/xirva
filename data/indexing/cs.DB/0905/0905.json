[{"id": "0905.1755", "submitter": "Raymond Chi-Wing Wong", "authors": "Raymond Chi-Wing Wong, Ada Wai-Chee Fu, Ke Wang, Yabo Xu, Philip S. Yu", "title": "Can the Utility of Anonymized Data be used for Privacy Breaches?", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Group based anonymization is the most widely studied approach for privacy\npreserving data publishing. This includes k-anonymity, l-diversity, and\nt-closeness, to name a few. The goal of this paper is to raise a fundamental\nissue on the privacy exposure of the current group based approach. This has\nbeen overlooked in the past. The group based anonymization approach basically\nhides each individual record behind a group to preserve data privacy. If not\nproperly anonymized, patterns can actually be derived from the published data\nand be used by the adversary to breach individual privacy. For example, from\nthe medical records released, if patterns such as people from certain countries\nrarely suffer from some disease can be derived, then the information can be\nused to imply linkage of other people in an anonymized group with this disease\nwith higher likelihood. We call the derived patterns from the published data\nthe foreground knowledge. This is in contrast to the background knowledge that\nthe adversary may obtain from other channels as studied in some previous work.\nFinally, we show by experiments that the attack is realistic in the privacy\nbenchmark dataset under the traditional group based anonymization approach.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2009 03:36:26 GMT"}], "update_date": "2009-05-13", "authors_parsed": [["Wong", "Raymond Chi-Wing", ""], ["Fu", "Ada Wai-Chee", ""], ["Wang", "Ke", ""], ["Xu", "Yabo", ""], ["Yu", "Philip S.", ""]]}, {"id": "0905.2200", "submitter": "Debprakash Patnaik", "authors": "Yong Cao, Debprakash Patnaik, Sean Ponce, Jeremy Archuleta, Patrick\n  Butler, Wu-chun Feng, and Naren Ramakrishnan", "title": "Towards Chip-on-Chip Neuroscience: Fast Mining of Frequent Episodes\n  Using Graphics Processors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational neuroscience is being revolutionized with the advent of\nmulti-electrode arrays that provide real-time, dynamic, perspectives into brain\nfunction. Mining event streams from these chips is critical to understanding\nthe firing patterns of neurons and to gaining insight into the underlying\ncellular activity. We present a GPGPU solution to mining spike trains. We focus\non mining frequent episodes which captures coordinated events across time even\nin the presence of intervening background/\"junk\" events. Our algorithmic\ncontributions are two-fold: MapConcatenate, a new computation-to-core mapping\nscheme, and a two-pass elimination approach to quickly find supported episodes\nfrom a large number of candidates. Together, they help realize a real-time\n\"chip-on-chip\" solution to neuroscience data mining, where one chip (the\nmulti-electrode array) supplies the spike train data and another (the GPGPU)\nmines it at a scale unachievable previously. Evaluation on both synthetic and\nreal datasets demonstrate the potential of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2009 21:04:03 GMT"}], "update_date": "2009-05-15", "authors_parsed": [["Cao", "Yong", ""], ["Patnaik", "Debprakash", ""], ["Ponce", "Sean", ""], ["Archuleta", "Jeremy", ""], ["Butler", "Patrick", ""], ["Feng", "Wu-chun", ""], ["Ramakrishnan", "Naren", ""]]}, {"id": "0905.2203", "submitter": "Debprakash Patnaik", "authors": "Debprakash Patnaik, Sean P. Ponce, Yong Cao, Naren Ramakrishnan", "title": "Accelerator-Oriented Algorithm Transformation for Temporal Data Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal data mining algorithms are becoming increasingly important in many\napplication domains including computational neuroscience, especially the\nanalysis of spike train data. While application scientists have been able to\nreadily gather multi-neuronal datasets, analysis capabilities have lagged\nbehind, due to both lack of powerful algorithms and inaccessibility to powerful\nhardware platforms. The advent of GPU architectures such as Nvidia's GTX 280\noffers a cost-effective option to bring these capabilities to the\nneuroscientist's desktop. Rather than port existing algorithms onto this\narchitecture, we advocate the need for algorithm transformation, i.e.,\nrethinking the design of the algorithm in a way that need not necessarily\nmirror its serial implementation strictly. We present a novel implementation of\na frequent episode discovery algorithm by revisiting \"in-the-large\" issues such\nas problem decomposition as well as \"in-the-small\" issues such as data layouts\nand memory access patterns. This is non-trivial because frequent episode\ndiscovery does not lend itself to GPU-friendly data-parallel mapping\nstrategies. Applications to many datasets and comparisons to CPU as well as\nprior GPU implementations showcase the advantages of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2009 21:18:31 GMT"}], "update_date": "2009-05-15", "authors_parsed": [["Patnaik", "Debprakash", ""], ["Ponce", "Sean P.", ""], ["Cao", "Yong", ""], ["Ramakrishnan", "Naren", ""]]}, {"id": "0905.2657", "submitter": "Daniel Lemire", "authors": "Kamel Aouiche, Daniel Lemire, Robert Godin", "title": "Web 2.0 OLAP: From Data Cubes to Tag Clouds", "comments": "Software at https://github.com/lemire/OLAPTagCloud. arXiv admin note:\n  substantial text overlap with arXiv:0710.2156", "journal-ref": "Lecture Notes in Business Information Processing Vol. 18, pages\n  51-64, 2009", "doi": "10.1007/978-3-642-01344-7_5", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Increasingly, business projects are ephemeral. New Business Intelligence\ntools must support ad-lib data sources and quick perusal. Meanwhile, tag clouds\nare a popular community-driven visualization technique. Hence, we investigate\ntag-cloud views with support for OLAP operations such as roll-ups, slices,\ndices, clustering, and drill-downs. As a case study, we implemented an\napplication where users can upload data and immediately navigate through its ad\nhoc dimensions. To support social networking, views can be easily shared and\nembedded in other Web sites. Algorithmically, our tag-cloud views are\napproximate range top-k queries over spontaneous data cubes. We present\nexperimental evidence that iceberg cuboids provide adequate online\napproximations. We benchmark several browser-oblivious tag-cloud layout\noptimizations.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2009 05:57:06 GMT"}, {"version": "v2", "created": "Tue, 15 Mar 2016 21:52:24 GMT"}], "update_date": "2016-03-17", "authors_parsed": [["Aouiche", "Kamel", ""], ["Lemire", "Daniel", ""], ["Godin", "Robert", ""]]}, {"id": "0905.3318", "submitter": "Maarten Hijzelendoorn", "authors": "Maarten Hijzelendoorn and Crit Cremers", "title": "An Object-Oriented and Fast Lexicon for Semantic Generation", "comments": "Paper presented at the 18th Computational Linguistics In the\n  Netherlands Meeting (CLIN), Nijmegen, 10 December 2007, 15pp", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DB cs.DS cs.IR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is about the technical design of a large computational lexicon,\nits storage, and its access from a Prolog environment. Traditionally, efficient\naccess and storage of data structures is implemented by a relational database\nmanagement system. In Delilah, a lexicon-based NLP system, efficient access to\nthe lexicon by the semantic generator is vital. We show that our highly\ndetailed HPSG-style lexical specifications do not fit well in the Relational\nModel, and that they cannot be efficiently retrieved. We argue that they fit\nmore naturally in the Object-Oriented Model. Although storage of objects is\nredundant, we claim that efficient access is still possible by applying\nindexing, and compression techniques from the Relational Model to the\nObject-Oriented Model. We demonstrate that it is possible to implement\nobject-oriented storage and fast access in ISO Prolog.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2009 14:12:42 GMT"}], "update_date": "2009-05-21", "authors_parsed": [["Hijzelendoorn", "Maarten", ""], ["Cremers", "Crit", ""]]}, {"id": "0905.4138", "submitter": "Christos Attikos", "authors": "Christos Attikos, Michael Doumpos", "title": "Faster estimation of the correlation fractal dimension using\n  box-counting", "comments": "4 pages, to appear in BCI 2009 - 4th Balkan Conference in Informatics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fractal dimension is widely adopted in spatial databases and data mining,\namong others as a measure of dataset skewness. State-of-the-art algorithms for\nestimating the fractal dimension exhibit linear runtime complexity whether\nbased on box-counting or approximation schemes. In this paper, we revisit a\ncorrelation fractal dimension estimation algorithm that redundantly rescans the\ndataset and, extending that work, we propose another linear, yet faster and as\naccurate method, which completes in a single pass.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2009 08:52:42 GMT"}], "update_date": "2009-05-27", "authors_parsed": [["Attikos", "Christos", ""], ["Doumpos", "Michael", ""]]}, {"id": "0905.4201", "submitter": "Florentina Pintea", "authors": "A.T. Akinwalle, F.T. Ibharalu", "title": "The Usefulness of Multilevel Hash Tables with Multiple Hash Functions in\n  Large Databases", "comments": "10 pages, exposed on 5th International Conference \"Actualities and\n  Perspectives on Hardware and Software\" - APHS2009, Timisoara, Romania", "journal-ref": "Ann. Univ. Tibiscus Comp. Sci. Series VII(2009),11-20", "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, attempt is made to select three good hash functions which\nuniformly distribute hash values that permute their internal states and allow\nthe input bits to generate different output bits. These functions are used in\ndifferent levels of hash tables that are coded in Java Programming Language and\na quite number of data records serve as primary data for testing the\nperformances. The result shows that the two-level hash tables with three\ndifferent hash functions give a superior performance over one-level hash table\nwith two hash functions or zero-level hash table with one function in term of\nreducing the conflict keys and quick lookup for a particular element. The\nresult assists to reduce the complexity of join operation in query language\nfrom O(n2) to O(1) by placing larger query result, if any, in multilevel hash\ntables with multiple hash functions and generate shorter query result.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2009 13:47:23 GMT"}], "update_date": "2009-05-27", "authors_parsed": [["Akinwalle", "A. T.", ""], ["Ibharalu", "F. T.", ""]]}, {"id": "0905.4605", "submitter": "Florentina Pintea", "authors": "Ovidiu Crista", "title": "Techniques for Securing Data Exchange between a Database Server and a\n  Client Program", "comments": "6 pages, exposed on 5th International Conference \"Actualities and\n  Perspectives on Hardware and Software\" - APHS2009, Timisoara, Romania", "journal-ref": "Ann. Univ. Tibiscus Comp. Sci. Series VII(2009),95-100", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of the presented work is to illustrate a method by which the data\nexchange between a standalone computer software and a shared database server\ncan be protected of unauthorized interceptation of the traffic in Internet\nnetwork, a transport network for data managed by those two systems,\ninterceptation by which an attacker could gain illegetimate access to the\ndatabase, threatening this way the data integrity and compromising the\ndatabase.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2009 10:29:20 GMT"}], "update_date": "2009-05-29", "authors_parsed": [["Crista", "Ovidiu", ""]]}, {"id": "0905.4713", "submitter": "Leonard Kwuida", "authors": "Leonard Kwuida, Rokia Missaoui, Lahcen Boumedjout, Jean Vaillancourt", "title": "Mining Generalized Patterns from Large Databases using Ontologies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Formal Concept Analysis (FCA) is a mathematical theory based on the\nformalization of the notions of concept and concept hierarchies. It has been\nsuccessfully applied to several Computer Science fields such as data\nmining,software engineering, and knowledge engineering, and in many domains\nlike medicine, psychology, linguistics and ecology. For instance, it has been\nexploited for the design, mapping and refinement of ontologies. In this paper,\nwe show how FCA can benefit from a given domain ontology by analyzing the\nimpact of a taxonomy (on objects and/or attributes) on the resulting concept\nlattice. We willmainly concentrate on the usage of a taxonomy to extract\ngeneralized patterns (i.e., knowledge generated from data when elements of a\ngiven domain ontology are used) in the form of concepts and rules, and improve\nnavigation through these patterns. To that end, we analyze three generalization\ncases and show their impact on the size of the generalized pattern set.\nDifferent scenarios of simultaneous generalizations on both objects and\nattributes are also discussed\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2009 18:34:37 GMT"}], "update_date": "2009-05-29", "authors_parsed": [["Kwuida", "Leonard", ""], ["Missaoui", "Rokia", ""], ["Boumedjout", "Lahcen", ""], ["Vaillancourt", "Jean", ""]]}, {"id": "0905.4761", "submitter": "Gregory Leighton", "authors": "Gregory Leighton, Denilson Barbosa", "title": "Optimizing XML Compression", "comments": "16 pages, extended version of paper accepted for XSym 2009", "journal-ref": null, "doi": "10.1007/978-3-642-03555-5_8", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The eXtensible Markup Language (XML) provides a powerful and flexible means\nof encoding and exchanging data. As it turns out, its main advantage as an\nencoding format (namely, its requirement that all open and close markup tags\nare present and properly balanced) yield also one of its main disadvantages:\nverbosity. XML-conscious compression techniques seek to overcome this drawback.\nMany of these techniques first separate XML structure from the document\ncontent, and then compress each independently. Further compression gains can be\nrealized by identifying and compressing together document content that is\nhighly similar, thereby amortizing the storage costs of auxiliary information\nrequired by the chosen compression algorithm. Additionally, the proper choice\nof compression algorithm is an important factor not only for the achievable\ncompression gain, but also for access performance. Hence, choosing a\ncompression configuration that optimizes compression gain requires one to\ndetermine (1) a partitioning strategy for document content, and (2) the best\navailable compression algorithm to apply to each set within this partition. In\nthis paper, we show that finding an optimal compression configuration with\nrespect to compression gain is an NP-hard optimization problem. This problem\nremains intractable even if one considers a single compression algorithm for\nall content. We also describe an approximation algorithm for selecting a\npartitioning strategy for document content based on the branch-and-bound\nparadigm.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2009 22:36:57 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Leighton", "Gregory", ""], ["Barbosa", "Denilson", ""]]}]