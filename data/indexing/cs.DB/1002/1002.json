[{"id": "1002.0139", "submitter": "Kadirvelu SivaKumar", "authors": "P.S Hiremath, Siddu P. Algur", "title": "Extraction of Flat and Nested Data Records from Web Pages", "comments": "10 Pages IEEE format, International Journal on Computer Science and\n  Engineering, IJCSE 2010, ISSN 0975-3397, Impact Factor 0.583", "journal-ref": "International Journal on Computer Science and Engineering, IJCSE,\n  Vol. 2, No. 1 January 2010", "doi": null, "report-no": "IJEST10-02-01-07", "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of identification and extraction of flat and\nnested data records from a given web page. With the explosive growth of\ninformation sources available on the World Wide Web, it has become increasingly\ndifficult to identify the relevant pieces of information, since web pages are\noften cluttered with irrelevant content like advertisements, navigation-panels,\ncopyright notices etc., surrounding the main content of the web page. Hence, it\nis useful to mine such data regions and data records in order to extract\ninformation from such web pages to provide value-added services. Currently\navailable automatic techniques to mine data regions and data records from web\npages are still unsatisfactory because of their poor performance. In this paper\na novel method to identify and extract the flat and nested data records from\nthe web pages automatically is proposed. It comprises of two steps : (1)\nIdentification and Extraction of the data regions based on visual clues\ninformation. (2) Identification and extraction of flat and nested data records\nfrom the data region of a web page automatically. For step1, a novel and more\neffective method is proposed, which finds the data regions formed by all types\nof tags using visual clues. For step2, a more effective and efficient method\nnamely, Visual Clue based Extraction of web Data (VCED), is proposed, which\nextracts each record from the data region and identifies it whether it is a\nflat or nested data record based on visual clue information the area covered by\nand the number of data items present in each record. Our experimental results\nshow that the proposed technique is effective and better than existing\ntechniques.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2010 16:39:26 GMT"}], "update_date": "2010-02-02", "authors_parsed": [["Hiremath", "P. S", ""], ["Algur", "Siddu P.", ""]]}, {"id": "1002.0383", "submitter": "Dakshina Ranjan Kisku", "authors": "Hunny Mehrotra, Dakshina Ranjan Kisku, V. Bhawani Radhika, Banshidhar\n  Majhi, Phalguni Gupta", "title": "Feature Level Clustering of Large Biometric Database", "comments": "4 pages, 2 figures, IAPR International Conference on Machine Vision\n  Applications, 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an efficient technique for partitioning large biometric\ndatabase during identification. In this technique feature vector which\ncomprises of global and local descriptors extracted from offline signature are\nused by fuzzy clustering technique to partition the database. As biometric\nfeatures posses no natural order of sorting, thus it is difficult to index them\nalphabetically or numerically. Hence, some supervised criteria is required to\npartition the search space. At the time of identification the fuzziness\ncriterion is introduced to find the nearest clusters for declaring the identity\nof query sample. The system is tested using bin-miss rate and performs better\nin comparison to traditional k-means approach.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2010 02:30:22 GMT"}], "update_date": "2010-02-03", "authors_parsed": [["Mehrotra", "Hunny", ""], ["Kisku", "Dakshina Ranjan", ""], ["Radhika", "V. Bhawani", ""], ["Majhi", "Banshidhar", ""], ["Gupta", "Phalguni", ""]]}, {"id": "1002.0963", "submitter": "Hoyoung Jeung", "authors": "Hoyoung Jeung, Man Lung Yiu, Xiaofang Zhou, Christian S. Jensen, Heng\n  Tao Shen", "title": "Discovery of Convoys in Trajectory Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As mobile devices with positioning capabilities continue to proliferate, data\nmanagement for so-called trajectory databases that capture the historical\nmovements of populations of moving objects becomes important. This paper\nconsiders the querying of such databases for convoys, a convoy being a group of\nobjects that have traveled together for some time. More specifically, this\npaper formalizes the concept of a convoy query using density-based notions, in\norder to capture groups of arbitrary extents and shapes. Convoy discovery is\nrelevant for real-life applications in throughput planning of trucks and\ncarpooling of vehicles. Although there has been extensive research on\ntrajectories in the literature, none of this can be applied to retrieve\ncorrectly exact convoy result sets. Motivated by this, we develop three\nefficient algorithms for convoy discovery that adopt the well-known\nfilter-refinement framework. In the filter step, we apply line-simplification\ntechniques on the trajectories and establish distance bounds between the\nsimplified trajectories. This permits efficient convoy discovery over the\nsimplified trajectories without missing any actual convoys. In the refinement\nstep, the candidate convoys are further processed to obtain the actual convoys.\nOur comprehensive empirical study offers insight into the properties of the\npaper's proposals and demonstrates that the proposals are effective and\nefficient on real-world trajectory data.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2010 11:34:18 GMT"}], "update_date": "2010-02-05", "authors_parsed": [["Jeung", "Hoyoung", ""], ["Yiu", "Man Lung", ""], ["Zhou", "Xiaofang", ""], ["Jensen", "Christian S.", ""], ["Shen", "Heng Tao", ""]]}, {"id": "1002.0971", "submitter": "Benjamin Nguyen", "authors": "Benjamin Nguyen (PRISM), Fran\\c{c}ois-Xavier Dudouet (LASP, IRISES),\n  Dario Colazzo (LRI), Antoine Vion (LEST), Ioana Manolescu (INRIA Saclay - Ile\n  de France), Pierre Senellart", "title": "The WebStand Project", "comments": null, "journal-ref": "WebSci'09: Society On-Line Conference, Greece (2009)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present the state of advancement of the French ANR WebStand\nproject. The objective of this project is to construct a customizable XML based\nwarehouse platform to acquire, transform, analyze, store, query and export data\nfrom the web, in particular mailing lists, with the final intension of using\nthis data to perform sociological studies focused on social groups of World\nWide Web, with a specific emphasis on the temporal aspects of this data. We are\ncurrently using this system to analyze the standardization process of the W3C,\nthrough its social network of standard setters.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2010 12:09:15 GMT"}], "update_date": "2010-02-05", "authors_parsed": [["Nguyen", "Benjamin", "", "PRISM"], ["Dudouet", "Fran\u00e7ois-Xavier", "", "LASP, IRISES"], ["Colazzo", "Dario", "", "LRI"], ["Vion", "Antoine", "", "LEST"], ["Manolescu", "Ioana", "", "INRIA Saclay - Ile\n  de France"], ["Senellart", "Pierre", ""]]}, {"id": "1002.1104", "submitter": "Fabio Vandin", "authors": "Adam Kirsch, Michael Mitzenmacher, Andrea Pietracaprina, Geppino\n  Pucci, Eli Upfal, Fabio Vandin", "title": "An Efficient Rigorous Approach for Identifying Statistically Significant\n  Frequent Itemsets", "comments": "A preliminary version of this work was presented in ACM PODS 2009. 20\n  pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As advances in technology allow for the collection, storage, and analysis of\nvast amounts of data, the task of screening and assessing the significance of\ndiscovered patterns is becoming a major challenge in data mining applications.\nIn this work, we address significance in the context of frequent itemset\nmining. Specifically, we develop a novel methodology to identify a meaningful\nsupport threshold s* for a dataset, such that the number of itemsets with\nsupport at least s* represents a substantial deviation from what would be\nexpected in a random dataset with the same number of transactions and the same\nindividual item frequencies. These itemsets can then be flagged as\nstatistically significant with a small false discovery rate. We present\nextensive experimental results to substantiate the effectiveness of our\nmethodology.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2010 23:33:47 GMT"}], "update_date": "2010-02-08", "authors_parsed": [["Kirsch", "Adam", ""], ["Mitzenmacher", "Michael", ""], ["Pietracaprina", "Andrea", ""], ["Pucci", "Geppino", ""], ["Upfal", "Eli", ""], ["Vandin", "Fabio", ""]]}, {"id": "1002.1143", "submitter": "Vishal Goyal", "authors": "Nadeem Mahmood, Aqil Burney, Kamran Ahsan", "title": "A Logical Temporal Relational Data Model", "comments": "International Journal of Computer Science Issues, IJCSI, Vol. 7,\n  Issue 1, No. 1, January 2010,\n  http://ijcsi.org/articles/A-Logical-Temporal-Relational-Data-Model.php", "journal-ref": "International Journal of Computer Science Issues, IJCSI, Vol. 7,\n  Issue 1, No. 1, January 2010,\n  http://ijcsi.org/articles/A-Logical-Temporal-Relational-Data-Model.php", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time is one of the most difficult aspects to handle in real world\napplications such as database systems. Relational database management systems\nproposed by Codd offer very little built-in query language support for temporal\ndata management. The model itself incorporates neither the concept of time nor\nany theory of temporal semantics. Many temporal extensions of the relational\nmodel have been proposed and some of them are also implemented. This paper\noffers a brief introduction to temporal database research. We propose a\nconceptual model for handling time varying attributes in the relational\ndatabase model with minimal temporal attributes.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2010 08:22:24 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2010 11:39:13 GMT"}], "update_date": "2010-02-08", "authors_parsed": [["Mahmood", "Nadeem", ""], ["Burney", "Aqil", ""], ["Ahsan", "Kamran", ""]]}, {"id": "1002.1150", "submitter": "Vishal Goyal", "authors": "Mahdi Esmaeili, Fazekas Gabor", "title": "Finding Sequential Patterns from Large Sequence Data", "comments": "International Journal of Computer Science Issues, IJCSI, Vol. 7,\n  Issue 1, No. 1, January 2010,\n  http://ijcsi.org/articles/Finding-Sequential-Patterns-from-Large-Sequence-Data.php", "journal-ref": "International Journal of Computer Science Issues, IJCSI, Vol. 7,\n  Issue 1, No. 1, January 2010,\n  http://ijcsi.org/articles/Finding-Sequential-Patterns-from-Large-Sequence-Data.php", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data mining is the task of discovering interesting patterns from large\namounts of data. There are many data mining tasks, such as classification,\nclustering, association rule mining, and sequential pattern mining. Sequential\npattern mining finds sets of data items that occur together frequently in some\nsequences. Sequential pattern mining, which extracts frequent subsequences from\na sequence database, has attracted a great deal of interest during the recent\ndata mining research because it is the basis of many applications, such as: web\nuser analysis, stock trend prediction, DNA sequence analysis, finding language\nor linguistic patterns from natural language texts, and using the history of\nsymptoms to predict certain kind of disease. The diversity of the applications\nmay not be possible to apply a single sequential pattern model to all these\nproblems. Each application may require a unique model and solution. A number of\nresearch projects were established in recent years to develop meaningful\nsequential pattern models and efficient algorithms for mining these patterns.\nIn this paper, we theoretically provided a brief overview three types of\nsequential patterns model.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2010 08:43:45 GMT"}], "update_date": "2010-02-08", "authors_parsed": [["Esmaeili", "Mahdi", ""], ["Gabor", "Fazekas", ""]]}, {"id": "1002.1159", "submitter": "Vishal Goyal", "authors": "Yuval Cohen", "title": "Mining The Successful Binary Combinations: Methodology and A Simple Case\n  Study", "comments": "International Journal of Computer Science Issues, IJCSI, Vol. 7,\n  Issue 1, No. 2, January 2010,\n  http://ijcsi.org/articles/Mining-The-Successful-Binary-Combinations-Methodology-and-A-Simple-Case-Study.php", "journal-ref": "International Journal of Computer Science Issues, IJCSI, Vol. 7,\n  Issue 1, No. 2, January 2010,\n  http://ijcsi.org/articles/Mining-The-Successful-Binary-Combinations-Methodology-and-A-Simple-Case-Study.php", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The importance of finding the characteristics leading to either a success or\na failure is one of the driving forces of data mining. The various application\nareas of finding success/failure factors cover vast variety of areas such as\ncredit risk evaluation and granting loans, micro array analysis, health factors\nand health risk factors, and parameter combination leading to a product\nsuccess. This paper presents a new approach for making inferences about\ndichotomous data. The objective is to determine rules that lead to a certain\nresult. The method consists of four phases: in the first phase, the data is\nprocessed into a binary format of a truth table, in the second phase; rules are\nfound by utilizing an algorithm that minimizes Boolean functions. In the third\nphase the rules are checked and filtered. In the fourth phase, simple rules\nthat involve one to two features are revealed.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2010 09:07:54 GMT"}], "update_date": "2010-02-08", "authors_parsed": [["Cohen", "Yuval", ""]]}, {"id": "1002.1185", "submitter": "Vishal Goyal", "authors": "Kanak Saxena, Rahul Shukla", "title": "Significant Interval and Frequent Pattern Discovery in Web Log Data", "comments": "International Journal of Computer Science Issues, IJCSI, Vol. 7,\n  Issue 1, No. 3, January 2010,\n  http://ijcsi.org/articles/Significant-Interval-and-Frequent-Pattern-Discovery-in-Web-Log-Data.php", "journal-ref": "International Journal of Computer Science Issues, IJCSI, Vol. 7,\n  Issue 1, No. 3, January 2010,\n  http://ijcsi.org/articles/Significant-Interval-and-Frequent-Pattern-Discovery-in-Web-Log-Data.php", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a considerable body of work on sequence mining of Web Log Data. We\nare using One Pass frequent Episode discovery (or FED) algorithm, takes a\ndifferent approach than the traditional apriori class of pattern detection\nalgorithms. In this approach significant intervals for each Website are\ncomputed first (independently) and these interval used for detecting frequent\npatterns/Episode and then the Analysis is performed on Significant Intervals\nand frequent patterns That can be used to forecast the user's behavior using\nprevious trends and this can be also used for advertising purpose. This type of\napplications predicts the Website interest. In this approach, time-series data\nare folded over a periodicity (day, week, etc.) Which are used to form the\nInterval? Significant intervals are discovered from these time points that\nsatisfy the criteria of minimum confidence and maximum interval length\nspecified by the user.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2010 10:30:51 GMT"}], "update_date": "2010-02-08", "authors_parsed": [["Saxena", "Kanak", ""], ["Shukla", "Rahul", ""]]}, {"id": "1002.2450", "submitter": "Alejandro Chinea Manrique De Lara", "authors": "Alejandro Chinea Manrique de Lara", "title": "Modeling the Probability of Failure on LDAP Binding Operations in\n  Iplanet Web Proxy 3.6 Server", "comments": "11 pages, 3 figures, Published in Sun MicroSystems Laboratories April\n  2002", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is devoted to the theoretical analysis of a problem derived from\ninteraction between two Iplanet products: Web Proxy Server and the Directory\nServer. In particular, a probabilistic and stochastic-approximation model is\nproposed to minimize the occurrence of LDAP connection failures in Iplanet Web\nProxy 3.6 Server. The proposed model serves not only to provide a\nparameterization of the aforementioned phenomena, but also to provide\nmeaningful insights illustrating and supporting these theoretical results. In\naddition, we shall also address practical considerations when estimating the\nparameters of the proposed model from experimental data. Finally, we shall\nprovide some interesting results from real-world data collected from our\ncustomers.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2010 23:20:40 GMT"}], "update_date": "2010-02-15", "authors_parsed": [["de Lara", "Alejandro Chinea Manrique", ""]]}, {"id": "1002.3195", "submitter": "Mahmud Hossain", "authors": "M. Shahriar Hossain, Michael Narayan and Naren Ramakrishnan", "title": "Efficiently Discovering Hammock Paths from Induced Similarity Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity networks are important abstractions in many information management\napplications such as recommender systems, corpora analysis, and medical\ninformatics. For instance, by inducing similarity networks between movies rated\nsimilarly by users, or between documents containing common terms, and or\nbetween clinical trials involving the same themes, we can aim to find the\nglobal structure of connectivities underlying the data, and use the network as\na basis to make connections between seemingly disparate entities. In the above\napplications, composing similarities between objects of interest finds uses in\nserendipitous recommendation, in storytelling, and in clinical diagnosis,\nrespectively. We present an algorithmic framework for traversing similarity\npaths using the notion of `hammock' paths which are generalization of\ntraditional paths. Our framework is exploratory in nature so that, given\nstarting and ending objects of interest, it explores candidate objects for path\nfollowing, and heuristics to admissibly estimate the potential for paths to\nlead to a desired destination. We present three diverse applications: exploring\nmovie similarities in the Netflix dataset, exploring abstract similarities\nacross the PubMed corpus, and exploring description similarities in a database\nof clinical trials. Experimental results demonstrate the potential of our\napproach for unstructured knowledge discovery in similarity networks.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2010 04:07:06 GMT"}], "update_date": "2010-02-18", "authors_parsed": [["Hossain", "M. Shahriar", ""], ["Narayan", "Michael", ""], ["Ramakrishnan", "Naren", ""]]}, {"id": "1002.3671", "submitter": "Manas Pathak", "authors": "Manas A. Pathak and Bhiksha Raj", "title": "Privacy-Preserving Protocols for Eigenvector Computation", "comments": "14 pages", "journal-ref": "Proceedings of ECML/PKDD Workshop on Privacy and Security issues\n  in Data Mining and Machine Learning, 2010", "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a protocol for computing the principal eigenvector\nof a collection of data matrices belonging to multiple semi-honest parties with\nprivacy constraints. Our proposed protocol is based on secure multi-party\ncomputation with a semi-honest arbitrator who deals with data encrypted by the\nother parties using an additive homomorphic cryptosystem. We augment the\nprotocol with randomization and obfuscation to make it difficult for any party\nto estimate properties of the data belonging to other parties from the\nintermediate steps. The previous approaches towards this problem were based on\nexpensive QR decomposition of correlation matrices, we present an efficient\nalgorithm using the power iteration method. We analyze the protocol for\ncorrectness, security, and efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2010 05:22:12 GMT"}, {"version": "v2", "created": "Sat, 1 May 2010 10:47:51 GMT"}, {"version": "v3", "created": "Wed, 28 Jul 2010 23:37:35 GMT"}], "update_date": "2010-07-30", "authors_parsed": [["Pathak", "Manas A.", ""], ["Raj", "Bhiksha", ""]]}, {"id": "1002.4315", "submitter": "Arnab Bhattacharya", "authors": "Sourav Dutta, Arnab Bhattacharya", "title": "Mining Statistically Significant Substrings Based on the Chi-Square\n  Measure", "comments": "10 pages, 7 figures, to appear in PAKDD 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the vast reservoirs of data stored worldwide, efficient mining of data\nfrom a large information store has emerged as a great challenge. Many databases\nlike that of intrusion detection systems, web-click records, player statistics,\ntexts, proteins etc., store strings or sequences. Searching for an unusual\npattern within such long strings of data has emerged as a requirement for\ndiverse applications. Given a string, the problem then is to identify the\nsubstrings that differs the most from the expected or normal behavior, i.e.,\nthe substrings that are statistically significant. In other words, these\nsubstrings are less likely to occur due to chance alone and may point to some\ninteresting information or phenomenon that warrants further exploration. To\nthis end, we use the chi-square measure. We propose two heuristics for\nretrieving the top-k substrings with the largest chi-square measure. We show\nthat the algorithms outperform other competing algorithms in the runtime, while\nmaintaining a high approximation ratio of more than 0.96.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2010 12:45:31 GMT"}, {"version": "v2", "created": "Sat, 6 Mar 2010 11:24:01 GMT"}], "update_date": "2010-03-09", "authors_parsed": [["Dutta", "Sourav", ""], ["Bhattacharya", "Arnab", ""]]}]