[{"id": "1204.0176", "submitter": "M. Rizwan Jameel Qureshi Dr.", "authors": "M. Rizwan Jameel Qureshi, Mehboob Sharif and Nayyar Iqbal", "title": "Using Fuzzy Logic to Evaluate Normalization Completeness for An Improved\n  Database Design", "comments": "8 Pages", "journal-ref": "International Journal of Information Technology and Computer\n  Science (IJTCS), Hong Kong, Vol. 4/2, pp. 48-55, March 2012", "doi": "10.5815/ijitcs.2012.02.07", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new approach, to measure normalization completeness for conceptual model,\nis introduced using quantitative fuzzy functionality in this paper. We measure\nthe normalization completeness of the conceptual model in two steps. In the\nfirst step, different normalization techniques are analyzed up to Boyce Codd\nNormal Form (BCNF) to find the current normal form of the relation. In the\nsecond step, fuzzy membership values are used to scale the normal form between\n0 and 1. Case studies to explain schema transformation rules and measurements.\nNormalization completeness is measured by considering completeness attributes,\npreventing attributes of the functional dependencies and total number of\nattributes such as if the functional dependency is non-preventing then the\nattributes of that functional dependency are completeness attributes. The\nattributes of functional dependency which prevent to go to the next normal form\nare called preventing attributes.\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2012 08:17:47 GMT"}], "update_date": "2012-04-03", "authors_parsed": [["Qureshi", "M. Rizwan Jameel", ""], ["Sharif", "Mehboob", ""], ["Iqbal", "Nayyar", ""]]}, {"id": "1204.0864", "submitter": "Hai Phan Nhat", "authors": "Phan Nhat Hai, Pascal Poncelet, Maguelonne Teisseire", "title": "GeT_Move: An Efficient and Unifying Spatio-Temporal Pattern Mining\n  Algorithm for Moving Objects", "comments": "17 pages, 24 figures, submitted to KDD, TKDD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Recent improvements in positioning technology has led to a much wider\navailability of massive moving object data. A crucial task is to find the\nmoving objects that travel together. Usually, these object sets are called\nspatio-temporal patterns. Due to the emergence of many different kinds of\nspatio-temporal patterns in recent years, different approaches have been\nproposed to extract them. However, each approach only focuses on mining a\nspecific kind of pattern. In addition to being a painstaking task due to the\nlarge number of algorithms used to mine and manage patterns, it is also time\nconsuming. Moreover, we have to execute these algorithms again whenever new\ndata are added to the existing database. To address these issues, we first\nredefine spatio-temporal patterns in the itemset context. Secondly, we propose\na unifying approach, named GeT_Move, which uses a frequent closed itemset-based\nspatio-temporal pattern-mining algorithm to mine and manage different\nspatio-temporal patterns. GeT_Move is implemented in two versions which are\nGeT_Move and Incremental GeT_Move. To optimize the efficiency and to free the\nparameters setting, we also propose a Parameter Free Incremental GeT_Move\nalgorithm. Comprehensive experiments are performed on real datasets as well as\nlarge synthetic datasets to demonstrate the effectiveness and efficiency of our\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2012 05:07:47 GMT"}], "update_date": "2016-11-26", "authors_parsed": [["Hai", "Phan Nhat", ""], ["Poncelet", "Pascal", ""], ["Teisseire", "Maguelonne", ""]]}, {"id": "1204.1185", "submitter": "Michal Batko", "authors": "Petra Budikova, Michal Batko, Pavel Zezula", "title": "Query Language for Complex Similarity Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For complex data types such as multimedia, traditional data management\nmethods are not suitable. Instead of attribute matching approaches, access\nmethods based on object similarity are becoming popular. Recently, this\nresulted in an intensive research of indexing and searching methods for the\nsimilarity-based retrieval. Nowadays, many efficient methods are already\navailable, but using them to build an actual search system still requires\nspecialists that tune the methods and build the system manually. Several\nattempts have already been made to provide a more convenient high-level\ninterface in a form of query languages for such systems, but these are limited\nto support only basic similarity queries. In this paper, we propose a new\nlanguage that allows to formulate content-based queries in a flexible way,\ntaking into account the functionality offered by a particular search engine in\nuse. To ensure this, the language is based on a general data model with an\nabstract set of operations. Consequently, the language supports various\nadvanced query operations such as similarity joins, reverse nearest neighbor\nqueries, or distinct kNN queries, as well as multi-object and multi-modal\nqueries. The language is primarily designed to be used with the MESSIF\nframework for content-based searching but can be employed by other retrieval\nsystems as well.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2012 11:30:49 GMT"}], "update_date": "2012-04-06", "authors_parsed": [["Budikova", "Petra", ""], ["Batko", "Michal", ""], ["Zezula", "Pavel", ""]]}, {"id": "1204.1598", "submitter": "Tejaswini Apte", "authors": "Tejaswini Apte, Dr. Maya Ingle, Dr. A.K. Goyal", "title": "Improving Seek Time for Column Store Using MMH Algorithm", "comments": null, "journal-ref": "(IJACSA) International Journal of Advanced Computer Science and\n  Applications Vol. 3, No.2, 2012", "doi": null, "report-no": null, "categories": "cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hash based search has, proven excellence on large data warehouses stored in\ncolumn store. Data distribution has significant impact on hash based search. To\nreduce impact of data distribution, we have proposed Memory Managed Hash (MMH)\nalgorithm that uses shift XOR group for Queries and Transactions in column\nstore. Our experiments show that MMH improves read and write throughput by 22%\nfor TPC-H distribution.\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2012 06:24:53 GMT"}], "update_date": "2012-04-10", "authors_parsed": [["Apte", "Tejaswini", ""], ["Ingle", "Dr. Maya", ""], ["Goyal", "Dr. A. K.", ""]]}, {"id": "1204.1710", "submitter": "Dhyanendra  Jain", "authors": "Dhyanendra Jain", "title": "Hiding Sensitive Association Rules without Altering the Support of\n  Sensitive Item(s)", "comments": "10 pages", "journal-ref": null, "doi": "10.5121/ijaia.2012.3207", "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Association rule mining is an important data-mining technique that finds\ninteresting association among a large set of data items. Since it may disclose\npatterns and various kinds of sensitive knowledge that are difficult to find\notherwise, it may pose a threat to the privacy of discovered confidential\ninformation. Such information is to be protected against unauthorized access.\nMany strategies had been proposed to hide the information. Some use distributed\ndatabases over several sites, data perturbation, clustering, and data\ndistortion techniques. Hiding sensitive rules problem, and still not\nsufficiently investigated, is the requirement to balance the confidentiality of\nthe disclosed data with the legitimate needs of the user. The proposed approach\nuses the data distortion technique where the position of the sensitive items is\naltered but its support is never changed. The size of the database remains the\nsame. It uses the idea of representative rules to prune the rules first and\nthen hides the sensitive rules. Advantage of this approach is that it hides\nmaximum number of rules however, the existing approaches fail to hide all the\ndesired rules, which are supposed to be hidden in minimum number of passes. The\npaper also compares of the proposed approach with existing ones.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2012 04:52:49 GMT"}], "update_date": "2012-04-10", "authors_parsed": [["Jain", "Dhyanendra", ""]]}, {"id": "1204.1754", "submitter": "Anish Das Sarma", "authors": "Foto N. Afrati, Anish Das Sarma, Semih Salihoglu, Jeffrey D. Ullman", "title": "Vision Paper: Towards an Understanding of the Limits of Map-Reduce\n  Computation", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A significant amount of recent research work has addressed the problem of\nsolving various data management problems in the cloud. The major algorithmic\nchallenges in map-reduce computations involve balancing a multitude of factors\nsuch as the number of machines available for mappers/reducers, their memory\nrequirements, and communication cost (total amount of data sent from mappers to\nreducers). Most past work provides custom solutions to specific problems, e.g.,\nperforming fuzzy joins in map-reduce, clustering, graph analyses, and so on.\nWhile some problems are amenable to very efficient map-reduce algorithms, some\nother problems do not lend themselves to a natural distribution, and have\nprovable lower bounds. Clearly, the ease of \"map-reducability\" is closely\nrelated to whether the problem can be partitioned into independent pieces,\nwhich are distributed across mappers/reducers. What makes a problem\ndistributable? Can we characterize general properties of problems that\ndetermine how easy or hard it is to find efficient map-reduce algorithms?\n  This is a vision paper that attempts to answer the questions described above.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2012 19:10:07 GMT"}], "update_date": "2012-04-10", "authors_parsed": [["Afrati", "Foto N.", ""], ["Sarma", "Anish Das", ""], ["Salihoglu", "Semih", ""], ["Ullman", "Jeffrey D.", ""]]}, {"id": "1204.2541", "submitter": "David Novak", "authors": "Petr Volny, David Novak, Pavel Zezula", "title": "Employing Subsequence Matching in Audio Data Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": "FIMU-RS-2011-04", "categories": "cs.SD cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We overview current problems of audio retrieval and time-series subsequence\nmatching. We discuss the usage of subsequence matching approaches in audio data\nprocessing, especially in automatic speech recognition (ASR) area and we aim at\nimproving performance of the retrieval process. To overcome the problems known\nfrom the time-series area like the occurrence of implementation bias and data\nbias we present a Subsequence Matching Framework as a tool for fast\nprototyping, building, and testing similarity search subsequence matching\napplications. The framework is build on top of MESSIF (Metric Similarity Search\nImplementation Framework) and thus the subsequence matching algorithms can\nexploit advanced similarity indexes in order to significantly increase their\nquery processing performance. To prove our concept we provide a design of\nquery-by-example spoken term detection type of application with the usage of\nphonetic posteriograms and subsequence matching approach.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2012 07:01:31 GMT"}], "update_date": "2012-04-13", "authors_parsed": [["Volny", "Petr", ""], ["Novak", "David", ""], ["Zezula", "Pavel", ""]]}, {"id": "1204.2606", "submitter": "Aleksandra Korolova", "authors": "Krishnaram Kenthapadi, Aleksandra Korolova, Ilya Mironov, Nina Mishra", "title": "Privacy via the Johnson-Lindenstrauss Transform", "comments": "24 pages", "journal-ref": "Journal of Privacy and Confidentiality, Volume 5, Issue 1, Pages\n  39-71, 2013", "doi": "10.29012/jpc.v5i1.625", "report-no": null, "categories": "cs.DS cs.CY cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose that party A collects private information about its users, where each\nuser's data is represented as a bit vector. Suppose that party B has a\nproprietary data mining algorithm that requires estimating the distance between\nusers, such as clustering or nearest neighbors. We ask if it is possible for\nparty A to publish some information about each user so that B can estimate the\ndistance between users without being able to infer any private bit of a user.\nOur method involves projecting each user's representation into a random,\nlower-dimensional space via a sparse Johnson-Lindenstrauss transform and then\nadding Gaussian noise to each entry of the lower-dimensional representation. We\nshow that the method preserves differential privacy---where the more privacy is\ndesired, the larger the variance of the Gaussian noise. Further, we show how to\napproximate the true distances between users via only the lower-dimensional,\nperturbed data. Finally, we consider other perturbation methods such as\nrandomized response and draw comparisons to sketch-based methods. While the\ngoal of releasing user-specific data to third parties is more broad than\npreserving distances, this work shows that distance computations with privacy\nis an achievable goal.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2012 03:06:58 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Kenthapadi", "Krishnaram", ""], ["Korolova", "Aleksandra", ""], ["Mironov", "Ilya", ""], ["Mishra", "Nina", ""]]}, {"id": "1204.2610", "submitter": "Kiran P", "authors": "P. Kiran, S Sathish Kumar and N.P. Kavya", "title": "A Novel Framework using Elliptic Curve Cryptography for Extremely Secure\n  Transmission in Distributed Privacy Preserving Data Mining", "comments": "8 pages", "journal-ref": "Advanced Computing: An International Journal ( ACIJ ), Vol.3,\n  No.2, March 2012", "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy Preserving Data Mining is a method which ensures privacy of\nindividual information during mining. Most important task involves retrieving\ninformation from multiple data bases which is distributed. The data once in the\ndata warehouse can be used by mining algorithms to retrieve confidential\ninformation. The proposed framework has two major tasks, secure transmission\nand privacy of confidential information during mining. Secure transmission is\nhandled by using elliptic curve cryptography and data distortion for privacy\npreservation ensuring highly secure environment.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2012 03:49:26 GMT"}], "update_date": "2012-04-13", "authors_parsed": [["Kiran", "P.", ""], ["Kumar", "S Sathish", ""], ["Kavya", "N. P.", ""]]}, {"id": "1204.2731", "submitter": "Anika Gro{\\ss}", "authors": "Anika Gross, Michael Hartung, Andreas Thor, Erhard Rahm", "title": "How do Ontology Mappings Change in the Life Sciences?", "comments": "Keywords: mapping evolution, ontology matching, ontology evolution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mappings between related ontologies are increasingly used to support data\nintegration and analysis tasks. Changes in the ontologies also require the\nadaptation of ontology mappings. So far the evolution of ontology mappings has\nreceived little attention albeit ontologies change continuously especially in\nthe life sciences. We therefore analyze how mappings between popular life\nscience ontologies evolve for different match algorithms. We also evaluate\nwhich semantic ontology changes primarily affect the mappings. We further\ninvestigate alternatives to predict or estimate the degree of future mapping\nchanges based on previous ontology and mapping transitions.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2012 14:05:37 GMT"}], "update_date": "2012-04-13", "authors_parsed": [["Gross", "Anika", ""], ["Hartung", "Michael", ""], ["Thor", "Andreas", ""], ["Rahm", "Erhard", ""]]}, {"id": "1204.3223", "submitter": "Minyar Sassi", "authors": "Oussama Tlili, Minyar Sassi, Habib Ounelli", "title": "Intelligent Database Flexible Querying System by Approximate Query\n  Processing", "comments": "8 pages, 5 figures, 9 tables", "journal-ref": "The Third International Conference on Advances in Databases,\n  Knowledge, and Data Applications (DBKDA 2011), January 23-28, 2011", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Database flexible querying is an alternative to the classic one for users.\nThe use of Formal Concepts Analysis (FCA) makes it possible to make approximate\nanswers that those turned over by a classic DataBase Management System (DBMS).\nSome applications do not need exact answers. However, flexible querying can be\nexpensive in response time. This time is more significant when the flexible\nquerying require the calculation of aggregate functions (\"Sum\", \"Avg\", \"Count\",\n\"Var\" etc.). In this paper, we propose an approach which tries to solve this\nproblem by using Approximate Query Processing (AQP).\n", "versions": [{"version": "v1", "created": "Sat, 14 Apr 2012 22:30:15 GMT"}], "update_date": "2012-04-17", "authors_parsed": [["Tlili", "Oussama", ""], ["Sassi", "Minyar", ""], ["Ounelli", "Habib", ""]]}, {"id": "1204.3432", "submitter": "Tomasz Gogacz", "authors": "T. Gogacz, J. Marcinkowski", "title": "Converging to the Chase - a Tool for Finite Controllability", "comments": null, "journal-ref": null, "doi": "10.1109/LICS.2013.61", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We solve a problem, stated in [CGP10], showing that Sticky Datalog, defined\nin the cited paper as an element of the Datalog\\pm project, has the finite\ncontrollability property. In order to do that, we develop a technique, which we\nbelieve can have further applications, of approximating Chase(D, T), for a\ndatabase instance D and some sets of tuple generating dependencies T, by an\ninfinite sequence of finite structures, all of them being models of T.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2012 10:13:43 GMT"}, {"version": "v2", "created": "Thu, 2 Aug 2012 09:52:21 GMT"}, {"version": "v3", "created": "Tue, 4 Nov 2014 16:17:12 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Gogacz", "T.", ""], ["Marcinkowski", "J.", ""]]}, {"id": "1204.3581", "submitter": "Giuseppe Ottaviano", "authors": "Roberto Grossi, Giuseppe Ottaviano", "title": "The Wavelet Trie: Maintaining an Indexed Sequence of Strings in\n  Compressed Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An indexed sequence of strings is a data structure for storing a string\nsequence that supports random access, searching, range counting and analytics\noperations, both for exact matches and prefix search. String sequences lie at\nthe core of column-oriented databases, log processing, and other storage and\nquery tasks. In these applications each string can appear several times and the\norder of the strings in the sequence is relevant. The prefix structure of the\nstrings is relevant as well: common prefixes are sought in strings to extract\ninteresting features from the sequence. Moreover, space-efficiency is highly\ndesirable as it translates directly into higher performance, since more data\ncan fit in fast memory.\n  We introduce and study the problem of compressed indexed sequence of strings,\nrepresenting indexed sequences of strings in nearly-optimal compressed space,\nboth in the static and dynamic settings, while preserving provably good\nperformance for the supported operations.\n  We present a new data structure for this problem, the Wavelet Trie, which\ncombines the classical Patricia Trie with the Wavelet Tree, a succinct data\nstructure for storing a compressed sequence. The resulting Wavelet Trie\nsmoothly adapts to a sequence of strings that changes over time. It improves on\nthe state-of-the-art compressed data structures by supporting a dynamic\nalphabet (i.e. the set of distinct strings) and prefix queries, both crucial\nrequirements in the aforementioned applications, and on traditional indexes by\nreducing space occupancy to close to the entropy of the sequence.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2012 17:50:35 GMT"}], "update_date": "2012-04-17", "authors_parsed": [["Grossi", "Roberto", ""], ["Ottaviano", "Giuseppe", ""]]}, {"id": "1204.3677", "submitter": "Sushovan De", "authors": "Yuheng Hu, Sushovan De, Yi Chen, Subbarao Kambhampati", "title": "Bayesian Data Cleaning for Web Data", "comments": "6 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data Cleaning is a long standing problem, which is growing in importance with\nthe mass of uncurated web data. State of the art approaches for handling\ninconsistent data are systems that learn and use conditional functional\ndependencies (CFDs) to rectify data. These methods learn data\npatterns--CFDs--from a clean sample of the data and use them to rectify the\ndirty/inconsistent data. While getting a clean training sample is feasible in\nenterprise data scenarios, it is infeasible in web databases where there is no\nseparate curated data. CFD based methods are unfortunately particularly\nsensitive to noise; we will empirically demonstrate that the number of CFDs\nlearned falls quite drastically with even a small amount of noise. In order to\novercome this limitation, we propose a fully probabilistic framework for\ncleaning data. Our approach involves learning both the generative and error\n(corruption) models of the data and using them to clean the data. For\ngenerative models, we learn Bayes networks from the data. For error models, we\nconsider a maximum entropy framework for combing multiple error processes. The\ngenerative and error models are learned directly from the noisy data. We\npresent the details of the framework and demonstrate its effectiveness in\nrectifying web data.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2012 00:59:53 GMT"}], "update_date": "2012-04-18", "authors_parsed": [["Hu", "Yuheng", ""], ["De", "Sushovan", ""], ["Chen", "Yi", ""], ["Kambhampati", "Subbarao", ""]]}, {"id": "1204.3726", "submitter": "Antoine Pigeau A.P.", "authors": "Guillaume Raschia and Martin Theobald and Ioana Manolescu", "title": "Proceedings of the first International Workshop On Open Data, WOD-2012", "comments": "Website of the workshop : https://sites.google.com/site/opendata2012/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  WOD-2012 aims at facilitating new trends and ideas from a broad range of\ntopics concerned within the widely-spread Open Data movement, from the\nviewpoint of computer science research.\n  While being most commonly known from the recent Linked Open Data movement,\nthe concept of publishing data explicitly as Open Data has meanwhile developed\nmany variants and facets that go beyond publishing large and highly structured\nRDF/S repositories. Open Data comprises text and semi-structured data, but also\nopen multi-modal contents, including music, images, and videos. With the\nincreasing amount of data that is published by governments (see, e.g.,\ndata.gov, data.gov.uk or data.gouv.fr), by international organizations\n(data.worldbank.org or data.undp.org) and by scientific communities (tdar.org,\ncds.u-strasbg.fr, GenBank, IRIS or KNB) explicitly under an Open Data policy,\nnew challenges arise not only due to the scale at which this data becomes\navailable.\n  A number of community-based conferences accommodate tracks or workshops which\nare dedicated to Open Data. However, WOD aims to be a premier venue to gather\nresearchers and practitioners who are contributing to and interested in the\nemerging field of managing Open Data from a computer science perspective.\nHence, it is a unique opportunity to find in a single place up-to-date\nscientific works on Web-scale Open Data issues that have so far only partially\nbeen addressed by different research communities such as Databases, Data Mining\nand Knowledge Management, Distributed Systems, Data Privacy, and Data\nVisualization.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2012 08:36:59 GMT"}, {"version": "v2", "created": "Wed, 9 May 2012 13:07:46 GMT"}, {"version": "v3", "created": "Mon, 21 May 2012 12:41:30 GMT"}], "update_date": "2012-05-22", "authors_parsed": [["Raschia", "Guillaume", ""], ["Theobald", "Martin", ""], ["Manolescu", "Ioana", ""]]}, {"id": "1204.4093", "submitter": "Casey Bennett", "authors": "Casey Bennett", "title": "Utilizing RxNorm to Support Practical Computing Applications: Capturing\n  Medication History in Live Electronic Health Records", "comments": "Appendix (including SQL/DDL Code) available by author request.\n  Keywords: RxNorm; Electronic Health Record; Medication History;\n  Interoperability; Unified Medical Language System; Search Optimization", "journal-ref": "Journal of Biomedical Informatics 45: 634-641 (2012)", "doi": "10.1016/j.jbi.2012.02.011", "report-no": null, "categories": "cs.DB cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RxNorm was utilized as the basis for direct-capture of medication history\ndata in a live EHR system deployed in a large, multi-state outpatient\nbehavioral healthcare provider in the United States serving over 75,000\ndistinct patients each year across 130 clinical locations. This tool\nincorporated auto-complete search functionality for medications and proper\ndosage identification assistance. The overarching goal was to understand if and\nhow standardized terminologies like RxNorm can be used to support practical\ncomputing applications in live EHR systems. We describe the stages of\nimplementation, approaches used to adapt RxNorm's data structure for the\nintended EHR application, and the challenges faced. We evaluate the\nimplementation using a four-factor framework addressing flexibility, speed,\ndata integrity, and medication coverage. RxNorm proved to be functional for the\nintended application, given appropriate adaptations to address high-speed\ninput/output (I/O) requirements of a live EHR and the flexibility required for\ndata entry in multiple potential clinical scenarios. Future research around\nsearch optimization for medication entry, user profiling, and linking RxNorm to\ndrug classification schemes holds great potential for improving the user\nexperience and utility of medication data in EHRs.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2012 14:27:42 GMT"}, {"version": "v2", "created": "Wed, 15 Aug 2012 17:09:00 GMT"}], "update_date": "2012-08-16", "authors_parsed": [["Bennett", "Casey", ""]]}, {"id": "1204.4685", "submitter": "Florian Rabe", "authors": "Florian Rabe", "title": "A Query Language for Formal Mathematical Libraries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most promising applications of mathematical knowledge management\nis search: Even if we restrict attention to the tiny fragment of mathematics\nthat has been formalized, the amount exceeds the comprehension of an individual\nhuman.\n  Based on the generic representation language MMT, we introduce the\nmathematical query language QMT: It combines simplicity, expressivity, and\nscalability while avoiding a commitment to a particular logical formalism. QMT\ncan integrate various search paradigms such as unification, semantic web, or\nXQuery style queries, and QMT queries can span different mathematical\nlibraries.\n  We have implemented QMT as a part of the MMT API. This combination provides a\nscalable indexing and query engine that can be readily applied to any library\nof mathematical knowledge. While our focus here is on libraries that are\navailable in a content markup language, QMT naturally extends to presentation\nand narration markup languages.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2012 17:36:30 GMT"}], "update_date": "2012-04-23", "authors_parsed": [["Rabe", "Florian", ""]]}, {"id": "1204.4927", "submitter": "Casey Bennett", "authors": "Casey Bennett, Tom Doub, Rebecca Selove", "title": "EHRs Connect Research and Practice: Where Predictive Modeling,\n  Artificial Intelligence, and Clinical Decision Support Intersect", "comments": "Keywords: Data Mining; Decision Support Systems, Clinical; Electronic\n  Health Records; Implementation; Evidence-Based Medicine; Data Warehouse;\n  (2012). EHRs Connect Research and Practice: Where Predictive Modeling,\n  Artificial Intelligence, and Clinical Decision Support Intersect. Health\n  Policy and Technology. arXiv admin note: substantial text overlap with\n  arXiv:1112.1668", "journal-ref": "Health Policy and Technology 1(2): 105-114 (2012)", "doi": "10.1016/j.hlpt.2012.03.001", "report-no": null, "categories": "cs.AI cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objectives: Electronic health records (EHRs) are only a first step in\ncapturing and utilizing health-related data - the challenge is turning that\ndata into useful information. Furthermore, EHRs are increasingly likely to\ninclude data relating to patient outcomes, functionality such as clinical\ndecision support, and genetic information as well, and, as such, can be seen as\nrepositories of increasingly valuable information about patients' health\nconditions and responses to treatment over time. Methods: We describe a case\nstudy of 423 patients treated by Centerstone within Tennessee and Indiana in\nwhich we utilized electronic health record data to generate predictive\nalgorithms of individual patient treatment response. Multiple models were\nconstructed using predictor variables derived from clinical, financial and\ngeographic data. Results: For the 423 patients, 101 deteriorated, 223 improved\nand in 99 there was no change in clinical condition. Based on modeling of\nvarious clinical indicators at baseline, the highest accuracy in predicting\nindividual patient response ranged from 70-72% within the models tested. In\nterms of individual predictors, the Centerstone Assessment of Recovery Level -\nAdult (CARLA) baseline score was most significant in predicting outcome over\ntime (odds ratio 4.1 + 2.27). Other variables with consistently significant\nimpact on outcome included payer, diagnostic category, location and provision\nof case management services. Conclusions: This approach represents a promising\navenue toward reducing the current gap between research and practice across\nhealthcare, developing data-driven clinical decision support based on\nreal-world populations, and serving as a component of embedded clinical\nartificial intelligences that \"learn\" over time.\n", "versions": [{"version": "v1", "created": "Sun, 22 Apr 2012 19:24:40 GMT"}], "update_date": "2012-08-20", "authors_parsed": [["Bennett", "Casey", ""], ["Doub", "Tom", ""], ["Selove", "Rebecca", ""]]}, {"id": "1204.4948", "submitter": "S{\\l}awomir Staworko", "authors": "Jakub Michaliszyn, Anca Muscholl, S{\\l}awek Staworko, Piotr Wieczorek\n  and Zhilin Wu", "title": "On Injective Embeddings of Tree Patterns", "comments": "Under conference submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study three different kinds of embeddings of tree patterns:\nweakly-injective, ancestor-preserving, and lca-preserving. While each of them\nis often referred to as injective embedding, they form a proper hierarchy and\ntheir computational properties vary (from P to NP-complete). We present a\nthorough study of the complexity of the model checking problem i.e., is there\nan embedding of a given tree pattern in a given tree, and we investigate the\nimpact of various restrictions imposed on the tree pattern: bound on the degree\nof a node, bound on the height, and type of allowed labels and edges.\n", "versions": [{"version": "v1", "created": "Sun, 22 Apr 2012 23:47:28 GMT"}, {"version": "v2", "created": "Sat, 28 Apr 2012 18:28:23 GMT"}], "update_date": "2012-05-01", "authors_parsed": [["Michaliszyn", "Jakub", ""], ["Muscholl", "Anca", ""], ["Staworko", "S\u0142awek", ""], ["Wieczorek", "Piotr", ""], ["Wu", "Zhilin", ""]]}, {"id": "1204.5648", "submitter": "Ibrahim El Bitar", "authors": "Ibrahim El Bitar, Fatima Zahra Belouadha and Ounsa Roudies", "title": "Taxonomy and synthesis of Web services querying languages", "comments": "7 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most works on Web services has focused on discovery, composition and\nselection processes of these kinds of services. Other few works were interested\nin how to represent Web services search queries. However, these queries cannot\nbe processed by ensuring a high level of performance without being adequately\nrepresented first. To this end, different query languages were designed. Even\nso, in the absence of a standard, these languages are quite various. Their\ndiversity makes it difficult choosing the most suitable language. In fact, this\nlanguage should be able to cover all types of preferences or requirements of\nclients such as their functional, nonfunctional,temporal or even specific\nconstraints as is the case of geographical or spatial constraints and meet\ntheir needs and preferences helping to provide them the best answer. It must\nalso be mutually simple and imposes no restrictions or at least not too many\nconstraints in terms of prior knowledge to use and also provide a formal or\nsemi-formal queries presentation to support their automatic post-processing. A\ncomparative study is eventually established to allow to reveal the advantages\nand limitations of various existing languages in this context. It is a\nsynthesis of this category of languages discussing their performance level and\ntheir capability to respond to various needs related to the Web services\nresearch and discovery case. The criterions identified at this stage may, in\nour opinion, constitute then the main pre-requisite that a language should\nsatisfy to be called perfect or to be a future standard.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2012 13:01:23 GMT"}], "update_date": "2012-04-26", "authors_parsed": [["Bitar", "Ibrahim El", ""], ["Belouadha", "Fatima Zahra", ""], ["Roudies", "Ounsa", ""]]}, {"id": "1204.6076", "submitter": "Kyriakos Mouratidis", "authors": "Kyriakos Mouratidis, Man Lung Yiu", "title": "Shortest Path Computation with No Information Leakage", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 8, pp.\n  692-703 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shortest path computation is one of the most common queries in location-based\nservices (LBSs). Although particularly useful, such queries raise serious\nprivacy concerns. Exposing to a (potentially untrusted) LBS the client's\nposition and her destination may reveal personal information, such as social\nhabits, health condition, shopping preferences, lifestyle choices, etc. The\nonly existing method for privacy-preserving shortest path computation follows\nthe obfuscation paradigm; it prevents the LBS from inferring the source and\ndestination of the query with a probability higher than a threshold. This\nimplies, however, that the LBS still deduces some information (albeit not\nexact) about the client's location and her destination. In this paper we aim at\nstrong privacy, where the adversary learns nothing about the shortest path\nquery. We achieve this via established private information retrieval\ntechniques, which we treat as black-box building blocks. Experiments on real,\nlarge-scale road networks assess the practicality of our schemes.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2012 23:25:02 GMT"}], "update_date": "2012-04-30", "authors_parsed": [["Mouratidis", "Kyriakos", ""], ["Yiu", "Man Lung", ""]]}, {"id": "1204.6077", "submitter": "Ahmed Metwally", "authors": "Ahmed Metwally, Christos Faloutsos", "title": "V-SMART-Join: A Scalable MapReduce Framework for All-Pair Similarity\n  Joins of Multisets and Vectors", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 8, pp.\n  704-715 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes V-SMART-Join, a scalable MapReduce-based framework for\ndiscovering all pairs of similar entities. The V-SMART-Join framework is\napplicable to sets, multisets, and vectors. V-SMART-Join is motivated by the\nobserved skew in the underlying distributions of Internet traffic, and is a\nfamily of 2-stage algorithms, where the first stage computes and joins the\npartial results, and the second stage computes the similarity exactly for all\ncandidate pairs. The V-SMART-Join algorithms are very efficient and scalable in\nthe number of entities, as well as their cardinalities. They were up to 30\ntimes faster than the state of the art algorithm, VCL, when compared on a real\ndataset of a small size. We also established the scalability of the proposed\nalgorithms by running them on a dataset of a realistic size, on which VCL never\nsucceeded to finish. Experiments were run using real datasets of IPs and\ncookies, where each IP is represented as a multiset of cookies, and the goal is\nto discover similar IPs to identify Internet proxies.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2012 23:25:14 GMT"}], "update_date": "2012-04-30", "authors_parsed": [["Metwally", "Ahmed", ""], ["Faloutsos", "Christos", ""]]}, {"id": "1204.6078", "submitter": "Yucheng Low", "authors": "Yucheng Low, Joseph Gonzalez, Aapo Kyrola, Danny Bickson, Carlos\n  Guestrin, Joseph M. Hellerstein", "title": "Distributed GraphLab: A Framework for Machine Learning in the Cloud", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 8, pp.\n  716-727 (2012)", "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While high-level data parallel frameworks, like MapReduce, simplify the\ndesign and implementation of large-scale data processing systems, they do not\nnaturally or efficiently support many important data mining and machine\nlearning algorithms and can lead to inefficient learning systems. To help fill\nthis critical void, we introduced the GraphLab abstraction which naturally\nexpresses asynchronous, dynamic, graph-parallel computation while ensuring data\nconsistency and achieving a high degree of parallel performance in the\nshared-memory setting. In this paper, we extend the GraphLab framework to the\nsubstantially more challenging distributed setting while preserving strong data\nconsistency guarantees. We develop graph based extensions to pipelined locking\nand data versioning to reduce network congestion and mitigate the effect of\nnetwork latency. We also introduce fault tolerance to the GraphLab abstraction\nusing the classic Chandy-Lamport snapshot algorithm and demonstrate how it can\nbe easily implemented by exploiting the GraphLab abstraction itself. Finally,\nwe evaluate our distributed implementation of the GraphLab abstraction on a\nlarge Amazon EC2 deployment and show 1-2 orders of magnitude performance gains\nover Hadoop-based implementations.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2012 23:25:20 GMT"}], "update_date": "2012-04-30", "authors_parsed": [["Low", "Yucheng", ""], ["Gonzalez", "Joseph", ""], ["Kyrola", "Aapo", ""], ["Bickson", "Danny", ""], ["Guestrin", "Carlos", ""], ["Hellerstein", "Joseph M.", ""]]}, {"id": "1204.6079", "submitter": "Rishabh Singh", "authors": "Rishabh Singh, Sumit Gulwani", "title": "Learning Semantic String Transformations from Examples", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 8, pp.\n  740-751 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of performing semantic transformations on strings,\nwhich may represent a variety of data types (or their combination) such as a\ncolumn in a relational table, time, date, currency, etc. Unlike syntactic\ntransformations, which are based on regular expressions and which interpret a\nstring as a sequence of characters, semantic transformations additionally\nrequire exploiting the semantics of the data type represented by the string,\nwhich may be encoded as a database of relational tables. Manually performing\nsuch transformations on a large collection of strings is error prone and\ncumbersome, while programmatic solutions are beyond the skill-set of end-users.\nWe present a programming by example technology that allows end-users to\nautomate such repetitive tasks. We describe an expressive transformation\nlanguage for semantic manipulation that combines table lookup operations and\nsyntactic manipulations. We then present a synthesis algorithm that can learn\nall transformations in the language that are consistent with the user-provided\nset of input-output examples. We have implemented this technology as an add-in\nfor the Microsoft Excel Spreadsheet system and have evaluated it successfully\nover several benchmarks picked from various Excel help-forums.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2012 23:25:35 GMT"}], "update_date": "2012-04-30", "authors_parsed": [["Singh", "Rishabh", ""], ["Gulwani", "Sumit", ""]]}, {"id": "1204.6080", "submitter": "Changbin Liu", "authors": "Changbin Liu, Lu Ren, Boon Thau Loo, Yun Mao, Prithwish Basu", "title": "Cologne: A Declarative Distributed Constraint Optimization Platform", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 8, pp.\n  752-763 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Cologne, a declarative optimization platform that enables\nconstraint optimization problems (COPs) to be declaratively specified and\nincrementally executed in distributed systems. Cologne integrates a declarative\nnetworking engine with an off-the-shelf constraint solver. We have developed\nthe Colog language that combines distributed Datalog used in declarative\nnetworking with language constructs for specifying goals and constraints used\nin COPs. Cologne uses novel query processing strategies for processing Colog\nprograms, by combining the use of bottom-up distributed Datalog evaluation with\ntop-down goal-oriented constraint solving. Using case studies based on cloud\nand wireless network optimizations, we demonstrate that Cologne (1) can\nflexibly support a wide range of policy-based optimizations in distributed\nsystems, (2) results in orders of magnitude less code compared to imperative\nimplementations, and (3) is highly efficient with low overhead and fast\nconvergence times.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2012 23:25:43 GMT"}], "update_date": "2012-04-30", "authors_parsed": [["Liu", "Changbin", ""], ["Ren", "Lu", ""], ["Loo", "Boon Thau", ""], ["Mao", "Yun", ""], ["Basu", "Prithwish", ""]]}, {"id": "1204.6081", "submitter": "Yi Zhang", "authors": "Yi Zhang, Jun Yang", "title": "Optimizing I/O for Big Array Analytics", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 8, pp.\n  764-775 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big array analytics is becoming indispensable in answering important\nscientific and business questions. Most analysis tasks consist of multiple\nsteps, each making one or multiple passes over the arrays to be analyzed and\ngenerating intermediate results. In the big data setting, I/O optimization is a\nkey to efficient analytics. In this paper, we develop a framework and\ntechniques for capturing a broad range of analysis tasks expressible in\nnested-loop forms, representing them in a declarative way, and optimizing their\nI/O by identifying sharing opportunities. Experiment results show that our\noptimizer is capable of finding execution plans that exploit nontrivial I/O\nsharing opportunities with significant savings.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2012 23:25:50 GMT"}], "update_date": "2012-04-30", "authors_parsed": [["Zhang", "Yi", ""], ["Yang", "Jun", ""]]}, {"id": "1204.6082", "submitter": "Peter Bailis", "authors": "Peter Bailis, Shivaram Venkataraman, Michael J. Franklin, Joseph M.\n  Hellerstein, Ion Stoica", "title": "Probabilistically Bounded Staleness for Practical Partial Quorums", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 8, pp.\n  776-787 (2012)", "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data store replication results in a fundamental trade-off between operation\nlatency and data consistency. In this paper, we examine this trade-off in the\ncontext of quorum-replicated data stores. Under partial, or non-strict quorum\nreplication, a data store waits for responses from a subset of replicas before\nanswering a query, without guaranteeing that read and write replica sets\nintersect. As deployed in practice, these configurations provide only basic\neventual consistency guarantees, with no limit to the recency of data returned.\nHowever, anecdotally, partial quorums are often \"good enough\" for practitioners\ngiven their latency benefits. In this work, we explain why partial quorums are\nregularly acceptable in practice, analyzing both the staleness of data they\nreturn and the latency benefits they offer. We introduce Probabilistically\nBounded Staleness (PBS) consistency, which provides expected bounds on\nstaleness with respect to both versions and wall clock time. We derive a\nclosed-form solution for versioned staleness as well as model real-time\nstaleness for representative Dynamo-style systems under internet-scale\nproduction workloads. Using PBS, we measure the latency-consistency trade-off\nfor partial quorum systems. We quantitatively demonstrate how eventually\nconsistent systems frequently return consistent data within tens of\nmilliseconds while offering significant latency benefits.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2012 23:26:03 GMT"}], "update_date": "2012-04-30", "authors_parsed": [["Bailis", "Peter", ""], ["Venkataraman", "Shivaram", ""], ["Franklin", "Michael J.", ""], ["Hellerstein", "Joseph M.", ""], ["Stoica", "Ion", ""]]}, {"id": "1204.6535", "submitter": "Sandeep Gupta", "authors": "Sandeep Gupta", "title": "Citations, Sequence Alignments, Contagion, and Semantics: On Acyclic\n  Structures and their Randomness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Datasets from several domains, such as life-sciences, semantic web, machine\nlearning, natural language processing, etc. are naturally structured as acyclic\ngraphs. These datasets, particularly those in bio-informatics and computational\nepidemiology, have grown tremendously over the last decade or so. Increasingly,\nas a consequence, there is a need to build and evaluate various strategies for\nprocessing acyclic structured graphs. Most of the proposed research models the\nreal world acyclic structures as random graphs, i.e., they are generated by\nrandomly selecting a subset of edges from all possible edges. Unfortunately the\ngraphs thus generated have predictable and degenerate structures, i.e., the\nresulting graphs will always have almost the same degree distribution and very\nshort paths.\n  Specifically, we show that if $O(n \\log n \\log n)$ edges are added to a\nbinary tree of $n$ nodes then with probability more than $O(1/(\\log n)^{1/n})$\nthe depth of all but $O({\\log \\log n} ^{\\log \\log n})$ vertices of the dag\ncollapses to 1. Experiments show that irregularity, as measured by distribution\nof length of random walks from root to leaves, is also predictable and small.\nThe degree distribution and random walk length properties of real world graphs\nfrom these domains are significantly different from random graphs of similar\nvertex and edge size.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2012 02:19:26 GMT"}, {"version": "v2", "created": "Thu, 26 Jul 2012 16:02:09 GMT"}, {"version": "v3", "created": "Fri, 26 Oct 2012 10:11:36 GMT"}, {"version": "v4", "created": "Tue, 20 Nov 2012 07:07:48 GMT"}, {"version": "v5", "created": "Thu, 17 Jan 2013 19:41:26 GMT"}, {"version": "v6", "created": "Sun, 31 Aug 2014 03:30:09 GMT"}], "update_date": "2014-09-02", "authors_parsed": [["Gupta", "Sandeep", ""]]}]