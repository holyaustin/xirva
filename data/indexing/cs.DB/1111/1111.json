[{"id": "1111.0045", "submitter": "I. Bhattacharya", "authors": "I. Bhattacharya, L. Getoor", "title": "Query-time Entity Resolution", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 30, pages\n  621-657, 2007", "doi": "10.1613/jair.2290", "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity resolution is the problem of reconciling database references\ncorresponding to the same real-world entities. Given the abundance of publicly\navailable databases that have unresolved entities, we motivate the problem of\nquery-time entity resolution quick and accurate resolution for answering\nqueries over such unclean databases at query-time. Since collective entity\nresolution approaches --- where related references are resolved jointly ---\nhave been shown to be more accurate than independent attribute-based resolution\nfor off-line entity resolution, we focus on developing new algorithms for\ncollective resolution for answering entity resolution queries at query-time.\nFor this purpose, we first formally show that, for collective resolution,\nprecision and recall for individual entities follow a geometric progression as\nneighbors at increasing distances are considered. Unfolding this progression\nleads naturally to a two stage expand and resolve query processing strategy. In\nthis strategy, we first extract the related records for a query using two novel\nexpansion operators, and then resolve the extracted records collectively. We\nthen show how the same strategy can be adapted for query-time entity resolution\nby identifying and resolving only those database references that are the most\nhelpful for processing the query. We validate our approach on two large\nreal-world publication databases where we show the usefulness of collective\nresolution and at the same time demonstrate the need for adaptive strategies\nfor query processing. We then show how the same queries can be answered in\nreal-time using our adaptive approach while preserving the gains of collective\nresolution. In addition to experiments on real datasets, we use synthetically\ngenerated data to empirically demonstrate the validity of the performance\ntrends predicted by our analysis of collective entity resolution over a wide\nrange of structural characteristics in the data.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2011 21:48:16 GMT"}], "update_date": "2011-11-02", "authors_parsed": [["Bhattacharya", "I.", ""], ["Getoor", "L.", ""]]}, {"id": "1111.0499", "submitter": "Rafael Grimson Dr.", "authors": "Rafael Grimson, Joos Heintz, Bart Kuijpers", "title": "Evaluating geometric queries using few arithmetic operations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB math.AG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\cp:=(P_1,...,P_s)$ be a given family of $n$-variate polynomials with\ninteger coefficients and suppose that the degrees and logarithmic heights of\nthese polynomials are bounded by $d$ and $h$, respectively. Suppose furthermore\nthat for each $1\\leq i\\leq s$ the polynomial $P_i$ can be evaluated using $L$\narithmetic operations (additions, subtractions, multiplications and the\nconstants 0 and 1). Assume that the family $\\cp$ is in a suitable sense\n\\emph{generic}. We construct a database $\\cal D$, supported by an algebraic\ncomputation tree, such that for each $x\\in [0,1]^n$ the query for the signs of\n$P_1(x),...,P_s(x)$ can be answered using $h d^{\\cO(n^2)}$ comparisons and $nL$\narithmetic operations between real numbers. The arithmetic-geometric tools\ndeveloped for the construction of $\\cal D$ are then employed to exhibit example\nclasses of systems of $n$ polynomial equations in $n$ unknowns whose\nconsistency may be checked using only few arithmetic operations, admitting\nhowever an exponential number of comparisons.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2011 17:57:20 GMT"}], "update_date": "2011-11-03", "authors_parsed": [["Grimson", "Rafael", ""], ["Heintz", "Joos", ""], ["Kuijpers", "Bart", ""]]}, {"id": "1111.0594", "submitter": "Andrey Nikolaev", "authors": "Andrey Nikolaev", "title": "Exploring Oracle RDBMS latches using Solaris DTrace", "comments": "14 pages, 6 figures, 6 tables. MEDIAS 2011 Conference. Limassol,\n  Cyprus", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rise of hundreds cores technologies bring again to the first plan the problem\nof interprocess synchronization in database engines. Spinlocks are widely used\nin contemporary DBMS to synchronize processes at microsecond timescale. Latches\nare Oracle RDBMS specific spinlocks. The latch contention is common to observe\nin contemporary high concurrency OLTP environments.\n  In contrast to system spinlocks used in operating systems kernels, latches\nwork in user context. Such user level spinlocks are influenced by context\npreemption and multitasking. Until recently there were no direct methods to\nmeasure effectiveness of user spinlocks. This became possible with the\nemergence of Solaris 10 Dynamic Tracing framework. DTrace allows tracing and\nprofiling both OS and user applications.\n  This work investigates the possibilities to diagnose and tune Oracle latches.\nIt explores the contemporary latch realization and spinning-blocking\nstrategies, analyses corresponding statistic counters.\n  A mathematical model developed to estimate analytically the effect of tuning\n_SPIN_COUNT value.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2011 18:20:36 GMT"}], "update_date": "2011-11-03", "authors_parsed": [["Nikolaev", "Andrey", ""]]}, {"id": "1111.2530", "submitter": "Ramesh Cindha", "authors": "C. Ramesh, K. V. Chalapati Rao, A. Govardhan", "title": "A semantically enriched web usage based recommendation model", "comments": null, "journal-ref": "International Journal of Computer Science & Information Technology\n  (IJCSIT) Vol 3, No 5, Oct 2011, 193-202", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid growth of internet technologies, Web has become a huge\nrepository of information and keeps growing exponentially under no editorial\ncontrol. However the human capability to read, access and understand Web\ncontent remains constant. This motivated researchers to provide Web\npersonalized online services such as Web recommendations to alleviate the\ninformation overload problem and provide tailored Web experiences to the Web\nusers. Recent studies show that Web usage mining has emerged as a popular\napproach in providing Web personalization. However conventional Web usage based\nrecommender systems are limited in their ability to use the domain knowledge of\nthe Web application. The focus is only on Web usage data. As a consequence the\nquality of the discovered patterns is low. In this paper, we propose a novel\nframework integrating semantic information in the Web usage mining process.\nSequential Pattern Mining technique is applied over the semantic space to\ndiscover the frequent sequential patterns. The frequent navigational patterns\nare extracted in the form of Ontology instances instead of Web page views and\nthe resultant semantic patterns are used for generating Web page\nrecommendations to the user. Experimental results shown are promising and\nproved that incorporating semantic information into Web usage mining process\ncan provide us with more interesting patterns which consequently make the\nrecommendation system more functional, smarter and comprehensive.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2011 17:34:05 GMT"}], "update_date": "2011-11-11", "authors_parsed": [["Ramesh", "C.", ""], ["Rao", "K. V. Chalapati", ""], ["Govardhan", "A.", ""]]}, {"id": "1111.2669", "submitter": "Geeta Bharamagoudar RUDRAGOUDA", "authors": "R.B. Geeta, Omkar Mamillapalli, Shasikumar G.Totad and Prasad Reddy\n  P.V.G.D", "title": "A Novel Approach for Web Page Set Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The one of the most time consuming steps for association rule mining is the\ncomputation of the frequency of the occurrences of itemsets in the database.\nThe hash table index approach converts a transaction database to an hash index\ntree by scanning the transaction database only once. Whenever user requests for\nany Uniform Resource Locator (URL), the request entry is stored in the Log File\nof the server. This paper presents the hash index table structure, a general\nand dense structure which provides web page set extraction from Log File of\nserver. This hash table provides information about the original database. Web\nPage set mining (WPs-Mine) provides a complete representation of the original\ndatabase. This approach works well for both sparse and dense data\ndistributions. Web page set mining supported by hash table index shows the\nperformance always comparable with and often better than algorithms accessing\ndata on flat files. Incremental update is feasible without reaccessing the\noriginal transactional database.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2011 06:24:49 GMT"}], "update_date": "2011-11-14", "authors_parsed": [["Geeta", "R. B.", ""], ["Mamillapalli", "Omkar", ""], ["Totad", "Shasikumar G.", ""], ["D", "Prasad Reddy P. V. G.", ""]]}, {"id": "1111.2852", "submitter": "Patrick Valduriez", "authors": "Patrick Valduriez (INRIA Sophia Antipolis, LIRMM)", "title": "Principles of Distributed Data Management in 2020?", "comments": null, "journal-ref": "Int. Conf. on Databases and Expert Systems Applications (DEXA)\n  6860 (2011) 1-11", "doi": "10.1007/978-3-642-23088-2", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advents of high-speed networks, fast commodity hardware, and the\nweb, distributed data sources have become ubiquitous. The third edition of the\n\\\"Ozsu-Valduriez textbook Principles of Distributed Database Systems [10]\nreflects the evolution of distributed data management and distributed database\nsystems. In this new edition, the fundamental principles of distributed data\nmanagement could be still presented based on the three dimensions of earlier\neditions: distribution, heterogeneity and autonomy of the data sources. In\nretrospect, the focus on fundamental principles and generic techniques has been\nuseful not only to understand and teach the material, but also to enable an\ninfinite number of variations. The primary application of these generic\ntechniques has been obviously for distributed and parallel DBMS versions.\nToday, to support the requirements of important data-intensive applications\n(e.g. social networks, web data analytics, scientific applications, etc.), new\ndistributed data management techniques and systems (e.g. MapReduce, Hadoop,\nSciDB, Peanut, Pig latin, etc.) are emerging and receiving much attention from\nthe research community. Although they do well in terms of\nconsistency/flexibility/performance trade-offs for specific applications, they\nseem to be ad-hoc and might hurt data interoperability. The key questions I\ndiscuss are: What are the fundamental principles behind the emerging solutions?\nIs there any generic architectural model, to explain those principles? Do we\nneed new foundations to look at data distribution?\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2011 20:44:38 GMT"}], "update_date": "2011-11-14", "authors_parsed": [["Valduriez", "Patrick", "", "INRIA Sophia Antipolis, LIRMM"]]}, {"id": "1111.3069", "submitter": "Laika Satish Mrs.", "authors": "Laika Satish (1), Sami Halawani (2) ((1) Lecturer, Faculty of\n  Computing and IT, King Abdul Aziz University, Rabigh, Saudi Arabia, (2) Dean,\n  Faculty of Computing and IT, King Abdul Aziz University Rabigh, Saudi Arabia)", "title": "A fusion algorithm for joins based on collections in Odra (Object\n  Database for Rapid Application development)", "comments": "ISSN (Online): 1694-0814 http://www.IJCSI.org", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 8,\n  Issue 4, No 2, 2011, 289-293", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present the functionality of a currently under development\ndatabase programming methodology called ODRA (Object Database for Rapid\nApplication development) which works fully on the object oriented principles.\nThe database programming language is called SBQL (Stack based query language).\nWe discuss some concepts in ODRA for e.g. the working of ODRA, how ODRA runtime\nenvironment operates, the interoperability of ODRA with .net and java .A view\nof ODRA's working with web services and xml. Currently the stages under\ndevelopment in ODRA are query optimization. So we present the prior work that\nis done in ODRA related to Query optimization and we also present a new fusion\nalgorithm of how ODRA can deal with joins based on collections like set, lists,\nand arrays for query optimization.\n", "versions": [{"version": "v1", "created": "Sun, 13 Nov 2011 22:06:36 GMT"}], "update_date": "2011-11-15", "authors_parsed": [["Satish", "Laika", ""], ["Halawani", "Sami", ""]]}, {"id": "1111.3270", "submitter": "Mehdi Kaytoue", "authors": "Mehdi Kaytoue (DCC - UFMG), Sergei O. Kuznetsov, Juraj Macko, Wagner\n  Meira (DCC - UFMG), Amedeo Napoli (INRIA Lorraine - LORIA)", "title": "Mining Biclusters of Similar Values with Triadic Concept Analysis", "comments": "Concept Lattices and their Applications (CLA) (2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biclustering numerical data became a popular data-mining task in the\nbeginning of 2000's, especially for analysing gene expression data. A bicluster\nreflects a strong association between a subset of objects and a subset of\nattributes in a numerical object/attribute data-table. So called biclusters of\nsimilar values can be thought as maximal sub-tables with close values. Only few\nmethods address a complete, correct and non redundant enumeration of such\npatterns, which is a well-known intractable problem, while no formal framework\nexists. In this paper, we introduce important links between biclustering and\nformal concept analysis. More specifically, we originally show that Triadic\nConcept Analysis (TCA), provides a nice mathematical framework for\nbiclustering. Interestingly, existing algorithms of TCA, that usually apply on\nbinary data, can be used (directly or with slight modifications) after a\npreprocessing step for extracting maximal biclusters of similar values.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2011 16:22:33 GMT"}], "update_date": "2011-11-15", "authors_parsed": [["Kaytoue", "Mehdi", "", "DCC - UFMG"], ["Kuznetsov", "Sergei O.", "", "DCC - UFMG"], ["Macko", "Juraj", "", "DCC - UFMG"], ["Meira", "Wagner", "", "DCC - UFMG"], ["Napoli", "Amedeo", "", "INRIA Lorraine - LORIA"]]}, {"id": "1111.3689", "submitter": "Anish Das Sarma", "authors": "Anish Das Sarma, Ankur Jain, Ashwin Machanavajjhala, Philip Bohannon", "title": "CBLOCK: An Automatic Blocking Mechanism for Large-Scale De-duplication\n  Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  De-duplication---identification of distinct records referring to the same\nreal-world entity---is a well-known challenge in data integration. Since very\nlarge datasets prohibit the comparison of every pair of records, {\\em blocking}\nhas been identified as a technique of dividing the dataset for pairwise\ncomparisons, thereby trading off {\\em recall} of identified duplicates for {\\em\nefficiency}. Traditional de-duplication tasks, while challenging, typically\ninvolved a fixed schema such as Census data or medical records. However, with\nthe presence of large, diverse sets of structured data on the web and the need\nto organize it effectively on content portals, de-duplication systems need to\nscale in a new dimension to handle a large number of schemas, tasks and data\nsets, while handling ever larger problem sizes. In addition, when working in a\nmap-reduce framework it is important that canopy formation be implemented as a\n{\\em hash function}, making the canopy design problem more challenging. We\npresent CBLOCK, a system that addresses these challenges. CBLOCK learns hash\nfunctions automatically from attribute domains and a labeled dataset consisting\nof duplicates. Subsequently, CBLOCK expresses blocking functions using a\nhierarchical tree structure composed of atomic hash functions. The application\nmay guide the automated blocking process based on architectural constraints,\nsuch as by specifying a maximum size of each block (based on memory\nrequirements), impose disjointness of blocks (in a grid environment), or\nspecify a particular objective function trading off recall for efficiency. As a\npost-processing step to automatically generated blocks, CBLOCK {\\em rolls-up}\nsmaller blocks to increase recall. We present experimental results on two\nlarge-scale de-duplication datasets at Yahoo!---consisting of over 140K movies\nand 40K restaurants respectively---and demonstrate the utility of CBLOCK.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2011 23:32:34 GMT"}], "update_date": "2011-11-17", "authors_parsed": [["Sarma", "Anish Das", ""], ["Jain", "Ankur", ""], ["Machanavajjhala", "Ashwin", ""], ["Bohannon", "Philip", ""]]}, {"id": "1111.3784", "submitter": "Hugo Buddelmeijer", "authors": "Hugo Buddelmeijer and Danny Boxhoorn and Edwin A. Valentijn", "title": "Automatic Optimized Discovery, Creation and Processing of Astronomical\n  Catalogs", "comments": "Accepted for publication in topical issue of Experimental Astronomy\n  on Astro-WISE information system", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the design of a novel way of handling astronomical catalogs in\nAstro-WISE in order to achieve the scalability required for the data produced\nby large scale surveys. A high level of automation and abstraction is achieved\nin order to facilitate interoperation with visualization software for\ninteractive exploration. At the same time flexibility in processing is enhanced\nand data is shared implicitly between scientists.\n  This is accomplished by using a data model that primarily stores how catalogs\nare derived; the contents of the catalogs are only created when necessary and\nstored only when beneficial for performance. Discovery of existing catalogs and\ncreation of new catalogs is done through the same process by directly\nrequesting the final set of sources (astronomical objects) and attributes\n(physical properties) that is required, for example from within visualization\nsoftware.\n  New catalogs are automatically created to provide attributes of sources for\nwhich no suitable existing catalogs can be found. These catalogs are defined to\ncontain the new attributes on the largest set of sources the calculation of the\nattributes is applicable to, facilitating reuse for future data requests.\nSubsequently, only those parts of the catalogs that are required for the\nrequested end product are actually processed, ensuring scalability.\n  The presented mechanisms primarily determine which catalogs are created and\nwhat data has to be processed and stored: the actual processing and storage\nitself is left to existing functionality of the underlying information system.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2011 12:49:55 GMT"}], "update_date": "2011-11-17", "authors_parsed": [["Buddelmeijer", "Hugo", ""], ["Boxhoorn", "Danny", ""], ["Valentijn", "Edwin A.", ""]]}, {"id": "1111.5518", "submitter": "Mohamed Quafafou", "authors": "Anis Ismail, Mohamed Quafafou, Gilles Nachouki, Mohammad Hajjar", "title": "Efficient Super-Peer-Based Queries Routing: Simulation and Evaluation", "comments": "Journal of Emerging Technologies in Web Intelligence, Vol 3, No 3\n  (2011), 206-216, Aug 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Peer-to-peer (P2P) Data-sharing systems now generate a significant portion of\ninternet traffic. P2P systems have emerged as a popular way to share huge\nvolumes of data. Requirements for widely distributed information systems\nsupporting virtual organizations have given rise to a new category of P2P\nsystems called schema- based. In such systems each peer is a database\nmanagement system in itself, ex-posing its own schema. A fundamental problem\nthat confronts peer-to-peer applications is the efficient location of the node\nthat stores a desired data item. In such settings, the main objective is the\nefficient search across peer databases by processing each incoming query\nwithout overly consuming bandwidth. The usability of these systems depends on\neffective techniques to find and retrieve data; however, efficient and\neffective routing of content- based queries is an emerging problem in P2P\nnetworks. In this paper, we propose an architecture, based on super-peers, and\nwe focus on query routing. Our approach considers that super-Peers having\nsimilar interests are grouped together for an efficient query routing method.\nIn such groups, called Knowledge-Super-Peers (KSP), super-peers submit queries\nthat are often processed by members of this group.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2011 15:07:55 GMT"}], "update_date": "2011-11-24", "authors_parsed": [["Ismail", "Anis", ""], ["Quafafou", "Mohamed", ""], ["Nachouki", "Gilles", ""], ["Hajjar", "Mohammad", ""]]}, {"id": "1111.5548", "submitter": "Milan Tasi\\'c", "authors": "Milan B. Tasi\\'c, Predrag S. Stanimirovi\\'c and Selver H. Pepi\\'c", "title": "Computation of generalized inverses using Php/MySql environment", "comments": "International Journal of Computer Mathematics, Volume 88, Issue 11,\n  2011", "journal-ref": null, "doi": "10.1080/00207160.2010.541453", "report-no": null, "categories": "cs.DB cs.DS math.NA", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The main aim of this paper is to develop a client/server-based model for\ncomputing the weighted Moore-Penrose inverse using the partitioning method as\nwell as for storage of generated results. The web application is developed in\nthe PHP/MySQL environment. The source code is open and free for testing by\nusing a web browser. Influence of different matrix representations and storage\nsystems on the computational time is investigated. The CPU time for searching\nthe previously stored pseudo-inverses is compared with the CPU time spent for\nnew computation of the same inverses.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2011 16:47:10 GMT"}], "update_date": "2011-11-24", "authors_parsed": [["Tasi\u0107", "Milan B.", ""], ["Stanimirovi\u0107", "Predrag S.", ""], ["Pepi\u0107", "Selver H.", ""]]}, {"id": "1111.5639", "submitter": "Seifedine Kadry Seifedine Kadry", "authors": "Seifedine Kadry, Mohamad Smaili, Hussam Kassem, Hassan Hayek", "title": "A New Technique to Backup and Restore DBMS using XML and .NET\n  Technologies", "comments": null, "journal-ref": "International Journal on Computer Science and Engineering Vol. 02,\n  No. 04, 2010, 1092-1102", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we proposed a new technique for backing up and restoring\ndifferent Database Management Systems (DBMS). The technique is enabling to\nbackup and restore a part of or the whole database using a unified interface\nusing ASP.NET and XML technologies. It presents a Web Solution allowing the\nadministrators to do their jobs from everywhere, locally or remotely. To show\nthe importance of our solution, we have taken two case studies, oracle 11g and\nSQL Server 2008.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2011 22:28:38 GMT"}], "update_date": "2011-11-28", "authors_parsed": [["Kadry", "Seifedine", ""], ["Smaili", "Mohamad", ""], ["Kassem", "Hussam", ""], ["Hayek", "Hassan", ""]]}, {"id": "1111.5687", "submitter": "Mehdi Kaytoue", "authors": "Baptiste Ducatel (INRIA Lorraine - LORIA), Mehdi Kaytoue (INRIA\n  Lorraine - LORIA), Florent Marcuola (INRIA Lorraine - LORIA), Amedeo Napoli\n  (INRIA Lorraine - LORIA), Laszlo Szathmary (INRIA Lorraine - LORIA)", "title": "Coron : Plate-forme d'extraction de connaissances dans les bases de\n  donn\\'ees", "comments": null, "journal-ref": "17\\`eme conf\\'erence en Reconnaissance des Formes et Intelligence\n  Artificielle (2010) 883-884", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coron is a domain and platform independent, multi-purposed data mining\ntoolkit, which incorporates not only a rich collection of data mining\nalgorithms, but also allows a number of auxiliary operations. To the best of\nour knowledge, a data mining toolkit designed specifically for itemset\nextraction and association rule generation like Coron does not exist elsewhere.\nCoron also provides support for preparing and filtering data, and for\ninterpreting the extracted units of knowledge.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2011 07:52:59 GMT"}], "update_date": "2011-11-28", "authors_parsed": [["Ducatel", "Baptiste", "", "INRIA Lorraine - LORIA"], ["Kaytoue", "Mehdi", "", "INRIA\n  Lorraine - LORIA"], ["Marcuola", "Florent", "", "INRIA Lorraine - LORIA"], ["Napoli", "Amedeo", "", "INRIA Lorraine - LORIA"], ["Szathmary", "Laszlo", "", "INRIA Lorraine - LORIA"]]}, {"id": "1111.5690", "submitter": "Mehdi Kaytoue", "authors": "Mehdi Kaytoue (INRIA Lorraine - LORIA), Florent Marcuola (INRIA\n  Lorraine - LORIA), Amedeo Napoli (INRIA Lorraine - LORIA), Laszlo Szathmary\n  (INRIA Lorraine - LORIA), Jean Villerd (INRIA Lorraine - LORIA)", "title": "The Coron System", "comments": null, "journal-ref": "8th International Conference on Formal Concept Analsis (ICFCA)\n  (2010) 55--58", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coron is a domain and platform independent, multi-purposed data mining\ntoolkit, which incorporates not only a rich collection of data mining\nalgorithms, but also allows a number of auxiliary operations. To the best of\nour knowledge, a data mining toolkit designed specifically for itemset\nextraction and association rule generation like Coron does not exist elsewhere.\nCoron also provides support for preparing and filtering data, and for\ninterpreting the extracted units of knowledge.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2011 07:56:18 GMT"}], "update_date": "2011-11-28", "authors_parsed": [["Kaytoue", "Mehdi", "", "INRIA Lorraine - LORIA"], ["Marcuola", "Florent", "", "INRIA\n  Lorraine - LORIA"], ["Napoli", "Amedeo", "", "INRIA Lorraine - LORIA"], ["Szathmary", "Laszlo", "", "INRIA Lorraine - LORIA"], ["Villerd", "Jean", "", "INRIA Lorraine - LORIA"]]}, {"id": "1111.6084", "submitter": "Gianvito Summa", "authors": "Angela Bonifati, Gianvito Summa, Esther Pacitti, Fady Draidi", "title": "Semantic Query Reformulation in Social PDMS", "comments": "29 pages, 8 figures, query rewriting in PDMS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider social peer-to-peer data management systems (PDMS), where each\npeer maintains both semantic mappings between its schema and some\nacquaintances, and social links with peer friends. In this context,\nreformulating a query from a peer's schema into other peer's schemas is a hard\nproblem, as it may generate as many rewritings as the set of mappings from that\npeer to the outside and transitively on, by eventually traversing the entire\nnetwork. However, not all the obtained rewritings are relevant to a given\nquery. In this paper, we address this problem by inspecting semantic mappings\nand social links to find only relevant rewritings. We propose a new notion of\n'relevance' of a query with respect to a mapping, and, based on this notion, a\nnew semantic query reformulation approach for social PDMS, which achieves great\naccuracy and flexibility. To find rapidly the most interesting mappings, we\ncombine several techniques: (i) social links are expressed as FOAF (Friend of a\nFriend) links to characterize peer's friendship and compact mapping summaries\nare used to obtain mapping descriptions; (ii) local semantic views are special\nviews that contain information about external mappings; and (iii) gossiping\ntechniques improve the search of relevant mappings. Our experimental\nevaluation, based on a prototype on top of PeerSim and a simulated network\ndemonstrate that our solution yields greater recall, compared to traditional\nquery translation approaches proposed in the literature.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2011 19:02:10 GMT"}], "update_date": "2011-11-28", "authors_parsed": [["Bonifati", "Angela", ""], ["Summa", "Gianvito", ""], ["Pacitti", "Esther", ""], ["Draidi", "Fady", ""]]}, {"id": "1111.6552", "submitter": "Souad Bouasker", "authors": "Souad Bouasker, Tarek Hamrouni, Sadok Ben Yahia", "title": "Nouvelle repr\\'esentation concise exacte des motifs corr\\'el\\'es rares :\n  Application \\`a la d\\'etection d'intrusions", "comments": "16 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlated rare pattern mining is an interesting issue in Data mining. In\nthis respect, the set of correlated rare patterns w.r.t. to the bond\ncorrelation measure was studied in a recent work, in which the RCPR concise\nexact representation of the set of correlated rare patterns was proposed.\nHowever, none algorithm was proposed in order to mine this representation and\nnone experiment was carried out to evaluate it. In this paper, we introduce the\nnew RcprMiner algorithm allowing an efficient extraction of RCPR. We also\npresent the IsRCP algorithm allowing the query of the RCPR representation in\naddition to the RCPRegeneration algorithm allowing the regeneration of the\nwhole set RCP of rare correlated patterns starting from this representation.\nThe carried out experiments highlight interesting compactness rates offered by\nRCPR. The effectiveness of the proposed classification method, based on generic\nrare correlated association rules derived from RCPR, has also been proved in\nthe context of intrusion detection.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2011 19:17:40 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2011 22:27:25 GMT"}, {"version": "v3", "created": "Sun, 4 Dec 2011 12:15:48 GMT"}, {"version": "v4", "created": "Tue, 6 Dec 2011 10:33:57 GMT"}, {"version": "v5", "created": "Wed, 7 Dec 2011 09:14:37 GMT"}, {"version": "v6", "created": "Sun, 11 Dec 2011 13:55:29 GMT"}, {"version": "v7", "created": "Tue, 7 Jan 2014 21:15:21 GMT"}, {"version": "v8", "created": "Fri, 24 Jan 2014 09:25:20 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Bouasker", "Souad", ""], ["Hamrouni", "Tarek", ""], ["Yahia", "Sadok Ben", ""]]}, {"id": "1111.6677", "submitter": "Chengfang Fang", "authors": "Chengfang Fang and Ee-Chien Chang", "title": "Publishing Location Dataset Differential Privately with Isotonic\n  Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of publishing location datasets, in particular 2D\nspatial pointsets, in a differentially private manner. Many existing mechanisms\nfocus on frequency counts of the points in some a priori partition of the\ndomain that is difficult to determine. We propose an approach that adds noise\ndirectly to the point, or to a group of neighboring points. Our approach is\nbased on the observation that, the sensitivity of sorting, as a function on\nsets of real numbers, can be bounded. Together with isotonic regression, the\ndataset can be accurately reconstructed. To extend the mechanism to higher\ndimension, we employ locality preserving function to map the dataset to a\nbounded interval. Although there are fundamental limits on the performance of\nlocality preserving functions, fortunately, our problem only requires distance\npreservation in the \"easier\" direction, and the well-known Hilbert\nspace-filling curve suffices to provide high accuracy. The publishing process\nis simple from the publisher's point of view: the publisher just needs to map\nthe data, sort them, group them, add Laplace noise and publish the dataset. The\nonly parameter to determine is the group size which can be chosen based on\npredicted generalization errors. Empirical study shows that the published\ndataset can also exploited to answer other queries, for example, range query\nand median query, accurately.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2011 03:18:16 GMT"}], "update_date": "2011-11-30", "authors_parsed": [["Fang", "Chengfang", ""], ["Chang", "Ee-Chien", ""]]}, {"id": "1111.6937", "submitter": "Matteo Riondato", "authors": "Matteo Riondato and Eli Upfal", "title": "Efficient Discovery of Association Rules and Frequent Itemsets through\n  Sampling with Tight Performance Guarantees", "comments": "19 pages, 7 figures. A shorter version of this paper appeared in the\n  proceedings of ECML PKDD 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The tasks of extracting (top-$K$) Frequent Itemsets (FI's) and Association\nRules (AR's) are fundamental primitives in data mining and database\napplications. Exact algorithms for these problems exist and are widely used,\nbut their running time is hindered by the need of scanning the entire dataset,\npossibly multiple times. High quality approximations of FI's and AR's are\nsufficient for most practical uses, and a number of recent works explored the\napplication of sampling for fast discovery of approximate solutions to the\nproblems. However, these works do not provide satisfactory performance\nguarantees on the quality of the approximation, due to the difficulty of\nbounding the probability of under- or over-sampling any one of an unknown\nnumber of frequent itemsets. In this work we circumvent this issue by applying\nthe statistical concept of \\emph{Vapnik-Chervonenkis (VC) dimension} to develop\na novel technique for providing tight bounds on the sample size that guarantees\napproximation within user-specified parameters. Our technique applies both to\nabsolute and to relative approximations of (top-$K$) FI's and AR's. The\nresulting sample size is linearly dependent on the VC-dimension of a range\nspace associated with the dataset to be mined. The main theoretical\ncontribution of this work is a proof that the VC-dimension of this range space\nis upper bounded by an easy-to-compute characteristic quantity of the dataset\nwhich we call \\emph{d-index}, and is the maximum integer $d$ such that the\ndataset contains at least $d$ transactions of length at least $d$ such that no\none of them is a superset of or equal to another. We show that this bound is\nstrict for a large class of datasets.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2011 19:11:50 GMT"}, {"version": "v2", "created": "Wed, 30 Nov 2011 14:45:50 GMT"}, {"version": "v3", "created": "Tue, 24 Apr 2012 02:39:09 GMT"}, {"version": "v4", "created": "Thu, 21 Jun 2012 12:56:59 GMT"}, {"version": "v5", "created": "Mon, 10 Dec 2012 20:07:02 GMT"}, {"version": "v6", "created": "Fri, 22 Feb 2013 14:32:31 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Riondato", "Matteo", ""], ["Upfal", "Eli", ""]]}, {"id": "1111.7164", "submitter": "Fabian M. Suchanek", "authors": "Fabian M. Suchanek, Serge Abiteboul, Pierre Senellart", "title": "PARIS: Probabilistic Alignment of Relations, Instances, and Schema", "comments": "VLDB2012. arXiv admin note: substantial text overlap with\n  arXiv:1105.5516", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 3, pp.\n  157-168 (2011)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main challenges that the Semantic Web faces is the integration of\na growing number of independently designed ontologies. In this work, we present\nPARIS, an approach for the automatic alignment of ontologies. PARIS aligns not\nonly instances, but also relations and classes. Alignments at the instance\nlevel cross-fertilize with alignments at the schema level. Thereby, our system\nprovides a truly holistic solution to the problem of ontology alignment. The\nheart of the approach is probabilistic, i.e., we measure degrees of matchings\nbased on probability estimates. This allows PARIS to run without any parameter\ntuning. We demonstrate the efficiency of the algorithm and its precision\nthrough extensive experiments. In particular, we obtain a precision of around\n90% in experiments with some of the world's largest ontologies.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2011 14:08:40 GMT"}], "update_date": "2011-12-01", "authors_parsed": [["Suchanek", "Fabian M.", ""], ["Abiteboul", "Serge", ""], ["Senellart", "Pierre", ""]]}, {"id": "1111.7165", "submitter": "Sayan Ranu", "authors": "Sayan Ranu, Ambuj K. Singh", "title": "Answering Top-k Queries Over a Mixture of Attractive and Repulsive\n  Dimensions", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 3, pp.\n  169-180 (2011)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we formulate a top-k query that compares objects in a database\nto a user-provided query object on a novel scoring function. The proposed\nscoring function combines the idea of attractive and repulsive dimensions into\na general framework to overcome the weakness of traditional distance or\nsimilarity measures. We study the properties of the proposed class of scoring\nfunctions and develop efficient and scalable index structures that index the\nisolines of the function. We demonstrate various scenarios where the query\nfinds application. Empirical evaluation demonstrates a performance gain of one\nto two orders of magnitude on querying time over existing state-of-the-art\ntop-k techniques. Further, a qualitative analysis is performed on a real\ndataset to highlight the potential of the proposed query in discovering hidden\ndata characteristics.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2011 14:09:11 GMT"}], "update_date": "2011-12-01", "authors_parsed": [["Ranu", "Sayan", ""], ["Singh", "Ambuj K.", ""]]}, {"id": "1111.7166", "submitter": "Michael Armbrust", "authors": "Michael Armbrust, Kristal Curtis, Tim Kraska, Armando Fox, Michael J.\n  Franklin, David A. Patterson", "title": "PIQL: Success-Tolerant Query Processing in the Cloud", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 3, pp.\n  181-192 (2011)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Newly-released web applications often succumb to a \"Success Disaster,\" where\noverloaded database machines and resulting high response times destroy a\npreviously good user experience. Unfortunately, the data independence provided\nby a traditional relational database system, while useful for agile\ndevelopment, only exacerbates the problem by hiding potentially expensive\nqueries under simple declarative expressions. As a result, developers of these\napplications are increasingly abandoning relational databases in favor of\nimperative code written against distributed key/value stores, losing the many\nbenefits of data independence in the process. Instead, we propose PIQL, a\ndeclarative language that also provides scale independence by calculating an\nupper bound on the number of key/value store operations that will be performed\nfor any query. Coupled with a service level objective (SLO) compliance\nprediction model and PIQL's scalable database architecture, these bounds make\nit easy for developers to write success-tolerant applications that support an\narbitrarily large number of users while still providing acceptable performance.\nIn this paper, we present the PIQL query processing system and evaluate its\nscale independence on hundreds of machines using two benchmarks, TPC-W and\nSCADr.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2011 14:09:39 GMT"}], "update_date": "2011-12-01", "authors_parsed": [["Armbrust", "Michael", ""], ["Curtis", "Kristal", ""], ["Kraska", "Tim", ""], ["Fox", "Armando", ""], ["Franklin", "Michael J.", ""], ["Patterson", "David A.", ""]]}, {"id": "1111.7167", "submitter": "Peixiang Zhao", "authors": "Peixiang Zhao, Charu C. Aggarwal, Min Wang", "title": "gSketch: On Query Estimation in Graph Streams", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 3, pp.\n  193-204 (2011)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many dynamic applications are built upon large network infrastructures, such\nas social networks, communication networks, biological networks and the Web.\nSuch applications create data that can be naturally modeled as graph streams,\nin which edges of the underlying graph are received and updated sequentially in\na form of a stream. It is often necessary and important to summarize the\nbehavior of graph streams in order to enable effective query processing.\nHowever, the sheer size and dynamic nature of graph streams present an enormous\nchallenge to existing graph management techniques. In this paper, we propose a\nnew graph sketch method, gSketch, which combines well studied synopses for\ntraditional data streams with a sketch partitioning technique, to estimate and\noptimize the responses to basic queries on graph streams. We consider two\ndifferent scenarios for query estimation: (1) A graph stream sample is\navailable; (2) Both a graph stream sample and a query workload sample are\navailable. Algorithms for different scenarios are designed respectively by\npartitioning a global sketch to a group of localized sketches in order to\noptimize the query estimation accuracy. We perform extensive experimental\nstudies on both real and synthetic data sets and demonstrate the power and\nrobustness of gSketch in comparison with the state-of-the-art global sketch\nmethod.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2011 14:10:01 GMT"}], "update_date": "2011-12-01", "authors_parsed": [["Zhao", "Peixiang", ""], ["Aggarwal", "Charu C.", ""], ["Wang", "Min", ""]]}, {"id": "1111.7168", "submitter": "Brian E. Ruttenberg", "authors": "Brian E. Ruttenberg, Ambuj K. Singh", "title": "Indexing the Earth Mover's Distance Using Normal Distributions", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 3, pp.\n  205-216 (2011)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Querying uncertain data sets (represented as probability distributions)\npresents many challenges due to the large amount of data involved and the\ndifficulties comparing uncertainty between distributions. The Earth Mover's\nDistance (EMD) has increasingly been employed to compare uncertain data due to\nits ability to effectively capture the differences between two distributions.\nComputing the EMD entails finding a solution to the transportation problem,\nwhich is computationally intensive. In this paper, we propose a new lower bound\nto the EMD and an index structure to significantly improve the performance of\nEMD based K-nearest neighbor (K-NN) queries on uncertain databases. We propose\na new lower bound to the EMD that approximates the EMD on a projection vector.\nEach distribution is projected onto a vector and approximated by a normal\ndistribution, as well as an accompanying error term. We then represent each\nnormal as a point in a Hough transformed space. We then use the concept of\nstochastic dominance to implement an efficient index structure in the\ntransformed space. We show that our method significantly decreases K-NN query\ntime on uncertain databases. The index structure also scales well with database\ncardinality. It is well suited for heterogeneous data sets, helping to keep EMD\nbased queries tractable as uncertain data sets become larger and more complex.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2011 14:10:17 GMT"}], "update_date": "2011-12-01", "authors_parsed": [["Ruttenberg", "Brian E.", ""], ["Singh", "Ambuj K.", ""]]}, {"id": "1111.7169", "submitter": "Georgios J. Fakas", "authors": "Georgios J. Fakas, Zhi Cai, Nikos Mamoulis", "title": "Size-l Object Summaries for Relational Keyword Search", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 3, pp.\n  229-240 (2011)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A previously proposed keyword search paradigm produces, as a query result, a\nranked list of Object Summaries (OSs). An OS is a tree structure of related\ntuples that summarizes all data held in a relational database about a\nparticular Data Subject (DS). However, some of these OSs are very large in size\nand therefore unfriendly to users that initially prefer synoptic information\nbefore proceeding to more comprehensive information about a particular DS. In\nthis paper, we investigate the effective and efficient retrieval of concise and\ninformative OSs. We argue that a good size-l OS should be a stand-alone and\nmeaningful synopsis of the most important information about the particular DS.\nMore precisely, we define a size-l OS as a partial OS composed of l important\ntuples. We propose three algorithms for the efficient generation of size-l OSs\n(in addition to the optimal approach which requires exponential time).\nExperimental evaluation on DBLP and TPC-H databases verifies the effectiveness\nand efficiency of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2011 14:11:28 GMT"}], "update_date": "2011-12-01", "authors_parsed": [["Fakas", "Georgios J.", ""], ["Cai", "Zhi", ""], ["Mamoulis", "Nikos", ""]]}, {"id": "1111.7170", "submitter": "Lujun Fang", "authors": "Lujun Fang, Anish Das Sarma, Cong Yu, Philip Bohannon", "title": "REX: Explaining Relationships between Entity Pairs", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 3, pp.\n  241-252 (2011)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge bases of entities and relations (either constructed manually or\nautomatically) are behind many real world search engines, including those at\nYahoo!, Microsoft, and Google. Those knowledge bases can be viewed as graphs\nwith nodes representing entities and edges representing (primary)\nrelationships, and various studies have been conducted on how to leverage them\nto answer entity seeking queries. Meanwhile, in a complementary direction,\nanalyses over the query logs have enabled researchers to identify entity pairs\nthat are statistically correlated. Such entity relationships are then presented\nto search users through the \"related searches\" feature in modern search\nengines. However, entity relationships thus discovered can often be \"puzzling\"\nto the users because why the entities are connected is often indescribable. In\nthis paper, we propose a novel problem called \"entity relationship\nexplanation\", which seeks to explain why a pair of entities are connected, and\nsolve this challenging problem by integrating the above two complementary\napproaches, i.e., we leverage the knowledge base to \"explain\" the connections\ndiscovered between entity pairs. More specifically, we present REX, a system\nthat takes a pair of entities in a given knowledge base as input and\nefficiently identifies a ranked list of relationship explanations. We formally\ndefine relationship explanations and analyze their desirable properties.\nFurthermore, we design and implement algorithms to efficiently enumerate and\nrank all relationship explanations based on multiple measures of\n\"interestingness.\" We perform extensive experiments over real web-scale data\ngathered from DBpedia and a commercial search engine, demonstrating the\nefficiency and scalability of REX. We also perform user studies to corroborate\nthe effectiveness of explanations generated by REX.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2011 14:11:53 GMT"}], "update_date": "2011-12-01", "authors_parsed": [["Fang", "Lujun", ""], ["Sarma", "Anish Das", ""], ["Yu", "Cong", ""], ["Bohannon", "Philip", ""]]}, {"id": "1111.7171", "submitter": "Guoliang Li", "authors": "Guoliang Li, Dong Deng, Jiannan Wang, Jianhua Feng", "title": "PASS-JOIN: A Partition-based Method for Similarity Joins", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 3, pp.\n  253-264 (2011)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an essential operation in data cleaning, the similarity join has attracted\nconsiderable attention from the database community. In this paper, we study\nstring similarity joins with edit-distance constraints, which find similar\nstring pairs from two large sets of strings whose edit distance is within a\ngiven threshold. Existing algorithms are efficient either for short strings or\nfor long strings, and there is no algorithm that can efficiently and adaptively\nsupport both short strings and long strings. To address this problem, we\npropose a partition-based method called Pass-Join. Pass-Join partitions a\nstring into a set of segments and creates inverted indices for the segments.\nThen for each string, Pass-Join selects some of its substrings and uses the\nselected substrings to find candidate pairs using the inverted indices. We\ndevise efficient techniques to select the substrings and prove that our method\ncan minimize the number of selected substrings. We develop novel pruning\ntechniques to efficiently verify the candidate pairs. Experimental results show\nthat our algorithms are efficient for both short strings and long strings, and\noutperform state-of-the-art methods on real datasets.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2011 14:12:22 GMT"}], "update_date": "2011-12-01", "authors_parsed": [["Li", "Guoliang", ""], ["Deng", "Dong", ""], ["Wang", "Jiannan", ""], ["Feng", "Jianhua", ""]]}, {"id": "1111.7224", "submitter": "Yiu-Kai Ng", "authors": "Rani Qumsiyeh, Maria S. Pera, Yiu-Kai Ng", "title": "Generating Exact- and Ranked Partially-Matched Answers to Questions in\n  Advertisements", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 3, pp.\n  217-228 (2011)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Taking advantage of the Web, many advertisements (ads for short) websites,\nwhich aspire to increase client's transactions and thus profits, offer\nsearching tools which allow users to (i) post keyword queries to capture their\ninformation needs or (ii) invoke form-based interfaces to create queries by\nselecting search options, such as a price range, filled-in entries, check\nboxes, or drop-down menus. These search mechanisms, however, are inadequate,\nsince they cannot be used to specify a natural-language query with rich\nsyntactic and semantic content, which can only be handled by a question\nanswering (QA) system. Furthermore, existing ads websites are incapable of\nevaluating arbitrary Boolean queries or retrieving partiallymatched answers\nthat might be of interest to the user whenever a user's search yields only a\nfew or no results at all. In solving these problems, we present a QA system for\nads, called CQAds, which (i) allows users to post a natural-language question Q\nfor retrieving relevant ads, if they exist, (ii) identifies ads as answers that\npartially-match the requested information expressed in Q, if insufficient or no\nanswers to Q can be retrieved, which are ordered using a similarity-ranking\napproach, and (iii) analyzes incomplete or ambiguous questions to perform the\n\"best guess\" in retrieving answers that \"best match\" the selection criteria\nspecified in Q. CQAds is also equipped with a Boolean model to evaluate Boolean\noperators that are either explicitly or implicitly specified in Q, i.e., with\nor without Boolean operators specified by the users, respectively. CQAds is\neasy to use, scalable to all ads domains, and more powerful than search tools\nprovided by existing ads websites, since its query-processing strategy\nretrieves relevant ads of higher quality and quantity. We have verified the\naccuracy of CQAds in retrieving ads on eight ads domains and compared\nit...[truncated].\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2011 16:08:06 GMT"}], "update_date": "2011-12-01", "authors_parsed": [["Qumsiyeh", "Rani", ""], ["Pera", "Maria S.", ""], ["Ng", "Yiu-Kai", ""]]}, {"id": "1111.7295", "submitter": "Prateek Jain", "authors": "Raajay Viswanathan, Prateek Jain, Srivatsan Laxman, Arvind Arasu", "title": "A Learning Framework for Self-Tuning Histograms", "comments": "Submitted to VLDB-2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of estimating self-tuning histograms\nusing query workloads. To this end, we propose a general learning theoretic\nformulation. Specifically, we use query feedback from a workload as training\ndata to estimate a histogram with a small memory footprint that minimizes the\nexpected error on future queries. Our formulation provides a framework in which\ndifferent approaches can be studied and developed. We first study the simple\nclass of equi-width histograms and present a learning algorithm, EquiHist, that\nis competitive in many settings. We also provide formal guarantees for\nequi-width histograms that highlight scenarios in which equi-width histograms\ncan be expected to succeed or fail. We then go beyond equi-width histograms and\npresent a novel learning algorithm, SpHist, for estimating general histograms.\nHere we use Haar wavelets to reduce the problem of learning histograms to that\nof learning a sparse vector. Both algorithms have multiple advantages over\nexisting methods: 1) simple and scalable extensions to multi-dimensional data,\n2) scalability with number of histogram buckets and size of query feedback, 3)\nnatural extensions to incorporate new feedback and handle database updates. We\ndemonstrate these advantages over the current state-of-the-art, ISOMER, through\ndetailed experiments on real and synthetic data. In particular, we show that\nSpHist obtains up to 50% less error than ISOMER on real-world multi-dimensional\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2011 20:17:29 GMT"}, {"version": "v2", "created": "Fri, 2 Dec 2011 16:01:50 GMT"}], "update_date": "2011-12-05", "authors_parsed": [["Viswanathan", "Raajay", ""], ["Jain", "Prateek", ""], ["Laxman", "Srivatsan", ""], ["Arasu", "Arvind", ""]]}]