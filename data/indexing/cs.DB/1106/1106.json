[{"id": "1106.0304", "submitter": "Jes\\'us Pardillo", "authors": "Jes\\'us Pardillo and Jose-Norberto Maz\\'on", "title": "Using Ontologies for the Design of Data Warehouses", "comments": "15 pages, 2 figures", "journal-ref": "International Journal of Database Management Systems (IJDMS),\n  Vol.3, No.2, May 2011", "doi": "10.5121/ijdms.2011.3205", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obtaining an implementation of a data warehouse is a complex task that forces\ndesigners to acquire wide knowledge of the domain, thus requiring a high level\nof expertise and becoming it a prone-to-fail task. Based on our experience, we\nhave detected a set of situations we have faced up with in real-world projects\nin which we believe that the use of ontologies will improve several aspects of\nthe design of data warehouses. The aim of this article is to describe several\nshortcomings of current data warehouse design approaches and discuss the\nbenefit of using ontologies to overcome them. This work is a starting point for\ndiscussing the convenience of using ontologies in data warehouse design.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2011 20:01:00 GMT"}], "update_date": "2011-06-03", "authors_parsed": [["Pardillo", "Jes\u00fas", ""], ["Maz\u00f3n", "Jose-Norberto", ""]]}, {"id": "1106.0718", "submitter": "Arun Kumar", "authors": "Arun Kumar, Christopher R\\'e", "title": "Probabilistic Management of OCR Data using an RDBMS", "comments": "41 pages including the appendix. Shorter version (without appendix)\n  to appear as a full research paper in VLDB 2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 4, pp.\n  322-333 (2011)", "doi": null, "report-no": "vol5no4/p322_arunkumar_vldb2012", "categories": "cs.DB cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The digitization of scanned forms and documents is changing the data sources\nthat enterprises manage. To integrate these new data sources with enterprise\ndata, the current state-of-the-art approach is to convert the images to ASCII\ntext using optical character recognition (OCR) software and then to store the\nresulting ASCII text in a relational database. The OCR problem is challenging,\nand so the output of OCR often contains errors. In turn, queries on the output\nof OCR may fail to retrieve relevant answers. State-of-the-art OCR programs,\ne.g., the OCR powering Google Books, use a probabilistic model that captures\nmany alternatives during the OCR process. Only when the results of OCR are\nstored in the database, do these approaches discard the uncertainty. In this\nwork, we propose to retain the probabilistic models produced by OCR process in\na relational database management system. A key technical challenge is that the\nprobabilistic data produced by OCR software is very large (a single book blows\nup to 2GB from 400kB as ASCII). As a result, a baseline solution that\nintegrates these models with an RDBMS is over 1000x slower versus standard text\nprocessing for single table select-project queries. However, many applications\nmay have quality-performance needs that are in between these two extremes of\nASCII and the complete model output by the OCR software. Thus, we propose a\nnovel approximation scheme called Staccato that allows a user to trade recall\nfor query performance. Additionally, we provide a formal analysis of our\nscheme's properties, and describe how we integrate our scheme with\nstandard-RDBMS text indexing.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2011 18:01:59 GMT"}, {"version": "v2", "created": "Tue, 7 Jun 2011 00:33:41 GMT"}, {"version": "v3", "created": "Wed, 31 Aug 2011 07:34:44 GMT"}, {"version": "v4", "created": "Fri, 6 Jan 2012 04:59:24 GMT"}], "update_date": "2012-01-09", "authors_parsed": [["Kumar", "Arun", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1106.1478", "submitter": "Leopoldo Bertossi", "authors": "M. Andrea Rodr\\'iguez and Leopoldo Bertossi and Monica Caniupan", "title": "Consistent Query Answering under Spatial Semantic Constraints", "comments": "Journal submission, 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consistent query answering is an inconsistency tolerant approach to obtaining\nsemantically correct answers from a database that may be inconsistent with\nrespect to its integrity constraints. In this work we formalize the notion of\nconsistent query answer for spatial databases and spatial semantic integrity\nconstraints. In order to do this, we first characterize conflicting spatial\ndata, and next, we define admissible instances that restore consistency while\nstaying close to the original instance. In this way we obtain a repair\nsemantics, which is used as an instrumental concept to define and possibly\nderive consistent query answers. We then concentrate on a class of spatial\ndenial constraints and spatial queries for which there exists an efficient\nstrategy to compute consistent query answers. This study applies inconsistency\ntolerance in spatial databases, rising research issues that shift the goal from\nthe consistency of a spatial database to the consistency of query answering.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2011 01:21:04 GMT"}], "update_date": "2011-06-09", "authors_parsed": [["Rodr\u00edguez", "M. Andrea", ""], ["Bertossi", "Leopoldo", ""], ["Caniupan", "Monica", ""]]}, {"id": "1106.1811", "submitter": "Arnab Bhattacharya", "authors": "Arnab Bhattacharya and B. Palvali Teja and Sourav Dutta", "title": "Caching Stars in the Sky: A Semantic Caching Approach to Accelerate\n  Skyline Queries", "comments": "11 pages; will be published in DEXA 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-criteria decision making has been made possible with the advent of\nskyline queries. However, processing such queries for high dimensional datasets\nremains a time consuming task. Real-time applications are thus infeasible,\nespecially for non-indexed skyline techniques where the datasets arrive online.\nIn this paper, we propose a caching mechanism that uses the semantics of\nprevious skyline queries to improve the processing time of a new query. In\naddition to exact queries, utilizing such special semantics allow accelerating\nrelated queries. We achieve this by generating partial result sets guaranteed\nto be in the skyline sets. We also propose an index structure for efficient\norganization of the cached queries. Experiments on synthetic and real datasets\nshow the effectiveness and scalability of our proposed methods.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2011 13:47:34 GMT"}, {"version": "v2", "created": "Fri, 10 Jun 2011 07:32:04 GMT"}], "update_date": "2011-06-13", "authors_parsed": [["Bhattacharya", "Arnab", ""], ["Teja", "B. Palvali", ""], ["Dutta", "Sourav", ""]]}, {"id": "1106.2587", "submitter": "Christopher Hoobin", "authors": "Christopher Hoobin, Simon J. Puglisi and Justin Zobel", "title": "Relative Lempel-Ziv Factorization for Efficient Storage and Retrieval of\n  Web Collections", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 3, pp.\n  265-273 (2011)", "doi": null, "report-no": "vol5no3/p265_christopherhoobin_vldb2012", "categories": "cs.DS cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compression techniques that support fast random access are a core component\nof any information system. Current state-of-the-art methods group documents\ninto fixed-sized blocks and compress each block with a general-purpose adaptive\nalgorithm such as GZIP. Random access to a specific document then requires\ndecompression of a block. The choice of block size is critical: it trades\nbetween compression effectiveness and document retrieval times. In this paper\nwe present a scalable compression method for large document collections that\nallows fast random access. We build a representative sample of the collection\nand use it as a dictionary in a LZ77-like encoding of the rest of the\ncollection, relative to the dictionary. We demonstrate on large collections,\nthat using a dictionary as small as 0.1% of the collection size, our algorithm\nis dramatically faster than previous methods, and in general gives much better\ncompression.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2011 00:53:40 GMT"}, {"version": "v2", "created": "Fri, 9 Dec 2011 03:26:13 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Hoobin", "Christopher", ""], ["Puglisi", "Simon J.", ""], ["Zobel", "Justin", ""]]}, {"id": "1106.3325", "submitter": "Daniel Wilkerson", "authors": "Daniel Shawcross Wilkerson, Simon Fredrick Vicente Goldsmith, Ryan\n  Barrett, Erick Armbrust, Robert Johnson, Alfred Fuller", "title": "Distributed Transactions for Google App Engine: Optimistic Distributed\n  Transactions built upon Local Multi-Version Concurrency Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.DS cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massively scalable web applications encounter a fundamental tension in\ncomputing between \"performance\" and \"correctness\": performance is often\naddressed by using a large and therefore distributed machine where programs are\nmulti-threaded and interruptible, whereas correctness requires data invariants\nto be maintained with certainty. A solution to this problem is \"transactions\"\n[Gray-Reuter].\n  Some distributed systems such as Google App Engine\n[http://code.google.com/appengine/docs/] provide transaction semantics but only\nfor functions that access one of a set of predefined local regions of the\ndatabase: a \"Local Transaction\" (LT)\n[http://code.google.com/appengine/docs/python/datastore/transactions.html]. To\naddress this problem we give a \"Distributed Transaction\" (DT) algorithm which\nprovides transaction semantics for functions that operate on any set of objects\ndistributed across the machine. Our algorithm is in an \"optimistic\"\n[http://en.wikipedia.org/wiki/Optimistic_concurrency_control] style. We assume\nSequential [Time-]Consistency\n[http://en.wikipedia.org/wiki/Sequential_consistency] for Local Transactions.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2011 00:00:22 GMT"}], "update_date": "2011-06-17", "authors_parsed": [["Wilkerson", "Daniel Shawcross", ""], ["Goldsmith", "Simon Fredrick Vicente", ""], ["Barrett", "Ryan", ""], ["Armbrust", "Erick", ""], ["Johnson", "Robert", ""], ["Fuller", "Alfred", ""]]}, {"id": "1106.3725", "submitter": "S{\\l}awomir Staworko", "authors": "S{\\l}awomir Staworko, Piotr Wieczorek", "title": "Learning XML Twig Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of learning XML queries, path queries and tree\npattern queries, from examples given by the user. A learning algorithm takes on\nthe input a set of XML documents with nodes annotated by the user and returns a\nquery that selects the nodes in a manner consistent with the annotation. We\nstudy two learning settings that differ with the types of annotations. In the\nfirst setting the user may only indicate required nodes that the query must\nreturn. In the second, more general, setting, the user may also indicate\nforbidden nodes that the query must not return. The query may or may not return\nany node with no annotation. We formalize what it means for a class of queries\nto be \\emph{learnable}. One requirement is the existence of a learning\nalgorithm that is sound i.e., always returns a query consistent with the\nexamples given by the user. Furthermore, the learning algorithm should be\ncomplete i.e., able to produce every query with a sufficiently rich example.\nOther requirements involve tractability of learning and its robustness to\nnonessential examples. We show that the classes of simple path queries and\npath-subsumption-free tree queries are learnable from positive examples. The\nlearnability of the full class of tree pattern queries (and the full class of\npath queries) remains an open question. We show also that adding negative\nexamples to the picture renders the learning unfeasible.\n  Published in ICDT 2012, Berlin.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jun 2011 09:29:54 GMT"}, {"version": "v2", "created": "Fri, 24 Jun 2011 12:48:41 GMT"}, {"version": "v3", "created": "Fri, 20 Apr 2012 20:28:35 GMT"}], "update_date": "2012-04-24", "authors_parsed": [["Staworko", "S\u0142awomir", ""], ["Wieczorek", "Piotr", ""]]}, {"id": "1106.3745", "submitter": "Marcelo Arenas", "authors": "Marcelo Arenas (Pontificia Universidad Catolica de Chile), Ronald\n  Fagin (IBM Research--Almaden), Alan Nash (Aleph One LLC)", "title": "Composition with Target Constraints", "comments": "This paper is an extended version of: M. Arenas, R. Fagin, and A.\n  Nash. Composition with Target Constraints. In 13th International Conference\n  on Database Theory (ICDT), pages 129-142, 2010", "journal-ref": "Logical Methods in Computer Science, Volume 7, Issue 3 (September\n  8, 2011) lmcs:905", "doi": "10.2168/LMCS-7(3:13)2011", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that the composition of schema mappings, each specified by\nsource-to-target tgds (st-tgds), can be specified by a second-order tgd (SO\ntgd). We consider the question of what happens when target constraints are\nallowed. Specifically, we consider the question of specifying the composition\nof standard schema mappings (those specified by st-tgds, target egds, and a\nweakly acyclic set of target tgds). We show that SO tgds, even with the\nassistance of arbitrary source constraints and target constraints, cannot\nspecify in general the composition of two standard schema mappings. Therefore,\nwe introduce source-to-target second-order dependencies (st-SO dependencies),\nwhich are similar to SO tgds, but allow equations in the conclusion. We show\nthat st-SO dependencies (along with target egds and target tgds) are sufficient\nto express the composition of every finite sequence of standard schema\nmappings, and further, every st-SO dependency specifies such a composition. In\naddition to this expressive power, we show that st-SO dependencies enjoy other\ndesirable properties. In particular, they have a polynomial-time chase that\ngenerates a universal solution. This universal solution can be used to find the\ncertain answers to unions of conjunctive queries in polynomial time. It is easy\nto show that the composition of an arbitrary number of standard schema mappings\nis equivalent to the composition of only two standard schema mappings. We show\nthat surprisingly, the analogous result holds also for schema mappings\nspecified by just st-tgds (no target constraints). This is proven by showing\nthat every SO tgd is equivalent to an unnested SO tgd (one where there is no\nnesting of function symbols). Similarly, we prove unnesting results for st-SO\ndependencies, with the same types of consequences.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jun 2011 13:37:19 GMT"}, {"version": "v2", "created": "Wed, 7 Sep 2011 08:39:13 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Arenas", "Marcelo", "", "Pontificia Universidad Catolica de Chile"], ["Fagin", "Ronald", "", "IBM Research--Almaden"], ["Nash", "Alan", "", "Aleph One LLC"]]}, {"id": "1106.3767", "submitter": "Georg Gottlob", "authors": "Georg Gottlob and Thomas Schwentick", "title": "Rewriting Ontological Queries into Small Nonrecursive Datalog Programs", "comments": "A shorter version is presented at the 24th International Workshop on\n  Description Logics, DL 2011, Barcelona, Spain, July 13-16, 2011. The present\n  version mainly extends the proof of Theorem 1 in Section 3. We plan to post\n  further extended versions of this paper in the near future", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the setting of ontological database access, where an Abox is\ngiven in form of a relational database D and where a Boolean conjunctive query\nq has to be evaluated against D modulo a Tbox T formulated in DL-Lite or Linear\nDatalog+/-. It is well-known that (T,q) can be rewritten into an equivalent\nnonrecursive Datalog program P that can be directly evaluated over D. However,\nfor Linear Datalog? or for DL-Lite versions that allow for role inclusion, the\nrewriting methods described so far result in a nonrecursive Datalog program P\nof size exponential in the joint size of T and q. This gives rise to the\ninteresting question of whether such a rewriting necessarily needs to be of\nexponential size. In this paper we show that it is actually possible to\ntranslate (T,q) into a polynomially sized equivalent nonrecursive Datalog\nprogram P.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jun 2011 18:20:07 GMT"}, {"version": "v2", "created": "Sun, 10 Jul 2011 18:26:56 GMT"}, {"version": "v3", "created": "Sat, 23 Jul 2011 10:19:21 GMT"}], "update_date": "2011-07-26", "authors_parsed": [["Gottlob", "Georg", ""], ["Schwentick", "Thomas", ""]]}, {"id": "1106.4475", "submitter": "Eirini Spyropoulou", "authors": "Eirini Spyropoulou and Tijl De Bie", "title": "Interesting Multi-Relational Patterns", "comments": "Accepted at ICDM'11", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining patterns from multi-relational data is a problem attracting increasing\ninterest within the data mining community. Traditional data mining approaches\nare typically developed for highly simplified types of data, such as an\nattribute-value table or a binary database, such that those methods are not\ndirectly applicable to multi-relational data. Nevertheless, multi-relational\ndata is a more truthful and therefore often also a more powerful representation\nof reality. Mining patterns of a suitably expressive syntax directly from this\nrepresentation, is thus a research problem of great importance. In this paper\nwe introduce a novel approach to mining patterns in multi-relational data. We\npropose a new syntax for multi-relational patterns as complete connected\nsubgraphs in a representation of the database as a K-partite graph. We show how\nthis pattern syntax is generally applicable to multirelational data, while it\nreduces to well-known tiles [7] when the data is a simple binary or\nattribute-value table. We propose RMiner, an efficient algorithm to mine such\npatterns, and we introduce a method for quantifying their interestingness when\ncontrasted with prior information of the data miner. Finally, we illustrate the\nusefulness of our approach by discussing results on real-world and synthetic\ndatabases.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2011 15:12:30 GMT"}, {"version": "v2", "created": "Mon, 12 Sep 2011 09:57:38 GMT"}], "update_date": "2011-09-13", "authors_parsed": [["Spyropoulou", "Eirini", ""], ["De Bie", "Tijl", ""]]}, {"id": "1106.4649", "submitter": "Gonzalo Navarro", "authors": "Gonzalo Navarro and Yakov Nekrich and Lu\\'is M. S. Russo", "title": "Space-Efficient Data-Analysis Queries on Grids", "comments": "20 pages, 2 figures, submitting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider various data-analysis queries on two-dimensional points. We give\nnew space/time tradeoffs over previous work on geometric queries such as\ndominance and rectangle visibility, and on semigroup and group queries such as\nsum, average, variance, minimum and maximum. We also introduce new solutions to\nqueries less frequently considered in the literature such as two-dimensional\nquantiles, majorities, successor/predecessor, mode, and various top-$k$\nqueries, considering static and dynamic scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2011 08:15:24 GMT"}, {"version": "v2", "created": "Sat, 31 Mar 2012 21:41:24 GMT"}], "update_date": "2012-04-03", "authors_parsed": [["Navarro", "Gonzalo", ""], ["Nekrich", "Yakov", ""], ["Russo", "Lu\u00eds M. S.", ""]]}, {"id": "1106.5113", "submitter": "Tamir Tassa", "authors": "Tamir Tassa", "title": "Secure Mining of Association Rules in Horizontally Distributed Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a protocol for secure mining of association rules in horizontally\ndistributed databases. The current leading protocol is that of Kantarcioglu and\nClifton (TKDE 2004). Our protocol, like theirs, is based on the Fast\nDistributed Mining (FDM) algorithm of Cheung et al. (PDIS 1996), which is an\nunsecured distributed version of the Apriori algorithm. The main ingredients in\nour protocol are two novel secure multi-party algorithms --- one that computes\nthe union of private subsets that each of the interacting players hold, and\nanother that tests the inclusion of an element held by one player in a subset\nheld by another. Our protocol offers enhanced privacy with respect to the\nprotocol of Kantarcioglu and Clifton. In addition, it is simpler and is\nsignificantly more efficient in terms of communication rounds, communication\ncost and computational cost.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jun 2011 08:51:05 GMT"}], "update_date": "2011-06-28", "authors_parsed": [["Tassa", "Tamir", ""]]}, {"id": "1106.5122", "submitter": "Marharyta Aleksandrova", "authors": "Oleg Chertov, Marharyta Aleksandrova", "title": "Clustering with Prototype Extraction for Census Data Analysis", "comments": "World Conference on Soft Computing WConSC-2011 (San Francisco State\n  University, California, USA), 8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Not long ago primary census data became available to publicity. It opened\nqualitatively new perspectives not only for researchers in demography and\nsociology, but also for those people, who somehow face processes occurring in\nsociety. In this paper authors propose using Data Mining methods for searching\nhidden patterns in census data. A novel clustering-based technique is described\nas well. It allows determining factors which influence people behavior, in\nparticular decision-making process (as an example, a decision whether to have a\nbaby or not). Proposed technique is based on clustering a set of respondents,\nfor whom a certain event have already happened (for instance, a baby was born),\nand discovering clusters' prototypes from a set of respondents, for whom this\nevent hasn't occurred yet. By means of analyzing clusters' and their\nprototypes' characteristics it is possible to identify which factors influence\nthe decision-making process. Authors also provide an experimental example of\nthe described approach usage.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jun 2011 12:26:43 GMT"}], "update_date": "2011-06-28", "authors_parsed": [["Chertov", "Oleg", ""], ["Aleksandrova", "Marharyta", ""]]}, {"id": "1106.5568", "submitter": "Lin Zhong", "authors": "Ardalan Amiri Sani, Wolfgang Richter, Xuan Bao, Trevor Narayan,\n  Mahadev Satyanarayanan, Lin Zhong, Romit Roy Choudhury", "title": "Opportunistic Content Search of Smartphone Photos", "comments": null, "journal-ref": null, "doi": null, "report-no": "Technical Report TR0627-2011, Rice University", "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photos taken by smartphone users can accidentally contain content that is\ntimely and valuable to others, often in real-time. We report the system design\nand evaluation of a distributed search system, Theia, for crowd-sourced\nreal-time content search of smartphone photos. Because smartphones are\nresource-constrained, Theia incorporates two key innovations to control search\ncost and improve search efficiency. Incremental Search expands search scope\nincrementally and exploits user feedback. Partitioned Search leverages the\ncloud to reduce the energy consumption of search in smartphones. Through user\nstudies, measurement studies, and field studies, we show that Theia reduces the\ncost per relevant photo by an average of 59%. It reduces the energy consumption\nof search by up to 55% and 81% compared to alternative strategies of executing\nentirely locally or entirely in the cloud. Search results from smartphones are\nobtained in seconds. Our experiments also suggest approaches to further improve\nthese results.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2011 05:36:11 GMT"}], "update_date": "2011-06-29", "authors_parsed": [["Sani", "Ardalan Amiri", ""], ["Richter", "Wolfgang", ""], ["Bao", "Xuan", ""], ["Narayan", "Trevor", ""], ["Satyanarayanan", "Mahadev", ""], ["Zhong", "Lin", ""], ["Choudhury", "Romit Roy", ""]]}, {"id": "1106.5979", "submitter": "Mohammed Eunus Ali Dr", "authors": "Mohammed Eunus Ali, Egemen Tanin, Rui Zhang, and Ramamohanarao\n  Kotagiri", "title": "Probabilistic Voronoi Diagrams for Probabilistic Moving Nearest Neighbor\n  Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large spectrum of applications such as location based services and\nenvironmental monitoring demand efficient query processing on uncertain\ndatabases. In this paper, we propose the probabilistic Voronoi diagram (PVD)\nfor processing moving nearest neighbor queries on uncertain data, namely the\nprobabilistic moving nearest neighbor (PMNN) queries. A PMNN query finds the\nmost probable nearest neighbor of a moving query point continuously. To process\nPMNN queries efficiently, we provide two techniques: a pre-computation approach\nand an incremental approach. In the pre-computation approach, we develop an\nalgorithm to efficiently evaluate PMNN queries based on the pre-computed PVD\nfor the entire data set. In the incremental approach, we propose an incremental\nprobabilistic safe region based technique that does not require to pre-compute\nthe whole PVD to answer the PMNN query. In this incremental approach, we\nexploit the knowledge for a known region to compute the lower bound of the\nprobability of an object being the nearest neighbor. Experimental results show\nthat our approaches significantly outperform a sampling based approach by\norders of magnitude in terms of I/O, query processing time, and communication\noverheads.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2011 15:49:36 GMT"}], "update_date": "2011-06-30", "authors_parsed": [["Ali", "Mohammed Eunus", ""], ["Tanin", "Egemen", ""], ["Zhang", "Rui", ""], ["Kotagiri", "Ramamohanarao", ""]]}]