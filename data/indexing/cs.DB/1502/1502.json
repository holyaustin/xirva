[{"id": "1502.00068", "submitter": "Ameet Talwalkar", "authors": "Evan R. Sparks, Ameet Talwalkar, Michael J. Franklin, Michael I.\n  Jordan, Tim Kraska", "title": "TuPAQ: An Efficient Planner for Large-scale Predictive Analytic Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of massive datasets combined with the development of\nsophisticated analytical techniques have enabled a wide variety of novel\napplications such as improved product recommendations, automatic image tagging,\nand improved speech-driven interfaces. These and many other applications can be\nsupported by Predictive Analytic Queries (PAQs). A major obstacle to supporting\nPAQs is the challenging and expensive process of identifying and training an\nappropriate predictive model. Recent efforts aiming to automate this process\nhave focused on single node implementations and have assumed that model\ntraining itself is a black box, thus limiting the effectiveness of such\napproaches on large-scale problems. In this work, we build upon these recent\nefforts and propose an integrated PAQ planning architecture that combines\nadvanced model search techniques, bandit resource allocation via runtime\nalgorithm introspection, and physical optimization via batching. The result is\nTuPAQ, a component of the MLbase system, which solves the PAQ planning problem\nwith comparable quality to exhaustive strategies but an order of magnitude more\nefficiently than the standard baseline approach, and can scale to models\ntrained on terabytes of data across hundreds of machines.\n", "versions": [{"version": "v1", "created": "Sat, 31 Jan 2015 04:51:58 GMT"}, {"version": "v2", "created": "Sun, 8 Mar 2015 22:02:24 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Sparks", "Evan R.", ""], ["Talwalkar", "Ameet", ""], ["Franklin", "Michael J.", ""], ["Jordan", "Michael I.", ""], ["Kraska", "Tim", ""]]}, {"id": "1502.00316", "submitter": "Emilio Ferrara", "authors": "Xiaoming Gao, Emilio Ferrara, Judy Qiu", "title": "Parallel clustering of high-dimensional social media data streams", "comments": "IEEE/ACM CCGrid 2015: 15th IEEE/ACM International Symposium on\n  Cluster, Cloud and Grid Computing, 2015", "journal-ref": "Cluster, Cloud and Grid Computing (CCGrid), 2015 15th IEEE/ACM\n  International Symposium on (pp. 323-332). IEEE. 2015", "doi": "10.1109/CCGrid.2015.19", "report-no": null, "categories": "cs.DC cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Cloud DIKW as an analysis environment supporting scientific\ndiscovery through integrated parallel batch and streaming processing, and apply\nit to one representative domain application: social media data stream\nclustering. Recent work demonstrated that high-quality clusters can be\ngenerated by representing the data points using high-dimensional vectors that\nreflect textual content and social network information. Due to the high cost of\nsimilarity computation, sequential implementations of even single-pass\nalgorithms cannot keep up with the speed of real-world streams. This paper\npresents our efforts to meet the constraints of real-time social stream\nclustering through parallelization. We focus on two system-level issues. Most\nstream processing engines like Apache Storm organize distributed workers in the\nform of a directed acyclic graph, making it difficult to dynamically\nsynchronize the state of parallel workers. We tackle this challenge by creating\na separate synchronization channel using a pub-sub messaging system. Due to the\nsparsity of the high-dimensional vectors, the size of centroids grows quickly\nas new data points are assigned to the clusters. Traditional synchronization\nthat directly broadcasts cluster centroids becomes too expensive and limits the\nscalability of the parallel algorithm. We address this problem by communicating\nonly dynamic changes of the clusters rather than the whole centroid vectors.\nOur algorithm under Cloud DIKW can process the Twitter 10% data stream in\nreal-time with 96-way parallelism. By natural improvements to Cloud DIKW,\nincluding advanced collective communication techniques developed in our Harp\nproject, we will be able to process the full Twitter stream in real-time with\n1000-way parallelism. Our use of powerful general software subsystems will\nenable many other applications that need integration of streaming and batch\ndata analytics.\n", "versions": [{"version": "v1", "created": "Sun, 1 Feb 2015 21:41:13 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Gao", "Xiaoming", ""], ["Ferrara", "Emilio", ""], ["Qiu", "Judy", ""]]}, {"id": "1502.00731", "submitter": "Jaeho Shin", "authors": "Jaeho Shin, Sen Wu, Feiran Wang, Christopher De Sa, Ce Zhang,\n  Christopher R\\'e", "title": "Incremental Knowledge Base Construction Using DeepDive", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Populating a database with unstructured information is a long-standing\nproblem in industry and research that encompasses problems of extraction,\ncleaning, and integration. Recent names used for this problem include dealing\nwith dark data and knowledge base construction (KBC). In this work, we describe\nDeepDive, a system that combines database and machine learning ideas to help\ndevelop KBC systems, and we present techniques to make the KBC process more\nefficient. We observe that the KBC process is iterative, and we develop\ntechniques to incrementally produce inference results for KBC systems. We\npropose two methods for incremental inference, based respectively on sampling\nand variational techniques. We also study the tradeoff space of these methods\nand develop a simple rule-based optimizer. DeepDive includes all of these\ncontributions, and we evaluate DeepDive on five KBC systems, showing that it\ncan speed up KBC inference tasks by up to two orders of magnitude with\nnegligible impact on quality.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 04:16:24 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2015 21:59:15 GMT"}, {"version": "v3", "created": "Sat, 18 Apr 2015 06:13:32 GMT"}, {"version": "v4", "created": "Mon, 15 Jun 2015 22:24:05 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Shin", "Jaeho", ""], ["Wu", "Sen", ""], ["Wang", "Feiran", ""], ["De Sa", "Christopher", ""], ["Zhang", "Ce", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1502.01539", "submitter": "Richard McClatchey", "authors": "Khawar Hasham, Kamran Munir, Jetendr Shamdasani, Richard McClatchey", "title": "Scientific Workflow Repeatability through Cloud-Aware Provenance", "comments": "6 pages; 5 figures; 3 tables in Proceedings of the Recomputability\n  2014 workshop of the 7th IEEE/ACM International Conference on Utility and\n  Cloud Computing (UCC 2014). London December 2014", "journal-ref": null, "doi": "10.1109/UCC.2014.155", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The transformations, analyses and interpretations of data in scientific\nworkflows are vital for the repeatability and reliability of scientific\nworkflows. This provenance of scientific workflows has been effectively carried\nout in Grid based scientific workflow systems. However, recent adoption of\nCloud-based scientific workflows present an opportunity to investigate the\nsuitability of existing approaches or propose new approaches to collect\nprovenance information from the Cloud and to utilize it for workflow\nrepeatability in the Cloud infrastructure. The dynamic nature of the Cloud in\ncomparison to the Grid makes it difficult because resources are provisioned\non-demand unlike the Grid. This paper presents a novel approach that can assist\nin mitigating this challenge. This approach can collect Cloud infrastructure\ninformation along with workflow provenance and can establish a mapping between\nthem. This mapping is later used to re-provision resources on the Cloud. The\nrepeatability of the workflow execution is performed by: (a) capturing the\nCloud infrastructure information (virtual machine configuration) along with the\nworkflow provenance, and (b) re-provisioning the similar resources on the Cloud\nand re-executing the workflow on them. The evaluation of an initial prototype\nsuggests that the proposed approach is feasible and can be investigated\nfurther.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 13:33:07 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Hasham", "Khawar", ""], ["Munir", "Kamran", ""], ["Shamdasani", "Jetendr", ""], ["McClatchey", "Richard", ""]]}, {"id": "1502.01545", "submitter": "Richard McClatchey", "authors": "Richard McClatchey, Andrew Branson, Jetendr Shamdasani, Zsolt Kovacs,\n  and the CRISTAL-ISE Consortium", "title": "Designing Traceability into Big Data Systems", "comments": "10 pages; 6 figures in Proceedings of the 5th Annual International\n  Conference on ICT: Big Data, Cloud and Security (ICT-BDCS 2015), Singapore\n  July 2015. arXiv admin note: text overlap with arXiv:1402.5764,\n  arXiv:1402.5753", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Providing an appropriate level of accessibility and traceability to data or\nprocess elements (so-called Items) in large volumes of data, often\nCloud-resident, is an essential requirement in the Big Data era.\nEnterprise-wide data systems need to be designed from the outset to support\nusage of such Items across the spectrum of business use rather than from any\nspecific application view. The design philosophy advocated in this paper is to\ndrive the design process using a so-called description-driven approach which\nenriches models with meta-data and description and focuses the design process\non Item re-use, thereby promoting traceability. Details are given of the\ndescription-driven design of big data systems at CERN, in health informatics\nand in business process management. Evidence is presented that the approach\nleads to design simplicity and consequent ease of management thanks to loose\ntyping and the adoption of a unified approach to Item management and usage.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 13:40:00 GMT"}], "update_date": "2015-02-06", "authors_parsed": [["McClatchey", "Richard", ""], ["Branson", "Andrew", ""], ["Shamdasani", "Jetendr", ""], ["Kovacs", "Zsolt", ""], ["Consortium", "the CRISTAL-ISE", ""]]}, {"id": "1502.01556", "submitter": "Richard McClatchey", "authors": "Bilal Arshad, Kamran Munir, Richard McClatchey and Saad Liaquat", "title": "Position Paper: Provenance Data Visualisation for Neuroimaging Analysis", "comments": "6 pages; 6 figures in Proceedings of the 12th International\n  Conference on Frontiers of Information Technology (FIT 2014) Islamabad,\n  Pakistan December 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualisation facilitates the understanding of scientific data both through\nexploration and explanation of visualised data. Provenance contributes to the\nunderstanding of data by containing the contributing factors behind a result.\nWith the significant increase in data volumes and algorithm complexity,\nclinical researchers are struggling with information tracking, analysis\nreproducibility and the verification of scientific output. Data coming from\nvarious heterogeneous sources (multiple sources with varying level of trust) in\na collaborative environment adds to the uncertainty of the scientific output.\nSystems are required that offer provenance data capture and visualisation\nsupport for analyses. We present an account for the need to visualise\nprovenance information in order to aid the process of verification of\nscientific outputs, comparison of analyses,progression and evolution of results\nfor neuroimaging analysis.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 14:05:25 GMT"}], "update_date": "2015-02-06", "authors_parsed": [["Arshad", "Bilal", ""], ["Munir", "Kamran", ""], ["McClatchey", "Richard", ""], ["Liaquat", "Saad", ""]]}, {"id": "1502.02242", "submitter": "Jelle Hellings", "authors": "Jelle Hellings", "title": "Querying for Paths in Graphs using Context-Free Path Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Navigational queries for graph-structured data, such as the regular path\nqueries and the context-free path queries, are usually evaluated to a relation\nof node-pairs $(m, n)$ such that there is a path from $m$ to $n$ satisfying the\nconditions of the query. Although this relational query semantics has practical\nvalue, we believe that the relational query semantics can only provide limited\ninsight in the structure of the graph data. To address the limits of the\nrelational query semantics, we introduce the all-path query semantics and the\nsingle-path query semantics. Under these path-based query semantics, a query is\nevaluated to all paths satisfying the conditions of the query, or,\nrespectively, to a single such path.\n  While focusing on context-free path queries, we provide a formal framework\nfor evaluating queries on graphs using both path-based query semantics. For the\nall-path query semantics, we show that the result of a query can be represented\nby a finite context-free grammar annotated with node-information relevant for\nderiving each path in the query result. For the single-path query semantics, we\npropose to search for a path of minimum length. We reduce the problem of\nfinding such a path of minimum length to finding a string of minimum length in\na context-free language, and for deriving such a string we propose a novel\nalgorithm.\n  Our initial results show that the path-based query semantics have added\npractical value and that query evaluation for both path-based query semantics\nis feasible, even when query results grow very large. For the single-path query\nsemantics, determining strict worst-case upper bounds on the size of the query\nresult remains the focus of future work.\n", "versions": [{"version": "v1", "created": "Sun, 8 Feb 2015 12:11:33 GMT"}, {"version": "v2", "created": "Sun, 31 Jan 2016 14:43:13 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["Hellings", "Jelle", ""]]}, {"id": "1502.02511", "submitter": "Behzad Ghanbarian", "authors": "Behzad Ghanbarian, Vahid Taslimitehrani, Guozhu Dong, Yakov A.\n  Pachepsky", "title": "Measurement Scale Effect on Prediction of Soil Water Retention Curve and\n  Saturated Hydraulic Conductivity", "comments": null, "journal-ref": "Journal of Hydrology (2015) Vol. 528 pp. 127-137", "doi": "10.1016/j.jhydrol.2015.06.024", "report-no": null, "categories": "cs.CE cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Soil water retention curve (SWRC) and saturated hydraulic conductivity (SHC)\nare key hydraulic properties for unsaturated zone hydrology and groundwater. In\nparticular, SWRC provides useful information on entry pore-size distribution,\nand SHC is required for flow and transport modeling in the hydrologic cycle.\nNot only the SWRC and SHC measurements are time-consuming, but also scale\ndependent. This means as soil column volume increases, variability of the SWRC\nand SHC decreases. Although prediction of the SWRC and SHC from available\nparameters, such as textural data, organic matter, and bulk density have been\nunder investigation for decades, up to now no research has focused on the\neffect of measurement scale on the soil hydraulic properties pedotransfer\nfunctions development. In the literature, several data mining approaches have\nbeen applied, such as multiple linear regression, artificial neural networks,\ngroup method of data handling. However, in this study we develop pedotransfer\nfunctions using a novel approach called contrast pattern aided regression\n(CPXR) and compare it with the multiple linear regression method. For this\npurpose, two databases including 210 and 213 soil samples are collected to\ndevelop and evaluate pedotransfer functions for the SWRC and SHC, respectively,\nfrom the UNSODA database. The 10-fold cross-validation method is applied to\nevaluate the accuracy and reliability of the proposed regression-based models.\nOur results show that including measurement scale parameters, such as sample\ninternal diameter and length could substantially improve the accuracy of the\nSWRC and SHC pedotransfer functions developed using the CPXR method, while this\nis not the case when MLR is used. Moreover, the CPXR method yields remarkably\nmore accurate soil water retention curve and saturated hydraulic conductivity\npredictions than the MLR approach.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 14:54:48 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Ghanbarian", "Behzad", ""], ["Taslimitehrani", "Vahid", ""], ["Dong", "Guozhu", ""], ["Pachepsky", "Yakov A.", ""]]}, {"id": "1502.02642", "submitter": "Slimane Oulad-Naoui", "authors": "Slimane Oulad-Naoui, Hadda Cherroun and Djelloul Ziadi", "title": "Mining Frequent Itemsets: a Formal Unification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CC cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is generally well agreed that developing a unifying theory is one of the\nmost important issues in Data Mining research. In the last two decades, a great\ndeal of work has been devoted to the algorithmic aspects of the Frequent\nItemset (FI) Mining problem. We are motivated by the need for formal modeling\nin the field. Thus, we introduce and analyze, in this theoretical study, a new\nmodel for the FI mining task. Indeed, we encode the itemsets as words over an\nordered alphabet, and state this problem by a formal series over the counting\nsemiring $(\\mathbb{N},+,\\times,0,1)$, whose range constitutes the itemsets and\nthe coefficients are their supports. This formalism offers many advantages in\nboth fundamental and practical aspects: the introduction of a clear and unified\ntheoretical framework through which we can express the main FI-approaches, the\npossibility of their generalization to mine other more complex objects, and\ntheir incrementalisation or parallelisation; in practice, we explain how this\nproblem can be seen as that of word recognition by an automaton, allowing an\nefficient implementation in $O(|Q|)$ space and $O(|\\mathcal{F}_L||Q|])$ time,\nwhere $Q$ is the set of states of the automaton used for representing the data,\nand $\\mathcal{F}_L$ the set of prefixial longest FI.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 20:28:46 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2015 19:13:44 GMT"}, {"version": "v3", "created": "Sun, 15 Dec 2019 21:10:41 GMT"}, {"version": "v4", "created": "Fri, 24 Jan 2020 23:46:00 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Oulad-Naoui", "Slimane", ""], ["Cherroun", "Hadda", ""], ["Ziadi", "Djelloul", ""]]}, {"id": "1502.03258", "submitter": "George  Fletcher", "authors": "George H. L. Fletcher, Marc Gyssens, Jan Paredaens, Dirk Van Gucht,\n  Yuqing Wu", "title": "Structural characterizations of the navigational expressiveness of\n  relation algebras on a tree", "comments": "58 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a document D in the form of an unordered node-labeled tree, we study\nthe expressiveness on D of various basic fragments of XPath, the core\nnavigational language on XML documents. Working from the perspective of these\nlanguages as fragments of Tarski's relation algebra, we give characterizations,\nin terms of the structure of D, for when a binary relation on its nodes is\ndefinable by an expression in these algebras. Since each pair of nodes in such\na relation represents a unique path in D, our results therefore capture the\nsets of paths in D definable in each of the fragments. We refer to this\nperspective on language semantics as the \"global view.\" In contrast with this\nglobal view, there is also a \"local view\" where one is interested in the nodes\nto which one can navigate starting from a particular node in the document. In\nthis view, we characterize when a set of nodes in D can be defined as the\nresult of applying an expression to a given node of D. All these definability\nresults, both in the global and the local view, are obtained by using a robust\ntwo-step methodology, which consists of first characterizing when two nodes\ncannot be distinguished by an expression in the respective fragments of XPath,\nand then bootstrapping these characterizations to the desired results.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 10:49:04 GMT"}], "update_date": "2015-02-12", "authors_parsed": [["Fletcher", "George H. L.", ""], ["Gyssens", "Marc", ""], ["Paredaens", "Jan", ""], ["Van Gucht", "Dirk", ""], ["Wu", "Yuqing", ""]]}, {"id": "1502.03519", "submitter": "Xin Luna Dong", "authors": "Xin Luna Dong, Evgeniy Gabrilovich, Kevin Murphy, Van Dang, Wilko\n  Horn, Camillo Lugaresi, Shaohua Sun, Wei Zhang", "title": "Knowledge-Based Trust: Estimating the Trustworthiness of Web Sources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quality of web sources has been traditionally evaluated using exogenous\nsignals such as the hyperlink structure of the graph. We propose a new approach\nthat relies on endogenous signals, namely, the correctness of factual\ninformation provided by the source. A source that has few false facts is\nconsidered to be trustworthy. The facts are automatically extracted from each\nsource by information extraction methods commonly used to construct knowledge\nbases. We propose a way to distinguish errors made in the extraction process\nfrom factual errors in the web source per se, by using joint inference in a\nnovel multi-layer probabilistic model. We call the trustworthiness score we\ncomputed Knowledge-Based Trust (KBT). On synthetic data, we show that our\nmethod can reliably compute the true trustworthiness levels of the sources. We\nthen apply it to a database of 2.8B facts extracted from the web, and thereby\nestimate the trustworthiness of 119M webpages. Manual evaluation of a subset of\nthe results confirms the effectiveness of the method.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 02:45:06 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Dong", "Xin Luna", ""], ["Gabrilovich", "Evgeniy", ""], ["Murphy", "Kevin", ""], ["Dang", "Van", ""], ["Horn", "Wilko", ""], ["Lugaresi", "Camillo", ""], ["Sun", "Shaohua", ""], ["Zhang", "Wei", ""]]}, {"id": "1502.04662", "submitter": "Tim Althoff", "authors": "Tim Althoff, Xin Luna Dong, Kevin Murphy, Safa Alai, Van Dang, Wei\n  Zhang", "title": "TimeMachine: Timeline Generation for Knowledge-Base Entities", "comments": "To appear at ACM SIGKDD KDD'15. 12pp, 7 fig. With appendix. Demo and\n  other info available at http://cs.stanford.edu/~althoff/timemachine/", "journal-ref": null, "doi": "10.1145/2783258.2783325", "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method called TIMEMACHINE to generate a timeline of events and\nrelations for entities in a knowledge base. For example for an actor, such a\ntimeline should show the most important professional and personal milestones\nand relationships such as works, awards, collaborations, and family\nrelationships. We develop three orthogonal timeline quality criteria that an\nideal timeline should satisfy: (1) it shows events that are relevant to the\nentity; (2) it shows events that are temporally diverse, so they distribute\nalong the time axis, avoiding visual crowding and allowing for easy user\ninteraction, such as zooming in and out; and (3) it shows events that are\ncontent diverse, so they contain many different types of events (e.g., for an\nactor, it should show movies and marriages and awards, not just movies). We\npresent an algorithm to generate such timelines for a given time period and\nscreen size, based on submodular optimization and web-co-occurrence statistics\nwith provable performance guarantees. A series of user studies using Mechanical\nTurk shows that all three quality criteria are crucial to produce quality\ntimelines and that our algorithm significantly outperforms various baseline and\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 18:53:01 GMT"}, {"version": "v2", "created": "Sat, 21 Feb 2015 07:02:11 GMT"}, {"version": "v3", "created": "Mon, 8 Jun 2015 20:39:26 GMT"}], "update_date": "2015-06-10", "authors_parsed": [["Althoff", "Tim", ""], ["Dong", "Xin Luna", ""], ["Murphy", "Kevin", ""], ["Alai", "Safa", ""], ["Dang", "Van", ""], ["Zhang", "Wei", ""]]}, {"id": "1502.05106", "submitter": "Saravanan Thirumuruganathan", "authors": "Habibur Rahman, Senjuti Basu Roy, Saravanan Thirumuruganathan, Sihem\n  Amer-Yahia, Gautam Das", "title": "\"The Whole Is Greater Than the Sum of Its Parts\": Optimization in\n  Collaborative Crowdsourcing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we initiate the investigation of optimization opportunities in\ncollaborative crowdsourcing. Many popular applications, such as collaborative\ndocument editing, sentence translation, or citizen science resort to this\nspecial form of human-based computing, where, crowd workers with appropriate\nskills and expertise are required to form groups to solve complex tasks.\nCentral to any collaborative crowdsourcing process is the aspect of successful\ncollaboration among the workers, which, for the first time, is formalized and\nthen optimized in this work. Our formalism considers two main\ncollaboration-related human factors, affinity and upper critical mass,\nappropriately adapted from organizational science and social theories. Our\ncontributions are (a) proposing a comprehensive model for collaborative\ncrowdsourcing optimization, (b) rigorous theoretical analyses to understand the\nhardness of the proposed problems, (c) an array of efficient exact and\napproximation algorithms with provable theoretical guarantees. Finally, we\npresent a detailed set of experimental results stemming from two real-world\ncollaborative crowdsourcing application us- ing Amazon Mechanical Turk, as well\nas conduct synthetic data analyses on scalability and qualitative aspects of\nour proposed algorithms. Our experimental results successfully demonstrate the\nefficacy of our proposed solutions.\n", "versions": [{"version": "v1", "created": "Wed, 18 Feb 2015 02:53:51 GMT"}, {"version": "v2", "created": "Sun, 12 Apr 2015 21:45:34 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Rahman", "Habibur", ""], ["Roy", "Senjuti Basu", ""], ["Thirumuruganathan", "Saravanan", ""], ["Amer-Yahia", "Sihem", ""], ["Das", "Gautam", ""]]}, {"id": "1502.05441", "submitter": "Ahmad Hassanat", "authors": "Ahmad B.A. Hassanat and Ghada Awad Altarawneh", "title": "Rule-and Dictionary-based Solution for Variations in Written Arabic\n  Names in Social Networks, Big Data, Accounting Systems and Large Databases", "comments": null, "journal-ref": "Research Journal of Applied Sciences, Engineering and Technology,\n  2014, 8(14): 1630-1638", "doi": null, "report-no": null, "categories": "cs.DB cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the problem that some Arabic names can be written in\nmultiple ways. When someone searches for only one form of a name, neither exact\nnor approximate matching is appropriate for returning the multiple variants of\nthe name. Exact matching requires the user to enter all forms of the name for\nthe search, and approximate matching yields names not among the variations of\nthe one being sought. In this paper, we attempt to solve the problem with a\ndictionary of all Arabic names mapped to their different (alternative) writing\nforms. We generated alternatives based on rules we derived from reviewing the\nfirst names of 9.9 million citizens and former citizens of Jordan. This\ndictionary can be used for both standardizing the written form when inserting a\nnew name into a database and for searching for the name and all its alternative\nwritten forms. Creating the dictionary automatically based on rules resulted in\nat least 7% erroneous acceptance errors and 7.9% erroneous rejection errors. We\naddressed the errors by manually editing the dictionary. The dictionary can be\nof help to real world-databases, with the qualification that manual editing\ndoes not guarantee 100% correctness.\n", "versions": [{"version": "v1", "created": "Wed, 18 Feb 2015 23:16:00 GMT"}], "update_date": "2015-02-20", "authors_parsed": [["Hassanat", "Ahmad B. A.", ""], ["Altarawneh", "Ghada Awad", ""]]}, {"id": "1502.05730", "submitter": "Evgeny Nikulchev", "authors": "Evgeniy Pluzhnik, Oleg Lukyanchikov, Evgeny Nikulchev, Simon Payain", "title": "Designing Applications with Distributed Databases in a Hybrid Cloud", "comments": "in WIT Transactions of Information and Communication Technologies,\n  2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing applications for use in a hybrid cloud has many features. These\ninclude dynamic virtualization management and an unknown route switching\ncustomers. This makes it impossible to evaluate the query and hence the optimal\ndistribution of data. In this paper, we formulate the main challenges of\ndesigning and simulation offer installation for processing.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2015 21:25:11 GMT"}], "update_date": "2015-02-23", "authors_parsed": [["Pluzhnik", "Evgeniy", ""], ["Lukyanchikov", "Oleg", ""], ["Nikulchev", "Evgeny", ""], ["Payain", "Simon", ""]]}, {"id": "1502.05844", "submitter": "Mona Dadjoo", "authors": "Mona Dadjoo, Esmaeil Kheirkhah", "title": "An Approach For Transforming of Relational Databases to OWL Ontology", "comments": "10 pages in International Journal of Web & Semantic Technology\n  (IJWesT) Vol.6, No.1, January 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid growth of documents, web pages, and other types of text content is a\nhuge challenge for the modern content management systems. One of the problems\nin the areas of information storage and retrieval is the lacking of semantic\ndata. Ontologies can present knowledge in sharable and repeatedly usable manner\nand provide an effective way to reduce the data volume overhead by encoding the\nstructure of a particular domain. Metadata in relational databases can be used\nto extract ontology from database in a special domain. According to solve the\nproblem of sharing and reusing of data, approaches based on transforming\nrelational database to ontology are proposed. In this paper we propose a method\nfor automatic ontology construction based on relational database. Mining and\nobtaining further components from relational database leads to obtain knowledge\nwith high semantic power and more expressiveness. Triggers are one of the\ndatabase components which could be transformed to the ontology model and\nincrease the amount of power and expressiveness of knowledge by presenting part\nof the knowledge dynamically\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2015 11:59:51 GMT"}], "update_date": "2015-02-23", "authors_parsed": [["Dadjoo", "Mona", ""], ["Kheirkhah", "Esmaeil", ""]]}, {"id": "1502.05943", "submitter": "Uwe Aickelin", "authors": "Jenna M. Reps, Uwe Aickelin, Jiangang Ma, Yanchun Zhang", "title": "Refining Adverse Drug Reactions using Association Rule Mining for\n  Electronic Healthcare Data", "comments": "IEEE International Conference of Data Mining: Data Mining in\n  Biomedical Informatics and Healthcare (DMBIH) Workshop 2014, 2014", "journal-ref": null, "doi": "10.1109/ICDMW.2014.53", "report-no": null, "categories": "cs.DB cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Side effects of prescribed medications are a common occurrence. Electronic\nhealthcare databases present the opportunity to identify new side effects\nefficiently but currently the methods are limited due to confounding (i.e. when\nan association between two variables is identified due to them both being\nassociated to a third variable).\n  In this paper we propose a proof of concept method that learns common\nassociations and uses this knowledge to automatically refine side effect\nsignals (i.e. exposure-outcome associations) by removing instances of the\nexposure-outcome associations that are caused by confounding. This leaves the\nsignal instances that are most likely to correspond to true side effect\noccurrences. We then calculate a novel measure termed the confounding-adjusted\nrisk value, a more accurate absolute risk value of a patient experiencing the\noutcome within 60 days of the exposure.\n  Tentative results suggest that the method works. For the four signals (i.e.\nexposure-outcome associations) investigated we are able to correctly filter the\nmajority of exposure-outcome instances that were unlikely to correspond to true\nside effects. The method is likely to improve when tuning the association rule\nmining parameters for specific health outcomes.\n  This paper shows that it may be possible to filter signals at a patient level\nbased on association rules learned from considering patients' medical\nhistories. However, additional work is required to develop a way to automate\nthe tuning of the method's parameters.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2015 17:14:17 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Reps", "Jenna M.", ""], ["Aickelin", "Uwe", ""], ["Ma", "Jiangang", ""], ["Zhang", "Yanchun", ""]]}, {"id": "1502.05947", "submitter": "Ryan Wisnesky", "authors": "Ryan Wisnesky, David I. Spivak, Patrick Schultz, Eswaran Subrahmanian", "title": "Functorial Data Migration: From Theory to Practice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe a functorial data migration scenario about the\nmanufacturing service capability of a distributed supply chain. The scenario is\na category-theoretic analog of an OWL ontology-based semantic enrichment\nscenario developed at the National Institute of Standards and Technology\n(NIST). The scenario is presented using, and is included with, the open-source\nFQL tool, available for download at categoricaldata.net/fql.html.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2015 17:41:40 GMT"}, {"version": "v2", "created": "Tue, 12 May 2015 22:47:57 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Wisnesky", "Ryan", ""], ["Spivak", "David I.", ""], ["Schultz", "Patrick", ""], ["Subrahmanian", "Eswaran", ""]]}, {"id": "1502.05955", "submitter": "Edith Cohen", "authors": "Edith Cohen", "title": "Stream Sampling for Frequency Cap Statistics", "comments": "21 pages, 4 figures, preliminary version will appear in KDD 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unaggregated data, in streamed or distributed form, is prevalent and come\nfrom diverse application domains which include interactions of users with web\nservices and IP traffic. Data elements have {\\em keys} (cookies, users,\nqueries) and elements with different keys interleave. Analytics on such data\ntypically utilizes statistics stated in terms of the frequencies of keys. The\ntwo most common statistics are {\\em distinct}, which is the number of active\nkeys in a specified segment, and {\\em sum}, which is the sum of the frequencies\nof keys in the segment. Both are special cases of {\\em cap} statistics, defined\nas the sum of frequencies {\\em capped} by a parameter $T$, which are popular in\nonline advertising platforms. Aggregation by key, however, is costly, requiring\nstate proportional to the number of distinct keys, and therefore we are\ninterested in estimating these statistics or more generally, sampling the data,\nwithout aggregation. We present a sampling framework for unaggregated data that\nuses a single pass (for streams) or two passes (for distributed data) and state\nproportional to the desired sample size. Our design provides the first\neffective solution for general frequency cap statistics. Our $\\ell$-capped\nsamples provide estimates with tight statistical guarantees for cap statistics\nwith $T=\\Theta(\\ell)$ and nonnegative unbiased estimates of {\\em any} monotone\nnon-decreasing frequency statistics. An added benefit of our unified design is\nfacilitating {\\em multi-objective samples}, which provide estimates with\nstatistical guarantees for a specified set of different statistics, using a\nsingle, smaller sample.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2015 17:53:45 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2015 13:49:41 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Cohen", "Edith", ""]]}, {"id": "1502.06823", "submitter": "Theodoros Rekatsinas", "authors": "Theodoros Rekatsinas, Amol Deshpande and Aditya Parameswaran", "title": "CrowdGather: Entity Extraction over Structured Domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourced entity extraction is often used to acquire data for many\napplications, including recommendation systems, construction of aggregated\nlistings and directories, and knowledge base construction. Current solutions\nfocus on entity extraction using a single query, e.g., only using \"give me\nanother restaurant\", when assembling a list of all restaurants. Due to the cost\nof human labor, solutions that focus on a single query can be highly\nimpractical.\n  In this paper, we leverage the fact that entity extraction often focuses on\n{\\em structured domains}, i.e., domains that are described by a collection of\nattributes, each potentially exhibiting hierarchical structure. Given such a\ndomain, we enable a richer space of queries, e.g., \"give me another Moroccan\nrestaurant in Manhattan that does takeout\". Naturally, enabling a richer space\nof queries comes with a host of issues, especially since many queries return\nempty answers. We develop new statistical tools that enable us to reason about\nthe gain of issuing {\\em additional queries} given little to no information,\nand show how we can exploit the overlaps across the results of queries for\ndifferent points of the data domain to obtain accurate estimates of the gain.\nWe cast the problem of {\\em budgeted entity extraction} over large domains as\nan adaptive optimization problem that seeks to maximize the number of extracted\nentities, while minimizing the overall extraction costs. We evaluate our\ntechniques with experiments on both synthetic and real-world datasets,\ndemonstrating a yield of up to 4X over competing approaches for the same\nbudget.\n", "versions": [{"version": "v1", "created": "Tue, 24 Feb 2015 14:41:15 GMT"}], "update_date": "2015-02-25", "authors_parsed": [["Rekatsinas", "Theodoros", ""], ["Deshpande", "Amol", ""], ["Parameswaran", "Aditya", ""]]}, {"id": "1502.07169", "submitter": "Wolf Roediger", "authors": "Wolf Roediger, Tobias Muehlbauer, Alfons Kemper, Thomas Neumann", "title": "High-Speed Query Processing over High-Speed Networks", "comments": "12 pages, accepted at VLDB 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern database clusters entail two levels of networks: connecting CPUs and\nNUMA regions inside a single server in the small and multiple servers in the\nlarge. The huge performance gap between these two types of networks used to\nslow down distributed query processing to such an extent that a cluster of\nmachines actually performed worse than a single many-core server. The increased\nmain-memory capacity of the cluster remained the sole benefit of such a\nscale-out.\n  The economic viability of high-speed interconnects such as InfiniBand has\nnarrowed this performance gap considerably. However, InfiniBand's higher\nnetwork bandwidth alone does not improve query performance as expected when the\ndistributed query engine is left unchanged. The scalability of distributed\nquery processing is impaired by TCP overheads, switch contention due to\nuncoordinated communication, and load imbalances resulting from the\ninflexibility of the classic exchange operator model. This paper presents the\nblueprint for a distributed query engine that addresses these problems by\nconsidering both levels of networks holistically. It consists of two parts:\nFirst, hybrid parallelism that distinguishes local and distributed parallelism\nfor better scalability in both the number of cores as well as servers. Second,\na novel communication multiplexer tailored for analytical database workloads\nusing remote direct memory access (RDMA) and low-latency network scheduling for\nhigh-speed communication with almost no CPU overhead. An extensive evaluation\nwithin the HyPer database system using the TPC-H benchmark shows that our\nholistic approach indeed enables high-speed query processing over high-speed\nnetworks.\n", "versions": [{"version": "v1", "created": "Wed, 25 Feb 2015 14:08:51 GMT"}, {"version": "v2", "created": "Thu, 7 May 2015 13:12:15 GMT"}, {"version": "v3", "created": "Mon, 21 Sep 2015 13:34:35 GMT"}, {"version": "v4", "created": "Mon, 2 Nov 2015 14:31:10 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Roediger", "Wolf", ""], ["Muehlbauer", "Tobias", ""], ["Kemper", "Alfons", ""], ["Neumann", "Thomas", ""]]}, {"id": "1502.07526", "submitter": "Ganzhao Yuan", "authors": "Ganzhao Yuan, Zhenjie Zhang, Marianne Winslett, Xiaokui Xiao, Yin\n  Yang, Zhifeng Hao", "title": "Optimizing Batch Linear Queries under Exact and Approximate Differential\n  Privacy", "comments": "ACM Transactions on Database Systems (ACM TODS). arXiv admin note:\n  text overlap with arXiv:1212.2309", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy is a promising privacy-preserving paradigm for\nstatistical query processing over sensitive data. It works by injecting random\nnoise into each query result, such that it is provably hard for the adversary\nto infer the presence or absence of any individual record from the published\nnoisy results. The main objective in differentially private query processing is\nto maximize the accuracy of the query results, while satisfying the privacy\nguarantees. Previous work, notably \\cite{LHR+10}, has suggested that with an\nappropriate strategy, processing a batch of correlated queries as a whole\nachieves considerably higher accuracy than answering them individually.\nHowever, to our knowledge there is currently no practical solution to find such\na strategy for an arbitrary query batch; existing methods either return\nstrategies of poor quality (often worse than naive methods) or require\nprohibitively expensive computations for even moderately large domains.\nMotivated by this, we propose low-rank mechanism (LRM), the first practical\ndifferentially private technique for answering batch linear queries with high\naccuracy. LRM works for both exact (i.e., $\\epsilon$-) and approximate (i.e.,\n($\\epsilon$, $\\delta$)-) differential privacy definitions. We derive the\nutility guarantees of LRM, and provide guidance on how to set the privacy\nparameters given the user's utility expectation. Extensive experiments using\nreal data demonstrate that our proposed method consistently outperforms\nstate-of-the-art query processing solutions under differential privacy, by\nlarge margins.\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 12:47:53 GMT"}], "update_date": "2015-02-27", "authors_parsed": [["Yuan", "Ganzhao", ""], ["Zhang", "Zhenjie", ""], ["Winslett", "Marianne", ""], ["Xiao", "Xiaokui", ""], ["Yang", "Yin", ""], ["Hao", "Zhifeng", ""]]}, {"id": "1502.07576", "submitter": "Hamida Seba", "authors": "Hamida Seba and Sofiane Lagraa and Elsen Ronando", "title": "Comparison Issues in Large Graphs: State of the Art and Future\n  Directions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph comparison is fundamentally important for many applications such as the\nanalysis of social networks and biological data and has been a significant\nresearch area in the pattern recognition and pattern analysis domains.\nNowadays, the graphs are large, they may have billions of nodes and edges.\nComparison issues in such huge graphs are a challenging research problem.\n  In this paper, we survey the research advances of comparison problems in\nlarge graphs. We review graph comparison and pattern matching approaches that\nfocus on large graphs. We categorize the existing approaches into three\nclasses: partition-based approaches, search space based approaches and summary\nbased approaches. All the existing algorithms in these approaches are described\nin detail and analyzed according to multiple metrics such as time complexity,\ntype of graphs or comparison concept. Finally, we identify directions for\nfuture research.\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 14:45:42 GMT"}], "update_date": "2015-02-27", "authors_parsed": [["Seba", "Hamida", ""], ["Lagraa", "Sofiane", ""], ["Ronando", "Elsen", ""]]}]