[{"id": "1608.00134", "submitter": "Georgios Drakopoulos II", "authors": "Georgios Drakopoulos, Vasileios Megalooikonomou", "title": "A Graph Framework for Multimodal Medical Information Processing", "comments": "We need to correct certain errors both in the software description as\n  well as in the algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal medical information processing is currently the epicenter of\nintense interdisciplinary research, as proper data fusion may lead to more\naccurate diagnoses. Moreover, multimodality may disambiguate cases of\nco-morbidity. This paper presents a framework for retrieving, analyzing, and\nstoring medical information as a multilayer graph, an abstract format suitable\nfor data fusion and further processing. At the same time, this paper addresses\nthe need for reliable medical information through co-author graph ranking. A\nuse case pertaining to frailty based on Python and Neo4j serves as an\nillustration of the proposed framework.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jul 2016 15:42:34 GMT"}, {"version": "v2", "created": "Wed, 22 Feb 2017 15:22:14 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Drakopoulos", "Georgios", ""], ["Megalooikonomou", "Vasileios", ""]]}, {"id": "1608.00796", "submitter": "Georgios Drakopoulos II", "authors": "Georgios Drakopoulos, Andreas Kanavos, Christos Makris, Vasileios\n  Megalooikonomou", "title": "The Storage And Analytics Potential Of HBase Over The Cloud: A Survey", "comments": "We need to correct some errors in our initial assumptions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Apache HBase, a mainstay of the emerging Hadoop ecosystem, is a NoSQL\nkey-value and column family hybrid database which, unlike a traditional RDBMS,\nis intentionally designed to scalably host large, semistructured, and\nheterogeneous data. Prime examples of such data are biosignals which are\ncharacterized by large volume, high volatility, and inherent\nmultidimensionality. This paper reviews how biomedical engineering has recently\ntaken advantage of HBase, with an emphasis over cloud, in order to reliably\nhost cardiovascular and respiratory time series. Moreover, the deployment of\noffline biomedical analytics over HBase is explored.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 12:57:52 GMT"}, {"version": "v2", "created": "Thu, 18 Aug 2016 11:04:33 GMT"}, {"version": "v3", "created": "Wed, 14 Sep 2016 16:03:40 GMT"}, {"version": "v4", "created": "Wed, 22 Feb 2017 15:25:27 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Drakopoulos", "Georgios", ""], ["Kanavos", "Andreas", ""], ["Makris", "Christos", ""], ["Megalooikonomou", "Vasileios", ""]]}, {"id": "1608.01013", "submitter": "Ting Xie", "authors": "Gokhan Kul, Duc Luong, Ting Xie, Patrick Coonan, Varun Chandola,\n  Oliver Kennedy, Shambhu Upadhyaya", "title": "Summarizing Large Query Logs in Ettu", "comments": "there are 12 pages, 8 figures, 4 tables and 28 referenced papers in\n  bibliography", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Database access logs are large, unwieldy, and hard for humans to inspect and\nsummarize. In spite of this, they remain the canonical go-to resource for tasks\nranging from performance tuning to security auditing. In this paper, we address\nthe challenge of compactly encoding large sequences of SQL queries for\npresentation to a human user. Our approach is based on the Weisfeiler-Lehman\n(WL) approximate graph isomorphism algorithm, which identifies salient features\nof a graph or in our case of an abstract syntax tree. Our generalization of WL\nallows us to define a distance metric for SQL queries, which in turn permits\nautomated clustering of queries. We also present two techniques for visualizing\nquery clusters, and an algorithm that allows these visualizations to be\nconstructed at interactive speeds. Finally, we evaluate our algorithms in the\ncontext of a motivating example: insider threat detection at a large US bank.\nWe show experimentally on real world query logs that (a) our distance metric\ncaptures a meaningful notion of similarity, and (b) the log summarization\nprocess is scalable and performant.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 21:43:09 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Kul", "Gokhan", ""], ["Luong", "Duc", ""], ["Xie", "Ting", ""], ["Coonan", "Patrick", ""], ["Chandola", "Varun", ""], ["Kennedy", "Oliver", ""], ["Upadhyaya", "Shambhu", ""]]}, {"id": "1608.02611", "submitter": "Zhan Li", "authors": "Zhan Li, Olga Papaemmanouil, Mitch Cherniack", "title": "OptMark: A Toolkit for Benchmarking Query Optimizers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query optimizers have long been considered as among the most complex\ncomponents of a database engine, while the assessment of an optimizer's quality\nremains a challenging task. Indeed, existing performance benchmarks for\ndatabase engines (like TPC benchmarks) produce a performance assessment of the\nquery runtime system rather than its query optimizer. To address this\nchallenge, this paper introduces OptMark, a toolkit for evaluating the quality\nof a query optimizer. OptMark is designed to offer a number of desirable\nproperties. First, it decouples the quality of an optimizer from the quality of\nits underlying execution engine. Second it evaluates independently both the\neffectiveness of an optimizer (i.e., quality of the chosen plans) and its\nefficiency (i.e., optimization time). OptMark includes also a generic\nbenchmarking toolkit that is minimum invasive to the DBMS that wishes to use\nit. Any DBMS can provide a system-specific implementation of a simple API that\nallows OptMark to run and generate benchmark scores for the specific system.\nThis paper discusses the metrics we propose for evaluating an optimizer's\nquality, the benchmark's design and the toolkit's API and functionality. We\nhave implemented OptMark on the open-source MySQL engine as well as two\ncommercial database systems. Using these implementations we are able to assess\nthe quality of the optimizers on these three systems based on the TPC-DS\nbenchmark queries.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 20:19:19 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Li", "Zhan", ""], ["Papaemmanouil", "Olga", ""], ["Cherniack", "Mitch", ""]]}, {"id": "1608.02800", "submitter": "Harsh Thakkar", "authors": "Harsh Thakkar, Mohnish Dubey, Gezim Sejdiu, Axel-Cyrille Ngonga Ngomo,\n  Jeremy Debattista, Christoph Lange, Jens Lehmann, S\\\"oren Auer, Maria-Esther\n  Vidal", "title": "LITMUS: An Open Extensible Framework for Benchmarking RDF Data\n  Management Solutions", "comments": "8 pages, 1 figure, position paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Developments in the context of Open, Big, and Linked Data have led to an\nenormous growth of structured data on the Web. To keep up with the pace of\nefficient consumption and management of the data at this rate, many data\nManagement solutions have been developed for specific tasks and applications.\nWe present LITMUS, a framework for benchmarking data management solutions.\nLITMUS goes beyond classical storage benchmarking frameworks by allowing for\nanalysing the performance of frameworks across query languages. In this\nposition paper we present the conceptual architecture of LITMUS as well as the\nconsiderations that led to this architecture.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 13:40:59 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Thakkar", "Harsh", ""], ["Dubey", "Mohnish", ""], ["Sejdiu", "Gezim", ""], ["Ngomo", "Axel-Cyrille Ngonga", ""], ["Debattista", "Jeremy", ""], ["Lange", "Christoph", ""], ["Lehmann", "Jens", ""], ["Auer", "S\u00f6ren", ""], ["Vidal", "Maria-Esther", ""]]}, {"id": "1608.03344", "submitter": "Chenwei Zhang", "authors": "Chenwei Zhang, Sihong Xie, Yaliang Li, Jing Gao, Wei Fan, Philip S. Yu", "title": "Multi-source Hierarchical Prediction Consolidation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In big data applications such as healthcare data mining, due to privacy\nconcerns, it is necessary to collect predictions from multiple information\nsources for the same instance, with raw features being discarded or withheld\nwhen aggregating multiple predictions. Besides, crowd-sourced labels need to be\naggregated to estimate the ground truth of the data. Because of the imperfect\npredictive models or human crowdsourcing workers, noisy and conflicting\ninformation is ubiquitous and inevitable. Although state-of-the-art aggregation\nmethods have been proposed to handle label spaces with flat structures, as the\nlabel space is becoming more and more complicated, aggregation under a label\nhierarchical structure becomes necessary but has been largely ignored. These\nlabel hierarchies can be quite informative as they are usually created by\ndomain experts to make sense of highly complex label correlations for many\nreal-world cases like protein functionality interactions or disease\nrelationships.\n  We propose a novel multi-source hierarchical prediction consolidation method\nto effectively exploits the complicated hierarchical label structures to\nresolve the noisy and conflicting information that inherently originates from\nmultiple imperfect sources. We formulate the problem as an optimization problem\nwith a closed-form solution. The proposed method captures the smoothness\noverall information sources as well as penalizing any consolidation result that\nviolates the constraints derived from the label hierarchy. The hierarchical\ninstance similarity, as well as the consolidation result, are inferred in a\ntotally unsupervised, iterative fashion. Experimental results on both synthetic\nand real-world datasets show the effectiveness of the proposed method over\nexisting alternatives.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 01:55:04 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Zhang", "Chenwei", ""], ["Xie", "Sihong", ""], ["Li", "Yaliang", ""], ["Gao", "Jing", ""], ["Fan", "Wei", ""], ["Yu", "Philip S.", ""]]}, {"id": "1608.03535", "submitter": "Fernando S\\'aenz-P\\'erez", "authors": "Fernando S\\'aenz-P\\'erez", "title": "Intuitionistic Logic Programming for SQL (Extended Abstract)", "comments": "Pre-proceedings paper presented at the 26th International Symposium\n  on Logic-Based Program Synthesis and Transformation (LOPSTR 2016), Edinburgh,\n  Scotland UK, 6-8 September 2016 (arXiv:1608.02534)", "journal-ref": null, "doi": null, "report-no": "LOPSTR/2016/44", "categories": "cs.PL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intuitionistic logic programming provides the notion of embedded implication\nin rule bodies, which can be used to reason about a current database modified\nby the antecedent. This can be applied to a system that translates SQL to\nDatalog to solve SQL WITH queries, for which relations are locally defined and\ncan therefore be understood as added to the current database. In addition,\nassumptions in SQL queries as either adding or removing data can be modelled in\nthis way as well, which is an interesting feature for decision-support\nscenarios. This work suggests a way to apply intuitionistic logic programming\nto SQL, and provides a pointer to a working system implementing this idea.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 17:01:37 GMT"}, {"version": "v2", "created": "Thu, 22 Sep 2016 17:25:10 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["S\u00e1enz-P\u00e9rez", "Fernando", ""]]}, {"id": "1608.03556", "submitter": "Nikos Bikakis", "authors": "Nikos Bikakis, Chrisa Tsinaraki, Nektarios Gioldasis, Ioannis\n  Stavrakantonakis, Stavros Christodoulakis", "title": "The XML and Semantic Web Worlds: Technologies, Interoperability and\n  Integration. A Survey of the State of the Art", "comments": "This paper appears in \"Semantic Hyper/Multi-media Adaptation: Schemes\n  and Applications\", Springer 2013. arXiv admin note: text overlap with\n  arXiv:1311.0536", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of the emergent Web of Data, a large number of organizations,\ninstitutes and companies (e.g., DBpedia, Geonames, PubMed ACM, IEEE, NASA, BBC)\nadopt the Linked Data practices and publish their data utilizing Semantic Web\n(SW) technologies. On the other hand, the dominant standard for information\nexchange in the Web today is XML. Many international standards (e.g., Dublin\nCore, MPEG-7, METS, TEI, IEEE LOM) have been expressed in XML Schema resulting\nto a large number of XML datasets. The SW and XML worlds and their developed\ninfrastructures are based on different data models, semantics and query\nlanguages. Thus, it is crucial to provide interoperability and integration\nmechanisms to bridge the gap between the SW and XML worlds. In this chapter, we\ngive an overview and a comparison of the technologies and the standards adopted\nby the XML and SW worlds. In addition, we outline the latest efforts from the\nW3C groups, including the latest working drafts and recommendations (e.g., OWL\n2, SPARQL 1.1, XML Schema 1.1). Moreover, we present a survey of the research\napproaches which aim to provide interoperability and integration between the\nXML and SW worlds. Finally, we present the SPARQL2XQuery and XS2OWL Frameworks,\nwhich bridge the gap and create an interoperable environment between the two\nworlds. These Frameworks provide mechanisms for: (a) Query translation (SPARQL\nto XQuery translation); (b) Mapping specification and generation (Ontology to\nXML Schema mapping); and (c) Schema transformation (XML Schema to OWL\ntransformation).\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 18:03:04 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Bikakis", "Nikos", ""], ["Tsinaraki", "Chrisa", ""], ["Gioldasis", "Nektarios", ""], ["Stavrakantonakis", "Ioannis", ""], ["Christodoulakis", "Stavros", ""]]}, {"id": "1608.03780", "submitter": "Thomas Moyer", "authors": "Thomas Moyer and Vijay Gadepally", "title": "High-throughput Ingest of Provenance Records into Accumulo", "comments": "6 pages, 4 figures, IEEE HPEC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whole-system data provenance provides deep insight into the processing of\ndata on a system, including detecting data integrity attacks. The downside to\nsystems that collect whole-system data provenance is the sheer volume of data\nthat is generated under many heavy workloads. In order to make provenance\nmetadata useful, it must be stored somewhere where it can be queried. This\nproblem becomes even more challenging when considering a network of\nprovenance-aware machines all collecting this metadata. In this paper, we\ninvestigate the use of D4M and Accumulo to support high-throughput data ingest\nof whole-system provenance data. We find that we are able to ingest 3,970 graph\ncomponents per second. Centrally storing the provenance metadata allows us to\nbuild systems that can detect and respond to data integrity attacks that are\ncaptured by the provenance system.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 12:56:46 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Moyer", "Thomas", ""], ["Gadepally", "Vijay", ""]]}, {"id": "1608.03889", "submitter": "Hao Wu", "authors": "Hao Wu, Maoyuan Sun, Jilles Vreeken, Nikolaj Tatti, Chris North, Naren\n  Ramakrishnan", "title": "Interactive and Iterative Discovery of Entity Network Subgraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph mining to extract interesting components has been studied in various\nguises, e.g., communities, dense subgraphs, cliques. However, most existing\nworks are based on notions of frequency and connectivity and do not capture\nsubjective interestingness from a user's viewpoint. Furthermore, existing\napproaches to mine graphs are not interactive and cannot incorporate user\nfeedbacks in any natural manner. In this paper, we address these gaps by\nproposing a graph maximum entropy model to discover surprising connected\nsubgraph patterns from entity graphs. This model is embedded in an interactive\nvisualization framework to enable human-in-the-loop, model-guided data\nexploration. Using case studies on real datasets, we demonstrate how\ninteractions between users and the maximum entropy model lead to faster and\nexplainable conclusions.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 19:56:14 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Wu", "Hao", ""], ["Sun", "Maoyuan", ""], ["Vreeken", "Jilles", ""], ["Tatti", "Nikolaj", ""], ["North", "Chris", ""], ["Ramakrishnan", "Naren", ""]]}, {"id": "1608.03960", "submitter": "Martin Kleppmann", "authors": "Martin Kleppmann, Alastair R. Beresford", "title": "A Conflict-Free Replicated JSON Datatype", "comments": null, "journal-ref": null, "doi": "10.1109/TPDS.2017.2697382", "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications model their data in a general-purpose storage format such\nas JSON. This data structure is modified by the application as a result of user\ninput. Such modifications are well understood if performed sequentially on a\nsingle copy of the data, but if the data is replicated and modified\nconcurrently on multiple devices, it is unclear what the semantics should be.\nIn this paper we present an algorithm and formal semantics for a JSON data\nstructure that automatically resolves concurrent modifications such that no\nupdates are lost, and such that all replicas converge towards the same state (a\nconflict-free replicated datatype or CRDT). It supports arbitrarily nested list\nand map types, which can be modified by insertion, deletion and assignment. The\nalgorithm performs all merging client-side and does not depend on ordering\nguarantees from the network, making it suitable for deployment on mobile\ndevices with poor network connectivity, in peer-to-peer networks, and in\nmessaging systems with end-to-end encryption.\n", "versions": [{"version": "v1", "created": "Sat, 13 Aug 2016 09:48:35 GMT"}, {"version": "v2", "created": "Wed, 22 Mar 2017 10:32:25 GMT"}, {"version": "v3", "created": "Tue, 15 Aug 2017 15:00:24 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Kleppmann", "Martin", ""], ["Beresford", "Alastair R.", ""]]}, {"id": "1608.04142", "submitter": "Leopoldo Bertossi", "authors": "Leopoldo Bertossi and Flavio Rizzolo", "title": "Contexts and Data Quality Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quality of data is context dependent. Starting from this intuition and\nexperience, we propose and develop a conceptual framework that captures in\nformal terms the notion of \"context-dependent data quality\". We start by\nproposing a generic and abstract notion of context, and also of its uses, in\ngeneral and in data management in particular. On this basis, we investigate\n\"data quality assessment\" and \"quality query answering\" as context-dependent\nactivities. A context for the assessment of a database D at hand is modeled as\nan external database schema, with possibly materialized or virtual data, and\nconnections to external data sources. The database D is put in context via\nmappings to the contextual schema, which produces a collection C of alternative\nclean versions of D. The quality of D is measured in terms of its distance to\nC. The class C} is also used to define and do \"quality query answering\". The\nproposed model allows for natural extensions, like the use of data quality\npredicates, the optimization of the access by the context to external data\nsources, and also the representation of contexts by means of more expressive\nontologies.\n", "versions": [{"version": "v1", "created": "Sun, 14 Aug 2016 21:43:20 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Bertossi", "Leopoldo", ""], ["Rizzolo", "Flavio", ""]]}, {"id": "1608.04348", "submitter": "Jeff Calder", "authors": "Bilal Abbasi, Jeff Calder, Adam M. Oberman", "title": "Anomaly detection and classification for streaming data using PDEs", "comments": null, "journal-ref": "SIAM Journal on Applied Math, 78(2), 921--941, 2018", "doi": "10.1137/17M1121184", "report-no": null, "categories": "cs.LG cs.CV cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nondominated sorting, also called Pareto Depth Analysis (PDA), is widely used\nin multi-objective optimization and has recently found important applications\nin multi-criteria anomaly detection. Recently, a partial differential equation\n(PDE) continuum limit was discovered for nondominated sorting leading to a very\nfast approximate sorting algorithm called PDE-based ranking. We propose in this\npaper a fast real-time streaming version of the PDA algorithm for anomaly\ndetection that exploits the computational advantages of PDE continuum limits.\nFurthermore, we derive new PDE continuum limits for sorting points within their\nnondominated layers and show how the new PDEs can be used to classify anomalies\nbased on which criterion was more significantly violated. We also prove\nstatistical convergence rates for PDE-based ranking, and present the results of\nnumerical experiments with both synthetic and real data.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 18:03:51 GMT"}, {"version": "v2", "created": "Wed, 15 Mar 2017 19:50:07 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Abbasi", "Bilal", ""], ["Calder", "Jeff", ""], ["Oberman", "Adam M.", ""]]}, {"id": "1608.04437", "submitter": "Mayank Kejriwal", "authors": "Mayank Kejriwal, Daniel P. Miranker", "title": "Self-contained NoSQL Resources for Cross-Domain RDF", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-domain knowledge bases such as DBpedia, Freebase and YAGO have emerged\nas encyclopedic hubs in the Web of Linked Data. Despite enabling several\npractical applications in the Semantic Web, the large-scale, schema-free nature\nof such graphs often precludes research groups from employing them widely as\nevaluation test cases for entity resolution and instance-based ontology\nalignment applications. Although the ground-truth linkages between the three\nknowledge bases above are available, they are not amenable to resource-limited\napplications. One reason is that the ground-truth files are not self-contained,\nmeaning that a researcher must usually perform a series of expensive joins\n(typically in MapReduce) to obtain usable information sets. In this paper, we\nupload several publicly licensed data resources to the public cloud and use\nsimple Hadoop clusters to compile, and make accessible, three cross-domain\nself-contained test cases involving linked instances from DBpedia, Freebase and\nYAGO. Self-containment is enabled by virtue of a simple NoSQL JSON-like\nserialization format. Potential applications for these resources, particularly\nrelated to testing transfer learning research hypotheses, are also briefly\ndescribed.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 23:32:31 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Kejriwal", "Mayank", ""], ["Miranker", "Daniel P.", ""]]}, {"id": "1608.04442", "submitter": "Mayank Kejriwal", "authors": "Mayank Kejriwal, Daniel P. Miranker", "title": "Experience: Type alignment on DBpedia and Freebase", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linked Open Data exhibits growth in both volume and variety of published\ndata. Due to this variety, instances of many different types (e.g. Person) can\nbe found in published datasets. Type alignment is the problem of automatically\nmatching types (in a possibly many-many fashion) between two such datasets.\nType alignment is an important preprocessing step in instance matching.\nInstance matching concerns identifying pairs of instances referring to the same\nunderlying entity. By performing type alignment a priori, only instances\nconforming to aligned types are processed together, leading to significant\nsavings. This article describes a type alignment experience with two\nlarge-scale cross-domain RDF knowledge graphs, DBpedia and Freebase, that\ncontain hundreds, or even thousands, of unique types. Specifically, we present\na MapReduce-based type alignment algorithm and show that there are at least\nthree reasonable ways of evaluating type alignment within the larger context of\ninstance matching. We comment on the consistency of those results, and note\nsome general observations for researchers evaluating similar algorithms on\ncross-domain graphs.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 23:56:08 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Kejriwal", "Mayank", ""], ["Miranker", "Daniel P.", ""]]}, {"id": "1608.04686", "submitter": "Abdur Rafay", "authors": "Abdur Rafay", "title": "Multi Query Optimization in GLADE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SQL-on-Hadoop systems, query optimization, data distribution over multiple\nnodes and parallelization techniques are few of the areas under extreme\nresearch these days. Big names like Amazon, Google, Microsoft and many more are\nworking on implementing systems for faster access of data from multiple nodes\nreducing data mobility and increasing the parallelization. Queries are\nretrieved and reviewed by the database systems in an efficient way in the least\namount of time by the introduction of various parallelization techniques by\nrunning the same query in parallel over different nodes carrying the data.\nApart from multi-threading parallelization, there is another way of\nparallelization that can be performed in order to further reduce retrieval time\nhence improving the efficiency of the system; parallelization on user queries\non top of a DBMS/RDBMS. In this paper, we will study one such technique of how\nmultiple queries can run simultaneously on a system in order to increase the\nefficiency by reducing the data retrieval from the storage. Maximum sharing of\nworkload has been performed by generating optimal and ubiquitous join plans for\na set of queries and then fed them to GLADE (Generalized Linear Aggregate\nDistribution Engine), a scalable distributed system for large scale data\nanalytics. Our main work is centered on generating GLADE join plans for a\nMulti-Query satisfying maximum number of queries in order to maximize data\nsharing and minimize data retrieval for each individual query.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 17:49:43 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Rafay", "Abdur", ""]]}, {"id": "1608.05564", "submitter": "Kai Herrmann", "authors": "Kai Herrmann, Hannes Voigt, Andreas Behrend, Jonas Rausch, Wolfgang\n  Lehner", "title": "Living in Parallel Realities -- Co-Existing Schema Versions with a\n  Bidirectional Database Evolution Language", "comments": null, "journal-ref": null, "doi": "10.1145/3035918.3064046", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce end-to-end support of co-existing schema versions within one\ndatabase. While it is state of the art to run multiple versions of a\ncontinuously developed application concurrently, it is hard to do the same for\ndatabases. In order to keep multiple co-existing schema versions alive; which\nare all accessing the same data set; developers usually employ handwritten\ndelta code (e.g. views and triggers in SQL). This delta code is hard to write\nand hard to maintain: if a database administrator decides to adapt the physical\ntable schema, all handwritten delta code needs to be adapted as well, which is\nexpensive and error-prone in practice. In this paper, we present InVerDa:\ndevelopers use the simple bidirectional database evolution language BiDEL,\nwhich carries enough information to generate all delta code automatically.\nWithout additional effort, new schema versions become immediately accessible\nand data changes in any version are visible in all schema versions at the same\ntime. InVerDa also allows for easily changing the physical table design without\naffecting the availability of co-existing schema versions. This greatly\nincreases robustness (orders of magnitude less lines of code) and allows for\nsignificant performance optimization. A main contribution is the formal\nevaluation that each schema version acts like a common full-fledged database\nschema independently of the chosen physical table design.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 10:44:42 GMT"}, {"version": "v2", "created": "Tue, 19 Sep 2017 12:06:13 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Herrmann", "Kai", ""], ["Voigt", "Hannes", ""], ["Behrend", "Andreas", ""], ["Rausch", "Jonas", ""], ["Lehner", "Wolfgang", ""]]}, {"id": "1608.05594", "submitter": "Giacomo Bergami", "authors": "Giacomo Bergami, Matteo Magnani, Danilo Montesi", "title": "On Joining Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In the graph database literature the term \"join\" does not refer to an\noperator used to merge two graphs. In particular, a counterpart of the\nrelational join is not present in existing graph query languages, and\nconsequently no efficient algorithms have been developed for this operator.\n  This paper provides two main contributions. First, we define a binary graph\njoin operator that acts on the vertices as a standard relational join and\ncombines the edges according to a user-defined semantics. Then we propose the\n\"CoGrouped Graph Conjunctive $\\theta$-Join\" algorithm running over data indexed\nin secondary memory. Our implementation outperforms the execution of the same\noperation in Cypher and SPARQL on major existing graph database management\nsystems by at least one order of magnitude, also including indexing and loading\ntime.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 13:09:04 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Bergami", "Giacomo", ""], ["Magnani", "Matteo", ""], ["Montesi", "Danilo", ""]]}, {"id": "1608.05678", "submitter": "Kayhan Dursun", "authors": "Kayhan Dursun, Carsten Binnig, Ugur Cetintemel, Tim Kraska", "title": "Revisiting Reuse in Main Memory Database Systems", "comments": "13 Pages, 11 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reusing intermediates in databases to speed-up analytical query processing\nhas been studied in the past. Existing solutions typically require intermediate\nresults of individual operators to be materialized into temporary tables to be\nconsidered for reuse in subsequent queries. However, these approaches are\nfundamentally ill-suited for use in modern main memory databases. The reason is\nthat modern main memory DBMSs are typically limited by the bandwidth of the\nmemory bus, thus query execution is heavily optimized to keep tuples in the CPU\ncaches and registers. To that end, adding additional materialization operations\ninto a query plan not only add additional traffic to the memory bus but more\nimportantly prevent the important cache- and register-locality opportunities\nresulting in high performance penalties.\n  In this paper we study a novel reuse model for intermediates, which caches\ninternal physical data structures materialized during query processing (due to\npipeline breakers) and externalizes them so that they become reusable for\nupcoming operations. We focus on hash tables, the most commonly used internal\ndata structure in main memory databases to perform join and aggregation\noperations. As queries arrive, our reuse-aware optimizer reasons about the\nreuse opportunities for hash tables, employing cost models that take into\naccount hash table statistics together with the CPU and data movement costs\nwithin the cache hierarchy. Experimental results, based on our HashStash\nprototype demonstrate performance gains of $2\\times$ for typical analytical\nworkloads with no additional overhead for materializing intermediates.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 17:36:12 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Dursun", "Kayhan", ""], ["Binnig", "Carsten", ""], ["Cetintemel", "Ugur", ""], ["Kraska", "Tim", ""]]}, {"id": "1608.06054", "submitter": "Qin Liu", "authors": "Qin Liu, Zhenguo Li, John C.S. Lui, Jiefeng Cheng", "title": "PowerWalk: Scalable Personalized PageRank via Random Walks with\n  Vertex-Centric Decomposition", "comments": "technical report of our full paper in CIKM 2016", "journal-ref": null, "doi": "10.1145/2983323.2983713", "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most methods for Personalized PageRank (PPR) precompute and store all\naccurate PPR vectors, and at query time, return the ones of interest directly.\nHowever, the storage and computation of all accurate PPR vectors can be\nprohibitive for large graphs, especially in caching them in memory for\nreal-time online querying. In this paper, we propose a distributed framework\nthat strikes a better balance between offline indexing and online querying. The\noffline indexing attains a fingerprint of the PPR vector of each vertex by\nperforming billions of \"short\" random walks in parallel across a cluster of\nmachines. We prove that our indexing method has an exponential convergence,\nachieving the same precision with previous methods using a much smaller number\nof random walks. At query time, the new PPR vector is composed by a linear\ncombination of related fingerprints, in a highly efficient vertex-centric\ndecomposition manner. Interestingly, the resulting PPR vector is much more\naccurate than its offline counterpart because it actually uses more random\nwalks in its estimation. More importantly, we show that such decomposition for\na batch of queries can be very efficiently processed using a shared\ndecomposition. Our implementation, PowerWalk, takes advantage of advanced\ndistributed graph engines and it outperforms the state-of-the-art algorithms by\norders of magnitude. Particularly, it responses to tens of thousands of queries\non graphs with billions of edges in just a few seconds.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 05:31:58 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Liu", "Qin", ""], ["Li", "Zhenguo", ""], ["Lui", "John C. S.", ""], ["Cheng", "Jiefeng", ""]]}, {"id": "1608.06169", "submitter": "Jaroslaw Szlichta", "authors": "Jaroslaw Szlichta, Parke Godfrey, Lukasz Golab, Mehdi Kargar, Divesh\n  Srivastava", "title": "Effective and Complete Discovery of Order Dependencies via Set-based\n  Axiomatization", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrity constraints (ICs) provide a valuable tool for expressing and\nenforcing application semantics. However, formulating constraints manually\nrequires domain expertise, is prone to human errors, and may be excessively\ntime consuming, especially on large datasets. Hence, proposals for automatic\ndiscovery have been made for some classes of ICs, such as functional\ndependencies (FDs), and recently, order dependencies (ODs). ODs properly\nsubsume FDs, as they can additionally express business rules involving order;\ne.g., an employee never has a higher salary while paying lower taxes compared\nwith another employee.\n  We address the limitations of prior work on OD discovery which has factorial\ncomplexity in the number of attributes, is incomplete (i.e., it does not\ndiscover valid ODs that cannot be inferred from the ones found) and is not\nconcise (i.e., it can result in \"redundant\" discovery and overly large\ndiscovery sets). We improve significantly on complexity, offer completeness,\nand define a compact canonical form. This is based on a novel polynomial\nmapping to a canonical form for ODs, and a sound and complete set of axioms\n(inference rules) for canonical ODs. This allows us to develop an efficient\nset-containment, lattice-driven OD discovery algorithm that uses the inference\nrules to prune the search space. Our algorithm has exponential worst-case time\ncomplexity in the number of attributes and linear complexity in the number of\ntuples. We prove that it produces a complete, minimal set of ODs (i.e., minimal\nwith regards to the canonical representation). Finally, using real and\nsynthetic datasets, we experimentally show orders-of-magnitude performance\nimprovements over the current state-of-the-art algorithm and demonstrate\neffectiveness of our techniques.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 14:03:46 GMT"}, {"version": "v2", "created": "Tue, 23 Aug 2016 19:54:06 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Szlichta", "Jaroslaw", ""], ["Godfrey", "Parke", ""], ["Golab", "Lukasz", ""], ["Kargar", "Mehdi", ""], ["Srivastava", "Divesh", ""]]}, {"id": "1608.06469", "submitter": "Jerome Darmont", "authors": "Ayb\\\"uk\\\"e Ozt\\\"urk (ERIC,ARAR), Louis Eyango (ARAR), Sylvie Yona\n  Waksman (ARAR), St\\'ephane Lallich (ERIC), J\\'er\\^ome Darmont (ERIC)", "title": "Warehousing Complex Archaeological Objects", "comments": "9th International and Interdisciplinary Conference on Modeling and\n  Using Context (CONTEXT 2015), Nov 2015, Larnaca, Cyprus. Springer,\n  Proceedings of the 9th International and Interdisciplinary Conference on\n  Modeling and Using Context (CONTEXT 2015), 9405, pp.226-239, 2015, Lecture\n  Notes in Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data organization is a difficult and essential component in cultural heritage\napplications. Over the years, a great amount of archaeological ceramic data\nhave been created and processed by various methods and devices. Such ceramic\ndata are stored in databases that concur to increase the amount of available\ninformation rapidly. However , such databases typically focus on one type of\nceramic descriptors, e.g., qualitative textual descriptions, petrographic or\nchemical analysis results, and do not interoperate. Thus, research involving\narchaeological ceramics cannot easily take advantage of combining all these\ntypes of information. In this application paper, we introduce an evolution of\nthe Ceramom database that includes text descriptors of archaeological features,\nchemical analysis results, and various images, including petrographic and\nfabric images. To illustrate what new analyses are permitted by such a\ndatabase, we source it to a data warehouse and present a sample on-line\nanalysis processing (OLAP) scenario to gain deep understanding of ceramic\ncontext.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 11:30:48 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Ozt\u00fcrk", "Ayb\u00fck\u00eb", "", "ERIC,ARAR"], ["Eyango", "Louis", "", "ARAR"], ["Waksman", "Sylvie Yona", "", "ARAR"], ["Lallich", "St\u00e9phane", "", "ERIC"], ["Darmont", "J\u00e9r\u00f4me", "", "ERIC"]]}, {"id": "1608.06617", "submitter": "Stacey Jeffery", "authors": "Stacey Jeffery and Fran\\c{c}ois Le Gall", "title": "Quantum Communication Complexity of Distributed Set Joins", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing set joins of two inputs is a common task in database theory.\nRecently, Van Gucht, Williams, Woodruff and Zhang [PODS 2015] considered the\ncomplexity of such problems in the natural model of (classical) two-party\ncommunication complexity and obtained tight bounds for the complexity of\nseveral important distributed set joins.\n  In this paper we initiate the study of the *quantum* communication complexity\nof distributed set joins. We design a quantum protocol for distributed Boolean\nmatrix multiplication, which corresponds to computing the composition join of\ntwo databases, showing that the product of two $n\\times n$ Boolean matrices,\neach owned by one of two respective parties, can be computed with\n$\\widetilde{O}(\\sqrt{n}\\ell^{3/4})$ qubits of communication, where $\\ell$\ndenotes the number of non-zero entries of the product. Since Van Gucht et al.\nshowed that the classical communication complexity of this problem is\n$\\widetilde{\\Theta}(n\\sqrt{\\ell})$, our quantum algorithm outperforms classical\nprotocols whenever the output matrix is sparse. We also show a quantum lower\nbound and a matching classical upper bound on the communication complexity of\ndistributed matrix multiplication over $\\mathbb{F}_2$.\n  Besides their applications to database theory, the communication complexity\nof set joins is interesting due to its connections to direct product theorems\nin communication complexity. In this work we also introduce a notion of\n*all-pairs* product theorem, and relate this notion to standard direct product\ntheorems in communication complexity.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 19:45:38 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Jeffery", "Stacey", ""], ["Gall", "Fran\u00e7ois Le", ""]]}, {"id": "1608.07485", "submitter": "Jason Lowe-Power", "authors": "Jason Lowe-Power and Mark D. Hill and David A. Wood", "title": "When to use 3D Die-Stacked Memory for Bandwidth-Constrained Big Data\n  Workloads", "comments": "Originally presented The Seventh workshop on Big Data Benchmarks,\n  Performance Optimization, and Emerging Hardware (BPOE-7).\n  http://www.bafst.com/events/asplos16/bpoe7/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Response time requirements for big data processing systems are shrinking. To\nmeet this strict response time requirement, many big data systems store all or\nmost of their data in main memory to reduce the access latency. Main memory\ncapacities have grown, and systems with 2 TB of main memory capacity available\ntoday. However, the rate at which processors can access this data--the memory\nbandwidth--has not grown at the same rate. In fact, some of these big-memory\nsystems can access less than 10% of their main memory capacity in one second\n(billions of processor cycles).\n  3D die-stacking is one promising solution to this bandwidth problem, and\nindustry is investing significantly in 3D die-stacking. We use a simple\nback-of-the-envelope-style model to characterize if and when the 3D die-stacked\narchitecture is more cost-effective than current architectures for in-memory\nbig data workloads. We find that die-stacking has much higher performance than\ncurrent systems (up to 256x lower response times), and it does not require\nexpensive memory over provisioning to meet real-time (10 ms) response time\nservice-level agreements. However, the power requirements of the die-stacked\nsystems are significantly higher (up to 50x) than current systems, and its\nmemory capacity is lower in many cases. Even in this limited case study, we\nfind 3D die-stacking is not a panacea. Today, die-stacking is the most\ncost-effective solution for strict SLAs and by reducing the power of the\ncompute chip and increasing memory densities die-stacking can be cost-effective\nunder other constraints in the future.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 15:04:20 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Lowe-Power", "Jason", ""], ["Hill", "Mark D.", ""], ["Wood", "David A.", ""]]}, {"id": "1608.07981", "submitter": "Timo Schindler", "authors": "Timo Schindler, Christoph Skornia", "title": "Secure Parallel Processing of Big Data Using Order-Preserving Encryption\n  on Google BigQuery", "comments": "6 pages, 2 figures, 1 table, Paper for conference: GI Informatik\n  2016, Klagenfurt, Heinrich C. Mayr, Martin Pinzger (Hrsg.): INFORMATIK 2016.\n  Lecture Notes in Informatics (LNI), Volume P-259, GI Bonn 2016. date of\n  publication: 26.09.2016, ISBN 978-3-88579-653-4, ISSN 1617-5468", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  With the increase of centralization of resources in IT-infrastructure and the\ngrowing amount of cloud services, database management systems (DBMS) will be\nmore and more outsourced to Infrastructure-as-a-Service (IaaS) providers. The\noutsourcing of entire databases, or the computation power for processing Big\nData to an external provider also means that the provider has full access to\nthe information contained in the database. In this article we propose a\nfeasible solution with Order-Preserving Encryption (OPE) and further, state of\nthe art, encryption methods to sort and process Big Data on external resources\nwithout exposing the unencrypted data to the IaaS provider. We also introduce a\nproof-of-concept client for Google BigQuery as example IaaS Provider.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 10:18:01 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Schindler", "Timo", ""], ["Skornia", "Christoph", ""]]}, {"id": "1608.08130", "submitter": "Olaf Hartig", "authors": "Magnus Knuth and Olaf Hartig and Harald Sack", "title": "Scheduling Refresh Queries for Keeping Results from a SPARQL Endpoint\n  Up-to-Date (Extended Version)", "comments": "This document is an extended version of a paper published in ODBASE\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many datasets change over time. As a consequence, long-running applications\nthat cache and repeatedly use query results obtained from a SPARQL endpoint may\nresubmit the queries regularly to ensure up-to-dateness of the results. While\nthis approach may be feasible if the number of such regular refresh queries is\nmanageable, with an increasing number of applications adopting this approach,\nthe SPARQL endpoint may become overloaded with such refresh queries. A more\nscalable approach would be to use a middle-ware component at which the\napplications register their queries and get notified with updated query results\nonce the results have changed. Then, this middle-ware can schedule the repeated\nexecution of the refresh queries without overloading the endpoint. In this\npaper, we study the problem of scheduling refresh queries for a large number of\nregistered queries by assuming an overload-avoiding upper bound on the length\nof a regular time slot available for testing refresh queries. We investigate a\nvariety of scheduling strategies and compare them experimentally in terms of\ntime slots needed before they recognize changes and number of changes that they\nmiss.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 16:16:36 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Knuth", "Magnus", ""], ["Hartig", "Olaf", ""], ["Sack", "Harald", ""]]}, {"id": "1608.08148", "submitter": "Olaf Hartig", "authors": "Olaf Hartig and Carlos Buil-Aranda", "title": "brTPF: Bindings-Restricted Triple Pattern Fragments (Extended Preprint)", "comments": "This document is an extended preprint of a paper published in the\n  proceedings of the ODBASE 2016 conference. In contrast to the proceedings\n  version, this document contains Appendixes A and B which present additional\n  experimental results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Triple Pattern Fragment (TPF) interface is a recent proposal for reducing\nserver load in Web-based approaches to execute SPARQL queries over public RDF\ndatasets. The price for less overloaded servers is a higher client-side load\nand a substantial increase in network load (in terms of both the number of HTTP\nrequests and data transfer). In this paper, we propose a slightly extended\ninterface that allows clients to attach intermediate results to triple pattern\nrequests. The response to such a request is expected to contain triples from\nthe underlying dataset that do not only match the given triple pattern (as in\nthe case of TPF), but that are guaranteed to contribute in a join with the\ngiven intermediate result. Our hypothesis is that a distributed query execution\nusing this extended interface can reduce the network load (in comparison to a\npure TPF-based query execution) without reducing the overall throughput of the\nclient-server system significantly. Our main contribution in this paper is\ntwofold: we empirically verify the hypothesis and provide an extensive\nexperimental comparison of our proposal and TPF.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 17:09:53 GMT"}, {"version": "v2", "created": "Tue, 30 Aug 2016 15:49:29 GMT"}], "update_date": "2016-08-31", "authors_parsed": [["Hartig", "Olaf", ""], ["Buil-Aranda", "Carlos", ""]]}, {"id": "1608.08252", "submitter": "Fabrizio Maria Maggi", "authors": "Hoang Nguyen, Marlon Dumas, Marcello La Rosa, Fabrizio Maria Maggi,\n  Suriadi Suriadi", "title": "Business Process Deviance Mining: Review and Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Business process deviance refers to the phenomenon whereby a subset of the\nexecutions of a business process deviate, in a negative or positive way, with\nrespect to its expected or desirable outcomes. Deviant executions of a business\nprocess include those that violate compliance rules, or executions that\nundershoot or exceed performance targets. Deviance mining is concerned with\nuncovering the reasons for deviant executions by analyzing business process\nevent logs. This article provides a systematic review and comparative\nevaluation of deviance mining approaches based on a family of data mining\ntechniques known as sequence classification. Using real-life logs from multiple\ndomains, we evaluate a range of feature types and classification methods in\nterms of their ability to accurately discriminate between normal and deviant\nexecutions of a process. We also analyze the interestingness of the rule sets\nextracted using different methods. We observe that feature sets extracted using\npattern mining techniques only slightly outperform simpler feature sets based\non counts of individual activity occurrences in a trace.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 21:14:01 GMT"}], "update_date": "2016-08-31", "authors_parsed": [["Nguyen", "Hoang", ""], ["Dumas", "Marlon", ""], ["La Rosa", "Marcello", ""], ["Maggi", "Fabrizio Maria", ""], ["Suriadi", "Suriadi", ""]]}, {"id": "1608.08258", "submitter": "Boris Glavic", "authors": "Bahareh Sadat Arab, Dieter Gawlick, Vasudha Krishnaswamy, Venkatesh\n  Radhakrishnan, Boris Glavic", "title": "Reenactment for Read-Committed Snapshot Isolation", "comments": "long versions of CIKM paper", "journal-ref": null, "doi": "10.1145/2983323.2983825", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Provenance for transactional updates is critical for many applications such\nas auditing and debugging of transactions. Recently, we have introduced\nMV-semirings, an extension of the semiring provenance model that supports\nupdates and transactions. Furthermore, we have proposed reenactment, a\ndeclarative form of replay with provenance capture, as an efficient and\nnon-invasive method for computing this type of provenance. However, this\napproach is limited to the snapshot isolation (SI) concurrency control protocol\nwhile many real world applications apply the read committed version of snapshot\nisolation (RC-SI) to improve performance at the cost of consistency. We present\nnon-trivial extensions of the model and reenactment approach to be able to\ncompute provenance of RC-SI transactions efficiently. In addition, we develop\ntechniques for applying reenactment across multiple RC-SI transactions. Our\nexperiments demonstrate that our implementation in the GProM system supports\nefficient re-construction and querying of provenance.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 21:37:43 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Arab", "Bahareh Sadat", ""], ["Gawlick", "Dieter", ""], ["Krishnaswamy", "Vasudha", ""], ["Radhakrishnan", "Venkatesh", ""], ["Glavic", "Boris", ""]]}]