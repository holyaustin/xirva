[{"id": "0806.0075", "submitter": "Sherif Sakr", "authors": "Sherif Sakr", "title": "An Experimental Investigation of XML Compression Tools", "comments": "http://xmlcompbench.sourceforge.net/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an extensive experimental study of the state-of-the-art\nof XML compression tools. The study reports the behavior of nine XML\ncompressors using a large corpus of XML documents which covers the different\nnatures and scales of XML documents. In addition to assessing and comparing the\nperformance characteristics of the evaluated XML compression tools, the study\ntries to assess the effectiveness and practicality of using these tools in the\nreal world. Finally, we provide some guidelines and recommen- dations which are\nuseful for helping developers and users for making an effective decision for\nselecting the most suitable XML compression tool for their needs.\n", "versions": [{"version": "v1", "created": "Sat, 31 May 2008 14:49:00 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Sakr", "Sherif", ""]]}, {"id": "0806.1071", "submitter": "Graham Cormode", "authors": "Graham Cormode and Minos Garofalakis", "title": "Histograms and Wavelets on Probabilistic Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing realization that uncertain information is a first-class\ncitizen in modern database management. As such, we need techniques to correctly\nand efficiently process uncertain data in database systems. In particular, data\nreduction techniques that can produce concise, accurate synopses of large\nprobabilistic relations are crucial. Similar to their deterministic relation\ncounterparts, such compact probabilistic data synopses can form the foundation\nfor human understanding and interactive data exploration, probabilistic query\nplanning and optimization, and fast approximate query processing in\nprobabilistic database systems.\n  In this paper, we introduce definitions and algorithms for building\nhistogram- and wavelet-based synopses on probabilistic data. The core problem\nis to choose a set of histogram bucket boundaries or wavelet coefficients to\noptimize the accuracy of the approximate representation of a collection of\nprobabilistic tuples under a given error metric. For a variety of different\nerror metrics, we devise efficient algorithms that construct optimal or near\noptimal B-term histogram and wavelet synopses. This requires careful analysis\nof the structure of the probability distributions, and novel extensions of\nknown dynamic-programming-based techniques for the deterministic domain. Our\nexperiments show that this approach clearly outperforms simple ideas, such as\nbuilding summaries for samples drawn from the data distribution, while taking\nequal or less time.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jun 2008 23:19:56 GMT"}], "update_date": "2008-06-09", "authors_parsed": [["Cormode", "Graham", ""], ["Garofalakis", "Minos", ""]]}, {"id": "0806.1816", "submitter": "Michael Mrissa", "authors": "M. Mrissa, Ph. Thiran, J-M. Jacquet, D. Benslimane and Z. Maamar", "title": "Cardinality heterogeneities in Web service composition: Issues and\n  solutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data exchanges between Web services engaged in a composition raise several\nheterogeneities. In this paper, we address the problem of data cardinality\nheterogeneity in a composition. Firstly, we build a theoretical framework to\ndescribe different aspects of Web services that relate to data cardinality, and\nsecondly, we solve this problem by developing a solution for cardinality\nmediation based on constraint logic programming.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jun 2008 09:05:21 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Mrissa", "M.", ""], ["Thiran", "Ph.", ""], ["Jacquet", "J-M.", ""], ["Benslimane", "D.", ""], ["Maamar", "Z.", ""]]}, {"id": "0806.3115", "submitter": "Daniel Hazel", "authors": "Dan Hazel (Technology One)", "title": "Using rational numbers to key nested sets", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": "DocSetID-311997", "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report details the generation and use of tree node ordering keys in a\nsingle relational database table. The keys for each node are calculated from\nthe keys of its parent, in such a way that the sort order places every node in\nthe tree before all of its descendants and after all siblings having a lower\nindex. The calculation from parent keys to child keys is simple, and reversible\nin the sense that the keys of every ancestor of a node can be calculated from\nthat node's keys without having to consult the database.\n  Proofs of the above properties of the key encoding process and of its\ncorrespondence to a finite continued fraction form are provided.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jun 2008 02:06:14 GMT"}], "update_date": "2008-06-20", "authors_parsed": [["Hazel", "Dan", "", "Technology One"]]}, {"id": "0806.3710", "submitter": "Stevan Harnad", "authors": "A. Blondin Masse, G. Chicoisne, Y. Gargouri, S. Harnad, O. Picard, O.\n  Marcotte", "title": "How Is Meaning Grounded in Dictionary Definitions?", "comments": "8 pages, 3 figures, TextGraphs-3 Workshop at the 22nd International\n  Conference on Computational Linguistics, Coling 2008, Manchester, 18-22\n  August, 2008", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meaning cannot be based on dictionary definitions all the way down: at some\npoint the circularity of definitions must be broken in some way, by grounding\nthe meanings of certain words in sensorimotor categories learned from\nexperience or shaped by evolution. This is the \"symbol grounding problem.\" We\nintroduce the concept of a reachable set -- a larger vocabulary whose meanings\ncan be learned from a smaller vocabulary through definition alone, as long as\nthe meanings of the smaller vocabulary are themselves already grounded. We\nprovide simple algorithms to compute reachable sets for any given dictionary.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jun 2008 15:53:05 GMT"}, {"version": "v2", "created": "Tue, 15 Jul 2008 01:59:09 GMT"}], "update_date": "2008-07-15", "authors_parsed": [["Masse", "A. Blondin", ""], ["Chicoisne", "G.", ""], ["Gargouri", "Y.", ""], ["Harnad", "S.", ""], ["Picard", "O.", ""], ["Marcotte", "O.", ""]]}, {"id": "0806.4627", "submitter": "Thomas Hornung", "authors": "Michael Schmidt, Thomas Hornung, Georg Lausen, Christoph Pinkel", "title": "SP2Bench: A SPARQL Performance Benchmark", "comments": "Conference paper to appear in Proc. ICDE'09", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the SPARQL query language for RDF has reached the W3C\nrecommendation status. In response to this emerging standard, the database\ncommunity is currently exploring efficient storage techniques for RDF data and\nevaluation strategies for SPARQL queries. A meaningful analysis and comparison\nof these approaches necessitates a comprehensive and universal benchmark\nplatform. To this end, we have developed SP^2Bench, a publicly available,\nlanguage-specific SPARQL performance benchmark. SP^2Bench is settled in the\nDBLP scenario and comprises both a data generator for creating arbitrarily\nlarge DBLP-like documents and a set of carefully designed benchmark queries.\nThe generated documents mirror key characteristics and social-world\ndistributions encountered in the original DBLP data set, while the queries\nimplement meaningful requests on top of this data, covering a variety of SPARQL\noperator constellations and RDF access patterns. As a proof of concept, we\napply SP^2Bench to existing engines and discuss their strengths and weaknesses\nthat follow immediately from the benchmark results.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2008 15:31:26 GMT"}, {"version": "v2", "created": "Tue, 21 Oct 2008 14:44:17 GMT"}], "update_date": "2008-10-21", "authors_parsed": [["Schmidt", "Michael", ""], ["Hornung", "Thomas", ""], ["Lausen", "Georg", ""], ["Pinkel", "Christoph", ""]]}, {"id": "0806.4703", "submitter": "Feng Li", "authors": "Feng Li and Shuigeng Zhou", "title": "Challenging More Updates: Towards Anonymous Re-publication of Fully\n  Dynamic Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing anonymization work has been done on static datasets, which have\nno update and need only one-time publication. Recent studies consider\nanonymizing dynamic datasets with external updates: the datasets are updated\nwith record insertions and/or deletions. This paper addresses a new problem:\nanonymous re-publication of datasets with internal updates, where the attribute\nvalues of each record are dynamically updated. This is an important and\nchallenging problem for attribute values of records are updating frequently in\npractice and existing methods are unable to deal with such a situation.\n  We initiate a formal study of anonymous re-publication of dynamic datasets\nwith internal updates, and show the invalidation of existing methods. We\nintroduce theoretical definition and analysis of dynamic datasets, and present\na general privacy disclosure framework that is applicable to all anonymous\nre-publication problems. We propose a new counterfeited generalization\nprinciple alled m-Distinct to effectively anonymize datasets with both external\nupdates and internal updates. We also develop an algorithm to generalize\ndatasets to meet m-Distinct. The experiments conducted on real-world data\ndemonstrate the effectiveness of the proposed solution.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jun 2008 16:24:03 GMT"}, {"version": "v2", "created": "Thu, 24 Jul 2008 08:24:57 GMT"}], "update_date": "2008-07-24", "authors_parsed": [["Li", "Feng", ""], ["Zhou", "Shuigeng", ""]]}, {"id": "0806.4749", "submitter": "Alexandr Savinov", "authors": "Alexandr Savinov", "title": "Nested Ordered Sets and their Use for Data Modelling", "comments": "15 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new approach to data modelling, called the\nconcept-oriented model (CoM), and describe its main features and\ncharacteristics including data semantics and operations. The distinguishing\nfeature of this model is that it is based on the formalism of nested ordered\nsets where any element participates in two structures simultaneously:\nhierarchical (nested) and multi-dimensional (ordered). An element of the model\nis postulated to consist of two parts, called identity and entity, and the\nwhole approach can be naturally broken into two branches: identity modelling\nand entity modelling. We also propose a new query language with the main\nconstruct, called concept, defined as a pair of two classes: identity class and\nentity class. We describe how its operations of projection, de-projection and\nproduct can be used to solve typical data modelling tasks.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jun 2008 11:38:06 GMT"}], "update_date": "2008-07-01", "authors_parsed": [["Savinov", "Alexandr", ""]]}, {"id": "0806.4787", "submitter": "Herman Haverkort", "authors": "Herman Haverkort and Freek van Walderveen", "title": "Locality and Bounding-Box Quality of Two-Dimensional Space-Filling\n  Curves", "comments": "24 pages, full version of paper to appear in ESA. Difference with\n  first version: minor editing; Fig. 2(m) corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Space-filling curves can be used to organise points in the plane into\nbounding-box hierarchies (such as R-trees). We develop measures of the\nbounding-box quality of space-filling curves that express how effective\ndifferent space-filling curves are for this purpose. We give general lower\nbounds on the bounding-box quality measures and on locality according to\nGotsman and Lindenbaum for a large class of space-filling curves. We describe a\ngeneric algorithm to approximate these and similar quality measures for any\ngiven curve. Using our algorithm we find good approximations of the locality\nand the bounding-box quality of several known and new space-filling curves.\nSurprisingly, some curves with relatively bad locality by Gotsman and\nLindenbaum's measure, have good bounding-box quality, while the curve with the\nbest-known locality has relatively bad bounding-box quality.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jun 2008 21:47:15 GMT"}, {"version": "v2", "created": "Sat, 12 Jul 2008 12:02:51 GMT"}], "update_date": "2008-07-12", "authors_parsed": [["Haverkort", "Herman", ""], ["van Walderveen", "Freek", ""]]}, {"id": "0806.4920", "submitter": "Tuyet-Tram Dang-Ngoc", "authors": "Tuyet-Tram Dang-Ngoc (PRISM), Georges Gardarin (PRISM)", "title": "Conception et Evaluation de XQuery dans une architecture de m\\'ediation\n  \"Tout-XML\"", "comments": null, "journal-ref": "Revue ISI (Integration de syst\\`emes d'information) : Num\\'ero\n  sp\\'ecial sur les Bases de Donn\\'ees Semi-structur\\'ees 8, 5-6 (2003) 11-25", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  XML has emerged as the leading language for representing and exchanging data\nnot only on the Web, but also in general in the enterprise. XQuery is emerging\nas the standard query language for XML. Thus, tools are required to mediate\nbetween XML queries and heterogeneous data sources to integrate data in XML.\nThis paper presents the XMedia mediator, a unique tool for integrating and\nquerying disparate heterogeneous information as unified XML views. It describes\nthe mediator architecture and focuses on the unique distributed query\nprocessing technology implemented in this component. Query evaluation is based\non an original XML algebra simply extending classical operators to process\ntuples of tree elements. Further, we present a set of performance evaluation on\na relational benchmark, which leads to discuss possible performance\nenhancements.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2008 15:23:20 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Dang-Ngoc", "Tuyet-Tram", "", "PRISM"], ["Gardarin", "Georges", "", "PRISM"]]}]