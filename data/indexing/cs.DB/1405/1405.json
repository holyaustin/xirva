[{"id": "1405.0253", "submitter": "Federica Panella", "authors": "Federica Panella", "title": "Approximate Query Answering in Inconsistent Databases", "comments": "54 pages. Added abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical algorithms for query optimization presuppose the absence of\ninconsistencies or uncertainties in the database and exploit only valid\nsemantic knowledge provided, e.g., by integrity constraints. Data inconsistency\nor uncertainty, however, is a widespread critical issue in ordinary databases:\ntotal integrity is often, in fact, an unrealistic assumption and violations to\nintegrity constraints may be introduced in several ways.\n  In this report we present an approach for semantic query optimization that,\ndifferently from the traditional ones, relies on not necessarily valid semantic\nknowledge, e.g., provided by violated or soft integrity constraints, or induced\nby applying data mining techniques. Query optimization that leverages invalid\nsemantic knowledge cannot guarantee the semantic equivalence between the\noriginal user's query and its rewriting: thus a query optimized by our approach\nyields approximate answers that can be provided to the users whenever fast but\npossibly partial responses are required. Also, we evaluate the impact of use of\ninvalid semantic knowledge in the rewriting of a query by computing a measure\nof the quality of the answer returned to the user, and we rely on the recent\ntheory of Belief Logic Programming to deal with the presence of possible\ncorrelation in the semantic knowledge used in the rewriting.\n", "versions": [{"version": "v1", "created": "Thu, 1 May 2014 19:09:39 GMT"}, {"version": "v2", "created": "Fri, 2 May 2014 10:02:31 GMT"}], "update_date": "2014-05-05", "authors_parsed": [["Panella", "Federica", ""]]}, {"id": "1405.1339", "submitter": "Wilhelmiina H\\\"am\\\"al\\\"ainen", "authors": "Wilhelmiina H\\\"am\\\"al\\\"ainen", "title": "General upper bounds for well-behaving goodness measures on dependency\n  rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the search for statistical dependency rules, a crucial task is to restrict\nthe search space by estimating upper bounds for the goodness of yet\nundiscovered rules. In this paper, we show that all well-behaving goodness\nmeasures achieve their maximal values in the same points. Therefore, the same\ngeneric search strategy can be applied with any of these measures. The notion\nof well-behaving measures is based on the classical axioms for any proper\ngoodness measures, and extended to negative dependencies, as well. As an\nexample, we show that several commonly used goodness measures are\nwell-behaving.\n", "versions": [{"version": "v1", "created": "Tue, 6 May 2014 16:22:38 GMT"}], "update_date": "2014-05-07", "authors_parsed": [["H\u00e4m\u00e4l\u00e4inen", "Wilhelmiina", ""]]}, {"id": "1405.1360", "submitter": "Wilhelmiina H\\\"am\\\"al\\\"ainen", "authors": "Wilhelmiina H\\\"am\\\"al\\\"ainen", "title": "Assessing the statistical significance of association rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An association rule is statistically significant, if it has a small\nprobability to occur by chance. It is well-known that the traditional\nfrequency-confidence framework does not produce statistically significant\nrules. It can both accept spurious rules (type 1 error) and reject significant\nrules (type 2 error). The same problem concerns other commonly used\ninterestingness measures and pruning heuristics.\n  In this paper, we inspect the most common measure functions - frequency,\nconfidence, degree of dependence, $\\chi^2$, correlation coefficient, and\n$J$-measure - and redundancy reduction techniques. For each technique, we\nanalyze whether it can make type 1 or type 2 error and the conditions under\nwhich the error occurs. In addition, we give new theoretical results which can\nbe use to guide the search for statistically significant association rules.\n", "versions": [{"version": "v1", "created": "Tue, 6 May 2014 16:54:20 GMT"}], "update_date": "2014-05-07", "authors_parsed": [["H\u00e4m\u00e4l\u00e4inen", "Wilhelmiina", ""]]}, {"id": "1405.1499", "submitter": "Abdul Quamar", "authors": "Abdul Quamar and Amol Deshpande and Jimmy Lin", "title": "NScale: Neighborhood-centric Large-Scale Graph Analytics in the Cloud", "comments": "26 pages, 15 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increasing interest in executing complex analyses over large\ngraphs, many of which require processing a large number of multi-hop\nneighborhoods or subgraphs. Examples include ego network analysis, motif\ncounting, personalized recommendations, and others. These tasks are not well\nserved by existing vertex-centric graph processing frameworks, where user\nprograms are only able to directly access the state of a single vertex. This\npaper introduces NSCALE, a novel end-to-end graph processing framework that\nenables the distributed execution of complex subgraph-centric analytics over\nlarge-scale graphs in the cloud. NSCALE enables users to write programs at the\nlevel of subgraphs rather than at the level of vertices. Unlike most previous\ngraph processing frameworks, which apply the user program to the entire graph,\nNSCALE allows users to declaratively specify subgraphs of interest. Our\nframework includes a novel graph extraction and packing (GEP) module that\nutilizes a cost-based optimizer to partition and pack the subgraphs of interest\ninto memory on as few machines as possible. The distributed execution engine\nthen takes over and runs the user program in parallel, while respecting the\nscope of the various subgraphs. Our experimental results show\norders-of-magnitude improvements in performance and drastic reductions in the\ncost of analytics compared to vertex-centric approaches.\n", "versions": [{"version": "v1", "created": "Wed, 7 May 2014 03:41:53 GMT"}, {"version": "v2", "created": "Fri, 14 Nov 2014 20:17:45 GMT"}, {"version": "v3", "created": "Wed, 30 Sep 2015 12:44:58 GMT"}], "update_date": "2015-10-01", "authors_parsed": [["Quamar", "Abdul", ""], ["Deshpande", "Amol", ""], ["Lin", "Jimmy", ""]]}, {"id": "1405.1705", "submitter": "Raman Grover", "authors": "Raman Grover, Michael J. Carey", "title": "Scalable Fault-Tolerant Data Feeds in AsterixDB", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe the support for data feed ingestion in AsterixDB,\nan open-source Big Data Management System (BDMS) that provides a platform for\nstorage and analysis of large volumes of semi-structured data. Data feeds are a\nmechanism for having continuous data arrive into a BDMS from external sources\nand incrementally populate a persisted dataset and associated indexes. The need\nto persist and index \"fast-flowing\" high-velocity data (and support ad hoc\nanalytical queries) is ubiquitous. However, the state of the art today involves\n'gluing' together different systems. AsterixDB is different in being a unified\nsystem with \"native support\" for data feed ingestion.\n  We discuss the challenges and present the design and implementation of the\nconcepts involved in modeling and managing data feeds in AsterixDB. AsterixDB\nallows the runtime behavior, allocation of resources and the offered degree of\nrobustness to be customized to suit the high-level application(s) that wish to\nconsume the ingested data. Initial experiments that evaluate scalability and\nfault-tolerance of AsterixDB data feeds facility are reported.\n", "versions": [{"version": "v1", "created": "Wed, 7 May 2014 19:14:42 GMT"}], "update_date": "2014-05-08", "authors_parsed": [["Grover", "Raman", ""], ["Carey", "Michael J.", ""]]}, {"id": "1405.1851", "submitter": "Cynthia  Selvi", "authors": "P. Cynthia Selvi, A.R.Mohammed Shanavas", "title": "Output Privacy Protection With Pattern-Based Heuristic Algorithm", "comments": "12 pages", "journal-ref": null, "doi": "10.5121/ijcsit.2014.6210", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy Preserving Data Mining(PPDM) is an ongoing research area aimed at\nbridging the gap between the collaborative data mining and data confidentiality\nThere are many different approaches which have been adopted for PPDM, of them\nthe rule hiding approach is used in this article. This approach ensures output\nprivacy that prevent the mined patterns(itemsets) from malicious inference\nproblems. An efficient algorithm named as Pattern-based Maxcover Algorithm is\nproposed with experimental results. This algorithm minimizes the dissimilarity\nbetween the source and the released database; Moreover the patterns protected\ncannot be retrieved from the released database by an adversary or counterpart\neven with an arbitrarily low support threshold.\n", "versions": [{"version": "v1", "created": "Thu, 8 May 2014 09:26:52 GMT"}], "update_date": "2014-05-09", "authors_parsed": [["Selvi", "P. Cynthia", ""], ["Shanavas", "A. R. Mohammed", ""]]}, {"id": "1405.1912", "submitter": "M\\'arta Czenky", "authors": "M\\'arta Czenky", "title": "The Efficiency Examination of Teaching of Different Normalization\n  Methods", "comments": null, "journal-ref": "International Journal of Database Management Systems ( IJDMS )\n  Vol.6, No.2, April 2014", "doi": "10.5121/ijdms.2014.6201", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalization is an important database design method, in the course of the\nteaching of data modeling the understanding and applying of this method cause\nproblems for students the most. For improving the efficiency of learning\nnormalization we looked for alternative normalization methods and introduced\nthem into education. We made a survey among engineer students how efficient\ncould they execute the normalization with different methods. We executed\nstatistical and data mining examinations to decide whether any of the methods\nresulted significantly better solutions.\n", "versions": [{"version": "v1", "created": "Thu, 8 May 2014 13:06:34 GMT"}], "update_date": "2014-05-09", "authors_parsed": [["Czenky", "M\u00e1rta", ""]]}, {"id": "1405.2848", "submitter": "Giorgio Orsi PhD", "authors": "Georg Gottlob, Giorgio Orsi, Andreas Pieris", "title": "Query Rewriting and Optimization for Ontological Databases", "comments": "arXiv admin note: text overlap with arXiv:1312.5914 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ontological queries are evaluated against a knowledge base consisting of an\nextensional database and an ontology (i.e., a set of logical assertions and\nconstraints which derive new intensional knowledge from the extensional\ndatabase), rather than directly on the extensional database. The evaluation and\noptimization of such queries is an intriguing new problem for database\nresearch. In this paper, we discuss two important aspects of this problem:\nquery rewriting and query optimization. Query rewriting consists of the\ncompilation of an ontological query into an equivalent first-order query\nagainst the underlying extensional database. We present a novel query rewriting\nalgorithm for rather general types of ontological constraints which is\nwell-suited for practical implementations. In particular, we show how a\nconjunctive query against a knowledge base, expressed using linear and sticky\nexistential rules, that is, members of the recently introduced Datalog+/-\nfamily of ontology languages, can be compiled into a union of conjunctive\nqueries (UCQ) against the underlying database. Ontological query optimization,\nin this context, attempts to improve this rewriting process so to produce\npossibly small and cost-effective UCQ rewritings for an input query.\n", "versions": [{"version": "v1", "created": "Mon, 12 May 2014 17:34:45 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Gottlob", "Georg", ""], ["Orsi", "Giorgio", ""], ["Pieris", "Andreas", ""]]}, {"id": "1405.3250", "submitter": "Eric Gribkoff", "authors": "Eric Gribkoff, Guy Van den Broeck, and Dan Suciu", "title": "Understanding the Complexity of Lifted Inference and Asymmetric Weighted\n  Model Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study lifted inference for the Weighted First-Order Model\nCounting problem (WFOMC), which counts the assignments that satisfy a given\nsentence in first-order logic (FOL); it has applications in Statistical\nRelational Learning (SRL) and Probabilistic Databases (PDB). We present several\nresults. First, we describe a lifted inference algorithm that generalizes prior\napproaches in SRL and PDB. Second, we provide a novel dichotomy result for a\nnon-trivial fragment of FO CNF sentences, showing that for each sentence the\nWFOMC problem is either in PTIME or #P-hard in the size of the input domain; we\nprove that, in the first case our algorithm solves the WFOMC problem in PTIME,\nand in the second case it fails. Third, we present several properties of the\nalgorithm. Finally, we discuss limitations of lifted inference for symmetric\nprobabilistic databases (where the weights of ground literals depend only on\nthe relation name, and not on the constants of the domain), and prove the\nimpossibility of a dichotomy result for the complexity of probabilistic\ninference for the entire language FOL.\n", "versions": [{"version": "v1", "created": "Tue, 13 May 2014 18:39:11 GMT"}, {"version": "v2", "created": "Tue, 29 Jul 2014 17:31:31 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Gribkoff", "Eric", ""], ["Broeck", "Guy Van den", ""], ["Suciu", "Dan", ""]]}, {"id": "1405.3631", "submitter": "Kian Win Ong", "authors": "Kian Win Ong and Yannis Papakonstantinou and Romain Vernoux", "title": "The SQL++ Query Language: Configurable, Unifying and Semi-structured", "comments": "13 pages, [14166]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NoSQL databases support semi-structured data, typically modeled as JSON. They\nalso provide limited (but expanding) query languages. Their idiomatic, non-SQL\nlanguage constructs, the many variations, and the lack of formal semantics\ninhibit deep understanding of the query languages, and also impede progress\ntowards clean, powerful, declarative query languages.\n  This paper specifies the syntax and semantics of SQL++, which is applicable\nto both JSON native stores and SQL databases. The SQL++ semi-structured data\nmodel is a superset of both JSON and the SQL data model. SQL++ offers powerful\ncomputational capabilities for processing semi-structured data akin to prior\nnon-relational query languages, notably OQL and XQuery. Yet, SQL++ is SQL\nbackwards compatible and is generalized towards JSON by introducing only a\nsmall number of query language extensions to SQL.\n  Recognizing that a query language standard is probably premature for the fast\nevolving area of NoSQL databases, SQL++ includes configuration options that\nformally itemize the semantics variations that language designers may choose\nfrom. The options often pertain to the treatment of semi-structuredness\n(missing attributes, heterogeneous types, etc), where more than one sensible\napproaches are possible.\n  SQL++ is unifying: By appropriate choices of configuration options, the SQL++\nsemantics can morph into the semantics of existing semi-structured database\nquery languages. The extensive experimental validation shows how SQL and four\nsemi-structured database query languages (MongoDB, Cassandra CQL, Couchbase\nN1QL and AsterixDB AQL) are formally described by appropriate settings of the\nconfiguration options.\n  Early adoption signs of SQL++ are positive: Version 4 of Couchbase's N1QL is\nexplained as syntactic sugar over SQL++. AsterixDB will soon support the full\nSQL++ and Apache Drill is in the process of aligning with SQL++.\n", "versions": [{"version": "v1", "created": "Wed, 14 May 2014 19:17:36 GMT"}, {"version": "v2", "created": "Thu, 5 Jun 2014 05:59:54 GMT"}, {"version": "v3", "created": "Fri, 15 Aug 2014 06:15:12 GMT"}, {"version": "v4", "created": "Tue, 30 Sep 2014 01:56:31 GMT"}, {"version": "v5", "created": "Sun, 1 Mar 2015 09:51:13 GMT"}, {"version": "v6", "created": "Thu, 19 Mar 2015 19:48:56 GMT"}, {"version": "v7", "created": "Wed, 29 Apr 2015 18:59:21 GMT"}, {"version": "v8", "created": "Mon, 14 Dec 2015 16:52:21 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Ong", "Kian Win", ""], ["Papakonstantinou", "Yannis", ""], ["Vernoux", "Romain", ""]]}, {"id": "1405.4027", "submitter": "Alex Thomo", "authors": "Ben Kimmett, Alex Thomo, S. Venkatesh", "title": "Three-Way Joins on MapReduce: An Experimental Study", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study three-way joins on MapReduce. Joins are very useful in a multitude\nof applications from data integration and traversing social networks, to mining\ngraphs and automata-based constructions. However, joins are expensive, even for\nmoderate data sets; we need efficient algorithms to perform distributed\ncomputation of joins using clusters of many machines. MapReduce has become an\nincreasingly popular distributed computing system and programming paradigm. We\nconsider a state-of-the-art MapReduce multi-way join algorithm by Afrati and\nUllman and show when it is appropriate for use on very large data sets. By\nproviding a detailed experimental study, we demonstrate that this algorithm\nscales much better than what is suggested by the original paper. However, if\nthe join result needs to be summarized or aggregated, as opposed to being only\nenumerated, then the aggregation step can be integrated into a cascade of\ntwo-way joins, making it more efficient than the other algorithm, and thus\nbecomes the preferred solution.\n", "versions": [{"version": "v1", "created": "Thu, 15 May 2014 22:25:09 GMT"}], "update_date": "2014-05-19", "authors_parsed": [["Kimmett", "Ben", ""], ["Thomo", "Alex", ""], ["Venkatesh", "S.", ""]]}, {"id": "1405.4228", "submitter": "Leopoldo Bertossi", "authors": "Leopoldo Bertossi and Babak Salimi", "title": "Unifying Causality, Diagnosis, Repairs and View-Updates in Databases", "comments": "On-line Proc. First International Workshop on Big Uncertain Data\n  (BUDA 2014). Co-located with ACM PODS 2014. arXiv admin note: text overlap\n  with arXiv:1404.6857", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we establish and point out connections between the notion of\nquery-answer causality in databases and database repairs, model-based diagnosis\nin its consistency-based and abductive versions, and database updates through\nviews. The mutual relationships among these areas of data management and\nknowledge representation shed light on each of them and help to share notions\nand results they have in common. In one way or another, these are all\napproaches to uncertainty management, which becomes even more relevant in the\ncontext of big data that have to be made sense of.\n", "versions": [{"version": "v1", "created": "Fri, 16 May 2014 16:18:56 GMT"}, {"version": "v2", "created": "Sat, 28 Jun 2014 23:35:47 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Bertossi", "Leopoldo", ""], ["Salimi", "Babak", ""]]}, {"id": "1405.4607", "submitter": "Bernardo Gon\\c{c}alves", "authors": "Bernardo Gon\\c{c}alves, Fabio Porto", "title": "$\\Upsilon$-DB: Managing scientific hypotheses as uncertain data", "comments": "To appear in PVLDB 2014", "journal-ref": "PVLDB 7(11):959-62, 2014", "doi": null, "report-no": null, "categories": "cs.DB cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In view of the paradigm shift that makes science ever more data-driven, we\nconsider deterministic scientific hypotheses as uncertain data. This vision\ncomprises a probabilistic database (p-DB) design methodology for the systematic\nconstruction and management of U-relational hypothesis DBs, viz.,\n$\\Upsilon$-DBs. It introduces hypothesis management as a promising new class of\napplications for p-DBs. We illustrate the potential of $\\Upsilon$-DB as a tool\nfor deep predictive analytics.\n", "versions": [{"version": "v1", "created": "Mon, 19 May 2014 05:09:50 GMT"}], "update_date": "2015-01-23", "authors_parsed": [["Gon\u00e7alves", "Bernardo", ""], ["Porto", "Fabio", ""]]}, {"id": "1405.4979", "submitter": "Razen Al-Harbi", "authors": "Razen Al-Harbi, Yasser Ebrahim, Panos Kalnis", "title": "PHD-Store: An Adaptive SPARQL Engine with Dynamic Partitioning for\n  Distributed RDF Repositories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many repositories utilize the versatile RDF model to publish data.\nRepositories are typically distributed and geographically remote, but data are\ninterconnected (e.g., the Semantic Web) and queried globally by a language such\nas SPARQL. Due to the network cost and the nature of the queries, the execution\ntime can be prohibitively high. Current solutions attempt to minimize the\nnetwork cost by redistributing all data in a preprocessing phase, but here are\ntwo drawbacks: (i) redistribution is based on heuristics that may not benefit\nmany of the future queries; and (ii) the preprocessing phase is very expensive\neven for moderate size datasets. In this paper we propose PHD-Store, a SPARQL\nengine for distributed RDF repositories. Our system does not assume any\nparticular initial data placement and does not require prepartitioning; hence,\nit minimizes the startup cost. Initially, PHD-Store answers queries using a\npotentially slow distributed semi-join algorithm, but adapts dynamically to the\nquery load by incrementally redistributing frequently accessed data.\nRedistribution is done in a way that future queries can benefit from fast\nhash-based parallel execution. Our experiments with synthetic and real data\nverify that PHD-Store scales to very large datasets; many repositories;\nconverges to comparable or better quality of partitioning than existing\nmethods; and executes large query loads 1 to 2 orders of magnitude faster than\nour competitors.\n", "versions": [{"version": "v1", "created": "Tue, 20 May 2014 07:44:03 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Al-Harbi", "Razen", ""], ["Ebrahim", "Yasser", ""], ["Kalnis", "Panos", ""]]}, {"id": "1405.5109", "submitter": "Michael Morak", "authors": "Michael Morak", "title": "The Impact of Disjunction on Reasoning under Existential Rules: Research\n  Summary", "comments": "A full version of a paper accepted to be presented at the Doctoral\n  Consortium of the 30th International Conference on Logic Programming (ICLP\n  2014), July 19-22, Vienna, Austria", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LO", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Datalog+/- is a Datalog-based language family enhanced with existential\nquantification in rule heads, equalities and negative constraints. Query\nanswering over databases with respect to a Datalog+/- theory is generally\nundecidable, however several syntactic restrictions have been proposed to\nremedy this fact. However, a useful and natural feature however is as of yet\nmissing from Datalog+/-: The ability to express uncertain knowledge, or\nchoices, using disjunction. It is the precise objective of the doctoral thesis\nherein discussed, to investigate the impact on the complexity of query\nanswering, of adding disjunction to well-known decidable Datalog+/- fragments,\nnamely guarded, sticky and weakly-acyclic Datalog+/- theories. For guarded\ntheories with disjunction, we obtain a strong 2EXP lower bound in the combined\ncomplexity, even for very restricted formalisms like fixed sets of\n(disjunctive) inclusion dependencies. For sticky theories, the query answering\nproblem becomes undecidable, even in the data complexity, and for\nweakly-acyclic query answering we see a reasonable and expected increase in\ncomplexity.\n", "versions": [{"version": "v1", "created": "Fri, 16 May 2014 04:40:20 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Morak", "Michael", ""]]}, {"id": "1405.5628", "submitter": "Alban Gabillon", "authors": "Fr\\'ed\\'eric Cuppens (LUSSI, Lab-STICC), Alban Gabillon (GePaSUD)", "title": "Cover Story Management", "comments": null, "journal-ref": "Data and Knowledge Engineering 37, 2 (2001) 177-201", "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a multilevel database, cover stories are usually managed using the\nambiguous technique of polyinstantiation. In this paper, we define a new\ntechnique to manage cover stories and propose a formal representation of a\nmultilevel database containing cover stories. Our model aims to be a generic\nmodel, that is, it can be interpreted for any kind of database (e.g.\nrelational, object- oriented etc). We then consider the problem of updating a\nmultilevel database containing cover stories managed with our technique.\n", "versions": [{"version": "v1", "created": "Thu, 22 May 2014 04:13:15 GMT"}], "update_date": "2014-07-09", "authors_parsed": [["Cuppens", "Fr\u00e9d\u00e9ric", "", "LUSSI, Lab-STICC"], ["Gabillon", "Alban", "", "GePaSUD"]]}, {"id": "1405.5645", "submitter": "Heike Stephan", "authors": "Heike Stephan, Stefan Brass", "title": "A Variant of Earley Deduction With Partial Evaluation", "comments": "WLP 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm for query evaluation given a logic program consisting\nof function-free Datalog rules. It is based on Earley Deduction [4, 6] and uses\na partial evaluation similar to the one we devel oped for our SLDMagic method\n[1]. With this, finite automata modeling the evaluation of given queries are\ngenerated. In certain cases, the new method is more efficient than SLDMagic and\nthe standard Magic Set method since it can process several deduction steps as\none.\n", "versions": [{"version": "v1", "created": "Thu, 22 May 2014 07:31:16 GMT"}], "update_date": "2014-05-23", "authors_parsed": [["Stephan", "Heike", ""], ["Brass", "Stefan", ""]]}, {"id": "1405.5661", "submitter": "Yan Kit Li", "authors": "Yan Kit Li, Min Xu, Chun Ho Ng, Patrick P. C. Lee", "title": "Efficient Hybrid Inline and Out-of-line Deduplication for Backup Storage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Backup storage systems often remove redundancy across backups via inline\ndeduplication, which works by referring duplicate chunks of the latest backup\nto those of existing backups. However, inline deduplication degrades restore\nperformance of the latest backup due to fragmentation, and complicates deletion\nof ex- pired backups due to the sharing of data chunks. While out-of-line\ndeduplication addresses the problems by forward-pointing existing duplicate\nchunks to those of the latest backup, it introduces additional I/Os of writing\nand removing duplicate chunks. We design and implement RevDedup, an efficient\nhybrid inline and out-of-line deduplication system for backup storage. It\napplies coarse-grained inline deduplication to remove duplicates of the latest\nbackup, and then fine-grained out-of-line reverse deduplication to remove\nduplicates from older backups. Our reverse deduplication design limits the I/O\noverhead and prepares for efficient deletion of expired backups. Through\nextensive testbed experiments using synthetic and real-world datasets, we show\nthat RevDedup can bring high performance to the backup, restore, and deletion\noperations, while maintaining high storage efficiency comparable to\nconventional inline deduplication.\n", "versions": [{"version": "v1", "created": "Thu, 22 May 2014 08:13:18 GMT"}], "update_date": "2014-05-23", "authors_parsed": [["Li", "Yan Kit", ""], ["Xu", "Min", ""], ["Ng", "Chun Ho", ""], ["Lee", "Patrick P. C.", ""]]}, {"id": "1405.5671", "submitter": "Alban Gabillon", "authors": "Alban Gabillon (GePaSUD)", "title": "A Logical Formalization of a Secure XML Database", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we first define a logical theory representing an XML database\nsupporting XPath as query language and XUpdate as modification language. We\nthen extend our theory with predicates allowing us to specify the security\npolicy protecting the database. The security policy includes rules addressing\nthe read and write privileges. We propose axioms to derive the database view\neach user is permitted to see. We also propose axioms to derive the new\ndatabase content after an update.\n", "versions": [{"version": "v1", "created": "Thu, 22 May 2014 08:56:01 GMT"}], "update_date": "2014-05-23", "authors_parsed": [["Gabillon", "Alban", "", "GePaSUD"]]}, {"id": "1405.5777", "submitter": "James Cheney", "authors": "James Cheney and Roly Perera", "title": "An Analytical Survey of Provenance Sanitization", "comments": "To appear, IPAW 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Security is likely becoming a critical factor in the future adoption of\nprovenance technology, because of the risk of inadvertent disclosure of\nsensitive information. In this survey paper we review the state of the art in\nsecure provenance, considering mechanisms for controlling access, and the\nextent to which these mechanisms preserve provenance integrity. We examine\nseven systems or approaches, comparing features and identifying areas for\nfuture work.\n", "versions": [{"version": "v1", "created": "Thu, 22 May 2014 14:51:13 GMT"}], "update_date": "2014-05-23", "authors_parsed": [["Cheney", "James", ""], ["Perera", "Roly", ""]]}, {"id": "1405.5829", "submitter": "Michele Dallachiesa", "authors": "Michele Dallachiesa and Charu Aggarwal and Themis Palpanas", "title": "Node Classification in Uncertain Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real applications that use and analyze networked data, the links in\nthe network graph may be erroneous, or derived from probabilistic techniques.\nIn such cases, the node classification problem can be challenging, since the\nunreliability of the links may affect the final results of the classification\nprocess. If the information about link reliability is not used explicitly, the\nclassification accuracy in the underlying network may be affected adversely. In\nthis paper, we focus on situations that require the analysis of the uncertainty\nthat is present in the graph structure. We study the novel problem of node\nclassification in uncertain graphs, by treating uncertainty as a first-class\ncitizen. We propose two techniques based on a Bayes model and automatic\nparameter selection, and show that the incorporation of uncertainty in the\nclassification process as a first-class citizen is beneficial. We\nexperimentally evaluate the proposed approach using different real data sets,\nand study the behavior of the algorithms under different conditions. The\nresults demonstrate the effectiveness and efficiency of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 22 May 2014 17:13:00 GMT"}], "update_date": "2014-05-23", "authors_parsed": [["Dallachiesa", "Michele", ""], ["Aggarwal", "Charu", ""], ["Palpanas", "Themis", ""]]}, {"id": "1405.5905", "submitter": "Bernardo Gon\\c{c}alves", "authors": "Bernardo Gon\\c{c}alves and Fabio Porto", "title": "Managing large-scale scientific hypotheses as uncertain and\n  probabilistic data with support for predictive analytics", "comments": "16 pages, 9 figures, 1 table", "journal-ref": "IEEE Computing in Science and Eng. 17(5):35-43, 2015", "doi": "10.1109/MCSE.2015.102", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sheer scale of high-resolution raw data generated by simulation has\nmotivated non-conventional approaches for data exploration referred as\n`immersive' and `in situ' query processing of the raw simulation data. Another\nstep towards supporting scientific progress is to enable data-driven hypothesis\nmanagement and predictive analytics out of simulation results. We present a\nsynthesis method and tool for encoding and managing competing hypotheses as\nuncertain data in a probabilistic database that can be conditioned in the\npresence of observations.\n", "versions": [{"version": "v1", "created": "Thu, 22 May 2014 20:43:51 GMT"}, {"version": "v2", "created": "Sun, 17 May 2015 23:01:31 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Gon\u00e7alves", "Bernardo", ""], ["Porto", "Fabio", ""]]}, {"id": "1405.6328", "submitter": "Renato dos Santos", "authors": "Renato P. dos Santos", "title": "Big Data as a Mediator in Science Teaching: A Proposal", "comments": "13 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ed-ph cs.CY cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We live in a digital world that, in 2010, crossed the mark of one zettabyte\ndata. This huge amount of data processed on computers extremely fast with\noptimized techniques allows one to find insights in new and emerging types of\ndata and content and to answer questions that were previously considered beyond\nreach. This is the idea of Big Data. Google now offers the Google Correlate\nanalysis public tool that, from a search term or a series of temporal or\nregional data, provides a list of queries on Google whose frequencies follow\npatterns that best correlate with the data, according to the Pearson\ndetermination coefficient R2. Of course, correlation does not imply causation.\nWe believe, however, that there is potential for these big data tools to find\nunexpected correlations that may serve as clues to interesting phenomena, from\nthe pedagogical and even scientific point of view. As far as we know, this is\nthe first proposal for the use of Big Data in Science Teaching, of\nconstructionist character, taking as mediators the computer and the public and\nfree tools such as Google Correlate. It also has an epistemological bias, not\nbeing merely a training in computational infrastructure or predictive\nanalytics, but aiming at providing students a better understanding of physical\nconcepts, such as phenomena, observation, measurement, physical laws, theory,\nand causality. With it, they would be able to become good Big Data specialists,\nthe so needed 'data scientists' to solve the challenges of Big Data.\n", "versions": [{"version": "v1", "created": "Sat, 24 May 2014 18:24:22 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Santos", "Renato P. dos", ""]]}, {"id": "1405.6500", "submitter": "Lei Gai", "authors": "Lei Gai, Wei Chen, Zhichao Xu, Changhe Qiu, and Tengjiao Wang", "title": "Towards Efficient Path Query on Social Network with Hybrid RDF\n  Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scalability and exibility of Resource Description Framework(RDF) model\nmake it ideally suited for representing online social networks(OSN). One basic\noperation in OSN is to find chains of relations,such as k-Hop friends. Property\npath query in SPARQL can express this type of operation, but its implementation\nsuffers from performance problem considering the ever growing data size and\ncomplexity of OSN.In this paper, we present a main memory/disk based hybrid RDF\ndata management framework for efficient property path query. In this hybrid\nframework, we realize an efficient in-memory algebra operator for property path\nquery using graph traversal, and estimate the cost of this operator to\ncooperate with existing cost-based optimization. Experiments on benchmark and\nreal dataset demonstrated that our approach can achieve a good tradeoff between\ndata load expense and online query performance.\n", "versions": [{"version": "v1", "created": "Mon, 26 May 2014 08:29:19 GMT"}, {"version": "v2", "created": "Tue, 10 Jun 2014 01:39:38 GMT"}], "update_date": "2016-11-25", "authors_parsed": [["Gai", "Lei", ""], ["Chen", "Wei", ""], ["Xu", "Zhichao", ""], ["Qiu", "Changhe", ""], ["Wang", "Tengjiao", ""]]}, {"id": "1405.7264", "submitter": "Matteo Interlandi", "authors": "Matteo Interlandi, Letizia Tanca", "title": "A Datalog-based Computational Model for Coordination-free, Data-Parallel\n  Systems", "comments": "Under consideration in Theory and Practice of Logic Programming\n  (TPLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing refers to maximizing efficiency by sharing computational and\nstorage resources, while data-parallel systems exploit the resources available\nin the cloud to perform parallel transformations over large amounts of data. In\nthe same line, considerable emphasis has been recently given to two apparently\ndisjoint research topics: data-parallel, and eventually consistent, distributed\nsystems. Declarative networking has been recently proposed to ease the task of\nprogramming in the cloud, by allowing the programmer to express only the\ndesired result and leave the implementation details to the responsibility of\nthe run-time system. In this context, we propose a study on a\nlogic-programming-based computational model for eventually consistent,\ndata-parallel systems, the keystone of which is provided by the recent finding\nthat the class of programs that can be computed in an eventually consistent,\ncoordination-free way is that of monotonic programs. This principle is called\nCALM and has been proven by Ameloot et al. for distributed, asynchronous\nsettings. We advocate that CALM should be employed as a basic theoretical tool\nalso for data-parallel systems, wherein computation usually proceeds\nsynchronously in rounds and where communication is assumed to be reliable. It\nis general opinion that coordination-freedom can be seen as a major\ndiscriminant factor. In this work we make the case that the current form of\nCALM does not hold in general for data-parallel systems, and show how, using\nnovel techniques, the satisfiability of the CALM principle can still be\nobtained although just for the subclass of programs called connected monotonic\nqueries. We complete the study with considerations on the relationships between\nour model and the one employed by Ameloot et al., showing that our techniques\nsubsume the latter when the synchronization constraints imposed on the system\nare loosened.\n", "versions": [{"version": "v1", "created": "Wed, 28 May 2014 14:47:50 GMT"}, {"version": "v2", "created": "Sat, 31 May 2014 16:21:23 GMT"}, {"version": "v3", "created": "Tue, 28 Mar 2017 02:50:14 GMT"}, {"version": "v4", "created": "Mon, 9 Jul 2018 00:59:39 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Interlandi", "Matteo", ""], ["Tanca", "Letizia", ""]]}, {"id": "1405.7461", "submitter": "Michael Gowanlock", "authors": "Michael G. Gowanlock and Henri Casanova", "title": "Technical Report: Parallel Distance Threshold Query Processing for\n  Spatiotemporal Trajectory Databases on the GPU", "comments": "35 pages, 16 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Processing moving object trajectories arises in many application domains and\nhas been addressed by practitioners in the spatiotemporal database and\nGeographical Information System communities. In this work, we focus on a\ntrajectory similarity search, the distance threshold query, which finds all\ntrajectories within a given distance d of a search trajectory over a time\ninterval. We demonstrate the performance of a multithreaded implementation\nwhich features the use of an R-tree index and which has high parallel\nefficiency (78%-90%). We introduce a GPGPU implementation which avoids the use\nof index-trees, and instead features a GPU-friendly indexing method. We compare\nthe performance of the multithreaded and GPU implementations, and show that a\nspeedup can be obtained using the latter. We propose two classes of algorithms,\nSetSplit and GreedySetSplit, to create efficient query batches that reduce\nmemory pressure and computational cost on the GPU. However, we find that using\nfixed-size batches is sufficiently efficient in practice. We develop an\nempirical performance model for our GPGPU implementation that can be used to\npredict the response time of the distance threshold query. This model can be\nused to pick a good query batch size.\n", "versions": [{"version": "v1", "created": "Thu, 29 May 2014 04:44:35 GMT"}, {"version": "v2", "created": "Sat, 13 Sep 2014 03:21:48 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Gowanlock", "Michael G.", ""], ["Casanova", "Henri", ""]]}, {"id": "1405.7584", "submitter": "Stefano Sartor", "authors": "Marco Frailis (1), Stefano Sartor (1), Andrea Zacchei (1), Marcello\n  Lodi (3), Roberto Cirami (1), Fabio Pasian (1), Massimo Trifoglio (2), Andrea\n  Bulgarelli (2), Fulvio Gianotti (2), Enrico Franceschi (2), Luciano Nicastro\n  (2), Vito Conforti (2), Andrea Zoli (2), Ricky Smart (4), Roberto Morbidelli\n  (4) and Mauro Dadina (2) ((1) INAF-Osservatorio Astronomico di Trieste, (2)\n  INAF-IASF Bologna, (3) Telescopio Nazionale Galileo FGG - INAF, (4)\n  INAF-Osservatorio Astrofisico di Torino)", "title": "DAS: a data management system for instrument tests and operations", "comments": "Accepted for pubblication on ADASS Conference Series", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Data Access System (DAS) is a metadata and data management software\nsystem, providing a reusable solution for the storage of data acquired both\nfrom telescopes and auxiliary data sources during the instrument development\nphases and operations. It is part of the Customizable Instrument WorkStation\nsystem (CIWS-FW), a framework for the storage, processing and quick-look at the\ndata acquired from scientific instruments. The DAS provides a data access layer\nmainly targeted to software applications: quick-look displays, pre-processing\npipelines and scientific workflows. It is logically organized in three main\ncomponents: an intuitive and compact Data Definition Language (DAS DDL) in XML\nformat, aimed for user-defined data types; an Application Programming Interface\n(DAS API), automatically adding classes and methods supporting the DDL data\ntypes, and providing an object-oriented query language; a data management\ncomponent, which maps the metadata of the DDL data types in a relational Data\nBase Management System (DBMS), and stores the data in a shared (network) file\nsystem. With the DAS DDL, developers define the data model for a particular\nproject, specifying for each data type the metadata attributes, the data format\nand layout (if applicable), and named references to related or aggregated data\ntypes. Together with the DDL user-defined data types, the DAS API acts as the\nonly interface to store, query and retrieve the metadata and data in the DAS\nsystem, providing both an abstract interface and a data model specific one in\nC, C++ and Python. The mapping of metadata in the back-end database is\nautomatic and supports several relational DBMSs, including MySQL, Oracle and\nPostgreSQL.\n", "versions": [{"version": "v1", "created": "Thu, 29 May 2014 15:12:22 GMT"}], "update_date": "2014-05-30", "authors_parsed": [["Frailis", "Marco", ""], ["Sartor", "Stefano", ""], ["Zacchei", "Andrea", ""], ["Lodi", "Marcello", ""], ["Cirami", "Roberto", ""], ["Pasian", "Fabio", ""], ["Trifoglio", "Massimo", ""], ["Bulgarelli", "Andrea", ""], ["Gianotti", "Fulvio", ""], ["Franceschi", "Enrico", ""], ["Nicastro", "Luciano", ""], ["Conforti", "Vito", ""], ["Zoli", "Andrea", ""], ["Smart", "Ricky", ""], ["Morbidelli", "Roberto", ""], ["Dadina", "Mauro", ""]]}]