[{"id": "1704.00115", "submitter": "Mostafa Milani", "authors": "Leopoldo Bertossi, Mostafa Milani", "title": "Ontological Multidimensional Data Models and Contextual Data Qality", "comments": "Journal submission (revised version addressing reviewers'\n  observations) Extended version of RuleML'15 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data quality assessment and data cleaning are context-dependent activities.\nMotivated by this observation, we propose the Ontological Multidimensional Data\nModel (OMD model), which can be used to model and represent contexts as\nlogic-based ontologies. The data under assessment is mapped into the context,\nfor additional analysis, processing, and quality data extraction. The resulting\ncontexts allow for the representation of dimensions, and multidimensional data\nquality assessment becomes possible. At the core of a multidimensional context\nwe include a generalized multidimensional data model and a Datalog+/- ontology\nwith provably good properties in terms of query answering. These main\ncomponents are used to represent dimension hierarchies, dimensional\nconstraints, dimensional rules, and define predicates for quality data\nspecification. Query answering relies upon and triggers navigation through\ndimension hierarchies, and becomes the basic tool for the extraction of quality\ndata. The OMD model is interesting per se, beyond applications to data quality.\nIt allows for a logic-based, and computationally tractable representation of\nmultidimensional data, extending previous multidimensional data models with\nadditional expressive power and functionalities.\n", "versions": [{"version": "v1", "created": "Sat, 1 Apr 2017 03:50:53 GMT"}, {"version": "v2", "created": "Sun, 13 Aug 2017 21:11:37 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Bertossi", "Leopoldo", ""], ["Milani", "Mostafa", ""]]}, {"id": "1704.00205", "submitter": "Shuo Han", "authors": "Shuo Han, Lei Zou, Jeffrey Xu Yu, Dongyan Zhao", "title": "Keyword Search on RDF Graphs - A Query Graph Assembly Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Keyword search provides ordinary users an easy-to-use interface for querying\nRDF data. Given the input keywords, in this paper, we study how to assemble a\nquery graph that is to represent user's query intention accurately and\nefficiently. Based on the input keywords, we first obtain the elementary query\ngraph building blocks, such as entity/class vertices and predicate edges. Then,\nwe formally define the query graph assembly (QGA) problem. Unfortunately, we\nprove theoretically that QGA is a NP-complete problem. In order to solve that,\nwe design some heuristic lower bounds and propose a bipartite graph\nmatching-based best-first search algorithm. The algorithm's time complexity is\n$O(k^{2l} \\cdot l^{3l})$, where $l$ is the number of the keywords and $k$ is a\ntunable parameter, i.e., the maximum number of candidate entity/class vertices\nand predicate edges allowed to match each keyword. Although QGA is intractable,\nboth $l$ and $k$ are small in practice. Furthermore, the algorithm's time\ncomplexity does not depend on the RDF graph size, which guarantees the good\nscalability of our system in large RDF graphs. Experiments on DBpedia and\nFreebase confirm the superiority of our system on both effectiveness and\nefficiency.\n", "versions": [{"version": "v1", "created": "Sat, 1 Apr 2017 18:17:28 GMT"}, {"version": "v2", "created": "Fri, 25 Aug 2017 14:51:52 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Han", "Shuo", ""], ["Zou", "Lei", ""], ["Yu", "Jeffrey Xu", ""], ["Zhao", "Dongyan", ""]]}, {"id": "1704.00485", "submitter": "Vraj Shah", "authors": "Vraj Shah, Arun Kumar, Xiaojin Zhu", "title": "Are Key-Foreign Key Joins Safe to Avoid when Learning High-Capacity\n  Classifiers?", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) over relational data is a booming area of the database\nindustry and academia. While several projects aim to build scalable and fast ML\nsystems, little work has addressed the pains of sourcing data and features for\nML tasks. Real-world relational databases typically have many tables (often,\ndozens) and data scientists often struggle to even obtain and join all possible\ntables that provide features for ML. In this context, Kumar et al. showed\nrecently that key-foreign key dependencies (KFKDs) between tables often lets us\navoid such joins without significantly affecting prediction accuracy--an idea\nthey called avoiding joins safely. While initially controversial, this idea has\nsince been used by multiple companies to reduce the burden of data sourcing for\nML. But their work applied only to linear classifiers. In this work, we verify\nif their results hold for three popular complex classifiers: decision trees,\nSVMs, and ANNs. We conduct an extensive experimental study using both\nreal-world datasets and simulations to analyze the effects of avoiding KFK\njoins on such models. Our results show that these high-capacity classifiers are\nsurprisingly and counter-intuitively more robust to avoiding KFK joins compared\nto linear classifiers, refuting an intuition from the prior work's analysis. We\nexplain this behavior intuitively and identify open questions at the\nintersection of data management and ML theoretical research. All of our code\nand datasets are available for download from\nhttp://cseweb.ucsd.edu/~arunkk/hamlet.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 09:16:58 GMT"}, {"version": "v2", "created": "Sun, 9 Apr 2017 04:02:56 GMT"}, {"version": "v3", "created": "Sun, 4 Jun 2017 19:02:20 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Shah", "Vraj", ""], ["Kumar", "Arun", ""], ["Zhu", "Xiaojin", ""]]}, {"id": "1704.00630", "submitter": "Joan Guisado-G\\'amez", "authors": "Arnau Prat-P\\'erez, Joan Guisado-G\\'amez, Xavier Fern\\'andez Salas,\n  Petr Koupy, Siegfried Depner, Davide Basilio Bartolini", "title": "Towards a property graph generator for benchmarking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of synthetic graph generators is a common practice among\ngraph-oriented benchmark designers, as it allows obtaining graphs with the\nrequired scale and characteristics. However, finding a graph generator that\naccurately fits the needs of a given benchmark is very difficult, thus\npractitioners end up creating ad-hoc ones. Such a task is usually\ntime-consuming, and often leads to reinventing the wheel. In this paper, we\nintroduce the conceptual design of DataSynth, a framework for property graphs\ngeneration with customizable schemas and characteristics. The goal of DataSynth\nis to assist benchmark designers in generating graphs efficiently and at scale,\nsaving from implementing their own generators. Additionally, DataSynth\nintroduces novel features barely explored so far, such as modeling the\ncorrelation between properties and the structure of the graph. This is achieved\nby a novel property-to-node matching algorithm for which we present preliminary\npromising results.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 15:04:01 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Prat-P\u00e9rez", "Arnau", ""], ["Guisado-G\u00e1mez", "Joan", ""], ["Salas", "Xavier Fern\u00e1ndez", ""], ["Koupy", "Petr", ""], ["Depner", "Siegfried", ""], ["Bartolini", "Davide Basilio", ""]]}, {"id": "1704.01014", "submitter": "Robert Rovetto", "authors": "Robert J. Rovetto", "title": "An Ontological Architecture for Orbital Debris Data", "comments": null, "journal-ref": "Earth Science Informatics (Aug 2015) 9:67, pp.67-82, Springer", "doi": "10.1007/s12145-015-0233-3", "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The orbital debris problem presents an opportunity for inter-agency and\ninternational cooperation toward the mutually beneficial goals of debris\nprevention, mitigation, remediation, and improved space situational awareness\n(SSA). Achieving these goals requires sharing orbital debris and other SSA\ndata. Toward this, I present an ontological architecture for the orbital debris\ndomain, taking steps in the creation of an orbital debris ontology (ODO). The\npurpose of this ontological system is to (I) represent general orbital debris\nand SSA domain knowledge, (II) structure, and standardize where needed, orbital\ndata and terminology, and (III) foster semantic interoperability and\ndata-sharing. In doing so I hope to (IV) contribute to solving the orbital\ndebris problem, improving peaceful global SSA, and ensuring safe space travel\nfor future generations.\n", "versions": [{"version": "v1", "created": "Sat, 1 Apr 2017 21:26:37 GMT"}, {"version": "v2", "created": "Thu, 28 Dec 2017 06:25:39 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Rovetto", "Robert J.", ""]]}, {"id": "1704.01087", "submitter": "Feras Saad", "authors": "Feras Saad, Leonardo Casarsa, Vikash Mansinghka", "title": "Probabilistic Search for Structured Data via Probabilistic Programming\n  and Nonparametric Bayes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Databases are widespread, yet extracting relevant data can be difficult.\nWithout substantial domain knowledge, multivariate search queries often return\nsparse or uninformative results. This paper introduces an approach for\nsearching structured data based on probabilistic programming and nonparametric\nBayes. Users specify queries in a probabilistic language that combines standard\nSQL database search operators with an information theoretic ranking function\ncalled predictive relevance. Predictive relevance can be calculated by a fast\nsparse matrix algorithm based on posterior samples from CrossCat, a\nnonparametric Bayesian model for high-dimensional, heterogeneously-typed data\ntables. The result is a flexible search technique that applies to a broad class\nof information retrieval problems, which we integrate into BayesDB, a\nprobabilistic programming platform for probabilistic data analysis. This paper\ndemonstrates applications to databases of US colleges, global macroeconomic\nindicators of public health, and classic cars. We found that human evaluators\noften prefer the results from probabilistic search to results from a standard\nbaseline.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 16:18:07 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Saad", "Feras", ""], ["Casarsa", "Leonardo", ""], ["Mansinghka", "Vikash", ""]]}, {"id": "1704.01218", "submitter": "Stephen Smart", "authors": "Stephen Smart, Christan Grant", "title": "Storing complex data sharing policies with the Min Mask Sketch", "comments": "8 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  More data is currently being collected and shared by software applications\nthan ever before. In many cases, the user is asked if either all or none of\ntheir data can be shared. We hypothesize that in some cases, users would like\nto share data in more complex ways. In order to implement the sharing of data\nusing more complicated privacy preferences, complex data sharing policies must\nbe used. These complex sharing policies require more space to store than a\nsimple \"all or nothing\" approach to data sharing. In this paper, we present a\nnew probabilistic data structure, called the Min Mask Sketch, to efficiently\nstore these complex data sharing policies. We describe an implementation for\nthe Min Mask Sketch in PostgreSQL and analyze the practicality and feasibility\nof using a probabilistic data structure for storing complex data sharing\npolicies.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 23:29:54 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Smart", "Stephen", ""], ["Grant", "Christan", ""]]}, {"id": "1704.01286", "submitter": "Thomas Zeume", "authors": "Thomas Zeume and Thomas Schwentick", "title": "Dynamic Conjunctive Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article investigates classes of queries maintainable by conjunctive\nqueries (CQs) and their extensions and restrictions in the dynamic complexity\nframework of Patnaik and Immerman. Starting from the basic language of\nquantifier-free conjunctions of positive atoms, it studies the impact of\nadditional operators and features - such as union, atomic negation and\nquantification - on the dynamic expressiveness, for the standard semantics as\nwell as for Delta-semantics.\n  Although many different combinations of these features are possible, they\nbasically yield five important fragments for the standard semantics,\ncharacterized by the addition of (1) arbitrary quantification and atomic\nnegation, (2) existential quantification and atomic negation, (3) existential\nquantification, (4) atomic negation (but no quantification), and by (5) no\naddition to the basic language at all. While fragments (3), (4) and (5) can be\nseparated, it remains open whether fragments (1), (2) and (3) are actually\ndifferent. The fragments arising from Delta-semantics are also subsumed by the\nstandard fragments (1), (2) and (4). The main fragments of DynFO that had been\nstudied in previous work, DynQF and DynProp, characterized by quantifier-free\nupdate programs with or without auxiliary functions, respectively, also fit\ninto this hierarchy: DynProp coincides with fragment (4) and DynQF is strictly\nabove fragment (4) and within fragment (3).\n  As a further result, all (statically) FO-definable queries are captured by\nfragment (2) and a complete characterization of these queries in terms of\nnon-recursive dynamic programs with existential update formulas with one\nexistential quantifier is given.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 06:59:15 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Zeume", "Thomas", ""], ["Schwentick", "Thomas", ""]]}, {"id": "1704.01355", "submitter": "Xuan Zhou", "authors": "Xuan Zhou, Xin Zhou, Zhengtai Yu, Hua Guo and Kian-Lee Tan", "title": "Decentralizing MVCC by Leveraging Visibility", "comments": "The previous version of the paper it entitled \"Posterior Snapshot\n  Isolation\". This new version made the following improvement: 1. we hope that\n  this verion is easier to follow. 2. we extended the CC mechanism to support\n  serializability", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiversion Concurrency Control (MVCC) is a widely adopted concurrency\ncontrol mechanism in database systems, which usually utilizes timestamps to\nresolve conflicts between transactions. However, centralized allocation of\ntimestamps is a potential bottleneck for parallel transaction management. This\nbottleneck is becoming increasingly visible with the rapidly growing degree of\nparallelism of today's computing platforms. This paper introduces Visibility\nbased Concurrency Control (ViCC), a series of CC mechanisms that allow\ntransactions to determine their timestamps autonomously, without relying on\ncentralized coordination. As such, ViCC can scale well, rendering it suitable\nfor various multicore and MPP platforms. Extensive experiments are conducted to\ndemonstrate its advantage over existing approaches.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 10:30:18 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 02:44:51 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Zhou", "Xuan", ""], ["Zhou", "Xin", ""], ["Yu", "Zhengtai", ""], ["Guo", "Hua", ""], ["Tan", "Kian-Lee", ""]]}, {"id": "1704.01770", "submitter": "Diego Garc\\'ia-Gil", "authors": "Diego Garc\\'ia-Gil, Juli\\'an Luengo, Salvador Garc\\'ia and Francisco\n  Herrera", "title": "Enabling Smart Data: Noise filtering in Big Data classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In any knowledge discovery process the value of extracted knowledge is\ndirectly related to the quality of the data used. Big Data problems, generated\nby massive growth in the scale of data observed in recent years, also follow\nthe same dictate. A common problem affecting data quality is the presence of\nnoise, particularly in classification problems, where label noise refers to the\nincorrect labeling of training instances, and is known to be a very disruptive\nfeature of data. However, in this Big Data era, the massive growth in the scale\nof the data poses a challenge to traditional proposals created to tackle noise,\nas they have difficulties coping with such a large amount of data. New\nalgorithms need to be proposed to treat the noise in Big Data problems,\nproviding high quality and clean data, also known as Smart Data. In this paper,\ntwo Big Data preprocessing approaches to remove noisy examples are proposed: an\nhomogeneous ensemble and an heterogeneous ensemble filter, with special\nemphasis in their scalability and performance traits. The obtained results show\nthat these proposals enable the practitioner to efficiently obtain a Smart\nDataset from any Big Data classification problem.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 10:06:52 GMT"}, {"version": "v2", "created": "Fri, 28 Jul 2017 11:39:16 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Garc\u00eda-Gil", "Diego", ""], ["Luengo", "Juli\u00e1n", ""], ["Garc\u00eda", "Salvador", ""], ["Herrera", "Francisco", ""]]}, {"id": "1704.01788", "submitter": "Christos Kalyvas", "authors": "Christos Kalyvas, Theodoros Tzouramanis", "title": "A Survey of Skyline Query Processing", "comments": "127 pages, 91 figures, 38 tables, 208 references,extended Survey", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Living in the Information Age allows almost everyone have access to a large\namount of information and options to choose from in order to fulfill their\nneeds. In many cases, the amount of information available and the rate of\nchange may hide the optimal and truly desired solution. This reveals the need\nof a mechanism that will highlight the best options to choose among every\npossible scenario. Based on this the skyline query was proposed which is a\ndecision support mechanism, that retrieves the valuefor- money options of a\ndataset by identifying the objects that present the optimal combination of the\ncharacteristics of the dataset. This paper surveys the state-of-the-art\ntechniques for skyline query processing, the numerous variations of the initial\nalgorithm that were proposed to solve similar problems and the\napplication-specific approaches that were developed to provide a solution\nefficiently in each case. Aditionally in each section a taxonomy is outlined\nalong with the key aspects of each algorithm and its relation to previous\nstudies.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 11:34:20 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Kalyvas", "Christos", ""], ["Tzouramanis", "Theodoros", ""]]}, {"id": "1704.02384", "submitter": "Eugene Wu", "authors": "Hamed Nilforoshan, Jiannan Wang, Eugene Wu", "title": "PreCog: Improving Crowdsourced Data Quality Before Acquisition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quality control in crowdsourcing systems is crucial. It is typically done\nafter data collection, often using additional crowdsourced tasks to assess and\nimprove the quality. These post-hoc methods can easily add cost and latency to\nthe acquisition process--particularly if collecting high-quality data is\nimportant. In this paper, we argue for pre-hoc interface optimizations based on\nfeedback that helps workers improve data quality before it is submitted and is\nwell suited to complement post-hoc techniques. We propose the Precog system\nthat explicitly supports such interface optimizations for common integrity\nconstraints as well as more ambiguous text acquisition tasks where quality is\nill-defined. We then develop the Segment-Predict-Explain pattern for detecting\nlow-quality text segments and generating prescriptive explanations to help the\nworker improve their text input. Our unique combination of segmentation and\nprescriptive explanation are necessary for Precog to collect 2x more\nhigh-quality text data than non-Precog approaches on two real domains.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 21:53:13 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Nilforoshan", "Hamed", ""], ["Wang", "Jiannan", ""], ["Wu", "Eugene", ""]]}, {"id": "1704.02855", "submitter": "Ioannis Giannakopoulos", "authors": "Ioannis Giannakopoulos, Dimitrios Tsoumakos and Nectarios Koziris", "title": "A Decision Tree Based Approach Towards Adaptive Profiling of Distributed\n  Applications", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The adoption of the distributed paradigm has allowed applications to increase\ntheir scalability, robustness and fault tolerance, but it has also complicated\ntheir structure, leading to an exponential growth of the applications'\nconfiguration space and increased difficulty in predicting their performance.\nIn this work, we describe a novel, automated profiling methodology that makes\nno assumptions on application structure. Our approach utilizes oblique Decision\nTrees in order to recursively partition an application's configuration space in\ndisjoint regions, choose a set of representative samples from each subregion\naccording to a defined policy and return a model for the entire space as a\ncomposition of linear models over each subregion. An extensive evaluation over\nreal-life applications and synthetic performance functions showcases that our\nscheme outperforms other state-of-the-art profiling methodologies. It\nparticularly excels at reflecting abnormalities and discontinuities of the\nperformance function, allowing the user to influence the sampling policy based\non the modeling accuracy and the space coverage.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 13:46:58 GMT"}, {"version": "v2", "created": "Sun, 21 May 2017 13:36:45 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Giannakopoulos", "Ioannis", ""], ["Tsoumakos", "Dimitrios", ""], ["Koziris", "Nectarios", ""]]}, {"id": "1704.02955", "submitter": "Yves van Gennip", "authors": "Yves van Gennip, Blake Hunter, Anna Ma, Daniel Moyer, Ryan de Vera,\n  Andrea L. Bertozzi", "title": "Unsupervised record matching with noisy and incomplete data", "comments": "24 pages, 17 figures; this second version has various significant\n  updates compared to version 1 as a result of the peer review process prior to\n  journal publication; we thank the reviewers for their comments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of duplicate detection in noisy and incomplete data:\ngiven a large data set in which each record has multiple entries (attributes),\ndetect which distinct records refer to the same real world entity. This task is\ncomplicated by noise (such as misspellings) and missing data, which can lead to\nrecords being different, despite referring to the same entity. Our method\nconsists of three main steps: creating a similarity score between records,\ngrouping records together into \"unique entities\", and refining the groups. We\ncompare various methods for creating similarity scores between noisy records,\nconsidering different combinations of string matching, term frequency-inverse\ndocument frequency methods, and n-gram techniques. In particular, we introduce\na vectorized soft term frequency-inverse document frequency method, with an\noptional refinement step. We also discuss two methods to deal with missing data\nin computing similarity scores.\n  We test our method on the Los Angeles Police Department Field Interview Card\ndata set, the Cora Citation Matching data set, and two sets of restaurant\nreview data. The results show that the methods that use words as the basic\nunits are preferable to those that use 3-grams. Moreover, in some (but\ncertainly not all) parameter ranges soft term frequency-inverse document\nfrequency methods can outperform the standard term frequency-inverse document\nfrequency method. The results also confirm that our method for automatically\ndetermining the number of groups typically works well in many cases and allows\nfor accurate results in the absence of a priori knowledge of the number of\nunique entities in the data set.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 17:05:40 GMT"}, {"version": "v2", "created": "Mon, 30 Apr 2018 10:20:39 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["van Gennip", "Yves", ""], ["Hunter", "Blake", ""], ["Ma", "Anna", ""], ["Moyer", "Daniel", ""], ["de Vera", "Ryan", ""], ["Bertozzi", "Andrea L.", ""]]}, {"id": "1704.02996", "submitter": "Rathijit Sen", "authors": "Rathijit Sen, Jianqiao Zhu, Jignesh M. Patel, and Somesh Jha", "title": "ROSA: R Optimizations with Static Analysis", "comments": "A talk on this work will be presented at RIOT 2017 (3rd Workshop on R\n  Implementation, Optimization and Tooling)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  R is a popular language and programming environment for data scientists. It\nis increasingly co-packaged with both relational and Hadoop-based data\nplatforms and can often be the most dominant computational component in data\nanalytics pipelines. Recent work has highlighted inefficiencies in executing R\nprograms, both in terms of execution time and memory requirements, which in\npractice limit the size of data that can be analyzed by R. This paper presents\nROSA, a static analysis framework to improve the performance and space\nefficiency of R programs. ROSA analyzes input programs to determine program\nproperties such as reaching definitions, live variables, aliased variables, and\ntypes of variables. These inferred properties enable program transformations\nsuch as C++ code translation, strength reduction, vectorization, code motion,\nin addition to interpretive optimizations such as avoiding redundant object\ncopies and performing in-place evaluations. An empirical evaluation shows\nsubstantial reductions by ROSA in execution time and memory consumption over\nboth CRAN R and Microsoft R Open.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 18:08:36 GMT"}, {"version": "v2", "created": "Mon, 3 Jul 2017 16:54:03 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Sen", "Rathijit", ""], ["Zhu", "Jianqiao", ""], ["Patel", "Jignesh M.", ""], ["Jha", "Somesh", ""]]}, {"id": "1704.03022", "submitter": "Thibault Sellam", "authors": "Haoci Zhang, Thibault Sellam and Eugene Wu", "title": "Precision Interfaces", "comments": null, "journal-ref": "Proceedings of the 2nd Workshop on Human-In-the-Loop Data\n  Analytics HILDA'17; 10:1--10:6 (2017)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building interactive tools to support data analysis is hard because it is not\nalways clear what to build and how to build it. To address this problem, we\npresent Precision Interfaces, a semi-automatic system to generate task-specific\ndata analytics interfaces. Precision Interface can turn a log of executed\nprograms into an interface, by identifying micro-variations between the\nprograms and mapping them to interface components. This paper focuses on SQL\nquery logs, but we can generalize the approach to other languages. Our system\noperates in two steps: it first build an interaction graph, which describes how\nthe queries can be transformed into each other. Then, it finds a set of UI\ncomponents that covers a maximal number of transformations. To restrict the\ndomain of changes to be detected, our system uses a domain-specific language,\nPILang. We give a full description of Precision Interface's components,\nshowcase an early prototype on real program logs and discuss future research\nopportunities.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 19:15:44 GMT"}, {"version": "v2", "created": "Fri, 30 Jun 2017 15:33:34 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Zhang", "Haoci", ""], ["Sellam", "Thibault", ""], ["Wu", "Eugene", ""]]}, {"id": "1704.03421", "submitter": "Malika Bendechache", "authors": "Malika Bendechache, Nhien-An Le-Khac, M-Tahar Kechadi", "title": "Efficient Large Scale Clustering based on Data Partitioning", "comments": "10 pages", "journal-ref": "Data Science and Advanced Analytics (DSAA), 2016 IEEE\n  International Conference on, 612--621, 2016", "doi": "10.1109/DSAA.2016.70", "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering techniques are very attractive for extracting and identifying\npatterns in datasets. However, their application to very large spatial datasets\npresents numerous challenges such as high-dimensionality data, heterogeneity,\nand high complexity of some algorithms. For instance, some algorithms may have\nlinear complexity but they require the domain knowledge in order to determine\ntheir input parameters. Distributed clustering techniques constitute a very\ngood alternative to the big data challenges (e.g.,Volume, Variety, Veracity,\nand Velocity). Usually these techniques consist of two phases. The first phase\ngenerates local models or patterns and the second one tends to aggregate the\nlocal results to obtain global models. While the first phase can be executed in\nparallel on each site and, therefore, efficient, the aggregation phase is\ncomplex, time consuming and may produce incorrect and ambiguous global clusters\nand therefore incorrect models. In this paper we propose a new distributed\nclustering approach to deal efficiently with both phases, generation of local\nresults and generation of global models by aggregation. For the first phase,\nour approach is capable of analysing the datasets located in each site using\ndifferent clustering techniques. The aggregation phase is designed in such a\nway that the final clusters are compact and accurate while the overall process\nis efficient in time and memory allocation. For the evaluation, we use two\nwell-known clustering algorithms, K-Means and DBSCAN. One of the key outputs of\nthis distributed clustering technique is that the number of global clusters is\ndynamic, no need to be fixed in advance. Experimental results show that the\napproach is scalable and produces high quality results.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 17:05:01 GMT"}, {"version": "v2", "created": "Mon, 26 Feb 2018 15:23:31 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Bendechache", "Malika", ""], ["Le-Khac", "Nhien-An", ""], ["Kechadi", "M-Tahar", ""]]}, {"id": "1704.03520", "submitter": "Niek Tax", "authors": "Felix Mannhardt and Niek Tax", "title": "Unsupervised Event Abstraction using Pattern Abstraction and Local\n  Process Models", "comments": "Accepted at Enabling Business Transformation by Business Process\n  Modeling, Development, and Support Working Conference 2017 (BPMDS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process mining analyzes business processes based on events stored in event\nlogs. However, some recorded events may correspond to activities on a very low\nlevel of abstraction. When events are recorded on a too low level of\ngranularity, process discovery methods tend to generate overgeneralizing\nprocess models. Grouping low-level events to higher level activities, i.e.,\nevent abstraction, can be used to discover better process models. Existing\nevent abstraction methods are mainly based on common sub-sequences and\nclustering techniques. In this paper, we propose to first discover local\nprocess models and then use those models to lift the event log to a higher\nlevel of abstraction. Our conjecture is that process models discovered on the\nobtained high-level event log return process models of higher quality: their\nfitness and precision scores are more balanced. We show this with preliminary\nresults on several real-life event logs.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 20:08:14 GMT"}, {"version": "v2", "created": "Tue, 16 May 2017 16:48:18 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Mannhardt", "Felix", ""], ["Tax", "Niek", ""]]}, {"id": "1704.03978", "submitter": "Sheng Wang", "authors": "Sheng Wang, Zhifeng Bao, J. Shane Culpepper, Timos Sellis, Gao Cong", "title": "Reverse k Nearest Neighbor Search over Trajectories", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GPS enables mobile devices to continuously provide new opportunities to\nimprove our daily lives. For example, the data collected in applications\ncreated by Uber or Public Transport Authorities can be used to plan\ntransportation routes, estimate capacities, and proactively identify low\ncoverage areas. In this paper, we study a new kind of query-Reverse k Nearest\nNeighbor Search over Trajectories (RkNNT), which can be used for route planning\nand capacity estimation. Given a set of existing routes DR, a set of passenger\ntransitions DT, and a query route Q, a RkNNT query returns all transitions that\ntake Q as one of its k nearest travel routes. To solve the problem, we first\ndevelop an index to handle dynamic trajectory updates, so that the most\nup-to-date transition data are available for answering a RkNNT query. Then we\nintroduce a filter refinement framework for processing RkNNT queries using the\nproposed indexes. Next, we show how to use RkNNT to solve the optimal route\nplanning problem MaxRkNNT (MinRkNNT), which is to search for the optimal route\nfrom a start location to an end location that could attract the maximum (or\nminimum) number of passengers based on a pre-defined travel distance threshold.\nExperiments on real datasets demonstrate the efficiency and scalability of our\napproaches. To the best of our best knowledge, this is the first work to study\nthe RkNNT problem for route planning.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 03:14:00 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Wang", "Sheng", ""], ["Bao", "Zhifeng", ""], ["Culpepper", "J. Shane", ""], ["Sellis", "Timos", ""], ["Cong", "Gao", ""]]}, {"id": "1704.04301", "submitter": "Nhien-An Le-Khac", "authors": "Nhien-An Le-Khac, Sammer Markos, M-Tahar Kechadi", "title": "A Tree-based Approach for Detecting Redundant Business Rules in very\n  Large Financial Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Net Asset Value (NAV) calculation and validation is the principle task of a\nfund administrator. If the NAV of a fund is calculated incorrectly then there\nis huge impact on the fund administrator; such as monetary compensation,\nreputational loss, or loss of business. In general, these companies use the\nsame methodology to calculate the NAV of a fund, however the type of fund in\nquestion dictates the set of business rules used to validate this. Today, most\nFund Administrators depend heavily on human resources due to the lack of an\nautomated standardized solutions, however due to economic climate and the need\nfor efficiency and costs reduction many banks are now looking for an automated\nsolution with minimal human interaction; i.e., straight through processing\n(STP). Within the scope of a collaboration project that focuses on building an\noptimal solution for NAV validation, in this paper, we will present a new\napproach for detecting correlated business rules. We also show how we evaluate\nthis approach using real-world financial data.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 23:26:26 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Le-Khac", "Nhien-An", ""], ["Markos", "Sammer", ""], ["Kechadi", "M-Tahar", ""]]}, {"id": "1704.04302", "submitter": "Nhien-An Le-Khac", "authors": "Nhien-An Le-Khac, M-Tahar Kechadi", "title": "On a Distributed Approach for Density-based Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient extraction of useful knowledge from these data is still a\nchallenge, mainly when the data is distributed, heterogeneous and of different\nquality depending on its corresponding local infrastructure. To reduce the\noverhead cost, most of the existing distributed clustering approaches generate\nglobal models by aggregating local results obtained on each individual node.\nThe complexity and quality of solutions depend highly on the quality of the\naggregation. In this respect, we proposed for distributed density-based\nclustering that both reduces the communication overheads due to the data\nexchange and improves the quality of the global models by considering the\nshapes of local clusters. From preliminary results we show that this algorithm\nis very promising.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 23:34:43 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Le-Khac", "Nhien-An", ""], ["Kechadi", "M-Tahar", ""]]}, {"id": "1704.04599", "submitter": "Arkan Al-Hamodi", "authors": "Arkan A. G. Al-Hamodi, Songfeng Lu", "title": "A novel approach for fast mining frequent itemsets use N-list structure\n  based on MapReduce", "comments": "11 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frequent Pattern Mining is a one field of the most significant topics in data\nmining. In recent years, many algorithms have been proposed for mining frequent\nitemsets. A new algorithm has been presented for mining frequent itemsets based\non N-list data structure called Prepost algorithm. The Prepost algorithm is\nenhanced by implementing compact PPC-tree with the general tree. Prepost\nalgorithm can only find a frequent itemsets with required (pre-order and\npost-order) for each node. In this chapter, we improved prepost algorithm based\non Hadoop platform (HPrepost), proposed using the Mapreduce programming model.\nThe main goals of proposed method are efficient mining frequent itemsets\nrequiring less running time and memory usage. We have conduct experiments for\nthe proposed scheme to compare with another algorithms. With dense datasets,\nwhich have a large average length of transactions, HPrepost is more effective\nthan frequent itemsets algorithms in terms of execution time and memory usage\nfor all min-sup. Generally, our algorithm outperforms algorithms in terms of\nruntime and memory usage with small thresholds and large datasets.\n", "versions": [{"version": "v1", "created": "Sat, 15 Apr 2017 07:23:40 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Al-Hamodi", "Arkan A. G.", ""], ["Lu", "Songfeng", ""]]}, {"id": "1704.04738", "submitter": "Dong Deng", "authors": "Dong Deng, Albert Kim, Samuel Madden and Michael Stonebraker", "title": "SilkMoth: An Efficient Method for Finding Related Sets with Maximum\n  Matching Constraints", "comments": "VLDB 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining if two sets are related - that is, if they have similar values or\nif one set contains the other - is an important problem with many applications\nin data cleaning, data integration, and information retrieval. A particularly\npopular metric that has been proposed is to measure the relatedness of two sets\nby treating the elements as vertices of a bipartite graph and calculating the\nscore of the maximum matching pairing between elements. Compared to other\nmetrics which require exact matchings between elements, this metric uses a\nsimilarity function to compare elements between the two sets, making it robust\nto small dissimilarities in elements and more useful for real-world, dirty\ndata. Unfortunately, the metric suffers from expensive computational cost,\ntaking O(n^3) time, where n is the number of elements in sets, for each\nset-to-set comparison. Thus for applications which try to search for all\npairings of related sets in a brute-force manner, the runtime becomes\nunacceptably large.\n  To address this challenge, we developed SilkMoth, a system capable of rapidly\ndiscovering related set pairs in collections of sets. Internally, SilkMoth\ncreates a signature for each set, with the property that any other set which is\nrelated must match the signature. SilkMoth then uses these signatures to prune\nthe search space, so only sets which match the signatures are left as\ncandidates. Finally, SilkMoth applies the maximum matching metric on remaining\ncandidates to verify which of these candidates are truly related sets. Thus, a\ncontribution of this paper is the characterization of the space of signatures\nwhich enable this property. We show that selecting the optimal signature in\nthis space is NP-complete, and based on insights from the characterization of\nthe space, we propose two novel filters which help to prune the candidates\nfurther before verification.\n", "versions": [{"version": "v1", "created": "Sun, 16 Apr 2017 09:03:14 GMT"}, {"version": "v2", "created": "Tue, 16 May 2017 18:10:51 GMT"}, {"version": "v3", "created": "Mon, 18 Dec 2017 02:11:58 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Deng", "Dong", ""], ["Kim", "Albert", ""], ["Madden", "Samuel", ""], ["Stonebraker", "Michael", ""]]}, {"id": "1704.05136", "submitter": "Leopoldo Bertossi", "authors": "Leopoldo Bertossi", "title": "The Causality/Repair Connection in Databases: Causality-Programs", "comments": "To appear in Proc. SUM'17 as short paper, 7-pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, answer-set programs that specify repairs of databases are used\nas a basis for solving computational and reasoning problems about causes for\nquery answers from databases.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 21:58:45 GMT"}, {"version": "v2", "created": "Mon, 26 Jun 2017 17:05:39 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Bertossi", "Leopoldo", ""]]}, {"id": "1704.05272", "submitter": "Miguel Martinez Pedreira", "authors": "M Martinez Pedreira and C Grigoras (for the ALICE Collaboration)", "title": "Scalable Global Grid catalogue for LHC Run3 and beyond", "comments": "Proceedings of the 22nd International Conference on Computing in High\n  Energy and Nuclear Physics, CHEP 2016, 10-14 October 2016, San Francisco.\n  Submitted to Journal of Physics: Conference Series (JPCS)", "journal-ref": null, "doi": "10.1088/1742-6596/898/9/092006", "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The AliEn (ALICE Environment) file catalogue is a global unique namespace\nproviding mapping between a UNIX-like logical name structure and the\ncorresponding physical files distributed over 80 storage elements worldwide.\nPowerful search tools and hierarchical metadata information are integral parts\nof the system and are used by the Grid jobs as well as local users to store and\naccess all files on the Grid storage elements. The catalogue has been in\nproduction since 2005 and over the past 11 years has grown to more than 2\nbillion logical file names. The backend is a set of distributed relational\ndatabases, ensuring smooth growth and fast access. Due to the anticipated fast\nfuture growth, we are looking for ways to enhance the performance and\nscalability by simplifying the catalogue schema while keeping the functionality\nintact. We investigated different backend solutions, such as distributed key\nvalue stores, as replacement for the relational database. This contribution\ncovers the architectural changes in the system, together with the technology\nevaluation, benchmark results and conclusions.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 11:18:21 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Pedreira", "M Martinez", "", "for the ALICE Collaboration"], ["Grigoras", "C", "", "for the ALICE Collaboration"]]}, {"id": "1704.05617", "submitter": "Chun-Nan Hsu", "authors": "Sanjeev Shenoy, Tsung-Ting Kuo, Rodney Gabriel, Julian McAuley and\n  Chun-Nan Hsu", "title": "Deduplication in a massive clinical note dataset", "comments": "Extended from the Master project report of Sanjeev Shenoy, Department\n  of Computer Science and Engineering, University of California, San Diego.\n  June 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Duplication, whether exact or partial, is a common issue in many datasets. In\nclinical notes data, duplication (and near duplication) can arise for many\nreasons, such as the pervasive use of templates, copy-pasting, or notes being\ngenerated by automated procedures. A key challenge in removing such near\nduplicates is the size of such datasets; our own dataset consists of more than\n10 million notes. To detect and correct such duplicates requires algorithms\nthat both accurate and highly scalable. We describe a solution based on\nMinhashing with Locality Sensitive Hashing. In this paper, we present the\ntheory behind this method and present a database-inspired approach to make the\nmethod scalable. We also present a clustering technique using disjoint sets to\nproduce dense clusters, which speeds up our algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 05:33:21 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Shenoy", "Sanjeev", ""], ["Kuo", "Tsung-Ting", ""], ["Gabriel", "Rodney", ""], ["McAuley", "Julian", ""], ["Hsu", "Chun-Nan", ""]]}, {"id": "1704.05730", "submitter": "Evaggelia Pitoura", "authors": "Evaggelia Pitoura, Panayiotis Tsaparas, Giorgos Flouris, Irini\n  Fundulaki, Panagiotis Papadakos, Serge Abiteboul, Gerhard Weikum", "title": "On Measuring Bias in Online Information", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bias in online information has recently become a pressing issue, with search\nengines, social networks and recommendation services being accused of\nexhibiting some form of bias. In this vision paper, we make the case for a\nsystematic approach towards measuring bias. To this end, we discuss formal\nmeasures for quantifying the various types of bias, we outline the system\ncomponents necessary for realizing them, and we highlight the related research\nchallenges and open problems.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 13:41:50 GMT"}, {"version": "v2", "created": "Tue, 3 Oct 2017 07:52:26 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Pitoura", "Evaggelia", ""], ["Tsaparas", "Panayiotis", ""], ["Flouris", "Giorgos", ""], ["Fundulaki", "Irini", ""], ["Papadakos", "Panagiotis", ""], ["Abiteboul", "Serge", ""], ["Weikum", "Gerhard", ""]]}, {"id": "1704.05860", "submitter": "Junye Wang", "authors": "Mihal Miu, Xiaokun Zhang, M. Ali Akber Dewan, Junye Wang", "title": "Aggregation and visualization of spatial data with application to\n  classification of land use and land cover", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aggregation and visualization of geographical data are an important part of\nenvironmental data mining, environmental modelling, and agricultural\nmanagement. However, it is difficult to aggregate geospatial data of the\nvarious formats, such as maps, census and survey data. This paper presents a\nframework named PlaniSphere, which can aggregate the various geospatial\ndatasets, and synthesizes raw data. We developed an algorithm in PlaniSphere to\naggregate remote sensing images with census data for classification and\nvisualization of land use and land cover (LULC). The results show that the\nframework is able to classify geospatial data sets of LULC from multiple\nformats. National census data sets can be used for calibration of remote\nsensing LULC classifications. This provides a new approach for the\nclassification of remote sensing data. This approach proposed in this paper\nshould be useful for LULC classification in environmental spatial analysis.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 18:01:29 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Miu", "Mihal", ""], ["Zhang", "Xiaokun", ""], ["Dewan", "M. Ali Akber", ""], ["Wang", "Junye", ""]]}, {"id": "1704.06860", "submitter": "Hien To", "authors": "Hien To, Cyrus Shahabi", "title": "Location Privacy in Spatial Crowdsourcing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial crowdsourcing (SC) is a new platform that engages individuals in\ncollecting and analyzing environmental, social and other spatiotemporal\ninformation. With SC, requesters outsource their spatiotemporal tasks to a set\nof workers, who will perform the tasks by physically traveling to the tasks'\nlocations. This chapter identifies privacy threats toward both workers and\nrequesters during the two main phases of spatial crowdsourcing, tasking and\nreporting. Tasking is the process of identifying which tasks should be assigned\nto which workers. This process is handled by a spatial crowdsourcing server\n(SC-server). The latter phase is reporting, in which workers travel to the\ntasks' locations, complete the tasks and upload their reports to the SC-server.\nThe challenge is to enable effective and efficient tasking as well as reporting\nin SC without disclosing the actual locations of workers (at least until they\nagree to perform a task) and the tasks themselves (at least to workers who are\nnot assigned to those tasks). This chapter aims to provide an overview of the\nstate-of-the-art in protecting users' location privacy in spatial\ncrowdsourcing. We provide a comparative study of a diverse set of solutions in\nterms of task publishing modes (push vs. pull), problem focuses (tasking and\nreporting), threats (server, requester and worker), and underlying technical\napproaches (from pseudonymity, cloaking, and perturbation to exchange-based and\nencryption-based techniques). The strengths and drawbacks of the techniques are\nhighlighted, leading to a discussion of open problems and future work.\n", "versions": [{"version": "v1", "created": "Sun, 23 Apr 2017 00:09:11 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["To", "Hien", ""], ["Shahabi", "Cyrus", ""]]}, {"id": "1704.06868", "submitter": "Hien To", "authors": "Luan Tran, Hien To, Liyue Fan, Cyrus Shahabi", "title": "A Real-Time Framework for Task Assignment in Hyperlocal Spatial\n  Crowdsourcing", "comments": "Acceptance date: March 2017, ACM Transactions on Intelligent Systems\n  and Technology (March 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial Crowdsourcing (SC) is a novel platform that engages individuals in\nthe act of collecting various types of spatial data. This method of data\ncollection can significantly reduce cost and turnover time, and is particularly\nuseful in urban environmental sensing, where traditional means fail to provide\nfine-grained field data. In this study, we introduce hyperlocal spatial\ncrowdsourcing, where all workers who are located within the spatiotemporal\nvicinity of a task are eligible to perform the task, e.g., reporting the\nprecipitation level at their area and time. In this setting, there is often a\nbudget constraint, either for every time period or for the entire campaign, on\nthe number of workers to activate to perform tasks. The challenge is thus to\nmaximize the number of assigned tasks under the budget constraint, despite the\ndynamic arrivals of workers and tasks. We introduce a taxonomy of several\nproblem variants, such as budget-per-time-period vs. budget-per-campaign and\nbinary-utility vs. distance-based-utility. We study the hardness of the task\nassignment problem in the offline setting and propose online heuristics which\nexploits the spatial and temporal knowledge acquired over time. Our experiments\nare conducted with spatial crowdsourcing workloads generated by the SCAWG tool\nand extensive results show the effectiveness and efficiency of our proposed\nsolutions.\n", "versions": [{"version": "v1", "created": "Sun, 23 Apr 2017 01:12:27 GMT"}, {"version": "v2", "created": "Wed, 26 Apr 2017 00:17:44 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Tran", "Luan", ""], ["To", "Hien", ""], ["Fan", "Liyue", ""], ["Shahabi", "Cyrus", ""]]}, {"id": "1704.07355", "submitter": "Fabien Andr\\'e", "authors": "Fabien Andr\\'e (Technicolor) and Anne-Marie Kermarrec (Inria) and\n  Nicolas Le Scouarnec (Technicolor)", "title": "Accelerated Nearest Neighbor Search with Quick ADC", "comments": "8 pages, 5 figures, published in Proceedings of ICMR'17, Bucharest,\n  Romania, June 06-09, 2017", "journal-ref": null, "doi": "10.1145/3078971.3078992", "report-no": null, "categories": "cs.CV cs.DB cs.IR cs.MM cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient Nearest Neighbor (NN) search in high-dimensional spaces is a\nfoundation of many multimedia retrieval systems. Because it offers low\nresponses times, Product Quantization (PQ) is a popular solution. PQ compresses\nhigh-dimensional vectors into short codes using several sub-quantizers, which\nenables in-RAM storage of large databases. This allows fast answers to NN\nqueries, without accessing the SSD or HDD. The key feature of PQ is that it can\ncompute distances between short codes and high-dimensional vectors using\ncache-resident lookup tables. The efficiency of this technique, named\nAsymmetric Distance Computation (ADC), remains limited because it performs many\ncache accesses.\n  In this paper, we introduce Quick ADC, a novel technique that achieves a 3 to\n6 times speedup over ADC by exploiting Single Instruction Multiple Data (SIMD)\nunits available in current CPUs. Efficiently exploiting SIMD requires\nalgorithmic changes to the ADC procedure. Namely, Quick ADC relies on two key\nmodifications of ADC: (i) the use 4-bit sub-quantizers instead of the standard\n8-bit sub-quantizers and (ii) the quantization of floating-point distances.\nThis allows Quick ADC to exceed the performance of state-of-the-art systems,\ne.g., it achieves a Recall@100 of 0.94 in 3.4 ms on 1 billion SIFT descriptors\n(128-bit codes).\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 17:49:37 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Andr\u00e9", "Fabien", "", "Technicolor"], ["Kermarrec", "Anne-Marie", "", "Inria"], ["Scouarnec", "Nicolas Le", "", "Technicolor"]]}, {"id": "1704.07405", "submitter": "Sabbir Ahmad", "authors": "Sabbir Ahmad, Rafi Kamal, Mohammed Eunus Ali, Jianzhong Qi, Peter\n  Scheuermann and Egemen Tanin", "title": "The Flexible Group Spatial Keyword Query", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new class of service for location based social networks, called\nthe Flexible Group Spatial Keyword Query, which enables a group of users to\ncollectively find a point of interest (POI) that optimizes an aggregate cost\nfunction combining both spatial distances and keyword similarities. In\naddition, our query service allows users to consider the tradeoffs between\nobtaining a sub-optimal solution for the entire group and obtaining an\noptimimized solution but only for a subgroup.\n  We propose algorithms to process three variants of the query: (i) the group\nnearest neighbor with keywords query, which finds a POI that optimizes the\naggregate cost function for the whole group of size n, (ii) the subgroup\nnearest neighbor with keywords query, which finds the optimal subgroup and a\nPOI that optimizes the aggregate cost function for a given subgroup size m (m\n<= n), and (iii) the multiple subgroup nearest neighbor with keywords query,\nwhich finds optimal subgroups and corresponding POIs for each of the subgroup\nsizes in the range [m, n]. We design query processing algorithms based on\nbranch-and-bound and best-first paradigms. Finally, we provide theoretical\nbounds and conduct extensive experiments with two real datasets which verify\nthe effectiveness and efficiency of the proposed algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 18:33:50 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Ahmad", "Sabbir", ""], ["Kamal", "Rafi", ""], ["Ali", "Mohammed Eunus", ""], ["Qi", "Jianzhong", ""], ["Scheuermann", "Peter", ""], ["Tanin", "Egemen", ""]]}, {"id": "1704.08101", "submitter": "Sebastiaan J. van Zelst", "authors": "Sebastiaan J. van Zelst, Boudewijn F. van Dongen, Wil M.P. van der\n  Aalst", "title": "Event Stream-Based Process Discovery using Abstract Representations", "comments": "Accepted for publication in \"Knowledge and Information Systems; \"\n  (Springer: http://link.springer.com/journal/10115)", "journal-ref": null, "doi": "10.1007/s10115-017-1060-2", "report-no": null, "categories": "cs.DB cs.AI cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of process discovery, originating from the area of process mining, is\nto discover a process model based on business process execution data. A\nmajority of process discovery techniques relies on an event log as an input. An\nevent log is a static source of historical data capturing the execution of a\nbusiness process. In this paper we focus on process discovery relying on online\nstreams of business process execution events. Learning process models from\nevent streams poses both challenges and opportunities, i.e. we need to handle\nunlimited amounts of data using finite memory and, preferably, constant time.\nWe propose a generic architecture that allows for adopting several classes of\nexisting process discovery techniques in context of event streams. Moreover, we\nprovide several instantiations of the architecture, accompanied by\nimplementations in the process mining tool-kit ProM (http://promtools.org).\nUsing these instantiations, we evaluate several dimensions of stream-based\nprocess discovery. The evaluation shows that the proposed architecture allows\nus to lift process discovery to the streaming domain.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 12:10:35 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["van Zelst", "Sebastiaan J.", ""], ["van Dongen", "Boudewijn F.", ""], ["van der Aalst", "Wil M. P.", ""]]}]