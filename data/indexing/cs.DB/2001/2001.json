[{"id": "2001.00315", "submitter": "Xiangyu Gao", "authors": "Dongjing Miao, Zhipeng Cai, Jianzhong Li, Xiangyu Gao, Xianmin Liu", "title": "Complexity and Efficient Algorithms for Data Inconsistency Evaluating\n  and Repairing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data inconsistency evaluating and repairing are major concerns in data\nquality management. As the basic computing task, optimal subset repair is not\nonly applied for cost estimation during the progress of database repairing, but\nalso directly used to derive the evaluation of database inconsistency.\nComputing an optimal subset repair is to find a minimum tuple set from an\ninconsistent database whose remove results in a consistent subset left. Tight\nbound on the complexity and efficient algorithms are still unknown. In this\npaper, we improve the existing complexity and algorithmic results, together\nwith a fast estimation on the size of optimal subset repair. We first\nstrengthen the dichotomy for optimal subset repair computation problem, we show\nthat it is not only APXcomplete, but also NPhard to approximate an optimal\nsubset repair with a factor better than $17/16$ for most cases. We second show\na $(2-0.5^{\\tiny\\sigma-1})$-approximation whenever given $\\sigma$ functional\ndependencies, and a $(2-\\eta_k+\\frac{\\eta_k}{k})$-approximation when an\n$\\eta_k$-portion of tuples have the $k$-quasi-Tur$\\acute{\\text{a}}$n property\nfor some $k>1$. We finally show a sublinear estimator on the size of optimal\n\\textit{S}-repair for subset queries, it outputs an estimation of a ratio\n$2n+\\epsilon n$ with a high probability, thus deriving an estimation of\nFD-inconsistency degree of a ratio $2+\\epsilon$. To support a variety of subset\nqueries for FD-inconsistency evaluation, we unify them as the\n$\\subseteq$-oracle which can answer membership-query, and return $p$ tuples\nuniformly sampled whenever given a number $p$. Experiments are conducted on\nrange queries as an implementation of $\\subseteq$-oracle, and results show the\nefficiency of our FD-inconsistency degree estimator.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 03:49:21 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 06:44:09 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Miao", "Dongjing", ""], ["Cai", "Zhipeng", ""], ["Li", "Jianzhong", ""], ["Gao", "Xiangyu", ""], ["Liu", "Xianmin", ""]]}, {"id": "2001.00338", "submitter": "Ryan Wisnesky", "authors": "Eric Daimler, Ryan Wisnesky", "title": "Informal Data Transformation Considered Harmful", "comments": "Proceedings paper for AAAI FSS-19: HAI19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we take the common position that AI systems are limited more by\nthe integrity of the data they are learning from than the sophistication of\ntheir algorithms, and we take the uncommon position that the solution to\nachieving better data integrity in the enterprise is not to clean and validate\ndata ex-post-facto whenever needed (the so-called data lake approach to data\nmanagement, which can lead to data scientists spending 80% of their time\ncleaning data), but rather to formally and automatically guarantee that data\nintegrity is preserved as it transformed (migrated, integrated, composed,\nqueried, viewed, etc) throughout the enterprise, so that data and programs that\ndepend on that data need not constantly be re-validated for every particular\nuse.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 06:26:36 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Daimler", "Eric", ""], ["Wisnesky", "Ryan", ""]]}, {"id": "2001.00432", "submitter": "Dominik Tomaszuk", "authors": "Dominik Tomaszuk, David Hyland-Wood", "title": "RDF 1.1: Knowledge Representation and Data Integration Language for the\n  Web", "comments": "26 pages, 4 figures", "journal-ref": "Symmetry 2020, 12, 84", "doi": "10.3390/sym12010084", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Resource Description Framework (RDF) can seen as a solution in today's\nlandscape of knowledge representation research. An RDF language has symmetrical\nfeatures because subjects and objects in triples can be interchangeably used.\nMoreover, the regularity and symmetry of the RDF language allow knowledge\nrepresentation that is easily processed by machines, and because its structure\nis similar to natural languages, it is reasonably readable for people. RDF\nprovides some useful features for generalized knowledge representation. Its\ndistributed nature, due to its identifier grounding in IRIs, naturally scales\nto the size of the Web. However, its use is often hidden from view and is,\ntherefore, one of the less well-known of the knowledge representation\nframeworks. Therefore, we summarise RDF v1.0 and v1.1 to broaden its audience\nwithin the knowledge representation community. This article reviews current\napproaches, tools, and applications for mapping from relational databases to\nRDF and from XML to RDF. We discuss RDF serializations, including formats with\nsupport for multiple graphs and we analyze RDF compression proposals. Finally,\nwe present a summarized formal definition of RDF 1.1 that provides additional\ninsights into the modeling of reification, blank nodes, and entailments.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 13:44:19 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Tomaszuk", "Dominik", ""], ["Hyland-Wood", "David", ""]]}, {"id": "2001.00688", "submitter": "Ruoyu Wang", "authors": "Ruoyu Wang (1 and 2), Xiaobo Hu (3), Daniel Sun (2, 4 and 5), Guoqiang\n  Li (1 and 4), Raymond Wong (2), Shiping Chen (5), Jianquan Liu (6) ((1)\n  Shanghai Jiao Tong University, (2) University of New South Wales, (3) Peking\n  University, (4) Enhitech Co. Ltd., (5) Data61, CSIRO, (6) Biometrics Research\n  Laboratories, NEC Corporation)", "title": "Statistical Detection of Collective Data Fraud", "comments": "6 pages, 6 figures and tables, submitted to ICME 2020", "journal-ref": "2020 IEEE International Conference on Multimedia and Expo (ICME),\n  London, United Kingdom, 2020, pp. 1-6", "doi": "10.1109/ICME46284.2020.9102889", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical divergence is widely applied in multimedia processing, basically\ndue to regularity and interpretable features displayed in data. However, in a\nbroader range of data realm, these advantages may no longer be feasible, and\ntherefore a more general approach is required. In data detection, statistical\ndivergence can be used as a similarity measurement based on collective\nfeatures. In this paper, we present a collective detection technique based on\nstatistical divergence. The technique extracts distribution similarities among\ndata collections, and then uses the statistical divergence to detect collective\nanomalies. Evaluation shows that it is applicable in the real world.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 02:05:08 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 02:59:09 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Wang", "Ruoyu", "", "1 and 2"], ["Hu", "Xiaobo", "", "2, 4 and 5"], ["Sun", "Daniel", "", "2, 4 and 5"], ["Li", "Guoqiang", "", "1 and 4"], ["Wong", "Raymond", ""], ["Chen", "Shiping", ""], ["Liu", "Jianquan", ""]]}, {"id": "2001.00888", "submitter": "Devin Petersohn", "authors": "Devin Petersohn, Stephen Macke, Doris Xin, William Ma, Doris Lee,\n  Xiangxi Mo, Joseph E. Gonzalez, Joseph M. Hellerstein, Anthony D. Joseph,\n  Aditya Parameswaran", "title": "Towards Scalable Dataframe Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dataframes are a popular abstraction to represent, prepare, and analyze data.\nDespite the remarkable success of dataframe libraries in Rand Python,\ndataframes face performance issues even on moderately large datasets. Moreover,\nthere is significant ambiguity regarding dataframe semantics. In this paper we\nlay out a vision and roadmap for scalable dataframe systems. To demonstrate the\npotential in this area, we report on our experience building MODIN, a scaled-up\nimplementation of the most widely-used and complex dataframe API today,\nPython's pandas. With pandas as a reference, we propose a simple data model and\nalgebra for dataframes to ground discussion in the field. Given this\nfoundation, we lay out an agenda of open research opportunities where the\ndistinct features of dataframes will require extending the state of the art in\nmany dimensions of data management. We discuss the implications of signature\ndata-frame features including flexible schemas, ordering, row/column\nequivalence, and data/metadata fluidity, as well as the piecemeal,\ntrial-and-error-based approach to interacting with dataframes.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 16:56:55 GMT"}, {"version": "v2", "created": "Mon, 6 Jan 2020 03:48:22 GMT"}, {"version": "v3", "created": "Mon, 13 Jan 2020 04:07:58 GMT"}, {"version": "v4", "created": "Tue, 2 Jun 2020 16:14:04 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Petersohn", "Devin", ""], ["Macke", "Stephen", ""], ["Xin", "Doris", ""], ["Ma", "William", ""], ["Lee", "Doris", ""], ["Mo", "Xiangxi", ""], ["Gonzalez", "Joseph E.", ""], ["Hellerstein", "Joseph M.", ""], ["Joseph", "Anthony D.", ""], ["Parameswaran", "Aditya", ""]]}, {"id": "2001.00975", "submitter": "Mahmoud Barhamgi", "authors": "Mahmoud Barhamgi, Charith Perera, Chia-Mu Yu, Djamal Benslimane, David\n  Camacho and Christine Bonnet", "title": "Privacy in Data Service Composition", "comments": null, "journal-ref": null, "doi": "10.1109/TSC.2019.2963309", "report-no": null, "categories": "cs.DB cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modern information systems different information features, about the same\nindividual, are often collected and managed by autonomous data collection\nservices that may have different privacy policies. Answering many end-users'\nlegitimate queries requires the integration of data from multiple such\nservices. However, data integration is often hindered by the lack of a trusted\nentity, often called a mediator, with which the services can share their data\nand delegate the enforcement of their privacy policies. In this paper, we\npropose a flexible privacy-preserving data integration approach for answering\ndata integration queries without the need for a trusted mediator. In our\napproach, services are allowed to enforce their privacy policies locally. The\nmediator is considered to be untrusted, and only has access to encrypted\ninformation to allow it to link data subjects across the different services.\nServices, by virtue of a new privacy requirement, dubbed k-Protection, limiting\nprivacy leaks, cannot infer information about the data held by each other.\nEnd-users, in turn, have access to privacy-sanitized data only. We evaluated\nour approach using an example and a real dataset from the healthcare\napplication domain. The results are promising from both the privacy\npreservation and the performance perspectives.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 20:21:45 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Barhamgi", "Mahmoud", ""], ["Perera", "Charith", ""], ["Yu", "Chia-Mu", ""], ["Benslimane", "Djamal", ""], ["Camacho", "David", ""], ["Bonnet", "Christine", ""]]}, {"id": "2001.01174", "submitter": "Dongfang Zhao", "authors": "Xinying Wang, Olamide Timothy Tawose, Feng Yan, Dongfang Zhao", "title": "Distributed Nonblocking Commit Protocols for Many-Party Cross-Blockchain\n  Transactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interoperability across multiple blockchains would play a critical role\nin future blockchain-based data management paradigm. Existing techniques either\nwork only for two blockchains or requires a centralized component to govern the\ncross-blockchain transaction execution, neither of which would meet the\nscalability requirement. This paper proposes a new distributed commit protocol,\nnamely \\textit{cross-blockchain transaction} (CBT), for conducting transactions\nacross an arbitrary number of blockchains without any centralized component.\nThe key idea of CBT is to extend the two-phase commit protocol with a heartbeat\nmechanism to ensure the liveness of CBT without introducing additional nodes or\nblockchains. We have implemented CBT and compared it to the state-of-the-art\nprotocols, demonstrating CBT's low overhead (3.6\\% between two blockchains,\nless than $1\\%$ among 32 or more blockchains) and high scalability (linear\nscalability on up to 64-blockchain transactions). In addition, we developed a\ngraphic user interface for users to virtually monitor the status of the\ncross-blockchain transactions.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2020 05:58:41 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Wang", "Xinying", ""], ["Tawose", "Olamide Timothy", ""], ["Yan", "Feng", ""], ["Zhao", "Dongfang", ""]]}, {"id": "2001.01491", "submitter": "Waqas Ahmed", "authors": "Saira Khan, Khalid Iqbal, Safi Faizullah, Muhammad Fahad, Jawad Ali,\n  Waqas Ahmed", "title": "Clustering based Privacy Preserving of Big Data using Fuzzification and\n  Anonymization Operation", "comments": "08 Page, 07 figures", "journal-ref": "International Journal of Advanced Computer Science and\n  Applications, Volume 10 Issue 12, 2019", "doi": "10.14569/IJACSA.2019.0101239", "report-no": null, "categories": "cs.DB cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Big Data is used by data miner for analysis purpose which may contain\nsensitive information. During the procedures it raises certain privacy\nchallenges for researchers. The existing privacy preserving methods use\ndifferent algorithms that results into limitation of data reconstruction while\nsecuring the sensitive data. This paper presents a clustering based privacy\npreservation probabilistic model of big data to secure sensitive\ninformation..model to attain minimum perturbation and maximum privacy. In our\nmodel, sensitive information is secured after identifying the sensitive data\nfrom data clusters to modify or generalize it.The resulting dataset is analysed\nto calculate the accuracy level of our model in terms of hidden data, lossed\ndata as result of reconstruction. Extensive experiements are carried out in\norder to demonstrate the results of our proposed model. Clustering based\nPrivacy preservation of individual data in big data with minimum perturbation\nand successful reconstruction highlights the significance of our model in\naddition to the use of standard performance evaluation measures.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 11:31:12 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Khan", "Saira", ""], ["Iqbal", "Khalid", ""], ["Faizullah", "Safi", ""], ["Fahad", "Muhammad", ""], ["Ali", "Jawad", ""], ["Ahmed", "Waqas", ""]]}, {"id": "2001.01902", "submitter": "Yiru Chen", "authors": "Yiru Chen, Eugene Wu", "title": "Monte Carlo Tree Search for Generating Interactive Data Analysis\n  Interfaces", "comments": "4 pages, 6 figures, The AAAI-20 Workshop on Intelligent Process\n  Automation (IPA-20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive tools like user interfaces help democratize data access for\nend-users by hiding underlying programming details and exposing the necessary\nwidget interface to users. Since customized interfaces are costly to build,\nautomated interface generation is desirable. SQL is the dominant way to analyze\ndata and there already exists logs to analyze data. Previous work proposed a\nsyntactic approach to analyze structural changes in SQL query logs and\nautomatically generates a set of widgets to express the changes. However, they\ndo not consider layout usability and the sequential order of queries in the\nlog. We propose to adopt Monte Carlo Tree Search(MCTS) to search for the\noptimal interface that accounts for hierarchical layout as well as the\nusability in terms of how easy to express the query log.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 06:01:30 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 03:18:57 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Chen", "Yiru", ""], ["Wu", "Eugene", ""]]}, {"id": "2001.02172", "submitter": "Philipp G\\\"otze", "authors": "Philipp G\\\"otze, Arun Kumar Tharanatha, Kai-Uwe Sattler", "title": "Data Structure Primitives on Persistent Memory: An Evaluation", "comments": "13 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Persistent Memory (PMem), as already available, e.g., with Intel Optane DC\nPersistent Memory, represents a very promising, next-generation memory solution\nwith a significant impact on database architectures. Several data structures\nfor this new technology and its properties have already been proposed. However,\nprimarily only complete structures are presented and evaluated. Thus, the\nimplications of the individual ideas and PMem features are concealed.\nTherefore, in this paper, we disassemble the structures presented so far,\nidentify their underlying design primitives, and assign them to appropriate\ndesign goals regarding PMem. As a result of our comprehensive experiments on\nreal PM hardware, we can reveal the trade-offs of the primitives for various\naccess patterns. This allowed us to pinpoint their best use cases as well as\nvulnerabilities. Besides our general insights regarding PMem-based data\nstructure design, we also discovered new combinations not examined in the\nliterature so far.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 16:56:23 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 13:51:59 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["G\u00f6tze", "Philipp", ""], ["Tharanatha", "Arun Kumar", ""], ["Sattler", "Kai-Uwe", ""]]}, {"id": "2001.02299", "submitter": "G\\'abor Sz\\'arnyas", "authors": "Renzo Angles, J\\'anos Benjamin Antal, Alex Averbuch, Peter Boncz, Orri\n  Erling, Andrey Gubichev, Vlad Haprian, Moritz Kaufmann, Josep Llu\\'is Larriba\n  Pey, Norbert Mart\\'inez, J\\'ozsef Marton, Marcus Paradies, Minh-Duc Pham,\n  Arnau Prat-P\\'erez, Mirko Spasi\\'c, Benjamin A. Steer, G\\'abor Sz\\'arnyas and\n  Jack Waudby", "title": "The LDBC Social Network Benchmark", "comments": "For the repository containing the source code of this technical\n  report, see https://github.com/ldbc/ldbc_snb_docs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PF cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Linked Data Benchmark Council's Social Network Benchmark (LDBC SNB) is an\neffort intended to test various functionalities of systems used for graph-like\ndata management. For this, LDBC SNB uses the recognizable scenario of operating\na social network, characterized by its graph-shaped data. LDBC SNB consists of\ntwo workloads that focus on different functionalities: the Interactive workload\n(interactive transactional queries) and the Business Intelligence workload\n(analytical queries). This document contains the definition of the Interactive\nWorkload and the first draft of the Business Intelligence Workload. This\nincludes a detailed explanation of the data used in the LDBC SNB benchmark, a\ndetailed description for all queries, and instructions on how to generate the\ndata and run the benchmark with the provided software.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 22:12:35 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 15:45:29 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Angles", "Renzo", ""], ["Antal", "J\u00e1nos Benjamin", ""], ["Averbuch", "Alex", ""], ["Boncz", "Peter", ""], ["Erling", "Orri", ""], ["Gubichev", "Andrey", ""], ["Haprian", "Vlad", ""], ["Kaufmann", "Moritz", ""], ["Pey", "Josep Llu\u00eds Larriba", ""], ["Mart\u00ednez", "Norbert", ""], ["Marton", "J\u00f3zsef", ""], ["Paradies", "Marcus", ""], ["Pham", "Minh-Duc", ""], ["Prat-P\u00e9rez", "Arnau", ""], ["Spasi\u0107", "Mirko", ""], ["Steer", "Benjamin A.", ""], ["Sz\u00e1rnyas", "G\u00e1bor", ""], ["Waudby", "Jack", ""]]}, {"id": "2001.02385", "submitter": "Bo Jiang", "authors": "Bo Jiang, Ming Li, Ravi Tandon", "title": "Local Information Privacy and Its Application to Privacy-Preserving Data\n  Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.IT cs.SY eess.SY math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study local information privacy (LIP), and design LIP based\nmechanisms for statistical aggregation while protecting users' privacy without\nrelying on a trusted third party. The notion of context-awareness is\nincorporated in LIP, which can be viewed as explicit modeling of the\nadversary's background knowledge. It enables the design of privacy-preserving\nmechanisms leveraging the prior distribution, which can potentially achieve a\nbetter utility-privacy tradeoff than context-free notions such as Local\nDifferential Privacy (LDP). We present an optimization framework to minimize\nthe mean square error in the data aggregation while protecting the privacy of\neach individual user's input data or a correlated latent variable while\nsatisfying LIP constraints. Then, we study two different types of applications:\n(weighted) summation and histogram estimation and derive the optimal\ncontext-aware data perturbation parameters for each case, based on randomized\nresponse type of mechanism. We further compare the utility-privacy tradeoff\nbetween LIP and LDP and theoretically explain why the incorporation of prior\nknowledge enlarges feasible regions of the perturbation parameters, which\nthereby leads to higher utility. We also extend the LIP-based privacy\nmechanisms to the more general case when exact prior knowledge is not\navailable. Finally, we validate our analysis by simulations using both\nsynthetic and real-world data. Results show that our LIP-based privacy\nmechanism provides better utility-privacy tradeoffs than LDP, and the advantage\nof LIP is even more significant when the prior distribution is more skewed.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 05:44:21 GMT"}, {"version": "v2", "created": "Sun, 29 Nov 2020 04:25:37 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Jiang", "Bo", ""], ["Li", "Ming", ""], ["Tandon", "Ravi", ""]]}, {"id": "2001.02562", "submitter": "Alessandro Berti Mr", "authors": "Alessandro Berti, Wil van der Aalst", "title": "Extracting Multiple Viewpoint Models from Relational Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Much time in process mining projects is spent on finding and understanding\ndata sources and extracting the event data needed. As a result, only a fraction\nof time is spent actually applying techniques to discover, control and predict\nthe business process. Moreover, current process mining techniques assume a\nsingle case notion. However, in reallife processes often different case notions\nare intertwined. For example, events of the same order handling process may\nrefer to customers, orders, order lines, deliveries, and payments. Therefore,\nwe propose to use Multiple Viewpoint (MVP) models that relate events through\nobjects and that relate activities through classes. The required event data are\nmuch closer to existing relational databases. MVP models provide a holistic\nview on the process, but also allow for the extraction of classical event logs\nusing different viewpoints. This way existing process mining techniques can be\nused for each viewpoint without the need for new data extractions and\ntransformations. We provide a toolchain allowing for the discovery of MVP\nmodels (annotated with performance and frequency information) from relational\ndatabases. Moreover, we demonstrate that classical process mining techniques\ncan be applied to any selected viewpoint.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 15:15:32 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Berti", "Alessandro", ""], ["van der Aalst", "Wil", ""]]}, {"id": "2001.02650", "submitter": "Benjamin Nguyen Ph.D.", "authors": "Benjamin Nguyen, Claude Castelluccia", "title": "Techniques d'anonymisation tabulaire : concepts et mise en oeuvre", "comments": "20 pages, in French", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this document, we present a state of the art of anonymization techniques\nfor classical tabular datasets. This article is geared towards a general public\nhaving some knowledge of mathematics and computer science, but with no need for\nspecific knowledge in anonymization. The objective of this document it to\nexplain anonymization concepts in order to be able to sanitize a dataset and\ncompute reindentification risk. The document contains a large number of\nexamples to help understand the calculations.\n  -----\n  Dans ce document, nous pr\\'esentons l'\\'etat de l'art des techniques\nd'anonymisation pour des bases de donn\\'ees classiques (i.e. des tables), \\`a\ndestination d'un public technique ayant une formation universitaire de base en\nmath\\'ematiques et informatique, mais non sp\\'ecialiste. L'objectif de ce\ndocument est d'expliquer les concepts permettant de r\\'ealiser une\nanonymisation de donn\\'ees tabulaires, et de calculer les risques de\nr\\'eidentification. Le document est largement compos\\'e d'exemples permettant\nau lecteur de comprendre comment mettre en oeuvre les calculs.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 17:50:09 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Nguyen", "Benjamin", ""], ["Castelluccia", "Claude", ""]]}, {"id": "2001.03010", "submitter": "Ida Mele", "authors": "Ida Mele, Nicola Tonellotto, Ophir Frieder, Raffaele Perego", "title": "Topical Result Caching in Web Search Engines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Caching search results is employed in information retrieval systems to\nexpedite query processing and reduce back-end server workload. Motivated by the\nobservation that queries belonging to different topics have different\ntemporal-locality patterns, we investigate a novel caching model called STD\n(Static-Topic-Dynamic cache). It improves traditional SDC (Static-Dynamic\nCache) that stores in a static cache the results of popular queries and manages\nthe dynamic cache with a replacement policy for intercepting the temporal\nvariations in the query stream. Our proposed caching scheme includes another\nlayer for topic-based caching, where the entries are allocated to different\ntopics (e.g., weather, education). The results of queries characterized by a\ntopic are kept in the fraction of the cache dedicated to it. This permits to\nadapt the cache-space utilization to the temporal locality of the various\ntopics and reduces cache misses due to those queries that are neither\nsufficiently popular to be in the static portion nor requested within\nshort-time intervals to be in the dynamic portion. We simulate different\nconfigurations for STD using two real-world query streams. Experiments\ndemonstrate that our approach outperforms SDC with an increase up to 3% in\nterms of hit rates, and up to 36% of gap reduction w.r.t. SDC from the\ntheoretical optimal caching algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 14:26:45 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Mele", "Ida", ""], ["Tonellotto", "Nicola", ""], ["Frieder", "Ophir", ""], ["Perego", "Raffaele", ""]]}, {"id": "2001.03128", "submitter": "Konstantinos Semertzidis", "authors": "Ioannis Kouvatis, Konstantinos Semertzidis, Maria Zerva, Evaggelia\n  Pitoura, Panayiotis Tsaparas", "title": "Forming Compatible Teams in Signed Networks", "comments": "In Proceedings of the 23rd International Conference on Extending\n  Database Technology (EDBT), 2020", "journal-ref": null, "doi": "10.5441/002/edbt.2020.33", "report-no": null, "categories": "cs.DS cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of team formation in a social network asks for a set of\nindividuals who not only have the required skills to perform a task but who can\nalso communicate effectively with each other. Existing work assumes that all\nlinks in a social network are positive, that is, they indicate friendship or\ncollaboration between individuals. However, it is often the case that the\nnetwork is signed, that is, it contains both positive and negative links,\ncorresponding to friend and foe relationships. Building on the concept of\nstructural balance, we provide definitions of compatibility between pairs of\nusers in a signed network, and algorithms for computing it. We then define the\nteam formation problem in signed networks, where we ask for a compatible team\nof individuals that can perform a task with small communication cost. We show\nthat the problem is NP-hard even when there are no communication cost\nconstraints, and we provide heuristic algorithms for solving it. We present\nexperimental results with real data to investigate the properties of the\ndifferent compatibility definitions, and the effectiveness of our algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 17:43:41 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 16:00:46 GMT"}, {"version": "v3", "created": "Wed, 22 Jan 2020 16:56:10 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Kouvatis", "Ioannis", ""], ["Semertzidis", "Konstantinos", ""], ["Zerva", "Maria", ""], ["Pitoura", "Evaggelia", ""], ["Tsaparas", "Panayiotis", ""]]}, {"id": "2001.03147", "submitter": "Carlos Baquero", "authors": "Ariel Shtul and Carlos Baquero and Paulo S\\'ergio Almeida", "title": "Age-Partitioned Bloom Filters", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bloom filters (BF) are widely used for approximate membership queries over a\nset of elements. BF variants allow removals, sets of unbounded size or querying\na sliding window over an unbounded stream. However, for this last case the best\ncurrent approaches are dictionary based (e.g., based on Cuckoo Filters or\nTinyTable), and it may seem that BF-based approaches will never be competitive\nto dictionary-based ones. In this paper we present Age-Partitioned Bloom\nFilters, a BF-based approach for duplicate detection in sliding windows that\nnot only is competitive in time-complexity, but has better space usage than\ncurrent dictionary-based approaches (e.g., SWAMP), at the cost of some moderate\nslack. APBFs retain the BF simplicity, unlike dictionary-based approaches,\nimportant for hardware-based implementations, and can integrate known\nimprovements such as double hashing or blocking. We present an Age-Partitioned\nBlocked Bloom Filter variant which can operate with 2-3 cache-line accesses per\ninsertion and around 2-4 per query, even for high accuracy filters.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 18:23:11 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Shtul", "Ariel", ""], ["Baquero", "Carlos", ""], ["Almeida", "Paulo S\u00e9rgio", ""]]}, {"id": "2001.03269", "submitter": "Sultan Almakdi", "authors": "Sultan Almakdi and Brajendra Panda", "title": "Designing a Bit-Based Model to Accelerate Query Processing Over\n  Encrypted Databases in Cloud", "comments": "19 pages, Accepted at CloudComp 2019 - 9th EAI International\n  Conference on Cloud Computing, Sydney, Australia, December 4-5, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Database users have started moving toward the use of cloud computing as a\nservice because it provides computation and storage needs at affordable prices.\nHowever, for most of the users, the concern of privacy plays a major role as\nthey cannot control data access once their data are outsourced, especially if\nthe cloud provider is curious about their data. Data encryption is an effective\nway to solve privacy concerns, but executing queries over encrypted data is a\nproblem that needs attention. In this research, we introduce a bit-based model\nto execute different relational algebra operators over encrypted databases at\nthe cloud without decrypting the data. To encrypt data, we use the randomized\nencryption algorithm (Advanced Encryption Standard-CBC) to provide the\nmaximum-security level. The idea is based on classifying attributes as\nsensitive and non-sensitive, where only sensitive attributes are encrypted. For\neach sensitive attribute, the table owner predefined the possible partition\ndomains on which the tuples will be encoded into bit vectors before the\nencryption. We store the bit vectors in an additional column(s) in the\nencrypted table in the cloud. We use those bits to retrieve only part of\nencrypted records that are candidates for a specific query. We implemented and\nevaluated our model and found that the proposed model is practical and success\nto minimize the range of the retrieved encrypted records to less than 30\npercent of the whole set of encrypted records in a table.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 01:13:46 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Almakdi", "Sultan", ""], ["Panda", "Brajendra", ""]]}, {"id": "2001.03284", "submitter": "Kwang Woo Nam", "authors": "Jang You Park, YongHee Jung, Wei Ding, Kwang Woo Nam", "title": "GeoCMS : Towards a Geo-Tagged Media Management System", "comments": null, "journal-ref": "Proceedings of FOSS4G 2019 Conference, Bucharest", "doi": "10.5194/isprs-archives-XLII-4-W14-185-2019", "report-no": null, "categories": "cs.DB cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose the design and implementation of the new geotagged\nmedia management system. A large amount of daily geo-tagged media data\ngenerated by user's smart phone, mobile device, dash cam and camera. Geotagged\nmedia, such as geovideos and geophotos, can be captured with spatial temporal\ninformation such as time, location, visible area, camera direction, moving\ndirection and visible distance information. Due to the increase in geo-tagged\nmultimedia data, the researches for efficient managing and mining geo-tagged\nmultimedia are newly expected to be a new area in database and data mining.\nThis paper proposes a geo-tagged media management system, so called Open\nGeoCMS(Geotagged media Contents Management System). Open GeoCMS is a new\nframework to manage geotagged media data on the web. Our framework supports\nvarious types which are for moving point, moving photo - a sequence of photos\nby a drone, moving double and moving video. Also, GeoCMS has the label viewer\nand editor system for photos and videos. The Open GeoCMS have been developed as\nan open source system.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 02:25:58 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Park", "Jang You", ""], ["Jung", "YongHee", ""], ["Ding", "Wei", ""], ["Nam", "Kwang Woo", ""]]}, {"id": "2001.03411", "submitter": "Xixi Lu", "authors": "Xixi Lu and Seyed Amin Tabatabaei and Mark Hoogendoorn and Hajo A.\n  Reijers", "title": "Trace Clustering on Very Large Event Data in Healthcare Using Frequent\n  Sequence Patterns", "comments": null, "journal-ref": "BPM 2019: Business Process Management pp 198-215", "doi": "10.1007/978-3-030-26619-6_14", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trace clustering has increasingly been applied to find homogenous process\nexecutions. However, current techniques have difficulties in finding a\nmeaningful and insightful clustering of patients on the basis of healthcare\ndata. The resulting clusters are often not in line with those of medical\nexperts, nor do the clusters guarantee to help return meaningful process maps\nof patients' clinical pathways. After all, a single hospital may conduct\nthousands of distinct activities and generate millions of events per year. In\nthis paper, we propose a novel trace clustering approach by using sample sets\nof patients provided by medical experts. More specifically, we learn frequent\nsequence patterns on a sample set, rank each patient based on the patterns, and\nuse an automated approach to determine the corresponding cluster. We find each\ncluster separately, while the frequent sequence patterns are used to discover a\nprocess map. The approach is implemented in ProM and evaluated using a large\ndata set obtained from a university medical center. The evaluation shows\nF1-scores of 0.7 for grouping kidney injury, 0.9 for diabetes, and 0.64 for\nhead/neck tumor, while the process maps show meaningful behavioral patterns of\nthe clinical pathways of these groups, according to the domain experts.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 13:07:41 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Lu", "Xixi", ""], ["Tabatabaei", "Seyed Amin", ""], ["Hoogendoorn", "Mark", ""], ["Reijers", "Hajo A.", ""]]}, {"id": "2001.03541", "submitter": "Amir Shaikhha", "authors": "Amir Shaikhha, Maximilian Schleich, Alexandru Ghita, Dan Olteanu", "title": "Multi-layer Optimizations for End-to-End Data Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of training machine learning models over\nmulti-relational data. The mainstream approach is to first construct the\ntraining dataset using a feature extraction query over input database and then\nuse a statistical software package of choice to train the model. In this paper\nwe introduce Iterative Functional Aggregate Queries (IFAQ), a framework that\nrealizes an alternative approach. IFAQ treats the feature extraction query and\nthe learning task as one program given in the IFAQ's domain-specific language,\nwhich captures a subset of Python commonly used in Jupyter notebooks for rapid\nprototyping of machine learning applications. The program is subject to several\nlayers of IFAQ optimizations, such as algebraic transformations, loop\ntransformations, schema specialization, data layout optimizations, and finally\ncompilation into efficient low-level C++ code specialized for the given\nworkload and data.\n  We show that a Scala implementation of IFAQ can outperform mlpack, Scikit,\nand TensorFlow by several orders of magnitude for linear regression and\nregression tree models over several relational datasets.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 16:14:44 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Shaikhha", "Amir", ""], ["Schleich", "Maximilian", ""], ["Ghita", "Alexandru", ""], ["Olteanu", "Dan", ""]]}, {"id": "2001.03829", "submitter": "Sen Zheng", "authors": "Sen Zheng, Renate A. Schmidt", "title": "Deciding the Loosely Guarded Fragment and Querying Its Horn Fragment\n  Using Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following query answering problem: Given a Boolean\nconjunctive query and a theory in the Horn loosely guarded fragment, the aim is\nto determine whether the query is entailed by the theory. In this paper, we\npresent a resolution decision procedure for the loosely guarded fragment, and\nuse such a procedure to answer Boolean conjunctive queries against the Horn\nloosely guarded fragment. The Horn loosely guarded fragment subsumes classes of\nrules that are prevalent in ontology-based query answering, such as Horn ALCHOI\nand guarded existential rules. Additionally, we identify star queries and cloud\nqueries, which using our procedure, can be answered against the loosely guarded\nfragment.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2020 02:29:18 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Zheng", "Sen", ""], ["Schmidt", "Renate A.", ""]]}, {"id": "2001.04174", "submitter": "Manuel Rigger", "authors": "Manuel Rigger and Zhendong Su", "title": "Testing Database Engines via Pivoted Query Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relational databases are used ubiquitously. They are managed by database\nmanagement systems (DBMS), which allow inserting, modifying, and querying data\nusing a domain-specific language called Structured Query Language (SQL).\nPopular DBMS have been extensively tested by fuzzers, which have been\nsuccessful in finding crash bugs. However, approaches to finding logic bugs,\nsuch as when a DBMS computes an incorrect result set, have remained mostly\nuntackled. Differential testing is an effective technique to test systems that\nsupport a common language by comparing the outputs of these systems. However,\nthis technique is ineffective for DBMS, because each DBMS typically supports\nits own SQL dialect. To this end, we devised a novel and general approach that\nwe have termed Pivoted Query Synthesis. The core idea of this approach is to\nautomatically generate queries for which we ensure that they fetch a specific,\nrandomly selected row, called the pivot row. If the DBMS fails to fetch the\npivot row, the likely cause is a bug in the DBMS. We tested our approach on\nthree widely-used and mature DBMS, namely SQLite, MySQL, and PostgreSQL. In\ntotal, we reported 123 bugs in these DBMS, 99 of which have been fixed or\nverified, demonstrating that the approach is highly effective and general. We\nexpect that the wide applicability and simplicity of our approach will enable\nthe improvement of robustness of many DBMS.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 12:00:16 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Rigger", "Manuel", ""], ["Su", "Zhendong", ""]]}, {"id": "2001.04175", "submitter": "Martin Thomas Horsch", "authors": "Martin Thomas Horsch, Silvia Chiacchiera, Youness Bami, Georg J.\n  Schmitz, Gabriele Mogni, Gerhard Goldbeck, and Emanuele Ghedini", "title": "Reliable and interoperable computational molecular engineering: 2.\n  Semantic interoperability based on the European Materials and Modelling\n  Ontology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cond-mat.mtrl-sci cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The European Materials and Modelling Ontology (EMMO) is a top-level ontology\ndesigned by the European Materials Modelling Council to facilitate semantic\ninteroperability between platforms, models, and tools in computational\nmolecular engineering, integrated computational materials engineering, and\nrelated applications of materials modelling and characterization. Additionally,\ndomain ontologies exist based on data technology developments from specific\nplatforms. The present work discusses the ongoing work on establishing a\nEuropean Virtual Marketplace Framework, into which diverse platforms can be\nintegrated. It addresses common challenges that arise when marketplace-level\ndomain ontologies are combined with a top-level ontology like the EMMO by\nontology alignment.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 12:02:37 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Horsch", "Martin Thomas", ""], ["Chiacchiera", "Silvia", ""], ["Bami", "Youness", ""], ["Schmitz", "Georg J.", ""], ["Mogni", "Gabriele", ""], ["Goldbeck", "Gerhard", ""], ["Ghedini", "Emanuele", ""]]}, {"id": "2001.04425", "submitter": "Hiba Arnaout", "authors": "Hiba Arnaout, Simon Razniewski, Gerhard Weikum, and Jeff Z. Pan", "title": "Negative Statements Considered Useful", "comments": "17 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge bases (KBs) about notable entities and their properties are an\nimportant asset in applications such as search, question answering and\ndialogue. All popular KBs capture virtually only positive statements, and\nabstain from taking any stance on statements not stored in the KB. This paper\nmakes the case for explicitly stating salient statements that do not hold.\nNegative statements are useful to overcome limitations of question answering,\nand can often contribute to informative summaries of entities. Due to the\nabundance of such invalid statements, any effort to compile them needs to\naddress ranking by saliency. We present a statistical inference method for\ncompiling and ranking negative statements,based on expectations from positive\nstatements of related entities in peer groups. Experimental results, with a\nvariety of datasets, show that the method can effectively discover notable\nnegative statements, and extrinsic studies underline their usefulness for\nentity summarization. Datasets and code are released as resources for further\nresearch.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 17:49:37 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 14:42:40 GMT"}, {"version": "v3", "created": "Mon, 20 Jan 2020 14:45:01 GMT"}, {"version": "v4", "created": "Tue, 8 Sep 2020 09:00:13 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Arnaout", "Hiba", ""], ["Razniewski", "Simon", ""], ["Weikum", "Gerhard", ""], ["Pan", "Jeff Z.", ""]]}, {"id": "2001.04443", "submitter": "Abdulwahab Alazeb", "authors": "Abdulwahab Alazeb and Brajendra Panda", "title": "Maintaining Data Integrity in Fog Computing Based Critical\n  Infrastructure Systems", "comments": "8 pages, accepted at The 2019 International Conference on\n  Computational Science and Computational Intelligence, Research\n  Track/Symposium on CSCI-ISCW: Cyber Warfare, Cyber Defense, & Cyber Security.\n  Will be published on IEEE CPS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The evolution of the utilization of technologies in nearly all aspects of\nlife has produced an enormous amount of data essential in a smart city.\nTherefore, maximizing the benefits of technologies such as cloud computing, fog\ncomputing, and the Internet of things is important to manage and manipulate\ndata in smart cities. However, certain types of data are sensitive and risky\nand may be infiltrated by malicious attacks. As a result, such data may be\ncorrupted, thereby causing concern. The damage inflicted by an attacker on a\nset of data can spread through an entire database. Valid transactions that have\nread corrupted data can update other data items based on the values read. In\nthis study, we introduce a unique model that uses fog computing in smart cities\nto manage utility service companies and consumer data. We also propose a novel\ntechnique to assess damage to data caused by an attack. Thus, original data can\nbe recovered, and a database can be returned to its consistent state as no\nattacking has occurred.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 18:24:35 GMT"}, {"version": "v2", "created": "Fri, 17 Jan 2020 15:45:19 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Alazeb", "Abdulwahab", ""], ["Panda", "Brajendra", ""]]}, {"id": "2001.04757", "submitter": "Evgeny Kharlamov", "authors": "Henrik Forssell and Evgeny Kharlamov and Evgenij Thorstensen", "title": "On Equivalence and Cores for Incomplete Databases in Open and Closed\n  Worlds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data exchange heavily relies on the notion of incomplete database instances.\nSeveral semantics for such instances have been proposed and include open (OWA),\nclosed (CWA), and open-closed (OCWA) world. For all these semantics important\nquestions are: whether one incomplete instance semantically implies another;\nwhen two are semantically equivalent; and whether a smaller or smallest\nsemantically equivalent instance exists. For OWA and CWA these questions are\nfully answered. For several variants of OCWA, however, they remain open. In\nthis work we adress these questions for Closed Powerset semantics and the OCWA\nsemantics of Libkin and Sirangelo, 2011. We define a new OCWA semantics, called\nOCWA*, in terms of homomorphic covers that subsumes both semantics, and\ncharacterize semantic implication and equivalence in terms of such covers. This\ncharacterization yields a guess-and-check algorithm to decide equivalence, and\nshows that the problem is NP-complete. For the minimization problem we show\nthat for several common notions of minimality there is in general no unique\nminimal equivalent instance for Closed Powerset semantics, and consequently not\nfor the more expressive OCWA* either. However, for Closed Powerset semantics we\nshow that one can find, for any incomplete database, a unique finite set of its\nsubinstances which are subinstances (up to renaming of nulls) of all instances\nsemantically equivalent to the original incomplete one. We study properties of\nthis set, and extend the analysis to OCWA*.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 13:10:01 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Forssell", "Henrik", ""], ["Kharlamov", "Evgeny", ""], ["Thorstensen", "Evgenij", ""]]}, {"id": "2001.04760", "submitter": "Rita Hartel", "authors": "Stefan B\\\"ottcher, Rita Hartel and Sven Peeters", "title": "Simulation computation in grammar-compressed graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Like [1], we present an algorithm to compute the simulation of a query\npattern in a graph of labeled nodes and unlabeled edges. However, our algorithm\nworks on a compressed graph grammar, instead of on the original graph. The\nspeed-up of our algorithm compared to the algorithm in [1] grows with the size\nof the graph and with the compression strength.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 13:19:41 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["B\u00f6ttcher", "Stefan", ""], ["Hartel", "Rita", ""], ["Peeters", "Sven", ""]]}, {"id": "2001.04907", "submitter": "Dmitry Krotov", "authors": "Chaitanya K. Ryali, John J. Hopfield, Leopold Grinberg, Dmitry Krotov", "title": "Bio-Inspired Hashing for Unsupervised Similarity Search", "comments": "Accepted for publication in ICML 2020", "journal-ref": "Proceedings of the International Conference on Machine Learning,\n  2020, pp.8739-8750", "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.IR q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fruit fly Drosophila's olfactory circuit has inspired a new locality\nsensitive hashing (LSH) algorithm, FlyHash. In contrast with classical LSH\nalgorithms that produce low dimensional hash codes, FlyHash produces sparse\nhigh-dimensional hash codes and has also been shown to have superior empirical\nperformance compared to classical LSH algorithms in similarity search. However,\nFlyHash uses random projections and cannot learn from data. Building on\ninspiration from FlyHash and the ubiquity of sparse expansive representations\nin neurobiology, our work proposes a novel hashing algorithm BioHash that\nproduces sparse high dimensional hash codes in a data-driven manner. We show\nthat BioHash outperforms previously published benchmarks for various hashing\nmethods. Since our learning algorithm is based on a local and biologically\nplausible synaptic plasticity rule, our work provides evidence for the proposal\nthat LSH might be a computational reason for the abundance of sparse expansive\nmotifs in a variety of biological systems. We also propose a convolutional\nvariant BioConvHash that further improves performance. From the perspective of\ncomputer science, BioHash and BioConvHash are fast, scalable and yield\ncompressed binary representations that are useful for similarity search.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 17:04:59 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 17:29:56 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Ryali", "Chaitanya K.", ""], ["Hopfield", "John J.", ""], ["Grinberg", "Leopold", ""], ["Krotov", "Dmitry", ""]]}, {"id": "2001.04910", "submitter": "Alejandro Corti\\~nas", "authors": "Alejandro Corti\\~nas, Miguel R. Luaces, Tirso V. Rodeiro", "title": "A Case Study on Visualizing Large Spatial Datasets in a Web-based Map\n  Viewer", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sk{\\l}odowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941", "journal-ref": "Proc. of the 18th International Conference On Web Engineering\n  (ICWE 2018), Springer International Publishing, Caceres (Spain), 2018, pp.\n  296-303", "doi": "10.1007/978-3-319-91662-0_23", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lately, many companies are using Mobile Workforce Management technologies\ncombined with information collected by sensors from mobile devices in order to\nimprove their business processes. Even for small companies, the information\nthat needs to be handled grows at a high rate, and most of the data collected\nhave a geographic dimension. Being able to visualize this data in real-time\nwithin a map viewer is a very important deal for these companies. In this paper\nwe focus on this topic, presenting a case study on visualizing large spatial\ndatasets. Particularly, since most of the Mobile Workforce Management software\nis web-based, we propose a solution suitable for this environment.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 17:09:34 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Corti\u00f1as", "Alejandro", ""], ["Luaces", "Miguel R.", ""], ["Rodeiro", "Tirso V.", ""]]}, {"id": "2001.05157", "submitter": "Runyu Zhang", "authors": "Jiaqi Dong, Runyu Zhang, Chaoshu Yang, Yujuan Tan, and Duo Liu", "title": "An Efficient and Wear-Leveling-Aware Frequent-Pattern Mining on\n  Non-Volatile Memory", "comments": null, "journal-ref": null, "doi": "10.1088/1742-6596/1570/1/012087", "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frequent-pattern mining is a common approach to reveal the valuable hidden\ntrends behind data. However, existing frequent-pattern mining algorithms are\ndesigned for DRAM, instead of persistent memories (PMs), which can lead to\nsevere performance and energy overhead due to the utterly different\ncharacteristics between DRAM and PMs when they are running on PMs. In this\npaper, we propose an efficient and Wear-leveling-aware Frequent-Pattern Mining\nscheme, WFPM, to solve this problem. The proposed WFPM is evaluated by a series\nof experiments based on realistic datasets from diversified application\nscenarios, where WFPM achieves 32.0% performance improvement and prolongs the\nNVM lifetime of header table by 7.4x over the EvFP-Tree.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 07:21:11 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Dong", "Jiaqi", ""], ["Zhang", "Runyu", ""], ["Yang", "Chaoshu", ""], ["Tan", "Yujuan", ""], ["Liu", "Duo", ""]]}, {"id": "2001.05581", "submitter": "Andreas Z\\\"ufle", "authors": "Tobias Emrich, Hans-Peter Kriegel, Andreas Z\\\"ufle, Peer Kr\\\"oger,\n  Matthias Renz", "title": "Complete and Sufficient Spatial Domination of Multidimensional\n  Rectangles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rectangles are used to approximate objects, or sets of objects, in a plethora\nof applications, systems and index structures. Many tasks, such as nearest\nneighbor search and similarity ranking, require to decide if objects in one\nrectangle A may, must, or must not be closer to objects in a second rectangle\nB, than objects in a third rectangle R. To decide this relation of \"Spatial\nDomination\" it can be shown that using minimum and maximum distances it is\noften impossible to detect spatial domination. This spatial gem provides a\nnecessary and sufficient decision criterion for spatial domination that can be\ncomputed efficiently even in higher dimensional space. In addition, this\nspatial gem provides an example, pseudocode and an implementation in Python.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 22:24:40 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Emrich", "Tobias", ""], ["Kriegel", "Hans-Peter", ""], ["Z\u00fcfle", "Andreas", ""], ["Kr\u00f6ger", "Peer", ""], ["Renz", "Matthias", ""]]}, {"id": "2001.05667", "submitter": "Shuhao Zhang", "authors": "Shuhao Zhang, Feng Zhang, Yingjun Wu, Bingsheng He, Paul Johns", "title": "Hardware-Conscious Stream Processing: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data stream processing systems (DSPSs) enable users to express and run stream\napplications to continuously process data streams. To achieve real-time data\nanalytics, recent researches keep focusing on optimizing the system latency and\nthroughput. Witnessing the recent great achievements in the computer\narchitecture community, researchers and practitioners have investigated the\npotential of adoption hardware-conscious stream processing by better utilizing\nmodern hardware capacity in DSPSs. In this paper, we conduct a systematic\nsurvey of recent work in the field, particularly along with the following three\ndirections: 1) computation optimization, 2) stream I/O optimization, and 3)\nquery deployment. Finally, we advise on potential future research directions.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 06:22:51 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Zhang", "Shuhao", ""], ["Zhang", "Feng", ""], ["Wu", "Yingjun", ""], ["He", "Bingsheng", ""], ["Johns", "Paul", ""]]}, {"id": "2001.05722", "submitter": "Yvonne M\\\"ulle", "authors": "Yvonne M\\\"ulle, Michael H. B\\\"ohlen", "title": "Query Results over Ongoing Databases that Remain Valid as Time Passes By\n  (Extended Version)", "comments": "Extended version of ICDE paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ongoing time point now is used to state that a tuple is valid from the start\npoint onward. For database systems ongoing time points have far-reaching\nimplications since they change continuously as time passes by. State-of-the-art\napproaches deal with ongoing time points by instantiating them to the reference\ntime. The instantiation yields query results that are only valid at the chosen\ntime and get invalidated as time passes by. We propose a solution that keeps\nongoing time points uninstantiated during query processing. We do so by\nevaluating predicates and functions at all possible reference times. This\nrenders query results independent of a specific reference time and yields\nresults that remain valid as time passes by. As query results, we propose\nongoing relations that include a reference time attribute. The value of the\nreference time attribute is restricted by predicates and functions on ongoing\nattributes. We describe and evaluate an efficient implementation of ongoing\ndata types and operations in PostgreSQL.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 10:07:25 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 12:28:16 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["M\u00fclle", "Yvonne", ""], ["B\u00f6hlen", "Michael H.", ""]]}, {"id": "2001.05996", "submitter": "Mahdi Bohlouli", "authors": "Mahdi Bohlouli, Jens Dalter, Mareike Dornh\\\"ofer, Johannes Zenkert,\n  Madjid Fathi", "title": "Knowledge Discovery from Social Media using Big Data provided Sentiment\n  Analysis (SoMABiT)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In todays competitive business world, being aware of customer needs and\nmarket-oriented production is a key success factor for industries. To this aim,\nthe use of efficient analytic algorithms ensures a better understanding of\ncustomer feedback and improves the next generation of products. Accordingly,\nthe dramatic increase in using social media in daily life provides beneficial\nsources for market analytics. But how traditional analytic algorithms and\nmethods can scale up for such disparate and multi-structured data sources is\nthe main challenge in this regard. This paper presents and discusses the\ntechnological and scientific focus of the SoMABiT as a social media analysis\nplatform using big data technology. Sentiment analysis has been employed in\norder to discover knowledge from social media. The use of MapReduce and\ndeveloping a distributed algorithm towards an integrated platform that can\nscale for any data volume and provide a social media-driven knowledge is the\nmain novelty of the proposed concept in comparison to the state-of-the-art\ntechnologies.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 18:53:59 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Bohlouli", "Mahdi", ""], ["Dalter", "Jens", ""], ["Dornh\u00f6fer", "Mareike", ""], ["Zenkert", "Johannes", ""], ["Fathi", "Madjid", ""]]}, {"id": "2001.05998", "submitter": "Mohamed Attia", "authors": "Islam Samy, Mohamed A. Attia, Ravi Tandon, Loukas Lazos", "title": "Latent-variable Private Information Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DB cs.IR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, content accessed by users (movies, videos, news\narticles, etc.) can leak sensitive latent attributes, such as religious and\npolitical views, sexual orientation, ethnicity, gender, and others. To prevent\nsuch information leakage, the goal of classical PIR is to hide the identity of\nthe content/message being accessed, which subsequently also hides the latent\nattributes. This solution, while private, can be too costly, particularly, when\nperfect (information-theoretic) privacy constraints are imposed. For instance,\nfor a single database holding $K$ messages, privately retrieving one message is\npossible if and only if the user downloads the entire database of $K$ messages.\nRetrieving content privately, however, may not be necessary to perfectly hide\nthe latent attributes.\n  Motivated by the above, we formulate and study the problem of latent-variable\nprivate information retrieval (LV-PIR), which aims at allowing the user\nefficiently retrieve one out of $K$ messages (indexed by $\\theta$) without\nrevealing any information about the latent variable (modeled by $S$). We focus\non the practically relevant setting of a single database and show that one can\nsignificantly reduce the download cost of LV-PIR (compared to the classical\nPIR) based on the correlation between $\\theta$ and $S$. We present a general\nscheme for LV-PIR as a function of the statistical relationship between\n$\\theta$ and $S$, and also provide new results on the capacity/download cost of\nLV-PIR. Several open problems and new directions are also discussed.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 18:58:22 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 17:49:21 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Samy", "Islam", ""], ["Attia", "Mohamed A.", ""], ["Tandon", "Ravi", ""], ["Lazos", "Loukas", ""]]}, {"id": "2001.06111", "submitter": "Kai Wang", "authors": "Kai Wang, Xuemin Lin, Lu Qin, Wenjie Zhang, Ying Zhang", "title": "Efficient Bitruss Decomposition for Large-scale Bipartite Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cohesive subgraph mining in bipartite graphs becomes a popular research topic\nrecently. An important structure k-bitruss is the maximal cohesive subgraph\nwhere each edge is contained in at least k butterflies (i.e., (2,\n2)-bicliques). In this paper, we study the bitruss decomposition problem which\naims to find all the k-bitrusses for k >= 0. The existing bottom-up techniques\nneed to iteratively peel the edges with the lowest butterfly support. In this\npeeling process, these techniques are time-consuming to enumerate all the\nsupporting butterflies for each edge. To relax this issue, we first propose a\nnovel online index -- the BE-Index which compresses butterflies into k-blooms\n(i.e., (2, k)-bicliques). Based on the BE-Index, the new bitruss decomposition\nalgorithm BiT-BU is proposed, along with two batch-based optimizations, to\naccomplish the butterfly enumeration of the peeling process in an efficient\nway. Furthermore, the BiT-PC algorithm is devised which is more efficient\nagainst handling the edges with high butterfly supports. We theoretically show\nthat our new algorithms significantly reduce the time complexities of the\nexisting algorithms. Also, we conduct extensive experiments on real datasets\nand the result demonstrates that our new techniques can speed up the\nstate-of-the-art techniques by up to two orders of magnitude.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 23:23:27 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Wang", "Kai", ""], ["Lin", "Xuemin", ""], ["Qin", "Lu", ""], ["Zhang", "Wenjie", ""], ["Zhang", "Ying", ""]]}, {"id": "2001.06358", "submitter": "Peter Lindner", "authors": "Martin Grohe, Benjamin Lucien Kaminski, Joost-Pieter Katoen, Peter\n  Lindner", "title": "Generative Datalog with Continuous Distributions", "comments": "Extended Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arguing for the need to combine declarative and probabilistic programming,\nB\\'ar\\'any et al. (TODS 2017) recently introduced a probabilistic extension of\nDatalog as a \"purely declarative probabilistic programming language.\" We\nrevisit this language and propose a more principled approach towards defining\nits semantics based on stochastic kernels and Markov processes - standard\nnotions from probability theory. This allows us to extend the semantics to\ncontinuous probability distributions, thereby settling an open problem posed by\nB\\'ar\\'any et al.\n  We show that our semantics is fairly robust, allowing both parallel execution\nand arbitrary chase orders when evaluating a program. We cast our semantics in\nthe framework of infinite probabilistic databases (Grohe and Lindner, ICDT\n2020), and show that the semantics remains meaningful even when the input of a\nprobabilistic Datalog program is an arbitrary probabilistic database.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 15:02:21 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 11:56:00 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Grohe", "Martin", ""], ["Kaminski", "Benjamin Lucien", ""], ["Katoen", "Joost-Pieter", ""], ["Lindner", "Peter", ""]]}, {"id": "2001.06543", "submitter": "Katsiaryna Mirylenka", "authors": "Evgeny Krivosheev, Mattia Atzeni, Katsiaryna Mirylenka, Paolo Scotton,\n  Fabio Casati", "title": "Siamese Graph Neural Networks for Data Integration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data integration has been studied extensively for decades and approached from\ndifferent angles. However, this domain still remains largely rule-driven and\nlacks universal automation. Recent development in machine learning and in\nparticular deep learning has opened the way to more general and more efficient\nsolutions to data integration problems. In this work, we propose a general\napproach to modeling and integrating entities from structured data, such as\nrelational databases, as well as unstructured sources, such as free text from\nnews articles. Our approach is designed to explicitly model and leverage\nrelations between entities, thereby using all available information and\npreserving as much context as possible. This is achieved by combining siamese\nand graph neural networks to propagate information between connected entities\nand support high scalability. We evaluate our method on the task of integrating\ndata about business entities, and we demonstrate that it outperforms standard\nrule-based systems, as well as other deep learning approaches that do not use\ngraph-based representations.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 21:51:55 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Krivosheev", "Evgeny", ""], ["Atzeni", "Mattia", ""], ["Mirylenka", "Katsiaryna", ""], ["Scotton", "Paolo", ""], ["Casati", "Fabio", ""]]}, {"id": "2001.06630", "submitter": "Shiqi Zhang", "authors": "Xinxun Zeng, Shiqi Zhang, Bo Tang", "title": "RCELF: A Residual-based Approach for Influence Maximization Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Influence Maximization Problem (IMP) is selecting a seed set of nodes in the\nsocial network to spread the influence as widely as possible. It has many\napplications in multiple domains, e.g., viral marketing is frequently used for\nnew products or activities advertisements. While it is a classic and\nwell-studied problem in computer science, unfortunately, all those proposed\ntechniques are compromising among time efficiency, memory consumption, and\nresult quality. In this paper, we conduct comprehensive experimental studies on\nthe state-of-the-art IMP approximate approaches to reveal the underlying\ntrade-off strategies. Interestingly, we find that even the state-of-the-art\napproaches are impractical when the propagation probability of the network have\nbeen taken into consideration. With the findings of existing approaches, we\npropose a novel residual-based approach (i.e., RCELF) for IMP, which i)\novercomes the deficiencies of existing approximate approaches, and ii) provides\ntheoretical guaranteed results with high efficiency in both time- and space-\nperspectives. We demonstrate the superiority of our proposal by extensive\nexperimental evaluation on real datasets.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2020 09:09:34 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2020 06:56:44 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Zeng", "Xinxun", ""], ["Zhang", "Shiqi", ""], ["Tang", "Bo", ""]]}, {"id": "2001.06640", "submitter": "Shuo Wang", "authors": "Shuo Wang, Tianle Chen, Shangyu Chen, Carsten Rudolph, Surya Nepal,\n  Marthie Grobler", "title": "OIAD: One-for-all Image Anomaly Detection with Disentanglement Learning", "comments": "arXiv admin note: text overlap with arXiv:1802.05983,\n  arXiv:1909.02755, arXiv:1804.03599 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection aims to recognize samples with anomalous and unusual\npatterns with respect to a set of normal data. This is significant for numerous\ndomain applications, such as industrial inspection, medical imaging, and\nsecurity enforcement. There are two key research challenges associated with\nexisting anomaly detection approaches: (1) many approaches perform well on\nlow-dimensional problems however the performance on high-dimensional instances,\nsuch as images, is limited; (2) many approaches often rely on traditional\nsupervised approaches and manual engineering of features, while the topic has\nnot been fully explored yet using modern deep learning approaches, even when\nthe well-label samples are limited. In this paper, we propose a One-for-all\nImage Anomaly Detection system (OIAD) based on disentangled learning using only\nclean samples. Our key insight is that the impact of small perturbation on the\nlatent representation can be bounded for normal samples while anomaly images\nare usually outside such bounded intervals, referred to as structure\nconsistency. We implement this idea and evaluate its performance for anomaly\ndetection. Our experiments with three datasets show that OIAD can detect over\n$90\\%$ of anomalies while maintaining a low false alarm rate. It can also\ndetect suspicious samples from samples labeled as clean, coincided with what\nhumans would deem unusual.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2020 09:57:37 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2020 09:00:14 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Wang", "Shuo", ""], ["Chen", "Tianle", ""], ["Chen", "Shangyu", ""], ["Rudolph", "Carsten", ""], ["Nepal", "Surya", ""], ["Grobler", "Marthie", ""]]}, {"id": "2001.06731", "submitter": "Jeremy Kepner", "authors": "Jeremy Kepner, Vijay Gadepally, Hayden Jananthan, Lauren Milechin,\n  Siddharth Samsi", "title": "AI Data Wrangling with Associative Arrays", "comments": "3 pages, 2 figures, 23 references, accepted for Northeast Database\n  day (NEDB) 2020. arXiv admin note: text overlap with arXiv:1907.04217", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The AI revolution is data driven. AI \"data wrangling\" is the process by which\nunusable data is transformed to support AI algorithm development (training) and\ndeployment (inference). Significant time is devoted to translating diverse data\nrepresentations supporting the many query and analysis steps found in an AI\npipeline. Rigorous mathematical representations of these data enables data\ntranslation and analysis optimization within and across steps. Associative\narray algebra provides a mathematical foundation that naturally describes the\ntabular structures and set mathematics that are the basis of databases.\nLikewise, the matrix operations and corresponding inference/training\ncalculations used by neural networks are also well described by associative\narrays. More surprisingly, a general denormalized form of hierarchical formats,\nsuch as XML and JSON, can be readily constructed. Finally, pivot tables, which\nare among the most widely used data analysis tools, naturally emerge from\nassociative array constructors. A common foundation in associative arrays\nprovides interoperability guarantees, proving that their operations are linear\nsystems with rigorous mathematical properties, such as, associativity,\ncommutativity, and distributivity that are critical to reordering\noptimizations.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2020 22:11:23 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Kepner", "Jeremy", ""], ["Gadepally", "Vijay", ""], ["Jananthan", "Hayden", ""], ["Milechin", "Lauren", ""], ["Samsi", "Siddharth", ""]]}, {"id": "2001.06770", "submitter": "Yueji Yang", "authors": "Yueji Yang, Anthony K. H. Tung", "title": "Efficient Radial Pattern Keyword Search on Knowledge Graphs in Parallel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, keyword search on Knowledge Graphs (KGs) becomes popular. Typical\nkeyword search approaches aim at finding a concise subgraph from a KG, which\ncan reflect a close relationship among all input keywords. The connection paths\nbetween keywords are selected in a way that leads to a result subgraph with a\nbetter semantic score. However, such a result may not meet user information\nneed because it relies on the scoring function to decide what keywords to link\ncloser. Therefore, such a result may miss close connections among some keywords\non which users intend to focus. In this paper, we propose a parallel keyword\nsearch engine, called RAKS. It allows users to specify a query as two sets of\nkeywords, namely central keywords and marginal keywords. Specifically, central\nkeywords are those keywords on which users focus more. Their relationships are\ndesired in the results. Marginal keywords are those less focused keywords.\nTheir connections to the central keywords are desired. In addition, they\nprovide additional information that helps discover better results in terms of\nuser intents. To improve the efficiency, we propose novel weighting and scoring\nschemes that boost the parallel execution during search while retrieving\nsemantically relevant results. We conduct extensive experiments to validate\nthat RAKS can work efficiently and effectively on open KGs with large size and\nvariety.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2020 04:24:28 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Yang", "Yueji", ""], ["Tung", "Anthony K. H.", ""]]}, {"id": "2001.06846", "submitter": "Yi Wang Dr.", "authors": "Yi Wang, Yang Yang, Weiguo Zhu, Yi Wu, Xu Yan, Yongfeng Liu, Yu Wang,\n  Liang Xie, Ziyao Gao, Wenjing Zhu, Xiang Chen, Wei Yan, Mingjie Tang, Yuan\n  Tang", "title": "SQLFlow: A Bridge between SQL and Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Industrial AI systems are mostly end-to-end machine learning (ML) workflows.\nA typical recommendation or business intelligence system includes many online\nmicro-services and offline jobs. We describe SQLFlow for developing such\nworkflows efficiently in SQL. SQL enables developers to write short programs\nfocusing on the purpose (what) and ignoring the procedure (how). Previous\ndatabase systems extended their SQL dialect to support ML. SQLFlow\n(https://sqlflow.org/sqlflow ) takes another strategy to work as a bridge over\nvarious database systems, including MySQL, Apache Hive, and Alibaba MaxCompute,\nand ML engines like TensorFlow, XGBoost, and scikit-learn. We extended SQL\nsyntax carefully to make the extension working with various SQL dialects. We\nimplement the extension by inventing a collaborative parsing algorithm. SQLFlow\nis efficient and expressive to a wide variety of ML techniques -- supervised\nand unsupervised learning; deep networks and tree models; visual model\nexplanation in addition to training and prediction; data processing and feature\nextraction in addition to ML. SQLFlow compiles a SQL program into a\nKubernetes-native workflow for fault-tolerable execution and on-cloud\ndeployment. Current industrial users include Ant Financial, DiDi, and Alibaba\nGroup.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2020 15:19:20 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Wang", "Yi", ""], ["Yang", "Yang", ""], ["Zhu", "Weiguo", ""], ["Wu", "Yi", ""], ["Yan", "Xu", ""], ["Liu", "Yongfeng", ""], ["Wang", "Yu", ""], ["Xie", "Liang", ""], ["Gao", "Ziyao", ""], ["Zhu", "Wenjing", ""], ["Chen", "Xiang", ""], ["Yan", "Wei", ""], ["Tang", "Mingjie", ""], ["Tang", "Yuan", ""]]}, {"id": "2001.06933", "submitter": "Sujaya Maiyya", "authors": "Sujaya Maiyya, Danny Hyun Bum Cho, Divyakant Agrawal, Amr El Abbadi", "title": "Fides: Managing Data on Untrusted Infrastructure", "comments": "14pages, 15 figures/graphs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Significant amounts of data are currently being stored and managed on\nthird-party servers. It is impractical for many small scale enterprises to own\ntheir private datacenters, hence renting third-party servers is a viable\nsolution for such businesses. But the increasing number of malicious attacks,\nboth internal and external, as well as buggy software on third-party servers is\ncausing clients to lose their trust in these external infrastructures. While\nsmall enterprises cannot avoid using external infrastructures, they need the\nright set of protocols to manage their data on untrusted infrastructures. In\nthis paper, we propose TFCommit, a novel atomic commitment protocol that\nexecutes transactions on data stored across multiple untrusted servers. To our\nknowledge, TFCommit is the first atomic commitment protocol to execute\ntransactions in an untrusted environment without using expensive Byzantine\nreplication. Using TFCommit, we propose an auditable data management system,\nFides, residing completely on untrustworthy infrastructure. As an auditable\nsystem, Fides guarantees the detection of potentially malicious failures\noccurring on untrusted servers using tamper-resistant logs with the support of\ncryptographic techniques. The experimental evaluation demonstrates the\nscalability and the relatively low overhead of our approach that allows\nexecuting transactions on untrusted infrastructure.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 01:20:55 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Maiyya", "Sujaya", ""], ["Cho", "Danny Hyun Bum", ""], ["Agrawal", "Divyakant", ""], ["Abbadi", "Amr El", ""]]}, {"id": "2001.06935", "submitter": "Jeremy Kepner", "authors": "Jeremy Kepner, Tim Davis, Chansup Byun, William Arcand, David Bestor,\n  William Bergeron, Vijay Gadepally, Matthew Hubbell, Michael Houle, Michael\n  Jones, Anna Klein, Peter Michaleas, Lauren Milechin, Julie Mullen, Andrew\n  Prout, Antonio Rosa, Siddharth Samsi, Charles Yee, Albert Reuther", "title": "75,000,000,000 Streaming Inserts/Second Using Hierarchical Hypersparse\n  GraphBLAS Matrices", "comments": "4 pages, 4 figures, 28 references, accepted to IPDPS GrAPL 2020.\n  arXiv admin note: substantial text overlap with arXiv:1907.04217", "journal-ref": null, "doi": "10.1109/IPDPSW50202.2020.00046", "report-no": null, "categories": "cs.DC cs.DB cs.DS cs.PF cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The SuiteSparse GraphBLAS C-library implements high performance hypersparse\nmatrices with bindings to a variety of languages (Python, Julia, and\nMatlab/Octave). GraphBLAS provides a lightweight in-memory database\nimplementation of hypersparse matrices that are ideal for analyzing many types\nof network data, while providing rigorous mathematical guarantees, such as\nlinearity. Streaming updates of hypersparse matrices put enormous pressure on\nthe memory hierarchy. This work benchmarks an implementation of hierarchical\nhypersparse matrices that reduces memory pressure and dramatically increases\nthe update rate into a hypersparse matrices. The parameters of hierarchical\nhypersparse matrices rely on controlling the number of entries in each level in\nthe hierarchy before an update is cascaded. The parameters are easily tunable\nto achieve optimal performance for a variety of applications. Hierarchical\nhypersparse matrices achieve over 1,000,000 updates per second in a single\ninstance. Scaling to 31,000 instances of hierarchical hypersparse matrices\narrays on 1,100 server nodes on the MIT SuperCloud achieved a sustained update\nrate of 75,000,000,000 updates per second. This capability allows the MIT\nSuperCloud to analyze extremely large streaming network data sets.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 01:41:17 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 20:50:53 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Kepner", "Jeremy", ""], ["Davis", "Tim", ""], ["Byun", "Chansup", ""], ["Arcand", "William", ""], ["Bestor", "David", ""], ["Bergeron", "William", ""], ["Gadepally", "Vijay", ""], ["Hubbell", "Matthew", ""], ["Houle", "Michael", ""], ["Jones", "Michael", ""], ["Klein", "Anna", ""], ["Michaleas", "Peter", ""], ["Milechin", "Lauren", ""], ["Mullen", "Julie", ""], ["Prout", "Andrew", ""], ["Rosa", "Antonio", ""], ["Samsi", "Siddharth", ""], ["Yee", "Charles", ""], ["Reuther", "Albert", ""]]}, {"id": "2001.07158", "submitter": "Suhas Thejaswi", "authors": "Suhas Thejaswi, Aristides Gionis, Juho Lauri", "title": "Finding path motifs in large temporal graphs using algebraic\n  fingerprints", "comments": "version prior to peer review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a family of pattern-detection problems in vertex-colored temporal\ngraphs. In particular, given a vertex-colored temporal graph and a multiset of\ncolors as a query, we search for temporal paths in the graph that contain the\ncolors specified in the query. These types of problems have several\napplications, for example in recommending tours for tourists or detecting\nabnormal behavior in a network of financial transactions. For the family of\npattern-detection problems we consider, we establish complexity results and\ndesign an algebraic-algorithmic framework based on constrained multilinear\nsieving. We demonstrate that our solution scales to massive graphs with up to a\nbillion edges for a multiset query with five colors and up to hundred million\nedges for a multiset query with ten colors, despite the problems being NP-hard.\nOur implementation, which is publicly available, exhibits practical edge-linear\nscalability and is highly optimized. For instance, in a real-world graph\ndataset with more than six million edges and a multiset query with ten colors,\nwe can extract an optimum solution in less than eight minutes on a Haswell\ndesktop with four cores.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 16:13:27 GMT"}, {"version": "v2", "created": "Sat, 25 Jan 2020 04:31:40 GMT"}, {"version": "v3", "created": "Fri, 24 Jul 2020 13:18:08 GMT"}, {"version": "v4", "created": "Mon, 27 Jul 2020 14:40:49 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Thejaswi", "Suhas", ""], ["Gionis", "Aristides", ""], ["Lauri", "Juho", ""]]}, {"id": "2001.07906", "submitter": "Arnau Prat-P\\'erez", "authors": "Angela Bonifati, Irena Holubov\\'a, Arnau Prat-P\\'erez, Sherif Sakr", "title": "Graph Generators: State of the Art and Open Challenges", "comments": "ACM Computing Surveys, 32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The abundance of interconnected data has fueled the design and implementation\nof graph generators reproducing real-world linking properties, or gauging the\neffectiveness of graph algorithms, techniques and applications manipulating\nthese data. We consider graph generation across multiple subfields, such as\nSemantic Web, graph databases, social networks, and community detection, along\nwith general graphs. Despite the disparate requirements of modern graph\ngenerators throughout these communities, we analyze them under a common\numbrella, reaching out the functionalities, the practical usage, and their\nsupported operations. We argue that this classification is serving the need of\nproviding scientists, researchers and practitioners with the right data\ngenerator at hand for their work. This survey provides a comprehensive overview\nof the state-of-the-art graph generators by focusing on those that are\npertinent and suitable for several data-intensive tasks. Finally, we discuss\nopen challenges and missing requirements of current graph generators along with\ntheir future extensions to new emerging fields.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 08:11:06 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Bonifati", "Angela", ""], ["Holubov\u00e1", "Irena", ""], ["Prat-P\u00e9rez", "Arnau", ""], ["Sakr", "Sherif", ""]]}, {"id": "2001.08329", "submitter": "Lei Gai", "authors": "Lei Gai", "title": "Leveraging Neighborhood Summaries for Efficient RDF Queries on RDBMS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using structural informations to summarize graph-structured RDF data is\nhelpful in tackling query performance issues. However, leveraging structural\nindexes needs to revise or even redesign the internal of RDF systems. Given an\nRDF dataset that have already been bulk loaded into a relational RDF system, we\naim at improving the query performance on such systems. We do so by summarizing\nneighborhood structures and encoding them into triples which can be managed\nalong side the exist instance data. At query time, we optimally select the\neffective structural patterns, and adding these patterns to the existing\nqueries to gain an improved query performance. Empirical evaluations shown the\neffectiveness of our method.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 01:27:41 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Gai", "Lei", ""]]}, {"id": "2001.08392", "submitter": "Jens D\\\"orpinghaus", "authors": "Jens D\\\"orpinghaus and Andreas Stefan, Bruce Schultz, Marc Jacobs", "title": "Towards context in large scale biomedical knowledge graphs", "comments": "26 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextual information is widely considered for NLP and knowledge discovery\nin life sciences since it highly influences the exact meaning of natural\nlanguage. The scientific challenge is not only to extract such context data,\nbut also to store this data for further query and discovery approaches. Here,\nwe propose a multiple step knowledge graph approach using labeled property\ngraphs based on polyglot persistence systems to utilize context data for\ncontext mining, graph queries, knowledge discovery and extraction. We introduce\nthe graph-theoretic foundation for a general context concept within semantic\nnetworks and show a proof-of-concept based on biomedical literature and text\nmining. Our test system contains a knowledge graph derived from the entirety of\nPubMed and SCAIView data and is enriched with text mining data and domain\nspecific language data using BEL. Here, context is a more general concept than\nannotations. This dense graph has more than 71M nodes and 850M relationships.\nWe discuss the impact of this novel approach with 27 real world use cases\nrepresented by graph queries.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 07:33:02 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["D\u00f6rpinghaus", "Jens", ""], ["Stefan", "Andreas", ""], ["Schultz", "Bruce", ""], ["Jacobs", "Marc", ""]]}, {"id": "2001.08529", "submitter": "Mustafa Ozdayi", "authors": "Mustafa Safa Ozdayi, Murat Kantarcioglu, and Bradley Malin", "title": "Leveraging Blockchain for Immutable Logging and Querying Across Multiple\n  Sites", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain has emerged as a decentralized and distributed framework that\nenables tamper-resilience and, thus, practical immutability for stored data.\nThis immutability property is important in scenarios where auditability is\ndesired, such as in maintaining access logs for sensitive healthcare and\nbiomedical data.However, the underlying data structure of blockchain, by\ndefault, does not provide capabilities to efficiently query the stored data. In\nthis investigation, we show that it is possible to efficiently run complex\naudit queries over the access log data stored on blockchains by using\nadditional key-value stores. This paper specifically reports on the approach we\ndesigned for the blockchain track of iDASH Privacy & Security Workshop 2018\ncompetition.Particularly, we implemented our solution and compared its loading\nand query-response performance with SQLite, a commonly used relational\ndatabase, using the data provided by the iDASH 2018 organizers. Depending on\nthe query type and the data size, the run time difference between blockchain\nbased query-response and SQLite based query-response ranged from 0.2 seconds to\n6 seconds. A deeper inspection revealed that range queries were the bottleneck\nof our solution which, nevertheless, scales up linearly. Concretely, this\ninvestigation demonstrates that blockchain-based systems can provide reasonable\nquery-response times to complex queries even if they only use simple key-value\nstores to manage their data. Consequently, we show that blockchains may be\nuseful for maintaining data with auditability and immutability requirements\nacross multiple sites.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 22:23:43 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2020 20:29:10 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Ozdayi", "Mustafa Safa", ""], ["Kantarcioglu", "Murat", ""], ["Malin", "Bradley", ""]]}, {"id": "2001.08688", "submitter": "Heng Zhang", "authors": "Heng Zhang, Yan Zhang, Guifei Jiang", "title": "Model-theoretic Characterizations of Existential Rule Languages", "comments": "17 pages, 2 figures, the full version of a paper submitted to IJCAI\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existential rules, a.k.a. dependencies in databases, and Datalog+/- in\nknowledge representation and reasoning recently, are a family of important\nlogical languages widely used in computer science and artificial intelligence.\nTowards a deep understanding of these languages in model theory, we establish\nmodel-theoretic characterizations for a number of existential rule languages\nsuch as (disjunctive) embedded dependencies, tuple-generating dependencies\n(TGDs), (frontier-)guarded TGDs and linear TGDs. All these characterizations\nhold for arbitrary structures, and most of them also work on the class of\nfinite structures. As a natural application of these characterizations,\ncomplexity bounds for the rewritability of above languages are also identified.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 17:29:18 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Zhang", "Heng", ""], ["Zhang", "Yan", ""], ["Jiang", "Guifei", ""]]}, {"id": "2001.08773", "submitter": "Yanjun Pan", "authors": "Yanjun Pan, Alon Efrat, Ming Li, Boyang Wang, Hanyu Quan, Joseph\n  Mitchell, Jie Gao and Esther Arkin", "title": "Data Inference from Encrypted Databases: A Multi-dimensional\n  Order-Preserving Matching Approach", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to increasing concerns of data privacy, databases are being encrypted\nbefore they are stored on an untrusted server. To enable search operations on\nthe encrypted data, searchable encryption techniques have been proposed.\nRepresentative schemes use order-preserving encryption (OPE) for supporting\nefficient Boolean queries on encrypted databases. Yet, recent works showed the\npossibility of inferring plaintext data from OPE-encrypted databases, merely\nusing the order-preserving constraints, or combined with an auxiliary plaintext\ndataset with similar frequency distribution. So far, the effectiveness of such\nattacks is limited to single-dimensional dense data (most values from the\ndomain are encrypted), but it remains challenging to achieve it on\nhigh-dimensional datasets (e.g., spatial data) which are often sparse in\nnature. In this paper, for the first time, we study data inference attacks on\nmulti-dimensional encrypted databases (with 2-D as a special case). We\nformulate it as a 2-D order-preserving matching problem and explore both\nunweighted and weighted cases, where the former maximizes the number of points\nmatched using only order information and the latter further considers points\nwith similar frequencies. We prove that the problem is NP-hard, and then\npropose a greedy algorithm, along with a polynomial-time algorithm with\napproximation guarantees. Experimental results on synthetic and real-world\ndatasets show that the data recovery rate is significantly enhanced compared\nwith the previous 1-D matching algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 19:23:15 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Pan", "Yanjun", ""], ["Efrat", "Alon", ""], ["Li", "Ming", ""], ["Wang", "Boyang", ""], ["Quan", "Hanyu", ""], ["Mitchell", "Joseph", ""], ["Gao", "Jie", ""], ["Arkin", "Esther", ""]]}, {"id": "2001.09052", "submitter": "David Chaves-Fraga", "authors": "David Chaves-Fraga, Edna Ruckhaus, Freddy Priyatna, Maria-Esther\n  Vidal, Oscar Corcho", "title": "Enhancing Virtual Ontology Based Access over Tabular Data with Morph-CSV", "comments": null, "journal-ref": null, "doi": "10.3233/SW-210432", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ontology-Based Data Access (OBDA) has traditionally focused on providing a\nunified view of heterogeneous datasets, either by materializing integrated data\ninto RDF or by performing on-the fly querying via SPARQL query translation. In\nthe specific case of tabular datasets represented as several CSV or Excel\nfiles, query translation approaches have been applied by considering each\nsource as a single table that can be loaded into a relational database\nmanagement system (RDBMS). Nevertheless, constraints over these tables are not\nrepresented; thus, neither consistency among attributes nor indexes over tables\nare enforced. As a consequence, efficiency of the SPARQL-to-SQL translation\nprocess may be affected, as well as the completeness of the answers produced\nduring the evaluation of the generated SQL query. Our work is focused on\napplying implicit constraints on the OBDA query translation process over\ntabular data. We propose Morph-CSV, a framework for querying tabular data that\nexploits information from typical OBDA inputs (e.g., mappings, queries) to\nenforce constraints that can be used together with any SPARQL-to-SQL OBDA\nengine. Morph-CSV relies on both a constraint component and a set of constraint\noperators. For a given set of constraints, the operators are applied to each\ntype of constraint with the aim of enhancing query completeness and\nperformance. We evaluate Morph-CSV in several domains: e-commerce with the BSBM\nbenchmark; transportation with a benchmark using the GTFS dataset from the\nMadrid subway; and biology with a use case extracted from the Bio2RDF project.\nWe compare and report the performance of two SPARQL-to-SQL OBDA engines,\nwithout and with the incorporation of MorphCSV. The observed results suggest\nthat Morph-CSV is able to speed up the total query execution time by up to two\norders of magnitude, while it is able to produce all the query answers.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 15:26:03 GMT"}, {"version": "v2", "created": "Sun, 21 Feb 2021 14:21:23 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Chaves-Fraga", "David", ""], ["Ruckhaus", "Edna", ""], ["Priyatna", "Freddy", ""], ["Vidal", "Maria-Esther", ""], ["Corcho", "Oscar", ""]]}, {"id": "2001.09078", "submitter": "Jacopo Urbani", "authors": "Jacopo Urbani and Ceriel Jacobs", "title": "Adaptive Low-level Storage of Very Large Knowledge Graphs", "comments": "Accepted WWW 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing availability and usage of Knowledge Graphs (KGs) on the Web\ncalls for scalable and general-purpose solutions to store this type of data\nstructures. We propose Trident, a novel storage architecture for very large KGs\non centralized systems. Trident uses several interlinked data structures to\nprovide fast access to nodes and edges, with the physical storage changing\ndepending on the topology of the graph to reduce the memory footprint. In\ncontrast to single architectures designed for single tasks, our approach offers\nan interface with few low-level and general-purpose primitives that can be used\nto implement tasks like SPARQL query answering, reasoning, or graph analytics.\nOur experiments show that Trident can handle graphs with 10^11 edges using\ninexpensive hardware, delivering competitive performance on multiple workloads.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 16:33:35 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Urbani", "Jacopo", ""], ["Jacobs", "Ceriel", ""]]}, {"id": "2001.09786", "submitter": "Somali Chaterji", "authors": "Somali Chaterji, Nathan DeLay, John Evans, Nathan Mosier, Bernard\n  Engel, Dennis Buckmaster and Ranveer Chandra", "title": "Artificial Intelligence for Digital Agriculture at Scale: Techniques,\n  Policies, and Challenges", "comments": "15 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital agriculture has the promise to transform agricultural throughput. It\ncan do this by applying data science and engineering for mapping input factors\nto crop throughput, while bounding the available resources. In addition, as the\ndata volumes and varieties increase with the increase in sensor deployment in\nagricultural fields, data engineering techniques will also be instrumental in\ncollection of distributed data as well as distributed processing of the data.\nThese have to be done such that the latency requirements of the end users and\napplications are satisfied. Understanding how farm technology and big data can\nimprove farm productivity can significantly increase the world's food\nproduction by 2050 in the face of constrained arable land and with the water\nlevels receding. While much has been written about digital agriculture's\npotential, little is known about the economic costs and benefits of these\nemergent systems. In particular, the on-farm decision making processes, both in\nterms of adoption and optimal implementation, have not been adequately\naddressed. For example, if some algorithm needs data from multiple data owners\nto be pooled together, that raises the question of data ownership. This paper\nis the first one to bring together the important questions that will guide the\nend-to-end pipeline for the evolution of a new generation of digital\nagricultural solutions, driving the next revolution in agriculture and\nsustainability under one umbrella.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 06:02:38 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Chaterji", "Somali", ""], ["DeLay", "Nathan", ""], ["Evans", "John", ""], ["Mosier", "Nathan", ""], ["Engel", "Bernard", ""], ["Buckmaster", "Dennis", ""], ["Chandra", "Ranveer", ""]]}, {"id": "2001.10301", "submitter": "Zohair Raza Hassan", "authors": "Zohair Raza Hassan, Mudassir Shabbir, Imdadullah Khan, Waseem Abbas", "title": "Estimating Descriptors for Large Graphs", "comments": "Accepted to PAKDD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embedding networks into a fixed dimensional feature space, while preserving\nits essential structural properties is a fundamental task in graph analytics.\nThese feature vectors (graph descriptors) are used to measure the pairwise\nsimilarity between graphs. This enables applying data mining algorithms (e.g\nclassification, clustering, or anomaly detection) on graph-structured data\nwhich have numerous applications in multiple domains. State-of-the-art\nalgorithms for computing descriptors require the entire graph to be in memory,\nentailing a huge memory footprint, and thus do not scale well to increasing\nsizes of real-world networks. In this work, we propose streaming algorithms to\nefficiently approximate descriptors by estimating counts of sub-graphs of order\n$k\\leq 4$, and thereby devise extensions of two existing graph comparison\nparadigms: the Graphlet Kernel and NetSimile. Our algorithms require a single\nscan over the edge stream, have space complexity that is a fraction of the\ninput size, and approximate embeddings via a simple sampling scheme. Our design\nexploits the trade-off between available memory and estimation accuracy to\nprovide a method that works well for limited memory requirements. We perform\nextensive experiments on real-world networks and demonstrate that our\nalgorithms scale well to massive graphs.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 13:03:58 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 06:32:59 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Hassan", "Zohair Raza", ""], ["Shabbir", "Mudassir", ""], ["Khan", "Imdadullah", ""], ["Abbas", "Waseem", ""]]}, {"id": "2001.10719", "submitter": "Lekshmi Beena Gopalakrishnan Nair", "authors": "Lekshmi B.G., Andreas Becher, Klaus Meyer-Wegener", "title": "Query-Sequence Optimization on a Reconfigurable Hardware-Accelerated\n  System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hardware acceleration of database query processing can be done with the help\nof FPGAs. In particular, they are partially reconfigurable during runtime,\nwhich allows for the runtime adaption of the hardware to a variety of queries.\nReconfiguration itself, however, takes some time. As the affected area of the\nFPGA is not available for computations during the reconfiguration, avoiding\nsome of the reconfigurations can improve overall performance. This paper\npresents optimizations based on query sequences, which reduces the impact of\nthe reconfigurations. Knowledge of coming queries is used to (I) speculatively\nstart reconfiguration already when a query is still running and (II) avoid\noverwriting of reconfigurable regions that will be used again in subsequent\nqueries. We evaluate our optimizations with a calibrated model and measurements\nfor various parameter values. Improvements in execution time of up to 21% can\nbe obtained even with sequences of only two queries\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 08:23:13 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["G.", "Lekshmi B.", ""], ["Becher", "Andreas", ""], ["Meyer-Wegener", "Klaus", ""]]}, {"id": "2001.11100", "submitter": "Gezim Sejdiu", "authors": "Gezim Sejdiu, Anisa Rula, Jens Lehmann, and Hajira Jabeen", "title": "A Scalable Framework for Quality Assessment of RDF Datasets", "comments": "International Semantic Web Conference (ISWC 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over the last years, Linked Data has grown continuously. Today, we count more\nthan 10,000 datasets being available online following Linked Data standards.\nThese standards allow data to be machine readable and inter-operable.\nNevertheless, many applications, such as data integration, search, and\ninterlinking, cannot take full advantage of Linked Data if it is of low\nquality. There exist a few approaches for the quality assessment of Linked\nData, but their performance degrades with the increase in data size and quickly\ngrows beyond the capabilities of a single machine. In this paper, we present\nDistQualityAssessment -- an open source implementation of quality assessment of\nlarge RDF datasets that can scale out to a cluster of machines. This is the\nfirst distributed, in-memory approach for computing different quality metrics\nfor large RDF datasets using Apache Spark. We also provide a quality assessment\npattern that can be used to generate new scalable metrics that can be applied\nto big data. The work presented here is integrated with the SANSA framework and\nhas been applied to at least three use cases beyond the SANSA community. The\nresults show that our approach is more generic, efficient, and scalable as\ncompared to previously proposed approaches.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 21:30:14 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Sejdiu", "Gezim", ""], ["Rula", "Anisa", ""], ["Lehmann", "Jens", ""], ["Jabeen", "Hajira", ""]]}, {"id": "2001.11102", "submitter": "Yifeng Gao", "authors": "Yifeng Gao, Jessica Lin, Constantin Brif", "title": "Ensemble Grammar Induction For Detecting Anomalies in Time Series", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Time series anomaly detection is an important task, with applications in a\nbroad variety of domains. Many approaches have been proposed in recent years,\nbut often they require that the length of the anomalies be known in advance and\nprovided as an input parameter. This limits the practicality of the algorithms,\nas such information is often unknown in advance, or anomalies with different\nlengths might co-exist in the data. To address this limitation, previously, a\nlinear time anomaly detection algorithm based on grammar induction has been\nproposed. While the algorithm can find variable-length patterns, it still\nrequires preselecting values for at least two parameters at the discretization\nstep. How to choose these parameter values properly is still an open problem.\nIn this paper, we introduce a grammar-induction-based anomaly detection method\nutilizing ensemble learning. Instead of using a particular choice of parameter\nvalues for anomaly detection, the method generates the final result based on a\nset of results obtained using different parameter values. We demonstrate that\nthe proposed ensemble approach can outperform existing grammar-induction-based\napproaches with different criteria for selection of parameter values. We also\nshow that the proposed approach can achieve performance similar to that of the\nstate-of-the-art distance-based anomaly detection algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 21:33:03 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Gao", "Yifeng", ""], ["Lin", "Jessica", ""], ["Brif", "Constantin", ""]]}, {"id": "2001.11324", "submitter": "Basit Qureshi", "authors": "Basit Qureshi and Yasir Javed", "title": "Proceedings of Symposium on Data Mining Applications 2014", "comments": "Proceedings of Symposium on Data Mining Applications 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Symposium on Data Mining and Applications (SDMA 2014) is aimed to gather\nresearchers and application developers from a wide range of data mining related\nareas such as statistics, computational intelligence, pattern recognition,\ndatabases, Big Data Mining and visualization. SDMA is organized by MEGDAM to\nadvance the state of the art in data mining research field and its various real\nworld applications. The symposium will provide opportunities for technical\ncollaboration among data mining and machine learning researchers around the\nSaudi Arabia, GCC countries and Middle-East region. Acceptance will be based\nprimarily on originality, significance and quality of contribution.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 07:30:00 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Qureshi", "Basit", ""], ["Javed", "Yasir", ""]]}, {"id": "2001.11506", "submitter": "Egor Pushkin", "authors": "Egor Pushkin", "title": "Theoretical Model and Practical Considerations for Data Lineage\n  Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We live in a world driven by data. The amount of it outgrows anyone's ability\nto oversee it or even observe its scope. Along with all the advances in the\nspace of data management, there is still a significant lack of formalism and\nstandardization around defining data ecosystems and processes occurring within\nthose. In order to address the issue we propose a notation for data flow\nmodeling and evaluate some of the most common applications of it based on\nreal-world use cases. To facilitate future work, we provide detailed reference\nof the data model we defined and consider potential programming paradigms.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 07:45:18 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Pushkin", "Egor", ""]]}, {"id": "2001.11692", "submitter": "Xinyan Dai", "authors": "Xinyan Dai, Xiao Yan, Kaiwen Zhou, Yuxuan Wang, Han Yang, James Cheng", "title": "Convolutional Embedding for Edit Distance", "comments": "Accepted by the 43rd International ACM SIGIR Conference on Research\n  and Development in Information Retrieval, 2020", "journal-ref": null, "doi": "10.1145/3397271.3401045", "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edit-distance-based string similarity search has many applications such as\nspell correction, data de-duplication, and sequence alignment. However,\ncomputing edit distance is known to have high complexity, which makes string\nsimilarity search challenging for large datasets. In this paper, we propose a\ndeep learning pipeline (called CNN-ED) that embeds edit distance into Euclidean\ndistance for fast approximate similarity search. A convolutional neural network\n(CNN) is used to generate fixed-length vector embeddings for a dataset of\nstrings and the loss function is a combination of the triplet loss and the\napproximation error. To justify our choice of using CNN instead of other\nstructures (e.g., RNN) as the model, theoretical analysis is conducted to show\nthat some basic operations in our CNN model preserve edit distance.\nExperimental results show that CNN-ED outperforms data-independent CGK\nembedding and RNN-based GRU embedding in terms of both accuracy and efficiency\nby a large margin. We also show that string similarity search can be\nsignificantly accelerated using CNN-based embeddings, sometimes by orders of\nmagnitude.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 07:53:10 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2020 02:38:26 GMT"}, {"version": "v3", "created": "Fri, 22 May 2020 06:27:37 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Dai", "Xinyan", ""], ["Yan", "Xiao", ""], ["Zhou", "Kaiwen", ""], ["Wang", "Yuxuan", ""], ["Yang", "Han", ""], ["Cheng", "James", ""]]}]