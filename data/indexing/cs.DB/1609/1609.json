[{"id": "1609.00090", "submitter": "Xin Huang", "authors": "Xin Huang, Laks V.S. Lakshmanan", "title": "Attribute Truss Community Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, community search over graphs has attracted significant attention\nand many algorithms have been developed for finding dense subgraphs from large\ngraphs that contain given query nodes. In applications such as analysis of\nprotein protein interaction (PPI) networks, citation graphs, and collaboration\nnetworks, nodes tend to have attributes. Unfortunately, previously developed\ncommunity search algorithms ignore these attributes and result in communities\nwith poor cohesion w.r.t. their node attributes. In this paper, we study the\nproblem of attribute-driven community search, that is, given an undirected\ngraph $G$ where nodes are associated with attributes, and an input query $Q$\nconsisting of nodes $V_q$ and attributes $W_q$, find the communities containing\n$V_q$, in which most community members are densely inter-connected and have\nsimilar attributes.\n  We formulate our problem of finding attributed truss communities (ATC), as\nfinding all connected and close k-truss subgraphs containing $V_q$, that are\nlocally maximal and have the largest attribute relevance score among such\nsubgraphs. We design a novel attribute relevance score function and establish\nits desirable properties. The problem is shown to be NP-hard. However, we\ndevelop an efficient greedy algorithmic framework, which finds a maximal\n$k$-truss containing $V_q$, and then iteratively removes the nodes with the\nleast popular attributes and shrinks the graph so as to satisfy community\nconstraints. We also build an elegant index to maintain the known $k$-truss\nstructure and attribute information, and propose efficient query processing\nalgorithms. Extensive experiments on large real-world networks with\nground-truth communities shows the efficiency and effectiveness of our proposed\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2016 02:19:36 GMT"}, {"version": "v2", "created": "Fri, 2 Sep 2016 00:30:30 GMT"}, {"version": "v3", "created": "Tue, 14 Feb 2017 18:12:53 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Huang", "Xin", ""], ["Lakshmanan", "Laks V. S.", ""]]}, {"id": "1609.00117", "submitter": "Katsumi Kumai", "authors": "Katsumi Kumai, Yuhki Shiraishi, Jianwei Zhang, Hiroyuki Kitagawa and\n  Atsuyuki Morishima", "title": "Group Rotation Type Crowdsourcing", "comments": "2 non-reference pages + reference-only page, HCOMP2016, WiP paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common workflow to perform a continuous human task stream is to divide\nworkers into groups, have one group perform the newly-arrived task, and rotate\nthe groups. We call this type of workflow the group rotation. This paper\naddresses the problem of how to manage Group Rotation Type Crowdsourcing, the\ngroup rotation in a crowdsourcing setting. In the group-rotation type\ncrowdsourcing, we must change the group structure dynamically because workers\ncome in and leave frequently. This paper proposes an approach to explore a\ndesign space of methods for group restructuring in the group rotation type\ncrowdsourcing.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2016 05:55:02 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Kumai", "Katsumi", ""], ["Shiraishi", "Yuhki", ""], ["Zhang", "Jianwei", ""], ["Kitagawa", "Hiroyuki", ""], ["Morishima", "Atsuyuki", ""]]}, {"id": "1609.00791", "submitter": "Ruochen Jiang", "authors": "Ruochen Jiang, Jiannan Wang", "title": "Reprowd: Crowdsourced Data Processing Made Reproducible", "comments": "HCOMP 2016 Work in Progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing is a multidisciplinary research area including disciplines like\nartificial intelligence, human-computer interaction, database, and social\nscience. To facilitate cooperation across disciplines, reproducibility is a\ncrucial factor, but unfortunately, it has not gotten enough attention in the\nHCOMP community. In this paper, we present Reprowd, a system aiming to make it\neasy to reproduce crowdsourced data processing research. We have open sourced\nReprowd at http://sfu-db.github.io/reprowd/.\n", "versions": [{"version": "v1", "created": "Sat, 3 Sep 2016 04:35:57 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Jiang", "Ruochen", ""], ["Wang", "Jiannan", ""]]}, {"id": "1609.00988", "submitter": "Nhien-An Le-Khac", "authors": "Nhien-An Le-Khac, Martin Bue, Michael Whelan, Tahar Kechadi", "title": "A clustering-based data reduction for very large spatio-temporal\n  datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, huge amounts of data are being collected with spatial and temporal\ncomponents from sources such as meteorological, satellite imagery etc.\nEfficient visualisation as well as discovery of useful knowledge from these\ndatasets is therefore very challenging and becoming a massive economic need.\nData Mining has emerged as the technology to discover hidden knowledge in very\nlarge amounts of data. Furthermore, data mining techniques could be applied to\ndecrease the large size of raw data by retrieving its useful knowledge as\nrepresentatives. As a consequence, instead of dealing with a large size of raw\ndata, we can use these representatives to visualise or to analyse without\nlosing important information. This paper presents a new approach based on\ndifferent clustering techniques for data reduction to help analyse very large\nspatio-temporal data. We also present and discuss preliminary results of this\napproach.\n", "versions": [{"version": "v1", "created": "Sun, 4 Sep 2016 20:35:18 GMT"}, {"version": "v2", "created": "Wed, 29 Mar 2017 18:55:18 GMT"}], "update_date": "2017-03-31", "authors_parsed": [["Le-Khac", "Nhien-An", ""], ["Bue", "Martin", ""], ["Whelan", "Michael", ""], ["Kechadi", "Tahar", ""]]}, {"id": "1609.00990", "submitter": "Nhien-An Le-Khac", "authors": "Nhien-An Le-Khac, Sammer Markos, Tahar Kechadi", "title": "A data mining-based solution for detecting suspicious money laundering\n  cases in an investment bank", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, money laundering poses a serious threat not only to financial\ninstitutions but also to the nation. This criminal activity is becoming more\nand more sophisticated and seems to have moved from the clichy of drug\ntrafficking to financing terrorism and surely not forgetting personal gain.\nMost international financial institutions have been implementing anti-money\nlaundering solutions to fight investment fraud. However, traditional\ninvestigative techniques consume numerous man-hours. Recently, data mining\napproaches have been developed and are considered as well-suited techniques for\ndetecting money laundering activities. Within the scope of a collaboration\nproject for the purpose of developing a new solution for the anti-money\nlaundering Units in an international investment bank, we proposed a simple and\nefficient data mining-based solution for anti-money laundering. In this paper,\nwe present this solution developed as a tool and show some preliminary\nexperiment results with real transaction datasets.\n", "versions": [{"version": "v1", "created": "Sun, 4 Sep 2016 21:03:32 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Le-Khac", "Nhien-An", ""], ["Markos", "Sammer", ""], ["Kechadi", "Tahar", ""]]}, {"id": "1609.01319", "submitter": "Jonas Schneider", "authors": "Jonas Schneider", "title": "Analytic Performance Model of a Main-Memory Index Structure", "comments": "Bachelor's Thesis, 84 pages. Correspondence to\n  mail@jonasschneider.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient evaluation of multi-dimensional range queries in a main-memory\ndatabase is an important, but difficult task. State-of-the-art techniques rely\non optimised sequential scans or tree-based structures. For range queries with\nsmall result sets, sequential scans exhibit poor asymptotic performance. Also,\nas the dimensionality of the data set increases, the performance of tree-based\nstructures degenerates due to the curse of dimensionality. Recent literature\nproposed the Elf, a main-memory structure that is optimised for the case of\nsuch multi-dimensional low-selectivity queries. The Elf outperforms other\nstate-of-the-art methods in manually tuned scenarios. However, choosing an\noptimal parameter configuration for the Elf is vital, since for poor\nconfigurations, the search performance degrades rapidly. Consequently, further\nknowledge about the behaviour of the Elf in different configurations is\nrequired to achieve robust performance. In this thesis, we therefore propose a\nnumerical cost model for the Elf. Like all main-memory index structures, the\nElf response time is not dominated by disk accesses, refusing a straightforward\nanalysis. Our model predicts the size and shape of the Elf region that is\nexamined during search. We propose that the response time of a search is linear\nto the size of this region. Furthermore, we study the impact of skewed data\ndistributions and correlations on the shape of the Elf. We find that they lead\nto behaviour that is accurately describable through simple reductions in\nattribute cardinality. Our experimental results indicate that for data sets of\nup to 15 dimensions, our cost model predicts the size of the examined Elf\nregion with relative errors below 5%. Furthermore, we find that the size of the\nElf region examined during search predicts the response time with an accuracy\nof 80%.\n", "versions": [{"version": "v1", "created": "Mon, 5 Sep 2016 20:40:57 GMT"}, {"version": "v2", "created": "Wed, 5 Apr 2017 09:51:57 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Schneider", "Jonas", ""]]}, {"id": "1609.01882", "submitter": "Matthijs Douze", "authors": "Matthijs Douze, Herv\\'e J\\'egou and Florent Perronnin", "title": "Polysemous codes", "comments": "The final (author) version of our ECCV'16 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of approximate nearest neighbor search in\nthe compressed domain. We introduce polysemous codes, which offer both the\ndistance estimation quality of product quantization and the efficient\ncomparison of binary codes with Hamming distance. Their design is inspired by\nalgorithms introduced in the 90's to construct channel-optimized vector\nquantizers. At search time, this dual interpretation accelerates the search.\nMost of the indexed vectors are filtered out with Hamming distance, letting\nonly a fraction of the vectors to be ranked with an asymmetric distance\nestimator.\n  The method is complementary with a coarse partitioning of the feature space\nsuch as the inverted multi-index. This is shown by our experiments performed on\nseveral public benchmarks such as the BIGANN dataset comprising one billion\nvectors, for which we report state-of-the-art results for query times below\n0.3\\,millisecond per core. Last but not least, our approach allows the\napproximate computation of the k-NN graph associated with the Yahoo Flickr\nCreative Commons 100M, described by CNN image descriptors, in less than 8 hours\non a single machine.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 08:45:19 GMT"}, {"version": "v2", "created": "Mon, 10 Oct 2016 23:00:00 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Douze", "Matthijs", ""], ["J\u00e9gou", "Herv\u00e9", ""], ["Perronnin", "Florent", ""]]}, {"id": "1609.01893", "submitter": "Ali Ben Ammar", "authors": "Ali Ben Ammar", "title": "Query Optimization Techniques In Graph Databases", "comments": null, "journal-ref": "International Journal of Database Management Systems ( IJDMS )\n  Vol.8, No.4, August 2016", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph databases (GDB) have recently been arisen to overcome the limits of\ntraditional databases for storing and managing data with graph-like structure.\nToday, they represent a requirement for many applications that manage\ngraph-like data, like social networks. Most of the techniques, applied to\noptimize queries in graph databases, have been used in traditional databases,\ndistribution systems... or they are inspired from graph theory. However, their\nreuse in graph databases should take care of the main characteristics of graph\ndatabases, such as dynamic structure, highly interconnected data, and ability\nto efficiently access data relationships. In this paper, we survey the query\noptimization techniques in graph databases. In particular, we focus on the\nfeatures they have introduced to improve querying graph-like data.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 09:08:28 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Ammar", "Ali Ben", ""]]}, {"id": "1609.02031", "submitter": "Nhien-An Le-Khac", "authors": "Nhien-An Le-Khac, Sammer Markos, Michael O'Neill, Anthony Brabazon and\n  Tahar Kechadi", "title": "An efficient Search Tool for an Anti-Money Laundering Application of an\n  Multi-national Bank's Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, money laundering (ML) poses a serious threat not only to financial\ninstitutions but also to the nations. This criminal activity is becoming more\nand more sophisticated and seems to have moved from the clichy of drug\ntrafficking to financing terrorism and surely not forgetting personal gain.\nMost of the financial institutions internationally have been implementing\nanti-money laundering solutions (AML) to fight investment fraud activities. In\nAML, the customer identification is an important task which helps AML experts\nto monitor customer habits: some being customer domicile, transactions that\nthey are involved in etc. However, simple query tools provided by current DBMS\nas well as naive approaches in customer searching may produce incorrect and\nambiguous results and their processing time is also very high due to the\ncomplexity of the database system architecture. In this paper, we present a new\napproach for identifying customers registered in an investment bank. This\napproach is developed as a tool that allows AML experts to quickly identify\ncustomers who are managed independently across separate databases. It is tested\non real-world datasets, which are real and large financial datasets. Some\npreliminary experimental results show that this new approach is efficient and\neffective.\n", "versions": [{"version": "v1", "created": "Sun, 4 Sep 2016 20:17:45 GMT"}, {"version": "v2", "created": "Tue, 28 Mar 2017 21:30:08 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["Le-Khac", "Nhien-An", ""], ["Markos", "Sammer", ""], ["O'Neill", "Michael", ""], ["Brabazon", "Anthony", ""], ["Kechadi", "Tahar", ""]]}, {"id": "1609.02104", "submitter": "Yue Wang", "authors": "Yue Wang, Alexandra Meliou, Gerome Miklau", "title": "A Consumer-Centric Market for Database Computation in the Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of public computing resources in the cloud has\nrevolutionized data analysis, but requesting cloud resources often involves\ncomplex decisions for consumers. Under the current pricing mechanisms, cloud\nservice providers offer several service options and charge consumers based on\nthe resources they use. Before they can decide which cloud resources to\nrequest, consumers have to estimate the completion time and cost of their\ncomputational tasks for different service options and possibly for different\nservice providers. This estimation is challenging even for expert cloud users.\nWe propose a new market-based framework for pricing computational tasks in the\ncloud. Our framework introduces an agent between consumers and cloud providers.\nThe agent takes data and computational tasks from users, estimates time and\ncost for evaluating the tasks, and returns to consumers contracts that specify\nthe price and completion time. Our framework can be applied directly to\nexisting cloud markets without altering the way cloud providers offer and price\nservices. In addition, it simplifies cloud use for consumers by allowing them\nto compare contracts, rather than choose resources directly. We present design,\nanalytical, and algorithmic contributions focusing on pricing computation\ncontracts, analyzing their properties, and optimizing them in complex\nworkflows. We conduct an experimental evaluation of our market framework over a\nreal-world cloud service and demonstrate empirically that our market ensures\nthree key properties: competitiveness, fairness, and resilience. Finally, we\npresent a fine-grained pricing mechanism for complex workflows and show that it\ncan increase agent profits by more than an order of magnitude in some cases.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 18:42:09 GMT"}, {"version": "v2", "created": "Sun, 11 Sep 2016 06:25:36 GMT"}, {"version": "v3", "created": "Fri, 16 Sep 2016 14:19:02 GMT"}, {"version": "v4", "created": "Thu, 17 Nov 2016 19:55:15 GMT"}, {"version": "v5", "created": "Tue, 22 Nov 2016 02:31:53 GMT"}, {"version": "v6", "created": "Fri, 16 Jun 2017 20:40:04 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Wang", "Yue", ""], ["Meliou", "Alexandra", ""], ["Miklau", "Gerome", ""]]}, {"id": "1609.03095", "submitter": "Davood Rafiei", "authors": "Zhaoyang Shao, Davood Rafiei, Themis Palpanas", "title": "Efficient Error-tolerant Search on Knowledge Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge-labeled graphs are widely used to describe relationships between\nentities in a database. Given a query subgraph that represents an example of\nwhat the user is searching for, we study the problem of efficiently searching\nfor similar subgraphs in a large data graph, where the similarity is defined in\nterms of the well-known graph edit distance. We call these queries\n\"error-tolerant exemplar queries\" since matches are allowed despite small\nvariations in the graph structure and the labels. The problem in its general\ncase is computationally intractable, but efficient solutions are reachable for\nlabeled graphs under well-behaved distribution of the labels, commonly found in\nknowledge graphs. We propose two efficient exact algorithms, based on a\nfiltering-and-verification framework, for finding subgraphs in a large data\ngraph that are isomorphic to a query graph under some edit operations. Our\nfiltering scheme, which uses the neighbourhood structure around a node and the\npresence or absence of paths, significantly reduces the number of candidates\nthat are passed to the verification stage. Moreover, we analyze the costs of\nour algorithms and the conditions under which one algorithm is expected to\noutperform the other. Our analysis identifies some of the variables that affect\nthe cost, including the number and the selectivity of query edge labels and the\ndegree of nodes in the data graph, and characterizes their relationships. We\nempirically evaluate the effectiveness of our filtering schemes and queries,\nthe efficiency of our algorithms and the reliability of our cost models on real\ndatasets.\n", "versions": [{"version": "v1", "created": "Sat, 10 Sep 2016 22:08:51 GMT"}, {"version": "v2", "created": "Tue, 13 Sep 2016 15:05:36 GMT"}, {"version": "v3", "created": "Fri, 1 Dec 2017 23:47:48 GMT"}, {"version": "v4", "created": "Mon, 11 May 2020 15:01:03 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Shao", "Zhaoyang", ""], ["Rafiei", "Davood", ""], ["Palpanas", "Themis", ""]]}, {"id": "1609.03346", "submitter": "Xuhui Li", "authors": "Xuhui Li", "title": "A Meaning-oriented Approach to Semantic Data Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic information is often represented as the entities and the\nrelationships among them with conventional semantic models. This approach is\nstraightforward but is not suitable for many posteriori requests in semantic\ndata modeling. In this paper, we propose a meaning-oriented approach to\nmodeling semantic data and establish a graph-based semantic data model. In this\napproach we use the meanings, i.e., the subjective views of the entities and\nrelationships, to describe the semantic information, and use the semantic\ngraphs containing the meaning nodes and the meta-meaning relations to specify\nthe taxonomy and the compound construction of the semantic concepts. We\ndemonstrate how this meaning-oriented approach can address many important\nsemantic representation issues, including dynamic specialization and natural\njoin.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 11:16:35 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Li", "Xuhui", ""]]}, {"id": "1609.03540", "submitter": "Babak Salimi", "authors": "Babak Salimi, Dan Suciu", "title": "ZaliQL: A SQL-Based Framework for Drawing Causal Inference from Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal inference from observational data is a subject of active research and\ndevelopment in statistics and computer science. Many toolkits have been\ndeveloped for this purpose that depends on statistical software. However, these\ntoolkits do not scale to large datasets. In this paper we describe a suite of\ntechniques for expressing causal inference tasks from observational data in\nSQL. This suite supports the state-of-the-art methods for causal inference and\nrun at scale within a database engine. In addition, we introduce several\noptimization techniques that significantly speedup causal inference, both in\nthe online and offline setting. We evaluate the quality and performance of our\ntechniques by experiments of real datasets.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 19:24:14 GMT"}, {"version": "v2", "created": "Tue, 13 Sep 2016 01:59:05 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Salimi", "Babak", ""], ["Suciu", "Dan", ""]]}, {"id": "1609.05020", "submitter": "Alejandro Vaisman Dr.", "authors": "Bart Kuijpers and Alejandro Vaisman", "title": "A Formal Algebra for OLAP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online Analytical Processing (OLAP) comprises tools and algorithms that allow\nquerying multidimensional databases. It is based on the multidimensional model,\nwhere data can be seen as a cube, where each cell contains one or more measures\ncan be aggregated along dimensions. Despite the extensive corpus of work in the\nfield, a standard language for OLAP is still needed, since there is no\nwell-defined, accepted semantics, for many of the usual OLAP operations. In\nthis paper, we address this problem, and present a set of operations for\nmanipulating a data cube. We clearly define the semantics of these operations,\nand prove that they can be composed, yielding a language powerful enough to\nexpress complex OLAP queries. We express these operations as a sequence of\natomic transformations over a fixed multidimensional matrix, whose cells\ncontain a sequence of measures. Each atomic transformation produces a new\nmeasure. When a sequence of transformations defines an OLAP operation, a flag\nis produced indicating which cells must be considered as input for the next\noperation. In this way, an elegant algebra is defined. Our main contribution,\nwith respect to other similar efforts in the field is that, for the first time,\na formal proof of the correctness of the operations is given, thus providing a\nclear semantics for them. We believe the present work will serve as a basis to\nbuild more solid practical tools for data analysis.\n", "versions": [{"version": "v1", "created": "Fri, 16 Sep 2016 12:17:34 GMT"}], "update_date": "2016-09-19", "authors_parsed": [["Kuijpers", "Bart", ""], ["Vaisman", "Alejandro", ""]]}, {"id": "1609.05096", "submitter": "Yongchao Tian", "authors": "Yongchao Tian, Ioannis Alagiannis, Erietta Liarou, Anastasia Ailamaki,\n  Pietro Michiardi, Marko Vukolic", "title": "DiNoDB: an Interactive-speed Query Engine for Ad-hoc Queries on\n  Temporary Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As data sets grow in size, analytics applications struggle to get instant\ninsight into large datasets. Modern applications involve heavy batch processing\njobs over large volumes of data and at the same time require efficient ad-hoc\ninteractive analytics on temporary data. Existing solutions, however, typically\nfocus on one of these two aspects, largely ignoring the need for synergy\nbetween the two. Consequently, interactive queries need to re-iterate costly\npasses through the entire dataset (e.g., data loading) that may provide\nmeaningful return on investment only when data is queried over a long period of\ntime. In this paper, we propose DiNoDB, an interactive-speed query engine for\nad-hoc queries on temporary data. DiNoDB avoids the expensive loading and\ntransformation phase that characterizes both traditional RDBMSs and current\ninteractive analytics solutions. It is tailored to modern workflows found in\nmachine learning and data exploration use cases, which often involve iterations\nof cycles of batch and interactive analytics on data that is typically useful\nfor a narrow processing window. The key innovation of DiNoDB is to piggyback on\nthe batch processing phase the creation of metadata that DiNoDB exploits to\nexpedite the interactive queries. Our experimental analysis demonstrates that\nDiNoDB achieves very good performance for a wide range of ad-hoc queries\ncompared to alternatives %such as Hive, Stado, SparkSQL and Impala.\n", "versions": [{"version": "v1", "created": "Fri, 16 Sep 2016 14:56:31 GMT"}], "update_date": "2016-09-19", "authors_parsed": [["Tian", "Yongchao", ""], ["Alagiannis", "Ioannis", ""], ["Liarou", "Erietta", ""], ["Ailamaki", "Anastasia", ""], ["Michiardi", "Pietro", ""], ["Vukolic", "Marko", ""]]}, {"id": "1609.05103", "submitter": "Martin Theobald", "authors": "Maximilian Dylla, Martin Theobald", "title": "Learning Tuple Probabilities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning the parameters of complex probabilistic-relational models from\nlabeled training data is a standard technique in machine learning, which has\nbeen intensively studied in the subfield of Statistical Relational Learning\n(SRL), but---so far---this is still an under-investigated topic in the context\nof Probabilistic Databases (PDBs). In this paper, we focus on learning the\nprobability values of base tuples in a PDB from labeled lineage formulas. The\nresulting learning problem can be viewed as the inverse problem to confidence\ncomputations in PDBs: given a set of labeled query answers, learn the\nprobability values of the base tuples, such that the marginal probabilities of\nthe query answers again yield in the assigned probability labels. We analyze\nthe learning problem from a theoretical perspective, cast it into an\noptimization problem, and provide an algorithm based on stochastic gradient\ndescent. Finally, we conclude by an experimental evaluation on three real-world\nand one synthetic dataset, thus comparing our approach to various techniques\nfrom SRL, reasoning in information extraction, and optimization.\n", "versions": [{"version": "v1", "created": "Fri, 16 Sep 2016 15:16:25 GMT"}, {"version": "v2", "created": "Tue, 20 Sep 2016 06:36:11 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Dylla", "Maximilian", ""], ["Theobald", "Martin", ""]]}, {"id": "1609.05113", "submitter": "Yongchao Tian", "authors": "Yongchao Tian, Pietro Michiardi, Marko Vukolic", "title": "Bleach: A Distributed Stream Data Cleaning System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of rule-based stream data cleaning,\nwhich sets stringent requirements on latency, rule dynamics and ability to cope\nwith the unbounded nature of data streams.\n  We design a system, called Bleach, which achieves real-time violation\ndetection and data repair on a dirty data stream. Bleach relies on efficient,\ncompact and distributed data structures to maintain the necessary state to\nrepair data, using an incremental version of the equivalence class algorithm.\nAdditionally, it supports rule dynamics and uses a \"cumulative\" sliding window\noperation to improve cleaning accuracy.\n  We evaluate a prototype of Bleach using a TPC-DS derived dirty data stream\nand observe its high throughput, low latency and high cleaning accuracy, even\nwith rule dynamics. Experimental results indicate superior performance of\nBleach compared to a baseline system built on the micro-batch streaming\nparadigm.\n", "versions": [{"version": "v1", "created": "Fri, 16 Sep 2016 15:52:44 GMT"}], "update_date": "2016-09-19", "authors_parsed": [["Tian", "Yongchao", ""], ["Michiardi", "Pietro", ""], ["Vukolic", "Marko", ""]]}, {"id": "1609.05293", "submitter": "Martin Theobald", "authors": "Sairam Gurajada, Martin Theobald", "title": "Distributed Processing of Generalized Graph-Pattern Queries in SPARQL\n  1.1", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient and scalable architecture for processing generalized\ngraph-pattern queries as they are specified by the current W3C recommendation\nof the SPARQL 1.1 \"Query Language\" component. Specifically, the class of\nqueries we consider consists of sets of SPARQL triple patterns with labeled\nproperty paths. From a relational perspective, this class resolves to\nconjunctive queries of relational joins with additional graph-reachability\npredicates. For the scalable, i.e., distributed, processing of this kind of\nqueries over very large RDF collections, we develop a suitable partitioning and\nindexing scheme, which allows us to shard the RDF triples over an entire\ncluster of compute nodes and to process an incoming SPARQL query over all of\nthe relevant graph partitions (and thus compute nodes) in parallel. Unlike most\nprior works in this field, we specifically aim at the unified optimization and\ndistributed processing of queries consisting of both relational joins and\ngraph-reachability predicates. All communication among the compute nodes is\nestablished via a proprietary, asynchronous communication protocol based on the\nMessage Passing Interface.\n", "versions": [{"version": "v1", "created": "Sat, 17 Sep 2016 08:06:28 GMT"}, {"version": "v2", "created": "Tue, 20 Dec 2016 13:58:40 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Gurajada", "Sairam", ""], ["Theobald", "Martin", ""]]}, {"id": "1609.05359", "submitter": "Praveen Rao", "authors": "Praveen Rao, Anas Katib, Daniel E. Lopez Barron", "title": "A Knowledge Ecosystem for the Food, Energy, and Water System", "comments": "KDD 2016 Workshop on Data Science for Food, Energy and Water, Aug\n  13-17, 2016, San Francisco, CA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Food, energy, and water (FEW) are key resources to sustain human life and\neconomic growth. There is an increasing stress on these interconnected\nresources due to population growth, natural disasters, and human activities.\nNew research is necessary to foster more efficient, more secure, and safer use\nof FEW resources in the U.S. and globally. In this position paper, we present\nthe idea of a knowledge ecosystem for enabling the semantic data integration of\nheterogeneous datasets in the FEW system to promote knowledge discovery and\nsuperior decision making through semantic reasoning. Rich, diverse datasets\npublished by U.S. federal agencies will be utilized. Our knowledge ecosystem\nwill build on Semantic Web technologies and advances in statistical relational\nlearning to (a) represent, integrate, and harmonize diverse data sources and\n(b) perform ontology-based reasoning to discover actionable insights from FEW\ndatasets.\n", "versions": [{"version": "v1", "created": "Sat, 17 Sep 2016 16:27:38 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Rao", "Praveen", ""], ["Katib", "Anas", ""], ["Barron", "Daniel E. Lopez", ""]]}, {"id": "1609.05401", "submitter": "Jose Alberto Garc\\'ia Guti\\'errez Sr.", "authors": "Jose A. Garc\\'ia Guti\\'errez", "title": "Applications of Data Mining (DM) in Science and Engineering: State of\n  the art and perspectives", "comments": "in Spanish", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The continuous increase in the availability of data of any kind, coupled with\nthe development of networks of high-speed communications, the popularization of\ncloud computing and the growth of data centers and the emergence of\nhigh-performance computing does essential the task to develop techniques that\nallow more efficient data processing and analyzing of large volumes datasets\nand extraction of valuable information. In the following pages we will discuss\nabout development of this field in recent decades, and its potential and\napplicability present in the various branches of scientific research. Also, we\ntry to review briefly the different families of algorithms that are included in\ndata mining research area, its scalability with increasing dimensionality of\nthe input data and how they can be addressed and what behavior different\nmethods in a scenario in which the information is distributed or decentralized\nprocessed so as to increment performance optimization in heterogeneous\nenvironments.\n", "versions": [{"version": "v1", "created": "Sat, 17 Sep 2016 22:22:17 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Guti\u00e9rrez", "Jose A. Garc\u00eda", ""]]}, {"id": "1609.06019", "submitter": "Lu\\'is Cruz-Filipe", "authors": "Lu\\'is Cruz-Filipe, Gra\\c{c}a Gaspar, Isabel Nunes, Peter\n  Schneider-Kamp", "title": "Active Integrity Constraints for Multi-Context Systems", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-49004-5_7", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a formalism to couple integrity constraints over general-purpose\nknowledge bases with actions that can be executed to restore consistency. This\nformalism generalizes active integrity constraints over databases. In the more\ngeneral setting of multi-context systems, adding repair suggestions to\nintegrity constraints allows defining simple iterative algorithms to find all\npossible grounded repairs - repairs for the global system that follow the\nsuggestions given by the actions in the individual rules. We apply our\nmethodology to ontologies, and show that it can express most relevant types of\nintegrity constraints in this domain.\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2016 05:03:58 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Cruz-Filipe", "Lu\u00eds", ""], ["Gaspar", "Gra\u00e7a", ""], ["Nunes", "Isabel", ""], ["Schneider-Kamp", "Peter", ""]]}, {"id": "1609.06265", "submitter": "Faizan Javed", "authors": "Janani Balaji, Faizan Javed, Mayank Kejriwal, Chris Min, Sam Sander\n  and Ozgur Ozturk", "title": "An Ensemble Blocking Scheme for Entity Resolution of Large and Sparse\n  Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity Resolution, also called record linkage or deduplication, refers to the\nprocess of identifying and merging duplicate versions of the same entity into a\nunified representation. The standard practice is to use a Rule based or Machine\nLearning based model that compares entity pairs and assigns a score to\nrepresent the pairs' Match/Non-Match status. However, performing an exhaustive\npair-wise comparison on all pairs of records leads to quadratic matcher\ncomplexity and hence a Blocking step is performed before the Matching to group\nsimilar entities into smaller blocks that the matcher can then examine\nexhaustively. Several blocking schemes have been developed to efficiently and\neffectively block the input dataset into manageable groups. At CareerBuilder\n(CB), we perform deduplication on massive datasets of people profiles collected\nfrom disparate sources with varying informational content. We observed that,\nemploying a single blocking technique did not cover the base for all possible\nscenarios due to the multi-faceted nature of our data sources. In this paper,\nwe describe our ensemble approach to blocking that combines two different\nblocking techniques to leverage their respective strengths.\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2016 17:44:28 GMT"}, {"version": "v2", "created": "Wed, 21 Sep 2016 00:26:17 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Balaji", "Janani", ""], ["Javed", "Faizan", ""], ["Kejriwal", "Mayank", ""], ["Min", "Chris", ""], ["Sander", "Sam", ""], ["Ozturk", "Ozgur", ""]]}, {"id": "1609.06526", "submitter": "Ladan Golshanara", "authors": "Ladan Golshanara, Jan Chomicki", "title": "Temporal Data Exchange", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data exchange is the problem of transforming data that is structured under a\nsource schema into data structured under another schema, called the target\nschema, so that both the source and target data satisfy the relationship\nbetween the schemas. Even though the formal framework of data exchange for\nrelational database systems is well-established, it does not immediately carry\nover to the settings of temporal data, which necessitates reasoning over\nunbounded periods of time. In this work, we study data exchange for temporal\ndata. We first motivate the need for two views of temporal data: the concrete\nview, which depicts how temporal data is compactly represented and on which the\nimplementations are based, and the abstract view, which defines the semantics\nof temporal data as a sequence of snapshots. We first extend the chase\nprocedure for the abstract view to have a conceptual basis for the data\nexchange for temporal databases. Considering non-temporal source-to-target\ntuple generating dependencies and equality generating dependencies, the chase\nalgorithm can be applied on each snapshot independently. Then we define a chase\nprocedure (called c-chase) on concrete instances and show the result of c-chase\non a concrete instance is semantically aligned with the result of chase on the\ncorresponding abstract instance. In order to interpret intervals as constants\nwhile checking if a dependency or a query is satisfied by a concrete database,\nwe will normalize the instance with respect to the dependency or the query. To\nobtain the semantic alignment, the nulls in the concrete view are annotated\nwith temporal information. Furthermore, we show that the result of the concrete\nchase provides a foundation for query answering. We define naive evaluation on\nthe result of the c-chase and show it produces certain answers.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 12:32:06 GMT"}, {"version": "v2", "created": "Thu, 14 Feb 2019 22:13:48 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Golshanara", "Ladan", ""], ["Chomicki", "Jan", ""]]}, {"id": "1609.06670", "submitter": "Natacha Crooks", "authors": "Natacha Crooks and Youer Pu and Lorenzo Alvisi and Allen Clement", "title": "Seeing is Believing: A Unified Model for Consistency and Isolation via\n  States", "comments": "11 pages with 29 pages appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a unified model of consistency and isolation that\nminimizes the gap between how these guarantees are defined and how they are\nperceived. Our approach is premised on a simple observation: applications view\nstorage systems as black-boxes that transition through a series of states, a\nsubset of which are observed by applications. For maximum clarity, isolation\nand consistency guarantees should be expressed as constraints on those states.\nInstead, these properties are currently expressed as constraints on operation\nhistories that are not visible to the application. We show that adopting a\nstate-based approach to expressing these guarantees brings forth several\nbenefits. First, it makes it easier to focus on the anomalies that a given\nisolation or consistency level allows (and that applications must deal with),\nrather than those that it proscribes. Second, it unifies the often disparate\ntheories of isolation and consistency and provides a structure for composing\nthese guarantees. We leverage this modularity to apply to transactions\n(independently of the isolation level under which they execute) the equivalence\nbetween causal consistency and session guarantees that Chockler et al. had\nproved for single operations. Third, it brings clarity to the increasingly\ncrowded field of proposed consistency and isolation properties by winnowing\nspurious distinctions: we find that the recently proposed parallel snapshot\nisolation introduced by Sovran et al. is in fact a specific implementation of\nan older guarantee, lazy consistency (or PL-2+), introduced by Adya et al.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 18:34:54 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Crooks", "Natacha", ""], ["Pu", "Youer", ""], ["Alvisi", "Lorenzo", ""], ["Clement", "Allen", ""]]}, {"id": "1609.07545", "submitter": "Jeremy Kepner", "authors": "Siddharth Samsi, Laura Brattain, William Arcand, David Bestor, Bill\n  Bergeron, Chansup Byun, Vijay Gadepally, Michael Houle, Matthew Hubbell,\n  Michael Jones, Anna Klein, Peter Michaleas, Lauren Milechin, Julie Mullen,\n  Andrew Prout, Antonio Rosa, Charles Yee, Jeremy Kepner, Albert Reuther", "title": "Benchmarking SciDB Data Import on HPC Systems", "comments": "5 pages, 4 figures, IEEE High Performance Extreme Computing (HPEC)\n  2016, best paper finalist", "journal-ref": null, "doi": "10.1109/HPEC.2016.7761617", "report-no": null, "categories": "cs.DB cs.DC cs.PF q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SciDB is a scalable, computational database management system that uses an\narray model for data storage. The array data model of SciDB makes it ideally\nsuited for storing and managing large amounts of imaging data. SciDB is\ndesigned to support advanced analytics in database, thus reducing the need for\nextracting data for analysis. It is designed to be massively parallel and can\nrun on commodity hardware in a high performance computing (HPC) environment. In\nthis paper, we present the performance of SciDB using simulated image data. The\nDynamic Distributed Dimensional Data Model (D4M) software is used to implement\nthe benchmark on a cluster running the MIT SuperCloud software stack. A peak\nperformance of 2.2M database inserts per second was achieved on a single node\nof this system. We also show that SciDB and the D4M toolbox provide more\nefficient ways to access random sub-volumes of massive datasets compared to the\ntraditional approaches of reading volumetric data from individual files. This\nwork describes the D4M and SciDB tools we developed and presents the initial\nperformance results. This performance was achieved by using parallel inserts, a\nin-database merging of arrays as well as supercomputing techniques, such as\ndistributed arrays and single-program-multiple-data programming.\n", "versions": [{"version": "v1", "created": "Sat, 24 Sep 2016 01:01:30 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Samsi", "Siddharth", ""], ["Brattain", "Laura", ""], ["Arcand", "William", ""], ["Bestor", "David", ""], ["Bergeron", "Bill", ""], ["Byun", "Chansup", ""], ["Gadepally", "Vijay", ""], ["Houle", "Michael", ""], ["Hubbell", "Matthew", ""], ["Jones", "Michael", ""], ["Klein", "Anna", ""], ["Michaleas", "Peter", ""], ["Milechin", "Lauren", ""], ["Mullen", "Julie", ""], ["Prout", "Andrew", ""], ["Rosa", "Antonio", ""], ["Yee", "Charles", ""], ["Kepner", "Jeremy", ""], ["Reuther", "Albert", ""]]}, {"id": "1609.07548", "submitter": "Jeremy Kepner", "authors": "Vijay Gadepally, Peinan Chen, Jennie Duggan, Aaron Elmore, Brandon\n  Haynes, Jeremy Kepner, Samuel Madden, Tim Mattson, Michael Stonebraker", "title": "The BigDAWG Polystore System and Architecture", "comments": "6 pages, 5 figures, IEEE High Performance Extreme Computing (HPEC)\n  conference 2016", "journal-ref": null, "doi": "10.1109/HPEC.2016.7761636", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Organizations are often faced with the challenge of providing data management\nsolutions for large, heterogenous datasets that may have different underlying\ndata and programming models. For example, a medical dataset may have\nunstructured text, relational data, time series waveforms and imagery. Trying\nto fit such datasets in a single data management system can have adverse\nperformance and efficiency effects. As a part of the Intel Science and\nTechnology Center on Big Data, we are developing a polystore system designed\nfor such problems. BigDAWG (short for the Big Data Analytics Working Group) is\na polystore system designed to work on complex problems that naturally span\nacross different processing or storage engines. BigDAWG provides an\narchitecture that supports diverse database systems working with different data\nmodels, support for the competing notions of location transparency and semantic\ncompleteness via islands and a middleware that provides a uniform multi--island\ninterface. Initial results from a prototype of the BigDAWG system applied to a\nmedical dataset validate polystore concepts. In this article, we will describe\npolystore databases, the current BigDAWG architecture and its application on\nthe MIMIC II medical dataset, initial performance results and our future\ndevelopment plans.\n", "versions": [{"version": "v1", "created": "Sat, 24 Sep 2016 01:14:06 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Gadepally", "Vijay", ""], ["Chen", "Peinan", ""], ["Duggan", "Jennie", ""], ["Elmore", "Aaron", ""], ["Haynes", "Brandon", ""], ["Kepner", "Jeremy", ""], ["Madden", "Samuel", ""], ["Mattson", "Tim", ""], ["Stonebraker", "Michael", ""]]}, {"id": "1609.07823", "submitter": "Jayanth Jayanth", "authors": "Jayanth Jayanth", "title": "Optimizations and Heuristics to improve Compression in Columnar Database\n  Systems", "comments": "Author keywords: Database Compression, Database Optimization,\n  Encoding Schemes, In-memory Columnar Databases. Key-phrases: block size,\n  cluster encoding, indirect encoding. Content Information: 4 pages, 5\n  sections, 2 figures, 1 table, 2 algorithms, 11 equations and 4 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In-memory columnar databases have become mainstream over the last decade and\nhave vastly improved the fast processing of large volumes of data through\nmulti-core parallelism and in-memory compression thereby eliminating the usual\nbottlenecks associated with disk-based databases. For scenarios, where the data\nvolume grows into terabytes and petabytes, keeping all the data in memory is\nexorbitantly expensive. Hence, the data is compressed efficiently using\ndifferent algorithms to exploit the multi-core parallelization technologies for\nquery processing. Several compression methods are studied for compressing the\ncolumn array, post Dictionary Encoding. In this paper, we will present two\nnovel optimizations in compression techniques - Block Size Optimized Cluster\nEncoding and Block Size Optimized Indirect Encoding - which perform better than\ntheir predecessors. In the end, we also propose heuristics to choose the best\nencoding amongst common compression schemes.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 00:44:51 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Jayanth", "Jayanth", ""]]}, {"id": "1609.07964", "submitter": "Jianzhong Qi", "authors": "Jianzhong Qi, Fei Zuo, Hanan Samet, Jia Cheng Yao", "title": "K-Regret Queries Using Multiplicative Utility Functions", "comments": null, "journal-ref": "ACM Transactions on Database Systems (TODS), 43, 2, Article 10\n  (August 2018)", "doi": "10.1145/3230634", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The k-regret query aims to return a size-k subset S of a database D such\nthat, for any query user that selects a data object from this size-k subset S\nrather than from database D, her regret ratio is minimized. The regret ratio\nhere is modeled by the relative difference in the optimality between the\nlocally optimal object in S and the globally optimal object in D. The\noptimality of a data object in turn is modeled by a utility function of the\nquery user. Unlike traditional top-k queries, the k-regret query does not\nminimize the regret ratio for a specific utility function. Instead, it\nconsiders a family of infinite utility functions F, and aims to find a size-k\nsubset that minimizes the maximum regret ratio of any utility function in F.\n  Studies on k-regret queries have focused on the family of additive utility\nfunctions, which have limitations in modeling individuals' preferences and\ndecision making processes, especially for a common observation called the\ndiminishing marginal rate of substitution (DMRS). We introduce k-regret queries\nwith multiplicative utility functions, which are more expressive in modeling\nthe DMRS, to overcome those limitations. We propose a query algorithm with\nbounded regret ratios. To showcase the applicability of the algorithm, we apply\nit to a special family of multiplicative utility functions, the Cobb-Douglas\nfamily of utility functions, and a closely related family of utility functions,\nthe Constant Elasticity of Substitution family of utility functions, both of\nwhich are frequently used utility functions in microeconomics. After a further\nstudy of the query properties, we propose a heuristic algorithm that produces\neven smaller regret ratios in practice. Extensive experiments on the proposed\nalgorithms confirm that they consistently achieve small maximum regret ratios.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 13:23:43 GMT"}, {"version": "v2", "created": "Tue, 27 Sep 2016 05:16:18 GMT"}, {"version": "v3", "created": "Sat, 1 Oct 2016 04:55:50 GMT"}, {"version": "v4", "created": "Tue, 11 Sep 2018 04:47:04 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Qi", "Jianzhong", ""], ["Zuo", "Fei", ""], ["Samet", "Hanan", ""], ["Yao", "Jia Cheng", ""]]}, {"id": "1609.07983", "submitter": "Maryam Fanaeepour", "authors": "Maryam Fanaeepour, Benjamin I. P. Rubinstein", "title": "Differentially-Private Counting of Users' Spatial Regions", "comments": "27 pages, 14 figures", "journal-ref": "Knowl.Inf.Syst 54 (2018) 5-32", "doi": "10.1007/s10115-017-1113-6", "report-no": null, "categories": "cs.DB cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining of spatial data is an enabling technology for mobile services,\nInternet-connected cars, and the Internet of Things. But the very\ndistinctiveness of spatial data that drives utility, can cost user privacy.\nPast work has focused upon points and trajectories for differentially-private\nrelease. In this work, we continue the tradition of privacy-preserving spatial\nanalytics, focusing not on point or path data, but on planar spatial regions.\nSuch data represents the area of a user's most frequent visitation---such as\n\"around home and nearby shops\". Specifically we consider the\ndifferentially-private release of data structures that support range queries\nfor counting users' spatial regions. Counting planar regions leads to unique\nchallenges not faced in existing work. A user's spatial region that straddles\nmultiple data structure cells can lead to duplicate counting at query time. We\nprovably avoid this pitfall by leveraging the Euler characteristic for the\nfirst time with differential privacy. To address the increased sensitivity of\nrange queries to spatial region data, we calibrate privacy-preserving noise\nusing bounded user region size and a constrained inference that uses robust\nleast absolute deviations. Our novel constrained inference reduces noise and\npromotes covertness by (privately) imposing consistency. We provide a full\nend-to-end theoretical analysis of both differential privacy and\nhigh-probability utility for our approach using concentration bounds. A\ncomprehensive experimental study on several real-world datasets establishes\npractical validity.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 14:24:53 GMT"}, {"version": "v2", "created": "Mon, 5 Feb 2018 20:17:15 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Fanaeepour", "Maryam", ""], ["Rubinstein", "Benjamin I. P.", ""]]}, {"id": "1609.08201", "submitter": "Shahriar Shariat", "authors": "Shahriar Shariat and Vladimir Pavlovic", "title": "Robust Time-Series Retrieval Using Probabilistic Adaptive Segmental\n  Alignment", "comments": null, "journal-ref": "Knowl Inf Syst (2016) 49: 91. doi:10.1007/s10115-015-0898-4", "doi": "10.1007/s10115-015-0898-4", "report-no": null, "categories": "cs.DB cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional pairwise sequence alignment is based on matching individual\nsamples from two sequences, under time monotonicity constraints. However, in\nmany application settings matching subsequences (segments) instead of\nindividual samples may bring in additional robustness to noise or local\nnon-causal perturbations. This paper presents an approach to segmental sequence\nalignment that jointly segments and aligns two sequences, generalizing the\ntraditional per-sample alignment. To accomplish this task, we introduce a\ndistance metric between segments based on average pairwise distances and then\npresent a modified pair-HMM (PHMM) that incorporates the proposed distance\nmetric to solve the joint segmentation and alignment task. We also propose a\nrelaxation to our model that improves the computational efficiency of the\ngeneric segmental PHMM. Our results demonstrate that this new measure of\nsequence similarity can lead to improved classification performance, while\nbeing resilient to noise, on a variety of sequence retrieval problems, from EEG\nto motion sequence classification.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 21:53:42 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Shariat", "Shahriar", ""], ["Pavlovic", "Vladimir", ""]]}, {"id": "1609.08228", "submitter": "Sourav Medya", "authors": "Sourav Medya, Petko Bogdanov, Ambuj Singh", "title": "Towards Scalable Network Delay Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reduction of end-to-end network delays is an optimization task with\napplications in multiple domains. Low delays enable improved information flow\nin social networks, quick spread of ideas in collaboration networks, low travel\ntimes for vehicles on road networks and increased rate of packets in the case\nof communication networks. Delay reduction can be achieved by both improving\nthe propagation capabilities of individual nodes and adding additional edges in\nthe network. One of the main challenges in such design problems is that the\neffects of local changes are not independent, and as a consequence, there is a\ncombinatorial search-space of possible improvements. Thus, minimizing the\ncumulative propagation delay requires novel scalable and data-driven\napproaches.\n  In this paper, we consider the problem of network delay minimization via node\nupgrades. Although the problem is NP-hard, we show that probabilistic\napproximation for a restricted version can be obtained. We design scalable and\nhigh-quality techniques for the general setting based on sampling and targeted\nto different models of delay distribution. Our methods scale almost linearly\nwith the graph size and consistently outperform competitors in quality.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 00:29:03 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Medya", "Sourav", ""], ["Bogdanov", "Petko", ""], ["Singh", "Ambuj", ""]]}, {"id": "1609.08266", "submitter": "Jihwan Lee", "authors": "Jihwan Lee, Keehwan Park, Sunil Prabhakar", "title": "Mining Statistically Significant Attribute Associations in Attributed\n  Graphs", "comments": "ICDM 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, graphs have been widely used to represent many different kinds of\nreal world data or observations such as social networks, protein-protein\nnetworks, road networks, and so on. In many cases, each node in a graph is\nassociated with a set of its attributes and it is critical to not only consider\nthe link structure of a graph but also use the attribute information to achieve\nmore meaningful results in various graph mining tasks. Most previous works with\nattributed graphs take into ac- count attribute relationships only between\nindividually connected nodes. However, it should be greatly valuable to find\nout which sets of attributes are associated with each other and whether they\nare statistically significant or not. Mining such significant associations, we\ncan uncover novel relationships among the sets of attributes in the graph. We\npropose an algorithm that can find those attribute associations efficiently and\neffectively, and show experimental results that confirm the high applicability\nof the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 05:53:45 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Lee", "Jihwan", ""], ["Park", "Keehwan", ""], ["Prabhakar", "Sunil", ""]]}, {"id": "1609.08431", "submitter": "Kaustubh Beedkar", "authors": "Kaustubh Beedkar and Rainer Gemulla", "title": "DESQ: Frequent Sequence Mining with Subsequence Constraints", "comments": "Long version of the paper accepted at the IEEE ICDM 2016 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frequent sequence mining methods often make use of constraints to control\nwhich subsequences should be mined. A variety of such subsequence constraints\nhas been studied in the literature, including length, gap, span,\nregular-expression, and hierarchy constraints. In this paper, we show that many\nsubsequence constraints---including and beyond those considered in the\nliterature---can be unified in a single framework. A unified treatment allows\nresearchers to study jointly many types of subsequence constraints (instead of\neach one individually) and helps to improve usability of pattern mining systems\nfor practitioners. In more detail, we propose a set of simple and intuitive\n\"pattern expressions\" to describe subsequence constraints and explore\nalgorithms for efficiently mining frequent subsequences under such general\nconstraints. Our algorithms translate pattern expressions to compressed finite\nstate transducers, which we use as computational model, and simulate these\ntransducers in a way suitable for frequent sequence mining. Our experimental\nstudy on real-world datasets indicates that our algorithms---although more\ngeneral---are competitive to existing state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 13:34:25 GMT"}, {"version": "v2", "created": "Thu, 13 Oct 2016 17:20:05 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["Beedkar", "Kaustubh", ""], ["Gemulla", "Rainer", ""]]}, {"id": "1609.08484", "submitter": "Gregor Joss\\'e", "authors": "Gregor Joss\\'e, Ying Lu, Tobias Emrich, Matthias Renz, Cyrus Shahabi,\n  Ugur Demiryurek, Matthias Schubert", "title": "Scenic Routes Now: Efficiently Solving the Time-Dependent Arc\n  Orienteering Problem", "comments": "13 pages, 11 figures, 1 table, 3 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper extends the Arc Orienteering Problem (AOP) to large road networks\nwith time-dependent travel times and time-dependent value gain, termed Twofold\nTime-Dependent AOP or 2TD-AOP for short. In its original definition, the\nNP-hard Orienteering Problem (OP) asks to find a path from a source to a\ndestination maximizing the accumulated value while not exceeding a cost budget.\nVariations of the OP and AOP have many practical applications such as mobile\ncrowdsourcing tasks (e.g., repairing and maintenance or dispatching field\nworkers), diverse logistics problems (e.g., crowd control or controlling\nwildfires) as well as several tourist guidance problems (e.g., generating trip\nrecommendations or navigating through theme parks). In the proposed 2TD-AOP,\ntravel times and value functions are assumed to be time-dependent. The dynamic\nvalues model, for instance, varying rewards in crowdsourcing tasks or varying\nurgency levels in damage control tasks. We discuss this novel problem, prove\nthe benefit of time-dependence empirically and present an efficient\napproximative solution, optimized for fast response systems. Our approach is\nthe first time-dependent variant of the AOP to be evaluated on a large scale,\nfine-grained, real-world road network. We show that optimal solutions are\ninfeasible and solutions to the static problem are often invalid. We propose an\napproximate dynamic programming solution which produces valid paths and is\norders of magnitude faster than any optimal solution.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 14:50:15 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Joss\u00e9", "Gregor", ""], ["Lu", "Ying", ""], ["Emrich", "Tobias", ""], ["Renz", "Matthias", ""], ["Shahabi", "Cyrus", ""], ["Demiryurek", "Ugur", ""], ["Schubert", "Matthias", ""]]}, {"id": "1609.08642", "submitter": "Jeremy Kepner", "authors": "Timothy Weale, Vijay Gadepally, Dylan Hutchison, Jeremy Kepner", "title": "Benchmarking the Graphulo Processing Framework", "comments": "5 pages, 4 figures, IEEE High Performance Extreme Computing (HPEC)\n  conference 2016", "journal-ref": null, "doi": "10.1109/HPEC.2016.7761640", "report-no": null, "categories": "cs.DB cs.MS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph algorithms have wide applicablity to a variety of domains and are often\nused on massive datasets. Recent standardization efforts such as the GraphBLAS\nspecify a set of key computational kernels that hardware and software\ndevelopers can adhere to. Graphulo is a processing framework that enables\nGraphBLAS kernels in the Apache Accumulo database. In our previous work, we\nhave demonstrated a core Graphulo operation called \\textit{TableMult} that\nperforms large-scale multiplication operations of database tables. In this\narticle, we present the results of scaling the Graphulo engine to larger\nproblems and scalablity when a greater number of resources is used.\nSpecifically, we present two experiments that demonstrate Graphulo scaling\nperformance is linear with the number of available resources. The first\nexperiment demonstrates cluster processing rates through Graphulo's TableMult\noperator on two large graphs, scaled between $2^{17}$ and $2^{19}$ vertices.\nThe second experiment uses TableMult to extract a random set of rows from a\nlarge graph ($2^{19}$ nodes) to simulate a cued graph analytic. These\nbenchmarking results are of relevance to Graphulo users who wish to apply\nGraphulo to their graph problems.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 20:09:03 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Weale", "Timothy", ""], ["Gadepally", "Vijay", ""], ["Hutchison", "Dylan", ""], ["Kepner", "Jeremy", ""]]}, {"id": "1609.09000", "submitter": "Till Sch\\\"afer", "authors": "Till Sch\\\"afer and Petra Mutzel", "title": "StruClus: Structural Clustering of Large-Scale Graph Databases", "comments": "10 pages, experimental evaluation, big data, subgraph mining,\n  clustering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a structural clustering algorithm for large-scale datasets of\nsmall labeled graphs, utilizing a frequent subgraph sampling strategy. A set of\nrepresentatives provides an intuitive description of each cluster, supports the\nclustering process, and helps to interpret the clustering results. The\nprojection-based nature of the clustering approach allows us to bypass\ndimensionality and feature extraction problems that arise in the context of\ngraph datasets reduced to pairwise distances or feature vectors. While\nachieving high quality and (human) interpretable clusterings, the runtime of\nthe algorithm only grows linearly with the number of graphs. Furthermore, the\napproach is easy to parallelize and therefore suitable for very large datasets.\nOur extensive experimental evaluation on synthetic and real world datasets\ndemonstrates the superiority of our approach over existing structural and\nsubspace clustering algorithms, both, from a runtime and quality point of view.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 16:43:12 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Sch\u00e4fer", "Till", ""], ["Mutzel", "Petra", ""]]}, {"id": "1609.09062", "submitter": "Zhiyong Shan", "authors": "Zhiyong Shan", "title": "A Study on Altering PostgreSQL from Multi-Processes Structure to\n  Multi-Threads Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to altering PostgreSQL database from multi-processes structure to\nmulti-threads structure is a difficult problem. In the paper, we bring forward\na comprehensive alteration scheme. Especially, put rational methods to account\nfor three difficult points: semaphores, signal processing and global variables.\nAt last, applied the scheme successfully to modify a famous open source DBMS.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 03:01:44 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Shan", "Zhiyong", ""]]}, {"id": "1609.09172", "submitter": "Yonghui Xiao", "authors": "Yonghui Xiao, Yilin Shen, Jinfei Liu, Li Xiong, Hongxia Jin, Xiaofeng\n  Xu", "title": "DPHMM: Customizable Data Release with Differential Privacy via Hidden\n  Markov Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hidden Markov model (HMM) has been well studied and extensively used. In this\npaper, we present DPHMM ({Differentially Private Hidden Markov Model}), an HMM\nembedded with a private data release mechanism, in which the privacy of the\ndata is protected through a graph. Specifically, we treat every state in Markov\nmodel as a node, and use a graph to represent the privacy policy, in which\n\"indistinguishability\" between states is denoted by edges between nodes. Due to\nthe temporal correlations in Markov model, we show that the graph may be\nreduced to a subgraph with disconnected nodes, which become unprotected and\nmight be exposed. To detect such privacy risk, we define sensitivity hull and\ndegree of protection based on the graph to capture the condition of information\nexposure. Then to tackle the detected exposure, we study how to build an\noptimal graph based on the existing graph. We also implement and evaluate the\nDPHMM on real-world datasets, showing that privacy and utility can be better\ntuned with customized policy graph.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 02:00:58 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Xiao", "Yonghui", ""], ["Shen", "Yilin", ""], ["Liu", "Jinfei", ""], ["Xiong", "Li", ""], ["Jin", "Hongxia", ""], ["Xu", "Xiaofeng", ""]]}, {"id": "1609.09196", "submitter": "Davis Blalock", "authors": "Davis W. Blalock, John V. Guttag", "title": "EXTRACT: Strong Examples from Weakly-Labeled Sensor Data", "comments": "To appear in IEEE International Conference on Data Mining 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to the rise of wearable and connected devices, sensor-generated time\nseries comprise a large and growing fraction of the world's data.\nUnfortunately, extracting value from this data can be challenging, since\nsensors report low-level signals (e.g., acceleration), not the high-level\nevents that are typically of interest (e.g., gestures). We introduce a\ntechnique to bridge this gap by automatically extracting examples of real-world\nevents in low-level data, given only a rough estimate of when these events have\ntaken place.\n  By identifying sets of features that repeat in the same temporal arrangement,\nwe isolate examples of such diverse events as human actions, power consumption\npatterns, and spoken words with up to 96% precision and recall. Our method is\nfast enough to run in real time and assumes only minimal knowledge of which\nvariables are relevant or the lengths of events. Our evaluation uses numerous\npublicly available datasets and over 1 million samples of manually labeled\nsensor data.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 04:02:31 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Blalock", "Davis W.", ""], ["Guttag", "John V.", ""]]}, {"id": "1609.09340", "submitter": "Elena Alfaro Martinez", "authors": "Elena Alfaro Martinez (BBVA Data & Analytics), Maria Hernandez Rubio\n  (BBVA Data & Analytics), Roberto Maestre Martinez (BBVA Data & Analytics),\n  Juan Murillo Arias (BBVA Data & Analytics), Dario Patane (BBVA Data &\n  Analytics), Amanda Zerbe (United Nations Global Pulse), Robert Kirkpatrick\n  (United Nations Global Pulse), Miguel Luengo-Oroz (United Nations Global\n  Pulse), Amanda Zerbe (United Nations Global Pulse)", "title": "Measuring Economic Resilience to Natural Disasters with Big Economic\n  Transaction Data", "comments": "Presented at the Data For Good Exchange 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research explores the potential to analyze bank card payments and ATM\ncash withdrawals in order to map and quantify how people are impacted by and\nrecover from natural disasters. Our approach defines a disaster-affected\ncommunity's economic recovery time as the time needed to return to baseline\nactivity levels in terms of number of bank card payments and ATM cash\nwithdrawals. For Hurricane Odile, which hit the state of Baja California Sur\n(BCS) in Mexico between 15 and 17 September 2014, we measured and mapped\ncommunities' economic recovery time, which ranged from 2 to 40 days in\ndifferent locations. We found that -- among individuals with a bank account --\nthe lower the income level, the shorter the time needed for economic activity\nto return to normal levels. Gender differences in recovery times were also\ndetected and quantified. In addition, our approach evaluated how communities\nprepared for the disaster by quantifying expenditure growth in food or gasoline\nbefore the hurricane struck. We believe this approach opens a new frontier in\nmeasuring the economic impact of disasters with high temporal and spatial\nresolution, and in understanding how populations bounce back and adapt.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 01:20:23 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Martinez", "Elena Alfaro", "", "BBVA Data & Analytics"], ["Rubio", "Maria Hernandez", "", "BBVA Data & Analytics"], ["Martinez", "Roberto Maestre", "", "BBVA Data & Analytics"], ["Arias", "Juan Murillo", "", "BBVA Data & Analytics"], ["Patane", "Dario", "", "BBVA Data &\n  Analytics"], ["Zerbe", "Amanda", "", "United Nations Global Pulse"], ["Kirkpatrick", "Robert", "", "United Nations Global Pulse"], ["Luengo-Oroz", "Miguel", "", "United Nations Global\n  Pulse"], ["Zerbe", "Amanda", "", "United Nations Global Pulse"]]}]