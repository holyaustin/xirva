[{"id": "1811.00602", "submitter": "Leonhard Spiegelberg", "authors": "Lorenzo De Stefani, Leonhard F. Spiegelberg, Tim Kraska, Eli Upfal", "title": "VizRec: A framework for secure data exploration via visual\n  representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual representations of data (visualizations) are tools of great importance\nand widespread use in data analytics as they provide users visual insight to\npatterns in the observed data in a simple and effective way. However, since\nvisualizations tools are applied to sample data, there is a a risk of\nvisualizing random fluctuations in the sample rather than a true pattern in the\ndata. This problem is even more significant when visualization is used to\nidentify interesting patterns among many possible possibilities, or to identify\nan interesting deviation in a pair of observations among many possible pairs,\nas commonly done in visual recommendation systems.\n  We present VizRec, a framework for improving the performance of visual\nrecommendation systems by quantifying the statistical significance of\nrecommended visualizations. The proposed methodology allows to control the\nprobability of misleading visual recommendations using both classical\nstatistical testing procedures and a novel application of the Vapnik\nChervonenkis (VC) dimension method which is a fundamental concept in\nstatistical learning theory.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 19:35:11 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["De Stefani", "Lorenzo", ""], ["Spiegelberg", "Leonhard F.", ""], ["Kraska", "Tim", ""], ["Upfal", "Eli", ""]]}, {"id": "1811.01313", "submitter": "Kasper Green Larsen", "authors": "Alireza Farhadi, MohammadTaghi Hajiaghayi, Kasper Green Larsen, Elaine\n  Shi", "title": "Lower Bounds for External Memory Integer Sorting via Network Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DB cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sorting extremely large datasets is a frequently occuring task in practice.\nThese datasets are usually much larger than the computer's main memory; thus\nexternal memory sorting algorithms, first introduced by Aggarwal and Vitter\n(1988), are often used. The complexity of comparison based external memory\nsorting has been understood for decades by now, however the situation remains\nelusive if we assume the keys to be sorted are integers. In internal memory,\none can sort a set of $n$ integer keys of $\\Theta(\\lg n)$ bits each in $O(n)$\ntime using the classic Radix Sort algorithm, however in external memory, there\nare no faster integer sorting algorithms known than the simple comparison based\nones. In this paper, we present a tight conditional lower bound on the\ncomplexity of external memory sorting of integers. Our lower bound is based on\na famous conjecture in network coding by Li and Li, who conjectured that\nnetwork coding cannot help anything beyond the standard multicommodity flow\nrate in undirected graphs. The only previous work connecting the Li and Li\nconjecture to lower bounds for algorithms is due to Adler et al. Adler et al.\nindeed obtain relatively simple lower bounds for oblivious algorithms (the\nmemory access pattern is fixed and independent of the input data).\nUnfortunately obliviousness is a strong limitations, especially for integer\nsorting: we show that the Li and Li conjecture implies an $\\Omega(n \\log n)$\nlower bound for internal memory oblivious sorting when the keys are $\\Theta(\\lg\nn)$ bits. This is in sharp contrast to the classic (non-oblivious) Radix Sort\nalgorithm. Indeed going beyond obliviousness is highly non-trivial; we need to\nintroduce several new methods and involved techniques, which are of their own\ninterest, to obtain our tight lower bound for external memory integer sorting.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 02:41:43 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Farhadi", "Alireza", ""], ["Hajiaghayi", "MohammadTaghi", ""], ["Larsen", "Kasper Green", ""], ["Shi", "Elaine", ""]]}, {"id": "1811.01660", "submitter": "Samaneh Jozashoori", "authors": "Samaneh Jozashoori, Tatiana Novikova, Maria-Esther Vidal", "title": "Data Integration for Supporting Biomedical Knowledge Graph Creation at\n  Large-Scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, following FAIR and open data principles, the number of\navailable big data including biomedical data has been increased exponentially.\nIn order to extract knowledge, these data should be curated, integrated, and\nsemantically described. Accordingly, several semantic integration techniques\nhave been developed; albeit effective, they may suffer from scalability in\nterms of different properties of big data. Even scaled-up approaches may be\nhighly costly because tasks of semantification, curation and integration are\nperformed independently. In order to overcome these issues, we devise ConMap, a\nsemantic integration approach which exploits knowledge encoded in ontology in\norder to describe mapping rules to perform these tasks at the same time.\nExperimental results performed on different data sets suggest that ConMap can\nsignificantly reduce the time required for knowledge graph creation by up to\n70\\% of the time that is consumed following a traditional approach.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 13:16:54 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Jozashoori", "Samaneh", ""], ["Novikova", "Tatiana", ""], ["Vidal", "Maria-Esther", ""]]}, {"id": "1811.02059", "submitter": "Yi Lu", "authors": "Yi Lu, Xiangyao Yu, Samuel Madden", "title": "STAR: Scaling Transactions through Asymmetric Replication", "comments": null, "journal-ref": "PVLDB, 12(11): 1316- 1329, 2019", "doi": "10.14778/3342263.3342270", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present STAR, a new distributed in-memory database with\nasymmetric replication. By employing a single-node non-partitioned architecture\nfor some replicas and a partitioned architecture for other replicas, STAR is\nable to efficiently run both highly partitionable workloads and workloads that\ninvolve cross-partition transactions. The key idea is a new phase-switching\nalgorithm where the execution of single-partition and cross-partition\ntransactions is separated. In the partitioned phase, single-partition\ntransactions are run on multiple machines in parallel to exploit more\nconcurrency. In the single-master phase, mastership for the entire database is\nswitched to a single designated master node, which can execute these\ntransactions without the use of expensive coordination protocols like two-phase\ncommit. Because the master node has a full copy of the database, this\nphase-switching can be done at negligible cost. Our experiments on two popular\nbenchmarks (YCSB and TPC-C) show that high availability via replication can\ncoexist with fast serializable transaction execution in distributed in-memory\ndatabases, with STAR outperforming systems that employ conventional concurrency\ncontrol and replication algorithms by up to one order of magnitude.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 22:20:25 GMT"}, {"version": "v2", "created": "Sat, 2 Feb 2019 03:37:35 GMT"}, {"version": "v3", "created": "Sun, 21 Jul 2019 13:09:23 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Lu", "Yi", ""], ["Yu", "Xiangyao", ""], ["Madden", "Samuel", ""]]}, {"id": "1811.02304", "submitter": "Pan Hu", "authors": "Pan Hu, Boris Motik, Ian Horrocks", "title": "Modular Materialisation of Datalog Programs", "comments": "Accepted at AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The semina\\\"ive algorithm can materialise all consequences of arbitrary\ndatalog rules, and it also forms the basis for incremental algorithms that\nupdate a materialisation as the input facts change. Certain (combinations of)\nrules, however, can be handled much more efficiently using custom algorithms.\nTo integrate such algorithms into a general reasoning approach that can handle\narbitrary rules, we propose a modular framework for materialisation computation\nand its maintenance. We split a datalog program into modules that can be\nhandled using specialised algorithms, and handle the remaining rules using the\nsemina\\\"ive algorithm. We also present two algorithms for computing the\ntransitive and the symmetric-transitive closure of a relation that can be used\nwithin our framework. Finally, we show empirically that our framework can\nhandle arbitrary datalog programs while outperforming existing approaches,\noften by orders of magnitude.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 11:51:10 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 23:57:54 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Hu", "Pan", ""], ["Motik", "Boris", ""], ["Horrocks", "Ian", ""]]}, {"id": "1811.02944", "submitter": "Antoine Amarilli", "authors": "Antoine Amarilli, Florent Capelli, Mika\\\"el Monet, Pierre Senellart", "title": "Connecting Knowledge Compilation Classes and Width Parameters", "comments": "46 pages. Extended version of arXiv:1709.06188. Up to the stylesheet,\n  page/environment numbering, minor formatting, and publisher-induced changes,\n  this is the exact content of the paper in Theory of Computing Systems\n  <https://link.springer.com/article/10.1007%2Fs00224-019-09930-2>. The\n  difference in the titles (missing \"and\") is an error introduced by the\n  publisher", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of knowledge compilation establishes the tractability of many tasks\nby studying how to compile them to Boolean circuit classes obeying some\nrequirements such as structuredness, decomposability, and determinism. However,\nin other settings such as intensional query evaluation on databases, we obtain\nBoolean circuits that satisfy some width bounds, e.g., they have bounded\ntreewidth or pathwidth. In this work, we give a systematic picture of many\ncircuit classes considered in knowledge compilation and show how they can be\nsystematically connected to width measures, through upper and lower bounds. Our\nupper bounds show that bounded-treewidth circuits can be constructively\nconverted to d-SDNNFs, in time linear in the circuit size and singly\nexponential in the treewidth; and that bounded-pathwidth circuits can similarly\nbe converted to uOBDDs. We show matching lower bounds on the compilation of\nmonotone DNF or CNF formulas to structured targets, assuming a constant bound\non the arity (size of clauses) and degree (number of occurrences of each\nvariable): any d-SDNNF (resp., SDNNF) for such a DNF (resp., CNF) must be of\nexponential size in its treewidth, and the same holds for uOBDDs (resp.,\nn-OBDDs) when considering pathwidth. Unlike most previous work, our bounds\napply to any formula of this class, not just a well-chosen family. Hence, we\nshow that pathwidth and treewidth respectively characterize the efficiency of\ncompiling monotone DNFs to uOBDDs and d-SDNNFs with compilation being singly\nexponential in the corresponding width parameter. We also show that our lower\nbounds on CNFs extend to unstructured compilation targets, with an exponential\nlower bound in the treewidth (resp., pathwidth) when compiling monotone CNFs of\nconstant arity and degree to DNNFs (resp., nFBDDs).\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 15:45:43 GMT"}, {"version": "v2", "created": "Sat, 20 Jul 2019 09:13:04 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Amarilli", "Antoine", ""], ["Capelli", "Florent", ""], ["Monet", "Mika\u00ebl", ""], ["Senellart", "Pierre", ""]]}, {"id": "1811.03277", "submitter": "EPTCS", "authors": "Bob Coecke (University of Oxford), Giovanni de Felice (University of\n  Oxford), Dan Marsden (University of Oxford), Alexis Toumi (University of\n  Oxford)", "title": "Towards Compositional Distributional Discourse Analysis", "comments": "In Proceedings CAPNS 2018, arXiv:1811.02701", "journal-ref": "EPTCS 283, 2018, pp. 1-12", "doi": "10.4204/EPTCS.283.1", "report-no": null, "categories": "cs.AI cs.CL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Categorical compositional distributional semantics provide a method to derive\nthe meaning of a sentence from the meaning of its individual words: the\ngrammatical reduction of a sentence automatically induces a linear map for\ncomposing the word vectors obtained from distributional semantics. In this\npaper, we extend this passage from word-to-sentence to sentence-to-discourse\ncomposition. To achieve this we introduce a notion of basic anaphoric\ndiscourses as a mid-level representation between natural language discourse\nformalised in terms of basic discourse representation structures (DRS); and\nknowledge base queries over the Semantic Web as described by basic graph\npatterns in the Resource Description Framework (RDF). This provides a\nhigh-level specification for compositional algorithms for question answering\nand anaphora resolution, and allows us to give a picture of natural language\nunderstanding as a process involving both statistical and logical resources.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 05:14:19 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Coecke", "Bob", "", "University of Oxford"], ["de Felice", "Giovanni", "", "University of\n  Oxford"], ["Marsden", "Dan", "", "University of Oxford"], ["Toumi", "Alexis", "", "University of\n  Oxford"]]}, {"id": "1811.04160", "submitter": "Hasan Jamil", "authors": "Josue Espinosa Godinez and Hasan M. Jamil", "title": "Meet Cyrus - The Query by Voice Mobile Assistant for the Tutoring and\n  Formative Assessment of SQL Learners", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being declarative, SQL stands a better chance at being the programming\nlanguage for conceptual computing next to natural language programming. We\nexamine the possibility of using SQL as a back-end for natural language\ndatabase programming. Distinctly from keyword based SQL querying, keyword\ndependence and SQL's table structure constraints are significantly less\npronounced in our approach. We present a mobile device voice query interface,\ncalled Cyrus, to arbitrary relational databases. Cyrus supports a large type of\nquery classes, sufficient for an entry level database class. Cyrus is also\napplication independent, allows test database adaptation, and not limited to\nspecific sets of keywords or natural language sentence structures. It's\ncooperative error reporting is more intuitive, and iOS based mobile platform is\nalso more accessible compared to most contemporary mobile and voice enabled\nsystems.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 23:55:14 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Godinez", "Josue Espinosa", ""], ["Jamil", "Hasan M.", ""]]}, {"id": "1811.04162", "submitter": "Hasan Jamil", "authors": "Patrick Vanvorce and Hasan M. Jamil", "title": "Computational Thinking with the Web Crowd using CodeMapper", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been argued that computational thinking should precede computer\nprogramming in the course of a career in computing. This argument is the basis\nfor the slogan \"logic first, syntax later\" and the development of many cryptic\nsyntax removed programming languages such as Scratch!, Blockly and Visual\nLogic. The goal is to focus on the structuring of the semantic relationships\namong the logical building blocks to yield solutions to computational problems.\nWhile this approach is helping novice programmers and early learners, the gap\nbetween computational thinking and professional programming using high level\nlanguages such as C++, Python and Java is quite wide. It is wide enough for\nabout one third students in first college computer science classes to drop out\nor fail. In this paper, we introduce a new programming platform, called the\nCodeMapper, in which learners are able to build computational logic in\nindependent modules and aggregate them to create complex modules. Code{\\em\nMapper} is an abstract development environment in which rapid visual\nprototyping of small to substantially large systems is possible by combining\nalready developed independent modules in logical steps. The challenge we\naddress involves supporting a visual development environment in which\n\"annotated code snippets\" authored by the masses in social computing sites such\nas SourceForge, StackOverflow or GitHub can also be used as is into prototypes\nand mapped to real executable programs. CodeMapper thus facilitates soft\ntransition from visual programming to syntax driven programming without having\nto practice syntax too heavily.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 23:55:56 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Vanvorce", "Patrick", ""], ["Jamil", "Hasan M.", ""]]}, {"id": "1811.04967", "submitter": "Yihe Huang", "authors": "Yihe Huang, Hao Bai, Eddie Kohler, Barbara Liskov, Liuba Shrira", "title": "The Impact of Timestamp Granularity in Optimistic Concurrency Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Optimistic concurrency control (OCC) can exploit the strengths of parallel\nhardware to provide excellent performance for uncontended transactions, and is\npopular in high-performance in-memory databases and transactional systems. But\nat high contention levels, OCC is susceptible to frequent aborts, leading to\nwasted work and degraded performance. Contention managers, mixed\noptimistic/pessimistic concurrency control algorithms, and novel\noptimistic-inspired concurrency control algorithms, such as TicToc, aim to\naddress this problem, but these mechanisms introduce sometimes-high overheads\nof their own. We show that in real-world benchmarks, traditional OCC can\noutperform these alternative mechanisms by simply adding fine-grained version\ntimestamps (using different timestamps for disjoint components of each record).\nWith fine-grained timestamps, OCC gets 1.14x TicToc's throughput in TPC-C at\n128 cores (previous work reported TicToc having 1.8x higher throughput than OCC\nat 80 hyperthreads). Our study shows that timestamp granularity has a greater\nimpact than previously thought on the performance of transaction processing\nsystems, and should not be overlooked in the push for faster concurrency\ncontrol schemes.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 19:15:27 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Huang", "Yihe", ""], ["Bai", "Hao", ""], ["Kohler", "Eddie", ""], ["Liskov", "Barbara", ""], ["Shrira", "Liuba", ""]]}, {"id": "1811.05065", "submitter": "Fei Pan", "authors": "Fei Pan, Hans-Arno Jacobsen", "title": "PanJoin: A Partition-based Adaptive Stream Join", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In stream processing, stream join is one of the critical sources of\nperformance bottlenecks. The sliding-window-based stream join provides a\nprecise result but consumes considerable computational resources. The current\nsolutions lack support for the join predicates on large windows. These\nalgorithms and their hardware accelerators are either limited to equi-join or\nuse a nested loop join to process all the requests.\n  In this paper, we present a new algorithm called PanJoin which has high\nthroughput on large windows and supports both equi-join and non-equi-join.\nPanJoin implements three new data structures to reduce computations during the\nprobing phase of stream join. We also implement the most hardware-friendly data\nstructure, called BI-Sort, on FPGA. Our evaluation shows that PanJoin\noutperforms several recently proposed stream join methods by more than 1000x,\nand it also adapts well to highly skewed data.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 02:01:32 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Pan", "Fei", ""], ["Jacobsen", "Hans-Arno", ""]]}, {"id": "1811.05160", "submitter": "Krist\\'of B\\'erczi", "authors": "Krist\\'of B\\'erczi, Endre Boros, Ond\\v{r}ej \\v{C}epek, Petr\n  Ku\\v{c}era, Kazuhisa Makino", "title": "Approximating minimum representations of key Horn functions", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Horn functions form a subclass of Boolean functions and appear in many\ndifferent areas of computer science and mathematics as a general tool to\ndescribe implications and dependencies. Finding minimum sized representations\nfor such functions with respect to most commonly used measures is a\ncomputationally hard problem that remains hard even for the important subclass\nof key Horn functions. In this paper we provide logarithmic factor\napproximation algorithms for key Horn functions with respect to all measures\nstudied in the literature for which the problem is known to be hard.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 08:30:17 GMT"}, {"version": "v2", "created": "Fri, 22 Mar 2019 15:06:51 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["B\u00e9rczi", "Krist\u00f3f", ""], ["Boros", "Endre", ""], ["\u010cepek", "Ond\u0159ej", ""], ["Ku\u010dera", "Petr", ""], ["Makino", "Kazuhisa", ""]]}, {"id": "1811.05361", "submitter": "Pavel Braslavski", "authors": "Ksenia Zhagorina, Pavel Braslavski, Vladimir Gusev", "title": "Personal Names Popularity Estimation and its Application to Record\n  Linkage", "comments": "This is an extended version of a short paper presented at ADBIS2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This study deals with a fairly simply formulated problem -- how to estimate\nthe number of people bearing the same full name in a large population.\nEstimation of name popularity can leverage personal name matching in databases\nand be of interest for many other domains. A distinctive feature of large\ncollections of names is that they contain a large number of unique items, which\nis challenging for statistical modeling. We investigate a number of statistical\ntechniques and also propose a simple yet effective method aimed at obtaining\nmore accurate count estimates. In our experiments we use a dataset containing\nabout 20 million name occurrences that correspond to about 13 million\nreal-world persons. We perform a thorough evaluation of the name count\nestimation methods and a record linkage experiment guided by name popularity\nestimates. Obtained results suggest that theoretically informed approaches\noutperform simple heuristics and can be useful in a variety of applications.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 15:26:30 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Zhagorina", "Ksenia", ""], ["Braslavski", "Pavel", ""], ["Gusev", "Vladimir", ""]]}, {"id": "1811.06224", "submitter": "Moritz Kulessa", "authors": "Moritz Kulessa, Alejandro Molina, Carsten Binnig, Benjamin Hilprecht,\n  Kristian Kersting", "title": "Model-based Approximate Query Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive visualizations are arguably the most important tool to explore,\nunderstand and convey facts about data. In the past years, the database\ncommunity has been working on different techniques for Approximate Query\nProcessing (AQP) that aim to deliver an approximate query result given a fixed\ntime bound to support interactive visualizations better. However, classical AQP\napproaches suffer from various problems that limit the applicability to support\nthe ad-hoc exploration of a new data set: (1) Classical AQP approaches that\nperform online sampling can support ad-hoc exploration queries but yield low\nquality if executed over rare subpopulations. (2) Classical AQP approaches that\nrely on offline sampling can use some form of biased sampling to mitigate these\nproblems but require a priori knowledge of the workload, which is often not\nrealistic if users want to explore a new database. In this paper, we present a\nnew approach to AQP called Model-based AQP that leverages generative models\nlearned over the complete database to answer SQL queries at interactive speeds.\nDifferent from classical AQP approaches, generative models allow us to compute\nresponses to ad-hoc queries and deliver high-quality estimates also over rare\nsubpopulations at the same time. In our experiments with real and synthetic\ndata sets, we show that Model-based AQP can in many scenarios return more\naccurate results in a shorter runtime. Furthermore, we think that our\ntechniques of using generative models presented in this paper can not only be\nused for AQP in databases but also has applications for other database problems\nincluding Query Optimization as well as Data Cleaning.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 08:14:53 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Kulessa", "Moritz", ""], ["Molina", "Alejandro", ""], ["Binnig", "Carsten", ""], ["Hilprecht", "Benjamin", ""], ["Kersting", "Kristian", ""]]}, {"id": "1811.07389", "submitter": "Wensheng Gan", "authors": "Wensheng Gan, Jerry Chun-Wei Lin, Han-Chieh Chao, Shyue-Liang Wang,\n  and Philip S. Yu", "title": "Privacy Preserving Utility Mining: A Survey", "comments": "2018 IEEE International Conference on Big Data, 10 pages", "journal-ref": "IEEE International Conference on Big Data (Big Data), 2018", "doi": "10.1109/BigData.2018.8622405", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In big data era, the collected data usually contains rich information and\nhidden knowledge. Utility-oriented pattern mining and analytics have shown a\npowerful ability to explore these ubiquitous data, which may be collected from\nvarious fields and applications, such as market basket analysis, retail,\nclick-stream analysis, medical analysis, and bioinformatics. However, analysis\nof these data with sensitive private information raises privacy concerns. To\nachieve better trade-off between utility maximizing and privacy preserving,\nPrivacy-Preserving Utility Mining (PPUM) has become a critical issue in recent\nyears. In this paper, we provide a comprehensive overview of PPUM. We first\npresent the background of utility mining, privacy-preserving data mining and\nPPUM, then introduce the related preliminaries and problem formulation of PPUM,\nas well as some key evaluation criteria for PPUM. In particular, we present and\ndiscuss the current state-of-the-art PPUM algorithms, as well as their\nadvantages and deficiencies in detail. Finally, we highlight and discuss some\ntechnical challenges and open directions for future research on PPUM.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 19:58:26 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Gan", "Wensheng", ""], ["Lin", "Jerry Chun-Wei", ""], ["Chao", "Han-Chieh", ""], ["Wang", "Shyue-Liang", ""], ["Yu", "Philip S.", ""]]}, {"id": "1811.07514", "submitter": "Shobeir Fakhraei", "authors": "Shobeir Fakhraei, Joel Mathew, Jose Luis Ambite", "title": "NSEEN: Neural Semantic Embedding for Entity Normalization", "comments": "Accepted for publication at ECML-PKDD 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.DB cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of human knowledge is encoded in text, available in scientific\npublications, books, and the web. Given the rapid growth of these resources, we\nneed automated methods to extract such knowledge into machine-processable\nstructures, such as knowledge graphs. An important task in this process is\nentity normalization, which consists of mapping noisy entity mentions in text\nto canonical entities in well-known reference sets. However, entity\nnormalization is a challenging problem; there often are many textual forms for\na canonical entity that may not be captured in the reference set, and entities\nmentioned in text may include many syntactic variations, or errors. The problem\nis particularly acute in scientific domains, such as biology. To address this\nproblem, we have developed a general, scalable solution based on a deep Siamese\nneural network model to embed the semantic information about the entities, as\nwell as their syntactic variations. We use these embeddings for fast mapping of\nnew entities to large reference sets, and empirically show the effectiveness of\nour framework in challenging bio-entity normalization datasets.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 06:04:13 GMT"}, {"version": "v2", "created": "Sat, 29 Jun 2019 07:19:08 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Fakhraei", "Shobeir", ""], ["Mathew", "Joel", ""], ["Ambite", "Jose Luis", ""]]}, {"id": "1811.07525", "submitter": "Po-Chun Kuo", "authors": "Tai-Yuan Chen and Wei-Ning Huang and Po-Chun Kuo and Hao Chung and\n  Tzu-Wei Chao", "title": "DEXON: A Highly Scalable, Decentralized DAG-Based Consensus Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A blockchain system is a replicated state machine that must be fault\ntolerant. When designing a blockchain system, there is usually a trade-off\nbetween decentralization, scalability, and security. In this paper, we propose\na novel blockchain system, DEXON, which achieves high scalability while\nremaining decentralized and robust in the real-world environment. We have two\nmain contributions. First, we present a highly scalable sharding framework for\nblockchain. This framework takes an arbitrary number of single chains and\ntransforms them into the \\textit{blocklattice} data structure, enabling\n\\textit{high scalability} and \\textit{low transaction confirmation latency}\nwith asymptotically optimal communication overhead. Second, we propose a\nsingle-chain protocol based on our novel verifiable random function and a new\nByzantine agreement that achieves high decentralization and low latency.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 06:59:50 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Chen", "Tai-Yuan", ""], ["Huang", "Wei-Ning", ""], ["Kuo", "Po-Chun", ""], ["Chung", "Hao", ""], ["Chao", "Tzu-Wei", ""]]}, {"id": "1811.07977", "submitter": "Tarique Siddiqui", "authors": "Tarique Siddiqui, Zesheng Wang, Paul Luh, Karrie Karahalios, Aditya\n  Parameswaran", "title": "ShapeSearch: A Flexible and Efficient System for Shape-based Exploration\n  of Trendlines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying trendline visualizations with desired patterns is a common and\nfundamental data exploration task. Existing visual analytics tools offer\nlimited flexibility and expressiveness for such tasks, especially when the\npattern of interest is under-specified and approximate, and do not scale well\nwhen the pattern searching needs are ad-hoc, as is often the case. We propose\nShapeSearch, an efficient and flexible pattern-searching tool, that enables the\nsearch for desired patterns via multiple mechanisms: sketch, natural-language,\nand visual regular expressions. We develop a novel shape querying algebra, with\na minimal set of primitives and operators that can express a large number of\nShapeSearch queries, and design a natural-language and regex-based parser to\nautomatically parse and translate user queries to the algebra representation.\nTo execute these queries within interactive response times, ShapeSearch uses a\nfast shape algebra-based execution engine with query-aware optimizations, and\nperceptually-aware scoring methodologies. We present a thorough evaluation of\nthe system, including a general-purpose user study, a case study involving\ngenomic data analysis, as well as performance experiments, comparing against\nstate-of-the-art time series shape matching approaches---that together\ndemonstrate the usability and scalability of ShapeSearch.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 20:54:12 GMT"}, {"version": "v2", "created": "Mon, 27 Jan 2020 17:14:01 GMT"}, {"version": "v3", "created": "Wed, 29 Jan 2020 21:11:37 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Siddiqui", "Tarique", ""], ["Wang", "Zesheng", ""], ["Luh", "Paul", ""], ["Karahalios", "Karrie", ""], ["Parameswaran", "Aditya", ""]]}, {"id": "1811.08143", "submitter": "Alessandro Berti Mr", "authors": "Alessandro Berti, Wil van der Aalst", "title": "StarStar Models: Process Analysis on top of Databases", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much time in process mining projects is spent on finding and understanding\ndata sources and extracting the event data needed. As a result, only a fraction\nof time is spent actually applying techniques to discover, control and predict\nthe business process. Moreover, there is a lack of techniques to display\nrelationships on top of databases without the need to express a complex query\nto get the required information. In this paper, a novel modeling technique that\nworks on top of databases is presented. This technique is able to show a\nmultigraph representing activities inferred from database events, connected\nwith edges that are annotated with frequency and performance information. The\nrepresentation may be the entry point to apply advanced process mining\ntechniques that work on classic event logs, as the model provides a simple way\nto retrieve a classic event log from a specified piece of model. Comparison\nwith similar techniques and an empirical evaluation are provided.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 09:32:11 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Berti", "Alessandro", ""], ["van der Aalst", "Wil", ""]]}, {"id": "1811.08181", "submitter": "Wolfgang Fischl", "authors": "Wolfgang Fischl, Georg Gottlob, Davide M. Longo, Reinhard Pichler", "title": "HyperBench: A Benchmark and Tool for Hypergraphs and Empirical Findings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To cope with the intractability of answering Conjunctive Queries (CQs) and\nsolving Constraint Satisfaction Problems (CSPs), several notions of hypergraph\ndecompositions have been proposed -- giving rise to different notions of width,\nnoticeably, plain, generalized, and fractional hypertree width (hw, ghw, and\nfhw). Given the increasing interest in using such decomposition methods in\npractice, a publicly accessible repository of decomposition software, as well\nas a large set of benchmarks, and a web-accessible workbench for inserting,\nanalysing, and retrieving hypergraphs are called for.\n  We address this need by providing (i) concrete implementations of hypergraph\ndecompositions (including new practical algorithms), (ii) a new, comprehensive\nbenchmark of hypergraphs stemming from disparate CQ and CSP collections, and\n(iii) HyperBench, our new web-inter\\-face for accessing the benchmark and the\nresults of our analyses. In addition, we describe a number of actual\nexperiments we carried out with this new infrastructure.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 11:18:22 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Fischl", "Wolfgang", ""], ["Gottlob", "Georg", ""], ["Longo", "Davide M.", ""], ["Pichler", "Reinhard", ""]]}, {"id": "1811.09248", "submitter": "Martin Koehler", "authors": "Martin Koehler and Alex Bogatu and Cristina Civili and Nikolaos\n  Konstantinou and Edward Abel and Alvaro A. A. Fernandes and John Keane and\n  Leonid Libkin and Norman W. Paton", "title": "Data Context Informed Data Wrangling", "comments": null, "journal-ref": "2017 IEEE International Conference on Big Data (Big Data), pp.\n  956-963, Boston, MA, 11-14 December, 2017", "doi": "10.1109/BigData.2017.8258015", "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The process of preparing potentially large and complex data sets for further\nanalysis or manual examination is often called data wrangling. In classical\nwarehousing environments, the steps in such a process have been carried out\nusing Extract-Transform-Load platforms, with significant manual involvement in\nspecifying, configuring or tuning many of them. Cost-effective data wrangling\nprocesses need to ensure that data wrangling steps benefit from automation\nwherever possible. In this paper, we define a methodology to fully automate an\nend-to-end data wrangling process incorporating data context, which associates\nportions of a target schema with potentially spurious extensional data of types\nthat are commonly available. Instance-based evidence together with data\nprofiling paves the way to inform automation in several steps within the\nwrangling process, specifically, matching, mapping validation, value format\ntransformation, and data repair. The approach is evaluated with real estate\ndata showing substantial improvements in the results of automated wrangling.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 17:34:35 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Koehler", "Martin", ""], ["Bogatu", "Alex", ""], ["Civili", "Cristina", ""], ["Konstantinou", "Nikolaos", ""], ["Abel", "Edward", ""], ["Fernandes", "Alvaro A. A.", ""], ["Keane", "John", ""], ["Libkin", "Leonid", ""], ["Paton", "Norman W.", ""]]}, {"id": "1811.09529", "submitter": "Agnieszka Lawrynowicz", "authors": "Dawid Wisniewski, Jedrzej Potoniec, Agnieszka Lawrynowicz, C. Maria\n  Keet", "title": "Competency Questions and SPARQL-OWL Queries Dataset and Analysis", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Competency Questions (CQs) are natural language questions outlining and\nconstraining the scope of knowledge represented by an ontology. Despite that\nCQs are a part of several ontology engineering methodologies, we have observed\nthat the actual publication of CQs for the available ontologies is very limited\nand even scarcer is the publication of their respective formalisations in terms\nof, e.g., SPARQL queries. This paper aims to contribute to addressing the\nengineering shortcomings of using CQs in ontology development, to facilitate\nwider use of CQs. In order to understand the relation between CQs and the\nqueries over the ontology to test the CQs on an ontology, we gather, analyse,\nand publicly release a set of 234 CQs and their translations to SPARQL-OWL for\nseveral ontologies in different domains developed by different groups. We\nanalysed the CQs in two principal ways. The first stage focused on a linguistic\nanalysis of the natural language text itself, i.e., a lexico-syntactic analysis\nwithout any presuppositions of ontology elements, and a subsequent step of\nsemantic analysis in order to find patterns. This increased diversity of CQ\nsources resulted in a 5-fold increase of hitherto published patterns, to 106\ndistinct CQ patterns, which have a limited subset of few patterns shared across\nthe CQ sets from the different ontologies. Next, we analysed the relation\nbetween the found CQ patterns and the 46 SPARQL-OWL query signatures, which\nrevealed that one CQ pattern may be realised by more than one SPARQL-OWL query\nsignature, and vice versa. We hope that our work will contribute to\nestablishing common practices, templates, automation, and user tools that will\nsupport CQ formulation, formalisation, execution, and general management.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 16:00:51 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Wisniewski", "Dawid", ""], ["Potoniec", "Jedrzej", ""], ["Lawrynowicz", "Agnieszka", ""], ["Keet", "C. Maria", ""]]}, {"id": "1811.10000", "submitter": "Patrick P. C. Lee", "authors": "Yongkun Li, Helen H. W. Chan, Patrick P. C. Lee, Yinlong Xu", "title": "Enabling Efficient Updates in KV Storage via Hashing: Design and\n  Performance Evaluation", "comments": "28 pages. Accepted by ACM Transactions on Storage", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Persistent key-value (KV) stores mostly build on the Log-Structured Merge\n(LSM) tree for high write performance, yet the LSM-tree suffers from the\ninherently high I/O amplification. KV separation mitigates I/O amplification by\nstoring only keys in the LSM-tree and values in separate storage. However, the\ncurrent KV separation design remains inefficient under update-intensive\nworkloads due to its high garbage collection (GC) overhead in value storage. We\npropose HashKV, which aims for high update performance atop KV separation under\nupdate-intensive workloads. HashKV uses hash-based data grouping, which\ndeterministically maps values to storage space so as to make both updates and\nGC efficient. We further relax the restriction of such deterministic mappings\nvia simple but useful design extensions. We extensively evaluate various design\naspects of HashKV. We show that HashKV achieves 4.6x update throughput and\n53.4% less write traffic compared to the current KV separation design. In\naddition, we demonstrate that we can integrate the design of HashKV with\nstate-of-the-art KV stores and improve their respective performance.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 12:52:26 GMT"}, {"version": "v2", "created": "Mon, 17 Jun 2019 08:21:13 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Li", "Yongkun", ""], ["Chan", "Helen H. W.", ""], ["Lee", "Patrick P. C.", ""], ["Xu", "Yinlong", ""]]}, {"id": "1811.10191", "submitter": "Alok Chauhan", "authors": "Alok Chauhan, Vijayakumar V, Layth Sliman", "title": "Ontology Matching Techniques: A Gold Standard Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typically an ontology matching technique is a combination of much different\ntype of matchers operating at various abstraction levels such as structure,\nsemantic, syntax, instance etc. An ontology matching technique which employs\nmatchers at all possible abstraction levels is expected to give, in general,\nbest results in terms of precision, recall and F-measure due to improvement in\nmatching opportunities and if we discount efficiency issues which may improve\nwith better computing resources such as parallel processing. A gold standard\nontology matching model is derived from a model classification of ontology\nmatching techniques. A suitable metric is also defined based on gold standard\nontology matching model. A review of various ontology matching techniques\nspecified in recent research papers in the area was undertaken to categorize an\nontology matching technique as per newly proposed gold standard model and a\nmetric value for the whole group was computed. The results of the above study\nsupport proposed gold standard ontology matching model.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 06:10:01 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Chauhan", "Alok", ""], ["V", "Vijayakumar", ""], ["Sliman", "Layth", ""]]}, {"id": "1811.10786", "submitter": "Zengjian Chen", "authors": "Zengjian Chen, Jiayi Liu, Yihe Deng, Kun He and John E. Hopcroft", "title": "Adaptive Wavelet Clustering for Highly Noisy Data", "comments": "11 pages,13 figures,ICDE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we make progress on the unsupervised task of mining arbitrarily\nshaped clusters in highly noisy datasets, which is a task present in many\nreal-world applications. Based on the fundamental work that first applies a\nwavelet transform to data clustering, we propose an adaptive clustering\nalgorithm, denoted as AdaWave, which exhibits favorable characteristics for\nclustering. By a self-adaptive thresholding technique, AdaWave is parameter\nfree and can handle data in various situations. It is deterministic, fast in\nlinear time, order-insensitive, shape-insensitive, robust to highly noisy data,\nand requires no pre-knowledge on data models. Moreover, AdaWave inherits the\nability from the wavelet transform to cluster data in different resolutions. We\nadopt the \"grid labeling\" data structure to drastically reduce the memory\nconsumption of the wavelet transform so that AdaWave can be used for relatively\nhigh dimensional data. Experiments on synthetic as well as natural datasets\ndemonstrate the effectiveness and efficiency of our proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 03:05:26 GMT"}, {"version": "v2", "created": "Mon, 7 Jan 2019 06:22:23 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Chen", "Zengjian", ""], ["Liu", "Jiayi", ""], ["Deng", "Yihe", ""], ["He", "Kun", ""], ["Hopcroft", "John E.", ""]]}, {"id": "1811.10835", "submitter": "Chen Yang", "authors": "Chen Yang, Zhihui Du, Xiaofeng Meng, Yongjie Du and Zhiqiang Duan", "title": "A Frequency Scaling based Performance Indicator Framework for Big Data\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is important for big data systems to identify their performance\nbottleneck. However, the popular indicators such as resource utilizations, are\noften misleading and incomparable with each other. In this paper, a novel\nindicator framework which can directly compare the impact of different\nindicators with each other is proposed to identify and analyze the performance\nbottleneck efficiently. A methodology which can construct the indicator from\nthe performance change with the CPU frequency scaling is described. Spark is\nused as an example of a big data system and two typical SQL benchmarks are used\nas the workloads to evaluate the proposed method. Experimental results show\nthat the proposed method is accurate compared with the resource utilization\nmethod and easy to implement compared with some white-box method. Meanwhile,\nthe analysis with our indicators lead to some interesting findings and valuable\nperformance optimization suggestions for big data systems.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 06:34:57 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Yang", "Chen", ""], ["Du", "Zhihui", ""], ["Meng", "Xiaofeng", ""], ["Du", "Yongjie", ""], ["Duan", "Zhiqiang", ""]]}, {"id": "1811.10855", "submitter": "Chen Yang", "authors": "Chen Yang, Xiaofeng Meng, Zhihui Du, Zhiqiang Duan and Yongjie Du", "title": "Data Management in Time-Domain Astronomy: Requirements and Challenges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In time-domain astronomy, we need to use the relational database to manage\nstar catalog data. With the development of sky survey technology, the size of\nstar catalog data is larger, and the speed of data generation is faster. So, in\nthis paper, we make a systematic and comprehensive introduction to process the\ndata in time-domain astronomy, and valuable research questions are detailed.\nThen, we list candidate systems usually used in astronomy and point out the\nadvantages and disadvantages of these systems. In addition, we present the key\ntechniques needed to deal with astronomical data. Finally, we summarize the\nchallenges faced by the design of our database prototype.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 07:54:43 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Yang", "Chen", ""], ["Meng", "Xiaofeng", ""], ["Du", "Zhihui", ""], ["Duan", "Zhiqiang", ""], ["Du", "Yongjie", ""]]}, {"id": "1811.10861", "submitter": "Chen Yang", "authors": "Chen Yang, Xiaofeng Meng, Zhihui Du, JiaMing Qiu, Kenan Liang, Yongjie\n  Du, Zhiqiang Duan, Xiaobin Ma and Zhijian Fang", "title": "AstroServ: Distributed Database for Serving Large-Scale Full Life-Cycle\n  Astronomical Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In time-domain astronomy, STLF (Short-Timescale and Large Field-of-view) sky\nsurvey is the latest way of sky observation. Compared to traditional sky survey\nwho can only find astronomical phenomena, STLF sky survey can even reveal how\nshort astronomical phenomena evolve. The difference does not only lead the new\nsurvey data but also the new analysis style. It requires that database behind\nSTLF sky survey should support continuous analysis on data streaming, real-time\nanalysis on short-term data and complex analysis on long-term historical data.\nIn addition, both insertion and query latencies have strict requirements to\nensure that scientific phenomena can be discovered. However, the existing\ndatabases cannot support our scenario. In this paper, we propose AstroServ, a\ndistributed system for analysis and management of large-scale and full\nlife-cycle astronomical data. AstroServ's core components include three data\nservice layers and a query engine. Each data service layer serves for a\nspecific time period of data and query engine can provide the uniform analysis\ninterface on different data. In addition, we also provide many applications\nincluding interactive analysis interface and data mining tool to help\nscientists efficiently use data. The experimental results show that AstroServ\ncan meet the strict performance requirements and the good recognition accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 08:08:39 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Yang", "Chen", ""], ["Meng", "Xiaofeng", ""], ["Du", "Zhihui", ""], ["Qiu", "JiaMing", ""], ["Liang", "Kenan", ""], ["Du", "Yongjie", ""], ["Duan", "Zhiqiang", ""], ["Ma", "Xiaobin", ""], ["Fang", "Zhijian", ""]]}, {"id": "1811.10865", "submitter": "Chen Yang", "authors": "Chen Yang, Xiaofeng Meng and Zhihui Du", "title": "Cloud based Real-Time and Low Latency Scientific Event Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Astronomy is well recognized as big data driven science. As the novel\nobservation infrastructures are developed, the sky survey cycles have been\nshortened from a few days to a few seconds, causing data processing pressure to\nshift from offline to online. However, existing scientific databases focus on\noffline analysis of long-term historical data, not real-time and low latency\nanalysis of large-scale newly arriving data.\n  In this paper, a cloud based method is proposed to efficiently analyze\nscientific events on large-scale newly arriving data. The solution is\nimplemented as a highly efficient system, namely Aserv. A set of compact data\nstore and index structures are proposed to describe the proposed scientific\nevents and a typical analysis pattern is formulized as a set of query\noperations. Domain aware filter, accuracy aware data partition, highly\nefficient index and frequently used statistical data designs are four key\nmethods to optimize the performance of Aserv. Experimental results under the\ntypical cloud environment show that the presented optimization mechanism can\nmeet the low latency demand for both large data insertion and scientific event\nanalysis. Aserv can insert 3.5 million rows of data within 3 seconds and\nperform the heaviest query on 6.7 billion rows of data also within 3 seconds.\nFurthermore, a performance model is given to help Aserv choose the right cloud\nresource setup to meet the guaranteed real-time performance requirement.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 08:19:06 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Yang", "Chen", ""], ["Meng", "Xiaofeng", ""], ["Du", "Zhihui", ""]]}, {"id": "1811.10955", "submitter": "Oren Kalinsky", "authors": "Oren Kalinsky, Oren Mishali, Aidan Hogan, Yoav Etsion, Benny Kimelfeld", "title": "Efficiently Charting RDF", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a visual query language for interactively exploring large-scale\nknowledge graphs. Starting from an overview, the user explores bar charts\nthrough three interactions: class expansion, property expansion, and\nsubject/object expansion. A major challenge faced is performance: a\nstate-of-the-art SPARQL engine may require tens of minutes to compute the\nmultiway join, grouping and counting required to render a bar chart. A\npromising alternative is to apply approximation through online aggregation,\ntrading precision for performance. However, state-of-the-art online aggregation\nalgorithms such as Wander Join have two limitations for our exploration\nscenario: (1) a high number of rejected paths slows the convergence of the\ncount estimations, and (2) no unbiased estimator exists for counts under the\ndistinct operator. We thus devise a specialized algorithm for online\naggregation that augments Wander Join with exact partial computations to reduce\nthe number of rejected paths encountered, as well as a novel estimator that we\nprove to be unbiased in the case of the distinct operator. In an experimental\nstudy with random interactions exploring two large-scale knowledge graphs, our\nalgorithm shows a clear reduction in error with respect to computation time\nversus Wander Join.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 13:11:16 GMT"}, {"version": "v2", "created": "Sat, 26 Jan 2019 11:11:47 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Kalinsky", "Oren", ""], ["Mishali", "Oren", ""], ["Hogan", "Aidan", ""], ["Etsion", "Yoav", ""], ["Kimelfeld", "Benny", ""]]}, {"id": "1811.11160", "submitter": "Yi-Peng Wei", "authors": "Yi-Peng Wei and Batuhan Arasli and Karim Banawan and Sennur Ulukus", "title": "The Capacity of Private Information Retrieval from Decentralized Uncoded\n  Caching Databases", "comments": "Submitted for publication, November 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.DB math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the private information retrieval (PIR) problem from\ndecentralized uncoded caching databases. There are two phases in our problem\nsetting, a caching phase, and a retrieval phase. In the caching phase, a data\ncenter containing all the $K$ files, where each file is of size $L$ bits, and\nseveral databases with storage size constraint $\\mu K L$ bits exist in the\nsystem. Each database independently chooses $\\mu K L$ bits out of the total\n$KL$ bits from the data center to cache through the same probability\ndistribution in a decentralized manner. In the retrieval phase, a user\n(retriever) accesses $N$ databases in addition to the data center, and wishes\nto retrieve a desired file privately. We characterize the optimal normalized\ndownload cost to be $\\frac{D}{L} = \\sum_{n=1}^{N+1} \\binom{N}{n-1} \\mu^{n-1}\n(1-\\mu)^{N+1-n} \\left( 1+ \\frac{1}{n} + \\dots+ \\frac{1}{n^{K-1}} \\right)$. We\nshow that uniform and random caching scheme which is originally proposed for\ndecentralized coded caching by Maddah-Ali and Niesen, along with Sun and Jafar\nretrieval scheme which is originally proposed for PIR from replicated databases\nsurprisingly result in the lowest normalized download cost. This is the\ndecentralized counterpart of the recent result of Attia, Kumar and Tandon for\nthe centralized case. The converse proof contains several ingredients such as\ninterference lower bound, induction lemma, replacing queries and answering\nstring random variables with the content of distributed databases, the nature\nof decentralized uncoded caching databases, and bit marginalization of joint\ncaching distributions.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 18:51:12 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Wei", "Yi-Peng", ""], ["Arasli", "Batuhan", ""], ["Banawan", "Karim", ""], ["Ulukus", "Sennur", ""]]}, {"id": "1811.11242", "submitter": "Gerrit van den Burg", "authors": "Gerrit J.J. van den Burg, Alfredo Nazabal, Charles Sutton", "title": "Wrangling Messy CSV Files by Detecting Row and Type Patterns", "comments": null, "journal-ref": "Data Mining and Knowledge Discovery (July, 2019)", "doi": "10.1007/s10618-019-00646-y", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that data scientists spend the majority of their time on\npreparing data for analysis. One of the first steps in this preparation phase\nis to load the data from the raw storage format. Comma-separated value (CSV)\nfiles are a popular format for tabular data due to their simplicity and\nostensible ease of use. However, formatting standards for CSV files are not\nfollowed consistently, so each file requires manual inspection and potentially\nrepair before the data can be loaded, an enormous waste of human effort for a\ntask that should be one of the simplest parts of data science. The first and\nmost essential step in retrieving data from CSV files is deciding on the\ndialect of the file, such as the cell delimiter and quote character. Existing\ndialect detection approaches are few and non-robust. In this paper, we propose\na dialect detection method based on a novel measure of data consistency of\nparsed data files. Our method achieves 97% overall accuracy on a large corpus\nof real-world CSV files and improves the accuracy on messy CSV files by almost\n22% compared to existing approaches, including those in the Python standard\nlibrary.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 20:26:33 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Burg", "Gerrit J. J. van den", ""], ["Nazabal", "Alfredo", ""], ["Sutton", "Charles", ""]]}, {"id": "1811.11561", "submitter": "Stefania Dumbrava", "authors": "Stefania Dumbrava, Angela Bonifati, Amaia Nazabal Ruiz Diaz, and\n  Romain Vuillemot", "title": "Approximate Evaluation of Label-Constrained Reachability Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current surge of interest in graph-based data models mirrors the usage of\nincreasingly complex reachability queries, as witnessed by recent analytical\nstudies on real-world graph query logs. Despite the maturity of graph DBMS\ncapabilities, complex label-constrained reachability queries, along with their\ncorresponding aggregate versions, remain difficult to evaluate. In this paper,\nwe focus on the approximate evaluation of counting label-constrained\nreachability queries. We offer a human-explainable solution to graph\nApproximate Query Processing (AQP). This consists of a summarization algorithm\n(GRASP), as well as of a custom visualization plug-in, which allows users to\nexplore the obtained summaries. We prove that the problem of node group\nminimization, associated to the creation of GRASP summaries, is NP-complete.\nNonetheless, our GRASP summaries are reasonably small in practice, even for\nlarge graph instances, and guarantee approximate graph query answering, paired\nwith controllable error estimates. We experimentally gauge the scalability and\nefficiency of our GRASP algorithm, and verify the accuracy and error estimation\nof the graph AQP module. To the best of our knowledge, ours is the first system\ncapable of handling visualization-driven approximate graph analytics for\ncomplex label-constrained reachability queries.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 13:51:39 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Dumbrava", "Stefania", ""], ["Bonifati", "Angela", ""], ["Diaz", "Amaia Nazabal Ruiz", ""], ["Vuillemot", "Romain", ""]]}, {"id": "1811.11593", "submitter": "Nikolaos Bikakis", "authors": "Nikos Bikakis, Vana Kalogeraki, Dimitrios Gunupulos", "title": "Attendance Maximization for Successful Social Event Planning", "comments": "This paper appears in 22nd Intl. Conf. on Extending Database\n  Technology (EDBT 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social event planning has received a great deal of attention in recent years\nwhere various entities, such as event planners and marketing companies,\norganizations, venues, or users in Event-based Social Networks, organize\nnumerous social events (e.g., festivals, conferences, promotion parties).\nRecent studies show that \"attendance\" is the most common metric used to capture\nthe success of social events, since the number of attendees has great impact on\nthe event's expected gains (e.g., revenue, artist/brand publicity). In this\nwork, we study the Social Event Scheduling (SES) problem which aims at\nidentifying and assigning social events to appropriate time slots, so that the\nnumber of events attendees is maximized. We show that, even in highly\nrestricted instances, the SES problem is NP-hard to be approximated over a\nfactor. To solve the SES problem, we design three efficient and scalable\nalgorithms. These algorithms exploit several novel schemes that we design. We\nconduct extensive experiments using several real and synthetic datasets, and\ndemonstrate that the proposed algorithms perform on average half the\ncomputations compared to the existing solution and, in several cases, are 3-5\ntimes faster.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 14:35:22 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Bikakis", "Nikos", ""], ["Kalogeraki", "Vana", ""], ["Gunupulos", "Dimitrios", ""]]}, {"id": "1811.12204", "submitter": "Erfan Zamanian", "authors": "Erfan Zamanian, Julian Shun, Carsten Binnig, Tim Kraska", "title": "Chiller: Contention-centric Transaction Execution and Data Partitioning\n  for Modern Networks", "comments": null, "journal-ref": null, "doi": "10.1145/3318464.3389724", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed transactions on high-overhead TCP/IP-based networks were\nconventionally considered to be prohibitively expensive and thus were avoided\nat all costs. To that end, the primary goal of almost any existing partitioning\nscheme is to minimize the number of cross-partition transactions. However, with\nthe new generation of fast RDMA-enabled networks, this assumption is no longer\nvalid. In fact, recent work has shown that distributed databases can scale even\nwhen the majority of transactions are cross-partition. In this paper, we first\nmake the case that the new bottleneck which hinders truly scalable transaction\nprocessing in modern RDMA-enabled databases is data contention, and that\noptimizing for data contention leads to different partitioning layouts than\noptimizing for the number of distributed transactions. We then present Chiller,\na new approach to data partitioning and transaction execution, which aims to\nminimize data contention for both local and distributed transactions. Finally,\nwe evaluate Chiller using various workloads, and show that our partitioning and\nexecution strategy outperforms traditional partitioning techniques which try to\navoid distributed transactions, by up to a factor of 2.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 14:45:48 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2020 13:28:33 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Zamanian", "Erfan", ""], ["Shun", "Julian", ""], ["Binnig", "Carsten", ""], ["Kraska", "Tim", ""]]}, {"id": "1811.12823", "submitter": "Daniil Polykovskiy", "authors": "Daniil Polykovskiy, Alexander Zhebrak, Benjamin Sanchez-Lengeling,\n  Sergey Golovanov, Oktai Tatanov, Stanislav Belyaev, Rauf Kurbanov, Aleksey\n  Artamonov, Vladimir Aladinskiy, Mark Veselov, Artur Kadurin, Simon Johansson,\n  Hongming Chen, Sergey Nikolenko, Alan Aspuru-Guzik, Alex Zhavoronkov", "title": "Molecular Sets (MOSES): A Benchmarking Platform for Molecular Generation\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DB stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Generative models are becoming a tool of choice for exploring the molecular\nspace. These models learn on a large training dataset and produce novel\nmolecular structures with similar properties. Generated structures can be\nutilized for virtual screening or training semi-supervised predictive models in\nthe downstream tasks. While there are plenty of generative models, it is\nunclear how to compare and rank them. In this work, we introduce a benchmarking\nplatform called Molecular Sets (MOSES) to standardize training and comparison\nof molecular generative models. MOSES provides a training and testing datasets,\nand a set of metrics to evaluate the quality and diversity of generated\nstructures. We have implemented and compared several molecular generation\nmodels and suggest to use our results as reference points for further\nadvancements in generative chemistry research. The platform and source code are\navailable at https://github.com/molecularsets/moses.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 08:48:20 GMT"}, {"version": "v2", "created": "Tue, 12 Mar 2019 20:03:21 GMT"}, {"version": "v3", "created": "Tue, 15 Oct 2019 07:23:45 GMT"}, {"version": "v4", "created": "Fri, 29 May 2020 16:15:59 GMT"}, {"version": "v5", "created": "Wed, 28 Oct 2020 14:11:16 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Polykovskiy", "Daniil", ""], ["Zhebrak", "Alexander", ""], ["Sanchez-Lengeling", "Benjamin", ""], ["Golovanov", "Sergey", ""], ["Tatanov", "Oktai", ""], ["Belyaev", "Stanislav", ""], ["Kurbanov", "Rauf", ""], ["Artamonov", "Aleksey", ""], ["Aladinskiy", "Vladimir", ""], ["Veselov", "Mark", ""], ["Kadurin", "Artur", ""], ["Johansson", "Simon", ""], ["Chen", "Hongming", ""], ["Nikolenko", "Sergey", ""], ["Aspuru-Guzik", "Alan", ""], ["Zhavoronkov", "Alex", ""]]}]