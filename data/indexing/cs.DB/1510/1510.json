[{"id": "1510.00552", "submitter": "Daniele Ramazzotti", "authors": "Francesco Bonchi, Sara Hajian, Bud Mishra, Daniele Ramazzotti", "title": "Exposing the Probabilistic Causal Structure of Discrimination", "comments": null, "journal-ref": null, "doi": "10.1007/s41060-016-0040-z", "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrimination discovery from data is an important task aiming at identifying\npatterns of illegal and unethical discriminatory activities against\nprotected-by-law groups, e.g., ethnic minorities. While any legally-valid proof\nof discrimination requires evidence of causality, the state-of-the-art methods\nare essentially correlation-based, albeit, as it is well known, correlation\ndoes not imply causation.\n  In this paper we take a principled causal approach to the data mining problem\nof discrimination detection in databases. Following Suppes' probabilistic\ncausation theory, we define a method to extract, from a dataset of historical\ndecision records, the causal structures existing among the attributes in the\ndata. The result is a type of constrained Bayesian network, which we dub\nSuppes-Bayes Causal Network (SBCN). Next, we develop a toolkit of methods based\non random walks on top of the SBCN, addressing different anti-discrimination\nlegal concepts, such as direct and indirect discrimination, group and\nindividual discrimination, genuine requirement, and favoritism. Our experiments\non real-world datasets confirm the inferential power of our approach in all\nthese different tasks.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2015 10:31:29 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2015 08:38:16 GMT"}, {"version": "v3", "created": "Wed, 8 Mar 2017 21:10:10 GMT"}], "update_date": "2017-03-10", "authors_parsed": [["Bonchi", "Francesco", ""], ["Hajian", "Sara", ""], ["Mishra", "Bud", ""], ["Ramazzotti", "Daniele", ""]]}, {"id": "1510.00917", "submitter": "Maurizio Naldi", "authors": "Maurizio Naldi and Giuseppe D'Acquisto", "title": "Differential Privacy: An Estimation Theory-Based Method for Choosing\n  Epsilon", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy is achieved by the introduction of Laplacian noise in\nthe response to a query, establishing a precise trade-off between the level of\ndifferential privacy and the accuracy of the database response (via the amount\nof noise introduced). However, the amount of noise to add is typically defined\nthrough the scale parameter of the Laplace distribution, whose use may not be\nso intuitive. In this paper we propose to use two parameters instead, related\nto the notion of interval estimation, which provide a more intuitive picture of\nhow precisely the true output of a counting query may be gauged from the\nnoise-polluted one (hence, how much the individual's privacy is protected).\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2015 09:40:16 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Naldi", "Maurizio", ""], ["D'Acquisto", "Giuseppe", ""]]}, {"id": "1510.02188", "submitter": "Zhi-Hong Deng", "authors": "Zhi-Hong Deng, Shulei Ma, He Liu", "title": "An Efficient Data Structure for Fast Mining High Utility Itemsets", "comments": "25 pages,9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel data structure called PUN-list, which\nmaintains both the utility information about an itemset and utility upper bound\nfor facilitating the processing of mining high utility itemsets. Based on\nPUN-lists, we present a method, called MIP (Mining high utility Itemset using\nPUN-Lists), for fast mining high utility itemsets. The efficiency of MIP is\nachieved with three techniques. First, itemsets are represented by a highly\ncondensed data structure, PUN-list, which avoids costly, repeatedly utility\ncomputation. Second, the utility of an itemset can be efficiently calculated by\nscanning the PUN-list of the itemset and the PUN-lists of long itemsets can be\nfast constructed by the PUN-lists of short itemsets. Third, by employing the\nutility upper bound lying in the PUN-lists as the pruning strategy, MIP\ndirectly discovers high utility itemsets from the search space, called\nset-enumeration tree, without generating numerous candidates. Extensive\nexperiments on various synthetic and real datasets show that PUN-list is very\neffective since MIP is at least an order of magnitude faster than recently\nreported algorithms on average.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2015 03:04:12 GMT"}], "update_date": "2015-10-09", "authors_parsed": [["Deng", "Zhi-Hong", ""], ["Ma", "Shulei", ""], ["Liu", "He", ""]]}, {"id": "1510.02219", "submitter": "Nan Tang", "authors": "Nan Tang, Qing Chen, Prasenjit Mitra", "title": "On Summarizing Graph Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph streams, which refer to the graph with edges being updated sequentially\nin a form of a stream, have wide applications such as cyber security, social\nnetworks and transportation networks. This paper studies the problem of\nsummarizing graph streams. Specifically, given a graph stream G, directed or\nundirected, the objective is to summarize G as S with much smaller (sublinear)\nspace, linear construction time and constant maintenance cost for each edge\nupdate, such that S allows many queries over G to be approximately conducted\nefficiently. Due to the sheer volume and highly dynamic nature of graph\nstreams, summarizing them remains a notoriously hard, if not impossible,\nproblem. The widely used practice of summarizing data streams is to treat each\nelement independently by e.g., hash- or sampling-based method, without keeping\ntrack of the connections between elements in a data stream, which gives these\nsummaries limited power in supporting complicated queries over graph streams.\nThis paper discusses a fundamentally different philosophy for summarizing graph\nstreams. We present gLava, a probabilistic graph model that, instead of\ntreating an edge (a stream element) as the operating unit, uses the finer\ngrained node in an element. This will naturally form a new graph sketch where\nedges capture the connections inside elements, and nodes maintain relationships\nacross elements. We discuss a wide range of supported graph queries and\nestablish theoretical error bounds for basic queries.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2015 07:58:31 GMT"}], "update_date": "2015-10-09", "authors_parsed": [["Tang", "Nan", ""], ["Chen", "Qing", ""], ["Mitra", "Prasenjit", ""]]}, {"id": "1510.02395", "submitter": "Mohammed Gollapalli Dr.", "authors": "Mohammed Gollapalli", "title": "Literature Review Of Attribute Level And Structure Level Data Linkage\n  Techniques", "comments": "20 pages", "journal-ref": "International Journal of Data Mining & Knowledge Management\n  Process (IJDKP) Vol.5, No.5, September 2015", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data Linkage is an important step that can provide valuable insights for\nevidence-based decision making, especially for crucial events. Performing\nsensible queries across heterogeneous databases containing millions of records\nis a complex task that requires a complete understanding of each contributing\ndatabases schema to define the structure of its information. The key aim is to\napproximate the structure and content of the induced data into a concise\nsynopsis in order to extract and link meaningful data-driven facts. We identify\nsuch problems as four major research issues in Data Linkage: associated costs\nin pair-wise matching, record matching overheads, semantic flow of information\nrestrictions, and single order classification limitations. In this paper, we\ngive a literature review of research in Data Linkage. The purpose for this\nreview is to establish a basic understanding of Data Linkage, and to discuss\nthe background in the Data Linkage research domain. Particularly, we focus on\nthe literature related to the recent advancements in Approximate Matching\nalgorithms at Attribute Level and Structure Level. Their efficiency,\nfunctionality and limitations are critically analysed and open-ended problems\nhave been exposed.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2015 12:38:24 GMT"}], "update_date": "2015-10-09", "authors_parsed": [["Gollapalli", "Mohammed", ""]]}, {"id": "1510.02824", "submitter": "Thomas Dybdahl Ahle", "authors": "Thomas D. Ahle and Rasmus Pagh and Ilya Razenshteyn and Francesco\n  Silvestri", "title": "On the Complexity of Inner Product Similarity Join", "comments": "in Proc. 35th ACM Symposium on Principles of Database Systems, 2016", "journal-ref": null, "doi": "10.1145/2902251.2902285", "report-no": null, "categories": "cs.DS cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of tasks in classification, information retrieval, recommendation\nsystems, and record linkage reduce to the core problem of inner product\nsimilarity join (IPS join): identifying pairs of vectors in a collection that\nhave a sufficiently large inner product. IPS join is well understood when\nvectors are normalized and some approximation of inner products is allowed.\nHowever, the general case where vectors may have any length appears much more\nchallenging. Recently, new upper bounds based on asymmetric locality-sensitive\nhashing (ALSH) and asymmetric embeddings have emerged, but little has been\nknown on the lower bound side. In this paper we initiate a systematic study of\ninner product similarity join, showing new lower and upper bounds. Our main\nresults are:\n  * Approximation hardness of IPS join in subquadratic time, assuming the\nstrong exponential time hypothesis.\n  * New upper and lower bounds for (A)LSH-based algorithms. In particular, we\nshow that asymmetry can be avoided by relaxing the LSH definition to only\nconsider the collision probability of distinct elements.\n  * A new indexing method for IPS based on linear sketches, implying that our\nhardness results are not far from being tight.\n  Our technical contributions include new asymmetric embeddings that may be of\nindependent interest. At the conceptual level we strive to provide greater\nclarity, for example by distinguishing among signed and unsigned variants of\nIPS join and shedding new light on the effect of asymmetry.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2015 21:10:12 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2015 09:45:18 GMT"}, {"version": "v3", "created": "Thu, 7 Apr 2016 11:36:25 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Ahle", "Thomas D.", ""], ["Pagh", "Rasmus", ""], ["Razenshteyn", "Ilya", ""], ["Silvestri", "Francesco", ""]]}, {"id": "1510.02886", "submitter": "Jian Dai", "authors": "Jian Dai, Bin Yang, Chenjuan Guo, Christian S. Jensen", "title": "Efficient and Accurate Path Cost Estimation Using Trajectory Data", "comments": "16pages, 42 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Using the growing volumes of vehicle trajectory data, it becomes increasingly\npossible to capture time-varying and uncertain travel costs in a road network,\nincluding travel time and fuel consumption. The current paradigm represents a\nroad network as a graph, assigns weights to the graph's edges by fragmenting\ntrajectories into small pieces that fit the underlying edges, and then applies\na routing algorithm to the resulting graph. We propose a new paradigm that\ntargets more accurate and more efficient estimation of the costs of paths by\nassociating weights with sub-paths in the road network. The paper provides a\nsolution to a foundational problem in this paradigm, namely that of computing\nthe time-varying cost distribution of a path.\n  The solution consists of several steps. We first learn a set of random\nvariables that capture the joint distributions of sub-paths that are covered by\nsufficient trajectories. Then, given a departure time and a path, we select an\noptimal subset of learned random variables such that the random variables'\ncorresponding paths together cover the path. This enables accurate joint\ndistribution estimation of the path, and by transferring the joint distribution\ninto a marginal distribution, the travel cost distribution of the path is\nobtained. The use of multiple learned random variables contends with data\nsparseness, the use of multi-dimensional histograms enables compact\nrepresentation of arbitrary joint distributions that fully capture the travel\ncost dependencies among the edges in paths. Empirical studies with substantial\ntrajectory data from two different cities offer insight into the design\nproperties of the proposed solution and suggest that the solution is effective\nin real-world settings.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2015 08:06:07 GMT"}, {"version": "v2", "created": "Sun, 18 Oct 2015 07:32:47 GMT"}, {"version": "v3", "created": "Fri, 4 Dec 2015 01:29:27 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Dai", "Jian", ""], ["Yang", "Bin", ""], ["Guo", "Chenjuan", ""], ["Jensen", "Christian S.", ""]]}, {"id": "1510.03149", "submitter": "Peng Cheng", "authors": "Peng Cheng, Xiang Lian, Lei Chen, Jinsong Han, Jizhong Zhao", "title": "Task Assignment on Multi-Skill Oriented Spatial Crowdsourcing", "comments": "15 pages", "journal-ref": null, "doi": "10.1109/TKDE.2016.2550041", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of mobile devices and crowdsourcing platforms, the\nspatial crowdsourcing has attracted much attention from the database community.\nSpecifically, the spatial crowdsourcing refers to sending location-based\nrequests to workers, based on their current positions. In this paper, we\nconsider a spatial crowdsourcing scenario, in which each worker has a set of\nqualified skills, whereas each spatial task (e.g., repairing a house,\ndecorating a room, and performing entertainment shows for a ceremony) is\ntime-constrained, under the budget constraint, and required a set of skills.\nUnder this scenario, we will study an important problem, namely multi-skill\nspatial crowdsourcing (MS-SC), which finds an optimal worker-and-task\nassignment strategy, such that skills between workers and tasks match with each\nother, and workers' benefits are maximized under the budget constraint. We\nprove that the MS-SC problem is NP-hard and intractable. Therefore, we propose\nthree effective heuristic approaches, including greedy, g-divide-and-conquer\nand cost-model-based adaptive algorithms to get worker-and-task assignments.\nThrough extensive experiments, we demonstrate the efficiency and effectiveness\nof our MS-SC processing approaches on both real and synthetic data sets.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2015 06:00:40 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2015 15:37:41 GMT"}, {"version": "v3", "created": "Tue, 20 Oct 2015 07:53:07 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Cheng", "Peng", ""], ["Lian", "Xiang", ""], ["Chen", "Lei", ""], ["Han", "Jinsong", ""], ["Zhao", "Jizhong", ""]]}, {"id": "1510.03302", "submitter": "Jaroslaw Szlichta", "authors": "Guilherme Damasio, Piotr Mierzejewski, Jaroslaw Szlichta, Calisto\n  Zuzarte", "title": "OptImatch: Semantic Web System with Knowledge Base for Query Performance\n  Problem Determination", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Database query performance problem determination is often performed by\nanalyzing query execution plans (QEPs) in addition to other performance data.\nAs the query workloads that organizations run, have become larger and more\ncomplex, analyzing QEPs manually even by experts has become a very time\nconsuming. Most performance diagnostic tools help with identifying problematic\nqueries and most query tuning tools address a limited number of known problems\nand recommendations. We present the OptImatch system that offers a way to (a)\nlook for varied user defined problem patterns in QEPs and (b) automatically get\nrecommendations from an expert provided and user customizable knowledge base.\nExisting approaches do not provide the ability to perform workload analysis\nwith flexible user defined patterns, as they lack the ability to impose a\nproper structure on QEPs. We introduce a novel semantic web system that allows\na relatively naive user to search for arbitrary patterns and to get\nrecommendations stored in a knowledge base either by experts or added by the\nuser tailored to the environment in which they operate. Our methodology\nincludes transforming a QEP into an RDF graph and transforming a GUI based\nuser-defined pattern into a SPARQL query through handlers. The SPARQL query is\nmatched against the abstracted RDF graph, and any matched portion of the\nabstracted RDF graph is relayed back to the user. With the knowledge base, the\nOptImatch system automatically scans and matches interesting stored patterns in\na statistical way as appropriate and returns the corresponding recommendations.\nAlthough the knowledge base patterns and solution recommendations are not in\nthe context of the user supplied QEPs, the context is adapted automatically\nthrough the handler tagging interface.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2015 14:31:19 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Damasio", "Guilherme", ""], ["Mierzejewski", "Piotr", ""], ["Szlichta", "Jaroslaw", ""], ["Zuzarte", "Calisto", ""]]}, {"id": "1510.03375", "submitter": "Irfan Ahmed", "authors": "Irshad Ahmed, Irfan Ahmed, Waseem Shahzad", "title": "Scaling up for high dimensional and high speed data streams: HSDStream", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel high speed clustering scheme for high dimensional\ndata streams. Data stream clustering has gained importance in different\napplications, for example, in network monitoring, intrusion detection, and\nreal-time sensing are few of those. High dimensional stream data is inherently\nmore complex when used for clustering because the evolving nature of the stream\ndata and high dimensionality make it non-trivial. In order to tackle this\nproblem, projected subspace within the high dimensions and limited window sized\ndata per unit of time are used for clustering purpose. We propose a High Speed\nand Dimensions data stream clustering scheme (HSDStream) which employs\nexponential moving averages to reduce the size of the memory and speed up the\nprocessing of projected subspace data stream. The proposed algorithm has been\ntested against HDDStream for cluster purity, memory usage, and the cluster\nsensitivity. Experimental results have been obtained for corrected KDD\nintrusion detection dataset. These results show that HSDStream outperforms the\nHDDStream in all performance metrics, especially the memory usage and the\nprocessing speed.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2015 17:47:18 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Ahmed", "Irshad", ""], ["Ahmed", "Irfan", ""], ["Shahzad", "Waseem", ""]]}, {"id": "1510.03409", "submitter": "Olivier Cur\\'e", "authors": "Olivier Cur\\'e, Hubert Naacke, Tendry Randriamalala, Bernd Amann", "title": "LiteMat: a scalable, cost-efficient inference encoding scheme for large\n  RDF graphs", "comments": "8 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of linked data sources and the size of the linked open data graph\nkeep growing every day. As a consequence, semantic RDF services are more and\nmore confronted with various \"big data\" problems. Query processing in the\npresence of inferences is one them. For instance, to complete the answer set of\nSPARQL queries, RDF database systems evaluate semantic RDFS relationships\n(subPropertyOf, subClassOf) through time-consuming query rewriting algorithms\nor space-consuming data materialization solutions. To reduce the memory\nfootprint and ease the exchange of large datasets, these systems generally\napply a dictionary approach for compressing triple data sizes by replacing\nresource identifiers (IRIs), blank nodes and literals with integer values. In\nthis article, we present a structured resource identification scheme using a\nclever encoding of concepts and property hierarchies for efficiently evaluating\nthe main common RDFS entailment rules while minimizing triple materialization\nand query rewriting. We will show how this encoding can be computed by a\nscalable parallel algorithm and directly be implemented over the Apache Spark\nframework. The efficiency of our encoding scheme is emphasized by an evaluation\nconducted over both synthetic and real world datasets.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2015 19:45:51 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Cur\u00e9", "Olivier", ""], ["Naacke", "Hubert", ""], ["Randriamalala", "Tendry", ""], ["Amann", "Bernd", ""]]}, {"id": "1510.03921", "submitter": "Yongjoo Park", "authors": "Yongjoo Park, Michael Cafarella, and Barzan Mozafari", "title": "Visualization-Aware Sampling for Very Large Databases", "comments": null, "journal-ref": "Data Engineering (ICDE), 2016 IEEE 32nd International Conference\n  on. IEEE, 2016", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive visualizations are crucial in ad hoc data exploration and\nanalysis. However, with the growing number of massive datasets, generating\nvisualizations in interactive timescales is increasingly challenging. One\napproach for improving the speed of the visualization tool is via data\nreduction in order to reduce the computational overhead, but at a potential\ncost in visualization accuracy. Common data reduction techniques, such as\nuniform and stratified sampling, do not exploit the fact that the sampled\ntuples will be transformed into a visualization for human consumption.\n  We propose a visualization-aware sampling (VAS) that guarantees high quality\nvisualizations with a small subset of the entire dataset. We validate our\nmethod when applied to scatter and map plots for three common visualization\ngoals: regression, density estimation, and clustering. The key to our sampling\nmethod's success is in choosing tuples which minimize a visualization-inspired\nloss function. Our user study confirms that optimizing this loss function\ncorrelates strongly with user success in using the resulting visualizations. We\nalso show the NP-hardness of our optimization problem and propose an efficient\napproximation algorithm. Our experiments show that, compared to previous\nmethods, (i) using the same sample size, VAS improves user's success by up to\n35% in various visualization tasks, and (ii) VAS can achieve a required\nvisualization quality up to 400 times faster.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2015 22:51:36 GMT"}, {"version": "v2", "created": "Mon, 23 Jan 2017 23:47:49 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Park", "Yongjoo", ""], ["Cafarella", "Michael", ""], ["Mozafari", "Barzan", ""]]}, {"id": "1510.03989", "submitter": "Lu\\'is Cruz-Filipe", "authors": "Lu\\'is Cruz-Filipe, Michael Franz, Artavazd Hakhverdyan, Marta\n  Ludovico, Isabel Nunes and Peter Schneider-Kamp", "title": "repAIrC: A Tool for Ensuring Data Consistency by Means of Active\n  Integrity Constraints", "comments": "IMADA-preprint-cs", "journal-ref": null, "doi": "10.5220/0005586400170026", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consistency of knowledge repositories is of prime importance in organization\nmanagement. Integrity constraints are a well-known vehicle for specifying data\nconsistency requirements in knowledge bases; in particular, active integrity\nconstraints go one step further, allowing the specification of preferred ways\nto overcome inconsistent situations in the context of database management. This\npaper describes a tool to validate an SQL database with respect to a given set\nof active integrity constraints, proposing possible repairs in case the\ndatabase is inconsistent. The tool is able to work with the different kinds of\nrepairs proposed in the literature, namely simple, founded, well-founded and\njustified repairs. It also implements strategies for parallelizing the search\nfor them, allowing the user both to compute partitions of independent or\nstratified active integrity constraints, and to apply these partitions to find\nrepairs of inconsistent databases efficiently in parallel.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2015 08:06:57 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Cruz-Filipe", "Lu\u00eds", ""], ["Franz", "Michael", ""], ["Hakhverdyan", "Artavazd", ""], ["Ludovico", "Marta", ""], ["Nunes", "Isabel", ""], ["Schneider-Kamp", "Peter", ""]]}, {"id": "1510.04130", "submitter": "Jaroslav Fowkes", "authors": "Jaroslav Fowkes and Charles Sutton", "title": "A Bayesian Network Model for Interesting Itemsets", "comments": "Supplementary material attached as Ancillary File; in PKDD 2016:\n  European Conference on Machine Learning and Knowledge Discovery in Databases", "journal-ref": null, "doi": "10.1007/978-3-319-46227-1_26", "report-no": null, "categories": "stat.ML cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining itemsets that are the most interesting under a statistical model of\nthe underlying data is a commonly used and well-studied technique for\nexploratory data analysis, with the most recent interestingness models\nexhibiting state of the art performance. Continuing this highly promising line\nof work, we propose the first, to the best of our knowledge, generative model\nover itemsets, in the form of a Bayesian network, and an associated novel\nmeasure of interestingness. Our model is able to efficiently infer interesting\nitemsets directly from the transaction database using structural EM, in which\nthe E-step employs the greedy approximation to weighted set cover. Our approach\nis theoretically simple, straightforward to implement, trivially parallelizable\nand retrieves itemsets whose quality is comparable to, if not better than,\nexisting state of the art algorithms as we demonstrate on several real-world\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2015 14:55:17 GMT"}, {"version": "v2", "created": "Fri, 11 Nov 2016 11:15:30 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Fowkes", "Jaroslav", ""], ["Sutton", "Charles", ""]]}, {"id": "1510.04347", "submitter": "Alan Davoust", "authors": "Alan Davoust and Babak Esfandiari", "title": "Processing Regular Path Queries on Arbitrarily Distributed Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regular Path Queries (RPQs) are a type of graph query where answers are pairs\nof nodes connected by a sequence of edges matching a regular expression. We\nstudy the techniques to process such queries on a distributed graph of data.\nWhile many techniques assume the location of each data element (node or edge)\nis known, when the components of the distributed system are autonomous, the\ndata will be arbitrarily distributed. As the different query processing\nstrategies are equivalently costly in the worst case, we isolate\nquery-dependent cost factors and present a method to choose between strategies,\nusing new query cost estimation techniques. We evaluate our techniques using\nmeaningful queries on biomedical data.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2015 23:31:41 GMT"}], "update_date": "2015-10-16", "authors_parsed": [["Davoust", "Alan", ""], ["Esfandiari", "Babak", ""]]}, {"id": "1510.04501", "submitter": "Alan Freihof Tygel", "authors": "Alan Tygel, S\\\"oren Auer, Jeremy Debattista, Fabrizio Orlandi, Maria\n  Luiza Machado Campos", "title": "Towards Cleaning-up Open Data Portals: A Metadata Reconciliation\n  Approach", "comments": "8 pages,10 Figures - Under Revision for ICSC2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents an approach for metadata reconciliation, curation and\nlinking for Open Governamental Data Portals (ODPs). ODPs have been lately the\nstandard solution for governments willing to put their public data available\nfor the society. Portal managers use several types of metadata to organize the\ndatasets, one of the most important ones being the tags. However, the tagging\nprocess is subject to many problems, such as synonyms, ambiguity or\nincoherence, among others. As our empiric analysis of ODPs shows, these issues\nare currently prevalent in most ODPs and effectively hinders the reuse of Open\nData. In order to address these problems, we develop and implement an approach\nfor tag reconciliation in Open Data Portals, encompassing local actions related\nto individual portals, and global actions for adding a semantic metadata layer\nabove individual portals. The local part aims to enhance the quality of tags in\na single portal, and the global part is meant to interlink ODPs by establishing\nrelations between tags.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 12:29:56 GMT"}], "update_date": "2015-10-16", "authors_parsed": [["Tygel", "Alan", ""], ["Auer", "S\u00f6ren", ""], ["Debattista", "Jeremy", ""], ["Orlandi", "Fabrizio", ""], ["Campos", "Maria Luiza Machado", ""]]}, {"id": "1510.05179", "submitter": "Jeremy Kepner", "authors": "Karia Dibert, Hayden Jansen, Jeremy Kepner", "title": "Algebraic Conditions for Generating Accurate Adjacency Arrays", "comments": "2015 IEEE MIT Undergraduate Research Technology Conference", "journal-ref": null, "doi": "10.1109/URTC.2015.7563745", "report-no": null, "categories": "cs.DB cs.DS math.RA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data processing systems impose multiple views on data as it is processed by\nthe system. These views include spreadsheets, databases, matrices, and graphs.\nAssociative arrays unify and simplify these different approaches into a common\ntwo-dimensional view of data. Graph construction, a fundamental operation in\nthe data processing pipeline, is typically done by multiplying the incidence\narray representations of a graph, $\\mathbf{E}_\\mathrm{in}$ and\n$\\mathbf{E}_\\mathrm{out}$, to produce an adjacency matrix of the graph that can\nbe processed with a variety of machine learning clustering techniques. This\nwork focuses on establishing the mathematical criteria to ensure that the\nmatrix product $\\mathbf{E}_\\mathrm{out}^\\intercal\\mathbf{E}_\\mathrm{in}$ is the\nadjacency array of the graph. It will then be shown that these criteria are\nalso necessary and sufficient for the remaining nonzero product of incidence\narrays, $\\mathbf{E}_\\mathrm{in}^\\intercal\\mathbf{E}_\\mathrm{out}$ to be the\nadjacency matrices of the reversed graph. Algebraic structures that comply with\nthe criteria will be identified and discussed.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2015 23:09:49 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Dibert", "Karia", ""], ["Jansen", "Hayden", ""], ["Kepner", "Jeremy", ""]]}, {"id": "1510.05555", "submitter": "Iovka Boneva", "authors": "Iovka Boneva and Jose E. Labra Gayo and Eric G. Prud'hommeaux and\n  S{\\l}awek Staworko", "title": "Shape Expressions Schemas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Shape Expressions (ShEx), an expressive schema language for RDF\ndesigned to provide a high-level, user friendly syntax with intuitive\nsemantics. ShEx allows to describe the vocabulary and the structure of an RDF\ngraph, and to constrain the allowed values for the properties of a node. It\nincludes an algebraic grouping operator, a choice operator, cardinalitiy\nconstraints for the number of allowed occurrences of a property, and negation.\nWe define the semantics of the language and illustrate it with examples. We\nthen present a validation algorithm that, given a node in an RDF graph and a\nconstraint defined by the ShEx schema, allows to check whether the node\nsatisfies that constraint. The algorithm outputs a proof that contains\ntrivially verifiable associations of nodes and the constraints that they\nsatisfy. The structure can be used for complex post-processing tasks, such as\ntransforming the RDF graph to other graph or tree structures, verifying more\ncomplex constraints, or debugging (w.r.t. the schema). We also show the\ninherent difficulty of error identification of ShEx.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2015 16:04:31 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2015 09:28:54 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Boneva", "Iovka", ""], ["Gayo", "Jose E. Labra", ""], ["Prud'hommeaux", "Eric G.", ""], ["Staworko", "S\u0142awek", ""]]}, {"id": "1510.05911", "submitter": "Tim Weninger PhD", "authors": "Baoxu Shi, Tim Weninger", "title": "Discriminative Predicate Path Mining for Fact Checking in Knowledge\n  Graphs", "comments": "17 pages, 4 Figures. To Appear in Knowledge Based Systems", "journal-ref": null, "doi": "10.1016/j.knosys.2016.04.015", "report-no": null, "categories": "cs.DB cs.AI cs.IR cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traditional fact checking by experts and analysts cannot keep pace with the\nvolume of newly created information. It is important and necessary, therefore,\nto enhance our ability to computationally determine whether some statement of\nfact is true or false. We view this problem as a link-prediction task in a\nknowledge graph, and present a discriminative path-based method for fact\nchecking in knowledge graphs that incorporates connectivity, type information,\nand predicate interactions. Given a statement S of the form (subject,\npredicate, object), for example, (Chicago, capitalOf, Illinois), our approach\nmines discriminative paths that alternatively define the generalized statement\n(U.S. city, predicate, U.S. state) and uses the mined rules to evaluate the\nveracity of statement S. We evaluate our approach by examining thousands of\nclaims related to history, geography, biology, and politics using a public,\nmillion node knowledge graph extracted from Wikipedia and PubMedDB. Not only\ndoes our approach significantly outperform related models, we also find that\nthe discriminative predicate path model is easily interpretable and provides\nsensible reasons for the final determination.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2015 14:31:31 GMT"}, {"version": "v2", "created": "Fri, 15 Apr 2016 14:43:30 GMT"}], "update_date": "2016-04-21", "authors_parsed": [["Shi", "Baoxu", ""], ["Weninger", "Tim", ""]]}, {"id": "1510.06437", "submitter": "Immanuel Trummer Mr.", "authors": "Immanuel Trummer and Christoph Koch", "title": "Multiple Query Optimization on the D-Wave 2X Adiabatic Quantum Computer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The D-Wave adiabatic quantum annealer solves hard combinatorial optimization\nproblems leveraging quantum physics. The newest version features over 1000\nqubits and was released in August 2015. We were given access to such a machine,\ncurrently hosted at NASA Ames Research Center in California, to explore the\npotential for hard optimization problems that arise in the context of\ndatabases.\n  In this paper, we tackle the problem of multiple query optimization (MQO). We\nshow how an MQO problem instance can be transformed into a mathematical formula\nthat complies with the restrictive input format accepted by the quantum\nannealer. This formula is translated into weights on and between qubits such\nthat the configuration minimizing the input formula can be found via a process\ncalled adiabatic quantum annealing. We analyze the asymptotic growth rate of\nthe number of required qubits in the MQO problem dimensions as the number of\nqubits is currently the main factor restricting applicability. We\nexperimentally compare the performance of the quantum annealer against other\nMQO algorithms executed on a traditional computer. While the problem sizes that\ncan be treated are currently limited, we already find a class of problem\ninstances where the quantum annealer is three orders of magnitude faster than\nother approaches.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2015 21:28:46 GMT"}], "update_date": "2015-10-23", "authors_parsed": [["Trummer", "Immanuel", ""], ["Koch", "Christoph", ""]]}, {"id": "1510.06916", "submitter": "Yuze Chi", "authors": "Yuze Chi, Guohao Dai, Yu Wang, Guangyu Sun, Guoliang Li, Huazhong Yang", "title": "NXgraph: An Efficient Graph Processing System on a Single Machine", "comments": null, "journal-ref": null, "doi": "10.1109/ICDE.2016.7498258", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies show that graph processing systems on a single machine can\nachieve competitive performance compared with cluster-based graph processing\nsystems. In this paper, we present NXgraph, an efficient graph processing\nsystem on a single machine. With the abstraction of vertex intervals and edge\nsub-shards, we propose the Destination-Sorted Sub-Shard (DSSS) structure to\nstore a graph. By dividing vertices and edges into intervals and sub-shards,\nNXgraph ensures graph data access locality and enables fine-grained scheduling.\nBy sorting edges within each sub-shard according to their destination vertices,\nNXgraph reduces write conflicts among different threads and achieves a high\ndegree of parallelism. Then, three updating strategies, i.e., Single-Phase\nUpdate (SPU), Double-Phase Update (DPU), and Mixed-Phase Update (MPU), are\nproposed in this paper. NXgraph can adaptively choose the fastest strategy for\ndifferent graph problems according to the graph size and the available memory\nresources to fully utilize the memory space and reduce the amount of data\ntransfer. All these three strategies exploit streamlined disk access pattern.\nExtensive experiments on three real-world graphs and five synthetic graphs show\nthat NXgraph can outperform GraphChi, TurboGraph, VENUS, and GridGraph in\nvarious situations. Moreover, NXgraph, running on a single commodity PC, can\nfinish an iteration of PageRank on the Twitter graph with 1.5 billion edges in\n2.05 seconds; while PowerGraph, a distributed graph processing system, needs\n3.6s to finish the same task.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2015 12:36:10 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Chi", "Yuze", ""], ["Dai", "Guohao", ""], ["Wang", "Yu", ""], ["Sun", "Guangyu", ""], ["Li", "Guoliang", ""], ["Yang", "Huazhong", ""]]}, {"id": "1510.07092", "submitter": "Peter Bailis", "authors": "Joseph E. Gonzalez, Peter Bailis, Michael I. Jordan, Michael J.\n  Franklin, Joseph M. Hellerstein, Ali Ghodsi, Ion Stoica", "title": "Asynchronous Complex Analytics in a Distributed Dataflow Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scalable distributed dataflow systems have recently experienced widespread\nadoption, with commodity dataflow engines such as Hadoop and Spark, and even\ncommodity SQL engines routinely supporting increasingly sophisticated analytics\ntasks (e.g., support vector machines, logistic regression, collaborative\nfiltering). However, these systems' synchronous (often Bulk Synchronous\nParallel) dataflow execution model is at odds with an increasingly important\ntrend in the machine learning community: the use of asynchrony via shared,\nmutable state (i.e., data races) in convex programming tasks, which has---in a\nsingle-node context---delivered noteworthy empirical performance gains and\ninspired new research into asynchronous algorithms. In this work, we attempt to\nbridge this gap by evaluating the use of lightweight, asynchronous state\ntransfer within a commodity dataflow engine. Specifically, we investigate the\nuse of asynchronous sideways information passing (ASIP) that presents\nsingle-stage parallel iterators with a Volcano-like intra-operator iterator\nthat can be used for asynchronous information passing. We port two synchronous\nconvex programming algorithms, stochastic gradient descent and the alternating\ndirection method of multipliers (ADMM), to use ASIPs. We evaluate an\nimplementation of ASIPs within on Apache Spark that exhibits considerable\nspeedups as well as a rich set of performance trade-offs in the use of these\nasynchronous algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2015 00:31:48 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Gonzalez", "Joseph E.", ""], ["Bailis", "Peter", ""], ["Jordan", "Michael I.", ""], ["Franklin", "Michael J.", ""], ["Hellerstein", "Joseph M.", ""], ["Ghodsi", "Ali", ""], ["Stoica", "Ion", ""]]}, {"id": "1510.07104", "submitter": "Qi Fan", "authors": "Qi Fan, Zhengkui Wang, Chee-Yong Chan and Kian-Lee Tan", "title": "Supporting Window Analytics over Large-scale Dynamic Graphs", "comments": "14 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In relational DBMS, window functions have been widely used to facilitate data\nanalytics. Surprisingly, while similar concepts have been employed for graph\nanalytics, there has been no explicit notions of graph window analytic\nfunctions. In this paper, we formally introduce window queries for graph\nanalytics. In such queries, for each vertex, the analysis is performed on a\nwindow of vertices defined based on the graph structure. In particular, we\nidentify two instantiations, namely the k-hop window and the topological\nwindow. We develop two novel indices, Dense Block index (DBIndex) and\nInheritance index (I-Index), to facilitate efficient processing of these two\ntypes of windows respectively. Extensive experiments are conducted over both\nreal and synthetic datasets with hundreds of millions of vertices and edges.\nExperimental results indicate that our proposed index-based query processing\nsolutions achieve four orders of magnitude of query performance gain than the\nnon-index algorithm and are superior over EAGR wrt scalability and efficiency.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2015 04:09:38 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Fan", "Qi", ""], ["Wang", "Zhengkui", ""], ["Chan", "Chee-Yong", ""], ["Tan", "Kian-Lee", ""]]}, {"id": "1510.07714", "submitter": "Rebecca Steorts", "authors": "Peter Sadosky, Anshumali Shrivastava, Megan Price, and Rebecca C.\n  Steorts", "title": "Blocking Methods Applied to Casualty Records from the Syrian Conflict", "comments": "25 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of death counts and associated standard errors is of great\nimportance in armed conflict such as the ongoing violence in Syria, as well as\nhistorical conflicts in Guatemala, Per\\'u, Colombia, Timor Leste, and Kosovo.\nFor example, statistical estimates of death counts were cited as important\nevidence in the trial of General Efra\\'in R\\'ios Montt for acts of genocide in\nGuatemala. Estimation relies on both record linkage and multiple systems\nestimation. A key first step in this process is identifying ways to partition\nthe records such that they are computationally manageable. This step is\nreferred to as blocking and is a major challenge for the Syrian database since\nit is sparse in the number of duplicate records and feature poor in its\nattributes. As a consequence, we propose locality sensitive hashing (LSH)\nmethods to overcome these challenges. We demonstrate the computational\nsuperiority and error rates of these methods by comparing our proposed approach\nwith others in the literature. We conclude with a discussion of many challenges\nof merging LSH with record linkage to achieve an estimate of the number of\nuniquely documented deaths in the Syrian conflict.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2015 22:59:44 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Sadosky", "Peter", ""], ["Shrivastava", "Anshumali", ""], ["Price", "Megan", ""], ["Steorts", "Rebecca C.", ""]]}, {"id": "1510.07749", "submitter": "Lei Gai", "authors": "Lei Gai, Wei Chen, Tengjiao Wang", "title": "A partition-based Summary-Graph-Driven Method for Efficient RDF Query\n  Processing", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RDF query optimization is a challenging problem. Although considerable\nfactors and their impacts on query efficiency have been investigated, this\nproblem still needs further investigation. We identify that decomposing query\ninto a series of light-weight operations is also effective in boosting query\nprocessing. Considering the linked nature of RDF data, the correlations among\noperations should be carefully handled. In this paper, we present SGDQ, a novel\nframework that features a partition-based Summary Graph Driven Query for\nefficient query processing. Basically, SGDQ partitions data and models\npartitions as a summary graph. A query is decomposed into subqueries that can\nbe answered without inter-partition processing. The final results are derived\nby perform summary graph matching and join the results generated by all matched\nsubqueries. In essence, SGDQ combines the merits of graph match processing and\nrelational join-based query implementation. It intentionally avoids maintain\nhuge intermediate results by organizing sub-query processing in a summary graph\ndriven fashion. Our extensive evaluations show that SGDQ is an effective\nframework for efficient RDF query processing. Its query performance\nconsistently outperforms the representative state-of-the-art systems.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2015 02:01:53 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Gai", "Lei", ""], ["Chen", "Wei", ""], ["Wang", "Tengjiao", ""]]}, {"id": "1510.08897", "submitter": "Kyriaki Dimitriadou", "authors": "Kyriaki Dimitriadou and Olga Papaemmanouil and Yanlei Diao", "title": "AIDE: An Automated Sample-based Approach for Interactive Data\n  Exploration", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we argue that database systems be augmented with an automated\ndata exploration service that methodically steers users through the data in a\nmeaningful way. Such an automated system is crucial for deriving insights from\ncomplex datasets found in many big data applications such as scientific and\nhealthcare applications as well as for reducing the human effort of data\nexploration. Towards this end, we present AIDE, an Automatic Interactive Data\nExploration framework that assists users in discovering new interesting data\npatterns and eliminate expensive ad-hoc exploratory queries.\n  AIDE relies on a seamless integration of classification algorithms and data\nmanagement optimization techniques that collectively strive to accurately learn\nthe user interests based on his relevance feedback on strategically collected\nsamples. We present a number of exploration techniques as well as optimizations\nthat minimize the number of samples presented to the user while offering\ninteractive performance. AIDE can deliver highly accurate query predictions for\nvery common conjunctive queries with small user effort while, given a\nreasonable number of samples, it can predict with high accuracy complex\ndisjunctive queries. It provides interactive performance as it limits the user\nwait time per iteration of exploration to less than a few seconds.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2015 20:50:05 GMT"}], "update_date": "2015-11-02", "authors_parsed": [["Dimitriadou", "Kyriaki", ""], ["Papaemmanouil", "Olga", ""], ["Diao", "Yanlei", ""]]}]