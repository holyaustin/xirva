[{"id": "1101.1110", "submitter": "Yael Amsterdamer", "authors": "Yael Amsterdamer, Daniel Deutch, Val Tannen", "title": "Provenance for Aggregate Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study in this paper provenance information for queries with aggregation.\nProvenance information was studied in the context of various query languages\nthat do not allow for aggregation, and recent work has suggested to capture\nprovenance by annotating the different database tuples with elements of a\ncommutative semiring and propagating the annotations through query evaluation.\nWe show that aggregate queries pose novel challenges rendering this approach\ninapplicable. Consequently, we propose a new approach, where we annotate with\nprovenance information not just tuples but also the individual values within\ntuples, using provenance to describe the values computation. We realize this\napproach in a concrete construction, first for \"simple\" queries where the\naggregation operator is the last one applied, and then for arbitrary (positive)\nrelational algebra queries with aggregation; the latter queries are shown to be\nmore challenging in this context. Finally, we use aggregation to encode queries\nwith difference, and study the semantics obtained for such queries on\nprovenance annotated databases.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jan 2011 22:20:29 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Amsterdamer", "Yael", ""], ["Deutch", "Daniel", ""], ["Tannen", "Val", ""]]}, {"id": "1101.2245", "submitter": "Michael Goodrich", "authors": "Michael T. Goodrich and Michael Mitzenmacher", "title": "Invertible Bloom Lookup Tables", "comments": "contains 4 figures, showing experimental performance", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a version of the Bloom filter data structure that supports not\nonly the insertion, deletion, and lookup of key-value pairs, but also allows a\ncomplete listing of its contents with high probability, as long the number of\nkey-value pairs is below a designed threshold. Our structure allows the number\nof key-value pairs to greatly exceed this threshold during normal operation.\nExceeding the threshold simply temporarily prevents content listing and reduces\nthe probability of a successful lookup. If later entries are deleted to return\nthe structure below the threshold, everything again functions appropriately. We\nalso show that simple variations of our structure are robust to certain\nstandard errors, such as the deletion of a key without a corresponding\ninsertion or the insertion of two distinct values for a key. The properties of\nour structure make it suitable for several applications, including database and\nnetworking applications that we highlight.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jan 2011 00:30:10 GMT"}, {"version": "v2", "created": "Tue, 3 May 2011 23:29:38 GMT"}, {"version": "v3", "created": "Sun, 4 Oct 2015 00:10:28 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Goodrich", "Michael T.", ""], ["Mitzenmacher", "Michael", ""]]}, {"id": "1101.2613", "submitter": "Thomas Bernecker", "authors": "Thomas Bernecker, Tobias Emrich, Hans-Peter Kriegel, Nikos Mamoulis,\n  Matthias Renz and Andreas Zuefle", "title": "A Novel Probabilistic Pruning Approach to Speed Up Similarity Queries in\n  Uncertain Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel, effective and efficient probabilistic\npruning criterion for probabilistic similarity queries on uncertain data. Our\napproach supports a general uncertainty model using continuous probabilistic\ndensity functions to describe the (possibly correlated) uncertain attributes of\nobjects. In a nutshell, the problem to be solved is to compute the PDF of the\nrandom variable denoted by the probabilistic domination count: Given an\nuncertain database object B, an uncertain reference object R and a set D of\nuncertain database objects in a multi-dimensional space, the probabilistic\ndomination count denotes the number of uncertain objects in D that are closer\nto R than B. This domination count can be used to answer a wide range of\nprobabilistic similarity queries. Specifically, we propose a novel geometric\npruning filter and introduce an iterative filter-refinement strategy for\nconservatively and progressively estimating the probabilistic domination count\nin an efficient way while keeping correctness according to the possible world\nsemantics. In an experimental evaluation, we show that our proposed technique\nallows to acquire tight probability bounds for the probabilistic domination\ncount quickly, even for large uncertain databases.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jan 2011 17:32:01 GMT"}, {"version": "v2", "created": "Thu, 5 May 2011 15:21:44 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Bernecker", "Thomas", ""], ["Emrich", "Tobias", ""], ["Kriegel", "Hans-Peter", ""], ["Mamoulis", "Nikos", ""], ["Renz", "Matthias", ""], ["Zuefle", "Andreas", ""]]}, {"id": "1101.4270", "submitter": "Rahmat Widia Sembiring", "authors": "Rahmat Widia Sembiring, Jasni Mohamad Zain, Abdullah Embong", "title": "A Comparative Agglomerative Hierarchical Clustering Method to Cluster\n  Implemented Course", "comments": "6 pages, 10 figures, published on Journal of Computing, Volume 2,\n  Issue 12, December 2010", "journal-ref": "Journal of Computing, Volume 2, Issue 12, December 2010", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many clustering methods, such as hierarchical clustering method.\nMost of the approaches to the clustering of variables encountered in the\nliterature are of hierarchical type. The great majority of hierarchical\napproaches to the clustering of variables are of agglomerative nature. The\nagglomerative hierarchical approach to clustering starts with each observation\nas its own cluster and then continually groups the observations into\nincreasingly larger groups. Higher Learning Institution (HLI) provides training\nto introduce final-year students to the real working environment. In this\nresearch will use Euclidean single linkage and complete linkage. MATLAB and HCE\n3.5 software will used to train data and cluster course implemented during\nindustrial training. This study indicates that different method will create a\ndifferent number of clusters.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jan 2011 08:00:33 GMT"}], "update_date": "2011-01-25", "authors_parsed": [["Sembiring", "Rahmat Widia", ""], ["Zain", "Jasni Mohamad", ""], ["Embong", "Abdullah", ""]]}, {"id": "1101.5334", "submitter": "Subbarao Kambhampati", "authors": "Ravi Gummadi, Anupam Khulbe, Aravind Kalavagattu, Sanil Salvi,\n  Subbarao Kambhampati", "title": "SmartInt: Using Mined Attribute Dependencies to Integrate Fragmented Web\n  Databases", "comments": null, "journal-ref": "A shorter version presented as a poster at WWW 2011 (Hyderabad)", "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many web databases can be seen as providing partial and overlapping\ninformation about entities in the world. To answer queries effectively, we need\nto integrate the information about the individual entities that are fragmented\nover multiple sources. At first blush this is just the inverse of traditional\ndatabase normalization problem - rather than go from a universal relation to\nnormalized tables, we want to reconstruct the universal relation given the\ntables (sources). The standard way of reconstructing the entities will involve\njoining the tables. Unfortunately, because of the autonomous and decentralized\nway in which the sources are populated, they often do not have Primary Key -\nForeign Key relations. While tables may share attributes, naive joins over\nthese shared attributes can result in reconstruction of many spurious entities\nthus seriously compromising precision. Our system, \\smartint\\ is aimed at\naddressing the problem of data integration in such scenarios. Given a query,\nour system uses the Approximate Functional Dependencies (AFDs) to piece\ntogether a tree of relevant tables to answer it. The result tuples produced by\nour system are able to strike a favorable balance between precision and recall.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jan 2011 16:43:20 GMT"}, {"version": "v2", "created": "Mon, 31 Jan 2011 20:43:17 GMT"}], "update_date": "2011-02-01", "authors_parsed": [["Gummadi", "Ravi", ""], ["Khulbe", "Anupam", ""], ["Kalavagattu", "Aravind", ""], ["Salvi", "Sanil", ""], ["Kambhampati", "Subbarao", ""]]}, {"id": "1101.5668", "submitter": "Joshila Grace jebin", "authors": "L.K. Joshila Grace, V.Maheswari, Dhinaharan Nagamalai", "title": "Analysis of Web Logs and Web User in Web Mining", "comments": "12 pages,6 figures,CCSIT 2011", "journal-ref": "International Journal of Network Security & Its Applications\n  (IJNSA), Vol.3, No.1, January 2011", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Log files contain information about User Name, IP Address, Time Stamp, Access\nRequest, number of Bytes Transferred, Result Status, URL that Referred and User\nAgent. The log files are maintained by the web servers. By analysing these log\nfiles gives a neat idea about the user. This paper gives a detailed discussion\nabout these log files, their formats, their creation, access procedures, their\nuses, various algorithms used and the additional parameters that can be used in\nthe log files which in turn gives way to an effective mining. It also provides\nthe idea of creating an extended log file and learning the user behaviour.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jan 2011 05:09:20 GMT"}], "update_date": "2011-02-01", "authors_parsed": [["Grace", "L. K. Joshila", ""], ["Maheswari", "V.", ""], ["Nagamalai", "Dhinaharan", ""]]}, {"id": "1101.5805", "submitter": "Matteo Riondato", "authors": "Matteo Riondato, Mert Akdere, Ugur Cetintemel, Stanley B. Zdonik, Eli\n  Upfal", "title": "The VC-Dimension of Queries and Selectivity Estimation Through Sampling", "comments": "20 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel method, based on the statistical concept of the\nVapnik-Chervonenkis dimension, to evaluate the selectivity (output cardinality)\nof SQL queries - a crucial step in optimizing the execution of large scale\ndatabase and data-mining operations. The major theoretical contribution of this\nwork, which is of independent interest, is an explicit bound to the\nVC-dimension of a range space defined by all possible outcomes of a collection\n(class) of queries. We prove that the VC-dimension is a function of the maximum\nnumber of Boolean operations in the selection predicate and of the maximum\nnumber of select and join operations in any individual query in the collection,\nbut it is neither a function of the number of queries in the collection nor of\nthe size (number of tuples) of the database. We leverage on this result and\ndevelop a method that, given a class of queries, builds a concise random sample\nof a database, such that with high probability the execution of any query in\nthe class on the sample provides an accurate estimate for the selectivity of\nthe query on the original large database. The error probability holds\nsimultaneously for the selectivity estimates of all queries in the collection,\nthus the same sample can be used to evaluate the selectivity of multiple\nqueries, and the sample needs to be refreshed only following major changes in\nthe database. The sample representation computed by our method is typically\nsufficiently small to be stored in main memory. We present extensive\nexperimental results, validating our theoretical analysis and demonstrating the\nadvantage of our technique when compared to complex selectivity estimation\ntechniques used in PostgreSQL and the Microsoft SQL Server.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jan 2011 19:13:43 GMT"}, {"version": "v2", "created": "Wed, 2 Feb 2011 15:57:00 GMT"}, {"version": "v3", "created": "Thu, 11 Aug 2011 20:56:03 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Riondato", "Matteo", ""], ["Akdere", "Mert", ""], ["Cetintemel", "Ugur", ""], ["Zdonik", "Stanley B.", ""], ["Upfal", "Eli", ""]]}, {"id": "1101.5938", "submitter": "Vojt\\v{e}ch P\\v{r}ehnal Mgr.", "authors": "Vojtech Prehnal", "title": "Dialog interface for dynamic data models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the new information system development methodology will be\nproposed. This methodology will enable the whole data model to be built and\nadjusted at the run time, without rebuilding the application. This will make\nthe user much more powerful and independent on the manufacturer of the system.\nIt will also cut the price and shorten the development time of the information\nsystems dramatically, because common business logic will not have to be\nimplemented for each individual table and the major part of the user interface\nwill be generated automatically.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jan 2011 12:39:26 GMT"}], "update_date": "2011-02-01", "authors_parsed": [["Prehnal", "Vojtech", ""]]}]