[{"id": "1902.00031", "submitter": "Christopher Baik", "authors": "Christopher Baik, H. V. Jagadish, Yunyao Li", "title": "Bridging the Semantic Gap with SQL Query Logs in Natural Language\n  Interfaces to Databases", "comments": "Accepted to IEEE International Conference on Data Engineering (ICDE)\n  2019", "journal-ref": null, "doi": "10.1109/ICDE.2019.00041", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A critical challenge in constructing a natural language interface to database\n(NLIDB) is bridging the semantic gap between a natural language query (NLQ) and\nthe underlying data. Two specific ways this challenge exhibits itself is\nthrough keyword mapping and join path inference. Keyword mapping is the task of\nmapping individual keywords in the original NLQ to database elements (such as\nrelations, attributes or values). It is challenging due to the ambiguity in\nmapping the user's mental model and diction to the schema definition and\ncontents of the underlying database. Join path inference is the process of\nselecting the relations and join conditions in the FROM clause of the final SQL\nquery, and is difficult because NLIDB users lack the knowledge of the database\nschema or SQL and therefore cannot explicitly specify the intermediate tables\nand joins needed to construct a final SQL query. In this paper, we propose\nleveraging information from the SQL query log of a database to enhance the\nperformance of existing NLIDBs with respect to these challenges. We present a\nsystem Templar that can be used to augment existing NLIDBs. Our extensive\nexperimental evaluation demonstrates the effectiveness of our approach, leading\nup to 138% improvement in top-1 accuracy in existing NLIDBs by leveraging SQL\nquery log information.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 19:00:45 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Baik", "Christopher", ""], ["Jagadish", "H. V.", ""], ["Li", "Yunyao", ""]]}, {"id": "1902.00132", "submitter": "Ryan Marcus", "authors": "Ryan Marcus, Olga Papaemmanouil", "title": "Plan-Structured Deep Neural Network Models for Query Performance\n  Prediction", "comments": null, "journal-ref": null, "doi": "10.14778/3342263.3342646", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query performance prediction, the task of predicting the latency of a query,\nis one of the most challenging problem in database management systems. Existing\napproaches rely on features and performance models engineered by human experts,\nbut often fail to capture the complex interactions between query operators and\ninput relations, and generally do not adapt naturally to workload\ncharacteristics and patterns in query execution plans. In this paper, we argue\nthat deep learning can be applied to the query performance prediction problem,\nand we introduce a novel neural network architecture for the task: a\nplan-structured neural network. Our approach eliminates the need for\nhuman-crafted feature selection and automatically discovers complex performance\nmodels both at the operator and query plan level. Our novel neural network\narchitecture can match the structure of any optimizer-selected query execution\nplan and predict its latency with high accuracy. We also propose a number of\noptimizations that reduce training overhead without sacrificing effectiveness.\nWe evaluated our techniques on various workloads and we demonstrate that our\nplan-structured neural network can outperform the state-of-the-art in query\nperformance prediction.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 23:43:35 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Marcus", "Ryan", ""], ["Papaemmanouil", "Olga", ""]]}, {"id": "1902.00585", "submitter": "Ahmet Kara", "authors": "Iman Elghandour and Ahmet Kara and Dan Olteanu and Stijn Vansummeren", "title": "Incremental Techniques for Large-Scale Dynamic Query Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications from various disciplines are now required to analyze fast\nevolving big data in real time. Various approaches for incremental processing\nof queries have been proposed over the years. Traditional approaches rely on\nupdating the results of a query when updates are streamed rather than\nre-computing these queries, and therefore, higher execution performance is\nexpected. However, they do not perform well for large databases that are\nupdated at high frequencies. Therefore, new algorithms and approaches have been\nproposed in the literature to address these challenges by, for instance,\nreducing the complexity of processing updates. Moreover, many of these\nalgorithms are now leveraging distributed streaming platforms such as Spark\nStreaming and Flink. In this tutorial, we briefly discuss legacy approaches for\nincremental query processing, and then give an overview of the new challenges\nintroduced due to processing big data streams. We then discuss in detail the\nrecently proposed algorithms that address some of these challenges. We\nemphasize the characteristics and algorithmic analysis of various proposed\napproaches and conclude by discussing future research directions.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 23:10:04 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Elghandour", "Iman", ""], ["Kara", "Ahmet", ""], ["Olteanu", "Dan", ""], ["Vansummeren", "Stijn", ""]]}, {"id": "1902.00609", "submitter": "Ningnan Zhou", "authors": "Ningnan Zhou, Xuan Zhou, Kian-lee Tan, Shan Wang", "title": "Transparent Concurrency Control: Decoupling Concurrency Control from\n  DBMS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For performance reasons, conventional DBMSes adopt monolithic architectures.\nA monolithic design cripples the adaptability of a DBMS, making it difficult to\ncustomize, to meet particular requirements of different applications. In this\npaper, we propose to completely separate the code of concurrency control (CC)\nfrom a monolithic DBMS. This allows us to add / remove functionalities or data\nstructures to / from a DBMS easily, without concerning the issues of data\nconsistency. As the separation deprives the concurrency controller of the\nknowledge about data organization and processing, it may incur severe\nperformance issues. To minimize the performance loss, we devised a two-level CC\nmechanism. At the operational level, we propose a robust scheduler that\nguarantees to complete any data operation at a manageable cost. At the\ntransactional level, the scheduler can utilize data semantics to achieve\nenhanced performance. Extensive experiments were conducted to demonstrate the\nfeasibility and effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2019 01:18:24 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Zhou", "Ningnan", ""], ["Zhou", "Xuan", ""], ["Tan", "Kian-lee", ""], ["Wang", "Shan", ""]]}, {"id": "1902.00624", "submitter": "Wahyudi Wahyudi", "authors": "Wahyudi, Masayu Leylia Khodra, Ary Setijadi Prihatmanto, Carmadi\n  Machbub", "title": "A Question Answering System Using Graph-Pattern Association Rules\n  (QAGPAR) On YAGO Knowledge Base", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A question answering system (QA System) was developed that uses graph-pattern\nassociation rules on the YAGO knowledge base. The answer as output of the\nsystem is provided based on a user question as input. If the answer is missing\nor unavailable in the database, then graph-pattern association rules are used\nto get the answer. The architecture of this question answering system is as\nfollows: question classification, graph component generation, query generation,\nand query processing. The question answering system uses association graph\npatterns in a waterfall model. In this paper, the architecture of the system is\ndescribed, specifically discussing its reasoning and performance capabilities.\nThe results of this research is that rules with high confidence and correct\nlogic produce correct answers, and vice versa\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2019 02:24:08 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Wahyudi", "", ""], ["Khodra", "Masayu Leylia", ""], ["Prihatmanto", "Ary Setijadi", ""], ["Machbub", "Carmadi", ""]]}, {"id": "1902.00846", "submitter": "Jeremy Kepner", "authors": "Jeremy Kepner, Vijay Gadepally, Lauren Milechin, Siddharth Samsi,\n  William Arcand, David Bestor, William Bergeron, Chansup Byun, Matthew\n  Hubbell, Micheal Houle, Micheal Jones, Anne Klein, Peter Michaleas, Julie\n  Mullen, Andrew Prout, Antonio Rosa, Charles Yee, Albert Reuther", "title": "A Billion Updates per Second Using 30,000 Hierarchical In-Memory D4M\n  Databases", "comments": "Northeast Database Data 2019 (MIT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing large scale networks requires high performance streaming updates of\ngraph representations of these data. Associative arrays are mathematical\nobjects combining properties of spreadsheets, databases, matrices, and graphs,\nand are well-suited for representing and analyzing streaming network data. The\nDynamic Distributed Dimensional Data Model (D4M) library implements associative\narrays in a variety of languages (Python, Julia, and Matlab/Octave) and\nprovides a lightweight in-memory database. Associative arrays are designed for\nblock updates. Streaming updates to a large associative array requires a\nhierarchical implementation to optimize the performance of the memory\nhierarchy. Running 34,000 instances of a hierarchical D4M associative arrays on\n1,100 server nodes on the MIT SuperCloud achieved a sustained update rate of\n1,900,000,000 updates per second. This capability allows the MIT SuperCloud to\nanalyze extremely large streaming network data sets.\n", "versions": [{"version": "v1", "created": "Sun, 3 Feb 2019 04:58:07 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Kepner", "Jeremy", ""], ["Gadepally", "Vijay", ""], ["Milechin", "Lauren", ""], ["Samsi", "Siddharth", ""], ["Arcand", "William", ""], ["Bestor", "David", ""], ["Bergeron", "William", ""], ["Byun", "Chansup", ""], ["Hubbell", "Matthew", ""], ["Houle", "Micheal", ""], ["Jones", "Micheal", ""], ["Klein", "Anne", ""], ["Michaleas", "Peter", ""], ["Mullen", "Julie", ""], ["Prout", "Andrew", ""], ["Rosa", "Antonio", ""], ["Yee", "Charles", ""], ["Reuther", "Albert", ""]]}, {"id": "1902.00952", "submitter": "Leshang Chen", "authors": "Leshang Chen, Susan Davidson", "title": "Automating Software Citation using GitCite", "comments": null, "journal-ref": "2020 IEEE 36th International Conference on Data Engineering\n  (ICDE), Dallas, TX, USA, 2020, pp. 1754-1757", "doi": "10.1109/ICDE48307.2020.00162", "report-no": null, "categories": "cs.DB cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to cite software and give credit to its authors and contributors\nis increasingly important. While the number of online open-source software\nrepositories has grown rapidly over the past few years, few are being properly\ncited when used due to the difficulty of creating appropriate citations and the\nlack of automated techniques. This paper presents GitCite, a model for software\ncitation with version control which enables citations to be inferred for any\nproject component based on a small number of explicit citations attached to\nsubdirectories/files, and an implementation that integrates with Git and\nGitHub. The implementation includes a browser extension and a local executable\ntool, which enable citations to be added/modified/deleted to software project\nrepositories and managed through functions such as fork/merge/copy.\n", "versions": [{"version": "v1", "created": "Sun, 3 Feb 2019 18:43:25 GMT"}, {"version": "v2", "created": "Fri, 21 Jun 2019 17:14:03 GMT"}, {"version": "v3", "created": "Thu, 1 Aug 2019 22:25:22 GMT"}, {"version": "v4", "created": "Tue, 14 Apr 2020 23:59:19 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Chen", "Leshang", ""], ["Davidson", "Susan", ""]]}, {"id": "1902.01304", "submitter": "Nantia Makrynioti", "authors": "Nantia Makrynioti (1), Vasilis Vassalos (1) ((1) Athens University of\n  Economics and Business)", "title": "Declarative Data Analytics: a Survey", "comments": "36 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The area of declarative data analytics explores the application of the\ndeclarative paradigm on data science and machine learning. It proposes\ndeclarative languages for expressing data analysis tasks and develops systems\nwhich optimize programs written in those languages. The execution engine can be\neither centralized or distributed, as the declarative paradigm advocates\nindependence from particular physical implementations. The survey explores a\nwide range of declarative data analysis frameworks by examining both the\nprogramming model and the optimization techniques used, in order to provide\nconclusions on the current state of the art in the area and identify open\nchallenges.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 16:52:40 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Makrynioti", "Nantia", ""], ["Vassalos", "Vasilis", ""]]}, {"id": "1902.01372", "submitter": "Amrita Mazumdar", "authors": "Amrita Mazumdar, Brandon Haynes, Magdalena Balazinska, Luis Ceze,\n  Alvin Cheung, Mark Oskin", "title": "Vignette: Perceptual Compression for Video Storage and Processing\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed videos constitute 70% of Internet traffic, and video upload growth\nrates far outpace compute and storage improvement trends. Past work in\nleveraging perceptual cues like saliency, i.e., regions where viewers focus\ntheir perceptual attention, reduces compressed video size while maintaining\nperceptual quality, but requires significant changes to video codecs and\nignores the data management of this perceptual information.\n  In this paper, we propose Vignette, a compression technique and storage\nmanager for perception-based video compression. Vignette complements\noff-the-shelf compression software and hardware codec implementations.\nVignette's compression technique uses a neural network to predict saliency\ninformation used during transcoding, and its storage manager integrates\nperceptual information into the video storage system to support a perceptual\ncompression feedback loop. Vignette's saliency-based optimizations reduce\nstorage by up to 95% with minimal quality loss, and Vignette videos lead to\npower savings of 50% on mobile phones during video playback. Our results\ndemonstrate the benefit of embedding information about the human visual system\ninto the architecture of video storage systems.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 18:39:44 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Mazumdar", "Amrita", ""], ["Haynes", "Brandon", ""], ["Balazinska", "Magdalena", ""], ["Ceze", "Luis", ""], ["Cheung", "Alvin", ""], ["Oskin", "Mark", ""]]}, {"id": "1902.01457", "submitter": "Mohammad Javad Amiri", "authors": "Mohammad Javad Amiri, Divyakant Agrawal, Amr El Abbadi", "title": "ParBlockchain: Leveraging Transaction Parallelism in Permissioned\n  Blockchain Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many existing blockchains do not adequately address all the characteristics\nof distributed system applications and suffer from serious architectural\nlimitations resulting in performance and confidentiality issues. While recent\npermissioned blockchain systems, have tried to overcome these limitations,\ntheir focus has mainly been on workloads with no-contention, i.e., no\nconflicting transactions. In this paper, we introduce OXII, a new paradigm for\npermissioned blockchains to support distributed applications that execute\nconcurrently. OXII is designed for workloads with (different degrees of)\ncontention. We then present ParBlockchain, a permissioned blockchain designed\nspecifically in the OXII paradigm. The evaluation of ParBlockchain using a\nseries of benchmarks reveals that its performance in workloads with any degree\nof contention is better than the state of the art permissioned blockchain\nsystems.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 20:56:04 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Amiri", "Mohammad Javad", ""], ["Agrawal", "Divyakant", ""], ["Abbadi", "Amr El", ""]]}, {"id": "1902.01740", "submitter": "Besnik Fetahu", "authors": "Besnik Fetahu, Avishek Anand, Maria Koutraki", "title": "TableNet: An Approach for Determining Fine-grained Relations for\n  Wikipedia Tables", "comments": "This is a full version of the short paper accepted at WWW 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wikipedia tables represent an important resource, where information is\norganized w.r.t table schemas consisting of columns. In turn each column, may\ncontain instance values that point to other Wikipedia articles or primitive\nvalues (e.g. numbers, strings etc.).\n  In this work, we focus on the problem of interlinking Wikipedia tables for\ntwo types of table relations: equivalent and subPartOf. Through such relations,\nwe can further harness semantically related information by accessing related\ntables or facts therein. Determining the relation type of a table pair is not\ntrivial, as it is dependent on the schemas, the values therein, and the\nsemantic overlap of the cell values in the corresponding tables.\n  We propose TableNet, an approach that constructs a knowledge graph of\ninterlinked tables with subPartOf and equivalent relations. TableNet consists\nof two main steps: (i) for any source table we provide an efficient algorithm\nto find all candidate related tables with high coverage, and (ii) a neural\nbased approach, which takes into account the table schemas, and the\ncorresponding table data, we determine with high accuracy the table relation\nfor a table pair.\n  We perform an extensive experimental evaluation on the entire Wikipedia with\nmore than 3.2 million tables. We show that with more than 88\\% we retain\nrelevant candidate tables pairs for alignment. Consequentially, with an\naccuracy of 90% we are able to align tables with subPartOf or equivalent\nrelations. Comparisons with existing competitors show that TableNet has\nsuperior performance in terms of coverage and alignment accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 15:25:20 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Fetahu", "Besnik", ""], ["Anand", "Avishek", ""], ["Koutraki", "Maria", ""]]}, {"id": "1902.02013", "submitter": "EPTCS", "authors": "J\\'anos Varga", "title": "Finding the Transitive Closure of Functional Dependencies using\n  Strategic Port Graph Rewriting", "comments": "In Proceedings TERMGRAPH 2018, arXiv:1902.01510", "journal-ref": "EPTCS 288, 2019, pp. 50-62", "doi": "10.4204/EPTCS.288.5", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach to the logical design of relational databases,\nbased on strategic port graph rewriting. We show how to model relational\nschemata as attributed port graphs and provide port graph rewriting rules to\nperform computations on functional dependencies. Using these rules we present a\nstrategic graph program to find the transitive closure of a set of functional\ndependencies. This program is sound, complete and terminating, assuming that\nthere are no cyclical dependencies in the schema.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 03:23:51 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Varga", "J\u00e1nos", ""]]}, {"id": "1902.02140", "submitter": "Ettore Rizza", "authors": "Ettore Rizza, Anne Chardonnens, Seth van Hooland", "title": "Close-reading of Linked Data: a case study in regards to the quality of\n  online authority files", "comments": "Workshop \"Dariah \"Trust and Understanding: the value of metadata in a\n  digitally joined-up world\" (14/05/2018, Brussels), preprint of the submission\n  to the journal \"Archives et Biblioth\\`eques de Belgique\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  More and more cultural institutions use Linked Data principles to share and\nconnect their collection metadata. In the archival field, initiatives emerge to\nexploit data contained in archival descriptions and adapt encoding standards to\nthe semantic web. In this context, online authority files can be used to enrich\nmetadata. However, relying on a decentralized network of knowledge bases such\nas Wikidata, DBpedia or even Viaf has its own difficulties. This paper aims to\noffer a critical view of these linked authority files by adopting a\nclose-reading approach. Through a practical case study, we intend to identify\nand illustrate the possibilities and limits of RDF triples compared to\ninstitutions' less structured metadata.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 12:33:07 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Rizza", "Ettore", ""], ["Chardonnens", "Anne", ""], ["van Hooland", "Seth", ""]]}, {"id": "1902.02392", "submitter": "Nikolaj Tatti", "authors": "Nikolaj Tatti, Jilles Vreeken", "title": "Finding Good Itemsets by Packing Data", "comments": null, "journal-ref": null, "doi": "10.1109/ICDM.2008.39", "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of selecting small groups of itemsets that represent the data\nwell has recently gained a lot of attention. We approach the problem by\nsearching for the itemsets that compress the data efficiently. As a compression\ntechnique we use decision trees combined with a refined version of MDL. More\nformally, assuming that the items are ordered, we create a decision tree for\neach item that may only depend on the previous items. Our approach allows us to\nfind complex interactions between the attributes, not just co-occurrences of\n1s. Further, we present a link between the itemsets and the decision trees and\nuse this link to export the itemsets from the decision trees. In this paper we\npresent two algorithms. The first one is a simple greedy approach that builds a\nfamily of itemsets directly from data. The second one, given a collection of\ncandidate itemsets, selects a small subset of these itemsets. Our experiments\nshow that these approaches result in compact and high quality descriptions of\nthe data.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 20:38:24 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Tatti", "Nikolaj", ""], ["Vreeken", "Jilles", ""]]}, {"id": "1902.02470", "submitter": "Jincheng Du", "authors": "Jincheng Du, Dan Fang, Mark Harvilla", "title": "PAI Data, Summary of the Project PAI Data Protocol", "comments": "8 pages, 3 figures; A Japanese translation is available in previous\n  version V2 [arXiv:1902.02470v2 ]; A Korean translation is available in\n  previous version V3 [arXiv:1902.02470v3 ]; A Simplified Chinese translation\n  is available in previous version V4 [arXiv:1902.02470v4];", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Project PAI Data Protocol (\"PAI Data\") is a specification that extends\nthe Project PAI Blockchain Protocol to include a method of securing and\nprovisioning access to arbitrary data. In the context of PAI Coin Development\nProposal (PDP) 2, this paper defines two important transaction types that PAI\nData supports: Storage Transactions, which facilitate storage of data and proof\nof ownership, and Sharing Transactions, designed to enable granting and\nrevocation of data access to designated recipients. A comparative analysis of\nPAI Data against similar blockchain-based file storage systems is also\npresented.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 04:26:17 GMT"}, {"version": "v2", "created": "Sun, 24 Feb 2019 07:21:47 GMT"}, {"version": "v3", "created": "Tue, 26 Feb 2019 17:37:23 GMT"}, {"version": "v4", "created": "Wed, 27 Feb 2019 04:09:32 GMT"}, {"version": "v5", "created": "Thu, 28 Feb 2019 01:54:59 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Du", "Jincheng", ""], ["Fang", "Dan", ""], ["Harvilla", "Mark", ""]]}, {"id": "1902.02698", "submitter": "Shaleen Deep", "authors": "Shaleen Deep, Paraschos Koutris", "title": "Ranked Enumeration of Conjunctive Query Results", "comments": "To appear in ICDT 2021. Comments/Suggestions are welcome!", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the enumeration of top-k answers for conjunctive queries\nagainst relational databases according to a given ranking function. The task is\nto design data structures and algorithms that allow for efficient enumeration\nafter a preprocessing phase. Our main contribution is a novel priority queue\nbased algorithm with near-optimal delay and non-trivial space guarantees that\nare output sensitive and depend on structure of the query. In particular, we\nexploit certain desirable properties of ranking functions that frequently occur\nin practice and degree information in the database instance, allowing for\nefficient enumeration. We introduce the notion of {\\em decomposable} and {\\em\ncompatible} ranking functions in conjunction with query decomposition, a\nproperty that allows for partial aggregation of tuple scores in order to\nefficiently enumerate the ranked output. We complement the algorithmic results\nwith lower bounds justifying why certain assumptions about properties of\nranking functions are necessary and discuss popular conjectures providing\nevidence for optimality of enumeration delay guarantees. Our results extend and\nimprove upon a long line of work that has studied ranked enumeration from both\ntheoretical and practical perspective.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 15:46:35 GMT"}, {"version": "v2", "created": "Sun, 10 Jan 2021 23:04:36 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Deep", "Shaleen", ""], ["Koutris", "Paraschos", ""]]}, {"id": "1902.03296", "submitter": "Konstantinos Blazakis", "authors": "Konstantinos Blazakis and Georgios Stavrakakis", "title": "Efficient Power Theft Detection for Residential Consumers Using Mean\n  Shift Data Mining Knowledge Discovery Process", "comments": "17 pages, 6 figures", "journal-ref": null, "doi": "10.5121/ijaia.2019.10106", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy theft constitutes an issue of great importance for electricity\noperators. The attempt to detect and reduce non-technical losses is a\nchallenging task due to insufficient inspection methods. With the evolution of\nadvanced metering infrastructure (AMI) in smart grids, a more complicated\nstatus quo in energy theft has emerged and many new technologies are being\nadopted to solve the problem. In order to identify illegal residential\nconsumers, a computational method of analyzing and identifying electricity\nconsumption patterns of consumers based on data mining techniques has been\npresented. Combining principal component analysis (PCA) with mean shift\nalgorithm for different power theft scenarios, we can now cope with the power\ntheft detection problem sufficiently. The overall research has shown\nencouraging results in residential consumers power theft detection that will\nhelp utilities to improve the reliability, security and operation of power\nnetwork.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 21:45:45 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Blazakis", "Konstantinos", ""], ["Stavrakakis", "Georgios", ""]]}, {"id": "1902.03297", "submitter": "Nikolaj Tatti", "authors": "Nikolaj Tatti", "title": "Probably the Best Itemsets", "comments": null, "journal-ref": null, "doi": "10.1145/1835804.1835843", "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main current challenges in itemset mining is to discover a small\nset of high-quality itemsets. In this paper we propose a new and general\napproach for measuring the quality of itemsets. The method is solidly founded\nin Bayesian statistics and decreases monotonically, allowing for efficient\ndiscovery of all interesting itemsets. The measure is defined by connecting\nstatistical models and collections of itemsets. This allows us to score\nindividual itemsets with the probability of them occuring in random models\nbuilt on the data.\n  As a concrete example of this framework we use exponential models. This class\nof models possesses many desirable properties. Most importantly, Occam's razor\nin Bayesian model selection provides a defence for the pattern explosion. As\ngeneral exponential models are infeasible in practice, we use decomposable\nmodels; a large sub-class for which the measure is solvable. For the actual\ncomputation of the score we sample models from the posterior distribution using\nan MCMC approach.\n  Experimentation on our method demonstrates the measure works in practice and\nresults in interpretable and insightful itemsets for both synthetic and\nreal-world data.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 18:51:59 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Tatti", "Nikolaj", ""]]}, {"id": "1902.03338", "submitter": "Shiva Shivakumar", "authors": "Catalin Popescu, Deepak Merugu, Giao Nguyen, Shiva Shivakumar", "title": "WarpFlow: Exploring Petabytes of Space-Time Data", "comments": "11 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  WarpFlow is a fast, interactive data querying and processing system with a\nfocus on petabyte-scale spatiotemporal datasets and Tesseract queries. With the\nrapid growth in smartphones and mobile navigation services, we now have an\nopportunity to radically improve urban mobility and reduce friction in how\npeople and packages move globally every minute-mile, with data. WarpFlow speeds\nup three key metrics for data engineers working on such datasets --\ntime-to-first-result, time-to-full-scale-result, and time-to-trained-model for\nmachine learning.\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2019 00:23:56 GMT"}, {"version": "v2", "created": "Tue, 12 Feb 2019 19:07:20 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Popescu", "Catalin", ""], ["Merugu", "Deepak", ""], ["Nguyen", "Giao", ""], ["Shivakumar", "Shiva", ""]]}, {"id": "1902.03522", "submitter": "Dmitrii Avdiukhin", "authors": "Dmitrii Avdiukhin, Sergey Pupyrev, Grigory Yaroslavtsev", "title": "Multi-Dimensional Balanced Graph Partitioning via Projected Gradient\n  Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by performance optimization of large-scale graph processing systems\nthat distribute the graph across multiple machines, we consider the balanced\ngraph partitioning problem. Compared to the previous work, we study the\nmulti-dimensional variant when balance according to multiple weight functions\nis required. As we demonstrate by experimental evaluation, such\nmulti-dimensional balance is important for achieving performance improvements\nfor typical distributed graph processing workloads. We propose a new scalable\ntechnique for the multidimensional balanced graph partitioning problem. The\nmethod is based on applying randomized projected gradient descent to a\nnon-convex continuous relaxation of the objective. We show how to implement the\nnew algorithm efficiently in both theory and practice utilizing various\napproaches for projection. Experiments with large-scale social networks\ncontaining up to hundreds of billions of edges indicate that our algorithm has\nsuperior performance compared with the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sun, 10 Feb 2019 00:23:16 GMT"}, {"version": "v2", "created": "Sat, 16 Feb 2019 00:25:12 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Avdiukhin", "Dmitrii", ""], ["Pupyrev", "Sergey", ""], ["Yaroslavtsev", "Grigory", ""]]}, {"id": "1902.03533", "submitter": "Wenxi Zeng", "authors": "Shuai Zhang, Wenxi Zeng, I-Ling Yen, Farokh B. Bastani", "title": "Semantically Enhanced Time Series Databases in IoT-Edge-Cloud\n  Infrastructure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many IoT systems are data intensive and are for the purpose of monitoring for\nfault detection and diagnosis of critical systems. A large volume of data\nsteadily come out of a large number of sensors in the monitoring system. Thus,\nwe need to consider how to store and manage these data. Existing time series\ndatabases (TSDBs) can be used for monitoring data storage, but they do not have\ngood models for describing the data streams stored in the database. In this\npaper, we develop a semantic model for the specification of the monitoring data\nstreams (time series data) in terms of which sensor generated the data stream,\nwhich metric of which entity the sensor is monitoring, what is the relation of\nthe entity to other entities in the system, which measurement unit is used for\nthe data stream, etc. We have also developed a tool suite, SE-TSDB, that can\nrun on top of existing TSDBs to help establish semantic specifications for data\nstreams and enable semantic-based data retrievals. With our semantic model for\nmonitoring data and our SE-TSDB tool suite, users can retrieve non-existing\ndata streams that can be automatically derived from the semantics. Users can\nalso retrieve data streams without knowing where they are. Semantic based\nretrieval is especially important in a large-scale integrated IoT-Edge-Cloud\nsystem, because of its sheer quantity of data, its huge number of computing and\nIoT devices that may store the data, and the dynamics in data migration and\nevolution. With better data semantics, data streams can be more effectively\ntracked and flexibly retrieved to help with timely data analysis and control\ndecision making anywhere and anytime.\n", "versions": [{"version": "v1", "created": "Sun, 10 Feb 2019 04:09:08 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Zhang", "Shuai", ""], ["Zeng", "Wenxi", ""], ["Yen", "I-Ling", ""], ["Bastani", "Farokh B.", ""]]}, {"id": "1902.03576", "submitter": "Nuno Pregui\\c{c}a", "authors": "Pedro Lopes, Jo\\~ao Sousa, Valter Balegas, Carla Ferreira, S\\'egio\n  Duarte, Annette Bieniusa, Rodrigo Rodrigues, Nuno Pregui\\c{c}a", "title": "Antidote SQL: Relaxed When Possible, Strict When Necessary", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geo-replication poses an inherent trade-off between low latency, high\navailability and strong consistency. While NoSQL databases favor low latency\nand high availability, relaxing consistency, more recent cloud databases favor\nstrong consistency and ease of programming, while still providing high\nscalability. In this paper, we present Antidote SQL, a database system that\nallows application developers to relax SQL consistency when possible. Unlike\nNoSQL databases, our approach enforces primary key, foreign key and check SQL\nconstraints even under relaxed consistency, which is sufficient for\nguaranteeing the correctness of many applications. To this end, we defined\nconcurrency semantics for SQL constraints under relaxed consistency and show\nhow to implement such semantics efficiently. For applications that require\nstrict SQL consistency, Antidote SQL provides support for such semantics at the\ncost of requiring coordination among replicas.\n", "versions": [{"version": "v1", "created": "Sun, 10 Feb 2019 11:25:32 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Lopes", "Pedro", ""], ["Sousa", "Jo\u00e3o", ""], ["Balegas", "Valter", ""], ["Ferreira", "Carla", ""], ["Duarte", "S\u00e9gio", ""], ["Bieniusa", "Annette", ""], ["Rodrigues", "Rodrigo", ""], ["Pregui\u00e7a", "Nuno", ""]]}, {"id": "1902.03948", "submitter": "Rebecca Wild", "authors": "Rebecca Wild, Matthew Hubbell, Jeremy Kepner", "title": "Scaling Big Data Platform for Big Data Pipeline", "comments": "Accepted to MIT Northeast Database Day 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monitoring and Managing High Performance Computing (HPC) systems and\nenvironments generate an ever growing amount of data. Making sense of this data\nand generating a platform where the data can be visualized for system\nadministrators and management to proactively identify system failures or\nunderstand the state of the system requires the platform to be as efficient and\nscalable as the underlying database tools used to store and analyze the data.\nIn this paper we will show how we leverage Accumulo, d4m, and Unity to generate\na 3D visualization platform to monitor and manage the Lincoln Laboratory\nSupercomputer systems and how we have had to retool our approach to scale with\nour systems.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 15:46:54 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Wild", "Rebecca", ""], ["Hubbell", "Matthew", ""], ["Kepner", "Jeremy", ""]]}, {"id": "1902.03975", "submitter": "Olivia Choudhury", "authors": "Olivia Choudhury, Noor Fairoza, Issa Sylla, Amar Das", "title": "A Blockchain Framework for Managing and Monitoring Data in Multi-Site\n  Clinical Trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cost of conducting multi-site clinical trials has significantly increased\nover time, with site monitoring, data management, and amendments being key\ndrivers. Clinical trial data management approaches typically rely on a central\ndatabase, and require manual efforts to encode and maintain data capture and\nreporting requirements. To reduce the administrative burden, time, and effort\nof ensuring data integrity and privacy in multi-site trials, we propose a novel\ndata management framework based on permissioned blockchain technology. We\ndemonstrate how our framework, which uses smart contracts and private channels,\nenables confidential data communication, protocol enforcement, and and an\nautomated audit trail. We compare this framework with the traditional data\nmanagement approach and evaluate its effectiveness in satisfying the major\nrequirements of multi-site clinical trials. We show that our framework ensures\nenforcement of IRB-related regulatory requirements across multiple sites and\nstakeholders.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 16:34:44 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Choudhury", "Olivia", ""], ["Fairoza", "Noor", ""], ["Sylla", "Issa", ""], ["Das", "Amar", ""]]}, {"id": "1902.04129", "submitter": "Yannis Tzitzikas", "authors": "Maria Psaraki and Yannis Tzitzikas", "title": "CPOI: A Compact Method to Archive Versioned RDF Triple-Sets", "comments": "29 pages (This version of the paper was written in 2013. It is made\n  public for being accessible to everyone.)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large amounts of RDF/S data are produced and published lately, and several\nmodern applications require the provision of versioning and archiving services\nover such datasets. In this paper we propose a novel storage index for\narchiving versions of such datasets, called CPOI (compact partial order index),\nthat exploits the fact that an RDF Knowledge Base (KB), is a graph (or\nequivalently a set of triples), and thus it has not a unique serialization (as\nit happens with text). If we want to keep stored several versions we actually\nwant to store multiple sets of triples. CPOI is a data structure for storing\nsuch sets aiming at reducing the storage space since this is important not only\nfor reducing storage costs, but also for reducing the various communication\ncosts and enabling hosting in main memory (and thus processing efficiently)\nlarge quantities of data. CPOI is based on a partial order structure over sets\nof triple identifiers, where the triple identifiers are represented in a gapped\nform using variable length encoding schemes. For this index we evaluate\nanalytically and experimentally various identifier assignment techniques and\ntheir space savings. The results show significant storage savings,\nspecifically, the storage space of the compressed sets in large and realistic\nsynthetic datasets is about the 8% of the size of the uncompressed sets.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 20:24:09 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Psaraki", "Maria", ""], ["Tzitzikas", "Yannis", ""]]}, {"id": "1902.04379", "submitter": "Martin Theobald", "authors": "Katerina Papaioannou, Martin Theobald, Michael B\\\"ohlen", "title": "Generalized Lineage-Aware Temporal Windows: Supporting Outer and Anti\n  Joins in Temporal-Probabilistic Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The result of a temporal-probabilistic (TP) join with negation includes, at\neach time point, the probability with which a tuple of a positive relation\n${\\bf p}$ matches none of the tuples in a negative relation ${\\bf n}$, for a\ngiven join condition $\\theta$. TP outer and anti joins thus resemble the\ncharacteristics of relational outer and anti joins also in the case when there\nexist time points at which input tuples from ${\\bf p}$ have non-zero\nprobabilities to be $true$ and input tuples from ${\\bf n}$ have non-zero\nprobabilities to be $false$, respectively. For the computation of TP joins with\nnegation, we introduce generalized lineage-aware temporal windows, a mechanism\nthat binds an output interval to the lineages of all the matching valid tuples\nof each input relation. We group the windows of two TP relations into three\ndisjoint sets based on the way attributes, lineage expressions and intervals\nare produced. We compute all windows in an incremental manner, and we show that\npipelined computations allow for the direct integration of our approach into\nPostgreSQL. We thereby alleviate the prevalent redundancies in the interval\ncomputations of existing approaches, which is proven by an extensive\nexperimental evaluation with real-world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 13:38:26 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Papaioannou", "Katerina", ""], ["Theobald", "Martin", ""], ["B\u00f6hlen", "Michael", ""]]}, {"id": "1902.04790", "submitter": "Thomas Minier", "authors": "Thomas Minier and Hala Skaf-Molli and Pascal Molli", "title": "SaGe: Web Preemption for Public SPARQL Query Services", "comments": "11 pages, to be published in Proceedings of the 2019 World Wide Web\n  Conference (WWW'19)", "journal-ref": "Proceedings of the 2019 World Wide Web Conference (WWW'19)", "doi": "10.1145/3308558.3313652", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To provide stable and responsive public SPARQL query services, data providers\nenforce quotas on server usage. Queries which exceed these quotas are\ninterrupted and deliver partial results. Such interruption is not an issue if\nit is possible to resume queries execution afterward. Unfortunately, there is\nno preemption model for the Web that allows for suspending and resuming SPARQL\nqueries. In this paper, we propose SaGe: a SPARQL query engine based on Web\npreemption. SaGe allows SPARQL queries to be suspended by the Web server after\na fixed time quantum and resumed upon client request. Web preemption is\ntractable only if its cost in time is negligible compared to the time quantum.\nThe challenge is to support the full SPARQL query language while keeping the\ncost of preemption negligible. Experimental results demonstrate that SaGe\noutperforms existing SPARQL query processing approaches by several orders of\nmagnitude in term of the average total query execution time and the time for\nfirst results.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 08:53:59 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Minier", "Thomas", ""], ["Skaf-Molli", "Hala", ""], ["Molli", "Pascal", ""]]}, {"id": "1902.04938", "submitter": "Boris Glavic", "authors": "Anton Dign\\\"os, Boris Glavic, Xing Niu, Michael B\\\"ohlen, Johann\n  Gamper", "title": "Snapshot Semantics for Temporal Multiset Relations (Extended Version)", "comments": "extended version of PVLDB paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Snapshot semantics is widely used for evaluating queries over temporal data:\ntemporal relations are seen as sequences of snapshot relations, and queries are\nevaluated at each snapshot. In this work, we demonstrate that current\napproaches for snapshot semantics over interval-timestamped multiset relations\nare subject to two bugs regarding snapshot aggregation and bag difference. We\nintroduce a novel temporal data model based on K-relations that overcomes these\nbugs and prove it to correctly encode snapshot semantics. Furthermore, we\npresent an efficient implementation of our model as a database middleware and\ndemonstrate experimentally that our approach is competitive with native\nimplementations and significantly outperforms such implementations on queries\nthat involve aggregation.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 14:54:40 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Dign\u00f6s", "Anton", ""], ["Glavic", "Boris", ""], ["Niu", "Xing", ""], ["B\u00f6hlen", "Michael", ""], ["Gamper", "Johann", ""]]}, {"id": "1902.05170", "submitter": "Christine Betts", "authors": "Christine Betts, Joanna Power, Waleed Ammar", "title": "GrapAL: Connecting the Dots in Scientific Literature", "comments": "To appear at ACL 2019 (Demonstration Track)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce GrapAL (Graph database of Academic Literature), a versatile tool\nfor exploring and investigating a knowledge base of scientific literature, that\nwas semi-automatically constructed using NLP methods. GrapAL satisfies a\nvariety of use cases and information needs requested by researchers. At the\ncore of GrapAL is a Neo4j graph database with an intuitive schema and a simple\nquery language. In this paper, we describe the basic elements of GrapAL, how to\nuse it, and several use cases such as finding experts on a given topic for peer\nreviewing, discovering indirect connections between biomedical entities and\ncomputing citation-based metrics. We open source the demo code to help other\nresearchers develop applications that build on GrapAL.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 00:07:26 GMT"}, {"version": "v2", "created": "Sun, 19 May 2019 19:11:16 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Betts", "Christine", ""], ["Power", "Joanna", ""], ["Ammar", "Waleed", ""]]}, {"id": "1902.06427", "submitter": "Eugenia Oshurko", "authors": "Angela Bonifati, Peter Furniss, Alastair Green, Russ Harmer, Eugenia\n  Oshurko, Hannes Voigt", "title": "Schema Validation and Evolution for Graph Databases", "comments": "36 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the maturity of commercial graph databases, little consensus has been\nreached so far on the standardization of data definition languages (DDLs) for\nproperty graphs (PG). The discussion on the characteristics of PG schemas is\nongoing in many standardization and community groups. Although some basic\naspects of a schema are already present in Neo4j 3.5, like in most commercial\ngraph databases, full support is missing allowing to constraint property graphs\nwith more or less flexibility. In this paper, we focus on two different\nperspectives from which a PG schema should be considered, as being descriptive\nor prescriptive, and we show how it would be possible to switch from one to\nanother as the application under development gains more stability. Apart from\nproposing concise schema DDL inspired by Cypher syntax, we show how schema\nvalidation can be enforced through homomorphisms between PG schemas and PG\ninstances; and how schema evolution can be described through the use of graph\nrewriting operations. Our prototypical implementation demonstrates feasibility\nand shows the need of offering high-level query primitives to accommodate\nflexible graph schema requirements as showcased in our work.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 07:17:26 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Bonifati", "Angela", ""], ["Furniss", "Peter", ""], ["Green", "Alastair", ""], ["Harmer", "Russ", ""], ["Oshurko", "Eugenia", ""], ["Voigt", "Hannes", ""]]}, {"id": "1902.06743", "submitter": "Nikolaj Tatti", "authors": "Nikolaj Tatti, Fabian Moerchen, Toon Calders", "title": "Finding Robust Itemsets Under Subsampling", "comments": "Journal version. The previous version is the conference version (DOI:\n  10.1109/ICDM.2011.69)", "journal-ref": null, "doi": "10.1145/2656261", "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining frequent patterns is plagued by the problem of pattern explosion\nmaking pattern reduction techniques a key challenge in pattern mining. In this\npaper we propose a novel theoretical framework for pattern reduction. We do\nthis by measuring the robustness of a property of an itemset such as closedness\nor non-derivability. The robustness of a property is the probability that this\nproperty holds on random subsets of the original data. We study four\nproperties: if an itemset is closed, free, non-derivable or totally shattered,\nand demonstrate how to compute the robustness analytically without actually\nsampling the data. Our concept of robustness has many advantages: Unlike\nstatistical approaches for reducing patterns, we do not assume a null\nhypothesis or any noise model and in contrast to noise tolerant or approximate\npatterns, the robust patterns for a given property are always a subset of the\npatterns with this property. If the underlying property is monotonic, then the\nmeasure is also monotonic, allowing us to efficiently mine robust itemsets. We\nfurther derive a parameter-free technique for ranking itemsets that can be used\nfor top-$k$ approaches. Our experiments demonstrate that we can successfully\nuse the robustness measure to reduce the number of patterns and that ranking\nyields interesting itemsets.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 16:19:52 GMT"}, {"version": "v2", "created": "Wed, 24 Apr 2019 03:29:49 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Tatti", "Nikolaj", ""], ["Moerchen", "Fabian", ""], ["Calders", "Toon", ""]]}, {"id": "1902.06792", "submitter": "Sobhan Moosavi", "authors": "Sobhan Moosavi, Mohammad Hossein Samavatian, Arnab Nandi, Srinivasan\n  Parthasarathy and Rajiv Ramnath", "title": "Short and Long-term Pattern Discovery Over Large-Scale\n  Geo-Spatiotemporal Data", "comments": "In Proceedings of the 25th ACM SIGKDD International Conference on\n  Knowledge Discovery and Data Mining", "journal-ref": null, "doi": "10.1145/3292500.3330755", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pattern discovery in geo-spatiotemporal data (such as traffic and weather\ndata) is about finding patterns of collocation, co-occurrence, cascading, or\ncause and effect between geospatial entities. Using simplistic definitions of\nspatiotemporal neighborhood (a common characteristic of the existing\ngeneral-purpose frameworks) is not semantically representative of\ngeo-spatiotemporal data. We therefore introduce a new geo-spatiotemporal\npattern discovery framework which defines a semantically correct definition of\nneighborhood; and then provides two capabilities, one to explore propagation\npatterns and the other to explore influential patterns. Propagation patterns\nreveal common cascading forms of geospatial entities in a region. Influential\npatterns demonstrate the impact of temporally long-term geospatial entities on\ntheir neighborhood. We apply this framework on a large dataset of traffic and\nweather data at countrywide scale, collected for the contiguous United States\nover two years. Our important findings include the identification of 90 common\npropagation patterns of traffic and weather entities (e.g., rain --> accident\n--> congestion), which results in identification of four categories of states\nwithin the US; and interesting influential patterns with respect to the\n\"location\", \"duration\", and \"type\" of long-term entities (e.g., a major\nconstruction --> more traffic incidents). These patterns and the categorization\nof the states provide useful insights on the driving habits and infrastructure\ncharacteristics of different regions in the US, and could be of significant\nvalue for applications such as urban planning and personalized insurance.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 03:07:21 GMT"}, {"version": "v2", "created": "Fri, 17 May 2019 21:34:14 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Moosavi", "Sobhan", ""], ["Samavatian", "Mohammad Hossein", ""], ["Nandi", "Arnab", ""], ["Parthasarathy", "Srinivasan", ""], ["Ramnath", "Rajiv", ""]]}, {"id": "1902.07159", "submitter": "Revanth Reddy Gangi Reddy", "authors": "Revanth Reddy, Sarath Chandar and Balaraman Ravindran", "title": "Edge Replacement Grammars: A Formal Language Approach for Generating\n  Graphs", "comments": "To be presented at SIAM International Conference on Data Mining\n  (SDM19). arXiv admin note: text overlap with arXiv:1802.08068,\n  arXiv:1608.03192 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs are increasingly becoming ubiquitous as models for structured data. A\ngenerative model that closely mimics the structural properties of a given set\nof graphs has utility in a variety of domains. Much of the existing work\nrequire that a large number of parameters, in fact exponential in size of the\ngraphs, be estimated from the data. We take a slightly different approach to\nthis problem, leveraging the extensive prior work in the formal graph grammar\nliterature. In this paper, we propose a graph generation model based on\nProbabilistic Edge Replacement Grammars (PERGs). We propose a variant of PERG\ncalled Restricted PERG (RPERG), which is analogous to PCFGs in string grammar\nliterature. With this restriction, we are able to derive a learning algorithm\nfor estimating the parameters of the grammar from graph data. We empirically\ndemonstrate on real life datasets that RPERGs outperform existing methods for\ngraph generation. We improve on the performance of the state-of-the-art\nHyperedge Replacement Grammar based graph generative model. Despite being a\ncontext free grammar, the proposed model is able to capture many of the\nstructural properties of real networks, such as degree distributions, power law\nand spectral characteristics.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 00:01:04 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Reddy", "Revanth", ""], ["Chandar", "Sarath", ""], ["Ravindran", "Balaraman", ""]]}, {"id": "1902.07165", "submitter": "Nikolaj Tatti", "authors": "Nikolaj Tatti, Jilles Vreeken", "title": "Comparing Apples and Oranges: Measuring Differences between Exploratory\n  Data Mining Results", "comments": "Journal version. The previous version is the conference version", "journal-ref": null, "doi": "10.1007/s10618-012-0275-9", "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deciding whether the results of two different mining algorithms provide\nsignificantly different information is an important, yet understudied, open\nproblem in exploratory data mining. Whether the goal is to select the most\ninformative result for analysis, or to decide which mining approach will most\nlikely provide the most novel insight, it is essential that we can tell how\ndifferent the information is that different results by possibly different\nmethods provide.\n  In this paper we take a first step towards comparing exploratory data mining\nresults on binary data. We propose to meaningfully convert results into sets of\nnoisy tiles, and compare between these sets by Maximum Entropy modelling and\nKullback-Leibler divergence, well-founded notions from Information Theory. We\nso construct a measure that is highly flexible, and allows us to naturally\ninclude background knowledge, such that differences in results can be measured\nfrom the perspective of what a user already knows. Furthermore, adding to its\ninterpretability, it coincides with Jaccard dissimilarity when we only consider\nexact tiles.\n  Our approach provides a means to study and tell differences between results\nof different exploratory data mining methods. As an application, we show that\nour measure can also be used to identify which parts of results best redescribe\nother results. Furthermore, we study its use for iterative data mining, where\none iteratively wants to find that result that will provide maximal novel\ninformation. Experimental evaluation shows our measure gives meaningful\nresults, correctly identifies methods that are similar in nature, automatically\nprovides sound redescriptions of results, and is highly applicable for\niterative data mining.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 16:57:43 GMT"}, {"version": "v2", "created": "Wed, 24 Apr 2019 23:41:08 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Tatti", "Nikolaj", ""], ["Vreeken", "Jilles", ""]]}, {"id": "1902.07353", "submitter": "Ethan Madison", "authors": "Ethan Madison and Zachary Zipper", "title": "In oder Aus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Bloom filters are data structures used to determine set membership of\nelements, with applications from string matching to networking and security\nproblems. These structures are favored because of their reduced memory\nconsumption and fast wallclock and asymptotic time bounds. Generally, Bloom\nfilters maintain constant membership query time, making them very fast in their\nniche. However, they are limited in their lack of a removal operation, as well\nas by their probabilistic nature. In this paper, we discuss various iterations\nof and alternatives to the generic Bloom filter that have been researched and\nimplemented to overcome their inherent limitations. Bloom filters, especially\nwhen used in conjunction with other data structures, are still powerful and\nefficient data structures; we further discuss their use in industy and research\nto optimize resource utilization.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 23:58:11 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Madison", "Ethan", ""], ["Zipper", "Zachary", ""]]}, {"id": "1902.07688", "submitter": "Feichen Shen PhD", "authors": "Feichen Shen", "title": "Towards Semantic Big Graph Analytics for Cross-Domain Knowledge\n  Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, the size of big linked data has grown rapidly and this\nnumber is still rising. Big linked data and knowledge bases come from different\ndomains such as life sciences, publications, media, social web, and so on.\nHowever, with the rapid increasing of data, it is very challenging for people\nto acquire a comprehensive collection of cross domain knowledge to meet their\nneeds. Under this circumstance, it is extremely difficult for people without\nexpertise to extract knowledge from various domains. Therefore, nowadays human\nlimited knowledge can't feed the high requirement for discovering large amount\nof cross domain knowledge. In this research, we present a big graph analytics\nframework aims at addressing this issue by providing semantic methods to\nfacilitate the management of big graph data from close domains in order to\ndiscover cross domain knowledge in a more accurate and efficient way.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 18:23:37 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Shen", "Feichen", ""]]}, {"id": "1902.07901", "submitter": "Anastasios Gounaris", "authors": "Theodoros Toliopoulos, Anastasios Gounaris, Kostas Tsichlas, Apostolos\n  Papadopoulos, Sandra Sampaio", "title": "Continuous Outlier Mining of Streaming Data in Flink", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we focus on distance-based outliers in a metric space, where\nthe status of an entity as to whether it is an outlier is based on the number\nof other entities in its neighborhood. In recent years, several solutions have\ntackled the problem of distance-based outliers in data streams, where outliers\nmust be mined continuously as new elements become available. An interesting\nresearch problem is to combine the streaming environment with massively\nparallel systems to provide scalable streambased algorithms. However, none of\nthe previously proposed techniques refer to a massively parallel setting. Our\nproposal fills this gap and investigates the challenges in transferring\nstate-of-the-art techniques to Apache Flink, a modern platform for intensive\nstreaming analytics. We thoroughly present the technical challenges encountered\nand the alternatives that may be applied. We show speed-ups of up to 117 (resp.\n2076) times over a naive parallel (resp. non-parallel) solution in Flink, by\nusing just an ordinary four-core machine and a real-world dataset. When moving\nto a three-machine cluster, due to less contention, we manage to achieve both\nbetter scalability in terms of the window slide size and the data\ndimensionality, and even higher speed-ups, e.g., by a factor of 510. Overall,\nour results demonstrate that oulier mining can be achieved in an efficient and\nscalable manner. The resulting techniques have been made publicly available as\nopen-source software.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 07:51:51 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Toliopoulos", "Theodoros", ""], ["Gounaris", "Anastasios", ""], ["Tsichlas", "Kostas", ""], ["Papadopoulos", "Apostolos", ""], ["Sampaio", "Sandra", ""]]}, {"id": "1902.08271", "submitter": "Xikui Wang", "authors": "Xikui Wang, Michael J. Carey", "title": "An IDEA: An Ingestion Framework for Data Enrichment in AsterixDB", "comments": "21 pages, 40 Figures, accepted in VLDB 2019", "journal-ref": null, "doi": "10.14778/3342263.3342628", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big Data today is being generated at an unprecedented rate from various\nsources such as sensors, applications, and devices, and it often needs to be\nenriched based on other reference information to support complex analytical\nqueries. Depending on the use case, the enrichment operations can be compiled\ncode, declarative queries, or machine learning models with different\ncomplexities. For enrichments that will be frequently used in the future, it\ncan be advantageous to push their computation into the ingestion pipeline so\nthat they can be stored (and queried) together with the data. In some cases,\nthe referenced information may change over time, so the ingestion pipeline\nshould be able to adapt to such changes to guarantee the currency and/or\ncorrectness of the enrichment results.\n  In this paper, we present a new data ingestion framework that supports data\ningestion at scale, enrichments requiring complex operations, and adaptiveness\nto reference data changes. We explain how this framework has been built on top\nof Apache AsterixDB and investigate its performance at scale under various\nworkloads.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 21:23:05 GMT"}, {"version": "v2", "created": "Thu, 28 Feb 2019 23:24:56 GMT"}, {"version": "v3", "created": "Fri, 23 Aug 2019 22:58:19 GMT"}, {"version": "v4", "created": "Sun, 24 May 2020 00:35:14 GMT"}, {"version": "v5", "created": "Sat, 15 Aug 2020 21:22:55 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Wang", "Xikui", ""], ["Carey", "Michael J.", ""]]}, {"id": "1902.08283", "submitter": "Babak Salimi", "authors": "Babak Salimi, Luke Rodriguez, Bill Howe, Dan Suciu", "title": "Capuchin: Causal Database Repair for Algorithmic Fairness", "comments": null, "journal-ref": "Proceedings of the 2019 International Conference on Management of\n  Data. ACM, 2019", "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fairness is increasingly recognized as a critical component of machine\nlearning systems. However, it is the underlying data on which these systems are\ntrained that often reflect discrimination, suggesting a database repair\nproblem. Existing treatments of fairness rely on statistical correlations that\ncan be fooled by statistical anomalies, such as Simpson's paradox. Proposals\nfor causality-based definitions of fairness can correctly model some of these\nsituations, but they require specification of the underlying causal models. In\nthis paper, we formalize the situation as a database repair problem, proving\nsufficient conditions for fair classifiers in terms of admissible variables as\nopposed to a complete causal model. We show that these conditions correctly\ncapture subtle fairness violations. We then use these conditions as the basis\nfor database repair algorithms that provide provable fairness guarantees about\nclassifiers trained on their training labels. We evaluate our algorithms on\nreal data, demonstrating improvement over the state of the art on multiple\nfairness metrics proposed in the literature while retaining high utility.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 22:13:29 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 00:20:52 GMT"}, {"version": "v3", "created": "Mon, 15 Jul 2019 23:34:33 GMT"}, {"version": "v4", "created": "Mon, 22 Jul 2019 01:35:59 GMT"}, {"version": "v5", "created": "Tue, 1 Oct 2019 19:37:23 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Salimi", "Babak", ""], ["Rodriguez", "Luke", ""], ["Howe", "Bill", ""], ["Suciu", "Dan", ""]]}, {"id": "1902.08291", "submitter": "Matthew Perron", "authors": "Matthew Perron, Zeyuan Shang, Tim Kraska, Michael Stonebraker", "title": "How I Learned to Stop Worrying and Love Re-optimization", "comments": "Short version appearing in ICDE 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cost-based query optimizers remain one of the most important components of\ndatabase management systems for analytic workloads. Though modern optimizers\nselect plans close to optimal performance in the common case, a small number of\nqueries are an order of magnitude slower than they could be. In this paper we\ninvestigate why this is still the case, despite decades of improvements to cost\nmodels, plan enumeration, and cardinality estimation. We demonstrate why we\nbelieve that a re-optimization mechanism is likely the most cost-effective way\nto improve end-to-end query performance. We find that even a simple\nre-optimization scheme can improve the latency of many poorly performing\nqueries. We demonstrate that re-optimization improves the end-to-end latency of\nthe top 20 longest running queries in the Join Order Benchmark by 27%,\nrealizing most of the benefit of perfect cardinality estimation.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 22:33:52 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2019 08:51:21 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Perron", "Matthew", ""], ["Shang", "Zeyuan", ""], ["Kraska", "Tim", ""], ["Stonebraker", "Michael", ""]]}, {"id": "1902.08318", "submitter": "Daniel Lemire", "authors": "Geoff Langdale, Daniel Lemire", "title": "Parsing Gigabytes of JSON per Second", "comments": "software: https://github.com/lemire/simdjson", "journal-ref": "The VLDB Journal, 28(6), 2019", "doi": "10.1007/s00778-019-00578-5", "report-no": null, "categories": "cs.DB cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  JavaScript Object Notation or JSON is a ubiquitous data exchange format on\nthe Web. Ingesting JSON documents can become a performance bottleneck due to\nthe sheer volume of data. We are thus motivated to make JSON parsing as fast as\npossible.\n  Despite the maturity of the problem of JSON parsing, we show that substantial\nspeedups are possible. We present the first standard-compliant JSON parser to\nprocess gigabytes of data per second on a single core, using commodity\nprocessors. We can use a quarter or fewer instructions than a state-of-the-art\nreference parser like RapidJSON. Unlike other validating parsers, our software\n(simdjson) makes extensive use of Single Instruction, Multiple Data (SIMD)\ninstructions. To ensure reproducibility, simdjson is freely available as\nopen-source software under a liberal license.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2019 00:24:01 GMT"}, {"version": "v2", "created": "Mon, 25 Feb 2019 19:45:23 GMT"}, {"version": "v3", "created": "Mon, 17 Jun 2019 21:51:55 GMT"}, {"version": "v4", "created": "Tue, 13 Aug 2019 00:34:45 GMT"}, {"version": "v5", "created": "Mon, 30 Dec 2019 23:10:47 GMT"}, {"version": "v6", "created": "Thu, 2 Jan 2020 14:56:46 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Langdale", "Geoff", ""], ["Lemire", "Daniel", ""]]}, {"id": "1902.09155", "submitter": "Hugo Ledoux", "authors": "Hugo Ledoux and Ken Arroyo Ohori and Kavisha Kumar and Bal\\'azs Dukai\n  and Anna Labetski and Stelios Vitalis", "title": "CityJSON: a compact and easy-to-use encoding of the CityGML data model", "comments": "preprint of the paper", "journal-ref": "Open Geospatial Data, Software and Standards. 4(4) 2019", "doi": "10.1186/s40965-019-0064-0", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The international standard CityGML is both a data model and an exchange\nformat to store digital 3D models of cities. While the data model is used by\nseveral cities, companies, and governments, in this paper we argue that its\nXML-based exchange format has several drawbacks. These drawbacks mean that it\nis difficult for developers to implement parsers for CityGML, and that\npractitioners have, as a consequence, to convert their data to other formats if\nthey want to exchange them with others. We present CityJSON, a new JSON-based\nexchange format for the CityGML data model (version 2.0.0). CityJSON was\ndesigned with programmers in mind, so that software and APIs supporting it can\nbe quickly built. It was also designed to be compact (a compression factor of\naround six with real-world datasets), and to be friendly for web and mobile\ndevelopment. We argue that it is considerably easier to use than the CityGML\nformat, both for reading and for creating datasets. We discuss in this paper\nthe main features of CityJSON, briefly present the different software packages\nto parse/view/edit/create files (including one to automatically convert between\nthe JSON and GML encodings), analyse how real-world datasets compare to those\nof CityGML, and we also introduce \\emph{Extensions}, which allow us to extend\nthe core data model in a documented manner.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 09:13:58 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 12:18:56 GMT"}, {"version": "v3", "created": "Fri, 24 Jan 2020 10:35:44 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Ledoux", "Hugo", ""], ["Ohori", "Ken Arroyo", ""], ["Kumar", "Kavisha", ""], ["Dukai", "Bal\u00e1zs", ""], ["Labetski", "Anna", ""], ["Vitalis", "Stelios", ""]]}, {"id": "1902.09512", "submitter": "Karim Banawan", "authors": "Karim Banawan and Batuhan Arasli and Yi-Peng Wei and Sennur Ulukus", "title": "The Capacity of Private Information Retrieval from Heterogeneous Uncoded\n  Caching Databases", "comments": "Submitted for publication, February 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.DB math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider private information retrieval (PIR) of a single file out of $K$\nfiles from $N$ non-colluding databases with heterogeneous storage constraints\n$\\mathbf{m}=(m_1, \\cdots, m_N)$. The aim of this work is to jointly design the\ncontent placement phase and the information retrieval phase in order to\nminimize the download cost in the PIR phase. We characterize the optimal PIR\ndownload cost as a linear program. By analyzing the structure of the optimal\nsolution of this linear program, we show that, surprisingly, the optimal\ndownload cost in our heterogeneous case matches its homogeneous counterpart\nwhere all databases have the same average storage constraint $\\mu=\\frac{1}{N}\n\\sum_{n=1}^{N} m_n$. Thus, we show that there is no loss in the PIR capacity\ndue to heterogeneity of storage spaces of the databases. We provide the optimum\ncontent placement explicitly for $N=3$.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 18:50:40 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Banawan", "Karim", ""], ["Arasli", "Batuhan", ""], ["Wei", "Yi-Peng", ""], ["Ulukus", "Sennur", ""]]}, {"id": "1902.09582", "submitter": "Wensheng Gan", "authors": "Wensheng Gan, Jerry Chun-Wei Lin, Jiexiong Zhang, Hongzhi Yin,\n  Philippe Fournier-Viger, Han-Chieh Chao, and Philip S. Yu", "title": "Utility Mining Across Multi-Dimensional Sequences", "comments": "Under review in IEEE TKDE, 14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge extraction from database is the fundamental task in database and\ndata mining community, which has been applied to a wide range of real-world\napplications and situations. Different from the support-based mining models,\nthe utility-oriented mining framework integrates the utility theory to provide\nmore informative and useful patterns. Time-dependent sequence data is commonly\nseen in real life. Sequence data has been widely utilized in many applications,\nsuch as analyzing sequential user behavior on the Web, influence maximization,\nroute planning, and targeted marketing. Unfortunately, all the existing\nalgorithms lose sight of the fact that the processed data not only contain rich\nfeatures (e.g., occur quantity, risk, profit, etc.), but also may be associated\nwith multi-dimensional auxiliary information, e.g., transaction sequence can be\nassociated with purchaser profile information. In this paper, we first\nformulate the problem of utility mining across multi-dimensional sequences, and\npropose a novel framework named MDUS to extract Multi-Dimensional\nUtility-oriented Sequential useful patterns. Two algorithms respectively named\nMDUS_EM and MDUS_SD are presented to address the formulated problem. The former\nalgorithm is based on database transformation, and the later one performs\npattern joins and a searching method to identify desired patterns across\nmulti-dimensional sequences. Extensive experiments are carried on five\nreal-life datasets and one synthetic dataset to show that the proposed\nalgorithms can effectively and efficiently discover the useful knowledge from\nmulti-dimensional sequential databases. Moreover, the MDUS framework can\nprovide better insight, and it is more adaptable to real-life situations than\nthe current existing models.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 19:40:44 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Gan", "Wensheng", ""], ["Lin", "Jerry Chun-Wei", ""], ["Zhang", "Jiexiong", ""], ["Yin", "Hongzhi", ""], ["Fournier-Viger", "Philippe", ""], ["Chao", "Han-Chieh", ""], ["Yu", "Philip S.", ""]]}, {"id": "1902.09584", "submitter": "Wensheng Gan", "authors": "Wensheng Gan, Jerry Chun-Wei Lin, Philippe Fournier-Viger, Han-Chieh\n  Chao, and Philip S Yu", "title": "Beyond Frequency: Utility Mining with Varied Item-Specific Minimum\n  Utility", "comments": "Under review in ACM Trans. on Data Science, 31 pages", "journal-ref": "ACM Transactions on Internet Technology, 2021", "doi": "10.1145/3425498", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Utility-oriented mining which integrates utility theory and data mining is a\nuseful tool for understanding economic consumer behavior. Traditional\nalgorithms for mining high-utility patterns (HUPs) applies a single/uniform\nminimum high-utility threshold (minutil) to obtain the set of HUPs, but in some\nreal-life circumstances, some specific products may bring lower utilities\ncompared with others, but their profit may offer some vital information.\nHowever, if minutil is set high, the patterns with low minutil are missed; if\nminutil is set low, the number of patterns becomes unmanageable. In this paper,\nan efficient one-phase utility-oriented pattern mining algorithm, called HIMU,\nis proposed for mining HUPs with varied item-specific minimum utility. A novel\ntree structure called a multiple item utility set-enumeration tree (MIU-tree),\nthe global sorted and the conditional downward closure properties are\nintroduced in HIMU. In addition, we extended the compact utility-list structure\nto keep the necessary information, and thus this one-phase HIMU model greatly\nreduces the computational costs and memory requirements. Moreover, two pruning\nstrategies are then extended to enhance the performance. We conducted extensive\nexperiments in several synthetic and real-world datasets; the results indicates\nthat the designed one-phase HIMU algorithm can address the \"rare item problem\"\nand has better performance than the state-of-the-art algorithms in terms of\nruntime, memory usage, and scalability. Furthermore, the enhanced algorithms\noutperform the non-optimized HIMU approach.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 19:41:15 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Gan", "Wensheng", ""], ["Lin", "Jerry Chun-Wei", ""], ["Fournier-Viger", "Philippe", ""], ["Chao", "Han-Chieh", ""], ["Yu", "Philip S", ""]]}, {"id": "1902.09586", "submitter": "Wensheng Gan", "authors": "Wensheng Gan, Jerry Chun-Wei Lin, Han-Chieh Chao, Athanasios V.\n  Vasilakos, and Philip S. Yu", "title": "Utility-driven Data Analytics on Uncertain Data", "comments": "Under review in IEEE Internet of Things Journal since 2018, 11 pages", "journal-ref": "IEEE Systems Journal, 2020", "doi": "10.1109/JSYST.2020.2979279", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern Internet of Things (IoT) applications generate massive amounts of\ndata, much of it in the form of objects/items of readings, events, and log\nentries. Specifically, most of the objects in these IoT data contain rich\nembedded information (e.g., frequency and uncertainty) and different level of\nimportance (e.g., unit utility of items, interestingness, cost, risk, or\nweight). Many existing approaches in data mining and analytics have limitations\nsuch as only the binary attribute is considered within a transaction, as well\nas all the objects/items having equal weights or importance. To solve these\ndrawbacks, a novel utility-driven data analytics algorithm named HUPNU is\npresented, to extract High-Utility patterns by considering both Positive and\nNegative unit utilities from Uncertain data. The qualified high-utility\npatterns can be effectively discovered for risk prediction, manufacturing\nmanagement, decision-making, among others. By using the developed vertical\nProbability-Utility list with the Positive-and-Negative utilities structure, as\nwell as several effective pruning strategies. Experiments showed that the\ndeveloped HUPNU approach performed great in mining the qualified patterns\nefficiently and effectively.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 19:43:26 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Gan", "Wensheng", ""], ["Lin", "Jerry Chun-Wei", ""], ["Chao", "Han-Chieh", ""], ["Vasilakos", "Athanasios V.", ""], ["Yu", "Philip S.", ""]]}, {"id": "1902.09661", "submitter": "Yuliang Li", "authors": "Yuliang Li, Aaron Xixuan Feng, Jinfeng Li, Saran Mumick, Alon Halevy,\n  Vivian Li, Wang-Chiew Tan", "title": "Subjective Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online users are constantly seeking experiences, such as a hotel with clean\nrooms and a lively bar, or a restaurant for a romantic rendezvous. However,\ne-commerce search engines only support queries involving objective attributes\nsuch as location, price, and cuisine, and any experiential data is relegated to\ntext reviews.\n  In order to support experiential queries, a database system needs to model\nsubjective data and also be able to process queries where the user can express\nvaried subjective experiences in words chosen by the user, in addition to\nspecifying predicates involving objective attributes. This paper introduces\nOpine, a subjective database system that addresses these challenges. We\nintroduce a data model for subjective databases. We describe how Opine\ntranslates subjective queries against the subjective database schema, which is\ndone by matching the user query phrases to the underlying schema. We also show\nhow the experiential conditions specified by the user can be combined and the\nresults aggregated and ranked. We demonstrate that subjective databases satisfy\nuser needs more effectively and accurately than alternative techniques through\nexperiments with real data of hotel and restaurant reviews.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 23:39:47 GMT"}, {"version": "v2", "created": "Sat, 2 Mar 2019 00:54:53 GMT"}, {"version": "v3", "created": "Mon, 3 Jun 2019 17:40:08 GMT"}, {"version": "v4", "created": "Wed, 24 Jul 2019 23:12:18 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Li", "Yuliang", ""], ["Feng", "Aaron Xixuan", ""], ["Li", "Jinfeng", ""], ["Mumick", "Saran", ""], ["Halevy", "Alon", ""], ["Li", "Vivian", ""], ["Tan", "Wang-Chiew", ""]]}, {"id": "1902.09711", "submitter": "Jing Yan", "authors": "Jing Nathan Yan, Oliver Schulte, Jiannan Wang, Reynold Cheng", "title": "Detecting Data Errors with Statistical Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A powerful approach to detecting erroneous data is to check which potentially\ndirty data records are incompatible with a user's domain knowledge. Previous\napproaches allow the user to specify domain knowledge in the form of logical\nconstraints (e.g., functional dependency and denial constraints). We extend the\nconstraint-based approach by introducing a novel class of statistical\nconstraints (SCs). An SC treats each column as a random variable, and enforces\nan independence or dependence relationship between two (or a few) random\nvariables. Statistical constraints are expressive, allowing the user to specify\na wide range of domain knowledge, beyond traditional integrity constraints.\nFurthermore, they work harmoniously with downstream statistical modeling. We\ndevelop CODED, an SC-Oriented Data Error Detection system that supports three\nkey tasks: (1) Checking whether an SC is violated or not on a given dataset,\n(2) Identify the top-k records that contribute the most to the violation of an\nSC, and (3) Checking whether a set of input SCs have conflicts or not. We\npresent effective solutions for each task. Experiments on synthetic and\nreal-world data illustrate how SCs apply to error detection, and provide\nevidence that CODED performs better than state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 02:44:20 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Yan", "Jing Nathan", ""], ["Schulte", "Oliver", ""], ["Wang", "Jiannan", ""], ["Cheng", "Reynold", ""]]}, {"id": "1902.09897", "submitter": "Alexandros Bampoulidis", "authors": "Alexandros Bampoulidis, Mihai Lupu", "title": "An Abstract View on the De-anonymization Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the recent years, the availability of datasets containing personal, but\nanonymized information has been continuously increasing. Extensive research has\nrevealed that such datasets are vulnerable to privacy breaches: being able to\nreveal sensitive information about individuals through deanonymization methods.\nHere, we provide a taxonomy of the research in de-anonymization.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 12:49:32 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Bampoulidis", "Alexandros", ""], ["Lupu", "Mihai", ""]]}, {"id": "1902.10514", "submitter": "Maria Maleshkova", "authors": "Frederik Buelthoff, Maria Maleshkova", "title": "RESTful or RESTless -- Current State of Today's Top Web APIs", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CY cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in the world of services on the Web show that both the\nnumber of available Web APIs as well as the applications built on top is\nconstantly increasing. This trend is commonly attributed to the wide adoption\nof the REST architectural principles. Still, the development of Web APIs is\nrather autonomous and it is up to the providers to decide how to implement,\nexpose and describe the Web APIs. The individual implementations are then\ncommonly documented in textual form as part of a webpage, showing a wide\nvariety in terms of content, structure and level of detail. As a result, client\napplication developers are forced to manually process and interpret the\ndocumentation. Before we can achieve a higher level of automation and can make\nany significant improvement to current practices and technologies, we need to\nreach a deeper understanding of their similarities and differences. Therefore,\nin this paper we present a thorough analysis of the most popular Web APIs\nthrough the examination of their documentation. We provide conclusions about\ncommon description forms, output types, usage of API parameters, invocation\nsupport, level of reusability, API granularity and authentication details. The\ncollected data builds a solid foundation for identifying deficiencies and can\nbe used as a basis for devising common standards and guidelines for Web API\ndevelopment.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 12:36:17 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Buelthoff", "Frederik", ""], ["Maleshkova", "Maria", ""]]}, {"id": "1902.10677", "submitter": "Tal Friedman", "authors": "Tal Friedman, Guy Van den Broeck", "title": "On Constrained Open-World Probabilistic Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing amounts of available data have led to a heightened need for\nrepresenting large-scale probabilistic knowledge bases. One approach is to use\na probabilistic database, a model with strong assumptions that allow for\nefficiently answering many interesting queries. Recent work on open-world\nprobabilistic databases strengthens the semantics of these probabilistic\ndatabases by discarding the assumption that any information not present in the\ndata must be false. While intuitive, these semantics are not sufficiently\nprecise to give reasonable answers to queries. We propose overcoming these\nissues by using constraints to restrict this open world. We provide an\nalgorithm for one class of queries, and establish a basic hardness result for\nanother. Finally, we propose an efficient and tight approximation for a large\nclass of queries.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 18:31:10 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 19:02:17 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Friedman", "Tal", ""], ["Broeck", "Guy Van den", ""]]}, {"id": "1902.10703", "submitter": "Natanael Arndt", "authors": "Natanael Arndt and Michael Martin", "title": "Decentralized Evolution and Consolidation of RDF Graphs", "comments": "ICWE 2017. arXiv admin note: substantial text overlap with\n  arXiv:1805.03721", "journal-ref": null, "doi": "10.1007/978-3-319-60131-1_2", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The World Wide Web and the Semantic Web are designed as a network of\ndistributed services and datasets. In this network and its genesis,\ncollaboration played and still plays a crucial role. But currently we only have\ncentral collaboration solutions for RDF data, such as SPARQL endpoints and wiki\nsystems, while decentralized solutions can enable applications for many more\nuse-cases. Inspired by a successful distributed source code management\nmethodology in software engineering a framework to support distributed\nevolution is proposed. The system is based on Git and provides distributed\ncollaboration on RDF graphs. This paper covers the formal expression of the\nevolution and consolidation of distributed datasets, the synchronization, as\nwell as other supporting operations.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 11:58:07 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Arndt", "Natanael", ""], ["Martin", "Michael", ""]]}, {"id": "1902.10999", "submitter": "Aditi Sharma", "authors": "Ravi Ranjan, Aditi Sharma", "title": "Evaluation of Frequent Itemset Mining Platforms using Apriori and\n  FP-Growth Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the overwhelming amount of complex and heterogeneous data pouring from\nany-where, any-time, and any-device, there is undeniably an era of Big Data.\nThe emergence of the Big Data as a disruptive technology for next generation of\nintelligent systems, has brought many issues of how to extract and make use of\nthe knowledge obtained from the data within short times, limited budget and\nunder high rates of data generation. Companies are recognizing that big data\ncan be used to make more accurate predictions, and can be used to enhance the\nbusiness with the help of appropriate association rule mining algorithm. To\nhelp these organizations, with which software and algorithm is more appropriate\nfor them depending on their dataset, we compared the most famous three\nMapReduce based software Hadoop, Spark, Flink on two widely used algorithms\nApriori and Fp-Growth on different scales of dataset.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 10:36:51 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Ranjan", "Ravi", ""], ["Sharma", "Aditi", ""]]}]