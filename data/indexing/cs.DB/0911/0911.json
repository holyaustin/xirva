[{"id": "0911.0505", "submitter": "Kirk D. Borne", "authors": "Kirk Borne (1) ((1) George Mason University)", "title": "Scientific Data Mining in Astronomy", "comments": "26 pages", "journal-ref": "Borne, K., in Next Generation of Data Mining (Taylor & Francis:\n  CRC Press), pp. 91-114 (2009)", "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DB cs.IR physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the application of data mining algorithms to research problems in\nastronomy. We posit that data mining has always been fundamental to\nastronomical research, since data mining is the basis of evidence-based\ndiscovery, including classification, clustering, and novelty discovery. These\nalgorithms represent a major set of computational tools for discovery in large\ndatabases, which will be increasingly essential in the era of data-intensive\nastronomy. Historical examples of data mining in astronomy are reviewed,\nfollowed by a discussion of one of the largest data-producing projects\nanticipated for the coming decade: the Large Synoptic Survey Telescope (LSST).\nTo facilitate data-driven discoveries in astronomy, we envision a new\ndata-oriented research paradigm for astronomy and astrophysics --\nastroinformatics. Astroinformatics is described as both a research approach and\nan educational imperative for modern data-intensive astronomy. An important\napplication area for large time-domain sky surveys (such as LSST) is the rapid\nidentification, characterization, and classification of real-time sky events\n(including moving objects, photometrically variable objects, and the appearance\nof transients). We describe one possible implementation of a classification\nbroker for such events, which incorporates several astroinformatics techniques:\nuser annotation, semantic tagging, metadata markup, heterogeneous data\nintegration, and distributed data mining. Examples of these types of\ncollaborative classification and discovery approaches within other science\ndisciplines are presented.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2009 05:34:54 GMT"}], "update_date": "2009-11-04", "authors_parsed": [["Borne", "Kirk", "", "George Mason University"]]}, {"id": "0911.0508", "submitter": "Ravindra Guravannavar", "authors": "Ravindra Guravannavar", "title": "Optimization and Evaluation of Nested Queries and Procedures", "comments": "Ph.D. thesis, 167 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many database applications perform complex data retrieval and update tasks.\nNested queries, and queries that invoke user-defined functions, which are\nwritten using a mix of procedural and SQL constructs, are often used in such\napplications. A straight-forward evaluation of such queries involves repeated\nexecution of parameterized sub-queries or blocks containing queries and\nprocedural code.\n  An important problem that arises while optimizing nested queries as well as\nqueries with joins, aggregates and set operations is the problem of finding an\noptimal sort order from a factorial number of possible sort orders. We show\nthat even a special case of this problem is NP-Hard, and present practical\nheuristics that are effective and easy to incorporate in existing query\noptimizers.\n  We also consider iterative execution of queries and updates inside complex\nprocedural blocks such as user-defined functions and stored procedures.\nParameter batching is an important means of improving performance as it enables\nset-orientated processing. The key challenge to parameter batching lies in\nrewriting a given procedure/function to process a batch of parameter values. We\npropose a solution, based on program analysis and rewrite rules, to automate\nthe generation of batched forms of procedures and replace iterative database\ncalls within imperative loops with a single call to the batched form.\n  We present experimental results for the proposed techniques, and the results\nshow significant gains in performance.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2009 06:25:23 GMT"}], "update_date": "2009-11-04", "authors_parsed": [["Guravannavar", "Ravindra", ""]]}, {"id": "0911.0781", "submitter": "Rdv Ijcsis", "authors": "Kanak Saxena, D.S Rajpoot", "title": "A Way to Understand Various Patterns of Data Mining Techniques for\n  Selected Domains", "comments": "6 pages IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS 2009, ISSN 1947 5500, Impact Factor 0.423,\n  http://sites.google.com/site/ijcsis/", "journal-ref": "International Journal of Computer Science and Information\n  Security, IJCSIS, Vol. 6, No. 1, pp. 186-191, October 2009, USA", "doi": null, "report-no": "ISSN 1947 5500", "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This has much in common with traditional work in statistics and machine\nlearning. However, there are important new issues which arise because of the\nsheer size of the data. One of the important problem in data mining is the\nClassification-rule learning which involves finding rules that partition given\ndata into predefined classes. In the data mining domain where millions of\nrecords and a large number of attributes are involved, the execution time of\nexisting algorithms can become prohibitive, particularly in interactive\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2009 11:19:55 GMT"}], "update_date": "2009-11-05", "authors_parsed": [["Saxena", "Kanak", ""], ["Rajpoot", "D. S", ""]]}, {"id": "0911.0801", "submitter": "D\\'aniel Marx", "authors": "D\\'aniel Marx", "title": "Tractable hypergraph properties for constraint satisfaction and\n  conjunctive queries", "comments": "Extended abstract appeared in STOC 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DB cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important question in the study of constraint satisfaction problems (CSP)\nis understanding how the graph or hypergraph describing the incidence structure\nof the constraints influences the complexity of the problem. For binary CSP\ninstances (i.e., where each constraint involves only two variables), the\nsituation is well understood: the complexity of the problem essentially depends\non the treewidth of the graph of the constraints. However, this is not the\ncorrect answer if constraints with unbounded number of variables are allowed,\nand in particular, for CSP instances arising from query evaluation problems in\ndatabase theory. Formally, if H is a class of hypergraphs, then let CSP(H) be\nCSP restricted to instances whose hypergraph is in H. Our goal is to\ncharacterize those classes of hypergraphs for which CSP(H) is polynomial-time\nsolvable or fixed-parameter tractable, parameterized by the number of\nvariables. Note that in the applications related to database query evaluation,\nwe usually assume that the number of variables is much smaller than the size of\nthe instance, thus parameterization by the number of variables is a meaningful\nquestion. The most general known property of H that makes CSP(H)\npolynomial-time solvable is bounded fractional hypertree width. Here we\nintroduce a new hypergraph measure called submodular width, and show that\nbounded submodular width of H implies that CSP(H) is fixed-parameter tractable.\nIn a matching hardness result, we show that if H has unbounded submodular\nwidth, then CSP(H) is not fixed-parameter tractable, unless the Exponential\nTime Hypothesis fails.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2009 14:07:38 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2010 09:35:29 GMT"}, {"version": "v3", "created": "Tue, 6 Dec 2011 15:12:49 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Marx", "D\u00e1niel", ""]]}, {"id": "0911.1691", "submitter": "Rasmus Resen Amossen", "authors": "Rasmus Resen Amossen", "title": "Vertical partitioning of relational OLTP databases using integer\n  programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A way to optimize performance of relational row store databases is to reduce\nthe row widths by vertically partitioning tables into table fractions in order\nto minimize the number of irrelevant columns/attributes read by each\ntransaction. This paper considers vertical partitioning algorithms for\nrelational row-store OLTP databases with an H-store-like architecture, meaning\nthat we would like to maximize the number of single-sited transactions. We\npresent a model for the vertical partitioning problem that, given a schema\ntogether with a vertical partitioning and a workload, estimates the costs\n(bytes read/written by storage layer access methods and bytes transferred\nbetween sites) of evaluating the workload on the given partitioning. The cost\nmodel allows for arbitrarily prioritizing load balancing of sites vs. total\ncost minimization. We show that finding a minimum-cost vertical partitioning in\nthis model is NP-hard and present two algorithms returning solutions in which\nsingle-sitedness of read queries is preserved while allowing column replication\n(which may allow a drastically reduced cost compared to disjoint partitioning).\nThe first algorithm is a quadratic integer program that finds optimal\nminimum-cost solutions with respect to the model, and the second algorithm is a\nmore scalable heuristic based on simulated annealing. Experiments show that the\nalgorithms can reduce the cost of the model objective by 37% when applied to\nthe TPC-C benchmark and the heuristic is shown to obtain solutions with cost\nclose to the ones found using the quadratic program.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2009 15:03:31 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2010 20:28:21 GMT"}, {"version": "v3", "created": "Tue, 16 Feb 2010 09:43:50 GMT"}], "update_date": "2010-02-16", "authors_parsed": [["Amossen", "Rasmus Resen", ""]]}, {"id": "0911.1813", "submitter": "Aaron Roth", "authors": "Aaron Roth, Tim Roughgarden", "title": "Interactive Privacy via the Median Mechanism", "comments": "Appeared in STOC 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CC cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a new interactive differentially private mechanism -- the median\nmechanism -- for answering arbitrary predicate queries that arrive online.\nRelative to fixed accuracy and privacy constraints, this mechanism can answer\nexponentially more queries than the previously best known interactive privacy\nmechanism (the Laplace mechanism, which independently perturbs each query\nresult). Our guarantee is almost the best possible, even for non-interactive\nprivacy mechanisms. Conceptually, the median mechanism is the first privacy\nmechanism capable of identifying and exploiting correlations among queries in\nan interactive setting.\n  We also give an efficient implementation of the median mechanism, with\nrunning time polynomial in the number of queries, the database size, and the\ndomain size. This efficient implementation guarantees privacy for all input\ndatabases, and accurate query results for almost all input databases. The\ndependence of the privacy on the number of queries in this mechanism improves\nover that of the best previously known efficient mechanism by a\nsuper-polynomial factor, even in the non-interactive setting.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2009 03:55:44 GMT"}, {"version": "v2", "created": "Wed, 19 Jan 2011 16:09:18 GMT"}], "update_date": "2011-01-20", "authors_parsed": [["Roth", "Aaron", ""], ["Roughgarden", "Tim", ""]]}, {"id": "0911.2942", "submitter": "Chris Giannella", "authors": "Chris Giannella, Kun Liu, Hillol Kargupta", "title": "Breaching Euclidean Distance-Preserving Data Perturbation Using Few\n  Known Inputs", "comments": "This is a major revision accounting for journal peer-review. Changes\n  include: removal of known sample attack, more citations added, an empirical\n  comparison against the algorithm of Kaplan et al. added", "journal-ref": "Data & Knowledge Engineering 83, pages 93-110, 2013", "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine Euclidean distance-preserving data perturbation as a tool for\nprivacy-preserving data mining. Such perturbations allow many important data\nmining algorithms e.g. hierarchical and k-means clustering), with only minor\nmodification, to be applied to the perturbed data and produce exactly the same\nresults as if applied to the original data. However, the issue of how well the\nprivacy of the original data is preserved needs careful study. We engage in\nthis study by assuming the role of an attacker armed with a small set of known\noriginal data tuples (inputs). Little work has been done examining this kind of\nattack when the number of known original tuples is less than the number of data\ndimensions. We focus on this important case, develop and rigorously analyze an\nattack that utilizes any number of known original tuples. The approach allows\nthe attacker to estimate the original data tuple associated with each perturbed\ntuple and calculate the probability that the estimation results in a privacy\nbreach. On a real 16-dimensional dataset, we show that the attacker, with 4\nknown original tuples, can estimate an original unknown tuple with less than 7%\nerror with probability exceeding 0.8.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2009 02:51:37 GMT"}, {"version": "v2", "created": "Wed, 2 Jan 2013 15:49:10 GMT"}], "update_date": "2013-01-03", "authors_parsed": [["Giannella", "Chris", ""], ["Liu", "Kun", ""], ["Kargupta", "Hillol", ""]]}, {"id": "0911.3600", "submitter": "Pasquale  De Meo", "authors": "P. De Meo, G. Quattrone, G. Terracina, D. Ursino", "title": "\"Almost automatic\" and semantic integration of XML Schemas at various\n  \"severity\" levels", "comments": "18 pages, 3 Figures, 3 Tables", "journal-ref": "Proc. of the International Conference on Cooperative Information\n  Systems (CoopIS 2003), pages 4 -21, Taormina, Italy, 2003. Lecture Notes in\n  Computer Science, Springer", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach for the integration of a set of XML\nSchemas. The proposed approach is specialized for XML, is almost automatic,\nsemantic and \"light\". As a further, original, peculiarity, it is parametric\nw.r.t. a \"severity\" level against which the integration task is performed. The\npaper describes the approach in all details, illustrates various theoretical\nresults, presents the experiments we have performed for testing it and,\nfinally, compares it with various related approaches already proposed in the\nliterature.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2009 16:59:38 GMT"}], "update_date": "2009-11-19", "authors_parsed": [["De Meo", "P.", ""], ["Quattrone", "G.", ""], ["Terracina", "G.", ""], ["Ursino", "D.", ""]]}, {"id": "0911.4329", "submitter": "Ki-Hoon Lee", "authors": "Ki-Hoon Lee, Kyu-Young Whang, Wook-Shin Han, and Min-Soo Kim", "title": "Structural Consistency: Enabling XML Keyword Search to Eliminate\n  Spurious Results Consistently", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  XML keyword search is a user-friendly way to query XML data using only\nkeywords. In XML keyword search, to achieve high precision without sacrificing\nrecall, it is important to remove spurious results not intended by the user.\nEfforts to eliminate spurious results have enjoyed some success by using the\nconcepts of LCA or its variants, SLCA and MLCA. However, existing methods still\ncould find many spurious results. The fundamental cause for the occurrence of\nspurious results is that the existing methods try to eliminate spurious results\nlocally without global examination of all the query results and, accordingly,\nsome spurious results are not consistently eliminated. In this paper, we\npropose a novel keyword search method that removes spurious results\nconsistently by exploiting the new concept of structural consistency.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2009 06:45:37 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2009 01:00:10 GMT"}], "update_date": "2009-11-24", "authors_parsed": [["Lee", "Ki-Hoon", ""], ["Whang", "Kyu-Young", ""], ["Han", "Wook-Shin", ""], ["Kim", "Min-Soo", ""]]}, {"id": "0911.5708", "submitter": "Benjamin Rubinstein", "authors": "Benjamin I. P. Rubinstein, Peter L. Bartlett, Ling Huang, Nina Taft", "title": "Learning in a Large Function Space: Privacy-Preserving Mechanisms for\n  SVM Learning", "comments": "21 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several recent studies in privacy-preserving learning have considered the\ntrade-off between utility or risk and the level of differential privacy\nguaranteed by mechanisms for statistical query processing. In this paper we\nstudy this trade-off in private Support Vector Machine (SVM) learning. We\npresent two efficient mechanisms, one for the case of finite-dimensional\nfeature mappings and one for potentially infinite-dimensional feature mappings\nwith translation-invariant kernels. For the case of translation-invariant\nkernels, the proposed mechanism minimizes regularized empirical risk in a\nrandom Reproducing Kernel Hilbert Space whose kernel uniformly approximates the\ndesired kernel with high probability. This technique, borrowed from large-scale\nlearning, allows the mechanism to respond with a finite encoding of the\nclassifier, even when the function class is of infinite VC dimension.\nDifferential privacy is established using a proof technique from algorithmic\nstability. Utility--the mechanism's response function is pointwise\nepsilon-close to non-private SVM with probability 1-delta--is proven by\nappealing to the smoothness of regularized empirical risk minimization with\nrespect to small perturbations to the feature mapping. We conclude with a lower\nbound on the optimal differential privacy of the SVM. This negative result\nstates that for any delta, no mechanism can be simultaneously\n(epsilon,delta)-useful and beta-differentially private for small epsilon and\nsmall beta.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2009 20:34:45 GMT"}], "update_date": "2009-12-01", "authors_parsed": [["Rubinstein", "Benjamin I. P.", ""], ["Bartlett", "Peter L.", ""], ["Huang", "Ling", ""], ["Taft", "Nina", ""]]}]