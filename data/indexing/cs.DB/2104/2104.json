[{"id": "2104.00087", "submitter": "Chinmay Soman", "authors": "Yupeng Fu, Chinmay Soman", "title": "Real-time Data Infrastructure at Uber", "comments": "To be published in Proceedings of the 2021 International Conference\n  on Management of Data (SIGMOD '21)", "journal-ref": null, "doi": "10.1145/3448016.3457552", "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uber's business is highly real-time in nature. PBs of data is continuously\nbeing collected from the end users such as Uber drivers, riders, restaurants,\neaters and so on everyday. There is a lot of valuable information to be\nprocessed and many decisions must be made in seconds for a variety of use cases\nsuch as customer incentives, fraud detection, machine learning model\nprediction. In addition, there is an increasing need to expose this ability to\ndifferent user categories, including engineers, data scientists, executives and\noperations personnel which adds to the complexity. In this paper, we present\nthe overall architecture of the real-time data infrastructure and identify\nthree scaling challenges that we need to continuously address for each\ncomponent in the architecture. At Uber, we heavily rely on open source\ntechnologies for the key areas of the infrastructure. On top of those\nopen-source software, we add significant improvements and customizations to\nmake the open-source solutions fit in Uber's environment and bridge the gaps to\nmeet Uber's unique scale and requirements. We then highlight several important\nuse cases and show their real-time solutions and tradeoffs. Finally, we reflect\non the lessons we learned as we built, operated and scaled these systems.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 20:02:41 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Fu", "Yupeng", ""], ["Soman", "Chinmay", ""]]}, {"id": "2104.00501", "submitter": "Alexander Renz-Wieland", "authors": "Alexander Renz-Wieland, Rainer Gemulla, Zoi Kaoudi, Volker Markl", "title": "Replicate or Relocate? Non-Uniform Access in Parameter Servers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Parameter servers (PSs) facilitate the implementation of distributed training\nfor large machine learning tasks. A key challenge for PS performance is that\nparameter access is non-uniform in many real-world machine learning tasks,\ni.e., different parameters exhibit drastically different access patterns. We\nidentify skew and nondeterminism as two major sources for non-uniformity.\nExisting PSs are ill-suited for managing such non-uniform access because they\nuniformly apply the same parameter management technique to all parameters. As\nconsequence, the performance of existing PSs is negatively affected and may\neven fall behind that of single node baselines. In this paper, we explore how\nPSs can manage non-uniform access efficiently. We find that it is key for PSs\nto support multiple management techniques and to leverage a well-suited\nmanagement technique for each parameter. We present Lapse2, a PS that\nreplicates hot spot parameters, relocates less frequently accessed parameters,\nand employs specialized techniques to manage nondeterminism that arises from\nrandom sampling. In our experimental study, Lapse2 outperformed existing,\nsingle-technique PSs by up to one order of magnitude and provided near-linear\nscalability across multiple machine learning tasks.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 14:52:32 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Renz-Wieland", "Alexander", ""], ["Gemulla", "Rainer", ""], ["Kaoudi", "Zoi", ""], ["Markl", "Volker", ""]]}, {"id": "2104.00841", "submitter": "Jinglin Peng", "authors": "Jinglin Peng, Weiyuan Wu, Brandon Lockhart, Song Bian, Jing Nathan\n  Yan, Linghao Xu, Zhixuan Chi, Jeffrey Rzeszotarski, Jiannan Wang", "title": "DataPrep.EDA: Task-Centric Exploratory Data Analysis for Statistical\n  Modeling in Python", "comments": null, "journal-ref": null, "doi": "10.1145/3448016.3457330", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploratory Data Analysis (EDA) is a crucial step in any data science\nproject. However, existing Python libraries fall short in supporting data\nscientists to complete common EDA tasks for statistical modeling. Their API\ndesign is either too low level, which is optimized for plotting rather than\nEDA, or too high level, which is hard to specify more fine-grained EDA tasks.\nIn response, we propose DataPrep.EDA, a novel task-centric EDA system in\nPython. DataPrep.EDA allows data scientists to declaratively specify a wide\nrange of EDA tasks in different granularity with a single function call. We\nidentify a number of challenges to implement DataPrep.EDA, and propose\neffective solutions to improve the scalability, usability, customizability of\nthe system. In particular, we discuss some lessons learned from using Dask to\nbuild the data processing pipelines for EDA tasks and describe our approaches\nto accelerate the pipelines. We conduct extensive experiments to compare\nDataPrep.EDA with Pandas-profiling, the state-of-the-art EDA system in Python.\nThe experiments show that DataPrep.EDA significantly outperforms\nPandas-profiling in terms of both speed and user experience. DataPrep.EDA is\nopen-sourced as an EDA component of DataPrep:\nhttps://github.com/sfu-db/dataprep.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 01:24:10 GMT"}, {"version": "v2", "created": "Sat, 10 Apr 2021 07:03:16 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Peng", "Jinglin", ""], ["Wu", "Weiyuan", ""], ["Lockhart", "Brandon", ""], ["Bian", "Song", ""], ["Yan", "Jing Nathan", ""], ["Xu", "Linghao", ""], ["Chi", "Zhixuan", ""], ["Rzeszotarski", "Jeffrey", ""], ["Wang", "Jiannan", ""]]}, {"id": "2104.00886", "submitter": "Seunghwan Min", "authors": "Seunghwan Min, Sung Gwan Park, Kunsoo Park, Dora Giammarresi, Giuseppe\n  F. Italiano, Wook-Shin Han", "title": "Symmetric Continuous Subgraph Matching with Bidirectional Dynamic\n  Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real datasets such as social media streams and cyber data sources,\ngraphs change over time through a graph update stream of edge insertions and\ndeletions. Detecting critical patterns in such dynamic graphs plays an\nimportant role in various application domains such as fraud detection, cyber\nsecurity, and recommendation systems for social networks. Given a dynamic data\ngraph and a query graph, the continuous subgraph matching problem is to find\nall positive matches for each edge insertion and all negative matches for each\nedge deletion. The state-of-the-art algorithm TurboFlux uses a spanning tree of\na query graph for filtering. However, using the spanning tree may have a low\npruning power because it does not take into account all edges of the query\ngraph. In this paper, we present a symmetric and much faster algorithm SymBi\nwhich maintains an auxiliary data structure based on a directed acyclic graph\ninstead of a spanning tree, which maintains the intermediate results of\nbidirectional dynamic programming between the query graph and the dynamic\ngraph. Extensive experiments with real and synthetic datasets show that SymBi\noutperforms the state-of-the-art algorithm by up to three orders of magnitude\nin terms of the elapsed time.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 05:13:13 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Min", "Seunghwan", ""], ["Park", "Sung Gwan", ""], ["Park", "Kunsoo", ""], ["Giammarresi", "Dora", ""], ["Italiano", "Giuseppe F.", ""], ["Han", "Wook-Shin", ""]]}, {"id": "2104.00984", "submitter": "Umair Qudus", "authors": "Umair Qudus, Muhammad Saleem, Axel-Cyrille Ngonga Ngomo, Young-koo Lee", "title": "An Empirical Evaluation of Cost-based Federated SPARQL Query Processing\n  Engines", "comments": "24 pages, Semantic Web, 2020, #article", "journal-ref": "Semantic Web 2020", "doi": null, "report-no": null, "categories": "cs.DB cs.LG cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Finding a good query plan is key to the optimization of query runtime. This\nholds in particular for cost-based federation engines, which make use of\ncardinality estimations to achieve this goal. A number of studies compare\nSPARQL federation engines across different performance metrics, including query\nruntime, result set completeness and correctness, number of sources selected\nand number of requests sent. Albeit informative, these metrics are generic and\nunable to quantify and evaluate the accuracy of the cardinality estimators of\ncost-based federation engines. To thoroughly evaluate cost-based federation\nengines, the effect of estimated cardinality errors on the overall query\nruntime performance must be measured. In this paper, we address this challenge\nby presenting novel evaluation metrics targeted at a fine-grained benchmarking\nof cost-based federated SPARQL query engines. We evaluate five cost-based\nfederated SPARQL query engines using existing as well as novel evaluation\nmetrics by using LargeRDFBench queries. Our results provide a detailed analysis\nof the experimental outcomes that reveal novel insights, useful for the\ndevelopment of future cost-based federated SPARQL query processing engines.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 11:01:25 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Qudus", "Umair", ""], ["Saleem", "Muhammad", ""], ["Ngomo", "Axel-Cyrille Ngonga", ""], ["Lee", "Young-koo", ""]]}, {"id": "2104.01126", "submitter": "Yiqiu Wang", "authors": "Yiqiu Wang, Shangdi Yu, Yan Gu, Julian Shun", "title": "Fast Parallel Algorithms for Euclidean Minimum Spanning Tree and\n  Hierarchical Spatial Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents new parallel algorithms for generating Euclidean minimum\nspanning trees and spatial clustering hierarchies (known as HDBSCAN$^*$). Our\napproach is based on generating a well-separated pair decomposition followed by\nusing Kruskal's minimum spanning tree algorithm and bichromatic closest pair\ncomputations. We introduce a new notion of well-separation to reduce the work\nand space of our algorithm for HDBSCAN$^*$. We also present a parallel\napproximate algorithm for OPTICS based on a recent sequential algorithm by Gan\nand Tao. Finally, we give a new parallel divide-and-conquer algorithm for\ncomputing the dendrogram and reachability plots, which are used in visualizing\nclusters of different scale that arise for both EMST and HDBSCAN$^*$. We show\nthat our algorithms are theoretically efficient: they have work (number of\noperations) matching their sequential counterparts, and polylogarithmic depth\n(parallel time).\n  We implement our algorithms and propose a memory optimization that requires\nonly a subset of well-separated pairs to be computed and materialized, leading\nto savings in both space (up to 10x) and time (up to 8x). Our experiments on\nlarge real-world and synthetic data sets using a 48-core machine show that our\nfastest algorithms outperform the best serial algorithms for the problems by\n11.13--55.89x, and existing parallel algorithms by at least an order of\nmagnitude.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 16:05:00 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Wang", "Yiqiu", ""], ["Yu", "Shangdi", ""], ["Gu", "Yan", ""], ["Shun", "Julian", ""]]}, {"id": "2104.01241", "submitter": "Darshana Balakrishnan", "authors": "Darshana Balakrishnan, Carl Nuessle, Oliver Kennedy, Lukasz Ziarek", "title": "TreeToaster: Towards an IVM-Optimized Compiler", "comments": "23 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A compiler's optimizer operates over abstract syntax trees (ASTs),\ncontinuously applying rewrite rules to replace subtrees of the AST with more\nefficient ones. Especially on large source repositories, even simply finding\nopportunities for a rewrite can be expensive, as optimizer traverses the AST\nnaively. In this paper, we leverage the need to repeatedly find rewrites, and\nexplore options for making the search faster through indexing and incremental\nview maintenance (IVM). Concretely, we consider bolt-on approaches that make\nuse of embedded IVM systems like DBToaster, as well as two new approaches:\nLabel-indexing and TreeToaster, an AST-specialized form of IVM. We integrate\nthese approaches into an existing just-in-time data structure compiler and show\nexperimentally that TreeToaster can significantly improve performance with\nminimal memory overheads.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 21:13:37 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Balakrishnan", "Darshana", ""], ["Nuessle", "Carl", ""], ["Kennedy", "Oliver", ""], ["Ziarek", "Lukasz", ""]]}, {"id": "2104.01305", "submitter": "Haoyu Huang", "authors": "Haoyu Huang and Shahram Ghandeharizadeh", "title": "Nova-LSM: A Distributed, Component-based LSM-tree Key-value Store", "comments": null, "journal-ref": null, "doi": "10.1145/3448016.3457297", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The cloud infrastructure motivates disaggregation of monolithic data stores\ninto components that are assembled together based on an application's workload.\nThis study investigates disaggregation of an LSM-tree key-value store into\ncomponents that communicate using RDMA. These components separate storage from\nprocessing, enabling processing components to share storage bandwidth and\nspace. The processing components scatter blocks of a file (SSTable) across an\narbitrary number of storage components and balance load across them using\npower-of-d. They construct ranges dynamically at runtime to parallelize\ncompaction and enhance performance. Each component has configuration knobs that\ncontrol its scalability. The resulting component-based system, Nova-LSM, is\nelastic. It outperforms its monolithic counterparts, both LevelDB and RocksDB,\nby several orders of magnitude with workloads that exhibit a skewed pattern of\naccess to data.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 03:38:41 GMT"}, {"version": "v2", "created": "Sat, 1 May 2021 17:49:46 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Huang", "Haoyu", ""], ["Ghandeharizadeh", "Shahram", ""]]}, {"id": "2104.01488", "submitter": "Chuan Lei", "authors": "Alina Vretinaris, Chuan Lei, Vasilis Efthymiou, Xiao Qin, Fatma\n  \\\"Ozcan", "title": "Medical Entity Disambiguation Using Graph Neural Networks", "comments": "To appear in SIGMOD 2021", "journal-ref": null, "doi": "10.1145/3448016.3457328", "report-no": null, "categories": "cs.IR cs.CL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical knowledge bases (KBs), distilled from biomedical literature and\nregulatory actions, are expected to provide high-quality information to\nfacilitate clinical decision making. Entity disambiguation (also referred to as\nentity linking) is considered as an essential task in unlocking the wealth of\nsuch medical KBs. However, existing medical entity disambiguation methods are\nnot adequate due to word discrepancies between the entities in the KB and the\ntext snippets in the source documents. Recently, graph neural networks (GNNs)\nhave proven to be very effective and provide state-of-the-art results for many\nreal-world applications with graph-structured data. In this paper, we introduce\nED-GNN based on three representative GNNs (GraphSAGE, R-GCN, and MAGNN) for\nmedical entity disambiguation. We develop two optimization techniques to\nfine-tune and improve ED-GNN. First, we introduce a novel strategy to represent\nentities that are mentioned in text snippets as a query graph. Second, we\ndesign an effective negative sampling strategy that identifies hard negative\nsamples to improve the model's disambiguation capability. Compared to the best\nperforming state-of-the-art solutions, our ED-GNN offers an average improvement\nof 7.3% in terms of F1 score on five real-world datasets.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 22:04:15 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Vretinaris", "Alina", ""], ["Lei", "Chuan", ""], ["Efthymiou", "Vasilis", ""], ["Qin", "Xiao", ""], ["\u00d6zcan", "Fatma", ""]]}, {"id": "2104.01671", "submitter": "Jaeho Bang", "authors": "Jaeho Bang, Pramod Chunduri, Joy Arulraj", "title": "EKO: Adaptive Sampling of Compressed Video Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers have presented systems for efficiently analysing video data at\nscale using sampling algorithms. While these systems effectively leverage the\ntemporal redundancy present in videos, they suffer from three limitations.\nFirst, they use traditional video storage formats are tailored for human\nconsumption. Second, they load and decode the entire compressed video in memory\nbefore applying the sampling algorithm. Third, the sampling algorithms often\nrequire labeled training data obtained using a specific deep learning model.\nThese limitations lead to lower accuracy, higher query execution time, and\nlarger memory footprint. In this paper, we present EKO, a storage engine for\nefficiently managing video data. EKO relies on two optimizations. First, it\nuses a novel unsupervised, adaptive sampling algorithm for identifying the key\nframes in a given video. Second, it stores the identified key frames in a\ncompressed representation that is optimized for machine consumption. We show\nthat EKO improves F1-score by up to 9% compared to the next best performing\nstate-of-the-art unsupervised, sampling algorithms by selecting more\nrepresentative frames. It reduces query execution time by 3X and memory\nfootprint by 10X in comparison to a widely-used, traditional video storage\nformat.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 19:28:45 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Bang", "Jaeho", ""], ["Chunduri", "Pramod", ""], ["Arulraj", "Joy", ""]]}, {"id": "2104.01744", "submitter": "Immanuel Trummer Mr.", "authors": "Junxiong Wang and Immanuel Trummer and Debabrota Basu", "title": "UDO: Universal Database Optimization using Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  UDO is a versatile tool for offline tuning of database systems for specific\nworkloads. UDO can consider a variety of tuning choices, reaching from picking\ntransaction code variants over index selections up to database system parameter\ntuning. UDO uses reinforcement learning to converge to near-optimal\nconfigurations, creating and evaluating different configurations via actual\nquery executions (instead of relying on simplifying cost models). To cater to\ndifferent parameter types, UDO distinguishes heavy parameters (which are\nexpensive to change, e.g. physical design parameters) from light parameters.\nSpecifically for optimizing heavy parameters, UDO uses reinforcement learning\nalgorithms that allow delaying the point at which the reward feedback becomes\navailable. This gives us the freedom to optimize the point in time and the\norder in which different configurations are created and evaluated (by\nbenchmarking a workload sample). UDO uses a cost-based planner to minimize\nreconfiguration overheads. For instance, it aims to amortize the creation of\nexpensive data structures by consecutively evaluating configurations using\nthem. We evaluate UDO on Postgres as well as MySQL and on TPC-H as well as\nTPC-C, optimizing a variety of light and heavy parameters concurrently.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 02:40:38 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Wang", "Junxiong", ""], ["Trummer", "Immanuel", ""], ["Basu", "Debabrota", ""]]}, {"id": "2104.01785", "submitter": "Yoshihiko Suhara", "authors": "Yoshihiko Suhara, Jinfeng Li, Yuliang Li, Dan Zhang, \\c{C}a\\u{g}atay\n  Demiralp, Chen Chen, Wang-Chiew Tan", "title": "Annotating Columns with Pre-trained Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring meta information about tables, such as column headers or\nrelationships between columns, is an active research topic in data management\nas we find many tables are missing some of this information. In this paper, we\nstudy the problem of annotating table columns (i.e., predicting column types\nand the relationships between columns) using only information from the table\nitself. We show that a multi-task learning approach (called Doduo), trained\nusing pre-trained language models on both tasks outperforms individual learning\napproaches. Experimental results show that Doduo establishes new\nstate-of-the-art performance on two benchmarks for the column type prediction\nand column relation prediction tasks with up to 4.0% and 11.9% improvements,\nrespectively. We also establish that Doduo can already perform the previous\nstate-of-the-art performance with a minimal number of tokens, only 8 tokens per\ncolumn.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 06:05:01 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Suhara", "Yoshihiko", ""], ["Li", "Jinfeng", ""], ["Li", "Yuliang", ""], ["Zhang", "Dan", ""], ["Demiralp", "\u00c7a\u011fatay", ""], ["Chen", "Chen", ""], ["Tan", "Wang-Chiew", ""]]}, {"id": "2104.02158", "submitter": "Yuta Nakamura", "authors": "Yuta Nakamura, Raza Ahmad, Tanu Malik", "title": "Content-defined Merkle Trees for Efficient Container Delivery", "comments": "Published on HiPC 2020 India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Containerization simplifies the sharing and deployment of applications when\nenvironments change in the software delivery chain. To deploy an application,\ncontainer delivery methods push and pull container images. These methods\noperate on file and layer (set of files) granularity, and introduce redundant\ndata within a container. Several container operations such as upgrading,\ninstalling, and maintaining become inefficient, because of copying and\nprovisioning of redundant data. In this paper, we reestablish recent results\nthat block-level deduplication reduces the size of individual containers, by\nverifying the result using content-defined chunking. Block-level deduplication,\nhowever, does not improve the efficiency of push/pull operations which must\ndetermine the specific blocks to transfer. We introduce a content-defined\nMerkle Tree (\\CDMT{}) over deduplicated storage in a container. \\CDMT{} indexes\ndeduplicated blocks and determines changes to blocks in logarithmic time on the\nclient. \\CDMT{} efficiently pushes and pulls container images from a registry,\nespecially as containers are upgraded and (re-)provisioned on a client. We also\ndescribe how a registry can efficiently maintain the \\CDMT{} index as new image\nversions are pushed. We show the scalability of \\CDMT{} over Merkle Trees in\nterms of disk and network I/O savings using 15 container images and 233 image\nversions from Docker Hub.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 21:13:07 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Nakamura", "Yuta", ""], ["Ahmad", "Raza", ""], ["Malik", "Tanu", ""]]}, {"id": "2104.02234", "submitter": "Dong He", "authors": "Dong He, Maureen Daum, Walter Cai, Magdalena Balazinska", "title": "DeepEverest: Accelerating Declarative Top-K Queries for Deep Neural\n  Network Interpretation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design, implement, and evaluate DeepEverest, a system for the efficient\nexecution of interpretation by example queries over the activation values of a\ndeep neural network. DeepEverest consists of an efficient indexing technique\nand a query execution algorithm with various optimizations. Experiments with\nour prototype implementation show that DeepEverest, using less than 20% of the\nstorage of full materialization, significantly accelerates individual queries\nby up to 63x and consistently outperforms other methods on multi-query\nworkloads that simulate DNN interpretation processes.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 01:56:09 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 23:15:16 GMT"}, {"version": "v3", "created": "Mon, 19 Jul 2021 04:51:10 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["He", "Dong", ""], ["Daum", "Maureen", ""], ["Cai", "Walter", ""], ["Balazinska", "Magdalena", ""]]}, {"id": "2104.03018", "submitter": "Peter Christen", "authors": "Sirintra Vaiwsri, Thilina Ranbaduge, Peter Christen, and Kee Siong Ng", "title": "Accurate and Efficient Suffix Tree Based Privacy-Preserving String\n  Matching", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of calculating similarities between strings held by different\norganizations without revealing these strings is an increasingly important\nproblem in areas such as health informatics, national censuses, genomics, and\nfraud detection. Most existing privacy-preserving string comparison functions\nare either based on comparing sets of encoded character q-grams, allow only\nexact matching of encrypted strings, or they are aimed at long genomic\nsequences that have a small alphabet. The set-based privacy-preserving\nsimilarity functions commonly used to compare name and address strings in the\ncontext of privacy-preserving record linkage do not take the positions of\nsub-strings into account. As a result, two very different strings can\npotentially be considered as an exact match leading to wrongly linked records.\nExisting set-based techniques also cannot identify the length of the longest\ncommon sub-string across two strings. In this paper we propose a novel approach\nfor accurate and efficient privacy-preserving string matching based on suffix\ntrees that are encoded using chained hashing. We incorporate a hashing based\nencoding technique upon the encoded suffixes to improve privacy against\nfrequency attacks such as those exploiting Benford's law. Our approach allows\nvarious operations to be performed without the strings to be compared being\nrevealed: the length of the longest common sub-string, do two strings have the\nsame beginning, middle or end, and the longest common sub-string similarity\nbetween two strings. These functions allow a more accurate comparison of, for\nexample, bank account, credit card, or telephone numbers, which cannot be\ncompared appropriately with existing privacy-preserving string matching\ntechniques. Our evaluation on several data sets with different types of strings\nvalidates the privacy and accuracy of our proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 09:45:22 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Vaiwsri", "Sirintra", ""], ["Ranbaduge", "Thilina", ""], ["Christen", "Peter", ""], ["Ng", "Kee Siong", ""]]}, {"id": "2104.03187", "submitter": "Pierangelo Di Sanzo", "authors": "Pierangelo Di Sanzo", "title": "A Preliminary Proposal for an Analytical Model for Evaluating the Impact\n  on Performance of Data Access Patterns in Transaction Execution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DB cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a preliminary proposal for an analytical model for evaluating the\nimpact on performance of data access patterns in concurrent transaction\nexecution. We consider the case of concurrency control protocols that use\nlocking to ensure isolation in the execution of transactions. We analyse\nscenarios where transactions access one or more sets of data items in the same\norder or in different order.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 15:29:12 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 17:42:19 GMT"}, {"version": "v3", "created": "Mon, 12 Apr 2021 10:49:48 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Di Sanzo", "Pierangelo", ""]]}, {"id": "2104.03224", "submitter": "Michael Kaufmann", "authors": "Michael Kaufmann, Gabriel Stechschulte, Anna Huber", "title": "Efficient and Accurate In-Database Machine Learning with SQL Code\n  Generation in Python", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Following an analysis of the advantages of SQL-based Machine Learning (ML)\nand a short literature survey of the field, we describe a novel method for\nIn-Database Machine Learning (IDBML). We contribute a process for SQL-code\ngeneration in Python using template macros in Jinja2 as well as the prototype\nimplementation of the process. We describe our implementation of the process to\ncompute multidimensional histogram (MDH) probability estimation in SQL. For\nthis, we contribute and implement a novel discretization method called equal\nquantized rank binning (EQRB) and equal-width binning (EWB). Based on this, we\nprovide data gathered in a benchmarking experiment for the quantitative\nempirical evaluation of our method and system using the Covertype dataset. We\nmeasured accuracy and computation time and compared it to Scikit Learn state of\nthe art classification algorithms. Using EWB, our multidimensional probability\nestimation was the fastest of all tested algorithms, while being only 1-2% less\naccurate than the best state of the art methods found (decision trees and\nrandom forests). Our method was significantly more accurate than Naive Bayes,\nwhich assumes independent one-dimensional probabilities and/or densities. Also,\nour method was significantly more accurate and faster than logistic regression.\nThis motivates for further research in accuracy improvement and in IDBML with\nSQL code generation for big data and larger-than-memory datasets.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 16:23:19 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 14:43:37 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Kaufmann", "Michael", ""], ["Stechschulte", "Gabriel", ""], ["Huber", "Anna", ""]]}, {"id": "2104.03353", "submitter": "A\\'ecio Solano Rodrigues Santos", "authors": "A\\'ecio Santos, Aline Bessa, Fernando Chirigati, Christopher Musco,\n  Juliana Freire", "title": "Correlation Sketches for Approximate Join-Correlation Queries", "comments": "Proceedings of the 2021 International Conference on Management of\n  Data (SIGMOD '21)", "journal-ref": null, "doi": "10.1145/3448016.3458456", "report-no": null, "categories": "cs.DB cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing availability of structured datasets, from Web tables and\nopen-data portals to enterprise data, opens up opportunities~to enrich\nanalytics and improve machine learning models through relational data\naugmentation. In this paper, we introduce a new class of data augmentation\nqueries: join-correlation queries. Given a column $Q$ and a join column $K_Q$\nfrom a query table $\\mathcal{T}_Q$, retrieve tables $\\mathcal{T}_X$ in a\ndataset collection such that $\\mathcal{T}_X$ is joinable with $\\mathcal{T}_Q$\non $K_Q$ and there is a column $C \\in \\mathcal{T}_X$ such that $Q$ is\ncorrelated with $C$. A na\\\"ive approach to evaluate these queries, which first\nfinds joinable tables and then explicitly joins and computes correlations\nbetween $Q$ and all columns of the discovered tables, is prohibitively\nexpensive. To efficiently support correlated column discovery, we 1) propose a\nsketching method that enables the construction of an index for a large number\nof tables and that provides accurate estimates for join-correlation queries,\nand 2) explore different scoring strategies that effectively rank the query\nresults based on how well the columns are correlated with the query. We carry\nout a detailed experimental evaluation, using both synthetic and real data,\nwhich shows that our sketches attain high accuracy and the scoring strategies\nlead to high-quality rankings.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 19:08:14 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Santos", "A\u00e9cio", ""], ["Bessa", "Aline", ""], ["Chirigati", "Fernando", ""], ["Musco", "Christopher", ""], ["Freire", "Juliana", ""]]}, {"id": "2104.03354", "submitter": "Shantanu Sharma", "authors": "Yin Li, Dhrubajyoti Ghosh, Peeyush Gupta, Sharad Mehrotra, Nisha\n  Panwar, Shantanu Sharma", "title": "Prism: Private Verifiable Set Computation over Multi-Owner Outsourced\n  Databases", "comments": "This paper has been accepted in ACM SIGMOD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR cs.DC cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes Prism, a secret sharing based approach to compute private\nset operations (i.e., intersection and union), as well as aggregates over\noutsourced databases belonging to multiple owners. Prism enables data owners to\npre-load the data onto non-colluding servers and exploits the additive and\nmultiplicative properties of secret-shares to compute the above-listed\noperations in (at most) two rounds of communication between the servers\n(storing the secret-shares) and the querier, resulting in a very efficient\nimplementation. Also, Prism does not require communication among the servers\nand supports result verification techniques for each operation to detect\nmalicious adversaries. Experimental results show that Prism scales both in\nterms of the number of data owners and database sizes, to which prior\napproaches do not scale.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 19:08:15 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Li", "Yin", ""], ["Ghosh", "Dhrubajyoti", ""], ["Gupta", "Peeyush", ""], ["Mehrotra", "Sharad", ""], ["Panwar", "Nisha", ""], ["Sharma", "Shantanu", ""]]}, {"id": "2104.03583", "submitter": "Yuli Jiang", "authors": "Yuli Jiang, Yu Rong, Hong Cheng, Xin Huang, Kangfei Zhao, Junzhou\n  Huang", "title": "QD-GCN: Query-Driven Graph Convolutional Networks for Attributed\n  Community Search", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, attributed community search, a related but different problem to\ncommunity detection and graph clustering, has been widely studied in the\nliterature. Compared with the community detection that finds all existing\nstatic communities from a graph, the attributed community search (ACS) is more\nchallenging since it aims to find dynamic communities with both cohesive\nstructures and homogeneous node attributes given arbitrary queries. To solve\nthe ACS problem, the most popular paradigm is to simplify the problem as two\nsub-problems, that is, structural matching and attribute filtering and deal\nwith them separately. However, in real-world graphs, the community structure\nand the node attributes are actually correlated to each other. In this vein,\ncurrent studies cannot capture these correlations which are vital for the ACS\nproblem.\n  In this paper, we propose Query-Driven Graph Convolutional Networks (QD-GCN),\nan end-to-end framework that unifies the community structure as well as node\nattribute to solve the ACS problem. In particular, QD-GCN leverages the Graph\nConvolutional Networks, which is a powerful tool to encode the graph topology\nand node attributes concurrently, as the backbones to extract the\nquery-dependent community information from the original graph. By utilizing\nthis query-dependent community information, QD-GCN is able to predict the\ntarget community given any queries. Experiments on real-world graphs with\nground-truth communities demonstrate that QD-GCN outperforms existing\nattributed community search algorithms in terms of both efficiency and\neffectiveness.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 07:52:48 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Jiang", "Yuli", ""], ["Rong", "Yu", ""], ["Cheng", "Hong", ""], ["Huang", "Xin", ""], ["Zhao", "Kangfei", ""], ["Huang", "Junzhou", ""]]}, {"id": "2104.03906", "submitter": "David Leslie", "authors": "David Leslie and Morgan Briggs", "title": "Explaining decisions made with AI: A workbook (Use case 1: AI-assisted\n  recruitment tool)", "comments": null, "journal-ref": null, "doi": "10.5281/zenodo.4624711", "report-no": null, "categories": "cs.CY cs.DB cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over the last two years, The Alan Turing Institute and the Information\nCommissioner's Office (ICO) have been working together to discover ways to\ntackle the difficult issues surrounding explainable AI. The ultimate product of\nthis joint endeavour, Explaining decisions made with AI, published in May 2020,\nis the most comprehensive practical guidance on AI explanation produced\nanywhere to date. We have put together this workbook to help support the uptake\nof that guidance. The goal of the workbook is to summarise some of main themes\nfrom Explaining decisions made with AI and then to provide the materials for a\nworkshop exercise that has been built around a use case created to help you\ngain a flavour of how to put the guidance into practice. In the first three\nsections, we run through the basics of Explaining decisions made with AI. We\nprovide a precis of the four principles of AI explainability, the typology of\nAI explanations, and the tasks involved in the explanation-aware design,\ndevelopment, and use of AI/ML systems. We then provide some reflection\nquestions, which are intended to be a launching pad for group discussion, and a\nstarting point for the case-study-based exercise that we have included as\nAppendix B. In Appendix A, we go into more detailed suggestions about how to\norganise the workshop. These recommendations are based on two workshops we had\nthe privilege of co-hosting with our colleagues from the ICO and Manchester\nMetropolitan University in January 2021. The participants of these workshops\ncame from both the private and public sectors, and we are extremely grateful to\nthem for their energy, enthusiasm, and tremendous insight. This workbook would\nsimply not exist without the commitment and keenness of all our collaborators\nand workshop participants.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 17:03:50 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Leslie", "David", ""], ["Briggs", "Morgan", ""]]}, {"id": "2104.03986", "submitter": "Arjit Jain", "authors": "Arjit Jain, Sunita Sarawagi, Prithviraj Sen", "title": "Deep Indexed Active Learning for Matching Heterogeneous Entity\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given two large lists of records, the task in entity resolution (ER) is to\nfind the pairs from the Cartesian product of the lists that correspond to the\nsame real world entity. Typically, passive learning methods on tasks like ER\nrequire large amounts of labeled data to yield useful models. Active Learning\nis a promising approach for ER in low resource settings. However, the search\nspace, to find informative samples for the user to label, grows quadratically\nfor instance-pair tasks making active learning hard to scale. Previous works,\nin this setting, rely on hand-crafted predicates, pre-trained language model\nembeddings, or rule learning to prune away unlikely pairs from the Cartesian\nproduct. This blocking step can miss out on important regions in the product\nspace leading to low recall. We propose DIAL, a scalable active learning\napproach that jointly learns embeddings to maximize recall for blocking and\naccuracy for matching blocked pairs. DIAL uses an Index-By-Committee framework,\nwhere each committee member learns representations based on powerful\ntransformer models. We highlight surprising differences between the matcher and\nthe blocker in the creation of the training data and the objective used to\ntrain their parameters. Experiments on five benchmark datasets and a\nmultilingual record matching dataset show the effectiveness of our approach in\nterms of precision, recall and running time. Code is available at\nhttps://github.com/ArjitJ/DIAL\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 18:00:19 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Jain", "Arjit", ""], ["Sarawagi", "Sunita", ""], ["Sen", "Prithviraj", ""]]}, {"id": "2104.04194", "submitter": "Srividya Subramanian", "authors": "Sihem Amer-Yahia (2), Georgia Koutrika (1), Frederic Bastian (7),\n  Theofilos Belmpas (1), Martin Braschler (9), Ursin Brunner (9), Diego\n  Calvanese (8), Maximilian Fabricius (5), Orest Gkini (1), Catherine Kosten\n  (9), Davide Lanti (8), Antonis Litke (6), Hendrik L\\\"ucke-Tieke (3),\n  Francesco Alessandro Massucci (6), Tarcisio Mendes de Farias (7), Alessandro\n  Mosca (8), Francesco Multari (6), Nikolaos Papadakis (4), Dimitris\n  Papadopoulos (4), Yogendra Patil (2), Aur\\'elien Personnaz (2), Guillem Rull\n  (6), Ana Sima (7), Ellery Smith (9), Dimitrios Skoutas (1), Srividya\n  Subramanian (5), Guohui Xiao (8), Kurt Stockinger (9) ((1) Athena Research\n  Center, Greece, (2) CNRS, University Grenoble Alpes, France, (3) Fraunhofer\n  IGD, Germany, (4) Infili, Greece, (5) Max Planck Institute, Germany, (6)\n  SIRIS Academic, Spain, (7) SIB Swiss Institute of Bioinformatics,\n  Switzerland, (8) Free University of Bozen-Bolzano, Italy, (9) ZHAW Zurich\n  University of Applied Sciences, Switzerland)", "title": "INODE: Building an End-to-End Data Exploration System in Practice\n  [Extended Vision]", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A full-fledged data exploration system must combine different access\nmodalities with a powerful concept of guiding the user in the exploration\nprocess, by being reactive and anticipative both for data discovery and for\ndata linking. Such systems are a real opportunity for our community to cater to\nusers with different domain and data science expertise. We introduce INODE --\nan end-to-end data exploration system -- that leverages, on the one hand,\nMachine Learning and, on the other hand, semantics for the purpose of Data\nManagement (DM). Our vision is to develop a classic unified, comprehensive\nplatform that provides extensive access to open datasets, and we demonstrate it\nin three significant use cases in the fields of Cancer Biomarker Reearch,\nResearch and Innovation Policy Making, and Astrophysics. INODE offers\nsustainable services in (a) data modeling and linking, (b) integrated query\nprocessing using natural language, (c) guidance, and (d) data exploration\nthrough visualization, thus facilitating the user in discovering new insights.\nWe demonstrate that our system is uniquely accessible to a wide range of users\nfrom larger scientific communities to the public. Finally, we briefly\nillustrate how this work paves the way for new research opportunities in DM.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 05:04:04 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Amer-Yahia", "Sihem", ""], ["Koutrika", "Georgia", ""], ["Bastian", "Frederic", ""], ["Belmpas", "Theofilos", ""], ["Braschler", "Martin", ""], ["Brunner", "Ursin", ""], ["Calvanese", "Diego", ""], ["Fabricius", "Maximilian", ""], ["Gkini", "Orest", ""], ["Kosten", "Catherine", ""], ["Lanti", "Davide", ""], ["Litke", "Antonis", ""], ["L\u00fccke-Tieke", "Hendrik", ""], ["Massucci", "Francesco Alessandro", ""], ["de Farias", "Tarcisio Mendes", ""], ["Mosca", "Alessandro", ""], ["Multari", "Francesco", ""], ["Papadakis", "Nikolaos", ""], ["Papadopoulos", "Dimitris", ""], ["Patil", "Yogendra", ""], ["Personnaz", "Aur\u00e9lien", ""], ["Rull", "Guillem", ""], ["Sima", "Ana", ""], ["Smith", "Ellery", ""], ["Skoutas", "Dimitrios", ""], ["Subramanian", "Srividya", ""], ["Xiao", "Guohui", ""], ["Stockinger", "Kurt", ""]]}, {"id": "2104.04406", "submitter": "Yang Song", "authors": "Yang Song, Yu Gu, Rui Zhang, Ge Yu", "title": "ProMIPS: Efficient High-Dimensional c-Approximate Maximum Inner Product\n  Search with a Lightweight Index", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the wide applications in recommendation systems, multi-class label\nprediction and deep learning, the Maximum Inner Product (MIP) search problem\nhas received extensive attention in recent years. Faced with large-scale\ndatasets containing high-dimensional feature vectors, the state-of-the-art\nLSH-based methods usually require a large number of hash tables or long hash\ncodes to ensure the searching quality, which takes up lots of index space and\ncauses excessive disk page accesses. In this paper, we relax the guarantee of\naccuracy for efficiency and propose an efficient method for c-Approximate\nMaximum Inner Product (c-AMIP) search with a lightweight iDistance index. We\nproject high-dimensional points to low-dimensional ones via 2-stable random\nprojections and derive probability-guaranteed searching conditions, by which\nthe c-AMIP results can be guaranteed in accuracy with arbitrary probabilities.\nTo further improve the efficiency, we propose Quick-Probe for quickly\ndetermining the searching bound satisfying the derived condition in advance,\navoiding the inefficient incremental searching process. Extensive experimental\nevaluations on four real datasets demonstrate that our method requires less\npre-processing cost including index size and pre-processing time. In addition,\ncompared to the state-of-the-art benchmark methods, it provides superior\nresults on searching quality in terms of overall ratio and recall, and\nefficiency in terms of page access and running time.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 14:55:19 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Song", "Yang", ""], ["Gu", "Yu", ""], ["Zhang", "Rui", ""], ["Yu", "Ge", ""]]}, {"id": "2104.04659", "submitter": "Yeye He", "authors": "Jie Song, Yeye He", "title": "Auto-Validate: Unsupervised Data Validation Using Data-Domain Patterns\n  Inferred from Data Lakes", "comments": "full version of a SIGMOD 2021 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Complex data pipelines are increasingly common in diverse applications such\nas BI reporting and ML modeling. These pipelines often recur regularly (e.g.,\ndaily or weekly), as BI reports need to be refreshed, and ML models need to be\nretrained. However, it is widely reported that in complex production pipelines,\nupstream data feeds can change in unexpected ways, causing downstream\napplications to break silently that are expensive to resolve.\n  Data validation has thus become an important topic, as evidenced by notable\nrecent efforts from Google and Amazon, where the objective is to catch data\nquality issues early as they arise in the pipelines. Our experience on\nproduction data suggests, however, that on string-valued data, these existing\napproaches yield high false-positive rates and frequently require human\nintervention. In this work, we develop a corpus-driven approach to\nauto-validate \\emph{machine-generated data} by inferring suitable\ndata-validation \"patterns\" that accurately describe the underlying data domain,\nwhich minimizes false positives while maximizing data quality issues caught.\nEvaluations using production data from real data lakes suggest that\nAuto-Validate is substantially more effective than existing methods. Part of\nthis technology ships as an Auto-Tag feature in Microsoft Azure Purview.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 01:15:48 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 17:29:18 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Song", "Jie", ""], ["He", "Yeye", ""]]}, {"id": "2104.04728", "submitter": "Chuanhou Gao", "authors": "Qiuqiang Lin, Chuanhou Gao", "title": "Discovering Categorical Main and Interaction Effects Based on\n  Association Rule Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growing size of data sets, feature selection becomes increasingly\nimportant. Taking interactions of original features into consideration will\nlead to extremely high dimension, especially when the features are categorical\nand one-hot encoding is applied. This makes it more worthwhile mining useful\nfeatures as well as their interactions. Association rule mining aims to extract\ninteresting correlations between items, but it is difficult to use rules as a\nqualified classifier themselves. Drawing inspiration from association rule\nmining, we come up with a method that uses association rules to select features\nand their interactions, then modify the algorithm for several practical\nconcerns. We analyze the computational complexity of the proposed algorithm to\nshow its efficiency. And the results of a series of experiments verify the\neffectiveness of the algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 10:13:07 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Lin", "Qiuqiang", ""], ["Gao", "Chuanhou", ""]]}, {"id": "2104.04758", "submitter": "Sam Thompson", "authors": "Dominik D. Freydenberger, Sam M. Thompson", "title": "Splitting Spanner Atoms: A Tool for Acyclic Core Spanners", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper investigates regex CQs with string equalities (SERCQs), a subclass\nof core spanners. As shown by Freydenberger, Kimelfeld, and Peterfreund (PODS\n2018), these queries are intractable, even if restricted to acyclic queries.\nThis previous result defines acyclicity by treating regex formulas as atoms. In\ncontrast to this, we propose an alternative definition by converting SERCQs\ninto FC-CQs -- conjunctive queries in FC, a logic that is based on word\nequations. We introduce a way to decompose word equations of unbounded arity\ninto a conjunction of binary word equations. If the result of the decomposition\nis acyclic, then evaluation and enumeration of results become tractable. The\nmain result of this work is an algorithm that decides in polynomial time\nwhether an FC-CQ can be decomposed into an acyclic FC-CQ. We also give an\nefficient conversion from synchronized SERCQs to FC-CQs with regular\nconstraints. As a consequence, tractability results for acyclic relational CQs\ndirectly translate to a large class of SERCQs.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 13:15:17 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Freydenberger", "Dominik D.", ""], ["Thompson", "Sam M.", ""]]}, {"id": "2104.04902", "submitter": "Huan Hu", "authors": "Huan Hu and Jianzhong Li", "title": "Sublinear Time Nearest Neighbor Search over Generalized Weighted\n  Manhattan Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nearest Neighbor Search (NNS) over generalized weighted distance is\nfundamental to a wide range of applications. The problem of NNS over the\ngeneralized weighted Square Euclidean distance has been studied in previous\nwork. However, numerous studies have shown that the Manhattan distance could be\nmore practical than the Euclidean distance for high-dimensional NNS. To the\nbest of our knowledge, no prior work presents a sublinear time solution to the\nproblem of NNS over the generalized weighted Manhattan distance. In this paper,\nwe propose two novel sublinear time hashing schemes ($d_w^{l_1},l_2$)-ALSH and\n($d_w^{l_1},\\theta$)-ALSH to solve the problem.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 02:52:46 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Hu", "Huan", ""], ["Li", "Jianzhong", ""]]}, {"id": "2104.05040", "submitter": "Zhenlong Li Dr.", "authors": "Zhenlong Li, Xiao Huang, Tao Hu, Huan Ning, Xinyue Ye, Xiaoming Li", "title": "ODT FLOW: A Scalable Platform for Extracting, Analyzing, and Sharing\n  Multi-source Multi-scale Human Mobility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In response to the soaring needs of human mobility data, especially during\ndisaster events such as the COVID-19 pandemic, and the associated big data\nchallenges, we develop a scalable online platform for extracting, analyzing,\nand sharing multi-source multi-scale human mobility flows. Within the platform,\nan origin-destination-time (ODT) data model is proposed to work with scalable\nquery engines to handle heterogenous mobility data in large volumes with\nextensive spatial coverage, which allows for efficient extraction, query, and\naggregation of billion-level origin-destination (OD) flows in parallel at the\nserver-side. An interactive spatial web portal, ODT Flow Explorer, is developed\nto allow users to explore multi-source mobility datasets with user-defined\nspatiotemporal scales. To promote reproducibility and replicability, we further\ndevelop ODT Flow REST APIs that provide researchers with the flexibility to\naccess the data programmatically via workflows, codes, and programs.\nDemonstrations are provided to illustrate the potential of the APIs integrating\nwith scientific workflows and with the Jupyter Notebook environment. We believe\nthe platform coupled with the derived multi-scale mobility data can assist\nhuman mobility monitoring and analysis during disaster events such as the\nongoing COVID-19 pandemic and benefit both scientific communities and the\ngeneral public in understanding human mobility dynamics.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 16:07:00 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Li", "Zhenlong", ""], ["Huang", "Xiao", ""], ["Hu", "Tao", ""], ["Ning", "Huan", ""], ["Ye", "Xinyue", ""], ["Li", "Xiaoming", ""]]}, {"id": "2104.05245", "submitter": "Ce Zhang", "authors": "Ji Liu, Ce Zhang", "title": "Distributed Learning Systems with First-order Methods", "comments": "Foundations and Trends in Databases: Vol. 9: No. 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scalable and efficient distributed learning is one of the main driving forces\nbehind the recent rapid advancement of machine learning and artificial\nintelligence. One prominent feature of this topic is that recent progresses\nhave been made by researchers in two communities: (1) the system community such\nas database, data management, and distributed systems, and (2) the machine\nlearning and mathematical optimization community. The interaction and knowledge\nsharing between these two communities has led to the rapid development of new\ndistributed learning systems and theory.\n  In this work, we hope to provide a brief introduction of some distributed\nlearning techniques that have recently been developed, namely lossy\ncommunication compression (e.g., quantization and sparsification), asynchronous\ncommunication, and decentralized communication. One special focus in this work\nis on making sure that it can be easily understood by researchers in both\ncommunities -- On the system side, we rely on a simplified system model hiding\nmany system details that are not necessary for the intuition behind the system\nspeedups; while, on the theory side, we rely on minimal assumptions and\nsignificantly simplify the proof of some recent work to achieve comparable\nresults.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 07:27:07 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Liu", "Ji", ""], ["Zhang", "Ce", ""]]}, {"id": "2104.05480", "submitter": "Tiantian Liu", "authors": "Tiantian Liu, Huan Li, Hua Lu, Muhammad Aamir Cheema, Lidan Shou", "title": "Towards Crowd-aware Indoor Path Planning (Extended Version)", "comments": "The extension of a VLDB'21 paper \"Towards Crowd-aware Indoor Path\n  Planning\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Indoor venues accommodate many people who collectively form crowds. Such\ncrowds in turn influence people's routing choices, e.g., people may prefer to\navoid crowded rooms when walking from A to B. This paper studies two types of\ncrowd-aware indoor path planning queries. The Indoor Crowd-Aware Fastest Path\nQuery (FPQ) finds a path with the shortest travel time in the presence of\ncrowds, whereas the Indoor Least Crowded Path Query (LCPQ) finds a path\nencountering the least objects en route. To process the queries, we design a\nunified framework with three major components. First, an indoor crowd model\norganizes indoor topology and captures object flows between rooms. Second, a\ntime-evolving population estimator derives room populations for a future\ntimestamp to support crowd-aware routing cost computations in query processing.\nThird, two exact and two approximate query processing algorithms process each\ntype of query. All algorithms are based on graph traversal over the indoor\ncrowd model and use the same search framework with different strategies of\nupdating the populations during the search process. All proposals are evaluated\nexperimentally on synthetic and real data. The experimental results demonstrate\nthe efficiency and scalability of our framework and query processing\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 14:04:32 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 10:13:46 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Liu", "Tiantian", ""], ["Li", "Huan", ""], ["Lu", "Hua", ""], ["Cheema", "Muhammad Aamir", ""], ["Shou", "Lidan", ""]]}, {"id": "2104.05520", "submitter": "Jiacheng Wu", "authors": "Jiacheng Wu, Yong Zhang, Shimin Chen, Jin Wang, Yu Chen, Chunxiao Xing", "title": "Updatable Learned Index with Precise Positions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Index plays an essential role in modern database engines to accelerate the\nquery processing. The new paradigm of \"learned index\" has significantly changed\nthe way of designing index structures in DBMS. The key insight is that indexes\ncould be regarded as learned models that predict the position of a lookup key\nin the dataset. While such studies show promising results in both lookup time\nand index size, they cannot efficiently support update operations. Although\nrecent studies have proposed some preliminary approaches to support update,\nthey are at the cost of scarifying the lookup performance as they suffer from\nthe overheads brought by imprecise predictions in the leaf nodes.\n  In this paper, we propose LIPP, a brand new framework of learned index to\naddress such issues. Similar with state-of-the-art learned index structures,\nLIPP is able to support all kinds of index operations, namely lookup query,\nrange query, insert, delete, update and bulkload. Meanwhile, we overcome the\nlimitations of previous studies by properly extending the tree structure when\ndealing with update operations so as to eliminate the deviation of location\npredicted by the models in the leaf nodes. Moreover, we further propose a\ndynamic adjustment strategy to ensure that the height of the tree index is\ntightly bounded and provide comprehensive theoretical analysis to illustrate\nit. We conduct an extensive set of experiments on several real-life and\nsynthetic datasets. The results demonstrate that our method consistently\noutperforms state-of-the-art solutions, achieving by up to 4x for a broader\nclass of workloads with different index operations.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 14:46:07 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 15:49:29 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Wu", "Jiacheng", ""], ["Zhang", "Yong", ""], ["Chen", "Shimin", ""], ["Wang", "Jin", ""], ["Chen", "Yu", ""], ["Xing", "Chunxiao", ""]]}, {"id": "2104.05983", "submitter": "Diptapriyo Majumdar", "authors": "Jason Crampton, Gregory Gutin, Diptapriyo Majumdar", "title": "Towards Better Understanding of User Authorization Query Problem via\n  Multi-variable Complexity Analysis", "comments": "Accepted for publication in ACM Transactions on Privacy and Security\n  (TOPS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User authorization queries in the context of role-based access control have\nattracted considerable interest in the last 15 years. Such queries are used to\ndetermine whether it is possible to allocate a set of roles to a user that\nenables the user to complete a task, in the sense that all the permissions\nrequired to complete the task are assigned to the roles in that set. Answering\nsuch a query, in general, must take into account a number of factors,\nincluding, but not limited to, the roles to which the user is assigned and\nconstraints on the sets of roles that can be activated. Answering such a query\nis known to be NP-hard. The presence of multiple parameters and the need to\nfind efficient and exact solutions to the problem suggest that a multi-variate\napproach will enable us to better understand the complexity of the user\nauthorization query problem (UAQ). In this paper, we establish a number of\ncomplexity results for UAQ. Specifically, we show the problem remains hard even\nwhen quite restrictive conditions are imposed on the structure of the problem.\nOur FPT results show that we have to use either a parameter with potentially\nquite large values or quite a restricted version of UAQ. Moreover, our second\nFPT algorithm is complex and requires sophisticated, state-of-the-art\ntechniques. In short, our results show that it is unlikely that all variants of\nUAQ that arise in practice can be solved reasonably quickly in general.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 07:31:00 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Crampton", "Jason", ""], ["Gutin", "Gregory", ""], ["Majumdar", "Diptapriyo", ""]]}, {"id": "2104.05994", "submitter": "Kostas Stefanidis", "authors": "Evaggelia Pitoura, Kostas Stefanidis, Georgia Koutrika", "title": "Fairness in Rankings and Recommendations: An Overview", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We increasingly depend on a variety of data-driven algorithmic systems to\nassist us in many aspects of life. Search engines and recommender systems\namongst others are used as sources of information and to help us in making all\nsort of decisions from selecting restaurants and books, to choosing friends and\ncareers. This has given rise to important concerns regarding the fairness of\nsuch systems. In this work, we aim at presenting a toolkit of definitions,\nmodels and methods used for ensuring fairness in rankings and recommendations.\nOur objectives are three-fold: (a) to provide a solid framework on a novel,\nquickly evolving, and impactful domain, (b) to present related methods and put\nthem into perspective, and (c) to highlight open challenges and research paths\nfor future work.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 07:54:16 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Pitoura", "Evaggelia", ""], ["Stefanidis", "Kostas", ""], ["Koutrika", "Georgia", ""]]}, {"id": "2104.06142", "submitter": "Pramod Chunduri", "authors": "Pramod Chunduri, Jaeho Bang, Yao Lu, Joy Arulraj", "title": "Zeus: Efficiently Localizing Actions in Videos using Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Detection and localization of actions in videos is an important problem in\npractice. A traffic analyst might be interested in studying the patterns in\nwhich vehicles move at a given intersection. State-of-the-art video analytics\nsystems are unable to efficiently and effectively answer such action queries.\nThe reasons are threefold. First, action detection and localization tasks\nrequire computationally expensive deep neural networks. Second, actions are\noften rare events. Third, actions are spread across a sequence of frames. It is\nimportant to take the entire sequence of frames into context for effectively\nanswering the query. It is critical to quickly skim through the irrelevant\nparts of the video to answer the action query efficiently.\n  In this paper, we present Zeus, a video analytics system tailored for\nanswering action queries. We propose a novel technique for efficiently\nanswering these queries using a deep reinforcement learning agent. Zeus trains\nan agent that learns to adaptively modify the input video segments to an action\nclassification network. The agent alters the input segments along three\ndimensions -- sampling rate, segment length, and resolution. Besides\nefficiency, Zeus is capable of answering the query at a user-specified target\naccuracy using a query optimizer that trains the agent based on an\naccuracy-aware reward function. Our evaluation of Zeus on a novel action\nlocalization dataset shows that it outperforms the state-of-the-art frame- and\nwindow-based techniques by up to 1.4x and 3x, respectively. Furthermore, unlike\nthe frame-based technique, it satisfies the user-specified target accuracy\nacross all the queries, at up to 2x higher accuracy, than frame-based methods.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 16:38:31 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 03:20:48 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Chunduri", "Pramod", ""], ["Bang", "Jaeho", ""], ["Lu", "Yao", ""], ["Arulraj", "Joy", ""]]}, {"id": "2104.06225", "submitter": "Daniel Waddington", "authors": "Daniel Waddington, Clem Dickey, Luna Xu, Moshik Hershcovitch,\n  Sangeetha Seshadri", "title": "A High-Performance Persistent Memory Key-Value Store with Near-Memory\n  Compute", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  MCAS (Memory Centric Active Storage) is a persistent memory tier for\nhigh-performance durable data storage. It is designed from the ground-up to\nprovide a key-value capability with low-latency guarantees and data durability\nthrough memory persistence and replication. To reduce data movement and make\nfurther gains in performance, we provide support for user-defined \"push-down\"\noperations (known as Active Data Objects) that can execute directly and safely\non the value-memory associated with one or more keys. The ADO mechanism allows\ncomplex pointer-based dynamic data structures (e.g., trees) to be stored and\noperated on in persistent memory. To this end, we examine a real-world use case\nfor MCAS-ADO in the handling of enterprise storage system metadata for\nContinuous Data Protection (CDP). This requires continuously updating complex\nmetadata that must be kept consistent and durable. In this paper, we i.)\npresent the MCAS-ADO system architecture, ii.) show how the CDP use case is\nimplemented, and finally iii.) give an evaluation of system performance in the\ncontext of this use case.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 17:06:43 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Waddington", "Daniel", ""], ["Dickey", "Clem", ""], ["Xu", "Luna", ""], ["Hershcovitch", "Moshik", ""], ["Seshadri", "Sangeetha", ""]]}, {"id": "2104.06460", "submitter": "Suman Banerjee", "authors": "Suman Banerjee", "title": "A Co\\mbox{-}Operative Game Theoretic Approach for the Budgeted Influence\n  Maximization Problem", "comments": "39 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given a social network of users with selection cost, the \\textsc{Budgeted\nInfluence Maximization Problem} (\\emph{BIM Problem} in short) asks for\nselecting a subset of the nodes (known as \\emph{seed nodes}) within an\nallocated budget for initial activation to maximize the influence in the\nnetwork. In this paper, we study this problem under the\n\\emph{co\\mbox{-}operative game theoretic} framework. We model this problem as a\nco\\mbox{-}operative game where the users of the network are the players and for\na group of users, the expected influence by them under the \\emph{Maximum\nInfluence Arborences} diffusion model is its utility. We call this game as\n\\emph{BIM Game} and show this is `non-convex' and `sub-additive'. Based on the\nproposed game\\mbox{-}theoretic model and using the solution concept called\n`Shapley Value', we propose an iterative algorithm for finding seed nodes. The\nproposed methodology is divided into mainly two broad steps: the first one is\ncomputing the approximate marginal gain in \\emph{Shapley Value} for all the\nnodes of the network, and the second one is selecting seed nodes from the\nsorted list until the budget is exhausted. We also show that the proposed\nmethodology can even be more effective when the community structure of the\nnetwork is exploited. The proposed methodologies have been implemented, and an\nextensive set of experiments have been conducted with three publicly available\nsocial network datasets. From the experiments, we observe that the seed set\nselected by the proposed methodologies lead to more number of influence nodes\ncompared to many standard and baseline methods from the literature with a\nreasonable computational overhead. In particular, if the community structure of\nthe network is exploited then there is an increase upto $2 \\%$ in number of\ninfluenced nodes.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 09:35:15 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Banerjee", "Suman", ""]]}, {"id": "2104.06701", "submitter": "Yuya Sasaki", "authors": "Yuya Sasaki, Keizo Hori, Daiki Nishihara, Sora Ohashi, Yusuke Wakuta,\n  Kei Harada, Makoto Onizuka, Yuki Arase, Shinji Shimojo, Kenji Doi, He Hongdi,\n  Zhong-Ren Peng", "title": "Smart City Data Analysis via Visualization of Correlated Attribute\n  Patterns", "comments": null, "journal-ref": "EDBT 2021", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Urban conditions are monitored by a wide variety of sensors that measure\nseveral attributes, such as temperature and traffic volume. The correlations of\nsensors help to analyze and understand the urban conditions accurately. The\ncorrelated attribute pattern (CAP) mining discovers correlations among multiple\nattributes from the sets of sensors spatially close to each other and\ntemporally correlated in their measurements. In this paper, we develop a\nvisualization system for CAP mining and demonstrate analysis of smart city\ndata. Our visualization system supports an intuitive understanding of mining\nresults via sensor locations on maps and temporal changes of their\nmeasurements. In our demonstration scenarios, we provide four smart city\ndatasets collected from China and Santander, Spain. We demonstrate that our\nsystem helps interactive analysis of smart city data.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 08:49:12 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Sasaki", "Yuya", ""], ["Hori", "Keizo", ""], ["Nishihara", "Daiki", ""], ["Ohashi", "Sora", ""], ["Wakuta", "Yusuke", ""], ["Harada", "Kei", ""], ["Onizuka", "Makoto", ""], ["Arase", "Yuki", ""], ["Shimojo", "Shinji", ""], ["Doi", "Kenji", ""], ["Hongdi", "He", ""], ["Peng", "Zhong-Ren", ""]]}, {"id": "2104.07010", "submitter": "Adri\\'an Bazaga", "authors": "Adri\\'an Bazaga and Nupur Gunwant and Gos Micklem", "title": "Translating synthetic natural language to database queries: a polyglot\n  deep learning framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DB cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The number of databases as well as their size and complexity is increasing.\nThis creates a barrier to use especially for non-experts, who have to come to\ngrips with the nature of the data, the way it has been represented in the\ndatabase, and the specific query languages or user interfaces by which data are\naccessed. These difficulties worsen in research settings, where it is common to\nwork with many different databases. One approach to improving this situation is\nto allow users to pose their queries in natural language.\n  In this work we describe a machine learning framework, Polyglotter, that in a\ngeneral way supports the mapping of natural language searches to database\nqueries. Importantly, it does not require the creation of manually annotated\ndata for training and therefore can be applied easily to multiple domains. The\nframework is polyglot in the sense that it supports multiple different database\nengines that are accessed with a variety of query languages, including SQL and\nCypher. Furthermore Polyglotter also supports multi-class queries.\n  Our results indicate that our framework performs well on both synthetic and\nreal databases, and may provide opportunities for database maintainers to\nimprove accessibility to their resources.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 17:43:51 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Bazaga", "Adri\u00e1n", ""], ["Gunwant", "Nupur", ""], ["Micklem", "Gos", ""]]}, {"id": "2104.07742", "submitter": "Manuel Dossinger", "authors": "Manuel Dossinger, Sebastian Michel", "title": "Optimizing Multiple Multi-Way Stream Joins", "comments": "13 pages, 9 figures. Short paper version of this article was\n  published in ICDE 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the joint optimization of multiple stream joins in a scale-out\narchitecture by tailoring prior work on multi-way stream joins to\npredicate-driven data partitioning schemes. We present an integer linear\nprogramming (ILP) formulation for selecting the partitioning and tuple routing\nwith minimal probe load and describe how routing and operator placement can be\nrewired dynamically at changing data characteristics and arrival or expiration\nof queries. The presented algorithms and optimization schemes are implemented\nin CLASH, a data stream processor developed in our group that translates\nqueries to deployable Apache Storm topologies after optimization. The\nexperiments conducted over real-world data exhibit the potential of multi-query\noptimization of multi-way stream joins and the effectiveness and feasibility of\nthe ILP optimization problem.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 19:48:47 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Dossinger", "Manuel", ""], ["Michel", "Sebastian", ""]]}, {"id": "2104.07776", "submitter": "Jonas Dann", "authors": "Jonas Dann and Daniel Ritter and Holger Fr\\\"oning", "title": "Demystifying Memory Access Patterns of FPGA-Based Graph Processing\n  Accelerators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in reprogrammable hardware (e.g., FPGAs) and memory\ntechnology (e.g., DDR4, HBM) promise to solve performance problems inherent to\ngraph processing like irregular memory access patterns on traditional hardware\n(e.g., CPU). While several of these graph accelerators were proposed in recent\nyears, it remains difficult to assess their performance and compare them on\ncommon graph workloads and accelerator platforms, due to few open source\nimplementations and excessive implementation effort.\n  In this work, we build on a simulation environment for graph processing\naccelerators, to make several existing accelerator approaches comparable. This\nallows us to study relevant performance dimensions such as partitioning schemes\nand memory technology, among others. The evaluation yields insights into the\nstrengths and weaknesses of current graph processing accelerators along these\ndimensions, and features a novel in-depth comparison.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 14:53:53 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Dann", "Jonas", ""], ["Ritter", "Daniel", ""], ["Fr\u00f6ning", "Holger", ""]]}, {"id": "2104.07926", "submitter": "Claudio Di Ciccio", "authors": "Alessio Cecconi, Adriano Augusto, and Claudio Di Ciccio", "title": "Detection of statistically significant differences between process\n  variants through declarative rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Services and products are often offered via the execution of processes that\nvary according to the context, requirements, or customisation needs. The\nanalysis of such process variants can highlight differences in the service\noutcome or quality, leading to process adjustments and improvement. Research in\nthe area of process mining has provided several methods for process variants\nanalysis. However, very few of those account for a statistical significance\nanalysis of their output. Moreover, those techniques detect differences at the\nlevel of process traces, single activities, or performance. In this paper, we\naim at describing the distinctive behavioural characteristics between variants\nexpressed in the form of declarative process rules. The contribution to the\nresearch area is two-pronged: the use of declarative rules for the explanation\nof the process variants and the statistical significance analysis of the\noutcome. We assess the proposed method by comparing its results to the most\nrecent process variants analysis methods. Our results demonstrate not only that\ndeclarative rules reveal differences at an unprecedented level of\nexpressiveness, but also that our method outperforms the state of the art in\nterms of execution time.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 07:06:33 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 21:18:46 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Cecconi", "Alessio", ""], ["Augusto", "Adriano", ""], ["Di Ciccio", "Claudio", ""]]}, {"id": "2104.08504", "submitter": "Suman Banerjee", "authors": "Suman Banerjee and Bithika Pal", "title": "Budgeted Influence and Earned Benefit Maximization with Tags in Social\n  Networks", "comments": "42 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given a social network, where each user is associated with a selection cost,\nthe problem of \\textsc{Budgeted Influence Maximization} (\\emph{BIM Problem} in\nshort) asks to choose a subset of them (known as seed users) within an\nallocated budget whose initial activation leads to the maximum number of\ninfluenced nodes. Existing Studies on this problem do not consider the\ntag-specific influence probability. However, in reality, influence probability\nbetween two users always depends upon the context (e.g., sports, politics,\netc.). To address this issue, in this paper we introduce the\n\\textsc{Tag\\mbox{-}Based Budgeted Influence Maximization problem} (\\emph{TBIM\nProblem} in short), where along with the other inputs, a tag set (each of them\nis also associated with a selection cost) is given, each edge of the network\nhas the tag specific influence probability, and here the goal is to select\ninfluential users as well as influential tags within the allocated budget to\nmaximize the influence. Considering the fact that real-world campaigns targeted\nin nature, we also study the \\textsc{Earned Benefit Maximization} Problem in\ntag specific influence probability setting, which formally we call the\n\\textsc{Tag\\mbox{-}Based Earned Benefit Maximization problem} (\\emph{TEBM\nProblem} in short). For this problem along with the inputs of the TBIM Problem,\nwe are given a subset of the nodes as target users, and each one of them is\nassociated with a benefit value that can be earned by influencing them.\nConsidering the fact that different tag has different popularity across the\ncommunities of the same network, we propose three methodologies that work based\non \\emph{effective marginal influence gain computation}. The proposed\nmethodologies have been analyzed for their time and space requirements.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 10:07:32 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Banerjee", "Suman", ""], ["Pal", "Bithika", ""]]}, {"id": "2104.08976", "submitter": "Joel Mackenzie", "authors": "Joel Mackenzie and Matthias Petri and Alistair Moffat", "title": "Anytime Ranking on Document-Ordered Indexes", "comments": "Accepted to ACM TOIS, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverted indexes continue to be a mainstay of text search engines, allowing\nefficient querying of large document collections. While there are a number of\npossible organizations, document-ordered indexes are the most common, since\nthey are amenable to various query types, support index updates, and allow for\nefficient dynamic pruning operations. One disadvantage with document-ordered\nindexes is that high-scoring documents can be distributed across the document\nidentifier space, meaning that index traversal algorithms that terminate early\nmight put search effectiveness at risk. The alternative is impact-ordered\nindexes, which primarily support top-k disjunctions, but also allow for anytime\nquery processing, where the search can be terminated at any time, with search\nquality improving as processing latency increases. Anytime query processing can\nbe used to effectively reduce high-percentile tail latency which is essential\nfor operational scenarios in which a service level agreement (SLA) imposes\nresponse time requirements. In this work, we show how document-ordered indexes\ncan be organized such that they can be queried in an anytime fashion, enabling\nstrict latency control with effective early termination. Our experiments show\nthat processing document-ordered topical segments selected by a simple score\nestimator outperforms existing anytime algorithms, and allows query runtimes to\nbe accurately limited in order to comply with SLA requirements.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 23:17:07 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 01:27:52 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Mackenzie", "Joel", ""], ["Petri", "Matthias", ""], ["Moffat", "Alistair", ""]]}, {"id": "2104.09509", "submitter": "Georgios Chatzigeorgakidis", "authors": "Georgios Chatzigeorgakidis, Dimitrios Skoutas, Kostas Patroumpas,\n  Themis Palpanas, Spiros Athanasiou, and Spiros Skiadopoulos", "title": "Local Similarity Search on Geolocated Time Series Using Hybrid Indexing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Geolocated time series, i.e., time series associated with certain locations,\nabound in many modern applications. In this paper, we consider hybrid queries\nfor retrieving geolocated time series based on filters that combine spatial\ndistance and time series similarity. For the latter, unlike existing work, we\nallow filtering based on local similarity, which is computed based on\nsubsequences rather than the entire length of each series, thus allowing the\ndiscovery of more fine-grained trends and patterns. To efficiently support such\nqueries, we first leverage the state-of-the-art BTSR-tree index, which utilizes\nbounds over both the locations and the shapes of time series to prune the\nsearch space. Moreover, we propose optimizations that check at specific\ntimestamps to identify candidate time series that may exceed the required local\nsimilarity threshold. To further increase pruning power, we introduce the\nSBTSR-tree index, an extension to BTSR-tree, which additionally segments the\ntime series temporally, allowing the construction of tighter bounds. Our\nexperimental results on several real-world datasets demonstrate that SBTSR-tree\ncan provide answers much faster for all examined query types. This paper has\nbeen published in the 27th International Conference on Advances in Geographic\nInformation Systems (ACM SIGSPATIAL 2019).\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 16:18:21 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Chatzigeorgakidis", "Georgios", ""], ["Skoutas", "Dimitrios", ""], ["Patroumpas", "Kostas", ""], ["Palpanas", "Themis", ""], ["Athanasiou", "Spiros", ""], ["Skiadopoulos", "Spiros", ""]]}, {"id": "2104.09677", "submitter": "Thilina Ranbaduge", "authors": "Thilina Ranbaduge, Peter Christen, Rainer Schnell", "title": "Large Scale Record Linkage in the Presence of Missing Data", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Record linkage is aimed at the accurate and efficient identification of\nrecords that represent the same entity within or across disparate databases. It\nis a fundamental task in data integration and increasingly required for\naccurate decision making in application domains ranging from health analytics\nto national security. Traditional record linkage techniques calculate string\nsimilarities between quasi-identifying (QID) values, such as the names and\naddresses of people. Errors, variations, and missing QID values can however\nlead to low linkage quality because the similarities between records cannot be\ncalculated accurately. To overcome this challenge, we propose a novel technique\nthat can accurately link records even when QID values contain errors or\nvariations, or are missing. We first generate attribute signatures\n(concatenated QID values) using an Apriori based selection of suitable QID\nattributes, and then relational signatures that encapsulate relationship\ninformation between records. Combined, these signatures can uniquely identify\nindividual records and facilitate fast and high quality linking of very large\ndatabases through accurate similarity calculations between records. We evaluate\nthe linkage quality and scalability of our approach using large real-world\ndatabases, showing that it can achieve high linkage quality even when the\ndatabases being linked contain substantial amounts of missing values and\nerrors.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 22:57:19 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Ranbaduge", "Thilina", ""], ["Christen", "Peter", ""], ["Schnell", "Rainer", ""]]}, {"id": "2104.09733", "submitter": "Ye Wang", "authors": "Ye Wang, Qing Wang, Henning Koehler, Yu Lin", "title": "Query-by-Sketch: Scaling Shortest Path Graph Queries on Very Large\n  Networks", "comments": null, "journal-ref": null, "doi": "10.1145/3448016.3452826", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing shortest paths is a fundamental operation in processing graph data.\nIn many real-world applications, discovering shortest paths between two\nvertices empowers us to make full use of the underlying structure to understand\nhow vertices are related in a graph, e.g. the strength of social ties between\nindividuals in a social network. In this paper, we study the\nshortest-path-graph problem that aims to efficiently compute a shortest path\ngraph containing exactly all shortest paths between any arbitrary pair of\nvertices on complex networks. Our goal is to design an exact solution that can\nscale to graphs with millions or billions of vertices and edges. To achieve\nhigh scalability, we propose a novel method, Query-by-Sketch (QbS), which\nefficiently leverages offline labelling (i.e., precomputed labels) to guide\nonline searching through a fast sketching process that summarizes the important\nstructural aspects of shortest paths in answering shortest-path-graph queries.\nWe theoretically prove the correctness of this method and analyze its\ncomputational complexity. To empirically verify the efficiency of QbS, we\nconduct experiments on 12 real-world datasets, among which the largest dataset\nhas 1.7 billion vertices and 7.8 billion edges. The experimental results show\nthat QbS can answer shortest-path graph queries in microseconds for\nmillion-scale graphs and less than half a second for billion-scale graphs.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 03:04:42 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Wang", "Ye", ""], ["Wang", "Qing", ""], ["Koehler", "Henning", ""], ["Lin", "Yu", ""]]}, {"id": "2104.09809", "submitter": "Alejandro \\'Alvarez-Ayll\\'on", "authors": "Alejandro Alvarez-Ayllon, Manuel Palomo-Duarte, Juan-Manuel Dodero", "title": "Inference of Common Multidimensional Equally-Distributed Attributes", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given two relations containing multiple measurements - possibly with\nuncertainties - our objective is to find which sets of attributes from the\nfirst have a corresponding set on the second, using exclusively a sample of the\ndata. This approach could be used even when the associated metadata is damaged,\nmissing or incomplete, or when the volume is too big for exact methods. This\nproblem is similar to the search of Inclusion Dependencies (IND), a type of\nrule over two relations asserting that for a set of attributes X from the\nfirst, every combination of values appears on a set Y from the second. Existing\nIND can be found exploiting the existence of a partial order relation called\nspecialization. However, this relation is based on set theory, requiring the\nvalues to be directly comparable. Statistical tests are an intuitive possible\nreplacement, but it has not been studied how would they affect the underlying\nassumptions. In this paper we formally review the effect that a statistical\napproach has over the inference rules applied to IND discovery. Our results\nconfirm the intuitive thought that statistical tests can be used, but not in a\ndirectly equivalent manner. We provide a workable alternative based on a\n\"hierarchy of null hypotheses\", allowing for the automatic discovery of\nmulti-dimensional equally distributed sets of attributes.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 07:46:49 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Alvarez-Ayllon", "Alejandro", ""], ["Palomo-Duarte", "Manuel", ""], ["Dodero", "Juan-Manuel", ""]]}, {"id": "2104.10939", "submitter": "Panagiotis Bouros", "authors": "George Christodoulou and Panagiotis Bouros and Nikos Mamoulis", "title": "HINT: A Hierarchical Index for Intervals in Main Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indexing intervals is a fundamental problem, finding a wide range of\napplications. Recent work on managing large collections of intervals in main\nmemory focused on overlap joins and temporal aggregation problems. In this\npaper, we propose novel and efficient in-memory indexing techniques for\nintervals, with a focus on interval range queries, which are a basic component\nof many search and analysis tasks. First, we propose an optimized version of a\nsingle-level (flat) domain-partitioning approach, which may have large space\nrequirements due to excessive replication. Then, we propose a hierarchical\npartitioning approach, which assigns each interval to at most two partitions\nper level and has controlled space requirements. Novel elements of our\ntechniques include the division of the intervals at each partition into groups\nbased on whether they begin inside or before the partition boundaries, reducing\nthe information stored at each partition to the absolutely necessary, and the\neffective handling of data sparsity and skew. Experimental results on real and\nsynthetic interval sets of different characteristics show that our approaches\nare typically one order of magnitude faster than the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 09:10:31 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Christodoulou", "George", ""], ["Bouros", "Panagiotis", ""], ["Mamoulis", "Nikos", ""]]}, {"id": "2104.11787", "submitter": "Andrea Hillenbrand", "authors": "Andrea Hillenbrand, Uta St\\\"orl, Shamil Nabiyev, Stefanie Scherzinger", "title": "MigCast in Monte Carlo: The Impact of Data Model Evolution in NoSQL\n  Databases", "comments": "16 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the development of NoSQL-backed software, the data model evolves\nnaturally alongside the application code. Especially in agile development, new\napplication releases are deployed frequently causing schema changes.\nEventually, decisions have to be made regarding the migration of versioned\nlegacy data which is persisted in the cloud-hosted production database. We\nsolve this schema evolution problem and present the results of near-exhaustive\ncalculations by means of which software project stakeholders can manage the\noperative costs for data model evolution and adapt their software release\nstrategy accordingly in order to comply with service-level agreements regarding\nthe competing metrics of migration costs and latency. We clarify conclusively\nhow data model evolution in NoSQL databases impacts the metrics while taking\nall relevant characteristics of migration scenarios into account. As\ncalculating all possible combinatorics in the search space of migration\nscenarios would by far exceed computational means, we used a probabilistic\nMonte Carlo method of repeated sampling, serving as a well-established means to\nbring the complexity of data model evolution under control. Our experiments\nshow the qualitative and quantitative impact on the performance of migration\nstrategies with respect to intensity and distribution of data entity accesses,\nthe kinds of schema changes, and the characteristics of the underlying data\nmodel.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 19:05:14 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Hillenbrand", "Andrea", ""], ["St\u00f6rl", "Uta", ""], ["Nabiyev", "Shamil", ""], ["Scherzinger", "Stefanie", ""]]}, {"id": "2104.12386", "submitter": "Ryuta Arisaka", "authors": "Ryuta Arisaka, Takayuki Ito", "title": "Relational Argumentation Semantics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a fresh perspective on argumentation semantics, to\nview them as a relational database. It offers encapsulation of the underlying\nargumentation graph, and allows us to understand argumentation semantics under\na single, relational perspective, leading to the concept of relational\nargumentation semantics. This is a direction to understand argumentation\nsemantics through a common formal language. We show that many existing\nsemantics such as explanation semantics, multi-agent semantics, and more\ntypical semantics, that have been proposed for specific purposes, are\nunderstood in the relational perspective.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 07:58:17 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Arisaka", "Ryuta", ""], ["Ito", "Takayuki", ""]]}, {"id": "2104.12615", "submitter": "Ingo M\\\"uller", "authors": "Dan Graur (1) and Ingo M\\\"uller (1) and Ghislain Fourny (1) and Gordon\n  T. Watts (2) and Mason Proffitt (2) and Gustavo Alonso (1) ((1) Department of\n  Computer Science, ETH Zurich, (2) Department of Physics, University of\n  Washington)", "title": "Evaluating Query Languages and Systems for High-Energy Physics Data", "comments": "Submitted to PVLDB Vol. 15 (VLDB 2022)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB hep-ex", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Query languages in general and SQL in particular are arguably one of the most\nsuccessful programming interfaces. Yet, in the domain of high-energy physics\n(HEP), they have found limited acceptance. This is surprising since data\nanalysis in HEP matches the SQL model well: it is fully structured data queried\nusing combinations of selections, projections, joins, and reductions. To gain\ninsights on why this is the case, in this paper we perform an exhaustive\nperformance and functionality analysis of several data processing platforms\n(Amazon Athena, Google Big Query, Presto, Rumble) and compare them to the new\nRDataFrame interface of the ROOT framework, the most commonly used system by\nparticle physicists today. The goal of the analysis is to identify the\npotential advantages and shortcomings of each system considering not only\nperformance but also cost for cloud deployments, suitability of the query\ndialect, and resulting query complexity. The analysis is done using a HEP\nworkload: the Analysis Description Languages (ADL) benchmark, created by\nphysicists to capture representative aspects of their data processing tasks.\nThe evaluation of these systems results in an interesting and rather complex\npicture of existing solutions: those offering the best possibilities in terms\nof expressiveness, conciseness, and usability turn out to be the slowest and\nmost expensive; the fastest ones are not the most cost-efficient and involve\ncomplex queries; RDataFrame, the baseline we use as a reference, is often\nfaster and cheaper but is currently facing scalability issues with large\nmulti-core machines. In the paper, we analyze all the aspects that lead to such\nresults and discuss how systems should evolve to better support HEP workloads.\nIn the process, we identify several weaknesses of existing systems that should\nbe relevant to a wide range of use cases beyond particle physics.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 14:34:20 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Graur", "Dan", ""], ["M\u00fcller", "Ingo", ""], ["Fourny", "Ghislain", ""], ["Watts", "Gordon T.", ""], ["Proffitt", "Mason", ""], ["Alonso", "Gustavo", ""]]}, {"id": "2104.12622", "submitter": "Elwin Huaman", "authors": "Elwin Huaman, Amar Tauqeer, Geni Bushati and Anna Fensel", "title": "Towards Knowledge Graphs Validation through Weighted Knowledge Sources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The performance of applications, such as personal assistants, search engines,\nand question-answering systems, rely on high-quality knowledge bases, a.k.a.\nKnowledge Graphs (KGs). To ensure their quality one important task is Knowledge\nValidation, which measures the degree to which statements or triples of a\nKnowledge Graph (KG) are correct. KGs inevitably contains incorrect and\nincomplete statements, which may hinder the adoption of such KGs in business\napplications as they are not trustworthy. In this paper, we propose and\nimplement a validation approach that computes a confidence score for every\ntriple and instance in a KG. The computed score is based on finding the same\ninstances across different weighted knowledge sources and comparing their\nfeatures. We evaluated the performance of our Validator by comparing a manually\nvalidated result against the output of the Validator. The experimental results\nshowed that compared with the manual validation, our Validator achieved as good\nprecision as the manual validation, although with certain limitations.\nFurthermore, we give insights and directions toward a better architecture to\ntackle KG validation.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 14:45:33 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Huaman", "Elwin", ""], ["Tauqeer", "Amar", ""], ["Bushati", "Geni", ""], ["Fensel", "Anna", ""]]}, {"id": "2104.12815", "submitter": "Xing Niu", "authors": "Xing Niu, Ziyu Liu, Pengyuan Li, Boris Glavic", "title": "Provenance-based Data Skipping (TechReport)", "comments": "20 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Database systems analyze queries to determine upfront which data is needed\nfor answering them and use indexes and other physical design techniques to\nspeed-up access to that data. However, for important classes of queries, e.g.,\nHAVING and top-k queries, it is impossible to determine up-front what data is\nrelevant. To overcome this limitation, we develop provenance-based data\nskipping (PBDS), a novel approach that generates provenance sketches to\nconcisely encode what data is relevant for a query. Once a provenance sketch\nhas been captured it is used to speed up subsequent queries. PBDS can exploit\nphysical design artifacts such as indexes and zone maps. Our approach\nsignificantly improves performance for both disk-based and main-memory database\nsystems.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 18:44:54 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 20:29:34 GMT"}, {"version": "v3", "created": "Thu, 27 May 2021 19:26:10 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Niu", "Xing", ""], ["Liu", "Ziyu", ""], ["Li", "Pengyuan", ""], ["Glavic", "Boris", ""]]}, {"id": "2104.13276", "submitter": "Gabriel Meseguer-Brocal", "authors": "Gabriel Meseguer-Brocal", "title": "MULTIMODAL ANALYSIS: Informed content estimation and audio source\n  separation", "comments": "Ph.D. dissertation. Thesis supervisor: Geoffroy Peeters. Jury:Laurent\n  Girin, Ga\\\"el Richard, Rachel Bittner, Elena Cabrio, Bruno Gas, Perfecto\n  Herrera Boyer, Antoine Liutkus", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.DB cs.IR cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This dissertation proposes the study of multimodal learning in the context of\nmusical signals. Throughout, we focus on the interaction between audio signals\nand text information. Among the many text sources related to music that can be\nused (e.g. reviews, metadata, or social network feedback), we concentrate on\nlyrics. The singing voice directly connects the audio signal and the text\ninformation in a unique way, combining melody and lyrics where a linguistic\ndimension complements the abstraction of musical instruments. Our study focuses\non the audio and lyrics interaction for targeting source separation and\ninformed content estimation.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 15:45:21 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 14:50:58 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Meseguer-Brocal", "Gabriel", ""]]}, {"id": "2104.13321", "submitter": "Tobias Skovgaard Jepsen", "authors": "Tobias Skovgaard Jepsen and Christian S. Jensen and Thomas Dyhre\n  Nielsen", "title": "UniTE -- The Best of Both Worlds: Unifying Function-Fitting and\n  Aggregation-Based Approaches to Travel Time and Travel Speed Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Travel time or speed estimation are part of many intelligent transportation\napplications. Existing estimation approaches rely on either function fitting or\naggregation and represent different trade-offs between generalizability and\naccuracy. Function-fitting approaches learn functions that map feature vectors\nof, e.g., routes, to travel time or speed estimates, which enables\ngeneralization to unseen routes. However, mapping functions are imperfect and\noffer poor accuracy in practice. Aggregation-based approaches instead form\nestimates by aggregating historical data, e.g., traversal data for routes. This\nenables very high accuracy given sufficient data. However, they rely on\nsimplistic heuristics when insufficient data is available, yielding poor\ngeneralizability. We present a Unifying approach to Travel time and speed\nEstimation (UniTE) that combines function-fitting and aggregation-based\napproaches into a unified framework that aims to achieve the generalizability\nof function-fitting approaches and the accuracy of aggregation-based\napproaches. An empirical study finds that an instance of UniTE can improve the\naccuracies of travel speed distribution and travel time estimation by $40-64\\%$\nand $3-23\\%$, respectively, compared to using function fitting or aggregation\nalone\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 16:55:24 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Jepsen", "Tobias Skovgaard", ""], ["Jensen", "Christian S.", ""], ["Nielsen", "Thomas Dyhre", ""]]}, {"id": "2104.13576", "submitter": "Markus Schr\\\"oder", "authors": "Markus Schr\\\"oder, Christian Jilek, Andreas Dengel", "title": "Dataset Generation Patterns for Evaluating Knowledge Graph Construction", "comments": "5 pages, submitted to ESWC demo track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Confidentiality hinders the publication of authentic, labeled datasets of\npersonal and enterprise data, although they could be useful for evaluating\nknowledge graph construction approaches in industrial scenarios. Therefore, our\nplan is to synthetically generate such data in a way that it appears as\nauthentic as possible. Based on our assumption that knowledge workers have\ncertain habits when they produce or manage data, generation patterns could be\ndiscovered which can be utilized by data generators to imitate real datasets.\nIn this paper, we initially derived 11 distinct patterns found in real\nspreadsheets from industry and demonstrate a suitable generator called Data\nSprout that is able to reproduce them. We describe how the generator produces\nspreadsheets in general and what altering effects the implemented patterns\nhave.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 05:47:32 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Schr\u00f6der", "Markus", ""], ["Jilek", "Christian", ""], ["Dengel", "Andreas", ""]]}, {"id": "2104.13600", "submitter": "Markus Schr\\\"oder", "authors": "Markus Schr\\\"oder, Christian Jilek, Andreas Dengel", "title": "Mapping Spreadsheets to RDF: Supporting Excel in RML", "comments": "6 pages, submitted to Second International Workshop on Knowledge\n  Graph Construction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The RDF Mapping Language (RML) enables, among other formats, the mapping of\ntabular data as Comma-Separated Values (CSV) files to RDF graphs.\nUnfortunately, the widely used spreadsheet format is currently neglected by its\nspecification and well-known implementations. Therefore, we extended one of the\ntools which is RML Mapper to support Microsoft Excel spreadsheet files and\ndemonstrate its capabilities in an interactive online demo. Our approach allows\nto access various meta data of spreadsheet cells in typical RML maps. Some\nexperimental features for more specific use cases are also provided. The\nimplementation code is publicly available in a GitHub fork.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 07:13:17 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Schr\u00f6der", "Markus", ""], ["Jilek", "Christian", ""], ["Dengel", "Andreas", ""]]}, {"id": "2104.13605", "submitter": "Markus Schr\\\"oder", "authors": "Markus Schr\\\"oder, Christian Jilek, Andreas Dengel", "title": "A Linked Data Application Framework to Enable Rapid Prototyping", "comments": "5 pages, demo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Application developers, in our experience, tend to hesitate when dealing with\nlinked data technologies. To reduce their initial hurdle and enable rapid\nprototyping, we propose in this paper a framework for building linked data\napplications. Our approach especially considers the participation of web\ndevelopers and non-technical users without much prior knowledge about linked\ndata concepts. Web developers are supported with bidirectional RDF to JSON\nconversions and suitable CRUD endpoints. Non-technical users can browse\nwebsites generated from JSON data by means of a template language. A\nprototypical open source implementation demonstrates its capabilities.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 07:31:20 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Schr\u00f6der", "Markus", ""], ["Jilek", "Christian", ""], ["Dengel", "Andreas", ""]]}, {"id": "2104.13744", "submitter": "Ana Claudia Sima", "authors": "Ana Claudia Sima, Tarcisio Mendes de Farias, Maria Anisimova,\n  Christophe Dessimoz, Marc Robinson-Rechavi, Erich Zbinden, and Kurt\n  Stockinger", "title": "Bio-SODA: Enabling Natural Language Question Answering over Knowledge\n  Graphs without Training Data", "comments": null, "journal-ref": "33rd International Conference on Scientific and Statistical\n  Database Management (SSDBM 2021)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The problem of natural language processing over structured data has become a\ngrowing research field, both within the relational database and the Semantic\nWeb community, with significant efforts involved in question answering over\nknowledge graphs (KGQA). However, many of these approaches are either\nspecifically targeted at open-domain question answering using DBpedia, or\nrequire large training datasets to translate a natural language question to\nSPARQL in order to query the knowledge graph. Hence, these approaches often\ncannot be applied directly to complex scientific datasets where no prior\ntraining data is available.\n  In this paper, we focus on the challenges of natural language processing over\nknowledge graphs of scientific datasets. In particular, we introduce Bio-SODA,\na natural language processing engine that does not require training data in the\nform of question-answer pairs for generating SPARQL queries. Bio-SODA uses a\ngeneric graph-based approach for translating user questions to a ranked list of\nSPARQL candidate queries. Furthermore, Bio-SODA uses a novel ranking algorithm\nthat includes node centrality as a measure of relevance for selecting the best\nSPARQL candidate query. Our experiments with real-world datasets across several\nscientific domains, including the official bioinformatics Question Answering\nover Linked Data (QALD) challenge, show that Bio-SODA outperforms publicly\navailable KGQA systems by an F1-score of least 20% and by an even higher factor\non more complex bioinformatics datasets.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 13:17:59 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 07:52:56 GMT"}, {"version": "v3", "created": "Thu, 27 May 2021 09:06:18 GMT"}, {"version": "v4", "created": "Mon, 14 Jun 2021 07:46:59 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Sima", "Ana Claudia", ""], ["de Farias", "Tarcisio Mendes", ""], ["Anisimova", "Maria", ""], ["Dessimoz", "Christophe", ""], ["Robinson-Rechavi", "Marc", ""], ["Zbinden", "Erich", ""], ["Stockinger", "Kurt", ""]]}, {"id": "2104.13793", "submitter": "Matthias Lanzinger", "authors": "Georg Gottlob, Matthias Lanzinger, Cem Okulmus, Reinhard Pichler", "title": "Fast Parallel Hypertree Decompositions in Logarithmic Recursion Depth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern trends in data collection are bringing current mainstream techniques\nfor database query processing to their limits. Consequently, various novel\napproaches for efficient query processing are being actively studied. One such\napproach is based on hypertree decompositions (HDs), which have been shown to\ncarry great potential to process complex queries more efficiently and with\nstronger theoretical guarantees. However, using HDs for query execution relies\non the difficult task of computing decompositions of the query structure, which\nguides the efficient execution of the query. From theoretical results we know\nthat the performance of purely sequential methods is inherently limited, yet\nthe problem is susceptible to parallelisation.\n  In this paper we propose the first algorithm for computing hypertree\ndecompositions that is well-suited for parallelisation. The proposed algorithm\nlog-k-decomp requires only a logarithmic number of recursion levels and\nadditionally allows for highly parallelised pruning of the search space by\nrestriction to balanced separators. We provide detailed experimental evaluation\nover the HyperBench benchmark and demonstrate that our approach is highly\neffective especially for complex queries.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 14:33:05 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Gottlob", "Georg", ""], ["Lanzinger", "Matthias", ""], ["Okulmus", "Cem", ""], ["Pichler", "Reinhard", ""]]}, {"id": "2104.14595", "submitter": "Hannah Bast", "authors": "Hannah Bast, Johannes Kalmbach, Theresa Klumpp, Florian Kramer, Niklas\n  Schnelle", "title": "Efficient SPARQL Autocompletion via SPARQL", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to achieve fast autocompletion for SPARQL queries on very large\nknowledge bases. At any position in the body of a SPARQL query, the\nautocompletion suggests matching subjects, predicates, or objects. The\nsuggestions are context-sensitive in the sense that they lead to a non-empty\nresult and are ranked by their relevance to the part of the query already\ntyped. The suggestions can be narrowed down by prefix search on the names and\naliases of the desired subject, predicate, or object. All suggestions are\nthemselves obtained via SPARQL queries, which we call autocompletion queries.\nFor existing SPARQL engines, these queries are impractically slow on large\nknowledge bases. We present various algorithmic and engineering improvements of\nan existing SPARQL engine such that these autocompletion queries are executed\nefficiently. We provide an extensive evaluation of a variety of suggestion\nmethods on three large knowledge bases, including Wikidata (6.9B triples). We\nexplore the trade-off between the relevance of the suggestions and the\nprocessing time of the autocompletion queries. We compare our results with two\nwidely used SPARQL engines, Virtuoso and Blazegraph. On Wikidata, we achieve\nfully sensitive suggestions with sub-second response times for over 90% of a\nlarge and diverse set of thousands of autocompletion queries. Materials for\nfull reproducibility, an interactive evaluation web app, and a demo are\navailable on: https://ad.informatik.uni-freiburg.de/publications .\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 18:29:39 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Bast", "Hannah", ""], ["Kalmbach", "Johannes", ""], ["Klumpp", "Theresa", ""], ["Kramer", "Florian", ""], ["Schnelle", "Niklas", ""]]}, {"id": "2104.14828", "submitter": "Mohamed-Amine Baazizi", "authors": "Mohamed-Amine Baazizi and Dario Colazzo and Giorgio Ghelli and Carlo\n  Sartiani and Stefanie Scherzinger", "title": "Not Elimination and Witness Generation for JSON Schema", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  JSON Schema is an evolving standard for the description of families of JSON\ndocuments. JSON Schema is a logical language, based on a set of assertions that\ndescribe features of the JSON value under analysis and on logical or structural\ncombinators for these assertions. As for any logical language, problems like\nsatisfaction, not-elimination, schema satisfiability, schema inclusion and\nequivalence, as well as witness generation, have both theoretical and practical\ninterest. While satisfaction is trivial, all other problems are quite\ndifficult, due to the combined presence of negation, recursion, and complex\nassertions in JSON Schema. To make things even more complex and interesting,\nJSON Schema is not algebraic, since we have both syntactic and semantic\ninteractions between different keywords in the same schema object.\n  With such motivations, we present in this paper an algebraic characterization\nof JSON Schema, obtained by adding opportune operators, and by mirroring\nexisting ones. We present then algebra-based approaches for dealing with\nnot-elimination and witness generation problems, which play a central role as\nthey lead to solutions for the other mentioned complex problems.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 08:21:58 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 15:22:01 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Baazizi", "Mohamed-Amine", ""], ["Colazzo", "Dario", ""], ["Ghelli", "Giorgio", ""], ["Sartiani", "Carlo", ""], ["Scherzinger", "Stefanie", ""]]}, {"id": "2104.14914", "submitter": "Garima Gaur", "authors": "Siddhant Arora, Vinayak Gupta, Garima Gaur, Srikanta Bedathur", "title": "BERT Meets Relational DB: Contextual Representations of Relational\n  Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DB cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we address the problem of learning low dimension\nrepresentation of entities on relational databases consisting of multiple\ntables. Embeddings help to capture semantics encoded in the database and can be\nused in a variety of settings like auto-completion of tables, fully-neural\nquery processing of relational joins queries, seamlessly handling missing\nvalues, and more. Current work is restricted to working with just single table,\nor using pretrained embeddings over an external corpus making them unsuitable\nfor use in real-world databases. In this work, we look into ways of using these\nattention-based model to learn embeddings for entities in the relational\ndatabase. We are inspired by BERT style pretraining methods and are interested\nin observing how they can be extended for representation learning on structured\ndatabases. We evaluate our approach of the autocompletion of relational\ndatabases and achieve improvement over standard baselines.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 11:23:26 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Arora", "Siddhant", ""], ["Gupta", "Vinayak", ""], ["Gaur", "Garima", ""], ["Bedathur", "Srikanta", ""]]}, {"id": "2104.15098", "submitter": "Immanuel Haffner", "authors": "Immanuel Haffner, Jens Dittrich", "title": "Fast Compilation and Execution of SQL Queries with WebAssembly", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Interpreted execution of queries, as in the vectorized model, suffers from\ninterpretation overheads. By compiling queries this interpretation overhead is\neliminated at the cost of a compilation phase that delays execution,\nsacrificing latency for throughput. For short-lived queries, minimizing latency\nis important, while for long-running queries throughput outweighs latency.\nBecause neither a purely interpretive model nor a purely compiling model can\nprovide low latency and high throughput, adaptive solutions emerged. Adaptive\nsystems seamlessly transition from interpreted to compiled execution, achieving\nlow latency for short-lived queries and high throughput for long-running\nqueries. However, these adaptive systems pose an immense development effort and\nrequire expert knowledge in both interpreter and compiler design.\n  In this work, we investigate query execution by compilation to WebAssembly.\nWe are able to compile even complex queries in less than a millisecond to\nmachine code with near-optimal performance. By delegating execution of\nWebAssembly to the V8 engine, we are able to seamlessly transition from rapidly\ncompiled yet non-optimized code to thoroughly optimized code during execution.\nOur approach provides both low latency and high throughput, is adaptive out of\nthe box, and is straight forward to implement. The drastically reduced\ncompilation times even enable us to explore generative programming of library\ncode, that is fully inlined by construction. Our experimental evaluation\nconfirms that our approach yields competitive and sometimes superior\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 16:22:56 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 07:44:27 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Haffner", "Immanuel", ""], ["Dittrich", "Jens", ""]]}]