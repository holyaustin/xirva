[{"id": "1409.0080", "submitter": "Wei Lu", "authors": "Wei Lu, Shanshan Chen, Keqian Li, Laks V.S. Lakshmanan", "title": "Show Me the Money: Dynamic Recommendations for Revenue Maximization", "comments": "Conference version published in PVLDB 7(14). To be presented in the\n  VLDB Conference 2015, in Hawaii. This version gives a detailed submodularity\n  proof", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.GT cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender Systems (RS) play a vital role in applications such as e-commerce\nand on-demand content streaming. Research on RS has mainly focused on the\ncustomer perspective, i.e., accurate prediction of user preferences and\nmaximization of user utilities. As a result, most existing techniques are not\nexplicitly built for revenue maximization, the primary business goal of\nenterprises. In this work, we explore and exploit a novel connection between RS\nand the profitability of a business. As recommendations can be seen as an\ninformation channel between a business and its customers, it is interesting and\nimportant to investigate how to make strategic dynamic recommendations leading\nto maximum possible revenue. To this end, we propose a novel \\model that takes\ninto account a variety of factors including prices, valuations, saturation\neffects, and competition amongst products. Under this model, we study the\nproblem of finding revenue-maximizing recommendation strategies over a finite\ntime horizon. We show that this problem is NP-hard, but approximation\nguarantees can be obtained for a slightly relaxed version, by establishing an\nelegant connection to matroid theory. Given the prohibitively high complexity\nof the approximation algorithm, we also design intelligent heuristics for the\noriginal problem. Finally, we conduct extensive experiments on two real and\nsynthetic datasets and demonstrate the efficiency, scalability, and\neffectiveness our algorithms, and that they significantly outperform several\nintuitive baselines.\n", "versions": [{"version": "v1", "created": "Sat, 30 Aug 2014 04:15:15 GMT"}, {"version": "v2", "created": "Sat, 6 Sep 2014 01:37:15 GMT"}, {"version": "v3", "created": "Tue, 25 Aug 2015 18:21:48 GMT"}], "update_date": "2015-08-26", "authors_parsed": [["Lu", "Wei", ""], ["Chen", "Shanshan", ""], ["Li", "Keqian", ""], ["Lakshmanan", "Laks V. S.", ""]]}, {"id": "1409.0351", "submitter": "Laurent Michel", "authors": "Laurent Michel, Christian Motch, Hoan Ngoc Nguyen and\n  Fran\\c{c}ois-Xavier Pineau", "title": "Building an Archive with Saada", "comments": "18 pages, 5 figures Special VO issue", "journal-ref": null, "doi": "10.1016/j.ascom.2014.08.001", "report-no": null, "categories": "astro-ph.IM cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Saada transforms a set of heterogeneous FITS files or VOTables of various\ncategories (images, tables, spectra ...) in a database without writing code.\nDatabases created with Saada come with a rich Web interface and an Application\nProgramming Interface (API). They support the four most common VO services.\nSuch databases can mix various categories of data in multiple collections. They\nallow a direct access to the original data while providing a homogenous view\nthanks to an internal data model compatible with the characterization axis\ndefined by the VO. The data collections can be bound to each other with\npersistent links making relevant browsing paths and allowing data-mining\noriented queries.\n", "versions": [{"version": "v1", "created": "Mon, 1 Sep 2014 10:20:51 GMT"}], "update_date": "2014-09-02", "authors_parsed": [["Michel", "Laurent", ""], ["Motch", "Christian", ""], ["Nguyen", "Hoan Ngoc", ""], ["Pineau", "Fran\u00e7ois-Xavier", ""]]}, {"id": "1409.0651", "submitter": "Koninika Pal", "authors": "Koninika Pal, Sebastian Michel", "title": "An LSH Index for Computing Kendall's Tau over Top-k Lists", "comments": "6 pages, 8 subfigures, presented in Seventeenth International\n  Workshop on the Web and Databases (WebDB 2014) co-located with ACM SIGMOD2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of similarity search within a set of top-k lists\nunder the Kendall's Tau distance function. This distance describes how related\ntwo rankings are in terms of concordantly and discordantly ordered items. As\ntop-k lists are usually very short compared to the global domain of possible\nitems to be ranked, creating an inverted index to look up overlapping lists is\npossible but does not capture tight enough the similarity measure. In this\nwork, we investigate locality sensitive hashing schemes for the Kendall's Tau\ndistance and evaluate the proposed methods using two real-world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 10:07:27 GMT"}], "update_date": "2014-09-03", "authors_parsed": [["Pal", "Koninika", ""], ["Michel", "Sebastian", ""]]}, {"id": "1409.0798", "submitter": "Aditya Parameswaran", "authors": "Anant Bhardwaj, Souvik Bhattacherjee, Amit Chavan, Amol Deshpande,\n  Aaron J. Elmore, Samuel Madden, Aditya G. Parameswaran", "title": "DataHub: Collaborative Data Science & Dataset Version Management at\n  Scale", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relational databases have limited support for data collaboration, where teams\ncollaboratively curate and analyze large datasets. Inspired by software version\ncontrol systems like git, we propose (a) a dataset version control system,\ngiving users the ability to create, branch, merge, difference and search large,\ndivergent collections of datasets, and (b) a platform, DataHub, that gives\nusers the ability to perform collaborative data analysis building on this\nversion control system. We outline the challenges in providing dataset version\ncontrol at scale.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 17:16:47 GMT"}], "update_date": "2014-09-03", "authors_parsed": [["Bhardwaj", "Anant", ""], ["Bhattacherjee", "Souvik", ""], ["Chavan", "Amit", ""], ["Deshpande", "Amol", ""], ["Elmore", "Aaron J.", ""], ["Madden", "Samuel", ""], ["Parameswaran", "Aditya G.", ""]]}, {"id": "1409.0980", "submitter": "Vilem Vychodil", "authors": "Vilem Vychodil", "title": "Monoidal functional dependencies", "comments": null, "journal-ref": "Journal of Computer and System Sciences 81(7) (2015) 1357-1372", "doi": "10.1016/j.jcss.2015.03.006", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a complete logic for reasoning with functional dependencies (FDs)\nwith semantics defined over classes of commutative integral partially ordered\nmonoids and complete residuated lattices. The dependencies allow us to express\nstronger relationships between attribute values than the ordinary FDs. In our\nsetting, the dependencies not only express that certain values are determined\nby others but also express that similar values of attributes imply similar\nvalues of other attributes. We show complete axiomatization using a system of\nArmstrong-like rules, comment on related computational issues, and the\nrelational vs. propositional semantics of the dependencies.\n", "versions": [{"version": "v1", "created": "Wed, 3 Sep 2014 08:06:22 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2015 20:17:42 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Vychodil", "Vilem", ""]]}, {"id": "1409.1055", "submitter": "Uwe Aickelin", "authors": "Diman Hassan, Uwe Aickelin and Christian Wagner", "title": "Comparison of Distance Metrics for Hierarchical Data in Medical\n  Databases", "comments": "Proceedings of the 2014 World Congress on Computational Intelligence\n  (WCCI 2014), pp. 3636-3643, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distance metrics are broadly used in different research areas and\napplications, such as bio-informatics, data mining and many other fields.\nHowever, there are some metrics, like pq-gram and Edit Distance used\nspecifically for data with a hierarchical structure. Other metrics used for\nnon-hierarchical data are the geometric and Hamming metrics. We have applied\nthese metrics to The Health Improvement Network (THIN) database which has some\nhierarchical data. The THIN data has to be converted into a tree-like structure\nfor the first group of metrics. For the second group of metrics, the data are\nconverted into a frequency table or matrix, then for all metrics, all distances\nare found and normalised. Based on this particular data set, our research\nquestion: which of these metrics is useful for THIN data? This paper compares\nthe metrics, particularly the pq-gram metric on finding the similarities of\npatients' data. It also investigates the similar patients who have the same\nclose distances as well as the metrics suitability for clustering the whole\npatient population. Our results show that the two groups of metrics perform\ndifferently as they represent different structures of the data. Nevertheless,\nall the metrics could represent some similar data of patients as well as\ndiscriminate sufficiently well in clustering the patient population using\n$k$-means clustering algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 3 Sep 2014 12:19:19 GMT"}], "update_date": "2014-09-04", "authors_parsed": [["Hassan", "Diman", ""], ["Aickelin", "Uwe", ""], ["Wagner", "Christian", ""]]}, {"id": "1409.1152", "submitter": "Tanay Kumar Saha", "authors": "Tanay Kumar Saha, Mohammad Al Hasan", "title": "FS^3: A Sampling based method for top-k Frequent Subgraph Mining", "comments": null, "journal-ref": "Statistical Analysis and Data Mining: The ASA Data Science Journal\n  8.4 (2015): 245-261", "doi": "10.1002/sam.11277", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining labeled subgraph is a popular research task in data mining because of\nits potential application in many different scientific domains. All the\nexisting methods for this task explicitly or implicitly solve the subgraph\nisomorphism task which is computationally expensive, so they suffer from the\nlack of scalability problem when the graphs in the input database are large. In\nthis work, we propose FS^3, which is a sampling based method. It mines a small\ncollection of subgraphs that are most frequent in the probabilistic sense. FS^3\nperforms a Markov Chain Monte Carlo (MCMC) sampling over the space of a\nfixed-size subgraphs such that the potentially frequent subgraphs are sampled\nmore often. Besides, FS^3 is equipped with an innovative queue manager. It\nstores the sampled subgraph in a finite queue over the course of mining in such\na manner that the top-k positions in the queue contain the most frequent\nsubgraphs. Our experiments on database of large graphs show that FS^3 is\nefficient, and it obtains subgraphs that are the most frequent amongst the\nsubgraphs of a given size.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 18:45:43 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 12:09:36 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Saha", "Tanay Kumar", ""], ["Hasan", "Mohammad Al", ""]]}, {"id": "1409.1292", "submitter": "Bolin Ding", "authors": "Mohan Yang and Bolin Ding and Surajit Chaudhuri and Kaushik\n  Chakrabarti", "title": "Finding Patterns in a Knowledge Base using Keywords to Compose Table\n  Answers", "comments": "VLDB 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to provide table answers to keyword queries against knowledge bases.\nFor queries referring to multiple entities, like \"Washington cities population\"\nand \"Mel Gibson movies\", it is better to represent each relevant answer as a\ntable which aggregates a set of entities or entity-joins within the same table\nscheme or pattern. In this paper, we study how to find highly relevant patterns\nin a knowledge base for user-given keyword queries to compose table answers. A\nknowledge base can be modeled as a directed graph called knowledge graph, where\nnodes represent entities in the knowledge base and edges represent the\nrelationships among them. Each node/edge is labeled with type and text. A\npattern is an aggregation of subtrees which contain all keywords in the texts\nand have the same structure and types on node/edges. We propose efficient\nalgorithms to find patterns that are relevant to the query for a class of\nscoring functions. We show the hardness of the problem in theory, and propose\npath-based indexes that are affordable in memory. Two query-processing\nalgorithms are proposed: one is fast in practice for small queries (with small\npatterns as answers) by utilizing the indexes; and the other one is better in\ntheory, with running time linear in the sizes of indexes and answers, which can\nhandle large queries better. We also conduct extensive experimental study to\ncompare our approaches with a naive adaption of known techniques.\n", "versions": [{"version": "v1", "created": "Thu, 4 Sep 2014 00:54:16 GMT"}], "update_date": "2014-09-05", "authors_parsed": [["Yang", "Mohan", ""], ["Ding", "Bolin", ""], ["Chaudhuri", "Surajit", ""], ["Chakrabarti", "Kaushik", ""]]}, {"id": "1409.1636", "submitter": "Xiufeng Liu", "authors": "Xiufeng Liu", "title": "Two-level Data Staging ETL for Transaction Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In data warehousing, Extract-Transform-Load (ETL) extracts the data from data\nsources into a central data warehouse regularly for the support of business\ndecision-makings. The data from transaction processing systems are featured\nwith the high frequent changes of insertion, update, and deletion. It is\nchallenging for ETL to propagate the changes to the data warehouse, and\nmaintain the change history. Moreover, ETL jobs typically run in a sequential\norder when processing the data with dependencies, which is not optimal, \\eg,\nwhen processing early-arriving data. In this paper, we propose a two-level data\nstaging ETL for handling transaction data. The proposed method detects the\nchanges of the data from transactional processing systems, identifies the\ncorresponding operation codes for the changes, and uses two staging databases\nto facilitate the data processing in an ETL process. The proposed ETL provides\nthe \"one-stop\" method for fast-changing, slowly-changing and early-arriving\ndata processing.\n", "versions": [{"version": "v1", "created": "Fri, 5 Sep 2014 01:11:03 GMT"}, {"version": "v2", "created": "Sat, 13 Sep 2014 16:34:23 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Liu", "Xiufeng", ""]]}, {"id": "1409.1639", "submitter": "Xiufeng Liu", "authors": "Xiufeng Liu", "title": "Optimizing ETL Dataflow Using Shared Caching and Parallelization Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extract-Transform-Load (ETL) handles large amount of data and manages\nworkload through dataflows. ETL dataflows are widely regarded as complex and\nexpensive operations in terms of time and system resources. In order to\nminimize the time and the resources required by ETL dataflows, this paper\npresents a framework to optimize dataflows using shared cache and\nparallelization techniques. The framework classifies the components in an ETL\ndataflow into different categories based on their data operation properties.\nThe framework then partitions the dataflow based on the classification at\ndifferent granularities. Furthermore, the framework applies optimization\ntechniques such as cache re-using, pipelining and multi-threading to the\nalready-partitioned dataflows. The proposed techniques reduce system memory\nfootprint and the frequency of copying data between different components, and\nalso take full advantage of the computing power of multi-core processors. The\nexperimental results show that the proposed optimization framework is 4.7 times\nfaster than the ordinary ETL dataflows (without using the proposed optimization\ntechniques), and outperforms the similar tool (Kettle).\n", "versions": [{"version": "v1", "created": "Fri, 5 Sep 2014 01:24:34 GMT"}], "update_date": "2014-09-08", "authors_parsed": [["Liu", "Xiufeng", ""]]}, {"id": "1409.2553", "submitter": "Yodsawalai Chodpathumwan", "authors": "Yodsawalai Chodpathumwan (1), Jose Picado (2), Arash Termehchy (2),\n  Alan Fern (2), Yizhou Sun (3) ((1) University of Illinois at\n  Urbana-Champaign, (2) Oregon State University, (3) Northeaster University)", "title": "Representation Independent Analytics Over Structured Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Database analytics algorithms leverage quantifiable structural properties of\nthe data to predict interesting concepts and relationships. The same\ninformation, however, can be represented using many different structures and\nthe structural properties observed over particular representations do not\nnecessarily hold for alternative structures. Thus, there is no guarantee that\ncurrent database analytics algorithms will still provide the correct insights,\nno matter what structures are chosen to organize the database. Because these\nalgorithms tend to be highly effective over some choices of structure, such as\nthat of the databases used to validate them, but not so effective with others,\ndatabase analytics has largely remained the province of experts who can find\nthe desired forms for these algorithms. We argue that in order to make database\nanalytics usable, we should use or develop algorithms that are effective over a\nwide range of choices of structural organizations. We introduce the notion of\nrepresentation independence, study its fundamental properties for a wide range\nof data analytics algorithms, and empirically analyze the amount of\nrepresentation independence of some popular database analytics algorithms. Our\nresults indicate that most algorithms are not generally representation\nindependent and find the characteristics of more representation independent\nheuristics under certain representational shifts.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 00:01:23 GMT"}], "update_date": "2014-09-10", "authors_parsed": [["Chodpathumwan", "Yodsawalai", ""], ["Picado", "Jose", ""], ["Termehchy", "Arash", ""], ["Fern", "Alan", ""], ["Sun", "Yizhou", ""]]}, {"id": "1409.2585", "submitter": "Georgios  Skoumas", "authors": "Georgios Skoumas and Klaus Arthur Schmid and Gregor Joss\\'e and\n  Andreas Z\\\"ufle and Mario A. Nascimento and Matthias Renz and Dieter Pfoser", "title": "Towards Knowledge-Enriched Path Computation", "comments": "Accepted as a short paper at ACM SIGSPATIAL GIS 2014", "journal-ref": null, "doi": "10.1145/2666310.2666485", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Directions and paths, as commonly provided by navigation systems, are usually\nderived considering absolute metrics, e.g., finding the shortest path within an\nunderlying road network. With the aid of crowdsourced geospatial data we aim at\nobtaining paths that do not only minimize distance but also lead through more\npopular areas using knowledge generated by users. We extract spatial relations\nsuch as \"nearby\" or \"next to\" from travel blogs, that define closeness between\npairs of points of interest (PoIs) and quantify each of these relations using a\nprobabilistic model. Subsequently, we create a relationship graph where each\nnode corresponds to a PoI and each edge describes the spatial connection\nbetween the respective PoIs. Using Bayesian inference we obtain a probabilistic\nmeasure of spatial closeness according to the crowd. Applying this measure to\nthe corresponding road network, we obtain an altered cost function which does\nnot exclusively rely on distance, and enriches an actual road networks taking\ncrowdsourced spatial relations into account. Finally, we propose two routing\nalgorithms on the enriched road networks. To evaluate our approach, we use\nFlickr photo data as a ground truth for popularity. Our experimental results --\nbased on real world datasets -- show that the paths computed w.r.t.\\ our\nalternative cost function yield competitive solutions in terms of path length\nwhile also providing more \"popular\" paths, making routing easier and more\ninformative for the user.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 09:51:01 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Skoumas", "Georgios", ""], ["Schmid", "Klaus Arthur", ""], ["Joss\u00e9", "Gregor", ""], ["Z\u00fcfle", "Andreas", ""], ["Nascimento", "Mario A.", ""], ["Renz", "Matthias", ""], ["Pfoser", "Dieter", ""]]}, {"id": "1409.2819", "submitter": "Nasser Ghadiri", "authors": "Bagher Saberi, Nasser Ghadiri", "title": "A Sample-Based Approach to Data Quality Assessment in Spatial Databases\n  with Application to Mobile Trajectory Nearest-Neighbor Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial data is playing an emerging role in new technologies such as web and\nmobile mapping and Geographic Information Systems (GIS). Important decisions in\npolitical, social and many other aspects of modern human life are being made\nusing location data. Decision makers in many countries are exploiting spatial\ndatabases for collecting information, analyzing them and planning for the\nfuture. In fact, not every spatial database is suitable for this type of\napplication. Inaccuracy, imprecision and other deficiencies are present in\nlocation data just as any other type of data and may have a negative impact on\ncredibility of any action taken based on unrefined information. So we need a\nmethod for evaluating the quality of spatial data and separating usable data\nfrom misleading data which leads to weak decisions. On the other hand, spatial\ndatabases are usually huge in size and therefore working with this type of data\nhas a negative impact on efficiency. To improve the efficiency of working with\nspatial big data, we need a method for shrinking the volume of data. Sampling\nis one of these methods, but its negative effects on the quality of data are\ninevitable. In this paper we are trying to show and assess this change in\nquality of spatial data that is a consequence of sampling. We used this\napproach for evaluating the quality of sampled spatial data related to mobile\nuser trajectories in China which are available in a well-known spatial\ndatabase. The results show that sample-based control of data quality will\nincrease the query performance significantly, without losing too much accuracy.\nBased on this results some future improvements are pointed out which will help\nto process location-based queries faster than before and to make more accurate\nlocation-based decisions in limited times.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 17:10:11 GMT"}, {"version": "v2", "created": "Sat, 14 May 2016 08:11:26 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Saberi", "Bagher", ""], ["Ghadiri", "Nasser", ""]]}, {"id": "1409.3021", "submitter": "Ibrahim El Bitar", "authors": "Ibrahim El Bitar, Fatima-Zahra Belouadha and Ounsa Roudies", "title": "Semantic web service discovery approaches: overview and limitations", "comments": "16 pages, 1 figure, 2 tables,\n  http://airccse.org/journal/ijcses/papers/5414ijcses02.pdf volume 5, Number 4,\n  2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The semantic Web service discovery has been given massive attention within\nthe last few years. With the increasing number of Web services available on the\nweb, looking for a particular service has become very difficult, especially\nwith the evolution of the clients needs. In this context, various approaches to\ndiscover semantic Web services have been proposed. In this paper, we compare\nthese approaches in order to assess their maturity and their adaptation to the\ncurrent domain requirements. The outcome of this comparison will help us to\nidentify the mechanisms that constitute the strengths of the existing\napproaches, and thereafter will serve as guideline to determine the basis for a\ndiscovery approach more adapted to the current context of Web services.\n", "versions": [{"version": "v1", "created": "Wed, 10 Sep 2014 11:01:04 GMT"}], "update_date": "2014-09-11", "authors_parsed": [["Bitar", "Ibrahim El", ""], ["Belouadha", "Fatima-Zahra", ""], ["Roudies", "Ounsa", ""]]}, {"id": "1409.3288", "submitter": "Olaf Hartig", "authors": "Olaf Hartig", "title": "Reconciliation of RDF* and Property Graphs", "comments": "slightly changed the definition of PGs and added the notion of\n  property uniqueness", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both the notion of Property Graphs (PG) and the Resource Description\nFramework (RDF) are commonly used models for representing graph-shaped data.\nWhile there exist some system-specific solutions to convert data from one model\nto the other, these solutions are not entirely compatible with one another and\nnone of them appears to be based on a formal foundation. In fact, for the PG\nmodel, there does not even exist a commonly agreed-upon formal definition.\n  The aim of this document is to reconcile both models formally. To this end,\nthe document proposes a formalization of the PG model and introduces\nwell-defined transformations between PGs and RDF. As a result, the document\nprovides a basis for the following two innovations: On one hand, by\nimplementing the RDF-to-PG transformations defined in this document, PG-based\nsystems can enable their users to load RDF data and make it accessible in a\ncompatible, system-independent manner using, e.g., the graph traversal language\nGremlin or the declarative graph query language Cypher. On the other hand, the\nPG-to-RDF transformation in this document enables RDF data management systems\nto support compatible, system-independent queries over the content of Property\nGraphs by using the standard RDF query language SPARQL. Additionally, this\ndocument represents a foundation for systematic research on relationships\nbetween the two models and between their query languages.\n", "versions": [{"version": "v1", "created": "Thu, 11 Sep 2014 01:15:06 GMT"}, {"version": "v2", "created": "Thu, 13 Nov 2014 08:14:35 GMT"}], "update_date": "2014-11-14", "authors_parsed": [["Hartig", "Olaf", ""]]}, {"id": "1409.3530", "submitter": "Alexandr Savinov", "authors": "Alexandr Savinov", "title": "Concept-oriented model: inference in hierarchical multidimensional space", "comments": "19 pages, 14 figures, Full version of the paper published in DATA\n  2012 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spite of its fundamental importance, inference has not been an inherent\nfunction of multidimensional models and analytical applications. These models\nare mainly aimed at numeric (quantitative) analysis where the notions of\ninference and semantics are not well defined. In this paper we argue that\ninference can be and should be integral part of multidimensional data models\nand analytical applications. It is demonstrated how inference can be defined\nusing only multidimensional terms like axes and coordinates as opposed to using\nlogic-based approaches. We propose a novel approach to inference in\nmultidimensional space based on the concept-oriented model of data and\nintroduce elementary operations which are then used to define constraint\npropagation and inference procedures. We describe a query language with\ninference operator and demonstrate its usefulness in solving complex analytical\ntasks.\n", "versions": [{"version": "v1", "created": "Thu, 11 Sep 2014 18:29:32 GMT"}], "update_date": "2014-09-12", "authors_parsed": [["Savinov", "Alexandr", ""]]}, {"id": "1409.3682", "submitter": "Caetano Sauer", "authors": "Caetano Sauer, Theo H\\\"arder", "title": "A novel recovery mechanism enabling fine-granularity locking and fast,\n  REDO-only recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a series of novel techniques and algorithms for transaction\ncommit, logging, recovery, and propagation control. In combination, they\nprovide a recovery component that maintains the persistent state of the\ndatabase (both log and data pages) always in a committed state. Recovery from\nsystem and media failures only requires only REDO operations, which can happen\nconcurrently with the processing of new transactions. The mechanism supports\nfine-granularity locking, partial rollbacks, and snapshot isolation for reader\ntransactions. Our design does not assume a specific hardware configuration such\nas non-volatile RAM or flash---it is designed for traditional disk\nenvironments. Nevertheless, it can exploit modern I/O devices for higher\ntransaction throughput and reduced recovery time with a high degree of\nflexibility.\n", "versions": [{"version": "v1", "created": "Fri, 12 Sep 2014 08:57:24 GMT"}], "update_date": "2014-09-15", "authors_parsed": [["Sauer", "Caetano", ""], ["H\u00e4rder", "Theo", ""]]}, {"id": "1409.3809", "submitter": "Daniel Crankshaw", "authors": "Daniel Crankshaw, Peter Bailis, Joseph E. Gonzalez, Haoyuan Li, Zhao\n  Zhang, Michael J. Franklin, Ali Ghodsi, Michael I. Jordan", "title": "The Missing Piece in Complex Analytics: Low Latency, Scalable Model\n  Management and Serving with Velox", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To support complex data-intensive applications such as personalized\nrecommendations, targeted advertising, and intelligent services, the data\nmanagement community has focused heavily on the design of systems to support\ntraining complex models on large datasets. Unfortunately, the design of these\nsystems largely ignores a critical component of the overall analytics process:\nthe deployment and serving of models at scale. In this work, we present Velox,\na new component of the Berkeley Data Analytics Stack. Velox is a data\nmanagement system for facilitating the next steps in real-world, large-scale\nanalytics pipelines: online model management, maintenance, and serving. Velox\nprovides end-user applications and services with a low-latency, intuitive\ninterface to models, transforming the raw statistical models currently trained\nusing existing offline large-scale compute frameworks into full-blown,\nend-to-end data products capable of recommending products, targeting\nadvertisements, and personalizing web content. To provide up-to-date results\nfor these complex models, Velox also facilitates lightweight online model\nmaintenance and selection (i.e., dynamic weighting). In this paper, we describe\nthe challenges and architectural considerations required to achieve this\nfunctionality, including the abilities to span online and offline systems, to\nadaptively adjust model materialization strategies, and to exploit inherent\nstatistical properties such as model error tolerance, all while operating at\n\"Big Data\" scale.\n", "versions": [{"version": "v1", "created": "Fri, 12 Sep 2014 18:12:24 GMT"}, {"version": "v2", "created": "Mon, 1 Dec 2014 23:20:30 GMT"}], "update_date": "2014-12-03", "authors_parsed": [["Crankshaw", "Daniel", ""], ["Bailis", "Peter", ""], ["Gonzalez", "Joseph E.", ""], ["Li", "Haoyuan", ""], ["Zhang", "Zhao", ""], ["Franklin", "Michael J.", ""], ["Ghodsi", "Ali", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1409.3867", "submitter": "Vishwakarma Singh", "authors": "Vishwakarma Singh and Ambuj K. Singh", "title": "Nearest Keyword Set Search in Multi-dimensional Datasets", "comments": "Accepted as Full Research Paper to ICDE 2014, Chicago, IL, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keyword-based search in text-rich multi-dimensional datasets facilitates many\nnovel applications and tools. In this paper, we consider objects that are\ntagged with keywords and are embedded in a vector space. For these datasets, we\nstudy queries that ask for the tightest groups of points satisfying a given set\nof keywords. We propose a novel method called ProMiSH (Projection and Multi\nScale Hashing) that uses random projection and hash-based index structures, and\nachieves high scalability and speedup. We present an exact and an approximate\nversion of the algorithm. Our empirical studies, both on real and synthetic\ndatasets, show that ProMiSH has a speedup of more than four orders over\nstate-of-the-art tree-based techniques. Our scalability tests on datasets of\nsizes up to 10 million and dimensions up to 100 for queries having up to 9\nkeywords show that ProMiSH scales linearly with the dataset size, the dataset\ndimension, the query size, and the result size.\n", "versions": [{"version": "v1", "created": "Fri, 12 Sep 2014 21:12:16 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Singh", "Vishwakarma", ""], ["Singh", "Ambuj K.", ""]]}, {"id": "1409.4161", "submitter": "Chengkai Li", "authors": "Abolfazl Asudeh, Gensheng Zhang, Naeemul Hassan, Chengkai Li, Gergely\n  V. Zaruba", "title": "Crowdsourcing Pareto-Optimal Object Finding by Pairwise Comparisons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the first study on crowdsourcing Pareto-optimal object finding, which\nhas applications in public opinion collection, group decision making, and\ninformation exploration. Departing from prior studies on crowdsourcing skyline\nand ranking queries, it considers the case where objects do not have explicit\nattributes and preference relations on objects are strict partial orders. The\npartial orders are derived by aggregating crowdsourcers' responses to pairwise\ncomparison questions. The goal is to find all Pareto-optimal objects by the\nfewest possible questions. It employs an iterative question-selection\nframework. Guided by the principle of eagerly identifying non-Pareto optimal\nobjects, the framework only chooses candidate questions which must satisfy\nthree conditions. This design is both sufficient and efficient, as it is proven\nto find a short terminal question sequence. The framework is further steered by\ntwo ideas---macro-ordering and micro-ordering. By different micro-ordering\nheuristics, the framework is instantiated into several algorithms with varying\npower in pruning questions. Experiment results using both real crowdsourcing\nmarketplace and simulations exhibited not only orders of magnitude reductions\nin questions when compared with a brute-force approach, but also\nclose-to-optimal performance from the most efficient instantiation.\n", "versions": [{"version": "v1", "created": "Mon, 15 Sep 2014 06:38:57 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Asudeh", "Abolfazl", ""], ["Zhang", "Gensheng", ""], ["Hassan", "Naeemul", ""], ["Li", "Chengkai", ""], ["Zaruba", "Gergely V.", ""]]}, {"id": "1409.4507", "submitter": "Awny Sayed", "authors": "Awny Sayed and Amal Almaqrashi", "title": "Scalable and Efficient Self-Join Processing technique in RDF data", "comments": "8-pages, 5-figures, International Journal of Computer Science Issues\n  (IJCSI), Volume 11, Issue 2. April 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient management of RDF data plays an important role in successfully\nunderstanding and fast querying data. Although the current approaches of\nindexing in RDF Triples such as property tables and vertically partitioned\nsolved many issues; however, they still suffer from the performance in the\ncomplex self-join queries and insert data in the same table. As an improvement\nin this paper, we propose an alternative solution to facilitate flexibility and\nefficiency in that queries and try to reach to the optimal solution to decrease\nthe self-joins as much as possible, this solution based on the idea of\n\"Recursive Mapping of Twin Tables\". Our main goal of Recursive Mapping of Twin\nTables (RMTT) approach is divided the main RDF Triple into two tables which\nhave the same structure of RDF Triple and insert the RDF data recursively. Our\nexperimental results compared the performance of join queries in vertically\npartitioned approach and the RMTT approach using very large RDF data, like DBLP\nand DBpedia datasets. Our experimental results with a number of complex\nsubmitted queries shows that our approach is highly scalable compared with\nRDF-3X approach and RMTT reduces the number of self-joins especially in complex\nqueries 3-4 times than RDF-3X approach\n", "versions": [{"version": "v1", "created": "Tue, 16 Sep 2014 05:21:06 GMT"}], "update_date": "2014-09-17", "authors_parsed": [["Sayed", "Awny", ""], ["Almaqrashi", "Amal", ""]]}, {"id": "1409.6052", "submitter": "Wolfgang Gatterbauer", "authors": "Wolfgang Gatterbauer and Dan Suciu", "title": "Oblivious Bounds on the Probability of Boolean Functions", "comments": "34 pages, 14 figures, supersedes: http://arxiv.org/abs/1105.2813", "journal-ref": "Pre-print for ACM Transactions on Database Systems, January 2014,\n  Vol 39, No 1, Article 5", "doi": "10.1145/2532641", "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops upper and lower bounds for the probability of Boolean\nfunctions by treating multiple occurrences of variables as independent and\nassigning them new individual probabilities. We call this approach dissociation\nand give an exact characterization of optimal oblivious bounds, i.e. when the\nnew probabilities are chosen independent of the probabilities of all other\nvariables. Our motivation comes from the weighted model counting problem (or,\nequivalently, the problem of computing the probability of a Boolean function),\nwhich is #P-hard in general. By performing several dissociations, one can\ntransform a Boolean formula whose probability is difficult to compute, into one\nwhose probability is easy to compute, and which is guaranteed to provide an\nupper or lower bound on the probability of the original formula by choosing\nappropriate probabilities for the dissociated variables. Our new bounds shed\nlight on the connection between previous relaxation-based and model-based\napproximations and unify them as concrete choices in a larger design space. We\nalso show how our theory allows a standard relational database management\nsystem (DBMS) to both upper and lower bound hard probabilistic queries in\nguaranteed polynomial time.\n", "versions": [{"version": "v1", "created": "Sun, 21 Sep 2014 23:32:34 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Gatterbauer", "Wolfgang", ""], ["Suciu", "Dan", ""]]}, {"id": "1409.6288", "submitter": "Mengmeng Liu", "authors": "Mengmeng Liu, Zachary G. Ives, and Boon Thau Loo", "title": "Enabling Incremental Query Re-Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As declarative query processing techniques expand in scope --- to the Web,\ndata streams, network routers, and cloud platforms --- there is an increasing\nneed for adaptive query processing techniques that can re-plan in the presence\nof failures or unanticipated performance changes. A status update on the data\ndistributions or the compute nodes may have significant repercussions on the\nchoice of which query plan should be running. Ideally, new system architectures\nwould be able to make cost-based decisions about reallocating work, migrating\ndata, etc., and react quickly as real-time status information becomes\navailable. Existing cost-based query optimizers are not incremental in nature,\nand must be run \"from scratch\" upon each status or cost update. Hence, they\ngenerally result in adaptive schemes that can only react slowly to updates.\n  An open question has been whether it is possible to build a cost-based\nre-optimization architecture for adaptive query processing in a streaming or\nrepeated query execution environment, e.g., by incrementally updating optimizer\nstate given new cost information. We show that this can be achieved\nbeneficially, especially for stream processing workloads. Our techniques build\nupon the recently proposed approach of formulating query plan enumeration as a\nset of recursive datalog queries; we develop a variety of novel optimization\napproaches to ensure effective pruning in both static and incremental cases. We\nimplement our solution within an existing research query processing system, and\nshow that it effectively supports cost-based initial optimization as well as\nfrequent adaptivity.\n", "versions": [{"version": "v1", "created": "Mon, 22 Sep 2014 19:38:31 GMT"}], "update_date": "2014-09-23", "authors_parsed": [["Liu", "Mengmeng", ""], ["Ives", "Zachary G.", ""], ["Loo", "Boon Thau", ""]]}, {"id": "1409.6428", "submitter": "Laure Berti", "authors": "Dalia Attia Waguih and Laure Berti-Equille (Qatar Computing Research\n  Institute)", "title": "Truth Discovery Algorithms: An Experimental Evaluation", "comments": "13 pages, 17 figures, Qatar Computing Research Institute Technical\n  Report, May 2014", "journal-ref": null, "doi": null, "report-no": "QCRI Technical Report, May 2014", "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  A fundamental problem in data fusion is to determine the veracity of\nmulti-source data in order to resolve conflicts. While previous work in truth\ndiscovery has proved to be useful in practice for specific settings, sources'\nbehavior or data set characteristics, there has been limited systematic\ncomparison of the competing methods in terms of efficiency, usability, and\nrepeatability. We remedy this deficit by providing a comprehensive review of 12\nstate-of-the art algorithms for truth discovery. We provide reference\nimplementations and an in-depth evaluation of the methods based on extensive\nexperiments on synthetic and real-world data. We analyze aspects of the problem\nthat have not been explicitly studied before, such as the impact of\ninitialization and parameter setting, convergence, and scalability. We provide\nan experimental framework for extensively comparing the methods in a wide range\nof truth discovery scenarios where source coverage, numbers and distributions\nof conflicts, and true positive claims can be controlled and used to evaluate\nthe quality and performance of the algorithms. Finally, we report comprehensive\nfindings obtained from the experiments and provide new insights for future\nresearch.\n", "versions": [{"version": "v1", "created": "Tue, 23 Sep 2014 07:11:31 GMT"}], "update_date": "2014-09-24", "authors_parsed": [["Waguih", "Dalia Attia", "", "Qatar Computing Research\n  Institute"], ["Berti-Equille", "Laure", "", "Qatar Computing Research\n  Institute"]]}, {"id": "1409.6548", "submitter": "Bernhard Rumpe", "authors": "Eshref Januzaj, Hans-Peter Kriegel, Martin Pfeifle", "title": "Scalable Density-Based Distributed Clustering", "comments": "12 pages, 11 figures", "journal-ref": "8th European Conference on Principles and Practice of Knowledge\n  Discovery in Databases (PKDD) Pisa, Italy, September 20-24, 2004, J.-F.\n  Boulicaut et al. (Eds.): LNAI 3202, pp 231-244, Springer-Verlag Berlin\n  Heidelberg 2004", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering has become an increasingly important task in analysing huge\namounts of data. Traditional applications require that all data has to be\nlocated at the site where it is scrutinized. Nowadays, large amounts of\nheterogeneous, complex data reside on different, independently working\ncomputers which are connected to each other via local or wide area networks. In\nthis paper, we propose a scalable density-based distributed clustering\nalgorithm which allows a user-defined trade-off between clustering quality and\nthe number of transmitted objects from the different local sites to a global\nserver site. Our approach consists of the following steps: First, we order all\nobjects located at a local site according to a quality criterion reflecting\ntheir suitability to serve as local representatives. Then we send the best of\nthese representatives to a server site where they are clustered with a slightly\nenhanced density-based clustering algorithm. This approach is very efficient,\nbecause the local detemination of suitable representatives can be carried out\nquickly and independently from each other. Furthermore, based on the scalable\nnumber of the most suitable local representatives, the global clustering can be\ndone very effectively and efficiently. In our experimental evaluation, we will\nshow that our new scalable density-based distributed clustering approach\nresults in high quality clusterings with scalable transmission cost.\n", "versions": [{"version": "v1", "created": "Mon, 22 Sep 2014 17:23:36 GMT"}], "update_date": "2014-09-24", "authors_parsed": [["Januzaj", "Eshref", ""], ["Kriegel", "Hans-Peter", ""], ["Pfeifle", "Martin", ""]]}, {"id": "1409.6848", "submitter": "Xinquan Chen", "authors": "Xinquan Chen", "title": "A New Clustering Algorithm Based on Near Neighbor Influence", "comments": "21 pages, 9 figures, and 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Clustering based on Near Neighbor Influence (CNNI), a new\nclustering algorithm which is inspired by the idea of near neighbor and the\nsuperposition principle of influence. In order to clearly describe this\nalgorithm, it introduces some important concepts, such as near neighbor point\nset, near neighbor influence, and similarity measure. By simulated experiments\nof some artificial data sets and seven real data sets, we observe that this\nalgorithm can often get good clustering quality when making proper value of\nsome parameters. At last, it gives some research expectations to popularize\nthis algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 24 Sep 2014 07:20:20 GMT"}], "update_date": "2014-09-25", "authors_parsed": [["Chen", "Xinquan", ""]]}, {"id": "1409.7311", "submitter": "Matthijs van Leeuwen", "authors": "Matthijs van Leeuwen and Antti Ukkonen", "title": "Estimating the pattern frequency spectrum inside the browser", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a browser application for estimating the number of frequent\npatterns, in particular itemsets, as well as the pattern frequency spectrum.\nThe pattern frequency spectrum is defined as the function that shows for every\nvalue of the frequency threshold $\\sigma$ the number of patterns that are\nfrequent in a given dataset. Our demo implements a recent algorithm proposed by\nthe authors for finding the spectrum. The demo is 100% JavaScript, and runs in\nall modern browsers. We observe that modern JavaScript engines can deliver\nperformance that makes it viable to run non-trivial data analysis algorithms in\nbrowser applications.\n", "versions": [{"version": "v1", "created": "Thu, 25 Sep 2014 16:08:14 GMT"}, {"version": "v2", "created": "Tue, 30 Sep 2014 15:22:51 GMT"}], "update_date": "2014-10-01", "authors_parsed": [["van Leeuwen", "Matthijs", ""], ["Ukkonen", "Antti", ""]]}, {"id": "1409.7472", "submitter": "Jiannan Wang", "authors": "Jiannan Wang, Guoliang Li, Tim Kraska, Michael J. Franklin, Jianhua\n  Feng", "title": "The Expected Optimal Labeling Order Problem for Crowdsourced Joins and\n  Entity Resolution", "comments": "This is a note for explaining an incorrect claim in our SIGMOD 2013\n  paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the SIGMOD 2013 conference, we published a paper extending our earlier\nwork on crowdsourced entity resolution to improve crowdsourced join processing\nby exploiting transitive relationships [Wang et al. 2013]. The VLDB 2014\nconference has a paper that follows up on our previous work [Vesdapunt et al.,\n2014], which points out and corrects a mistake we made in our SIGMOD paper.\nSpecifically, in Section 4.2 of our SIGMOD paper, we defined the \"Expected\nOptimal Labeling Order\" (EOLO) problem, and proposed an algorithm for solving\nit. We incorrectly claimed that our algorithm is optimal. In their paper,\nVesdapunt et al. show that the problem is actually NP-Hard, and based on that\nobservation, propose a new algorithm to solve it. In this note, we would like\nto put the Vesdapunt et al. results in context, something we believe that their\npaper does not adequately do.\n", "versions": [{"version": "v1", "created": "Fri, 26 Sep 2014 06:01:00 GMT"}], "update_date": "2014-09-29", "authors_parsed": [["Wang", "Jiannan", ""], ["Li", "Guoliang", ""], ["Kraska", "Tim", ""], ["Franklin", "Michael J.", ""], ["Feng", "Jianhua", ""]]}, {"id": "1409.7777", "submitter": "Thomas Guyet", "authors": "Thomas Guyet (INRIA - IRISA), Yves Moinard (INRIA - IRISA), Ren\\'e\n  Quiniou (INRIA - IRISA)", "title": "Using Answer Set Programming for pattern mining", "comments": "Intelligence Artificielle Fondamentale (2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Serial pattern mining consists in extracting the frequent sequential patterns\nfrom a unique sequence of itemsets. This paper explores the ability of a\ndeclarative language, such as Answer Set Programming (ASP), to solve this issue\nefficiently. We propose several ASP implementations of the frequent sequential\npattern mining task: a non-incremental and an incremental resolution. The\nresults show that the incremental resolution is more efficient than the\nnon-incremental one, but both ASP programs are less efficient than dedicated\nalgorithms. Nonetheless, this approach can be seen as a first step toward a\ngeneric framework for sequential pattern mining with constraints.\n", "versions": [{"version": "v1", "created": "Sat, 27 Sep 2014 07:27:17 GMT"}], "update_date": "2014-09-30", "authors_parsed": [["Guyet", "Thomas", "", "INRIA - IRISA"], ["Moinard", "Yves", "", "INRIA - IRISA"], ["Quiniou", "Ren\u00e9", "", "INRIA - IRISA"]]}]