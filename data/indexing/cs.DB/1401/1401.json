[{"id": "1401.0069", "submitter": "Davide Martinenghi", "authors": "Davide Martinenghi", "title": "Determining Relevant Relations for Datalog Queries under Access\n  Limitations is Undecidable", "comments": "2 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Access limitations are restrictions in the way in which the tuples of a\nrelation can be accessed. Under access limitations, query answering becomes\nmore complex than in the traditional case, with no guarantee that the answer\ntuples that can be extracted (aka maximal answer) are all those that would be\nfound without access limitations (aka complete answer). The field of query\nanswering under access limitations has been broadly investigated in the past.\nAttention has been devoted to the problem of determining relations that are\nrelevant for a query, i.e., those (possibly off-query) relations that might\nneed to be accessed in order to find all tuples in the maximal answer. In this\nshort paper, we show that relevance is undecidable for Datalog queries.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2013 01:59:21 GMT"}, {"version": "v2", "created": "Fri, 3 Jan 2014 11:18:31 GMT"}], "update_date": "2014-01-06", "authors_parsed": [["Martinenghi", "Davide", ""]]}, {"id": "1401.0494", "submitter": "Minyar Sassi", "authors": "Ines Benali-Sougui, Minyar Sassi-Hidri, Amel Grissa-Touzi", "title": "Flexible SQLf query based on fuzzy linguistic summaries", "comments": null, "journal-ref": "International Conference on Control, Engineering & Information\n  Technology (CEIT), Proceedings Engineering & Technology, Vol. 1, pp. 175-180,\n  2013", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data is often partially known, vague or ambiguous in many real world\napplications. To deal with such imprecise information, fuzziness is introduced\nin the classical model. SQLf is one of the practical language to deal with\nflexible fuzzy querying in Fuzzy DataBases (FDB). However, with a huge amount\nof fuzzy data, the necessity to work with synthetic views became a challenge\nfor many DB community researchers. The present work deals with Flexible SQLf\nquery based on fuzzy linguistic summaries. We use the fuzzy summaries produced\nby our Fuzzy-SaintEtiq approach. It provides a description of objects depending\non the fuzzy linguistic labels specified as selection criteria.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2014 18:14:33 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Benali-Sougui", "Ines", ""], ["Sassi-Hidri", "Minyar", ""], ["Grissa-Touzi", "Amel", ""]]}, {"id": "1401.0987", "submitter": "Chi Jin", "authors": "Chi Jin, Ziteng Wang, Junliang Huang, Yiqiao Zhong, Liwei Wang", "title": "Differentially Private Data Releasing for Smooth Queries with Synthetic\n  Database Output", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider accurately answering smooth queries while preserving differential\nprivacy. A query is said to be $K$-smooth if it is specified by a function\ndefined on $[-1,1]^d$ whose partial derivatives up to order $K$ are all\nbounded. We develop an $\\epsilon$-differentially private mechanism for the\nclass of $K$-smooth queries. The major advantage of the algorithm is that it\noutputs a synthetic database. In real applications, a synthetic database output\nis appealing. Our mechanism achieves an accuracy of $O\n(n^{-\\frac{K}{2d+K}}/\\epsilon )$, and runs in polynomial time. We also\ngeneralize the mechanism to preserve $(\\epsilon, \\delta)$-differential privacy\nwith slightly improved accuracy. Extensive experiments on benchmark datasets\ndemonstrate that the mechanisms have good accuracy and are efficient.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2014 05:12:01 GMT"}], "update_date": "2014-01-07", "authors_parsed": [["Jin", "Chi", ""], ["Wang", "Ziteng", ""], ["Huang", "Junliang", ""], ["Zhong", "Yiqiao", ""], ["Wang", "Liwei", ""]]}, {"id": "1401.1043", "submitter": "Ibrahim A", "authors": "A. Ibrahim, Shivakumar Sastry and P.S. Sastry", "title": "Discovering Compressing Serial Episodes from Event Sequences", "comments": "27 pages 3 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most pattern mining methods output a very large number of frequent patterns\nand isolating a small but relevant subset is a challenging problem of current\ninterest in frequent pattern mining. In this paper we consider discovery of a\nsmall set of relevant frequent episodes from data sequences. We make use of the\nMinimum Description Length principle to formulate the problem of selecting a\nsubset of episodes. Using an interesting class of serial episodes with\ninter-event constraints and a novel encoding scheme for data using such\nepisodes, we present algorithms for discovering small set of episodes that\nachieve good data compression. Using an example of the data streams obtained\nfrom distributed sensors in a composable coupled conveyor system, we show that\nour method is very effective in unearthing highly relevant episodes and that\nour scheme also achieves good data compression.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2014 11:16:01 GMT"}, {"version": "v2", "created": "Sat, 11 Oct 2014 10:47:52 GMT"}], "update_date": "2014-10-14", "authors_parsed": [["Ibrahim", "A.", ""], ["Sastry", "Shivakumar", ""], ["Sastry", "P. S.", ""]]}, {"id": "1401.1174", "submitter": "Hessam Zakerzadeh", "authors": "Hessam Zakerzadeh, Charu C. Aggrawal and Ken Barker", "title": "Towards Breaking the Curse of Dimensionality for High-Dimensional\n  Privacy: An Extended Version", "comments": "13 pages, An extended version of the paper accepted in 2014 SIAM\n  international conference on Data Mining", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The curse of dimensionality has remained a challenge for a wide variety of\nalgorithms in data mining, clustering, classification and privacy. Recently, it\nwas shown that an increasing dimensionality makes the data resistant to\neffective privacy. The theoretical results seem to suggest that the\ndimensionality curse is a fundamental barrier to privacy preservation. However,\nin practice, we show that some of the common properties of real data can be\nleveraged in order to greatly ameliorate the negative effects of the curse of\ndimensionality. In real data sets, many dimensions contain high levels of\ninter-attribute correlations. Such correlations enable the use of a process\nknown as vertical fragmentation in order to decompose the data into vertical\nsubsets of smaller dimensionality. An information-theoretic criterion of mutual\ninformation is used in the vertical decomposition process. This allows the use\nof an anonymization process, which is based on combining results from multiple\nindependent fragments. We present a general approach which can be applied to\nthe k-anonymity, l-diversity, and t-closeness models. In the presence of\ninter-attribute correlations, such an approach continues to be much more robust\nin higher dimensionality, without losing accuracy. We present experimental\nresults illustrating the effectiveness of the approach. This approach is\nresilient enough to prevent identity, attribute, and membership disclosure\nattack.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2014 19:35:23 GMT"}], "update_date": "2014-01-07", "authors_parsed": [["Zakerzadeh", "Hessam", ""], ["Aggrawal", "Charu C.", ""], ["Barker", "Ken", ""]]}, {"id": "1401.1302", "submitter": "Saravanan Thirumuruanathan", "authors": "Senjuti Basu Roy, Ioanna Lykourentzou, Saravanan Thirumuruganathan,\n  Sihem Amer-Yahia, Gautam Das", "title": "Optimization in Knowledge-Intensive Crowdsourcing", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SmartCrowd, a framework for optimizing collaborative\nknowledge-intensive crowdsourcing. SmartCrowd distinguishes itself by\naccounting for human factors in the process of assigning tasks to workers.\nHuman factors designate workers' expertise in different skills, their expected\nminimum wage, and their availability. In SmartCrowd, we formulate task\nassignment as an optimization problem, and rely on pre-indexing workers and\nmaintaining the indexes adaptively, in such a way that the task assignment\nprocess gets optimized both qualitatively, and computation time-wise. We\npresent rigorous theoretical analyses of the optimization problem and propose\noptimal and approximation algorithms. We finally perform extensive performance\nand quality experiments using real and synthetic data to demonstrate that\nadaptive indexing in SmartCrowd is necessary to achieve efficient high quality\ntask assignment.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2014 08:02:45 GMT"}], "update_date": "2014-01-08", "authors_parsed": [["Roy", "Senjuti Basu", ""], ["Lykourentzou", "Ioanna", ""], ["Thirumuruganathan", "Saravanan", ""], ["Amer-Yahia", "Sihem", ""], ["Das", "Gautam", ""]]}, {"id": "1401.1406", "submitter": "Jianfeng Zhan", "authors": "Lei Wang, Jianfeng Zhan, Chunjie Luo, Yuqing Zhu, Qiang Yang,\n  Yongqiang He, Wanling Gao, Zhen Jia, Yingjie Shi, Shujie Zhang, Chen Zheng,\n  Gang Lu, Kent Zhan, Xiaona Li, and Bizhu Qiu", "title": "BigDataBench: a Big Data Benchmark Suite from Internet Services", "comments": "12 pages, 6 figures, The 20th IEEE International Symposium On High\n  Performance Computer Architecture (HPCA-2014), February 15-19, 2014, Orlando,\n  Florida, USA", "journal-ref": null, "doi": "10.1109/HPCA.2014.6835958", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As architecture, systems, and data management communities pay greater\nattention to innovative big data systems and architectures, the pressure of\nbenchmarking and evaluating these systems rises. Considering the broad use of\nbig data systems, big data benchmarks must include diversity of data and\nworkloads. Most of the state-of-the-art big data benchmarking efforts target\nevaluating specific types of applications or system software stacks, and hence\nthey are not qualified for serving the purposes mentioned above. This paper\npresents our joint research efforts on this issue with several industrial\npartners. Our big data benchmark suite BigDataBench not only covers broad\napplication scenarios, but also includes diverse and representative data sets.\nBigDataBench is publicly available from http://prof.ict.ac.cn/BigDataBench .\nAlso, we comprehensively characterize 19 big data workloads included in\nBigDataBench with varying data inputs. On a typical state-of-practice\nprocessor, Intel Xeon E5645, we have the following observations: First, in\ncomparison with the traditional benchmarks: including PARSEC, HPCC, and\nSPECCPU, big data applications have very low operation intensity; Second, the\nvolume of data input has non-negligible impact on micro-architecture\ncharacteristics, which may impose challenges for simulation-based big data\narchitecture research; Last but not least, corroborating the observations in\nCloudSuite and DCBench (which use smaller data inputs), we find that the\nnumbers of L1 instruction cache misses per 1000 instructions of the big data\napplications are higher than in the traditional benchmarks; also, we find that\nL3 caches are effective for the big data applications, corroborating the\nobservation in DCBench.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2014 12:35:31 GMT"}, {"version": "v2", "created": "Sat, 22 Feb 2014 20:25:01 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Wang", "Lei", ""], ["Zhan", "Jianfeng", ""], ["Luo", "Chunjie", ""], ["Zhu", "Yuqing", ""], ["Yang", "Qiang", ""], ["He", "Yongqiang", ""], ["Gao", "Wanling", ""], ["Jia", "Zhen", ""], ["Shi", "Yingjie", ""], ["Zhang", "Shujie", ""], ["Zheng", "Chen", ""], ["Lu", "Gang", ""], ["Zhan", "Kent", ""], ["Li", "Xiaona", ""], ["Qiu", "Bizhu", ""]]}, {"id": "1401.1872", "submitter": "Paraschos Koutris", "authors": "Paul Beame, Paraschos Koutris and Dan Suciu", "title": "Skew in Parallel Query Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of computing a conjunctive query q in parallel, using p\nof servers, on a large database. We consider algorithms with one round of\ncommunication, and study the complexity of the communication. We are especially\ninterested in the case where the data is skewed, which is a major challenge for\nscalable parallel query processing. We establish a tight connection between the\nfractional edge packings of the query and the amount of communication, in two\ncases. First, in the case when the only statistics on the database are the\ncardinalities of the input relations, and the data is skew-free, we provide\nmatching upper and lower bounds (up to a poly log p factor) expressed in terms\nof fractional edge packings of the query q. Second, in the case when the\nrelations are skewed and the heavy hitters and their frequencies are known, we\nprovide upper and lower bounds (up to a poly log p factor) expressed in terms\nof packings of residual queries obtained by specializing the query to a heavy\nhitter. All our lower bounds are expressed in the strongest form, as number of\nbits needed to be communicated between processors with unlimited computational\npower. Our results generalizes some prior results on uniform databases (where\neach relation is a matching) [4], and other lower bounds for the MapReduce\nmodel [1].\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2014 01:07:06 GMT"}], "update_date": "2014-01-10", "authors_parsed": [["Beame", "Paul", ""], ["Koutris", "Paraschos", ""], ["Suciu", "Dan", ""]]}, {"id": "1401.1919", "submitter": "Silu Huang", "authors": "Silu Huang, James Cheng, Huanhuan Wu", "title": "Temporal Graph Traversals: Definitions, Algorithms, and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A temporal graph is a graph in which connections between vertices are active\nat specific times, and such temporal information leads to completely new\npatterns and knowledge that are not present in a non-temporal graph. In this\npaper, we study traversal problems in a temporal graph. Graph traversals, such\nas DFS and BFS, are basic operations for processing and studying a graph. While\nboth DFS and BFS are well-known simple concepts, it is non-trivial to adopt the\nsame notions from a non-temporal graph to a temporal graph. We analyze the\ndifficulties of defining temporal graph traversals and propose new definitions\nof DFS and BFS for a temporal graph. We investigate the properties of temporal\nDFS and BFS, and propose efficient algorithms with optimal complexity. In\nparticular, we also study important applications of temporal DFS and BFS. We\nverify the efficiency and importance of our graph traversal algorithms in real\nworld temporal graphs.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2014 08:04:53 GMT"}], "update_date": "2014-01-10", "authors_parsed": [["Huang", "Silu", ""], ["Cheng", "James", ""], ["Wu", "Huanhuan", ""]]}, {"id": "1401.2101", "submitter": "Massimo Carro", "authors": "Massimo Carro", "title": "NoSQL Databases", "comments": "57 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this document, I present the main notions of NoSQL databases and compare\nfour selected products (Riak, MongoDB, Cassandra, Neo4J) according to their\ncapabilities with respect to consistency, availability, and partition\ntolerance, as well as performance. I also propose a few criteria for selecting\nthe right tool for the right situation.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2014 17:55:33 GMT"}], "update_date": "2014-01-10", "authors_parsed": [["Carro", "Massimo", ""]]}, {"id": "1401.2250", "submitter": "Md. Palash Uddin Palash", "authors": "Md. Palash Uddin, Ashfaque Ahmed, Md. Delowar Hossain, Masud Ibn\n  Afjal, and Shah Md. Tanvir Siddiquee", "title": "High speed data retrieval from national data center (ndc) reducing time\n  and ignoring spelling error in search key based on double metaphone algorithm", "comments": "Page: 23-32, International Journal of Computer Science, Engineering\n  and Applications (IJCSEA) Vol.2, No.6, December 2013,\n  http://airccse.org/journal/ijcsea/index.html", "journal-ref": null, "doi": "10.5121/ijcsea.2013.3601", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast and efficient data management is one of the demanding technologies of\ntodays aspect. This paper proposes a system which makes the working procedures\nof present manual system of storing and retrieving huge citizens information of\nBangladesh automated and increases its effectiveness. The implemented search\nmethodology is user friendly and efficient enough for high speed data retrieval\nignoring spelling error in the input keywords used for searching a particular\ncitizen. The main concern in this research is minimizing the total searching\ntime for a given keyword. This can be done if we can pre-establish the idea of\ngetting the data belonging to the searching keyword. The primary and secondary\nkey-code generated by the Double Metaphone Algorithm for each word is used to\nestablish that idea about the word. This algorithm is used for creating the map\nof the original database, through which the keyword is matched against the\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2014 08:21:22 GMT"}], "update_date": "2014-01-13", "authors_parsed": [["Uddin", "Md. Palash", ""], ["Ahmed", "Ashfaque", ""], ["Hossain", "Md. Delowar", ""], ["Afjal", "Masud Ibn", ""], ["Siddiquee", "Shah Md. Tanvir", ""]]}, {"id": "1401.2327", "submitter": "Waqas Nawaz", "authors": "Kamran Najeebullah, Kifayat Ullah Khan, Waqas Nawaz, Young-Koo Lee", "title": "BPP: Large Graph Storage for Efficient Disk Based Processing", "comments": "5 pages, Published in ICCA, 2013", "journal-ref": "Advanced Science and Technology Letters Vol.30 (ICCA 2013),\n  pp.117-121", "doi": "10.14257/astl.2013.30.25", "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Processing very large graphs like social networks, biological and chemical\ncompounds is a challenging task. Distributed graph processing systems process\nthe billion-scale graphs efficiently but incur overheads of efficient\npartitioning and distribution of the graph over a cluster of nodes. Distributed\nprocessing also requires cluster management and fault tolerance. In order to\novercome these problems GraphChi was proposed recently. GraphChi significantly\noutperformed all the representative distributed processing frameworks. Still,\nwe observe that GraphChi incurs some serious degradation in performance due to\n1) high number of non-sequential I/Os for processing every chunk of graph; and\n2) lack of true parallelism to process the graph. In this paper we propose a\nsimple yet powerful engine BiShard Parallel Processor (BPP) to efficiently\nprocess billions-scale graphs on a single PC. We extend the storage structure\nproposed by GraphChi and introduce a new processing model called BiShard\nParallel (BP). BP enables full CPU parallelism for processing the graph and\nsignificantly reduces the number of non-sequential I/Os required to process\nevery chunk of the graph. Our experiments on real large graphs show that our\nsolution significantly outperforms GraphChi.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2014 13:36:21 GMT"}], "update_date": "2014-01-13", "authors_parsed": [["Najeebullah", "Kamran", ""], ["Khan", "Kifayat Ullah", ""], ["Nawaz", "Waqas", ""], ["Lee", "Young-Koo", ""]]}, {"id": "1401.2571", "submitter": "Mahmood A. Rashid", "authors": "Mahmood A. Rashid, Md Tamjidul Hoque and Abdul Sattar", "title": "Association Rules Mining Based Clinical Observations", "comments": "5 pages, MEDINFO 2010, C. Safran et al. (Eds.), IOS Press", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Healthcare institutes enrich the repository of patients' disease related\ninformation in an increasing manner which could have been more useful by\ncarrying out relational analysis. Data mining algorithms are proven to be quite\nuseful in exploring useful correlations from larger data repositories. In this\npaper we have implemented Association Rules mining based a novel idea for\nfinding co-occurrences of diseases carried by a patient using the healthcare\nrepository. We have developed a system-prototype for Clinical State Correlation\nPrediction (CSCP) which extracts data from patients' healthcare database,\ntransforms the OLTP data into a Data Warehouse by generating association rules.\nThe CSCP system helps reveal relations among the diseases. The CSCP system\npredicts the correlation(s) among primary disease (the disease for which the\npatient visits the doctor) and secondary disease/s (which is/are other\nassociated disease/s carried by the same patient having the primary disease).\n", "versions": [{"version": "v1", "created": "Sat, 11 Jan 2014 21:56:45 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Rashid", "Mahmood A.", ""], ["Hoque", "Md Tamjidul", ""], ["Sattar", "Abdul", ""]]}, {"id": "1401.2690", "submitter": "Shuai Ma", "authors": "Shuai Ma, Kaiyu Feng, Haixun Wang, Jianxin Li, Jinpeng Huai", "title": "Distance Landmarks Revisited for Road Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Computing shortest distances is one of the fundamental problems on graphs,\nand remains a {\\em challenging} task today. {\\em Distance} {\\em landmarks} have\nbeen recently studied for shortest distance queries with an auxiliary data\nstructure, referred to as {\\em landmark} {\\em covers}. This paper studies how\nto apply distance landmarks for fast {\\em exact} shortest distance query\nanswering on large road graphs. However, the {\\em direct} application of\ndistance landmarks is {\\em impractical} due to the high space and time cost. To\nrectify this problem, we investigate novel techniques that can be seamlessly\ncombined with distance landmarks. We first propose a notion of {\\em hybrid\nlandmark covers}, a revision of landmark covers. Second, we propose a notion of\n{\\em agents}, each of which represents a small subgraph and holds good\nproperties for fast distance query answering. We also show that agents can be\ncomputed in {\\em linear time}. Third, we introduce graph partitions to deal\nwith the remaining subgraph that cannot be captured by agents. Fourth, we\ndevelop a unified framework that seamlessly integrates our proposed techniques\nand existing optimization techniques, for fast shortest distance query\nanswering. Finally, we experimentally verify that our techniques significantly\nimprove the efficiency of shortest distance queries, using real-life road\ngraphs.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2014 00:56:12 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Ma", "Shuai", ""], ["Feng", "Kaiyu", ""], ["Wang", "Haixun", ""], ["Li", "Jianxin", ""], ["Huai", "Jinpeng", ""]]}, {"id": "1401.2911", "submitter": "Raju Dara", "authors": "Raju Dara, Dr.Ch. Satyanarayana, Dr.A.Govardhan", "title": "Front End Data Cleaning And Transformation In Standard Printed Form\n  Using Neural Models", "comments": "11 pages, 13 figures, International Journal on Computational Sciences\n  & Applications (IJCSA) Vol.3, No.6, December 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Front end of data collection and loading into database manually may cause\npotential errors in data sets and a very time consuming process. Scanning of a\ndata document in the form of an image and recognition of corresponding\ninformation in that image can be considered as a possible solution of this\nchallenge. This paper presents an automated solution for the problem of data\ncleansing and recognition of user written data to transform into standard\nprinted format with the help of artificial neural networks. Three different\nneural models namely direct, correlation based and hierarchical have been\ndeveloped to handle this issue. In a very hostile input environment, the\nsolution is developed to justify the proposed logic.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2014 13:55:29 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Dara", "Raju", ""], ["Satyanarayana", "Dr. Ch.", ""], ["Govardhan", "Dr. A.", ""]]}, {"id": "1401.3201", "submitter": "Chongjing Sun", "authors": "Chongjing Sun, Philip S. Yu, Xiangnan Kong and Yan Fu", "title": "Privacy Preserving Social Network Publication Against Mutual Friend\n  Attacks", "comments": "10 pages, 11 figures, extended version of a paper in the 4th IEEE\n  International Workshop on Privacy Aspects of Data Mining(PADM2013)", "journal-ref": null, "doi": "10.1109/ICDMW.2013.71", "report-no": null, "categories": "cs.DB cs.CR cs.SI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Publishing social network data for research purposes has raised serious\nconcerns for individual privacy. There exist many privacy-preserving works that\ncan deal with different attack models. In this paper, we introduce a novel\nprivacy attack model and refer it as a mutual friend attack. In this model, the\nadversary can re-identify a pair of friends by using their number of mutual\nfriends. To address this issue, we propose a new anonymity concept, called\nk-NMF anonymity, i.e., k-anonymity on the number of mutual friends, which\nensures that there exist at least k-1 other friend pairs in the graph that\nshare the same number of mutual friends. We devise algorithms to achieve the\nk-NMF anonymity while preserving the original vertex set in the sense that we\nallow the occasional addition but no deletion of vertices. Further we give an\nalgorithm to ensure the k-degree anonymity in addition to the k-NMF anonymity.\nThe experimental results on real-word datasets demonstrate that our approach\ncan preserve the privacy and utility of social networks effectively against\nmutual friend attacks.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2013 03:52:41 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Sun", "Chongjing", ""], ["Yu", "Philip S.", ""], ["Kong", "Xiangnan", ""], ["Fu", "Yan", ""]]}, {"id": "1401.3531", "submitter": "Ben Fulcher", "authors": "Ben D. Fulcher and Nick S. Jones", "title": "Highly comparative feature-based time-series classification", "comments": null, "journal-ref": "IEEE Trans. Knowl. Data Eng. 26, 3026 (2014)", "doi": "10.1109/TKDE.2014.2316504", "report-no": null, "categories": "cs.LG cs.AI cs.DB physics.data-an q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A highly comparative, feature-based approach to time series classification is\nintroduced that uses an extensive database of algorithms to extract thousands\nof interpretable features from time series. These features are derived from\nacross the scientific time-series analysis literature, and include summaries of\ntime series in terms of their correlation structure, distribution, entropy,\nstationarity, scaling properties, and fits to a range of time-series models.\nAfter computing thousands of features for each time series in a training set,\nthose that are most informative of the class structure are selected using\ngreedy forward feature selection with a linear classifier. The resulting\nfeature-based classifiers automatically learn the differences between classes\nusing a reduced number of time-series properties, and circumvent the need to\ncalculate distances between time series. Representing time series in this way\nresults in orders of magnitude of dimensionality reduction, allowing the method\nto perform well on very large datasets containing long time series or time\nseries of different lengths. For many of the datasets studied, classification\nperformance exceeded that of conventional instance-based classifiers, including\none nearest neighbor classifiers using Euclidean distances and dynamic time\nwarping and, most importantly, the features selected provide an understanding\nof the properties of the dataset, insight that can guide further scientific\ninvestigation.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 09:41:50 GMT"}, {"version": "v2", "created": "Fri, 9 May 2014 00:05:57 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Fulcher", "Ben D.", ""], ["Jones", "Nick S.", ""]]}, {"id": "1401.3690", "submitter": "Christian Stump", "authors": "Chris Berg and Viviane Pons and Travis Scrimshaw and Jessica Striker\n  and Christian Stump", "title": "FindStat - the combinatorial statistics database", "comments": "2 pages, project description", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The FindStat project at www.FindStat.org provides an online platform for\nmathematicians, particularly for combinatorialists, to gather information about\ncombinatorial statistics and their relations. This outline provides an overview\nover the project.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 18:04:36 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Berg", "Chris", ""], ["Pons", "Viviane", ""], ["Scrimshaw", "Travis", ""], ["Striker", "Jessica", ""], ["Stump", "Christian", ""]]}, {"id": "1401.3874", "submitter": "Fei Wu", "authors": "Fei Wu, Jayant Madhavan, Alon Halevy", "title": "Identifying Aspects for Web-Search Queries", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 40, pages\n  677-700, 2011", "doi": "10.1613/jair.3182", "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many web-search queries serve as the beginning of an exploration of an\nunknown space of information, rather than looking for a specific web page. To\nanswer such queries effec- tively, the search engine should attempt to organize\nthe space of relevant information in a way that facilitates exploration. We\ndescribe the Aspector system that computes aspects for a given query. Each\naspect is a set of search queries that together represent a distinct\ninformation need relevant to the original search query. To serve as an\neffective means to explore the space, Aspector computes aspects that are\northogonal to each other and to have high combined coverage. Aspector combines\ntwo sources of information to compute aspects. We discover candidate aspects by\nanalyzing query logs, and cluster them to eliminate redundancies. We then use a\nmass-collaboration knowledge base (e.g., Wikipedia) to compute candidate\naspects for queries that occur less frequently and to group together aspects\nthat are likely to be \"semantically\" related. We present a user study that\nindicates that the aspects we compute are rated favorably against three\ncompeting alternatives -related searches proposed by Google, cluster labels\nassigned by the Clusty search engine, and navigational searches proposed by\nBing.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:09:56 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Wu", "Fei", ""], ["Madhavan", "Jayant", ""], ["Halevy", "Alon", ""]]}, {"id": "1401.4840", "submitter": "Tomasz Gogacz", "authors": "Tomasz Gogacz and Jerzy Marcinkowski", "title": "Termination of oblivious chase is undecidable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We show that all--instances termination of chase is undecidable. More\nprecisely, there is no algorithm deciding, for a given set $\\cal T$ consisting\nof Tuple Generating Dependencies (a.k.a. Datalog$^\\exists$ program), whether\nthe $\\cal T$-chase on $D$ will terminate for every finite database instance\n$D$. Our method applies to Oblivious Chase, Semi-Oblivious Chase and -- after a\nslight modification -- also for Standard Chase. This means that we give a\n(negative) solution to the all--instances termination problem for all version\nof chase that are usually considered.\n  The arity we need for our undecidability proof is three. We also show that\nthe problem is EXPSPACE-hard for binary signatures, but decidability for this\ncase is left open.\n  Both the proofs -- for ternary and binary signatures -- are easy. Once you\nknow them.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2014 09:43:59 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2014 07:39:31 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Gogacz", "Tomasz", ""], ["Marcinkowski", "Jerzy", ""]]}, {"id": "1401.4872", "submitter": "Ayman Bahaa-Eldin", "authors": "Hany Nashat Gabra, Ayman Mohammad Bahaa-Eldin, Huda Korashy", "title": "Classification of IDS Alerts with Data Mining Techniques", "comments": "2012 International Conference on Internet Study (NETs2012), Bangkok,\n  Thailand", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A data mining technique to reduce the amount of false alerts within an IDS\nsystem is proposed. The new technique achieves an accuracy of 99% compared to\n97% by the current systems.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2014 11:58:23 GMT"}], "update_date": "2014-01-21", "authors_parsed": [["Gabra", "Hany Nashat", ""], ["Bahaa-Eldin", "Ayman Mohammad", ""], ["Korashy", "Huda", ""]]}, {"id": "1401.5051", "submitter": "Olivier Cur\\'e", "authors": "Olivier Cur\\'e and Guillaume Blin and Dominique Revuz and David Faye", "title": "WaterFowl, a Compact, Self-indexed RDF Store with Inference-enabled\n  Dictionaries", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel approach -- called WaterFowl -- for the\nstorage of RDF triples that addresses some key issues in the contexts of big\ndata and the Semantic Web. The architecture of our prototype, largely based on\nthe use of succinct data structures, enables the representation of triples in a\nself-indexed, compact manner without requiring decompression at query answering\ntime. Moreover, it is adapted to efficiently support RDF and RDFS entailment\nregimes thanks to an optimized encoding of ontology concepts and properties\nthat does not require a complete inference materialization or extensive query\nrewriting algorithms. This approach implies to make a distinction between the\nterminological and the assertional components of the knowledge base early in\nthe process of data preparation, i.e., preprocessing the data before storing it\nin our structures. The paper describes the complete architecture of this system\nand presents some preliminary results obtained from evaluations conducted on\nour first prototype.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2014 20:48:05 GMT"}], "update_date": "2014-01-21", "authors_parsed": [["Cur\u00e9", "Olivier", ""], ["Blin", "Guillaume", ""], ["Revuz", "Dominique", ""], ["Faye", "David", ""]]}, {"id": "1401.5465", "submitter": "Wanling Gao", "authors": "Zijian Ming, Chunjie Luo, Wanling Gao, Rui Han, Qiang Yang, Lei Wang,\n  and Jianfeng Zhan", "title": "BDGS: A Scalable Big Data Generator Suite in Big Data Benchmarking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data generation is a key issue in big data benchmarking that aims to generate\napplication-specific data sets to meet the 4V requirements of big data.\nSpecifically, big data generators need to generate scalable data (Volume) of\ndifferent types (Variety) under controllable generation rates (Velocity) while\nkeeping the important characteristics of raw data (Veracity). This gives rise\nto various new challenges about how we design generators efficiently and\nsuccessfully. To date, most existing techniques can only generate limited types\nof data and support specific big data systems such as Hadoop. Hence we develop\na tool, called Big Data Generator Suite (BDGS), to efficiently generate\nscalable big data while employing data models derived from real data to\npreserve data veracity. The effectiveness of BDGS is demonstrated by developing\nsix data generators covering three representative data types (structured,\nsemi-structured and unstructured) and three data sources (text, graph, and\ntable data).\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2014 02:17:52 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2014 08:13:48 GMT"}, {"version": "v3", "created": "Thu, 27 Feb 2014 03:40:26 GMT"}], "update_date": "2014-02-28", "authors_parsed": [["Ming", "Zijian", ""], ["Luo", "Chunjie", ""], ["Gao", "Wanling", ""], ["Han", "Rui", ""], ["Yang", "Qiang", ""], ["Wang", "Lei", ""], ["Zhan", "Jianfeng", ""]]}, {"id": "1401.6360", "submitter": "Luc Bouganim", "authors": "Niv Dayan (IT), Martin Kjaer Svendsen (IT), Matias Bjorling, Philippe\n  Bonnet (IT), Luc Bouganim (PRISM, INRIA Rocquencourt)", "title": "EagleTree: Exploring the Design Space of SSD-Based Algorithms", "comments": "39th International Conference on Very Large Data Bases (VLDB) (2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solid State Drives (SSDs) are a moving target for system designers: they are\nblack boxes, their internals are undocumented, and their performance\ncharacteristics vary across models. There is no appropriate analytical model\nand experimenting with commercial SSDs is cumbersome, as it requires a careful\nexperimental methodology to ensure repeatability. Worse, performance results\nobtained on a given SSD cannot be generalized. Overall, it is impossible to\nexplore how a given algorithm, say a hash join or LSM-tree insertions,\nleverages the intrinsic parallelism of a modern SSD, or how a slight change in\nthe internals of an SSD would impact its overall performance. In this paper, we\npropose a new SSD simulation framework, named EagleTree, which addresses these\nproblems, and enables a principled study of SSD-Based algorithms. The\ndemonstration scenario illustrates the design space for algorithms based on an\nSSD-based IO stack, and shows how researchers and practitioners can use\nEagleTree to perform tractable explorations of this complex design space.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2014 14:54:41 GMT"}], "update_date": "2014-01-27", "authors_parsed": [["Dayan", "Niv", "", "IT"], ["Svendsen", "Martin Kjaer", "", "IT"], ["Bjorling", "Matias", "", "IT"], ["Bonnet", "Philippe", "", "IT"], ["Bouganim", "Luc", "", "PRISM, INRIA Rocquencourt"]]}, {"id": "1401.6399", "submitter": "Daniel Lemire", "authors": "Daniel Lemire, Leonid Boytsov, Nathan Kurz", "title": "SIMD Compression and the Intersection of Sorted Integers", "comments": null, "journal-ref": "Software: Practice and Experience Volume 46, Issue 6, pages\n  723-749, June 2016", "doi": "10.1002/spe.2326", "report-no": null, "categories": "cs.IR cs.DB cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sorted lists of integers are commonly used in inverted indexes and database\nsystems. They are often compressed in memory. We can use the SIMD instructions\navailable in common processors to boost the speed of integer compression\nschemes. Our S4-BP128-D4 scheme uses as little as 0.7 CPU cycles per decoded\ninteger while still providing state-of-the-art compression.\n  However, if the subsequent processing of the integers is slow, the effort\nspent on optimizing decoding speed can be wasted. To show that it does not have\nto be so, we (1) vectorize and optimize the intersection of posting lists; (2)\nintroduce the SIMD Galloping algorithm. We exploit the fact that one SIMD\ninstruction can compare 4 pairs of integers at once.\n  We experiment with two TREC text collections, GOV2 and ClueWeb09 (Category\nB), using logs from the TREC million-query track. We show that using only the\nSIMD instructions ubiquitous in all modern CPUs, our techniques for conjunctive\nqueries can double the speed of a state-of-the-art approach.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2014 16:53:37 GMT"}, {"version": "v10", "created": "Fri, 6 Mar 2015 19:50:27 GMT"}, {"version": "v11", "created": "Thu, 12 Mar 2015 01:32:13 GMT"}, {"version": "v12", "created": "Thu, 7 May 2015 00:52:07 GMT"}, {"version": "v13", "created": "Mon, 20 Apr 2020 19:42:24 GMT"}, {"version": "v2", "created": "Mon, 27 Jan 2014 16:37:12 GMT"}, {"version": "v3", "created": "Tue, 4 Feb 2014 01:02:27 GMT"}, {"version": "v4", "created": "Thu, 27 Feb 2014 23:38:50 GMT"}, {"version": "v5", "created": "Mon, 28 Apr 2014 19:15:10 GMT"}, {"version": "v6", "created": "Thu, 15 May 2014 14:57:33 GMT"}, {"version": "v7", "created": "Thu, 17 Jul 2014 19:42:12 GMT"}, {"version": "v8", "created": "Wed, 23 Jul 2014 19:16:39 GMT"}, {"version": "v9", "created": "Wed, 24 Dec 2014 20:30:29 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Lemire", "Daniel", ""], ["Boytsov", "Leonid", ""], ["Kurz", "Nathan", ""]]}, {"id": "1401.6628", "submitter": "Yuqing Zhu", "authors": "Yuqing Zhu, Jianfeng Zhan, Chuliang Weng, Raghunath Nambiar, Jinchao\n  Zhang, Xingzhen Chen, and Lei Wang", "title": "BigOP: Generating Comprehensive Big Data Workloads as a Benchmarking\n  Framework", "comments": "10 pages, 3 figures", "journal-ref": "Database Systems for Advanced Applications: 19th International\n  Conference, DASFAA 2014, Bali, Indonesia, April 21-24, 2014", "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big Data is considered proprietary asset of companies, organizations, and\neven nations. Turning big data into real treasure requires the support of big\ndata systems. A variety of commercial and open source products have been\nunleashed for big data storage and processing. While big data users are facing\nthe choice of which system best suits their needs, big data system developers\nare facing the question of how to evaluate their systems with regard to general\nbig data processing needs. System benchmarking is the classic way of meeting\nthe above demands. However, existent big data benchmarks either fail to\nrepresent the variety of big data processing requirements, or target only one\nspecific platform, e.g. Hadoop.\n  In this paper, with our industrial partners, we present BigOP, an end-to-end\nsystem benchmarking framework, featuring the abstraction of representative\nOperation sets, workload Patterns, and prescribed tests. BigOP is part of an\nopen-source big data benchmarking project, BigDataBench. BigOP's abstraction\nmodel not only guides the development of BigDataBench, but also enables\nautomatic generation of tests with comprehensive workloads.\n  We illustrate the feasibility of BigOP by implementing an automatic test\ngeneration tool and benchmarking against three widely used big data processing\nsystems, i.e. Hadoop, Spark and MySQL Cluster. Three tests targeting three\ndifferent application scenarios are prescribed. The tests involve relational\ndata, text data and graph data, as well as all operations and workload\npatterns. We report results following test specifications.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jan 2014 08:41:50 GMT"}, {"version": "v2", "created": "Wed, 27 Dec 2017 03:56:44 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Zhu", "Yuqing", ""], ["Zhan", "Jianfeng", ""], ["Weng", "Chuliang", ""], ["Nambiar", "Raghunath", ""], ["Zhang", "Jinchao", ""], ["Chen", "Xingzhen", ""], ["Wang", "Lei", ""]]}, {"id": "1401.6887", "submitter": "Waqas Nawaz", "authors": "Kifayat Ullah Khan, Kamran Najeebullah, Waqas Nawaz, Young-Koo Lee", "title": "OLAP on Structurally Significant Data in Graphs", "comments": "4 Pages, Accepted in 5th International Conference on Data Mining and\n  Intelligent Information Technology Applications (ICMIA) 2013, Korea", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Summarized data analysis of graphs using OLAP (Online Analytical Processing)\nis very popular these days. However due to high dimensionality and large size,\nit is not easy to decide which data should be aggregated for OLAP analysis.\nThough iceberg cubing is useful, but it is unaware of the significance of\ndimensional values with respect to the structure of the graph. In this paper,\nwe propose a Structural Significance, SS, measure to identify the structurally\nsignificant dimensional values in each dimension. This leads to structure aware\npruning. We then propose an algorithm, iGraphCubing, to compute the graph cube\nto analyze the structurally significant data using the proposed measure. We\nevaluated the proposed ideas on real and synthetic data sets and observed very\nencouraging results.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2014 15:16:17 GMT"}], "update_date": "2014-01-28", "authors_parsed": [["Khan", "Kifayat Ullah", ""], ["Najeebullah", "Kamran", ""], ["Nawaz", "Waqas", ""], ["Lee", "Young-Koo", ""]]}, {"id": "1401.7584", "submitter": "Michael Kohlhase", "authors": "Michael Kohlhase, Corneliu Prodescu, Christian Liguda", "title": "XLSearch: A Search Engine for Spreadsheets", "comments": "12 Pages. 10 B&W & Colour Figures. Proc. European Spreadsheet Risks\n  Int. Grp. (EuSpRIG) 2013, ISBN: 978-1-9054045-1-3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spreadsheets are end-user programs and domain models that are heavily\nemployed in administration, financial forecasting, education, and science\nbecause of their intuitive, flexible, and direct approach to computation. As a\nresult, institutions are swamped by millions of spreadsheets that are becoming\nincreasingly difficult to manage, access, and control.\n  This note presents the XLSearch system, a novel search engine for\nspreadsheets. It indexes spreadsheet formulae and efficiently answers formula\nqueries via unification (a complex query language that allows metavariables in\nboth the query as well as the index). But a web-based search engine is only one\napplication of the underlying technology: Spreadsheet formula export to web\nstandards like MathML combined with formula indexing can be used to find\nsimilar spreadsheets or common formula errors.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2014 20:38:05 GMT"}], "update_date": "2014-01-30", "authors_parsed": [["Kohlhase", "Michael", ""], ["Prodescu", "Corneliu", ""], ["Liguda", "Christian", ""]]}, {"id": "1401.7733", "submitter": "Sunil Kumar c", "authors": "C.Sunil Kumar, J.Seetha, S.R.Vinotha", "title": "Security Implications of Distributed Database Management System Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Security features must be addressed when escalating a distributed database.\nThe choice between the object oriented and the relational data model, several\nfactors should be considered. The most important of these factors are single\nand multilevel access controls (MAC), protection and integrity maintenance.\nWhile determining which distributed database replica will be more secure for a\nparticular function, the choice should not be made exclusively on the basis of\navailable security features. One should also query the effectiveness and\nefficiency of the delivery of these characteristics. In this paper, the\nsecurity strengths and weaknesses of both database models and the thorough\nproblems initiate in the distributed environment are conversed.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2014 04:43:22 GMT"}], "update_date": "2014-01-31", "authors_parsed": [["Kumar", "C. Sunil", ""], ["Seetha", "J.", ""], ["Vinotha", "S. R.", ""]]}, {"id": "1401.8201", "submitter": "Dimitri Surinx", "authors": "George H.L. Fletcher, Marc Gyssens, Dirk Leinders, Dimitri Surinx, Jan\n  Van den Bussche, Dirk Van Gucht, Stijn Vansummeren, Yuqing Wu", "title": "Relative Expressive Power of Navigational Querying on Graphs", "comments": "An extended abstract announcing the results of this paper was\n  presented at the 14th International Conference on Database Theory, Uppsala,\n  Sweden, March 2011", "journal-ref": "Information Sciences (2015), pp. 390-406", "doi": "10.1016/j.ins.2014.11.031", "report-no": null, "categories": "cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by both established and new applications, we study navigational\nquery languages for graphs (binary relations). The simplest language has only\nthe two operators union and composition, together with the identity relation.\nWe make more powerful languages by adding any of the following operators:\nintersection; set difference; projection; coprojection; converse; and the\ndiversity relation. All these operators map binary relations to binary\nrelations. We compare the expressive power of all resulting languages. We do\nthis not only for general path queries (queries where the result may be any\nbinary relation) but also for boolean or yes/no queries (expressed by the\nnonemptiness of an expression). For both cases, we present the complete Hasse\ndiagram of relative expressiveness. In particular the Hasse diagram for boolean\nqueries contains some nontrivial separations and a few surprising collapses.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2014 15:54:31 GMT"}, {"version": "v2", "created": "Fri, 28 Nov 2014 09:28:35 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Fletcher", "George H. L.", ""], ["Gyssens", "Marc", ""], ["Leinders", "Dirk", ""], ["Surinx", "Dimitri", ""], ["Bussche", "Jan Van den", ""], ["Van Gucht", "Dirk", ""], ["Vansummeren", "Stijn", ""], ["Wu", "Yuqing", ""]]}]