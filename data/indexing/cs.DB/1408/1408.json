[{"id": "1408.0517", "submitter": "Vijay Gadepally", "authors": "Vijay Gadepally and Jeremy Kepner", "title": "Big Data Dimensional Analysis", "comments": "From IEEE HPEC 2014", "journal-ref": null, "doi": "10.1109/HPEC.2014.7040944", "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to collect and analyze large amounts of data is a growing problem\nwithin the scientific community. The growing gap between data and users calls\nfor innovative tools that address the challenges faced by big data volume,\nvelocity and variety. One of the main challenges associated with big data\nvariety is automatically understanding the underlying structures and patterns\nof the data. Such an understanding is required as a pre-requisite to the\napplication of advanced analytics to the data. Further, big data sets often\ncontain anomalies and errors that are difficult to know a priori. Current\napproaches to understanding data structure are drawn from the traditional\ndatabase ontology design. These approaches are effective, but often require too\nmuch human involvement to be effective for the volume, velocity and variety of\ndata encountered by big data systems. Dimensional Data Analysis (DDA) is a\nproposed technique that allows big data analysts to quickly understand the\noverall structure of a big dataset, determine anomalies. DDA exploits\nstructures that exist in a wide class of data to quickly determine the nature\nof the data and its statical anomalies. DDA leverages existing schemas that are\nemployed in big data databases today. This paper presents DDA, applies it to a\nnumber of data sets, and measures its performance. The overhead of DDA is low\nand can be applied to existing big data systems without greatly impacting their\ncomputing requirements.\n", "versions": [{"version": "v1", "created": "Sun, 3 Aug 2014 17:22:01 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Gadepally", "Vijay", ""], ["Kepner", "Jeremy", ""]]}, {"id": "1408.0528", "submitter": "Xiaocheng Huang", "authors": "Xiaocheng Huang, Zhuowei Bao, Susan B. Davidson, Tova Milo, Xiaojie\n  Yuan", "title": "Answering Regular Path Queries on Workflow Provenance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel approach for efficiently evaluating regular path\nqueries over provenance graphs of workflows that may include recursion. The\napproach assumes that an execution g of a workflow G is labeled with\nquery-agnostic reachability labels using an existing technique. At query time,\ngiven g, G and a regular path query R, the approach decomposes R into a set of\nsubqueries R1, ..., Rk that are safe for G. For each safe subquery Ri, G is\nrewritten so that, using the reachability labels of nodes in g, whether or not\nthere is a path which matches Ri between two nodes can be decided in constant\ntime. The results of each safe subquery are then composed, possibly with some\nsmall unsafe remainder, to produce an answer to R. The approach results in an\nalgorithm that significantly reduces the number of subqueries k over existing\ntechniques by increasing their size and complexity, and that evaluates each\nsubquery in time bounded by its input and output size. Experimental results\ndemonstrate the benefit of this approach.\n", "versions": [{"version": "v1", "created": "Sun, 3 Aug 2014 19:02:51 GMT"}, {"version": "v2", "created": "Tue, 5 Aug 2014 01:32:18 GMT"}], "update_date": "2014-08-06", "authors_parsed": [["Huang", "Xiaocheng", ""], ["Bao", "Zhuowei", ""], ["Davidson", "Susan B.", ""], ["Milo", "Tova", ""], ["Yuan", "Xiaojie", ""]]}, {"id": "1408.0890", "submitter": "Stefan Mengel", "authors": "Hubie Chen, Stefan Mengel", "title": "A Trichotomy in the Complexity of Counting Answers to Conjunctive\n  Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conjunctive queries are basic and heavily studied database queries; in\nrelational algebra, they are the select-project-join queries. In this article,\nwe study the fundamental problem of counting, given a conjunctive query and a\nrelational database, the number of answers to the query on the database. In\nparticular, we study the complexity of this problem relative to sets of\nconjunctive queries. We present a trichotomy theorem, which shows essentially\nthat this problem on a set of conjunctive queries is either tractable,\nequivalent to the parameterized CLIQUE problem, or as hard as the parameterized\ncounting CLIQUE problem; the criteria describing which of these situations\noccurs is simply stated, in terms of graph-theoretic conditions.\n", "versions": [{"version": "v1", "created": "Tue, 5 Aug 2014 08:46:17 GMT"}, {"version": "v2", "created": "Mon, 19 Jan 2015 20:09:48 GMT"}, {"version": "v3", "created": "Wed, 21 Jan 2015 20:16:52 GMT"}], "update_date": "2015-01-22", "authors_parsed": [["Chen", "Hubie", ""], ["Mengel", "Stefan", ""]]}, {"id": "1408.0926", "submitter": "James Cheney", "authors": "Harry Halpin and James Cheney", "title": "Dynamic Provenance for SPARQL Update", "comments": "Pre-publication version of ISWC 2014 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the Semantic Web currently can exhibit provenance information by using\nthe W3C PROV standards, there is a \"missing link\" in connecting PROV to storing\nand querying for dynamic changes to RDF graphs using SPARQL. Solving this\nproblem would be required for such clear use-cases as the creation of version\ncontrol systems for RDF. While some provenance models and annotation techniques\nfor storing and querying provenance data originally developed with databases or\nworkflows in mind transfer readily to RDF and SPARQL, these techniques do not\nreadily adapt to describing changes in dynamic RDF datasets over time. In this\npaper we explore how to adapt the dynamic copy-paste provenance model of\nBuneman et al. [2] to RDF datasets that change over time in response to SPARQL\nupdates, how to represent the resulting provenance records themselves as RDF in\na manner compatible with W3C PROV, and how the provenance information can be\ndefined by reinterpreting SPARQL updates. The primary contribution of this\npaper is a semantic framework that enables the semantics of SPARQL Update to be\nused as the basis for a 'cut-and-paste' provenance model in a principled\nmanner.\n", "versions": [{"version": "v1", "created": "Tue, 5 Aug 2014 11:17:13 GMT"}], "update_date": "2014-08-06", "authors_parsed": [["Halpin", "Harry", ""], ["Cheney", "James", ""]]}, {"id": "1408.1011", "submitter": "Faegheh Hasibi", "authors": "Faegheh Hasibi, Svein Erik Bratsberg", "title": "Non-hierarchical Structures: How to Model and Index Overlaps?", "comments": "The paper has been accepted at the Balisage 2014 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Overlap is a common phenomenon seen when structural components of a digital\nobject are neither disjoint nor nested inside each other. Overlapping\ncomponents resist reduction to a structural hierarchy, and tree-based indexing\nand query processing techniques cannot be used for them. Our solution to this\ndata modeling problem is TGSA (Tree-like Graph for Structural Annotations), a\nnovel extension of the XML data model for non-hierarchical structures. We\nintroduce an algorithm for constructing TGSA from annotated documents; the\nalgorithm can efficiently process non-hierarchical structures and is associated\nwith formal proofs, ensuring that transformation of the document to the data\nmodel is valid. To enable high performance query analysis in large data\nrepositories, we further introduce an extension of XML pre-post indexing for\nnon-hierarchical structures, which can process both reachability and\noverlapping relationships.\n", "versions": [{"version": "v1", "created": "Tue, 5 Aug 2014 16:07:11 GMT"}, {"version": "v2", "created": "Wed, 6 Aug 2014 23:12:12 GMT"}, {"version": "v3", "created": "Sat, 8 Oct 2016 06:46:54 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Hasibi", "Faegheh", ""], ["Bratsberg", "Svein Erik", ""]]}, {"id": "1408.1675", "submitter": "James Cheney", "authors": "James Cheney and Amal Ahmed and Umut A. Acar", "title": "Database Queries that Explain their Work", "comments": "PPDP 2014", "journal-ref": null, "doi": "10.1145/2643135.2643143", "report-no": null, "categories": "cs.PL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Provenance for database queries or scientific workflows is often motivated as\nproviding explanation, increasing understanding of the underlying data sources\nand processes used to compute the query, and reproducibility, the capability to\nrecompute the results on different inputs, possibly specialized to a part of\nthe output. Many provenance systems claim to provide such capabilities;\nhowever, most lack formal definitions or guarantees of these properties, while\nothers provide formal guarantees only for relatively limited classes of\nchanges. Building on recent work on provenance traces and slicing for\nfunctional programming languages, we introduce a detailed tracing model of\nprovenance for multiset-valued Nested Relational Calculus, define trace slicing\nalgorithms that extract subtraces needed to explain or recompute specific parts\nof the output, and define query slicing and differencing techniques that\nsupport explanation. We state and prove correctness properties for these\ntechniques and present a proof-of-concept implementation in Haskell.\n", "versions": [{"version": "v1", "created": "Thu, 7 Aug 2014 18:29:38 GMT"}, {"version": "v2", "created": "Fri, 8 Aug 2014 17:02:26 GMT"}, {"version": "v3", "created": "Tue, 12 Aug 2014 10:50:02 GMT"}], "update_date": "2014-08-13", "authors_parsed": [["Cheney", "James", ""], ["Ahmed", "Amal", ""], ["Acar", "Umut A.", ""]]}, {"id": "1408.2081", "submitter": "Tomasz Gogacz", "authors": "Tomasz Gogacz and Jerzy Marcinkowski", "title": "On the BDD/FC Conjecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Bounded Derivation Depth property (BDD) and Finite Controllability (FC) are\ntwo properties of sets of datalog rules and tuple generating dependencies\n(known as Datalog +/- programs), which recently attracted some attention. We\nconjecture that the first of these properties implies the second, and support\nthis conjecture by some evidence proving, among other results, that it holds\ntrue for all theories over binary signature.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 11:09:00 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Gogacz", "Tomasz", ""], ["Marcinkowski", "Jerzy", ""]]}, {"id": "1408.2154", "submitter": "Ismael Gonzalez Yero", "authors": "Rolando Trujillo-Rasua and Ismael G. Yero", "title": "k-Metric Antidimension: a Privacy Measure for Social Graphs", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G = (V, E)$ be a simple connected graph and $S = \\{w_1, \\cdots, w_t\\}\n\\subseteq V$ an ordered subset of vertices. The metric representation of a\nvertex $u\\in V$ with respect to $S$ is the $t$-vector $r(u|S) = (d_G(u, w_1),\n\\cdots, d_G(u, w_t))$, where $d_G(u, v)$ represents the length of a shortest\n$u-v$ path in $G$. The set $S$ is called a resolving set for $G$ if $r(u|S) =\nr(v|S)$ implies $u = v$ for every $u, v \\in V$. The smallest cardinality of a\nresolving set is the metric dimension of $G$. In this article we propose, to\nthe best of our knowledge, a new problem in Graph Theory that resembles to the\naforementioned metric dimension problem. We call $S$ a $k$-antiresolving set if\n$k$ is the largest positive integer such that for every vertex $v \\in V-S$\nthere exist other $k-1$ different vertices $v_1, \\cdots, v_{k-1} \\in V-S$ with\n$r(v|S) = r(v_1|S) = \\cdots = r(v_{k-1}|S)$, \\emph{i.e.}, $v$ and $v_1, \\cdots,\nv_{k-1}$ have the same metric representation with respect to $S$. The\n$k$-metric antidimension of $G$ is the minimum cardinality among all the\n$k$-antiresolving sets for $G$.\n  In this article, we introduce a novel privacy measure, named $(k,\n\\ell)$-anonymity and based on the $k$-metric antidimension problem, aimed at\nevaluating the resistance of social graphs to active attacks. We, therefore,\npropose a true-biased algorithm for computing the $k$-metric antidimension of\nrandom graphs. The success rate of our algorithm, according to empirical\nresults, is above $80 \\%$ and $90 \\%$ when looking for a $k$-antiresolving\nbasis and a $k$-antiresolving set respectively. We also investigate theoretical\nproperties of the $k$-antiresolving sets and the $k$-metric antidimension of\ngraphs. In particular, we focus on paths, cycles, complete bipartite graphs and\ntrees.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 21:08:48 GMT"}, {"version": "v2", "created": "Sat, 21 Mar 2015 20:46:30 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Trujillo-Rasua", "Rolando", ""], ["Yero", "Ismael G.", ""]]}, {"id": "1408.2468", "submitter": "Christoph Lange", "authors": "Jeremy Debattista and Christoph Lange and S\\\"oren Auer", "title": "Representing Dataset Quality Metadata using Multi-Dimensional Views", "comments": "Preprint of a paper submitted to the forthcoming SEMANTiCS 2014, 4-5\n  September 2014, Leipzig, Germany", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data quality is commonly defined as fitness for use. The problem of\nidentifying quality of data is faced by many data consumers. Data publishers\noften do not have the means to identify quality problems in their data. To make\nthe task for both stakeholders easier, we have developed the Dataset Quality\nOntology (daQ). daQ is a core vocabulary for representing the results of\nquality benchmarking of a linked dataset. It represents quality metadata as\nmulti-dimensional and statistical observations using the Data Cube vocabulary.\nQuality metadata are organised as a self-contained graph, which can, e.g., be\nembedded into linked open datasets. We discuss the design considerations, give\nexamples for extending daQ by custom quality metrics, and present use cases\nsuch as analysing data versions, browsing datasets by quality, and link\nidentification. We finally discuss how data cube visualisation tools enable\ndata publishers and consumers to analyse better the quality of their data.\n", "versions": [{"version": "v1", "created": "Mon, 11 Aug 2014 17:00:40 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Debattista", "Jeremy", ""], ["Lange", "Christoph", ""], ["Auer", "S\u00f6ren", ""]]}, {"id": "1408.2800", "submitter": "Nikos Bikakis", "authors": "Nikos Bikakis, Chrisa Tsinaraki, Ioannis Stavrakantonakis, Stavros\n  Christodoulakis", "title": "Supporting SPARQL Update Queries in RDF-XML Integration", "comments": "13th International Semantic Web Conference (ISWC '14)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Web of Data encourages organizations and companies to publish their data\naccording to the Linked Data practices and offer SPARQL endpoints. On the other\nhand, the dominant standard for information exchange is XML. The SPARQL2XQuery\nFramework focuses on the automatic translation of SPARQL queries in XQuery\nexpressions in order to access XML data across the Web. In this paper, we\noutline our ongoing work on supporting update queries in the RDF-XML\nintegration scenario.\n", "versions": [{"version": "v1", "created": "Tue, 12 Aug 2014 18:53:23 GMT"}, {"version": "v2", "created": "Wed, 27 Aug 2014 12:33:33 GMT"}], "update_date": "2014-08-28", "authors_parsed": [["Bikakis", "Nikos", ""], ["Tsinaraki", "Chrisa", ""], ["Stavrakantonakis", "Ioannis", ""], ["Christodoulakis", "Stavros", ""]]}, {"id": "1408.2927", "submitter": "Jingdong Wang", "authors": "Jingdong Wang, Heng Tao Shen, Jingkuan Song, and Jianqiu Ji", "title": "Hashing for Similarity Search: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity search (nearest neighbor search) is a problem of pursuing the data\nitems whose distances to a query item are the smallest from a large database.\nVarious methods have been developed to address this problem, and recently a lot\nof efforts have been devoted to approximate search. In this paper, we present a\nsurvey on one of the main solutions, hashing, which has been widely studied\nsince the pioneering work locality sensitive hashing. We divide the hashing\nalgorithms two main categories: locality sensitive hashing, which designs hash\nfunctions without exploring the data distribution and learning to hash, which\nlearns hash functions according the data distribution, and review them from\nvarious aspects, including hash function design and distance measure and search\nscheme in the hash coding space.\n", "versions": [{"version": "v1", "created": "Wed, 13 Aug 2014 07:29:12 GMT"}], "update_date": "2014-08-14", "authors_parsed": [["Wang", "Jingdong", ""], ["Shen", "Heng Tao", ""], ["Song", "Jingkuan", ""], ["Ji", "Jianqiu", ""]]}, {"id": "1408.3148", "submitter": "Nikos Bikakis", "authors": "Nikos Bikakis, Melina Skourla, George Papastefanatos", "title": "rdf:SynopsViz - A Framework for Hierarchical Linked Data Visual\n  Exploration and Analysis", "comments": "11th Extended Semantic Web Conference (ESWC '14)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of data visualization is to offer intuitive ways for information\nperception and manipulation, especially for non-expert users. The Web of Data\nhas realized the availability of a huge amount of datasets. However, the volume\nand heterogeneity of available information make it difficult for humans to\nmanually explore and analyse large datasets. In this paper, we present\nrdf:SynopsViz, a tool for hierarchical charting and visual exploration of\nLinked Open Data (LOD). Hierarchical LOD exploration is based on the creation\nof multiple levels of hierarchically related groups of resources based on the\nvalues of one or more properties. The adopted hierarchical model provides\neffective information abstraction and summarization. Also, it allows efficient\n-on the fly- statistic computations, using aggregations over the hierarchy\nlevels.\n", "versions": [{"version": "v1", "created": "Wed, 13 Aug 2014 21:16:31 GMT"}, {"version": "v2", "created": "Tue, 27 Jun 2017 13:03:21 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Bikakis", "Nikos", ""], ["Skourla", "Melina", ""], ["Papastefanatos", "George", ""]]}, {"id": "1408.3175", "submitter": "Sidda Reddy Velichalamala", "authors": "V.Sidda Reddy, Dr.T.V.Rao and Dr.A.Govardhan", "title": "Mining Frequent Itemsets (MFI) over Data Streams: Variable Window Size\n  (VWS) by Context Variation Analysis (CVA) of the Streaming Transactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The challenges with respect to mining frequent items over data streaming\nengaging variable window size and low memory space are addressed in this\nresearch paper. To check the varying point of context change in streaming\ntransaction we have developed a window structure which will be in two levels\nand supports in fixing the window size instantly and controls the\nheterogeneities and assures homogeneities among transactions added to the\nwindow. To minimize the memory utilization, computational cost and improve the\nprocess scalability, this design will allow fixing the coverage or support at\nwindow level. Here in this document, an incremental mining of frequent\nitem-sets from the window and a context variation analysis approach are being\nintroduced. The complete technology that we are presenting in this document is\nnamed as Mining Frequent Item-sets using Variable Window Size fixed by Context\nVariation Analysis (MFI-VWS-CVA). There are clear boundaries among frequent and\ninfrequent item-sets in specific item-sets. In this design we have used window\nsize change to represent the conceptual drift in an information stream. As it\nwere, whenever there is a problem in setting window size effectively the\nitem-set will be infrequent. The experiments that we have executed and\ndocumented proved that the algorithm that we have designed is much efficient\nthan that of existing.\n", "versions": [{"version": "v1", "created": "Thu, 14 Aug 2014 00:43:46 GMT"}], "update_date": "2014-08-15", "authors_parsed": [["Reddy", "V. Sidda", ""], ["Rao", "Dr. T. V.", ""], ["Govardhan", "Dr. A.", ""]]}, {"id": "1408.4072", "submitter": "Eugene Wu", "authors": "Leilani Battle, Edward Benson, Aditya Parameswaran, Eugene Wu", "title": "Indexing Cost Sensitive Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive models are often used for real-time decision making. However,\ntypical machine learning techniques ignore feature evaluation cost, and focus\nsolely on the accuracy of the machine learning models obtained utilizing all\nthe features available. We develop algorithms and indexes to support\ncost-sensitive prediction, i.e., making decisions using machine learning models\ntaking feature evaluation cost into account. Given an item and a online\ncomputation cost (i.e., time) budget, we present two approaches to return an\nappropriately chosen machine learning model that will run within the specified\ntime on the given item. The first approach returns the optimal machine learning\nmodel, i.e., one with the highest accuracy, that runs within the specified\ntime, but requires significant up-front precomputation time. The second\napproach returns a possibly sub- optimal machine learning model, but requires\nlittle up-front precomputation time. We study these two algorithms in detail\nand characterize the scenarios (using real and synthetic data) in which each\nperforms well. Unlike prior work that focuses on a narrow domain or a specific\nalgorithm, our techniques are very general: they apply to any cost-sensitive\nprediction scenario on any machine learning algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 15 Aug 2014 07:21:48 GMT"}], "update_date": "2014-08-19", "authors_parsed": [["Battle", "Leilani", ""], ["Benson", "Edward", ""], ["Parameswaran", "Aditya", ""], ["Wu", "Eugene", ""]]}, {"id": "1408.4468", "submitter": "David Toman", "authors": "David Toman and Grant Weddell", "title": "Undecidability of Finite Model Reasoning in DLFD", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We resolve an open problem concerning finite logical implication for path\nfunctional dependencies (PFDs).\n", "versions": [{"version": "v1", "created": "Tue, 19 Aug 2014 20:14:34 GMT"}], "update_date": "2014-08-21", "authors_parsed": [["Toman", "David", ""], ["Weddell", "Grant", ""]]}, {"id": "1408.4793", "submitter": "Luca Matteis", "authors": "Luca Matteis", "title": "Restpark: Minimal RESTful API for Retrieving RDF Triples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  How do RDF datasets currently get published on the Web? They are either\navailable as large RDF files, which need to be downloaded and processed\nlocally, or they exist behind complex SPARQL endpoints. By providing a RESTful\nAPI that can access triple data, we allow users to query a dataset through a\nsimple interface based on just a couple of HTTP parameters. If RDF resources\nwere published this way we could quickly build applications that depend on\nthese datasets, without having to download and process them locally. This is\nwhat Restpark is: a set of HTTP GET parameters that servers need to handle, and\nrespond with JSON-LD.\n", "versions": [{"version": "v1", "created": "Tue, 19 Aug 2014 22:57:41 GMT"}], "update_date": "2014-08-22", "authors_parsed": [["Matteis", "Luca", ""]]}, {"id": "1408.5094", "submitter": "Marco Montali", "authors": "Diego Calvanese and Marco Montali and Montserrat Estanol and Ernest\n  Teniente", "title": "Verifiable UML Artifact-Centric Business Process Models (Extended\n  Version)", "comments": "Extended version of \"Verifiable UML Artifact-Centric Business Process\n  Models\" - to appear in the Proceedings of CIKM 2014", "journal-ref": null, "doi": "10.1145/2661829.2662050", "report-no": null, "categories": "cs.DB cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artifact-centric business process models have gained increasing momentum\nrecently due to their ability to combine structural (i.e., data related) with\ndynamical (i.e., process related) aspects. In particular, two main lines of\nresearch have been pursued so far: one tailored to business artefact modeling\nlanguages and methodologies, the other focused on the foundations for their\nformal verification. In this paper, we merge these two lines of research, by\nshowing how recent theoretical decidability results for verification can be\nfruitfully transferred to a concrete UML-based modeling methodology. In\nparticular, we identify additional steps in the methodology that, in\nsignificant cases, guarantee the possibility of verifying the resulting models\nagainst rich first-order temporal properties. Notably, our results can be\nseamlessly transferred to different languages for the specification of the\nartifact lifecycles.\n", "versions": [{"version": "v1", "created": "Thu, 21 Aug 2014 18:22:21 GMT"}, {"version": "v2", "created": "Tue, 26 Aug 2014 14:09:33 GMT"}], "update_date": "2014-08-27", "authors_parsed": [["Calvanese", "Diego", ""], ["Montali", "Marco", ""], ["Estanol", "Montserrat", ""], ["Teniente", "Ernest", ""]]}, {"id": "1408.5389", "submitter": "Zhensong Qian", "authors": "Zhensong Qian, Oliver Schulte and Yan Sun", "title": "Computing Multi-Relational Sufficient Statistics for Large Databases", "comments": "11pages, 8 figures, 8 tables, CIKM'14,November 3--7, 2014, Shanghai,\n  China", "journal-ref": null, "doi": "10.1145/2661829.2662010", "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Databases contain information about which relationships do and do not hold\namong entities. To make this information accessible for statistical analysis\nrequires computing sufficient statistics that combine information from\ndifferent database tables. Such statistics may involve any number of {\\em\npositive and negative} relationships. With a naive enumeration approach,\ncomputing sufficient statistics for negative relationships is feasible only for\nsmall databases. We solve this problem with a new dynamic programming algorithm\nthat performs a virtual join, where the requisite counts are computed without\nmaterializing join tables. Contingency table algebra is a new extension of\nrelational algebra, that facilitates the efficient implementation of this\nM\\\"obius virtual join operation. The M\\\"obius Join scales to large datasets\n(over 1M tuples) with complex schemas. Empirical evaluation with seven\nbenchmark datasets showed that information about the presence and absence of\nlinks can be exploited in feature selection, association rule mining, and\nBayesian network learning.\n", "versions": [{"version": "v1", "created": "Fri, 22 Aug 2014 19:12:19 GMT"}], "update_date": "2014-10-23", "authors_parsed": [["Qian", "Zhensong", ""], ["Schulte", "Oliver", ""], ["Sun", "Yan", ""]]}, {"id": "1408.5460", "submitter": "Priyanka Verma MRS", "authors": "Priyanka Verma, Nishtha Kesswani", "title": "Web Usage mining framework for Data Cleaning and IP address\n  Identification", "comments": "4 pages, IJASCSE,online published by 5th sept 2014 at following link\n  http://www.ijascse.org/publications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The World Wide Web is the most wide known information source that is easily\navailable and searchable. It consists of billions of interconnected documents\nWeb pages are authored by millions of people. Accesses made by various users to\npages are recorded inside web logs. These log files exist in various formats.\nBecause of increase in usage of web, size of web log files is increasing at a\nmuch faster rate. Web mining is application of data mining technique to these\nlog files. It can be of three types Web usage mining, Web structure mining and\nWeb content mining. Web Usage mining is mining of usage patterns of users which\ncan then be used to personalize web sites and create attractive web sites. It\nconsists of three main phases: Preprocessing, Pattern discovery and Pattern\nanalysis. In this paper we focus on Data cleaning and IP Address identification\nstages of preprocessing. Methodology has been proposed for both the stages. At\nthe end conclusion is made about number of users left after IP address\nidentification.\n", "versions": [{"version": "v1", "created": "Sat, 23 Aug 2014 05:31:35 GMT"}], "update_date": "2014-08-26", "authors_parsed": [["Verma", "Priyanka", ""], ["Kesswani", "Nishtha", ""]]}, {"id": "1408.5894", "submitter": "Georgios  Skoumas", "authors": "Georgios Skoumas, Dieter Pfoser, Anastasios Kyrillidis", "title": "Location Estimation Using Crowdsourced Geospatial Narratives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The \"crowd\" has become a very important geospatial data provider. Subsumed\nunder the term Volunteered Geographic Information (VGI), non-expert users have\nbeen providing a wealth of quantitative geospatial data online. With spatial\nreasoning being a basic form of human cognition, narratives expressing\ngeospatial experiences, e.g., travel blogs, would provide an even bigger source\nof geospatial data. Textual narratives typically contain qualitative data in\nthe form of objects and spatial relationships. The scope of this work is (i) to\nextract these relationships from user-generated texts, (ii) to quantify them\nand (iii) to reason about object locations based only on this qualitative data.\nWe use information extraction methods to identify toponyms and spatial\nrelationships and to formulate a quantitative approach based on distance and\norientation features to represent the latter. Positional probability\ndistributions for spatial relationships are determined by means of a greedy\nExpectation Maximization-based (EM) algorithm. These estimates are then used to\n\"triangulate\" the positions of unknown object locations. Experiments using a\ntext corpus harvested from travel blog sites establish the considerable\nlocation estimation accuracy of the proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 25 Aug 2014 15:05:54 GMT"}], "update_date": "2014-08-27", "authors_parsed": [["Skoumas", "Georgios", ""], ["Pfoser", "Dieter", ""], ["Kyrillidis", "Anastasios", ""]]}, {"id": "1408.6395", "submitter": "Fariz Darari", "authors": "Fariz Darari, Simon Razniewski, Werner Nutt", "title": "Bridging the Semantic Gap between RDF and SPARQL using Completeness\n  Statements [Extended Version]", "comments": "This paper is an extended version with proofs of a poster paper at\n  ISWC 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RDF data is often treated as incomplete, following the Open-World Assumption.\nOn the other hand, SPARQL, the standard query language over RDF, usually\nfollows the Closed-World Assumption, assuming RDF data to be complete. This\ngives rise to a semantic gap between RDF and SPARQL. In this paper, we address\nhow to close the semantic gap between RDF and SPARQL in terms of certain\nanswers and possible answers using completeness statements.\n", "versions": [{"version": "v1", "created": "Wed, 27 Aug 2014 12:31:17 GMT"}, {"version": "v2", "created": "Wed, 3 Sep 2014 19:09:38 GMT"}], "update_date": "2014-09-04", "authors_parsed": [["Darari", "Fariz", ""], ["Razniewski", "Simon", ""], ["Nutt", "Werner", ""]]}, {"id": "1408.6589", "submitter": "Wentao Wu", "authors": "Wentao Wu, Xi Wu, Hakan Hac{\\i}g\\\"um\\\"u\\c{s}, Jeffrey F. Naughton", "title": "Uncertainty Aware Query Execution Time Prediction", "comments": "This is the extended version of a paper with the same title and\n  authors that appears in the Proceedings of the VLDB Endowment (PVLDB), Vol.\n  7(14), 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting query execution time is a fundamental issue underlying many\ndatabase management tasks. Existing predictors rely on information such as\ncardinality estimates and system performance constants that are difficult to\nknow exactly. As a result, accurate prediction still remains elusive for many\nqueries. However, existing predictors provide a single, point estimate of the\ntrue execution time, but fail to characterize the uncertainty in the\nprediction. In this paper, we take a first step towards providing uncertainty\ninformation along with query execution time predictions. We use the query\noptimizer's cost model to represent the query execution time as a function of\nthe selectivities of operators in the query plan as well as the constants that\ndescribe the cost of CPU and I/O operations in the system. By treating these\nquantities as random variables rather than constants, we show that with low\noverhead we can infer the distribution of likely prediction errors. We further\nshow that the estimated prediction errors by our proposed techniques are\nstrongly correlated with the actual prediction errors.\n", "versions": [{"version": "v1", "created": "Wed, 27 Aug 2014 23:21:58 GMT"}], "update_date": "2014-08-29", "authors_parsed": [["Wu", "Wentao", ""], ["Wu", "Xi", ""], ["Hac\u0131g\u00fcm\u00fc\u015f", "Hakan", ""], ["Naughton", "Jeffrey F.", ""]]}, {"id": "1408.6691", "submitter": "Luca Matteis", "authors": "Luca Matteis", "title": "VoID-graph: Visualize Linked Datasets on the Web", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The Linked Open Data (LOD) cloud diagram is a picture that helps us grasp the\ncontents and the links of globally available data sets. Such diagram has been a\npowerful dissemination method for the Linked Data movement, allowing people to\nglance at the size and structure of this distributed, interconnected database.\nHowever, generating such image for third-party datasets can be a quite complex\ntask as it requires the installation and understanding of a variety of tools\nwhich are not easy to setup. In this paper we present VoID-graph\n(http://lmatteis.github.io/void-graph/), a standalone web-tool that, given a\nVoID description, can visualize a diagram similar to the LOD cloud. It is novel\nbecause the diagram is autonomously shaped from VoID descriptions directly\nwithin a Web-browser, which doesn't require any server cooperation. This makes\nit not only easy to use, as no installation or configuration is required, but\nalso makes it more sustainable, as it is built using Open Web standards such as\nJavaScript and SVG.\n", "versions": [{"version": "v1", "created": "Thu, 28 Aug 2014 12:01:51 GMT"}], "update_date": "2014-08-29", "authors_parsed": [["Matteis", "Luca", ""]]}, {"id": "1408.6916", "submitter": "Jiannan Wang", "authors": "Jiannan Wang and Guoliang Li and Tim Kraska and Michael J. Franklin\n  and Jianhua Feng", "title": "Leveraging Transitive Relations for Crowdsourced Joins", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of crowdsourced query processing systems has recently\nattracted a significant attention in the database community. A variety of\ncrowdsourced queries have been investigated. In this paper, we focus on the\ncrowdsourced join query which aims to utilize humans to find all pairs of\nmatching objects from two collections. As a human-only solution is expensive,\nwe adopt a hybrid human-machine approach which first uses machines to generate\na candidate set of matching pairs, and then asks humans to label the pairs in\nthe candidate set as either matching or non-matching. Given the candidate\npairs, existing approaches will publish all pairs for verification to a\ncrowdsourcing platform. However, they neglect the fact that the pairs satisfy\ntransitive relations. As an example, if $o_1$ matches with $o_2$, and $o_2$\nmatches with $o_3$, then we can deduce that $o_1$ matches with $o_3$ without\nneeding to crowdsource $(o_1, o_3)$. To this end, we study how to leverage\ntransitive relations for crowdsourced joins. We propose a hybrid\ntransitive-relations and crowdsourcing labeling framework which aims to\ncrowdsource the minimum number of pairs to label all the candidate pairs. We\nprove the optimal labeling order in an ideal setting and propose a heuristic\nlabeling order in practice. We devise a parallel labeling algorithm to\nefficiently crowdsource the pairs following the order. We evaluate our\napproaches in both simulated environment and a real crowdsourcing platform.\nExperimental results show that our approaches with transitive relations can\nsave much more money and time than existing methods, with a little loss in the\nresult quality.\n", "versions": [{"version": "v1", "created": "Fri, 29 Aug 2014 03:36:29 GMT"}, {"version": "v2", "created": "Fri, 26 Sep 2014 05:37:41 GMT"}], "update_date": "2014-09-29", "authors_parsed": [["Wang", "Jiannan", ""], ["Li", "Guoliang", ""], ["Kraska", "Tim", ""], ["Franklin", "Michael J.", ""], ["Feng", "Jianhua", ""]]}]