[{"id": "1911.00231", "submitter": "Konstantinos Karanasos", "authors": "Konstantinos Karanasos, Matteo Interlandi, Doris Xin, Fotis Psallidas,\n  Rathijit Sen, Kwanghyun Park, Ivan Popivanov, Supun Nakandal, Subru Krishnan,\n  Markus Weimer, Yuan Yu, Raghu Ramakrishnan, Carlo Curino", "title": "Extending Relational Query Processing with ML Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The broadening adoption of machine learning in the enterprise is increasing\nthe pressure for strict governance and cost-effective performance, in\nparticular for the common and consequential steps of model storage and\ninference. The RDBMS provides a natural starting point, given its mature\ninfrastructure for fast data access and processing, along with support for\nenterprise features (e.g., encryption, auditing, high-availability). To take\nadvantage of all of the above, we need to address a key concern: Can in-RDBMS\nscoring of ML models match (outperform?) the performance of dedicated\nframeworks? We answer the above positively by building Raven, a system that\nleverages native integration of ML runtimes (i.e., ONNX Runtime) deep within\nSQL Server, and a unified intermediate representation (IR) to enable advanced\ncross-optimizations between ML and DB operators. In this optimization space, we\ndiscover the most exciting research opportunities that combine DB/Compiler/ML\nthinking. Our initial evaluation on real data demonstrates performance gains of\nup to 5.5x from the native integration of ML in SQL Server, and up to 24x from\ncross-optimizations--we will demonstrate Raven live during the conference talk.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 07:09:08 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Karanasos", "Konstantinos", ""], ["Interlandi", "Matteo", ""], ["Xin", "Doris", ""], ["Psallidas", "Fotis", ""], ["Sen", "Rathijit", ""], ["Park", "Kwanghyun", ""], ["Popivanov", "Ivan", ""], ["Nakandal", "Supun", ""], ["Krishnan", "Subru", ""], ["Weimer", "Markus", ""], ["Yu", "Yuan", ""], ["Ramakrishnan", "Raghu", ""], ["Curino", "Carlo", ""]]}, {"id": "1911.00336", "submitter": "Roni Mateless", "authors": "Roni Mateless, Michael Segal and Robert Moskovitch", "title": "THAAD: Efficient Matching Queries under Temporal Abstraction for Anomaly\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a novel algorithm and efficient data structure for\nanomaly detection based on temporal data. Time-series data are represented by a\nsequence of symbolic time intervals, describing increasing and decreasing\ntrends, in a compact way using gradient temporal abstraction technique. Then we\nidentify unusual subsequences in the resulting sequence using dynamic data\nstructure based on the geometric observations supporting polylogarithmic update\nand query times. Moreover, we introduce a new parameter to control the pairwise\ndifference between the corresponding symbols in addition to a distance metric\nbetween the subsequences. Experimental results on a public DNS network traffic\ndataset show the superiority of our approach compared to the baselines.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 12:33:34 GMT"}, {"version": "v2", "created": "Mon, 4 Nov 2019 06:53:53 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Mateless", "Roni", ""], ["Segal", "Michael", ""], ["Moskovitch", "Robert", ""]]}, {"id": "1911.00524", "submitter": "Sadok Ben Yahia", "authors": "Sadok Ben Yahia", "title": "Contributions to the Formalization and Extraction of Generic Bases of\n  Association Rules", "comments": "in French, HDR thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this thesis, a detailed study shows that closed itemsets and minimal\ngenerators play a key role for concisely representing both frequent itemsets\nand association rules. These itemsets structure the search space into\nequivalence classes such that each class gathers the itemsets appearing in the\nsame subset aka objects or transactions of the given data. In this respect, we\nproposed lossless reductions of the minimal generator set thanks to a new\nsubstitution-based process. Our theoretical results are extended to the\nassociation rule framework in order to reduce as much as possible the number of\nretained rules without information loss. We then give a thorough formal study\nof the related inference mechanism allowing to derive all redundant association\nrules, starting from the retained ones. We also lead a thorough exploration of\nthe disjunctive search space, where itemsets are characterized by their\nrespective disjunctive supports, instead of the conjunctive ones. This\nexploration is motivated by the fact that, in some applications, such\ninformation brings richer knowledge to the end-users. To obtain a redundancy\nfree representation of the disjunctive search space, an interesting solution\nconsists in selecting a unique element to represent itemsets covering the same\nset of data. Two itemsets are equivalent if their respective items cover the\nsame set of data. In this regard, we introduced a new operator dedicated to\nthis task. In each induced equivalence class, minimal elements are called\nessential itemsets, while the largest one is called disjunctive closed itemset.\nThe introduced operator is then at the roots of new concise representations of\nfrequent itemsets. We also exploit the disjunctive search space to derive\ngeneralized association rules. These latter rules generalize classic ones to\nalso offer disjunction and negation connectors between items, in addition to\nthe conjunctive one.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 18:00:47 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Yahia", "Sadok Ben", ""]]}, {"id": "1911.00598", "submitter": "Paolo Pareti Dr.", "authors": "Paolo Pareti, George Konstantinidis, Timothy J. Norman, Murat\n  \\c{S}ensoy", "title": "SHACL Constraints with Inference Rules", "comments": null, "journal-ref": "In International Semantic Web Conference, pp. 539-557. Springer,\n  Cham, 2019", "doi": "10.1007/978-3-030-30793-6_31", "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Shapes Constraint Language (SHACL) has been recently introduced as a W3C\nrecommendation to define constraints that can be validated against RDF graphs.\nInteractions of SHACL with other Semantic Web technologies, such as ontologies\nor reasoners, is a matter of ongoing research. In this paper we study the\ninteraction of a subset of SHACL with inference rules expressed in datalog. On\nthe one hand, SHACL constraints can be used to define a \"schema\" for graph\ndatasets. On the other hand, inference rules can lead to the discovery of new\nfacts that do not match the original schema. Given a set of SHACL constraints\nand a set of datalog rules, we present a method to detect which constraints\ncould be violated by the application of the inference rules on some graph\ninstance of the schema, and update the original schema, i.e, the set of SHACL\nconstraints, in order to capture the new facts that can be inferred. We provide\ntheoretical and experimental results of the various components of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 21:49:49 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Pareti", "Paolo", ""], ["Konstantinidis", "George", ""], ["Norman", "Timothy J.", ""], ["\u015eensoy", "Murat", ""]]}, {"id": "1911.00602", "submitter": "William Croft", "authors": "William Lee Croft, J\\\"org-R\\\"udiger Sack, Wei Shi", "title": "Differential Privacy Via a Truncated and Normalized Laplace Mechanism", "comments": "This is a pre-print of an article published in Journal of Computer\n  Science and Technology. The final authenticated version is available online\n  at: https://doi.org/10.1007/s11390-020-0193-z", "journal-ref": null, "doi": "10.1007/s11390-020-0193-z", "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When querying databases containing sensitive information, the privacy of\nindividuals stored in the database has to be guaranteed. Such guarantees are\nprovided by differentially private mechanisms which add controlled noise to the\nquery responses. However, most such mechanisms do not take into consideration\nthe valid range of the query being posed. Thus, noisy responses that fall\noutside of this range may potentially be produced. To rectify this and\ntherefore improve the utility of the mechanism, the commonly used Laplace\ndistribution can be truncated to the valid range of the query and then\nnormalized. However, such a data-dependent operation of normalization leaks\nadditional information about the true query response thereby violating the\ndifferential privacy guarantee.\n  Here, we propose a new method which preserves the differential privacy\nguarantee through a careful determination of an appropriate scaling parameter\nfor the Laplace distribution. We also generalize the privacy guarantee in the\ncontext of the Laplace distribution to account for data-dependent normalization\nfactors and study this guarantee for different classes of range constraint\nconfigurations. We provide derivations of the optimal scaling parameter (i.e.,\nthe minimal value that preserves differential privacy) for each class or\nprovide an approximation thereof. As a consequence of this work, one can use\nthe Laplace distribution to answer queries in a range-adherent and\ndifferentially private manner.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 22:05:52 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2020 14:31:30 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Croft", "William Lee", ""], ["Sack", "J\u00f6rg-R\u00fcdiger", ""], ["Shi", "Wei", ""]]}, {"id": "1911.00675", "submitter": "Otmar Ertl", "authors": "Otmar Ertl", "title": "ProbMinHash -- A Class of Locality-Sensitive Hash Algorithms for the\n  (Probability) Jaccard Similarity", "comments": "to be published in TKDE, source code available at\n  https://github.com/oertl/probminhash", "journal-ref": null, "doi": "10.1109/TKDE.2020.3021176", "report-no": null, "categories": "cs.DS cs.DB cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The probability Jaccard similarity was recently proposed as a natural\ngeneralization of the Jaccard similarity to measure the proximity of sets whose\nelements are associated with relative frequencies or probabilities. In\ncombination with a hash algorithm that maps those weighted sets to compact\nsignatures which allow fast estimation of pairwise similarities, it constitutes\na valuable method for big data applications such as near-duplicate detection,\nnearest neighbor search, or clustering. This paper introduces a class of\none-pass locality-sensitive hash algorithms that are orders of magnitude faster\nthan the original approach. The performance gain is achieved by calculating\nsignature components not independently, but collectively. Four different\nalgorithms are proposed based on this idea. Two of them are statistically\nequivalent to the original approach and can be used as drop-in replacements.\nThe other two may even improve the estimation error by introducing statistical\ndependence between signature components. Moreover, the presented techniques can\nbe specialized for the conventional Jaccard similarity, resulting in highly\nefficient algorithms that outperform traditional minwise hashing and that are\nable to compete with the state of the art.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 07:58:10 GMT"}, {"version": "v2", "created": "Sun, 5 Apr 2020 21:16:52 GMT"}, {"version": "v3", "created": "Mon, 26 Oct 2020 10:29:53 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Ertl", "Otmar", ""]]}, {"id": "1911.00837", "submitter": "Suyash Gupta", "authors": "Suyash Gupta, Jelle Hellings, Mohammad Sadoghi", "title": "RCC: Resilient Concurrent Consensus for High-Throughput Secure\n  Transaction Processing", "comments": "To appear in the proceedings of 37th IEEE International Conference on\n  Data Engineering (ICDE) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, we saw the emergence of consensus-based database systems that\npromise resilience against failures, strong data provenance, and federated data\nmanagement. Typically, these fully-replicated systems are operated on top of a\nprimary-backup consensus protocol, which limits the throughput of these systems\nto the capabilities of a single replica (the primary). To push throughput\nbeyond this single-replica limit, we propose concurrent consensus. In\nconcurrent consensus, replicas independently propose transactions, thereby\nreducing the influence of any single replica on performance. To put this idea\nin practice, we propose our RCC paradigm that can turn any primary-backup\nconsensus protocol into a concurrent consensus protocol by running many\nconsensus instances concurrently. RCC is designed with performance in mind and\nrequires minimal coordination between instances. Furthermore, RCC also promises\nincreased resilience against failures. We put the design of RCC to the test by\nimplementing it in ResilientDB, our high-performance resilient blockchain\nfabric, and comparing it with state-of-the-art primary-backup consensus\nprotocols. Our experiments show that RCC achieves up to 2.75x higher throughput\nthan other consensus protocols and can be scaled to 91 replicas.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2019 06:03:16 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 20:44:22 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Gupta", "Suyash", ""], ["Hellings", "Jelle", ""], ["Sadoghi", "Mohammad", ""]]}, {"id": "1911.00838", "submitter": "Suyash Gupta", "authors": "Suyash Gupta, Jelle Hellings, Sajjad Rahnama, Mohammad Sadoghi", "title": "Proof-of-Execution: Reaching Consensus through Fault-Tolerant\n  Speculation", "comments": "To appear in the proceedings of 24th International Conference on\n  Extending Database Technology (EDBT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-party data management and blockchain systems require data sharing among\nparticipants. To provide resilient and consistent data sharing, transactions\nengines rely on Byzantine FaultTolerant consensus (BFT), which enables\noperations during failures and malicious behavior. Unfortunately, existing BFT\nprotocols are unsuitable for high-throughput applications due to their high\ncomputational costs, high communication costs, high client latencies, and/or\nreliance on twin-paths and non-faulty clients. In this paper, we present the\nProof-of-Execution consensus protocol (PoE) that alleviates these challenges.\nAt the core of PoE are out-of-order processing and speculative execution, which\nallow PoE to execute transactions before consensus is reached among the\nreplicas. With these techniques, PoE manages to reduce the costs of BFT in\nnormal cases, while guaranteeing reliable consensus for clients in all cases.\nWe envision the use of PoE in high-throughput multi-party data-management and\nblockchain systems. To validate this vision, we implement PoE in our efficient\nResilientDB fabric and extensively evaluate PoE against several\nstate-of-the-art BFT protocols. Our evaluation showcases that PoE achieves\nup-to-80% higher throughputs than existing BFT protocols in the presence of\nfailures.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2019 06:03:22 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 01:27:18 GMT"}, {"version": "v3", "created": "Tue, 23 Feb 2021 01:12:31 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Gupta", "Suyash", ""], ["Hellings", "Jelle", ""], ["Rahnama", "Sajjad", ""], ["Sadoghi", "Mohammad", ""]]}, {"id": "1911.00949", "submitter": "Zhongfang Zhuang", "authors": "Zhongfang Zhuang, Xiangnan Kong, Elke Rundensteiner, Jihane Zouaoui,\n  Aditya Arora", "title": "Attributed Sequence Embedding", "comments": "Accepted by IEEE Big Data 2019", "journal-ref": null, "doi": "10.1109/BigData47090.2019.9006481", "report-no": null, "categories": "cs.LG cs.CL cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining tasks over sequential data, such as clickstreams and gene sequences,\nrequire a careful design of embeddings usable by learning algorithms. Recent\nresearch in feature learning has been extended to sequential data, where each\ninstance consists of a sequence of heterogeneous items with a variable length.\nHowever, many real-world applications often involve attributed sequences, where\neach instance is composed of both a sequence of categorical items and a set of\nattributes. In this paper, we study this new problem of attributed sequence\nembedding, where the goal is to learn the representations of attributed\nsequences in an unsupervised fashion. This problem is core to many important\ndata mining tasks ranging from user behavior analysis to the clustering of gene\nsequences. This problem is challenging due to the dependencies between\nsequences and their associated attributes. We propose a deep multimodal\nlearning framework, called NAS, to produce embeddings of attributed sequences.\nThe embeddings are task independent and can be used on various mining tasks of\nattributed sequences. We demonstrate the effectiveness of our embeddings of\nattributed sequences in various unsupervised learning tasks on real-world\ndatasets.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2019 19:16:51 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Zhuang", "Zhongfang", ""], ["Kong", "Xiangnan", ""], ["Rundensteiner", "Elke", ""], ["Zouaoui", "Jihane", ""], ["Arora", "Aditya", ""]]}, {"id": "1911.01030", "submitter": "Caihua Shan", "authors": "Caihua Shan, Nikos Mamoulis, Reynold Cheng, Guoliang Li, Xiang Li and\n  Yuqiu Qian", "title": "An End-to-End Deep RL Framework for Task Arrangement in Crowdsourcing\n  Platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a Deep Reinforcement Learning (RL) framework for\ntask arrangement, which is a critical problem for the success of crowdsourcing\nplatforms. Previous works conduct the personalized recommendation of tasks to\nworkers via supervised learning methods. However, the majority of them only\nconsider the benefit of either workers or requesters independently. In\naddition, they cannot handle the dynamic environment and may produce\nsub-optimal results. To address these issues, we utilize Deep Q-Network (DQN),\nan RL-based method combined with a neural network to estimate the expected\nlong-term return of recommending a task. DQN inherently considers the immediate\nand future reward simultaneously and can be updated in real-time to deal with\nevolving data and dynamic changes. Furthermore, we design two DQNs that capture\nthe benefit of both workers and requesters and maximize the profit of the\nplatform. To learn value functions in DQN effectively, we also propose novel\nstate representations, carefully design the computation of Q values, and\npredict transition probabilities and future states. Experiments on synthetic\nand real datasets demonstrate the superior performance of our framework.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 04:53:11 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Shan", "Caihua", ""], ["Mamoulis", "Nikos", ""], ["Cheng", "Reynold", ""], ["Li", "Guoliang", ""], ["Li", "Xiang", ""], ["Qian", "Yuqiu", ""]]}, {"id": "1911.01042", "submitter": "Caihua Shan", "authors": "Caihua Shan, Leong Hou U, Nikos Mamoulis, Reynold Cheng, and Xiang Li", "title": "A General Early-Stopping Module for Crowdsourced Ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing can be used to determine a total order for an object set (e.g.,\nthe top-10 NBA players) based on crowd opinions. This ranking problem is often\ndecomposed into a set of microtasks (e.g., pairwise comparisons). These\nmicrotasks are passed to a large number of workers and their answers are\naggregated to infer the ranking. The number of microtasks depends on the budget\nallocated for the problem. Intuitively, the higher the number of microtask\nanswers, the more accurate the ranking becomes. However, it is often hard to\ndecide the budget required for an accurate ranking. We study how a ranking\nprocess can be terminated early, and yet achieve a high-quality ranking and\ngreat savings in the budget. We use statistical tools to estimate the quality\nof the ranking result at any stage of the crowdsourcing process and terminate\nthe process as soon as the desired quality is achieved. Our proposed\nearly-stopping module can be seamlessly integrated with most existing inference\nalgorithms and task assignment methods. We conduct extensive experiments and\nshow that our early-stopping module is better than other existing general\nstopping criteria. We also implement a prototype system to demonstrate the\nusability and effectiveness of our approach in practice.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 06:15:52 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Shan", "Caihua", ""], ["U", "Leong Hou", ""], ["Mamoulis", "Nikos", ""], ["Cheng", "Reynold", ""], ["Li", "Xiang", ""]]}, {"id": "1911.01225", "submitter": "Fan (Fred) Lin", "authors": "Fred Lin, Keyur Muzumdar, Nikolay Pavlovich Laptev, Mihai-Valentin\n  Curelea, Seunghak Lee, Sriram Sankar", "title": "Fast Dimensional Analysis for Root Cause Investigation in a Large-Scale\n  Service Environment", "comments": "13 pages", "journal-ref": "POMACS 2020", "doi": "10.1145/3392149", "report-no": null, "categories": "cs.DC cs.DB cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Root cause analysis in a large-scale production environment is challenging\ndue to the complexity of services running across global data centers. Due to\nthe distributed nature of a large-scale system, the various hardware, software,\nand tooling logs are often maintained separately, making it difficult to review\nthe logs jointly for understanding production issues. Another challenge in\nreviewing the logs for identifying issues is the scale - there could easily be\nmillions of entities, each described by hundreds of features. In this paper we\npresent a fast dimensional analysis framework that automates the root cause\nanalysis on structured logs with improved scalability.\n  We first explore item-sets, i.e. combinations of feature values, that could\nidentify groups of samples with sufficient support for the target failures\nusing the Apriori algorithm and a subsequent improvement, FP-Growth. These\nalgorithms were designed for frequent item-set mining and association rule\nlearning over transactional databases. After applying them on structured logs,\nwe select the item-sets that are most unique to the target failures based on\nlift. We propose pre-processing steps with the use of a large-scale real-time\ndatabase and post-processing techniques and parallelism to further speed up the\nanalysis and improve interpretability, and demonstrate that such optimization\nis necessary for handling large-scale production datasets. We have successfully\nrolled out this approach for root cause investigation purposes in a large-scale\ninfrastructure. We also present the setup and results from multiple production\nuse cases in this paper.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 01:03:01 GMT"}, {"version": "v2", "created": "Thu, 6 Feb 2020 19:49:04 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Lin", "Fred", ""], ["Muzumdar", "Keyur", ""], ["Laptev", "Nikolay Pavlovich", ""], ["Curelea", "Mihai-Valentin", ""], ["Lee", "Seunghak", ""], ["Sankar", "Sriram", ""]]}, {"id": "1911.01231", "submitter": "Mohammad Mahdi Dehshibi Dr.", "authors": "Mohammad Fazlali and Sara Moazezi Eftekhar and Mohammad Mahdi Dehshibi\n  and Hadi Tabatabaee Malazi and Masoud Nosrati", "title": "Raft Consensus Algorithm: an Effective Substitute for Paxos in High\n  Throughput P2P-based Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  One of the significant problem in peer-to-peer databases is collision\nproblem. These databases do not rely on a central leader that is a reason to\nincrease scalability and fault tolerance. Utilizing these systems in high\nthroughput computing cause more flexibility in computing system and meanwhile\nsolve the problems in most of the computing systems which are depend on a\ncentral nodes. There are limited researches in this scope and they seem are not\nsuitable for using in a large scale. In this paper, we used Cassandra which is\na distributed database based on peer-to-peer network as a high throughput\ncomputing system. Cassandra uses Paxos to elect central leader by default that\ncauses collision problem. Among existent consensus algorithms Raft separates\nthe key elements of consensus, such as leader election, so enforces a stronger\ndegree of coherency to reduce the number of states that must be considered,\nsuch as collision.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 14:07:58 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Fazlali", "Mohammad", ""], ["Eftekhar", "Sara Moazezi", ""], ["Dehshibi", "Mohammad Mahdi", ""], ["Malazi", "Hadi Tabatabaee", ""], ["Nosrati", "Masoud", ""]]}, {"id": "1911.01248", "submitter": "Diego Moussallem", "authors": "Axel-Cyrille Ngonga Ngomo and Diego Moussallem and Lorenz B\\\"uhmann", "title": "A Holistic Natural Language Generation Framework for the Semantic Web", "comments": "International Conference Recent Advances in Natural Language\n  Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the ever-growing generation of data for the Semantic Web comes an\nincreasing demand for this data to be made available to non-semantic Web\nexperts. One way of achieving this goal is to translate the languages of the\nSemantic Web into natural language. We present LD2NL, a framework for\nverbalizing the three key languages of the Semantic Web, i.e., RDF, OWL, and\nSPARQL. Our framework is based on a bottom-up approach to verbalization. We\nevaluated LD2NL in an open survey with 86 persons. Our results suggest that our\nframework can generate verbalizations that are close to natural languages and\nthat can be easily understood by non-experts. Therewith, it enables non-domain\nexperts to interpret Semantic Web data with more than 91\\% of the accuracy of\ndomain experts.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 14:35:59 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Ngomo", "Axel-Cyrille Ngonga", ""], ["Moussallem", "Diego", ""], ["B\u00fchmann", "Lorenz", ""]]}, {"id": "1911.01270", "submitter": "Rabah Tighilt Ferhat", "authors": "Amal Ait Brahim, Rabah Tighilt Ferhat, Gilles Zurfluh", "title": "Incremental extraction of a NoSQL database model using an MDA-based\n  process", "comments": null, "journal-ref": "Publication of the 2019 World Congress in Computer Science,\n  Computer Engineering, and Applied Computing (CSCE 19 ) July 29 - August 01 ,\n  2019 | Las Vegas, Nevada, USA", "doi": null, "report-no": "ISBN: 1-60132-505-3", "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the need to use NoSQL systems to store and exploit big data\nhas been steadily increasing. Most of these systems are characterized by the\nproperty \"schema less\" which means absence of the data model when creating a\ndatabase. This property brings an undeniable flexibility by allowing the\nevolution of the model during the exploitation of the base. However, the\nexpression of queries requires a precise knowledge of this model. In this\npaper, we propose an incremental process to extract the model while operating\nthe document-oriented NoSQL database. To do this, we use the Model Driven\nArchitecture (MDA) that provides a formal framework for automatic model\ntransformation. From the insert, delete and update queries executed on the\ndatabase, we propose formal transformation rules with QVT to generate the\nphysical model of the NoSQL database. An experimentation of the extraction\nprocess was performed on a medical application.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 15:07:28 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Brahim", "Amal Ait", ""], ["Ferhat", "Rabah Tighilt", ""], ["Zurfluh", "Gilles", ""]]}, {"id": "1911.01633", "submitter": "Sina Shaham", "authors": "Sina Shaham, Saba Rafieian, Ming Ding, Mahyar Shirvanimoghaddam, and\n  Zihuai Lin", "title": "On the Importance of Location Privacy for Users of Location Based\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Do people care about their location privacy while using location-based\nservice apps? This paper aims to answer this question and several other\nhypotheses through a survey, and review the privacy preservation techniques.\nOur results indicate that privacy is indeed an influential factor in the\nselection of location-based apps by users.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 06:03:16 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Shaham", "Sina", ""], ["Rafieian", "Saba", ""], ["Ding", "Ming", ""], ["Shirvanimoghaddam", "Mahyar", ""], ["Lin", "Zihuai", ""]]}, {"id": "1911.01867", "submitter": "Ayman Taha", "authors": "Ayman Taha, Hoda M.Onsi, Mohammed Nour El din, Osman M. Hegazy", "title": "A Model for Spatial Outlier Detection Based on Weighted Neighborhood\n  Relationship", "comments": "Geographic Information Systems (GIS), Spatial Data Mining (SDM),\n  Spatial Data Computing (SDC), Spatial Outlier Detection (SOD), Spatial\n  Autocorrelation, Neighborhood relationship. arXiv admin note: substantial\n  text overlap with arXiv:1601.07241", "journal-ref": "Egyptian Informatics Journal 2, 2005", "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial outliers are used to discover inconsistent objects producing\nimplicit, hidden, and interesting knowledge, which has an effective role in\ndecision-making process. In this paper, we propose a model to redefine the\nspatial neighborhood relationship by considering weights of the most effective\nparameters of neighboring objects in a given spatial data set. The spatial\nparameters, which are taken into our consideration, are distance, cost, and\nnumber of direct connections between neighboring objects. This model is\nadaptable to be applied on polygonal objects. The proposed model is applied to\na GIS system supporting literacy project in Fayoum governorate.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 12:40:38 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Taha", "Ayman", ""], ["Onsi", "Hoda M.", ""], ["din", "Mohammed Nour El", ""], ["Hegazy", "Osman M.", ""]]}, {"id": "1911.01874", "submitter": "Abel Dasylva Dr.", "authors": "Abel Dasylva, Arthur Goussanou, David Ajavon and Hanan Abousaleh", "title": "Revisiting the probabilistic method of record linkage", "comments": "Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DB cs.IR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In theory, the probabilistic linkage method provides two distinct advantages\nover non-probabilistic methods, including minimal rates of linkage error and\naccurate measures of these rates for data users. However, implementations can\nfall short of these expectations either because the conditional independence\nassumption is made, or because a model with interactions is used but lacks the\nidentification property. In official statistics, this is currently the main\nchallenge to the automated production and use of linked data. To address this\nchallenge, a new methodology is described for proper linkage problems, where\nmatched records may be identified with a probability that is bounded away from\nzero, regardless of the population size. It models the number of neighbours of\na given record, i.e. the number of resembling records. To be specific, the\nproposed model is a finite mixture where each component is the sum of a\nBernoulli variable with an independent Poisson variable. It has the\nidentification property and yields solutions for many longstanding problems,\nincluding the evaluation of blocking criteria and the estimation of linkage\nerrors for probabilistic or non-probabilistic linkages, all without clerical\nreviews or conditional independence assumptions. Thus it also enables\nunsupervised machine learning solutions for record linkage problems.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 15:28:46 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Dasylva", "Abel", ""], ["Goussanou", "Arthur", ""], ["Ajavon", "David", ""], ["Abousaleh", "Hanan", ""]]}, {"id": "1911.02095", "submitter": "Gowri Nayar", "authors": "Edward E. Seabolt, Gowri Nayar, Harsha Krishnareddy, Akshay Agarwal,\n  Kristen L. Beck, Ignacio Terrizzano, Eser Kandogan, Mary Roth, Vandana\n  Mukherjee, and James H. Kaufman", "title": "IBM Functional Genomics Platform, A Cloud-Based Platform for Studying\n  Microbial Life at Scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid growth in biological sequence data is revolutionizing our\nunderstanding of genotypic diversity and challenging conventional approaches to\ninformatics. With the increasing availability of genomic data, traditional\nbioinformatic tools require substantial computational time and the creation of\never-larger indices each time a researcher seeks to gain insight from the data.\nTo address these challenges, we pre-computed important relationships between\nbiological entities spanning the Central Dogma of Molecular Biology and\ncaptured this information in a relational database. The database can be queried\nacross hundreds of millions of entities and returns results in a fraction of\nthe time required by traditional methods. In this paper, we describe\n\\textit{IBM Functional Genomics Platform} (formerly known as OMXWare), a\ncomprehensive database relating genotype to phenotype for bacterial life.\nContinually updated, IBM Functional Genomics Platform today contains data\nderived from 200,000 curated, self-consistently assembled genomes. The database\nstores functional data for over 68 million genes, 52 million proteins, and 239\nmillion domains with associated biological activity annotations from Gene\nOntology, KEGG, MetaCyc, and Reactome. IBM Functional Genomics Platform maps\nall of the many-to-many connections between each biological entity including\nthe originating genome, gene, protein, and protein domain. Various microbial\nstudies, from infectious disease to environmental health, can benefit from the\nrich data and connections. We describe the data selection, the pipeline to\ncreate and update the IBM Functional Genomics Platform, and the developer tools\n(Python SDK and REST APIs) which allow researchers to efficiently study\nmicrobial life at scale.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 21:32:25 GMT"}, {"version": "v2", "created": "Sun, 15 Mar 2020 19:29:18 GMT"}, {"version": "v3", "created": "Mon, 30 Mar 2020 23:14:33 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Seabolt", "Edward E.", ""], ["Nayar", "Gowri", ""], ["Krishnareddy", "Harsha", ""], ["Agarwal", "Akshay", ""], ["Beck", "Kristen L.", ""], ["Terrizzano", "Ignacio", ""], ["Kandogan", "Eser", ""], ["Roth", "Mary", ""], ["Mukherjee", "Vandana", ""], ["Kaufman", "James H.", ""]]}, {"id": "1911.02105", "submitter": "Ivens Portugal", "authors": "Ivens Portugal, Paulo Alencar, Donald Cowan", "title": "Spatial-Temporal Cluster Relations -- A Foundation for Trajectory\n  Cluster Lifetime Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial-temporal data, that is information about objects that exist at a\nparticular location and time period, are rich in value and, as a consequence,\nthe target of so many initiative efforts. Clustering approaches aim at grouping\ndatapoints based on similar properties for classification tasks. These\napproaches have been widely used in domains such as human mobility, ecology,\nhealth and astronomy. However, clustering approaches typically address only the\nstatic nature of a cluster, and do not take into consideration its dynamic\naspects. A desirable approach needs to investigate relations between dynamic\nclusters and their elements that can be used to derive new insights about what\nhappened to the clusters during their lifetimes. A fundamental step towards\nthis goal is to provide a formal definition of spatial-temporal cluster\nrelations. This report introduces, describes, and formalizes 14 novel\nspatial-temporal cluster relations that may occur during the existence of a\ncluster and involve both trajectory-cluster membership conditions and\ncluster-cluster comparisons. We evaluate the proposed relations with a\ndiscussion on how they are able to interpret complex cases that are difficult\nto be distinguished without a formal relation specification. We conclude the\nreport by summarizing our results and describing avenues for further research.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 22:00:15 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Portugal", "Ivens", ""], ["Alencar", "Paulo", ""], ["Cowan", "Donald", ""]]}, {"id": "1911.02282", "submitter": "Claudia Malzer", "authors": "Claudia Malzer and Marcus Baum", "title": "A Hybrid Approach To Hierarchical Density-based Cluster Selection", "comments": "6 pages. Conference: 2020 IEEE International Conference on\n  Multisensor Fusion and Integration for Intelligent Systems (MFI)", "journal-ref": "2020 IEEE International Conference on Multisensor Fusion and\n  Integration for Intelligent Systems (MFI), Karlsruhe, Germany, 2020, pp.\n  223-228", "doi": "10.1109/MFI49285.2020.9235263", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  HDBSCAN is a density-based clustering algorithm that constructs a cluster\nhierarchy tree and then uses a specific stability measure to extract flat\nclusters from the tree. We show how the application of an additional threshold\nvalue can result in a combination of DBSCAN* and HDBSCAN clusters, and\ndemonstrate potential benefits of this hybrid approach when clustering data of\nvariable densities. In particular, our approach is useful in scenarios where we\nrequire a low minimum cluster size but want to avoid an abundance of\nmicro-clusters in high-density regions. The method can directly be applied to\nHDBSCAN's tree of cluster candidates and does not require any modifications to\nthe hierarchy itself. It can easily be integrated as an addition to existing\nHDBSCAN implementations.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 09:59:56 GMT"}, {"version": "v2", "created": "Sun, 8 Dec 2019 09:47:23 GMT"}, {"version": "v3", "created": "Thu, 10 Dec 2020 09:25:13 GMT"}, {"version": "v4", "created": "Thu, 21 Jan 2021 13:39:38 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Malzer", "Claudia", ""], ["Baum", "Marcus", ""]]}, {"id": "1911.02646", "submitter": "Noreen Jamil", "authors": "M.Asif Naeem, Erum Mehmood, M G Abbas, Noreen Jamil", "title": "Optimizing Semi-Stream CACHEJOIN for Near-Real-Time Data Warehousing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Streaming data join is a critical process in the field of near-real-time data\nwarehousing. For this purpose, an adaptive semi-stream join algorithm called\nCACHEJOIN (Cache Join) focusing non-uniform stream data is provided in the\nliterature. However, this algorithm cannot exploit the memory and CPU resources\noptimally and consequently it leaves its service rate suboptimal due to\nsequential execution of both of its phases, called stream-probing (SP) phase\nand disk-probing (DP) phase. By integrating the advantages of CACHEJOIN, in\nthis paper we present two modifications in it. First is called P-CACHEJOIN\n(Parallel Cache Join) that enables the parallel processing of two phases in\nCACHEJOIN. This increases number of joined stream records and therefore\nimproves throughput considerably. Second is called OP-CACHEJOIN (Optimized\nParallel Cache Join) that implements a parallel loading of stored data into\nmemory while the DP phase is executing. We present the performance analysis of\nboth of our approaches with existing CACHEJOIN empirically using synthetic\nskewed dataset.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 11:37:56 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Naeem", "M. Asif", ""], ["Mehmood", "Erum", ""], ["Abbas", "M G", ""], ["Jamil", "Noreen", ""]]}, {"id": "1911.02668", "submitter": "Julien Corman", "authors": "Julien Corman and Guohui Xiao", "title": "Certain Answers to a SPARQL Query over a Knowledge Base (extended\n  version)", "comments": "This is the extended version of a article published at the 9th Joint\n  International Semantic Technology Conference (JIST 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Ontology-Mediated Query Answering (OMQA) is a well-established framework to\nanswer queries over an RDFS or OWL Knowledge Base (KB). OMQA was originally\ndesigned for unions of conjunctive queries (UCQs), and based on certain\nanswers. More recently, OMQA has been extended to SPARQL queries, but to our\nknowledge, none of the efforts made in this direction (either in the\nliterature, or the so-called SPARQL entailment regimes) is able to capture both\ncertain answers for UCQs and the standard interpretation of SPARQL over a plain\ngraph. We formalize these as requirements to be met by any semantics aiming at\nconciliating certain answers and SPARQL answers, and define three additional\nrequirements, which generalize to KBs some basic properties of SPARQL answers.\nThen we show that a semantics can be defined that satisfies all requirements\nfor SPARQL queries with SELECT, UNION, and OPTIONAL, and for DLs with the\ncanonical model property. We also investigate combined complexity for query\nanswering under such a semantics over DL-Lite R KBs. In particular, we show for\ndifferent fragments of SPARQL that known upper-bounds for query answering over\na plain graph are matched.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 23:05:08 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 16:20:03 GMT"}, {"version": "v3", "created": "Thu, 21 Nov 2019 13:06:28 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Corman", "Julien", ""], ["Xiao", "Guohui", ""]]}, {"id": "1911.02788", "submitter": "Yang Li", "authors": "Yang Li, Gang Liu, Junbin Gao, Zhenwen He, Mingyuan Bai and Chengjun\n  Li", "title": "Efficient Spatial Nearest Neighbor Queries Based on Multi-layer Voronoi\n  Diagrams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nearest neighbor (NN) problem is an important scientific problem. The NN\nquery, to find the closest one to a given query point among a set of points, is\nwidely used in applications such as density estimation, pattern classification,\ninformation retrieval and spatial analysis. A direct generalization of the NN\nquery is the k nearest neighbors (kNN) query, where the k closest point are\nrequired to be found. Since NN and kNN problems were raised, many algorithms\nhave been proposed to solve them. It has been indicated in literature that the\nonly method to solve these problems exactly with sublinear time complexity, is\nto filter out the unnecessary spatial computation by using the pre-processing\nstructure, commonly referred to as the spatial index. The recently proposed\nspatial indices available for NN search, are almost constructed through spatial\npartition. These indices are tree-like, and the tree-like hierarchical\nstructure can usually significantly improve the efficiency of NN search.\nHowever, when the data are distributed extremely unevenly, it is difficult to\nsatisfy both the balance of the tree and the non-overlap of the subspace\ncorresponding to the nodes. Thus the acceleration performance of the tree-like\nindices is severely jeopardized. In this paper, we propose a non-tree spatial\nindex which consists of multiple layers of Voronoi diagrams (MVD). This index\ncan entirely avoid the dilemma tree-like structures face, and solve the NN\nproblems stably with logarithmic time complexity. Furthermore, it is convenient\nto achieve kNN search by extending NN search on MVD. In the experiments, we\nevaluate the efficiency of this indexing for both NN search and kNN search by\ncomparing with VoR-tree, R-tree and kd-tree. The experiments indicate that\ncompared to NN search and kNN search with the other three indices, these two\nsearch methods have significantly higher efficiency with MVD.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 07:57:38 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Li", "Yang", ""], ["Liu", "Gang", ""], ["Gao", "Junbin", ""], ["He", "Zhenwen", ""], ["Bai", "Mingyuan", ""], ["Li", "Chengjun", ""]]}, {"id": "1911.03291", "submitter": "Loick Bonniot", "authors": "Lo\\\"ick Bonniot (WIDE), Christoph Neumann, Fran\\c{c}ois Ta\\\"iani\n  (WIDE)", "title": "PnyxDB: a Lightweight Leaderless Democratic Byzantine Fault Tolerant\n  Replicated Datastore", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Byzantine-Fault-Tolerant (BFT) systems are rapidly emerging as a viable\ntechnology for production-grade systems, notably in closed consortia\ndeployments for nancial and supply-chain applications. Unfortunately, most\nalgorithms proposed so far to coordinate these systems suffer from substantial\nscalability issues, and lack important features to implement Internet-scale\ngovernance mechanisms. In this paper, we observe that many application\nworkloads offer little concurrency, and propose PnyxDB, an\neventually-consistent Byzantine Fault Tolerant replicated datastore that\nexhibits both high scalability and low latency. Our approach is based on\nconditional endorsements, that allow nodes to specify the set of transactions\nthat must not be committed for the endorsement to be valid. In addition to its\nhigh scalability, PnyxDB supports application-level voting, i.e. individual\nnodes are able to endorse or reject a transaction according to\napplication-defined policies without compromising consistency. We provide a\ncomparison against BFTSMaRt and Tendermint, two competitors with different\ndesign aims, and show that our implementation speeds up commit latencies by a\nfactor of 11, remaining below 5 seconds in a worldwide geodistributed\ndeployment of 180 nodes.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 14:43:26 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Bonniot", "Lo\u00efck", "", "WIDE"], ["Neumann", "Christoph", "", "WIDE"], ["Ta\u00efani", "Fran\u00e7ois", "", "WIDE"]]}, {"id": "1911.03623", "submitter": "Kommy Weldemariam Dr", "authors": "Reginald Bryant, Celia Cintas, Isaac Wambugu, Andrew Kinai, Komminist\n  Weldemariam", "title": "Analyzing Bias in Sensitive Personal Information Used to Train Financial\n  Models", "comments": "5 pages, 4 figures, IEEE Global Conference on Signal and Information\n  Processing (GlobalSIP 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Bias in data can have unintended consequences that propagate to the design,\ndevelopment, and deployment of machine learning models. In the financial\nservices sector, this can result in discrimination from certain financial\ninstruments and services. At the same time, data privacy is of paramount\nimportance, and recent data breaches have seen reputational damage for large\ninstitutions. Presented in this paper is a trusted model-lifecycle management\nplatform that attempts to ensure consumer data protection, anonymization, and\nfairness. Specifically, we examine how datasets can be reproduced using deep\nlearning techniques to effectively retain important statistical features in\ndatasets whilst simultaneously protecting data privacy and enabling safe and\nsecure sharing of sensitive personal information beyond the current\nstate-of-practice.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 06:43:21 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Bryant", "Reginald", ""], ["Cintas", "Celia", ""], ["Wambugu", "Isaac", ""], ["Kinai", "Andrew", ""], ["Weldemariam", "Komminist", ""]]}, {"id": "1911.03679", "submitter": "Kevin Kappelmann", "authors": "Kevin Kappelmann", "title": "Decision Procedures for Guarded Logics", "comments": "A thesis submitted in partial fulfilment for the degree of MSc in\n  Mathematics and Foundations of Computer Science at the University of Oxford.\n  Update 25.03.2021: added missing constraint in definition of simple\n  saturation algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important class of decidable first-order logic fragments are those\nsatisfying a guardedness condition, such as the guarded fragment (GF). Usually,\ndecidability for these logics is closely linked to the tree-like model property\n- the fact that satisfying models can be taken to have tree-like form. Decision\nprocedures for the guarded fragment based on the tree-like model property are\ndifficult to implement. An alternative approach, based on restricting\nfirst-order resolution, has been proposed, and this shows more promise from the\npoint of view of implementation. In this work, we connect the tree-like model\nproperty of the guarded fragment with the resolution-based approach. We derive\nefficient resolution-based rewriting algorithms that solve the Quantifier-Free\nQuery Answering Problem under Guarded Tuple Generating Dependencies (GTGDs) and\nDisjunctive Guarded Tuple Generating Dependencies (DisGTGDs). The Query\nAnswering Problem for these classes subsumes many cases of GF satisfiability.\nOur algorithms, in addition to making the connection to the tree-like model\nproperty clear, give a natural account of the selection and ordering strategies\nused by resolution procedures for the guarded fragment. We also believe that\nour rewriting algorithm for the special case of GTGDs may prove itself valuable\nin practice as it does not require any Skolemisation step and its theoretical\nruntime outperforms those of known GF resolution procedures in case of fixed\ndependencies. Moreover, we show a novel normalisation procedure for the widely\nused chase procedure in case of (disjunctive) GTGDs, which could be useful for\nfuture studies.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 12:44:21 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 13:50:53 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Kappelmann", "Kevin", ""]]}, {"id": "1911.04078", "submitter": "Kai Li", "authors": "Kai Li, Yuzhe Tang, Jiaqi Chen, Zhehu Yuan, Cheng Xu, Jianliang Xu", "title": "Cost-Effective Data Feeds to Blockchains via Workload-Adaptive Data\n  Replication", "comments": "Blockchain storage replication, Data feed, GRuB, 20 pages, Middleware\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feeding external data to a blockchain, a.k.a. data feed, is an essential task\nto enable blockchain interoperability and support emerging cross-domain\napplications, notably stablecoins. Given the data-intensive feeds in real life\n(e.g., high-frequency price updates) and the high cost in using blockchain,\nnamely Gas, it is imperative to reduce the Gas cost of data feeds. Motivated by\nthe constant-changing workloads in finance and other applications, this work\nfocuses on designing a dynamic, workload-aware approach for cost effectiveness\nin Gas. This design space is understudied in the existing blockchain research\nwhich has so far focused on static data placement.\n  This work presents GRuB, a cost-effective data feed that dynamically\nreplicates data between the blockchain and an off-chain cloud storage. GRuB's\ndata replication is workload-adaptive by monitoring the current workload and\nmaking online decisions w.r.t. data replication. A series of online algorithms\nare proposed that achieve the bounded worst-case cost in blockchain's Gas. GRuB\nruns the decision-making components on the untrusted cloud off-chain for lower\nGas costs, and employs a security protocol to authenticate the data transferred\nbetween the blockchain and cloud. The overall GRuB system can autonomously\nachieve low Gas costs with changing workloads.\n  We built a GRuB prototype functional with Ethereum and Google LevelDB, and\nsupported real applications in stablecoins. Under real workloads collected from\nthe Ethereum contract-call history and mixed workloads of YCSB, we\nsystematically evaluate GRuB's cost which shows a saving of Gas by 10% ~ 74%,\nwith comparison to the baselines of static data-placement.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 05:08:41 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2019 02:03:56 GMT"}, {"version": "v3", "created": "Mon, 14 Sep 2020 23:10:55 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Li", "Kai", ""], ["Tang", "Yuzhe", ""], ["Chen", "Jiaqi", ""], ["Yuan", "Zhehu", ""], ["Xu", "Cheng", ""], ["Xu", "Jianliang", ""]]}, {"id": "1911.04226", "submitter": "Takao Murakami", "authors": "Takao Murakami, Koki Hamada, Yusuke Kawamoto, Takuma Hatano", "title": "Privacy-Preserving Multiple Tensor Factorization for Synthesizing\n  Large-Scale Location Traces with Cluster-Specific Features", "comments": "This is a full version of the paper accepted at PETS 2021 (The 21st\n  Privacy Enhancing Technologies Symposium)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the widespread use of LBSs (Location-based Services), synthesizing\nlocation traces plays an increasingly important role in analyzing spatial big\ndata while protecting user privacy. In particular, a synthetic trace that\npreserves a feature specific to a cluster of users (e.g., those who commute by\ntrain, those who go shopping) is important for various geo-data analysis tasks\nand for providing a synthetic location dataset. Although location synthesizers\nhave been widely studied, existing synthesizers do not provide sufficient\nutility, privacy, or scalability, hence are not practical for large-scale\nlocation traces. To overcome this issue, we propose a novel location\nsynthesizer called PPMTF (Privacy-Preserving Multiple Tensor Factorization). We\nmodel various statistical features of the original traces by a transition-count\ntensor and a visit-count tensor. We factorize these two tensors simultaneously\nvia multiple tensor factorization, and train factor matrices via posterior\nsampling. Then we synthesize traces from reconstructed tensors, and perform a\nplausible deniability test for a synthetic trace. We comprehensively evaluate\nPPMTF using two datasets. Our experimental results show that PPMTF preserves\nvarious statistical features including cluster-specific features, protects user\nprivacy, and synthesizes large-scale location traces in practical time. PPMTF\nalso significantly outperforms the state-of-the-art methods in terms of utility\nand scalability at the same level of privacy.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 13:08:30 GMT"}, {"version": "v2", "created": "Fri, 15 Nov 2019 13:43:03 GMT"}, {"version": "v3", "created": "Sat, 21 Dec 2019 05:48:29 GMT"}, {"version": "v4", "created": "Thu, 27 Feb 2020 19:27:56 GMT"}, {"version": "v5", "created": "Mon, 2 Mar 2020 23:49:57 GMT"}, {"version": "v6", "created": "Sun, 31 May 2020 18:49:35 GMT"}, {"version": "v7", "created": "Thu, 19 Nov 2020 01:02:55 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Murakami", "Takao", ""], ["Hamada", "Koki", ""], ["Kawamoto", "Yusuke", ""], ["Hatano", "Takuma", ""]]}, {"id": "1911.04948", "submitter": "Laurel Orr", "authors": "Laurel Orr, Magdalena Balazinska, and Dan Suciu", "title": "EntropyDB: A Probabilistic Approach to Approximate Query Processing", "comments": "arXiv admin note: text overlap with arXiv:1703.03856", "journal-ref": "VLDB Journal 2019", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present EntropyDB, an interactive data exploration system that uses a\nprobabilistic approach to generate a small, query-able summary of a dataset.\nDeparting from traditional summarization techniques, we use the Principle of\nMaximum Entropy to generate a probabilistic representation of the data that can\nbe used to give approximate query answers. We develop the theoretical framework\nand formulation of our probabilistic representation and show how to use it to\nanswer queries. We then present solving techniques, give two critical\noptimizations to improve preprocessing time and query execution time, and\nexplore methods to reduce query error. Lastly, we experimentally evaluate our\nwork using a 5 GB dataset of flights within the United States and a 210 GB\ndataset from an astronomy particle simulation. While our current work only\nsupports linear queries, we show that our technique can successfully answer\nqueries faster than sampling while introducing, on average, no more error than\nsampling and can better distinguish between rare and nonexistent values. We\nalso discuss extensions that can allow for data updates and linear queries over\njoins.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 23:15:13 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Orr", "Laurel", ""], ["Balazinska", "Magdalena", ""], ["Suciu", "Dan", ""]]}, {"id": "1911.05195", "submitter": "Vitaliy Tsyganok", "authors": "Vitaliy Tsyganok, Sergii Kadenko, Oleh Andriichuk", "title": "From Open Source Intelligence to Decision Making: a Hybrid Approach", "comments": "14 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide an overview of tools enabling users to utilize data from open\nsources for decision-making support in weakly-structured subject domains.\nPresently, it is impossible to replace expert data with data from open sources\nin the process of decision-making. Although organization of expert sessions\nrequires much time and costs a lot, due to insufficient level of natural\nlanguage processing technology development, we still have to engage experts and\nknowledge engineers in decision-making process. Information, obtained from\nexperts and open sources, is processed, aggregated, and used as basis of\nrecommendations, provided to decision-maker. As an example of a\nweakly-structured domain, we consider information conflicts and operations. For\nthis domain we propose a hybrid decision support methodology, using data\nprovided by both experts and open sources. The methodology is based on\nhierarchic decomposition of the main goal of an information operation. Using\nthe data obtained from experts and open sources, we build the knowledge base of\nsubject domain in the form of a weighted graph. It represents a hierarchy of\nfactors influencing the main goal. Besides intensity, the impact of each factor\nis characterized by delay and duration. With these parameters taken into\naccount, main goal achievement degree is calculated, and changes of target\nparameters of information operation object are monitored. In order to\nillustrate the suggested hybrid approach, we consider a real-life example,\nwhere we detect, monitor, and analyze actions intended to discredit the\nNational academy of sciences of Ukraine. For this purpose, we use specialized\ndecision-making support and content monitoring software.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 20:08:11 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Tsyganok", "Vitaliy", ""], ["Kadenko", "Sergii", ""], ["Andriichuk", "Oleh", ""]]}, {"id": "1911.05582", "submitter": "Nikolaos Tziavelis", "authors": "Nikolaos Tziavelis, Deepak Ajwani, Wolfgang Gatterbauer, Mirek\n  Riedewald, Xiaofeng Yang", "title": "Optimal Algorithms for Ranked Enumeration of Answers to Full Conjunctive\n  Queries", "comments": "50 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study ranked enumeration of join-query results according to very general\norders defined by selective dioids. Our main contribution is a framework for\nranked enumeration over a class of dynamic programming problems that\ngeneralizes seemingly different problems that had been studied in isolation. To\nthis end, we extend classic algorithms that find the k-shortest paths in a\nweighted graph. For full conjunctive queries, including cyclic ones, our\napproach is optimal in terms of the time to return the top result and the delay\nbetween results. These optimality properties are derived for the widely used\nnotion of data complexity, which treats query size as a constant. By performing\na careful cost analysis, we are able to uncover a previously unknown tradeoff\nbetween two incomparable enumeration approaches: one has lower complexity when\nthe number of returned results is small, the other when the number is very\nlarge. We theoretically and empirically demonstrate the superiority of our\ntechniques over batch algorithms, which produce the full result and then sort\nit. Our technique is not only faster for returning the first few results, but\non some inputs beats the batch algorithm even when all results are produced.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 16:21:36 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 17:49:09 GMT"}, {"version": "v3", "created": "Fri, 11 Sep 2020 19:21:22 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Tziavelis", "Nikolaos", ""], ["Ajwani", "Deepak", ""], ["Gatterbauer", "Wolfgang", ""], ["Riedewald", "Mirek", ""], ["Yang", "Xiaofeng", ""]]}, {"id": "1911.05900", "submitter": "Yasunori Ishihara", "authors": "Soichiro Hidaka, Yasunori Ishihara, Zachary G. Ives", "title": "Proceedings of the Third Workshop on Software Foundations for Data\n  Interoperability (SFDI2019+), October 28, 2019, Fukuoka, Japan", "comments": "Proceedings of the Third Workshop on Software Foundations for Data\n  Interoperability (SFDI2019+), October 28, 2019, Fukuoka, Japan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the papers presented at the Third Workshop on Software\nFoundations for Data Interoperability (SFDI2019+) held on October 28, 2019, in\nFukuoka, co-located with the 11th Asia-Pasific Symposium on Internetware\n(Internetware 2019). One regular paper and six short papers have been accepted\nfor presentation, each of which has its unique ideas and/or interesting results\non interoperability of autonomic distributed data. Moreover, SFDI2019+ featured\ntwo keynote talks by Hiroyuki Seki (Nagoya University) and Zinovy Diskin\n(McMaster University), which introduce concepts and directions novel to the\nseries of SFDI workshops so far.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 02:32:16 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Hidaka", "Soichiro", ""], ["Ishihara", "Yasunori", ""], ["Ives", "Zachary G.", ""]]}, {"id": "1911.05921", "submitter": "Van-Dang Tran", "authors": "Van-Dang Tran, Hiroyuki Kato, Zhenjiang Hu", "title": "Programmable View Update Strategies on Relations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  View update is an important mechanism that allows updates on a view by\ntranslating them into the corresponding updates on the base relations. The\nexisting literature has shown the ambiguity of translating view updates. To\naddress this ambiguity, we propose a robust language-based approach for making\nview update strategies programmable and validatable. Specifically, we introduce\na novel approach to use Datalog to describe these update strategies. We propose\na validation algorithm to check the well-behavedness of the written Datalog\nprograms. We present a fragment of the Datalog language for which our\nvalidation is both sound and complete. This fragment not only has good\nproperties in theory but is also useful for solving practical view updates.\nFurthermore, we develop an algorithm for optimizing user-written programs to\nefficiently implement updatable views in relational database management\nsystems. We have implemented our proposed approach. The experimental results\nshow that our framework is feasible and efficient in practice.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 03:40:32 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2020 04:08:05 GMT"}, {"version": "v3", "created": "Mon, 31 Aug 2020 16:07:10 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Tran", "Van-Dang", ""], ["Kato", "Hiroyuki", ""], ["Hu", "Zhenjiang", ""]]}, {"id": "1911.06217", "submitter": "Tobias Skovgaard Jepsen", "authors": "Tobias Skovgaard Jepsen, Christian S. Jensen, Thomas Dyhre Nielsen", "title": "On Network Embedding for Machine Learning on Road Networks: A Case Study\n  on the Danish Road Network", "comments": "Best Paper at the 3rd IEEE International Workshop on Big Spatial Data\n  (BSD 2018)", "journal-ref": "2018 IEEE International Conference on Big Data (Big Data), 2018,\n  pp. 3422-3431", "doi": "10.1109/BigData.2018.8622416", "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Road networks are a type of spatial network, where edges may be associated\nwith qualitative information such as road type and speed limit. Unfortunately,\nsuch information is often incomplete; for instance, OpenStreetMap only has\nspeed limits for 13% of all Danish road segments. This is problematic for\nanalysis tasks that rely on such information for machine learning. To enable\nmachine learning in such circumstances, one may consider the application of\nnetwork embedding methods to extract structural information from the network.\nHowever, these methods have so far mostly been used in the context of social\nnetworks, which differ significantly from road networks in terms of, e.g., node\ndegree and level of homophily (which are key to the performance of many network\nembedding methods). We analyze the use of network embedding methods,\nspecifically node2vec, for learning road segment embeddings in road networks.\nDue to the often limited availability of information on other relevant road\ncharacteristics, the analysis focuses on leveraging the spatial network\nstructure. Our results suggest that network embedding methods can indeed be\nused for deriving relevant network features (that may, e.g, be used for\npredicting speed limits), but that the qualities of the embeddings differ from\nembeddings for social networks.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 16:18:36 GMT"}, {"version": "v2", "created": "Fri, 15 Nov 2019 10:51:56 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Jepsen", "Tobias Skovgaard", ""], ["Jensen", "Christian S.", ""], ["Nielsen", "Thomas Dyhre", ""]]}, {"id": "1911.06311", "submitter": "\\c{C}a\\u{g}atay Demiralp", "authors": "Dan Zhang and Yoshihiko Suhara and Jinfeng Li and Madelon Hulsebos and\n  \\c{C}a\\u{g}atay Demiralp and Wang-Chiew Tan", "title": "Sato: Contextual Semantic Type Detection in Tables", "comments": "VLDB'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting the semantic types of data columns in relational tables is\nimportant for various data preparation and information retrieval tasks such as\ndata cleaning, schema matching, data discovery, and semantic search. However,\nexisting detection approaches either perform poorly with dirty data, support\nonly a limited number of semantic types, fail to incorporate the table context\nof columns or rely on large sample sizes for training data. We introduce Sato,\na hybrid machine learning model to automatically detect the semantic types of\ncolumns in tables, exploiting the signals from the context as well as the\ncolumn values. Sato combines a deep learning model trained on a large-scale\ntable corpus with topic modeling and structured prediction to achieve\nsupport-weighted and macro average F1 scores of 0.925 and 0.735, respectively,\nexceeding the state-of-the-art performance by a significant margin. We\nextensively analyze the overall and per-type performance of Sato, discussing\nhow individual modeling components, as well as feature categories, contribute\nto its performance.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 18:51:59 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 03:47:14 GMT"}, {"version": "v3", "created": "Wed, 3 Jun 2020 04:54:28 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Zhang", "Dan", ""], ["Suhara", "Yoshihiko", ""], ["Li", "Jinfeng", ""], ["Hulsebos", "Madelon", ""], ["Demiralp", "\u00c7a\u011fatay", ""], ["Tan", "Wang-Chiew", ""]]}, {"id": "1911.06577", "submitter": "Maximilian Schleich", "authors": "Maximilian Schleich, Dan Olteanu, Mahmoud Abo-Khamis, Hung Q. Ngo,\n  XuanLong Nguyen", "title": "Learning Models over Relational Data: A Brief Tutorial", "comments": "10 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This tutorial overviews the state of the art in learning models over\nrelational databases and makes the case for a first-principles approach that\nexploits recent developments in database research.\n  The input to learning classification and regression models is a training\ndataset defined by feature extraction queries over relational databases. The\nmainstream approach to learning over relational data is to materialize the\ntraining dataset, export it out of the database, and then learn over it using a\nstatistical package. This approach can be expensive as it requires the\nmaterialization of the training dataset. An alternative approach is to cast the\nmachine learning problem as a database problem by transforming the\ndata-intensive component of the learning task into a batch of aggregates over\nthe feature extraction query and by computing this batch directly over the\ninput database.\n  The tutorial highlights a variety of techniques developed by the database\ntheory and systems communities to improve the performance of the learning task.\nThey rely on structural properties of the relational data and of the feature\nextraction query, including algebraic (semi-ring), combinatorial (hypertree\nwidth), statistical (sampling), or geometric (distance) structure. They also\nrely on factorized computation, code specialization, query compilation, and\nparallelization.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 11:50:00 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Schleich", "Maximilian", ""], ["Olteanu", "Dan", ""], ["Abo-Khamis", "Mahmoud", ""], ["Ngo", "Hung Q.", ""], ["Nguyen", "XuanLong", ""]]}, {"id": "1911.06980", "submitter": "Dongeun Lee", "authors": "Dongeun Lee, Alex Sim, Jaesik Choi, Kesheng Wu", "title": "IDEALEM: Statistical Similarity Based Data Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications such as scientific simulation, sensing, and power grid\nmonitoring tend to generate massive amounts of data, which should be compressed\nfirst prior to storage and transmission. These data, mostly comprised of\nfloating-point values, are known to be difficult to compress using lossless\ncompression. A few compression methods based on lossy compression have been\nproposed to compress this seemingly incompressible data. Unfortunately, they\nare all designed to minimize the Euclidean distance between the original data\nand the decompressed data, which fundamentally limits compression performance.\nWe recently proposed a new class of lossy compression based on statistical\nsimilarity, called IDEALEM, which was also provided as a software package.\nIDEALEM has demonstrated its performance by reducing data volume much more than\nstate-of-the-art compression methods while preserving unique patterns of data.\nIDEALEM can operate in two different modes depending on the stationarity of\ninput data. This paper presents compression performance analyses of these two\nmodes, and investigates the difference between two transform techniques\ntargeted for non-stationary data. This paper also discusses the data\nreconstruction quality of IDEALEM using spectral analysis and shows that\nimportant frequency components in application domain are well preserved. We\nexpand the capability of IDEALEM by adding a new min/max check that facilitates\npreserving significant patterns lasting only for a brief duration which were\npreviously hard to capture. This min/max check also accelerates the encoding\nprocess significantly. Experiments show IDEALEM preserves significant patterns\nin the original data with faster encoding time.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 07:24:13 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Lee", "Dongeun", ""], ["Sim", "Alex", ""], ["Choi", "Jaesik", ""], ["Wu", "Kesheng", ""]]}, {"id": "1911.07151", "submitter": "Siddharth Dawar", "authors": "Siddharth Dawar, Vikram Goyal, Debajyoti Bera", "title": "A one-phase tree-based algorithm for mining high-utility itemsets from a\n  transaction database", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-utility itemset mining finds itemsets from a transaction database with\nutility no less than a fixed user-defined threshold. The utility of an itemset\nis defined as the sum of the utilities of its item. Several algorithms were\nproposed to mine high-utility itemsets. However, no state-of-the-art algorithm\nperforms consistently good across dense and sparse datasets. In this paper, we\npropose a novel data structure called Utility-Tree, and a tree-based algorithm\ncalled UT-Miner that mines high-utility itemsets in one-phase only without\ngenerating any candidates and uses a lightweight construction method to reduce\nthe cost of creating projected databases during the search space exploration.\nThe transaction information is stored compactly with every node of the\nUtility-Tree, and the information is computed efficiently during the recursive\ninvocation of the algorithm. Experimental results on several real-life dense\nand sparse datasets reveal that UT-Miner is among the top-performing efficient\nalgorithms across different datasets.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 04:47:16 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Dawar", "Siddharth", ""], ["Goyal", "Vikram", ""], ["Bera", "Debajyoti", ""]]}, {"id": "1911.07225", "submitter": "Alexandr Savinov", "authors": "Alexandr Savinov", "title": "Concept-oriented model: Modeling and processing data using functions", "comments": "14 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new logical data model, called the concept-oriented model\n(COM). It uses mathematical functions as first-class constructs for data\nrepresentation and data processing as opposed to using exclusively sets in\nconventional set-oriented models. Functions and function composition are used\nas primary semantic units for describing data connectivity instead of relations\nand relation composition (join), respectively. Grouping and aggregation are\nalso performed by using (accumulate) functions providing an alternative to\ngroup-by and reduce operations. This model was implemented in an open source\ndata processing toolkit examples of which are used to illustrate the model and\nits operations. The main benefit of this model is that typical data processing\ntasks become simpler and more natural when using functions in comparison to\nadopting sets and set operations.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 12:38:47 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Savinov", "Alexandr", ""]]}, {"id": "1911.07585", "submitter": "Valentina Anita Carriero", "authors": "Valentina Anita Carriero and Aldo Gangemi and Maria Letizia Mancinelli\n  and Andrea Giovanni Nuzzolese and Valentina Presutti and Chiara Veninata", "title": "Pattern-based design applied to cultural heritage knowledge graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ontology Design Patterns (ODPs) have become an established and recognised\npractice for guaranteeing good quality ontology engineering. There are several\nODP repositories where ODPs are shared as well as ontology design methodologies\nrecommending their reuse. Performing rigorous testing is recommended as well\nfor supporting ontology maintenance and validating the resulting resource\nagainst its motivating requirements. Nevertheless, it is less than\nstraightforward to find guidelines on how to apply such methodologies for\ndeveloping domain-specific knowledge graphs. ArCo is the knowledge graph of\nItalian Cultural Heritage and has been developed by using eXtreme Design (XD),\nan ODP- and test-driven methodology. During its development, XD has been\nadapted to the need of the CH domain e.g. gathering requirements from an open,\ndiverse community of consumers, a new ODP has been defined and many have been\nspecialised to address specific CH requirements. This paper presents ArCo and\ndescribes how to apply XD to the development and validation of a CH knowledge\ngraph, also detailing the (intellectual) process implemented for matching the\nencountered modelling problems to ODPs. Relevant contributions also include a\nnovel web tool for supporting unit-testing of knowledge graphs, a rigorous\nevaluation of ArCo, and a discussion of methodological lessons learned during\nArCo development.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 12:33:33 GMT"}, {"version": "v2", "created": "Sat, 20 Jun 2020 08:51:59 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Carriero", "Valentina Anita", ""], ["Gangemi", "Aldo", ""], ["Mancinelli", "Maria Letizia", ""], ["Nuzzolese", "Andrea Giovanni", ""], ["Presutti", "Valentina", ""], ["Veninata", "Chiara", ""]]}, {"id": "1911.07673", "submitter": "Fabrizio Orlandi", "authors": "Ademar Crotti Junior and Fabrizio Orlandi and Declan O'Sullivan and\n  Christian Dirschl and Quentin Reul", "title": "Using Mapping Languages for Building Legal Knowledge Graphs from XML\n  Files", "comments": "Presented at the 2nd International Contextualized Knowledge Graphs\n  Workshop (CKG'19) at the 18th International Semantic Web Conference (ISWC)\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents our experience on building RDF knowledge graphs for an\nindustrial use case in the legal domain. The information contained in legal\ninformation systems are often accessed through simple keyword interfaces and\npresented as a simple list of hits. In order to improve search accuracy one may\navail of knowledge graphs, where the semantics of the data can be made\nexplicit. Significant research effort has been invested in the area of building\nknowledge graphs from semi-structured text documents, such as XML, with the\nprevailing approach being the use of mapping languages. In this paper, we\npresent a semantic model for representing legal documents together with an\nindustrial use case. We also present a set of use case requirements based on\nthe proposed semantic model, which are used to compare and discuss the use of\nstate-of-the-art mapping languages for building knowledge graphs for legal\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 14:50:31 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Junior", "Ademar Crotti", ""], ["Orlandi", "Fabrizio", ""], ["O'Sullivan", "Declan", ""], ["Dirschl", "Christian", ""], ["Reul", "Quentin", ""]]}, {"id": "1911.08277", "submitter": "Bas R. J. Bolmer", "authors": "Bas R.J. Bolmer, Monique Taverne, Marco Scherer", "title": "Exploring the added value of blockchain technology for the healthcare\n  domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DB cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report, the University Medical Center Groningen (UMCG) has written\ndown lessons learned on how blockchain technology can have an impact on the\nhealthcare domain. By looking at two use-cases, the hospital challenged several\nteams, participating in an open innovation program and blockchain hackathon, to\nfind a solution that showed the added value of the technology for patient care\nand scientific research. Besides this practical perspective, the report also\nconsiders literature discussing the current state of blockchain technology in\nregard to developments in the healthcare domain (touching on patient\nempowerment, data management, regulations, and interoperability between\nhealthcare systems).\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 17:00:00 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Bolmer", "Bas R. J.", ""], ["Taverne", "Monique", ""], ["Scherer", "Marco", ""]]}, {"id": "1911.08376", "submitter": "Guillermo de Bernardo", "authors": "Nieves R. Brisaboa, Ana Cerdeira-Pena, Guillermo de Bernardo, Gonzalo\n  Navarro, Oscar Pedreira", "title": "Extending General Compact Querieable Representations to GIS Applications", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sklodowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941,", "journal-ref": "Information Sciences 2020", "doi": "10.1016/j.ins.2019.08.007", "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The raster model is commonly used for the representation of images in many\ndomains, and is especially useful in Geographic Information Systems (GIS) to\nstore information about continuous variables of the space (elevation,\ntemperature, etc.). Current representations of raster data are usually designed\nfor external memory or, when stored in main memory, lack efficient query\ncapabilities. In this paper we propose compact representations to efficiently\nstore and query raster datasets in main memory. We present different\nrepresentations for binary raster data, general raster data and time-evolving\nraster data. We experimentally compare our proposals with traditional storage\nmechanisms such as linear quadtrees or compressed GeoTIFF files. Results show\nthat our structures are up to 10 times smaller than classical linear quadtrees,\nand even comparable in space to non-querieable representations of raster data,\nwhile efficiently answering a number of typical queries.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 16:18:15 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Brisaboa", "Nieves R.", ""], ["Cerdeira-Pena", "Ana", ""], ["de Bernardo", "Guillermo", ""], ["Navarro", "Gonzalo", ""], ["Pedreira", "Oscar", ""]]}, {"id": "1911.08614", "submitter": "Casey Cole", "authors": "Casey A Cole, Christopher Ott, Diego Valdes, Homayoun Valafar", "title": "PDBMine: A Reformulation of the Protein Data Bank to Facilitate\n  Structural Data Mining", "comments": "6 pages, 8 figures, IEEE Annual Conf. on Computational Science &\n  Computational Intelligence (CSCI), December 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale initiatives such as the Human Genome Project, Structural\nGenomics, and individual research teams have provided large deposits of genomic\nand proteomic data. The transfer of data to knowledge has become one of the\nexisting challenges, which is a consequence of capturing data in databases that\nare optimally designed for archiving and not mining. In this research, we have\ntargeted the Protein Databank (PDB) and demonstrated a transformation of its\ncontent, named PDBMine, that reduces storage space by an order of magnitude,\nand allows for powerful mining in relation to the topic of protein structure\ndetermination. We have demonstrated the utility of PDBMine in exploring the\nprevalence of dimeric and trimeric amino acid sequences and provided a\nmechanism of predicting protein structure.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 22:25:25 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Cole", "Casey A", ""], ["Ott", "Christopher", ""], ["Valdes", "Diego", ""], ["Valafar", "Homayoun", ""]]}, {"id": "1911.09016", "submitter": "Suela Isaj", "authors": "Suela Isaj, Torben Bach Pedersen, Esteban Zim\\'anyi", "title": "Multi-Source Spatial Entity Linkage", "comments": null, "journal-ref": null, "doi": "10.1109/TKDE.2020.2990491", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Besides the traditional cartographic data sources, spatial information can\nalso be derived from location-based sources. However, even though different\nlocation-based sources refer to the same physical world, each one has only\npartial coverage of the spatial entities, describe them with different\nattributes, and sometimes provide contradicting information. Hence, we\nintroduce the spatial entity linkage problem, which finds which pairs of\nspatial entities belong to the same physical spatial entity. Our proposed\nsolution (QuadSky) starts with a time-efficient spatial blocking technique\n(QuadFlex), compares pairwise the spatial entities in the same block, ranks the\npairs using Pareto optimality with the SkyRank algorithm, and finally,\nclassifies the pairs with our novel SkyEx-* family of algorithms that yield\n0.85 precision and 0.85 recall for a manually labeled dataset of 1,500 pairs\nand 0.87 precision and 0.6 recall for a semi-manually labeled dataset of\n777,452 pairs. Moreover, we provide a theoretical guarantee and formalize the\nSkyEx-FES algorithm that explores only 27% of the skylines without any loss in\nF-measure. Furthermore, our fully unsupervised algorithm SkyEx-D approximates\nthe optimal result with an F-measure loss of just 0.01. Finally, QuadSky\nprovides the best trade-off between precision and recall, and the best\nF-measure compared to the existing baselines and clustering techniques, and\napproximates the results of supervised learning solutions.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 16:38:05 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 12:38:11 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Isaj", "Suela", ""], ["Pedersen", "Torben Bach", ""], ["Zim\u00e1nyi", "Esteban", ""]]}, {"id": "1911.09208", "submitter": "Suyash Gupta", "authors": "Suyash Gupta, Sajjad Rahnama, Mohammad Sadoghi", "title": "Permissioned Blockchain Through the Looking Glass: Architectural and\n  Implementation Lessons Learned", "comments": "To appear in the proceedings of 40th IEEE International Conference on\n  Distributed Computing Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the inception of Bitcoin, the distributed systems community has shown\ninterest in the design of efficient blockchain systems. However, initial\nblockchain applications (like Bitcoin) attain very low throughput, which has\npromoted the design of permissioned blockchain systems. These permissioned\nblockchain systems employ classical Byzantine-Fault Tolerant (BFT) protocols to\nreach consensus. However, existing permissioned blockchain systems still attain\nlow throughputs (of the order 10K txns/s). As a result, existing works blame\nthis low throughput on the associated BFT protocol and expend resources in\ndeveloping optimized protocols. We believe such blames only depict a one-sided\nstory. In specific, we raise a simple question: can a well-crafted system based\non a classical BFT protocol outperform a modern protocol? We show that\ndesigning such a well-crafted system is possible and illustrate that even if\nsuch a system employs a three-phase protocol, it can outperform another system\nutilizing a single-phase protocol. This endeavor requires us to dissect a\npermissioned blockchain system and highlight different factors that affect its\nperformance. Based on our insights, we present the design of our\nenterprise-grade, high-throughput yielding permissioned blockchain system,\nResilientDB, that employs multi-threaded deep pipelines, to balance tasks at a\nreplica, and provides guidelines for future designs.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 23:07:53 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 04:42:18 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Gupta", "Suyash", ""], ["Rahnama", "Sajjad", ""], ["Sadoghi", "Mohammad", ""]]}, {"id": "1911.09356", "submitter": "Cristina Cornelio PhD", "authors": "Mustafa Canim, Cristina Cornelio, Arun Iyengar, Ryan Musa, Mariano\n  Rodrigez Muro", "title": "Schemaless Queries over Document Tables with Dependencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unstructured enterprise data such as reports, manuals and guidelines often\ncontain tables. The traditional way of integrating data from these tables is\nthrough a two-step process of table detection/extraction and mapping the table\nlayouts to an appropriate schema. This can be an expensive process. In this\npaper we show that by using semantic technologies (RDF/SPARQL and database\ndependencies) paired with a simple but powerful way to transform tables with\nnon-relational layouts, it is possible to offer query answering services over\nthese tables with minimal manual work or domain-specific mappings. Our method\nenables users to exploit data in tables embedded in documents with little\neffort, not only for simple retrieval queries, but also for structured queries\nthat require joining multiple interrelated tables.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 09:20:24 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Canim", "Mustafa", ""], ["Cornelio", "Cristina", ""], ["Iyengar", "Arun", ""], ["Musa", "Ryan", ""], ["Muro", "Mariano Rodrigez", ""]]}, {"id": "1911.09373", "submitter": "Zeyi Wen", "authors": "Zeyi Wen, Zeyu Huang and Rui Zhang", "title": "Entity Extraction with Knowledge from Web Scale Corpora", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity extraction is an important task in text mining and natural language\nprocessing. A popular method for entity extraction is by comparing substrings\nfrom free text against a dictionary of entities. In this paper, we present\nseveral techniques as a post-processing step for improving the effectiveness of\nthe existing entity extraction technique. These techniques utilise models\ntrained with the web-scale corpora which makes our techniques robust and\nversatile. Experiments show that our techniques bring a notable improvement on\nefficiency and effectiveness.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 10:01:16 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Wen", "Zeyi", ""], ["Huang", "Zeyu", ""], ["Zhang", "Rui", ""]]}, {"id": "1911.10418", "submitter": "Yue Zhao", "authors": "Yue Zhao and Maciej K. Hryniewicki", "title": "DCSO: Dynamic Combination of Detector Scores for Outlier Ensembles", "comments": "ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD),\n  Outlier Detection De-constructed Workshop, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selecting and combining the outlier scores of different base detectors used\nwithin outlier ensembles can be quite challenging in the absence of ground\ntruth. In this paper, an unsupervised outlier detector combination framework\ncalled DCSO is proposed, demonstrated and assessed for the dynamic selection of\nmost competent base detectors, with an emphasis on data locality. The proposed\nDCSO framework first defines the local region of a test instance by its k\nnearest neighbors and then identifies the top-performing base detectors within\nthe local region. Experimental results on ten benchmark datasets demonstrate\nthat DCSO provides consistent performance improvement over existing static\ncombination approaches in mining outlying objects. To facilitate\ninterpretability and reliability of the proposed method, DCSO is analyzed using\nboth theoretical frameworks and visualization techniques, and presented\nalongside empirical parameter setting instructions that can be used to improve\nthe overall performance.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 21:16:00 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Zhao", "Yue", ""], ["Hryniewicki", "Maciej K.", ""]]}, {"id": "1911.11181", "submitter": "Mansaf Alam Dr", "authors": "Samiya Khan, Xiufeng Liu, Syed Arshad Ali, Mansaf Alam", "title": "Bivariate, Cluster and Suitability Analysis of NoSQL Solutions for\n  Different Application Areas", "comments": "arXiv admin note: substantial text overlap with arXiv:1904.11498", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data systems development is full of challenges in view of the variety of\napplication areas and domains that this technology promises to serve.\nTypically, fundamental design decisions involved in big data systems design\ninclude choosing appropriate storage and computing infrastructures. In this age\nof heterogeneous systems that integrate different technologies for development\nof an optimized solution to a specific real world problem, big data systems are\nnot an exception to any such rule. As far as the storage aspect of any big data\nsystem is concerned, the primary facet in this regard is a storage\ninfrastructure and NoSQL is the right technology that fulfills its\nrequirements. However, every big data application has variable data\ncharacteristics and thus, the corresponding data fits into a different data\nmodel. Moreover, the requirements of different applications vary on the basis\nof budget and functionality. This paper presents a feature analysis of 80 NoSQL\nsolutions, elaborating on the criteria and points that a developer must\nconsider while making a possible choice. Bivariate analysis of dataset created\nfor the identified NoSQL solutions was performed to establish relationship\nbetween 9 features. Furthermore, cluster analysis of the dataset was used to\ncreate categories of solutions to present a statistically supported\nclassification scheme. Finally, applications for different solutions were\nreviewed and classified under domain-specific categories. Random forest\nclassification was used to determine the most relevant features for\napplications and correspondingly a decision tree-based prediction model was\nproposed, implemented and deployed in the form of a web application to\ndetermine the suitability of a NoSQL solution for an application area.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 07:16:04 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Khan", "Samiya", ""], ["Liu", "Xiufeng", ""], ["Ali", "Syed Arshad", ""], ["Alam", "Mansaf", ""]]}, {"id": "1911.11184", "submitter": "Parisa Ataei", "authors": "Parisa Ataei, Qiaoran Li, Eric Walkingshaw, Arash Termehchy", "title": "Managing Variability in Relational Databases by VDBMS", "comments": "15 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Variability inherently exists in databases in various contexts which creates\ndatabase variants. For example, variants of a database could have different\nschemas/content (database evolution problem), variants of a database could root\nfrom different sources (data integration problem), variants of a database could\nbe deployed differently for specific application domain (deploying a database\nfor different configurations of a software system), etc. Unfortunately, while\nthere are specific solutions to each of the problems arising in these contexts,\nthere is no general solution that accounts for variability in databases and\naddresses managing variability within a database. In this paper, we formally\ndefine variational databases (VDBs) and statically-typed variational relational\nalgebra (VRA) to query VDBs---both database and queries explicitly account for\nvariation. We also design and implement variational database management system\n(VDBMS) to run variational queries over a VDB effectively and efficiently. To\nassess this, we generate two VDBs from real-world databases in the context of\nsoftware development and database evolution with a set of experimental queries\nfor each.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 19:33:25 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Ataei", "Parisa", ""], ["Li", "Qiaoran", ""], ["Walkingshaw", "Eric", ""], ["Termehchy", "Arash", ""]]}, {"id": "1911.11212", "submitter": "Richard Dosselmann", "authors": "Richard Dosselmann, Mehdi Sadeqi, Howard J. Hamilton", "title": "A Tutorial on Computing $t$-Closeness", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a tutorial of the computation of $t$-closeness. An\nestablished model in the domain of privacy preserving data publishing,\n$t$-closeness is a measure of the earth mover's distance between two\ndistributions of an anonymized database table. This tutorial includes three\nexamples that showcase the full computation of $t$-closeness in terms of both\nnumerical and categorical attributes. Calculations are carried out using the\ndefinition of the earth mover's distance and weighted order distance. This\npaper includes detailed explanations and calculations not found elsewhere in\nthe literature. An efficient algorithm to calculate the $t$-closeness of a\ntable is also presented.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 20:24:24 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Dosselmann", "Richard", ""], ["Sadeqi", "Mehdi", ""], ["Hamilton", "Howard J.", ""]]}, {"id": "1911.11286", "submitter": "Mohammad Roohitavaf", "authors": "Mohammad Roohitavaf, Kun Ren, Gene Zhang, Sami Ben-romdhane", "title": "LogPlayer: Fault-tolerant Exactly-once Delivery using gRPC Asynchronous\n  Streaming", "comments": "12 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the design of our LogPlayer that is a component\nresponsible for fault-tolerant delivery of transactional mutations recorded on\na WAL to the backend storage shards. LogPlayer relies on gRPC for asynchronous\nstreaming. However, the design provided in this paper can be used with other\nasynchronous streaming platforms. We model check the correctness of LogPlayer\nby TLA+. In particular, our TLA+ specification shows that LogPlayer guarantees\nin-order exactly-once delivery of WAL entries to the storage shards, even in\nthe presence of shards or LogPlayer failures. Our experiments show LogPlayer is\ncapable of efficient delivery with sub-millisecond latency, and it is\nsignificantly more efficient than Apache Kafka for designing a WAL system with\nexactly-once guarantee.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 00:00:35 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Roohitavaf", "Mohammad", ""], ["Ren", "Kun", ""], ["Zhang", "Gene", ""], ["Ben-romdhane", "Sami", ""]]}, {"id": "1911.11359", "submitter": "Heng Zhang", "authors": "Heng Zhang, Yan Zhang, Jia-Huai You, Zhiyong Feng, Guifei Jiang", "title": "Towards Universal Languages for Tractable Ontology Mediated Query\n  Answering", "comments": "10 pages, 1 figure, the full version of a paper accepted for AAAI\n  2020. Some typos have been corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An ontology language for ontology mediated query answering (OMQA-language) is\nuniversal for a family of OMQA-languages if it is the most expressive one among\nthis family. In this paper, we focus on three families of tractable\nOMQA-languages, including first-order rewritable languages and languages whose\ndata complexity of the query answering is in AC0 or PTIME. On the negative\nside, we prove that there is, in general, no universal language for each of\nthese families of languages. On the positive side, we propose a novel property,\nthe locality, to approximate the first-order rewritability, and show that there\nexists a language of disjunctive embedded dependencies that is universal for\nthe family of OMQA-languages with locality. All of these results apply to OMQA\nwith query languages such as conjunctive queries, unions of conjunctive queries\nand acyclic conjunctive queries.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 06:07:20 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 12:37:48 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Zhang", "Heng", ""], ["Zhang", "Yan", ""], ["You", "Jia-Huai", ""], ["Feng", "Zhiyong", ""], ["Jiang", "Guifei", ""]]}, {"id": "1911.11387", "submitter": "Yidong Song", "authors": "Gang Wu, Yidong Song, Guodong Zhao, Wei Sun, Donghong Han, Baiyou\n  Qiao, Guoren Wang, Ye Yuan", "title": "Cracking In-Memory Database Index A Case Study for Adaptive Radix Tree\n  Index", "comments": "12 pages,15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Indexes provide a method to access data in databases quickly. It can improve\nthe response speed of subsequent queries by building a complete index in\nadvance. However, it also leads to a huge overhead of the continuous updating\nduring creating the index. An in-memory database usually has a higher query\nprocessing performance than disk databases and is more suitable for real-time\nquery processing. Therefore, there is an urgent need to reduce the index\ncreation and update cost for in-memory databases. Database cracking technology\nis currently recognized as an effective method to reduce the index\ninitialization time. However, conventional cracking algorithms are focused on\nsimple column data structure rather than those complex index structure for\nin-memory databases. In order to show the feasibility of in-memory database\nindex cracking and promote to future more extensive research, this paper\nconducted a case study on the Adaptive Radix Tree (ART), a popular tree index\nstructure of in-memory databases. On the basis of carefully examining the ART\nindex construction overhead, an algorithm using auxiliary data structures to\ncrack the ART index is proposed.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 08:00:29 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Wu", "Gang", ""], ["Song", "Yidong", ""], ["Zhao", "Guodong", ""], ["Sun", "Wei", ""], ["Han", "Donghong", ""], ["Qiao", "Baiyou", ""], ["Wang", "Guoren", ""], ["Yuan", "Ye", ""]]}, {"id": "1911.11543", "submitter": "Shruti Jadon", "authors": "Tanvi Sahay, Ankita Mehta, Shruti Jadon", "title": "Schema Matching using Machine Learning", "comments": "7 pages, 2 figures, 2 tables", "journal-ref": null, "doi": "10.1109/SPIN48934.2020.9071272", "report-no": null, "categories": "cs.DB cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Schema Matching is a method of finding attributes that are either similar to\neach other linguistically or represent the same information. In this project,\nwe take a hybrid approach at solving this problem by making use of both the\nprovided data and the schema name to perform one to one schema matching and\nintroduce the creation of a global dictionary to achieve one to many schema\nmatching. We experiment with two methods of one to one matching and compare\nboth based on their F-scores, precision, and recall. We also compare our method\nwith the ones previously suggested and highlight differences between them.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 02:40:09 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Sahay", "Tanvi", ""], ["Mehta", "Ankita", ""], ["Jadon", "Shruti", ""]]}, {"id": "1911.11689", "submitter": "Kurt Stockinger", "authors": "Jonas Heitz, Kurt Stockinger", "title": "Join Query Optimization with Deep Reinforcement Learning Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Join query optimization is a complex task and is central to the performance\nof query processing. In fact it belongs to the class of NP-hard problems.\nTraditional query optimizers use dynamic programming (DP) methods combined with\na set of rules and restrictions to avoid exhaustive enumeration of all possible\njoin orders. However, DP methods are very resource intensive. Moreover, given\nsimplifying assumptions of attribute independence, traditional query optimizers\nrely on erroneous cost estimations, which can lead to suboptimal query plans.\nRecent success of deep reinforcement learning (DRL) creates new opportunities\nfor the field of query optimization to tackle the above-mentioned problems. In\nthis paper, we present our DRL-based Fully Observed Optimizer (FOOP) which is a\ngeneric query optimization framework that enables plugging in different machine\nlearning algorithms. The main idea of FOOP is to use a data-adaptive learning\nquery optimizer that avoids exhaustive enumerations of join orders and is thus\nsignificantly faster than traditional approaches based on dynamic programming.\nIn particular, we evaluate various DRL-algorithms and show that Proximal Policy\nOptimization significantly outperforms Q-learning based algorithms. Finally we\ndemonstrate how ensemble learning techniques combined with DRL can further\nimprove the query optimizer.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 16:48:25 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Heitz", "Jonas", ""], ["Stockinger", "Kurt", ""]]}, {"id": "1911.11725", "submitter": "Nino Arsov", "authors": "Nino Arsov, Goran Velinov, Aleksandar S. Dimovski, Bojana Koteska,\n  Dragan Sahpaski, Margina Kon-Popovska", "title": "Prediction of Horizontal Data Partitioning Through Query Execution Cost\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The excessively increased volume of data in modern data management systems\ndemands an improved system performance, frequently provided by data\ndistribution, system scalability and performance optimization techniques.\nOptimized horizontal data partitioning has a significant influence of\ndistributed data management systems. An optimally partitioned schema found in\nthe early phase of logical database design without loading of real data in the\nsystem and its adaptation to changes of business environment are very important\nfor a successful implementation, system scalability and performance\nimprovement. In this paper we present a novel approach for finding an optimal\nhorizontally partitioned schema that manifests a minimal total execution cost\nof a given database workload. Our approach is based on a formal model that\nenables abstraction of the predicates in the workload queries, and are\nsubsequently used to define all relational fragments. This approach has\npredictive features acquired by simulation of horizontal partitioning, without\nloading any data into the partitions, but instead, altering the statistics in\nthe database catalogs. We define an optimization problem and employ a genetic\nalgorithm (GA) to find an approximately optimal horizontally partitioned\nschema. The solutions to the optimization problem are evaluated using\nPostgreSQL's query optimizer. The initial experimental evaluation of our\napproach confirms its efficiency and correctness, and the numbers imply that\nthe approach is effective in reducing the workload execution cost.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 18:02:58 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Arsov", "Nino", ""], ["Velinov", "Goran", ""], ["Dimovski", "Aleksandar S.", ""], ["Koteska", "Bojana", ""], ["Sahpaski", "Dragan", ""], ["Kon-Popovska", "Margina", ""]]}, {"id": "1911.11727", "submitter": "Matthew Perron", "authors": "Matthew Perron, Raul Castro Fernandez, David DeWitt, Samuel Madden", "title": "Starling: A Scalable Query Engine on Cloud Function Services", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much like on-premises systems, the natural choice for running database\nanalytics workloads in the cloud is to provision a cluster of nodes to run a\ndatabase instance. However, analytics workloads are often bursty or low volume,\nleaving clusters idle much of the time, meaning customers pay for compute\nresources even when unused. The ability of cloud function services, such as AWS\nLambda or Azure Functions, to run small, fine granularity tasks make them\nappear to be a natural choice for query processing in such settings. But\nimplementing an analytics system on cloud functions comes with its own set of\nchallenges. These include managing hundreds of tiny stateless\nresource-constrained workers, handling stragglers, and shuffling data through\nopaque cloud services. In this paper we present Starling, a query execution\nengine built on cloud function services that employs number of techniques to\nmitigate these challenges, providing interactive query latency at a lower total\ncost than provisioned systems with low-to-moderate utilization. In particular,\non a 1TB TPC-H dataset in cloud storage, Starling is less expensive than the\nbest provisioned systems for workloads when queries arrive 1 minute apart or\nmore. Starling also has lower latency than competing systems reading from cloud\nobject stores and can scale to larger datasets.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 18:03:16 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Perron", "Matthew", ""], ["Fernandez", "Raul Castro", ""], ["DeWitt", "David", ""], ["Madden", "Samuel", ""]]}, {"id": "1911.11876", "submitter": "Raul Castro Fernandez", "authors": "Raul Castro Fernandez, Nan Tang, Mourad Ouzzani, Michael Stonebraker,\n  Samuel Madden", "title": "Dataset-On-Demand: Automatic View Search and Presentation for Data\n  Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Many data problems are solved when the right view of a combination of\ndatasets is identified. Finding such a view is challenging because of the many\ntables spread across many databases, data lakes, and cloud storage in modern\norganizations. Finding relevant tables, and identifying how to combine them is\na difficult and time-consuming process that hampers users' productivity.\n  In this paper, we describe Dataset-On-Demand (DoD), a system that lets users\nspecify the schema of the view they want, and have the system find views for\nthem. With many underlying data sources, the number of resulting views for any\ngiven query is high, and the burden of choosing the right one is onerous to\nusers. DoD uses a new method, 4C, to reduce the size of the view choice space\nfor users. 4C classifies views into 4 classes: compatible views are exactly the\nsame, contained views present a subsumption relationship, complementary views\nare unionable, and contradictory views have incompatible values that indicate\nfundamental differences between views. These 4 classes permit different\npresentation strategies to reduce the total number of views a user must\nconsider.\n  We evaluate DoD on two key metrics of interest: its ability to reduce the\nsize of the choice space, and the end to end performance. DoD finds all views\nwithin minutes, and reduces the number of views presented to users by 2-10x.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 23:18:04 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Fernandez", "Raul Castro", ""], ["Tang", "Nan", ""], ["Ouzzani", "Mourad", ""], ["Stonebraker", "Michael", ""], ["Madden", "Samuel", ""]]}, {"id": "1911.12060", "submitter": "Jun Zhao", "authors": "Jun Zhao, Teng Wang, Tao Bai, Kwok-Yan Lam, Zhiying Xu, Shuyu Shi,\n  Xuebin Ren, Xinyu Yang, Yang Liu, Han Yu", "title": "Reviewing and Improving the Gaussian Mechanism for Differential Privacy", "comments": "23 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.CY cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy provides a rigorous framework to quantify data privacy,\nand has received considerable interest recently. A randomized mechanism\nsatisfying $(\\epsilon, \\delta)$-differential privacy (DP) roughly means that,\nexcept with a small probability $\\delta$, altering a record in a dataset cannot\nchange the probability that an output is seen by more than a multiplicative\nfactor $e^{\\epsilon} $. A well-known solution to $(\\epsilon, \\delta)$-DP is the\nGaussian mechanism initiated by Dwork et al. [1] in 2006 with an improvement by\nDwork and Roth [2] in 2014, where a Gaussian noise amount $\\sqrt{2\\ln\n\\frac{2}{\\delta}} \\times \\frac{\\Delta}{\\epsilon}$ of [1] or $\\sqrt{2\\ln\n\\frac{1.25}{\\delta}} \\times \\frac{\\Delta}{\\epsilon}$ of [2] is added\nindependently to each dimension of the query result, for a query with\n$\\ell_2$-sensitivity $\\Delta$. Although both classical Gaussian mechanisms\n[1,2] assume $0 < \\epsilon \\leq 1$, our review finds that many studies in the\nliterature have used the classical Gaussian mechanisms under values of\n$\\epsilon$ and $\\delta$ where the added noise amounts of [1,2] do not achieve\n$(\\epsilon,\\delta)$-DP. We obtain such result by analyzing the optimal noise\namount $\\sigma_{DP-OPT}$ for $(\\epsilon,\\delta)$-DP and identifying $\\epsilon$\nand $\\delta$ where the noise amounts of classical mechanisms are even less than\n$\\sigma_{DP-OPT}$.\n  Since $\\sigma_{DP-OPT}$ has no closed-form expression and needs to be\napproximated in an iterative manner, we propose Gaussian mechanisms by deriving\nclosed-form upper bounds for $\\sigma_{DP-OPT}$. Our mechanisms achieve\n$(\\epsilon,\\delta)$-DP for any $\\epsilon$, while the classical mechanisms [1,2]\ndo not achieve $(\\epsilon,\\delta)$-DP for large $\\epsilon$ given $\\delta$.\nMoreover, the utilities of our mechanisms improve those of [1,2] and are close\nto that of the optimal yet more computationally expensive Gaussian mechanism.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 10:26:50 GMT"}, {"version": "v2", "created": "Sat, 7 Dec 2019 04:13:50 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Zhao", "Jun", ""], ["Wang", "Teng", ""], ["Bai", "Tao", ""], ["Lam", "Kwok-Yan", ""], ["Xu", "Zhiying", ""], ["Shi", "Shuyu", ""], ["Ren", "Xuebin", ""], ["Yang", "Xinyu", ""], ["Liu", "Yang", ""], ["Yu", "Han", ""]]}, {"id": "1911.12587", "submitter": "Julia Stoyanovich", "authors": "Sebastian Schelter, Yuxuan He, Jatin Khilnani, Julia Stoyanovich", "title": "FairPrep: Promoting Data to a First-Class Citizen in Studies on\n  Fairness-Enhancing Interventions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The importance of incorporating ethics and legal compliance into\nmachine-assisted decision-making is broadly recognized. Further, several lines\nof recent work have argued that critical opportunities for improving data\nquality and representativeness, controlling for bias, and allowing humans to\noversee and impact computational processes are missed if we do not consider the\nlifecycle stages upstream from model training and deployment. Yet, very little\nhas been done to date to provide system-level support to data scientists who\nwish to develop and deploy responsible machine learning methods. We aim to fill\nthis gap and present FairPrep, a design and evaluation framework for\nfairness-enhancing interventions.\n  FairPrep is based on a developer-centered design, and helps data scientists\nfollow best practices in software engineering and machine learning. As part of\nour contribution, we identify shortcomings in existing empirical studies for\nanalyzing fairness-enhancing interventions. We then show how FairPrep can be\nused to measure the impact of sound best practices, such as hyperparameter\ntuning and feature scaling. In particular, our results suggest that the high\nvariability of the outcomes of fairness-enhancing interventions observed in\nprevious studies is often an artifact of a lack of hyperparameter tuning.\nFurther, we show that the choice of a data cleaning method can impact the\neffectiveness of fairness-enhancing interventions.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 08:28:46 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Schelter", "Sebastian", ""], ["He", "Yuxuan", ""], ["Khilnani", "Jatin", ""], ["Stoyanovich", "Julia", ""]]}, {"id": "1911.12674", "submitter": "Michael G\\\"unther", "authors": "Michael G\\\"unther, Maik Thiele, Wolfgang Lehner", "title": "RETRO: Relation Retrofitting For In-Database Machine Learning on Textual\n  Data", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are massive amounts of textual data residing in databases, valuable for\nmany machine learning (ML) tasks. Since ML techniques depend on numerical input\nrepresentations, word embeddings are increasingly utilized to convert symbolic\nrepresentations such as text into meaningful numbers. However, a naive\none-to-one mapping of each word in a database to a word embedding vector is not\nsufficient and would lead to poor accuracies in ML tasks. Thus, we argue to\nadditionally incorporate the information given by the database schema into the\nembedding, e.g. which words appear in the same column or are related to each\nother. In this paper, we propose RETRO (RElational reTROfitting), a novel\napproach to learn numerical representations of text values in databases,\ncapturing the best of both worlds, the rich information encoded by word\nembeddings and the relational information encoded by database tables. We\nformulate relation retrofitting as a learning problem and present an efficient\nalgorithm solving it. We investigate the impact of various hyperparameters on\nthe learning problem and derive good settings for all of them. Our evaluation\nshows that the proposed embeddings are ready-to-use for many ML tasks such as\nclassification and regression and even outperform state-of-the-art techniques\nin integration tasks such as null value imputation and link prediction.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 12:37:26 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2020 07:55:38 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["G\u00fcnther", "Michael", ""], ["Thiele", "Maik", ""], ["Lehner", "Wolfgang", ""]]}, {"id": "1911.12877", "submitter": "Bo Wu", "authors": "Daniel Mawhirter, Sam Reinehr, Connor Holmes, Tongping Liu, Bo Wu", "title": "GraphZero: Breaking Symmetry for Efficient Graph Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph mining for structural patterns is a fundamental task in many\napplications. Compilation-based graph mining systems, represented by AutoMine,\ngenerate specialized algorithms for the provided patterns and substantially\noutperform other systems. However, the generated code causes substantial\ncomputation redundancy and the compilation process incurs too much overhead to\nbe used online, both due to the inherent symmetry in the structural patterns.\n  In this paper, we propose an optimizing compiler, GraphZero, to completely\naddress these limitations through symmetry breaking based on group theory.\nGraphZero implements three novel techniques. First, its schedule explorer\nefficiently prunes the schedule space without missing any high-performance\nschedule. Second, it automatically generates and enforces a set of restrictions\nto eliminate computation redundancy. Third, it generalizes orientation, a\nsurprisingly effective optimization that was mainly used for clique patterns,\nto apply to arbitrary patterns. Evaluated on multiple graph mining applications\nand complex patterns with 7 real-world graph datasets, GraphZero demonstrates\nup to 40X performance improvement and up to 197X reduction on schedule\ngeneration overhead over AutoMine.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 22:07:22 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Mawhirter", "Daniel", ""], ["Reinehr", "Sam", ""], ["Holmes", "Connor", ""], ["Liu", "Tongping", ""], ["Wu", "Bo", ""]]}, {"id": "1911.12930", "submitter": "Dinusha Vatsalan", "authors": "Dinusha Vatsalan, Peter Christen, and Erhard Rahm", "title": "Incremental Clustering Techniques for Multi-Party Privacy-Preserving\n  Record Linkage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy-Preserving Record Linkage (PPRL) supports the integration of\nsensitive information from multiple datasets, in particular the\nprivacy-preserving matching of records referring to the same entity. PPRL has\ngained much attention in many application areas, with the most prominent ones\nin the healthcare domain. PPRL techniques tackle this problem by conducting\nlinkage on masked (encoded) values. Employing PPRL on records from multiple\n(more than two) parties/sources (multi-party PPRL, MP-PPRL) is an increasingly\nimportant but challenging problem that so far has not been sufficiently solved.\nExisting MP-PPRL approaches are limited to finding only those entities that are\npresent in all parties thereby missing entities that match only in a subset of\nparties. Furthermore, previous MP-PPRL approaches face substantial scalability\nlimitations due to the need of a large number of comparisons between masked\nrecords. We thus propose and evaluate new MP-PPRL approaches that find matches\nin any subset of parties and still scale to many parties. Our approaches\nmaintain all matches within clusters, where these clusters are incrementally\nextended or refined by considering records from one party after the other. An\nempirical evaluation using multiple real datasets ranging from 3 to 26 parties\neach containing up to $5$ million records validates that our protocols are\nefficient, and significantly outperform existing MP-PPRL approaches in terms of\nlinkage quality and scalability.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 02:53:35 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Vatsalan", "Dinusha", ""], ["Christen", "Peter", ""], ["Rahm", "Erhard", ""]]}, {"id": "1911.12933", "submitter": "Batya Kenig", "authors": "Batya Kenig, Pranay Mundra, Guna Prasad, Babak Salimi, Dan Suciu", "title": "Mining Approximate Acyclic Schemes from Relations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acyclic schemes have numerous applications in databases and in machine\nlearning, such as improved design, more efficient storage, and increased\nperformance for queries and machine learning algorithms. Multivalued\ndependencies (MVDs) are the building blocks of acyclic schemes. The discovery\nfrom data of both MVDs and acyclic schemes is more challenging than other forms\nof data dependencies, such as Functional Dependencies, because these\ndependencies do not hold on subsets of data, and because they are very\nsensitive to noise in the data; for example a single wrong or missing tuple may\ninvalidate the schema. In this paper we present Maimon, a system for\ndiscovering approximate acyclic schemes and MVDs from data. We give a\nprincipled definition of approximation, by using notions from information\ntheory, then describe the two components of Maimon: mining for approximate\nMVDs, then reconstructing acyclic schemes from approximate MVDs. We conduct an\nexperimental evaluation of Maimon on 20 real-world datasets, and show that it\ncan scale up to 1M rows, and up to 30 columns.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 03:09:41 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Kenig", "Batya", ""], ["Mundra", "Pranay", ""], ["Prasad", "Guna", ""], ["Salimi", "Babak", ""], ["Suciu", "Dan", ""]]}, {"id": "1911.13014", "submitter": "Andreas Kipf", "authors": "Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons\n  Kemper, Tim Kraska, Thomas Neumann", "title": "SOSD: A Benchmark for Learned Indexes", "comments": "NeurIPS 2019 Workshop on Machine Learning for Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A groundswell of recent work has focused on improving data management systems\nwith learned components. Specifically, work on learned index structures has\nproposed replacing traditional index structures, such as B-trees, with learned\nmodels. Given the decades of research committed to improving index structures,\nthere is significant skepticism about whether learned indexes actually\noutperform state-of-the-art implementations of traditional structures on\nreal-world data. To answer this question, we propose a new benchmarking\nframework that comes with a variety of real-world datasets and baseline\nimplementations to compare against. We also show preliminary results for\nselected index structures, and find that learned models indeed often outperform\nstate-of-the-art implementations, and are therefore a promising direction for\nfuture research.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 09:35:04 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Kipf", "Andreas", ""], ["Marcus", "Ryan", ""], ["van Renen", "Alexander", ""], ["Stoian", "Mihail", ""], ["Kemper", "Alfons", ""], ["Kraska", "Tim", ""], ["Neumann", "Thomas", ""]]}]