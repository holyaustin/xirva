[{"id": "1709.00700", "submitter": "Sebastian Bre{\\ss}", "authors": "Sebastian Bre{\\ss} and Bastian K\\\"ocher and Henning Funke and Tilmann\n  Rabl and Volker Markl", "title": "Generating Custom Code for Efficient Query Execution on Heterogeneous\n  Processors", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Processor manufacturers build increasingly specialized processors to mitigate\nthe effects of the power wall to deliver improved performance. Currently,\ndatabase engines are manually optimized for each processor: A costly and error\nprone process.\n  In this paper, we propose concepts to enable the database engine to perform\nper-processor optimization automatically. Our core idea is to create variants\nof generated code and to learn a fast variant for each processor. We create\nvariants by modifying parallelization strategies, specializing data structures,\nand applying different code transformations.\n  Our experimental results show that the performance of variants may diverge up\nto two orders of magnitude. Therefore, we need to generate custom code for each\nprocessor to achieve peak performance. We show that our approach finds a fast\ncustom variant for multi-core CPUs, GPUs, and MICs.\n", "versions": [{"version": "v1", "created": "Sun, 3 Sep 2017 11:16:31 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Bre\u00df", "Sebastian", ""], ["K\u00f6cher", "Bastian", ""], ["Funke", "Henning", ""], ["Rabl", "Tilmann", ""], ["Markl", "Volker", ""]]}, {"id": "1709.01142", "submitter": "Sebastian Neef", "authors": "Sebastian Neef", "title": "Implementation and Evaluation of a Framework to calculate Impact\n  Measures for Wikipedia Authors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.DB cs.DC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wikipedia, an open collaborative website, can be edited by anyone, even\nanonymously, thus becoming victim to ill-intentioned changes. Therefore,\nranking Wikipedia authors by calculating impact measures based on the edit\nhistory can help to identify reputational users or harmful activity such as\nvandalism \\cite{Adler:2008:MAC:1822258.1822279}. However, processing millions\nof edits on one system can take a long time. The author implements an open\nsource framework to calculate such rankings in a distributed way (MapReduce)\nand evaluates its performance on various sized datasets. A reimplementation of\nthe contribution measures by \\citeauthor{Adler:2008:MAC:1822258.1822279}\ndemonstrates its extensibility and usability, as well as problems of handling\nhuge datasets and their possible resolutions. The results put different\nperformance optimizations into perspective and show that horizontal scaling can\ndecrease the total processing time.\n", "versions": [{"version": "v1", "created": "Sat, 26 Aug 2017 21:33:02 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Neef", "Sebastian", ""]]}, {"id": "1709.01190", "submitter": "Yiqiu Wang", "authors": "Yiqiu Wang, Anshumali Shrivastava, Jonathan Wang, Junghee Ryu", "title": "FLASH: Randomized Algorithms Accelerated over CPU-GPU for Ultra-High\n  Dimensional Similarity Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.DC cs.IR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present FLASH (\\textbf{F}ast \\textbf{L}SH \\textbf{A}lgorithm for\n\\textbf{S}imilarity search accelerated with \\textbf{H}PC), a similarity search\nsystem for ultra-high dimensional datasets on a single machine, that does not\nrequire similarity computations and is tailored for high-performance computing\nplatforms. By leveraging a LSH style randomized indexing procedure and\ncombining it with several principled techniques, such as reservoir sampling,\nrecent advances in one-pass minwise hashing, and count based estimations, we\nreduce the computational and parallelization costs of similarity search, while\nretaining sound theoretical guarantees.\n  We evaluate FLASH on several real, high-dimensional datasets from different\ndomains, including text, malicious URL, click-through prediction, social\nnetworks, etc. Our experiments shed new light on the difficulties associated\nwith datasets having several million dimensions. Current state-of-the-art\nimplementations either fail on the presented scale or are orders of magnitude\nslower than FLASH. FLASH is capable of computing an approximate k-NN graph,\nfrom scratch, over the full webspam dataset (1.3 billion nonzeros) in less than\n10 seconds. Computing a full k-NN graph in less than 10 seconds on the webspam\ndataset, using brute-force ($n^2D$), will require at least 20 teraflops. We\nprovide CPU and GPU implementations of FLASH for replicability of our results.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 23:09:19 GMT"}, {"version": "v2", "created": "Tue, 3 Jul 2018 07:09:23 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Wang", "Yiqiu", ""], ["Shrivastava", "Anshumali", ""], ["Wang", "Jonathan", ""], ["Ryu", "Junghee", ""]]}, {"id": "1709.01600", "submitter": "Ahmet Kara", "authors": "Ahmet Kara and Dan Olteanu", "title": "Covers of Query Results", "comments": "33 pages. Notation simplified", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce succinct lossless representations of query results called\ncovers. They are subsets of the query results that correspond to minimal edge\ncovers in the hypergraphs of these results.\n  We first study covers whose structures are given by fractional hypertree\ndecompositions of join queries. For any decomposition of a query, we give\nasymptotically tight size bounds for the covers of the query result over that\ndecomposition and show that such covers can be computed in worst-case optimal\ntime up to a logarithmic factor in the database size. For acyclic join queries,\nwe can compute covers compositionally using query plans with a new operator\ncalled cover-join. The tuples in the query result can be enumerated from any of\nits covers with linearithmic pre-computation time and constant delay.\n  We then generalize covers from joins to functional aggregate queries that\nexpress a host of computational problems such as aggregate-join queries,\nin-database optimization, matrix chain multiplication, and inference in\nprobabilistic graphical models.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 21:31:02 GMT"}, {"version": "v2", "created": "Wed, 10 Jan 2018 19:55:24 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Kara", "Ahmet", ""], ["Olteanu", "Dan", ""]]}, {"id": "1709.02343", "submitter": "Arnab Bhattacharya", "authors": "Shubhadip Mitra, Priya Saraf, Arnab Bhattacharya", "title": "TIPS: Mining Top-K Locations to Minimize User-Inconvenience for\n  Trajectory-Aware Services", "comments": null, "journal-ref": "TKDE, 2019", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facility location problems aim to identify the best locations to set up new\nservices. Majority of the existing works typically assume that the users are\nstatic. However, there exists a wide array of services such as fuel stations,\nATMs, food joints, etc., that are widely accessed by mobile users besides the\nstatic ones. Such trajectory-aware services should, therefore, factor in the\ntrajectories of its users rather than simply their static locations. In this\nwork, we introduce the problem of optimal placement of facility locations for\nsuch trajectory-aware services that minimize the user inconvenience. The\ninconvenience of a user is the extra distance traveled by her from her regular\npath to avail a service. We call this the TIPS problem (Trajectory-aware\nInconvenience-minimizing Placement of Services) and consider two variants of\nit. The goal of the first variant, MAXTIPS, is to minimize the maximum\ninconvenience faced by any user, while that of the second, AVGTIPS, is to\nminimize the average inconvenience over all the users. We show that both these\nproblems are NP-hard, and propose multiple efficient heuristics to solve them.\nEmpirical evaluation on real urban-scale road networks validate the efficiency\nand effectiveness of the proposed heuristics.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 16:40:20 GMT"}, {"version": "v2", "created": "Fri, 8 Sep 2017 07:55:13 GMT"}, {"version": "v3", "created": "Fri, 2 Aug 2019 04:38:02 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Mitra", "Shubhadip", ""], ["Saraf", "Priya", ""], ["Bhattacharya", "Arnab", ""]]}, {"id": "1709.02457", "submitter": "Ali Pesaranghader", "authors": "Ali Pesaranghader, Herna Viktor and Eric Paquet", "title": "Reservoir of Diverse Adaptive Learners and Stacking Fast Hoeffding Drift\n  Detection Methods for Evolving Data Streams", "comments": "42 pages, and 14 figures", "journal-ref": null, "doi": "10.1007/s10994-018-5719-z", "report-no": null, "categories": "stat.ML cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last decade has seen a surge of interest in adaptive learning algorithms\nfor data stream classification, with applications ranging from predicting ozone\nlevel peaks, learning stock market indicators, to detecting computer security\nviolations. In addition, a number of methods have been developed to detect\nconcept drifts in these streams. Consider a scenario where we have a number of\nclassifiers with diverse learning styles and different drift detectors.\nIntuitively, the current 'best' (classifier, detector) pair is application\ndependent and may change as a result of the stream evolution. Our research\nbuilds on this observation. We introduce the $\\mbox{Tornado}$ framework that\nimplements a reservoir of diverse classifiers, together with a variety of drift\ndetection algorithms. In our framework, all (classifier, detector) pairs\nproceed, in parallel, to construct models against the evolving data streams. At\nany point in time, we select the pair which currently yields the best\nperformance. We further incorporate two novel stacking-based drift detection\nmethods, namely the $\\mbox{FHDDMS}$ and $\\mbox{FHDDMS}_{add}$ approaches. The\nexperimental evaluation confirms that the current 'best' (classifier, detector)\npair is not only heavily dependent on the characteristics of the stream, but\nalso that this selection evolves as the stream flows. Further, our\n$\\mbox{FHDDMS}$ variants detect concept drifts accurately in a timely fashion\nwhile outperforming the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 21:19:24 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Pesaranghader", "Ali", ""], ["Viktor", "Herna", ""], ["Paquet", "Eric", ""]]}, {"id": "1709.02489", "submitter": "Harry Kalodner", "authors": "Harry Kalodner, Steven Goldfeder, Alishah Chator, Malte M\\\"oser, and\n  Arvind Narayanan", "title": "BlockSci: Design and applications of a blockchain analysis platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of blockchain data is useful for both scientific research and\ncommercial applications. We present BlockSci, an open-source software platform\nfor blockchain analysis. BlockSci is versatile in its support for different\nblockchains and analysis tasks. It incorporates an in-memory, analytical\n(rather than transactional) database, making it several hundred times faster\nthan existing tools. We describe BlockSci's design and present four analyses\nthat illustrate its capabilities.\n  This is a working paper that accompanies the first public release of\nBlockSci, available at https://github.com/citp/BlockSci. We seek input from the\ncommunity to further develop the software and explore other potential\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 00:11:38 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["Kalodner", "Harry", ""], ["Goldfeder", "Steven", ""], ["Chator", "Alishah", ""], ["M\u00f6ser", "Malte", ""], ["Narayanan", "Arvind", ""]]}, {"id": "1709.02529", "submitter": "Ahmed Mahmood", "authors": "Ahmed R. Mahmood, Ahmed M. Aly, Walid G. Aref", "title": "FAST: Frequency-Aware Spatio-Textual Indexing for In-Memory Continuous\n  Filter Query Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications need to process massive streams of spatio-textual data in\nreal-time against continuous spatio-textual queries. For example, in\nlocation-aware ad targeting publish/subscribe systems, it is required to\ndisseminate millions of ads and promotions to millions of users based on the\nlocations and textual profiles of users. In this paper, we study indexing of\ncontinuous spatio-textual queries. There exist several related spatio-textual\nindexes that typically integrate a spatial index with a textual index. However,\nthese indexes usually have a high demand for main-memory and assume that the\nentire vocabulary of keywords is known in advance. Also, these indexes do not\nsuccessfully capture the variations in the frequencies of keywords across\ndifferent spatial regions and treat frequent and infrequent keywords in the\nsame way. Moreover, existing indexes do not adapt to the changes in workload\nover space and time. For example, some keywords may be trending at certain\ntimes in certain locations and this may change as time passes. This affects the\nindexing and searching performance of existing indexes significantly. In this\npaper, we introduce FAST, a Frequency-Aware Spatio-Textual index for continuous\nspatio-textual queries. FAST is a main-memory index that requires up to one\nthird of the memory needed by the state-of-the-art index. FAST does not assume\nprior knowledge of the entire vocabulary of indexed objects. FAST adaptively\naccounts for the difference in the frequencies of keywords within their\ncorresponding spatial regions to automatically choose the best indexing\napproach that optimizes the insertion and search times. Extensive experimental\nevaluation using real and synthetic datasets demonstrates that FAST is up to 3x\nfaster in search time and 5x faster in insertion time than the state-of-the-art\nindexes.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 04:22:41 GMT"}, {"version": "v2", "created": "Wed, 4 Oct 2017 20:05:07 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Mahmood", "Ahmed R.", ""], ["Aly", "Ahmed M.", ""], ["Aref", "Walid G.", ""]]}, {"id": "1709.02610", "submitter": "Nachshon Cohen", "authors": "Nachshon Cohen, Michal Friedman, James R. Larus", "title": "Efficient Logging in Non-Volatile Memory by Exploiting Coherency\n  Protocols", "comments": null, "journal-ref": null, "doi": "10.1145/3133891", "report-no": null, "categories": "cs.DC cs.DB cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Non-volatile memory (NVM) technologies such as PCM, ReRAM and STT-RAM allow\nprocessors to directly write values to persistent storage at speeds that are\nsignificantly faster than previous durable media such as hard drives or SSDs.\nMany applications of NVM are constructed on a logging subsystem, which enables\noperations to appear to execute atomically and facilitates recovery from\nfailures. Writes to NVM, however, pass through a processor's memory system,\nwhich can delay and reorder them and can impair the correctness and cost of\nlogging algorithms.\n  Reordering arises because of out-of-order execution in a CPU and the\ninter-processor cache coherence protocol. By carefully considering the\nproperties of these reorderings, this paper develops a logging protocol that\nrequires only one round trip to non-volatile memory while avoiding expensive\ncomputations. We show how to extend the logging protocol to building a\npersistent set (hash map) that also requires only a single round trip to\nnon-volatile memory for insertion, updating, or deletion.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 09:35:29 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["Cohen", "Nachshon", ""], ["Friedman", "Michal", ""], ["Larus", "James R.", ""]]}, {"id": "1709.02968", "submitter": "Chao-Lin Liu", "authors": "Chao-Lin Liu and Hongsu Wang", "title": "Matrix and Graph Operations for Relationship Inference: An Illustration\n  with the Kinship Inference in the China Biographical Database", "comments": "3 pages, 3 figures, 2017 Annual Meeting of the Japanese Association\n  for Digital Humanities", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biographical databases contain diverse information about individuals. Person\nnames, birth information, career, friends, family and special achievements are\nsome possible items in the record for an individual. The relationships between\nindividuals, such as kinship and friendship, provide invaluable insights about\nhidden communities which are not directly recorded in databases. We show that\nsome simple matrix and graph-based operations are effective for inferring\nrelationships among individuals, and illustrate the main ideas with the China\nBiographical Database (CBDB).\n", "versions": [{"version": "v1", "created": "Sat, 9 Sep 2017 16:01:08 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Liu", "Chao-Lin", ""], ["Wang", "Hongsu", ""]]}, {"id": "1709.03147", "submitter": "Kijung Shin", "authors": "Kijung Shin", "title": "WRS: Waiting Room Sampling for Accurate Triangle Counting in Real Graph\n  Streams", "comments": "to be published in IEEE International Conference on Data Mining 2017\n  (ICDM-2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  If we cannot store all edges in a graph stream, which edges should we store\nto estimate the triangle count accurately?\n  Counting triangles (i.e., cycles of length three) is a fundamental graph\nproblem with many applications in social network analysis, web mining, anomaly\ndetection, etc. Recently, much effort has been made to accurately estimate\nglobal and local triangle counts in streaming settings with limited space.\nAlthough existing methods use sampling techniques without considering temporal\ndependencies in edges, we observe temporal locality in real dynamic graphs.\nThat is, future edges are more likely to form triangles with recent edges than\nwith older edges.\n  In this work, we propose a single-pass streaming algorithm called\nWaiting-Room Sampling (WRS) for global and local triangle counting. WRS\nexploits the temporal locality by always storing the most recent edges, which\nfuture edges are more likely to form triangles with, in the waiting room, while\nit uses reservoir sampling for the remaining edges. Our theoretical and\nempirical analyses show that WRS is: (a) Fast and 'any time': runs in linear\ntime, always maintaining and updating estimates while new edges arrive, (b)\nEffective: yields up to 47% smaller estimation error than its best competitors,\nand (c) Theoretically sound: gives unbiased estimates with small variances\nunder the temporal locality.\n", "versions": [{"version": "v1", "created": "Sun, 10 Sep 2017 17:47:04 GMT"}, {"version": "v2", "created": "Tue, 19 Sep 2017 12:40:11 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Shin", "Kijung", ""]]}, {"id": "1709.03188", "submitter": "Siddhartha Sahu", "authors": "Siddhartha Sahu, Amine Mhedhbi, Semih Salihoglu, Jimmy Lin, M. Tamer\n  \\\"Ozsu", "title": "The Ubiquity of Large Graphs and Surprising Challenges of Graph\n  Processing: Extended Survey", "comments": null, "journal-ref": "The VLDB Journal, 2019", "doi": "10.1007/s00778-019-00548-x", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph processing is becoming increasingly prevalent across many application\ndomains. In spite of this prevalence, there is little research about how graphs\nare actually used in practice. We performed an extensive study that consisted\nof an online survey of 89 users, a review of the mailing lists, source\nrepositories, and whitepapers of a large suite of graph software products, and\nin-person interviews with 6 users and 2 developers of these products. Our\nonline survey aimed at understanding: (i) the types of graphs users have; (ii)\nthe graph computations users run; (iii) the types of graph software users use;\nand (iv) the major challenges users face when processing their graphs. We\ndescribe the participants' responses to our questions highlighting common\npatterns and challenges. Based on our interviews and survey of the rest of our\nsources, we were able to answer some new questions that were raised by\nparticipants' responses to our online survey and understand the specific\napplications that use graph data and software. Our study revealed surprising\nfacts about graph processing in practice. In particular, real-world graphs\nrepresent a very diverse range of entities and are often very large,\nscalability and visualization are undeniably the most pressing challenges faced\nby participants, and data integration, recommendations, and fraud detection are\nvery popular applications supported by existing graph software. We hope these\nfindings can guide future research.\n", "versions": [{"version": "v1", "created": "Sun, 10 Sep 2017 22:25:13 GMT"}, {"version": "v2", "created": "Tue, 19 Dec 2017 09:21:50 GMT"}, {"version": "v3", "created": "Wed, 4 Sep 2019 20:55:10 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Sahu", "Siddhartha", ""], ["Mhedhbi", "Amine", ""], ["Salihoglu", "Semih", ""], ["Lin", "Jimmy", ""], ["\u00d6zsu", "M. Tamer", ""]]}, {"id": "1709.03221", "submitter": "Yuriy Brun", "authors": "Sainyam Galhotra, Yuriy Brun, Alexandra Meliou", "title": "Fairness Testing: Testing Software for Discrimination", "comments": "Sainyam Galhotra, Yuriy Brun, and Alexandra Meliou. 2017. Fairness\n  Testing: Testing Software for Discrimination. In Proceedings of 2017 11th\n  Joint Meeting of the European Software Engineering Conference and the ACM\n  SIGSOFT Symposium on the Foundations of Software Engineering (ESEC/FSE),\n  Paderborn, Germany, September 4-8, 2017 (ESEC/FSE'17).\n  https://doi.org/10.1145/3106237.3106277, ESEC/FSE, 2017", "journal-ref": null, "doi": "10.1145/3106237.3106277", "report-no": null, "categories": "cs.SE cs.AI cs.CY cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper defines software fairness and discrimination and develops a\ntesting-based method for measuring if and how much software discriminates,\nfocusing on causality in discriminatory behavior. Evidence of software\ndiscrimination has been found in modern software systems that recommend\ncriminal sentences, grant access to financial products, and determine who is\nallowed to participate in promotions. Our approach, Themis, generates efficient\ntest suites to measure discrimination. Given a schema describing valid system\ninputs, Themis generates discrimination tests automatically and does not\nrequire an oracle. We evaluate Themis on 20 software systems, 12 of which come\nfrom prior work with explicit focus on avoiding discrimination. We find that\n(1) Themis is effective at discovering software discrimination, (2)\nstate-of-the-art techniques for removing discrimination from algorithms fail in\nmany situations, at times discriminating against as much as 98% of an input\nsubdomain, (3) Themis optimizations are effective at producing efficient test\nsuites for measuring discrimination, and (4) Themis is more efficient on\nsystems that exhibit more discrimination. We thus demonstrate that fairness\ntesting is a critical aspect of the software development cycle in domains with\npossible discrimination and provide initial tools for measuring software\ndiscrimination.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 02:45:22 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Galhotra", "Sainyam", ""], ["Brun", "Yuriy", ""], ["Meliou", "Alexandra", ""]]}, {"id": "1709.03267", "submitter": "Thomas Guyet", "authors": "Thomas Guyet (LACODAM), Ren\\'e Quiniou (LACODAM, Inria), V\\'eronique\n  Masson (UR1, LACODAM)", "title": "Mining relevant interval rules", "comments": "International Conference on Formal Concept Analysis, Jun 2017,\n  Rennes, France. Supplementary proceedings of International Conference on\n  Formal Concept Analysis (ICFCA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article extends the method of Garriga et al. for mining relevant rules\nto numerical attributes by extracting interval-based pattern rules. We propose\nan algorithm that extracts such rules from numerical datasets using the\ninterval-pattern approach from Kaytoue et al. This algorithm has been\nimplemented and evaluated on real datasets.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 07:18:58 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Guyet", "Thomas", "", "LACODAM"], ["Quiniou", "Ren\u00e9", "", "LACODAM, Inria"], ["Masson", "V\u00e9ronique", "", "UR1, LACODAM"]]}, {"id": "1709.03270", "submitter": "Thomas Guyet", "authors": "Ahmed Samet (UR1, LACODAM), Thomas Guyet (LACODAM), Benjamin\n  Negrevergne (LAMSADE), Tien-Tuan Dao, Tuan Nha Hoang, Marie-Christine Ho Ba\n  Tho", "title": "Expert Opinion Extraction from a Biomedical Database", "comments": null, "journal-ref": "Conference on Symbolic and Quantitative Approaches to Reasoning\n  with Uncertainty (ECSQARU), Jul 2017, Lugano, Switzerland. Springer, 31 (LNCS\n  10369), pp.1 - 12, 2017, Proceedings of 14th European Conference on Symbolic\n  and Quantitative Approaches to Reasoning with Uncertainty", "doi": "10.1016/S0888-613X(02)00066-X", "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle the problem of extracting frequent opinions from\nuncertain databases. We introduce the foundation of an opinion mining approach\nwith the definition of pattern and support measure. The support measure is\nderived from the commitment definition. A new algorithm called OpMiner that\nextracts the set of frequent opinions modelled as a mass functions is detailed.\nFinally, we apply our approach on a real-world biomedical database that stores\nopinions of experts to evaluate the reliability level of biomedical data.\nPerformance analysis showed a better quality patterns for our proposed model in\ncomparison with literature-based methods.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 07:23:00 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Samet", "Ahmed", "", "UR1, LACODAM"], ["Guyet", "Thomas", "", "LACODAM"], ["Negrevergne", "Benjamin", "", "LAMSADE"], ["Dao", "Tien-Tuan", ""], ["Hoang", "Tuan Nha", ""], ["Tho", "Marie-Christine Ho Ba", ""]]}, {"id": "1709.03685", "submitter": "Pavle Subotic", "authors": "Herbert Jordan, Bernhard Scholz, Pavle Suboti\\'c", "title": "Optimal On The Fly Index Selection in Polynomial Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The index selection problem (ISP) is an important problem for accelerating\nthe execution of relational queries, and it has received a lot of attention as\na combinatorial knapsack problem in the past. Various solutions to this very\nhard problem have been provided. In contrast to existing literature, we change\nthe underlying assumptions of the problem definition: we adapt the problem for\nsystems that store relations in memory, and use complex specification\nlanguages, e.g., Datalog. In our framework, we decompose complex queries into\nprimitive searches that select tuples in a relation for which an equality\npredicate holds. A primitive search can be accelerated by an index exhibiting a\nworst-case run-time complexity of log-linear time in the size of the output\nresult of the primitive search. However, the overheads associated with\nmaintaining indexes are very costly in terms of memory and computing time.\n  In this work, we present an optimal polynomial-time algorithm that finds the\nminimal set of indexes of a relation for a given set of primitive searches. An\nindex may cover more than one primitive search due to the algebraic properties\nof the search predicate, which is a conjunction of equalities over the\nattributes of a relation. The index search space exhibits a exponential\ncomplexity in the number of attributes in a relation, and, hence brute-force\nalgorithms searching for solutions in the index domain are infeasible. As a\nscaffolding for designing a polynomial-time algorithm, we build a partial order\non search operations and use a constructive version of Dilworth's theorem. We\nshow a strong relationship between chains of primitive searches (forming a\npartial order) and indexes. We demonstrate the effectiveness and efficiency of\nour algorithm for an in-memory Datalog compiler that is able to process\nrelations with billions of entries in memory.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 04:10:51 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Jordan", "Herbert", ""], ["Scholz", "Bernhard", ""], ["Suboti\u0107", "Pavle", ""]]}, {"id": "1709.03949", "submitter": "Andreas Kosmatopoulos", "authors": "Spyros Sioutas, Kostas Tsichlas, Andreas Kosmatopoulos, Apostolos N.\n  Papadopoulos, Dimitrios Tsoumakos, Katerina Doka", "title": "Skyline Queries in O(1) time?", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The skyline of a set $P$ of points ($SKY(P)$) consists of the \"best\" points\nwith respect to minimization or maximization of the attribute values. A point\n$p$ dominates another point $q$ if $p$ is as good as $q$ in all dimensions and\nit is strictly better than $q$ in at least one dimension. In this work, we\nfocus on the static $2$-d space and provide expected performance guarantees for\n$3$-sided Range Skyline Queries on the Grid, where $N$ is the cardinality of\n$P$, $B$ the size of a disk block, and $R$ the capacity of main memory. We\npresent the MLR-tree, which offers optimal expected cost for finding planar\nskyline points in a $3$-sided query rectangle, $q=[a,b]\\times(-\\infty,d]$, in\nboth RAM and I/O model on the grid $[1,M]\\times [1,M]$, by single scanning only\nthe points contained in $SKY(P)$. In particular, it supports skyline queries in\na $3$-sided range in $O(t\\cdot t_{PAM}(N))$ time ($O((t/B)\\cdot t_{PAM}(N))$\nI/Os), where $t$ is the answer size and $t_{PAM}(N)$ the time required for\nanswering predecessor queries for $d$ in a PAM (Predecessor Access Method)\nstructure, which is a special component of MLR-tree and stores efficiently\nroot-to-leaf paths or sub-paths. By choosing PAM structures with $O(1)$\nexpected time for predecessor queries under discrete $\\mu$-random distributions\nof the $x$ and $y$ coordinates, MLR-tree supports skyline queries in optimal\n$O(t)$ expected time ($O(t/B)$ expected number of I/Os) with high probability.\nThe space cost becomes superlinear and can be reduced to linear for many\nspecial practical cases. If we choose a PAM structure with $O(1)$ amortized\ntime for batched predecessor queries (under no assumption on distributions of\nthe $x$ and $y$ coordinates), MLR-tree supports batched skyline queries in\noptimal $O(t)$ amortized time, however the space becomes exponential. In\ndynamic case, the update time complexity is affected by a $O(log^{2}N)$ factor.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 16:51:28 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Sioutas", "Spyros", ""], ["Tsichlas", "Kostas", ""], ["Kosmatopoulos", "Andreas", ""], ["Papadopoulos", "Apostolos N.", ""], ["Tsoumakos", "Dimitrios", ""], ["Doka", "Katerina", ""]]}, {"id": "1709.04284", "submitter": "Felix Martin Schuhknecht", "authors": "Ankur Sharma, Felix Martin Schuhknecht, Jens Dittrich", "title": "Accelerating Analytical Processing in MVCC using Fine-Granular\n  High-Frequency Virtual Snapshotting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient transactional management is a delicate task. As systems face\ntransactions of inherently different types, ranging from point updates to long\nrunning analytical computations, it is hard to satisfy their individual\nrequirements with a single processing component. Unfortunately, most systems\nnowadays rely on such a single component that implements its parallelism using\nmulti-version concurrency control (MVCC). While MVCC parallelizes short-running\nOLTP transactions very well, it struggles in the presence of mixed workloads\ncontaining long-running scan-centric OLAP queries, as scans have to work their\nway through large amounts of versioned data. To overcome this problem, we\npropose a system, which reintroduces the concept of heterogeneous transaction\nprocessing: OLAP transactions are outsourced to run on separate (virtual)\nsnapshots while OLTP transactions run on the most recent representation of the\ndatabase. Inside both components, MVCC ensures a high degree of concurrency.\nThe biggest challenge of such a heterogeneous approach is to generate the\nsnapshots at a high frequency. Previous approaches heavily suffered from the\ntremendous cost of snapshot creation. In our system, we overcome the\nrestrictions of the OS by introducing a custom system call vm_snapshot, that is\nhand-tailored to our precise needs: it allows fine-granular snapshot creation\nat very high frequencies, rendering the snapshot creation phase orders of\nmagnitudes faster than state-of-the-art approaches. Our experimental evaluation\non a heterogeneous workload based on TPC-H transactions and handcrafted OLTP\ntransactions shows that our system enables significantly higher analytical\ntransaction throughputs on mixed workloads than homogeneous approaches. In this\nsense, we introduce a system that accelerates Analytical processing by\nintroducing custom Kernel functionalities: AnKerDB.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 12:30:30 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Sharma", "Ankur", ""], ["Schuhknecht", "Felix Martin", ""], ["Dittrich", "Jens", ""]]}, {"id": "1709.04290", "submitter": "Michel De Rougemont", "authors": "Michel de Rougemont, Guillaume Vimont", "title": "Approximate Integration of streaming data", "comments": "17 pages;Revue des Nouvelles Technologies de l'Information, 2017", "journal-ref": null, "doi": null, "report-no": "vol.RNTI-B-13", "categories": "cs.SI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We approximate analytic queries on streaming data with a weighted reservoir\nsampling. For a stream of tuples of a Datawarehouse we show how to approximate\nsome OLAP queries. For a stream of graph edges from a Social Network, we\napproximate the communities as the large connected components of the edges in\nthe reservoir. We show that for a model of random graphs which follow a power\nlaw degree distribution, the community detection algorithm is a good\napproximation. Given two streams of graph edges from two Sources, we define the\n{\\em Community Correlation} as the fraction of the nodes in communities in both\nstreams. Although we do not store the edges of the streams, we can approximate\nthe Community Correlation and define the {\\em Integration of two streams}. We\nillustrate this approach with Twitter streams, associated with TV programs.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 12:37:29 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["de Rougemont", "Michel", ""], ["Vimont", "Guillaume", ""]]}, {"id": "1709.04545", "submitter": "Antonio Cavalcante Araujo Neto", "authors": "Antonio Cavalcante Araujo Neto, Joerg Sander, Ricardo J. G. B.\n  Campello, Mario A. Nascimento", "title": "Efficient Computation of Multiple Density-Based Clustering Hierarchies", "comments": "A short version of this paper appears at IEEE ICDM 2017. Corrected\n  typos. Revised abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  HDBSCAN*, a state-of-the-art density-based hierarchical clustering method,\nproduces a hierarchical organization of clusters in a dataset w.r.t. a\nparameter mpts. While the performance of HDBSCAN* is robust w.r.t. mpts in the\nsense that a small change in mpts typically leads to only a small or no change\nin the clustering structure, choosing a \"good\" mpts value can be challenging:\ndepending on the data distribution, a high or low value for mpts may be more\nappropriate, and certain data clusters may reveal themselves at different\nvalues of mpts. To explore results for a range of mpts values, however, one has\nto run HDBSCAN* for each value in the range independently, which is\ncomputationally inefficient. In this paper, we propose an efficient approach to\ncompute all HDBSCAN* hierarchies for a range of mpts values by replacing the\ngraph used by HDBSCAN* with a much smaller graph that is guaranteed to contain\nthe required information. An extensive experimental evaluation shows that with\nour approach one can obtain over one hundred hierarchies for the computational\ncost equivalent to running HDBSCAN* about 2 times.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 21:24:42 GMT"}, {"version": "v2", "created": "Fri, 22 Sep 2017 22:08:11 GMT"}, {"version": "v3", "created": "Thu, 7 Jun 2018 23:05:58 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Neto", "Antonio Cavalcante Araujo", ""], ["Sander", "Joerg", ""], ["Campello", "Ricardo J. G. B.", ""], ["Nascimento", "Mario A.", ""]]}, {"id": "1709.04747", "submitter": "Jerome Darmont", "authors": "Ciprian-Octavian Truic\\u{a} (UPB), J\\'er\\^ome Darmont (ERIC)", "title": "T${}^2$K${}^2$: The Twitter Top-K Keywords Benchmark", "comments": null, "journal-ref": "21st European Conference on Advances in Databases and Information\n  Systems (ADBIS 2017), Sep 2017, Nicosie, Cyprus. Springer, Communications in\n  Computer and Information Science, 767, pp.21-28, 2017, New Trends in\n  Databases and Information Systems", "doi": "10.1007/978-3-319-67162-8", "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information retrieval from textual data focuses on the construction of\nvocabularies that contain weighted term tuples. Such vocabularies can then be\nexploited by various text analysis algorithms to extract new knowledge, e.g.,\ntop-k keywords, top-k documents, etc. Top-k keywords are casually used for\nvarious purposes, are often computed on-the-fly, and thus must be efficiently\ncomputed. To compare competing weighting schemes and database implementations,\nbenchmarking is customary. To the best of our knowledge, no benchmark currently\naddresses these problems. Hence, in this paper, we present a top-k keywords\nbenchmark, T${}^2$K${}^2$, which features a real tweet dataset and queries with\nvarious complexities and selectivities. T${}^2$K${}^2$ helps evaluate weighting\nschemes and database implementations in terms of computing performance. To\nillustrate T${}^2$K${}^2$'s relevance and genericity, we successfully performed\ntests on the TF-IDF and Okapi BM25 weighting schemes, on one hand, and on\ndifferent relational (Oracle, PostgreSQL) and document-oriented (MongoDB)\ndatabase implementations, on the other hand.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 13:01:51 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Truic\u0103", "Ciprian-Octavian", "", "UPB"], ["Darmont", "J\u00e9r\u00f4me", "", "ERIC"]]}, {"id": "1709.05183", "submitter": "Demian Hespe", "authors": "Demian Hespe, Martin Weidner, Jonathan Dees, Peter Sanders", "title": "Fast OLAP Query Execution in Main Memory on Large Data in a Cluster", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Main memory column-stores have proven to be efficient for processing\nanalytical queries. Still, there has been much less work in the context of\nclusters. Using only a single machine poses several restrictions: Processing\npower and data volume are bounded to the number of cores and main memory\nfitting on one tightly coupled system. To enable the processing of larger data\nsets, switching to a cluster becomes necessary. In this work, we explore\ntechniques for efficient execution of analytical SQL queries on large amounts\nof data in a parallel database cluster while making maximal use of the\navailable hardware. This includes precompiled query plans for efficient CPU\nutilization, full parallelization on single nodes and across the cluster, and\nefficient inter-node communication. We implement all features in a prototype\nfor running a subset of TPC-H benchmark queries. We evaluate our implementation\nusing a 128 node cluster running TPC-H queries with 30 000 gigabyte of\nuncompressed data.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 13:08:07 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Hespe", "Demian", ""], ["Weidner", "Martin", ""], ["Dees", "Jonathan", ""], ["Sanders", "Peter", ""]]}, {"id": "1709.05197", "submitter": "Philipp Grulich", "authors": "Philipp M. Grulich", "title": "Scalable real-time processing with Spark Streaming: implementation and\n  design of a Car Information System", "comments": "Hamburg University of Applied Sciences, Bachelor Thesis, 2016, in\n  German, 113 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Streaming data processing is a hot topic in big data these days, because it\nmade it possible to process a huge amount of events within a low latency. One\nof the most common used open-source stream processing platforms is Spark\nStreaming, which is demonstrated and discussed based on a real-world use-case\nin this paper. The use-case is about a Car Information System, which is an\nexample for a classic stream processing system. First the System is de- signed\nand engineered, whereby the application architecture is created carefully,\nbecause it should be adaptable for similar use-cases. At the end of this paper\nthe CIS and Spark Streaming is evaluated by the use of the Goal Question Metric\nmodel. The evaluation proves that Spark Streaming is capable to create stream\nprocessing in a scalable and fault tolerant manner. But it also shows that\nSpark is a very fast moving project, which could cause problems during the\ndevelopment and maintenance of a software project.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 11:13:17 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Grulich", "Philipp M.", ""]]}, {"id": "1709.05369", "submitter": "Cristian Riveros", "authors": "Marco Bucchi, Alejandro Grez, Cristian Riveros, Mart\\'in Ugarte", "title": "Foundations of Complex Event Processing", "comments": "Conference version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex Event Processing (CEP) has emerged as the unifying field for\ntechnologies that require processing and correlating distributed data sources\nin real-time. CEP finds applications in diverse domains, which has resulted in\na large number of proposals for expressing and processing complex events.\nHowever, existing CEP languages lack from a clear semantics, making them hard\nto understand and generalize. Moreover, there are no general techniques for\nevaluating CEP query languages with clear performance guarantees.\n  In this paper we embark on the task of giving a rigorous and efficient\nframework to CEP. We propose a formal language for specifying complex events,\ncalled CEL, that contains the main features used in the literature and has a\ndenotational and compositional semantics. We also formalize the so-called\nselection strategies, which had only been presented as by-design extensions to\nexisting frameworks. With a well-defined semantics at hand, we study how to\nefficiently evaluate CEL for processing complex events in the case of unary\nfilters. We start by studying the syntactical properties of CEL and propose\nrewriting optimization techniques for simplifying the evaluation of formulas.\nThen, we introduce a formal computational model for CEP, called complex event\nautomata (CEA), and study how to compile CEL formulas into CEA. Furthermore, we\nprovide efficient algorithms for evaluating CEA over event streams using\nconstant time per event followed by constant-delay enumeration of the results.\nBy gathering these results together, we propose a framework for efficiently\nevaluating CEL with unary filters. Finally, we show experimentally that this\nframework consistently outperforms the competition, and even over trivial\nqueries can be orders of magnitude more efficient.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 19:01:15 GMT"}, {"version": "v2", "created": "Thu, 30 Aug 2018 15:52:44 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Bucchi", "Marco", ""], ["Grez", "Alejandro", ""], ["Riveros", "Cristian", ""], ["Ugarte", "Mart\u00edn", ""]]}, {"id": "1709.05376", "submitter": "Christiane Engels", "authors": "Christiane Engels, Andreas Behrend and Stefan Brass", "title": "A Rule-Based Approach to Analyzing Database Schema Objects with Datalog", "comments": "Pre-proceedings paper presented at the 27th International Symposium\n  on Logic-Based Program Synthesis and Transformation (LOPSTR 2017), Namur,\n  Belgium, 10-12 October 2017 (arXiv:1708.07854)", "journal-ref": null, "doi": null, "report-no": "LOPSTR/2017/18", "categories": "cs.PL cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Database schema elements such as tables, views, triggers and functions are\ntypically defined with many interrelationships. In order to support database\nusers in understanding a given schema, a rule-based approach for analyzing the\nrespective dependencies is proposed using Datalog expressions. We show that\nmany interesting properties of schema elements can be systematically determined\nthis way. The expressiveness of the proposed analysis is exemplarily shown with\nthe problem of computing induced functional dependencies for derived relations.\nThe propagation of functional dependencies plays an important role in data\nintegration and query optimization but represents an undecidable problem in\ngeneral. And yet, our rule-based analysis covers all relational operators as\nwell as linear recursive expressions in a systematic way showing the depth of\nanalysis possible by our proposal. The analysis of functional dependencies is\nwell-integrated in a uniform approach to analyzing dependencies between schema\nelements in general.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 19:23:30 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Engels", "Christiane", ""], ["Behrend", "Andreas", ""], ["Brass", "Stefan", ""]]}, {"id": "1709.05874", "submitter": "Fernando Almeida Dr.", "authors": "Jose Ferreira, Fernando Almeida, Jose Monteiro", "title": "Building an Effective Data Warehousing for Financial Sector", "comments": "10 pages", "journal-ref": "Automatic Control and Information Sciences, 3(1), 2017", "doi": "10.12691/acis-3-1-4", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents the implementation process of a Data Warehouse and a\nmultidimensional analysis of business data for a holding company in the\nfinancial sector. The goal is to create a business intelligence system that, in\na simple, quick but also versatile way, allows the access to updated,\naggregated, real and/or projected information, regarding bank account balances.\nThe established system extracts and processes the operational database\ninformation which supports cash management information by using Integration\nServices and Analysis Services tools from Microsoft SQL Server. The end-user\ninterface is a pivot table, properly arranged to explore the information\navailable by the produced cube. The results have shown that the adoption of\nonline analytical processing cubes offers better performance and provides a\nmore automated and robust process to analyze current and provisional aggregated\nfinancial data balances compared to the current process based on static reports\nbuilt from transactional databases.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 11:50:28 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Ferreira", "Jose", ""], ["Almeida", "Fernando", ""], ["Monteiro", "Jose", ""]]}, {"id": "1709.06176", "submitter": "Julia Stoyanovich", "authors": "Julia Stoyanovich and Matthew Gilbride and Vera Zaychik Moffitt", "title": "Zooming in on NYC taxi data with Portal", "comments": "Presented at Data Science for Social Good (DSSG) 2017:\n  https://dssg.uchicago.edu/data-science-for-social-good-conference-2017/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop a methodology for analyzing transportation data at\ndifferent levels of temporal and geographic granularity, and apply our\nmethodology to the TLC Trip Record Dataset, made publicly available by the NYC\nTaxi & Limousine Commission. This data is naturally represented by a set of\ntrajectories, annotated with time and with additional information such as\npassenger count and cost. We analyze TLC data to identify hotspots, which point\nto lack of convenient public transportation options, and popular routes, which\nmotivate ride-sharing solutions or addition of a bus route.\n  Our methodology is based on using a system called Portal, which implements\nefficient representations and principled analysis methods for evolving graphs.\nPortal is implemented on top of Apache Spark, a popular distributed data\nprocessing system, is inter-operable with other Spark libraries like SparkSQL,\nand supports sophisticated kinds of analysis of evolving graphs efficiently.\nPortal is currently under development in the Data, Responsibly Lab at Drexel.\nWe plan to release Portal in the open source in Fall 2017.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 21:55:47 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Stoyanovich", "Julia", ""], ["Gilbride", "Matthew", ""], ["Moffitt", "Vera Zaychik", ""]]}, {"id": "1709.06185", "submitter": "Antoine Amarilli", "authors": "Antoine Amarilli, Pierre Bourhis, Stefan Mengel", "title": "Enumeration on Trees under Relabelings", "comments": "37 pages including appendix, 31 references. This is the full version\n  with proofs of the corresponding ICDT'18 publication, and it integrates all\n  reviewer feedback. Except for the additional appendices, the contents are\n  exactly the same as in the conference version", "journal-ref": null, "doi": "10.4230/LIPIcs.ICDT.2018.5", "report-no": null, "categories": "cs.DB cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study how to evaluate MSO queries with free variables on trees, within the\nframework of enumeration algorithms. Previous work has shown how to enumerate\nanswers with linear-time preprocessing and delay linear in the size of each\noutput, i.e., constant-delay for free first-order variables. We extend this\nresult to support relabelings, a restricted kind of update operations on trees\nwhich allows us to change the node labels. Our main result shows that we can\nenumerate the answers of MSO queries on trees with linear-time preprocessing\nand delay linear in each answer, while supporting node relabelings in\nlogarithmic time. To prove this, we reuse the circuit-based enumeration\nstructure from our earlier work, and develop techniques to maintain its index\nunder node relabelings. We also show how enumeration under relabelings can be\napplied to evaluate practical query languages, such as aggregate, group-by, and\nparameterized queries.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 22:08:44 GMT"}, {"version": "v2", "created": "Sat, 6 Jan 2018 22:41:39 GMT"}, {"version": "v3", "created": "Thu, 31 May 2018 12:44:22 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Amarilli", "Antoine", ""], ["Bourhis", "Pierre", ""], ["Mengel", "Stefan", ""]]}, {"id": "1709.06186", "submitter": "Shaleen Deep", "authors": "Shaleen Deep and Paraschos Koutris", "title": "Compressed Representations of Conjunctive Query Results", "comments": "To appear in PODS'18; 35 pages; comments welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relational queries, and in particular join queries, often generate large\noutput results when executed over a huge dataset. In such cases, it is often\ninfeasible to store the whole materialized output if we plan to reuse it\nfurther down a data processing pipeline. Motivated by this problem, we study\nthe construction of space-efficient compressed representations of the output of\nconjunctive queries, with the goal of supporting the efficient access of the\nintermediate compressed result for a given access pattern. In particular, we\ninitiate the study of an important tradeoff: minimizing the space necessary to\nstore the compressed result, versus minimizing the answer time and delay for an\naccess request over the result. Our main contribution is a novel parameterized\ndata structure, which can be tuned to trade off space for answer time. The\ntradeoff allows us to control the space requirement of the data structure\nprecisely, and depends both on the structure of the query and the access\npattern. We show how we can use the data structure in conjunction with query\ndecomposition techniques, in order to efficiently represent the outputs for\nseveral classes of conjunctive queries.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 22:09:24 GMT"}, {"version": "v2", "created": "Wed, 27 Dec 2017 01:03:39 GMT"}, {"version": "v3", "created": "Tue, 27 Mar 2018 04:11:59 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Deep", "Shaleen", ""], ["Koutris", "Paraschos", ""]]}, {"id": "1709.06188", "submitter": "Antoine Amarilli", "authors": "Antoine Amarilli, Mika\\\"el Monet, Pierre Senellart", "title": "Connecting Width and Structure in Knowledge Compilation (Extended\n  Version)", "comments": "33 pages, no figures, 40 references. This is the full version with\n  proofs of the corresponding ICDT'18 publication, and it integrates all\n  reviewer feedback. Except for the additional appendices, and except for\n  formatting differences and inessential changes, the contents are the same as\n  in the conference version", "journal-ref": null, "doi": "10.4230/LIPIcs.ICDT.2018.6", "report-no": null, "categories": "cs.DB cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Several query evaluation tasks can be done via knowledge compilation: the\nquery result is compiled as a lineage circuit from which the answer can be\ndetermined. For such tasks, it is important to leverage some width parameters\nof the circuit, such as bounded treewidth or pathwidth, to convert the circuit\nto structured classes, e.g., deterministic structured NNFs (d-SDNNFs) or OBDDs.\nIn this work, we show how to connect the width of circuits to the size of their\nstructured representation, through upper and lower bounds. For the upper bound,\nwe show how bounded-treewidth circuits can be converted to a d-SDNNF, in time\nlinear in the circuit size. Our bound, unlike existing results, is constructive\nand only singly exponential in the treewidth. We show a related lower bound on\nmonotone DNF or CNF formulas, assuming a constant bound on the arity (size of\nclauses) and degree (number of occurrences of each variable). Specifically, any\nd-SDNNF (resp., SDNNF) for such a DNF (resp., CNF) must be of exponential size\nin its treewidth; and the same holds for pathwidth when compiling to OBDDs. Our\nlower bounds, in contrast with most previous work, apply to any formula of this\nclass, not just a well-chosen family. Hence, for our language of DNF and CNF,\npathwidth and treewidth respectively characterize the efficiency of compiling\nto OBDDs and (d-)SDNNFs, that is, compilation is singly exponential in the\nwidth parameter. We conclude by applying our lower bound results to the task of\nquery evaluation.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 22:15:11 GMT"}, {"version": "v2", "created": "Sat, 6 Jan 2018 22:42:42 GMT"}, {"version": "v3", "created": "Thu, 31 May 2018 12:39:01 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Amarilli", "Antoine", ""], ["Monet", "Mika\u00ebl", ""], ["Senellart", "Pierre", ""]]}, {"id": "1709.06202", "submitter": "Arslan Munir", "authors": "Avishek Bose, Arslan Munir, and Neda Shabani", "title": "A Comparative Quantitative Analysis of Contemporary Big Data Clustering\n  Algorithms for Market Segmentation in Hospitality Industry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The hospitality industry is one of the data-rich industries that receives\nhuge Volumes of data streaming at high Velocity with considerably Variety,\nVeracity, and Variability. These properties make the data analysis in the\nhospitality industry a big data problem. Meeting the customers' expectations is\na key factor in the hospitality industry to grasp the customers' loyalty. To\nachieve this goal, marketing professionals in this industry actively look for\nways to utilize their data in the best possible manner and advance their data\nanalytic solutions, such as identifying a unique market segmentation clustering\nand developing a recommendation system. In this paper, we present a\ncomprehensive literature review of existing big data clustering algorithms and\ntheir advantages and disadvantages for various use cases. We implement the\nexisting big data clustering algorithms and provide a quantitative comparison\nof the performance of different clustering algorithms for different scenarios.\nWe also present our insights and recommendations regarding the suitability of\ndifferent big data clustering algorithms for different use cases. These\nrecommendations will be helpful for hoteliers in selecting the appropriate\nmarket segmentation clustering algorithm for different clustering datasets to\nimprove the customer experience and maximize the hotel revenue.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 23:47:44 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Bose", "Avishek", ""], ["Munir", "Arslan", ""], ["Shabani", "Neda", ""]]}, {"id": "1709.06416", "submitter": "Shoumik Palkar", "authors": "Shoumik Palkar, James Thomas, Deepak Narayanan, Anil Shanbhag, Rahul\n  Palamuttam, Holger Pirk, Malte Schwarzkopf, Saman Amarasinghe, Samuel Madden,\n  and Matei Zaharia", "title": "Weld: Rethinking the Interface Between Data-Intensive Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data analytics applications combine multiple functions from different\nlibraries and frameworks. Even when each function is optimized in isolation,\nthe performance of the combined application can be an order of magnitude below\nhardware limits due to extensive data movement across these functions. To\naddress this problem, we propose Weld, a new interface between data-intensive\nlibraries that can optimize across disjoint libraries and functions. Weld\nexposes a lazily-evaluated API where diverse functions can submit their\ncomputations in a simple but general intermediate representation that captures\ntheir data-parallel structure. It then optimizes data movement across these\nfunctions and emits efficient code for diverse hardware. Weld can be integrated\ninto existing frameworks such as Spark, TensorFlow, Pandas and NumPy without\nchanging their user-facing APIs. We demonstrate that Weld can speed up\napplications using these frameworks by up to 29x.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 05:37:20 GMT"}, {"version": "v2", "created": "Tue, 24 Oct 2017 20:35:12 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Palkar", "Shoumik", ""], ["Thomas", "James", ""], ["Narayanan", "Deepak", ""], ["Shanbhag", "Anil", ""], ["Palamuttam", "Rahul", ""], ["Pirk", "Holger", ""], ["Schwarzkopf", "Malte", ""], ["Amarasinghe", "Saman", ""], ["Madden", "Samuel", ""], ["Zaharia", "Matei", ""]]}, {"id": "1709.06715", "submitter": "Mohamed Hassan", "authors": "Mohamed S. Hassan, Tatiana Kuznetsova, Hyun Chai Jeong, Walid G. Aref,\n  Mohammad Sadoghi", "title": "Empowering In-Memory Relational Database Engines with Native Graph\n  Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The plethora of graphs and relational data give rise to many interesting\ngraph-relational queries in various domains, e.g., finding related proteins\nsatisfying relational predicates in a biological network. The maturity of\nRDBMSs motivated academia and industry to invest efforts in leveraging RDBMSs\nfor graph processing, where efficiency is proven for vital graph queries.\nHowever, none of these efforts process graphs natively inside the RDBMS, which\nis particularly challenging due to the impedance mismatch between the\nrelational and the graph models. In this paper, we propose to treat graphs as\nfirst-class citizens inside the relational engine so that operations on graphs\nare executed natively inside the RDBMS. We realize our approach inside VoltDB,\nan open-source in-memory relational database, and name this realization\nGRFusion. The SQL and the query engine of GRFusion are empowered to\ndeclaratively define graphs and execute cross-data-model query plans formed by\ngraph and relational operators, resulting in up to four orders-of-magnitude in\nquery-time speedup w.r.t. state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 03:51:41 GMT"}, {"version": "v2", "created": "Thu, 12 Oct 2017 16:19:42 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Hassan", "Mohamed S.", ""], ["Kuznetsova", "Tatiana", ""], ["Jeong", "Hyun Chai", ""], ["Aref", "Walid G.", ""], ["Sadoghi", "Mohammad", ""]]}, {"id": "1709.06723", "submitter": "Mohamed Hassan", "authors": "Mohamed S. Hassan, Bruno Ribeiro, Walid G. Aref", "title": "SBG-Sketch: A Self-Balanced Sketch for Labeled-Graph Stream\n  Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications in various domains rely on processing graph streams, e.g.,\ncommunication logs of a cloud-troubleshooting system, road-network traffic\nupdates, and interactions on a social network. A labeled-graph stream refers to\na sequence of streamed edges that form a labeled graph. Label-aware\napplications need to filter the graph stream before performing a graph\noperation. Due to the large volume and high velocity of these streams, it is\noften more practical to incrementally build a lossy-compressed version of the\ngraph, and use this lossy version to approximately evaluate graph queries.\nChallenges arise when the queries are unknown in advance but are associated\nwith filtering predicates based on edge labels. Surprisingly common, and\nespecially challenging, are labeled-graph streams that have highly skewed label\ndistributions that might also vary over time. This paper introduces\nSelf-Balanced Graph Sketch (SBG-Sketch, for short), a graphical sketch for\nsummarizing and querying labeled-graph streams that can cope with all these\nchallenges. SBG-Sketch maintains synopsis for both the edge attributes (e.g.,\nedge weight) as well as the topology of the streamed graph. SBG-Sketch allows\nefficient processing of graph-traversal queries, e.g., reachability queries.\nExperimental results over a variety of real graph streams show SBG-Sketch to\nreduce the estimation errors of state-of-the-art methods by up to 99%.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 04:37:02 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Hassan", "Mohamed S.", ""], ["Ribeiro", "Bruno", ""], ["Aref", "Walid G.", ""]]}, {"id": "1709.06745", "submitter": "Huiju Wang Dr", "authors": "Huiju Wang, Zhengkui Wang, Kian-Lee Tan, Chee-Yong Chan, Qi Fan, Xiao\n  Yue", "title": "VCExplorer: A Interactive Graph Exploration Framework Based on Hub\n  Vertices with Graph Consolidation", "comments": "11 pages, 8 figures and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs have been widely used to model different information networks, such as\nthe Web, biological networks and social networks (e.g. Twitter). Due to the\nsize and complexity of these graphs, how to explore and utilize these graphs\nhas become a very challenging problem. In this paper, we propose, VCExplorer, a\nnew interactive graph exploration framework that integrates the strengths of\ngraph visualization and graph summarization. Unlike existing graph\nvisualization tools where vertices of a graph may be clustered into a smaller\ncollection of super/virtual vertices, VCExplorer displays a small number of\nactual source graph vertices (called hubs) and summaries of the information\nbetween these vertices. We refer to such a graph as a HA-graph (Hub-based\nAggregation Graph). This allows users to appreciate the relationship between\nthe hubs, rather than super/virtual vertices. Users can navigate through the\nHA- graph by \"drilling down\" into the summaries between hubs to display more\nhubs. We illustrate how the graph aggregation techniques can be integrated into\nthe exploring framework as the consolidated information to users. In addition,\nwe propose efficient graph aggregation algorithms over multiple subgraphs via\ncomputation sharing. Extensive experimental evaluations have been conducted\nusing both real and synthetic datasets and the results indicate the\neffectiveness and efficiency of VCExplorer for exploration.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 07:23:03 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Wang", "Huiju", ""], ["Wang", "Zhengkui", ""], ["Tan", "Kian-Lee", ""], ["Chan", "Chee-Yong", ""], ["Fan", "Qi", ""], ["Yue", "Xiao", ""]]}, {"id": "1709.06810", "submitter": "Lijun Chang", "authors": "Lijun Chang, Xing Feng, Xuemin Lin, Lu Qin, Wenjie Zhang", "title": "Efficient Graph Edit Distance Computation and Verification via\n  Anchor-aware Lower Bound Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph edit distance (GED) is an important similarity measure adopted in a\nsimilarity-based analysis between two graphs, and computing GED is a primitive\noperator in graph database analysis. Partially due to the NP-hardness, the\nexisting techniques for computing GED are only able to process very small\ngraphs with less than 30 vertices. Motivated by this, in this paper we\nsystematically study the problems of both GED computation, and GED verification\n(i.e., verify whether the GED between two graphs is no larger than a user-given\nthreshold). Firstly, we develop a unified framework that can be instantiated\ninto either a best-first search approach AStar+ or a depth-first search\napproach DFS+. Secondly, we design anchor-aware lower bound estimation\ntechniques to compute tighter lower bounds for intermediate search states,\nwhich significantly reduce the search spaces of both AStar+ and DFS+. We also\npropose efficient techniques to compute the lower bounds. Thirdly, based on our\nunified framework, we contrast AStar+ with DFS+ regarding their time and space\ncomplexities, and recommend that AStar+ is better than DFS+ by having a much\nsmaller search space. Extensive empirical studies validate that AStar+ performs\nbetter than DFS+, and show that our AStar+-BMa approach outperforms the\nstate-of-the-art technique by more than four orders of magnitude.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 10:55:22 GMT"}, {"version": "v2", "created": "Sun, 1 Oct 2017 02:52:40 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Chang", "Lijun", ""], ["Feng", "Xing", ""], ["Lin", "Xuemin", ""], ["Qin", "Lu", ""], ["Zhang", "Wenjie", ""]]}, {"id": "1709.06907", "submitter": "Simon Razniewski", "authors": "Simon Razniewski, Vevake Balaraman, Werner Nutt", "title": "Doctoral Advisor or Medical Condition: Towards Entity-specific Rankings\n  of Knowledge Base Properties [Extended Version]", "comments": "Extended version of an ADMA 2017 conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In knowledge bases such as Wikidata, it is possible to assert a large set of\nproperties for entities, ranging from generic ones such as name and place of\nbirth to highly profession-specific or background-specific ones such as\ndoctoral advisor or medical condition. Determining a preference or ranking in\nthis large set is a challenge in tasks such as prioritisation of edits or\nnatural-language generation. Most previous approaches to ranking knowledge base\nproperties are purely data-driven, that is, as we show, mistake frequency for\ninterestingness.\n  In this work, we have developed a human-annotated dataset of 350 preference\njudgments among pairs of knowledge base properties for fixed entities. From\nthis set, we isolate a subset of pairs for which humans show a high level of\nagreement (87.5% on average). We show, however, that baseline and\nstate-of-the-art techniques achieve only 61.3% precision in predicting human\npreferences for this subset.\n  We then analyze what contributes to one property being rated as more\nimportant than another one, and identify that at least three factors play a\nrole, namely (i) general frequency, (ii) applicability to similar entities and\n(iii) semantic similarity between property and entity. We experimentally\nanalyze the contribution of each factor and show that a combination of\ntechniques addressing all the three factors achieves 74% precision on the task.\n  The dataset is available at\nwww.kaggle.com/srazniewski/wikidatapropertyranking.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 14:43:08 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Razniewski", "Simon", ""], ["Balaraman", "Vevake", ""], ["Nutt", "Werner", ""]]}, {"id": "1709.06955", "submitter": "Zhewei Wei", "authors": "Yu Liu, Bolong Zheng, Xiaodong He, Zhewei Wei, Xiaokui Xiao, Kai\n  Zheng, Jiaheng Lu", "title": "ProbeSim: Scalable Single-Source and Top-k SimRank Computations on\n  Dynamic Graphs", "comments": "15 pages", "journal-ref": null, "doi": "10.14778/3151113.3151115", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-source and top-$k$ SimRank queries are two important types of\nsimilarity search in graphs with numerous applications in web mining, social\nnetwork analysis, spam detection, etc. A plethora of techniques have been\nproposed for these two types of queries, but very few can efficiently support\nsimilarity search over large dynamic graphs, due to either significant\npreprocessing time or large space overheads.\n  This paper presents ProbeSim, an index-free algorithm for single-source and\ntop-$k$ SimRank queries that provides a non-trivial theoretical guarantee in\nthe absolute error of query results. ProbeSim estimates SimRank similarities\nwithout precomputing any indexing structures, and thus can naturally support\nreal-time SimRank queries on dynamic graphs. Besides the theoretical guarantee,\nProbeSim also offers satisfying practical efficiency and effectiveness due to\nseveral non-trivial optimizations. We conduct extensive experiments on a number\nof benchmark datasets, which demonstrate that our solutions significantly\noutperform the existing methods in terms of efficiency and effectiveness.\nNotably, our experiments include the first empirical study that evaluates the\neffectiveness of SimRank algorithms on graphs with billion edges, using the\nidea of pooling.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 16:32:29 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2018 07:42:17 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Liu", "Yu", ""], ["Zheng", "Bolong", ""], ["He", "Xiaodong", ""], ["Wei", "Zhewei", ""], ["Xiao", "Xiaokui", ""], ["Zheng", "Kai", ""], ["Lu", "Jiaheng", ""]]}, {"id": "1709.07493", "submitter": "Sherif Sakr", "authors": "Radwa Elshawi and Sherif Sakr", "title": "Big Data Systems Meet Machine Learning Challenges: Towards Big Data\n  Science as a Service", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, we have been witnessing huge advancements in the scale of data we\nroutinely generate and collect in pretty much everything we do, as well as our\nability to exploit modern technologies to process, analyze and understand this\ndata. The intersection of these trends is what is called, nowadays, as Big Data\nScience. Cloud computing represents a practical and cost-effective solution for\nsupporting Big Data storage, processing and for sophisticated analytics\napplications. We analyze in details the building blocks of the software stack\nfor supporting big data science as a commodity service for data scientists. We\nprovide various insights about the latest ongoing developments and open\nchallenges in this domain.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 18:50:32 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Elshawi", "Radwa", ""], ["Sakr", "Sherif", ""]]}, {"id": "1709.07594", "submitter": "Sudhakar Singh", "authors": "Sudhakar Singh, Rakhi Garg, P. K. Mishra", "title": "A Comparative Study of Association Rule Mining Algorithms on Grid and\n  Cloud Platform", "comments": "8 pages, preprint", "journal-ref": "International Journal of Emerging Technologies in Computational\n  and Applied Sciences, 2014", "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Association rule mining is a time consuming process due to involving both\ndata intensive and computation intensive nature. In order to mine large volume\nof data and to enhance the scalability and performance of existing sequential\nassociation rule mining algorithms, parallel and distributed algorithms are\ndeveloped. These traditional parallel and distributed algorithms are based on\nhomogeneous platform and are not lucrative for heterogeneous platform such as\ngrid and cloud. This requires design of new algorithms which address the issues\nof good data set partition and distribution, load balancing strategy,\noptimization of communication and synchronization technique among processors in\nsuch heterogeneous system. Grid and cloud are the emerging platform for\ndistributed data processing and various association rule mining algorithms have\nbeen proposed on such platforms. This survey article integrates the brief\narchitectural aspect of distributed system, various recent approaches of grid\nbased and cloud based association rule mining algorithms with comparative\nperception. We differentiate between approaches of association rule mining\nalgorithms developed on these architectures on the basis of data locality,\nprogramming paradigm, fault tolerance, communication cost, partition and\ndistribution of data sets. Although it is not complete in order to cover all\nalgorithms, yet it can be very useful for the new researchers working in the\ndirection of distributed association rule mining algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 05:06:18 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Singh", "Sudhakar", ""], ["Garg", "Rakhi", ""], ["Mishra", "P. K.", ""]]}, {"id": "1709.07821", "submitter": "Daniel Lemire", "authors": "Daniel Lemire, Owen Kaser, Nathan Kurz, Luca Deri, Chris O'Hara,\n  Fran\\c{c}ois Saint-Jacques, Gregory Ssi-Yan-Kai", "title": "Roaring Bitmaps: Implementation of an Optimized Software Library", "comments": null, "journal-ref": "Software: Practice and Experience Volume 48, Issue 4 April 2018\n  Pages 867-895", "doi": "10.1002/spe.2560", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Compressed bitmap indexes are used in systems such as Git or Oracle to\naccelerate queries. They represent sets and often support operations such as\nunions, intersections, differences, and symmetric differences. Several\nimportant systems such as Elasticsearch, Apache Spark, Netflix's Atlas,\nLinkedIn's Pinot, Metamarkets' Druid, Pilosa, Apache Hive, Apache Tez,\nMicrosoft Visual Studio Team Services and Apache Kylin rely on a specific type\nof compressed bitmap index called Roaring. We present an optimized software\nlibrary written in C implementing Roaring bitmaps: CRoaring. It benefits from\nseveral algorithms designed for the single-instruction-multiple-data (SIMD)\ninstructions available on commodity processors. In particular, we present\nvectorized algorithms to compute the intersection, union, difference and\nsymmetric difference between arrays. We benchmark the library against a wide\nrange of competitive alternatives, identifying weaknesses and strengths in our\nsoftware. Our work is available under a liberal open-source license.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 15:56:49 GMT"}, {"version": "v2", "created": "Tue, 5 Dec 2017 02:11:02 GMT"}, {"version": "v3", "created": "Fri, 5 Jan 2018 22:02:10 GMT"}, {"version": "v4", "created": "Mon, 13 Jul 2020 15:02:06 GMT"}, {"version": "v5", "created": "Wed, 14 Apr 2021 12:26:42 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Lemire", "Daniel", ""], ["Kaser", "Owen", ""], ["Kurz", "Nathan", ""], ["Deri", "Luca", ""], ["O'Hara", "Chris", ""], ["Saint-Jacques", "Fran\u00e7ois", ""], ["Ssi-Yan-Kai", "Gregory", ""]]}, {"id": "1709.07941", "submitter": "Janis Kalofolias", "authors": "Janis Kalofolias, Mario Boley and Jilles Vreeken", "title": "Efficiently Discovering Locally Exceptional yet Globally Representative\n  Subgroups", "comments": "10 pages, To appear in ICDM17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subgroup discovery is a local pattern mining technique to find interpretable\ndescriptions of sub-populations that stand out on a given target variable. That\nis, these sub-populations are exceptional with regard to the global\ndistribution. In this paper we argue that in many applications, such as\nscientific discovery, subgroups are only useful if they are additionally\nrepresentative of the global distribution with regard to a control variable.\nThat is, when the distribution of this control variable is the same, or almost\nthe same, as over the whole data.\n  We formalise this objective function and give an efficient algorithm to\ncompute its tight optimistic estimator for the case of a numeric target and a\nbinary control variable. This enables us to use the branch-and-bound framework\nto efficiently discover the top-$k$ subgroups that are both exceptional as well\nas representative. Experimental evaluation on a wide range of datasets shows\nthat with this algorithm we discover meaningful representative patterns and are\nup to orders of magnitude faster in terms of node evaluations as well as time.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 20:49:41 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Kalofolias", "Janis", ""], ["Boley", "Mario", ""], ["Vreeken", "Jilles", ""]]}, {"id": "1709.08028", "submitter": "Noreddine Gherabi", "authors": "Noreddine Gherabi, Redouane Nejjahi, and Abderrahim Marzouk", "title": "Towards Classification of Web ontologies using the Horizontal and\n  Vertical Segmentation", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-64719-7", "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The new era of the Web is known as the semantic Web or the Web of data. The\nsemantic Web depends on ontologies that are seen as one of its pillars. The\nbigger these ontologies, the greater their exploitation. However, when these\nontologies become too big other problems may appear, such as the complexity to\ncharge big files in memory, the time it needs to download such files and\nespecially the time it needs to make reasoning on them. We discuss in this\npaper approaches for segmenting such big Web ontologies as well as its\nusefulness. The segmentation method extracts from an existing ontology a\nsegment that represents a layer or a generation in the existing ontology; i.e.\na horizontally extraction. The extracted segment should be itself an ontology.\n", "versions": [{"version": "v1", "created": "Sat, 23 Sep 2017 09:20:34 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Gherabi", "Noreddine", ""], ["Nejjahi", "Redouane", ""], ["Marzouk", "Abderrahim", ""]]}, {"id": "1709.08083", "submitter": "Lingyang Chu", "authors": "Lingyang Chu, Zhefeng Wang, Jian Pei, Yanyan Zhang, Yu Yang, Enhong\n  Chen", "title": "Finding Theme Communities from Database Networks", "comments": null, "journal-ref": "Proceedings of the VLDB Endowment, Vol. 12, No. 10. 2019", "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a database network where each vertex is associated with a transaction\ndatabase, we are interested in finding theme communities. Here, a theme\ncommunity is a cohesive subgraph such that a common pattern is frequent in all\ntransaction databases associated with the vertices in the subgraph. Finding all\ntheme communities from a database network enjoys many novel applications.\nHowever, it is challenging since even counting the number of all theme\ncommunities in a database network is #P-hard. Inspired by the observation that\na theme community shrinks when the length of the pattern increases, we\ninvestigate several properties of theme communities and develop TCFI, a\nscalable algorithm that uses these properties to effectively prune the patterns\nthat cannot form any theme community. We also design TC-Tree, a scalable\nalgorithm that decomposes and indexes theme communities efficiently. Retrieving\na ranked list of theme communities from a TC-Tree of hundreds of millions of\ntheme communities takes less than 1 second. Extensive experiments and a case\nstudy demonstrate the effectiveness and scalability of TCFI and TC-Tree in\ndiscovering and querying meaningful theme communities from large database\nnetworks.\n", "versions": [{"version": "v1", "created": "Sat, 23 Sep 2017 17:35:25 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 17:14:11 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Chu", "Lingyang", ""], ["Wang", "Zhefeng", ""], ["Pei", "Jian", ""], ["Zhang", "Yanyan", ""], ["Yang", "Yu", ""], ["Chen", "Enhong", ""]]}, {"id": "1709.08312", "submitter": "Afroza Sultana", "authors": "Afroza Sultana and Chengkai Li", "title": "Continuous Monitoring of Pareto Frontiers on Partially Ordered\n  Attributes for Many Users", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of continuous object dissemination---given a large\nnumber of users and continuously arriving new objects, deliver an object to all\nusers who prefer the object. Many real world applications analyze users'\npreferences for effective object dissemination. For continuously arriving\nobjects, timely finding users who prefer a new object is challenging. In this\npaper, we consider an append-only table of objects with multiple attributes and\nusers' preferences on individual attributes are modeled as strict partial\norders. An object is preferred by a user if it belongs to the Pareto frontier\nwith respect to the user's partial orders. Users' preferences can be similar.\nExploiting shared computation across similar preferences of different users, we\ndesign algorithms to find target users of a new object. In order to find users\nof similar preferences, we study the novel problem of clustering users'\npreferences that are represented as partial orders. We also present an\napproximate solution of the problem of finding target users which is more\nefficient than the exact one while ensuring sufficient accuracy. Furthermore,\nwe extend the algorithms to operate under the semantics of sliding window. We\npresent the results from comprehensive experiments for evaluating the\nefficiency and effectiveness of the proposed techniques.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 04:29:54 GMT"}, {"version": "v2", "created": "Fri, 13 Oct 2017 21:46:17 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Sultana", "Afroza", ""], ["Li", "Chengkai", ""]]}, {"id": "1709.08359", "submitter": "Robert Brijder", "authors": "Robert Brijder, Floris Geerts, Jan Van den Bussche, Timmy Weerwag", "title": "On the expressive power of query languages for matrices", "comments": "21 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the expressive power of $\\mathsf{MATLANG}$, a formal language\nfor matrix manipulation based on common matrix operations and linear algebra.\nThe language can be extended with the operation $\\mathsf{inv}$ of inverting a\nmatrix. In $\\mathsf{MATLANG}+\\mathsf{inv}$ we can compute the transitive\nclosure of directed graphs, whereas we show that this is not possible without\ninversion. Indeed we show that the basic language can be simulated in the\nrelational algebra with arithmetic operations, grouping, and summation. We also\nconsider an operation $\\mathsf{eigen}$ for diagonalizing a matrix, which is\ndefined so that different eigenvectors returned for a same eigenvalue are\northogonal. We show that $\\mathsf{inv}$ can be expressed in\n$\\mathsf{MATLANG}+\\mathsf{eigen}$. We put forward the open question whether\nthere are boolean queries about matrices, or generic queries about graphs,\nexpressible in $\\mathsf{MATLANG} + \\mathsf{eigen}$ but not in\n$\\mathsf{MATLANG}+\\mathsf{inv}$. The evaluation problem for $\\mathsf{MATLANG} +\n\\mathsf{eigen}$ is shown to be complete for the complexity class $\\exists\n\\mathbf{R}$.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 08:05:00 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Brijder", "Robert", ""], ["Geerts", "Floris", ""], ["Bussche", "Jan Van den", ""], ["Weerwag", "Timmy", ""]]}, {"id": "1709.08470", "submitter": "Yuan-Yen Tai", "authors": "Yuan-Yen Tai", "title": "An efficient clustering algorithm from the measure of local Gaussian\n  distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, I will introduce a fast and novel clustering algorithm based\non Gaussian distribution and it can guarantee the separation of each cluster\ncentroid as a given parameter, $d_s$. The worst run time complexity of this\nalgorithm is approximately $\\sim$O$(T\\times N \\times \\log(N))$ where $T$ is the\niteration steps and $N$ is the number of features.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 15:39:03 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2019 14:07:45 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Tai", "Yuan-Yen", ""]]}, {"id": "1709.08880", "submitter": "Noreddine Gherabi", "authors": "Noreddine Gherabi, Abdelhadi Daoui, and Abderrahim Marzouk", "title": "An enhanced method to compute the similarity between concepts of\n  ontology", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-64719-7", "report-no": null, "categories": "cs.DB cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the use of ontologies in several domains such as semantic web,\ninformation retrieval, artificial intelligence, the concept of similarity\nmeasuring has become a very important domain of research. Therefore, in the\ncurrent paper, we propose our method of similarity measuring which uses the\nDijkstra algorithm to define and compute the shortest path. Then, we use this\none to compute the semantic distance between two concepts defined in the same\nhierarchy of ontology. Afterward, we base on this result to compute the\nsemantic similarity. Finally, we present an experimental comparison between our\nmethod and other methods of similarity measuring.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 08:18:15 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Gherabi", "Noreddine", ""], ["Daoui", "Abdelhadi", ""], ["Marzouk", "Abderrahim", ""]]}, {"id": "1709.08990", "submitter": "Daniel Lemire", "authors": "Daniel Lemire, Nathan Kurz, Christoph Rupp", "title": "Stream VByte: Faster Byte-Oriented Integer Compression", "comments": null, "journal-ref": "Information Processing Letters 130, February 2018, Pages 1-6", "doi": "10.1016/j.ipl.2017.09.011", "report-no": null, "categories": "cs.IR cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Arrays of integers are often compressed in search engines. Though there are\nmany ways to compress integers, we are interested in the popular byte-oriented\ninteger compression techniques (e.g., VByte or Google's Varint-GB). They are\nappealing due to their simplicity and engineering convenience. Amazon's\nvarint-G8IU is one of the fastest byte-oriented compression technique published\nso far. It makes judicious use of the powerful single-instruction-multiple-data\n(SIMD) instructions available in commodity processors. To surpass varint-G8IU,\nwe present Stream VByte, a novel byte-oriented compression technique that\nseparates the control stream from the encoded data. Like varint-G8IU, Stream\nVByte is well suited for SIMD instructions. We show that Stream VByte decoding\ncan be up to twice as fast as varint-G8IU decoding over real data sets. In this\nsense, Stream VByte establishes new speed records for byte-oriented integer\ncompression, at times exceeding the speed of the memcpy function. On a 3.4GHz\nHaswell processor, it decodes more than 4 billion differentially-coded integers\nper second from RAM to L1 cache.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 14:45:20 GMT"}, {"version": "v2", "created": "Wed, 27 Sep 2017 19:53:20 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Lemire", "Daniel", ""], ["Kurz", "Nathan", ""], ["Rupp", "Christoph", ""]]}, {"id": "1709.09003", "submitter": "Fernando Martinez Plumed", "authors": "Fernando Mart\\'inez-Plumed, Lidia Contreras-Ochando, C\\`esar Ferri,\n  Peter Flach, Jos\\'e Hern\\'andez-Orallo, Meelis Kull, Nicolas Lachiche,\n  Mar\\'ia Jos\\'e Ram\\'irez-Quintana", "title": "CASP-DM: Context Aware Standard Process for Data Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an extension of the Cross Industry Standard Process for Data\nMining (CRISPDM) which addresses specific challenges of machine learning and\ndata mining for context and model reuse handling. This new general\ncontext-aware process model is mapped with CRISP-DM reference model proposing\nsome new or enhanced outputs.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 07:59:41 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Mart\u00ednez-Plumed", "Fernando", ""], ["Contreras-Ochando", "Lidia", ""], ["Ferri", "C\u00e8sar", ""], ["Flach", "Peter", ""], ["Hern\u00e1ndez-Orallo", "Jos\u00e9", ""], ["Kull", "Meelis", ""], ["Lachiche", "Nicolas", ""], ["Ram\u00edrez-Quintana", "Mar\u00eda Jos\u00e9", ""]]}, {"id": "1709.09099", "submitter": "Chiwan Park", "authors": "Chiwan Park, Ha-Myung Park, Minji Yoon, U Kang", "title": "PMV: Pre-partitioned Generalized Matrix-Vector Multiplication for\n  Scalable Graph Mining", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we analyze enormous networks including the Web and social networks\nwhich have hundreds of billions of nodes and edges? Network analyses have been\nconducted by various graph mining methods including shortest path computation,\nPageRank, connected component computation, random walk with restart, etc. These\ngraph mining methods can be expressed as generalized matrix-vector\nmultiplication which consists of few operations inspired by typical\nmatrix-vector multiplication. Recently, several graph processing systems based\non matrix-vector multiplication or their own primitives have been proposed to\ndeal with large graphs; however, they all have failed on Web-scale graphs due\nto insufficient memory space or the lack of consideration for I/O costs. In\nthis paper, we propose PMV (Pre-partitioned generalized Matrix-Vector\nmultiplication), a scalable distributed graph mining method based on\ngeneralized matrix-vector multiplication on distributed systems. PMV\nsignificantly decreases the communication cost, which is the main bottleneck of\ndistributed systems, by partitioning the input graph in advance and judiciously\napplying execution strategies based on the density of the pre-partitioned\nsub-matrices. Experiments show that PMV succeeds in processing up to 16x larger\ngraphs than existing distributed memory-based graph mining methods, and\nrequires 9x less time than previous disk-based graph mining methods by reducing\nI/O costs significantly.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 15:53:15 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Park", "Chiwan", ""], ["Park", "Ha-Myung", ""], ["Yoon", "Minji", ""], ["Kang", "U", ""]]}, {"id": "1709.09287", "submitter": "Kaiyu Feng", "authors": "Kaiyu Feng, Tao Guo, Gao Cong, Sourav S. Bhowmicks, Shuai Ma", "title": "SURGE: Continuous Detection of Bursty Regions Over a Stream of Spatial\n  Objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the proliferation of mobile devices and location-based services,\ncontinuous generation of massive volume of streaming spatial objects (i.e.,\ngeo-tagged data) opens up new opportunities to address real-world problems by\nanalyzing them. In this paper, we present a novel continuous bursty region\ndetection problem that aims to continuously detect a bursty region of a given\nsize in a specified geographical area from a stream of spatial objects.\nSpecifically, a bursty region shows maximum spike in the number of spatial\nobjects in a given time window. The problem is useful in addressing several\nreal-world challenges such as surge pricing problem in online transportation\nand disease outbreak detection. To solve the problem, we propose an exact\nsolution and two approximate solutions, and the approximation ratio is\n$\\frac{1-\\alpha}{4}$ in terms of the burst score, where $\\alpha$ is a parameter\nto control the burst score. We further extend these solutions to support\ndetection of top-$k$ bursty regions. Extensive experiments with real-world data\nare conducted to demonstrate the efficiency and effectiveness of our solutions.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 23:58:29 GMT"}, {"version": "v2", "created": "Thu, 28 Sep 2017 06:36:04 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Feng", "Kaiyu", ""], ["Guo", "Tao", ""], ["Cong", "Gao", ""], ["Bhowmicks", "Sourav S.", ""], ["Ma", "Shuai", ""]]}, {"id": "1709.09471", "submitter": "Rong Zhu", "authors": "Rong Zhu, Zhaonian Zou, Jianzhong Li", "title": "Diversified Coherent Core Search on Multi-Layer Graphs", "comments": "Full Version, 17 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining dense subgraphs on multi-layer graphs is an interesting problem, which\nhas witnessed lots of applications in practice. To overcome the limitations of\nthe quasi-clique-based approach, we propose d-coherent core (d-CC), a new\nnotion of dense subgraph on multi-layer graphs, which has several elegant\nproperties. We formalize the diversified coherent core search (DCCS) problem,\nwhich finds k d-CCs that can cover the largest number of vertices. We propose a\ngreedy algorithm with an approximation ratio of 1 - 1/e and two search\nalgorithms with an approximation ratio of 1/4. The experiments verify that the\nsearch algorithms are faster than the greedy algorithm and produce comparably\ngood results as the greedy algorithm in practice. As opposed to the\nquasi-clique-based approach, our DCCS algorithms can fast detect larger dense\nsubgraphs that cover most of the quasi-clique-based results.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 12:23:54 GMT"}, {"version": "v2", "created": "Thu, 28 Sep 2017 04:20:38 GMT"}, {"version": "v3", "created": "Sun, 1 Oct 2017 02:09:45 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Zhu", "Rong", ""], ["Zou", "Zhaonian", ""], ["Li", "Jianzhong", ""]]}, {"id": "1709.09575", "submitter": "Michael Wehner", "authors": "Eli Dart, Michael F. Wehner, Prabhat", "title": "An Assessment of Data Transfer Performance for Large-Scale Climate Data\n  Analysis and Recommendations for the Data Infrastructure for CMIP6", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We document the data transfer workflow, data transfer performance, and other\naspects of staging approximately 56 terabytes of climate model output data from\nthe distributed Coupled Model Intercomparison Project (CMIP5) archive to the\nNational Energy Research Supercomputing Center (NERSC) at the Lawrence Berkeley\nNational Laboratory required for tracking and characterizing extratropical\nstorms, a phenomena of importance in the mid-latitudes. We present this\nanalysis to illustrate the current challenges in assembling multi-model data\nsets at major computing facilities for large-scale studies of CMIP5 data.\nBecause of the larger archive size of the upcoming CMIP6 phase of model\nintercomparison, we expect such data transfers to become of increasing\nimportance, and perhaps of routine necessity. We find that data transfer rates\nusing the ESGF are often slower than what is typically available to US\nresidences and that there is significant room for improvement in the data\ntransfer capabilities of the ESGF portal and data centers both in terms of\nworkflow mechanics and in data transfer performance. We believe performance\nimprovements of at least an order of magnitude are within technical reach using\ncurrent best practices, as illustrated by the performance we achieved in\ntransferring the complete raw data set between two high performance computing\nfacilities. To achieve these performance improvements, we recommend: that\ncurrent best practices (such as the Science DMZ model) be applied to the data\nservers and networks at ESGF data centers; that sufficient financial and human\nresources be devoted at the ESGF data centers for systems and network\nengineering tasks to support high performance data movement; and that\nperformance metrics for data transfer between ESGF data centers and major\ncomputing facilities used for climate data analysis be established, regularly\ntested, and published.\n", "versions": [{"version": "v1", "created": "Sat, 26 Aug 2017 03:31:29 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Dart", "Eli", ""], ["Wehner", "Michael F.", ""], ["Prabhat", "", ""]]}, {"id": "1709.09850", "submitter": "Vince Grolmusz", "authors": "Balazs Szalkai and Vince Grolmusz", "title": "SCARF: A Biomedical Association Rule Finding Webserver", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of enormous datasets with missing data entries is a standard\ntask in biological and medical data processing. Large-scale, multi-institution\nclinical studies are the typical examples of such datasets. These sets make\npossible the search for multi-parametric relations since from the plenty of the\ndata one is likely to find a satisfying number of subjects with the required\nparameter ensembles. Specifically, finding combinatorial biomarkers for some\ngiven condition also needs a very large dataset to analyze. For this goal,\nstatistical regression analysis is not the preferred tool of choice, since (i)\nthe {\\em a priori} knowledge of the parameter-sets to analyze is missing, and\n(ii) typically relatively few subjects have the interesting parameter-value\nensembles for the analysis. For fast and automatic multi-parametric relation\ndiscovery association-rule finding tools are used for more than two decades in\nthe data-mining community. Here we present the SCARF webserver for {\\em\ngeneralized} association rule mining. Association rules are of the form: $a$\nAND $b$ AND ... AND $x \\rightarrow y$, meaning that the presence of properties\n$a$ AND $b$ AND ... AND $x$ implies property $y$; our algorithm finds\ngeneralized association rules, since it also finds logical disjunctions (i.e.,\nORs) at the left-hand side, allowing the discovery of more complex rules in a\nmore compressed form in the database. This feature also helps reducing the\ntypically very large result-tables of such studies, since allowing ORs in the\nleft-hand side of a single rule could include dozens of classical rules. The\ncapabilities of the SCARF algorithm were demonstrated in mining the Alzheimer's\ndatabase of the Coalition Against Major Diseases (CAMD) in our recent\npublication (Archives of Gerontology and Geriatrics Vol. 73, pp. 300-307,\n2017). Here we describe the webserver implementation of the algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 08:47:04 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Szalkai", "Balazs", ""], ["Grolmusz", "Vince", ""]]}, {"id": "1709.10039", "submitter": "Christoph Berkholz", "authors": "Christoph Berkholz, Jens Keppeler, Nicole Schweikardt", "title": "Answering UCQs under updates and in the presence of integrity\n  constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the query evaluation problem for fixed queries over fully\ndynamic databases where tuples can be inserted or deleted. The task is to\ndesign a dynamic data structure that can immediately report the new result of a\nfixed query after every database update. We consider unions of conjunctive\nqueries (UCQs) and focus on the query evaluation tasks testing (decide whether\nan input tuple belongs to the query result), enumeration (enumerate, without\nrepetition, all tuples in the query result), and counting (output the number of\ntuples in the query result).\n  We identify three increasingly restrictive classes of UCQs which we call\nt-hierarchical, q-hierarchical, and exhaustively q-hierarchical UCQs. Our main\nresults provide the following dichotomies: If the query's homomorphic core is\nt-hierarchical (q-hierarchical, exhaustively q-hierarchical), then the testing\n(enumeration, counting) problem can be solved with constant update time and\nconstant testing time (delay, counting time). Otherwise, it cannot be solved\nwith sublinear update time and sublinear testing time (delay, counting time),\nunless the OV-conjecture and/or the OMv-conjecture fails.\n  We also study the complexity of query evaluation in the dynamic setting in\nthe presence of integrity constraints, and we obtain according dichotomy\nresults for the special case of small domain constraints (i.e., constraints\nwhich state that all values in a particular column of a relation belong to a\nfixed domain of constant size).\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 16:15:37 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Berkholz", "Christoph", ""], ["Keppeler", "Jens", ""], ["Schweikardt", "Nicole", ""]]}, {"id": "1709.10072", "submitter": "Qiqi Yan", "authors": "Mukund Sundararajan and Qiqi Yan", "title": "A Simple and Efficient MapReduce Algorithm for Data Cube Materialization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data cube materialization is a classical database operator introduced in Gray\net al.~(Data Mining and Knowledge Discovery, Vol.~1), which is critical for\nmany analysis tasks. Nandi et al.~(Transactions on Knowledge and Data\nEngineering, Vol.~6) first studied cube materialization for large scale\ndatasets using the MapReduce framework, and proposed a sophisticated\nmodification of a simple broadcast algorithm to handle a dataset with a 216GB\ncube size within 25 minutes with 2k machines in 2012. We take a different\napproach, and propose a simple MapReduce algorithm which (1) minimizes the\ntotal number of copy-add operations, (2) leverages locality of computation, and\n(3) balances work evenly across machines. As a result, the algorithm shows\nexcellent performance, and materialized a real dataset with a cube size of\n35.0G tuples and 1.75T bytes in 54 minutes, with 0.4k machines in 2014.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 17:26:32 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Sundararajan", "Mukund", ""], ["Yan", "Qiqi", ""]]}, {"id": "1709.10436", "submitter": "Dong Deng", "authors": "Dong Deng, Wenbo Tao, Ziawasch Abedjan, Ahmed Elmagarmid, Guoliang Li,\n  Ihab F. Ilyas, Samuel Madden, Mourad Ouzzani, Michael Stonebraker, Nan Tang", "title": "Unsupervised String Transformation Learning for Entity Consolidation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data integration has been a long-standing challenge in data management with\nmany applications. A key step in data integration is entity consolidation. It\ntakes a collection of clusters of duplicate records as input and produces a\nsingle \"golden record\" for each cluster, which contains the canonical value for\neach attribute. Truth discovery and data fusion methods, as well as Master Data\nManagement (MDM) systems, can be used for entity consolidation. However, to\nachieve better results, the variant values (i.e., values that are logically the\nsame with different formats) in the clusters need to be consolidated before\napplying these methods.\n  For this purpose, we propose a data-driven method to standardize the variant\nvalues based on two observations: (1) the variant values usually can be\ntransformed to the same representation (e.g., \"Mary Lee\" and \"Lee, Mary\") and\n(2) the same transformation often appears repeatedly across different clusters\n(e.g., transpose the first and last name). Our approach first uses an\nunsupervised method to generate groups of value pairs that can be transformed\nin the same way (i.e., they share a transformation). Then the groups are\npresented to a human for verification and the approved ones are used to\nstandardize the data. In a real-world dataset with 17,497 records, our method\nachieved 75% recall and 99.5% precision in standardizing variant values by\nasking a human 100 yes/no questions, which completely outperformed a state of\nthe art data wrangling tool.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 14:48:56 GMT"}, {"version": "v2", "created": "Mon, 13 Nov 2017 23:04:24 GMT"}, {"version": "v3", "created": "Mon, 18 Dec 2017 02:04:12 GMT"}, {"version": "v4", "created": "Mon, 30 Jul 2018 06:08:30 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Deng", "Dong", ""], ["Tao", "Wenbo", ""], ["Abedjan", "Ziawasch", ""], ["Elmagarmid", "Ahmed", ""], ["Li", "Guoliang", ""], ["Ilyas", "Ihab F.", ""], ["Madden", "Samuel", ""], ["Ouzzani", "Mourad", ""], ["Stonebraker", "Michael", ""], ["Tang", "Nan", ""]]}]