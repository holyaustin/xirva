[{"id": "0803.0032", "submitter": "Srivatsava Ranjit Ganta", "authors": "Srivatsava Ranjit Ganta, Shiva Prasad Kasiviswanathan, Adam Smith", "title": "Composition Attacks and Auxiliary Information in Data Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy is an increasingly important aspect of data publishing. Reasoning\nabout privacy, however, is fraught with pitfalls. One of the most significant\nis the auxiliary information (also called external knowledge, background\nknowledge, or side information) that an adversary gleans from other channels\nsuch as the web, public records, or domain knowledge. This paper explores how\none can reason about privacy in the face of rich, realistic sources of\nauxiliary information. Specifically, we investigate the effectiveness of\ncurrent anonymization schemes in preserving privacy when multiple organizations\nindependently release anonymized data about overlapping populations. 1. We\ninvestigate composition attacks, in which an adversary uses independent\nanonymized releases to breach privacy. We explain why recently proposed models\nof limited auxiliary information fail to capture composition attacks. Our\nexperiments demonstrate that even a simple instance of a composition attack can\nbreach privacy in practice, for a large class of currently proposed techniques.\nThe class includes k-anonymity and several recent variants. 2. On a more\npositive note, certain randomization-based notions of privacy (such as\ndifferential privacy) provably resist composition attacks and, in fact, the use\nof arbitrary side information. This resistance enables stand-alone design of\nanonymization schemes, without the need for explicitly keeping track of other\nreleases. We provide a precise formulation of this property, and prove that an\nimportant class of relaxations of differential privacy also satisfy the\nproperty. This significantly enlarges the class of protocols known to enable\nmodular design.\n", "versions": [{"version": "v1", "created": "Sat, 1 Mar 2008 00:36:12 GMT"}, {"version": "v2", "created": "Mon, 31 Mar 2008 16:23:40 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Ganta", "Srivatsava Ranjit", ""], ["Kasiviswanathan", "Shiva Prasad", ""], ["Smith", "Adam", ""]]}, {"id": "0803.0450", "submitter": "Debprakash Patnaik", "authors": "Debprakash Patnaik (Electical Engg. Dept., Indian Institute of\n  Science, Bangalore), and P. S. Sastry (Electrical Engg. Dept., Indian\n  Institute of Science, Bangalore), and K. P. Unnikrishnan (General Motors R&D,\n  Warren)", "title": "Inferring Neuronal Network Connectivity from Spike Data: A Temporal\n  Datamining Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the functioning of a neural system in terms of its underlying\ncircuitry is an important problem in neuroscience. Recent developments in\nelectrophysiology and imaging allow one to simultaneously record activities of\nhundreds of neurons. Inferring the underlying neuronal connectivity patterns\nfrom such multi-neuronal spike train data streams is a challenging statistical\nand computational problem. This task involves finding significant temporal\npatterns from vast amounts of symbolic time series data. In this paper we show\nthat the frequent episode mining methods from the field of temporal data mining\ncan be very useful in this context. In the frequent episode discovery\nframework, the data is viewed as a sequence of events, each of which is\ncharacterized by an event type and its time of occurrence and episodes are\ncertain types of temporal patterns in such data. Here we show that, using the\nset of discovered frequent episodes from multi-neuronal data, one can infer\ndifferent types of connectivity patterns in the neural system that generated\nit. For this purpose, we introduce the notion of mining for frequent episodes\nunder certain temporal constraints; the structure of these temporal constraints\nis motivated by the application. We present algorithms for discovering serial\nand parallel episodes under these temporal constraints. Through extensive\nsimulation studies we demonstrate that these methods are useful for unearthing\npatterns of neuronal network connectivity.\n", "versions": [{"version": "v1", "created": "Tue, 4 Mar 2008 14:11:38 GMT"}, {"version": "v2", "created": "Tue, 11 Mar 2008 02:24:13 GMT"}], "update_date": "2008-03-11", "authors_parsed": [["Patnaik", "Debprakash", "", "Electical Engg. Dept., Indian Institute of\n  Science, Bangalore"], ["Sastry", "P. S.", "", "Electrical Engg. Dept., Indian\n  Institute of Science, Bangalore"], ["Unnikrishnan", "K. P.", "", "General Motors R&D,\n  Warren"]]}, {"id": "0803.0924", "submitter": "Shiva Kasiviswanathan", "authors": "Shiva Prasad Kasiviswanathan, Homin K. Lee, Kobbi Nissim, Sofya\n  Raskhodnikova, and Adam Smith", "title": "What Can We Learn Privately?", "comments": "35 pages, 2 figures", "journal-ref": "SIAM Journal of Computing 40(3) (2011) 793-826", "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning problems form an important category of computational tasks that\ngeneralizes many of the computations researchers apply to large real-life data\nsets. We ask: what concept classes can be learned privately, namely, by an\nalgorithm whose output does not depend too heavily on any one input or specific\ntraining example? More precisely, we investigate learning algorithms that\nsatisfy differential privacy, a notion that provides strong confidentiality\nguarantees in contexts where aggregate information is released about a database\ncontaining sensitive information about individuals. We demonstrate that,\nignoring computational constraints, it is possible to privately agnostically\nlearn any concept class using a sample size approximately logarithmic in the\ncardinality of the concept class. Therefore, almost anything learnable is\nlearnable privately: specifically, if a concept class is learnable by a\n(non-private) algorithm with polynomial sample complexity and output size, then\nit can be learned privately using a polynomial number of samples. We also\npresent a computationally efficient private PAC learner for the class of parity\nfunctions. Local (or randomized response) algorithms are a practical class of\nprivate algorithms that have received extensive investigation. We provide a\nprecise characterization of local private learning algorithms. We show that a\nconcept class is learnable by a local algorithm if and only if it is learnable\nin the statistical query (SQ) model. Finally, we present a separation between\nthe power of interactive and noninteractive local learning algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 6 Mar 2008 17:50:07 GMT"}, {"version": "v2", "created": "Mon, 14 Apr 2008 16:18:44 GMT"}, {"version": "v3", "created": "Fri, 19 Feb 2010 01:47:02 GMT"}], "update_date": "2012-10-10", "authors_parsed": [["Kasiviswanathan", "Shiva Prasad", ""], ["Lee", "Homin K.", ""], ["Nissim", "Kobbi", ""], ["Raskhodnikova", "Sofya", ""], ["Smith", "Adam", ""]]}, {"id": "0803.0954", "submitter": "Michael Hahsler", "authors": "Michael Hahsler, Christian Buchta, and Kurt Hornik", "title": "Selective association rule generation", "comments": null, "journal-ref": "Computational Statistics, 2007. Online First, Published: 25 July\n  2007", "doi": "10.1007/s00180-007-0062-z", "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining association rules is a popular and well researched method for\ndiscovering interesting relations between variables in large databases. A\npractical problem is that at medium to low support values often a large number\nof frequent itemsets and an even larger number of association rules are found\nin a database. A widely used approach is to gradually increase minimum support\nand minimum confidence or to filter the found rules using increasingly strict\nconstraints on additional measures of interestingness until the set of rules\nfound is reduced to a manageable size. In this paper we describe a different\napproach which is based on the idea to first define a set of ``interesting''\nitemsets (e.g., by a mixture of mining and expert knowledge) and then, in a\nsecond step to selectively generate rules for only these itemsets. The main\nadvantage of this approach over increasing thresholds or filtering rules is\nthat the number of rules found is significantly reduced while at the same time\nit is not necessary to increase the support and confidence thresholds which\nmight lead to missing important information in the database.\n", "versions": [{"version": "v1", "created": "Thu, 6 Mar 2008 19:43:35 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Hahsler", "Michael", ""], ["Buchta", "Christian", ""], ["Hornik", "Kurt", ""]]}, {"id": "0803.0966", "submitter": "Michael Hahsler", "authors": "Michael Hahsler and Kurt Hornik", "title": "New probabilistic interest measures for association rules", "comments": null, "journal-ref": "Intelligent Data Analysis, 11(5):437-455, 2007", "doi": null, "report-no": null, "categories": "cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining association rules is an important technique for discovering meaningful\npatterns in transaction databases. Many different measures of interestingness\nhave been proposed for association rules. However, these measures fail to take\nthe probabilistic properties of the mined data into account. In this paper, we\nstart with presenting a simple probabilistic framework for transaction data\nwhich can be used to simulate transaction data when no associations are\npresent. We use such data and a real-world database from a grocery outlet to\nexplore the behavior of confidence and lift, two popular interest measures used\nfor rule mining. The results show that confidence is systematically influenced\nby the frequency of the items in the left hand side of rules and that lift\nperforms poorly to filter random noise in transaction data. Based on the\nprobabilistic framework we develop two new interest measures, hyper-lift and\nhyper-confidence, which can be used to filter or order mined association rules.\nThe new measures show significantly better performance than lift for\napplications where spurious rules are problematic.\n", "versions": [{"version": "v1", "created": "Thu, 6 Mar 2008 20:17:19 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Hahsler", "Michael", ""], ["Hornik", "Kurt", ""]]}, {"id": "0803.1555", "submitter": "Bart Moelans", "authors": "Bart Kuijpers, Vanessa Lemmens, Bart Moelans and Karl Tuyls", "title": "Privacy Preserving ID3 over Horizontally, Vertically and Grid\n  Partitioned Data", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider privacy preserving decision tree induction via ID3 in the case\nwhere the training data is horizontally or vertically distributed. Furthermore,\nwe consider the same problem in the case where the data is both horizontally\nand vertically distributed, a situation we refer to as grid partitioned data.\nWe give an algorithm for privacy preserving ID3 over horizontally partitioned\ndata involving more than two parties. For grid partitioned data, we discuss two\ndifferent evaluation methods for preserving privacy ID3, namely, first merging\nhorizontally and developing vertically or first merging vertically and next\ndeveloping horizontally. Next to introducing privacy preserving data mining\nover grid-partitioned data, the main contribution of this paper is that we\nshow, by means of a complexity analysis that the former evaluation method is\nthe more efficient.\n", "versions": [{"version": "v1", "created": "Tue, 11 Mar 2008 11:18:52 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Kuijpers", "Bart", ""], ["Lemmens", "Vanessa", ""], ["Moelans", "Bart", ""], ["Tuyls", "Karl", ""]]}, {"id": "0803.2212", "submitter": "Dan Olteanu", "authors": "Christoph Koch and Dan Olteanu", "title": "Conditioning Probabilistic Databases", "comments": "13 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Past research on probabilistic databases has studied the problem of answering\nqueries on a static database. Application scenarios of probabilistic databases\nhowever often involve the conditioning of a database using additional\ninformation in the form of new evidence. The conditioning problem is thus to\ntransform a probabilistic database of priors into a posterior probabilistic\ndatabase which is materialized for subsequent query processing or further\nrefinement. It turns out that the conditioning problem is closely related to\nthe problem of computing exact tuple confidence values.\n  It is known that exact confidence computation is an NP-hard problem. This has\nled researchers to consider approximation techniques for confidence\ncomputation. However, neither conditioning nor exact confidence computation can\nbe solved using such techniques.\n  In this paper we present efficient techniques for both problems. We study\nseveral problem decomposition methods and heuristics that are based on the most\nsuccessful search techniques from constraint satisfaction, such as the\nDavis-Putnam algorithm. We complement this with a thorough experimental\nevaluation of the algorithms proposed. Our experiments show that our exact\nalgorithms scale well to realistic database sizes and can in some scenarios\ncompete with the most efficient previous approximation algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 14 Mar 2008 17:23:34 GMT"}, {"version": "v2", "created": "Mon, 16 Jun 2008 16:20:06 GMT"}], "update_date": "2008-06-16", "authors_parsed": [["Koch", "Christoph", ""], ["Olteanu", "Dan", ""]]}, {"id": "0803.2559", "submitter": "James Bailey", "authors": "James Bailey and Guozhu Dong and Anthony Widjaja To", "title": "Logical Queries over Views: Decidability and Expressiveness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of deciding satisfiability of first order logic queries\nover views, our aim being to delimit the boundary between the decidable and the\nundecidable fragments of this language. Views currently occupy a central place\nin database research, due to their role in applications such as information\nintegration and data warehousing. Our main result is the identification of a\ndecidable class of first order queries over unary conjunctive views that\ngeneralises the decidability of the classical class of first order sentences\nover unary relations, known as the Lowenheim class. We then demonstrate how\nvarious extensions of this class lead to undecidability and also provide some\nexpressivity results. Besides its theoretical interest, our new decidable class\nis potentially interesting for use in applications such as deciding implication\nof complex dependencies, analysis of a restricted class of active database\nrules, and ontology reasoning.\n", "versions": [{"version": "v1", "created": "Tue, 18 Mar 2008 02:07:12 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Bailey", "James", ""], ["Dong", "Guozhu", ""], ["To", "Anthony Widjaja", ""]]}, {"id": "0803.3224", "submitter": "Michael Hahsler", "authors": "Michael Hahsler", "title": "A Model-Based Frequency Constraint for Mining Associations from\n  Transaction Data", "comments": null, "journal-ref": "Michael Hahsler. A model-based frequency constraint for mining\n  associations from transaction data. Data Mining and Knowledge Discovery,\n  13(2):137-166, September 2006", "doi": "10.1007/s10618-005-0026-2", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining frequent itemsets is a popular method for finding associated items in\ndatabases. For this method, support, the co-occurrence frequency of the items\nwhich form an association, is used as the primary indicator of the\nassociations's significance. A single user-specified support threshold is used\nto decided if associations should be further investigated. Support has some\nknown problems with rare items, favors shorter itemsets and sometimes produces\nmisleading associations.\n  In this paper we develop a novel model-based frequency constraint as an\nalternative to a single, user-specified minimum support. The constraint\nutilizes knowledge of the process generating transaction data by applying a\nsimple stochastic mixture model (the NB model) which allows for transaction\ndata's typically highly skewed item frequency distribution. A user-specified\nprecision threshold is used together with the model to find local frequency\nthresholds for groups of itemsets. Based on the constraint we develop the\nnotion of NB-frequent itemsets and adapt a mining algorithm to find all\nNB-frequent itemsets in a database. In experiments with publicly available\ntransaction databases we show that the new constraint provides improvements\nover a single minimum support threshold and that the precision threshold is\nmore robust and easier to set and interpret by the user.\n", "versions": [{"version": "v1", "created": "Fri, 21 Mar 2008 20:39:53 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Hahsler", "Michael", ""]]}, {"id": "0803.3404", "submitter": "Wesley Calvert", "authors": "Wesley Calvert and John E. Porter", "title": "Some results on $\\mathbb{R}$-computable structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This survey paper examines the effective model theory obtained with the BSS\nmodel of real number computation. It treats the following topics: computable\nordinals, satisfaction of computable infinitary formulas, forcing as a\nconstruction technique, effective categoricity, effective topology, and\nrelations with other models for the effective theory of uncountable structures.\n", "versions": [{"version": "v1", "created": "Mon, 24 Mar 2008 13:59:53 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2009 17:03:07 GMT"}], "update_date": "2009-06-09", "authors_parsed": [["Calvert", "Wesley", ""], ["Porter", "John E.", ""]]}, {"id": "0803.3693", "submitter": "Rasmus Pagh", "authors": "Martin Dietzfelbinger and Rasmus Pagh", "title": "Succinct Data Structures for Retrieval and Approximate Membership", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The retrieval problem is the problem of associating data with keys in a set.\nFormally, the data structure must store a function f: U ->{0,1}^r that has\nspecified values on the elements of a given set S, a subset of U, |S|=n, but\nmay have any value on elements outside S. Minimal perfect hashing makes it\npossible to avoid storing the set S, but this induces a space overhead of\nTheta(n) bits in addition to the nr bits needed for function values. In this\npaper we show how to eliminate this overhead. Moreover, we show that for any k\nquery time O(k) can be achieved using space that is within a factor 1+e^{-k} of\noptimal, asymptotically for large n. If we allow logarithmic evaluation time,\nthe additive overhead can be reduced to O(log log n) bits whp. The time to\nconstruct the data structure is O(n), expected. A main technical ingredient is\nto utilize existing tight bounds on the probability of almost square random\nmatrices with rows of low weight to have full row rank. In addition to direct\nconstructions, we point out a close connection between retrieval structures and\nhash tables where keys are stored in an array and some kind of probing scheme\nis used. Further, we propose a general reduction that transfers the results on\nretrieval into analogous results on approximate membership, a problem\ntraditionally addressed using Bloom filters. Again, we show how to eliminate\nthe space overhead present in previously known methods, and get arbitrarily\nclose to the lower bound. The evaluation procedures of our data structures are\nextremely simple (similar to a Bloom filter). For the results stated above we\nassume free access to fully random hash functions. However, we show how to\njustify this assumption using extra space o(n) to simulate full randomness on a\nRAM.\n", "versions": [{"version": "v1", "created": "Wed, 26 Mar 2008 10:53:49 GMT"}], "update_date": "2008-03-27", "authors_parsed": [["Dietzfelbinger", "Martin", ""], ["Pagh", "Rasmus", ""]]}, {"id": "0803.3946", "submitter": "Shiva Kasiviswanathan", "authors": "Shiva Prasad Kasiviswanathan and Adam Smith", "title": "On the `Semantics' of Differential Privacy: A Bayesian Formulation", "comments": "Older version of this paper was titled: \"A Note on Differential\n  Privacy: Defining Resistance to Arbitrary Side Information\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy is a definition of \"privacy'\" for algorithms that\nanalyze and publish information about statistical databases. It is often\nclaimed that differential privacy provides guarantees against adversaries with\narbitrary side information. In this paper, we provide a precise formulation of\nthese guarantees in terms of the inferences drawn by a Bayesian adversary. We\nshow that this formulation is satisfied by both \"vanilla\" differential privacy\nas well as a relaxation known as (epsilon,delta)-differential privacy. Our\nformulation follows the ideas originally due to Dwork and McSherry [Dwork\n2006]. This paper is, to our knowledge, the first place such a formulation\nappears explicitly. The analysis of the relaxed definition is new to this\npaper, and provides some concrete guidance for setting parameters when using\n(epsilon,delta)-differential privacy.\n", "versions": [{"version": "v1", "created": "Thu, 27 Mar 2008 15:00:45 GMT"}, {"version": "v2", "created": "Sat, 21 Sep 2013 00:15:57 GMT"}, {"version": "v3", "created": "Tue, 22 Dec 2015 20:58:26 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["Kasiviswanathan", "Shiva Prasad", ""], ["Smith", "Adam", ""]]}]