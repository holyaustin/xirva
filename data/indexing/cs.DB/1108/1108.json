[{"id": "1108.0017", "submitter": "Parasaran Raman", "authors": "Jeff M. Phillips, Parasaran Raman, and Suresh Venkatasubramanian", "title": "Generating a Diverse Set of High-Quality Clusterings", "comments": "12 Pages, 5 Figures, 2nd MultiClust Workshop at ECML PKDD 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a new framework for generating multiple good quality partitions\n(clusterings) of a single data set. Our approach decomposes this problem into\ntwo components, generating many high-quality partitions, and then grouping\nthese partitions to obtain k representatives. The decomposition makes the\napproach extremely modular and allows us to optimize various criteria that\ncontrol the choice of representative partitions.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2011 21:07:51 GMT"}], "update_date": "2011-08-02", "authors_parsed": [["Phillips", "Jeff M.", ""], ["Raman", "Parasaran", ""], ["Venkatasubramanian", "Suresh", ""]]}, {"id": "1108.0186", "submitter": "Yuan Hong", "authors": "Yuan Hong, Jaideep Vaidya, Haibing Lu, Mingrui Wu", "title": "Differentially Private Search Log Sanitization with Optimal Output\n  Utility", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web search logs contain extremely sensitive data, as evidenced by the recent\nAOL incident. However, storing and analyzing search logs can be very useful for\nmany purposes (i.e. investigating human behavior). Thus, an important research\nquestion is how to privately sanitize search logs. Several search log\nanonymization techniques have been proposed with concrete privacy models.\nHowever, in all of these solutions, the output utility of the techniques is\nonly evaluated rather than being maximized in any fashion. Indeed, for\neffective search log anonymization, it is desirable to derive the optimal\n(maximum utility) output while meeting the privacy standard. In this paper, we\npropose utility-maximizing sanitization based on the rigorous privacy standard\nof differential privacy, in the context of search logs. Specifically, we\nutilize optimization models to maximize the output utility of the sanitization\nfor different applications, while ensuring that the production process\nsatisfies differential privacy. An added benefit is that our novel\nrandomization strategy ensures that the schema of the output is identical to\nthat of the input. A comprehensive evaluation on real search logs validates the\napproach and demonstrates its robustness and scalability.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jul 2011 15:55:48 GMT"}, {"version": "v2", "created": "Fri, 5 Aug 2011 22:12:25 GMT"}, {"version": "v3", "created": "Thu, 13 Oct 2011 01:45:36 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Hong", "Yuan", ""], ["Vaidya", "Jaideep", ""], ["Lu", "Haibing", ""], ["Wu", "Mingrui", ""]]}, {"id": "1108.0294", "submitter": "Ce Zhang", "authors": "Feng Niu, Ce Zhang, Christopher R\\'e, Jude Shavlik", "title": "Scaling Inference for Markov Logic with a Task-Decomposition Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications in large-scale knowledge base construction, we\nstudy the problem of scaling up a sophisticated statistical inference framework\ncalled Markov Logic Networks (MLNs). Our approach, Felix, uses the idea of\nLagrangian relaxation from mathematical programming to decompose a program into\nsmaller tasks while preserving the joint-inference property of the original\nMLN. The advantage is that we can use highly scalable specialized algorithms\nfor common tasks such as classification and coreference. We propose an\narchitecture to support Lagrangian relaxation in an RDBMS which we show enables\nscalable joint inference for MLNs. We empirically validate that Felix is\nsignificantly more scalable and efficient than prior approaches to MLN\ninference by constructing a knowledge base from 1.8M documents as part of the\nTAC challenge. We show that Felix scales and achieves state-of-the-art quality\nnumbers. In contrast, prior approaches do not scale even to a subset of the\ncorpus that is three orders of magnitude smaller.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2011 12:08:00 GMT"}, {"version": "v2", "created": "Tue, 6 Mar 2012 00:58:13 GMT"}, {"version": "v3", "created": "Wed, 7 Mar 2012 16:10:23 GMT"}, {"version": "v4", "created": "Mon, 12 Mar 2012 01:36:32 GMT"}], "update_date": "2012-03-13", "authors_parsed": [["Niu", "Feng", ""], ["Zhang", "Ce", ""], ["R\u00e9", "Christopher", ""], ["Shavlik", "Jude", ""]]}, {"id": "1108.0729", "submitter": "Eduardo Cunha de Almeida Eduardo Almeida", "authors": "Eduardo Cunha de Almeida", "title": "Estudo de Viabilidade de uma Plataforma de Baixo Custo para Data\n  Warehouse", "comments": "Masters dissertation, 90 pages, 2004. (Advisor: Marcos Sfair\n  Suny\\'e); Masters dissertation, Universidade Federal do Paran\\'a, 2004", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Often corporations need tools to improve their decision making in a\ncompetitive market. In general, these tools are based on data warehouse\nplatforms to mange and analyze large amounts of data. However, several of these\ncorporations do not have enough resources to buy such platforms because of the\nhigh cost. This work is dedicated to a feasibility study of a low cost platform\nto data warehouse. We consider as a low cost platform the use of open source\nsoftware like the PostgreSQL database system and the GNU/Linux operational\nsystem. We verify the feasibility of this platform by executing two benchmarks\nthat simulate a data warehouse workload. The workload reproduces a multi-user\nenvironment with the execution of complex queries, which executes:\naggregations, nested sub queries, multi joins, in-line views and more.\nConsidering the results we were able to highlight some problems on the\nPostgreSQL database system, and discuss improvements in the context of data\nwarehouse.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2011 02:41:02 GMT"}], "update_date": "2011-08-04", "authors_parsed": [["de Almeida", "Eduardo Cunha", ""]]}, {"id": "1108.0831", "submitter": "Alejandro Vaisman Dr.", "authors": "Pablo Bisceglia and Leticia Gomez and Alejandro Vaisman", "title": "Towards Spatio-Temporal SOLAP", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The integration of Geographic Information Systems (GIS) and On-Line\nAnalytical Processing (OLAP), denoted SOLAP, is aimed at exploring and\nanalyzing spatial data. In real-world SOLAP applications, spatial and\nnon-spatial data are subject to changes. In this paper we present a temporal\nquery language for SOLAP, called TPiet-QL, supporting so-called discrete\nchanges (for example, in land use or cadastral applications there are\nsituations where parcels are merged or split). TPiet-QL allows expressing\nintegrated GIS-OLAP queries in an scenario where spatial objects change across\ntime.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2011 12:36:23 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Bisceglia", "Pablo", ""], ["Gomez", "Leticia", ""], ["Vaisman", "Alejandro", ""]]}, {"id": "1108.0895", "submitter": "Ping Li", "authors": "Ping Li and Christian Konig", "title": "Accurate Estimators for Improving Minwise Hashing and b-Bit Minwise\n  Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DB cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minwise hashing is the standard technique in the context of search and\ndatabases for efficiently estimating set (e.g., high-dimensional 0/1 vector)\nsimilarities. Recently, b-bit minwise hashing was proposed which significantly\nimproves upon the original minwise hashing in practice by storing only the\nlowest b bits of each hashed value, as opposed to using 64 bits. b-bit hashing\nis particularly effective in applications which mainly concern sets of high\nsimilarities (e.g., the resemblance >0.5). However, there are other important\napplications in which not just pairs of high similarities matter. For example,\nmany learning algorithms require all pairwise similarities and it is expected\nthat only a small fraction of the pairs are similar. Furthermore, many\napplications care more about containment (e.g., how much one object is\ncontained by another object) than the resemblance. In this paper, we show that\nthe estimators for minwise hashing and b-bit minwise hashing used in the\ncurrent practice can be systematically improved and the improvements are most\nsignificant for set pairs of low resemblance and high containment.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2011 17:08:11 GMT"}], "update_date": "2011-08-04", "authors_parsed": [["Li", "Ping", ""], ["Konig", "Christian", ""]]}, {"id": "1108.1045", "submitter": "Asha T", "authors": "Asha.T, S. Natarajan and K.N.B. Murthy", "title": "A Data Mining Approach to the Diagnosis of Tuberculosis by Cascading\n  Clustering and Classification", "comments": "8 pages", "journal-ref": "Journal of computing, volume 3, issue 4,April 2011, ISSN 2151-9617", "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a methodology for the automated detection and classification\nof Tuberculosis(TB) is presented. Tuberculosis is a disease caused by\nmycobacterium which spreads through the air and attacks low immune bodies\neasily. Our methodology is based on clustering and classification that\nclassifies TB into two categories, Pulmonary Tuberculosis(PTB) and retroviral\nPTB(RPTB) that is those with Human Immunodeficiency Virus (HIV) infection.\nInitially K-means clustering is used to group the TB data into two clusters and\nassigns classes to clusters. Subsequently multiple different classification\nalgorithms are trained on the result set to build the final classifier model\nbased on K-fold cross validation method. This methodology is evaluated using\n700 raw TB data obtained from a city hospital. The best obtained accuracy was\n98.7% from support vector machine (SVM) compared to other classifiers. The\nproposed approach helps doctors in their diagnosis decisions and also in their\ntreatment planning procedures for different categories.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2011 10:52:51 GMT"}], "update_date": "2011-08-05", "authors_parsed": [["T", "Asha.", ""], ["Natarajan", "S.", ""], ["Murthy", "K. N. B.", ""]]}, {"id": "1108.1228", "submitter": "Dominic Tsang", "authors": "Dominic Tsang, Sanjay Chawla", "title": "An index for regular expression queries: Design and implementation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The like regular expression predicate has been part of the SQL standard since\nat least 1989. However, despite its popularity and wide usage, database vendors\nprovide only limited indexing support for regular expression queries which\nalmost always require a full table scan.\n  In this paper we propose a rigorous and robust approach for providing\nindexing support for regular expression queries. Our approach consists of\nformulating the indexing problem as a combinatorial optimization problem. We\nbegin with a database, abstracted as a collection of strings. From this data\nset we generate a query workload. The input to the optimization problem is the\ndatabase and the workload. The output is a set of multigrams (substrings) which\ncan be used as keys to records which satisfy the query workload. The multigrams\ncan then be integrated with the data structure (like B+ trees) to provide\nindexing support for the queries. We provide a deterministic and a randomized\napproximation algorithm (with provable guarantees) to solve the optimization\nproblem. Extensive experiments on synthetic data sets demonstrate that our\napproach is accurate and efficient.\n  We also present a case study on PROSITE patterns - which are complex regular\nexpression signatures for classes of proteins. Again, we are able to\ndemonstrate the utility of our indexing approach in terms of accuracy and\nefficiency. Thus, perhaps for the first time, there is a robust and practical\nindexing mechanism for an important class of database queries.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2011 23:14:25 GMT"}, {"version": "v2", "created": "Mon, 15 Aug 2011 13:50:00 GMT"}], "update_date": "2011-08-16", "authors_parsed": [["Tsang", "Dominic", ""], ["Chawla", "Sanjay", ""]]}, {"id": "1108.1378", "submitter": "Anis Ismail", "authors": "Anis Ismail, Mohamed Quafafou, Nicolas Durand and Mohammad Hajjar", "title": "An Efficient Architecture for Information Retrieval in P2P Context Using\n  Hypergraph", "comments": "2o pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Peer-to-peer (P2P) Data-sharing systems now generate a significant portion of\nInternet traffic. P2P systems have emerged as an accepted way to share enormous\nvolumes of data. Needs for widely distributed information systems supporting\nvirtual organizations have given rise to a new category of P2P systems called\nschema-based. In such systems each peer is a database management system in\nitself, ex-posing its own schema. In such a setting, the main objective is the\nefficient search across peer databases by processing each incoming query\nwithout overly consuming bandwidth. The usability of these systems depends on\nsuccessful techniques to find and retrieve data; however, efficient and\neffective routing of content-based queries is an emerging problem in P2P\nnetworks. This work was attended as an attempt to motivate the use of mining\nalgorithms in the P2P context may improve the significantly the efficiency of\nsuch methods. Our proposed method based respectively on combination of\nclustering with hypergraphs. We use ECCLAT to build approximate clustering and\ndiscovering meaningful clusters with slight overlapping. We use an algorithm\nMTMINER to extract all minimal transversals of a hypergraph (clusters) for\nquery routing. The set of clusters improves the robustness in queries routing\nmechanism and scalability in P2P Network. We compare the performance of our\nmethod with the baseline one considering the queries routing problem. Our\nexperimental results prove that our proposed methods generate impressive levels\nof performance and scalability with with respect to important criteria such as\nresponse time, precision and recall.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2011 18:26:33 GMT"}], "update_date": "2011-08-08", "authors_parsed": [["Ismail", "Anis", ""], ["Quafafou", "Mohamed", ""], ["Durand", "Nicolas", ""], ["Hajjar", "Mohammad", ""]]}, {"id": "1108.1925", "submitter": "Eric Peukert", "authors": "Eric Peukert, Julian Eberius, Erhard Rahm", "title": "Rule-based Construction of Matching Processes", "comments": "10 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mapping complex metadata structures is crucial in a number of domains such as\ndata integration, ontology alignment or model management. To speed up that\nprocess automatic matching systems were developed to compute mapping\nsuggestions that can be corrected by a user. However, constructing and tuning\nmatch strategies still requires a high manual effort by matching experts as\nwell as correct mappings to evaluate generated mappings. We therefore propose a\nself-configuring schema matching system that is able to automatically adapt to\nthe given mapping problem at hand. Our approach is based on analyzing the input\nschemas as well as intermediate matching results. A variety of matching rules\nuse the analysis results to automatically construct and adapt an underlying\nmatching process for a given match task. We comprehensively evaluate our\napproach on different mapping problems from the schema, ontology and model\nmanagement domains. The evaluation shows that our system is able to robustly\nreturn good quality mappings across different mapping problems and domains.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2011 13:40:29 GMT"}], "update_date": "2011-08-10", "authors_parsed": [["Peukert", "Eric", ""], ["Eberius", "Julian", ""], ["Rahm", "Erhard", ""]]}, {"id": "1108.3915", "submitter": "Tien Tuan Anh Dinh", "authors": "Dinh Tien Tuan Anh and Wang Wenqiang and Anwitaman Datta", "title": "City on the Sky: Flexible, Secure Data Sharing on the Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.NI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Sharing data from various sources and of diverse kinds, and fusing them\ntogether for sophisticated analytics and mash-up applications are emerging\ntrends, and are prerequisites for grand visions such as that of cyber-physical\nsystems enabled smart cities. Cloud infrastructure can enable such data sharing\nboth because it can scale easily to an arbitrary volume of data and computation\nneeds on demand, as well as because of natural collocation of diverse such data\nsets within the infrastructure. However, in order to convince data owners that\ntheir data are well protected while being shared among cloud users, the cloud\nplatform needs to provide flexible mechanisms for the users to express the\nconstraints (access rules) subject to which the data should be shared, and\nlikewise, enforce them effectively. We study a comprehensive set of practical\nscenarios where data sharing needs to be enforced by methods such as\naggregation, windowed frame, value constrains, etc., and observe that existing\nbasic access control mechanisms do not provide adequate flexibility to enable\neffective data sharing in a secure and controlled manner. In this paper, we\nthus propose a framework for cloud that extends popular XACML model\nsignificantly by integrating flexible access control decisions and data access\nin a seamless fashion. We have prototyped the framework and deployed it on\ncommercial cloud environment for experimental runs to test the efficacy of our\napproach and evaluate the performance of the implemented prototype.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2011 08:46:28 GMT"}, {"version": "v2", "created": "Mon, 22 Aug 2011 03:44:11 GMT"}, {"version": "v3", "created": "Wed, 21 Sep 2011 19:01:12 GMT"}], "update_date": "2011-09-22", "authors_parsed": [["Anh", "Dinh Tien Tuan", ""], ["Wenqiang", "Wang", ""], ["Datta", "Anwitaman", ""]]}, {"id": "1108.4516", "submitter": "Yanwei Xu", "authors": "Yanwei XU", "title": "Scalable Continual Top-k Keyword Search in Relational Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keyword search in relational databases has been widely studied in recent\nyears because it does not require users neither to master a certain structured\nquery language nor to know the complex underlying database schemas. Most of\nexisting methods focus on answering snapshot keyword queries in static\ndatabases. In practice, however, databases are updated frequently, and users\nmay have long-term interests on specific topics. To deal with such a situation,\nit is necessary to build effective and efficient facility in a database system\nto support continual keyword queries.\n  In this paper, we propose an efficient method for answering continual top-$k$\nkeyword queries over relational databases. The proposed method is built on an\nexisting scheme of keyword search on relational data streams, but incorporates\nthe ranking mechanisms into the query processing methods and makes two\nimprovements to support efficient top-$k$ keyword search in relational\ndatabases. Compared to the existing methods, our method is more efficient both\nin computing the top-$k$ results in a static database and in maintaining the\ntop-$k$ results when the database continually being updated. Experimental\nresults validate the effectiveness and efficiency of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2011 07:52:52 GMT"}], "update_date": "2011-08-24", "authors_parsed": [["XU", "Yanwei", ""]]}, {"id": "1108.4596", "submitter": "Benjamin Nguyen", "authors": "Benjamin Nguyen (PRISM, INRIA Rocquencourt), Antoine Vion (LEST),\n  Fran\\c{c}ois-Xavier Dudouet (IRISES), Dario Colazzo (LRI, INRIA Saclay - Ile\n  de France), Ioana Manolescu (LRI, INRIA Saclay - Ile de France), Pierre\n  Senellart", "title": "XML content warehousing: Improving sociological studies of mailing lists\n  and web data", "comments": null, "journal-ref": "Bulletin de M\\'ethodologie Sociologique (BMS) (2011) 27p", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the guidelines for an XML-based approach for the\nsociological study of Web data such as the analysis of mailing lists or\ndatabases available online. The use of an XML warehouse is a flexible solution\nfor storing and processing this kind of data. We propose an implemented\nsolution and show possible applications with our case study of profiles of\nexperts involved in W3C standard-setting activity. We illustrate the\nsociological use of semi-structured databases by presenting our XML Schema for\nmailing-list warehousing. An XML Schema allows many adjunctions or crossings of\ndata sources, without modifying existing data sets, while allowing possible\nstructural evolution. We also show that the existence of hidden data implies\nincreased complexity for traditional SQL users. XML content warehousing allows\naltogether exhaustive warehousing and recursive queries through contents, with\nfar less dependence on the initial storage. We finally present the possibility\nof exporting the data stored in the warehouse to commonly-used advanced\nsoftware devoted to sociological analysis.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2011 13:31:56 GMT"}], "update_date": "2011-08-24", "authors_parsed": [["Nguyen", "Benjamin", "", "PRISM, INRIA Rocquencourt"], ["Vion", "Antoine", "", "LEST"], ["Dudouet", "Fran\u00e7ois-Xavier", "", "IRISES"], ["Colazzo", "Dario", "", "LRI, INRIA Saclay - Ile\n  de France"], ["Manolescu", "Ioana", "", "LRI, INRIA Saclay - Ile de France"], ["Senellart", "Pierre", ""]]}, {"id": "1108.5095", "submitter": "Marcin Kik", "authors": "Marcin Kik", "title": "RBO Protocol: Broadcasting Huge Databases for Tiny Receivers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.DC cs.DM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a protocol (called RBO) for broadcasting long streams of\nsingle-packet messages over radio channel for tiny, battery powered, receivers.\nThe messages are labeled by the keys from some linearly ordered set. The sender\nrepeatedly broadcasts a sequence of many (possibly millions) of messages, while\neach receiver is interested in reception of a message with a specified key\nwithin this sequence. The transmission is arranged so that the receiver can\nwake up in arbitrary moment and find the nearest transmission of its searched\nmessage. Even if it does not know the position of the message in the sequence,\nit needs only to receive a small number of (the headers of) other messages to\nlocate it properly. Thus it can save energy by keeping the radio switched off\nmost of the time. We show that bit-reversal permutation has \"recursive\nbisection properties\" and, as a consequence, RBO can be implemented very\nefficiently with only constant number of $\\log_2 n$-bit variables, where $n$ is\nthe total number of messages in the sequence. The total number of the required\nreceptions is at most $2\\log_2 n +2$ in the model with perfect synchronization.\nThe basic procedure of RBO (computation of the time slot for the next required\nreception) requires only $O(\\log^3 n)$ bit-wise operations. We propose\nimplementation mechanisms for realistic model (with imperfect synchronization),\nfor operating systems (such as e.g. TinyOS).\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2011 14:05:30 GMT"}], "update_date": "2011-08-26", "authors_parsed": [["Kik", "Marcin", ""]]}, {"id": "1108.5253", "submitter": "Bay Vo", "authors": "Bay Vo, Bac Le", "title": "A Frequent Closed Itemsets Lattice-based Approach for Mining Minimal\n  Non-Redundant Association Rules", "comments": "11 pages", "journal-ref": "International Journal of Database Theory and Application, Vol.4,\n  No.2, 2011", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  There are many algorithms developed for improvement the time of mining\nfrequent itemsets (FI) or frequent closed itemsets (FCI). However, the\nalgorithms which deal with the time of generating association rules were not\nput in deep research. In reality, in case of a database containing many FI/FCI\n(from ten thousands up to millions), the time of generating association rules\nis much larger than that of mining FI/FCI. Therefore, this paper presents an\napplication of frequent closed itemsets lattice (FCIL) for mining minimal\nnon-redundant association rules (MNAR) to reduce a lot of time for generating\nrules. Firstly, we use CHARM-L for building FCIL. After that, based on FCIL, an\nalgorithm for fast generating MNAR will be proposed. Experimental results show\nthat the proposed algorithm is much faster than frequent itemsets lattice-based\nalgorithm in the mining time.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2011 08:07:22 GMT"}], "update_date": "2011-08-29", "authors_parsed": [["Vo", "Bay", ""], ["Le", "Bac", ""]]}, {"id": "1108.5451", "submitter": "Andreas Behrend", "authors": "Andreas Behrend", "title": "A Uniform Fixpoint Approach to the Implementation of Inference Methods\n  for Deductive Databases", "comments": "to appear in the Proceedings of the 19th International Conference on\n  Applications of Declarative Programming and Knowledge Management (INAP 2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the research area of deductive databases three different database\ntasks have been deeply investigated: query evaluation, update propagation and\nview updating. Over the last thirty years various inference mechanisms have\nbeen proposed for realizing these main functionalities of a rule-based system.\nHowever, these inference mechanisms have been rarely used in commercial DB\nsystems until now. One important reason for this is the lack of a uniform\napproach well-suited for implementation in an SQL-based system. In this paper,\nwe present such a uniform approach in form of a new version of the soft\nconsequence operator. Additionally, we present improved transformation-based\napproaches to query optimization and update propagation and view updating which\nare all using this operator as underlying evaluation mechanism.\n", "versions": [{"version": "v1", "created": "Sat, 27 Aug 2011 14:09:08 GMT"}], "update_date": "2011-08-30", "authors_parsed": [["Behrend", "Andreas", ""]]}, {"id": "1108.5592", "submitter": "Sivakumar Madesan", "authors": "Abhishek Taneja, R.K.Chauhan", "title": "A Performance Study of Data Mining Techniques: Multiple Linear\n  Regression vs. Factor Analysis", "comments": "Data mining, Multiple Linear Regression, Factor Analysis, Principal\n  Component Regression, Maximum Liklihood Regression, Generalized Least Square\n  Regression", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing volume of data usually creates an interesting challenge for the\nneed of data analysis tools that discover regularities in these data. Data\nmining has emerged as disciplines that contribute tools for data analysis,\ndiscovery of hidden knowledge, and autonomous decision making in many\napplication domains. The purpose of this study is to compare the performance of\ntwo data mining techniques viz., factor analysis and multiple linear regression\nfor different sample sizes on three unique sets of data. The performance of the\ntwo data mining techniques is compared on following parameters like mean square\nerror (MSE), R-square, R-Square adjusted, condition number, root mean square\nerror(RMSE), number of variables included in the prediction model, modified\ncoefficient of efficiency, F-value, and test of normality. These parameters\nhave been computed using various data mining tools like SPSS, XLstat, Stata,\nand MS-Excel. It is seen that for all the given dataset, factor analysis\noutperform multiple linear regression. But the absolute value of prediction\naccuracy varied between the three datasets indicating that the data\ndistribution and data characteristics play a major role in choosing the correct\nprediction technique.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2011 07:08:13 GMT"}], "update_date": "2011-08-30", "authors_parsed": [["Taneja", "Abhishek", ""], ["Chauhan", "R. K.", ""]]}, {"id": "1108.5619", "submitter": "Sivakumar Madesan", "authors": "Karanjit Singh and Shuchita Bhasin", "title": "Modification of GTD from Flat File Format to OLAP for Data Mining", "comments": "GTD, OLAP, Data Mining, Terror Databases", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document is part of original research work by the authors in a bid to\nexplore new fields for applying Data Mining Techniques. The sample data is part\nof a large data set from University of Maryland (UMD) and outlines how more\nmeaningful patterns can be discovered by preprocessing the data in the form of\nOLAP cubes.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2011 07:06:03 GMT"}], "update_date": "2011-08-30", "authors_parsed": [["Singh", "Karanjit", ""], ["Bhasin", "Shuchita", ""]]}, {"id": "1108.6016", "submitter": "Benjamin Rubinstein", "authors": "Jim Gemmell and Benjamin I. P. Rubinstein and Ashok K. Chandra", "title": "Improving Entity Resolution with Global Constraints", "comments": "10 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": "MSR-TR-2011-100", "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some of the greatest advances in web search have come from leveraging\nsocio-economic properties of online user behavior. Past advances include\nPageRank, anchor text, hubs-authorities, and TF-IDF. In this paper, we\ninvestigate another socio-economic property that, to our knowledge, has not yet\nbeen exploited: sites that create lists of entities, such as IMDB and Netflix,\nhave an incentive to avoid gratuitous duplicates. We leverage this property to\nresolve entities across the different web sites, and find that we can obtain\nsubstantial improvements in resolution accuracy. This improvement in accuracy\nalso translates into robustness, which often reduces the amount of training\ndata that must be labeled for comparing entities across many sites.\nFurthermore, the technique provides robustness when resolving sites that have\nsome duplicates, even without first removing these duplicates. We present\nalgorithms with very strong precision and recall, and show that max weight\nmatching, while appearing to be a natural choice turns out to have poor\nperformance in some situations. The presented techniques are now being used in\nthe back-end entity resolution system at a major Internet search engine.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2011 17:30:54 GMT"}], "update_date": "2011-08-31", "authors_parsed": [["Gemmell", "Jim", ""], ["Rubinstein", "Benjamin I. P.", ""], ["Chandra", "Ashok K.", ""]]}, {"id": "1108.6198", "submitter": "Gadda Koteswara Rao", "authors": "G.Koteswara Rao, Shubhamoy Dey", "title": "Decision Support for e-Governance: A Text Mining Approach", "comments": "19 Pages, 7 Figures", "journal-ref": "International Journal of Managing Information Technology (IJMIT)\n  Vol.3, No.3, 2011, 73-91", "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Information and communication technology has the capability to improve the\nprocess by which governments involve citizens in formulating public policy and\npublic projects. Even though much of government regulations may now be in\ndigital form (and often available online), due to their complexity and\ndiversity, identifying the ones relevant to a particular context is a\nnon-trivial task. Similarly, with the advent of a number of electronic online\nforums, social networking sites and blogs, the opportunity of gathering\ncitizens' petitions and stakeholders' views on government policy and proposals\nhas increased greatly, but the volume and the complexity of analyzing\nunstructured data makes this difficult. On the other hand, text mining has come\na long way from simple keyword search, and matured into a discipline capable of\ndealing with much more complex tasks. In this paper we discuss how text-mining\ntechniques can help in retrieval of information and relationships from textual\ndata sources, thereby assisting policy makers in discovering associations\nbetween policies and citizens' opinions expressed in electronic public forums\nand blogs etc. We also present here, an integrated text mining based\narchitecture for e-governance decision support along with a discussion on the\nIndian scenario.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2011 11:48:57 GMT"}], "update_date": "2011-09-01", "authors_parsed": [["Rao", "G. Koteswara", ""], ["Dey", "Shubhamoy", ""]]}, {"id": "1108.6328", "submitter": "Olaf Hartig", "authors": "Olaf Hartig and Johann-Christoph Freytag", "title": "Foundations of Traversal Based Query Execution over Linked Data\n  (Extended Version)", "comments": "v5: aligned with the final version published in HT'2012; v4: new\n  Sec.3.2, added the concept of 'LD machine decidability', added the concept of\n  'seed identifiers', 20 pages; v3: new Sec.6.5, rewritten Abstract and intro,\n  proofs in single column format, 19 pages; v2: completed missing proof, 16\n  pages; v1: 15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query execution over the Web of Linked Data has attracted much attention\nrecently. A particularly interesting approach is link traversal based query\nexecution which proposes to integrate the traversal of data links into the\nconstruction of query results. Hence -in contrast to traditional query\nexecution paradigms- this approach does not assume a fixed set of relevant data\nsources beforehand; instead, it discovers data on the fly and, thus, enables\napplications to tap the full potential of the Web.\n  While several authors study possibilities to implement the idea of link\ntraversal based query execution and to optimize query execution in this\ncontext, no work exists that discusses the theoretical foundations of the\napproach in general. Our paper fills this gap.\n  We introduce a well-defined semantics for queries that may be executed using\nthe link traversal based approach. Based on this semantics we formally analyze\nproperties of such queries. In particular, we study the computability of\nqueries as well as the implications of querying a potentially infinite Web of\nLinked Data. Our results show that query computation in general is not\nguaranteed to terminate and that for any given query it is undecidable whether\nthe execution terminates. Furthermore, we define an abstract execution model\nthat captures the integration of link traversal into the query execution\nprocess. Based on this model we prove the soundness and completeness of link\ntraversal based query execution and analyze an existing implementation\napproach..\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2011 19:22:48 GMT"}, {"version": "v2", "created": "Thu, 1 Sep 2011 19:00:46 GMT"}, {"version": "v3", "created": "Thu, 10 Nov 2011 15:30:02 GMT"}, {"version": "v4", "created": "Wed, 22 Feb 2012 12:07:44 GMT"}, {"version": "v5", "created": "Fri, 20 Apr 2012 18:36:24 GMT"}], "update_date": "2012-04-23", "authors_parsed": [["Hartig", "Olaf", ""], ["Freytag", "Johann-Christoph", ""]]}]