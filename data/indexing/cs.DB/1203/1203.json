[{"id": "1203.0024", "submitter": "Marco Montali", "authors": "Babak Bagheri Hariri, Diego Calvanese, Giuseppe De Giacomo, Alin\n  Deutsch, Marco Montali", "title": "Verification of Relational Data-Centric Dynamic Systems with External\n  Services", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-centric dynamic systems are systems where both the process controlling\nthe dynamics and the manipulation of data are equally central. In this paper we\nstudy verification of (first-order) mu-calculus variants over relational\ndata-centric dynamic systems, where data are represented by a full-fledged\nrelational database, and the process is described in terms of atomic actions\nthat evolve the database. The execution of such actions may involve calls to\nexternal services, providing fresh data inserted into the system. As a result\nsuch systems are typically infinite-state. We show that verification is\nundecidable in general, and we isolate notable cases, where decidability is\nachieved. Specifically we start by considering service calls that return values\ndeterministically (depending only on passed parameters). We show that in a\nmu-calculus variant that preserves knowledge of objects appeared along a run we\nget decidability under the assumption that the fresh data introduced along a\nrun are bounded, though they might not be bounded in the overall system. In\nfact we tie such a result to a notion related to weak acyclicity studied in\ndata exchange. Then, we move to nondeterministic services where the assumption\nof data bounded run would result in a bound on the service calls that can be\ninvoked during the execution and hence would be too restrictive. So we\ninvestigate decidability under the assumption that knowledge of objects is\npreserved only if they are continuously present. We show that if infinitely\nmany values occur in a run but do not accumulate in the same state, then we get\nagain decidability. We give syntactic conditions to avoid this accumulation\nthrough the novel notion of \"generate-recall acyclicity\", which takes into\nconsideration that every service call activation generates new values that\ncannot be accumulated indefinitely.\n", "versions": [{"version": "v1", "created": "Wed, 29 Feb 2012 21:41:50 GMT"}], "update_date": "2012-03-02", "authors_parsed": [["Hariri", "Babak Bagheri", ""], ["Calvanese", "Diego", ""], ["De Giacomo", "Giuseppe", ""], ["Deutsch", "Alin", ""], ["Montali", "Marco", ""]]}, {"id": "1203.0055", "submitter": "Stratos Idreos", "authors": "Felix Halim, Stratos Idreos, Panagiotis Karras, Roland H. C. Yap", "title": "Stochastic Database Cracking: Towards Robust Adaptive Indexing in\n  Main-Memory Column-Stores", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 6, pp.\n  502-513 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern business applications and scientific databases call for inherently\ndynamic data storage environments. Such environments are characterized by two\nchallenging features: (a) they have little idle system time to devote on\nphysical design; and (b) there is little, if any, a priori workload knowledge,\nwhile the query and data workload keeps changing dynamically. In such\nenvironments, traditional approaches to index building and maintenance cannot\napply. Database cracking has been proposed as a solution that allows on-the-fly\nphysical data reorganization, as a collateral effect of query processing.\nCracking aims to continuously and automatically adapt indexes to the workload\nat hand, without human intervention. Indexes are built incrementally,\nadaptively, and on demand. Nevertheless, as we show, existing adaptive indexing\nmethods fail to deliver workload-robustness; they perform much better with\nrandom workloads than with others. This frailty derives from the inelasticity\nwith which these approaches interpret each query as a hint on how data should\nbe stored. Current cracking schemes blindly reorganize the data within each\nquery's range, even if that results into successive expensive operations with\nminimal indexing benefit. In this paper, we introduce stochastic cracking, a\nsignificantly more resilient approach to adaptive indexing. Stochastic cracking\nalso uses each query as a hint on how to reorganize data, but not blindly so;\nit gains resilience and avoids performance bottlenecks by deliberately applying\ncertain arbitrary choices in its decision-making. Thereby, we bring adaptive\nindexing forward to a mature formulation that confers the workload-robustness\nprevious approaches lacked. Our extensive experimental study verifies that\nstochastic cracking maintains the desired properties of original database\ncracking while at the same time it performs well with diverse realistic\nworkloads.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2012 00:16:29 GMT"}], "update_date": "2012-03-02", "authors_parsed": [["Halim", "Felix", ""], ["Idreos", "Stratos", ""], ["Karras", "Panagiotis", ""], ["Yap", "Roland H. C.", ""]]}, {"id": "1203.0056", "submitter": "Georgios Giannikis", "authors": "Georgios Giannikis, Gustavo Alonso, Donald Kossmann", "title": "SharedDB: Killing One Thousand Queries With One Stone", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 6, pp.\n  526-537 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional database systems are built around the query-at-a-time model. This\napproach tries to optimize performance in a best-effort way. Unfortunately,\nbest effort is not good enough for many modern applications. These applications\nrequire response time guarantees in high load situations. This paper describes\nthe design of a new database architecture that is based on batching queries and\nshared computation across possibly hundreds of concurrent queries and updates.\nPerformance experiments with the TPC-W benchmark show that the performance of\nour implementation, SharedDB, is indeed robust across a wide range of dynamic\nworkloads.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2012 00:17:13 GMT"}], "update_date": "2012-03-02", "authors_parsed": [["Giannikis", "Georgios", ""], ["Alonso", "Gustavo", ""], ["Kossmann", "Donald", ""]]}, {"id": "1203.0057", "submitter": "Joachim Selke", "authors": "Joachim Selke, Christoph Lofi, Wolf-Tilo Balke", "title": "Pushing the Boundaries of Crowd-enabled Databases with Query-driven\n  Schema Expansion", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 6, pp.\n  538-549 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By incorporating human workers into the query execution process crowd-enabled\ndatabases facilitate intelligent, social capabilities like completing missing\ndata at query time or performing cognitive operators. But despite all their\nflexibility, crowd-enabled databases still maintain rigid schemas. In this\npaper, we extend crowd-enabled databases by flexible query-driven schema\nexpansion, allowing the addition of new attributes to the database at query\ntime. However, the number of crowd-sourced mini-tasks to fill in missing values\nmay often be prohibitively large and the resulting data quality is doubtful.\nInstead of simple crowd-sourcing to obtain all values individually, we leverage\nthe user-generated data found in the Social Web: By exploiting user ratings we\nbuild perceptual spaces, i.e., highly-compressed representations of opinions,\nimpressions, and perceptions of large numbers of users. Using few training\nsamples obtained by expert crowd sourcing, we then can extract all missing data\nautomatically from the perceptual space with high quality and at low costs.\nExtensive experiments show that our approach can boost both performance and\nquality of crowd-enabled databases, while also providing the flexibility to\nexpand schemas in a query-driven fashion.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2012 00:17:22 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Selke", "Joachim", ""], ["Lofi", "Christoph", ""], ["Balke", "Wolf-Tilo", ""]]}, {"id": "1203.0058", "submitter": "Bo Zhao", "authors": "Bo Zhao, Benjamin I. P. Rubinstein, Jim Gemmell, Jiawei Han", "title": "A Bayesian Approach to Discovering Truth from Conflicting Sources for\n  Data Integration", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 6, pp.\n  550-561 (2012)", "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In practical data integration systems, it is common for the data sources\nbeing integrated to provide conflicting information about the same entity.\nConsequently, a major challenge for data integration is to derive the most\ncomplete and accurate integrated records from diverse and sometimes conflicting\nsources. We term this challenge the truth finding problem. We observe that some\nsources are generally more reliable than others, and therefore a good model of\nsource quality is the key to solving the truth finding problem. In this work,\nwe propose a probabilistic graphical model that can automatically infer true\nrecords and source quality without any supervision. In contrast to previous\nmethods, our principled approach leverages a generative process of two types of\nerrors (false positive and false negative) by modeling two different aspects of\nsource quality. In so doing, ours is also the first approach designed to merge\nmulti-valued attribute types. Our method is scalable, due to an efficient\nsampling-based inference algorithm that needs very few iterations in practice\nand enjoys linear time complexity, with an even faster incremental variant.\nExperiments on two real world datasets show that our new method outperforms\nexisting state-of-the-art approaches to the truth finding problem.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2012 00:17:31 GMT"}], "update_date": "2012-03-05", "authors_parsed": [["Zhao", "Bo", ""], ["Rubinstein", "Benjamin I. P.", ""], ["Gemmell", "Jim", ""], ["Han", "Jiawei", ""]]}, {"id": "1203.0059", "submitter": "Prasang Upadhyaya", "authors": "Prasang Upadhyaya, Magdalena Balazinska, Dan Suciu", "title": "How to Price Shared Optimizations in the Cloud", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 6, pp.\n  562-573 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-management-as-a-service systems are increasingly being used in\ncollaborative settings, where multiple users access common datasets. Cloud\nproviders have the choice to implement various optimizations, such as indexing\nor materialized views, to accelerate queries over these datasets. Each\noptimization carries a cost and may benefit multiple users. This creates a\nmajor challenge: how to select which optimizations to perform and how to share\ntheir cost among users. The problem is especially challenging when users are\nselfish and will only report their true values for different optimizations if\ndoing so maximizes their utility. In this paper, we present a new approach for\nselecting and pricing shared optimizations by using Mechanism Design. We first\nshow how to apply the Shapley Value Mechanism to the simple case of selecting\nand pricing additive optimizations, assuming an offline game where all users\naccess the service for the same time-period. Second, we extend the approach to\nonline scenarios where users come and go. Finally, we consider the case of\nsubstitutive optimizations. We show analytically that our mechanisms induce\ntruth- fulness and recover the optimization costs. We also show experimentally\nthat our mechanisms yield higher utility than the state-of-the-art approach\nbased on regret accumulation.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2012 00:17:40 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Upadhyaya", "Prasang", ""], ["Balazinska", "Magdalena", ""], ["Suciu", "Dan", ""]]}, {"id": "1203.0060", "submitter": "Albert Angel", "authors": "Albert Angel, Nick Koudas, Nikos Sarkas, Divesh Srivastava", "title": "Dense Subgraph Maintenance under Streaming Edge Weight Updates for\n  Real-time Story Identification", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 6, pp.\n  574-585 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed an unprecedented proliferation of social media.\nPeople around the globe author, every day, millions of blog posts, social\nnetwork status updates, etc. This rich stream of information can be used to\nidentify, on an ongoing basis, emerging stories, and events that capture\npopular attention. Stories can be identified via groups of tightly-coupled\nreal-world entities, namely the people, locations, products, etc., that are\ninvolved in the story. The sheer scale, and rapid evolution of the data\ninvolved necessitate highly efficient techniques for identifying important\nstories at every point of time. The main challenge in real-time story\nidentification is the maintenance of dense subgraphs (corresponding to groups\nof tightly-coupled entities) under streaming edge weight updates (resulting\nfrom a stream of user-generated content). This is the first work to study the\nefficient maintenance of dense subgraphs under such streaming edge weight\nupdates. For a wide range of definitions of density, we derive theoretical\nresults regarding the magnitude of change that a single edge weight update can\ncause. Based on these, we propose a novel algorithm, DYNDENS, which outperforms\nadaptations of existing techniques to this setting, and yields meaningful\nresults. Our approach is validated by a thorough experimental evaluation on\nlarge-scale real and synthetic datasets.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2012 00:17:48 GMT"}], "update_date": "2012-03-02", "authors_parsed": [["Angel", "Albert", ""], ["Koudas", "Nick", ""], ["Sarkas", "Nikos", ""], ["Srivastava", "Divesh", ""]]}, {"id": "1203.0061", "submitter": "Iman Elghandour", "authors": "Iman Elghandour, Ashraf Aboulnaga", "title": "ReStore: Reusing Results of MapReduce Jobs", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 6, pp.\n  586-597 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing large scale data has emerged as an important activity for many\norganizations in the past few years. This large scale data analysis is\nfacilitated by the MapReduce programming and execution model and its\nimplementations, most notably Hadoop. Users of MapReduce often have analysis\ntasks that are too complex to express as individual MapReduce jobs. Instead,\nthey use high-level query languages such as Pig, Hive, or Jaql to express their\ncomplex tasks. The compilers of these languages translate queries into\nworkflows of MapReduce jobs. Each job in these workflows reads its input from\nthe distributed file system used by the MapReduce system and produces output\nthat is stored in this distributed file system and read as input by the next\njob in the workflow. The current practice is to delete these intermediate\nresults from the distributed file system at the end of executing the workflow.\nOne way to improve the performance of workflows of MapReduce jobs is to keep\nthese intermediate results and reuse them for future workflows submitted to the\nsystem. In this paper, we present ReStore, a system that manages the storage\nand reuse of such intermediate results. ReStore can reuse the output of whole\nMapReduce jobs that are part of a workflow, and it can also create additional\nreuse opportunities by materializing and storing the output of query execution\noperators that are executed within a MapReduce job. We have implemented ReStore\nas an extension to the Pig dataflow system on top of Hadoop, and we\nexperimentally demonstrate significant speedups on queries from the PigMix\nbenchmark.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2012 00:17:58 GMT"}], "update_date": "2012-03-02", "authors_parsed": [["Elghandour", "Iman", ""], ["Aboulnaga", "Ashraf", ""]]}, {"id": "1203.0077", "submitter": "Balder ten Cate", "authors": "Vince Barany, Balder ten Cate, and Martin Otto", "title": "Queries with Guarded Negation (full version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A well-established and fundamental insight in database theory is that\nnegation (also known as complementation) tends to make queries difficult to\nprocess and difficult to reason about. Many basic problems are decidable and\nadmit practical algorithms in the case of unions of conjunctive queries, but\nbecome difficult or even undecidable when queries are allowed to contain\nnegation. Inspired by recent results in finite model theory, we consider a\nrestricted form of negation, guarded negation. We introduce a fragment of SQL,\ncalled GN-SQL, as well as a fragment of Datalog with stratified negation,\ncalled GN-Datalog, that allow only guarded negation, and we show that these\nquery languages are computationally well behaved, in terms of testing query\ncontainment, query evaluation, open-world query answering, and boundedness.\nGN-SQL and GN-Datalog subsume a number of well known query languages and\nconstraint languages, such as unions of conjunctive queries, monadic Datalog,\nand frontier-guarded tgds. In addition, an analysis of standard benchmark\nworkloads shows that most usage of negation in SQL in practice is guarded\nnegation.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2012 02:34:00 GMT"}], "update_date": "2012-03-02", "authors_parsed": [["Barany", "Vince", ""], ["Cate", "Balder ten", ""], ["Otto", "Martin", ""]]}, {"id": "1203.0160", "submitter": "Yingyi Bu Yingyi Bu", "authors": "Yingyi Bu, Vinayak Borkar, Michael J. Carey, Joshua Rosen, Neoklis\n  Polyzotis, Tyson Condie, Markus Weimer, Raghu Ramakrishnan", "title": "Scaling Datalog for Machine Learning on Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the case for a declarative foundation for\ndata-intensive machine learning systems. Instead of creating a new system for\neach specific flavor of machine learning task, or hardcoding new optimizations,\nwe argue for the use of recursive queries to program a variety of machine\nlearning systems. By taking this approach, database query optimization\ntechniques can be utilized to identify effective execution plans, and the\nresulting runtime plans can be executed on a single unified data-parallel query\nprocessing engine. As a proof of concept, we consider two programming\nmodels--Pregel and Iterative Map-Reduce-Update---from the machine learning\ndomain, and show how they can be captured in Datalog, tuned for a specific\ntask, and then compiled into an optimized physical plan. Experiments performed\non a large computing cluster with real data demonstrate that this declarative\napproach can provide very good performance while offering both increased\ngenerality and programming ease.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2012 11:43:43 GMT"}, {"version": "v2", "created": "Fri, 2 Mar 2012 10:14:58 GMT"}], "update_date": "2012-03-05", "authors_parsed": [["Bu", "Yingyi", ""], ["Borkar", "Vinayak", ""], ["Carey", "Michael J.", ""], ["Rosen", "Joshua", ""], ["Polyzotis", "Neoklis", ""], ["Condie", "Tyson", ""], ["Weimer", "Markus", ""], ["Ramakrishnan", "Raghu", ""]]}, {"id": "1203.0617", "submitter": "Yonghui Xiao", "authors": "Yonghui Xiao and Li Xiong", "title": "Bayesian inference under differential privacy", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference is an important technique throughout statistics. The\nessence of Beyesian inference is to derive the posterior belief updated from\nprior belief by the learned information, which is a set of differentially\nprivate answers under differential privacy. Although Bayesian inference can be\nused in a variety of applications, it becomes theoretically hard to solve when\nthe number of differentially private answers is large. To facilitate Bayesian\ninference under differential privacy, this paper proposes a systematic\nmechanism. The key step of the mechanism is the implementation of Bayesian\nupdating with the best linear unbiased estimator derived by Gauss-Markov\ntheorem. In addition, we also apply the proposed inference mechanism into an\nonline queryanswering system, the novelty of which is that the utility for\nusers is guaranteed by Bayesian inference in the form of credible interval and\nconfidence level. Theoretical and experimental analysis are shown to\ndemonstrate the efficiency and effectiveness of both inference mechanism and\nonline query-answering system.\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2012 07:19:56 GMT"}, {"version": "v2", "created": "Fri, 9 Nov 2012 02:36:57 GMT"}], "update_date": "2012-11-12", "authors_parsed": [["Xiao", "Yonghui", ""], ["Xiong", "Li", ""]]}, {"id": "1203.1569", "submitter": "Olaf Hartig", "authors": "Olaf Hartig", "title": "SPARQL for a Web of Linked Data: Semantics and Computability (Extended\n  Version)", "comments": "v2: 55 pages, added Appendix D about constant reachability criteria,\n  aligned with the final version published in ESWC 2012; v1: 52 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The World Wide Web currently evolves into a Web of Linked Data where content\nproviders publish and link data as they have done with hypertext for the last\n20 years. While the declarative query language SPARQL is the de facto for\nquerying a-priory defined sets of data from the Web, no language exists for\nquerying the Web of Linked Data itself. However, it seems natural to ask\nwhether SPARQL is also suitable for such a purpose.\n  In this paper we formally investigate the applicability of SPARQL as a query\nlanguage for Linked Data on the Web. In particular, we study two query models:\n1) a full-Web semantics where the scope of a query is the complete set of\nLinked Data on the Web and 2) a family of reachability-based semantics which\nrestrict the scope to data that is reachable by traversing certain data links.\nFor both models we discuss properties such as monotonicity and computability as\nwell as the implications of querying a Web that is infinitely large due to data\ngenerating servers.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2012 18:53:35 GMT"}, {"version": "v2", "created": "Fri, 6 Apr 2012 18:36:54 GMT"}], "update_date": "2012-04-09", "authors_parsed": [["Hartig", "Olaf", ""]]}, {"id": "1203.1878", "submitter": "Saptarsi Goswami Mr", "authors": "Saptarsi Goswami, Samiran Ghosh, Amlan Chakrabarti", "title": "Outlier detection from ETL Execution trace", "comments": "2011 3rd International Conference on Electronics Computer Technology\n  (ICECT 2011)", "journal-ref": null, "doi": "10.1109/ICECTECH.2011.5942112", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Extract, Transform, Load (ETL) is an integral part of Data Warehousing (DW)\nimplementation. The commercial tools that are used for this purpose captures\nlot of execution trace in form of various log files with plethora of\ninformation. However there has been hardly any initiative where any proactive\nanalyses have been done on the ETL logs to improve their efficiency. In this\npaper we utilize outlier detection technique to find the processes varying most\nfrom the group in terms of execution trace. As our experiment was carried on\nactual production processes, any outlier we would consider as a signal rather\nthan a noise. To identify the input parameters for the outlier detection\nalgorithm we employ a survey among developer community with varied mix of\nexperience and expertise. We use simple text parsing to extract these features\nfrom the logs, as shortlisted from the survey. Subsequently we applied outlier\ndetection technique (Clustering based) on the logs. By this process we reduced\nour domain of detailed analysis from 500 logs to 44 logs (8 Percentage). Among\nthe 5 outlier cluster, 2 of them are genuine concern, while the other 3 figure\nout because of the huge number of rows involved.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2012 18:30:25 GMT"}], "update_date": "2012-03-09", "authors_parsed": [["Goswami", "Saptarsi", ""], ["Ghosh", "Samiran", ""], ["Chakrabarti", "Amlan", ""]]}, {"id": "1203.1952", "submitter": "Hung Ngo", "authors": "Hung Q. Ngo and Ely Porat and Christopher R\\'e and Atri Rudra", "title": "Worst-case Optimal Join Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient join processing is one of the most fundamental and well-studied\ntasks in database research. In this work, we examine algorithms for natural\njoin queries over many relations and describe a novel algorithm to process\nthese queries optimally in terms of worst-case data complexity. Our result\nbuilds on recent work by Atserias, Grohe, and Marx, who gave bounds on the size\nof a full conjunctive query in terms of the sizes of the individual relations\nin the body of the query. These bounds, however, are not constructive: they\nrely on Shearer's entropy inequality which is information-theoretic. Thus, the\nprevious results leave open the question of whether there exist algorithms\nwhose running time achieve these optimal bounds. An answer to this question may\nbe interesting to database practice, as it is known that any algorithm based on\nthe traditional select-project-join style plans typically employed in an RDBMS\nare asymptotically slower than the optimal for some queries. We construct an\nalgorithm whose running time is worst-case optimal for all natural join\nqueries. Our result may be of independent interest, as our algorithm also\nyields a constructive proof of the general fractional cover bound by Atserias,\nGrohe, and Marx without using Shearer's inequality. This bound implies two\nfamous inequalities in geometry: the Loomis-Whitney inequality and the\nBollob\\'as-Thomason inequality. Hence, our results algorithmically prove these\ninequalities as well. Finally, we discuss how our algorithm can be used to\ncompute a relaxed notion of joins.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2012 22:35:29 GMT"}], "update_date": "2012-03-12", "authors_parsed": [["Ngo", "Hung Q.", ""], ["Porat", "Ely", ""], ["R\u00e9", "Christopher", ""], ["Rudra", "Atri", ""]]}, {"id": "1203.2000", "submitter": "Soni madhulatha Tagaram", "authors": "T Soni Madhulatha", "title": "Overview of streaming-data algorithms", "comments": "10 pages", "journal-ref": "Advanced Computing: An International Journal ( ACIJ ), November\n  2011, Volume 2, Number 6 Advanced Computing: An International Journal ( ACIJ\n  ) ISSN : 2229 - 6727 [Online] ; 2229 - 726X [Print]", "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Due to recent advances in data collection techniques, massive amounts of data\nare being collected at an extremely fast pace. Also, these data are potentially\nunbounded. Boundless streams of data collected from sensors, equipments, and\nother data sources are referred to as data streams. Various data mining tasks\ncan be performed on data streams in search of interesting patterns. This paper\nstudies a particular data mining task, clustering, which can be used as the\nfirst step in many knowledge discovery processes. By grouping data streams into\nhomogeneous clusters, data miners can learn about data characteristics which\ncan then be developed into classification models for new data or predictive\nmodels for unknown events. Recent research addresses the problem of data-stream\nmining to deal with applications that require processing huge amounts of data\nsuch as sensor data analysis and financial applications. For such analysis,\nsingle-pass algorithms that consume a small amount of memory are critical.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2012 06:59:40 GMT"}], "update_date": "2012-03-12", "authors_parsed": [["Madhulatha", "T Soni", ""]]}, {"id": "1203.2002", "submitter": "Soni madhulatha Tagaram", "authors": "T Soni Madhulatha", "title": "Graph partitioning advance clustering technique", "comments": "14 pages", "journal-ref": "International Journal of Computer Science and Engineering\n  Survey(IJCSES), February 2012, Volume 3, Number 1", "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Clustering is a common technique for statistical data analysis, Clustering is\nthe process of grouping the data into classes or clusters so that objects\nwithin a cluster have high similarity in comparison to one another, but are\nvery dissimilar to objects in other clusters. Dissimilarities are assessed\nbased on the attribute values describing the objects. Often, distance measures\nare used. Clustering is an unsupervised learning technique, where interesting\npatterns and structures can be found directly from very large data sets with\nlittle or none of the background knowledge. This paper also considers the\npartitioning of m-dimensional lattice graphs using Fiedler's approach, which\nrequires the determination of the eigenvector belonging to the second smallest\nEigenvalue of the Laplacian with K-means partitioning algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2012 07:08:10 GMT"}], "update_date": "2012-03-12", "authors_parsed": [["Madhulatha", "T Soni", ""]]}, {"id": "1203.2574", "submitter": "Xixuan Feng", "authors": "Xixuan Feng, Arun Kumar, Ben Recht and Christopher R\\'e", "title": "Towards a Unified Architecture for in-RDBMS Analytics", "comments": "Extended version of a SIGMOD 2012 full paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing use of statistical data analysis in enterprise applications\nhas created an arms race among database vendors to offer ever more\nsophisticated in-database analytics. One challenge in this race is that each\nnew statistical technique must be implemented from scratch in the RDBMS, which\nleads to a lengthy and complex development process. We argue that the root\ncause for this overhead is the lack of a unified architecture for in-database\nanalytics. Our main contribution in this work is to take a step towards such a\nunified architecture. A key benefit of our unified architecture is that\nperformance optimizations for analytics techniques can be studied generically\ninstead of an ad hoc, per-technique fashion. In particular, our technical\ncontributions are theoretical and empirical studies of two key factors that we\nfound impact performance: the order data is stored, and parallelization of\ncomputations on a single-node multicore RDBMS. We demonstrate the feasibility\nof our architecture by integrating several popular analytics techniques into\ntwo commercial and one open-source RDBMS. Our architecture requires changes to\nonly a few dozen lines of code to integrate a new statistical technique. We\nthen compare our approach with the native analytics tools offered by the\ncommercial RDBMSes on various analytics tasks, and validate that our approach\nachieves competitive or higher performance, while still achieving the same\nquality.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2012 18:07:58 GMT"}, {"version": "v2", "created": "Wed, 14 Mar 2012 18:21:13 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Feng", "Xixuan", ""], ["Kumar", "Arun", ""], ["Recht", "Ben", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1203.2672", "submitter": "Dan Olteanu", "authors": "Nurzhan Bakibayev, Dan Olteanu, and Jakub Z\\'avodn\\'y", "title": "FDB: A Query Engine for Factorised Relational Databases", "comments": "12 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factorised databases are relational databases that use compact factorised\nrepresentations at the physical layer to reduce data redundancy and boost query\nperformance. This paper introduces FDB, an in-memory query engine for\nselect-project-join queries on factorised databases. Key components of FDB are\nnovel algorithms for query optimisation and evaluation that exploit the\nsuccinctness brought by data factorisation. Experiments show that for data sets\nwith many-to-many relationships FDB can outperform relational engines by orders\nof magnitude.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2012 23:19:09 GMT"}], "update_date": "2012-03-14", "authors_parsed": [["Bakibayev", "Nurzhan", ""], ["Olteanu", "Dan", ""], ["Z\u00e1vodn\u00fd", "Jakub", ""]]}, {"id": "1203.2886", "submitter": "Medha Atre", "authors": "Medha Atre, Vineet Chaoji, Mohammed J. Zaki", "title": "BitPath -- Label Order Constrained Reachability Queries over Large\n  Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": "RPI-CS 12-02", "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we focus on the following constrained reachability problem over\nedge-labeled graphs like RDF -- \"given source node x, destination node y, and a\nsequence of edge labels (a, b, c, d), is there a path between the two nodes\nsuch that the edge labels on the path satisfy a regular expression\n\"*a.*b.*c.*d.*\". A \"*\" before \"a\" allows any other edge label to appear on the\npath before edge \"a\". \"a.*\" forces at least one edge with label \"a\". \".*\" after\n\"a\" allows zero or more edge labels after \"a\" and before \"b\". Our query\nprocessing algorithm uses simple divide-and-conquer and greedy pruning\nprocedures to limit the search space. However, our graph indexing technique --\nbased on \"compressed bit-vectors\" -- allows indexing large graphs which\notherwise would have been infeasible. We have evaluated our approach on graphs\nwith more than 22 million edges and 6 million nodes -- much larger compared to\nthe datasets used in the contemporary work on path queries.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2012 18:11:55 GMT"}], "update_date": "2012-03-14", "authors_parsed": [["Atre", "Medha", ""], ["Chaoji", "Vineet", ""], ["Zaki", "Mohammed J.", ""]]}, {"id": "1203.2987", "submitter": "Saurabh  Pal", "authors": "Surjeet Kumar Yadav, Brijesh Bharadwaj and Saurabh Pal", "title": "Mining Education Data to Predict Student's Retention: A comparative\n  Study", "comments": "5 pages. arXiv admin note: substantial text overlap with\n  arXiv:1202.4815", "journal-ref": "(IJCSIS) International Journal of Computer Science and Information\n  Security, Vol. 10, No. 2, 2012, pp113-117", "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main objective of higher education is to provide quality education to\nstudents. One way to achieve highest level of quality in higher education\nsystem is by discovering knowledge for prediction regarding enrolment of\nstudents in a course. This paper presents a data mining project to generate\npredictive models for student retention management. Given new records of\nincoming students, these predictive models can produce short accurate\nprediction lists identifying students who tend to need the support from the\nstudent retention program most. This paper examines the quality of the\npredictive models generated by the machine learning algorithms. The results\nshow that some of the machines learning algorithms are able to establish\neffective predictive models from the existing student retention data.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2012 02:23:22 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Yadav", "Surjeet Kumar", ""], ["Bharadwaj", "Brijesh", ""], ["Pal", "Saurabh", ""]]}, {"id": "1203.3324", "submitter": "Al-Sakib Khan Pathan", "authors": "Diallo Abdoulaye Kindy, Al-Sakib Khan Pathan", "title": "A Detailed Survey on Various Aspects of SQL Injection in Web\n  Applications: Vulnerabilities, Innovative Attacks, and Remedies", "comments": "Due to unresolved publication fee issue, authors withdrew paper after\n  acceptance in INFORMATION Journal, Japan. To be published elsewhere", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In today's world, Web applications play a very important role in individual\nlife as well as in any country's development. Web applications have gone\nthrough a very rapid growth in the recent years and their adoption is moving\nfaster than that was expected few years ago. Now-a-days, billions of\ntransactions are done online with the aid of different Web applications. Though\nthese applications are used by hundreds of people, in many cases the security\nlevel is weak, which makes them vulnerable to get compromised. In most of the\nscenarios, a user has to be identified before any communication is established\nwith the backend database. An arbitrary user should not be allowed access to\nthe system without proof of valid credentials. However, a crafted injection\ngives access to unauthorized users. This is mostly accomplished via SQL\nInjection input. In spite of the development of different approaches to prevent\nSQL injection, it still remains an alarming threat to Web applications. In this\npaper, we present a detailed survey on various types of SQL Injection\nvulnerabilities, attacks, and their prevention techniques. Alongside presenting\nour findings from the study, we also note down future expectations and possible\ndevelopment of countermeasures against SQL Injection attacks.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:18:20 GMT"}, {"version": "v2", "created": "Mon, 17 Jun 2013 08:48:50 GMT"}], "update_date": "2013-06-18", "authors_parsed": [["Kindy", "Diallo Abdoulaye", ""], ["Pathan", "Al-Sakib Khan", ""]]}, {"id": "1203.3589", "submitter": "Eya Ben Ahmed", "authors": "Eya Ben Ahmed, Ahlem Nabli, Fa\\\"iez Gargouri", "title": "Building MultiView Analyst Profile From Multidimensional Query Logs:\n  From Consensual to Conflicting Preferences", "comments": "8 pages", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 9,\n  Issue 1, No 2, January 2012 ISSN (Online): 1694-0814 www.IJCSI.org", "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In order to provide suitable results to the analyst needs, user preferences\nsummarization is widely used in several domains. In this paper, we introduce a\nnew approach for user profile construction from OLAP query logs. The key idea\nis to learn the user's preferences by drawing the evidence from OLAP logs. In\nfact, the analyst preferences are clustered into three main pools : (i)\nconsensual or non conflicting preferences referring to same preferences for all\nanalysts; (ii) semi-conflicting preferences corresponding to similar\npreferences for some analysts; (iii) conflicting preferences related to\ndisjoint preferences for all analysts. To build generic and global model\naccurately describing the analyst, we enrich the obtained characteristics\nthrough including several views, namely the personal view, the professional\nview and the behavioral view. After that, the multiview profile extracted from\nmultidimensional database can be annotated.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 23:22:47 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Ahmed", "Eya Ben", ""], ["Nabli", "Ahlem", ""], ["Gargouri", "Fa\u00efez", ""]]}, {"id": "1203.4157", "submitter": "Saptarsi Goswami Mr", "authors": "Saptarsi Goswami, Amlan Chakrabarti", "title": "Quartile Clustering: A quartile based technique for Generating\n  Meaningful Clusters", "comments": "ISSN 2151-9617", "journal-ref": "Journal of Computing, Volume 4, Issue 2, February 2012, 48-55", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is one of the main tasks in exploratory data analysis and\ndescriptive statistics where the main objective is partitioning observations in\ngroups. Clustering has a broad range of application in varied domains like\nclimate, business, information retrieval, biology, psychology, to name a few. A\nvariety of methods and algorithms have been developed for clustering tasks in\nthe last few decades. We observe that most of these algorithms define a cluster\nin terms of value of the attributes, density, distance etc. However these\ndefinitions fail to attach a clear meaning/semantics to the generated clusters.\nWe argue that clusters having understandable and distinct semantics defined in\nterms of quartiles/halves are more appealing to business analysts than the\nclusters defined by data boundaries or prototypes. On the samepremise, we\npropose our new algorithm named as quartile clustering technique. Through a\nseries of experiments we establish efficacy of this algorithm. We demonstrate\nthat the quartile clustering technique adds clear meaning to each of the\nclusters compared to K-means. We use DB Index to measure goodness of the\nclusters and show our method is comparable to EM (Expectation Maximization),\nPAM (Partition around Medoid) and K Means. We have explored its capability in\ndetecting outlier and the benefit of added semantics. We discuss some of the\nlimitations in its present form and also provide a rough direction in\naddressing the issue of merging the generated clusters.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2012 16:38:05 GMT"}], "update_date": "2012-03-20", "authors_parsed": [["Goswami", "Saptarsi", ""], ["Chakrabarti", "Amlan", ""]]}, {"id": "1203.4163", "submitter": "Saptarsi Goswami Mr", "authors": "Saptarsi Goswami, Samiran Ghosh, Amlan Chakrabarti", "title": "Outlier Detection Techniques for SQL and ETL Tuning", "comments": null, "journal-ref": "International Journal of Computer Applications (0975 - 8887)\n  Volume 23 - No.8, June 2011", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RDBMS is the heart for both OLTP and OLAP types of applications. For both\ntypes of applications thousands of queries expressed in terms of SQL are\nexecuted on daily basis. All the commercial DBMS engines capture various\nattributes in system tables about these executed queries. These queries need to\nconform to best practices and need to be tuned to ensure optimal performance.\nWhile we use checklists, often tools to enforce the same, a black box technique\non the queries for profiling, outlier detection is not employed for a summary\nlevel understanding. This is the motivation of the paper, as this not only\npoints out to inefficiencies built in the system, but also has the potential to\npoint evolving best practices and inappropriate usage. Certainly this can\nreduce latency in information flow and optimal utilization of hardware and\nsoftware capacity. In this paper we start with formulating the problem. We\nexplore four outlier detection techniques. We apply these techniques over rich\ncorpora of production queries and analyze the results. We also explore benefit\nof an ensemble approach. We conclude with future courses of action. The same\nphilosophy we have used for optimization of extraction, transform, load (ETL)\njobs in one of our previous work. We give a brief introduction of the same in\nsection four.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2012 16:54:39 GMT"}], "update_date": "2012-03-20", "authors_parsed": [["Goswami", "Saptarsi", ""], ["Ghosh", "Samiran", ""], ["Chakrabarti", "Amlan", ""]]}, {"id": "1203.4380", "submitter": "Natalia Vanetik Dr", "authors": "Natalia Vanetik", "title": "Analyzing closed frequent itemsets with convex polytopes", "comments": "Published as another paper with different data model", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frequent itemsets form a polytope and can be found and analyzed with Linear\nProgramming.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2012 10:41:29 GMT"}, {"version": "v2", "created": "Wed, 4 Jul 2012 07:04:47 GMT"}, {"version": "v3", "created": "Mon, 16 Jun 2014 12:06:27 GMT"}, {"version": "v4", "created": "Thu, 30 Jul 2020 19:49:01 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Vanetik", "Natalia", ""]]}, {"id": "1203.4732", "submitter": "Gianluca Della Vedova", "authors": "Paola Bonizzoni, Peter J. Cameron, Gianluca Della Vedova, Alberto\n  Leporati, Giancarlo Mauri", "title": "A Unifying Framework to Characterize the Power of a Language to Express\n  Relations", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this extended abstract we provide a unifying framework that can be used to\ncharacterize and compare the expressive power of query languages for different\ndata base models. The framework is based upon the new idea of valid partition,\nthat is a partition of the elements of a given data base, where each class of\nthe partition is composed by elements that cannot be separated (distinguished)\naccording to some level of information contained in the data base. We describe\ntwo applications of this new framework, first by deriving a new syntactic\ncharacterization of the expressive power of relational algebra which is\nequivalent to the one given by Paredaens, and subsequently by studying the\nexpressive power of a simple graph-based data model.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2012 13:34:38 GMT"}], "update_date": "2012-03-22", "authors_parsed": [["Bonizzoni", "Paola", ""], ["Cameron", "Peter J.", ""], ["Della Vedova", "Gianluca", ""], ["Leporati", "Alberto", ""], ["Mauri", "Giancarlo", ""]]}, {"id": "1203.4903", "submitter": "Edith Cohen", "authors": "Edith Cohen", "title": "Distance Queries from Sampled Data: Accurate and Efficient", "comments": "13 pages; This is a full version of a KDD 2014 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distance queries are a basic tool in data analysis. They are used for\ndetection and localization of change for the purpose of anomaly detection,\nmonitoring, or planning. Distance queries are particularly useful when data\nsets such as measurements, snapshots of a system, content, traffic matrices,\nand activity logs are collected repeatedly.\n  Random sampling, which can be efficiently performed over streamed or\ndistributed data, is an important tool for scalable data analysis. The sample\nconstitutes an extremely flexible summary, which naturally supports domain\nqueries and scalable estimation of statistics, which can be specified after the\nsample is generated. The effectiveness of a sample as a summary, however,\nhinges on the estimators we have.\n  We derive novel estimators for estimating $L_p$ distance from sampled data.\nOur estimators apply with the most common weighted sampling schemes: Poisson\nProbability Proportional to Size (PPS) and its fixed sample size variants. They\nalso apply when the samples of different data sets are independent or\ncoordinated. Our estimators are admissible (Pareto optimal in terms of\nvariance) and have compelling properties.\n  We study the performance of our Manhattan and Euclidean distance ($p=1,2$)\nestimators on diverse datasets, demonstrating scalability and accuracy even\nwhen a small fraction of the data is sampled. Our work, for the first time,\nfacilitates effective distance estimation over sampled data.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2012 08:06:09 GMT"}, {"version": "v2", "created": "Fri, 15 Feb 2013 20:10:58 GMT"}, {"version": "v3", "created": "Sun, 8 Jun 2014 13:06:42 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Cohen", "Edith", ""]]}, {"id": "1203.5387", "submitter": "Vibhor  Rastogi", "authors": "Vibhor Rastogi, Ashwin Machanavajjhala, Laukik Chitnis, Anish Das\n  Sarma", "title": "Finding Connected Components on Map-reduce in Logarithmic Rounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a large graph G = (V,E) with millions of nodes and edges, how do we\ncompute its connected components efficiently? Recent work addresses this\nproblem in map-reduce, where a fundamental trade-off exists between the number\nof map-reduce rounds and the communication of each round. Denoting d the\ndiameter of the graph, and n the number of nodes in the largest component, all\nprior map-reduce techniques either require d rounds, or require about n|V| +\n|E| communication per round. We propose two randomized map-reduce algorithms --\n(i) Hash-Greater-To-Min, which provably requires at most 3log(n) rounds with\nhigh probability, and at most 2(|V| + |E|) communication per round, and (ii)\nHash-to-Min, which has a worse theoretical complexity, but in practice\ncompletes in at most 2log(d) rounds and 3(|V| + |E|) communication per rounds.\n  Our techniques for connected components can be applied to clustering as well.\nWe propose a novel algorithm for agglomerative single linkage clustering in\nmap-reduce. This is the first algorithm that can provably compute a clustering\nin at most O(log(n)) rounds, where n is the size of the largest cluster. We\nshow the effectiveness of all our algorithms through detailed experiments on\nlarge synthetic as well as real-world datasets.\n", "versions": [{"version": "v1", "created": "Sat, 24 Mar 2012 05:16:27 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2012 01:50:51 GMT"}], "update_date": "2012-11-14", "authors_parsed": [["Rastogi", "Vibhor", ""], ["Machanavajjhala", "Ashwin", ""], ["Chitnis", "Laukik", ""], ["Sarma", "Anish Das", ""]]}, {"id": "1203.5485", "submitter": "Sameer Agarwal", "authors": "Sameer Agarwal, Aurojit Panda, Barzan Mozafari, Samuel Madden, Ion\n  Stoica", "title": "BlinkDB: Queries with Bounded Errors and Bounded Response Times on Very\n  Large Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present BlinkDB, a massively parallel, sampling-based\napproximate query engine for running ad-hoc, interactive SQL queries on large\nvolumes of data. The key insight that BlinkDB builds on is that one can often\nmake reasonable decisions in the absence of perfect answers. For example,\nreliably detecting a malfunctioning server using a distributed collection of\nsystem logs does not require analyzing every request processed by the system.\nBased on this insight, BlinkDB allows one to trade-off query accuracy for\nresponse time, enabling interactive queries over massive data by running\nqueries on data samples and presenting results annotated with meaningful error\nbars. To achieve this, BlinkDB uses two key ideas that differentiate it from\nprevious work in this area: (1) an adaptive optimization framework that builds\nand maintains a set of multi-dimensional, multi-resolution samples from\noriginal data over time, and (2) a dynamic sample selection strategy that\nselects an appropriately sized sample based on a query's accuracy and/or\nresponse time requirements. We have built an open-source version of BlinkDB and\nvalidated its effectiveness using the well-known TPC-H benchmark as well as a\nreal-world analytic workload derived from Conviva Inc. Our experiments on a 100\nnode cluster show that BlinkDB can answer a wide range of queries from a\nreal-world query trace on up to 17 TBs of data in less than 2 seconds (over\n100\\times faster than Hive), within an error of 2 - 10%.\n", "versions": [{"version": "v1", "created": "Sun, 25 Mar 2012 11:11:21 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2012 19:01:31 GMT"}], "update_date": "2012-06-20", "authors_parsed": [["Agarwal", "Sameer", ""], ["Panda", "Aurojit", ""], ["Mozafari", "Barzan", ""], ["Madden", "Samuel", ""], ["Stoica", "Ion", ""]]}, {"id": "1203.5675", "submitter": "Amitabha Roy", "authors": "Amitabha Roy", "title": "Memory Hierarchy Sensitive Graph Layout", "comments": "Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining large graphs for information is becoming an increasingly important\nworkload due to the plethora of graph structured data becoming available. An\naspect of graph algorithms that has hitherto not received much interest is the\neffect of memory hierarchy on accesses. A typical system today has multiple\nlevels in the memory hierarchy with differing units of locality; ranging across\ncache lines, TLB entries and DRAM pages. We postulate that it is possible to\nallocate graph structured data in main memory in a way as to improve the\nspatial locality of the data. Previous approaches to improving cache locality\nhave focused only on a single unit of locality, either the cache line or\nvirtual memory page. On the other hand cache oblivious algorithms can optimise\nlayout for all levels of the memory hierarchy but unfortunately need to be\nspecially designed for individual data structures. In this paper we explore\nhierarchical blocking as a technique for closing this gap. We require as input\na specification of the units of locality in the memory hierarchy and lay out\nthe input graph accordingly by copying its nodes using a hierarchy of breadth\nfirst searches. We start with a basic algorithm that is limited to trees and\nthen extend it to arbitrary graphs. Our most efficient version requires only a\nconstant amount of additional space. We have implemented versions of the\nalgorithm in various environments: for C programs interfaced with macros, as an\nextension to the Boost object oriented graph library and finally as a\nmodification to the traversal phase of the semispace garbage collector in the\nJikes Java virtual machine. Our results show significant improvements in the\naccess time to graphs of various structure.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2012 14:12:48 GMT"}], "update_date": "2012-03-27", "authors_parsed": [["Roy", "Amitabha", ""]]}, {"id": "1203.6049", "submitter": "Gene Pang", "authors": "Tim Kraska, Gene Pang, Michael J. Franklin, Samuel Madden", "title": "MDCC: Multi-Data Center Consistency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Replicating data across multiple data centers not only allows moving the data\ncloser to the user and, thus, reduces latency for applications, but also\nincreases the availability in the event of a data center failure. Therefore, it\nis not surprising that companies like Google, Yahoo, and Netflix already\nreplicate user data across geographically different regions.\n  However, replication across data centers is expensive. Inter-data center\nnetwork delays are in the hundreds of milliseconds and vary significantly.\nSynchronous wide-area replication is therefore considered to be unfeasible with\nstrong consistency and current solutions either settle for asynchronous\nreplication which implies the risk of losing data in the event of failures,\nrestrict consistency to small partitions, or give up consistency entirely. With\nMDCC (Multi-Data Center Consistency), we describe the first optimistic commit\nprotocol, that does not require a master or partitioning, and is strongly\nconsistent at a cost similar to eventually consistent protocols. MDCC can\ncommit transactions in a single round-trip across data centers in the normal\noperational case. We further propose a new programming model which empowers the\napplication developer to handle longer and unpredictable latencies caused by\ninter-data center communication. Our evaluation using the TPC-W benchmark with\nMDCC deployed across 5 geographically diverse data centers shows that MDCC is\nable to achieve throughput and latency similar to eventually consistent quorum\nprotocols and that MDCC is able to sustain a data center outage without a\nsignificant impact on response times while guaranteeing strong consistency.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2012 19:03:53 GMT"}], "update_date": "2012-03-28", "authors_parsed": [["Kraska", "Tim", ""], ["Pang", "Gene", ""], ["Franklin", "Michael J.", ""], ["Madden", "Samuel", ""]]}, {"id": "1203.6400", "submitter": "Nodira Khoussainova", "authors": "Nodira Khoussainova, Magdalena Balazinska, Dan Suciu", "title": "PerfXplain: Debugging MapReduce Job Performance", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 7, pp.\n  598-609 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While users today have access to many tools that assist in performing large\nscale data analysis tasks, understanding the performance characteristics of\ntheir parallel computations, such as MapReduce jobs, remains difficult. We\npresent PerfXplain, a system that enables users to ask questions about the\nrelative performances (i.e., runtimes) of pairs of MapReduce jobs. PerfXplain\nprovides a new query language for articulating performance queries and an\nalgorithm for generating explanations from a log of past MapReduce job\nexecutions. We formally define the notion of an explanation together with three\nmetrics, relevance, precision, and generality, that measure explanation\nquality. We present the explanation-generation algorithm based on techniques\nrelated to decision-tree building. We evaluate the approach on a log of past\nexecutions on Amazon EC2, and show that our approach can generate quality\nexplanations, outperforming two naive explanation-generation methods.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2012 00:04:35 GMT"}], "update_date": "2012-03-30", "authors_parsed": [["Khoussainova", "Nodira", ""], ["Balazinska", "Magdalena", ""], ["Suciu", "Dan", ""]]}, {"id": "1203.6401", "submitter": "Francesco Gullo", "authors": "Francesco Gullo, Andrea Tagarelli", "title": "Uncertain Centroid based Partitional Clustering of Uncertain Data", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 7, pp.\n  610-621 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering uncertain data has emerged as a challenging task in uncertain data\nmanagement and mining. Thanks to a computational complexity advantage over\nother clustering paradigms, partitional clustering has been particularly\nstudied and a number of algorithms have been developed. While existing\nproposals differ mainly in the notions of cluster centroid and clustering\nobjective function, little attention has been given to an analysis of their\ncharacteristics and limits. In this work, we theoretically investigate major\nexisting methods of partitional clustering, and alternatively propose a\nwell-founded approach to clustering uncertain data based on a novel notion of\ncluster centroid. A cluster centroid is seen as an uncertain object defined in\nterms of a random variable whose realizations are derived based on all\ndeterministic representations of the objects to be clustered. As demonstrated\ntheoretically and experimentally, this allows for better representing a cluster\nof uncertain objects, thus supporting a consistently improved clustering\nperformance while maintaining comparable efficiency with existing partitional\nclustering algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2012 00:05:40 GMT"}], "update_date": "2012-03-30", "authors_parsed": [["Gullo", "Francesco", ""], ["Tagarelli", "Andrea", ""]]}, {"id": "1203.6402", "submitter": "Benjamin Moseley", "authors": "Bahman Bahmani, Benjamin Moseley, Andrea Vattani, Ravi Kumar, Sergei\n  Vassilvitskii", "title": "Scalable K-Means++", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 7, pp.\n  622-633 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over half a century old and showing no signs of aging, k-means remains one of\nthe most popular data processing algorithms. As is well-known, a proper\ninitialization of k-means is crucial for obtaining a good final solution. The\nrecently proposed k-means++ initialization algorithm achieves this, obtaining\nan initial set of centers that is provably close to the optimum solution. A\nmajor downside of the k-means++ is its inherent sequential nature, which limits\nits applicability to massive data: one must make k passes over the data to find\na good initial set of centers. In this work we show how to drastically reduce\nthe number of passes needed to obtain, in parallel, a good initialization. This\nis unlike prevailing efforts on parallelizing k-means that have mostly focused\non the post-initialization phases of k-means. We prove that our proposed\ninitialization algorithm k-means|| obtains a nearly optimal solution after a\nlogarithmic number of passes, and then show that in practice a constant number\nof passes suffices. Experimental evaluation on real-world large-scale data\ndemonstrates that k-means|| outperforms k-means++ in both sequential and\nparallel settings.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2012 00:06:07 GMT"}], "update_date": "2012-03-30", "authors_parsed": [["Bahmani", "Bahman", ""], ["Moseley", "Benjamin", ""], ["Vattani", "Andrea", ""], ["Kumar", "Ravi", ""], ["Vassilvitskii", "Sergei", ""]]}, {"id": "1203.6403", "submitter": "Michael Benedikt", "authors": "Michael Benedikt, Pierre Bourhis, Clemens Ley", "title": "Querying Schemas With Access Restrictions", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 7, pp.\n  634-645 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study verification of systems whose transitions consist of accesses to a\nWeb-based data-source. An access is a lookup on a relation within a relational\ndatabase, fixing values for a set of positions in the relation. For example, a\ntransition can represent access to a Web form, where the user is restricted to\nfilling in values for a particular set of fields. We look at verifying\nproperties of a schema describing the possible accesses of such a system. We\npresent a language where one can describe the properties of an access path, and\nalso specify additional restrictions on accesses that are enforced by the\nschema. Our main property language, AccLTL, is based on a first-order extension\nof linear-time temporal logic, interpreting access paths as sequences of\nrelational structures. We also present a lower-level automaton model,\nAautomata, which AccLTL specifications can compile into. We show that AccLTL\nand A-automata can express static analysis problems related to \"querying with\nlimited access patterns\" that have been studied in the database literature in\nthe past, such as whether an access is relevant to answering a query, and\nwhether two queries are equivalent in the accessible data they can return. We\nprove decidability and complexity results for several restrictions and variants\nof AccLTL, and explain which properties of paths can be expressed in each\nrestriction.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2012 00:06:16 GMT"}], "update_date": "2012-03-30", "authors_parsed": [["Benedikt", "Michael", ""], ["Bourhis", "Pierre", ""], ["Ley", "Clemens", ""]]}, {"id": "1203.6404", "submitter": "Goetz Graefe", "authors": "Goetz Graefe, Harumi Kuno", "title": "Definition, Detection, and Recovery of Single-Page Failures, a Fourth\n  Class of Database Failures", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 7, pp.\n  646-655 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The three traditional failure classes are system, media, and transaction\nfailures. Sometimes, however, modern storage exhibits failures that differ from\nall of those. In order to capture and describe such cases, single-page failures\nare introduced as a fourth failure class. This class encompasses all failures\nto read a data page correctly and with plausible contents despite all\ncorrection attempts in lower system levels. Efficient recovery seems to require\na new data structure called the page recovery index. Its transactional\nmaintenance can be accomplished writing the same number of log records as\ntoday's efficient implementations of logging and recovery. Detection and\nrecovery of a single-page failure can be sufficiently fast that the affected\ndata access is merely delayed, without the need to abort the transaction.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2012 00:06:48 GMT"}], "update_date": "2012-03-30", "authors_parsed": [["Graefe", "Goetz", ""], ["Kuno", "Harumi", ""]]}, {"id": "1203.6405", "submitter": "Stratos Idreos", "authors": "Goetz Graefe, Felix Halim, Stratos Idreos, Harumi Kuno, Stefan\n  Manegold", "title": "Concurrency Control for Adaptive Indexing", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 7, pp.\n  656-667 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive indexing initializes and optimizes indexes incrementally, as a side\neffect of query processing. The goal is to achieve the benefits of indexes\nwhile hiding or minimizing the costs of index creation. However,\nindex-optimizing side effects seem to turn read-only queries into update\ntransactions that might, for example, create lock contention. This paper\nstudies concurrency control in the context of adaptive indexing. We show that\nthe design and implementation of adaptive indexing rigorously separates index\nstructures from index contents; this relaxes the constraints and requirements\nduring adaptive indexing compared to those of traditional index updates. Our\ndesign adapts to the fact that an adaptive index is refined continuously, and\nexploits any concurrency opportunities in a dynamic way. A detailed\nexperimental analysis demonstrates that (a) adaptive indexing maintains its\nadaptive properties even when running concurrent queries, (b) adaptive indexing\ncan exploit the opportunity for parallelism due to concurrent queries, (c) the\nnumber of concurrency conflicts and any concurrency administration overheads\nfollow an adaptive behavior, decreasing as the workload evolves and adapting to\nthe workload needs.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2012 00:07:00 GMT"}], "update_date": "2012-03-30", "authors_parsed": [["Graefe", "Goetz", ""], ["Halim", "Felix", ""], ["Idreos", "Stratos", ""], ["Kuno", "Harumi", ""], ["Manegold", "Stefan", ""]]}, {"id": "1203.6406", "submitter": "Nilesh Dalvi", "authors": "Nilesh Dalvi, Ashwin Machanavajjhala, Bo Pang", "title": "An Analysis of Structured Data on the Web", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 7, pp.\n  680-691 (2012)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyze the nature and distribution of structured data on\nthe Web. Web-scale information extraction, or the problem of creating\nstructured tables using extraction from the entire web, is gathering lots of\nresearch interest. We perform a study to understand and quantify the value of\nWeb-scale extraction, and how structured information is distributed amongst top\naggregator websites and tail sites for various interesting domains. We believe\nthis is the first study of its kind, and gives us new insights for information\nextraction over the Web.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2012 00:07:19 GMT"}], "update_date": "2012-03-30", "authors_parsed": [["Dalvi", "Nilesh", ""], ["Machanavajjhala", "Ashwin", ""], ["Pang", "Bo", ""]]}, {"id": "1203.6454", "submitter": "Jasni Mohamad Zain", "authors": "Mohammed Adam Ibrahim Fakharaldien, Jasni Mohamed Zain and Norrozila\n  Sulaiman", "title": "XRecursive: An Efficient Method to Store and Query XML Documents", "comments": null, "journal-ref": "Australian Journal of Basic and Applied Sciences Volume 5, Issue\n  12, December 2011, Pages 2910-2916", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Storing XML documents in a relational database is a promising solution\nbecause relational databases are mature and scale very well and they have the\nadvantages that in a relational database XML data and structured data can\ncoexist making it possible to build application that involve both kinds of data\nwith little extra effort . In this paper, we propose an algorithm schema named\nXRecursive that translates XML documents to relational database according to\nthe proposed storing structure. The steps and algorithm are given in details to\ndescribe how to use the storing structure to storage and query XML documents in\nrelational database. Then we report our experimental results on a real database\nto show the performance of our method in some features.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2012 07:43:18 GMT"}], "update_date": "2012-03-30", "authors_parsed": [["Fakharaldien", "Mohammed Adam Ibrahim", ""], ["Zain", "Jasni Mohamed", ""], ["Sulaiman", "Norrozila", ""]]}]