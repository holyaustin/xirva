[{"id": "1411.0064", "submitter": "Siyuan Liu", "authors": "Lingyang Chu, Shuhui Wang, Siyuan Liu, Qingming Huang, Jian Pei", "title": "ALID: Scalable Dominant Cluster Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting dominant clusters is important in many analytic applications. The\nstate-of-the-art methods find dense subgraphs on the affinity graph as the\ndominant clusters. However, the time and space complexity of those methods are\ndominated by the construction of the affinity graph, which is quadratic with\nrespect to the number of data points, and thus impractical on large data sets.\nTo tackle the challenge, in this paper, we apply Evolutionary Game Theory (EGT)\nand develop a scalable algorithm, Approximate Localized Infection Immunization\nDynamics (ALID). The major idea is to perform Localized Infection Immunization\nDynamics (LID) to find dense subgraph within local range of the affinity graph.\nLID is further scaled up with guaranteed high efficiency and detection quality\nby an estimated Region of Interest (ROI) and a carefully designed Candidate\nInfective Vertex Search method (CIVS). ALID only constructs small local\naffinity graphs and has a time complexity of O(C(a^*+ {\\delta})n) and a space\ncomplexity of O(a^*(a^*+ {\\delta})), where a^* is the size of the largest\ndominant cluster and C << n and {\\delta} << n are small constants. We\ndemonstrate by extensive experiments on both synthetic data and real world data\nthat ALID achieves state-of-the-art detection quality with much lower time and\nspace cost on single machine. We also demonstrate the encouraging\nparallelization performance of ALID by implementing the Parallel ALID (PALID)\non Apache Spark. PALID processes 50 million SIFT data points in 2.29 hours,\nachieving a speedup ratio of 7.51 with 8 executors.\n", "versions": [{"version": "v1", "created": "Sat, 1 Nov 2014 04:13:59 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Chu", "Lingyang", ""], ["Wang", "Shuhui", ""], ["Liu", "Siyuan", ""], ["Huang", "Qingming", ""], ["Pei", "Jian", ""]]}, {"id": "1411.0189", "submitter": "Xinquan Chen", "authors": "Xinquan Chen", "title": "Synchronization Clustering based on a Linearized Version of Vicsek model", "comments": "37 pages, 9 figures, 3 tabels, 27 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a kind of effective synchronization clustering method\nbased on a linearized version of Vicsek model. This method can be represented\nby an Effective Synchronization Clustering algorithm (ESynC), an Improved\nversion of ESynC algorithm (IESynC), a Shrinking Synchronization Clustering\nalgorithm based on another linear Vicsek model (SSynC), and an effective\nMulti-level Synchronization Clustering algorithm (MSynC). After some analysis\nand comparisions, we find that ESynC algorithm based on the Linearized version\nof the Vicsek model has better synchronization effect than SynC algorithm based\non an extensive Kuramoto model and a similar synchronization clustering\nalgorithm based on the original Vicsek model. By simulated experiments of some\nartificial data sets, we observe that ESynC algorithm, IESynC algorithm, and\nSSynC algorithm can get better synchronization effect although it needs less\niterative times and less time than SynC algorithm. In some simulations, we also\nobserve that IESynC algorithm and SSynC algorithm can get some improvements in\ntime cost than ESynC algorithm. At last, it gives some research expectations to\npopularize this algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 2 Nov 2014 01:09:00 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Chen", "Xinquan", ""]]}, {"id": "1411.1455", "submitter": "Saravanan Thirumuruganathan", "authors": "Md Farhadur Rahman, Weimo Liu, Saravanan Thirumuruganathan, Nan Zhang,\n  Gautam Das", "title": "Rank-Based Inference over Web Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there has been much research in Ranked Retrieval model in\nstructured databases, especially those in web databases. With this model, a\nsearch query returns top-k tuples according to not just exact matches of\nselection conditions, but a suitable ranking function. This paper studies a\nnovel problem on the privacy implications of database ranking. The motivation\nis a novel yet serious privacy leakage we found on real-world web databases\nwhich is caused by the ranking function design. Many such databases feature\nprivate attributes - e.g., a social network allows users to specify certain\nattributes as only visible to him/herself, but not to others. While these\nwebsites generally respect the privacy settings by not directly displaying\nprivate attribute values in search query answers, many of them nevertheless\ntake into account such private attributes in the ranking function design. The\nconventional belief might be that tuple ranks alone are not enough to reveal\nthe private attribute values. Our investigation, however, shows that this is\nnot the case in reality.\n  To address the problem, we introduce a taxonomy of the problem space with two\ndimensions, (1) the type of query interface and (2) the capability of\nadversaries. For each subspace, we develop a novel technique which either\nguarantees the successful inference of private attributes, or does so for a\nsignificant portion of real-world tuples. We demonstrate the effectiveness and\nefficiency of our techniques through theoretical analysis, extensive\nexperiments over real-world datasets, as well as successful online attacks over\nwebsites with tens to hundreds of millions of users - e.g., Amazon Goodreads\nand Renren.com.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2014 00:06:44 GMT"}, {"version": "v2", "created": "Tue, 11 Nov 2014 00:36:10 GMT"}, {"version": "v3", "created": "Mon, 2 Feb 2015 07:37:56 GMT"}, {"version": "v4", "created": "Mon, 6 Apr 2015 02:03:27 GMT"}], "update_date": "2015-04-07", "authors_parsed": [["Rahman", "Md Farhadur", ""], ["Liu", "Weimo", ""], ["Thirumuruganathan", "Saravanan", ""], ["Zhang", "Nan", ""], ["Das", "Gautam", ""]]}, {"id": "1411.2275", "submitter": "Khaled Elbassioni", "authors": "Khaled M. Elbassioni", "title": "On Finding Minimal Infrequent Elements in Multi-dimensional Data Defined\n  over Partially Ordered Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider databases in which each attribute takes values from a partially\nordered set (poset). This allows one to model a number of interesting scenarios\narising in different applications, including quantitative databases,\ntaxonomies, and databases in which each attribute is an interval representing\nthe duration of a certain event occurring over time. A natural problem that\narises in such circumstances is the following: given a database $\\mathcal{D}$\nand a threshold value $t$, find all collections of \"generalizations\" of\nattributes which are \"supported\" by less than $t$ transactions from\n$\\mathcal{D}$. We call such collections infrequent elements. Due to\nmonotonicity, we can reduce the output size by considering only \\emph{minimal}\ninfrequent elements. We study the complexity of finding all minimal infrequent\nelements for some interesting classes of posets. We show how this problem can\nbe applied to mining association rules in different types of databases, and to\nfinding \"sparse regions\" or \"holes\" in quantitative data or in databases\nrecording the time intervals during which a re-occurring event appears over\ntime. Our main focus will be on these applications rather than on the\ncorrectness or analysis of the given algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 9 Nov 2014 20:09:53 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Elbassioni", "Khaled M.", ""]]}, {"id": "1411.2351", "submitter": "Wim Martens", "authors": "Wim Martens, Frank Neven, Stijn Vansummeren", "title": "SCULPT: A Schema Language for Tabular Data on the Web", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the recent working effort towards a recommendation by the World\nWide Web Consortium (W3C) for tabular data and metadata on the Web, we present\nin this paper a concept for a schema language for tabular web data called\nSCULPT. The language consists of rules constraining and defining the structure\nof regions in the table. These regions are defined through the novel formalism\nof region selection expressions. We present a formal model for SCULPT and\nobtain a linear time combined complexity evaluation algorithm. In addition, we\nconsider weak and strong streaming evaluation for SCULPT and present a fragment\nfor each of these streaming variants. Finally, we discuss several extensions of\nSCULPT including alternative semantics, types, complex content, and explore\nregion selection expressions as a basis for a transformation language.\n", "versions": [{"version": "v1", "created": "Mon, 10 Nov 2014 08:57:47 GMT"}, {"version": "v2", "created": "Tue, 11 Nov 2014 08:40:52 GMT"}], "update_date": "2014-11-12", "authors_parsed": [["Martens", "Wim", ""], ["Neven", "Frank", ""], ["Vansummeren", "Stijn", ""]]}, {"id": "1411.2499", "submitter": "Radhakrishnan Delhibabu", "authors": "Radhakrishnan Delhibabu", "title": "Comparative Study of View Update Algorithms in Rational Choice Theory", "comments": "http://link.springer.com/article/10.1007/s10489-014-0580-7. arXiv\n  admin note: substantial text overlap with arXiv:1407.3512, arXiv:1301.5154", "journal-ref": null, "doi": "10.1007/s10489-014-0580-7", "report-no": null, "categories": "cs.AI cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dynamics of belief and knowledge is one of the major components of any\nautonomous system that should be able to incorporate new pieces of information.\nWe show that knowledge base dynamics has interesting connection with kernel\nchange via hitting set and abduction. The approach extends and integrates\nstandard techniques for efficient query answering and integrity checking. The\ngeneration of hitting set is carried out through a hyper tableaux calculus and\nmagic set that is focused on the goal of minimality. Many different view update\nalgorithms have been proposed in the literature to address this problem. The\npresent paper provides a comparative study of view update algorithms in\nrational approach.\n", "versions": [{"version": "v1", "created": "Mon, 10 Nov 2014 16:43:24 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Delhibabu", "Radhakrishnan", ""]]}, {"id": "1411.2516", "submitter": "Giorgio Stefanoni", "authors": "Giorgio Stefanoni and Boris Motik", "title": "Answering Conjunctive Queries over $\\mathcal{EL}$ Knowledge Bases with\n  Transitive and Reflexive Roles", "comments": "Extended version of a paper to appear on AAAI-15. In this version of\n  the report, we fixed a few typos; all the results are unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answering conjunctive queries (CQs) over $\\mathcal{EL}$ knowledge bases (KBs)\nwith complex role inclusions is PSPACE-hard and in PSPACE in certain cases;\nhowever, if complex role inclusions are restricted to role transitivity, the\ntight upper complexity bound has so far been unknown. Furthermore, the existing\nalgorithms cannot handle reflexive roles, and they are not practicable.\nFinally, the problem is tractable for acyclic CQs and $\\mathcal{ELH}$, and\nNP-complete for unrestricted CQs and $\\mathcal{ELHO}$ KBs. In this paper we\ncomplete the complexity landscape of CQ answering for several important cases.\nIn particular, we present a practicable NP algorithm for answering CQs over\n$\\mathcal{ELHO}^s$ KBs---a logic containing all of OWL 2 EL, but with complex\nrole inclusions restricted to role transitivity. Our preliminary evaluation\nsuggests that the algorithm can be suitable for practical use. Moreover, we\nshow that, even for a restricted class of so-called arborescent acyclic\nqueries, CQ answering over $\\mathcal{EL}$ KBs becomes NP-hard in the presence\nof either transitive or reflexive roles. Finally, we show that answering\narborescent CQs over $\\mathcal{ELHO}$ KBs is tractable, whereas answering\nacyclic CQs is NP-hard.\n", "versions": [{"version": "v1", "created": "Mon, 10 Nov 2014 17:50:06 GMT"}, {"version": "v2", "created": "Tue, 11 Nov 2014 15:25:06 GMT"}, {"version": "v3", "created": "Wed, 13 May 2015 15:30:39 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Stefanoni", "Giorgio", ""], ["Motik", "Boris", ""]]}, {"id": "1411.2855", "submitter": "Simon Razniewski", "authors": "Simon Razniewski", "title": "Query-driven Data Completeness Management (PhD Thesis)", "comments": "Change to previous version: Fixed the date on the title page", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge about data completeness is essentially in data-supported decision\nmaking. In this thesis we present a framework for metadata-based assessment of\ndatabase completeness. We discuss how to express information about data\ncompleteness and how to use such information to draw conclusions about the\ncompleteness of query answers. In particular, we introduce formalisms for\nstating completeness for parts of relational databases. We then present\ntechniques for drawing inferences between such statements and statements about\nthe completeness of query answers, and show how the techniques can be extended\nto databases that contain null values. We show that the framework for\nrelational databases can be transferred to RDF data, and that a similar\nframework can also be applied to spatial data. We also discuss how completeness\ninformation can be verified over processes, and introduce a data-aware process\nmodel that allows this verification.\n", "versions": [{"version": "v1", "created": "Tue, 11 Nov 2014 15:39:04 GMT"}, {"version": "v2", "created": "Tue, 27 Jan 2015 20:54:31 GMT"}, {"version": "v3", "created": "Wed, 1 Apr 2015 14:12:43 GMT"}], "update_date": "2015-04-02", "authors_parsed": [["Razniewski", "Simon", ""]]}, {"id": "1411.3212", "submitter": "Francesco Lettich", "authors": "Francesco Lettich, Salvatore Orlando, Claudio Silvestri and Christian\n  S. Jensen", "title": "Manycore processing of repeated range queries over massive moving\n  objects observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to timely process significant amounts of continuously updated\nspatial data is mandatory for an increasing number of applications. Parallelism\nenables such applications to face this data-intensive challenge and allows the\ndevised systems to feature low latency and high scalability. In this paper we\nfocus on a specific data-intensive problem, concerning the repeated processing\nof huge amounts of range queries over massive sets of moving objects, where the\nspatial extents of queries and objects are continuously modified over time. To\ntackle this problem and significantly accelerate query processing we devise a\nhybrid CPU/GPU pipeline that compresses data output and save query processing\nwork. The devised system relies on an ad-hoc spatial index leading to a problem\ndecomposition that results in a set of independent data-parallel tasks. The\nindex is based on a point-region quadtree space decomposition and allows to\ntackle effectively a broad range of spatial object distributions, even those\nvery skewed. Also, to deal with the architectural peculiarities and limitations\nof the GPUs, we adopt non-trivial GPU data structures that avoid the need of\nlocked memory accesses and favour coalesced memory accesses, thus enhancing the\noverall memory throughput. To the best of our knowledge this is the first work\nthat exploits GPUs to efficiently solve repeated range queries over massive\nsets of continuously moving objects, characterized by highly skewed spatial\ndistributions. In comparison with state-of-the-art CPU-based implementations,\nour method highlights significant speedups in the order of 14x-20x, depending\non the datasets, even when considering very cheap GPUs.\n", "versions": [{"version": "v1", "created": "Wed, 12 Nov 2014 15:46:39 GMT"}], "update_date": "2014-11-13", "authors_parsed": [["Lettich", "Francesco", ""], ["Orlando", "Salvatore", ""], ["Silvestri", "Claudio", ""], ["Jensen", "Christian S.", ""]]}, {"id": "1411.3374", "submitter": "Manas Joglekar", "authors": "Manas Joglekar, Hector Garcia-Molina, Aditya Parameswaran, Christopher\n  Re", "title": "Exploiting Correlations for Expensive Predicate Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User Defined Function(UDFs) are used increasingly to augment query languages\nwith extra, application dependent functionality. Selection queries involving\nUDF predicates tend to be expensive, either in terms of monetary cost or\nlatency. In this paper, we study ways to efficiently evaluate selection queries\nwith UDF predicates. We provide a family of techniques for processing queries\nat low cost while satisfying user-specified precision and recall constraints.\nOur techniques are applicable to a variety of scenarios including when\nselection probabilities of tuples are available beforehand, when this\ninformation is available but noisy, or when no such prior information is\navailable. We also generalize our techniques to more complex queries. Finally,\nwe test our techniques on real datasets, and show that they achieve significant\nsavings in cost of up to $80\\%$, while incurring only a small reduction in\naccuracy.\n", "versions": [{"version": "v1", "created": "Wed, 12 Nov 2014 22:00:08 GMT"}], "update_date": "2014-11-14", "authors_parsed": [["Joglekar", "Manas", ""], ["Garcia-Molina", "Hector", ""], ["Parameswaran", "Aditya", ""], ["Re", "Christopher", ""]]}, {"id": "1411.3377", "submitter": "Manas Joglekar", "authors": "Manas Joglekar, Hector Garcia-Molina, Aditya Parameswaran", "title": "Comprehensive and Reliable Crowd Assessment Algorithms", "comments": "ICDE 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating workers is a critical aspect of any crowdsourcing system. In this\npaper, we devise techniques for evaluating workers by finding confidence\nintervals on their error rates. Unlike prior work, we focus on\n\"conciseness\"---that is, giving as tight a confidence interval as possible.\nConciseness is of utmost importance because it allows us to be sure that we\nhave the best guarantee possible on worker error rate. Also unlike prior work,\nwe provide techniques that work under very general scenarios, such as when not\nall workers have attempted every task (a fairly common scenario in practice),\nwhen tasks have non-boolean responses, and when workers have different biases\nfor positive and negative tasks. We demonstrate conciseness as well as accuracy\nof our confidence intervals by testing them on a variety of conditions and\nmultiple real-world datasets.\n", "versions": [{"version": "v1", "created": "Wed, 12 Nov 2014 22:09:17 GMT"}], "update_date": "2014-11-14", "authors_parsed": [["Joglekar", "Manas", ""], ["Garcia-Molina", "Hector", ""], ["Parameswaran", "Aditya", ""]]}, {"id": "1411.3622", "submitter": "Robert Piro", "authors": "Boris Motik, Yavor Nenov, Robert Piro, Ian Horrocks", "title": "Handling owl:sameAs via Rewriting", "comments": "This is the technical report supporting the AAAI 2015 Conference\n  submission with the same title", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rewriting is widely used to optimise owl:sameAs reasoning in materialisation\nbased OWL 2 RL systems. We investigate issues related to both the correctness\nand efficiency of rewriting, and present an algorithm that guarantees\ncorrectness, improves efficiency, and can be effectively parallelised. Our\nevaluation shows that our approach can reduce reasoning times on practical data\nsets by orders of magnitude.\n", "versions": [{"version": "v1", "created": "Thu, 13 Nov 2014 17:30:03 GMT"}], "update_date": "2014-11-14", "authors_parsed": [["Motik", "Boris", ""], ["Nenov", "Yavor", ""], ["Piro", "Robert", ""], ["Horrocks", "Ian", ""]]}, {"id": "1411.3787", "submitter": "Ping Li", "authors": "Anshumali Shrivastava, Ping Li", "title": "Asymmetric Minwise Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DB cs.DS cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minwise hashing (Minhash) is a widely popular indexing scheme in practice.\nMinhash is designed for estimating set resemblance and is known to be\nsuboptimal in many applications where the desired measure is set overlap (i.e.,\ninner product between binary vectors) or set containment. Minhash has inherent\nbias towards smaller sets, which adversely affects its performance in\napplications where such a penalization is not desirable. In this paper, we\npropose asymmetric minwise hashing (MH-ALSH), to provide a solution to this\nproblem. The new scheme utilizes asymmetric transformations to cancel the bias\nof traditional minhash towards smaller sets, making the final \"collision\nprobability\" monotonic in the inner product. Our theoretical comparisons show\nthat for the task of retrieving with binary inner products asymmetric minhash\nis provably better than traditional minhash and other recently proposed hashing\nalgorithms for general inner products. Thus, we obtain an algorithmic\nimprovement over existing approaches in the literature. Experimental\nevaluations on four publicly available high-dimensional datasets validate our\nclaims and the proposed scheme outperforms, often significantly, other hashing\nalgorithms on the task of near neighbor retrieval with set containment. Our\nproposal is simple and easy to implement in practice.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 04:18:33 GMT"}], "update_date": "2014-11-17", "authors_parsed": [["Shrivastava", "Anshumali", ""], ["Li", "Ping", ""]]}, {"id": "1411.3811", "submitter": "Todor Ivanov", "authors": "Todor Ivanov, Roberto V. Zicari, Sead Izberovic, Karsten Tolle", "title": "Performance Evaluation of Virtualized Hadoop Clusters", "comments": null, "journal-ref": null, "doi": null, "report-no": "Technical Report No. 2014-1", "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report we investigate the performance of Hadoop clusters, deployed\nwith separated storage and compute layers, on top of a hypervisor managing a\nsingle physical host. We have analyzed and evaluated the different Hadoop\ncluster configurations by running CPU bound and I/O bound workloads. The report\nis structured as follows: Section 2 provides a brief description of the\ntechnologies involved in our study. An overview of the experimental platform,\nsetup test and configurations are presented in Section 3. Our benchmark\nmethodology is defined in Section 4. The performed experiments together with\nthe evaluation of the results are presented in Section 5. Finally, Section 6\nconcludes with lessons learned.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 07:14:05 GMT"}], "update_date": "2014-11-17", "authors_parsed": [["Ivanov", "Todor", ""], ["Zicari", "Roberto V.", ""], ["Izberovic", "Sead", ""], ["Tolle", "Karsten", ""]]}, {"id": "1411.3969", "submitter": "Mario Lezoche", "authors": "Y. Liao (PPGEPS), M. Lezoche (CRAN), H. Panetto (CRAN), N. Boudjlida\n  (INRIA Nancy - Grand Est / LORIA), Eduardo Rocha Loures (ISE)", "title": "Formal Semantic Annotations for Models Interoperability in a PLM\n  environment", "comments": null, "journal-ref": "Proceedings of the 19th IFAC World Congress, 2014, Aug 2014, Cape\n  Town International Convention Centre, Cape Town, South Africa, South Africa.\n  International Federation of Automatic Control, World Congress, Volume # 19 |\n  Part# 1, 19 (1), pp.2382-2393, World Congress. http://www.ifac2014.org/", "doi": "10.3182/20140824-6-ZA-1003.02551", "report-no": null, "categories": "cs.SE cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, the need for system interoperability in or across enterprises has\nbecome more and more ubiquitous. Lots of research works have been carried out\nin the information exchange, transformation, discovery and reuse. One of the\nmain challenges in these researches is to overcome the semantic heterogeneity\nbetween enterprise applications along the lifecycle of a product. As a possible\nsolution to assist the semantic interoperability, semantic annotation has\ngained more and more attentions and is widely used in different domains. In\nthis paper, based on the investigation of the context and the related works, we\nidentify some existing drawbacks and propose a formal semantic annotation\napproach to support the semantics enrichment of models in a PLM environment.\n", "versions": [{"version": "v1", "created": "Thu, 13 Nov 2014 19:53:59 GMT"}], "update_date": "2014-11-17", "authors_parsed": [["Liao", "Y.", "", "PPGEPS"], ["Lezoche", "M.", "", "CRAN"], ["Panetto", "H.", "", "CRAN"], ["Boudjlida", "N.", "", "INRIA Nancy - Grand Est / LORIA"], ["Loures", "Eduardo Rocha", "", "ISE"]]}, {"id": "1411.4044", "submitter": "Todor Ivanov", "authors": "Todor Ivanov, Raik Niemann, Sead Izberovic, Marten Rosselli, Karsten\n  Tolle, Roberto V. Zicari", "title": "Benchmarking DataStax Enterprise/Cassandra with HiBench", "comments": "arXiv admin note: substantial text overlap with arXiv:1411.3811", "journal-ref": null, "doi": null, "report-no": "Technical Report No. 2014-2", "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report evaluates the new analytical capabilities of DataStax Enterprise\n(DSE) [1] through the use of standard Hadoop workloads. In particular, we run\nexperiments with CPU and I/O bound micro-benchmarks as well as OLAP-style\nanalytical query workloads. The performed tests should show that DSE is capable\nof successfully executing Hadoop applications without the need to adapt them\nfor the underlying Cassandra distributed storage system [2]. Due to the\nCassandra File System (CFS) [3], which supports the Hadoop Distributed File\nSystem API, Hadoop stack applications should seamlessly run in DSE. The report\nis structured as follows: Section 2 provides a brief description of the\ntechnologies involved in our study. An overview of our used hardware and\nsoftware components of the experimental environment is given in Section 3. Our\nbenchmark methodology is defined in Section 4. The performed experiments\ntogether with the evaluation of the results are presented in Section 5.\nFinally, Section 6 concludes with lessons learned.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 07:17:54 GMT"}, {"version": "v2", "created": "Tue, 16 Dec 2014 14:34:47 GMT"}], "update_date": "2014-12-17", "authors_parsed": [["Ivanov", "Todor", ""], ["Niemann", "Raik", ""], ["Izberovic", "Sead", ""], ["Rosselli", "Marten", ""], ["Tolle", "Karsten", ""], ["Zicari", "Roberto V.", ""]]}, {"id": "1411.4266", "submitter": "Shuai Ma", "authors": "Shuai Ma, Jia Li, Chunming Hu, Xuelian Lin, Jinpeng Huai", "title": "Big Graph Search: Challenges and Techniques", "comments": null, "journal-ref": "Frontiers of Computer Science 10(3): 387-398 (2016)", "doi": "10.1007/s11704-015-4515-1.", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On one hand, compared with traditional relational and XML models, graphs have\nmore expressive power and are widely used today. On the other hand, various\napplications of social computing trigger the pressing need of a new search\nparadigm. In this article, we argue that big graph search is the one filling\nthis gap. To show this, we first introduce the application of graph search in\nvarious scenarios. We then formalize the graph search problem, and give an\nanalysis of graph search from an evolutionary point of view, followed by the\nevidences from both the industry and academia. After that, we analyze the\ndifficulties and challenges of big graph search. Finally, we present three\nclasses of techniques towards big graph search: query techniques, data\ntechniques and distributed computing techniques.\n", "versions": [{"version": "v1", "created": "Sun, 16 Nov 2014 14:59:24 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Ma", "Shuai", ""], ["Li", "Jia", ""], ["Hu", "Chunming", ""], ["Lin", "Xuelian", ""], ["Huai", "Jinpeng", ""]]}, {"id": "1411.4516", "submitter": "Marco Montali", "authors": "Diego Calvanese and Giorgio Delzanno and Marco Montali", "title": "Verification of Relational Multiagent Systems with Data Types (Extended\n  Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the extension of relational multiagent systems (RMASs), where agents\nmanipulate full-fledged relational databases, with data types and facets\nequipped with domain-specific, rigid relations (such as total orders).\nSpecifically, we focus on design-time verification of RMASs against rich\nfirst-order temporal properties expressed in a variant of first-order\nmu-calculus with quantification across states. We build on previous\ndecidability results under the \"state-bounded\" assumption, i.e., in each single\nstate only a bounded number of data objects is stored in the agent databases,\nwhile unboundedly many can be encountered over time. We recast this condition,\nshowing decidability in presence of dense, linear orders, and facets defined on\ntop of them. Our approach is based on the construction of a finite-state, sound\nand complete abstraction of the original system, in which dense linear orders\nare reformulated as non-rigid relations working on the active domain of the\nsystem only. We also show undecidability when including a data type equipped\nwith the successor relation.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 15:49:35 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Calvanese", "Diego", ""], ["Delzanno", "Giorgio", ""], ["Montali", "Marco", ""]]}, {"id": "1411.4940", "submitter": "Xiaofeng Xu", "authors": "Xiaofeng Xu, Li Xiong, Vaidy Sunderam, Jinfei Liu and Jun Luo", "title": "Speed Partitioning for Indexing Moving Objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indexing moving objects has been extensively studied in the past decades.\nMoving objects, such as vehicles and mobile device users, usually exhibit some\npatterns on their velocities, which can be utilized for velocity-based\npartitioning to improve performance of the indexes. Existing velocity-based\npartitioning techniques rely on some kinds of heuristics rather than\nanalytically calculate the optimal solution. In this paper, we propose a novel\nspeed partitioning technique based on a formal analysis over speed values of\nthe moving objects. We first show that speed partitioning will significantly\nreduce the search space expansion which has direct impacts on query performance\nof the indexes. Next we formulate the optimal speed partitioning problem based\non search space expansion analysis and then compute the optimal solution using\ndynamic programming. We then build the partitioned indexing system where\nqueries are duplicated and processed in each index partition. Extensive\nexperiments demonstrate that our method dramatically improves the performance\nof indexes for moving objects and outperforms other state-of-the-art\nvelocity-based partitioning approaches.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 17:50:46 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2015 14:28:33 GMT"}], "update_date": "2015-04-23", "authors_parsed": [["Xu", "Xiaofeng", ""], ["Xiong", "Li", ""], ["Sunderam", "Vaidy", ""], ["Liu", "Jinfei", ""], ["Luo", "Jun", ""]]}, {"id": "1411.5014", "submitter": "Shubhanshu Gupta", "authors": "Shubhanshu Gupta", "title": "Music Data Analysis: A State-of-the-art Survey", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Music accounts for a significant chunk of interest among various online\nactivities. This is reflected by wide array of alternatives offered in music\nrelated web/mobile apps, information portals, featuring millions of artists,\nsongs and events attracting user activity at similar scale. Availability of\nlarge scale structured and unstructured data has attracted similar level of\nattention by data science community. This paper attempts to offer current\nstate-of-the-art in music related analysis. Various approaches involving\nmachine learning, information theory, social network analysis, semantic web and\nlinked open data are represented in the form of taxonomy along with data\nsources and use cases addressed by the research community.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 14:19:28 GMT"}], "update_date": "2014-11-20", "authors_parsed": [["Gupta", "Shubhanshu", ""]]}, {"id": "1411.5196", "submitter": "Bernardo Gon\\c{c}alves", "authors": "Bernardo Gon\\c{c}alves and Fabio Porto", "title": "Design-theoretic encoding of deterministic hypotheses as constraints and\n  correlations into U-relational databases", "comments": "17 pages, 7 figures, submitted to ACM PODS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In view of the paradigm shift that makes science ever more data-driven, in\nthis paper we consider deterministic scientific hypotheses as uncertain data.\nIn the form of mathematical equations, hypotheses symmetrically relate aspects\nof the studied phenomena. For computing predictions, however, deterministic\nhypotheses are used asymmetrically as functions. We refer to Simon's notion of\nstructural equations in order to extract the (so-called) causal ordering\nembedded in a hypothesis. Then we encode it into a set of functional\ndependencies (fd's) that is basic input to a design-theoretic method for the\nsynthesis of U-relational databases (DB's). The causal ordering captured from a\nformally-specified system of mathematical equations into fd's determines not\nonly the constraints (structure), but also the correlations (uncertainty\nchaining) hidden in the hypothesis predictive data. We show how to process it\neffectively through original algorithms for encoding and reasoning on the given\nhypotheses as constraints and correlations into U-relational DB's. The method\nis applicable to both quantitative and qualitative hypotheses and has underwent\ninitial tests in a realistic use case from computational science.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2014 12:20:55 GMT"}], "update_date": "2014-11-20", "authors_parsed": [["Gon\u00e7alves", "Bernardo", ""], ["Porto", "Fabio", ""]]}, {"id": "1411.5220", "submitter": "Heng Zhang", "authors": "Heng Zhang, Yan Zhang, Jia-Huai You", "title": "Existential Rule Languages with Finite Chase: Complexity and\n  Expressiveness", "comments": "Extended version of a paper to appear on AAAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite chase, or alternatively chase termination, is an important condition\nto ensure the decidability of existential rule languages. In the past few\nyears, a number of rule languages with finite chase have been studied. In this\nwork, we propose a novel approach for classifying the rule languages with\nfinite chase. Using this approach, a family of decidable rule languages, which\nextend the existing languages with the finite chase property, are naturally\ndefined. We then study the complexity of these languages. Although all of them\nare tractable for data complexity, we show that their combined complexity can\nbe arbitrarily high. Furthermore, we prove that all the rule languages with\nfinite chase that extend the weakly acyclic language are of the same\nexpressiveness as the weakly acyclic one, while rule languages with higher\ncombined complexity are in general more succinct than those with lower combined\ncomplexity.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2014 13:37:22 GMT"}, {"version": "v2", "created": "Sat, 22 Nov 2014 05:36:49 GMT"}, {"version": "v3", "created": "Thu, 8 Jan 2015 22:53:11 GMT"}], "update_date": "2015-01-12", "authors_parsed": [["Zhang", "Heng", ""], ["Zhang", "Yan", ""], ["You", "Jia-Huai", ""]]}, {"id": "1411.5943", "submitter": "Krzysztof Nienartowicz", "authors": "Krzysztof Nienartowicz, Diego Ord\\'o\\~nez Blanco, Leanne Guy, Berry\n  Holl, Isabelle Lecoeur-Ta\\\"ibi, Nami Mowlavi, Lorenzo Rimoldini, Idoia Ruiz,\n  Maria S\\\"uveges, Laurent Eyer", "title": "Time series data mining for the Gaia variability analysis", "comments": "4 pages, 3 figures. appears in the Proc. of the 2014 conference on\n  Big Data from Space (BiDS14), European Commission, Joint Research Centre, P.\n  Soille, P. G. Marchetti (eds)", "journal-ref": null, "doi": "10.2788/1823", "report-no": null, "categories": "astro-ph.IM cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaia is an ESA cornerstone mission, which was successfully launched December\n2013 and commenced operations in July 2014. Within the Gaia Data Processing and\nAnalysis consortium, Coordination Unit 7 (CU7) is responsible for the\nvariability analysis of over a billion celestial sources and nearly 4 billion\nassociated time series (photometric, spectrophotometric, and spectroscopic),\nencoding information in over 800 billion observations during the 5 years of the\nmission, resulting in a petabyte scale analytical problem. In this article, we\nbriefly describe the solutions we developed to address the challenges of time\nseries variability analysis: from the structure for a distributed data-oriented\nscientific collaboration to architectural choices and specific components used.\nOur approach is based on Open Source components with a distributed, partitioned\ndatabase as the core to handle incrementally: ingestion, distributed\nprocessing, analysis, results and export in a constrained time window.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2014 16:32:19 GMT"}], "update_date": "2014-11-24", "authors_parsed": [["Nienartowicz", "Krzysztof", ""], ["Blanco", "Diego Ord\u00f3\u00f1ez", ""], ["Guy", "Leanne", ""], ["Holl", "Berry", ""], ["Lecoeur-Ta\u00efbi", "Isabelle", ""], ["Mowlavi", "Nami", ""], ["Rimoldini", "Lorenzo", ""], ["Ruiz", "Idoia", ""], ["S\u00fcveges", "Maria", ""], ["Eyer", "Laurent", ""]]}, {"id": "1411.6224", "submitter": "Akshita Bhandari", "authors": "Akshita Bhandari, Ashutosh Gupta, Debasis Das", "title": "Improvised Apriori Algorithm using frequent pattern tree for real time\n  applications in data mining", "comments": "7 pages, 4 figures, 7 tables, published in Elsevier Procedia in\n  Computer Science and presented in ICICT 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Apriori Algorithm is one of the most important algorithm which is used to\nextract frequent itemsets from large database and get the association rule for\ndiscovering the knowledge. It basically requires two important things: minimum\nsupport and minimum confidence. First, we check whether the items are greater\nthan or equal to the minimum support and we find the frequent itemsets\nrespectively. Secondly, the minimum confidence constraint is used to form\nassociation rules. Based on this algorithm, this paper indicates the limitation\nof the original Apriori algorithm of wasting time and space for scanning the\nwhole database searching on the frequent itemsets, and present an improvement\non Apriori.\n", "versions": [{"version": "v1", "created": "Sun, 23 Nov 2014 10:55:19 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Bhandari", "Akshita", ""], ["Gupta", "Ashutosh", ""], ["Das", "Debasis", ""]]}, {"id": "1411.6335", "submitter": "Peng Peng", "authors": "Peng Peng, Lei Zou, Dongyan Zhao", "title": "On The Marriage of SPARQL and Keywords", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although SPARQL has been the predominant query language over RDF graphs, some\nquery intentions cannot be well captured by only using SPARQL syntax. On the\nother hand, the keyword search enjoys widespread usage because of its intuitive\nway of specifying information needs but suffers from the problem of low\nprecision. To maximize the advantages of both SPARQL and keyword search, we\nintroduce a novel paradigm that combines both of them and propose a hybrid\nquery (called an SK query) that integrates SPARQL and keyword search. In order\nto answer SK queries efficiently, a structural index is devised, based on a\nnovel integrated query algorithm is proposed. We evaluate our method in large\nreal RDF graphs and experiments demonstrate both effectiveness and efficiency\nof our method.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 02:53:49 GMT"}, {"version": "v2", "created": "Tue, 25 Nov 2014 07:56:07 GMT"}, {"version": "v3", "created": "Sun, 30 Nov 2014 07:53:24 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Peng", "Peng", ""], ["Zou", "Lei", ""], ["Zhao", "Dongyan", ""]]}, {"id": "1411.6562", "submitter": "Manas Joglekar", "authors": "Manas Joglekar, Hector Garcia-Molina, Aditya Parameswaran", "title": "Evaluating the Crowd with Confidence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Worker quality control is a crucial aspect of crowdsourcing systems;\ntypically occupying a large fraction of the time and money invested on\ncrowdsourcing. In this work, we devise techniques to generate confidence\nintervals for worker error rate estimates, thereby enabling a better evaluation\nof worker quality. We show that our techniques generate correct confidence\nintervals on a range of real-world datasets, and demonstrate wide applicability\nby using them to evict poorly performing workers, and provide confidence\nintervals on the accuracy of the answers.\n", "versions": [{"version": "v1", "created": "Wed, 12 Nov 2014 23:50:43 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Joglekar", "Manas", ""], ["Garcia-Molina", "Hector", ""], ["Parameswaran", "Aditya", ""]]}, {"id": "1411.6704", "submitter": "Bikash Chandra", "authors": "Bikash Chandra, Bhupesh Chawda, Biplab Kar, K. V. Maheshwara Reddy,\n  Shetal Shah, S. Sudarshan", "title": "Data Generation for Testing and Grading SQL Queries", "comments": "34 pages, The final publication is available at Springer via\n  http://dx.doi.org/10.1007/s00778-015-0395-0", "journal-ref": null, "doi": "10.1007/s00778-015-0395-0", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correctness of SQL queries is usually tested by executing the queries on one\nor more datasets. Erroneous queries are often the results of small changes, or\nmutations of the correct query. A mutation Q' of a query Q is killed by a\ndataset D if Q(D) $\\neq$ Q'(D). Earlier work on the XData system showed how to\ngenerate datasets that kill all mutations in a class of mutations that included\njoin type and comparison operation mutations.\n  In this paper, we extend the XData data generation techniques to handle a\nwider variety of SQL queries and a much larger class of mutations. We have also\nbuilt a system for grading SQL queries using the datasets generated by XData.\nWe present a study of the effectiveness of the datasets generated by the\nextended XData approach, using a variety of queries including queries submitted\nby students as part of a database course. We show that the XData datasets\noutperform predefined datasets as well as manual grading done earlier by\nteaching assistants, while also avoiding the drudgery of manual correction.\nThus, we believe that our techniques will be of great value to database course\ninstructors and TAs, particularly to those of MOOCs. It will also be valuable\nto database application developers and testers for testing SQL queries.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2014 02:06:02 GMT"}, {"version": "v2", "created": "Tue, 9 Dec 2014 14:13:18 GMT"}, {"version": "v3", "created": "Wed, 13 May 2015 04:33:40 GMT"}, {"version": "v4", "created": "Mon, 13 Jul 2015 11:57:44 GMT"}, {"version": "v5", "created": "Tue, 2 May 2017 10:46:40 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Chandra", "Bikash", ""], ["Chawda", "Bhupesh", ""], ["Kar", "Biplab", ""], ["Reddy", "K. V. Maheshwara", ""], ["Shah", "Shetal", ""], ["Sudarshan", "S.", ""]]}, {"id": "1411.6763", "submitter": "Peng Peng", "authors": "Peng Peng, Lei Zou, M. Tamer \\\"Ozsu, Lei Chen, Dongyan Zhao", "title": "Processing SPARQL Queries Over Distributed RDF Graphs", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose techniques for processing SPARQL queries over a large RDF graph in\na distributed environment. We adopt a \"partial evaluation and assembly\"\nframework. Answering a SPARQL query Q is equivalent to finding subgraph matches\nof the query graph Q over RDF graph G. Based on properties of subgraph matching\nover a distributed graph, we introduce local partial match as partial answers\nin each fragment of RDF graph G. For assembly, we propose two methods:\ncentralized and distributed assembly. We analyze our algorithms from both\ntheoretically and experimentally. Extensive experiments over both real and\nbenchmark RDF repositories of billions of triples confirm that our method is\nsuperior to the state-of-the-art methods in both the system's performance and\nscalability.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2014 08:36:58 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2015 07:20:19 GMT"}, {"version": "v3", "created": "Sat, 27 Feb 2016 03:00:53 GMT"}, {"version": "v4", "created": "Mon, 21 Mar 2016 13:58:00 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Peng", "Peng", ""], ["Zou", "Lei", ""], ["\u00d6zsu", "M. Tamer", ""], ["Chen", "Lei", ""], ["Zhao", "Dongyan", ""]]}, {"id": "1411.7343", "submitter": "Abm Moniruzzaman", "authors": "A B M Moniruzzaman", "title": "NewSQL: Towards Next-Generation Scalable RDBMS for Online Transaction\n  Processing (OLTP) for Big Data Management", "comments": "11 pages, 04 figures and 04 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  One of the key advances in resolving the big-data problem has been the\nemergence of an alternative database technology. Today, classic RDBMS are\ncomplemented by a rich set of alternative Data Management Systems (DMS)\nspecially designed to handle the volume, variety, velocity and variability\nofBig Data collections; these DMS include NoSQL, NewSQL and Search-based\nsystems. NewSQL is a class of modern relational database management systems\n(RDBMS) that provide the same scalable performance of NoSQL systems for online\ntransaction processing (OLTP) read-write workloads while still maintaining the\nACID guarantees of a traditional database system. This paper discusses about\nNewSQL data management system; and compares with NoSQL and with traditional\ndatabase system. This paper covers architecture, characteristics,\nclassification of NewSQL databases for online transaction processing (OLTP) for\nBig data management. It also provides the list ofpopular NoSQL as well as\nNewSQL databases in separate categorized tables. This paper compares SQL based\nRDBMS, NoSQL and NewSQL databases with set of metrics; as well as, addressed\nsome research issues ofNoSQL and NewSQL.\n", "versions": [{"version": "v1", "created": "Wed, 26 Nov 2014 19:28:46 GMT"}], "update_date": "2014-11-27", "authors_parsed": [["Moniruzzaman", "A B M", ""]]}, {"id": "1411.7419", "submitter": "Bernardo Gon\\c{c}alves", "authors": "Bernardo Gon\\c{c}alves, Frederico C. Silva, Fabio Porto", "title": "$\\Upsilon$-DB: A system for data-driven hypothesis management and\n  analytics", "comments": "6 pages, 7 figures, submitted to ACM SIGMOD 2015, Demo track. arXiv\n  admin note: substantial text overlap with arXiv:1411.5196", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vision of $\\Upsilon$-DB introduces deterministic scientific hypotheses as\na kind of uncertain and probabilistic data, and opens some key technical\nchallenges for enabling data-driven hypothesis management and analytics. The\n$\\Upsilon$-DB system addresses those challenges throughout a\ndesign-by-synthesis pipeline that defines its architecture. It processes\nhypotheses from their XML-based extraction to encoding as uncertain and\nprobabilistic U-relational data, and eventually to their conditioning in the\npresence of observations. In this demo we present a first prototype of the\n$\\Upsilon$-DB system. We showcase its core innovative features by means of use\ncase scenarios in computational science in which the hypotheses are extracted\nfrom a model repository on the web and evaluated (rated/ranked) as\nprobabilistic data.\n", "versions": [{"version": "v1", "created": "Wed, 26 Nov 2014 23:04:34 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Gon\u00e7alves", "Bernardo", ""], ["Silva", "Frederico C.", ""], ["Porto", "Fabio", ""]]}, {"id": "1411.7469", "submitter": "Sanjay Chakraborty", "authors": "Lopamudra Dey and Sanjay Chakraborty", "title": "Canonical PSO Based k-Means Clustering Approach for Real Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  \"Clustering\" the significance and application of this technique is spread\nover various fields. Clustering is an unsupervised process in data mining, that\nis why the proper evaluation of the results and measuring the compactness and\nseparability of the clusters are important issues.The procedure of evaluating\nthe results of a clustering algorithm is known as cluster validity measure.\nDifferent types of indexes are used to solve different types of problems and\nindices selection depends on the kind of available data.This paper first\nproposes Canonical PSO based K-means clustering algorithm and also analyses\nsome important clustering indices (intercluster, intracluster) and then\nevaluates the effects of those indices on real-time air pollution\ndatabase,wholesale customer, wine, and vehicle datasets using typical K-means,\nCanonical PSO based K-means, simple PSO based K-means,DBSCAN, and Hierarchical\nclustering algorithms.This paper also describes the nature of the clusters and\nfinally compares the performances of these clustering algorithms according to\nthe validity assessment. It also defines which algorithm will be more desirable\namong all these algorithms to make proper compact clusters on this particular\nreal life datasets. It actually deals with the behaviour of these clustering\nalgorithms with respect to validation indexes and represents their results of\nevaluation in terms of mathematical and graphical forms.\n", "versions": [{"version": "v1", "created": "Thu, 27 Nov 2014 04:50:30 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Dey", "Lopamudra", ""], ["Chakraborty", "Sanjay", ""]]}]