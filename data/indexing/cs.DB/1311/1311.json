[{"id": "1311.0059", "submitter": "Jian Wen", "authors": "Jian Wen, Vinayak R. Borkar, Michael J. Carey and Vassilis J. Tsotras", "title": "Revisiting Aggregation for Data Intensive Applications: A Performance\n  Study", "comments": "25 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aggregation has been an important operation since the early days of\nrelational databases. Today's Big Data applications bring further challenges\nwhen processing aggregation queries, demanding adaptive aggregation algorithms\nthat can process large volumes of data relative to a potentially limited memory\nbudget (especially in multiuser settings). Despite its importance, the design\nand evaluation of aggregation algorithms has not received the same attention\nthat other basic operators, such as joins, have received in the literature. As\na result, when considering which aggregation algorithm(s) to implement in a new\nparallel Big Data processing platform (AsterixDB), we faced a lack of \"off the\nshelf\" answers that we could simply read about and then implement based on\nprior performance studies.\n  In this paper we revisit the engineering of efficient local aggregation\nalgorithms for use in Big Data platforms. We discuss the salient implementation\ndetails of several candidate algorithms and present an in-depth experimental\nperformance study to guide future Big Data engine developers. We show that the\nefficient implementation of the aggregation operator for a Big Data platform is\nnon-trivial and that many factors, including memory usage, spilling strategy,\nand I/O and CPU cost, should be considered. Further, we introduce precise cost\nmodels that can help in choosing an appropriate algorithm based on input\nparameters including memory budget, grouping key cardinality, and data skew.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2013 22:51:50 GMT"}], "update_date": "2013-11-04", "authors_parsed": [["Wen", "Jian", ""], ["Borkar", "Vinayak R.", ""], ["Carey", "Michael J.", ""], ["Tsotras", "Vassilis J.", ""]]}, {"id": "1311.0320", "submitter": "Zhijie Wang", "authors": "Jack Wang", "title": "An Improved Solution for Restricted and Uncertain TRQ", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CSPTRQ is an interesting problem and its has attracted much attention. The\nCSPTRQ is a variant of the traditional PTRQ. As objects moving in a\nconstrained-space are common, clearly, it can also find many applications. At\nthe first sight, our problem can be easily tackled by extending existing\nmethods used to answer the PTRQ. Unfortunately, those classical techniques are\nnot well suitable for our problem, due to a set of new challenges. We develop\ntargeted solutions and demonstrate the efficiency and effectiveness of the\nproposed methods through extensive experiments.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2013 22:52:00 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2013 02:13:15 GMT"}, {"version": "v3", "created": "Tue, 3 Jul 2018 15:27:41 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Wang", "Jack", ""]]}, {"id": "1311.0350", "submitter": "Thabet Slimani", "authors": "Thabet Slimani, Amor Lazzez", "title": "Sequential Mining: Patterns and Algorithms Analysis", "comments": "10 pages", "journal-ref": "International Journal of Computer and Electronics Research, Volume\n  2, Issue 5, October 2013, pp 639-647", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents and analysis the common existing sequential pattern\nmining algorithms. It presents a classifying study of sequential pattern-mining\nalgorithms into five extensive classes. First, on the basis of Apriori-based\nalgorithm, second on Breadth First Search-based strategy, third on Depth First\nSearch strategy, fourth on sequential closed-pattern algorithm and five on the\nbasis of incremental pattern mining algorithms. At the end, a comparative\nanalysis is done on the basis of important key features supported by various\nalgorithms. This study gives an enhancement in the understanding of the\napproaches of sequential pattern mining.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2013 06:55:10 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Slimani", "Thabet", ""], ["Lazzez", "Amor", ""]]}, {"id": "1311.0505", "submitter": "Dang Hoan Tran", "authors": "Dang-Hoan Tran", "title": "Automated Change Detection and Reactive Clustering in Multivariate\n  Streaming Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many automated systems need the capability of automatic change detection\nwithout the given detection threshold. This paper presents an automated change\ndetection algorithm in streaming multivariate data. Two overlapping windows are\nused to quantify the changes. While a window is used as the reference window\nfrom which the clustering is created, the other called the current window\ncaptures the newly incoming data points. A newly incoming data point can be\nconsidered a change point if it is not a member of any cluster. As our\nclustering-based change detector does not require detection threshold, it is an\nautomated detector. Based on this change detector, we propose a reactive\nclustering algorithm for streaming data. Our empirical results show that, our\nclustering-based change detector works well with multivariate streaming data.\nThe detection accuracy depends on the number of clusters in the reference\nwindow, the window width.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2013 18:48:50 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Tran", "Dang-Hoan", ""]]}, {"id": "1311.0536", "submitter": "Nikos Bikakis", "authors": "Nikos Bikakis, Chrisa Tsinaraki, Ioannis Stavrakantonakis, Nektarios\n  Gioldasis, Stavros Christodoulakis", "title": "The SPARQL2XQuery Interoperability Framework. Utilizing Schema Mapping,\n  Schema Transformation and Query Translation to Integrate XML and the Semantic\n  Web", "comments": "To appear in World Wide Web Journal (WWWJ), Springer 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Web of Data is an open environment consisting of a great number of large\ninter-linked RDF datasets from various domains. In this environment,\norganizations and companies adopt the Linked Data practices utilizing Semantic\nWeb (SW) technologies, in order to publish their data and offer SPARQL\nendpoints (i.e., SPARQL-based search services). On the other hand, the dominant\nstandard for information exchange in the Web today is XML. The SW and XML\nworlds and their developed infrastructures are based on different data models,\nsemantics and query languages. Thus, it is crucial to develop interoperability\nmechanisms that allow the Web of Data users to access XML datasets, using\nSPARQL, from their own working environments. It is unrealistic to expect that\nall the existing legacy data (e.g., Relational, XML, etc.) will be transformed\ninto SW data. Therefore, publishing legacy data as Linked Data and providing\nSPARQL endpoints over them has become a major research challenge. In this\ndirection, we introduce the SPARQL2XQuery Framework which creates an\ninteroperable environment, where SPARQL queries are automatically translated to\nXQuery queries, in order to access XML data across the Web. The SPARQL2XQuery\nFramework provides a mapping model for the expression of OWL-RDF/S to XML\nSchema mappings as well as a method for SPARQL to XQuery translation. To this\nend, our Framework supports both manual and automatic mapping specification\nbetween ontologies and XML Schemas. In the automatic mapping specification\nscenario, the SPARQL2XQuery exploits the XS2OWL component which transforms XML\nSchemas into OWL ontologies. Finally, extensive experiments have been conducted\nin order to evaluate the schema transformation, mapping generation, query\ntranslation and query evaluation efficiency, using both real and synthetic\ndatasets.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2013 21:57:48 GMT"}, {"version": "v2", "created": "Thu, 26 Dec 2013 00:20:14 GMT"}, {"version": "v3", "created": "Thu, 2 Jan 2014 02:53:19 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Bikakis", "Nikos", ""], ["Tsinaraki", "Chrisa", ""], ["Stavrakantonakis", "Ioannis", ""], ["Gioldasis", "Nektarios", ""], ["Christodoulakis", "Stavros", ""]]}, {"id": "1311.0805", "submitter": "Nikolaos Korfiatis", "authors": "Todor Ivanov, Nikolaos Korfiatis, Roberto V. Zicari", "title": "On the inequality of the 3V's of Big Data Architectural Paradigms: A\n  case for heterogeneity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The well-known 3V architectural paradigm for Big Data introduced by Laney\n(2011), provides a simplified framework for defining the architecture of a big\ndata platform to be deployed in various scenarios tackling processing of\nmassive datasets. While additional components such as Variability and Veracity\nhave been discussed as an extension to the 3V model, the basic components\n(volume, variety, velocity) provide a quantitative framework while variability\nand veracity target a more qualitative approach. In this paper we argue why the\nbasic 3V's are not equal due to the different requirements that need to be\ncovered in case higher demands for a particular \"V\". Similar to other\nconjectures such as the CAP theorem 3V based architectures differ on their\nimplementation. We call this paradigm heterogeneity and we provide a taxonomy\nof the existing tools (as of 2013) covering the Hadoop ecosystem from the\nperspective of heterogeneity. This paper contributes on the understanding of\nthe Hadoop ecosystem from the perspective of different workloads and aims to\nhelp researchers and practitioners on the design of scalable platforms\ntargeting different operational needs.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2013 18:29:45 GMT"}, {"version": "v2", "created": "Sat, 16 Nov 2013 19:21:45 GMT"}], "update_date": "2013-11-19", "authors_parsed": [["Ivanov", "Todor", ""], ["Korfiatis", "Nikolaos", ""], ["Zicari", "Roberto V.", ""]]}, {"id": "1311.0841", "submitter": "L\\'aszl\\'o Dobos", "authors": "L\\'aszl\\'o Dobos, J\\'anos Sz\\\"ule, Tam\\'as Bodn\\'ar, Tam\\'as Hanyecz,\n  Tam\\'as Seb\\H{o}k, D\\'aniel Kondor, Zs\\'ofia Kallus, J\\'ozsef St\\'eger,\n  Istv\\'an Csabai and G\\'abor Vattay", "title": "A multi-terabyte relational database for geo-tagged social network data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite their relatively low sampling factor, the freely available, randomly\nsampled status streams of Twitter are very useful sources of geographically\nembedded social network data. To statistically analyze the information Twitter\nprovides via these streams, we have collected a year's worth of data and built\na multi-terabyte relational database from it. The database is designed for fast\ndata loading and to support a wide range of studies focusing on the statistics\nand geographic features of social networks, as well as on the linguistic\nanalysis of tweets. In this paper we present the method of data collection, the\ndatabase design, the data loading procedure and special treatment of geo-tagged\nand multi-lingual data. We also provide some SQL recipes for computing network\nstatistics.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2013 20:39:16 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2013 19:26:29 GMT"}], "update_date": "2013-11-06", "authors_parsed": [["Dobos", "L\u00e1szl\u00f3", ""], ["Sz\u00fcle", "J\u00e1nos", ""], ["Bodn\u00e1r", "Tam\u00e1s", ""], ["Hanyecz", "Tam\u00e1s", ""], ["Seb\u0151k", "Tam\u00e1s", ""], ["Kondor", "D\u00e1niel", ""], ["Kallus", "Zs\u00f3fia", ""], ["St\u00e9ger", "J\u00f3zsef", ""], ["Csabai", "Istv\u00e1n", ""], ["Vattay", "G\u00e1bor", ""]]}, {"id": "1311.1181", "submitter": "Imane Hilal Ms", "authors": "I. Hilal, N. Afifi, R. Filali Hilali, M. Ouzzif", "title": "Toward a New Approach for Modeling Dependability of Data Warehouse\n  System", "comments": "10 pages, (IJCSIS) International Journal of Computer Science and\n  Information Security, Vol. 11, No. 6, June 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SE", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The sustainability of any Data Warehouse System (DWS) is closely correlated\nwith user satisfaction. Therefore, analysts, designers and developers focused\nmore on achieving all its functionality, without considering others kinds of\nrequirement such as dependability s aspects. Moreover, these latter are often\nconsidered as properties of the system that will must be checked and corrected\nonce the project is completed. The practice of \"fix it later\" can cause the\nobsolescence of the entire Data Warehouse System. Therefore, it requires the\nadoption of a methodology that will ensure the integration of aspects of\ndependability since the early stages of project DWS. In this paper, we first\ndefine the concepts related to dependability of DWS. Then we present our\napproach inspired from the MDA (Model Driven Architecture) approach to model\ndependability s aspects namely: availability, reliability, maintainability and\nsecurity, taking into account their interaction.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2013 15:19:04 GMT"}], "update_date": "2013-11-06", "authors_parsed": [["Hilal", "I.", ""], ["Afifi", "N.", ""], ["Hilali", "R. Filali", ""], ["Ouzzif", "M.", ""]]}, {"id": "1311.1626", "submitter": "Hasan Jamil", "authors": "Belma Yelbay, S. Ilker Birbil, Kerem Bulbul, and Hasan M. Jamil", "title": "Trade-offs Computing Minimum Hub Cover toward Optimized Graph Query\n  Processing", "comments": "12 pages, 6 figures and 2 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  As techniques for graph query processing mature, the need for optimization is\nincreasingly becoming an imperative. Indices are one of the key ingredients\ntoward efficient query processing strategies via cost-based optimization. Due\nto the apparent absence of a common representation model, it is difficult to\nmake a focused effort toward developing access structures, metrics to evaluate\nquery costs, and choose alternatives. In this context, recent interests in\ncovering-based graph matching appears to be a promising direction of research.\nIn this paper, our goal is to formally introduce a new graph representation\nmodel, called Minimum Hub Cover, and demonstrate that this representation\noffers interesting strategic advantages, facilitates construction of candidate\ngraphs from graph fragments, and helps leverage indices in novel ways for query\noptimization. However, similar to other covering problems, minimum hub cover is\nNP-hard, and thus is a natural candidate for optimization. We claim that\ncomputing the minimum hub cover leads to substantial cost reduction for graph\nquery processing. We present a computational characterization of minimum hub\ncover based on integer programming to substantiate our claim and investigate\nits computational cost on various graph types.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2013 10:17:24 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2013 01:48:52 GMT"}], "update_date": "2013-11-11", "authors_parsed": [["Yelbay", "Belma", ""], ["Birbil", "S. Ilker", ""], ["Bulbul", "Kerem", ""], ["Jamil", "Hasan M.", ""]]}, {"id": "1311.1764", "submitter": "Amel Grissa", "authors": "Amel Grissa Touzi, Hela Ben Massoud and Alaya Ayadi", "title": "Automatic ontology generation for data mining using fca and clustering", "comments": "10pages, 8 figures KEOD 2013, accepted but not enregistrement De:\n  KEOD Secretariat [keod.secretariat@insticc.org] Envoy\\'e: mardi 21 mai 2013\n  10:47 We are happy to inform you that the regular paper you have submitted to\n  KEOD, with number 34, entitled \"Automatic Ontology Generation for Data Mining\n  Using FCA and Clustering\", has been accepted as a Short Paper. arXiv admin\n  note: text overlap with arXiv:1310.7829 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the increased need for formalized representations of the domain\nof Data Mining, the success of using Formal Concept Analysis (FCA) and Ontology\nin several Computer Science fields, we present in this paper a new approach for\nautomatic generation of Fuzzy Ontology of Data Mining (FODM), through the\nfusion of conceptual clustering, fuzzy logic, and FCA. In our approach, we\npropose to generate ontology taking in consideration another degree of\ngranularity into the process of generation. Indeed, we suggest to define an\nontology between classes resulting from a preliminary classification on the\ndata. We prove that this approach optimize the definition of the ontology,\noffered a better interpretation of the data and optimized both the space memory\nand the execution time for exploiting this data.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2013 17:48:24 GMT"}], "update_date": "2013-11-08", "authors_parsed": [["Touzi", "Amel Grissa", ""], ["Massoud", "Hela Ben", ""], ["Ayadi", "Alaya", ""]]}, {"id": "1311.2100", "submitter": "Nandish Jayaram", "authors": "Nandish Jayaram and Arijit Khan and Chengkai Li and Xifeng Yan and\n  Ramez Elmasri", "title": "Querying Knowledge Graphs by Example Entity Tuples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We witness an unprecedented proliferation of knowledge graphs that record\nmillions of entities and their relationships. While knowledge graphs are\nstructure-flexible and content rich, they are difficult to use. The challenge\nlies in the gap between their overwhelming complexity and the limited database\nknowledge of non-professional users. If writing structured queries over simple\ntables is difficult, complex graphs are only harder to query. As an initial\nstep toward improving the usability of knowledge graphs, we propose to query\nsuch data by example entity tuples, without requiring users to form complex\ngraph queries. Our system, GQBE (Graph Query By Example), automatically derives\na weighted hidden maximal query graph based on input query tuples, to capture a\nuser's query intent. It efficiently finds and ranks the top approximate answer\ntuples. For fast query processing, GQBE only partially evaluates query graphs.\nWe conducted experiments and user studies on the large Freebase and DBpedia\ndatasets and observed appealing accuracy and efficiency. Our system provides a\ncomplementary approach to the existing keyword-based methods, facilitating\nuser-friendly graph querying. To the best of our knowledge, there was no such\nproposal in the past in the context of graphs.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2013 22:47:39 GMT"}], "update_date": "2013-11-12", "authors_parsed": [["Jayaram", "Nandish", ""], ["Khan", "Arijit", ""], ["Li", "Chengkai", ""], ["Yan", "Xifeng", ""], ["Elmasri", "Ramez", ""]]}, {"id": "1311.2342", "submitter": "Carlos R. Rivero", "authors": "Carlos R. Rivero, Hasan M. Jamil", "title": "Anatomy of Graph Matching based on an XQuery and RDF Implementation", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs are becoming one of the most popular data modeling paradigms since\nthey are able to model complex relationships that cannot be easily captured\nusing traditional data models. One of the major tasks of graph management is\ngraph matching, which aims to find all of the subgraphs in a data graph that\nmatch a query graph. In the literature, proposals in this context are\nclassified into two different categories: graph-at-a-time, which process the\nwhole query graph at the same time, and vertex-at-a-time, which process a\nsingle vertex of the query graph at the same time. In this paper, we propose a\nnew vertex-at-a-time proposal that is based on graphlets, each of which\ncomprises a vertex of a graph, all of the immediate neighbors of that vertex,\nand all of the edges that relate those neighbors. Furthermore, we also use the\nconcept of minimum hub covers, each of which comprises a subset of vertices in\nthe query graph that account for all of the edges in that graph. We present the\nalgorithms of our proposal and describe an implementation based on XQuery and\nRDF. Our evaluation results show that our proposal is appealing to perform\ngraph matching.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2013 03:15:38 GMT"}], "update_date": "2013-11-12", "authors_parsed": [["Rivero", "Carlos R.", ""], ["Jamil", "Hasan M.", ""]]}, {"id": "1311.3312", "submitter": "Thomas Cerqueus", "authors": "Vanessa Ayala-Rivera, Patrick McDonagh, Thomas Cerqueus, Liam Murphy", "title": "Synthetic Data Generation using Benerator Tool", "comments": "12 pages, 5 figures, 10 references", "journal-ref": null, "doi": null, "report-no": "UCD-CSI-2013-03", "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Datasets of different characteristics are needed by the research community\nfor experimental purposes. However, real data may be difficult to obtain due to\nprivacy concerns. Moreover, real data may not meet specific characteristics\nwhich are needed to verify new approaches under certain conditions. Given these\nlimitations, the use of synthetic data is a viable alternative to complement\nthe real data. In this report, we describe the process followed to generate\nsynthetic data using Benerator, a publicly available tool. The results show\nthat the synthetic data preserves a high level of accuracy compared to the\noriginal data. The generated datasets correspond to microdata containing\nrecords with social, economic and demographic data which mimics the\ndistribution of aggregated statistics from the 2011 Irish Census data.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2013 21:14:40 GMT"}], "update_date": "2013-11-15", "authors_parsed": [["Ayala-Rivera", "Vanessa", ""], ["McDonagh", "Patrick", ""], ["Cerqueus", "Thomas", ""], ["Murphy", "Liam", ""]]}, {"id": "1311.3355", "submitter": "Yongqun He", "authors": "Yongqun He, Zoushuang Xiang", "title": "HINO: a BFO-aligned ontology representing human molecular interactions\n  and pathways", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB q-bio.MN", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Many database resources, such as Reactome, collect manually annotated\nreactions, interactions, and pathways from peer-reviewed publications. The\ninteractors (e.g., a protein), interactions, and pathways in these data\nresources are often represented as instances in using BioPAX, a standard\npathway data exchange format. However, these interactions are better\nrepresented as classes (or universals) since they always occur given\nappropriate conditions. This study aims to represent various human interaction\npathways and networks as classes via a formal ontology aligned with the Basic\nFormal Ontology (BFO). Towards this goal, the Human Interaction Network\nOntology (HINO) was generated by extending the BFO-aligned Interaction Network\nOntology (INO). All human pathways and associated processes and interactors\nlisted in Reactome and represented in BioPAX were first converted to ontology\nclasses by aligning them under INO. Related terms and associated relations and\nhierarchies from external ontologies (e.g., CHEBI and GO) were also retrieved\nand imported into HINO. HINO ontology terms were resolved in the linked\nontology data server Ontobee. The RDF triples stored in the RDF triple store\nare queryable through a SPARQL program. Such an ontology system supports\nadvanced pathway data integration and applications.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2013 01:07:11 GMT"}], "update_date": "2013-11-15", "authors_parsed": [["He", "Yongqun", ""], ["Xiang", "Zoushuang", ""]]}, {"id": "1311.3879", "submitter": "Jerome Euzenat", "authors": "Faisal Alkhateeb, J\\'er\\^ome Euzenat (INRIA Grenoble Rh\\^one-Alpes /\n  LIG Laboratoire d'Informatique de Grenoble)", "title": "Answering SPARQL queries modulo RDF Schema with paths", "comments": "RR-8394; alkhateeb2003a", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SPARQL is the standard query language for RDF graphs. In its strict\ninstantiation, it only offers querying according to the RDF semantics and would\nthus ignore the semantics of data expressed with respect to (RDF) schemas or\n(OWL) ontologies. Several extensions to SPARQL have been proposed to query RDF\ndata modulo RDFS, i.e., interpreting the query with RDFS semantics and/or\nconsidering external ontologies. We introduce a general framework which allows\nfor expressing query answering modulo a particular semantics in an homogeneous\nway. In this paper, we discuss extensions of SPARQL that use regular\nexpressions to navigate RDF graphs and may be used to answer queries\nconsidering RDFS semantics. We also consider their embedding as extensions of\nSPARQL. These SPARQL extensions are interpreted within the proposed framework\nand their drawbacks are presented. In particular, we show that the PSPARQL\nquery language, a strict extension of SPARQL offering transitive closure,\nallows for answering SPARQL queries modulo RDFS graphs with the same complexity\nas SPARQL through a simple transformation of the queries. We also consider\nlanguages which, in addition to paths, provide constraints. In particular, we\npresent and compare nSPARQL and our proposal CPSPARQL. We show that CPSPARQL is\nexpressive enough to answer full SPARQL queries modulo RDFS. Finally, we\ncompare the expressiveness and complexity of both nSPARQL and the corresponding\nfragment of CPSPARQL, that we call cpSPARQL. We show that both languages have\nthe same complexity through cpSPARQL, being a proper extension of SPARQL graph\npatterns, is more expressive than nSPARQL.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2013 15:34:26 GMT"}], "update_date": "2013-11-18", "authors_parsed": [["Alkhateeb", "Faisal", "", "INRIA Grenoble Rh\u00f4ne-Alpes /\n  LIG Laboratoire d'Informatique de Grenoble"], ["Euzenat", "J\u00e9r\u00f4me", "", "INRIA Grenoble Rh\u00f4ne-Alpes /\n  LIG Laboratoire d'Informatique de Grenoble"]]}, {"id": "1311.4040", "submitter": "Miklos Kalman", "authors": "Miklos Kalman, Ferenc Havasi", "title": "Enhanced XML Validation using SRML", "comments": "18 pages", "journal-ref": "International Journal of Web & Semantic Technology (IJWesT) Vol.4,\n  No.4, October 2013", "doi": "10.5121/ijwest.2013.4401", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data validation is becoming more and more important with the ever-growing\namount of data being consumed and transmitted by systems over the Internet. It\nis important to ensure that the data being sent is valid as it may contain\nentry errors, which may be consumed by different systems causing further\nerrors. XML has become the defacto standard for data transfer. The XML Schema\nDefinition language (XSD) was created to help XML structural validation and\nprovide a schema for data type restrictions, however it does not allow for more\ncomplex situations. In this article we introduce a way to provide rule based\nXML validation and correction through the extension and improvement of our SRML\nmetalanguage. We also explore the option of applying it in a database as a\ntrigger for CRUD operations allowing more granular dataset validation on an\natomic level allowing for more complex dataset record validation rules.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2013 09:59:05 GMT"}], "update_date": "2013-11-19", "authors_parsed": [["Kalman", "Miklos", ""], ["Havasi", "Ferenc", ""]]}, {"id": "1311.4088", "submitter": "Adel Alinezhad kolaei", "authors": "Adel Alinezhad Kolaei and Marzieh Ahmadzadeh", "title": "The Optimization of Running Queries in Relational Databases Using\n  ANT-Colony Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The issue of optimizing queries is a cost-sensitive process and with respect\nto the number of associated tables in a query, its number of permutations grows\nexponentially. On one hand, in comparison with other operators in relational\ndatabase, join operator is the most difficult and complicated one in terms of\noptimization for reducing its runtime. Accordingly, various algorithms have so\nfar been proposed to solve this problem. On the other hand, the success of any\ndatabase management system (DBMS) means exploiting the query model. In the\ncurrent paper, the heuristic ant algorithm has been proposed to solve this\nproblem and improve the runtime of join operation. Experiments and observed\nresults reveal the efficiency of this algorithm compared to its similar\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2013 18:43:19 GMT"}], "update_date": "2013-11-19", "authors_parsed": [["Kolaei", "Adel Alinezhad", ""], ["Ahmadzadeh", "Marzieh", ""]]}, {"id": "1311.4121", "submitter": "Thabet Slimani", "authors": "Thabet Slimani", "title": "Application of Rough Set Theory in Data Mining", "comments": "10 pages", "journal-ref": "International journal of Computer Science & Network Solutions,\n  Volume 1. No3, pages 1-10, November 2013", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rough set theory is a new method that deals with vagueness and uncertainty\nemphasized in decision making. Data mining is a discipline that has an\nimportant contribution to data analysis, discovery of new meaningful knowledge,\nand autonomous decision making. The rough set theory offers a viable approach\nfor decision rule extraction from data.This paper, introduces the fundamental\nconcepts of rough set theory and other aspects of data mining, a discussion of\ndata representation with rough set theory including pairs of attribute-value\nblocks, information tables reducts, indiscernibility relation and decision\ntables. Additionally, the rough set approach to lower and upper approximations\nand certain possible rule sets concepts are introduced. Finally, some\ndescription about applications of the data mining system with rough set theory\nis included.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2013 06:27:50 GMT"}], "update_date": "2013-11-19", "authors_parsed": [["Slimani", "Thabet", ""]]}, {"id": "1311.4529", "submitter": "Afroza Sultana", "authors": "Afroza Sultana, Naeemul Hassan, Chengkai Li, Jun Yang, Cong Yu", "title": "Incremental Discovery of Prominent Situational Facts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the novel problem of finding new, prominent situational facts, which\nare emerging statements about objects that stand out within certain contexts.\nMany such facts are newsworthy---e.g., an athlete's outstanding performance in\na game, or a viral video's impressive popularity. Effective and efficient\nidentification of these facts assists journalists in reporting, one of the main\ngoals of computational journalism. Technically, we consider an ever-growing\ntable of objects with dimension and measure attributes. A situational fact is a\n\"contextual\" skyline tuple that stands out against historical tuples in a\ncontext, specified by a conjunctive constraint involving dimension attributes,\nwhen a set of measure attributes are compared. New tuples are constantly added\nto the table, reflecting events happening in the real world. Our goal is to\ndiscover constraint-measure pairs that qualify a new tuple as a contextual\nskyline tuple, and discover them quickly before the event becomes yesterday's\nnews. A brute-force approach requires exhaustive comparison with every tuple,\nunder every constraint, and in every measure subspace. We design algorithms in\nresponse to these challenges using three corresponding ideas---tuple reduction,\nconstraint pruning, and sharing computation across measure subspaces. We also\nadopt a simple prominence measure to rank the discovered facts when they are\nnumerous. Experiments over two real datasets validate the effectiveness and\nefficiency of our techniques.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2013 20:44:13 GMT"}, {"version": "v2", "created": "Wed, 26 Mar 2014 16:43:25 GMT"}], "update_date": "2014-03-27", "authors_parsed": [["Sultana", "Afroza", ""], ["Hassan", "Naeemul", ""], ["Li", "Chengkai", ""], ["Yang", "Jun", ""], ["Yu", "Cong", ""]]}, {"id": "1311.4610", "submitter": "V\\'ictor Cuevas-Vicentt\\'in", "authors": "V\\'ictor Cuevas-Vicentt\\'in, Saumen Dey, Sven K\\\"ohler, Sean Riddle,\n  Bertram Lud\\\"ascher", "title": "Scientific Workflows and Provenance: Introduction and Research\n  Opportunities", "comments": "12 pages, 2 figures", "journal-ref": "Datenbank-Spektrum, November 2012, Volume 12, Issue 3, pp 193-203", "doi": "10.1007/s13222-012-0100-z", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific workflows are becoming increasingly popular for compute-intensive\nand data-intensive scientific applications. The vision and promise of\nscientific workflows includes rapid, easy workflow design, reuse, scalable\nexecution, and other advantages, e.g., to facilitate \"reproducible science\"\nthrough provenance (e.g., data lineage) support. However, as described in the\npaper, important research challenges remain. While the database community has\nstudied (business) workflow technologies extensively in the past, most current\nwork in scientific workflows seems to be done outside of the database\ncommunity, e.g., by practitioners and researchers in the computational sciences\nand eScience. We provide a brief introduction to scientific workflows and\nprovenance, and identify areas and problems that suggest new opportunities for\ndatabase research.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2013 02:23:03 GMT"}, {"version": "v2", "created": "Sat, 23 Nov 2013 07:03:52 GMT"}], "update_date": "2013-11-26", "authors_parsed": [["Cuevas-Vicentt\u00edn", "V\u00edctor", ""], ["Dey", "Saumen", ""], ["K\u00f6hler", "Sven", ""], ["Riddle", "Sean", ""], ["Lud\u00e4scher", "Bertram", ""]]}, {"id": "1311.4900", "submitter": "Sonali Goyal", "authors": "Sudhakar Ranjan, Komal K. Bhatia", "title": "Query Interface Integrator For Domain Specific Hidden Web", "comments": "8 Pages. International Journal of Computer Engineering and\n  Applications, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web is title admittance today mainly relies on search engines. A large amount\nof data is hidden in the databases behind the search interfaces referred to as\nHidden web, which needs to be indexed so in order to serve user query. In this\npaper database and data mining techniques are used for query interface\nintegration. The query interface must resemble the look and feel of local\ninterface as much as possible despite being automatically generated without\nhuman support.This technique keeps the related documents in the same domain so\nthat searching of documents becomes more efficient in terms of time complexity.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2013 06:25:40 GMT"}], "update_date": "2013-11-21", "authors_parsed": [["Ranjan", "Sudhakar", ""], ["Bhatia", "Komal K.", ""]]}, {"id": "1311.5013", "submitter": "Srivatsan Sridharan", "authors": "Srivatsan Sridharan, Kausal Malladi, Yamini Muralitharan", "title": "Data Mining Model for the Data Retrieval from Central Server\n  Configuration", "comments": "9 Pages, 10 References, 6 Figures presented in ACITY 2013 Conference", "journal-ref": "International Journal of Computer Science & Information Technology\n  (IJCSIT) Vol 5, No 5, October 2013", "doi": "10.5121/ijcsit.2013.5514", "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A server, which is to keep track of heavy document traffic, is unable to\nfilter the documents that are most relevant and updated for continuous text\nsearch queries. This paper focuses on handling continuous text extraction\nsustaining high document traffic. The main objective is to retrieve recent\nupdated documents that are most relevant to the query by applying sliding\nwindow technique. Our solution indexes the streamed documents in the main\nmemory with structure based on the principles of inverted file, and processes\ndocument arrival and expiration events with incremental threshold-based method.\nIt also ensures elimination of duplicate document retrieval using unsupervised\nduplicate detection. The documents are ranked based on user feedback and given\nhigher priority for retrieval.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2013 11:14:58 GMT"}], "update_date": "2013-11-21", "authors_parsed": [["Sridharan", "Srivatsan", ""], ["Malladi", "Kausal", ""], ["Muralitharan", "Yamini", ""]]}, {"id": "1311.5204", "submitter": "Georgios  Skoumas", "authors": "Georgios Skoumas, Dieter Pfoser, Anastasios Kyrillidis", "title": "On Quantifying Qualitative Geospatial Data: A Probabilistic Approach", "comments": null, "journal-ref": "Proceeding GEOCROWD '13 Proceedings of the Second ACM SIGSPATIAL\n  International Workshop on Crowdsourced and Volunteered Geographic Information\n  Pages 71-78 ACM New York, NY, USA \\c{opyright}2013 table of contents ISBN:\n  978-1-4503-2528-8", "doi": "10.1145/2534732.2534742", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Living in the era of data deluge, we have witnessed a web content explosion,\nlargely due to the massive availability of User-Generated Content (UGC). In\nthis work, we specifically consider the problem of geospatial information\nextraction and representation, where one can exploit diverse sources of\ninformation (such as image and audio data, text data, etc), going beyond\ntraditional volunteered geographic information. Our ambition is to include\navailable narrative information in an effort to better explain geospatial\nrelationships: with spatial reasoning being a basic form of human cognition,\nnarratives expressing such experiences typically contain qualitative spatial\ndata, i.e., spatial objects and spatial relationships.\n  To this end, we formulate a quantitative approach for the representation of\nqualitative spatial relations extracted from UGC in the form of texts. The\nproposed method quantifies such relations based on multiple text observations.\nSuch observations provide distance and orientation features which are utilized\nby a greedy Expectation Maximization-based (EM) algorithm to infer a\nprobability distribution over predefined spatial relationships; the latter\nrepresent the quantified relationships under user-defined probabilistic\nassumptions. We evaluate the applicability and quality of the proposed approach\nusing real UGC data originating from an actual travel blog text corpus. To\nverify the quality of the result, we generate grid-based maps visualizing the\nspatial extent of the various relations.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2013 09:06:59 GMT"}], "update_date": "2013-11-21", "authors_parsed": [["Skoumas", "Georgios", ""], ["Pfoser", "Dieter", ""], ["Kyrillidis", "Anastasios", ""]]}, {"id": "1311.5572", "submitter": "EPTCS", "authors": "Kazuki Miyahara (NAIST), Kenji Hashimoto (Nagoya University), Hiroyuki\n  Seki (Nagoya University)", "title": "Node Query Preservation for Deterministic Linear Top-Down Tree\n  Transducers", "comments": "In Proceedings TTATT 2013, arXiv:1311.5058", "journal-ref": "EPTCS 134, 2013, pp. 27-37", "doi": "10.4204/EPTCS.134.4", "report-no": null, "categories": "cs.FL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the decidability of node query preservation problems for\nXML document transformations. We assume a transformation given by a\ndeterministic linear top-down data tree transducer (abbreviated as DLT^V) and\nan n-ary query based on runs of a tree automaton. We say that a DLT^V Tr\nstrongly preserves a query Q if there is a query Q' such that for every\ndocument t, the answer set of Q' for Tr(t) is equal to the answer set of Q for\nt. Also we say that Tr weakly preserves Q if there is a query Q' such that for\nevery t_d in the range of Tr, the answer set of Q' for t_d is equal to the\nunion of the answer set of Q for t such that t_d = Tr(t). We show that the weak\npreservation problem is coNP-complete and the strong preservation problem is in\n2-EXPTIME.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2013 21:00:51 GMT"}], "update_date": "2013-11-25", "authors_parsed": [["Miyahara", "Kazuki", "", "NAIST"], ["Hashimoto", "Kenji", "", "Nagoya University"], ["Seki", "Hiroyuki", "", "Nagoya University"]]}, {"id": "1311.5573", "submitter": "EPTCS", "authors": "Sebastian Maneth (University of Edinburgh), Tom Sebastian (Innovimax\n  and INRIA)", "title": "XPath Node Selection over Grammar-Compressed Trees", "comments": "In Proceedings TTATT 2013, arXiv:1311.5058", "journal-ref": "EPTCS 134, 2013, pp. 38-48", "doi": "10.4204/EPTCS.134.5", "report-no": null, "categories": "cs.DB cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  XML document markup is highly repetitive and therefore well compressible\nusing grammar-based compression. Downward, navigational XPath can be executed\nover grammar-compressed trees in PTIME: the query is translated into an\nautomaton which is executed in one pass over the grammar. This result is\nwell-known and has been mentioned before. Here we present precise bounds on the\ntime complexity of this problem, in terms of big-O notation. For a given\ngrammar and XPath query, we consider three different tasks: (1) to count the\nnumber of nodes selected by the query, (2) to materialize the pre-order numbers\nof the selected nodes, and (3) to serialize the subtrees at the selected nodes.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2013 21:01:02 GMT"}], "update_date": "2013-11-25", "authors_parsed": [["Maneth", "Sebastian", "", "University of Edinburgh"], ["Sebastian", "Tom", "", "Innovimax\n  and INRIA"]]}, {"id": "1311.5663", "submitter": "Zhengkui  Wang", "authors": "Zhengkui Wang, Yan Chu, Kian-Lee Tan, Divyakant Agrawal, Amr EI\n  Abbadi, Xiaolong Xu", "title": "Scalable Data Cube Analysis over Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data cubes are widely used as a powerful tool to provide multidimensional\nviews in data warehousing and On-Line Analytical Processing (OLAP). However,\nwith increasing data sizes, it is becoming computationally expensive to perform\ndata cube analysis. The problem is exacerbated by the demand of supporting more\ncomplicated aggregate functions (e.g. CORRELATION, Statistical Analysis) as\nwell as supporting frequent view updates in data cubes. This calls for new\nscalable and efficient data cube analysis systems. In this paper, we introduce\nHaCube, an extension of MapReduce, designed for efficient parallel data cube\nanalysis on large-scale data by taking advantages from both MapReduce (in terms\nof scalability) and parallel DBMS (in terms of efficiency). We also provide a\ngeneral data cube materialization algorithm which is able to facilitate the\nfeatures in MapReduce-like systems towards an efficient data cube computation.\nFurthermore, we demonstrate how HaCube supports view maintenance through either\nincremental computation (e.g. used for SUM or COUNT) or recomputation (e.g.\nused for MEDIAN or CORRELATION). We implement HaCube by extending Hadoop and\nevaluate it based on the TPC-D benchmark over billions of tuples on a cluster\nwith over 320 cores. The experimental results demonstrate the efficiency,\nscalability and practicality of HaCube for cube analysis over a large amount of\ndata in a distributed environment.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2013 07:48:25 GMT"}], "update_date": "2013-11-25", "authors_parsed": [["Wang", "Zhengkui", ""], ["Chu", "Yan", ""], ["Tan", "Kian-Lee", ""], ["Agrawal", "Divyakant", ""], ["Abbadi", "Amr EI", ""], ["Xu", "Xiaolong", ""]]}, {"id": "1311.5685", "submitter": "Blesson Varghese", "authors": "Blesson Varghese and Andrew Rau-Chaplin", "title": "Data Challenges in High-Performance Risk Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Risk Analytics is important to quantify, manage and analyse risks from the\nmanufacturing to the financial setting. In this paper, the data challenges in\nthe three stages of the high-performance risk analytics pipeline, namely risk\nmodelling, portfolio risk management and dynamic financial analysis is\npresented.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2013 09:22:03 GMT"}], "update_date": "2013-11-25", "authors_parsed": [["Varghese", "Blesson", ""], ["Rau-Chaplin", "Andrew", ""]]}, {"id": "1311.6165", "submitter": "Ying Long", "authors": "Ying Long and Xingjian Liu", "title": "Automated identification and characterization of parcels (AICP) with\n  OpenStreetMap and Points of Interest", "comments": "26 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Against the paucity of urban parcels in China, this paper proposes a method\nto automatically identify and characterize parcels (AICP) with OpenStreetMap\n(OSM) and Points of Interest (POI) data. Parcels are the basic spatial units\nfor fine-scale urban modeling, urban studies, as well as spatial planning.\nConventional ways of identification and characterization of parcels rely on\nremote sensing and field surveys, which are labor intensive and\nresource-consuming. Poorly developed digital infrastructure, limited resources,\nand institutional barriers have all hampered the gathering and application of\nparcel data in developing countries. Against this backdrop, we employ OSM road\nnetworks to identify parcel geometries and POI data to infer parcel\ncharacteristics. A vector-based CA model is adopted to select urban parcels.\nThe method is applied to the entire state of China and identifies 82,645 urban\nparcels in 297 cities. Notwithstanding all the caveats of open and/or\ncrowd-sourced data, our approach could produce reasonably good approximation of\nparcels identified from conventional methods, thus having the potential to\nbecome a useful supplement.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2013 20:24:41 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2014 15:01:12 GMT"}, {"version": "v3", "created": "Mon, 30 Mar 2015 12:04:33 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Long", "Ying", ""], ["Liu", "Xingjian", ""]]}, {"id": "1311.6335", "submitter": "Astrid Rheinl\\\"ander", "authors": "Astrid Rheinl\\\"ander, Arvid Heise, Fabian Hueske, Ulf Leser, Felix\n  Naumann", "title": "SOFA: An Extensible Logical Optimizer for UDF-heavy Dataflows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen an increased interest in large-scale analytical\ndataflows on non-relational data. These dataflows are compiled into execution\ngraphs scheduled on large compute clusters. In many novel application areas the\npredominant building blocks of such dataflows are user-defined predicates or\nfunctions (UDFs). However, the heavy use of UDFs is not well taken into account\nfor dataflow optimization in current systems.\n  SOFA is a novel and extensible optimizer for UDF-heavy dataflows. It builds\non a concise set of properties for describing the semantics of Map/Reduce-style\nUDFs and a small set of rewrite rules, which use these properties to find a\nmuch larger number of semantically equivalent plan rewrites than possible with\ntraditional techniques. A salient feature of our approach is extensibility: We\narrange user-defined operators and their properties into a subsumption\nhierarchy, which considerably eases integration and optimization of new\noperators. We evaluate SOFA on a selection of UDF-heavy dataflows from\ndifferent domains and compare its performance to three other algorithms for\ndataflow optimization. Our experiments reveal that SOFA finds efficient plans,\noutperforming the best plans found by its competitors by a factor of up to 6.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2013 15:26:49 GMT"}], "update_date": "2013-11-26", "authors_parsed": [["Rheinl\u00e4nder", "Astrid", ""], ["Heise", "Arvid", ""], ["Hueske", "Fabian", ""], ["Leser", "Ulf", ""], ["Naumann", "Felix", ""]]}, {"id": "1311.6543", "submitter": "Haibin Zhang", "authors": "Haibin Zhang (1), Yan Wang (1), Xiuzhen Zhang (2), Ee-Peng Lim (3),\n  ((1) Macquarie University, Sydney, Australia, (2) RMIT University, Melbourne,\n  Australia, (3) Singapore Management University, Singapore)", "title": "ReputationPro: The Efficient Approaches to Contextual Transaction Trust\n  Computation in E-Commerce Environments", "comments": "This paper has been withdrawn by the author due to the loss of some\n  figures", "journal-ref": "ACM Trans. Web 9, 1, Article 2 (January 2015)", "doi": "10.1145/2697390", "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In e-commerce environments, the trustworthiness of a seller is utterly\nimportant to potential buyers, especially when the seller is unknown to them.\nMost existing trust evaluation models compute a single value to reflect the\ngeneral trust level of a seller without taking any transaction context\ninformation into account. In this paper, we first present a trust vector\nconsisting of three values for Contextual Transaction Trust (CTT). In the\ncomputation of three CTT values, the identified three important context\ndimensions, including product category, transaction amount and transaction\ntime, are taken into account. In particular, with different parameters\nregarding context dimensions that are specified by a buyer, different sets of\nCTT values can be calculated. As a result, all these values can outline the\nreputation profile of a seller that indicates the dynamic trust levels of a\nseller in different product categories, price ranges, time periods, and any\nnecessary combination of them. We term this new model as ReputationPro.\nHowever, in ReputationPro, the computation of reputation profile requires novel\nalgorithms for the precomputation of aggregates over large-scale ratings and\ntransaction data of three context dimensions as well as new data structures for\nappropriately indexing aggregation results to promptly answer buyers' CTT\nrequests. To solve these challenging problems, we then propose a new index\nscheme CMK-tree. After that, we further extend CMK-tree and propose a\nCMK-treeRS approach to reducing the storage space allocated to each seller.\nFinally, the experimental results illustrate that the CMK-tree is superior in\nefficiency for computing CTT values to all three existing approaches in the\nliterature. In addition, though with reduced storage space, the CMK-treeRS\napproach can further improve the performance in answering buyers' CTT queries.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2013 02:32:24 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2013 04:36:20 GMT"}, {"version": "v3", "created": "Mon, 5 May 2014 23:27:25 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Zhang", "Haibin", ""], ["Wang", "Yan", ""], ["Zhang", "Xiuzhen", ""], ["Lim", "Ee-Peng", ""]]}, {"id": "1311.6570", "submitter": "Keisuke Nakano", "authors": "Shizuya Hakuta, Sebastian Maneth, Keisuke Nakano, Hideya Iwasaki", "title": "XQuery Streaming by Forest Transducers", "comments": "Full version of the paper in the Proceedings of the 30th IEEE\n  International Conference on Data Engineering (ICDE 2014)", "journal-ref": null, "doi": "10.1109/ICDE.2014.6816714", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Streaming of XML transformations is a challenging task and only very few\nsystems support streaming. Research approaches generally define custom\nfragments of XQuery and XPath that are amenable to streaming, and then design\ncustom algorithms for each fragment. These languages have several shortcomings.\nHere we take a more principles approach to the problem of streaming\nXQuery-based transformations. We start with an elegant transducer model for\nwhich many static analysis problems are well-understood: the Macro Forest\nTransducer (MFT). We show that a large fragment of XQuery can be translated\ninto MFTs --- indeed, a fragment of XQuery, that can express important features\nthat are missing from other XQuery stream engines, such as GCX: our fragment of\nXQuery supports XPath predicates and let-statements. We then rely on a\nstreaming execution engine for MFTs, one which uses a well-founded set of\noptimizations from functional programming, such as strictness analysis and\ndeforestation. Our prototype achieves time and memory efficiency comparable to\nthe fastest known engine for XQuery streaming, GCX. This is surprising because\nour engine relies on the OCaml built in garbage collector and does not use any\nspecialized buffer management, while GCX's efficiency is due to clever and\nexplicit buffer management.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2013 07:17:23 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2013 11:56:24 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Hakuta", "Shizuya", ""], ["Maneth", "Sebastian", ""], ["Nakano", "Keisuke", ""], ["Iwasaki", "Hideya", ""]]}, {"id": "1311.6578", "submitter": "Debajyoti Mukhopadhyay Prof.", "authors": "Vrushali Randhe, Archana Chougule, Debajyoti Mukhopadhyay", "title": "Reverse Proxy Framework using Sanitization Technique for Intrusion\n  Prevention in Database", "comments": "9 pages, 6 figures, 3 tables; CIIT 2013 International Conference,\n  Mumbai", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing importance of the internet in our day to day life, data\nsecurity in web application has become very crucial. Ever increasing on line\nand real time transaction services have led to manifold rise in the problems\nassociated with the database security. Attacker uses illegal and unauthorized\napproaches to hijack the confidential information like username, password and\nother vital details. Hence the real time transaction requires security against\nweb based attacks. SQL injection and cross site scripting attack are the most\ncommon application layer attack. The SQL injection attacker pass SQL statement\nthrough a web applications input fields, URL or hidden parameters and get\naccess to the database or update it. The attacker take a benefit from user\nprovided data in such a way that the users input is handled as a SQL code.\nUsing this vulnerability an attacker can execute SQL commands directly on the\ndatabase. SQL injection attacks are most serious threats which take users input\nand integrate it into SQL query. Reverse Proxy is a technique which is used to\nsanitize the users inputs that may transform into a database attack. In this\ntechnique a data redirector program redirects the users input to the proxy\nserver before it is sent to the application server. At the proxy server, data\ncleaning algorithm is triggered using a sanitizing application. In this\nframework we include detection and sanitization of the tainted information\nbeing sent to the database and innovate a new prototype.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2013 07:52:02 GMT"}], "update_date": "2013-11-27", "authors_parsed": [["Randhe", "Vrushali", ""], ["Chougule", "Archana", ""], ["Mukhopadhyay", "Debajyoti", ""]]}, {"id": "1311.6714", "submitter": "Rita Hartel", "authors": "Stefan B\\\"ottcher, Rita Hartel, Jonathan Rabe", "title": "Efficient XML Keyword Search based on DAG-Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In contrast to XML query languages as e.g. XPath which require knowledge on\nthe query language as well as on the document structure, keyword search is open\nto anybody. As the size of XML sources grows rapidly, the need for efficient\nsearch indices on XML data that support keyword search increases. In this\npaper, we present an approach of XML keyword search which is based on the DAG\nof the XML data, where repeated substructures are considered only once, and\ntherefore, have to be searched only once. As our performance evaluation shows,\nthis DAG-based extension of the set intersection search algorithm[1], [2], can\nlead to search times that are on large documents more than twice as fast as the\nsearch times of the XML-based approach. Additionally, we utilize a smaller\nindex, i.e., we consume less main memory to compute the results.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2013 15:55:41 GMT"}], "update_date": "2013-11-27", "authors_parsed": [["B\u00f6ttcher", "Stefan", ""], ["Hartel", "Rita", ""], ["Rabe", "Jonathan", ""]]}, {"id": "1311.6876", "submitter": "Yuan Yao", "authors": "Yuan Yao, Hanghang Tong, Tao Xie, Leman Akoglu, Feng Xu, Jian Lu", "title": "Want a Good Answer? Ask a Good Question First!", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.IR cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community Question Answering (CQA) websites have become valuable repositories\nwhich host a massive volume of human knowledge. To maximize the utility of such\nknowledge, it is essential to evaluate the quality of an existing question or\nanswer, especially soon after it is posted on the CQA website.\n  In this paper, we study the problem of inferring the quality of questions and\nanswers through a case study of a software CQA (Stack Overflow). Our key\nfinding is that the quality of an answer is strongly positively correlated with\nthat of its question. Armed with this observation, we propose a family of\nalgorithms to jointly predict the quality of questions and answers, for both\nquantifying numerical quality scores and differentiating the high-quality\nquestions/answers from those of low quality. We conduct extensive experimental\nevaluations to demonstrate the effectiveness and efficiency of our methods.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2013 06:39:25 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Yao", "Yuan", ""], ["Tong", "Hanghang", ""], ["Xie", "Tao", ""], ["Akoglu", "Leman", ""], ["Xu", "Feng", ""], ["Lu", "Jian", ""]]}, {"id": "1311.6907", "submitter": "Samir Loudni", "authors": "Jean-Philippe M\\'etivier and Samir Loudni and Thierry Charnois", "title": "A Constraint Programming Approach for Mining Sequential Patterns in a\n  Sequence Database", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constraint-based pattern discovery is at the core of numerous data mining\ntasks. Patterns are extracted with respect to a given set of constraints\n(frequency, closedness, size, etc). In the context of sequential pattern\nmining, a large number of devoted techniques have been developed for solving\nparticular classes of constraints. The aim of this paper is to investigate the\nuse of Constraint Programming (CP) to model and mine sequential patterns in a\nsequence database. Our CP approach offers a natural way to simultaneously\ncombine in a same framework a large set of constraints coming from various\norigins. Experiments show the feasibility and the interest of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2013 09:44:43 GMT"}], "update_date": "2013-11-28", "authors_parsed": [["M\u00e9tivier", "Jean-Philippe", ""], ["Loudni", "Samir", ""], ["Charnois", "Thierry", ""]]}, {"id": "1311.7200", "submitter": "Debajyoti Mukhopadhyay Prof.", "authors": "Ayan Chakraborty, Shiladitya Munshi, Debajyoti Mukhopadhyay", "title": "Searching and Establishment of S-P-O Relationships for Linked RDF Graphs\n  : An Adaptive Approach", "comments": "5 pages, 4 figures. arXiv admin note: text overlap with\n  arXiv:1107.1104 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the coming era of semantic web linked data analysis is a very burning\nissue for efficient searching and retrieval of information. One way of\nestablishing this link is to implement subject predicate object relationship\nthrough Set Theory approach which is already done in our previous work. For\nanalyzing inter relationship between two RDF Graphs, RDF- Schema (RDFS) should\nalso be taken care of. In the present paper, an adaptive combination rule based\nframework has been proposed for establishment of S P O relationship and RDF\nGraph searching is reported. Hence the identification of criteria for\ninter-relationship of RDF Graphs opens up new road in semantic search.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2013 04:10:55 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Chakraborty", "Ayan", ""], ["Munshi", "Shiladitya", ""], ["Mukhopadhyay", "Debajyoti", ""]]}, {"id": "1311.7219", "submitter": "Trupti Kodinariya prof", "authors": "Trupti M. Kodinariya Dr. Prashant R. Makwana", "title": "Partitioning Clustering algorithms for handling numerical and\n  categorical data: a review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is widely used in different field such as biology, psychology, and\neconomics. Most traditional clustering algorithms are limited to handling\ndatasets that contain either numeric or categorical attributes. However,\ndatasets with mixed types of attributes are common in real life data mining\napplications. In this paper, we review partitioning based algorithm such as\nK-prototype, Extension of K-prototype, K-histogram, Fuzzy approaches, genetic\napproaches, etc. These algorithm works on both numerical and categorical data.\nThe approaches has been proposed to handle mixed data are based on four\ndifferent perceptive: i) split data set into two part such that each part\ncontain either numerical or categorical data, then apply separate clustering\nalgorithm on each data set, finally combined the result of both clustering\nalgorithm, ii) converting categorical attribute into numerical attribute and\napply numerical attribute clustering algorithm; iii) discrimination of\nnumerical attribute and apply categorical based clustering algorithm; iv)\nConversion of the categorical attributes into binary ones and apply any\nnumerical based clustering algorithm\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2013 07:06:56 GMT"}, {"version": "v2", "created": "Thu, 12 Jun 2014 09:45:46 GMT"}, {"version": "v3", "created": "Tue, 2 Jul 2019 07:33:16 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Makwana", "Trupti M. Kodinariya Dr. Prashant R.", ""]]}, {"id": "1311.7307", "submitter": "Radu Ciucanu", "authors": "Iovka Boneva, Radu Ciucanu, S{\\l}awek Staworko", "title": "Schemas for Unordered XML on a DIME", "comments": "Theory of Computing Systems", "journal-ref": null, "doi": "10.1007/s00224-014-9593-1", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate schema languages for unordered XML having no relative order\namong siblings. First, we propose unordered regular expressions (UREs),\nessentially regular expressions with unordered concatenation instead of\nstandard concatenation, that define languages of unordered words to model the\nallowed content of a node (i.e., collections of the labels of children).\nHowever, unrestricted UREs are computationally too expensive as we show the\nintractability of two fundamental decision problems for UREs: membership of an\nunordered word to the language of a URE and containment of two UREs.\nConsequently, we propose a practical and tractable restriction of UREs,\ndisjunctive interval multiplicity expressions (DIMEs).\n  Next, we employ DIMEs to define languages of unordered trees and propose two\nschema languages: disjunctive interval multiplicity schema (DIMS), and its\nrestriction, disjunction-free interval multiplicity schema (IMS). We study the\ncomplexity of the following static analysis problems: schema satisfiability,\nmembership of a tree to the language of a schema, schema containment, as well\nas twig query satisfiability, implication, and containment in the presence of\nschema. Finally, we study the expressive power of the proposed schema languages\nand compare them with yardstick languages of unordered trees (FO, MSO, and\nPresburger constraints) and DTDs under commutative closure. Our results show\nthat the proposed schema languages are capable of expressing many practical\nlanguages of unordered trees and enjoy desirable computational properties.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2013 13:03:12 GMT"}, {"version": "v2", "created": "Sun, 9 Feb 2014 16:44:00 GMT"}, {"version": "v3", "created": "Wed, 30 Jul 2014 15:50:41 GMT"}, {"version": "v4", "created": "Tue, 28 Oct 2014 08:22:24 GMT"}], "update_date": "2014-10-29", "authors_parsed": [["Boneva", "Iovka", ""], ["Ciucanu", "Radu", ""], ["Staworko", "S\u0142awek", ""]]}]