[{"id": "2012.00058", "submitter": "Trang Le", "authors": "Joseph D. Romano, Trang T. Le, William La Cava, John T. Gregg, Daniel\n  J. Goldberg, Natasha L. Ray, Praneel Chakraborty, Daniel Himmelstein, Weixuan\n  Fu, and Jason H. Moore", "title": "PMLB v1.0: An open source dataset collection for benchmarking machine\n  learning methods", "comments": "4 pages, 1 figure. *: These authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motivation: Novel machine learning and statistical modeling studies rely on\nstandardized comparisons to existing methods using well-studied benchmark\ndatasets. Few tools exist that provide rapid access to many of these datasets\nthrough a standardized, user-friendly interface that integrates well with\npopular data science workflows.\n  Results: This release of PMLB provides the largest collection of diverse,\npublic benchmark datasets for evaluating new machine learning and data science\nmethods aggregated in one location. v1.0 introduces a number of critical\nimprovements developed following discussions with the open-source community.\n  Availability: PMLB is available at https://github.com/EpistasisLab/pmlb.\nPython and R interfaces for PMLB can be installed through the Python Package\nIndex and Comprehensive R Archive Network, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 19:21:44 GMT"}, {"version": "v2", "created": "Sun, 4 Apr 2021 20:31:09 GMT"}, {"version": "v3", "created": "Tue, 6 Apr 2021 12:37:35 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Romano", "Joseph D.", ""], ["Le", "Trang T.", ""], ["La Cava", "William", ""], ["Gregg", "John T.", ""], ["Goldberg", "Daniel J.", ""], ["Ray", "Natasha L.", ""], ["Chakraborty", "Praneel", ""], ["Himmelstein", "Daniel", ""], ["Fu", "Weixuan", ""], ["Moore", "Jason H.", ""]]}, {"id": "2012.00135", "submitter": "Yingtai Xiao", "authors": "Yingtai Xiao, Zeyu Ding, Yuxin Wang, Danfeng Zhang, Daniel Kifer", "title": "Optimizing Fitness-For-Use of Differentially Private Linear Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In practice, differentially private data releases are designed to support a\nvariety of applications. A data release is fit for use if it meets target\naccuracy requirements for each application. In this paper, we consider the\nproblem of answering linear queries under differential privacy subject to\nper-query accuracy constraints. Existing practical frameworks like the matrix\nmechanism do not provide such fine-grained control (they optimize total error,\nwhich allows some query answers to be more accurate than necessary, at the\nexpense of other queries that become no longer useful). Thus, we design a\nfitness-for-use strategy that adds privacy-preserving Gaussian noise to query\nanswers. The covariance structure of the noise is optimized to meet the\nfine-grained accuracy requirements while minimizing the cost to privacy.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 22:10:21 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 01:14:08 GMT"}, {"version": "v3", "created": "Mon, 28 Dec 2020 16:22:22 GMT"}, {"version": "v4", "created": "Fri, 19 Mar 2021 02:26:31 GMT"}, {"version": "v5", "created": "Thu, 1 Apr 2021 11:24:38 GMT"}, {"version": "v6", "created": "Mon, 14 Jun 2021 17:30:36 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Xiao", "Yingtai", ""], ["Ding", "Zeyu", ""], ["Wang", "Yuxin", ""], ["Zhang", "Danfeng", ""], ["Kifer", "Daniel", ""]]}, {"id": "2012.00290", "submitter": "Donghuo Zeng", "authors": "Donghuo Zeng, Yi Yu, Keizo Oyama", "title": "MusicTM-Dataset for Joint Representation Learning among Sheet Music,\n  Lyrics, and Musical Audio", "comments": "12 pages, 5 figures, 2 tables", "journal-ref": "CSMT2020", "doi": null, "report-no": null, "categories": "cs.SD cs.DB cs.IR cs.MM eess.AS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This work present a music dataset named MusicTM-Dataset, which is utilized in\nimproving the representation learning ability of different types of cross-modal\nretrieval (CMR). Little large music dataset including three modalities is\navailable for learning representations for CMR. To collect a music dataset, we\nexpand the original musical notation to synthesize audio and generated\nsheet-music image, and build musical notation based sheet-music image, audio\nclip and syllable-denotation text as fine-grained alignment, such that the\nMusicTM-Dataset can be exploited to receive shared representation for\nmultimodal data points. The MusicTM-Dataset presents 3 kinds of modalities,\nwhich consists of the image of sheet-music, the text of lyrics and synthesized\naudio, their representations are extracted by some advanced models. In this\npaper, we introduce the background of music dataset and express the process of\nour data collection. Based on our dataset, we achieve some basic methods for\nCMR tasks. The MusicTM-Dataset are accessible in https:\n//github.com/dddzeng/MusicTM-Dataset.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 06:18:53 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 14:20:13 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Zeng", "Donghuo", ""], ["Yu", "Yi", ""], ["Oyama", "Keizo", ""]]}, {"id": "2012.00472", "submitter": "Martin Kleppmann", "authors": "Martin Kleppmann, Heidi Howard", "title": "Byzantine Eventual Consistency and the Fundamental Limits of\n  Peer-to-Peer Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sybil attacks, in which a large number of adversary-controlled nodes join a\nnetwork, are a concern for many peer-to-peer database systems, necessitating\nexpensive countermeasures such as proof-of-work. However, there is a category\nof database applications that are, by design, immune to Sybil attacks because\nthey can tolerate arbitrary numbers of Byzantine-faulty nodes. In this paper,\nwe characterize this category of applications using a consistency model we call\nByzantine Eventual Consistency (BEC). We introduce an algorithm that guarantees\nBEC based on Byzantine causal broadcast, prove its correctness, and demonstrate\nnear-optimal performance in a prototype implementation.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 13:24:09 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Kleppmann", "Martin", ""], ["Howard", "Heidi", ""]]}, {"id": "2012.00697", "submitter": "\\c{C}a\\u{g}atay Demiralp", "authors": "James Gale and Max Seiden and Gretchen Atwood and Jason Frantz and Rob\n  Woollen and \\c{C}a\\u{g}atay Demiralp", "title": "Sigma Worksheet: Interactive Construction of OLAP Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The new generation of cloud data warehouses (CDWs) brings large amounts of\ndata and compute power closer to users in enterprises. The ability to directly\naccess the warehouse data, interactively analyze and explore it at scale can\nempower users to improve their decision making cycles. However, existing tools\nfor analyzing data in CDWs are either limited in ad-hoc transformations or\ndifficult to use for business users, the largest user segment in enterprises.\nHere we introduce Sigma Worksheet, a new interactive system that enables users\nto easily perform ad-hoc visual analysis of data in CDWs at scale. For this,\nSigma Worksheet provides an accessible spreadsheet-like interface for data\nanalysis through direct manipulation. Sigma Worksheet dynamically constructs\nmatching SQL queries from user interactions on this familiar interface,\nbuilding on the versatility and expressivity of SQL. Sigma Worksheet executes\nconstructed queries directly on CDWs, leveraging the superior characteristics\nof the new generation CDWs, including scalability. To evaluate Sigma Worksheet,\nwe first demonstrate its expressivity through two real life use cases, cohort\nanalysis and sessionization. We then measure the performance of the Worksheet\ngenerated queries with a set of experiments using the TPC-H benchmark. Results\nshow the performance of our compiled SQL queries is comparable to that of the\nreference queries of the benchmark. Finally, to assess the usefulness of Sigma\nWorksheet in deployment, we elicit feedback through a 100-person survey\nfollowed by a semi-structured interview study with 70 participants. We find\nthat Sigma Worksheet is easier to use and learn, improving the productivity of\nusers. Our findings also suggest Sigma Worksheet can further improve user\nexperience by providing guidance to users at various steps of data analysis.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 17:56:59 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 05:42:36 GMT"}, {"version": "v3", "created": "Thu, 6 May 2021 00:42:53 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Gale", "James", ""], ["Seiden", "Max", ""], ["Atwood", "Gretchen", ""], ["Frantz", "Jason", ""], ["Woollen", "Rob", ""], ["Demiralp", "\u00c7a\u011fatay", ""]]}, {"id": "2012.00890", "submitter": "Javier Flores", "authors": "Javier Flores, Sergi Nadal, Oscar Romero", "title": "Scalable Data Discovery Using Profiles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of discovering joinable datasets at scale. This is, how\nto automatically discover pairs of attributes in a massive collection of\nindependent, heterogeneous datasets that can be joined. Exact (e.g., based on\ndistinct values) and hash-based (e.g., based on locality-sensitive hashing)\ntechniques require indexing the entire dataset, which is unattainable at scale.\nTo overcome this issue, we approach the problem from a learning perspective\nrelying on profiles. These are succinct representations that capture the\nunderlying characteristics of the schemata and data values of datasets, which\ncan be efficiently extracted in a distributed and parallel fashion. Profiles\nare then compared, to predict the quality of a join operation among a pair of\nattributes from different datasets. In contrast to the state-of-the-art, we\ndefine a novel notion of join quality that relies on a metric considering both\nthe containment and cardinality proportions between candidate attributes. We\nimplement our approach in a system called NextiaJD, and present extensive\nexperiments to show the predictive performance and computational efficiency of\nour method. Our experiments show that NextiaJD obtains similar predictive\nperformance to that of hash-based methods, yet we are able to scale-up to\nlarger volumes of data. Also, NextiaJD generates a considerably less amount of\nfalse positives, which is a desirable feature at scale.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 23:29:46 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 19:43:13 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Flores", "Javier", ""], ["Nadal", "Sergi", ""], ["Romero", "Oscar", ""]]}, {"id": "2012.01184", "submitter": "J\\'er\\^ome Darmont", "authors": "Pegdwend\\'e Sawadogo, J\\'er\\^ome Darmont, Fabien Duchateau", "title": "Feedback from the participants of the ADBIS, TPDL and EDA 2020 joint\n  conferences", "comments": "7 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DB cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents the way the joint ADBIS, TPDL and EDA 2020 conferences\nwere organized online and the results of the participant survey conducted\nthereafter. We present the lessons learned from the participants' feedback.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 17:02:26 GMT"}, {"version": "v2", "created": "Sat, 19 Dec 2020 22:03:07 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Sawadogo", "Pegdwend\u00e9", ""], ["Darmont", "J\u00e9r\u00f4me", ""], ["Duchateau", "Fabien", ""]]}, {"id": "2012.01229", "submitter": "Roee Shraga PhD", "authors": "Roee Shraga, Ofra Amir, Avigdor Gal", "title": "Learning to Characterize Matching Experts", "comments": "Accepted by the 37th IEEE International Conference on Data\n  Engineering (ICDE 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching is a task at the heart of any data integration process, aimed at\nidentifying correspondences among data elements. Matching problems were\ntraditionally solved in a semi-automatic manner, with correspondences being\ngenerated by matching algorithms and outcomes subsequently validated by human\nexperts. Human-in-the-loop data integration has been recently challenged by the\nintroduction of big data and recent studies have analyzed obstacles to\neffective human matching and validation. In this work we characterize human\nmatching experts, those humans whose proposed correspondences can mostly be\ntrusted to be valid. We provide a novel framework for characterizing matching\nexperts that, accompanied with a novel set of features, can be used to identify\nreliable and valuable human experts. We demonstrate the usefulness of our\napproach using an extensive empirical evaluation. In particular, we show that\nour approach can improve matching results by filtering out inexpert matchers.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 14:16:38 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Shraga", "Roee", ""], ["Amir", "Ofra", ""], ["Gal", "Avigdor", ""]]}, {"id": "2012.01592", "submitter": "Zeyu Ding", "authors": "Zeyu Ding, Yuxin Wang, Yingtai Xiao, Guanhong Wang, Danfeng Zhang,\n  Daniel Kifer", "title": "Free Gap Estimates from the Exponential Mechanism, Sparse Vector, Noisy\n  Max and Related Algorithms", "comments": "arXiv admin note: substantial text overlap with arXiv:1904.12773", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Private selection algorithms, such as the Exponential Mechanism, Noisy Max\nand Sparse Vector, are used to select items (such as queries with large\nanswers) from a set of candidates, while controlling privacy leakage in the\nunderlying data. Such algorithms serve as building blocks for more complex\ndifferentially private algorithms. In this paper we show that these algorithms\ncan release additional information related to the gaps between the selected\nitems and the other candidates for free (i.e., at no additional privacy cost).\nThis free gap information can improve the accuracy of certain follow-up\ncounting queries by up to 66%. We obtain these results from a careful privacy\nanalysis of these algorithms. Based on this analysis, we further propose novel\nhybrid algorithms that can dynamically save additional privacy budget.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 23:28:27 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Ding", "Zeyu", ""], ["Wang", "Yuxin", ""], ["Xiao", "Yingtai", ""], ["Wang", "Guanhong", ""], ["Zhang", "Danfeng", ""], ["Kifer", "Daniel", ""]]}, {"id": "2012.01658", "submitter": "EPTCS", "authors": "Dominique Duval (CNRS and Univ. Grenoble Alpes, France), Rachid\n  Echahed (CNRS and Univ. Grenoble Alpes, France), Fr\\'ed\\'eric Prost (CNRS and\n  Univ. Grenoble Alpes, France)", "title": "An Algebraic Graph Transformation Approach for RDF and SPARQL", "comments": "In Proceedings GCM 2020, arXiv:2012.01181. arXiv admin note:\n  substantial text overlap with arXiv:1910.07519", "journal-ref": "EPTCS 330, 2020, pp. 55-70", "doi": "10.4204/EPTCS.330.4", "report-no": null, "categories": "cs.DB cs.PL cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the recommendations of the World Wide Web Consortium (W3C) about\nRDF framework and its associated query language SPARQL. We propose a new formal\nframework based on category theory which provides clear and concise formal\ndefinitions of the main basic features of RDF and SPARQL. We define RDF graphs\nas well as SPARQL basic graph patterns as objects of some nested categories.\nThis allows one to clarify, in particular, the role of blank nodes.\nFurthermore, we consider basic SPARQL CONSTRUCT and SELECT queries and\nformalize their operational semantics following a novel algebraic graph\ntransformation approach called POIM.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 02:27:57 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Duval", "Dominique", "", "CNRS and Univ. Grenoble Alpes, France"], ["Echahed", "Rachid", "", "CNRS and Univ. Grenoble Alpes, France"], ["Prost", "Fr\u00e9d\u00e9ric", "", "CNRS and\n  Univ. Grenoble Alpes, France"]]}, {"id": "2012.01661", "submitter": "EPTCS", "authors": "Russ Harmer (Univ Lyon, EnsL, UCBL, CNRS, LIP, France), Eugenia\n  Oshurko (Univ Lyon, EnsL, UCBL, CNRS, LIP, France)", "title": "Reversibility and Composition of Rewriting in Hierarchies", "comments": "In Proceedings GCM 2020, arXiv:2012.01181", "journal-ref": "EPTCS 330, 2020, pp. 145-162", "doi": "10.4204/EPTCS.330.9", "report-no": null, "categories": "cs.LO cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study how graph transformations based on sesqui-pushout\nrewriting can be reversed and how the composition of rewrites can be\nconstructed. We illustrate how such reversibility and composition can be used\nto design an audit trail system for individual graphs and graph hierarchies.\nThis provides us with a compact way to maintain the history of updates of an\nobject, including its multiple versions. The main application of the designed\nframework is an audit trail of updates to knowledge represented by hierarchies\nof graphs. Therefore, we introduce the notion of rule hierarchy that represents\na transformation of the entire hierarchy, study how rule hierarchies can be\napplied to hierarchies and analyse the conditions under which this application\nis reversible. We then present a theory for constructing the composition of\nconsecutive hierarchy rewrites. The prototype audit trail system for\ntransformations in hierarchies of simple graphs with attributes is implemented\nas part of the ReGraph Python library.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 02:29:28 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Harmer", "Russ", "", "Univ Lyon, EnsL, UCBL, CNRS, LIP, France"], ["Oshurko", "Eugenia", "", "Univ Lyon, EnsL, UCBL, CNRS, LIP, France"]]}, {"id": "2012.01917", "submitter": "Davide Lanti", "authors": "Diego Calvanese and Avigdor Gal and Davide Lanti and Marco Montali and\n  Alessandro Mosca and Roee Shraga", "title": "Mapping Patterns for Virtual Knowledge Graphs", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Virtual Knowledge Graphs (VKG) constitute one of the most promising paradigms\nfor integrating and accessing legacy data sources. A critical bottleneck in the\nintegration process involves the definition, validation, and maintenance of\nmappings that link data sources to a domain ontology. To support the management\nof mappings throughout their entire lifecycle, we propose a comprehensive\ncatalog of sophisticated mapping patterns that emerge when linking databases to\nontologies. To do so, we build on well-established methodologies and patterns\nstudied in data management, data analysis, and conceptual modeling. These are\nextended and refined through the analysis of concrete VKG benchmarks and\nreal-world use cases, and considering the inherent impedance mismatch between\ndata sources and ontologies. We validate our catalog on the considered VKG\nscenarios, showing that it covers the vast majority of patterns present\ntherein.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 13:54:52 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Calvanese", "Diego", ""], ["Gal", "Avigdor", ""], ["Lanti", "Davide", ""], ["Montali", "Marco", ""], ["Mosca", "Alessandro", ""], ["Shraga", "Roee", ""]]}, {"id": "2012.01945", "submitter": "Xuliang Zhu", "authors": "Xuliang Zhu, Xin Huang, Byron Choi, Jiaxin Jiang, Zhaonian Zou,\n  Jianliang Xu", "title": "Budget Constrained Interactive Search for Multiple Targets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Interactive graph search leverages human intelligence to categorize target\nlabels in a hierarchy, which are useful for image classification, product\ncategorization, and database search. However, many existing studies of\ninteractive graph search aim at identifying a single target optimally, and\nsuffer from the limitations of asking too many questions and not being able to\nhandle multiple targets. To address these two limitations, in this paper, we\nstudy a new problem of budget constrained interactive graph search for multiple\ntargets called kBM-IGS-problem. Specifically, given a set of multiple targets T\nin a hierarchy, and two parameters k and b, the goal is to identify a k-sized\nset of selections S such that the closeness between selections S and targets T\nis as small as possible, by asking at most a budget of b questions. We\ntheoretically analyze the updating rules and design a penalty function to\ncapture the closeness between selections and targets. To tackle the\nkBM-IGS-problem, we develop a novel framework to ask questions using the best\nvertex with the largest expected gain, which makes a balanced trade-off between\ntarget probability and benefit gain. Based on the kBM-IGS framework, we first\npropose an efficient algorithm STBIS to handle the SingleTarget problem, which\nis a special case of kBM-IGS. Then, we propose a dynamic programming based\nmethod kBM-DP to tackle the MultipleTargets problem. To further improve\nefficiency, we propose two heuristic but efficient algorithms kBM-Topk and\nkBM-DP+. kBM-Topk develops a variant gain function and selects the top-k\nvertices independently. kBM-DP+ uses an upper bound of gains and prunes\ndisqualified vertices to save computations. Experiments on large real-world\ndatasets with ground-truth targets verify both the effectiveness and efficiency\nof our proposed algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 14:20:09 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 03:22:23 GMT"}, {"version": "v3", "created": "Thu, 17 Dec 2020 07:05:05 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Zhu", "Xuliang", ""], ["Huang", "Xin", ""], ["Choi", "Byron", ""], ["Jiang", "Jiaxin", ""], ["Zou", "Zhaonian", ""], ["Xu", "Jianliang", ""]]}, {"id": "2012.02006", "submitter": "Shenghua Liu", "authors": "Jiabao Zhang, Shenghua Liu, Wenting Hou, Siddharth Bhatia, Huawei\n  Shen, Wenjian Yu, Xueqi Cheng", "title": "AugSplicing: Synchronized Behavior Detection in Streaming Tensors", "comments": "AAAI Conference on Artificial Intelligence (AAAI), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG cs.SI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  How can we track synchronized behavior in a stream of time-stamped tuples,\nsuch as mobile devices installing and uninstalling applications in the\nlockstep, to boost their ranks in the app store? We model such tuples as\nentries in a streaming tensor, which augments attribute sizes in its modes over\ntime. Synchronized behavior tends to form dense blocks (i.e. subtensors) in\nsuch a tensor, signaling anomalous behavior, or interesting communities.\nHowever, existing dense block detection methods are either based on a static\ntensor, or lack an efficient algorithm in a streaming setting. Therefore, we\npropose a fast streaming algorithm, AugSplicing, which can detect the top dense\nblocks by incrementally splicing the previous detection with the incoming ones\nin new tuples, avoiding re-runs over all the history data at every tracking\ntime step. AugSplicing is based on a splicing condition that guides the\nalgorithm (Section 4). Compared to the state-of-the-art methods, our method is\n(1) effective to detect fraudulent behavior in installing data of real-world\napps and find a synchronized group of students with interesting features in\ncampus Wi-Fi data; (2) robust with splicing theory for dense block detection;\n(3) streaming and faster than the existing streaming algorithm, with closely\ncomparable accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 15:39:58 GMT"}, {"version": "v2", "created": "Sat, 5 Dec 2020 06:48:30 GMT"}, {"version": "v3", "created": "Tue, 15 Dec 2020 11:17:10 GMT"}, {"version": "v4", "created": "Tue, 5 Jan 2021 01:53:49 GMT"}, {"version": "v5", "created": "Tue, 30 Mar 2021 14:42:58 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Zhang", "Jiabao", ""], ["Liu", "Shenghua", ""], ["Hou", "Wenting", ""], ["Bhatia", "Siddharth", ""], ["Shen", "Huawei", ""], ["Yu", "Wenjian", ""], ["Cheng", "Xueqi", ""]]}, {"id": "2012.02258", "submitter": "Faisal Nawab", "authors": "Faisal Nawab", "title": "WedgeChain: A Trusted Edge-Cloud Store With Asynchronous (Lazy) Trust", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose WedgeChain, a data store that spans both edge and cloud nodes (an\nedge-cloud system). WedgeChain consists of a logging layer and a data indexing\nlayer. In this study, we encounter two challenges: (1) edge nodes are untrusted\nand potentially malicious, and (2) edge-cloud coordination is expensive.\nWedgeChain tackles these challenges by the following proposals: (1) Lazy\n(asynchronous) certification: where data is committed at the untrusted edge and\nthen lazily certified at the cloud node. This lazy certification method takes\nadvantage of the observation that an untrusted edge node is unlikely to act\nmaliciously if it knows it will be detected (and punished) eventually. Our lazy\ncertification method guarantees that malicious acts (i.e., lying) are\neventually detected. (2) Data-free certification: our lazy certification method\nonly needs to send digests of data to the cloud, instead of sending all data to\nthe cloud, which enables saving network and cloud resources and reduce costs.\n(3) LSMerkle: we extend a trusted index (mLSM) to enable indexing data at the\nedge while utilizing lazy and data-free certification.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 20:50:22 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Nawab", "Faisal", ""]]}, {"id": "2012.02454", "submitter": "Jerome Darmont", "authors": "J\\'er\\^ome Darmont (ERIC), C\\'ecile Favre (ERIC), Sabine Loudcher\n  (ERIC), Camille No\\^us", "title": "Data Lakes for Digital Humanities", "comments": "Data and Digital Humanities Track", "journal-ref": "2nd International Digital Tools & Uses Congress (DTUC 2020), Oct\n  2020, Hammamet, Tunisia. pp.38-41", "doi": "10.1145/3423603.3424004", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional data in Digital Humanities projects bear various formats\n(structured, semi-structured, textual) and need substantial transformations\n(encoding and tagging, stemming, lemmatization, etc.) to be managed and\nanalyzed. To fully master this process, we propose the use of data lakes as a\nsolution to data siloing and big data variety problems. We describe data lake\nprojects we currently run in close collaboration with researchers in humanities\nand social sciences and discuss the lessons learned running these projects.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 08:18:48 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Darmont", "J\u00e9r\u00f4me", "", "ERIC"], ["Favre", "C\u00e9cile", "", "ERIC"], ["Loudcher", "Sabine", "", "ERIC"], ["No\u00fbs", "Camille", ""]]}, {"id": "2012.02469", "submitter": "Ju Fan", "authors": "Nan Tang, Ju Fan, Fangyi Li, Jianhong Tu, Xiaoyong Du, Guoliang Li,\n  Sam Madden, Mourad Ouzzani", "title": "RPT: Relational Pre-trained Transformer Is Almost All You Need towards\n  Democratizing Data Preparation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can AI help automate human-easy but computer-hard data preparation tasks that\nburden data scientists, practitioners, and crowd workers? We answer this\nquestion by presenting RPT, a denoising auto-encoder for tuple-to-X models (X\ncould be tuple, token, label, JSON, and so on). RPT is pre-trained for a\ntuple-to-tuple model by corrupting the input tuple and then learning a model to\nreconstruct the original tuple. It adopts a Transformer-based neural\ntranslation architecture that consists of a bidirectional encoder (similar to\nBERT) and a left-to-right autoregressive decoder (similar to GPT), leading to a\ngeneralization of both BERT and GPT. The pre-trained RPT can already support\nseveral common data preparation tasks such as data cleaning, auto-completion\nand schema matching. Better still, RPT can be fine-tuned on a wide range of\ndata preparation tasks, such as value normalization, data transformation, data\nannotation, etc. To complement RPT, we also discuss several appealing\ntechniques such as collaborative training and few-shot learning for entity\nresolution, and few-shot learning and NLP question-answering for information\nextraction. In addition, we identify a series of research opportunities to\nadvance the field of data preparation.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 08:52:05 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 08:28:30 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Tang", "Nan", ""], ["Fan", "Ju", ""], ["Li", "Fangyi", ""], ["Tu", "Jianhong", ""], ["Du", "Xiaoyong", ""], ["Li", "Guoliang", ""], ["Madden", "Sam", ""], ["Ouzzani", "Mourad", ""]]}, {"id": "2012.02619", "submitter": "Mohamed-Bachir Belaid", "authors": "Christian Bessiere, Mohamed-Bachir Belaid, Nadjib Lazaar", "title": "Computational Complexity of Three Central Problems in Itemset Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Itemset mining is one of the most studied tasks in knowledge discovery. In\nthis paper we analyze the computational complexity of three central itemset\nmining problems. We prove that mining confident rules with a given item in the\nhead is NP-hard. We prove that mining high utility itemsets is NP-hard. We\nfinally prove that mining maximal or closed itemsets is coNP-hard as soon as\nthe users can specify constraints on the kind of itemsets they are interested\nin.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 14:26:21 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 09:57:19 GMT"}, {"version": "v3", "created": "Tue, 8 Dec 2020 11:17:14 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Bessiere", "Christian", ""], ["Belaid", "Mohamed-Bachir", ""], ["Lazaar", "Nadjib", ""]]}, {"id": "2012.02710", "submitter": "Stefanie Rinderle-Ma", "authors": "Conrad Indiono, Stefanie Rinderle-Ma", "title": "Hiperfact: In-Memory High Performance Fact Processing -- Rethinking the\n  Rete Inference Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Rete forward inference algorithm forms the basis for many rule engines\ndeployed today, but it exhibits the following problems: (1) the caching of all\nintermediate join results, (2) the processing of all rules regardless of the\nnecessity to do so (stemming from the underlying forward inference approach),\n(3) not defining the join order of rules and its conditions, significantly\naffecting the final run-time performance, and finally (4) pointer chasing due\nto the overall network structure, leading to inefficient usage of the CPU\ncaches caused by random access patterns. The Hiperfact approach aims to\novercome these shortcomings by (1) choosing cache efficient data structures on\nthe primary rank 1 fact index storage and intermediate join result storage\nlevels, (2) introducing island fact processing for determining the join order\nby ensuring minimal intermediate join result construction, and (3) introducing\nderivation trees to allow for parallel read/write access and lazy rule\nevaluation. The experimental evaluations show that the Hiperfact prototype\nengine implementing the approach achieves significant improvements in respect\nto both inference and query performance. Moreover, the proposed Hiperfact\nengine is compared to existing engines in the context of a comprehensive\nbenchmark.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 16:38:01 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Indiono", "Conrad", ""], ["Rinderle-Ma", "Stefanie", ""]]}, {"id": "2012.03513", "submitter": "Zhaoqiang Chen", "authors": "Qun Chen, Zhaoqiang Chen, Youcef Nafa, Tianyi Duan, Zhanhuai Li", "title": "Adaptive Deep Learning for Entity Resolution by Risk Analysis", "comments": "31 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state-of-the-art performance on entity resolution (ER) has been achieved\nby deep learning. However, deep models are usually trained on large quantities\nof accurately labeled training data, and can not be easily tuned towards a\ntarget workload. Unfortunately, in real scenarios, there may not be sufficient\nlabeled training data, and even worse, their distribution is usually more or\nless different from the target workload even when they come from the same\ndomain.\n  To alleviate the said limitations, this paper proposes a novel risk-based\napproach to tune a deep model towards a target workload by its particular\ncharacteristics. Built on the recent advances on risk analysis for ER, the\nproposed approach first trains a deep model on labeled training data, and then\nfine-tunes it by minimizing its estimated misprediction risk on unlabeled\ntarget data. Our theoretical analysis shows that risk-based adaptive training\ncan correct the label status of a mispredicted instance with a fairly good\nchance. We have also empirically validated the efficacy of the proposed\napproach on real benchmark data by a comparative study. Our extensive\nexperiments show that it can considerably improve the performance of deep\nmodels. Furthermore, in the scenario of distribution misalignment, it can\nsimilarly outperform the state-of-the-art alternative of transfer learning by\nconsiderable margins. Using ER as a test case, we demonstrate that risk-based\nadaptive training is a promising approach potentially applicable to various\nchallenging classification tasks.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 08:05:46 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 09:33:34 GMT"}, {"version": "v3", "created": "Sat, 13 Mar 2021 03:18:58 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Chen", "Qun", ""], ["Chen", "Zhaoqiang", ""], ["Nafa", "Youcef", ""], ["Duan", "Tianyi", ""], ["Li", "Zhanhuai", ""]]}, {"id": "2012.03540", "submitter": "Rong Zhu", "authors": "Rong Zhu, Andreas Pfadler, Ziniu Wu, Yuxing Han, Xiaoke Yang, Feng Ye,\n  Zhenping Qian, Jingren Zhou, Bin Cui", "title": "Efficient and Scalable Structure Learning for Bayesian Networks:\n  Algorithms and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Structure Learning for Bayesian network (BN) is an important problem with\nextensive research. It plays central roles in a wide variety of applications in\nAlibaba Group. However, existing structure learning algorithms suffer from\nconsiderable limitations in real world applications due to their low efficiency\nand poor scalability. To resolve this, we propose a new structure learning\nalgorithm LEAST, which comprehensively fulfills our business requirements as it\nattains high accuracy, efficiency and scalability at the same time. The core\nidea of LEAST is to formulate the structure learning into a continuous\nconstrained optimization problem, with a novel differentiable constraint\nfunction measuring the acyclicity of the resulting graph. Unlike with existing\nwork, our constraint function is built on the spectral radius of the graph and\ncould be evaluated in near linear time w.r.t. the graph node size. Based on it,\nLEAST can be efficiently implemented with low storage overhead. According to\nour benchmark evaluation, LEAST runs 1 to 2 orders of magnitude faster than\nstate of the art method with comparable accuracy, and it is able to scale on\nBNs with up to hundreds of thousands of variables. In our production\nenvironment, LEAST is deployed and serves for more than 20 applications with\nthousands of executions per day. We describe a concrete scenario in a ticket\nbooking service in Alibaba, where LEAST is applied to build a near real-time\nautomatic anomaly detection and root error cause analysis system. We also show\nthat LEAST unlocks the possibility of applying BN structure learning in new\nareas, such as large-scale gene expression data analysis and explainable\nrecommendation system.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 09:11:08 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Zhu", "Rong", ""], ["Pfadler", "Andreas", ""], ["Wu", "Ziniu", ""], ["Han", "Yuxing", ""], ["Yang", "Xiaoke", ""], ["Ye", "Feng", ""], ["Qian", "Zhenping", ""], ["Zhou", "Jingren", ""], ["Cui", "Bin", ""]]}, {"id": "2012.04295", "submitter": "Mohsin Iqbal", "authors": "Mohsin Iqbal and Matteo Lissandrini and Torben Bach Pederse", "title": "A Foundation for Spatio-Textual-Temporal Cube Analytics (Extended\n  Version)", "comments": "14 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large amounts of spatial, textual, and temporal data are being produced\ndaily. This is data containing an unstructured component (text), a spatial\ncomponent (geographic position), and a time component (timestamp). Therefore,\nthere is a need for a powerful and general way of analyzing spatial, textual,\nand temporal data together. In this paper, we define and formalize the\nSpatio-Textual-Temporal Cube structure to enable combined effective and\nefficient analytical queries over spatial, textual, and temporal data. Our\nnovel data model over spatio-textual-temporal objects enables novel joint and\nintegrated spatial, textual, and temporal insights that are hard to obtain\nusing existing methods. Moreover, we introduce the new concept of\nspatio-textual-temporal measures with associated novel\nspatio-textual-temporal-OLAP operators. To allow for efficient large-scale\nanalytics, we present a pre-aggregation framework for the exact and approximate\ncomputation of spatio-textual-temporal measures. Our comprehensive experimental\nevaluation on a real-world Twitter dataset confirms that our proposed methods\nreduce query response time by 1-5 orders of magnitude compared to the No\nMaterialization baseline and decrease storage cost between 97% and 99.9%\ncompared to the Full Materialization baseline while adding only a negligible\noverhead in the Spatio-Textual-Temporal Cube construction time. Moreover,\napproximate computation achieves an accuracy between 90% and 100% while\nreducing query response time by 3-5 orders of magnitude compared to No\nMaterialization.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 09:15:02 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Iqbal", "Mohsin", ""], ["Lissandrini", "Matteo", ""], ["Pederse", "Torben Bach", ""]]}, {"id": "2012.04361", "submitter": "Genoveva Vargas-Solar", "authors": "Genoveva Vargas-Solar (LAFMIA, LIRIS), Ana-Sagrario Castillo-Camporro,\n  Jos\\'e Zechinelli-Martini (UDLAP), Javier Espinosa-Oviedo (LAFMIA)", "title": "From Data Harvesting to Querying for Making Urban Territories Smart", "comments": null, "journal-ref": "Carlos Alberto Ochoa. Innovative Applications in Smart Cities,\n  Taylor and Francis, In press", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter provides a summarized, critical and analytical point of view of\nthe data-centric solutions that are currently applied for addressing urban\nproblems in cities. These solutions lead to the use of urban computing\ntechniques to address their daily life issues. Data-centric solutions have\nbecome popular due to the emergence of data science. The chapter describes and\ndiscusses the type of urban challenges and how data science in urban computing\ncan face them. Current solutions address a spectrum that goes from data\nharvesting techniques to decision making support. Finally, the chapter also\nputs in perspective families of strategies developed in the state of the art\nfor addressing urban problems and exhibits guidelines that can lead to a\nmethodological understanding of these strategies.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 11:15:48 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Vargas-Solar", "Genoveva", "", "LAFMIA, LIRIS"], ["Castillo-Camporro", "Ana-Sagrario", "", "UDLAP"], ["Zechinelli-Martini", "Jos\u00e9", "", "UDLAP"], ["Espinosa-Oviedo", "Javier", "", "LAFMIA"]]}, {"id": "2012.04553", "submitter": "Keval Vora", "authors": "Kasra Jamshidi and Keval Vora", "title": "Pattern Morphing for Efficient Graph Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Graph mining applications analyze the structural properties of large graphs,\nand they do so by finding subgraph isomorphisms, which makes them\ncomputationally intensive. Existing graph mining techniques including both\ncustom graph mining applications and general-purpose graph mining systems,\ndevelop efficient execution plans to speed up the exploration of the given\nquery patterns that represent subgraph structures of interest.\n  In this paper, we step beyond the traditional philosophy of optimizing the\nexecution plans for a given set of patterns, and exploit the sub-structural\nsimilarities across different query patterns. We propose Pattern Morphing, a\ntechnique that enables structure-aware algebra over patterns to accurately\ninfer the results for a given set of patterns using the results of a completely\ndifferent set of patterns that are less expensive to compute. Pattern morphing\n\"morphs\" (or converts) a given set of query patterns into alternative patterns,\nwhile retaining full equivalency. It is a general technique that supports\nvarious operations over matches of a pattern beyond just counting (e.g.,\nsupport calculation, enumeration, etc.), making it widely applicable to various\ngraph mining applications like Motif Counting and Frequent Subgraph Mining.\nSince pattern morphing mainly transforms query patterns before their\nexploration starts, it can be easily incorporated in existing general-purpose\ngraph mining systems. We evaluate the effectiveness of pattern morphing by\nincorporating it in Peregrine, a recent state-of-the-art graph mining system,\nand show that pattern morphing significantly improves the performance of\ndifferent graph mining applications.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 16:46:31 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Jamshidi", "Kasra", ""], ["Vora", "Keval", ""]]}, {"id": "2012.06171", "submitter": "Alexandru Iosup", "authors": "Sherif Sakr, Angela Bonifati, Hannes Voigt, Alexandru Iosup, Khaled\n  Ammar, Renzo Angles, Walid Aref, Marcelo Arenas, Maciej Besta, Peter A.\n  Boncz, Khuzaima Daudjee, Emanuele Della Valle, Stefania Dumbrava, Olaf\n  Hartig, Bernhard Haslhofer, Tim Hegeman, Jan Hidders, Katja Hose, Adriana\n  Iamnitchi, Vasiliki Kalavri, Hugo Kapp, Wim Martens, M. Tamer \\\"Ozsu, Eric\n  Peukert, Stefan Plantikow, Mohamed Ragab, Matei R. Ripeanu, Semih Salihoglu,\n  Christian Schulz, Petra Selmer, Juan F. Sequeda, Joshua Shinavier, G\\'abor\n  Sz\\'arnyas, Riccardo Tommasini, Antonino Tumeo, Alexandru Uta, Ana Lucia\n  Varbanescu, Hsiang-Yun Wu, Nikolay Yakovets, Da Yan, Eiko Yoneki", "title": "The Future is Big Graphs! A Community View on Graph Processing Systems", "comments": "12 pages, 3 figures, collaboration between the large-scale systems\n  and data management communities, work started at the Dagstuhl Seminar 19491\n  on Big Graph Processing Systems, to be published in the Communications of the\n  ACM", "journal-ref": null, "doi": "10.1145/3434642", "report-no": null, "categories": "cs.DC cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Graphs are by nature unifying abstractions that can leverage\ninterconnectedness to represent, explore, predict, and explain real- and\ndigital-world phenomena. Although real users and consumers of graph instances\nand graph workloads understand these abstractions, future problems will require\nnew abstractions and systems. What needs to happen in the next decade for big\ngraph processing to continue to succeed?\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 07:35:07 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Sakr", "Sherif", ""], ["Bonifati", "Angela", ""], ["Voigt", "Hannes", ""], ["Iosup", "Alexandru", ""], ["Ammar", "Khaled", ""], ["Angles", "Renzo", ""], ["Aref", "Walid", ""], ["Arenas", "Marcelo", ""], ["Besta", "Maciej", ""], ["Boncz", "Peter A.", ""], ["Daudjee", "Khuzaima", ""], ["Della Valle", "Emanuele", ""], ["Dumbrava", "Stefania", ""], ["Hartig", "Olaf", ""], ["Haslhofer", "Bernhard", ""], ["Hegeman", "Tim", ""], ["Hidders", "Jan", ""], ["Hose", "Katja", ""], ["Iamnitchi", "Adriana", ""], ["Kalavri", "Vasiliki", ""], ["Kapp", "Hugo", ""], ["Martens", "Wim", ""], ["\u00d6zsu", "M. Tamer", ""], ["Peukert", "Eric", ""], ["Plantikow", "Stefan", ""], ["Ragab", "Mohamed", ""], ["Ripeanu", "Matei R.", ""], ["Salihoglu", "Semih", ""], ["Schulz", "Christian", ""], ["Selmer", "Petra", ""], ["Sequeda", "Juan F.", ""], ["Shinavier", "Joshua", ""], ["Sz\u00e1rnyas", "G\u00e1bor", ""], ["Tommasini", "Riccardo", ""], ["Tumeo", "Antonino", ""], ["Uta", "Alexandru", ""], ["Varbanescu", "Ana Lucia", ""], ["Wu", "Hsiang-Yun", ""], ["Yakovets", "Nikolay", ""], ["Yan", "Da", ""], ["Yoneki", "Eiko", ""]]}, {"id": "2012.06237", "submitter": "Ugo Comignani", "authors": "Ugo Comignani, Laure Berti-\\'Equille, No\\\"el Novelli", "title": "Discovering Multi-Table Functional Dependencies Without Full Join\n  Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of discovering join FDs, i.e., functional\ndependencies (FDs) that hold on multiple joined tables. We leverage logical\ninference, selective mining, and sampling and show that we can discover most of\nthe exact join FDs from the single tables participating to the join and avoid\nthe full computation of the join result. We propose algorithms to speed-up the\njoin FD discovery process and mine FDs on the fly only from necessary data\npartitions. We introduce JEDI (Join dEpendency DIscovery), our solution to\ndiscover join FDs without computation of the full join beforehand. Our\nexperiments on a range of real-world and synthetic data demonstrate the\nbenefits of our method over existing FD discovery methods that need to\nprecompute the join results before discovering the FDs. We show that the\nperformance depends on the cardinalities and coverage of the join attribute\nvalues: for join operations with low coverage, JEDI with selective mining\noutperforms the competing methods using the straightforward approach of full\njoin computation by one order of magnitude in terms of runtime and can discover\nthree-quarters of the exact join FDs using mainly logical inference in half of\nits total execution time on average. For higher join coverage, JEDI with\nsampling reaches precision of 1 with only 63% of the table input size on\naverage.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 10:56:57 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Comignani", "Ugo", ""], ["Berti-\u00c9quille", "Laure", ""], ["Novelli", "No\u00ebl", ""]]}, {"id": "2012.06246", "submitter": "Christian Requena-Mesa", "authors": "Christian Requena-Mesa, Vitus Benson, Joachim Denzler, Jakob Runge and\n  Markus Reichstein", "title": "EarthNet2021: A novel large-scale dataset and challenge for forecasting\n  localized climate impacts", "comments": "4 pages, presented at Tackling Climate Change with Machine Learning\n  at NeurIPS2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Climate change is global, yet its concrete impacts can strongly vary between\ndifferent locations in the same region. Seasonal weather forecasts currently\noperate at the mesoscale (> 1 km). For more targeted mitigation and adaptation,\nmodelling impacts to < 100 m is needed. Yet, the relationship between driving\nvariables and Earth's surface at such local scales remains unresolved by\ncurrent physical models. Large Earth observation datasets now enable us to\ncreate machine learning models capable of translating coarse weather\ninformation into high-resolution Earth surface forecasts. Here, we define\nhigh-resolution Earth surface forecasting as video prediction of satellite\nimagery conditional on mesoscale weather forecasts. Video prediction has been\ntackled with deep learning models. Developing such models requires\nanalysis-ready datasets. We introduce EarthNet2021, a new, curated dataset\ncontaining target spatio-temporal Sentinel 2 satellite imagery at 20 m\nresolution, matched with high-resolution topography and mesoscale (1.28 km)\nweather variables. With over 32000 samples it is suitable for training deep\nneural networks. Comparing multiple Earth surface forecasts is not trivial.\nHence, we define the EarthNetScore, a novel ranking criterion for models\nforecasting Earth surface reflectance. For model intercomparison we frame\nEarthNet2021 as a challenge with four tracks based on different test sets.\nThese allow evaluation of model validity and robustness as well as model\napplicability to extreme events and the complete annual vegetation cycle. In\naddition to forecasting directly observable weather impacts through\nsatellite-derived vegetation indices, capable Earth surface models will enable\ndownstream applications such as crop yield prediction, forest health\nassessments, coastline management, or biodiversity monitoring. Find data, code,\nand how to participate at www.earthnet.tech .\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 11:21:00 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Requena-Mesa", "Christian", ""], ["Benson", "Vitus", ""], ["Denzler", "Joachim", ""], ["Runge", "Jakob", ""], ["Reichstein", "Markus", ""]]}, {"id": "2012.06446", "submitter": "Xiufeng Liu", "authors": "Xiufeng Liu, Rongling Li, Yi Wang, Per Sieverts Nielsen", "title": "SEGSys: A mapping system for segmentation analysis in energy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Customer segmentation analysis can give valuable insights into the energy\nefficiency of residential buildings. This paper presents a mapping system,\nSEGSys that enables segmentation analysis at the individual and the\nneighborhood levels. SEGSys supports the online and offline classification of\ncustomers based on their daily consumption patterns and consumption intensity.\nIt also supports the segmentation analysis according to the social\ncharacteristics of customers of individual households or neighborhoods, as well\nas spatial geometries. SEGSys uses a three-layer architecture to model the\nsegmentation system, including the data layer, the service layer, and the\npresentation layer. The data layer models data into a star schema within a data\nwarehouse, the service layer provides data service through a RESTful interface,\nand the presentation layer interacts with users through a visual map. This\npaper showcases the system on the segmentation analysis using an electricity\nconsumption data set and validates the effectiveness of the system.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 16:13:24 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Liu", "Xiufeng", ""], ["Li", "Rongling", ""], ["Wang", "Yi", ""], ["Nielsen", "Per Sieverts", ""]]}, {"id": "2012.06635", "submitter": "Yufeng Luo", "authors": "Yufeng Luo, Roland Haas, Qian Zhang, Gabrielle Allen", "title": "DataVault: A Data Storage Infrastructure for the Einstein Toolkit", "comments": "17 pages, 3 figures, 2 tables", "journal-ref": null, "doi": "10.1088/1361-6382/abf9b5", "report-no": null, "categories": "gr-qc cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data sharing is essential in the numerical simulations research. We introduce\na data repository, DataVault, that is designed for data sharing, search and\nanalysis. A comparative study of existing repositories is performed to analyze\nfeatures that are critical to a data repository. We describe the architecture,\nworkflow, and deployment of DataVault, and provide three use-case scenarios for\ndifferent communities to facilitate the use and application of DataVault.\nPotential features are proposed and we outline the future development for these\nfeatures.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 21:05:32 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 16:30:29 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Luo", "Yufeng", ""], ["Haas", "Roland", ""], ["Zhang", "Qian", ""], ["Allen", "Gabrielle", ""]]}, {"id": "2012.06683", "submitter": "Vikram Nathan", "authors": "Vikram Nathan, Jialin Ding, Tim Kraska, Mohammad Alizadeh", "title": "Cortex: Harnessing Correlations to Boost Query Performance", "comments": "13 pages, including references. Under submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Databases employ indexes to filter out irrelevant records, which reduces scan\noverhead and speeds up query execution. However, this optimization is only\navailable to queries that filter on the indexed attribute. To extend these\nspeedups to queries on other attributes, database systems have turned to\nsecondary and multi-dimensional indexes. Unfortunately, these approaches are\nrestrictive: secondary indexes have a large memory footprint and can only speed\nup queries that access a small number of records, and multi-dimensional indexes\ncannot scale to more than a handful of columns. We present Cortex, an approach\nthat takes advantage of correlations to extend the reach of primary indexes to\nmore attributes. Unlike prior work, Cortex can adapt itself to any existing\nprimary index, whether single or multi-dimensional, to harness a broad variety\nof correlations, such as those that exist between more than two attributes or\nhave a large number of outliers. We demonstrate that on real datasets\nexhibiting these diverse types of correlations, Cortex matches or outperforms\ntraditional secondary indexes with $5\\times$ less space, and it is $2-8\\times$\nfaster than existing approaches to indexing correlations.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 00:22:51 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Nathan", "Vikram", ""], ["Ding", "Jialin", ""], ["Kraska", "Tim", ""], ["Alizadeh", "Mohammad", ""]]}, {"id": "2012.06743", "submitter": "Changbo Qu", "authors": "Xiaoying Wang, Changbo Qu, Weiyuan Wu, Jiannan Wang, Qingqing Zhou", "title": "Are We Ready For Learned Cardinality Estimation?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardinality estimation is a fundamental but long unresolved problem in query\noptimization. Recently, multiple papers from different research groups\nconsistently report that learned models have the potential to replace existing\ncardinality estimators. In this paper, we ask a forward-thinking question: Are\nwe ready to deploy these learned cardinality models in production? Our study\nconsists of three main parts. Firstly, we focus on the static environment\n(i.e., no data updates) and compare five new learned methods with eight\ntraditional methods on four real-world datasets under a unified workload\nsetting. The results show that learned models are indeed more accurate than\ntraditional methods, but they often suffer from high training and inference\ncosts. Secondly, we explore whether these learned models are ready for dynamic\nenvironments (i.e., frequent data updates). We find that they cannot catch up\nwith fast data up-dates and return large errors for different reasons. For less\nfrequent updates, they can perform better but there is no clear winner among\nthemselves. Thirdly, we take a deeper look into learned models and explore when\nthey may go wrong. Our results show that the performance of learned methods can\nbe greatly affected by the changes in correlation, skewness, or domain size.\nMore importantly, their behaviors are much harder to interpret and often\nunpredictable. Based on these findings, we identify two promising research\ndirections (control the cost of learned models and make learned models\ntrustworthy) and suggest a number of research opportunities. We hope that our\nstudy can guide researchers and practitioners to work together to eventually\npush learned cardinality estimators into real database systems.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 06:52:11 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 07:04:05 GMT"}, {"version": "v3", "created": "Mon, 15 Mar 2021 23:35:27 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Wang", "Xiaoying", ""], ["Qu", "Changbo", ""], ["Wu", "Weiyuan", ""], ["Wang", "Jiannan", ""], ["Zhou", "Qingqing", ""]]}, {"id": "2012.06802", "submitter": "Li Zeng", "authors": "Li Zeng and Yan Jiang and Weixin Lu and Lei Zou", "title": "Deep Analysis on Subgraph Isomorphism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subgraph isomorphism is a well-known NP-hard problem which is widely used in\nmany applications, such as social network analysis and knowledge graph query.\nIts performance is often limited by the inherent hardness. Several insightful\nworks have been done since 2012, mainly optimizing pruning rules and matching\norders to accelerate enumerating all isomorphic subgraphs. Nevertheless, their\ncorrectness and performance are not well studied. First, different languages\nare used in implementation with different compilation flags. Second,\nexperiments are not done on the same platform and the same datasets. Third,\nsome ideas of different works are even complementary. Last but not least, there\nexist errors when applying some algorithms. In this paper, we address these\nproblems by re-implementing seven representative subgraph isomorphism\nalgorithms as well as their improved versions, and conducting comprehensive\nexperiments on various graphs. The results show pros and cons of\nstate-of-the-art solutions and explore new approaches to optimization.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 12:29:38 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 15:28:09 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Zeng", "Li", ""], ["Jiang", "Yan", ""], ["Lu", "Weixin", ""], ["Zou", "Lei", ""]]}, {"id": "2012.06966", "submitter": "Zhixin Qi", "authors": "Zhixin Qi, Hongzhi Wang, Haoran Zhang", "title": "A Dual-Store Structure for Knowledge Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To effectively manage increasing knowledge graphs in various domains, a hot\nresearch topic, knowledge graph storage management, has emerged. Existing\nmethods are classified to relational stores and native graph stores. Relational\nstores are able to store large-scale knowledge graphs and convenient in\nupdating knowledge, but the query performance weakens obviously when the\nselectivity of a knowledge graph query is large. Native graph stores are\nefficient in processing complex knowledge graph queries due to its index-free\nadjacent property, but they are inapplicable to manage a large-scale knowledge\ngraph due to limited storage budgets or inflexible updating process. Motivated\nby this, we propose a dual-store structure which leverages a graph store to\naccelerate the complex query process in the relational store. However, it is\nchallenging to determine what data to transfer from relational store to graph\nstore at what time. To address this problem, we formulate it as a Markov\nDecision Process and derive a physical design tuner DOTIL based on\nreinforcement learning. With DOTIL, the dual-store structure is adaptive to\ndynamic changing workloads. Experimental results on real knowledge graphs\ndemonstrate that our proposed dual-store structure improves query performance\nup to average 43.72% compared with the most commonly used relational stores.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 05:33:19 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Qi", "Zhixin", ""], ["Wang", "Hongzhi", ""], ["Zhang", "Haoran", ""]]}, {"id": "2012.06981", "submitter": "Stephen Macke", "authors": "Stephen Macke, Hongpu Gong, Doris Jung-Lin Lee, Andrew Head, Doris\n  Xin, Aditya Parameswaran", "title": "Fine-Grained Lineage for Safer Notebook Interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DB cs.HC cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computational notebooks have emerged as the platform of choice for data\nscience and analytical workflows, enabling rapid iteration and exploration. By\nkeeping intermediate program state in memory and segmenting units of execution\ninto so-called \"cells\", notebooks allow users to execute their workflows\ninteractively and enjoy particularly tight feedback. However, as cells are\nadded, removed, reordered, and rerun, this hidden intermediate state\naccumulates in a way that is not necessarily correlated with the notebook's\nvisible code, making execution behavior difficult to reason about, and leading\nto errors and lack of reproducibility. We present NBSafety, a custom Jupyter\nkernel that uses runtime tracing and static analysis to automatically manage\nlineage associated with cell execution and global notebook state. NBSafety\ndetects and prevents errors that users make during unaided notebook\ninteractions, all while preserving the flexibility of existing notebook\nsemantics. We evaluate NBSafety's ability to prevent erroneous interactions by\nreplaying and analyzing 666 real notebook sessions. Of these, NBSafety\nidentified 117 sessions with potential safety errors, and in the remaining 549\nsessions, the cells that NBSafety identified as resolving safety issues were\nmore than $7\\times$ more likely to be selected by users for re-execution\ncompared to a random baseline, even though the users were not using NBSafety\nand were therefore not influenced by its suggestions.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 06:50:31 GMT"}, {"version": "v2", "created": "Sat, 19 Jun 2021 20:20:44 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Macke", "Stephen", ""], ["Gong", "Hongpu", ""], ["Lee", "Doris Jung-Lin", ""], ["Head", "Andrew", ""], ["Xin", "Doris", ""], ["Parameswaran", "Aditya", ""]]}, {"id": "2012.07060", "submitter": "Ji Han Dr", "authors": "Ji Han, Serhad Sarica, Feng Shi, Jianxi Luo", "title": "Semantic Networks for Engineering Design: A Survey", "comments": "12 pages, 2 tables, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There have been growing uses of semantic networks in the past decade, such as\nleveraging large-scale pre-trained graph knowledge databases for various\nnatural language processing (NLP) tasks in engineering design research.\nTherefore, the paper provides a survey of the research that has employed\nsemantic networks in the engineering design research community. The survey\nreveals that engineering design researchers have primarily relied on WordNet,\nConceptNet, and other common-sense semantic network databases trained on\nnon-engineering data sources to develop methods or tools for engineering\ndesign. Meanwhile, there are emerging efforts to mine large scale technical\npublication and patent databases to construct engineering-contextualized\nsemantic network databases, e.g., B-Link and TechNet, to support NLP in\nengineering design. On this basis, we recommend future research directions for\nthe construction and applications of engineering-related semantic networks in\nengineering design research and practice.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 13:36:20 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Han", "Ji", ""], ["Sarica", "Serhad", ""], ["Shi", "Feng", ""], ["Luo", "Jianxi", ""]]}, {"id": "2012.07108", "submitter": "Olivier Cur\\'e", "authors": "Weiqin Xu and Olivier Cur\\'e and Philippe Calvez", "title": "Knowledge Graph Management on the Edge", "comments": "13 pages, 14 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge computing emerges as an innovative platform for services requiring low\nlatency decision making. Its success partly depends on the existence of\nefficient data management systems. We consider that knowledge graph management\nsystems have a key role to play in this context due to their data integration\nand reasoning features. In this paper, we present SuccinctEdge, a compact,\ndecompression-free, self-index, in-memory RDF store that can answer SPARQL\nqueries, including those requiring reasoning services associated to some\nontology. We provide details on its design and implementation before\ndemonstrating its efficiency on real-world and synthetic datasets.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 17:25:26 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Xu", "Weiqin", ""], ["Cur\u00e9", "Olivier", ""], ["Calvez", "Philippe", ""]]}, {"id": "2012.07309", "submitter": "Ruihong Huang", "authors": "Ruihong Huang, Jianmin Wang", "title": "Event Data Quality: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event data are prevalent in diverse domains such as financial trading,\nbusiness workflows and industrial IoT nowadays. An event is often characterized\nby several attributes denoting the meaning associated with the corresponding\noccurrence time/duration. From traditional operational systems in enterprises\nto online systems for Web services, event data is generated from physical world\nuninterruptedly. However, due to the variety and veracity features of Big data,\nevent data generated from heterogeneous and dirty sources could have very\ndifferent event representations and data quality issues. In this work, we\nsummarize several typical works on studying data quality issues of event data,\nincluding: (1) event matching, (2) event error detection, (3) event data\nrepair, and (4) approximate pattern matching.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 07:49:04 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Huang", "Ruihong", ""], ["Wang", "Jianmin", ""]]}, {"id": "2012.08083", "submitter": "Alireza Samadian", "authors": "Mahmoud Abo-Khamis, Sungjin Im, Benjamin Moseley, Kirk Pruhs, Alireza\n  Samadian", "title": "Instance Optimal Join Size Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of efficiently estimating the size of the inner join\nof a collection of preprocessed relational tables from the perspective of\ninstance optimality analysis. The run time of instance optimal algorithms is\ncomparable to the minimum time needed to verify the correctness of a solution.\nPreviously instance optimal algorithms were only known when the size of the\njoin was small (as one component of their run time that was linear in the join\nsize). We give an instance optimal algorithm for estimating the join size for\nall instances, including when the join size is large, by removing the\ndependency on the join size. As a byproduct, we show how to sample rows from\nthe join uniformly at random in a comparable amount of time.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 04:24:18 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Abo-Khamis", "Mahmoud", ""], ["Im", "Sungjin", ""], ["Moseley", "Benjamin", ""], ["Pruhs", "Kirk", ""], ["Samadian", "Alireza", ""]]}, {"id": "2012.08105", "submitter": "Panpan Li", "authors": "Panpan Li, Yikun Gong, Chen Wang", "title": "Schema Extraction on Semi-structured Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the continuous development of NoSQL databases, more and more developers\nchoose to use semi-structured data for development and data management, which\nputs forward requirements for schema management of semi-structured data stored\nin NoSQL databases. Schema extraction plays an important role in understanding\nschemas, optimizing queries, and validating data consistency. Therefore, in\nthis survey we investigate structural methods based on tree and graph and\nstatistical methods based on distributed architecture and machine learning to\nextract schemas. The schemas obtained by the structural methods are more\ninterpretable, and the statistical methods have better applicability and\ngeneralization ability. Moreover, we also investigate tools and systems for\nschemas extraction. Schema extraction tools are mainly used for spark or NoSQL\ndatabases, and are suitable for small datasets or simple application\nenvironments. The system mainly focuses on the extraction and management of\nschemas in large data sets and complex application scenarios. Furthermore, we\nalso compare these techniques to facilitate data managers' choice.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 05:57:41 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Li", "Panpan", ""], ["Gong", "Yikun", ""], ["Wang", "Chen", ""]]}, {"id": "2012.08146", "submitter": "Amol Kelkar", "authors": "Amol Kelkar, Nachiketa Rajpurohit, Utkarsh Mittal and Peter Relan", "title": "Generation of complex database queries and API calls from natural\n  language utterances", "comments": "Accepted at WeCNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generating queries corresponding to natural language questions is a long\nstanding problem. Traditional methods lack language flexibility, while newer\nsequence-to-sequence models require large amount of data. Schema-agnostic\nsequence-to-sequence models can be fine-tuned for a specific schema using a\nsmall dataset but these models have relatively low accuracy. We present a\nmethod that transforms the query generation problem into an intent\nclassification and slot filling problem. This method can work using small\ndatasets. For questions similar to the ones in the training dataset, it\nproduces complex queries with high accuracy. For other questions, it can use a\ntemplate-based approach or predict query pieces to construct the queries, still\nat a higher accuracy than sequence-to-sequence models. On a real-world dataset,\na schema fine-tuned state-of-the-art generative model had 60\\% exact match\naccuracy for the query generation task, while our method resulted in 92\\% exact\nmatch accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 08:28:52 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Kelkar", "Amol", ""], ["Rajpurohit", "Nachiketa", ""], ["Mittal", "Utkarsh", ""], ["Relan", "Peter", ""]]}, {"id": "2012.08325", "submitter": "Romain David", "authors": "Caterina Caracciolo (FAO), Sophie Aubin (DipSO), Clement Jonquet\n  (LIRMM, UM, CNRS), Emna Amdouni (LIRMM, UM, CNRS), Romain David (MISTEA,\n  ERINHA-AISBL), Leyla Garcia (ZB MED), Brandon Whitehead, Catherine Roussey\n  (UR TSCF, INRAE), Armando Stellato, Ferdinando Villa (BC3)", "title": "39 Hints to Facilitate the Use of Semantics for Data on Agriculture and\n  Nutrition", "comments": null, "journal-ref": "CODATA Data Science Journal, Committee on Data for Science and\n  Technology (CODATA), 2020, 19 (1)", "doi": "10.5334/dsj-2020-047", "report-no": null, "categories": "q-bio.OT cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we report on the outputs and adoption of the Agrisemantics\nWorking Group of the Research Data Alliance (RDA), consisting of a set of\nrecommendations to facilitate the adoption of semantic technologies and methods\nfor the purpose of data interoperability in the field of agriculture and\nnutrition. From 2016 to 2019, the group gathered researchers and practitioners\nat the crossing point between information technology and agricultural science,\nto study all aspects in the life cycle of semantic resources:\nconceptualization, edition, sharing, standardization, services, alignment, long\nterm support. First, the working group realized a landscape study, a study of\nthe uses of semantics in agrifood, then collected use cases for the\nexploitation of semantics resources-a generic term to encompass vocabularies,\nterminologies, thesauri, ontologies. The resulting requirements were\nsynthesized into 39 \"hints\" for users and developers of semantic resources, and\nproviders of semantic resource services. We believe adopting these\nrecommendations will engage agrifood sciences in a necessary transition to\nleverage data production, sharing and reuse and the adoption of the FAIR data\nprinciples. The paper includes examples of adoption of those requirements, and\na discussion of their contribution to the field of data science.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 14:36:21 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Caracciolo", "Caterina", "", "FAO"], ["Aubin", "Sophie", "", "DipSO"], ["Jonquet", "Clement", "", "LIRMM, UM, CNRS"], ["Amdouni", "Emna", "", "LIRMM, UM, CNRS"], ["David", "Romain", "", "MISTEA,\n  ERINHA-AISBL"], ["Garcia", "Leyla", "", "ZB MED"], ["Whitehead", "Brandon", "", "UR TSCF, INRAE"], ["Roussey", "Catherine", "", "UR TSCF, INRAE"], ["Stellato", "Armando", "", "BC3"], ["Villa", "Ferdinando", "", "BC3"]]}, {"id": "2012.08594", "submitter": "Udayan Khurana", "authors": "Udayan Khurana and Sainyam Galhotra", "title": "Semantic Annotation for Tabular Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting semantic concept of columns in tabular data is of particular\ninterest to many applications ranging from data integration, cleaning, search\nto feature engineering and model building in machine learning. Recently,\nseveral works have proposed supervised learning-based or heuristic\npattern-based approaches to semantic type annotation. Both have shortcomings\nthat prevent them from generalizing over a large number of concepts or\nexamples. Many neural network based methods also present scalability issues.\nAdditionally, none of the known methods works well for numerical data. We\npropose $C^2$, a column to concept mapper that is based on a maximum likelihood\nestimation approach through ensembles. It is able to effectively utilize vast\namounts of, albeit somewhat noisy, openly available table corpora in addition\nto two popular knowledge graphs to perform effective and efficient concept\nprediction for structured data. We demonstrate the effectiveness of $C^2$ over\navailable techniques on 9 datasets, the most comprehensive comparison on this\ntopic so far.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 20:08:19 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Khurana", "Udayan", ""], ["Galhotra", "Sainyam", ""]]}, {"id": "2012.08830", "submitter": "Angelos-Christos Anadiotis", "authors": "Angelos-Christos Anadiotis, Oana Balalau, Catarina Conceicao, Helena\n  Galhardas, Mhd Yamen Haddad, Ioana Manolescu, Tayeb Merabti, Jingmao You", "title": "Graph integration of structured, semistructured and unstructured data\n  for data journalism", "comments": "40 pages, 9 figures. arXiv admin note: substantial text overlap with\n  arXiv:2007.12488, arXiv:2009.04283", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Digital data is a gold mine for modern journalism. However, datasets which\ninterest journalists are extremely heterogeneous, ranging from highly\nstructured (relational databases), semi-structured (JSON, XML, HTML), graphs\n(e.g., RDF), and text. Journalists (and other classes of users lacking advanced\nIT expertise, such as most non-governmental-organizations, or small public\nadministrations) need to be able to make sense of such heterogeneous corpora,\neven if they lack the ability to define and deploy custom\nextract-transform-load workflows, especially for dynamically varying sets of\ndata sources.\n  We describe a complete approach for integrating dynamic sets of heterogeneous\ndatasets along the lines described above: the challenges we faced to make such\ngraphs useful, allow their integration to scale, and the solutions we proposed\nfor these problems. Our approach is implemented within the ConnectionLens\nsystem; we validate it through a set of experiments.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 09:59:27 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Anadiotis", "Angelos-Christos", ""], ["Balalau", "Oana", ""], ["Conceicao", "Catarina", ""], ["Galhardas", "Helena", ""], ["Haddad", "Mhd Yamen", ""], ["Manolescu", "Ioana", ""], ["Merabti", "Tayeb", ""], ["You", "Jingmao", ""]]}, {"id": "2012.09329", "submitter": "Tiantu Xu", "authors": "Tiantu Xu, Kaiwen Shen, Yang Fu, Humphrey Shi, Felix Xiaozhu Lin", "title": "Clique: Spatiotemporal Object Re-identification at the City Scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object re-identification (ReID) is a key application of city-scale cameras.\nWhile classic ReID tasks are often considered as image retrieval, we treat them\nas spatiotemporal queries for locations and times in which the target object\nappeared. Spatiotemporal reID is challenged by the accuracy limitation in\ncomputer vision algorithms and the colossal videos from city cameras. We\npresent Clique, a practical ReID engine that builds upon two new techniques:\n(1) Clique assesses target occurrences by clustering fuzzy object features\nextracted by ReID algorithms, with each cluster representing the general\nimpression of a distinct object to be matched against the input; (2) to search\nin videos, Clique samples cameras to maximize the spatiotemporal coverage and\nincrementally adds cameras for processing on demand. Through evaluation on 25\nhours of videos from 25 cameras, Clique reached a high accuracy of 0.87 (recall\nat 5) across 70 queries and runs at 830x of video realtime in achieving high\naccuracy.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 00:05:50 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Xu", "Tiantu", ""], ["Shen", "Kaiwen", ""], ["Fu", "Yang", ""], ["Shi", "Humphrey", ""], ["Lin", "Felix Xiaozhu", ""]]}, {"id": "2012.10004", "submitter": "Jingyu Shao Mr.", "authors": "Jingyu Shao, Qing Wang, Asiri Wijesinghe, Erhard Rahm", "title": "ErGAN: Generative Adversarial Networks for Entity Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Entity resolution targets at identifying records that represent the same\nreal-world entity from one or more datasets. A major challenge in\nlearning-based entity resolution is how to reduce the label cost for training.\nDue to the quadratic nature of record pair comparison, labeling is a costly\ntask that often requires a significant effort from human experts. Inspired by\nrecent advances of generative adversarial network (GAN), we propose a novel\ndeep learning method, called ErGAN, to address the challenge. ErGAN consists of\ntwo key components: a label generator and a discriminator which are optimized\nalternatively through adversarial learning. To alleviate the issues of\noverfitting and highly imbalanced distribution, we design two novel modules for\ndiversity and propagation, which can greatly improve the model generalization\npower. We have conducted extensive experiments to empirically verify the\nlabeling and learning efficiency of ErGAN. The experimental results show that\nErGAN beats the state-of-the-art baselines, including unsupervised,\nsemi-supervised, and unsupervised learning methods.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 01:33:58 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Shao", "Jingyu", ""], ["Wang", "Qing", ""], ["Wijesinghe", "Asiri", ""], ["Rahm", "Erhard", ""]]}, {"id": "2012.10314", "submitter": "Rachit Agarwal", "authors": "Rachit Agarwal, Tarek Elsaleh, Elias Tragos", "title": "GDPR-inspired IoT Ontology enabling Semantic Interoperability,\n  Federation of Deployments and Privacy-Preserving Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Testing and experimentation are crucial for promoting innovation and building\nsystems that can evolve to meet high levels of service quality. IoT data that\nbelong to users and from which their personal information can be inferred are\nfrequently shared in the background of IoT systems with third parties for\nexperimentation and building quality services. This data sharing raises privacy\nconcerns especially since in most cases the data are gathered and shared\nwithout the user's knowledge or explicit consent or for different purposes than\nthe one for which the data were initially gathered. With the introduction of\nGDPR, IoT systems and experimentation platforms that federate data from\ndifferent deployments, testbeds and data providers must be privacy-preserving.\nThe wide adoption of IoT applications in scenarios ranging from smart cities to\nIndustry 4.0 has raised concerns with respect to the privacy of users' data\ncollected using IoT devices. Many experimental smart city applications are also\nusing crowdsourcing data. Inspired by the GDPR requirements, we propose an IoT\nontology built using available standards that enhances privacy, enables\nsemantic interoperability between IoT deployments and supports the development\nof privacy-preserving experimental IoT applications. On top, we propose\nrecommendations on how to efficiently use the ontology within IoT testbed and\nfederating platforms. Our ontology is validated for different quality\nassessment criteria using standard validation tools. We focus on\n\"experimentation\" without loss of generality, because it covers scenarios from\nboth research and industry, that are directly linked with innovation.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 15:58:46 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Agarwal", "Rachit", ""], ["Elsaleh", "Tarek", ""], ["Tragos", "Elias", ""]]}, {"id": "2012.11128", "submitter": "Zhengmin Lai", "authors": "Zhengmin Lai, You Peng, Shiyu Yang, Xuemin Lin, Wenjie Zhang", "title": "PEFP: Efficient k-hop Constrained s-t Simple Path Enumeration on FPGA", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph plays a vital role in representing entities and their relationships in\na variety of fields, such as e-commerce networks, social networks and\nbiological networks. Given two vertices s and t, one of the fundamental\nproblems in graph databases is to investigate the relationships between s and\nt. A well-studied problem in such area is k-hop constrained s-t simple path\nenumeration. Nevertheless, all existing algorithms targeting this problem\nfollow the DFS-based paradigm, which cannot scale up well. Moreover, using\nhardware devices like FPGA to accelerate graph computation has become popular.\nMotivated by this, in this paper, we propose the first FPGA-based algorithm\nPEFP to solve the problem of k-hop constrained s-t simple path enumeration\nefficiently. On the host side, we propose a preprocessing algorithm Pre-BFS to\nreduce the graph size and search space. On the FPGA side in PEFP, we propose a\nnovel DFS-based batching technique to save on-chip memory efficiently. In\naddition, we also propose caching techniques to cache necessary data in BRAM,\nwhich overcome the latency bottleneck brought by the read/write operations\nfrom/to FPGA DRAM. Finally, we propose a data separation technique to enable\ndataflow optimization for the path verification module; hence the sub-stages in\nthat module can be executed in parallel. Comprehensive experiments show that\nPEFP outperforms the state-of-the-art algorithm JOIN by more than 1 order of\nmagnitude by average, and up to 2 orders of magnitude in terms of preprocessing\ntime, query processing time and total time, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 05:37:18 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 11:05:58 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Lai", "Zhengmin", ""], ["Peng", "You", ""], ["Yang", "Shiyu", ""], ["Lin", "Xuemin", ""], ["Zhang", "Wenjie", ""]]}, {"id": "2012.11188", "submitter": "Tom Tseng", "authors": "Tom Tseng, Laxman Dhulipala, Julian Shun", "title": "Parallel Index-Based Structural Graph Clustering and Its Approximation", "comments": null, "journal-ref": null, "doi": "10.1145/3448016.3457278", "report-no": null, "categories": "cs.DB cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SCAN (Structural Clustering Algorithm for Networks) is a well-studied, widely\nused graph clustering algorithm. For large graphs, however, sequential SCAN\nvariants are prohibitively slow, and parallel SCAN variants do not effectively\nshare work among queries with different SCAN parameter settings. Since users of\nSCAN often explore many parameter settings to find good clusterings, it is\nworthwhile to precompute an index that speeds up queries.\n  This paper presents a practical and provably efficient parallel index-based\nSCAN algorithm based on GS*-Index, a recent sequential algorithm. Our parallel\nalgorithm improves upon the asymptotic work of the sequential algorithm by\nusing integer sorting. It is also highly parallel, achieving logarithmic span\n(parallel time) for both index construction and clustering queries.\nFurthermore, we apply locality-sensitive hashing (LSH) to design a novel\napproximate SCAN algorithm and prove guarantees for its clustering behavior.\n  We present an experimental evaluation of our algorithms on large real-world\ngraphs. On a 48-core machine with two-way hyper-threading, our parallel index\nconstruction achieves 50--151$\\times$ speedup over the construction of\nGS*-Index. In fact, even on a single thread, our index construction algorithm\nis faster than GS*-Index. Our parallel index query implementation achieves\n5--32$\\times$ speedup over GS*-Index queries across a range of SCAN parameter\nvalues, and our implementation is always faster than ppSCAN, a state-of-the-art\nparallel SCAN algorithm. Moreover, our experiments show that applying LSH\nresults in faster index construction while maintaining good clustering quality.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 09:07:44 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 21:51:50 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Tseng", "Tom", ""], ["Dhulipala", "Laxman", ""], ["Shun", "Julian", ""]]}, {"id": "2012.11269", "submitter": "Piotr Ostropolski-Nalewaja", "authors": "Piotr Ostropolski-Nalewaja, Jerzy Marcinkowski, David Carral and\n  Sebastian Rudolph", "title": "A Journey to the Frontiers of Query Rewritability", "comments": "Removed faulty observation, fixed everything that depended on it", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is about (first order) query rewritability in the context of\ntheory-mediated query answering. The starting point of our journey is the\nFUS/FES conjecture, saying that if a theory is core-terminating (FES) and\nadmits query rewriting (BDD, FUS) then it is uniformly bounded. We show that\nthis conjecture is true for a wide class of \"local\" BDD theories. Then we ask\nhow non-local can a BDD theory actually be and we discover phenomena which we\nthink are quite counter-intuitive.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 11:54:35 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 16:09:38 GMT"}, {"version": "v3", "created": "Mon, 3 May 2021 18:52:50 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Ostropolski-Nalewaja", "Piotr", ""], ["Marcinkowski", "Jerzy", ""], ["Carral", "David", ""], ["Rudolph", "Sebastian", ""]]}, {"id": "2012.11965", "submitter": "Nikolaos Tziavelis", "authors": "Nofar Carmeli, Nikolaos Tziavelis, Wolfgang Gatterbauer, Benny\n  Kimelfeld, Mirek Riedewald", "title": "Tractable Orders for Direct Access to Ranked Answers of Conjunctive\n  Queries", "comments": "17 pages", "journal-ref": null, "doi": "10.1145/3452021.3458331", "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the question of when we can provide logarithmic-time direct access\nto the k-th answer to a Conjunctive Query (CQ) with a specified ordering over\nthe answers, following a preprocessing step that constructs a data structure in\ntime quasilinear in the size of the database. Specifically, we embark on the\nchallenge of identifying the tractable answer orderings that allow for ranked\ndirect access with such complexity guarantees. We begin with lexicographic\norderings and give a decidable characterization (under conventional complexity\nassumptions) of the class of tractable lexicographic orderings for every CQ\nwithout self-joins. We then continue to the more general orderings by the sum\nof attribute weights and show for it that ranked direct access is tractable\nonly in trivial cases. Hence, to better understand the computational challenge\nat hand, we consider the more modest task of providing access to only a single\nanswer (i.e., finding the answer at a given position) - a task that we refer to\nas the selection problem. We indeed achieve a quasilinear-time algorithm for a\nsubset of the class of full CQs without self-joins, by adopting a solution of\nFrederickson and Johnson to the classic problem of selection over sorted\nmatrices. We further prove that none of the other queries in this class admit\nsuch an algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 12:33:51 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2020 09:33:22 GMT"}, {"version": "v3", "created": "Mon, 12 Apr 2021 15:43:23 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Carmeli", "Nofar", ""], ["Tziavelis", "Nikolaos", ""], ["Gatterbauer", "Wolfgang", ""], ["Kimelfeld", "Benny", ""], ["Riedewald", "Mirek", ""]]}, {"id": "2012.12013", "submitter": "Innar Liiv", "authors": "Innar Liiv", "title": "SARS-CoV-2 Coronavirus Data Compression Benchmark", "comments": "6 pages, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces a lossless data compression competition that benchmarks\nsolutions (computer programs) by the compressed size of the 44,981 concatenated\nSARS-CoV-2 sequences, with a total uncompressed size of 1,339,868,341 bytes.\nThe data, downloaded on 13 December 2020, from the severe acute respiratory\nsyndrome coronavirus 2 data hub of ncbi.nlm.nih.gov is presented in FASTA and\n2Bit format. The aim of this competition is to encourage multidisciplinary\nresearch to find the shortest lossless description for the sequences and to\ndemonstrate that data compression can serve as an objective and repeatable\nmeasure to align scientific breakthroughs across disciplines. The shortest\ndescription of the data is the best model; therefore, further reducing the size\nof this description requires a fundamental understanding of the underlying\ncontext and data. This paper presents preliminary results with multiple\nwell-known compression algorithms for baseline measurements, and insights\nregarding promising research avenues. The competition's progress will be\nreported at \\url{https://coronavirus.innar.com}, and the benchmark is open for\nall to participate and contribute.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 16:41:59 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Liiv", "Innar", ""]]}, {"id": "2012.12028", "submitter": "Mark van der Loo", "authors": "Mark P.J. van der Loo and Edwin de Jonge", "title": "Data Validation", "comments": "7 pages, 1 figure. In Wiley StatsRef: Statistics Reference Online\n  (2020)", "journal-ref": null, "doi": "10.1002/9781118445112", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Data validation is the activity where one decides whether or not a particular\ndata set is fit for a given purpose. Formalizing the requirements that drive\nthis decision process allows for unambiguous communication of the requirements,\nautomation of the decision process, and opens up ways to maintain and\ninvestigate the decision process itself. The purpose of this article is to\nformalize the definition of data validation and to demonstrate some of the\nproperties that can be derived from this definition. In particular, it is shown\nhow a formal view of the concept permits a classification of data quality\nrequirements, allowing them to be ordered in increasing levels of complexity.\nSome subtleties arising from combining possibly many such requirements are\npointed out as well.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 12:58:13 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["van der Loo", "Mark P. J.", ""], ["de Jonge", "Edwin", ""]]}, {"id": "2012.12031", "submitter": "Majid Rafiei", "authors": "Majid Rafiei and Wil M.P. van der Aalst", "title": "Towards Quantifying Privacy in Process Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Process mining employs event logs to provide insights into the actual\nprocesses. Event logs are recorded by information systems and contain valuable\ninformation helping organizations to improve their processes. However, these\ndata also include highly sensitive private information which is a major concern\nwhen applying process mining. Therefore, privacy preservation in process mining\nis growing in importance, and new techniques are being introduced. The\neffectiveness of the proposed privacy preservation techniques needs to be\nevaluated. It is important to measure both sensitive data protection and data\nutility preservation. In this paper, we propose an approach to quantify the\neffectiveness of privacy preservation techniques. We introduce two measures for\nquantifying disclosure risks to evaluate the sensitive data protection aspect.\nMoreover, a measure is proposed to quantify data utility preservation for the\nmain process mining activities. The proposed measures have been tested using\nvarious real-life event logs.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 10:04:54 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Rafiei", "Majid", ""], ["van der Aalst", "Wil M. P.", ""]]}, {"id": "2012.12126", "submitter": "Albert Atserias", "authors": "Albert Atserias and Phokion G. Kolaitis", "title": "Structure and Complexity of Bag Consistency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Since the early days of relational databases, it was realized that acyclic\nhypergraphs give rise to database schemas with desirable structural and\nalgorithmic properties. In a by-now classical paper, Beeri, Fagin, Maier, and\nYannakakis established several different equivalent characterizations of\nacyclicity; in particular, they showed that the sets of attributes of a schema\nform an acyclic hypergraph if and only if the local-to-global consistency\nproperty for relations over that schema holds, which means that every\ncollection of pairwise consistent relations over the schema is globally\nconsistent. Even though real-life databases consist of bags (multisets), there\nhas not been a study of the interplay between local consistency and global\nconsistency for bags. We embark on such a study here and we first show that the\nsets of attributes of a schema form an acyclic hypergraph if and only if the\nlocal-to global consistency property for bags over that schema holds. After\nthis, we explore algorithmic aspects of global consistency for bags by\nanalyzing the computational complexity of the global consistency problem for\nbags: given a collection of bags, are these bags globally consistent? We show\nthat this problem is in NP, even when the schema is part of the input. We then\nestablish the following dichotomy theorem for fixed schemas: if the schema is\nacyclic, then the global consistency problem for bags is solvable in polynomial\ntime, while if the schema is cyclic, then the global consistency problem for\nbags is NP-complete. The latter result contrasts sharply with the state of\naffairs for relations, where, for each fixed schema, the global consistency\nproblem for relations is solvable in polynomial time.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 16:04:24 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Atserias", "Albert", ""], ["Kolaitis", "Phokion G.", ""]]}, {"id": "2012.12362", "submitter": "Rolysent Paredes", "authors": "Rolysent K Paredes and Alexander A. Hernandez", "title": "Designing an Adaptive Bandwidth Management for Higher Education\n  Institutions", "comments": null, "journal-ref": null, "doi": "10.25147/ijcsr.2017.001.1.22", "report-no": null, "categories": "cs.NI cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Purpose: This study proposes an adaptive bandwidth management system which\ncan be explicitly used by educational institutions. The primary goal of the\nsystem is to increase the bandwidth of the users who access more on educational\nwebsites. Through this proposed bandwidth management, the users of the campus\nnetworks is encouraged to utilize the internet for educational purposes.\n  Method: The weblog from a university's pfSense proxy server was utilized and\nundergo Web Usage Mining (WUM) to determine the number of educational and\nnon-educational websites accessed by the users. Certain formulas were used in\nthe computation of the bandwidth which was dynamically assigned to the users. A\nprototyping technique was applied in developing adaptive bandwidth management\nsystem. The prototype was simulated and evaluated by experts in compliance with\nISO/IEC 14598-6 and ISO/IEC 9126-1 standards.\n  Results: This study found that the prototype is capable of adjusting the\nbandwidth of the network users dynamically. The users who browsed more on\neducational websites or contents were assigned with higher bandwidth compared\nto those who are not. Further, the evaluated prototype met the software\nstandards of ISO.\n  Conclusion: The proposed adaptive bandwidth management can contribute to the\ncontinuous development in the area of computer networking, especially in\ndesigning and managing campus networks. It also helps the network\nadministrators or IT managers in allocating bandwidth with minimal effort.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 11:59:29 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Paredes", "Rolysent K", ""], ["Hernandez", "Alexander A.", ""]]}, {"id": "2012.12501", "submitter": "Deniz Alt{\\i}nb\\\"uken", "authors": "Hussam Abu-Libdeh, Deniz Alt{\\i}nb\\\"uken, Alex Beutel, Ed H. Chi,\n  Lyric Doshi, Tim Kraska, Xiaozhou (Steve) Li, Andy Ly, Christopher Olston", "title": "Learned Indexes for a Google-scale Disk-based Database", "comments": "4 pages, Presented at Workshop on ML for Systems at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  There is great excitement about learned index structures, but understandable\nskepticism about the practicality of a new method uprooting decades of research\non B-Trees. In this paper, we work to remove some of that uncertainty by\ndemonstrating how a learned index can be integrated in a distributed,\ndisk-based database system: Google's Bigtable. We detail several design\ndecisions we made to integrate learned indexes in Bigtable. Our results show\nthat integrating learned index significantly improves the end-to-end read\nlatency and throughput for Bigtable.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 05:56:45 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Abu-Libdeh", "Hussam", "", "Steve"], ["Alt\u0131nb\u00fcken", "Deniz", "", "Steve"], ["Beutel", "Alex", "", "Steve"], ["Chi", "Ed H.", "", "Steve"], ["Doshi", "Lyric", "", "Steve"], ["Kraska", "Tim", "", "Steve"], ["Xiaozhou", "", "", "Steve"], ["Li", "", ""], ["Ly", "Andy", ""], ["Olston", "Christopher", ""]]}, {"id": "2012.12627", "submitter": "Xi Victoria Lin", "authors": "Xi Victoria Lin and Richard Socher and Caiming Xiong", "title": "Bridging Textual and Tabular Data for Cross-Domain Text-to-SQL Semantic\n  Parsing", "comments": "EMNLP Findings 2020 long paper extended; 23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DB cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present BRIDGE, a powerful sequential architecture for modeling\ndependencies between natural language questions and relational databases in\ncross-DB semantic parsing. BRIDGE represents the question and DB schema in a\ntagged sequence where a subset of the fields are augmented with cell values\nmentioned in the question. The hybrid sequence is encoded by BERT with minimal\nsubsequent layers and the text-DB contextualization is realized via the\nfine-tuned deep attention in BERT. Combined with a pointer-generator decoder\nwith schema-consistency driven search space pruning, BRIDGE attained\nstate-of-the-art performance on popular cross-DB text-to-SQL benchmarks, Spider\n(71.1\\% dev, 67.5\\% test with ensemble model) and WikiSQL (92.6\\% dev, 91.9\\%\ntest). Our analysis shows that BRIDGE effectively captures the desired\ncross-modal dependencies and has the potential to generalize to more text-DB\nrelated tasks. Our implementation is available at\n\\url{https://github.com/salesforce/TabularSemanticParsing}.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 12:33:52 GMT"}, {"version": "v2", "created": "Thu, 31 Dec 2020 01:02:40 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Lin", "Xi Victoria", ""], ["Socher", "Richard", ""], ["Xiong", "Caiming", ""]]}, {"id": "2012.13198", "submitter": "Liat Peterfreund", "authors": "Leonid Libkin, Liat Peterfreund", "title": "Handling SQL Nulls with Two-Valued Logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The design of SQL is based on a three-valued logic (3VL), rather than the\nfamiliar Boolean logic with truth values true and false, to accommodate the\nadditional truth value unknown for handling nulls. It is viewed as\nindispensable for SQL expressiveness but is at the same time much criticized\nfor leading to unintuitive behavior of queries and thus being a source of\nprogrammer mistakes.\n  We show that, contrary to the widely held view, SQL could have been designed\nbased on the standard Boolean logic, without any loss of expressiveness and\nwithout giving up nulls. The approach itself follows SQL's evaluation which\nonly retains tuples for which conditions in the WHERE clause evaluate to true.\nWe show that conflating unknown, resulting from nulls, with false leads to an\nequally expressive version of SQL that does not use the third truth value.\nQueries written under the two-valued semantics can be efficiently translated\ninto the standard SQL and thus executed on any existing RDBMS. These results\ncover the core of the SQL 1999 Standard, including SELECT-FROM-WHERE-GROUP\nBY-HAVING queries extended with subqueries and IN/EXISTS/ANY/ALL conditions,\nand recursive queries. We provide two extensions of this result showing that no\nother way of converting 3VL into Boolean logic, nor any other many-valued logic\nfor treating nulls could have possibly led to a more expressive language.\n  These results not only present small modifications of SQL that eliminate the\nsource of many programmer errors without the need to reimplement database\ninternals, but they also strongly suggest that new query languages for various\ndata models do not have to follow the much criticized SQL's three-valued\napproach.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 11:21:55 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Libkin", "Leonid", ""], ["Peterfreund", "Liat", ""]]}, {"id": "2012.13677", "submitter": "Song-Kyoo Amang Kim Ph.D.", "authors": "Song-Kyoo (Amang) Kim", "title": "Toward Compact Data from Big Data", "comments": "This paper has been accepted in the 2020 IEEE-ICITIS Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Bigdata is a dataset of which size is beyond the ability of handling a\nvaluable raw material that can be refined and distilled into valuable specific\ninsights. Compact data is a method that optimizes the big dataset that gives\nbest assets without handling complex bigdata. The compact dataset contains the\nmaximum knowledge patterns at fine grained level for effective and personalized\nutilization of bigdata systems without bigdata. The compact data method is a\ntailor-made design which depends on problem situations. Various compact data\ntechniques have been demonstrated into various data-driven research area in the\npaper.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 04:45:40 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Song-Kyoo", "", "", "Amang"], ["Kim", "", ""]]}, {"id": "2012.13685", "submitter": "Xiaoying Wu", "authors": "Xiaoying Wu, Dimitri Theodoratos, Nikos Mamoulis", "title": "Discovering Closed and Maximal Embedded Patterns from Large Tree Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We address the problem of summarizing embedded tree patterns extracted from\nlarge data trees. We do so by defining and mining closed and maximal embedded\nunordered tree patterns from a single large data tree. We design an embedded\nfrequent pattern mining algorithm extended with a local closedness checking\ntechnique. This algorithm is called {\\em closedEmbTM-prune} as it eagerly\neliminates non-closed patterns. To mitigate the generation of intermediate\npatterns, we devise pattern search space pruning rules to proactively detect\nand prune branches in the pattern search space which do not correspond to\nclosed patterns. The pruning rules are accommodated into the extended embedded\npattern miner to produce a new algorithm, called {\\em closedEmbTM-prune}, for\nmining all the closed and maximal embedded frequent patterns from large data\ntrees. Our extensive experiments on synthetic and real large-tree datasets\ndemonstrate that, on dense datasets, {\\em closedEmbTM-prune} not only generates\na complete closed and maximal pattern set which is substantially smaller than\nthat generated by the embedded pattern miner, but also runs much faster with\nnegligible overhead on pattern pruning.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 07:08:42 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Wu", "Xiaoying", ""], ["Theodoratos", "Dimitri", ""], ["Mamoulis", "Nikos", ""]]}, {"id": "2012.14234", "submitter": "Bowen Hao", "authors": "Bowen Hao, Jing Zhang, Cuiping Li, Hong Chen, Hongzhi Yin", "title": "Recommending Courses in MOOCs for Jobs: An Auto Weak Supervision\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of massive open online courses (MOOCs) demands an effective\nway of course recommendation for jobs posted in recruitment websites,\nespecially for the people who take MOOCs to find new jobs. Despite the advances\nof supervised ranking models, the lack of enough supervised signals prevents us\nfrom directly learning a supervised ranking model. This paper proposes a\ngeneral automated weak supervision framework AutoWeakS via reinforcement\nlearning to solve the problem. On the one hand, the framework enables training\nmultiple supervised ranking models upon the pseudo labels produced by multiple\nunsupervised ranking models. On the other hand, the framework enables\nautomatically searching the optimal combination of these supervised and\nunsupervised models. Systematically, we evaluate the proposed model on several\ndatasets of jobs from different recruitment websites and courses from a MOOCs\nplatform. Experiments show that our model significantly outperforms the\nclassical unsupervised, supervised and weak supervision baselines.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 14:03:18 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Hao", "Bowen", ""], ["Zhang", "Jing", ""], ["Li", "Cuiping", ""], ["Chen", "Hong", ""], ["Yin", "Hongzhi", ""]]}, {"id": "2012.14420", "submitter": "Junya Arai", "authors": "Junya Arai, Makoto Onizuka, Yasuhiro Fujiwara, and Sotetsu Iwamura", "title": "Fast Subgraph Matching by Exploiting Search Failures", "comments": "This paper is a translation of the following Japanese paper published\n  in March 2018: Junya Arai, Makoto Onizuka, Yasuhiro Fujiwara, and Sotetsu\n  Iwamura. Fast Subgraph Matching Using a History of Search Failures.\n  Proceedings of the 10th Forum on Data Engineering and Information Management\n  (DEIM), pp. 1--9, mar 2018, available at\n  https://db-event.jpn.org/deim2018/data/papers/1.pdf (In Japanese)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subgraph matching is a compute-intensive problem that asks to enumerate all\nthe isomorphic embeddings of a query graph within a data graph. This problem is\ngenerally solved with backtracking, which recursively evolves every possible\npartial embedding until it becomes an isomorphic embedding or is found unable\nto become it. While existing methods reduce the search space by analyzing graph\nstructures before starting the backtracking, it is often ineffective for\ncomplex graphs. In this paper, we propose an efficient algorithm for subgraph\nmatching that performs on-the-fly pruning during the backtracking. Our main\nidea is to `learn from failure'. That is, our algorithm generates failure\npatterns when a partial embedding is found unable to become an isomorphic\nembedding. Then, in the subsequent process of the backtracking, our algorithm\nprunes partial embeddings matched with a failure pattern. This pruning does not\nchange the result because failure patterns are designed to represent the\nconditions that never yield an isomorphic embedding. Additionally, we introduce\nan efficient representation of failure patterns for constant-time pattern\nmatching. The experimental results show that our method improves the\nperformance by up to 10000 times than existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 18:56:52 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Arai", "Junya", ""], ["Onizuka", "Makoto", ""], ["Fujiwara", "Yasuhiro", ""], ["Iwamura", "Sotetsu", ""]]}, {"id": "2012.14555", "submitter": "Xiaoou Ding", "authors": "Xiaoou Ding, Hongzhi Wang, Jiaxuan Su, Chen Wang", "title": "Misplaced Subsequences Repairing with Application to Multivariate\n  Industrial Time Series Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both the volume and the collection velocity of time series generated by\nmonitoring sensors are increasing in the Internet of Things (IoT). Data\nmanagement and analysis requires high quality and applicability of the IoT\ndata. However, errors are prevalent in original time series data. Inconsistency\nin time series is a serious data quality problem existing widely in IoT. Such\nproblem could be hardly solved by existing techniques. Motivated by this, we\ndefine an inconsistent subsequences problem in multivariate time series, and\npropose an integrity data repair approach to solve inconsistent problems. Our\nproposed repairing method consists of two parts: (1) we design effective\nanomaly detection method to discover latent inconsistent subsequences in the\nIoT time series; and (2) we develop repair algorithms to precisely locate the\nstart and finish time of inconsistent intervals, and provide reliable repairing\nstrategies. A thorough experiment on two real-life datasets verifies the\nsuperiority of our method compared to other practical approaches. Experimental\nresults also show that our method captures and repairs inconsistency problems\neffectively in industrial time series in complex IIoT scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 01:26:26 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2021 00:09:42 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Ding", "Xiaoou", ""], ["Wang", "Hongzhi", ""], ["Su", "Jiaxuan", ""], ["Wang", "Chen", ""]]}, {"id": "2012.14743", "submitter": "Amir Shaikhha", "authors": "Ziniu Wu, Amir Shaikhha, Rong Zhu, Kai Zeng, Yuxing Han, Jingren Zhou", "title": "BayesCard: Revitilizing Bayesian Frameworks for Cardinality Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardinality estimation (CardEst) is an essential component in query\noptimizers and a fundamental problem in DBMS. A desired CardEst method should\nattain good algorithm performance, be stable to varied data settings, and be\nfriendly to system deployment. However, no existing CardEst method can fulfill\nthe three criteria at the same time. Traditional methods often have significant\nalgorithm drawbacks such as large estimation errors. Recently proposed deep\nlearning based methods largely improve the estimation accuracy but their\nperformance can be greatly affected by data and often difficult for system\ndeployment.\n  In this paper, we revitalize the Bayesian networks (BN) for CardEst by\nincorporating the techniques of probabilistic programming languages. We present\nBayesCard, the first framework that inherits the advantages of BNs, i.e., high\nestimation accuracy and interpretability, while overcomes their drawbacks, i.e.\nlow structure learning and inference efficiency. This makes BayesCard a perfect\ncandidate for commercial DBMS deployment. Our experimental results on several\nsingle-table and multi-table benchmarks indicate BayesCard's superiority over\nexisting state-of-the-art CardEst methods: BayesCard achieves comparable or\nbetter accuracy, 1-2 orders of magnitude faster inference time, 1-3 orders\nfaster training time, 1-3 orders smaller model size, and 1-2 orders faster\nupdates. Meanwhile, BayesCard keeps stable performance when varying data with\ndifferent settings. We also deploy BayesCard into PostgreSQL. On the IMDB\nbenchmark workload, it improves the end-to-end query time by 13.3%, which is\nvery close to the optimal result of 14.2% using an oracle of true cardinality.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 13:21:18 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 10:54:33 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Wu", "Ziniu", ""], ["Shaikhha", "Amir", ""], ["Zhu", "Rong", ""], ["Zeng", "Kai", ""], ["Han", "Yuxing", ""], ["Zhou", "Jingren", ""]]}, {"id": "2012.14800", "submitter": "Anna Fariha", "authors": "Anna Fariha, Lucy Cousins, Narges Mahyar, Alexandra Meliou", "title": "Example-Driven User Intent Discovery: Empowering Users to Cross the SQL\n  Barrier Through Query by Example", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional data systems require specialized technical skills where users\nneed to understand the data organization and write precise queries to access\ndata. Therefore, novice users who lack technical expertise face hurdles in\nperusing and analyzing data. Existing tools assist in formulating queries\nthrough keyword search, query recommendation, and query auto-completion, but\nstill require some technical expertise. An alternative method for accessing\ndata is Query by Example (QBE), where users express their data exploration\nintent simply by providing examples of their intended data. We study a\nstate-of-the-art QBE system called SQuID, and contrast it with traditional SQL\nquerying. Our comparative user studies demonstrate that users with varying\nexpertise are significantly more effective and efficient with SQuID than SQL.\nWe find that SQuID eliminates the barriers in studying the database schema,\nformalizing task semantics, and writing syntactically correct SQL queries, and\nthus, substantially alleviates the need for technical expertise in data\nexploration.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 15:22:59 GMT"}, {"version": "v2", "created": "Sat, 2 Jan 2021 14:15:15 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Fariha", "Anna", ""], ["Cousins", "Lucy", ""], ["Mahyar", "Narges", ""], ["Meliou", "Alexandra", ""]]}, {"id": "2012.15267", "submitter": "Patrick Brosi", "authors": "Hannah Bast, Patrick Brosi and Markus N\\\"ather", "title": "Similarity Classification of Public Transit Stations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the following problem: given two public transit station identifiers\nA and B, each with a label and a geographic coordinate, decide whether A and B\ndescribe the same station. For example, for \"St Pancras International\" at\n(51.5306, -0.1253) and \"London St Pancras\" at (51.5319, -0.1269), the answer\nwould be \"Yes\". This problem frequently arises in areas where public transit\ndata is used, for example in geographic information systems, schedule merging,\nroute planning, or map matching. We consider several baseline methods based on\ngeographic distance and simple string similarity measures. We also experiment\nwith more elaborate string similarity measures and manually created\nnormalization rules. Our experiments show that these baseline methods produce\ngood, but not fully satisfactory results. We therefore develop an approach\nbased on a random forest classifier which is trained on matching trigrams\nbetween two stations, their distance, and their position on an interwoven grid.\nAll approaches are evaluated on extensive ground truth datasets we generated\nfrom OpenStreetMap (OSM) data: (1) The union of Great Britain and Ireland and\n(2) the union of Germany, Switzerland, and Austria. On all datasets, our\nlearning-based approach achieves an F1 score of over 99%, while even the most\nelaborate baseline approach (based on TFIDF scores and the geographic distance)\nachieves an F1 score of at most 94%, and a naive approach of using a\ngeographical distance threshold achieves an F1 score of only 75%. Both our\ntraining and testing datasets are publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 18:27:11 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Bast", "Hannah", ""], ["Brosi", "Patrick", ""], ["N\u00e4ther", "Markus", ""]]}, {"id": "2012.15381", "submitter": "Sergio Cabello", "authors": "Sergio Cabello", "title": "Faster Distance-Based Representative Skyline and $k$-Center Along Pareto\n  Front in the Plane", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DB cs.DS math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of computing the \\emph{distance-based representative\nskyline} in the plane, a problem introduced by Tao, Ding, Lin and Pei [Proc.\n25th IEEE International Conference on Data Engineering (ICDE), 2009] and\nindependently considered by Dupin, Nielsen and Talbi [Optimization and Learning\n- Third International Conference, OLA 2020] in the context of multi-objective\noptimization. Given a set $P$ of $n$ points in the plane and a parameter $k$,\nthe task is to select $k$ points of the skyline defined by $P$ (also known as\nPareto front for $P$) to minimize the maximum distance from the points of the\nskyline to the selected points. We show that the problem can be solved in\n$O(n\\log h)$ time, where $h$ is the number of points in the skyline of $P$. We\nalso show that the decision problem can be solved in $O(n\\log k)$ time and the\noptimization problem can be solved in $O(n \\log k + n \\operatorname{loglog} n)$\ntime. This improves previous algorithms and is optimal for a large range of\nvalues of $k$.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 00:34:57 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Cabello", "Sergio", ""]]}, {"id": "2012.15570", "submitter": "Alexandr Savinov", "authors": "Alexandr Savinov", "title": "On the importance of functions in data modeling", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we argue that representing entity properties by tuple\nattributes, as evangelized in most set-oriented data models, is a controversial\nmethod conflicting with the principle of tuple immutability. As a principled\nsolution to this problem of tuple immutability on one hand and the need to\nmodify tuple attributes on the other hand, we propose to use mathematical\nfunctions for representing entity properties. In this approach, immutable\ntuples are intended for representing the existence of entities while mutable\nfunctions (mappings between sets) are used for representing entity properties.\nIn this model, called the concept-oriented model (COM), functions are made\nfirst-class elements along with sets, and both functions and sets are used to\nrepresent and process data in a simpler and more natural way in comparison to\npurely set-oriented models.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 12:09:54 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Savinov", "Alexandr", ""]]}, {"id": "2012.15596", "submitter": "Ilia Petrov", "authors": "Christian Riegger, Arthur Bernhardt, Bernhard Moessner, Ilia Petrov", "title": "bloomRF: On Performing Range-Queries with Bloom-Filters based on\n  Piecewise-Monotone Hash Functions and Dyadic Trace-Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce bloomRF as a unified method for approximate membership testing\nthat supports both point- and range-queries on a single data structure. bloomRF\nextends Bloom-Filters with range query support and may replace them. The core\nidea is to employ a dyadic interval scheme to determine the set of dyadic\nintervals covering a data point, which are then encoded and inserted. bloomRF\nintroduces Dyadic Trace-Trees as novel data structure that represents those\ncovering intervals implicitly. A Trace-Tree encoding scheme represents the set\nof covering intervals efficiently, in a compact bit representation.\nFurthermore, bloomRF introduces novel piecewise-monotone hash functions that\nare locally order-preserving and thus support range querying. We present an\nefficient membership computation method for range-queries. Although, bloomRF is\ndesigned for integers it also supports string and floating-point data types. It\ncan also handle multiple attributes and serve as multi-attribute filter.\n  We evaluate bloomRF in RocksDB and in a standalone library. bloomRF is more\nefficient and outperforms existing point-range-filters by up to 4x across a\nrange of settings.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 13:17:37 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Riegger", "Christian", ""], ["Bernhardt", "Arthur", ""], ["Moessner", "Bernhard", ""], ["Petrov", "Ilia", ""]]}, {"id": "2012.15713", "submitter": "Chang Ge", "authors": "Chang Ge, Shubhankar Mohapatra, Xi He, Ihab F. Ilyas", "title": "Kamino: Constraint-Aware Differentially Private Data Synthesis", "comments": "Update based on reviewers' comments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Organizations are increasingly relying on data to support decisions. When\ndata contains private and sensitive information, the data owner often desires\nto publish a synthetic database instance that is similarly useful as the true\ndata, while ensuring the privacy of individual data records. Existing\ndifferentially private data synthesis methods aim to generate useful data based\non applications, but they fail in keeping one of the most fundamental data\nproperties of the structured data -- the underlying correlations and\ndependencies among tuples and attributes (i.e., the structure of the data).\nThis structure is often expressed as integrity and schema constraints, or with\na probabilistic generative process. As a result, the synthesized data is not\nuseful for any downstream tasks that require this structure to be preserved.\n  This work presents Kamino, a data synthesis system to ensure differential\nprivacy and to preserve the structure and correlations present in the original\ndataset. Kamino takes as input of a database instance, along with its schema\n(including integrity constraints), and produces a synthetic database instance\nwith differential privacy and structure preservation guarantees. We empirically\nshow that while preserving the structure of the data, Kamino achieves\ncomparable and even better usefulness in applications of training\nclassification models and answering marginal queries than the state-of-the-art\nmethods of differentially private data synthesis.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 17:08:19 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 15:48:21 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Ge", "Chang", ""], ["Mohapatra", "Shubhankar", ""], ["He", "Xi", ""], ["Ilyas", "Ihab F.", ""]]}]