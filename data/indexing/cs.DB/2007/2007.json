[{"id": "2007.00114", "submitter": "Oc\\'eane Boulais", "authors": "Oc\\'eane Boulais, Ben Woodward, Brian Schlining, Lonny Lundsten, Kevin\n  Barnard, Katy Croff Bell, and Kakani Katija", "title": "FathomNet: An underwater image training database for ocean exploration\n  and discovery", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Thousands of hours of marine video data are collected annually from remotely\noperated vehicles (ROVs) and other underwater assets. However, current manual\nmethods of analysis impede the full utilization of collected data for real time\nalgorithms for ROV and large biodiversity analyses. FathomNet is a novel\nbaseline image training set, optimized to accelerate development of modern,\nintelligent, and automated analysis of underwater imagery. Our seed data set\nconsists of an expertly annotated and continuously maintained database with\nmore than 26,000 hours of videotape, 6.8 million annotations, and 4,349 terms\nin the knowledge base. FathomNet leverages this data set by providing imagery,\nlocalizations, and class labels of underwater concepts in order to enable\nmachine learning algorithm development. To date, there are more than 80,000\nimages and 106,000 localizations for 233 different classes, including midwater\nand benthic organisms. Our experiments consisted of training various deep\nlearning algorithms with approaches to address weakly supervised localization,\nimage labeling, object detection and classification which prove to be\npromising. While we find quality results on prediction for this new dataset,\nour results indicate that we are ultimately in need of a larger data set for\nocean exploration.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 21:23:06 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 14:09:24 GMT"}, {"version": "v3", "created": "Fri, 10 Jul 2020 04:14:21 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Boulais", "Oc\u00e9ane", ""], ["Woodward", "Ben", ""], ["Schlining", "Brian", ""], ["Lundsten", "Lonny", ""], ["Barnard", "Kevin", ""], ["Bell", "Katy Croff", ""], ["Katija", "Kakani", ""]]}, {"id": "2007.00461", "submitter": "Sabrina Kirrane", "authors": "Sabrina Kirrane, Alessandra Mileo, Axel Polleres, and Stefan Decker", "title": "Query Based Access Control for Linked Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years we have seen significant advances in the technology used to\nboth publish and consume Linked Data. However, in order to support the next\ngeneration of ebusiness applications on top of interlinked machine readable\ndata suitable forms of access control need to be put in place. Although a\nnumber of access control models and frameworks have been put forward, very\nlittle research has been conducted into the security implications associated\nwith granting access to partial data or the correctness of the proposed access\ncontrol mechanisms. Therefore the contributions of this paper are two fold: we\npropose a query rewriting algorithm which can be used to partially restrict\naccess to SPARQL 1.1 queries and updates; and we demonstrate how a set of\ncriteria, which was originally used to verify that an access control policy\nholds over different database states, can be adapted to verify the correctness\nof access control via query rewriting.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 13:01:45 GMT"}, {"version": "v2", "created": "Thu, 31 Dec 2020 07:38:17 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Kirrane", "Sabrina", ""], ["Mileo", "Alessandra", ""], ["Polleres", "Axel", ""], ["Decker", "Stefan", ""]]}, {"id": "2007.00999", "submitter": "Dhammika Pieris", "authors": "Dhammika Pieris, M.C Wijegunesekera, N.G.J Dias", "title": "ER model Partitioning: Towards Trustworthy Automated Systems Development", "comments": "9 pages, 5 figures. International Journal of Advanced Computer\n  Science and Applications, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In database development, a conceptual model is created, in the form of an\nEntity-relationship(ER) model, and transformed to a relational database schema\n(RDS) to create the database. However, some important information represented\non the ER model may not be transformed and represented on the RDS. This\nsituation causes a loss of information during the transformation process. With\na view to preserving information, in our previous study, we standardized the\ntransformation process as a one-to-one and onto mapping from the ER model to\nthe RDS. For this purpose, we modified the ER model and the transformation\nalgorithm resolving some deficiencies existed in them. Since the mapping was\nestablished using a few real-world cases as a basis and for verification\npurposes, a formal-proof is necessary to validate the work. Thus, the ongoing\nresearch aiming to create a proof will show how a given ER model can be\npartitioned into a unique set of segments and use it to represent the ER model\nitself. How the findings can be used to complete the proof in the future will\nalso be explained. Significance of the research on automating database\ndevelopment, teaching conceptual modeling, and using formal methods will also\nbe discussed.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 10:17:26 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Pieris", "Dhammika", ""], ["Wijegunesekera", "M. C", ""], ["Dias", "N. G. J", ""]]}, {"id": "2007.01946", "submitter": "Tomas Martin", "authors": "Tomas Martin, Guy Francoeur, Petko Valtchev", "title": "CICLAD: A Fast and Memory-efficient Closed Itemset Miner for Streams", "comments": "KDD20", "journal-ref": null, "doi": "10.1145/3394486.3403232", "report-no": null, "categories": "cs.DB cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining association rules from data streams is a challenging task due to the\n(typically) limited resources available vs. the large size of the result.\nFrequent closed itemsets (FCI) enable an efficient first step, yet current FCI\nstream miners are not optimal on resource consumption, e.g. they store a large\nnumber of extra itemsets at an additional cost. In a search for a better\nstorage-efficiency trade-off, we designed Ciclad,an intersection-based\nsliding-window FCI miner. Leveraging in-depth insights into FCI evolution, it\ncombines minimal storage with quick access. Experimental results indicate\nCiclad's memory imprint is much lower and its performances globally better than\ncompetitor methods.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 21:50:35 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Martin", "Tomas", ""], ["Francoeur", "Guy", ""], ["Valtchev", "Petko", ""]]}, {"id": "2007.01973", "submitter": "Besat Kassaie", "authors": "Besat Kassaie and Frank Wm. Tompa", "title": "Detecting Opportunities for Differential Maintenance of Extracted Views", "comments": "19 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-structured and unstructured data management is challenging, but many of\nthe problems encountered are analogous to problems already addressed in the\nrelational context. In the area of information extraction, for example, the\nshift from engineering ad hoc, application-specific extraction rules towards\nusing expressive languages such as CPSL and AQL creates opportunities to\npropose solutions that can be applied to a wide range of extraction programs.\nIn this work, we focus on extracted view maintenance, a problem that is\nwell-motivated and thoroughly addressed in the relational setting. In\nparticular, we formalize and address the problem of keeping extracted relations\nconsistent with source documents that can be arbitrarily updated. We formally\ncharacterize three classes of document updates, namely those that are\nirrelevant, autonomously computable, and pseudo-irrelevant with respect to a\ngiven extractor. Finally, we propose algorithms to detect pseudo-irrelevant\ndocument updates with respect to extractors that are expressed as document\nspanners, a model of information extraction inspired by SystemT.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 00:24:02 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Kassaie", "Besat", ""], ["Tompa", "Frank Wm.", ""]]}, {"id": "2007.02013", "submitter": "Mahawaga Arachchige Pathum Chamikara", "authors": "Pathum Chamikara Mahawaga Arachchige, Peter Bertok, Ibrahim Khalil,\n  Dongxi Liu, Seyit Camtepe", "title": "PPaaS: Privacy Preservation as a Service", "comments": null, "journal-ref": null, "doi": "10.1016/j.comcom.2021.04.006", "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personally identifiable information (PII) can find its way into cyberspace\nthrough various channels, and many potential sources can leak such information.\nData sharing (e.g. cross-agency data sharing) for machine learning and\nanalytics is one of the important components in data science. However, due to\nprivacy concerns, data should be enforced with strong privacy guarantees before\nsharing. Different privacy-preserving approaches were developed for privacy\npreserving data sharing; however, identifying the best privacy-preservation\napproach for the privacy-preservation of a certain dataset is still a\nchallenge. Different parameters can influence the efficacy of the process, such\nas the characteristics of the input dataset, the strength of the\nprivacy-preservation approach, and the expected level of utility of the\nresulting dataset (on the corresponding data mining application such as\nclassification). This paper presents a framework named \\underline{P}rivacy\n\\underline{P}reservation \\underline{a}s \\underline{a} \\underline{S}ervice\n(PPaaS) to reduce this complexity. The proposed method employs selective\nprivacy preservation via data perturbation and looks at different dynamics that\ncan influence the quality of the privacy preservation of a dataset. PPaaS\nincludes pools of data perturbation methods, and for each application and the\ninput dataset, PPaaS selects the most suitable data perturbation approach after\nrigorous evaluation. It enhances the usability of privacy-preserving methods\nwithin its pool; it is a generic platform that can be used to sanitize big data\nin a granular, application-specific manner by employing a suitable combination\nof diverse privacy-preserving algorithms to provide a proper balance between\nprivacy and utility.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 05:44:50 GMT"}, {"version": "v2", "created": "Tue, 29 Dec 2020 00:46:49 GMT"}, {"version": "v3", "created": "Wed, 21 Apr 2021 13:30:54 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Arachchige", "Pathum Chamikara Mahawaga", ""], ["Bertok", "Peter", ""], ["Khalil", "Ibrahim", ""], ["Liu", "Dongxi", ""], ["Camtepe", "Seyit", ""]]}, {"id": "2007.02384", "submitter": "Bortik Bandyopadhyay", "authors": "Bortik Bandyopadhyay, Pranav Maneriker, Vedang Patel, Saumya\n  Yashmohini Sahai, Ping Zhang, Srinivasan Parthasarathy", "title": "DrugDBEmbed : Semantic Queries on Relational Database using Supervised\n  Column Encodings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional relational databases contain a lot of latent semantic information\nthat have largely remained untapped due to the difficulty involved in\nautomatically extracting such information. Recent works have proposed\nunsupervised machine learning approaches to extract such hidden information by\ntextifying the database columns and then projecting the text tokens onto a\nfixed dimensional semantic vector space. However, in certain databases,\ntask-specific class labels may be available, which unsupervised approaches are\nunable to lever in a principled manner. Also, when embeddings are generated at\nindividual token level, then column encoding of multi-token text column has to\nbe computed by taking the average of the vectors of the tokens present in that\ncolumn for any given row. Such averaging approach may not produce the best\nsemantic vector representation of the multi-token text column, as observed\nwhile encoding paragraphs or documents in natural language processing domain.\nWith these shortcomings in mind, we propose a supervised machine learning\napproach using a Bi-LSTM based sequence encoder to directly generate column\nencodings for multi-token text columns of the DrugBank database, which contains\ngold standard drug-drug interaction (DDI) labels. Our text data driven encoding\napproach achieves very high Accuracy on the supervised DDI prediction task for\nsome columns and we use those supervised column encodings to simulate and\nevaluate the Analogy SQL queries on relational data to demonstrate the efficacy\nof our technique.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 16:53:17 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Bandyopadhyay", "Bortik", ""], ["Maneriker", "Pranav", ""], ["Patel", "Vedang", ""], ["Sahai", "Saumya Yashmohini", ""], ["Zhang", "Ping", ""], ["Parthasarathy", "Srinivasan", ""]]}, {"id": "2007.03014", "submitter": "Ahmed Al-Baghdadi Mr.", "authors": "Ahmed Al-Baghdadi and Xiang Lian", "title": "Topic-based Community Search over Spatial-Social Networks (Technical\n  Report)", "comments": "15 pages, 12 figures, and 3 tables. To appear in the PVLDB'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the community search problem has attracted significant attention,\ndue to its wide spectrum of real-world applications such as event organization,\nfriend recommendation, advertisement in e-commence, and so on. Given a query\nvertex, the community search problem finds dense subgraph that contains the\nquery vertex. In social networks, users have multiple check-in locations,\ninfluence score, and profile information (keywords). Most previous studies that\nsolve the CS problem over social networks usually neglect such information in a\ncommunity. In this paper, we propose a novel problem, named community search\nover spatial-social networks (TCS-SSN), which retrieves community with high\nsocial influence, small traveling time, and covering certain keywords. In order\nto tackle the TCS-SSN problem over the spatial-social networks, we design\neffective pruning techniques to reduce the problem search space. We also\npropose an effective indexing mechanism, namely social-spatial index, to\nfacilitate the community query, and develop an efficient query answering\nalgorithm via index traversal. We verify the efficiency and effectiveness of\nour pruning techniques, indexing mechanism, and query processing algorithm\nthrough extensive experiments on real-world and synthetic data sets under\nvarious parameter settings.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 19:00:36 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Al-Baghdadi", "Ahmed", ""], ["Lian", "Xiang", ""]]}, {"id": "2007.04001", "submitter": "Pim Verschuuren", "authors": "Pim Verschuuren, Serena Palazzo, Tom Powell, Steve Sutton, Alfred\n  Pilgrim, Michele Faucci Giannelli", "title": "Supervised machine learning techniques for data matching based on\n  similarity metrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Businesses, governmental bodies and NGO's have an ever-increasing amount of\ndata at their disposal from which they try to extract valuable information.\nOften, this needs to be done not only accurately but also within a short time\nframe. Clean and consistent data is therefore crucial. Data matching is the\nfield that tries to identify instances in data that refer to the same\nreal-world entity. In this study, machine learning techniques are combined with\nstring similarity functions to the field of data matching. A dataset of\ninvoices from a variety of businesses and organizations was preprocessed with a\ngrouping scheme to reduce pair dimensionality and a set of similarity functions\nwas used to quantify similarity between invoice pairs. The resulting invoice\npair dataset was then used to train and validate a neural network and a boosted\ndecision tree. The performance was compared with a solution from FISCAL\nTechnologies as a benchmark against currently available deduplication\nsolutions. Both the neural network and boosted decision tree showed equal to\nbetter performance.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 10:04:35 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Verschuuren", "Pim", ""], ["Palazzo", "Serena", ""], ["Powell", "Tom", ""], ["Sutton", "Steve", ""], ["Pilgrim", "Alfred", ""], ["Giannelli", "Michele Faucci", ""]]}, {"id": "2007.04411", "submitter": "Suman Banerjee", "authors": "Suman Banerjee, Bithika Pal", "title": "An Efficient Updation Approach for Enumerating Maximal $(\\Delta,\n  \\gamma)$\\mbox{-}Cliques of a Temporal Network", "comments": "37 Pages, Submitted to a Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a temporal network $\\mathcal{G}(\\mathcal{V}, \\mathcal{E},\n\\mathcal{T})$, $(\\mathcal{X},[t_a,t_b])$ (where $\\mathcal{X} \\subseteq\n\\mathcal{V}(\\mathcal{G})$ and $[t_a,t_b] \\subseteq \\mathcal{T}$) is said to be\na $(\\Delta, \\gamma)$\\mbox{-}clique of $\\mathcal{G}$, if for every pair of\nvertices in $\\mathcal{X}$, there must exist at least $\\gamma$ links in each\n$\\Delta$ duration within the time interval $[t_a,t_b]$. Enumerating such\nmaximal cliques is an important problem in temporal network analysis, as it\nreveals contact pattern among the nodes of $\\mathcal{G}$. In this paper, we\nstudy the maximal $(\\Delta, \\gamma)$\\mbox{-}clique enumeration problem in\nonline setting; i.e.; the entire link set of the network is not known in\nadvance, and the links are coming as a batch in an iterative manner. Suppose,\nthe link set till time stamp $T_{1}$ (i.e., $\\mathcal{E}^{T_{1}}$), and its\ncorresponding $(\\Delta, \\gamma)$-clique set are known. In the next batch (till\ntime $T_{2}$), a new set of links (denoted as $\\mathcal{E}^{(T_1,T_2]}$) is\narrived.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 20:24:20 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Banerjee", "Suman", ""], ["Pal", "Bithika", ""]]}, {"id": "2007.04450", "submitter": "Amir Gilad", "authors": "Daniel Deutch, Nave Frost, Amir Gilad, Oren Sheffer", "title": "T-REx: Table Repair Explanations", "comments": null, "journal-ref": "In Proceedings of the 2020 ACM SIGMOD. Association for Computing\n  Machinery, New York, NY, USA, pages: 2765 to 2768 (2020)", "doi": "10.1145/3318464.3384700", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data repair is a common and crucial step in many frameworks today, as\napplications may use data from different sources and of different levels of\ncredibility. Thus, this step has been the focus of many works, proposing\ndiverse approaches. To assist users in understanding the output of such data\nrepair algorithms, we propose T-REx, a system for providing data repair\nexplanations through Shapley values. The system is generic and not specific to\na given repair algorithm or approach: it treats the algorithm as a black box.\nGiven a specific table cell selected by the user, T-REx employs Shapley values\nto explain the significance of each constraint and each table cell in the\nrepair of the cell of interest. T-REx then ranks the constraints and table\ncells according to their importance in the repair of this cell. This\nexplanation allows users to understand the repair process, as well as to act\nbased on this knowledge, to modify the most influencing constraints or the\noriginal database.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 21:39:36 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Deutch", "Daniel", ""], ["Frost", "Nave", ""], ["Gilad", "Amir", ""], ["Sheffer", "Oren", ""]]}, {"id": "2007.04453", "submitter": "Nave Frost", "authors": "Naama Boer, Daniel Deutch, Nave Frost, Tova Milo", "title": "Just in Time: Personal Temporal Insights for Altering Model Decisions", "comments": null, "journal-ref": "2019 IEEE 35th International Conference on Data Engineering\n  (ICDE), Macao, Macao, 2019, pp. 1988 -- 1991", "doi": "10.1109/ICDE.2019.00221", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interpretability of complex Machine Learning models is coming to be a\ncritical social concern, as they are increasingly used in human-related\ndecision-making processes such as resume filtering or loan applications.\nIndividuals receiving an undesired classification are likely to call for an\nexplanation -- preferably one that specifies what they should do in order to\nalter that decision when they reapply in the future. Existing work focuses on a\nsingle ML model and a single point in time, whereas in practice, both models\nand data evolve over time: an explanation for an application rejection in 2018\nmay be irrelevant in 2019 since in the meantime both the model and the\napplicant's data can change. To this end, we propose a novel framework that\nprovides users with insights and plans for changing their classification in\nparticular future time points. The solution is based on combining\nstate-of-the-art algorithms for (single) model explanations, ones for\npredicting future models, and database-style querying of the obtained\nexplanations. We propose to demonstrate the usefulness of our solution in the\ncontext of loan applications, and interactively engage the audience in\ncomputing and viewing suggestions tailored for applicants based on their unique\ncharacteristic.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 21:58:41 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Boer", "Naama", ""], ["Deutch", "Daniel", ""], ["Frost", "Nave", ""], ["Milo", "Tova", ""]]}, {"id": "2007.04454", "submitter": "Nave Frost", "authors": "Daniel Deutch, Nave Frost, Amir Gilad", "title": "Explaining Natural Language Query Results", "comments": null, "journal-ref": "The VLDB Journal 29, pp. 485--508 (2020)", "doi": "10.1007/s00778-019-00584-7", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple lines of research have developed Natural Language (NL) interfaces\nfor formulating database queries. We build upon this work, but focus on\npresenting a highly detailed form of the answers in NL. The answers that we\npresent are importantly based on the provenance of tuples in the query result,\ndetailing not only the results but also their explanations. We develop a novel\nmethod for transforming provenance information to NL, by leveraging the\noriginal NL query structure. Furthermore, since provenance information is\ntypically large and complex, we present two solutions for its effective\npresentation as NL text: one that is based on provenance factorization, with\nnovel desiderata relevant to the NL case, and one that is based on\nsummarization. We have implemented our solution in an end-to-end system\nsupporting questions, answers and provenance, all expressed in NL. Our\nexperiments, including a user study, indicate the quality of our solution and\nits scalability.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 22:00:01 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Deutch", "Daniel", ""], ["Frost", "Nave", ""], ["Gilad", "Amir", ""]]}, {"id": "2007.04503", "submitter": "Hongbo Yin", "authors": "Hongbo Yin, Hong Gao, Binghao Wang, Sirui Li, Jianzhong Li", "title": "Efficient Trajectory Compression and Range Query Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, there are ubiquitousness of GPS sensors in various devices\ncollecting, transmitting and storing tremendous trajectory data. However, such\nan unprecedented scale of GPS data has posed an urgent demand for not only an\neffective storage mechanism but also an efficient query mechanism. Line\nsimplification in online mode, searving as a mainstream trajectory compression\nmethod, plays an important role to attack this issue. But for the existing\nalgorithms, either their time cost is extremely high, or the accuracy loss\nafter the compression is completely unacceptable. To attack this issue, we\npropose $\\epsilon \\_$Region based Online trajectory Compression with Error\nbounded (ROCE for short), which makes the best balance among the accuracy loss,\nthe time cost and the compression rate. The range query serves as a primitive,\nyet quite essential operation on analyzing trajectories. Each trajectory is\nusually seen as a sequence of discrete points, and in most previous work, a\ntrajectory is judged to be overlapped with the query region R iff there is at\nleast one point in this trajectory falling in R. But this traditional criteria\nis not suitable when the queried trajectories are compressed, because there may\nbe hundreds of points discarded between each two adjacent points and the points\nin each compressed trajectory are quite sparse. And many trajectories could be\nmissing in the result set. To address this, in this paper, a new criteria based\non the probability and an efficient Range Query processing algorithm on\nCompressed trajectories RQC are proposed. In addition, an efficient index\n\\emph{ASP\\_tree} and lots of novel techniques are also presented to accelerate\nthe processing of trajectory compression and range queries obviously. Extensive\nexperiments have been done on multiple real datasets, and the results\ndemonstrate superior performance of our methods.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 01:39:16 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 10:19:20 GMT"}, {"version": "v3", "created": "Sun, 11 Oct 2020 12:44:42 GMT"}, {"version": "v4", "created": "Tue, 13 Oct 2020 04:51:50 GMT"}, {"version": "v5", "created": "Fri, 7 May 2021 14:51:36 GMT"}, {"version": "v6", "created": "Thu, 1 Jul 2021 02:55:49 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Yin", "Hongbo", ""], ["Gao", "Hong", ""], ["Wang", "Binghao", ""], ["Li", "Sirui", ""], ["Li", "Jianzhong", ""]]}, {"id": "2007.04696", "submitter": "Anastasija Nikiforova", "authors": "Anastasija Nikiforova, Zane Bicevska", "title": "Application of LEAN Principles to Improve Business Processes: a Case\n  Study in Latvian IT Company", "comments": "24 pages, 7 figure, 4 tables, Baltic J. Modern Computing", "journal-ref": "Baltic J. Modern Computing, Vol. 6 (2018), No. 3, 247-270", "doi": "10.22364/bjmc.2018.6.3.03", "report-no": null, "categories": "cs.SE cs.CY cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The research deals with application of the LEAN principles to business\nprocesses of a typical IT company. The paper discusses LEAN principles\namplifying advantages and shortcomings of their application. The authors\nsuggest use of the LEAN principles as a tool to identify improvement potential\nfor IT company's business processes and work-flow efficiency. During a case\nstudy the implementation of LEAN principles has been exemplified in business\nprocesses of a particular Latvian IT company. The obtained results and\nconclusions can be used for meaningful and successful application of LEAN\nprinciples and methods in projects of other IT companies.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 10:42:58 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Nikiforova", "Anastasija", ""], ["Bicevska", "Zane", ""]]}, {"id": "2007.04697", "submitter": "Anastasija Nikiforova", "authors": "Anastasija Nikiforova", "title": "Open Data Quality Evaluation: A Comparative Analysis of Open Data in\n  Latvia", "comments": "24 pages, 2 tables, 3 figures, Baltic J. Modern Computing", "journal-ref": "Baltic J. Modern Computing, Vol. 6(2018), No. 4, 363-386", "doi": "10.22364/bjmc.2018.6.4.04", "report-no": null, "categories": "cs.DB cs.CY cs.IR stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays open data is entering the mainstream - it is free available for\nevery stakeholder and is often used in business decision-making. It is\nimportant to be sure data is trustable and error-free as its quality problems\ncan lead to huge losses. The research discusses how (open) data quality could\nbe assessed. It also covers main points which should be considered developing a\ndata quality management solution. One specific approach is applied to several\nLatvian open data sets. The research provides a step-by-step open data sets\nanalysis guide and summarizes its results. It is also shown there could exist\ndifferences in data quality depending on data supplier (centralized and\ndecentralized data releases) and, unfortunately, trustable data supplier cannot\nguarantee data quality problems absence. There are also underlined common data\nquality problems detected not only in Latvian open data but also in open data\nof 3 European countries.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 10:43:28 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Nikiforova", "Anastasija", ""]]}, {"id": "2007.05373", "submitter": "Joris Dugueperoux", "authors": "Joris Dugu\\'ep\\'eroux (DRUID), Tristan Allard (DRUID)", "title": "From Task Tuning to Task Assignment in Privacy-Preserving Crowdsourcing\n  Platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.DB cs.DC cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Specialized worker profiles of crowdsourcing platforms may contain a large\namount of identifying and possibly sensitive personal information (e.g.,\npersonal preferences, skills, available slots, available devices) raising\nstrong privacy concerns. This led to the design of privacy-preserving\ncrowdsourcing platforms, that aim at enabling efficient crowd-sourcing\nprocesses while providing strong privacy guarantees even when the platform is\nnot fully trusted. In this paper, we propose two contributions. First, we\npropose the PKD algorithm with the goal of supporting a large variety of\naggregate usages of worker profiles within a privacy-preserving crowdsourcing\nplatform. The PKD algorithm combines together homomorphic encryption and\ndifferential privacy for computing (perturbed) partitions of the\nmulti-dimensional space of skills of the actual population of workers and a\n(perturbed) COUNT of workers per partition. Second, we propose to benefit from\nrecent progresses in Private Information Retrieval techniques in order to\ndesign a solution to task assignment that is both private and affordable. We\nperform an in-depth study of the problem of using PIR techniques for proposing\ntasks to workers, show that it is NP-Hard, and come up with the PKD PIR Packing\nheuristic that groups tasks together according to the partitioning output by\nthe PKD algorithm. In a nutshell, we design the PKD algorithm and the PKD PIR\nPacking heuristic, we prove formally their security against honest-but-curious\nworkers and/or platform, we analyze their complexities, and we demonstrate\ntheir quality and affordability in real-life scenarios through an extensive\nexperimental evaluation performed over both synthetic and realistic datasets.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 13:21:18 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Dugu\u00e9p\u00e9roux", "Joris", "", "DRUID"], ["Allard", "Tristan", "", "DRUID"]]}, {"id": "2007.05389", "submitter": "Yuval Moskovitch", "authors": "Daniel Deutch, Yuval Moskovitch and Noam Rinetzky", "title": "COBRA: Compression via Abstraction of Provenance for Hypothetical\n  Reasoning", "comments": null, "journal-ref": "2019 IEEE 35th International Conference on Data Engineering\n  (ICDE), Macao, Macao, 2019, pp. 2016--2019", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data analytics often involves hypothetical reasoning: repeatedly modifying\nthe data and observing the induced effect on the computation result of a\ndata-centric application. Recent work has proposed to leverage ideas from data\nprovenance tracking towards supporting efficient hypothetical reasoning:\ninstead of a costly re-execution of the underlying application, one may assign\nvalues to a pre-computed provenance expression. A prime challenge in leveraging\nthis approach for large-scale data and complex applications lies in the size of\nthe provenance. To this end, we present a framework that allows to reduce\nprovenance size. Our approach is based on reducing the provenance granularity\nusing abstraction. We propose a demonstration of COBRA, a system that allows\nexamine the effect of the provenance compression on the anticipated analysis\nresults. We will demonstrate the usefulness of COBRA in the context of business\ndata analysis.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 13:55:09 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Deutch", "Daniel", ""], ["Moskovitch", "Yuval", ""], ["Rinetzky", "Noam", ""]]}, {"id": "2007.05400", "submitter": "Yuval Moskovitch", "authors": "Daniel Deutch, Yuval Moskovitch and Noam Rinetzky", "title": "Hypothetical Reasoning via Provenance Abstraction", "comments": null, "journal-ref": "Proceedings of the 2019 International Conference on Management of\n  Data, SIGMOD Conference 2019, Amsterdam, The Netherlands, pages 537--554", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data analytics often involves hypothetical reasoning: repeatedly modifying\nthe data and observing the induced effect on the computation result of a\ndata-centric application. Previous work has shown that fine-grained data\nprovenance can help make such an analysis more efficient: instead of a costly\nre-execution of the underlying application, hypothetical scenarios are applied\nto a pre-computed provenance expression. However, storing provenance for\ncomplex queries and large-scale data leads to a significant overhead, which is\noften a barrier to the incorporation of provenance-based solutions.\n  To this end, we present a framework that allows to reduce provenance size.\nOur approach is based on reducing the provenance granularity using user defined\nabstraction trees over the provenance variables; the granularity is based on\nthe anticipated hypothetical scenarios. We formalize the tradeoff between\nprovenance size and supported granularity of the hypothetical reasoning, and\nstudy the complexity of the resulting optimization problem, provide efficient\nalgorithms for tractable cases and heuristics for others. We experimentally\nstudy the performance of our solution for various queries and abstraction\ntrees. Our study shows that the algorithms generally lead to substantial\nspeedup of hypothetical reasoning, with a reasonable loss of accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 14:05:33 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Deutch", "Daniel", ""], ["Moskovitch", "Yuval", ""], ["Rinetzky", "Noam", ""]]}, {"id": "2007.05437", "submitter": "Jinbin Huang", "authors": "Jinbin Huang, Xin Huang, and Jianliang Xu", "title": "Truss-based Structural Diversity Search in Large Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social decisions made by individuals are easily influenced by information\nfrom their social neighborhoods. A key predictor of social contagion is the\nmultiplicity of social contexts inside the individual's contact neighborhood,\nwhich is termed structural diversity. However, the existing models have limited\ndecomposability for analyzing large-scale networks, and suffer from the\ninaccurate reflection of social context diversity. In this paper, we propose a\nnew truss-based structural diversity model to overcome the weak\ndecomposability. Based on this model, we study a novel problem of truss-based\nstructural diversity search in a graph G, that is, to find the r vertices with\nthe highest truss-based structural diversity and return their social contexts.\no tackle this problem, we propose an online structural diversity search\nalgorithm in $O(\\rho(m+\\mathcal{T}))$ time, where $\\rho$, $m$, and\n$\\mathcal{T}$ are respectively the arboricity, the number of edges, and the\nnumber of triangles in $G$. To improve the efficiency, we design an elegant and\ncompact index, called TSD-index, for further expediting the search process. We\nfurther optimize the structure of TSD-index into a highly compressed GCT-index.\nOur GCT-index-based structural diversity search utilizes the global triangle\ninformation for fast index construction and finds answers in $O(m)$ time.\nExtensive experiments demonstrate the effectiveness and efficiency of our\nproposed model and algorithms, against state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 15:17:39 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 02:05:06 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Huang", "Jinbin", ""], ["Huang", "Xin", ""], ["Xu", "Jianliang", ""]]}, {"id": "2007.05463", "submitter": "Yuval Moskovitch", "authors": "Pierre Bourhis, Daniel Deutch and Yuval Moskovitch", "title": "Equivalence-Invariant Algebraic Provenance for Hyperplane Update Queries", "comments": null, "journal-ref": "Proceedings of the 2020 International Conference on Management of\n  Data, SIGMOD Conference 2020, online conference [Portland, OR, USA], pages:\n  415--429", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The algebraic approach for provenance tracking, originating in the semiring\nmodel of Green et. al, has proven useful as an abstract way of handling\nmetadata. Commutative Semirings were shown to be the \"correct\" algebraic\nstructure for Union of Conjunctive Queries, in the sense that its use allows\nprovenance to be invariant under certain expected query equivalence axioms.\n  In this paper we present the first (to our knowledge) algebraic provenance\nmodel, for a fragment of update queries, that is invariant under set\nequivalence. The fragment that we focus on is that of hyperplane queries,\npreviously studied in multiple lines of work. Our algebraic provenance\nstructure and corresponding provenance-aware semantics are based on the sound\nand complete axiomatization of Karabeg and Vianu. We demonstrate that our\nconstruction can guide the design of concrete provenance model instances for\ndifferent applications. We further study the efficient generation and storage\nof provenance for hyperplane update queries. We show that a naive algorithm can\nlead to an exponentially large provenance expression, but remedy this by\npresenting a normal form which we show may be efficiently computed alongside\nquery evaluation. We experimentally study the performance of our solution and\ndemonstrate its scalability and usefulness, and in particular the effectiveness\nof our normal form representation.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 16:05:36 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Bourhis", "Pierre", ""], ["Deutch", "Daniel", ""], ["Moskovitch", "Yuval", ""]]}, {"id": "2007.05572", "submitter": "Zongheng Yang", "authors": "Eric Liang, Zongheng Yang, Ion Stoica, Pieter Abbeel, Yan Duan, Xi\n  Chen", "title": "Variable Skipping for Autoregressive Range Density Estimation", "comments": "ICML 2020. Code released at: https://var-skip.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep autoregressive models compute point likelihood estimates of individual\ndata points. However, many applications (i.e., database cardinality estimation)\nrequire estimating range densities, a capability that is under-explored by\ncurrent neural density estimation literature. In these applications, fast and\naccurate range density estimates over high-dimensional data directly impact\nuser-perceived performance. In this paper, we explore a technique, variable\nskipping, for accelerating range density estimation over deep autoregressive\nmodels. This technique exploits the sparse structure of range density queries\nto avoid sampling unnecessary variables during approximate inference. We show\nthat variable skipping provides 10-100$\\times$ efficiency improvements when\ntargeting challenging high-quantile error metrics, enables complex applications\nsuch as text pattern matching, and can be realized via a simple data\naugmentation procedure without changing the usual maximum likelihood objective.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 19:01:40 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Liang", "Eric", ""], ["Yang", "Zongheng", ""], ["Stoica", "Ion", ""], ["Abbeel", "Pieter", ""], ["Duan", "Yan", ""], ["Chen", "Xi", ""]]}, {"id": "2007.05651", "submitter": "Jinfeng Li", "authors": "Jinfeng Li, Yuliang Li, Xiaolan Wang, Wang-Chiew Tan", "title": "Deep or Simple Models for Semantic Tagging? It Depends on your Data\n  [Experiments]", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic tagging, which has extensive applications in text mining, predicts\nwhether a given piece of text conveys the meaning of a given semantic tag. The\nproblem of semantic tagging is largely solved with supervised learning and\ntoday, deep learning models are widely perceived to be better for semantic\ntagging. However, there is no comprehensive study supporting the popular\nbelief. Practitioners often have to train different types of models for each\nsemantic tagging task to identify the best model. This process is both\nexpensive and inefficient.\n  We embark on a systematic study to investigate the following question: Are\ndeep models the best performing model for all semantic tagging tasks? To answer\nthis question, we compare deep models against \"simple models\" over datasets\nwith varying characteristics. Specifically, we select three prevalent deep\nmodels (i.e. CNN, LSTM, and BERT) and two simple models (i.e. LR and SVM), and\ncompare their performance on the semantic tagging task over 21 datasets.\nResults show that the size, the label ratio, and the label cleanliness of a\ndataset significantly impact the quality of semantic tagging. Simple models\nachieve similar tagging quality to deep models on large datasets, but the\nruntime of simple models is much shorter. Moreover, simple models can achieve\nbetter tagging quality than deep models when targeting datasets show worse\nlabel cleanliness and/or more severe imbalance. Based on these findings, our\nstudy can systematically guide practitioners in selecting the right learning\nmodel for their semantic tagging task.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 00:05:50 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 22:45:08 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Li", "Jinfeng", ""], ["Li", "Yuliang", ""], ["Wang", "Xiaolan", ""], ["Tan", "Wang-Chiew", ""]]}, {"id": "2007.05881", "submitter": "Marko Smilevski", "authors": "Marko Smilevski", "title": "Applying recent advances in Visual Question Answering to Record Linkage", "comments": "48 pages, 15 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DB stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Multi-modal Record Linkage is the process of matching multi-modal records\nfrom multiple sources that represent the same entity. This field has not been\nexplored in research and we propose two solutions based on Deep Learning\narchitectures that are inspired by recent work in Visual Question Answering.\nThe neural networks we propose use two different fusion modules, the Recurrent\nNeural Network + Convolutional Neural Network fusion module and the Stacked\nAttention Network fusion module, that jointly combine the visual and the\ntextual data of the records. The output of these fusion models is the input of\na Siamese Neural Network that computes the similarity of the records. Using\ndata from the Avito Duplicate Advertisements Detection dataset, we train these\nsolutions and from the experiments, we concluded that the Recurrent Neural\nNetwork + Convolutional Neural Network fusion module outperforms a simple model\nthat uses hand-crafted features. We also find that the Recurrent Neural Network\n+ Convolutional Neural Network fusion module classifies dissimilar\nadvertisements as similar more frequently if their average description is\nbigger than 40 words. We conclude that the reason for this is that the longer\nadvertisements have a different distribution then the shorter advertisements\nwho are more prevalent in the dataset. In the end, we also conclude that\nfurther research needs to be done with the Stacked Attention Network, to\nfurther explore the effects of the visual data on the performance of the fusion\nmodules.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 01:24:47 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Smilevski", "Marko", ""]]}, {"id": "2007.06292", "submitter": "Piyush Yadav", "authors": "Piyush Yadav, Dhaval Salwala, Edward Curry", "title": "Knowledge Graph Driven Approach to Represent Video Streams for\n  Spatiotemporal Event Pattern Matching in Complex Event Processing", "comments": "31 pages, 14 Figures, Publication accepted in International Journal\n  of Graph Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.DB cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex Event Processing (CEP) is an event processing paradigm to perform\nreal-time analytics over streaming data and match high-level event patterns.\nPresently, CEP is limited to process structured data stream. Video streams are\ncomplicated due to their unstructured data model and limit CEP systems to\nperform matching over them. This work introduces a graph-based structure for\ncontinuous evolving video streams, which enables the CEP system to query\ncomplex video event patterns. We propose the Video Event Knowledge Graph\n(VEKG), a graph driven representation of video data. VEKG models video objects\nas nodes and their relationship interaction as edges over time and space. It\ncreates a semantic knowledge representation of video data derived from the\ndetection of high-level semantic concepts from the video using an ensemble of\ndeep learning models. A CEP-based state optimization - VEKG-Time Aggregated\nGraph (VEKG-TAG) is proposed over VEKG representation for faster event\ndetection. VEKG-TAG is a spatiotemporal graph aggregation method that provides\na summarized view of the VEKG graph over a given time length. We defined a set\nof nine event pattern rules for two domains (Activity Recognition and Traffic\nManagement), which act as a query and applied over VEKG graphs to discover\ncomplex event patterns. To show the efficacy of our approach, we performed\nextensive experiments over 801 video clips across 10 datasets. The proposed\nVEKG approach was compared with other state-of-the-art methods and was able to\ndetect complex event patterns over videos with F-Score ranging from 0.44 to\n0.90. In the given experiments, the optimized VEKG-TAG was able to reduce 99%\nand 93% of VEKG nodes and edges, respectively, with 5.19X faster search time,\nachieving sub-second median latency of 4-20 milliseconds.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 10:20:58 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Yadav", "Piyush", ""], ["Salwala", "Dhaval", ""], ["Curry", "Edward", ""]]}, {"id": "2007.06300", "submitter": "Marta Arias", "authors": "Christian Lezcano, Marta Arias", "title": "Synthetic Dataset Generation with Itemset-Based Generative Models", "comments": "IEEE International Symposium on Software Reliability Engineering\n  Workshops (ISSREW@RDSA 2019), Oct 2019", "journal-ref": null, "doi": "10.1109/ISSREW.2019.00086", "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes three different data generators, tailored to\ntransactional datasets, based on existing itemset-based generative models. All\nthese generators are intuitive and easy to implement and show satisfactory\nperformance. The quality of each generator is assessed by means of three\ndifferent methods that capture how well the original dataset structure is\npreserved.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 10:37:21 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Lezcano", "Christian", ""], ["Arias", "Marta", ""]]}, {"id": "2007.06507", "submitter": "Lee Braine", "authors": "Aishwarya Nair, Lee Braine", "title": "Industry Adoption Scenarios for Authoritative Data Stores using the ISDA\n  Common Domain Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we explore opportunities for the post-trade industry to\nstandardise and simplify in order to significantly increase efficiency and\nreduce costs. We start by summarising relevant industry problems (inconsistent\nprocesses, inconsistent data, and duplicated data) and the corresponding\npotential industry solutions (process standardisation, data standardisation,\nand authoritative data stores). This includes transitioning to the\nInternational Swaps and Derivatives Association (ISDA) Common Domain Model\n(CDM) as a standard set of digital representations for the events and processes\nthroughout the lifecycle of a trade. We then explore how financial market\ninfrastructures could operate authoritative data stores that make CDM business\nevents available to broker-dealers, considering both traditional centralised\nmodels and potential decentralised models. For both models, there are many\npossible adoption scenarios (depending on each broker-dealer's degree of\nintegration with the authoritative data store and usage of the CDM) and we\nidentify some of the key scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 16:59:03 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Nair", "Aishwarya", ""], ["Braine", "Lee", ""]]}, {"id": "2007.06540", "submitter": "Anastasija Nikiforova", "authors": "Anastasija Nikiforova", "title": "Open Data Quality", "comments": "10 pages, 3 figures, 13th International Baltic Conference on\n  Databases and Information Systems & The Baltic DB&IS 2018 Doctoral Consortium\n  (Baltic DB&IS 2018) At: Lithuania, Trakai, Volume: 2158. arXiv admin note:\n  substantial text overlap with arXiv:2007.04697", "journal-ref": "Baltic DB&IS 2018 Joint Proceedings of the Conference Forum and\n  Doctoral Consortium", "doi": null, "report-no": null, "categories": "cs.DB cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The research discusses how (open) data quality could be described, what\nshould be considered developing a data quality management solution and how it\ncould be applied to open data to check its quality. The proposed approach\nfocuses on development of data quality specification which can be executed to\nget data quality evaluation results, find errors in data and possible problems\nwhich must be solved. The proposed approach is applied to several open data\nsets to evaluate their quality. Open data is very popular, free available for\nevery stakeholder - it is often used to make business decisions. It is\nimportant to be sure that this data is trustable and error-free as its quality\nproblems can lead to huge losses.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 11:10:22 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Nikiforova", "Anastasija", ""]]}, {"id": "2007.07060", "submitter": "Dharmen Punjani", "authors": "Dharmen Punjani, Markos Iliakis, Theodoros Stefou, Kuldeep Singh,\n  Andreas Both, Manolis Koubarakis, Iosif Angelidis, Konstantina Bereta, Themis\n  Beris, Dimitris Bilidas, Theofilos Ioannidis, Nikolaos Karalis, Christoph\n  Lange, Despina-Athanasia Pantazi, Christos Papaloukas, Georgios Stamoulis", "title": "Template-Based Question Answering over Linked Geospatial Data", "comments": "27 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large amounts of geospatial data have been made available recently on the\nlinked open data cloud and the portals of many national cartographic agencies\n(e.g., OpenStreetMap data, administrative geographies of various countries, or\nland cover/land use data sets). These datasets use various geospatial\nvocabularies and can be queried using SPARQL or its OGC-standardized extension\nGeoSPARQL. In this paper, we go beyond these approaches to offer a\nquestion-answering engine for natural language questions on top of linked\ngeospatial data sources. Our system has been implemented as re-usable\ncomponents of the Frankenstein question answering architecture. We give a\ndetailed description of the system's architecture, its underlying algorithms,\nand its evaluation using a set of 201 natural language questions. The set of\nquestions is offered to the research community as a gold standard dataset for\nthe comparative evaluation of future geospatial question answering engines.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 14:35:48 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 15:41:33 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Punjani", "Dharmen", ""], ["Iliakis", "Markos", ""], ["Stefou", "Theodoros", ""], ["Singh", "Kuldeep", ""], ["Both", "Andreas", ""], ["Koubarakis", "Manolis", ""], ["Angelidis", "Iosif", ""], ["Bereta", "Konstantina", ""], ["Beris", "Themis", ""], ["Bilidas", "Dimitris", ""], ["Ioannidis", "Theofilos", ""], ["Karalis", "Nikolaos", ""], ["Lange", "Christoph", ""], ["Pantazi", "Despina-Athanasia", ""], ["Papaloukas", "Christos", ""], ["Stamoulis", "Georgios", ""]]}, {"id": "2007.07595", "submitter": "Daniel Ritter", "authors": "Jonas Dann, Daniel Ritter, Holger Fr\\\"oning", "title": "Non-Relational Databases on FPGAs: Survey, Design Decisions, Challenges", "comments": "FPGA, hardware acceleration, non-relational databases, graph,\n  document, key-value; 28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-relational database systems (NRDS), such as graph, document, key-value,\nand wide-column, have gained much attention in various trending (business)\napplication domains like smart logistics, social network analysis, and medical\napplications, due to their data model variety and scalability. The broad data\nvariety and sheer size of datasets pose unique challenges for the system design\nand runtime (incl. power consumption). While CPU performance scaling becomes\nincreasingly more difficult, we argue that NRDS can benefit from adding field\nprogrammable gate arrays (FPGAs) as accelerators. However, FPGA-accelerated\nNRDS have not been systematically studied, yet.\n  To facilitate understanding of this emerging domain, we explore the fit of\nFPGA acceleration for NRDS with a focus on data model variety. We define the\nterm NRDS class as a group of non-relational database systems supporting the\nsame data model. This survey describes and categorizes the inherent differences\nand non-trivial trade-offs of relevant NRDS classes as well as their\ncommonalities in the context of common design decisions when building such a\nsystem with FPGAs. For example, we found in the literature that for key-value\nstores the FPGA should be placed into the system as a smart network interface\ncard (SmartNIC) to benefit from direct access of the FPGA to the network.\nHowever, more complex data models and processing of other classes (e.g., graph\nand document) commonly require more elaborate near-data or socket accelerator\nplacements where the FPGA respectively has the only or shared access to main\nmemory. Across the different classes, FPGAs can be used as communication layer\nor for acceleration of operators and data access. We close with open research\nand engineering challenges to outline the future of FPGA-accelerated NRDS.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 10:22:02 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Dann", "Jonas", ""], ["Ritter", "Daniel", ""], ["Fr\u00f6ning", "Holger", ""]]}, {"id": "2007.07817", "submitter": "Piyush Yadav", "authors": "Piyush Yadav, Edward Curry", "title": "VidCEP: Complex Event Processing Framework to Detect Spatiotemporal\n  Patterns in Video Streams", "comments": "10 pages, 19 figures, Paper published in IEEE BigData 2019", "journal-ref": null, "doi": "10.1109/BigData47090.2019.9006018", "report-no": null, "categories": "cs.CV cs.DB cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video data is highly expressive and has traditionally been very difficult for\na machine to interpret. Querying event patterns from video streams is\nchallenging due to its unstructured representation. Middleware systems such as\nComplex Event Processing (CEP) mine patterns from data streams and send\nnotifications to users in a timely fashion. Current CEP systems have inherent\nlimitations to query video streams due to their unstructured data model and\nlack of expressive query language. In this work, we focus on a CEP framework\nwhere users can define high-level expressive queries over videos to detect a\nrange of spatiotemporal event patterns. In this context, we propose: i) VidCEP,\nan in-memory, on the fly, near real-time complex event matching framework for\nvideo streams. The system uses a graph-based event representation for video\nstreams which enables the detection of high-level semantic concepts from video\nusing cascades of Deep Neural Network models, ii) a Video Event Query language\n(VEQL) to express high-level user queries for video streams in CEP, iii) a\ncomplex event matcher to detect spatiotemporal video event patterns by matching\nexpressive user queries over video data. The proposed approach detects\nspatiotemporal video event patterns with an F-score ranging from 0.66 to 0.89.\nVidCEP maintains near real-time performance with an average throughput of 70\nframes per second for 5 parallel videos with sub-second matching latency.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 16:43:37 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Yadav", "Piyush", ""], ["Curry", "Edward", ""]]}, {"id": "2007.07858", "submitter": "Haneen Mohammed", "authors": "Haneen Mohammed, Ziyun Wei, Eugene Wu, Ravi Netravali", "title": "Continuous Prefetch for Interactive Data Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive data visualization and exploration (DVE) applications are often\nnetwork-bottlenecked due to bursty request patterns, large response sizes, and\nheterogeneous deployments over a range of networks and devices. This makes it\ndifficult to ensure consistently low response times (< 100ms). Khameleon is a\nframework for DVE applications that uses a novel combination of prefetching and\nresponse tuning to dynamically trade-off response quality for low latency.\nKhameleon exploits DVE's approximation tolerance: immediate lower-quality\nresponses are preferable to waiting for complete results. To this end,\nKhameleon progressively encodes responses, and runs a server-side scheduler\nthat proactively streams portions of responses using available bandwidth to\nmaximize user's perceived interactivity. The scheduler involves a complex\noptimization based on available resources, predicted user interactions, and\nresponse quality levels; yet, decisions must also be real-time. To overcome\nthis, Khameleon uses a fast greedy approximation which closely mimics the\noptimal approach. Using image exploration and visualization applications with\nreal user interaction traces, we show that across a wide range of network and\nclient resource conditions, Khameleon outperforms classic prefetching\napproaches that benefit from perfect prediction models: response latencies with\nKhameleon are never higher, and typically between 2 to 3 orders of magnitude\nlower while response quality remains within 50%-80%.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 17:23:48 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Mohammed", "Haneen", ""], ["Wei", "Ziyun", ""], ["Wu", "Eugene", ""], ["Netravali", "Ravi", ""]]}, {"id": "2007.08292", "submitter": "Manuel Rigger", "authors": "Manuel Rigger, Zhendong Su", "title": "Detecting Optimization Bugs in Database Engines via Non-Optimizing\n  Reference Engine Construction", "comments": "Accepted at the 28th ACM Joint European Software Engineering\n  Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE\n  '20)", "journal-ref": null, "doi": "10.1145/3368089.3409710", "report-no": null, "categories": "cs.SE cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Database Management Systems (DBMS) are used ubiquitously. To efficiently\naccess data, they apply sophisticated optimizations. Incorrect optimizations\ncan result in logic bugs, which cause a query to compute an incorrect result\nset. We propose Non-Optimizing Reference Engine Construction (NoREC), a\nfully-automatic approach to detect optimization bugs in DBMS. Conceptually,\nthis approach aims to evaluate a query by an optimizing and a non-optimizing\nversion of a DBMS, to then detect differences in their returned result set,\nwhich would indicate a bug in the DBMS. Obtaining a non-optimizing version of a\nDBMS is challenging, because DBMS typically provide limited control over\noptimizations. Our core insight is that a given, potentially randomly-generated\noptimized query can be rewritten to one that the DBMS cannot optimize.\nEvaluating this unoptimized query effectively corresponds to a non-optimizing\nreference engine executing the original query. We evaluated NoREC in an\nextensive testing campaign on four widely-used DBMS, namely PostgreSQL,\nMariaDB, SQLite, and CockroachDB. We found 159 previously unknown bugs in the\nlatest versions of these systems, 141 of which have been fixed by the\ndevelopers. Of these, 51 were optimization bugs, while the remaining were error\nand crash bugs. Our results suggest that NoREC is effective, general and\nrequires little implementation effort, which makes the technique widely\napplicable in practice.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 12:30:21 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Rigger", "Manuel", ""], ["Su", "Zhendong", ""]]}, {"id": "2007.08821", "submitter": "Pierre Monnin", "authors": "Pierre Monnin and Emmanuel Bresso and Miguel Couceiro and Malika\n  Sma\\\"il-Tabbone and Amedeo Napoli and Adrien Coulet", "title": "Tackling scalability issues in mining path patterns from knowledge\n  graphs: a preliminary study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Features mined from knowledge graphs are widely used within multiple\nknowledge discovery tasks such as classification or fact-checking. Here, we\nconsider a given set of vertices, called seed vertices, and focus on mining\ntheir associated neighboring vertices, paths, and, more generally, path\npatterns that involve classes of ontologies linked with knowledge graphs. Due\nto the combinatorial nature and the increasing size of real-world knowledge\ngraphs, the task of mining these patterns immediately entails scalability\nissues. In this paper, we address these issues by proposing a pattern mining\napproach that relies on a set of constraints (e.g., support or degree\nthresholds) and the monotonicity property. As our motivation comes from the\nmining of real-world knowledge graphs, we illustrate our approach with PGxLOD,\na biomedical knowledge graph.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 08:36:26 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2020 13:33:15 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Monnin", "Pierre", ""], ["Bresso", "Emmanuel", ""], ["Couceiro", "Miguel", ""], ["Sma\u00efl-Tabbone", "Malika", ""], ["Napoli", "Amedeo", ""], ["Coulet", "Adrien", ""]]}, {"id": "2007.08876", "submitter": "Matthias Lanzinger", "authors": "Matthias Lanzinger", "title": "Tractability Beyond $\\beta$-Acyclicity for Conjunctive Queries with\n  Negation", "comments": null, "journal-ref": null, "doi": "10.1145/3452021.3458308", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous fundamental database and reasoning problems are known to be NP-hard\nin general but tractable on instances where the underlying hypergraph structure\nis $\\beta$-acyclic. Despite the importance of many of these problems, there has\nbeen little success in generalizing these results beyond acyclicity.\n  In this paper, we take on this challenge and propose nest-set width, a novel\ngeneralization of hypergraph $\\beta$-acyclicity. We demonstrate that nest-set\nwidth has desirable properties and algorithmic significance. In particular,\nevaluation of boolean conjunctive queries with negation is tractable for\nclasses with bounded nest-set width. Furthermore, propositional satisfiability\nis fixed-parameter tractable when parameterized by nest-set width.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 10:10:04 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 18:53:32 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Lanzinger", "Matthias", ""]]}, {"id": "2007.09141", "submitter": "Fei Chiang", "authors": "Mostafa Milani, Yu Huang, Fei Chiang", "title": "Diversifying Anonymized Data with Diversity Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently introduced privacy legislation has aimed to restrict and control the\namount of personal data published by companies and shared to third parties.\nMuch of this real data is not only sensitive requiring anonymization, but also\ncontains characteristic details from a variety of individuals. This diversity\nis desirable in many applications ranging from Web search to drug and product\ndevelopment. Unfortunately, data anonymization techniques have largely ignored\ndiversity in its published result. This inadvertently propagates underlying\nbias in subsequent data analysis. We study the problem of finding a diverse\nanonymized data instance where diversity is measured via a set of diversity\nconstraints. We formalize diversity constraints and study their foundations\nsuch as implication and satisfiability. We show that determining the existence\nof a diverse, anonymized instance can be done in PTIME, and we present a\nclustering-based algorithm. We conduct extensive experiments using real and\nsynthetic data showing the effectiveness of our techniques, and improvement\nover existing baselines. Our work aligns with recent trends towards responsible\ndata science by coupling diversity with privacy-preserving data publishing.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 17:58:34 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Milani", "Mostafa", ""], ["Huang", "Yu", ""], ["Chiang", "Fei", ""]]}, {"id": "2007.09261", "submitter": "Vassilis Digalakis Jr.", "authors": "Dimitris Bertsimas and Vassilis Digalakis Jr", "title": "Frequency Estimation in Data Streams: Learning the Optimal Hashing\n  Scheme", "comments": "Submitted to IEEE Transactions on Knowledge and Data Engineering on\n  07/2020. Revised on 05/2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach for the problem of frequency estimation in data\nstreams that is based on optimization and machine learning. Contrary to\nstate-of-the-art streaming frequency estimation algorithms, which heavily rely\non random hashing to maintain the frequency distribution of the data steam\nusing limited storage, the proposed approach exploits an observed stream prefix\nto near-optimally hash elements and compress the target frequency distribution.\nWe develop an exact mixed-integer linear optimization formulation, which\nenables us to compute optimal or near-optimal hashing schemes for elements seen\nin the observed stream prefix; then, we use machine learning to hash unseen\nelements. Further, we develop an efficient block coordinate descent algorithm,\nwhich, as we empirically show, produces high quality solutions, and, in a\nspecial case, we are able to solve the proposed formulation exactly in linear\ntime using dynamic programming. We empirically evaluate the proposed approach\nboth on synthetic datasets and on real-world search query data. We show that\nthe proposed approach outperforms existing approaches by one to two orders of\nmagnitude in terms of its average (per element) estimation error and by 45-90%\nin terms of its expected magnitude of estimation error.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 22:15:22 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 04:38:39 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Bertsimas", "Dimitris", ""], ["Digalakis", "Vassilis", "Jr"]]}, {"id": "2007.09352", "submitter": "Amin Jalali", "authors": "Amin Jalali", "title": "Graph-based process mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process mining is an area of research that supports discovering information\nabout business processes from their execution event logs. The increasing amount\nof event logs in organizations challenges current process mining techniques,\nwhich tend to load data into the memory of a computer. This issue limits the\norganizations to apply process mining on a large scale and introduces risks due\nto the lack of data management capabilities. Therefore, this paper introduces\nand formalizes a new approach to store and retrieve event logs into/from graph\ndatabases. It defines an algorithm to compute Directly Follows Graph (DFG)\ninside the graph database, which shifts the heavy computation parts of process\nmining into the graph database. Calculating DFG in graph databases enables\nleveraging the graph databases' horizontal and vertical scaling capabilities in\nfavor of applying process mining on a large scale. Besides, it removes the\nrequirement to move data into analysts' computer. Thus, it enables using data\nmanagement capabilities in graph databases. We implemented this approach in\nNeo4j and evaluated its performance compared with current techniques using a\nreal log file. The result shows that our approach enables the calculation of\nDFG when the data is much bigger than the computational memory. It also shows\nbetter performance when dicing data into small chunks.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 07:15:32 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Jalali", "Amin", ""]]}, {"id": "2007.09589", "submitter": "Supun Kamburugamuve", "authors": "Chathura Widanage, Niranda Perera, Vibhatha Abeykoon, Supun\n  Kamburugamuve, Thejaka Amila Kanewala, Hasara Maithree, Pulasthi\n  Wickramasinghe, Ahmet Uyar, Gurhan Gunduz, and Geoffrey Fox", "title": "High Performance Data Engineering Everywhere", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The amazing advances being made in the fields of machine and deep learning\nare a highlight of the Big Data era for both enterprise and research\ncommunities. Modern applications require resources beyond a single node's\nability to provide. However this is just a small part of the issues facing the\noverall data processing environment, which must also support a raft of data\nengineering for pre- and post-data processing, communication, and system\nintegration. An important requirement of data analytics tools is to be able to\neasily integrate with existing frameworks in a multitude of languages, thereby\nincreasing user productivity and efficiency. All this demands an efficient and\nhighly distributed integrated approach for data processing, yet many of today's\npopular data analytics tools are unable to satisfy all these requirements at\nthe same time.\n  In this paper we present Cylon, an open-source high performance distributed\ndata processing library that can be seamlessly integrated with existing Big\nData and AI/ML frameworks. It is developed with a flexible C++ core on top of a\ncompact data structure and exposes language bindings to C++, Java, and Python.\nWe discuss Cylon's architecture in detail, and reveal how it can be imported as\na library to existing applications or operate as a standalone framework.\nInitial experiments show that Cylon enhances popular tools such as Apache Spark\nand Dask with major performance improvements for key operations and better\ncomponent linkages. Finally, we show how its design enables Cylon to be used\ncross-platform with minimum overhead, which includes popular AI tools such as\nPyTorch, Tensorflow, and Jupyter notebooks.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 04:28:42 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Widanage", "Chathura", ""], ["Perera", "Niranda", ""], ["Abeykoon", "Vibhatha", ""], ["Kamburugamuve", "Supun", ""], ["Kanewala", "Thejaka Amila", ""], ["Maithree", "Hasara", ""], ["Wickramasinghe", "Pulasthi", ""], ["Uyar", "Ahmet", ""], ["Gunduz", "Gurhan", ""], ["Fox", "Geoffrey", ""]]}, {"id": "2007.09634", "submitter": "Yanhao Wang", "authors": "Yanhao Wang, Michael Mathioudakis, Yuchen Li, Kian-Lee Tan", "title": "GRMR: Generalized Regret-Minimizing Representatives", "comments": "23 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Extracting a small subset of representative tuples from a large database is\nan important task in multi-criteria decision making. The regret-minimizing set\n(RMS) problem is recently proposed for representative discovery from databases.\nSpecifically, for a set of tuples (points) in $d$ dimensions, an RMS problem\nfinds the smallest subset such that, for any possible ranking function, the\nrelative difference in scores between the top-ranked point in the subset and\nthe top-ranked point in the entire database is within a parameter $\\varepsilon\n\\in (0,1)$. Although RMS and its variations have been extensively investigated\nin the literature, existing approaches only consider the class of nonnegative\n(monotonic) linear functions for ranking, which have limitations in modeling\nuser preferences and decision-making processes.\n  To address this issue, we define the generalized regret-minimizing\nrepresentative (GRMR) problem that extends RMS by taking into account all\nlinear functions including non-monotonic ones with negative weights. For\ntwo-dimensional databases, we propose an optimal algorithm for GRMR via a\ntransformation into the shortest cycle problem in a directed graph. Since GRMR\nis proven to be NP-hard even in three dimensions, we further develop a\npolynomial-time heuristic algorithm for GRMR on databases in arbitrary\ndimensions. Finally, we conduct extensive experiments on real and synthetic\ndatasets to confirm the efficiency, effectiveness, and scalability of our\nproposed algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 09:27:48 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Wang", "Yanhao", ""], ["Mathioudakis", "Michael", ""], ["Li", "Yuchen", ""], ["Tan", "Kian-Lee", ""]]}, {"id": "2007.09733", "submitter": "Jo\\~ao Louren\\c{c}o", "authors": "Tiago M. Vale, Jo\\~ao Leit\\~ao, Nuno Pregui\\c{c}a, Rodrigo Rodrigues,\n  Ricardo J. Dias, Jo\\~ao M. Louren\\c{c}o", "title": "Lazy State Determination: More concurrency for contending linearizable\n  transactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concurrency control algorithms in transactional systems limits\nconcurrency to provide strong semantics, which leads to poor performance under\nhigh contention. As a consequence, many transactional systems eschew strong\nsemantics to achieve acceptable performance. We show that by leveraging\nsemantic information associated with the transactional programs to increase\nconcurrency, it is possible to significantly improve performance while\nmaintaining linearizability. To this end, we introduce the lazy state\ndetermination API to easily expose the semantics of application transactions to\nthe database, and propose new optimistic and pessimistic concurrency control\nalgorithms that leverage this information to safely increase concurrency in the\npresence of contention. Our evaluation shows that our approach can achieve up\nto 5x more throughput with 1.5c less latency than standard techniques in the\npopular TPC-C benchmark.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 18:02:05 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Vale", "Tiago M.", ""], ["Leit\u00e3o", "Jo\u00e3o", ""], ["Pregui\u00e7a", "Nuno", ""], ["Rodrigues", "Rodrigo", ""], ["Dias", "Ricardo J.", ""], ["Louren\u00e7o", "Jo\u00e3o M.", ""]]}, {"id": "2007.10385", "submitter": "Xing Shi", "authors": "Xing Shi and Chao Wang", "title": "Support Aggregate Analytic Window Function over Large Data by Spilling", "comments": "6 pages, conference", "journal-ref": null, "doi": "10.1088/1742-6596/1673/1/012001", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analytic function, also called window function, is to query the aggregation\nof data over a sliding window. For example, a simple query over the online\nstock platform is to return the average price of a stock of the last three\ndays. These functions are commonly used features in SQL databases. They are\nsupported in most of the commercial databases. With the increasing usage of\ncloud data infra and machine learning technology, the frequency of queries with\nanalytic window functions rises. Some analytic functions only require const\nspace in memory to store the state, such as SUM, AVG, while others require\nlinear space, such as MIN, MAX. When the window is extremely large, the memory\nspace to store the state may be too large. In this case, we need to spill the\nstate to disk, which is a heavy operation. In this paper, we proposed an\nalgorithm to manipulate the state data in the disk to reduce the disk I/O to\nmake spill available and efficiency. We analyze the complexity of the algorithm\nwith different data distribution.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 18:12:21 GMT"}], "update_date": "2020-12-30", "authors_parsed": [["Shi", "Xing", ""], ["Wang", "Chao", ""]]}, {"id": "2007.10568", "submitter": "Chi Zhang", "authors": "Chi Zhang, Ryan Marcus, Anat Kleiman, Olga Papaemmanouil", "title": "Buffer Pool Aware Query Scheduling via Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this extended abstract, we propose a new technique for query scheduling\nwith the explicit goal of reducing disk reads and thus implicitly increasing\nquery performance. We introduce \\system, a learned scheduler that leverages\noverlapping data reads among incoming queries and learns a scheduling strategy\nthat improves cache hits. \\system relies on deep reinforcement learning to\nproduce workload-specific scheduling strategies that focus on long-term\nperformance benefits while being adaptive to previously-unseen data access\npatterns. We present results from a proof-of-concept prototype, demonstrating\nthat learned schedulers can offer significant performance improvements over\nhand-crafted scheduling heuristics. Ultimately, we make the case that this is a\npromising research direction in the intersection of machine learning and\ndatabases.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 02:28:59 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Zhang", "Chi", ""], ["Marcus", "Ryan", ""], ["Kleiman", "Anat", ""], ["Papaemmanouil", "Olga", ""]]}, {"id": "2007.10781", "submitter": "Alaettin Zubaro\\u{g}lu", "authors": "Alaettin Zubaro\\u{g}lu and Volkan Atalay", "title": "Data Stream Clustering: A Review", "comments": "Has been accepted for publication in Artificial Intelligence Review", "journal-ref": null, "doi": "10.1007/s10462-020-09874-x", "report-no": null, "categories": "cs.LG cs.AI cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Number of connected devices is steadily increasing and these devices\ncontinuously generate data streams. Real-time processing of data streams is\narousing interest despite many challenges. Clustering is one of the most\nsuitable methods for real-time data stream processing, because it can be\napplied with less prior information about the data and it does not need labeled\ninstances. However, data stream clustering differs from traditional clustering\nin many aspects and it has several challenging issues. Here, we provide\ninformation regarding the concepts and common characteristics of data streams,\nsuch as concept drift, data structures for data streams, time window models and\noutlier detection. We comprehensively review recent data stream clustering\nalgorithms and analyze them in terms of the base clustering technique,\ncomputational complexity and clustering accuracy. A comparison of these\nalgorithms is given along with still open problems. We indicate popular data\nstream repositories and datasets, stream processing tools and platforms. Open\nproblems about data stream clustering are also discussed.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 20:35:09 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Zubaro\u011flu", "Alaettin", ""], ["Atalay", "Volkan", ""]]}, {"id": "2007.10938", "submitter": "Francesca Noardo", "authors": "Francesca Noardo, Ken Arroyo Ohori, Filip Biljecki, Claire Ellul, Lars\n  Harrie, Thomas Krijnen, Helen Eriksson, Jordi van Liempt, Maria Pla, Antonio\n  Ruiz, Dean Hintz, Nina Krueger, Cristina Leoni, Leire Leoz, Diana Moraru,\n  Stelios Vitalis, Philipp Willkomm, Jantien Stoter", "title": "Reference study of CityGML software support: the GeoBIM benchmark 2019\n  -- Part II", "comments": "preprint of the paper", "journal-ref": "Transactions in GIS, ISSN: 1361-1682, 2020", "doi": "10.1111/tgis.12710", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  OGC CityGML is an open standard for 3D city models intended to foster\ninteroperability and support various applications. However, through our\npractical experience and discussions with practitioners, we have noticed\nseveral problems related to the implementation of the standard and the use of\nstandardized data. Nevertheless, a systematic investigation of these issues has\nnever been performed, and there is thus insufficient evidence that can be used\nfor tackling the problems. The GeoBIM benchmark project is aimed at finding\nsuch evidence by involving external volunteers, reporting on tools behaviour\nabout relevant aspects (geometry, semantics, georeferencing, functionalities),\nanalysed and described in this paper. This study explicitly pointed out the\ncritical points embedded in the format as an evidence base for future\ndevelopment. This paper is in tandem with Part I, describing the results of the\nbenchmark related to IFC, counterpart of CityGML within building information\nmodelling.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 16:55:36 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 17:28:40 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Noardo", "Francesca", ""], ["Ohori", "Ken Arroyo", ""], ["Biljecki", "Filip", ""], ["Ellul", "Claire", ""], ["Harrie", "Lars", ""], ["Krijnen", "Thomas", ""], ["Eriksson", "Helen", ""], ["van Liempt", "Jordi", ""], ["Pla", "Maria", ""], ["Ruiz", "Antonio", ""], ["Hintz", "Dean", ""], ["Krueger", "Nina", ""], ["Leoni", "Cristina", ""], ["Leoz", "Leire", ""], ["Moraru", "Diana", ""], ["Vitalis", "Stelios", ""], ["Willkomm", "Philipp", ""], ["Stoter", "Jantien", ""]]}, {"id": "2007.10951", "submitter": "Francesca Noardo", "authors": "Francesca Noardo, Thomas Krijnen, Ken Arroyo Ohori, Filip Biljecki,\n  Claire Ellul, Lars Harrie, Helen Eriksson, Lorenzo Polia, Nebras Salheb,\n  Helga Tauscher, Jordi van Liempt, Hendrik Goerne, Dean Hintz, Tim Kaiser,\n  Cristina Leoni, Artur Warchol, Jantien Stoter", "title": "Reference study of IFC software support: the GeoBIM benchmark 2019 --\n  Part I", "comments": "Pre-print of the paper. arXiv admin note: text overlap with\n  arXiv:2007.10938", "journal-ref": "Transactions in GIS, 2021", "doi": "10.1111/tgis.12709", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  IFC, buildingSMART open standard for Building Information Models, is\nunderused with respect to its promising potential, since, according to the\nexperience of practitioners and researchers working with BIM, issues in the\nstandard's implementation and use prevent its effective use. Nevertheless, a\nsystematic investigation of these issues has never been performed, and there is\nthus insufficient evidence for tackling the problems. The GeoBIM benchmark\nproject is aimed at finding such evidence by involving external volunteers,\nreporting on tools behaviour about relevant aspects (geometry, semantics,\ngeoreferencing, functionalities), analysed and described in this paper.\nInterestingly, few patterns are detectable about the behaviour of different\nsoftware with standardised data, and significant issues are found in their\nsupport of the standards, probably due to the very high complexity of the\nstandard data model. This paper is in tandem with Part II, describing the\nresults of the benchmark related to CityGML, counterpart of IFC within\ngeoinformation.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 17:11:21 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 17:47:04 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Noardo", "Francesca", ""], ["Krijnen", "Thomas", ""], ["Ohori", "Ken Arroyo", ""], ["Biljecki", "Filip", ""], ["Ellul", "Claire", ""], ["Harrie", "Lars", ""], ["Eriksson", "Helen", ""], ["Polia", "Lorenzo", ""], ["Salheb", "Nebras", ""], ["Tauscher", "Helga", ""], ["van Liempt", "Jordi", ""], ["Goerne", "Hendrik", ""], ["Hintz", "Dean", ""], ["Kaiser", "Tim", ""], ["Leoni", "Cristina", ""], ["Warchol", "Artur", ""], ["Stoter", "Jantien", ""]]}, {"id": "2007.11112", "submitter": "Christos Kozyrakis", "authors": "Michael Cafarella and David DeWitt and Vijay Gadepally and Jeremy\n  Kepner and Christos Kozyrakis and Tim Kraska and Michael Stonebraker and\n  Matei Zaharia", "title": "DBOS: A Proposal for a Data-Centric Operating System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS cs.AR cs.DB cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current operating systems are complex systems that were designed before\ntoday's computing environments. This makes it difficult for them to meet the\nscalability, heterogeneity, availability, and security challenges in current\ncloud and parallel computing environments. To address these problems, we\npropose a radically new OS design based on data-centric architecture: all\noperating system state should be represented uniformly as database tables, and\noperations on this state should be made via queries from otherwise stateless\ntasks. This design makes it easy to scale and evolve the OS without\nwhole-system refactoring, inspect and debug system state, upgrade components\nwithout downtime, manage decisions using machine learning, and implement\nsophisticated security features. We discuss how a database OS (DBOS) can\nimprove the programmability and performance of many of today's most important\napplications and propose a plan for the development of a DBOS proof of concept.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 22:01:00 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Cafarella", "Michael", ""], ["DeWitt", "David", ""], ["Gadepally", "Vijay", ""], ["Kepner", "Jeremy", ""], ["Kozyrakis", "Christos", ""], ["Kraska", "Tim", ""], ["Stonebraker", "Michael", ""], ["Zaharia", "Matei", ""]]}, {"id": "2007.11651", "submitter": "Tin Vu", "authors": "Tin Vu, Ahmed Eldawy", "title": "R*-Grove: Balanced Spatial Partitioning for Large-scale Datasets", "comments": "29 pages, to be published in Frontiers in Big Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid growth of big spatial data urged the research community to develop\nseveral big spatial data systems. Regardless of their architecture, one of the\nfundamental requirements of all these systems is to spatially partition the\ndata efficiently across machines. The core challenges of big spatial\npartitioning are building high spatial quality partitions while simultaneously\ntaking advantages of distributed processing models by providing load balanced\npartitions. Previous works on big spatial partitioning are to reuse existing\nindex search trees as-is, e.g., the R-tree family, STR, Kd-tree, and Quad-tree,\nby building a temporary tree for a sample of the input and use its leaf nodes\nas partition boundaries. However, we show in this paper that none of those\ntechniques has addressed the mentioned challenges completely. This paper\nproposes a novel partitioning method, termed R*-Grove, which can partition very\nlarge spatial datasets into high quality partitions with excellent load balance\nand block utilization. This appealing property allows R*-Grove to outperform\nexisting techniques in spatial query processing. R*-Grove can be easily\nintegrated into any big data platforms such as Apache Spark or Apache Hadoop.\nOur experiments show that R*-Grove outperforms the existing partitioning\ntechniques for big spatial data systems. With all the proposed work publicly\navailable as open source, we envision that R*-Grove will be adopted by the\ncommunity to better serve big spatial data research.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 20:08:41 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Vu", "Tin", ""], ["Eldawy", "Ahmed", ""]]}, {"id": "2007.11659", "submitter": "Gianluca Demartini", "authors": "Edgar Meij, Tara Safavi, Chenyan Xiong, Gianluca Demartini, Miriam\n  Redi, Fatma \\\"Ozcan", "title": "Proceedings of the KG-BIAS Workshop 2020 at AKBC 2020", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The KG-BIAS 2020 workshop touches on biases and how they surface in knowledge\ngraphs (KGs), biases in the source data that is used to create KGs, methods for\nmeasuring or remediating bias in KGs, but also identifying other biases such as\nhow and which languages are represented in automatically constructed KGs or how\npersonal KGs might incur inherent biases. The goal of this workshop is to\nuncover how various types of biases are introduced into KGs, investigate how to\nmeasure, and propose methods to remediate them.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 22:37:57 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Meij", "Edgar", ""], ["Safavi", "Tara", ""], ["Xiong", "Chenyan", ""], ["Demartini", "Gianluca", ""], ["Redi", "Miriam", ""], ["\u00d6zcan", "Fatma", ""]]}, {"id": "2007.11881", "submitter": "Xiaolong Wan", "authors": "Xiaolong Wan, Hongzhi Wang", "title": "Reachability Queries with Label and Substructure Constraints on\n  Knowledge Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since knowledge graphs (KGs) describe and model the relationships between\nentities and concepts in the real world, reasoning on KGs often correspond to\nthe reachability queries with label and substructure constraints (LSCR).\nSpecially, for a search path p, LSCR queries not only require that the labels\nof the edges passed by p are in a certain label set, but also claim that a\nvertex in p could satisfy a certain substructure constraint. LSCR queries is\nmuch more complex than the label-constraint reachability (LCR) queries, and\nthere is no efficient solution for LSCR queries on KGs, to the best of our\nknowledge. Motivated by this, we introduce two solutions for such queries on\nKGs, UIS and INS. The former can also be utilized for general edge-labeled\ngraphs, and is relatively handy for practical implementation. The latter is an\nefficient local-index-based informed search strategy. An extensive experimental\nevaluation, on both synthetic and real KGs, illustrates that our solutions can\nefficiently process LSCR queries on KGs.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 09:40:50 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Wan", "Xiaolong", ""], ["Wang", "Hongzhi", ""]]}, {"id": "2007.12488", "submitter": "Ioana Manolescu", "authors": "Oana Balalau (CEDAR), Catarina Concei\\c{c}{\\~a}o (INESC-ID, IST),\n  Helena Galhardas (INESC-ID, IST), Ioana Manolescu (CEDAR), Tayeb Merabti\n  (CEDAR), Jingmao You (CEDAR, IP Paris), Youssr Youssef (CEDAR, ENSAE, IP\n  Paris)", "title": "Graph integration of structured, semistructured and unstructured data\n  for data journalism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, journalism is facilitated by the existence of large amounts of\ndigital data sources, including many Open Data ones. Such data sources are\nextremely heterogeneous, ranging from highly struc-tured (relational\ndatabases), semi-structured (JSON, XML, HTML), graphs (e.g., RDF), and text.\nJournalists (and other classes of users lacking advanced IT expertise, such as\nmost non-governmental-organizations, or small public administrations) need to\nbe able to make sense of such heterogeneous corpora, even if they lack the\nability to de ne and deploy custom extract-transform-load work ows. These are\ndi cult to set up not only for arbitrary heterogeneous inputs , but also given\nthat users may want to add (or remove) datasets to (from) the corpus. We\ndescribe a complete approach for integrating dynamic sets of heterogeneous data\nsources along the lines described above: the challenges we faced to make such\ngraphs useful, allow their integration to scale, and the solutions we proposed\nfor these problems. Our approach is implemented within the ConnectionLens\nsystem; we validate it through a set of experiments.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 08:55:09 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 08:07:09 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Balalau", "Oana", "", "CEDAR"], ["Concei\u00e7{\u00e3}o", "Catarina", "", "INESC-ID, IST"], ["Galhardas", "Helena", "", "INESC-ID, IST"], ["Manolescu", "Ioana", "", "CEDAR"], ["Merabti", "Tayeb", "", "CEDAR"], ["You", "Jingmao", "", "CEDAR, IP Paris"], ["Youssef", "Youssr", "", "CEDAR, ENSAE, IP\n  Paris"]]}, {"id": "2007.12799", "submitter": "Leopoldo Bertossi", "authors": "Leopoldo Bertossi", "title": "Score-Based Explanations in Data Management and Machine Learning", "comments": "Companion paper for a tutorial at the Scalable Uncertainty Management\n  Conference (SUM'20). To appear in Proc. SUM'20. Minor fixes made", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe some approaches to explanations for observed outcomes in data\nmanagement and machine learning. They are based on the assignment of numerical\nscores to predefined and potentially relevant inputs. More specifically, we\nconsider explanations for query answers in databases, and for results from\nclassification models. The described approaches are mostly of a causal and\ncounterfactual nature. We argue for the need to bring domain and semantic\nknowledge into score computations; and suggest some ways to do this.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 23:13:27 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 01:53:53 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Bertossi", "Leopoldo", ""]]}, {"id": "2007.13005", "submitter": "Daniel Kang", "authors": "Daniel Kang, Ankit Mathur, Teja Veeramacheneni, Peter Bailis, Matei\n  Zaharia", "title": "Jointly Optimizing Preprocessing and Inference for DNN-based Visual\n  Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep neural networks (DNNs) are an increasingly popular way to query\nlarge corpora of data, their significant runtime remains an active area of\nresearch. As a result, researchers have proposed systems and optimizations to\nreduce these costs by allowing users to trade off accuracy and speed. In this\nwork, we examine end-to-end DNN execution in visual analytics systems on modern\naccelerators. Through a novel measurement study, we show that the preprocessing\nof data (e.g., decoding, resizing) can be the bottleneck in many visual\nanalytics systems on modern hardware.\n  To address the bottleneck of preprocessing, we introduce two optimizations\nfor end-to-end visual analytics systems. First, we introduce novel methods of\nachieving accuracy and throughput trade-offs by using natively present,\nlow-resolution visual data. Second, we develop a runtime engine for efficient\nvisual DNN inference. This runtime engine a) efficiently pipelines\npreprocessing and DNN execution for inference, b) places preprocessing\noperations on the CPU or GPU in a hardware- and input-aware manner, and c)\nefficiently manages memory and threading for high throughput execution. We\nimplement these optimizations in a novel system, Smol, and evaluate Smol on\neight visual datasets. We show that its optimizations can achieve up to 5.9x\nend-to-end throughput improvements at a fixed accuracy over recent work in\nvisual analytics.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 20:26:05 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Kang", "Daniel", ""], ["Mathur", "Ankit", ""], ["Veeramacheneni", "Teja", ""], ["Bailis", "Peter", ""], ["Zaharia", "Matei", ""]]}, {"id": "2007.13053", "submitter": "Yanhong Annie Liu", "authors": "Yanhong A. Liu and Scott D. Stoller", "title": "Recursive Rules with Aggregation: A Simple Unified Semantics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex reasoning problems are most clearly and easily specified using\nlogical rules, especially recursive rules with aggregation such as counts and\nsums for practical applications. Unfortunately, the meaning of such rules has\nbeen a significant challenge, leading to many different conflicting semantics.\n  This paper describes a unified semantics for recursive rules with\naggregation, extending the unified founded semantics and constraint semantics\nfor recursive rules with negation. The key idea is to support simple expression\nof the different assumptions underlying different semantics, and orthogonally\ninterpret aggregation operations straightforwardly using their simple usual\nmeaning.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 04:42:44 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Liu", "Yanhong A.", ""], ["Stoller", "Scott D.", ""]]}, {"id": "2007.14009", "submitter": "Esther Galbrun", "authors": "Esther Galbrun", "title": "The Minimum Description Length Principle for Pattern Mining: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is about the Minimum Description Length (MDL) principle applied to\npattern mining. The length of this description is kept to the minimum.\n  Mining patterns is a core task in data analysis and, beyond issues of\nefficient enumeration, the selection of patterns constitutes a major challenge.\nThe MDL principle, a model selection method grounded in information theory, has\nbeen applied to pattern mining with the aim to obtain compact high-quality sets\nof patterns. After giving an outline of relevant concepts from information\ntheory and coding, as well as of work on the theory behind the MDL and similar\nprinciples, we review MDL-based methods for mining various types of data and\npatterns. Finally, we open a discussion on some issues regarding these methods,\nand highlight currently active related data analysis problems.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 06:24:39 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 13:43:54 GMT"}, {"version": "v3", "created": "Wed, 28 Jul 2021 05:19:57 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Galbrun", "Esther", ""]]}, {"id": "2007.14169", "submitter": "Matthias Lanzinger", "authors": "Hubie Chen, Georg Gottlob, Matthias Lanzinger, Reinhard Pichler", "title": "Semantic Width and the Fixed-Parameter Tractability of Constraint\n  Satisfaction Problems", "comments": "Full and extended version of the IJCAI2020 paper with the same title", "journal-ref": null, "doi": "10.24963/ijcai.2020/239", "report-no": null, "categories": "cs.CC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constraint satisfaction problems (CSPs) are an important formal framework for\nthe uniform treatment of various prominent AI tasks, e.g., coloring or\nscheduling problems. Solving CSPs is, in general, known to be NP-complete and\nfixed-parameter intractable when parameterized by their constraint scopes. We\ngive a characterization of those classes of CSPs for which the problem becomes\nfixed-parameter tractable.\n  Our characterization significantly increases the utility of the CSP framework\nby making it possible to decide the fixed-parameter tractability of problems\nvia their CSP formulations.\n  We further extend our characterization to the evaluation of unions of\nconjunctive queries, a fundamental problem in databases. Furthermore, we\nprovide some new insight on the frontier of PTIME solvability of CSPs.\n  In particular, we observe that bounded fractional hypertree width is more\ngeneral than bounded hypertree width only for classes that exhibit a certain\ntype of exponential growth.\n  The presented work resolves a long-standing open problem and yields powerful\nnew tools for complexity research in AI and database theory.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 12:44:03 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Chen", "Hubie", ""], ["Gottlob", "Georg", ""], ["Lanzinger", "Matthias", ""], ["Pichler", "Reinhard", ""]]}, {"id": "2007.14244", "submitter": "Gabriel Paludo Licks", "authors": "Gabriel Paludo Licks and Felipe Meneguzzi", "title": "Automated Database Indexing using Model-free Reinforcement Learning", "comments": "8 pages, 5 figures (some have subfigures), 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Configuring databases for efficient querying is a complex task, often carried\nout by a database administrator. Solving the problem of building indexes that\ntruly optimize database access requires a substantial amount of database and\ndomain knowledge, the lack of which often results in wasted space and memory\nfor irrelevant indexes, possibly jeopardizing database performance for querying\nand certainly degrading performance for updating. We develop an architecture to\nsolve the problem of automatically indexing a database by using reinforcement\nlearning to optimize queries by indexing data throughout the lifetime of a\ndatabase. In our experimental evaluation, our architecture shows superior\nperformance compared to related work on reinforcement learning and genetic\nalgorithms, maintaining near-optimal index configurations and efficiently\nscaling to large databases.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 14:36:55 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Licks", "Gabriel Paludo", ""], ["Meneguzzi", "Felipe", ""]]}, {"id": "2007.14569", "submitter": "Long Gong", "authors": "Long Gong, Ziheng Liu, Liang Liu, Jun Xu, Mitsunori Ogihara, Tong Yang", "title": "Space- and Computationally-Efficient Set Reconciliation via Parity\n  Bitmap Sketch (PBS)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Set reconciliation is a fundamental algorithmic problem that arises in many\nnetworking, system, and database applications. In this problem, two large sets\nA and B of objects (bitcoins, files, records, etc.) are stored respectively at\ntwo different network-connected hosts, which we name Alice and Bob\nrespectively. Alice and Bob communicate with each other to learn $A\\Delta B$,\nthe difference between A and B, and as a result the reconciled set $A\\bigcup\nB$.\n  Current set reconciliation schemes are based on either Invertible Bloom\nFilters (IBF) or Error-Correction Codes (ECC). The former has a low\ncomputational complexity of O(d), where d is the cardinality of $A\\Delta B$,\nbut has a high communication overhead that is several times larger than the\ntheoretical minimum. The latter has a low communication overhead close to the\ntheoretical minimum, but has a much higher computational complexity of\n$O(d^2)$. In this work, we propose Parity Bitmap Sketch (PBS), an ECC- based\nset reconciliation scheme that gets the better of both worlds: PBS has both a\nlow computational complexity of O(d) just like IBF-based solutions and a low\ncommunication overhead of roughly twice the theoretical minimum. A separate\ncontribution of this work is a novel rigorous analytical framework that can be\nused for the precise calculation of various performance metrics and for the\nnear-optimal parameter tuning of PBS.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 03:15:23 GMT"}, {"version": "v2", "created": "Sun, 9 Aug 2020 06:29:14 GMT"}, {"version": "v3", "created": "Sat, 15 Aug 2020 22:47:02 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Gong", "Long", ""], ["Liu", "Ziheng", ""], ["Liu", "Liang", ""], ["Xu", "Jun", ""], ["Ogihara", "Mitsunori", ""], ["Yang", "Tong", ""]]}, {"id": "2007.14864", "submitter": "Arnab Bhattacharya", "authors": "Garima Gaur and Arnab Bhattacharya and Srikanta Bedathur", "title": "How and Why is An Answer (Still) Correct? Maintaining Provenance in\n  Dynamic Knowledge Graphs", "comments": null, "journal-ref": "CIKM 2020", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graphs (KGs) have increasingly become the backbone of many critical\nknowledge-centric applications. Most large-scale KGs used in practice are\nautomatically constructed based on an ensemble of extraction techniques applied\nover diverse data sources. Therefore, it is important to establish the\nprovenance of results for a query to determine how these were computed.\nProvenance is shown to be useful for assigning confidence scores to the\nresults, for debugging the KG generation itself, and for providing answer\nexplanations. In many such applications, certain queries are registered as\nstanding queries since their answers are needed often. However, KGs keep\ncontinuously changing due to reasons such as changes in the source data,\nimprovements to the extraction techniques, refinement/enrichment of\ninformation, and so on. This brings us to the issue of efficiently maintaining\nthe provenance polynomials of complex graph pattern queries for dynamic and\nlarge KGs instead of having to recompute them from scratch each time the KG is\nupdated. Addressing these issues, we present HUKA which uses provenance\npolynomials for tracking the derivation of query results over knowledge graphs\nby encoding the edges involved in generating the answer. More importantly, HUKA\nalso maintains these provenance polynomials in the face of updates---insertions\nas well as deletions of facts---to the underlying KG. Experimental results over\nlarge real-world KGs such as YAGO and DBpedia with various benchmark SPARQL\nquery workloads reveals that HUKA can be almost 50 times faster than existing\nsystems for provenance computation on dynamic KGs.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 14:32:19 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Gaur", "Garima", ""], ["Bhattacharya", "Arnab", ""], ["Bedathur", "Srikanta", ""]]}, {"id": "2007.14997", "submitter": "Xing Shi", "authors": "Xing Shi and Chao Wang", "title": "Aggregate Analytic Window Query over Spatial Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analytic window query is a commonly used query in the relational databases.\nIt answers the aggregations of data over a sliding window. For example, to get\nthe average prices of a stock for each day. However, it is not supported in the\nspatial databases. Because the spatial data are not in a one-dimension space,\nthere is no straightforward way to extend the original analytic window query to\nspatial databases. But these queries are useful and meaningful. For example, to\nfind the average number of visits for all the POIs in the circle with a fixed\nradius for each POI as the centre. In this paper, we define the aggregate\nanalytic window query over spatial data and propose algorithms for grid index\nand tree-index. We also analyze the complexity of the algorithms to prove they\nare efficient and practical.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 09:47:54 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Shi", "Xing", ""], ["Wang", "Chao", ""]]}, {"id": "2007.15280", "submitter": "Xi Victoria Lin", "authors": "Jichuan Zeng, Xi Victoria Lin, Caiming Xiong, Richard Socher, Michael\n  R. Lyu, Irwin King, Steven C.H. Hoi", "title": "Photon: A Robust Cross-Domain Text-to-SQL System", "comments": "ACL 2020 system demonstration paper extended . The first two authors\n  contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language interfaces to databases (NLIDB) democratize end user access\nto relational data. Due to fundamental differences between natural language\ncommunication and programming, it is common for end users to issue questions\nthat are ambiguous to the system or fall outside the semantic scope of its\nunderlying query language. We present Photon, a robust, modular, cross-domain\nNLIDB that can flag natural language input to which a SQL mapping cannot be\nimmediately determined. Photon consists of a strong neural semantic parser\n(63.2\\% structure accuracy on the Spider dev benchmark), a human-in-the-loop\nquestion corrector, a SQL executor and a response generator. The question\ncorrector is a discriminative neural sequence editor which detects confusion\nspan(s) in the input question and suggests rephrasing until a translatable\ninput is given by the user or a maximum number of iterations are conducted.\nExperiments on simulated data show that the proposed method effectively\nimproves the robustness of text-to-SQL system against untranslatable user\ninput. The live demo of our system is available at http://naturalsql.com.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 07:44:48 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 08:59:06 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Zeng", "Jichuan", ""], ["Lin", "Xi Victoria", ""], ["Xiong", "Caiming", ""], ["Socher", "Richard", ""], ["Lyu", "Michael R.", ""], ["King", "Irwin", ""], ["Hoi", "Steven C. H.", ""]]}, {"id": "2007.15634", "submitter": "Ralph Foorthuis", "authors": "Ralph Foorthuis", "title": "On the Nature and Types of Anomalies: A Review of Deviations in Data", "comments": "39 pages (30 pages content), 10 figures and 3 tables. Preprint;\n  comments will be appreciated. Improvements in version 3: Added new anomaly\n  subtypes; Tightening of definitions; Additional examples from new literature;\n  Various minor additions and improvements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LG stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomalies are occurrences in a dataset that are in some way unusual and do\nnot fit the general patterns. The concept of the anomaly is usually ill-defined\nand perceived as vague and domain-dependent. Moreover, despite some 250 years\nof publications on the topic, no comprehensive and concrete overviews of the\ndifferent types of anomalies have hitherto been published. By means of an\nextensive literature review this study therefore offers the first theoretically\nprincipled and domain-independent typology of data anomalies, and presents a\nfull overview of anomaly types and subtypes. To concretely define the concept\nof the anomaly and its different manifestations, the typology employs five\ndimensions: data type, cardinality of relationship, anomaly level, data\nstructure, and data distribution. These fundamental and data-centric dimensions\nnaturally yield 3 broad groups, 9 basic types and 63 subtypes of anomalies. The\ntypology facilitates the evaluation of the functional capabilities of anomaly\ndetection algorithms, contributes to explainable data science, and provides\ninsights into relevant topics such as local versus global anomalies.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 17:55:11 GMT"}, {"version": "v2", "created": "Sun, 27 Dec 2020 22:15:50 GMT"}, {"version": "v3", "created": "Sat, 15 May 2021 11:45:58 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Foorthuis", "Ralph", ""]]}, {"id": "2007.15904", "submitter": "Wenbo Tao", "authors": "Wenbo Tao, Xinli Hou, Adam Sah, Leilani Battle, Remco Chang and\n  Michael Stonebraker", "title": "Kyrix-S: Authoring Scalable Scatterplot Visualizations of Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Static scatterplots often suffer from the overdraw problem on big datasets\nwhere object overlap causes undesirable visual clutter. The use of zooming in\nscatterplots can help alleviate this problem. With multiple zoom levels, more\nscreen real estate is available, allowing objects to be placed in a less\ncrowded way. We call this type of visualization scalable scatterplot\nvisualizations, or SSV for short. Despite the potential of SSVs, existing\nsystems and toolkits fall short in supporting the authoring of SSVs due to\nthree limitations. First, many systems have limited scalability, assuming that\ndata fits in the memory of one computer. Second, too much developer work, e.g.,\nusing custom code to generate mark layouts or render objects, is required.\nThird, many systems focus on only a small subset of the SSV design space (e.g.\nsupporting a specific type of visual marks). To address these limitations, we\nhave developed Kyrix-S, a system for easy authoring of SSVs at scale. Kyrix-S\nderives a declarative grammar that enables specification of a variety of SSVs\nin a few tens of lines of code, based on an existing survey of scatterplot\ntasks and designs. The declarative grammar is supported by a distributed layout\nalgorithm which automatically places visual marks onto zoom levels. We store\ndata in a multi-node database and use multi-node spatial indexes to achieve\ninteractive browsing of large SSVs. Extensive experiments show that 1) Kyrix-S\nenables interactive browsing of SSVs of billions of objects, with response\ntimes under 500ms and 2) Kyrix-S achieves 4X-9X reduction in specification\ncompared to a state-of-the-art authoring system.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 08:36:12 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Tao", "Wenbo", ""], ["Hou", "Xinli", ""], ["Sah", "Adam", ""], ["Battle", "Leilani", ""], ["Chang", "Remco", ""], ["Stonebraker", "Michael", ""]]}, {"id": "2007.16040", "submitter": "Simon Schiff", "authors": "Simon Schiff and \\\"Ozg\\\"ur \\\"Ozcep", "title": "Bounded-Memory Criteria for Streams with Application Time", "comments": "11 pages, 2 figures", "journal-ref": "Proceedings of the Thirty-Third International Florida Artificial\n  Intelligence Research Society Conference (2019) 148-153", "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bounded-memory computability continues to be in the focus of those areas of\nAI and databases that deal with feasible computations over streams---be it\nfeasible arithmetical calculations on low-level streams or feasible query\nanswering for declaratively specified queries on relational data streams or\neven feasible query answering for high-level queries on streams w.r.t. a set of\nconstraints in an ontology such as in the paradigm of Ontology-Based Data\nAccess (OBDA). In classical OBDA, a high-level query is answered by\ntransforming it into a query on data source level. The transformation requires\na rewriting step, where knowledge from an ontology is incorporated into the\nquery, followed by an unfolding step with respect to a set of mappings. Given\nan OBDA setting it is very difficult to decide, whether and how a query can be\nanswered efficiently. In particular it is difficult to decide whether a query\ncan be answered in bounded memory, i.e., in constant space w.r.t. an infinitely\ngrowing prefix of a data stream. This work presents criteria for bounded-memory\ncomputability of select-project-join (SPJ) queries over streams with\napplication time. Deciding whether an SPJ query can be answered in constant\nspace is easier than for high-level queries, as neither an ontology nor a set\nof mappings are part of the input. Using the transformation process of\nclassical OBDA, these criteria then can help deciding the efficiency of\nanswering high-level queries on streams.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 12:05:04 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Schiff", "Simon", ""], ["\u00d6zcep", "\u00d6zg\u00fcr", ""]]}, {"id": "2007.16079", "submitter": "Silvio Peroni", "authors": "Marilena Daquino, Ivan Heibi, Silvio Peroni, David Shotton", "title": "Creating RESTful APIs over SPARQL endpoints using RAMOSE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semantic Web technologies are widely used for storing RDF data and making\nthem available on the Web through SPARQL endpoints, queryable using the SPARQL\nquery language. While the use of SPARQL endpoints is strongly supported by\nSemantic Web experts, it hinders broader use of RDF data by common Web users,\nengineers and developers unfamiliar with Semantic Web technologies, who\nnormally rely on Web RESTful APIs for querying Web-available data and creating\napplications over them. To solve this problem, we have developed RAMOSE, a\ngeneric tool developed in Python to create REST APIs over SPARQL endpoints.\nThrough the creation of source-specific textual configuration files, RAMOSE\nenables the querying of SPARQL endpoints via simple Web RESTful API calls that\nreturn either JSON or CSV-formatted data, thus hiding all the intrinsic\ncomplexities of SPARQL and RDF from common Web users. We provide evidence that\nthe use of RAMOSE to provide REST API access to RDF data within OpenCitations\ntriplestores is beneficial in terms of the number of queries made by external\nusers to such RDF data using the RAMOSE API compared with the direct access via\nthe SPARQL endpoint. Our findings show the importance for suppliers of RDF data\nof having an alternative API access service, which enables its use by those\nwith no (or little) experience in Semantic Web technologies and the SPARQL\nquery language. RAMOSE can be used both to query any SPARQL endpoint and to\nquery any other Web API, and thus it represents an easy generic technical\nsolution for service providers who wish to create an API service to access\nLinked Data stored as RDF in a conventional triplestore.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 13:53:29 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 11:29:29 GMT"}, {"version": "v3", "created": "Tue, 9 Mar 2021 15:06:02 GMT"}, {"version": "v4", "created": "Sun, 30 May 2021 14:17:17 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Daquino", "Marilena", ""], ["Heibi", "Ivan", ""], ["Peroni", "Silvio", ""], ["Shotton", "David", ""]]}, {"id": "2007.16208", "submitter": "Flora D. Salim", "authors": "Kyle K. Qin, Flora D. Salim, Yongli Ren, Wei Shao, Mark Heimann, Danai\n  Koutra", "title": "G-CREWE: Graph CompREssion With Embedding for Network Alignment", "comments": "10 pages, accepted at the 29th ACM International Conference\n  onInformation and Knowledge Management (CIKM 20)", "journal-ref": null, "doi": "10.1145/3340531.3411924", "report-no": null, "categories": "cs.SI cs.DB cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network alignment is useful for multiple applications that require\nincreasingly large graphs to be processed. Existing research approaches this as\nan optimization problem or computes the similarity based on node\nrepresentations. However, the process of aligning every pair of nodes between\nrelatively large networks is time-consuming and resource-intensive. In this\npaper, we propose a framework, called G-CREWE (Graph CompREssion With\nEmbedding) to solve the network alignment problem. G-CREWE uses node embeddings\nto align the networks on two levels of resolution, a fine resolution given by\nthe original network and a coarse resolution given by a compressed version, to\nachieve an efficient and effective network alignment. The framework first\nextracts node features and learns the node embedding via a Graph Convolutional\nNetwork (GCN). Then, node embedding helps to guide the process of graph\ncompression and finally improve the alignment performance. As part of G-CREWE,\nwe also propose a new compression mechanism called MERGE (Minimum dEgRee\nneiGhbors comprEssion) to reduce the size of the input networks while\npreserving the consistency in their topological structure. Experiments on all\nreal networks show that our method is more than twice as fast as the most\ncompetitive existing methods while maintaining high accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 05:30:21 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Qin", "Kyle K.", ""], ["Salim", "Flora D.", ""], ["Ren", "Yongli", ""], ["Shao", "Wei", ""], ["Heimann", "Mark", ""], ["Koutra", "Danai", ""]]}]