[{"id": "1707.00186", "submitter": "Ahmad Siddiqui Dr", "authors": "Mohd Muntjir, Ahmad Tasnim Siddiqui", "title": "An Enhanced Framework with Advanced Study to Incorporate the Searching\n  of E-Commerce Products Using Modernization of Database Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study aims to inspect and evaluate the integration of database queries\nand their use in e-commerce product searches. It has been observed that\ne-commerce is one of the most prominent trends, which have been emerged in the\nbusiness world, for the past decade. E-commerce has gained tremendous\npopularity, as it offers higher flexibility, cost efficiency, effectiveness,\nand convenience, to both, consumers and businesses. Large number of retailing\ncompanies has adopted this technology, in order to expand their operations,\nacross of the globe; hence they needs to have highly responsive and integrated\ndatabases. In this regard, the approach of database queries is found to be the\nmost appropriate and adequate techniques, as it simplifies the searches of\ne-commerce products.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jul 2017 18:33:39 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Muntjir", "Mohd", ""], ["Siddiqui", "Ahmad Tasnim", ""]]}, {"id": "1707.00297", "submitter": "Minyar Sassi", "authors": "Mohamed Ali Zoghlami, Olfa Arfaoui, Minyar Sassi Hidri, Rahma Ben Ayed", "title": "Classification non supervis\\'ee des donn\\'ees h\\'et\\'erog\\`enes \\`a\n  large \\'echelle", "comments": "6 pages, in French, 8 figures", "journal-ref": "Conf\\'erence Internationale Francophone sur la Science de\n  Donn\\'ees - Les 23\\`emes Rencontres annuelles de la Soci\\'et\\'e Francophone\n  de Classification (AAFD & SFC), Marrakech, Maroc, pp. 37-42, 2016", "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When it comes to cluster massive data, response time, disk access and quality\nof formed classes becoming major issues for companies. It is in this context\nthat we have come to define a clustering framework for large scale\nheterogeneous data that contributes to the resolution of these issues. The\nproposed framework is based on, firstly, the descriptive analysis based on MCA,\nand secondly, the MapReduce paradigm in a large scale environment. The results\nare encouraging and prove the efficiency of the hybrid deployment on response\nquality and time component as on qualitative and quantitative data.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jul 2017 14:26:51 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Zoghlami", "Mohamed Ali", ""], ["Arfaoui", "Olfa", ""], ["Hidri", "Minyar Sassi", ""], ["Ayed", "Rahma Ben", ""]]}, {"id": "1707.00670", "submitter": "Piotr S. Maciag", "authors": "Piotr S. Maci\\k{a}g", "title": "Efficient Discovering of Top-K Sequential Patterns in Event-Based\n  Spatio-Temporal Data", "comments": "8 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of discovering sequential patterns from event-based\nspatio-temporal data. The dataset is described by a set of event types and\ntheir instances. Based on the given dataset, the task is to discover all\nsignificant sequential patterns denoting some attraction relation between event\ntypes occurring in a pattern. Already proposed algorithms discover all\nsignificant sequential patterns based on the significance threshold, which\nminimal value is given by an expert. Due to the nature of described data and\ncomplexity of discovered patterns, it may be very difficult to provide\nreasonable value of significance threshold. We consider the problem of\neffective discovering of K most important patterns in a given dataset (that is\ndiscovering of Top-K patterns).\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 17:47:10 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Maci\u0105g", "Piotr S.", ""]]}, {"id": "1707.00721", "submitter": "Vijay Gadepally", "authors": "Vijay Gadepally, Kyle OBrien, Adam Dziedzic, Aaron Elmore, Jeremy\n  Kepner, Samuel Madden, Tim Mattson, Jennie Rogers, Zuohao She, Michael\n  Stonebraker", "title": "Version 0.1 of the BigDAWG Polystore System", "comments": "Accepted to IEEE HPEC 2017", "journal-ref": null, "doi": "10.1109/HPEC.2017.8091077", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A polystore system is a database management system (DBMS) composed of\nintegrated heterogeneous database engines and multiple programming languages.\nBy matching data to the storage engine best suited to its needs, complex\nanalytics run faster and flexible storage choices helps improve data\norganization. BigDAWG (Big Data Working Group) is our reference implementation\nof a polystore system. In this paper, we describe the current BigDAWG software\nrelease which supports PostgreSQL, Accumulo and SciDB. We describe the overall\narchitecture, API and initial results of applying BigDAWG to the MIMIC II\nmedical dataset.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 18:24:24 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Gadepally", "Vijay", ""], ["OBrien", "Kyle", ""], ["Dziedzic", "Adam", ""], ["Elmore", "Aaron", ""], ["Kepner", "Jeremy", ""], ["Madden", "Samuel", ""], ["Mattson", "Tim", ""], ["Rogers", "Jennie", ""], ["She", "Zuohao", ""], ["Stonebraker", "Michael", ""]]}, {"id": "1707.00825", "submitter": "Juan Colmenares", "authors": "Juan A. Colmenares, Reza Dorrigiv and Daniel G. Waddington", "title": "Ingestion, Indexing and Retrieval of High-Velocity Multidimensional\n  Sensor Data on a Single Node", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multidimensional data are becoming more prevalent, partly due to the rise of\nthe Internet of Things (IoT), and with that the need to ingest and analyze data\nstreams at rates higher than before. Some industrial IoT applications require\ningesting millions of records per second, while processing queries on recently\ningested and historical data. Unfortunately, existing database systems suited\nto multidimensional data exhibit low per-node ingestion performance, and even\nif they can scale horizontally in distributed settings, they require large\nnumber of nodes to meet such ingest demands. For this reason, in this paper we\nevaluate a single-node multidimensional data store for high-velocity sensor\ndata. Its design centers around a two-level indexing structure, wherein the\nglobal index is an in-memory R*-tree and the local indices are serialized\nkd-trees. This study is confined to records with numerical indexing fields and\nrange queries, and covers ingest throughput, query response time, and storage\nfootprint. We show that the adopted design streamlines data ingestion and\noffers ingress rates two orders of magnitude higher than those of Percona\nServer, SQLite, and Druid. Our prototype also reports query response times\ncomparable to or better than those of Percona Server and Druid, and compares\nfavorably in terms of storage footprint. In addition, we evaluate a kd-tree\npartitioning based scheme for grouping incoming streamed data records. Compared\nto a random scheme, this scheme produces less overlap between groups of\nstreamed records, but contrary to what we expected, such reduced overlap does\nnot translate into better query performance. By contrast, the local indices\nprove much more beneficial to query performance. We believe the experience\nreported in this paper is valuable to practitioners and researchers alike\ninterested in building database systems for high-velocity multidimensional\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 06:30:37 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Colmenares", "Juan A.", ""], ["Dorrigiv", "Reza", ""], ["Waddington", "Daniel G.", ""]]}, {"id": "1707.00827", "submitter": "Domagoj Vrgo\\v{c}", "authors": "Francisco Maturana, Cristian Riveros and Domagoj Vrgo\\v{c}", "title": "Document Spanners for Extracting Incomplete Information: Expressiveness\n  and Complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rule-based information extraction has lately received a fair amount of\nattention from the database community, with several languages appearing in the\nlast few years. Although information extraction systems are intended to deal\nwith semistructured data, all language proposals introduced so far are designed\nto output relations, thus making them incapable of handling incomplete\ninformation. To remedy the situation, we propose to extend information\nextraction languages with the ability to use mappings, thus allowing us to work\nwith documents which have missing or optional parts. Using this approach, we\nsimplify the semantics of regex formulas and extraction rules, two previously\ndefined methods for extracting information, extend them with the ability to\nhandle incomplete data, and study how they compare in terms of expressive\npower. We also study computational properties of these languages, focusing on\nthe query enumeration problem, as well as satisfiability and containment.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 06:41:17 GMT"}, {"version": "v2", "created": "Fri, 29 Dec 2017 16:58:20 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["Maturana", "Francisco", ""], ["Riveros", "Cristian", ""], ["Vrgo\u010d", "Domagoj", ""]]}, {"id": "1707.00904", "submitter": "Ken-ichiro Ishikawa", "authors": "Ken-ichiro Ishikawa", "title": "Sequential Checking: Reallocation-Free Data-Distribution Algorithm for\n  Scale-out Storage", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using tape or optical devices for scale-out storage is one option for storing\na vast amount of data. However, it is impossible or almost impossible to\nrewrite data with such devices. Thus, scale-out storage using such devices\ncannot use standard data-distribution algorithms because they rewrite data for\nmoving between servers constituting the scale-out storage when the server\nconfiguration is changed. Although using rewritable devices for scale-out\nstorage, when server capacity is huge, rewriting data is very hard when server\nconstitution is changed. In this paper, a data-distribution algorithm called\nSequential Checking is proposed, which can be used for scale-out storage\ncomposed of devices that are hardly able to rewrite data. Sequential Checking\n1) does not need to move data between servers when the server configuration is\nchanged, 2) distribute data, the amount of which depends on the server's\nvolume, 3) select a unique server when datum is written, and 4) select servers\nwhen datum is read (there are few such server(s) in most cases) and find out a\nunique server that stores the newest datum from them. These basic\ncharacteristics were confirmed through proofs and simulations. Data can be read\nby accessing 1.98 servers on average from a storage comprising 256 servers\nunder a realistic condition. And it is confirmed by evaluations in real\nenvironment that access time is acceptable. Sequential Checking makes selecting\nscale-out storage using tape or optical devices or using huge capacity servers\nrealistic.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 10:52:39 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Ishikawa", "Ken-ichiro", ""]]}, {"id": "1707.01007", "submitter": "Rustam Azimov", "authors": "Rustam Azimov, Semyon Grigorev", "title": "Context-Free Path Querying by Matrix Multiplication", "comments": "9 pages, 11 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph data models are widely used in many areas, for example, bioinformatics,\ngraph databases. In these areas, it is often required to process queries for\nlarge graphs. Some of the most common graph queries are navigational queries.\nThe result of query evaluation is a set of implicit relations between nodes of\nthe graph, i.e. paths in the graph. A natural way to specify these relations is\nby specifying paths using formal grammars over the alphabet of edge labels. An\nanswer to a context-free path query in this approach is usually a set of\ntriples (A, m, n) such that there is a path from the node m to the node n,\nwhose labeling is derived from a non-terminal A of the given context-free\ngrammar. This type of queries is evaluated using the relational query\nsemantics. Another example of path query semantics is the single-path query\nsemantics which requires presenting a single path from the node m to the node\nn, whose labeling is derived from a non-terminal A for all triples (A, m, n)\nevaluated using the relational query semantics. There is a number of algorithms\nfor query evaluation which use these semantics but all of them perform poorly\non large graphs. One of the most common technique for efficient big data\nprocessing is the use of a graphics processing unit (GPU) to perform\ncomputations, but these algorithms do not allow to use this technique\nefficiently. In this paper, we show how the context-free path query evaluation\nusing these query semantics can be reduced to the calculation of the matrix\ntransitive closure. Also, we propose an algorithm for context-free path query\nevaluation which uses relational query semantics and is based on matrix\noperations that make it possible to speed up computations by using a GPU.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 14:22:18 GMT"}, {"version": "v2", "created": "Tue, 19 Dec 2017 11:52:45 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Azimov", "Rustam", ""], ["Grigorev", "Semyon", ""]]}, {"id": "1707.01223", "submitter": "Jinfei Liu", "authors": "Jinfei Liu, Li Xiong, Qiuchen Zhang, Jian Pei and Jun Luo", "title": "Eclipse: Practicability Beyond kNN and Skyline", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $k$ nearest neighbor ($k$NN) query is a fundamental problem in databases.\nGiven a set of multidimensional data points and a query point, $k$NN returns\nthe $k$ nearest neighbors based on a scoring function such as weighted sum\ngiven an attribute weight vector. However, the attribute weight vector can be\ndifficult to specify in practice. Skyline returns the points including all\npossible nearest neighbors without requiring the exact attribute weight vector\nor a scoring function but the number of returned points can be prohibitively\nlarge for practical use.\n  In this paper, we propose a novel \\emph{eclipse} definition which provides a\nmore flexible and customizable definition than the classic $1$NN and skyline.\nIn eclipse, users can specify a range of attribute weights and control the\nnumber of returned points. We show that both $1$NN and skyline are\ninstantiations of eclipse. To compute eclipse points, we propose a baseline\nalgorithm with time complexity of $O(n^22^{d-1})$, and an improved $O(n\\log\n^{d-1}n)$ time transformation-based algorithm by transforming the eclipse\nproblem to the skyline problem, where $n$ is the number of points and $d$ is\nthe number of dimensions. Furthermore, we propose a novel index-based algorithm\nutilizing duality transform with much better efficiency. The experimental\nresults on the real NBA dataset and the synthetic datasets demonstrate the\neffectiveness and efficiency of our eclipse algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 06:03:02 GMT"}, {"version": "v2", "created": "Tue, 16 Oct 2018 19:17:00 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Liu", "Jinfei", ""], ["Xiong", "Li", ""], ["Zhang", "Qiuchen", ""], ["Pei", "Jian", ""], ["Luo", "Jun", ""]]}, {"id": "1707.01341", "submitter": "Ahmad Siddiqui Dr", "authors": "Ahmad Tasnim Siddiqui, Mohd. Muntjir", "title": "A Modern Approach to Integrate Database Queries for Searching E-Commerce\n  Product", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  E commerce refers to the utilization of electronic data transmission for\nenhancing business processes and implementing business strategies. Explicit\ncomponents of e commerce include providing after sales services, promoting\nservices or products to services, processing payment, engaging in transaction\nprocesses, identifying customer needs, processing payment and creating services\nor products. In recent times, the use of e commerce has become too common among\nthe people. However, the growing demand of e commerce sites have made essential\nfor the databases to support direct querying of the Web page. This research\naims to explore and evaluate the integration of database queries and their uses\nin searching of electronic commerce products. It has been analyzed that e\ncommerce is one of the most outstanding trends, which have been emerged in the\ncommerce world, for the last decades. Therefore, this study was undertaken to\nexamine the benefits of integrating database queries with e commerce product\nsearches. The findings of this study suggested that database queries are\nextremely valuable for e commerce sites as they make product searches simpler\nand accurate. In this context, the approach of integrating database queries is\nfound to be the most suitable and satisfactory, as it simplifies the searching\nof e commerce products.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jul 2017 18:46:00 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Siddiqui", "Ahmad Tasnim", ""], ["Muntjir", "Mohd.", ""]]}, {"id": "1707.01414", "submitter": "Chunbin Lin", "authors": "Jaqueline Brito, Korhan Demirkaya, Boursier Etienne, Yannis Katsis,\n  Chunbin Lin, Yannis Papakonstantinou", "title": "Efficient Approximate Query Answering over Sensor Data with\n  Deterministic Error Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent proliferation of sensor data, there is an increasing need for\nthe efficient evaluation of analytical queries over multiple sensor datasets.\nThe magnitude of such datasets makes exact query answering infeasible, leading\nresearchers into the development of approximate query answering approaches.\nHowever, existing approximate query answering algorithms are not suited for the\nefficient processing of queries over sensor data, as they exhibit at least one\nof the following shortcomings: (a) They do not provide deterministic error\nguarantees, resorting to weaker probabilistic error guarantees that are in many\ncases not acceptable, (b) they allow queries only over a single dataset, thus\nnot supporting the multitude of queries over multiple datasets that appear in\npractice, such as correlation or cross-correlation and (c) they support\nrelational data in general and thus miss speedup opportunities created by the\nspecial nature of sensor data, which are not random but follow a typically\nsmooth underlying phenomenon.\n  To address these problems, we propose PlatoDB; a system that exploits the\nnature of sensor data to compress them and provide efficient processing of\nqueries over multiple sensor datasets, while providing deterministic error\nguarantees. PlatoDB achieves the above through a novel architecture that (a) at\ndata import time pre-processes each dataset, creating for it an intermediate\nhierarchical data structure that provides a hierarchy of summarizations of the\ndataset together with appropriate error measures and (b) at query processing\ntime leverages the pre-computed data structures to compute an approximate\nanswer and deterministic error guarantees for ad hoc queries even when these\ncombine multiple datasets.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 14:22:05 GMT"}, {"version": "v2", "created": "Mon, 18 Sep 2017 01:32:08 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Brito", "Jaqueline", ""], ["Demirkaya", "Korhan", ""], ["Etienne", "Boursier", ""], ["Katsis", "Yannis", ""], ["Lin", "Chunbin", ""], ["Papakonstantinou", "Yannis", ""]]}, {"id": "1707.01869", "submitter": "Shantanu Sharma", "authors": "Shlomi Dolev, Patricia Florissi, Ehud Gudes, Shantanu Sharma, Ido\n  Singer", "title": "A Survey on Geographically Distributed Big-Data Processing using\n  MapReduce", "comments": "IEEE Transactions on Big Data; Accepted June 2017. 20 pages", "journal-ref": null, "doi": "10.1109/TBDATA.2017.2723473", "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hadoop and Spark are widely used distributed processing frameworks for\nlarge-scale data processing in an efficient and fault-tolerant manner on\nprivate or public clouds. These big-data processing systems are extensively\nused by many industries, e.g., Google, Facebook, and Amazon, for solving a\nlarge class of problems, e.g., search, clustering, log analysis, different\ntypes of join operations, matrix multiplication, pattern matching, and social\nnetwork analysis. However, all these popular systems have a major drawback in\nterms of locally distributed computations, which prevent them in implementing\ngeographically distributed data processing. The increasing amount of\ngeographically distributed massive data is pushing industries and academia to\nrethink the current big-data processing systems. The novel frameworks, which\nwill be beyond state-of-the-art architectures and technologies involved in the\ncurrent system, are expected to process geographically distributed data at\ntheir locations without moving entire raw datasets to a single location. In\nthis paper, we investigate and discuss challenges and requirements in designing\ngeographically distributed data processing frameworks and protocols. We\nclassify and study batch processing (MapReduce-based systems), stream\nprocessing (Spark-based systems), and SQL-style processing geo-distributed\nframeworks, models, and algorithms with their overhead issues.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 17:04:46 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Dolev", "Shlomi", ""], ["Florissi", "Patricia", ""], ["Gudes", "Ehud", ""], ["Sharma", "Shantanu", ""], ["Singer", "Ido", ""]]}, {"id": "1707.02047", "submitter": "Zhuoyue Zhao", "authors": "Zhuoyue Zhao, Jialing Pei, Eric Lo, Kenny Q. Zhu, Chris Liu", "title": "InferSpark: Statistical Inference at Scale", "comments": "13 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Apache Spark stack has enabled fast large-scale data processing. Despite\na rich library of statistical models and inference algorithms, it does not give\ndomain users the ability to develop their own models. The emergence of\nprobabilistic programming languages has showed the promise of developing\nsophisticated probabilistic models in a succinct and programmatic way. These\nframeworks have the potential of automatically generating inference algorithms\nfor the user defined models and answering various statistical queries about the\nmodel. It is a perfect time to unite these two great directions to produce a\nprogrammable big data analysis framework. We thus propose, InferSpark, a\nprobabilistic programming framework on top of Apache Spark. Efficient\nstatistical inference can be easily implemented on this framework and inference\nprocess can leverage the distributed main memory processing power of Spark.\nThis framework makes statistical inference on big data possible and speed up\nthe penetration of probabilistic programming into the data engineering domain.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 06:02:50 GMT"}, {"version": "v2", "created": "Mon, 9 Oct 2017 15:16:54 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Zhao", "Zhuoyue", ""], ["Pei", "Jialing", ""], ["Lo", "Eric", ""], ["Zhu", "Kenny Q.", ""], ["Liu", "Chris", ""]]}, {"id": "1707.03327", "submitter": "Daniel Torres-Salinas Dr", "authors": "Daniel Torres-Salinas, Christian Gumpenberger, Juan Gorraiz", "title": "PlumX As a Potential Tool to Assess the Macroscopic Multidimensional\n  Impact of Books", "comments": null, "journal-ref": null, "doi": "10.3389/frma.2017.00005", "report-no": "11 pages", "categories": "cs.DL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main purpose of this macro-study is to shed light on the broad impact of\nbooks. For this purpose, the impact of a very large collection of books has\nbeen analyzed by using PlumX, an analytical tool providing a great number of\ndifferent metrics provided by various tools.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 19:02:22 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Torres-Salinas", "Daniel", ""], ["Gumpenberger", "Christian", ""], ["Gorraiz", "Juan", ""]]}, {"id": "1707.03350", "submitter": "Kiumars Soltani", "authors": "Kiumars Soltani, Anand Padmanabhan, Shaowen Wang", "title": "MovePattern: Interactive Framework to Provide Scalable Visualization of\n  Movement Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid growth of movement data sources such as GPS traces, traffic\nnetworks and social media have provided analysts with the opportunity to\nexplore collective patterns of geographical movements in a nearly real-time\nfashion. A fast and interactive visualization framework can help analysts to\nunderstand these massive and dynamically changing datasets. However, previous\nstudies on movement visualization either ignore the unique properties of\ngeographical movement or are unable to handle today's massive data. In this\npaper, we develop MovePattern, a novel framework to 1) efficiently construct a\nconcise multi-level view of movements using a scalable and spatially-aware\nMapReduce-based approach and 2) present a fast and highly interactive webbased\nenvironment which engages vector-based visualization to include on-the-fly\ncustomization and the ability to enhance analytical functions by storing\nmetadata for both places and movements. We evaluate the framework using the\nmovements of Twitter users captured from geo-tagged tweets. The experiments\nconfirmed that our framework is able to aggregate close to 180 million\nmovements in a few minutes. In addition, we run series of stress tests on the\nfront-end of the framework to ensure that simultaneous user queries do not lead\nto long latency in the user response.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 03:57:32 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Soltani", "Kiumars", ""], ["Padmanabhan", "Anand", ""], ["Wang", "Shaowen", ""]]}, {"id": "1707.03602", "submitter": "Serkan Ayvaz", "authors": "Serkan Ayvaz, Mehmet Aydar", "title": "Using RDF Summary Graph For Keyword-based Semantic Searches", "comments": "5th International Conference on Advanced Technology & Sciences\n  (ICAT'17). Istanbul, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Semantic Web began to emerge as its standards and technologies developed\nrapidly in the recent years. The continuing development of Semantic Web\ntechnologies has facilitated publishing explicit semantics with data on the Web\nin RDF data model. This study proposes a semantic search framework to support\nefficient keyword-based semantic search on RDF data utilizing near neighbor\nexplorations. The framework augments the search results with the resources in\nclose proximity by utilizing the entity type semantics. Along with the search\nresults, the system generates a relevance confidence score measuring the\ninferred semantic relatedness of returned entities based on the degree of\nsimilarity. Furthermore, the evaluations assessing the effectiveness of the\nframework and the accuracy of the results are presented.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 08:54:48 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Ayvaz", "Serkan", ""], ["Aydar", "Mehmet", ""]]}, {"id": "1707.03877", "submitter": "Cagatay Demiralp", "authors": "\\c{C}a\\u{g}atay Demiralp, Peter J. Haas, Srinivasan Parthasarathy,\n  Tejaswini Pedapati", "title": "Foresight: Recommending Visual Insights", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current tools for exploratory data analysis (EDA) require users to manually\nselect data attributes, statistical computations and visual encodings. This can\nbe daunting for large-scale, complex data. We introduce Foresight, a system\nthat helps the user rapidly discover visual insights from large\nhigh-dimensional datasets. Formally, an \"insight\" is a strong manifestation of\na statistical property of the data, e.g., high correlation between two\nattributes, high skewness or concentration about the mean of a single\nattribute, a strong clustering of values, and so on. For each insight type,\nForesight initially presents visualizations of the top k instances in the data,\nbased on an appropriate ranking metric. The user can then look at \"nearby\"\ninsights by issuing \"insight queries\" containing constraints on insight\nstrengths and data attributes. Thus the user can directly explore the space of\ninsights, rather than the space of data dimensions and visual encodings as in\nother visual recommender systems. Foresight also provides \"global\" views of\ninsight space to help orient the user and ensure a thorough exploration\nprocess. Furthermore, Foresight facilitates interactive exploration of large\ndatasets through fast, approximate sketching.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 19:18:12 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Demiralp", "\u00c7a\u011fatay", ""], ["Haas", "Peter J.", ""], ["Parthasarathy", "Srinivasan", ""], ["Pedapati", "Tejaswini", ""]]}, {"id": "1707.05340", "submitter": "Meng Wang", "authors": "Meng Wang, Jiaheng Zhang, Jun Liu, Wei Hu, Sen Wang, Xue Li, Wenqiang\n  Liu", "title": "PDD Graph: Bridging Electronic Medical Records and Biomedical Knowledge\n  Graphs via Entity Linking", "comments": "9 pages,5 figures,accepted by ISWC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic medical records contain multi-format electronic medical data that\nconsist of an abundance of medical knowledge. Facing with patient's symptoms,\nexperienced caregivers make right medical decisions based on their professional\nknowledge that accurately grasps relationships between symptoms, diagnosis and\ncorresponding treatments. In this paper, we aim to capture these relationships\nby constructing a large and high-quality heterogenous graph linking patients,\ndiseases, and drugs (PDD) in EMRs. Specifically, we propose a novel framework\nto extract important medical entities from MIMIC-III (Medical Information Mart\nfor Intensive Care III) and automatically link them with the existing\nbiomedical knowledge graphs, including ICD-9 ontology and DrugBank. The PDD\ngraph presented in this paper is accessible on the Web via the SPARQL endpoint,\nand provides a pathway for medical discovery and applications, such as\neffective treatment recommendations.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 18:04:27 GMT"}, {"version": "v2", "created": "Mon, 24 Jul 2017 14:36:18 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Wang", "Meng", ""], ["Zhang", "Jiaheng", ""], ["Liu", "Jun", ""], ["Hu", "Wei", ""], ["Wang", "Sen", ""], ["Li", "Xue", ""], ["Liu", "Wenqiang", ""]]}, {"id": "1707.05364", "submitter": "Shahzad Ahmed Mr.", "authors": "Shahzad Ahmed, M. Usman Ali, Javed Ferzund, Muhammad Atif Sarwar,\n  Abbas Rehman and Atif Mehmood", "title": "Modern Data Formats for Big Bioinformatics Data Analytics", "comments": "12 Pages, 20 figures and 2 Tables", "journal-ref": "International Journal of Advanced Computer Science and\n  Applications, Issue 8, No. 4, page 366-377 (1 May 2017)", "doi": "10.14569/IJACSA.2017.080450", "report-no": null, "categories": "cs.DB cs.CY cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Next Generation Sequencing (NGS) technology has resulted in massive amounts\nof proteomics and genomics data. This data is of no use if it is not properly\nanalyzed. ETL (Extraction, Transformation, Loading) is an important step in\ndesigning data analytics applications. ETL requires proper understanding of\nfeatures of data. Data format plays a key role in understanding of data,\nrepresentation of data, space required to store data, data I/O during\nprocessing of data, intermediate results of processing, in-memory analysis of\ndata and overall time required to process data. Different data mining and\nmachine learning algorithms require input data in specific types and formats.\nThis paper explores the data formats used by different tools and algorithms and\nalso presents modern data formats that are used on Big Data Platform. It will\nhelp researchers and developers in choosing appropriate data format to be used\nfor a particular tool or algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 11:35:53 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Ahmed", "Shahzad", ""], ["Ali", "M. Usman", ""], ["Ferzund", "Javed", ""], ["Sarwar", "Muhammad Atif", ""], ["Rehman", "Abbas", ""], ["Mehmood", "Atif", ""]]}, {"id": "1707.05681", "submitter": "Mohan Yang", "authors": "Carlo Zaniolo, Mohan Yang, Matteo Interlandi, Ariyam Das, Alexander\n  Shkapsky, Tyson Condie", "title": "Fixpoint Semantics and Optimization of Recursive Datalog Programs with\n  Aggregates", "comments": "Paper presented at the 33nd International Conference on Logic\n  Programming (ICLP 2017), Melbourne, Australia, August 28 to September 1,\n  2017. 16 pages, LaTeX (arXiv:1707.05681)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A very desirable Datalog extension investigated by many researchers in the\nlast thirty years consists in allowing the use of the basic SQL aggregates min,\nmax, count and sum in recursive rules. In this paper, we propose a simple\ncomprehensive solution that extends the declarative least-fixpoint semantics of\nHorn Clauses, along with the optimization techniques used in the bottom-up\nimplementation approach adopted by many Datalog systems. We start by\nidentifying a large class of programs of great practical interest in which the\nuse of min or max in recursive rules does not compromise the declarative\nfixpoint semantics of the programs using those rules. Then, we revisit the\nmonotonic versions of count and sum aggregates proposed in (Mazuran et al.\n2013b) and named, respectively, mcount and msum. Since mcount, and also msum on\npositive numbers, are monotonic in the lattice of set-containment, they\npreserve the fixpoint semantics of Horn Clauses. However, in many applications\nof practical interest, their use can lead to inefficiencies, that can be\neliminated by combining them with max, whereby mcount and msum become the\nstandard count and sum. Therefore, the semantics and optimization techniques of\nDatalog are extended to recursive programs with min, max, count and sum, making\npossible the advanced applications of superior performance and scalability\ndemonstrated by BigDatalog (Shkapsky et al. 2016) and Datalog-MC (Yang et al.\n2017). This paper is under consideration for acceptance in TPLP.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 15:25:35 GMT"}, {"version": "v2", "created": "Fri, 21 Jul 2017 06:32:51 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Zaniolo", "Carlo", ""], ["Yang", "Mohan", ""], ["Interlandi", "Matteo", ""], ["Das", "Ariyam", ""], ["Shkapsky", "Alexander", ""], ["Condie", "Tyson", ""]]}, {"id": "1707.05945", "submitter": "Martin Grohe", "authors": "Martin Grohe and Nicole Schweikardt", "title": "First-Order Query Evaluation with Cardinality Conditions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an extension of first-order logic that allows to express cardinality\nconditions in a similar way as SQL's COUNT operator. The corresponding logic\nFOC(P) was introduced by Kuske and Schweikardt (LICS'17), who showed that query\nevaluation for this logic is fixed-parameter tractable on classes of structures\n(or databases) of bounded degree. In the present paper, we first show that the\nfixed-parameter tractability of FOC(P) cannot even be generalised to very\nsimple classes of structures of unbounded degree such as unranked trees or\nstrings with a linear order relation.\n  Then we identify a fragment FOC1(P) of FOC(P) which is still sufficiently\nstrong to express standard applications of SQL's COUNT operator. Our main\nresult shows that query evaluation for FOC1(P) is fixed-parameter tractable\nwith almost linear running time on nowhere dense classes of structures. As a\ncorollary, we also obtain a fixed-parameter tractable algorithm for counting\nthe number of tuples satisfying a query over nowhere dense classes of\nstructures.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 06:12:34 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Grohe", "Martin", ""], ["Schweikardt", "Nicole", ""]]}, {"id": "1707.06315", "submitter": "Tianyu Wang", "authors": "Tianyu Wang, Marco Morucci, M. Usaid Awan, Yameng Liu, Sudeepa Roy,\n  Cynthia Rudin, Alexander Volfovsky", "title": "FLAME: A Fast Large-scale Almost Matching Exactly Approach to Causal\n  Inference", "comments": null, "journal-ref": "Journal of Machine Learning Research, 22 (2021) 1-41", "doi": null, "report-no": null, "categories": "stat.ML cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A classical problem in causal inference is that of matching, where treatment\nunits need to be matched to control units based on covariate information. In\nthis work, we propose a method that computes high quality almost-exact matches\nfor high-dimensional categorical datasets. This method, called FLAME (Fast\nLarge-scale Almost Matching Exactly), learns a distance metric for matching\nusing a hold-out training data set. In order to perform matching efficiently\nfor large datasets, FLAME leverages techniques that are natural for query\nprocessing in the area of database management, and two implementations of FLAME\nare provided: the first uses SQL queries and the second uses bit-vector\ntechniques. The algorithm starts by constructing matches of the highest quality\n(exact matches on all covariates), and successively eliminates variables in\norder to match exactly on as many variables as possible, while still\nmaintaining interpretable high-quality matches and balance between treatment\nand control groups. We leverage these high quality matches to estimate\nconditional average treatment effects (CATEs). Our experiments show that FLAME\nscales to huge datasets with millions of observations where existing\nstate-of-the-art methods fail, and that it achieves significantly better\nperformance than other matching methods.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 22:35:40 GMT"}, {"version": "v2", "created": "Fri, 19 Jan 2018 19:58:11 GMT"}, {"version": "v3", "created": "Thu, 22 Feb 2018 04:48:46 GMT"}, {"version": "v4", "created": "Thu, 31 Jan 2019 06:22:07 GMT"}, {"version": "v5", "created": "Sat, 22 Jun 2019 01:17:35 GMT"}, {"version": "v6", "created": "Wed, 16 Oct 2019 22:16:39 GMT"}, {"version": "v7", "created": "Sat, 21 Dec 2019 01:33:09 GMT"}, {"version": "v8", "created": "Fri, 11 Sep 2020 19:24:33 GMT"}, {"version": "v9", "created": "Thu, 4 Feb 2021 11:47:13 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Wang", "Tianyu", ""], ["Morucci", "Marco", ""], ["Awan", "M. Usaid", ""], ["Liu", "Yameng", ""], ["Roy", "Sudeepa", ""], ["Rudin", "Cynthia", ""], ["Volfovsky", "Alexander", ""]]}, {"id": "1707.06406", "submitter": "Stasinos Konstantopoulos", "authors": "Antonis Troumpoukis, Stasinos Konstantopoulos, and Angelos\n  Charalambidis", "title": "An extension of SPARQL for expressing qualitative preferences", "comments": "Accepted to the 2017 International Semantic Web Conference, Vienna,\n  October 2017", "journal-ref": null, "doi": "10.1007/978-3-319-68288-4_42", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present SPREFQL, an extension of the SPARQL language that\nallows appending a PREFER clause that expresses \"soft\" preferences over the\nquery results obtained by the main body of the query. The extension does not\nadd expressivity and any SPREFQL query can be transformed to an equivalent\nstandard SPARQL query. However, clearly separating preferences from the \"hard\"\npatterns and filters in the WHERE clause gives queries where the intention of\nthe client is more cleanly expressed, an advantage for both human readability\nand machine optimization. In the paper we formally define the syntax and the\nsemantics of the extension and we also provide empirical evidence that\noptimizations specific to SPREFQL improve run-time efficiency by comparison to\nthe usually applied optimizations on the equivalent standard SPARQL query.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 08:00:32 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Troumpoukis", "Antonis", ""], ["Konstantopoulos", "Stasinos", ""], ["Charalambidis", "Angelos", ""]]}, {"id": "1707.06507", "submitter": "Vivek Shah", "authors": "Vivek Shah, Marcos Antonio Vaz Salles", "title": "Actor-Relational Database Systems: A Manifesto", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive data-intensive applications are becoming ever more pervasive in\ndomains such as finance, web applications, mobile computing, and Internet of\nThings. Increasingly, these applications are being deployed in sophisticated\nparallel and distributed hardware infrastructures. With this growing diversity\nof the software and hardware landscape, there is a pressure on programming\nmodels and systems to enable developers to design modular, scalable, efficient,\nand consistent data-intensive applications. In response to this challenge,\nrecent research has advocated the integration of actor programming models and\ndatabase management. This integration promises to help developers build\nlogically distributed micro-applications well adapted to modern hardware trends\nas opposed to existing approaches targeted at optimizing monolithic\napplications.\n  Towards this aim, in this paper we analyze, make the case for, and present a\nbroad vision of actor-relational database systems. We argue why the time is\nripe today to examine the research opportunities afforded by this emerging\nsystem paradigm. Based on this discussion, we present design principles as well\nas candidate feature sets to help concretize the vision for such systems. To\nillustrate the usefulness of the proposed feature set and motivate the need for\nthis class of systems, we show a detailed case study inspired by a smart\nsupermarket application with self-checkout, along with evidence for performance\nbenefits on modern hardware.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 13:52:21 GMT"}, {"version": "v2", "created": "Mon, 10 Sep 2018 12:26:01 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Shah", "Vivek", ""], ["Salles", "Marcos Antonio Vaz", ""]]}, {"id": "1707.06814", "submitter": "Johan von Tangen Sivertsen M.Sc", "authors": "Tobias Christiani, Rasmus Pagh and Johan Sivertsen", "title": "Scalable and robust set similarity join", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Set similarity join is a fundamental and well-studied database operator. It\nis usually studied in the exact setting where the goal is to compute all pairs\nof sets that exceed a given similarity threshold (measured e.g. as Jaccard\nsimilarity). But set similarity join is often used in settings where 100%\nrecall may not be important --- indeed, where the exact set similarity join is\nitself only an approximation of the desired result set.\n  We present a new randomized algorithm for set similarity join that can\nachieve any desired recall up to 100%, and show theoretically and empirically\nthat it significantly improves on existing methods. The present\nstate-of-the-art exact methods are based on prefix-filtering, the performance\nof which depends on the data set having many rare tokens. Our method is robust\nagainst the absence of such structure in the data. At 90% recall our algorithm\nis often more than an order of magnitude faster than state-of-the-art exact\nmethods, depending on how well a data set lends itself to prefix filtering. Our\nexperiments on benchmark data sets also show that the method is several times\nfaster than comparable approximate methods. Our algorithm makes use of recent\ntheoretical advances in high-dimensional sketching and indexing that we believe\nto be of wider relevance to the data engineering community.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 09:50:36 GMT"}, {"version": "v2", "created": "Thu, 25 Jan 2018 14:39:14 GMT"}, {"version": "v3", "created": "Fri, 2 Mar 2018 12:11:58 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Christiani", "Tobias", ""], ["Pagh", "Rasmus", ""], ["Sivertsen", "Johan", ""]]}, {"id": "1707.06974", "submitter": "Guohui Xiao", "authors": "Davide Lanti, Guohui Xiao, Diego Calvanese", "title": "Cost-Driven Ontology-Based Data Access (Extended Version)", "comments": "Extended version of the ISWC 17 paper \"Cost-Driven Ontology-Based\n  Data Access\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In ontology-based data access (OBDA), users are provided with a conceptual\nview of a (relational) data source that abstracts away details about data\nstorage. This conceptual view is realized through an ontology that is connected\nto the data source through declarative mappings, and query answering is carried\nout by translating the user queries over the conceptual view into SQL queries\nover the data source. Standard translation techniques in OBDA try to transform\nthe user query into a union of conjunctive queries (UCQ), following the\nheuristic argument that UCQs can be efficiently evaluated by modern relational\ndatabase engines. In this work, we show that translating to UCQs is not always\nthe best choice, and that, under certain conditions on the interplay between\nthe ontology, the map- pings, and the statistics of the data, alternative\ntranslations can be evaluated much more efficiently. To find the best\ntranslation, we devise a cost model together with a novel cardinality\nestimation that takes into account all such OBDA components. Our experiments\nconfirm that (i) alternatives to the UCQ translation might produce queries that\nare orders of magnitude more efficient, and (ii) the cost model we propose is\nfaithful to the actual query evaluation cost, and hence is well suited to\nselect the best translation.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 17:08:59 GMT"}, {"version": "v2", "created": "Fri, 2 Feb 2018 17:10:08 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["Lanti", "Davide", ""], ["Xiao", "Guohui", ""], ["Calvanese", "Diego", ""]]}, {"id": "1707.07222", "submitter": "Antoine Amarilli", "authors": "Antoine Amarilli, Mouhamadou Lamine Ba, Daniel Deutch, Pierre\n  Senellart", "title": "Possible and Certain Answers for Queries over Order-Incomplete Data", "comments": "This paper is the full version with appendices of the TIME'17\n  article. See also the upcoming journal version: arXiv:1801.06396. Important\n  note: This version (version 2) removes some results because we found a bug in\n  their proofs. See Appendix G for detailed explanations. The journal version\n  also omits the affected results (and does not contain Appendix G)", "journal-ref": null, "doi": "10.4230/LIPIcs.TIME.2017.4", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To combine and query ordered data from multiple sources, one needs to handle\nuncertainty about the possible orderings. Examples of such \"order-incomplete\"\ndata include integrated event sequences such as log entries, lists of\nproperties (e.g., hotels and restaurants) ranked by an unknown function\nreflecting relevance or customer ratings, and documents edited concurrently\nwith an uncertain order on edits. This paper introduces a query language for\norder-incomplete data, based on the positive relational algebra with\norder-aware accumulation. We use partial orders to represent order-incomplete\ndata, and study possible and certain answers for queries in this context. We\nshow that these problems are respectively NP-complete and coNP-complete, but\nidentify many tractable cases depending on the query operators or input partial\norders.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jul 2017 22:10:49 GMT"}, {"version": "v2", "created": "Fri, 26 Jan 2018 23:40:11 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Amarilli", "Antoine", ""], ["Ba", "Mouhamadou Lamine", ""], ["Deutch", "Daniel", ""], ["Senellart", "Pierre", ""]]}, {"id": "1707.07623", "submitter": "Oren Kalinsky", "authors": "Oren Mishali, Tal Yahav, Oren Kalinsky, Benny Kimelfeld", "title": "eLinda: Explorer for Linked Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To realize the premise of the Semantic Web towards knowledgeable machines,\none might often integrate an application with emerging RDF graphs.\nNevertheless, capturing the content of a rich and open RDF graph by existing\ntools requires both time and expertise. We demonstrate eLinda - an explorer for\nLinked Data. The challenge addressed by eLinda is that of understanding the\nrich content of a given RDF graph. The core functionality is an exploration\npath, where each step produces a bar chart (histogram) that visualizes the\ndistribution of classes in a set of nodes (URIs). In turn, each bar represents\na set of nodes that can be further expanded through the bar chart in the path.\nWe allow three types of explorations: subclass distribution, property\ndistribution, and object distribution for a property of choice.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 16:03:06 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Mishali", "Oren", ""], ["Yahav", "Tal", ""], ["Kalinsky", "Oren", ""], ["Kimelfeld", "Benny", ""]]}, {"id": "1707.07794", "submitter": "Parisa Kordjamshidi", "authors": "Parisa Kordjamshidi, Sameer Singh, Daniel Khashabi, Christos\n  Christodoulopoulos, Mark Summons, Saurabh Sinha, Dan Roth", "title": "Relational Learning and Feature Extraction by Querying over\n  Heterogeneous Information Networks", "comments": "Seventh International Workshop on Statistical Relational AI, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real world systems need to operate on heterogeneous information networks\nthat consist of numerous interacting components of different types. Examples\ninclude systems that perform data analysis on biological information networks;\nsocial networks; and information extraction systems processing unstructured\ndata to convert raw text to knowledge graphs. Many previous works describe\nspecialized approaches to perform specific types of analysis, mining and\nlearning on such networks. In this work, we propose a unified framework\nconsisting of a data model -a graph with a first order schema along with a\ndeclarative language for constructing, querying and manipulating such networks\nin ways that facilitate relational and structured machine learning. In\nparticular, we provide an initial prototype for a relational and graph\ntraversal query language where queries are directly used as relational features\nfor structured machine learning models. Feature extraction is performed by\nmaking declarative graph traversal queries. Learning and inference models can\ndirectly operate on this relational representation and augment it with new data\nand knowledge that, in turn, is integrated seamlessly into the relational\nstructure to support new predictions. We demonstrate this system's capabilities\nby showcasing tasks in natural language processing and computational biology\ndomains.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 02:32:33 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Kordjamshidi", "Parisa", ""], ["Singh", "Sameer", ""], ["Khashabi", "Daniel", ""], ["Christodoulopoulos", "Christos", ""], ["Summons", "Mark", ""], ["Sinha", "Saurabh", ""], ["Roth", "Dan", ""]]}, {"id": "1707.08259", "submitter": "Elham Shahab", "authors": "Amir Mohammad Saba, Elham Shahab, Hadi Abdolrahimpour, Mahsa Hakimi,\n  Akbar Moazzam", "title": "A Comparative Analysis of XML Documents, XML Enabled Databases and\n  Native XML Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing popularity of XML data and a great need for a database\nmanagement system able to store, retrieve and manipulate XML-based data in an\nefficient manner, database research communities and software industries have\ntried to respond to this requirement. XML-enabled database and native XML\ndatabase are two approaches that have been proposed to address this challenge.\nThese two approaches are a legacy database systems which are extended to store,\nretrieve and manipulate XML-based data. The major objective of this paper is to\nexplore and compare between the two approaches and reach to some criteria to\nhave a suitable guideline to select the best approach in each circumstance. In\ngeneral, native XML database systems have more ability in comparison with\nXML-enabled database system for managing XML-based data\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 00:21:23 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Saba", "Amir Mohammad", ""], ["Shahab", "Elham", ""], ["Abdolrahimpour", "Hadi", ""], ["Hakimi", "Mahsa", ""], ["Moazzam", "Akbar", ""]]}, {"id": "1707.08272", "submitter": "Apurba Das", "authors": "Apurba Das, Srikanta Tirthapura", "title": "A Change-Sensitive Algorithm for Maintaining Maximal Bicliques in a\n  Dynamic Bipartite Graph", "comments": "12 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the maintenance of maximal bicliques from a dynamic bipartite\ngraph that changes over time due to the addition or deletion of edges. When the\nset of edges in a graph changes, we are interested in knowing the change in the\nset of maximal bicliques (the \"change\"), rather than in knowing the set of\nmaximal bicliques that remain unaffected. The challenge in an efficient\nalgorithm is to enumerate the change without explicitly enumerating the set of\nall maximal bicliques. In this work, we present (1) near-tight bounds on the\nmagnitude of change in the set of maximal bicliques of a graph, due to a change\nin the edge set (2) a \"change-sensitive\" algorithm for enumerating the change\nin the set of maximal bicliques, whose time complexity is proportional to the\nmagnitude of change that actually occurred in the set of maximal bicliques in\nthe graph. To our knowledge, these are the first algorithms for enumerating\nmaximal bicliques in a dynamic graph, with such provable performance\nguarantees. Our algorithms are easy to implement, and experimental results show\nthat their performance exceeds that of current baseline implementations by\norders of magnitude.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 02:01:22 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Das", "Apurba", ""], ["Tirthapura", "Srikanta", ""]]}, {"id": "1707.08482", "submitter": "Joachim Biskup", "authors": "Joachim Biskup, Cornelia Tadros, Jaouad Zarouali", "title": "Confidentiality enforcement by hybrid control of information flows", "comments": "44 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An information owner, possessing diverse data sources, might want to offer\ninformation services based on these sources to cooperation partners and to this\nend interact with these partners by receiving and sending messages, which the\nowner on his part generates by program execution. Independently from data\nrepresentation or its physical storage, information release to a partner might\nbe restricted by the owner's confidentiality policy on an integrated, unified\nview of the sources. Such a policy should even be enforced if the partner as an\nintelligent and only semi-honest attacker attempts to infer hidden information\nfrom message data, also employing background knowledge. For this problem of\ninference control, we present a framework for a unified, holistic control of\ninformation flow induced by program-based processing of the data sources to\nmessages sent to a cooperation partner. Our framework expands on and combines\nestablished concepts for confidentiality enforcement and its verification and\nis instantiated in a Java environment. More specifically, as a hybrid control\nwe combine gradual release of information via declassification, enforced by\nstatic program analysis using a security type system, with a dynamic monitoring\napproach. The dynamic monitoring employs flow tracking for generalizing values\nto be declassified under confidentiality policy compliance.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 15:02:45 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Biskup", "Joachim", ""], ["Tadros", "Cornelia", ""], ["Zarouali", "Jaouad", ""]]}, {"id": "1707.08524", "submitter": "Clark Alexander", "authors": "Clark Alexander, Sofya Akhmametyeva", "title": "The Shape Metric for Clustering Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct a method by which we can calculate the precision with which an\nalgorithm identifies the shape of a cluster. We present our results for several\nwell known clustering algorithms and suggest ways to improve performance for\nnewer algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 16:21:32 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Alexander", "Clark", ""], ["Akhmametyeva", "Sofya", ""]]}, {"id": "1707.08743", "submitter": "EPTCS", "authors": "Willem Conradie, Sabine Frittella, Alessandra Palmigiano, Michele\n  Piazzai, Apostolos Tzimoulis, Nachoem M. Wijnberg", "title": "Toward an Epistemic-Logical Theory of Categorization", "comments": "In Proceedings TARK 2017, arXiv:1707.08250", "journal-ref": "EPTCS 251, 2017, pp. 167-186", "doi": "10.4204/EPTCS.251.12", "report-no": null, "categories": "cs.LO cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Categorization systems are widely studied in psychology, sociology, and\norganization theory as information-structuring devices which are critical to\ndecision-making processes. In the present paper, we introduce a sound and\ncomplete epistemic logic of categories and agents' categorical perception. The\nKripke-style semantics of this logic is given in terms of data structures based\non two domains: one domain representing objects (e.g. market products) and one\ndomain representing the features of the objects which are relevant to the\nagents' decision-making. We use this framework to discuss and propose\nlogic-based formalizations of some core concepts from psychological,\nsociological, and organizational research in categorization theory.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 07:48:07 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Conradie", "Willem", ""], ["Frittella", "Sabine", ""], ["Palmigiano", "Alessandra", ""], ["Piazzai", "Michele", ""], ["Tzimoulis", "Apostolos", ""], ["Wijnberg", "Nachoem M.", ""]]}, {"id": "1707.09930", "submitter": "Boris Glavic", "authors": "Xing Niu, Bahareh Sadat Arab, Seokki Lee, Su Feng, Xun Zou, Dieter\n  Gawlick, Vasudha Krishnaswamy, Zhen Hua Liu, Boris Glavic", "title": "Debugging Transactions and Tracking their Provenance with Reenactment", "comments": "to appear as \"Debugging Transactions and Tracking their Provenance\n  with Reenactment\" in PVDLB 2017, vol 10., nr. 12", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Debugging transactions and understanding their execution are of immense\nimportance for developing OLAP applications, to trace causes of errors in\nproduction systems, and to audit the operations of a database. However,\ndebugging transactions is hard for several reasons: 1) after the execution of a\ntransaction, its input is no longer available for debugging, 2) internal states\nof a transaction are typically not accessible, and 3) the execution of a\ntransaction may be affected by concurrently running transactions. We present a\ndebugger for transactions that enables non-invasive, post-mortem debugging of\ntransactions with provenance tracking and supports what-if scenarios (changes\nto transaction code or data). Using reenactment, a declarative replay technique\nwe have developed, a transaction is replayed over the state of the DB seen by\nits original execution including all its interactions with concurrently\nexecuted transactions from the history. Importantly, our approach uses the\ntemporal database and audit logging capabilities available in many DBMS and\ndoes not require any modifications to the underlying database system nor\ntransactional workload.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 16:00:05 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Niu", "Xing", ""], ["Arab", "Bahareh Sadat", ""], ["Lee", "Seokki", ""], ["Feng", "Su", ""], ["Zou", "Xun", ""], ["Gawlick", "Dieter", ""], ["Krishnaswamy", "Vasudha", ""], ["Liu", "Zhen Hua", ""], ["Glavic", "Boris", ""]]}]