[{"id": "1310.0141", "submitter": "Alexander Russakovsky", "authors": "Alexander Russakovsky", "title": "Hopping over Big Data: Accelerating Ad-hoc OLAP Queries with Grasshopper\n  Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a family of algorithms for fast subset filtering within\nordered sets of integers representing composite keys. Applications include\nsignificant acceleration of (ad-hoc) analytic queries against a data warehouse\nwithout any additional indexing. The algorithms work for point, range and set\nrestrictions on multiple attributes, in any combination, and are inherently\nmultidimensional. The main idea consists in intelligent combination of\nsequential crawling with jumps over large portions of irrelevant keys. The way\nto combine them is adaptive to characteristics of the underlying data store.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2013 05:02:14 GMT"}, {"version": "v2", "created": "Sun, 22 Feb 2015 18:47:09 GMT"}, {"version": "v3", "created": "Tue, 24 Feb 2015 06:09:16 GMT"}, {"version": "v4", "created": "Wed, 25 Feb 2015 18:42:57 GMT"}], "update_date": "2015-02-26", "authors_parsed": [["Russakovsky", "Alexander", ""]]}, {"id": "1310.0229", "submitter": "Jordi Casas-Roma", "authors": "Jordi Casas-Roma and Jordi Herrera-Joancomart\\'i and Vicen\\c{c} Torra", "title": "Evolutionary Algorithm for Graph Anonymization", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In recent years there has been a significant increase in the use of graphs as\na tool for representing information. It is very important to preserve the\nprivacy of users when one wants to publish this information, especially in the\ncase of social graphs. In this case, it is essential to implement an\nanonymization process in the data in order to preserve users' privacy. In this\npaper we present an algorithm for graph anonymization, called Evolutionary\nAlgorithm for Graph Anonymization (EAGA), based on edge modifications to\npreserve the k-anonymity model.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2013 10:31:31 GMT"}, {"version": "v2", "created": "Wed, 26 Mar 2014 10:36:22 GMT"}], "update_date": "2014-03-27", "authors_parsed": [["Casas-Roma", "Jordi", ""], ["Herrera-Joancomart\u00ed", "Jordi", ""], ["Torra", "Vicen\u00e7", ""]]}, {"id": "1310.1161", "submitter": "Arko Provo Mukherjee", "authors": "Bibudh Lahiri, Arko Provo Mukherjee, Srikanta Tirthapura", "title": "Identifying Correlated Heavy-Hitters in a Two-Dimensional Data Stream", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider online mining of correlated heavy-hitters from a data stream.\nGiven a stream of two-dimensional data, a correlated aggregate query first\nextracts a substream by applying a predicate along a primary dimension, and\nthen computes an aggregate along a secondary dimension. Prior work on\nidentifying heavy-hitters in streams has almost exclusively focused on\nidentifying heavy-hitters on a single dimensional stream, and these yield\nlittle insight into the properties of heavy-hitters along other dimensions. In\ntypical applications however, an analyst is interested not only in identifying\nheavy-hitters, but also in understanding further properties such as: what other\nitems appear frequently along with a heavy-hitter, or what is the frequency\ndistribution of items that appear along with the heavy-hitters. We consider\nqueries of the following form: In a stream S of (x, y) tuples, on the substream\nH of all x values that are heavy-hitters, maintain those y values that occur\nfrequently with the x values in H. We call this problem as Correlated\nHeavy-Hitters (CHH). We formulate an approximate formulation of CHH\nidentification, and present an algorithm for tracking CHHs on a data stream.\nThe algorithm is easy to implement and uses workspace which is orders of\nmagnitude smaller than the stream itself. We present provable guarantees on the\nmaximum error, as well as detailed experimental results that demonstrate the\nspace-accuracy trade-off.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2013 03:48:41 GMT"}], "update_date": "2013-10-07", "authors_parsed": [["Lahiri", "Bibudh", ""], ["Mukherjee", "Arko Provo", ""], ["Tirthapura", "Srikanta", ""]]}, {"id": "1310.1190", "submitter": "Sanjay Kumar Padhi san", "authors": "Priyanka Dash, Ranjita Rout, Satya Bhusan Pratihari, Sanjay Kumar\n  Padhi", "title": "Review on Fragment Allocation by using Clustering Technique in\n  Distributed Database System", "comments": "9 pages,3 figures", "journal-ref": "IJCSN,October,2013,Volume-2 Issue-5", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Considerable Progress has been made in the last few years in improving the\nperformance of the distributed database systems. The development of Fragment\nallocation models in Distributed database is becoming difficult due to the\ncomplexity of huge number of sites and their communication considerations.\nUnder such conditions, simulation of clustering and data allocation is adequate\ntools for understanding and evaluating the performance of data allocation in\nDistributed databases. Clustering sites and fragment allocation are key\nchallenges in Distributed database performance, and are considered to be\nefficient methods that have a major role in reducing transferred and accessed\ndata during the execution of applications. In this paper a review on Fragment\nallocation by using Clustering technique is given in Distributed Database\nSystem.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2013 07:43:00 GMT"}], "update_date": "2013-10-07", "authors_parsed": [["Dash", "Priyanka", ""], ["Rout", "Ranjita", ""], ["Pratihari", "Satya Bhusan", ""], ["Padhi", "Sanjay Kumar", ""]]}, {"id": "1310.1316", "submitter": "Andr \\`e Frochaux", "authors": "Andr\\'e Frochaux, Nicole Schweikardt", "title": "A note on monadic datalog on unranked trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the article 'Recursive queries on trees and data trees' (ICDT'13),\nAbiteboul et al., asked whether the containment problem for monadic datalog\nover unordered unranked labeled trees using the child relation and the\ndescendant relation is decidable. This note gives a positive answer to this\nquestion, as well as an overview of the relative expressive power of monadic\ndatalog on various representations of unranked trees.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2013 15:21:25 GMT"}], "update_date": "2013-10-07", "authors_parsed": [["Frochaux", "Andr\u00e9", ""], ["Schweikardt", "Nicole", ""]]}, {"id": "1310.2066", "submitter": "Vinay Kumar", "authors": "Vinay Kumar and Reema Thareja", "title": "A Simplified Approach for Quality Management in Data Warehouse", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data warehousing is continuously gaining importance as organizations are\nrealizing the benefits of decision oriented data bases. However, the stumbling\nblock to this rapid development is data quality issues at various stages of\ndata warehousing. Quality can be defined as a measure of excellence or a state\nfree from defects. Users appreciate quality products and available literature\nsuggests that many organization`s have significant data quality problems that\nhave substantial social and economic impacts. A metadata based quality system\nis introduced to manage quality of data in data warehouse. The approach is used\nto analyze the quality of data warehouse system by checking the expected value\nof quality parameters with that of actual values. The proposed approach is\nsupported with a metadata framework that can store additional information to\nanalyze the quality parameters, whenever required.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2013 10:04:50 GMT"}], "update_date": "2013-10-09", "authors_parsed": [["Kumar", "Vinay", ""], ["Thareja", "Reema", ""]]}, {"id": "1310.2079", "submitter": "Faten Kharbat", "authors": "Ajayeb Abu Dabbes and Faten Kharbat", "title": "Mining The Relationship Between Demographic Variables And Brand\n  Associations", "comments": "10 pages, 2 tables, 1 figure, Journal paper", "journal-ref": "Abu Daabes, A. and Kharbat F., (2013), Mining the Relationship\n  Between Demographic Variables and Brand Associations, The International\n  Journal of Managing Value and Supply Chains, vol 4, no 3", "doi": "10.5121/ijmvsc.2013.4301", "report-no": null, "categories": "cs.CY cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research aims to mine the relationship between demographic variables and\nbrand associations, and study the relative importance of these variables. The\nstudy is conducted on fast-food restaurant brands chains in Jordan. The result\nranks and evaluates the demographic variables in relation with the brand\nassociations for the selected sample. Discovering brand associations according\nto demographic variables reveals many facts and linkages in the context of\nJordanian culture. Suggestions are given accordingly for marketers to benefits\nfrom to build their strategies and direct their decisions. Also, data mining\ntechnique used in this study reflects a new trend for studying and analyzing\nmarketing samples.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2013 10:30:28 GMT"}], "update_date": "2013-10-09", "authors_parsed": [["Dabbes", "Ajayeb Abu", ""], ["Kharbat", "Faten", ""]]}, {"id": "1310.2367", "submitter": "Dhanamma Jagli", "authors": "Mrs.Dhanamma Jagli, Ms.Priyanka Gaikwad, Ms.Shubhangi Gunjal,\n  Mr.Chaitanya Bilaware", "title": "Handy Annotations within Oracle 10g", "comments": "5Pages,2Figures", "journal-ref": "International Journal of Scientific & Engineering Research Volume\n  4, Issue 1, January-2013", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes practical observations during the Database system Lab.\nOracle 10g DBMS is used in the data base system lab and performed SQL queries\nbased many concepts like Data Definition Language Commands (DDL), Data\nModification Language Commands ((DML), Views, Integrity Constraints, Aggregate\nfunctions, Joins and Abstract type . While performing practical during the lab\nsession, many problems occurred, in order to solve them many text books and\nwebsites referred but could not obtain expected help from them. Even though by\nspending much time in the database labs with Oracle 10g, tried in numerous\nways, as a final point expected output is achieved. This paper describes\nannotations which were experimentally proved in the Database lab.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2013 06:22:15 GMT"}], "update_date": "2013-10-10", "authors_parsed": [["Jagli", "Mrs. Dhanamma", ""], ["Gaikwad", "Ms. Priyanka", ""], ["Gunjal", "Ms. Shubhangi", ""], ["Bilaware", "Mr. Chaitanya", ""]]}, {"id": "1310.2375", "submitter": "Dhanamma Jagli", "authors": "Dhanamma Jagli, Sangeeta Oswal", "title": "Web Usage Mining: Pattern Discovery and Forecasting", "comments": null, "journal-ref": "IFRSA International Journal of Data Warehousing & Mining |Vol\n  2|issue4|November 2012", "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web usage mining: automatic discovery of patterns in clickstreams and\nassociated data collected or generated as a result of user interactions with\none or more Web sites. This paper describes web usage mining for our college\nlog files to analyze the behavioral patterns and profiles of users interacting\nwith a Web site. The discovered patterns are represented as clusters that are\nfrequently accessed by groups of visitors with common interests. In this paper,\nthe visitors and hits were forecasted to predict the further access statistics.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2013 07:19:40 GMT"}], "update_date": "2013-10-10", "authors_parsed": [["Jagli", "Dhanamma", ""], ["Oswal", "Sangeeta", ""]]}, {"id": "1310.3107", "submitter": "Marek Zawirski", "authors": "Marek Zawirski (INRIA Rocquencourt, LIP6), Annette Bieniusa, Valter\n  Balegas (CITI), S\\'ergio Duarte (CITI), Carlos Baquero (Universidade do Minho\n  Departamento de Inform\\'atica), Marc Shapiro (INRIA Rocquencourt, LIP6), Nuno\n  Pregui\\c{c}a (CITI)", "title": "SwiftCloud: Fault-Tolerant Geo-Replication Integrated all the Way to the\n  Client Machine", "comments": null, "journal-ref": "N&deg; RR-8347 (2013)", "doi": null, "report-no": "RR-8347", "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Client-side logic and storage are increasingly used in web and mobile\napplications to improve response time and availability. Current approaches tend\nto be ad-hoc and poorly integrated with the server-side logic. We present a\nprincipled approach to integrate client- and server-side storage. We support\nmergeable and strongly consistent transactions that target either client or\nserver replicas and provide access to causally-consistent snapshots\nefficiently. In the presence of infrastructure faults, a client-assisted\nfailover solution allows client execution to resume immediately and seamlessly\naccess consistent snapshots without waiting. We implement this approach in\nSwiftCloud, the first transactional system to bring geo-replication all the way\nto the client machine. Example applications show that our programming model is\nuseful across a range of application areas. Our experimental evaluation shows\nthat SwiftCloud provides better fault tolerance and at the same time can\nimprove both latency and throughput by up to an order of magnitude, compared to\nclassical geo-replication techniques.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2013 12:38:58 GMT"}], "update_date": "2013-10-14", "authors_parsed": [["Zawirski", "Marek", "", "INRIA Rocquencourt, LIP6"], ["Bieniusa", "Annette", "", "CITI"], ["Balegas", "Valter", "", "CITI"], ["Duarte", "S\u00e9rgio", "", "CITI"], ["Baquero", "Carlos", "", "Universidade do Minho\n  Departamento de Inform\u00e1tica"], ["Shapiro", "Marc", "", "INRIA Rocquencourt, LIP6"], ["Pregui\u00e7a", "Nuno", "", "CITI"]]}, {"id": "1310.3314", "submitter": "Hung Ngo", "authors": "Hung Q. Ngo and Christopher Re and Atri Rudra", "title": "Skew Strikes Back: New Developments in the Theory of Join Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating the relational join is one of the central algorithmic and most\nwell-studied problems in database systems. A staggering number of variants have\nbeen considered including Block-Nested loop join, Hash-Join, Grace, Sort-merge\nfor discussions of more modern issues). Commercial database engines use finely\ntuned join heuristics that take into account a wide variety of factors\nincluding the selectivity of various predicates, memory, IO, etc. In spite of\nthis study of join queries, the textbook description of join processing is\nsuboptimal. This survey describes recent results on join algorithms that have\nprovable worst-case optimality runtime guarantees. We survey recent work and\nprovide a simpler and unified description of these algorithms that we hope is\nuseful for theory-minded readers, algorithm designers, and systems\nimplementors.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2013 00:08:02 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2013 15:39:40 GMT"}], "update_date": "2013-10-17", "authors_parsed": [["Ngo", "Hung Q.", ""], ["Re", "Christopher", ""], ["Rudra", "Atri", ""]]}, {"id": "1310.3939", "submitter": "Domenico Sacca'", "authors": "Domenico Sacca', Edoardo Serra, Pietro Dicosta, Antonio Piccolo", "title": "Multi-Sorted Inverse Frequent Itemsets Mining", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of novel platforms and techniques for emerging \"Big Data\"\napplications requires the availability of real-life datasets for data-driven\nexperiments, which are however out of reach for academic research in most cases\nas they are typically proprietary. A possible solution is to use synthesized\ndatasets that reflect patterns of real ones in order to ensure high quality\nexperimental findings. A first step in this direction is to use inverse mining\ntechniques such as inverse frequent itemset mining (IFM) that consists of\ngenerating a transactional database satisfying given support constraints on the\nitemsets in an input set, that are typically the frequent ones. This paper\nintroduces an extension of IFM, called many-sorted IFM, where the schemes for\nthe datasets to be generated are those typical of Big Tables as required in\nemerging big data applications, e.g., social network analytics.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2013 07:38:36 GMT"}], "update_date": "2013-10-16", "authors_parsed": [["Sacca'", "Domenico", ""], ["Serra", "Edoardo", ""], ["Dicosta", "Pietro", ""], ["Piccolo", "Antonio", ""]]}, {"id": "1310.4136", "submitter": "Thiago S. F. X. Teixeira", "authors": "Thiago S. F. X. Teixeira, George Teodoro, Eduardo Valle, Joel H. Saltz", "title": "Scalable Locality-Sensitive Hashing for Similarity Search in\n  High-Dimensional, Large-Scale Multimedia Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity search is critical for many database applications, including the\nincreasingly popular online services for Content-Based Multimedia Retrieval\n(CBMR). These services, which include image search engines, must handle an\noverwhelming volume of data, while keeping low response times. Thus,\nscalability is imperative for similarity search in Web-scale applications, but\nmost existing methods are sequential and target shared-memory machines. Here we\naddress these issues with a distributed, efficient, and scalable index based on\nLocality-Sensitive Hashing (LSH). LSH is one of the most efficient and popular\ntechniques for similarity search, but its poor referential locality properties\nhas made its implementation a challenging problem. Our solution is based on a\nwidely asynchronous dataflow parallelization with a number of optimizations\nthat include a hierarchical parallelization to decouple indexing and data\nstorage, locality-aware data partition strategies to reduce message passing,\nand multi-probing to limit memory usage. The proposed parallelization attained\nan efficiency of 90% in a distributed system with about 800 CPU cores. In\nparticular, the original locality-aware data partition reduced the number of\nmessages exchanged in 30%. Our parallel LSH was evaluated using the largest\npublic dataset for similarity search (to the best of our knowledge) with $10^9$\n128-d SIFT descriptors extracted from Web images. This is two orders of\nmagnitude larger than datasets that previous LSH parallelizations could handle.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2013 18:21:39 GMT"}], "update_date": "2013-10-16", "authors_parsed": [["Teixeira", "Thiago S. F. X.", ""], ["Teodoro", "George", ""], ["Valle", "Eduardo", ""], ["Saltz", "Joel H.", ""]]}, {"id": "1310.4647", "submitter": "Kodge B. G.", "authors": "Sudhir B Jagtap, Kodge B. G", "title": "Census Data Mining and Data Analysis using WEKA", "comments": "06 pages, 03 figures. International Conference in Emerging Trends in\n  Science, Technology and Management-2013, Singapore", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Data mining (also known as knowledge discovery from databases) is the process\nof extraction of hidden, previously unknown and potentially useful information\nfrom databases. The outcome of the extracted data can be analyzed for the\nfuture planning and development perspectives. In this paper, we have made an\nattempt to demonstrate how one can extract the local (district) level census,\nsocio-economic and population related other data for knowledge discovery and\ntheir analysis using the powerful data mining tool Weka.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2013 10:21:58 GMT"}], "update_date": "2013-10-18", "authors_parsed": [["Jagtap", "Sudhir B", ""], ["G", "Kodge B.", ""]]}, {"id": "1310.4802", "submitter": "Xavier Martinez-Palau", "authors": "Xavier Martinez-Palau, David Dominguez-Sal, Reza Akbarinia, Patrick\n  Valduriez, Josep Llu\\'is Larriba-Pey", "title": "On Demand Memory Specialization for Distributed Graph Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose the DN-tree that is a data structure to build lossy\nsummaries of the frequent data access patterns of the queries in a distributed\ngraph data management system. These compact representations allow us an\nefficient communication of the data structure in distributed systems. We\nexploit this data structure with a new \\textit{Dynamic Data Partitioning}\nstrategy (DYDAP) that assigns the portions of the graph according to historical\ndata access patterns, and guarantees a small network communication and a\ncomputational load balance in distributed graph queries. This method is able to\nadapt dynamically to new workloads and evolve when the query distribution\nchanges. Our experiments show that DYDAP yields a throughput up to an order of\nmagnitude higher than previous methods based on cache specialization, in a\nvariety of scenarios, and the average response time of the system is divided by\ntwo.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2013 15:04:06 GMT"}], "update_date": "2013-10-18", "authors_parsed": [["Martinez-Palau", "Xavier", ""], ["Dominguez-Sal", "David", ""], ["Akbarinia", "Reza", ""], ["Valduriez", "Patrick", ""], ["Larriba-Pey", "Josep Llu\u00eds", ""]]}, {"id": "1310.4954", "submitter": "Miguel A. Martinez-Prieto", "authors": "Sandra \\'Alvarez-Garc\\'ia and Nieves R. Brisaboa and Javier D.\n  Fern\\'andez and Miguel A. Mart\\'inez-Prieto and Gonzalo Navarro", "title": "Compressed Vertical Partitioning for Full-In-Memory RDF Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Web of Data has been gaining momentum and this leads to increasingly\npublish more semi-structured datasets following the RDF model, based on atomic\ntriple units of subject, predicate, and object. Although it is a simple model,\ncompression methods become necessary because datasets are increasingly larger\nand various scalability issues arise around their organization and storage.\nThis requirement is more restrictive in RDF stores because efficient SPARQL\nresolution on the compressed RDF datasets is also required.\n  This article introduces a novel RDF indexing technique (called k2-triples)\nsupporting efficient SPARQL resolution in compressed space. k2-triples, uses\nthe predicate to vertically partition the dataset into disjoint subsets of\npairs (subject, object), one per predicate. These subsets are represented as\nbinary matrices in which 1-bits mean that the corresponding triple exists in\nthe dataset. This model results in very sparse matrices, which are efficiently\ncompressed using k2-trees. We enhance this model with two compact indexes\nlisting the predicates related to each different subject and object, in order\nto address the specific weaknesses of vertically partitioned representations.\nThe resulting technique not only achieves by far the most compressed\nrepresentations, but also the best overall performance for RDF retrieval in our\nexperiments. Our approach uses up to 10 times less space than a state of the\nart baseline, and outperforms its performance by several order of magnitude on\nthe most basic query patterns. In addition, we optimize traditional join\nalgorithms on k2-triples and define a novel one leveraging its specific\nfeatures. Our experimental results show that our technique overcomes\ntraditional vertical partitioning for join resolution, reporting the best\nnumbers for joins in which the non-joined nodes are provided, and being\ncompetitive in the majority of the cases.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2013 08:58:01 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2013 09:00:47 GMT"}], "update_date": "2013-10-22", "authors_parsed": [["\u00c1lvarez-Garc\u00eda", "Sandra", ""], ["Brisaboa", "Nieves R.", ""], ["Fern\u00e1ndez", "Javier D.", ""], ["Mart\u00ednez-Prieto", "Miguel A.", ""], ["Navarro", "Gonzalo", ""]]}, {"id": "1310.5254", "submitter": "Ijaz Bukhari ijaz bukhari", "authors": "Syed Ijaz Ahmad Bukhari", "title": "Real Time Data Warehouse", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data Warehouse (DW) is an essential part of Business Intelligence. DW emerged\nas a fast growing reporting and analysis technique in early 1980s. Today, it\nhas almost replaced relational databases. However, with passage of time, static\nand historic data of DWs could not produce Real Time reporting and analysis,\nthus giving a way to emerge the Idea of Real Time Data Warehouse (RTDW).\nAlthough, there are problems with RTDWs, but with advancement in technology and\nresearchers focus, RTDWs will be able to generate real time reports, analysis\nand forecasting.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2013 17:30:48 GMT"}], "update_date": "2013-10-22", "authors_parsed": [["Bukhari", "Syed Ijaz Ahmad", ""]]}, {"id": "1310.5463", "submitter": "Ioanna Lykourentzou", "authors": "Muhammad Imran, Ioanna Lykourentzou, Yannick Naudet, Carlos Castillo", "title": "Engineering Crowdsourced Stream Processing Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A crowdsourced stream processing system (CSP) is a system that incorporates\ncrowdsourced tasks in the processing of a data stream. This can be seen as\nenabling crowdsourcing work to be applied on a sample of large-scale data at\nhigh speed, or equivalently, enabling stream processing to employ human\nintelligence. It also leads to a substantial expansion of the capabilities of\ndata processing systems. Engineering a CSP system requires the combination of\nhuman and machine computation elements. From a general systems theory\nperspective, this means taking into account inherited as well as emerging\nproperties from both these elements. In this paper, we position CSP systems\nwithin a broader taxonomy, outline a series of design principles and evaluation\nmetrics, present an extensible framework for their design, and describe several\ndesign patterns. We showcase the capabilities of CSP systems by performing a\ncase study that applies our proposed framework to the design and analysis of a\nreal system (AIDR) that classifies social media messages during time-critical\ncrisis events. Results show that compared to a pure stream processing system,\nAIDR can achieve a higher data classification accuracy, while compared to a\npure crowdsourcing solution, the system makes better use of human workers by\nrequiring much less manual work effort.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2013 08:46:29 GMT"}, {"version": "v2", "created": "Fri, 25 Jul 2014 21:59:16 GMT"}, {"version": "v3", "created": "Mon, 4 Aug 2014 08:54:40 GMT"}], "update_date": "2014-08-05", "authors_parsed": [["Imran", "Muhammad", ""], ["Lykourentzou", "Ioanna", ""], ["Naudet", "Yannick", ""], ["Castillo", "Carlos", ""]]}, {"id": "1310.5841", "submitter": "Naoual Mouhni", "authors": "Naoual Mouhni, Abderrafiaa Elkalay", "title": "Ontology based data warehouses federation management system", "comments": "6 pages", "journal-ref": "IJCSI Intenational Journal of Computer Science Issues, Vol. 10,\n  Issue 4, No 1, July 2013", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data warehouses are nowadays an important component in every competitive\nsystem, it's one of the main components on which business intelligence is\nbased. We can even say that many companies are climbing to the next level and\nuse a set of Data warehouses to provide the complete information or it's\ngenerally due to fusion of two or many companies. these Data warehouses can be\nheterogeneous and geographically separated, this structure is what we call\nfederation, and even if the components are physically separated, they are\nlogically seen as a single component. generally, these items are heterogeneous\nwhich make it difficult to create the logical federation schema,and the\nexecution of user queries a complicated mission. In this paper, we will fill\nthis gap by proposing an extension of an existent algorithm in order to treat\ndifferent schema types (star, snow flack) including the treatment of\nhierarchies dimension using ontology\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2013 08:50:41 GMT"}], "update_date": "2013-10-23", "authors_parsed": [["Mouhni", "Naoual", ""], ["Elkalay", "Abderrafiaa", ""]]}, {"id": "1310.6257", "submitter": "Wolfgang Gatterbauer", "authors": "Wolfgang Gatterbauer, Dan Suciu", "title": "Dissociation and Propagation for Approximate Lifted Inference with\n  Standard Relational Database Management Systems", "comments": "33 pages, 27 figures, pre-print for VLDBJ full version of\n  arXiv:1412.1069 [PVLDB 8(5):629-640, 2015: \"Approximate lifted inference with\n  probabilistic databases\", http://www.vldb.org/pvldb/vol8/p629-gatterbauer.pdf\n  ]. Former working title: \"Dissociation and Propagation for Efficient Query\n  Evaluation over Probabilistic Databases\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic inference over large data sets is a challenging data management\nproblem since exact inference is generally #P-hard and is most often solved\napproximately with sampling-based methods today. This paper proposes an\nalternative approach for approximate evaluation of conjunctive queries with\nstandard relational databases: In our approach, every query is evaluated\nentirely in the database engine by evaluating a fixed number of query plans,\neach providing an upper bound on the true probability, then taking their\nminimum. We provide an algorithm that takes into account important schema\ninformation to enumerate only the minimal necessary plans among all possible\nplans. Importantly, this algorithm is a strict generalization of all known\nPTIME self-join-free conjunctive queries: A query is in PTIME if and only if\nour algorithm returns one single plan. Furthermore, our approach is a\ngeneralization of a family of efficient ranking methods from graphs to\nhypergraphs. We also adapt three relational query optimization techniques to\nevaluate all necessary plans very fast. We give a detailed experimental\nevaluation of our approach and, in the process, provide a new way of thinking\nabout the value of probabilistic methods over non-probabilistic methods for\nranking query answers. We also note that the techniques developed in this paper\napply immediately to lifted inference from statistical relational models since\nlifted inference corresponds to PTIME plans in probabilistic databases.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2013 15:14:41 GMT"}, {"version": "v2", "created": "Tue, 12 Aug 2014 16:40:04 GMT"}, {"version": "v3", "created": "Sat, 2 Jan 2016 15:10:53 GMT"}, {"version": "v4", "created": "Tue, 14 Jun 2016 15:22:13 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Gatterbauer", "Wolfgang", ""], ["Suciu", "Dan", ""]]}, {"id": "1310.6780", "submitter": "Arko Provo Mukherjee", "authors": "Arko Provo Mukherjee and Pan Xu and Srikanta Tirthapura", "title": "Mining Maximal Cliques from an Uncertain Graph", "comments": "ICDE 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider mining dense substructures (maximal cliques) from an uncertain\ngraph, which is a probability distribution on a set of deterministic graphs.\nFor parameter 0 < {\\alpha} < 1, we present a precise definition of an\n{\\alpha}-maximal clique in an uncertain graph. We present matching upper and\nlower bounds on the number of {\\alpha}-maximal cliques possible within an\nuncertain graph. We present an algorithm to enumerate {\\alpha}-maximal cliques\nin an uncertain graph whose worst-case runtime is near-optimal, and an\nexperimental evaluation showing the practical utility of the algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2013 21:29:44 GMT"}, {"version": "v2", "created": "Fri, 9 May 2014 21:25:37 GMT"}, {"version": "v3", "created": "Wed, 22 Oct 2014 07:55:18 GMT"}], "update_date": "2014-10-23", "authors_parsed": [["Mukherjee", "Arko Provo", ""], ["Xu", "Pan", ""], ["Tirthapura", "Srikanta", ""]]}, {"id": "1310.6833", "submitter": "Mary Sowjanya alamanda", "authors": "A.M.Sowjanya, M.Shashi", "title": "New Proximity Estimate for Incremental Update of Non-uniformly\n  Distributed Clusters", "comments": "19 pages", "journal-ref": "International Journal of Data Mining & Knowledge Management\n  Process (IJDKP) Vol.3, No.5, September 2013", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conventional clustering algorithms mine static databases and generate a\nset of patterns in the form of clusters. Many real life databases keep growing\nincrementally. For such dynamic databases, the patterns extracted from the\noriginal database become obsolete. Thus the conventional clustering algorithms\nare not suitable for incremental databases due to lack of capability to modify\nthe clustering results in accordance with recent updates. In this paper, the\nauthor proposes a new incremental clustering algorithm called CFICA(Cluster\nFeature-Based Incremental Clustering Approach for numerical data) to handle\nnumerical data and suggests a new proximity metric called Inverse Proximity\nEstimate (IPE) which considers the proximity of a data point to a cluster\nrepresentative as well as its proximity to a farthest point in its vicinity.\nCFICA makes use of the proposed proximity metric to determine the membership of\na data point into a cluster.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2013 06:50:48 GMT"}], "update_date": "2013-10-28", "authors_parsed": [["Sowjanya", "A. M.", ""], ["Shashi", "M.", ""]]}, {"id": "1310.7205", "submitter": "Moritz Schattka", "authors": "Moritz Schattka", "title": "Algorithms for Timed Consistency Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the major challenges in distributed systems is establishing\nconsistency among replicated data in a timely fashion. While the consistent\nordering of events has been extensively researched, the time span to reach a\nconsistent state is mostly considered an effect of the chosen consistency\nmodel, rather than being considered a parameter itself. This paper argues that\nit is possible to give guarantees on the timely consistency of an operation.\nSubsequent to an update the cloud and all connected clients will either be\nconsistent with the update within the defined upper bound of time or the update\nwill be returned. This paper suggests the respective algorithms and protocols\ncapable of producing such comprehensive Timed Consistency, as conceptually\nproposed by Torres-Rojas et al. The solution offers business customers an\nincreasing level of predictability and adjustability. The temporal certainty\nconcerning the execution makes the cloud a more attractive tool for\ntime-critical or mission-critical applications fearing the poor availability of\nStrong Consistency in cloud environments.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2013 15:39:18 GMT"}], "update_date": "2013-10-29", "authors_parsed": [["Schattka", "Moritz", ""]]}, {"id": "1310.7297", "submitter": "Farhana Murtaza Choudhury", "authors": "Farhana Murtaza Choudhury, Mohammed Eunus Ali, Sarah Masud, Suman\n  Nath, Ishat E Rabban", "title": "Scalable Visibility Color Map Construction in Spatial Databases", "comments": "12 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in 3D modeling provide us with real 3D datasets to answer\nqueries, such as \"What is the best position for a new billboard?\" and \"Which\nhotel room has the best view?\" in the presence of obstacles. These applications\nrequire measuring and differentiating the visibility of an object (target) from\ndifferent viewpoints in a dataspace, e.g., a billboard may be seen from two\nviewpoints but is readable only from the viewpoint closer to the target. In\nthis paper, we formulate the above problem of quantifying the visibility of\n(from) a target object from (of) the surrounding area with a visibility color\nmap (VCM). A VCM is essentially defined as a surface color map of the space,\nwhere each viewpoint of the space is assigned a color value that denotes the\nvisibility measure of the target from that viewpoint. Measuring the visibility\nof a target even from a single viewpoint is an expensive operation, as we need\nto consider factors such as distance, angle, and obstacles between the\nviewpoint and the target. Hence, a straightforward approach to construct the\nVCM that requires visibility computation for every viewpoint of the surrounding\nspace of the target, is prohibitively expensive in terms of both I/Os and\ncomputation, especially for a real dataset comprising of thousands of\nobstacles. We propose an efficient approach to compute the VCM based on a key\nproperty of the human vision that eliminates the necessity of computing the\nvisibility for a large number of viewpoints of the space. To further reduce the\ncomputational overhead, we propose two approximations; namely, minimum bounding\nrectangle and tangential approaches with guaranteed error bounds. Our extensive\nexperiments demonstrate the effectiveness and efficiency of our solutions to\nconstruct the VCM for real 2D and 3D datasets.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2013 02:38:26 GMT"}], "update_date": "2013-10-29", "authors_parsed": [["Choudhury", "Farhana Murtaza", ""], ["Ali", "Mohammed Eunus", ""], ["Masud", "Sarah", ""], ["Nath", "Suman", ""], ["Rabban", "Ishat E", ""]]}, {"id": "1310.7829", "submitter": "Minyar Sassi", "authors": "Ines Benali Sougui, Minyar Sassi Hidri, Amel Grissa-Touzi", "title": "About Summarization in Large Fuzzy Databases", "comments": null, "journal-ref": "The 5th International Conference on Advances in Databases,\n  Knowledge, and Data Applications (DBKDA), pp. 87-94, 2013", "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Moved by the need increased for modeling of the fuzzy data, the success of\nthe systems of exact generation of summary of data, we propose in this paper, a\nnew approach of generation of summary from fuzzy data called Fuzzy-SaintEtiQ.\nThis approach is an extension of the SaintEtiQ model to support the fuzzy data.\nIt presents the following optimizations such as 1) the minimization of the\nexpert risk; 2) the construction of a more detailed and more precise summaries\nhierarchy, and 3) the co-operation with the user by giving him fuzzy summaries\nin different hierarchical levels\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2013 15:17:39 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Sougui", "Ines Benali", ""], ["Hidri", "Minyar Sassi", ""], ["Grissa-Touzi", "Amel", ""]]}, {"id": "1310.8462", "submitter": "Radhakrishnan B", "authors": "Radhakrishnan B, Shineraj G, Anver Muhammed K.M", "title": "Application of Data Mining In Marketing", "comments": "06 Pages, 02 Figures, 01 Table, Volume 2, Issue 5", "journal-ref": "IJCSN - International Journal of Computer Science and Network -\n  October 2013", "doi": null, "report-no": "IJCSN-2013-2-5-47", "categories": "cs.DB cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most important problems in modern finance is finding efficient\nways to summarize and visualize the stock market data to give individuals or\ninstitutions useful information about the market behavior for investment\ndecisions. The enormous amount of valuable data generated by the stock market\nhas attracted researchers to explore this problem domain using different\nmethodologies. Potential significant benefits of solving these problems\nmotivated extensive research for years. The research in data mining has gained\na high attraction due to the importance of its applications and the increasing\ngeneration information. This paper provides an overview of application of data\nmining techniques such as decision tree. Also, this paper reveals progressive\napplications in addition to existing gap and less considered area and\ndetermines the future works for researchers.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2013 11:16:42 GMT"}], "update_date": "2013-11-01", "authors_parsed": [["B", "Radhakrishnan", ""], ["G", "Shineraj", ""], ["M", "Anver Muhammed K.", ""]]}]