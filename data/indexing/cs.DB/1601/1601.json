[{"id": "1601.00073", "submitter": "Oliver Kennedy", "authors": "Arindam Nandi, Ying Yang, Oliver Kennedy, Boris Glavic, Ronny Fehling,\n  Zhen Hua Liu, Dieter Gawlick", "title": "Mimir: Bringing CTables into Practice", "comments": "Under submission; The first two authors should be considered a joint\n  first-author", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present state of the art in analytics requires high upfront investment of\nhuman effort and computational resources to curate datasets, even before the\nfirst query is posed. So-called pay-as-you-go data curation techniques allow\nthese high costs to be spread out, first by enabling queries over uncertain and\nincomplete data, and then by assessing the quality of the query results. We\ndescribe the design of a system, called Mimir, around a recently introduced\nclass of probabilistic pay-as-you-go data cleaning operators called Lenses.\nMimir wraps around any deterministic database engine using JDBC, extending it\nwith support for probabilistic query processing. Queries processed through\nMimir produce uncertainty-annotated result cursors that allow client\napplications to quickly assess result quality and provenance. We also present a\nGUI that provides analysts with an interactive tool for exploring the\nuncertainty exposed by the system. Finally, we present optimizations that make\nLenses scalable, and validate this claim through experimental evidence.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2016 11:21:33 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Nandi", "Arindam", ""], ["Yang", "Ying", ""], ["Kennedy", "Oliver", ""], ["Glavic", "Boris", ""], ["Fehling", "Ronny", ""], ["Liu", "Zhen Hua", ""], ["Gawlick", "Dieter", ""]]}, {"id": "1601.00159", "submitter": "Zhongle Xie", "authors": "Zhongle Xie, Qingchao Cai, H.V. Jagadish, Beng Chin Ooi, Weng-Fai Wong", "title": "PI : a Parallel in-memory skip list based Index", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Due to the coarse granularity of data accesses and the heavy use of latches,\nindices in the B-tree family are not efficient for in-memory databases,\nespecially in the context of today's multi-core architecture.\n  In this paper, we present PI, a Parallel in-memory skip list based Index that\nlends itself naturally to the parallel and concurrent environment, particularly\nwith non-uniform memory access. In PI, incoming queries are collected, and\ndisjointly distributed among multiple threads for processing to avoid the use\nof latches. For each query, PI traverses the index in a Breadth-First-Search\n(BFS) manner to find the list node with the matching key, exploiting SIMD\nprocessing to speed up the search process. In order for query processing to be\nlatch-free, PI employs a light-weight communication protocol that enables\nthreads to re-distribute the query workload among themselves such that each\nlist node that will be modified as a result of query processing will be\naccessed by exactly one thread. We conducted extensive experiments, and the\nresults show that PI can be up to three times as fast as the Masstree, a\nstate-of-the-art B-tree based index.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2016 10:11:57 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Xie", "Zhongle", ""], ["Cai", "Qingchao", ""], ["Jagadish", "H. V.", ""], ["Ooi", "Beng Chin", ""], ["Wong", "Weng-Fai", ""]]}, {"id": "1601.00182", "submitter": "Qingchao Cai", "authors": "Dawei Jiang, Qingchao Cai, Gang Chen, H. V. Jagadish, Beng Chin Ooi,\n  Kian-Lee Tan, Anthony K. H. Tung", "title": "Cohort Query Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern Internet applications often produce a large volume of user activity\nrecords. Data analysts are interested in cohort analysis, or finding unusual\nuser behavioral trends, in these large tables of activity records. In a\ntraditional database system, cohort analysis queries are both painful to\nspecify and expensive to evaluate. We propose to extend database systems to\nsupport cohort analysis. We do so by extending SQL with three new operators. We\ndevise three different evaluation schemes for cohort query processing. Two of\nthem adopt a non-intrusive approach. The third approach employs a columnar\nbased evaluation scheme with optimizations specifically designed for cohort\nquery processing. Our experimental results confirm the performance benefits of\nour proposed columnar database system, compared against the two non-intrusive\napproaches that implement cohort queries on top of regular relational\ndatabases.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2016 15:21:19 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2016 03:59:49 GMT"}, {"version": "v3", "created": "Tue, 3 May 2016 03:41:57 GMT"}, {"version": "v4", "created": "Wed, 4 May 2016 12:00:41 GMT"}], "update_date": "2016-05-05", "authors_parsed": [["Jiang", "Dawei", ""], ["Cai", "Qingchao", ""], ["Chen", "Gang", ""], ["Jagadish", "H. V.", ""], ["Ooi", "Beng Chin", ""], ["Tan", "Kian-Lee", ""], ["Tung", "Anthony K. H.", ""]]}, {"id": "1601.00524", "submitter": "Vadim Tropashko", "authors": "Vadim Tropashko", "title": "Ideal Databases", "comments": "Amended the introduction; added CoCoA section", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From algebraic geometry perspective database relations are succinctly defined\nas Finite Varieties. After establishing basic framework, we give analytic proof\nof Heath theorem from Database Dependency theory. Next, we leverage\nAlgebra/Geometry dictionary and focus on algebraic counterparts of finite\nvarieties, polynomial ideals. It is well known that intersection and sum of\nideals are lattice operations. We generalize this fact to ideals from different\nrings, therefore establishing that algebra of ideals is Relational Lattice. The\nfinal stop is casting the framework into Linear Algebra, and traversing to\nQuantum Theory.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2015 22:45:56 GMT"}, {"version": "v2", "created": "Tue, 12 Dec 2017 18:24:33 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Tropashko", "Vadim", ""]]}, {"id": "1601.00707", "submitter": "M. Tamer Ozsu", "authors": "M. Tamer \\\"Ozsu", "title": "A Survey of RDF Data Management Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RDF is increasingly being used to encode data for the semantic web and for\ndata exchange. There have been a large number of works that address RDF data\nmanagement. In this paper we provide an overview of these works.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2016 00:17:21 GMT"}], "update_date": "2016-01-06", "authors_parsed": [["\u00d6zsu", "M. Tamer", ""]]}, {"id": "1601.00738", "submitter": "Jiaan Zeng", "authors": "Jiaan Zeng", "title": "Resource Sharing for Multi-Tenant NoSQL Data Store in Cloud", "comments": "PhD dissertation, December 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.DB cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-tenancy hosting of users in cloud NoSQL data stores is favored by cloud\nproviders because it enables resource sharing at low operating cost.\nMulti-tenancy takes several forms depending on whether the back-end file system\nis a local file system (LFS) or a parallel file system (PFS), and on whether\ntenants are independent or share data across tenants. In this thesis I focus on\nand propose solutions to two cases: independent data-local file system, and\nshared data-parallel file system.\n  In the independent data-local file system case, resource contention occurs\nunder certain conditions in Cassandra and HBase, two state-of-the-art NoSQL\nstores, causing performance degradation for one tenant by another. We\ninvestigate the interference and propose two approaches. The first provides a\nscheduling scheme that can approximate resource consumption, adapt to workload\ndynamics and work in a distributed fashion. The second introduces a\nworkload-aware resource reservation approach to prevent interference. The\napproach relies on a performance model obtained offline and plans the\nreservation according to different workload resource demands. Results show the\napproaches together can prevent interference and adapt to dynamic workloads\nunder multi-tenancy.\n  In the shared data-parallel file system case, it has been shown that running\na distributed NoSQL store over PFS for shared data across tenants is not cost\neffective. Overheads are introduced due to the unawareness of the NoSQL store\nof PFS. This dissertation targets the key-value store (KVS), a specific form of\nNoSQL stores, and proposes a lightweight KVS over a parallel file system to\nimprove efficiency. The solution is built on an embedded KVS for high\nperformance but uses novel data structures to support concurrent writes.\nResults show the proposed system outperforms Cassandra and Voldemort in several\ndifferent workloads.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2016 05:15:12 GMT"}], "update_date": "2016-01-06", "authors_parsed": [["Zeng", "Jiaan", ""]]}, {"id": "1601.01746", "submitter": "Yiyang Zhou", "authors": "Shoujian Yu, Yiyang Zhou", "title": "A Prefixed-Itemset-Based Improvement For Apriori Algorithm", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Association rules is a very important part of data mining. It is used to find\nthe interesting patterns from transaction databases. Apriori algorithm is one\nof the most classical algorithms of association rules, but it has the\nbottleneck in efficiency. In this article, we proposed a prefixed-itemset-based\ndata structure for candidate itemset generation, with the help of the structure\nwe managed to improve the efficiency of the classical Apriori algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2016 02:06:05 GMT"}], "update_date": "2016-01-11", "authors_parsed": [["Yu", "Shoujian", ""], ["Zhou", "Yiyang", ""]]}, {"id": "1601.01770", "submitter": "Albert Haque", "authors": "Albert Haque", "title": "A MapReduce Approach to NoSQL RDF Databases", "comments": "Undergraduate Honors Thesis, December 2013, The University of Texas\n  at Austin, Department of Computer Science. Report# HR-13-13 (honors theses)", "journal-ref": null, "doi": null, "report-no": "HR-13-13", "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the increased need to house and process large volumes of\ndata has prompted the need for distributed storage and querying systems. The\ngrowth of machine-readable RDF triples has prompted both industry and academia\nto develop new database systems, called NoSQL, with characteristics that differ\nfrom classical databases. Many of these systems compromise ACID properties for\nincreased horizontal scalability and data availability. This thesis concerns\nthe development and evaluation of a NoSQL triplestore. Triplestores are\ndatabase management systems central to emerging technologies such as the\nSemantic Web and linked data. The evaluation spans several benchmarks,\nincluding the two most commonly used in triplestore evaluation, the Berlin\nSPARQL Benchmark, and the DBpedia benchmark, a query workload that operates an\nRDF representation of Wikipedia. Results reveal that the join algorithm used by\nthe system plays a critical role in dictating query runtimes. Distributed graph\ndatabases must carefully optimize queries before generating MapReduce query\nplans as network traffic for large datasets can become prohibitive if the query\nis executed naively.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2016 05:04:26 GMT"}], "update_date": "2016-01-11", "authors_parsed": [["Haque", "Albert", ""]]}, {"id": "1601.02034", "submitter": "Ayush Jain", "authors": "Ayush Jain, Joon Young Seo, Karan Goel, Andrew Kuznetsov, Aditya\n  Parameswaran, Hari Sundaram", "title": "It's just a matter of perspective(s): Crowd-Powered Consensus\n  Organization of Corpora", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of organizing a collection of objects - images, videos -\ninto clusters, using crowdsourcing. This problem is notoriously hard for\ncomputers to do automatically, and even with crowd workers, is challenging to\norchestrate: (a) workers may cluster based on different latent hierarchies or\nperspectives; (b) workers may cluster at different granularities even when\nclustering using the same perspective; and (c) workers may only see a small\nportion of the objects when deciding how to cluster them (and therefore have\nlimited understanding of the \"big picture\"). We develop cost-efficient,\naccurate algorithms for identifying the consensus organization (i.e., the\norganizing perspective most workers prefer to employ), and incorporate these\nalgorithms into a cost-effective workflow for organizing a collection of\nobjects, termed ORCHESTRA. We compare our algorithms with other algorithms for\nclustering, on a variety of real-world datasets, and demonstrate that ORCHESTRA\norganizes items better and at significantly lower costs.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2016 21:31:56 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Jain", "Ayush", ""], ["Seo", "Joon Young", ""], ["Goel", "Karan", ""], ["Kuznetsov", "Andrew", ""], ["Parameswaran", "Aditya", ""], ["Sundaram", "Hari", ""]]}, {"id": "1601.02433", "submitter": "Lavdim Halilaj", "authors": "Lavdim Halilaj, Irl\\'an Grangel-Gonz\\'alez, G\\\"okhan Coskun and\n  S\\\"oren Auer", "title": "Git4Voc: Git-based Versioning for Collaborative Vocabulary Development", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Collaborative vocabulary development in the context of data integration is\nthe process of finding consensus between the experts of the different systems\nand domains. The complexity of this process is increased with the number of\ninvolved people, the variety of the systems to be integrated and the dynamics\nof their domain. In this paper we advocate that the realization of a powerful\nversion control system is the heart of the problem. Driven by this idea and the\nsuccess of Git in the context of software development, we investigate the\napplicability of Git for collaborative vocabulary development. Even though\nvocabulary development and software development have much more similarities\nthan differences there are still important differences. These need to be\nconsidered within the development of a successful versioning and collaboration\nsystem for vocabulary development. Therefore, this paper starts by presenting\nthe challenges we were faced with during the creation of vocabularies\ncollaboratively and discusses its distinction to software development. Based on\nthese insights we propose Git4Voc which comprises guidelines how Git can be\nadopted to vocabulary development. Finally, we demonstrate how Git hooks can be\nimplemented to go beyond the plain functionality of Git by realizing\nvocabulary-specific features like syntactic validation and semantic diffs.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2016 13:11:51 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Halilaj", "Lavdim", ""], ["Grangel-Gonz\u00e1lez", "Irl\u00e1n", ""], ["Coskun", "G\u00f6khan", ""], ["Auer", "S\u00f6ren", ""]]}, {"id": "1601.02650", "submitter": "Dominik Tomaszuk Dr", "authors": "Dominik Tomaszuk", "title": "Inference rules for RDF(S) and OWL in N3Logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents inference rules for Resource Description Framework (RDF),\nRDF Schema (RDFS) and Web Ontology Language (OWL). Our formalization is based\non Notation 3 Logic, which extended RDF by logical symbols and created Semantic\nWeb logic for deductive RDF graph stores. We also propose OWL-P that is a\nlightweight formalism of OWL and supports soft inferences by omitting complex\nlanguage constructs.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2016 21:18:59 GMT"}], "update_date": "2016-01-13", "authors_parsed": [["Tomaszuk", "Dominik", ""]]}, {"id": "1601.02848", "submitter": "Vilem Vychodil", "authors": "Vilem Vychodil", "title": "Invariance to ordinal transformations in rank-aware databases", "comments": null, "journal-ref": "Information Sciences 378 (2017), 75-98", "doi": "10.1016/j.ins.2016.10.044", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study influence of ordinal transformations on results of queries in\nrank-aware databases which derive their operations with ranked relations from\ntotally ordered structures of scores with infima acting as aggregation\nfunctions. We introduce notions of ordinal containment and equivalence of\nranked relations and prove that infima-based algebraic operations with ranked\nrelations are invariant to ordinal transformations: Queries applied to original\nand transformed data yield results which are equivalent in terms of the order\ngiven by scores, meaning that top-k results of queries remain the same. We show\nthis important property is preserved in alternative query systems based of\nrelational calculi developed in context of G\\\"odel logic. We comment on\nrelationship to monotone query evaluation and show that the results can be\nattained in alternative rank-aware approaches.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2016 13:17:21 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Vychodil", "Vilem", ""]]}, {"id": "1601.03229", "submitter": "Jun Zhang", "authors": "Jun Zhang and Xiaokui Xiao and Xing Xie", "title": "PrivTree: A Differentially Private Algorithm for Hierarchical\n  Decompositions", "comments": "A short version of this paper will appear in SIGMOD 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set D of tuples defined on a domain Omega, we study differentially\nprivate algorithms for constructing a histogram over Omega to approximate the\ntuple distribution in D. Existing solutions for the problem mostly adopt a\nhierarchical decomposition approach, which recursively splits Omega into\nsub-domains and computes a noisy tuple count for each sub-domain, until all\nnoisy counts are below a certain threshold. This approach, however, requires\nthat we (i) impose a limit h on the recursion depth in the splitting of Omega\nand (ii) set the noise in each count to be proportional to h. This leads to\ninferior data utility due to the following dilemma: if we use a small h, then\nthe resulting histogram would be too coarse-grained to provide an accurate\napproximation of data distribution; meanwhile, a large h would yield a\nfine-grained histogram, but its quality would be severely degraded by the\nincreased amount of noise in the tuple counts.\n  To remedy the deficiency of existing solutions, we present PrivTree, a\nhistogram construction algorithm that also applies hierarchical decomposition\nbut features a crucial (and somewhat surprising) improvement: when deciding\nwhether or not to split a sub-domain, the amount of noise required in the\ncorresponding tuple count is independent of the recursive depth. This enables\nPrivTree to adaptively generate high-quality histograms without even asking for\na pre-defined threshold on the depth of sub-domain splitting. As concrete\nexamples, we demonstrate an application of PrivTree in modelling spatial data,\nand show that it can also be extended to handle sequence data (where the\ndecision in sub-domain splitting is not based on tuple counts but a more\nsophisticated measure). Our experiments on a variety of real datasets show that\nPrivTree significantly outperforms the states of the art in terms of data\nutility.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2016 13:17:08 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2016 02:51:31 GMT"}], "update_date": "2016-01-18", "authors_parsed": [["Zhang", "Jun", ""], ["Xiao", "Xiaokui", ""], ["Xie", "Xing", ""]]}, {"id": "1601.03240", "submitter": "Hubie Chen", "authors": "Hubie Chen, Stefan Mengel", "title": "Counting Answers to Existential Positive Queries: A Complexity\n  Classification", "comments": "arXiv admin note: substantial text overlap with arXiv:1501.07195", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existential positive formulas form a fragment of first-order logic that\nincludes and is semantically equivalent to unions of conjunctive queries, one\nof the most important and well-studied classes of queries in database theory.\nWe consider the complexity of counting the number of answers to existential\npositive formulas on finite structures and give a trichotomy theorem on query\nclasses, in the setting of bounded arity. This theorem generalizes and unifies\nseveral known results on the complexity of conjunctive queries and unions of\nconjunctive queries.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2016 13:40:55 GMT"}, {"version": "v2", "created": "Wed, 20 Apr 2016 13:42:55 GMT"}], "update_date": "2016-04-21", "authors_parsed": [["Chen", "Hubie", ""], ["Mengel", "Stefan", ""]]}, {"id": "1601.03797", "submitter": "Sanjay Krishnan", "authors": "Sanjay Krishnan, Jiannan Wang, Eugene Wu, Michael J. Franklin, Ken\n  Goldberg", "title": "ActiveClean: Interactive Data Cleaning While Learning Convex Loss Models", "comments": "Pre-print", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data cleaning is often an important step to ensure that predictive models,\nsuch as regression and classification, are not affected by systematic errors\nsuch as inconsistent, out-of-date, or outlier data. Identifying dirty data is\noften a manual and iterative process, and can be challenging on large datasets.\nHowever, many data cleaning workflows can introduce subtle biases into the\ntraining processes due to violation of independence assumptions. We propose\nActiveClean, a progressive cleaning approach where the model is updated\nincrementally instead of re-training and can guarantee accuracy on partially\ncleaned data. ActiveClean supports a popular class of models called convex loss\nmodels (e.g., linear regression and SVMs). ActiveClean also leverages the\nstructure of a user's model to prioritize cleaning those records likely to\naffect the results. We evaluate ActiveClean on five real-world datasets UCI\nAdult, UCI EEG, MNIST, Dollars For Docs, and WorldBank with both real and\nsynthetic errors. Our results suggest that our proposed optimizations can\nimprove model accuracy by up-to 2.5x for the same amount of data cleaned.\nFurthermore for a fixed cleaning budget and on all real dirty datasets,\nActiveClean returns more accurate models than uniform sampling and Active\nLearning.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2016 02:02:00 GMT"}], "update_date": "2016-01-18", "authors_parsed": [["Krishnan", "Sanjay", ""], ["Wang", "Jiannan", ""], ["Wu", "Eugene", ""], ["Franklin", "Michael J.", ""], ["Goldberg", "Ken", ""]]}, {"id": "1601.03892", "submitter": "Massimo Cafaro", "authors": "Massimo Cafaro, Marco Pulimeno, Italo Epicoco and Giovanni Aloisio", "title": "Mining frequent items in the time fading model", "comments": "To appear in Information Sciences, Elsevier", "journal-ref": "Information Sciences, Elsevier, 2016, Volume 370-371, pp.221-238", "doi": "10.1016/j.ins.2016.07.077", "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present FDCMSS, a new sketch-based algorithm for mining frequent items in\ndata streams. The algorithm cleverly combines key ideas borrowed from forward\ndecay, the Count-Min and the Space Saving algorithms. It works in the time\nfading model, mining data streams according to the cash register model. We\nformally prove its correctness and show, through extensive experimental\nresults, that our algorithm outperforms $\\lambda$-HCount, a recently developed\nalgorithm, with regard to speed, space used, precision attained and error\ncommitted on both synthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2016 12:21:47 GMT"}, {"version": "v2", "created": "Wed, 29 Jun 2016 10:00:22 GMT"}, {"version": "v3", "created": "Tue, 2 Aug 2016 08:24:12 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Cafaro", "Massimo", ""], ["Pulimeno", "Marco", ""], ["Epicoco", "Italo", ""], ["Aloisio", "Giovanni", ""]]}, {"id": "1601.04036", "submitter": "K. Eric Harper", "authors": "K. Eric Harper, Thijmen de Gooijer, Johannes O. Schmitt, David Cox", "title": "Microdatabases for the Industrial Internet", "comments": "5 pages, 2 figures, pending submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Industrial Internet market is targeted to grow by trillions of US dollars\nby the year 2030, driven by adoption, deployment and integration of billions of\nintelligent devices and their associated data. This digital expansion faces a\nnumber of significant challenges, including reliable data management, security\nand privacy. Realizing the benefits from this evolution is made more difficult\nbecause a typical industrial plant includes multiple vendors and legacy\ntechnology stacks. Aggregating all the raw data to a single data center before\nperforming analysis increases response times, raising performance concerns in\ntraditional markets and requiring a compromise between data duplication and\ndata access performance. Similar to the way microservices can integrate\ndisparate information technologies without imposing monolithic cross-cutting\narchitecture impacts, we propose microdatabases to manage the data\nheterogeneity of the Industrial Internet while allowing records to be captured\nand secured close to the industrial processes, but also be made available near\nthe applications that can benefit from the data. A microdatabase is an\nabstraction of a data store that standardizes and protects the interactions\nbetween distributed data sources, providers and consumers. It integrates an\ninformation model with discoverable object types that can be browsed\ninteractively and programmatically, and supports repository instances that\nevolve with their own lifecycles. The microdatabase abstraction is independent\nof technology choice and was designed based on solicitation and review of\nindustry stakeholder concerns.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2016 19:12:00 GMT"}], "update_date": "2016-01-18", "authors_parsed": [["Harper", "K. Eric", ""], ["de Gooijer", "Thijmen", ""], ["Schmitt", "Johannes O.", ""], ["Cox", "David", ""]]}, {"id": "1601.04084", "submitter": "Mohammad Sadoghi", "authors": "Mohammad Sadoghi, Souvik Bhattacherjee, Bishwaranjan Bhattacharjee,\n  Mustafa Canim", "title": "L-Store: A Real-time OLTP and OLAP System", "comments": "22 pages, 10 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arguably data is the new natural resource in the enterprise world with an\nunprecedented degree of proliferation. But to derive real-time actionable\ninsights from the data, it is important to bridge the gap between managing the\ndata that is being updated at a high velocity (i.e., OLTP) and analyzing a\nlarge volume of data (i.e., OLAP). However, there has been a divide where\nspecialized solutions were often deployed to support either OLTP or OLAP\nworkloads but not both; thus, limiting the analysis to stale and possibly\nirrelevant data. In this paper, we present Lineage-based Data Store (L-Store)\nthat combines the real-time processing of transactional and analytical\nworkloads within a single unified engine by introducing a novel lineage-based\nstorage architecture. By exploiting the lineage, we develop a contention-free\nand lazy staging of columnar data from a write-optimized form (suitable for\nOLTP) into a read-optimized form (suitable for OLAP) in a transactionally\nconsistent approach that also supports querying and retaining the current and\nhistoric data. Our working prototype of L-Store demonstrates its superiority\ncompared to state-of-the-art approaches under a comprehensive experimental\nevaluation.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2016 21:44:39 GMT"}, {"version": "v2", "created": "Sun, 26 Feb 2017 04:14:43 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Sadoghi", "Mohammad", ""], ["Bhattacherjee", "Souvik", ""], ["Bhattacharjee", "Bishwaranjan", ""], ["Canim", "Mustafa", ""]]}, {"id": "1601.04385", "submitter": "George Djorgovski", "authors": "S. G. Djorgovski, M. J. Graham, C. Donalek, A. A. Mahabal, A. J.\n  Drake, M. Turmon, T. Fuchs", "title": "Real-Time Data Mining of Massive Data Streams from Synoptic Sky Surveys", "comments": "14 pages, an invited paper for a special issue of Future Generation\n  Computer Systems, Elsevier Publ. (2015). This is an expanded version of a\n  paper arXiv:1407.3502 presented at the IEEE e-Science 2014 conf., with some\n  new content", "journal-ref": null, "doi": "10.1016/j.future.2015.10.013", "report-no": null, "categories": "astro-ph.IM cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The nature of scientific and technological data collection is evolving\nrapidly: data volumes and rates grow exponentially, with increasing complexity\nand information content, and there has been a transition from static data sets\nto data streams that must be analyzed in real time. Interesting or anomalous\nphenomena must be quickly characterized and followed up with additional\nmeasurements via optimal deployment of limited assets. Modern astronomy\npresents a variety of such phenomena in the form of transient events in digital\nsynoptic sky surveys, including cosmic explosions (supernovae, gamma ray\nbursts), relativistic phenomena (black hole formation, jets), potentially\nhazardous asteroids, etc. We have been developing a set of machine learning\ntools to detect, classify and plan a response to transient events for astronomy\napplications, using the Catalina Real-time Transient Survey (CRTS) as a\nscientific and methodological testbed. The ability to respond rapidly to the\npotentially most interesting events is a key bottleneck that limits the\nscientific returns from the current and anticipated synoptic sky surveys.\nSimilar challenge arise in other contexts, from environmental monitoring using\nsensor networks to autonomous spacecraft systems. Given the exponential growth\nof data rates, and the time-critical response, we need a fully automated and\nrobust approach. We describe the results obtained to date, and the possible\nfuture developments.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2016 02:06:45 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Djorgovski", "S. G.", ""], ["Graham", "M. J.", ""], ["Donalek", "C.", ""], ["Mahabal", "A. A.", ""], ["Drake", "A. J.", ""], ["Turmon", "M.", ""], ["Fuchs", "T.", ""]]}, {"id": "1601.04602", "submitter": "Kevin Taylor-Sakyi", "authors": "Kevin Taylor-Sakyi", "title": "Big Data: Understanding Big Data", "comments": "8 pages, Big Data Analytics, Data Storage, MapReduce,\n  Knowledge-Space, Big Data Inconsistencies", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Steve Jobs, one of the greatest visionaries of our time was quoted in 1996\nsaying \"a lot of times, people do not know what they want until you show it to\nthem\" [38] indicating he advocated products to be developed based on human\nintuition rather than research. With the advancements of mobile devices, social\nnetworks and the Internet of Things, enormous amounts of complex data, both\nstructured and unstructured are being captured in hope to allow organizations\nto make better business decisions as data is now vital for an organizations\nsuccess. These enormous amounts of data are referred to as Big Data, which\nenables a competitive advantage over rivals when processed and analyzed\nappropriately. However Big Data Analytics has a few concerns including\nManagement of Data-lifecycle, Privacy & Security, and Data Representation. This\npaper reviews the fundamental concept of Big Data, the Data Storage domain, the\nMapReduce programming paradigm used in processing these large datasets, and\nfocuses on two case studies showing the effectiveness of Big Data Analytics and\npresents how it could be of greater good in the future if handled\nappropriately.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2016 19:10:43 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Taylor-Sakyi", "Kevin", ""]]}, {"id": "1601.04814", "submitter": "Gianmarco De Francisci Morales", "authors": "Gianmarco De Francisci Morales and Aristides Gionis", "title": "Streaming Similarity Self-Join", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and study the problem of computing the similarity self-join in a\nstreaming context (SSSJ), where the input is an unbounded stream of items\narriving continuously. The goal is to find all pairs of items in the stream\nwhose similarity is greater than a given threshold. The simplest formulation of\nthe problem requires unbounded memory, and thus, it is intractable. To make the\nproblem feasible, we introduce the notion of time-dependent similarity: the\nsimilarity of two items decreases with the difference in their arrival time. By\nleveraging the properties of this time-dependent similarity function, we design\ntwo algorithmic frameworks to solve the sssj problem. The first one, MiniBatch\n(MB), uses existing index-based filtering techniques for the static version of\nthe problem, and combines them in a pipeline. The second framework, Streaming\n(STR), adds time filtering to the existing indexes, and integrates new\ntime-based bounds deeply in the working of the algorithms. We also introduce a\nnew indexing technique (L2), which is based on an existing state-of-the-art\nindexing technique (L2AP), but is optimized for the streaming case. Extensive\nexperiments show that the STR algorithm, when instantiated with the L2 index,\nis the most scalable option across a wide array of datasets and parameters.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2016 07:34:17 GMT"}, {"version": "v2", "created": "Tue, 8 Mar 2016 09:14:27 GMT"}], "update_date": "2016-03-09", "authors_parsed": [["Morales", "Gianmarco De Francisci", ""], ["Gionis", "Aristides", ""]]}, {"id": "1601.04980", "submitter": "Lu\\'is Cruz-Filipe", "authors": "Lu\\'is Cruz-Filipe, Isabel Nunes and Peter Schneider-Kamp", "title": "Integrity Constraints for General-Purpose Knowledge Bases", "comments": "FoIKS 2016", "journal-ref": null, "doi": "10.1007/978-3-319-30024-5_13", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrity constraints in databases have been studied extensively since the\n1980s, and they are considered essential to guarantee database integrity. In\nrecent years, several authors have studied how the same notion can be adapted\nto reasoning frameworks, in such a way that they achieve the purpose of\nguaranteeing a system's consistency, but are kept separate from the reasoning\nmechanisms.\n  In this paper we focus on multi-context systems, a general-purpose framework\nfor combining heterogeneous reasoning systems, enhancing them with a notion of\nintegrity constraints that generalizes the corresponding concept in the\ndatabase world.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2016 16:24:43 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Cruz-Filipe", "Lu\u00eds", ""], ["Nunes", "Isabel", ""], ["Schneider-Kamp", "Peter", ""]]}, {"id": "1601.05118", "submitter": "Niranjan Kamat", "authors": "Niranjan Kamat, Arnab Nandi", "title": "Perfect and Maximum Randomness in Stratified Sampling over Joins", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supporting sampling in the presence of joins is an important problem in data\nanalysis, but is inherently challenging due to the need to avoid correlation\nbetween output tuples. Current solutions provide either correlated or\nnon-correlated samples. Sampling might not always be feasible in the\nnon-correlated sampling-based approaches -- the sample size or intermediate\ndata size might be exceedingly large. On the other hand, a correlated sample\nmay not be representative of the join. This paper presents a \\emph{unified}\nstrategy towards join sampling, while considering sample correlation every step\nof the way. We provide two key contributions. First, in the case where a\n\\emph{correlated} sample is \\emph{acceptable}, we provide techniques, for all\njoin types, to sample base relations so that their join is \\emph{as random as\npossible}. Second, in the case where a correlated sample is \\emph{not\nacceptable}, we provide enhancements to the state-of-the-art algorithms to\nreduce their execution time and intermediate data size.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2016 22:18:56 GMT"}, {"version": "v2", "created": "Tue, 14 Feb 2017 18:57:23 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Kamat", "Niranjan", ""], ["Nandi", "Arnab", ""]]}, {"id": "1601.05270", "submitter": "Sidra Faisal", "authors": "Sidra Faisal, Kemele M. Endris, Saeedeh Shekarpour, S\\\"oren Auer", "title": "Co-evolution of RDF Datasets", "comments": "18 pages, 4 figures, Accepted in ICWE, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linking Data initiatives have fostered the publication of large number of RDF\ndatasets in the Linked Open Data (LOD) cloud, as well as the development of\nquery processing infrastructures to access these data in a federated fashion.\nHowever, different experimental studies have shown that availability of LOD\ndatasets cannot be always ensured, being RDF data replication required for\nenvisioning reliable federated query frameworks. Albeit enhancing data\navailability, RDF data replication requires synchronization and conflict\nresolution when replicas and source datasets are allowed to change data over\ntime, i.e., co-evolution management needs to be provided to ensure consistency.\nIn this paper, we tackle the problem of RDF data co-evolution and devise an\napproach for conflict resolution during co-evolution of RDF datasets. Our\nproposed approach is property-oriented and allows for exploiting semantics\nabout RDF properties during co-evolution management. The quality of our\napproach is empirically evaluated in different scenarios on the DBpedia-live\ndataset. Experimental results suggest that proposed proposed techniques have a\npositive impact on the quality of data in source datasets and replicas.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2016 13:46:24 GMT"}, {"version": "v2", "created": "Mon, 28 Mar 2016 18:21:40 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Faisal", "Sidra", ""], ["Endris", "Kemele M.", ""], ["Shekarpour", "Saeedeh", ""], ["Auer", "S\u00f6ren", ""]]}, {"id": "1601.05335", "submitter": "Kong Hyeok", "authors": "Hyeok Kong, Cholyong Jong, Unhyok Ryang", "title": "Implementation of Association Rule Mining for Network Intrusion\n  Detection", "comments": "I have something wrong in submitting the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern intrusion detection systems are based on data mining and\ndatabase-centric architecture, where a number of data mining techniques have\nbeen found. Among the most popular techniques, association rule mining is one\nof the important topics in data mining research. This approach determines\ninteresting relationships between large sets of data items. This technique was\ninitially applied to the so-called market basket analysis, which aims at\nfinding regularities in shopping behaviour of customers of supermarkets. In\ncontrast to dataset for market basket analysis, which takes usually hundreds of\nattributes, network audit databases face tens of attributes. So the typical\nApriori algorithm of association rule mining, which needs so many database\nscans, can be improved, dealing with such characteristics of transaction\ndatabase. In this paper we propose an impoved Apriori algorithm, very useful in\npractice, using scan of network audit database only once by transaction cutting\nand hashing.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2016 17:15:44 GMT"}, {"version": "v2", "created": "Thu, 4 Aug 2016 01:03:15 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Kong", "Hyeok", ""], ["Jong", "Cholyong", ""], ["Ryang", "Unhyok", ""]]}, {"id": "1601.05748", "submitter": "Wentao Wu", "authors": "Wentao Wu, Jeffrey F. Naughton, Harneet Singh", "title": "Sampling-Based Query Re-Optimization", "comments": "This is the extended version of a paper with the same title and\n  authors that appears in the Proceedings of the ACM SIGMOD International\n  Conference on Management of Data (SIGMOD 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite of decades of work, query optimizers still make mistakes on\n\"difficult\" queries because of bad cardinality estimates, often due to the\ninteraction of multiple predicates and correlations in the data. In this paper,\nwe propose a low-cost post-processing step that can take a plan produced by the\noptimizer, detect when it is likely to have made such a mistake, and take steps\nto fix it. Specifically, our solution is a sampling-based iterative procedure\nthat requires almost no changes to the original query optimizer or query\nevaluation mechanism of the system. We show that this indeed imposes low\noverhead and catches cases where three widely used optimizers (PostgreSQL and\ntwo commercial systems) make large errors.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2016 18:46:18 GMT"}], "update_date": "2016-01-22", "authors_parsed": [["Wu", "Wentao", ""], ["Naughton", "Jeffrey F.", ""], ["Singh", "Harneet", ""]]}, {"id": "1601.05893", "submitter": "Hans De Sterck", "authors": "Shawn Brunsting, Hans De Sterck, Remco Dolman, Teun van Sprundel", "title": "GeoTextTagger: High-Precision Location Tagging of Textual Documents\n  using a Natural Language Processing Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Location tagging, also known as geotagging or geolocation, is the process of\nassigning geographical coordinates to input data. In this paper we present an\nalgorithm for location tagging of textual documents. Our approach makes use of\nprevious work in natural language processing by using a state-of-the-art\npart-of-speech tagger and named entity recognizer to find blocks of text which\nmay refer to locations. A knowledge base (OpenStreatMap) is then used to find a\nlist of possible locations for each block. Finally, one location is chosen for\neach block by assigning distance-based scores to each location and repeatedly\nselecting the location and block with the best score. We tested our geolocation\nalgorithm with Wikipedia articles about topics with a well-defined geographical\nlocation that are geotagged by the articles' authors, where classification\napproaches have achieved median errors as low as 11 km, with attainable\naccuracy limited by the class size. Our approach achieved a 10th percentile\nerror of 490 metres and median error of 54 kilometres on the Wikipedia dataset\nwe used. When considering the five location tags with the greatest scores, 50%\nof articles were assigned at least one tag within 8.5 kilometres of the\narticle's author-assigned true location. We also tested our approach on Twitter\nmessages that are tagged with the location from which the message was sent.\nTwitter texts are challenging because they are short and unstructured and often\ndo not contain words referring to the location they were sent from, but we\nobtain potentially useful results. We explain how we use the Spark framework\nfor data analytics to collect and process our test data. In general,\nclassification-based approaches for location tagging may be reaching their\nupper accuracy limit, but our precision-focused approach has high accuracy for\nsome texts and shows significant potential for improvement overall.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 07:09:54 GMT"}], "update_date": "2016-01-25", "authors_parsed": [["Brunsting", "Shawn", ""], ["De Sterck", "Hans", ""], ["Dolman", "Remco", ""], ["van Sprundel", "Teun", ""]]}, {"id": "1601.05909", "submitter": "Huanhuan Wu", "authors": "Huanhuan Wu, Yuzhen Huang, James Cheng, Jinfeng Li, Yiping Ke", "title": "Efficient Processing of Reachability and Time-Based Path Queries in a\n  Temporal Graph", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A temporal graph is a graph in which vertices communicate with each other at\nspecific time, e.g., $A$ calls $B$ at 11 a.m. and talks for 7 minutes, which is\nmodeled by an edge from $A$ to $B$ with starting time \"11 a.m.\" and duration \"7\nmins\". Temporal graphs can be used to model many networks with time-related\nactivities, but efficient algorithms for analyzing temporal graphs are severely\ninadequate. We study fundamental problems such as answering reachability and\ntime-based path queries in a temporal graph, and propose an efficient indexing\ntechnique specifically designed for processing these queries in a temporal\ngraph. Our results show that our method is efficient and scalable in both index\nconstruction and query processing.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 08:49:39 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2016 04:27:35 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Wu", "Huanhuan", ""], ["Huang", "Yuzhen", ""], ["Cheng", "James", ""], ["Li", "Jinfeng", ""], ["Ke", "Yiping", ""]]}, {"id": "1601.06311", "submitter": "Apurba Das", "authors": "Apurba Das, Michael Svendsen, Srikanta Tirthapura", "title": "Incremental Maintenance of Maximal Cliques in a Dynamic Graph", "comments": "18 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the maintenance of the set of all maximal cliques in a dynamic\ngraph that is changing through the addition or deletion of edges. We present\nnearly tight bounds on the magnitude of change in the set of maximal cliques,\nas well as the first change-sensitive algorithms for clique maintenance, whose\nruntime is proportional to the magnitude of the change in the set of maximal\ncliques. We present experimental results showing these algorithms are efficient\nin practice and are faster than prior work by two to three orders of magnitude.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2016 21:15:26 GMT"}, {"version": "v2", "created": "Thu, 29 Jun 2017 03:23:59 GMT"}, {"version": "v3", "created": "Sat, 17 Mar 2018 22:50:56 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Das", "Apurba", ""], ["Svendsen", "Michael", ""], ["Tirthapura", "Srikanta", ""]]}, {"id": "1601.06316", "submitter": "Arlei Lopes Da Silva", "authors": "Arlei Silva, Ramya Raghavendra, Mudhakar Srivatsa, Ambuj K. Singh", "title": "Prediction-based Online Trajectory Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent spatio-temporal data applications, such as car-shar\\-ing and smart\ncities, impose new challenges regarding the scalability and timeliness of data\nprocessing systems. Trajectory compression is a promising approach for scaling\nup spatio-temporal databases. However, existing techniques fail to address the\nonline setting, in which a compressed version of a trajectory stream has to be\nmaintained over time. In this paper, we introduce ONTRAC, a new framework for\nmap-matched online trajectory compression. ONTRAC learns prediction models for\nsuppressing updates to a trajectory database using training data. Two\nprediction schemes are proposed, one for road segments via a Markov model and\nanother for travel-times by combining Quadratic Programming and Expectation\nMaximization. Experiments show that ONTRAC outperforms the state-of-the-art\noffline technique even when long update delays (4 mininutes) are allowed and\nachieves up to 21 times higher compression ratio for travel-times. Moreover,\nour approach increases database scalability by up to one order of magnitude.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2016 22:33:03 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2016 20:52:30 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Silva", "Arlei", ""], ["Raghavendra", "Ramya", ""], ["Srivatsa", "Mudhakar", ""], ["Singh", "Ambuj K.", ""]]}, {"id": "1601.06497", "submitter": "Da Yan", "authors": "Da Yan, James Cheng, M. Tamer \\\"Ozsu, Fan Yang, Yi Lu, John C.S. Lui,\n  Qizhen Zhang, Wilfred Ng", "title": "Quegel: A General-Purpose Query-Centric Framework for Querying Big\n  Graphs", "comments": "This is a full version of our VLDB paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pioneered by Google's Pregel, many distributed systems have been developed\nfor large-scale graph analytics. These systems expose the user-friendly \"think\nlike a vertex\" programming interface to users, and exhibit good horizontal\nscalability. However, these systems are designed for tasks where the majority\nof graph vertices participate in computation, but are not suitable for\nprocessing light-workload graph queries where only a small fraction of vertices\nneed to be accessed. The programming paradigm adopted by these systems can\nseriously under-utilize the resources in a cluster for graph query processing.\nIn this work, we develop a new open-source system, called Quegel, for querying\nbig graphs, which treats queries as first-class citizens in the design of its\ncomputing model. Users only need to specify the Pregel-like algorithm for a\ngeneric query, and Quegel processes light-workload graph queries on demand\nusing a novel superstep-sharing execution model to effectively utilize the\ncluster resources. Quegel further provides a convenient interface for\nconstructing graph indexes, which significantly improve query performance but\nare not supported by existing graph-parallel systems. Our experiments verified\nthat Quegel is highly efficient in answering various types of graph queries and\nis up to orders of magnitude faster than existing systems.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 07:27:53 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Yan", "Da", ""], ["Cheng", "James", ""], ["\u00d6zsu", "M. Tamer", ""], ["Yang", "Fan", ""], ["Lu", "Yi", ""], ["Lui", "John C. S.", ""], ["Zhang", "Qizhen", ""], ["Ng", "Wilfred", ""]]}, {"id": "1601.07241", "submitter": "Ayman Taha Ayman Taha", "authors": "Ayman Taha", "title": "Knowledge Discovery In GIS Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent geographic information system (IGIS) is one of the promising\ntopics in GIS field. It aims at making GIS tools more sensitive for large\nvolumes of data stored inside GIS systems by integrating GIS with other\ncomputer sciences such as Expert system (ES) Data Warehouse (DW), Decision\nSupport System (DSS), or Knowledge Discovery Database (KDD). One of the main\nbranches of IGIS is the Geographic Knowledge Discovery (GKD) which tries to\ndiscover the implicit knowledge in the spatial databases. The main difference\nbetween traditional KDD techniques and GKD techniques is hidden in the nature\nof spatial data sets. In other words in the traditional data set the values of\neach object are supposed to be independent from other objects in the same data\nset, whereas the spatial dataset tends to be highly correlated according to the\nfirst law of geography. The spatial outlier detection is one of the most\npopular spatial data mining techniques which is used to detect spatial objects\nwhose non-spatial attributes values are extremely different from those of their\nneighboring objects. Analyzing the behavior of these objects may produce an\ninteresting knowledge, which has an effective role in the decision-making\nprocess. In this thesis, a new definition for the spatial neighborhood\nrelationship by is proposed considering the weights of the most effective\nparameters of neighboring objects in a given spatial dataset. The spatial\nparameters taken into our consideration are; distance, cost, and number of\ndirect connections between neighboring objects. A new model to detect spatial\noutliers is also presented based on the new definition of the spatial\nneighborhood relationship. This model is adapted to be applied to polygonal\nobjects. The proposed model is applied to an existing project for supporting\nliteracy in Fayoum governorate in Arab Republic of Egypt (ARE).\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2016 01:28:50 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Taha", "Ayman", ""]]}, {"id": "1601.07539", "submitter": "Xiaolan Wang Xiaolan Wang", "authors": "Xiaolan Wang, Alexandra Meliou, Eugene Wu", "title": "QFix: Diagnosing errors through query histories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven applications rely on the correctness of their data to function\nproperly and effectively. Errors in data can be incredibly costly and\ndisruptive, leading to loss of revenue, incorrect conclusions, and misguided\npolicy decisions. While data cleaning tools can purge datasets of many errors\nbefore the data is used, applications and users interacting with the data can\nintroduce new errors. Subsequent valid updates can obscure these errors and\npropagate them through the dataset causing more discrepancies. Even when some\nof these discrepancies are discovered, they are often corrected superficially,\non a case-by-case basis, further obscuring the true underlying cause, and\nmaking detection of the remaining errors harder. In this paper, we propose\nQFix, a framework that derives explanations and repairs for discrepancies in\nrelational data, by analyzing the effect of queries that operated on the data\nand identifying potential mistakes in those queries. QFix is flexible, handling\nscenarios where only a subset of the true discrepancies is known, and robust to\ndifferent types of update workloads. We make four important contributions: (a)\nwe formalize the problem of diagnosing the causes of data errors based on the\nqueries that operated on and introduced errors to a dataset; (b) we develop\nexact methods for deriving diagnoses and fixes for identified errors using\nstate-of-the-art tools; (c) we present several optimization techniques that\nimprove our basic approach without compromising accuracy, and (d) we leverage a\ntradeoff between accuracy and performance to scale diagnosis to large datasets\nand query logs, while achieving near-optimal results. We demonstrate the\neffectiveness of QFix through extensive evaluation over benchmark and synthetic\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2016 20:40:06 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2016 21:29:47 GMT"}], "update_date": "2016-02-15", "authors_parsed": [["Wang", "Xiaolan", ""], ["Meliou", "Alexandra", ""], ["Wu", "Eugene", ""]]}, {"id": "1601.08059", "submitter": "Nikos Bikakis", "authors": "Nikos Bikakis, Timos Sellis", "title": "Exploration and Visualization in the Web of Big Linked Data: A Survey of\n  the State of the Art", "comments": "6th International Workshop on Linked Web Data Management (LWDM 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data exploration and visualization systems are of great importance in the Big\nData era. Exploring and visualizing very large datasets has become a major\nresearch challenge, of which scalability is a vital requirement. In this\nsurvey, we describe the major prerequisites and challenges that should be\naddressed by the modern exploration and visualization systems. Considering\nthese challenges, we present how state-of-the-art approaches from the Database\nand Information Visualization communities attempt to handle them. Finally, we\nsurvey the systems developed by Semantic Web community in the context of the\nWeb of Linked Data, and discuss to which extent these satisfy the contemporary\nrequirements.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2016 11:30:44 GMT"}], "update_date": "2016-02-01", "authors_parsed": [["Bikakis", "Nikos", ""], ["Sellis", "Timos", ""]]}, {"id": "1601.08221", "submitter": "Ryan Marcus", "authors": "Ryan Marcus, Olga Papaemmanouil", "title": "WiSeDB: A Learning-based Workload Management Advisor for Cloud Databases", "comments": null, "journal-ref": "Proceedings of the VLDB Endowment Volume 9 Issue 10, June 2016", "doi": "10.14778/2977797.2977804", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Workload management for cloud databases must deal with the tasks of resource\nprovisioning, query placement and query scheduling in a manner that meets the\napplication's performance goals while minimizing the cost of using cloud\nresources. Existing solutions have approached these three challenges in\nisolation, and with only a particular type of performance goal in mind. In this\npaper, we introduce WiSeDB, a learning-based framework for generating holistic\nworkload management solutions customized to application-defined performance\nmetrics and workload characteristics. Our approach relies on supervised\nlearning to train cost-effective decision tree models for guiding query\nplacement, scheduling, and resource provisioning decisions. Applications can\nuse these models for both batch and online scheduling of incoming workloads. A\nunique feature of our system is that it can adapt its offline model to\nstricter/looser performance goals with minimal re-training. This allows us to\npresent alternative workload management strategies that address the typical\nperformance vs. cost trade-off of cloud services. Experimental results show\nthat our approach has very low training overhead while offering low cost\nstrategies for a variety of performance goals and workload characteristics.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2016 18:55:55 GMT"}, {"version": "v2", "created": "Sun, 28 Feb 2016 21:30:44 GMT"}, {"version": "v3", "created": "Sat, 30 Apr 2016 18:42:17 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Marcus", "Ryan", ""], ["Papaemmanouil", "Olga", ""]]}]