[{"id": "1506.00307", "submitter": "Emad Soroush", "authors": "Emad Soroush, Magdalena Balazinska, Simon Krughoff, Andrew Connolly", "title": "Efficient Iterative Processing in the SciDB Parallel Array Engine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many scientific data-intensive applications perform iterative computations on\narray data. There exist multiple engines specialized for array processing.\nThese engines efficiently support various types of operations, but none\nincludes native support for iterative processing. In this paper, we develop a\nmodel for iterative array computations and a series of optimizations. We\nevaluate the benefits of an optimized, native support for iterative array\nprocessing on the SciDB engine and real workloads from the astronomy domain.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2015 23:37:58 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Soroush", "Emad", ""], ["Balazinska", "Magdalena", ""], ["Krughoff", "Simon", ""], ["Connolly", "Andrew", ""]]}, {"id": "1506.00394", "submitter": "Marcus Paradies", "authors": "Marcus Paradies and Michael Rudolf and Wolfgang Lehner", "title": "GraphVista: Interactive Exploration Of Large Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The potential to gain business insights from graph-structured data through\ngraph analytics is increasingly attracting companies from a variety of\nindustries, ranging from web companies to traditional enterprise businesses. To\nanalyze a graph, a user often executes isolated graph queries using a dedicated\ninterface---a procedural graph programming interface or a declarative graph\nquery language. The results are then returned and displayed using a specific\nvisualization technique. This follows the classical ad-hoc\nQuery$\\rightarrow$Result interaction paradigm and often requires multiple query\niterations until an interesting aspect in the graph data is identified. This is\ncaused on the one hand by the schema flexibility of graph data and on the other\nhand by the intricacies of declarative graph query languages. To lower the\nburden for the user to explore an unknown graph without prior knowledge of a\ngraph query language, visual graph exploration provides an effective and\nintuitive query interface to navigate through the graph interactively.\n  We demonstrate GRAPHVISTA, a graph visualization and exploration tool that\ncan seamlessly combine ad-hoc querying and interactive graph exploration within\nthe same query session. In our demonstration, conference attendees will see\nGRAPHVISTA running against a large real-world graph data set. They will start\nby identifying entry points of interest with the help of ad-hoc queries and\nwill then discover the graph interactively through visual graph exploration.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2015 08:55:26 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2015 07:27:53 GMT"}], "update_date": "2015-07-10", "authors_parsed": [["Paradies", "Marcus", ""], ["Rudolf", "Michael", ""], ["Lehner", "Wolfgang", ""]]}, {"id": "1506.00548", "submitter": "Martin Junghanns", "authors": "Martin Junghanns, Andr\\'e Petermann, Kevin G\\'omez and Erhard Rahm", "title": "GRADOOP: Scalable Graph Data Management and Analytics with Hadoop", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many Big Data applications in business and science require the management and\nanalysis of huge amounts of graph data. Previous approaches for graph analytics\nsuch as graph databases and parallel graph processing systems (e.g., Pregel)\neither lack sufficient scalability or flexibility and expressiveness. We are\ntherefore developing a new end-to-end approach for graph data management and\nanalysis based on the Hadoop ecosystem, called Gradoop (Graph analytics on\nHadoop). Gradoop is designed around the so-called Extended Property Graph Data\nModel (EPGM) supporting semantically rich, schema-free graph data within many\ndistinct graphs. A set of high-level operators is provided for analyzing both\nsingle graphs and collections of graphs. Based on these operators, we propose a\ndomain-specific language to define analytical workflows. The Gradoop graph\nstore is currently utilizing HBase for distributed storage of graph data in\nHadoop clusters. An initial version of Gradoop has been used to analyze graph\ndata for business intelligence and social network analysis.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2015 15:50:33 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2015 06:52:13 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Junghanns", "Martin", ""], ["Petermann", "Andr\u00e9", ""], ["G\u00f3mez", "Kevin", ""], ["Rahm", "Erhard", ""]]}, {"id": "1506.00743", "submitter": "Xiaowang Zhang", "authors": "Xiaowang Zhang, Zhiyong Feng, Xin Wang, Guozheng Rao, Wenrui Wu", "title": "Context-Free Path Queries on RDF Graphs", "comments": "25 pages", "journal-ref": "In Proceedings of the 15th International Semantic Web Conference\n  (ISWC 2016), Kobe, Japan, LNCS 9981, pp. 632-648, 2016", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Navigational graph queries are an important class of queries that canextract\nimplicit binary relations over the nodes of input graphs. Most of the\nnavigational query languages used in the RDF community, e.g. property paths in\nW3C SPARQL 1.1 and nested regular expressions in nSPARQL, are based on the\nregular expressions. It is known that regular expressions have limited\nexpressivity; for instance, some natural queries, like same generation-queries,\nare not expressible with regular expressions. To overcome this limitation, in\nthis paper, we present cfSPARQL, an extension of SPARQL query language equipped\nwith context-free grammars. The cfSPARQL language is strictly more expressive\nthan property paths and nested expressions. The additional expressivity can be\nused for modelling graph similarities, graph summarization and ontology\nalignment. Despite the increasing expressivity, we show that cfSPARQL still\nenjoys a low computational complexity and can be evaluated efficiently.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2015 03:39:59 GMT"}, {"version": "v2", "created": "Sun, 1 May 2016 07:56:48 GMT"}, {"version": "v3", "created": "Fri, 7 Oct 2016 04:43:56 GMT"}], "update_date": "2016-10-10", "authors_parsed": [["Zhang", "Xiaowang", ""], ["Feng", "Zhiyong", ""], ["Wang", "Xin", ""], ["Rao", "Guozheng", ""], ["Wu", "Wenrui", ""]]}, {"id": "1506.01094", "submitter": "Kelvin Guu", "authors": "Kelvin Guu, John Miller, Percy Liang", "title": "Traversing Knowledge Graphs in Vector Space", "comments": "2015 Conference on Empirical Methods on Natural Language Processing\n  (EMNLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Path queries on a knowledge graph can be used to answer compositional\nquestions such as \"What languages are spoken by people living in Lisbon?\".\nHowever, knowledge graphs often have missing facts (edges) which disrupts path\nqueries. Recent models for knowledge base completion impute missing facts by\nembedding knowledge graphs in vector spaces. We show that these models can be\nrecursively applied to answer path queries, but that they suffer from cascading\nerrors. This motivates a new \"compositional\" training objective, which\ndramatically improves all models' ability to answer path queries, in some cases\nmore than doubling accuracy. On a standard knowledge base completion task, we\nalso demonstrate that compositional training acts as a novel form of structural\nregularization, reliably improving performance across all base models (reducing\nerrors by up to 43%) and achieving new state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 00:38:25 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2015 05:16:24 GMT"}], "update_date": "2015-08-20", "authors_parsed": [["Guu", "Kelvin", ""], ["Miller", "John", ""], ["Liang", "Percy", ""]]}, {"id": "1506.01188", "submitter": "Silviu Maniu", "authors": "Siyu Lei, Silviu Maniu, Luyi Mo, Reynold Cheng, Pierre Senellart", "title": "Online Influence Maximization (Extended Version)", "comments": "13 pages. To appear in KDD 2015. Extended version", "journal-ref": null, "doi": "10.1145/2783258.2783271", "report-no": null, "categories": "cs.SI cs.DB physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social networks are commonly used for marketing purposes. For example, free\nsamples of a product can be given to a few influential social network users (or\n\"seed nodes\"), with the hope that they will convince their friends to buy it.\nOne way to formalize marketers' objective is through influence maximization (or\nIM), whose goal is to find the best seed nodes to activate under a fixed\nbudget, so that the number of people who get influenced in the end is\nmaximized. Recent solutions to IM rely on the influence probability that a user\ninfluences another one. However, this probability information may be\nunavailable or incomplete. In this paper, we study IM in the absence of\ncomplete information on influence probability. We call this problem Online\nInfluence Maximization (OIM) since we learn influence probabilities at the same\ntime we run influence campaigns. To solve OIM, we propose a multiple-trial\napproach, where (1) some seed nodes are selected based on existing influence\ninformation; (2) an influence campaign is started with these seed nodes; and\n(3) users' feedback is used to update influence information. We adopt the\nExplore-Exploit strategy, which can select seed nodes using either the current\ninfluence probability estimation (exploit), or the confidence bound on the\nestimation (explore). Any existing IM algorithm can be used in this framework.\nWe also develop an incremental algorithm that can significantly reduce the\noverhead of handling users' feedback information. Our experiments show that our\nsolution is more effective than traditional IM methods on the partial\ninformation.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 10:08:37 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Lei", "Siyu", ""], ["Maniu", "Silviu", ""], ["Mo", "Luyi", ""], ["Cheng", "Reynold", ""], ["Senellart", "Pierre", ""]]}, {"id": "1506.01333", "submitter": "Praveen Rao", "authors": "Vasil Slavov, Anas Katib, Praveen Rao, Srivenu Paturi, Dinesh\n  Barenkala", "title": "Fast Processing of SPARQL Queries on RDF Quadruples", "comments": "This paper was published in the 17th International Workshop on the\n  Web and Databases (WebDB 2014), Snowbird, UT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose a new approach for fast processing of SPARQL\nqueries on large RDF datasets containing RDF quadruples (or quads). Our\napproach called RIQ employs a decrease-and-conquer strategy: Rather than\nindexing the entire RDF dataset, RIQ identifies groups of similar RDF graphs\nand indexes each group separately. During query processing, RIQ uses a novel\nfiltering index to first identify candidate groups that may contain matches for\nthe query. On these candidates, it executes optimized queries using a\nconventional SPARQL processor to produce the final results. Our initial\nperformance evaluation results are promising: Using a synthetic and a real\ndataset, each containing about 1.4 billion quads, we show that RIQ outperforms\nRDF-3X and Jena TDB on a variety of SPARQL queries.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 17:50:35 GMT"}, {"version": "v2", "created": "Thu, 14 Apr 2016 22:40:43 GMT"}], "update_date": "2016-04-18", "authors_parsed": [["Slavov", "Vasil", ""], ["Katib", "Anas", ""], ["Rao", "Praveen", ""], ["Paturi", "Srivenu", ""], ["Barenkala", "Dinesh", ""]]}, {"id": "1506.01406", "submitter": "Jose Rodrigues Jr", "authors": "Hugo Gualdron, Robson Cordeiro, Jose Rodrigues-Jr, Duen Chau, Minsuk\n  Kahng, U Kang", "title": "M-Flash: Fast Billion-scale Graph Computation Using a Bimodal Block\n  Processing Model", "comments": "Hugo Gualdron, Robson Cordeiro, Jose Rodrigues-Jr, Duen Chau, Minsuk\n  Kahng, U Kang (2016) M-Flash: Fast Billion-scale Graph Computation Using a\n  Bimodal Block Processing Model, In: ECML-PKDD16, pages 623-640, LNCS,\n  Springer", "journal-ref": null, "doi": "10.1007/978-3-319-46227-1_39", "report-no": null, "categories": "cs.DB cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent graph computation approaches have demonstrated that a single PC can\nperform efficiently on billion-scale graphs. While these approaches achieve\nscalability by optimizing I/O operations, they do not fully exploit the\ncapabilities of modern hard drives and processors. To overcome their\nperformance, in this work, we introduce the Bimodal Block Processing (BBP), an\ninnovation that is able to boost the graph computation by minimizing the I/O\ncost even further. With this strategy, we achieved the following contributions:\n(1) M-Flash, the fastest graph computation framework to date; (2) a flexible\nand simple programming model to easily implement popular and essential graph\nalgorithms, including the first single-machine billion-scale eigensolver; and\n(3) extensive experiments on real graphs with up to 6.6 billion edges,\ndemonstrating M-Flash's consistent and significant speedup.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 20:56:30 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2015 17:38:52 GMT"}, {"version": "v3", "created": "Mon, 25 Jan 2016 16:04:40 GMT"}, {"version": "v4", "created": "Tue, 5 Jul 2016 19:01:36 GMT"}, {"version": "v5", "created": "Wed, 14 Sep 2016 20:26:33 GMT"}], "update_date": "2016-09-16", "authors_parsed": [["Gualdron", "Hugo", ""], ["Cordeiro", "Robson", ""], ["Rodrigues-Jr", "Jose", ""], ["Chau", "Duen", ""], ["Kahng", "Minsuk", ""], ["Kang", "U", ""]]}, {"id": "1506.01973", "submitter": "Hyungyu Shin", "authors": "Jinha Kim (1 and 2), Hyungyu Shin (1), Wook-Shin Han (1), Sungpack\n  Hong (2), and Hassan Chafi (2) ((1) POSTECH, South Korea, (2) Oracle Labs,\n  USA)", "title": "Taming Subgraph Isomorphism for RDF Query Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RDF data are used to model knowledge in various areas such as life sciences,\nSemantic Web, bioinformatics, and social graphs. The size of real RDF data\nreaches billions of triples. This calls for a framework for efficiently\nprocessing RDF data. The core function of processing RDF data is subgraph\npattern matching. There have been two completely different directions for\nsupporting efficient subgraph pattern matching. One direction is to develop\nspecialized RDF query processing engines exploiting the properties of RDF data\nfor the last decade, while the other direction is to develop efficient subgraph\nisomorphism algorithms for general, labeled graphs for over 30 years. Although\nboth directions have a similar goal (i.e., finding subgraphs in data graphs for\na given query graph), they have been independently researched without clear\nreason. We argue that a subgraph isomorphism algorithm can be easily modified\nto handle the graph homomorphism, which is the RDF pattern matching semantics,\nby just removing the injectivity constraint. In this paper, based on the\nstate-of-the-art subgraph isomorphism algorithm, we propose an in-memory\nsolution, TurboHOM++, which is tamed for the RDF processing, and we compare it\nwith the representative RDF processing engines for several RDF benchmarks in a\nserver machine where billions of triples can be loaded in memory. In order to\nspeed up TurboHOM++, we also provide a simple yet effective transformation and\na series of optimization techniques. Extensive experiments using several RDF\nbenchmarks show that TurboHOM++ consistently and significantly outperforms the\nrepresentative RDF engines. Specifically, TurboHOM++ outperforms its\ncompetitors by up to five orders of magnitude.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2015 17:00:47 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2015 08:54:48 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Kim", "Jinha", "", "1 and 2"], ["Shin", "Hyungyu", ""], ["Han", "Wook-Shin", ""], ["Hong", "Sungpack", ""], ["Chafi", "Hassan", ""]]}, {"id": "1506.03163", "submitter": "Leonid Boytsov", "authors": "Bilegsaikhan Naidan, Leonid Boytsov, Eric Nyberg", "title": "Permutation Search Methods are Efficient, Yet Faster Search is Possible", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We survey permutation-based methods for approximate k-nearest neighbor\nsearch. In these methods, every data point is represented by a ranked list of\npivots sorted by the distance to this point. Such ranked lists are called\npermutations. The underpinning assumption is that, for both metric and\nnon-metric spaces, the distance between permutations is a good proxy for the\ndistance between original points. Thus, it should be possible to efficiently\nretrieve most true nearest neighbors by examining only a tiny subset of data\npoints whose permutations are similar to the permutation of a query. We further\ntest this assumption by carrying out an extensive experimental evaluation where\npermutation methods are pitted against state-of-the art benchmarks (the\nmulti-probe LSH, the VP-tree, and proximity-graph based retrieval) on a variety\nof realistically large data set from the image and textual domain. The focus is\non the high-accuracy retrieval methods for generic spaces. Additionally, we\nassume that both data and indices are stored in main memory. We find\npermutation methods to be reasonably efficient and describe a setup where these\nmethods are most useful. To ease reproducibility, we make our software and data\nsets publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 04:50:29 GMT"}, {"version": "v2", "created": "Sun, 14 Jun 2015 10:21:06 GMT"}, {"version": "v3", "created": "Sun, 21 Jun 2015 20:35:03 GMT"}, {"version": "v4", "created": "Mon, 31 Oct 2016 18:50:48 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Naidan", "Bilegsaikhan", ""], ["Boytsov", "Leonid", ""], ["Nyberg", "Eric", ""]]}, {"id": "1506.03625", "submitter": "Paolo Guagliardo", "authors": "Ingo Feinerer, Enrico Franconi and Paolo Guagliardo", "title": "Lossless Selection Views under Conditional Domain Constraints", "comments": "16 pages, 1 figure, 1 table. Full version with complete proofs.\n  Instead of this version, please cite\n  http://dx.doi.org/10.1109/TKDE.2014.2334327", "journal-ref": "IEEE Transactions on Knowledge and Data Engineering, vol. 27, no.\n  2, pp. 504-517, February 2015", "doi": "10.1109/TKDE.2014.2334327", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A set of views defined by selection queries splits a database relation into\nsub-relations, each containing a subset of the original rows. This\ndecomposition into horizontal fragments is lossless when the initial relation\ncan be reconstructed from the fragments by union. In this paper, we consider\nhorizontal decomposition in a setting where some of the attributes in the\ndatabase schema are interpreted over a specific domain, on which a set of\nspecial predicates and functions is defined.\n  We study losslessness in the presence of integrity constraints on the\ndatabase schema. We consider the class of conditional domain constraints\n(CDCs), which restrict the values that the interpreted attributes may take\nwhenever a certain condition holds on the non-interpreted ones, and investigate\nlossless horizontal decomposition under CDCs in isolation, as well as in\ncombination with functional and unary inclusion dependencies.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2015 11:06:58 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Feinerer", "Ingo", ""], ["Franconi", "Enrico", ""], ["Guagliardo", "Paolo", ""]]}, {"id": "1506.03837", "submitter": "Weinan Zhang", "authors": "Weinan Zhang, Jun Wang", "title": "Statistical Arbitrage Mining for Display Advertising", "comments": "In the proceedings of the 21st ACM SIGKDD international conference on\n  Knowledge discovery and data mining (KDD 2015)", "journal-ref": null, "doi": "10.1145/2783258.2783269", "report-no": null, "categories": "cs.GT cs.DB", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  We study and formulate arbitrage in display advertising. Real-Time Bidding\n(RTB) mimics stock spot exchanges and utilises computers to algorithmically buy\ndisplay ads per impression via a real-time auction. Despite the new automation,\nthe ad markets are still informationally inefficient due to the heavily\nfragmented marketplaces. Two display impressions with similar or identical\neffectiveness (e.g., measured by conversion or click-through rates for a\ntargeted audience) may sell for quite different prices at different market\nsegments or pricing schemes. In this paper, we propose a novel data mining\nparadigm called Statistical Arbitrage Mining (SAM) focusing on mining and\nexploiting price discrepancies between two pricing schemes. In essence, our\nSAMer is a meta-bidder that hedges advertisers' risk between CPA (cost per\naction)-based campaigns and CPM (cost per mille impressions)-based ad\ninventories; it statistically assesses the potential profit and cost for an\nincoming CPM bid request against a portfolio of CPA campaigns based on the\nestimated conversion rate, bid landscape and other statistics learned from\nhistorical data. In SAM, (i) functional optimisation is utilised to seek for\noptimal bidding to maximise the expected arbitrage net profit, and (ii) a\nportfolio-based risk management solution is leveraged to reallocate bid volume\nand budget across the set of campaigns to make a risk and return trade-off. We\npropose to jointly optimise both components in an EM fashion with high\nefficiency to help the meta-bidder successfully catch the transient statistical\narbitrage opportunities in RTB. Both the offline experiments on a real-world\nlarge-scale dataset and online A/B tests on a commercial platform demonstrate\nthe effectiveness of our proposed solution in exploiting arbitrage in various\nmodel settings and market environments.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2015 21:05:26 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Zhang", "Weinan", ""], ["Wang", "Jun", ""]]}, {"id": "1506.04006", "submitter": "Sahar Vahdati", "authors": "Sahar Vahdati, Farah Karim, Jyun-Yao Huang, and Christoph Lange", "title": "Mapping Large Scale Research Metadata to Linked Data: A Performance\n  Comparison of HBase, CSV and XML", "comments": "Accepted in 0th Metadata and Semantics Research Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DL cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  OpenAIRE, the Open Access Infrastructure for Research in Europe, comprises a\ndatabase of all EC FP7 and H2020 funded research projects, including metadata\nof their results (publications and datasets). These data are stored in an HBase\nNoSQL database, post-processed, and exposed as HTML for human consumption, and\nas XML through a web service interface. As an intermediate format to facilitate\nstatistical computations, CSV is generated internally. To interlink the\nOpenAIRE data with related data on the Web, we aim at exporting them as Linked\nOpen Data (LOD). The LOD export is required to integrate into the overall data\nprocessing workflow, where derived data are regenerated from the base data\nevery day. We thus faced the challenge of identifying the best-performing\nconversion approach.We evaluated the performances of creating LOD by a\nMapReduce job on top of HBase, by mapping the intermediate CSV files, and by\nmapping the XML output.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 12:40:03 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2015 12:37:36 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Vahdati", "Sahar", ""], ["Karim", "Farah", ""], ["Huang", "Jyun-Yao", ""], ["Lange", "Christoph", ""]]}, {"id": "1506.04299", "submitter": "Leopoldo Bertossi", "authors": "Babak Salimi and Leopoldo Bertossi", "title": "Query-Answer Causality in Databases: Abductive Diagnosis and\n  View-Updates", "comments": "To appear in Proc. UAI Causal Inference Workshop, 2015. One example\n  was fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causality has been recently introduced in databases, to model, characterize\nand possibly compute causes for query results (answers). Connections between\nquery causality and consistency-based diagnosis and database repairs (wrt.\nintegrity constrain violations) have been established in the literature. In\nthis work we establish connections between query causality and abductive\ndiagnosis and the view-update problem. The unveiled relationships allow us to\nobtain new complexity results for query causality -the main focus of our work-\nand also for the two other areas.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2015 17:33:47 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2015 18:55:13 GMT"}, {"version": "v3", "created": "Mon, 13 Jul 2015 09:15:22 GMT"}, {"version": "v4", "created": "Tue, 18 Aug 2015 23:05:44 GMT"}, {"version": "v5", "created": "Sun, 20 Sep 2015 03:41:29 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Salimi", "Babak", ""], ["Bertossi", "Leopoldo", ""]]}, {"id": "1506.04333", "submitter": "Nikos Bikakis", "authors": "Nikos Bikakis, John Liagouris, Maria Krommyda, George Papastefanatos,\n  Timos Sellis", "title": "Towards Scalable Visual Exploration of Very Large RDF Graphs", "comments": "12th Extended Semantic Web Conference (ESWC 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we outline our work on developing a disk-based infrastructure\nfor efficient visualization and graph exploration operations over very large\ngraphs. The proposed platform, called graphVizdb, is based on a novel technique\nfor indexing and storing the graph. Particularly, the graph layout is indexed\nwith a spatial data structure, i.e., an R-tree, and stored in a database. In\nruntime, user operations are translated into efficient spatial operations\n(i.e., window queries) in the backend.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2015 23:23:21 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2015 21:32:30 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Bikakis", "Nikos", ""], ["Liagouris", "John", ""], ["Krommyda", "Maria", ""], ["Papastefanatos", "George", ""], ["Sellis", "Timos", ""]]}, {"id": "1506.04815", "submitter": "Amit Chavan", "authors": "Amit Chavan, Silu Huang, Amol Deshpande, Aaron Elmore, Samuel Madden\n  and Aditya Parameswaran", "title": "Towards a unified query language for provenance and versioning", "comments": "Theory and Practice of Provenance, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Organizations and teams collect and acquire data from various sources, such\nas social interactions, financial transactions, sensor data, and genome\nsequencers. Different teams in an organization as well as different data\nscientists within a team are interested in extracting a variety of insights\nwhich require combining and collaboratively analyzing datasets in diverse ways.\nDataHub is a system that aims to provide robust version control and provenance\nmanagement for such a scenario. To be truly useful for collaborative data\nscience, one also needs the ability to specify queries and analysis tasks over\nthe versioning and the provenance information in a unified manner. In this\npaper, we present an initial design of our query language, called VQuel, that\naims to support such unified querying over both types of information, as well\nas the intermediate and final results of analyses. We also discuss some of the\nkey language design and implementation challenges moving forward.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 01:32:51 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Chavan", "Amit", ""], ["Huang", "Silu", ""], ["Deshpande", "Amol", ""], ["Elmore", "Aaron", ""], ["Madden", "Samuel", ""], ["Parameswaran", "Aditya", ""]]}, {"id": "1506.05158", "submitter": "Taylor Arnold", "authors": "Taylor Arnold", "title": "An Entropy Maximizing Geohash for Distributed Spatiotemporal Database\n  Indexing", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a modification of the standard geohash algorithm based on maximum\nentropy encoding in which the data volume is approximately constant for a given\nhash prefix length. Distributed spatiotemporal databases, which typically\nrequire interleaving spatial and temporal elements into a single key, reap\nlarge benefits from a balanced geohash by creating a consistent ratio between\nspatial and temporal precision even across areas of varying data density. This\nproperty is also useful for indexing purely spatial datasets, where the load\ndistribution of large range scans is an important aspect of query performance.\nWe apply our algorithm to data generated proportional to population as given by\ncensus block population counts provided from the US Census Bureau.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 21:54:12 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Arnold", "Taylor", ""]]}, {"id": "1506.07087", "submitter": "Sudhir Tirumalasetty", "authors": "Sudhir Tirumalasetty, Aruna Jadda, Sreenivasa Reddy Edara", "title": "An Enhanced Apriori Algorithm for Discovering Frequent Patterns with\n  Optimal Number of Scans", "comments": "in International Journal of Computer Science Issues 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data mining is wide spreading its applications in several areas. There are\ndifferent tasks in mining which provides solutions for wide variety of problems\nin order to discover knowledge. Among those tasks association mining plays a\npivotal role for identifying frequent patterns. Among the available association\nmining algorithms Apriori algorithm is one of the most prevalent and dominant\nalgorithm which is used to discover frequent patterns. This algorithm is used\nto discover frequent patterns from small to large databases. This paper points\ntoward the inadequacy of the tangible Apriori algorithm of wasting time for\nscanning the whole transactional database for discovering association rules and\nproposes an enhancement on Apriori algorithm to overcome this problem. This\nenhancement is obtained by dropping the amount of time used in scanning the\ntransactional database by just limiting the number of transactions while\ncalculating the frequency of an item or item-pairs. This improved version of\nApriori algorithm optimizes the time used for scanning the whole transactional\ndatabase.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2015 16:47:12 GMT"}], "update_date": "2015-06-24", "authors_parsed": [["Tirumalasetty", "Sudhir", ""], ["Jadda", "Aruna", ""], ["Edara", "Sreenivasa Reddy", ""]]}, {"id": "1506.07208", "submitter": "Ji\\v{r}\\'i N\\'advorn\\'ik", "authors": "Ing. Ji\\v{r}\\'i N\\'advorn\\'ik", "title": "Cross-matching Engine for Incremental Photometric Sky Survey", "comments": "57 pages, 36 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB astro-ph.SR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For light curve generation, a pre-planned photometry survey is needed\nnowadays, where all of the exposure coordinates have to be given and don't\nchange during the survey. This thesis shows it is not required and we can\ndata-mine these light curves from astronomical data that was never meant for\nthis purpose. With this approach, we can recycle all of the photometric surveys\nin the world and generate light curves of observed objects for them.\n  This thesis is addressing mostly the catalog generation process, which is\nneeded for creating the light curves. In practice, it focuses on one of the\nmost important problems in astroinformatics which is clustering data volumes on\nBig Data scale where most of the traditional techniques stagger. We consider a\nwide variety of possible solutions from the view of performance, scalability,\ndistributability, etc. We defined criteria for time and memory complexity which\nwe evaluated for all of the tested solutions. Furthermore, we created quality\nstandards which we also take into account when evaluating the results.\n  We are using relational databases as a starting point of our implementation\nand compare them with the newest technologies potentially usable for solving\nour problem. These are noSQL Array databases or transferring the heavy\ncomputations of clustering towards supercomputers by using parallelism.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2015 22:35:50 GMT"}], "update_date": "2015-06-25", "authors_parsed": [["N\u00e1dvorn\u00edk", "Ing. Ji\u0159\u00ed", ""]]}, {"id": "1506.07943", "submitter": "Lei Wang", "authors": "Lei Wang, Jianfeng Zhan, Zhen Jia, Rui Han", "title": "Characterization and Architectural Implications of Big Data Workloads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data areas are expanding in a fast way in terms of increasing workloads\nand runtime systems, and this situation imposes a serious challenge to workload\ncharacterization, which is the foundation of innovative system and architecture\ndesign. The previous major efforts on big data benchmarking either propose a\ncomprehensive but a large amount of workloads, or only select a few workloads\naccording to so-called popularity, which may lead to partial or even biased\nobservations. In this paper, on the basis of a comprehensive big data benchmark\nsuite---BigDataBench, we reduced 77 workloads to 17 representative workloads\nfrom a micro-architectural perspective. On a typical state-of-practice\nplatform---Intel Xeon E5645, we compare the representative big data workloads\nwith SPECINT, SPECCFP, PARSEC, CloudSuite and HPCC. After a comprehensive\nworkload characterization, we have the following observations. First, the big\ndata workloads are data movement dominated computing with more branch\noperations, taking up to 92% percentage in terms of instruction mix, which\nplaces them in a different class from Desktop (SPEC CPU2006), CMP (PARSEC), HPC\n(HPCC) workloads. Second, corroborating the previous work, Hadoop and Spark\nbased big data workloads have higher front-end stalls. Comparing with the\ntraditional workloads i. e. PARSEC, the big data workloads have larger\ninstructions footprint. But we also note that, in addition to varied\ninstruction-level parallelism, there are significant disparities of front-end\nefficiencies among different big data workloads. Third, we found complex\nsoftware stacks that fail to use state-of-practise processors efficiently are\none of the main factors leading to high front-end stalls. For the same\nworkloads, the L1I cache miss rates have one order of magnitude differences\namong diverse implementations with different software stacks.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2015 02:29:22 GMT"}], "update_date": "2015-06-29", "authors_parsed": [["Wang", "Lei", ""], ["Zhan", "Jianfeng", ""], ["Jia", "Zhen", ""], ["Han", "Rui", ""]]}, {"id": "1506.07950", "submitter": "Rafal Scherer", "authors": "Marcin Korytkowski, Rafal Scherer, Pawel Staszewski and Piotr Woldan", "title": "Bag-of-Features Image Indexing and Classification in Microsoft SQL\n  Server Relational Database", "comments": "2015 IEEE 2nd International Conference on Cybernetics (CYBCONF),\n  Gdynia, Poland, 24-26 June 2015", "journal-ref": null, "doi": "10.1109/CYBConf.2015.7175981", "report-no": null, "categories": "cs.DB cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel relational database architecture aimed to visual\nobjects classification and retrieval. The framework is based on the\nbag-of-features image representation model combined with the Support Vector\nMachine classification and is integrated in a Microsoft SQL Server database.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2015 03:24:39 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Korytkowski", "Marcin", ""], ["Scherer", "Rafal", ""], ["Staszewski", "Pawel", ""], ["Woldan", "Piotr", ""]]}, {"id": "1506.08454", "submitter": "Vijil Chenthamarakshan", "authors": "Vijil Chenthamarakshan, Prasad M Desphande, Raghu Krishnapuram,\n  Ramakrishna Varadarajan, Knut Stolze", "title": "WYSIWYE: An Algebra for Expressing Spatial and Textual Rules for Visual\n  Information Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The visual layout of a webpage can provide valuable clues for certain types\nof Information Extraction (IE) tasks. In traditional rule based IE frameworks,\nthese layout cues are mapped to rules that operate on the HTML source of the\nwebpages. In contrast, we have developed a framework in which the rules can be\nspecified directly at the layout level. This has many advantages, since the\nhigher level of abstraction leads to simpler extraction rules that are largely\nindependent of the source code of the page, and, therefore, more robust. It can\nalso enable specification of new types of rules that are not otherwise\npossible. To the best of our knowledge, there is no general framework that\nallows declarative specification of information extraction rules based on\nspatial layout. Our framework is complementary to traditional text based rules\nframework and allows a seamless combination of spatial layout based rules with\ntraditional text based rules. We describe the algebra that enables such a\nsystem and its efficient implementation using standard relational and text\nindexing features of a relational database. We demonstrate the simplicity and\nefficiency of this system for a task involving the extraction of software\nsystem requirements from software product pages.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2015 21:17:26 GMT"}, {"version": "v2", "created": "Tue, 27 Sep 2016 19:49:41 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Chenthamarakshan", "Vijil", ""], ["Desphande", "Prasad M", ""], ["Krishnapuram", "Raghu", ""], ["Varadarajan", "Ramakrishna", ""], ["Stolze", "Knut", ""]]}, {"id": "1506.08506", "submitter": "Jeremy Kepner", "authors": "Andrew Prout, Jeremy Kepner, Peter Michaleas, William Arcand, David\n  Bestor, Bill Bergeron, Chansup Byun, Lauren Edwards, Vijay Gadepally, Matthew\n  Hubbell, Julie Mullen, Antonio Rosa, Charles Yee, Albert Reuther", "title": "Enabling On-Demand Database Computing with MIT SuperCloud Database\n  Management System", "comments": "6 pages; accepted to IEEE High Performance Extreme Computing (HPEC)\n  conference 2015. arXiv admin note: text overlap with arXiv:1406.4923", "journal-ref": null, "doi": "10.1109/HPEC.2015.7322482", "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The MIT SuperCloud database management system allows for rapid creation and\nflexible execution of a variety of the latest scientific databases, including\nApache Accumulo and SciDB. It is designed to permit these databases to run on a\nHigh Performance Computing Cluster (HPCC) platform as seamlessly as any other\nHPCC job. It ensures the seamless migration of the databases to the resources\nassigned by the HPCC scheduler and centralized storage of the database files\nwhen not running. It also permits snapshotting of databases to allow\nresearchers to experiment and push the limits of the technology without\nconcerns for data or productivity loss if the database becomes unstable.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 04:47:20 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Prout", "Andrew", ""], ["Kepner", "Jeremy", ""], ["Michaleas", "Peter", ""], ["Arcand", "William", ""], ["Bestor", "David", ""], ["Bergeron", "Bill", ""], ["Byun", "Chansup", ""], ["Edwards", "Lauren", ""], ["Gadepally", "Vijay", ""], ["Hubbell", "Matthew", ""], ["Mullen", "Julie", ""], ["Rosa", "Antonio", ""], ["Yee", "Charles", ""], ["Reuther", "Albert", ""]]}, {"id": "1506.08800", "submitter": "Karla Saur", "authors": "Karla Saur and Tudor Dumitra\\c{s} and Michael Hicks", "title": "Evolving NoSQL Databases Without Downtime", "comments": "Update to writing/structure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NoSQL databases like Redis, Cassandra, and MongoDB are increasingly popular\nbecause they are flexible, lightweight, and easy to work with. Applications\nthat use these databases will evolve over time, sometimes necessitating (or\npreferring) a change to the format or organization of the data. The problem we\naddress in this paper is: How can we support the evolution of high-availability\napplications and their NoSQL data online, without excessive delays or\ninterruptions, even in the presence of backward-incompatible data format\nchanges?\n  We present KVolve, an extension to the popular Redis NoSQL database, as a\nsolution to this problem. KVolve permits a developer to submit an upgrade\nspecification that defines how to transform existing data to the newest\nversion. This transformation is applied lazily as applications interact with\nthe database, thus avoiding long pause times. We demonstrate that KVolve is\nexpressive enough to support substantial practical updates, including format\nchanges to RedisFS, a Redis-backed file system, while imposing essentially no\noverhead in general use and minimal pause times during updates.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 19:41:22 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2015 17:41:30 GMT"}, {"version": "v3", "created": "Mon, 25 Apr 2016 02:13:59 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Saur", "Karla", ""], ["Dumitra\u015f", "Tudor", ""], ["Hicks", "Michael", ""]]}, {"id": "1506.08908", "submitter": "Venkata Vamsikrishna Meduri", "authors": "Sushovan De, Yuheng Hu, Meduri Venkata Vamsikrishna, Yi Chen, and\n  Subbarao Kambhampati", "title": "BayesWipe: A Scalable Probabilistic Framework for Cleaning BigData", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent efforts in data cleaning of structured data have focused exclusively\non problems like data deduplication, record matching, and data standardization;\nnone of the approaches addressing these problems focus on fixing incorrect\nattribute values in tuples. Correcting values in tuples is typically performed\nby a minimum cost repair of tuples that violate static constraints like CFDs\n(which have to be provided by domain experts, or learned from a clean sample of\nthe database). In this paper, we provide a method for correcting individual\nattribute values in a structured database using a Bayesian generative model and\na statistical error model learned from the noisy database directly. We thus\navoid the necessity for a domain expert or clean master data. We also show how\nto efficiently perform consistent query answering using this model over a dirty\ndatabase, in case write permissions to the database are unavailable. We\nevaluate our methods over both synthetic and real data.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 00:33:56 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["De", "Sushovan", ""], ["Hu", "Yuheng", ""], ["Vamsikrishna", "Meduri Venkata", ""], ["Chen", "Yi", ""], ["Kambhampati", "Subbarao", ""]]}, {"id": "1506.08978", "submitter": "Michael Bar-Sinai", "authors": "Michael Bar-Sinai", "title": "Big Data Technology Literature Review", "comments": "10 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A short overview of various algorithms and technologies that are helpful for\nbig data storage and manipulation. Includes pointers to papers for further\nreading, and, where applicable, pointers to open source projects implementing a\ndescribed storage type.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 07:55:46 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2015 21:30:41 GMT"}], "update_date": "2015-12-04", "authors_parsed": [["Bar-Sinai", "Michael", ""]]}]