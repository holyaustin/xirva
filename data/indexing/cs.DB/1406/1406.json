[{"id": "1406.0349", "submitter": "Jan Van den Bussche", "authors": "Tony Tan, Jan Van den Bussche, Xiaowang Zhang", "title": "Undecidability of satisfiability in the algebra of finite binary\n  relations with union, composition, and difference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider expressions built up from binary relation names using the\noperators union, composition, and set difference. We show that it is\nundecidable to test whether a given such expression $e$ is finitely\nsatisfiable, i.e., whether there exist finite binary relations that can be\nsubstituted for the relation names so that $e$ evaluates to a nonempty result.\nThis result already holds in restriction to expressions that mention just a\nsingle relation name, and where the difference operator can be nested at most\nonce.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jun 2014 13:02:39 GMT"}], "update_date": "2014-06-03", "authors_parsed": [["Tan", "Tony", ""], ["Bussche", "Jan Van den", ""], ["Zhang", "Xiaowang", ""]]}, {"id": "1406.0435", "submitter": "Jun-Sung Kim", "authors": "Jun-Sung Kim, Kyu-Young Whang, Hyuk-Yoon Kwon, and Il-Yeol Song", "title": "Odysseus/DFS: Integration of DBMS and Distributed File System for\n  Transaction Processing of Big Data", "comments": "35 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The relational DBMS (RDBMS) has been widely used since it supports various\nhigh-level functionalities such as SQL, schemas, indexes, and transactions that\ndo not exist in the O/S file system. But, a recent advent of big data\ntechnology facilitates development of new systems that sacrifice the DBMS\nfunctionality in order to efficiently manage large-scale data. Those so-called\nNoSQL systems use a distributed file system, which support scalability and\nreliability. They support scalability of the system by storing data into a\nlarge number of low-cost commodity hardware and support reliability by storing\nthe data in replica. However, they have a drawback that they do not adequately\nsupport high-level DBMS functionality. In this paper, we propose an\narchitecture of a DBMS that uses the DFS as storage. With this novel\narchitecture, the DBMS is capable of supporting scalability and reliability of\nthe DFS as well as high-level functionality of DBMS. Thus, a DBMS can utilize a\nvirtually unlimited storage space provided by the DFS, rendering it to be\nsuitable for big data analytics. As part of the architecture of the DBMS, we\npropose the notion of the meta DFS file, which allows the DBMS to use the DFS\nas the storage, and an efficient transaction management method including\nrecovery and concurrency control. We implement this architecture in\nOdysseus/DFS, an integration of the Odysseus relational DBMS, that has been\nbeing developed at KAIST for over 24 years, with the DFS. Our experiments on\ntransaction processing show that, due to the high-level functionality of\nOdysseus/DFS, it outperforms Hbase, which is a representative open-source NoSQL\nsystem. We also show that, compared with an RDBMS with local storage, the\nperformance of Odysseus/DFS is comparable or marginally degraded, showing that\nthe overhead of Odysseus/DFS for supporting scalability by using the DFS as the\nstorage is not significant.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jun 2014 16:32:05 GMT"}], "update_date": "2014-06-03", "authors_parsed": [["Kim", "Jun-Sung", ""], ["Whang", "Kyu-Young", ""], ["Kwon", "Hyuk-Yoon", ""], ["Song", "Il-Yeol", ""]]}, {"id": "1406.0905", "submitter": "Paolo Missier", "authors": "Paolo Missier, Simon Woodman, Hugo Hiden, Paul Watson", "title": "Provenance and data differencing for workflow reproducibility analysis", "comments": null, "journal-ref": "Provenance and data differencing for workflow reproducibility\n  analysis Missier, P.; Woodman, S.; Hiden, H.; and Watson, P. Concurrency and\n  Computation: Practice and Experience, . 2013", "doi": "10.1002/cpe.3035", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the foundations of science is that researchers must publish the\nmethodology used to achieve their results so that others can attempt to\nreproduce them. This has the added benefit of allowing methods to be adopted\nand adapted for other purposes. In the field of e-Science, services -- often\nchoreographed through workflow, process data to generate results. The\nreproduction of results is often not straightforward as the computational\nobjects may not be made available or may have been updated since the results\nwere generated. For example, services are often updated to fix bugs or improve\nalgorithms. This paper addresses these problems in three ways. Firstly, it\nintroduces a new framework to clarify the range of meanings of\n\"reproducibility\". Secondly, it describes a new algorithm, \\PDIFF, that uses a\ncomparison of workflow provenance traces to determine whether an experiment has\nbeen reproduced; the main innovation is that if this is not the case then the\nspecific point(s) of divergence are identified through graph analysis,\nassisting any researcher wishing to understand those differences. One key\nfeature is support for user-defined, semantic data comparison operators.\nFinally, the paper describes an implementation of \\PDIFF that leverages the\npower of the e-Science Central platform which enacts workflows in the cloud. As\nwell as automatically generating a provenance trace for consumption by \\PDIFF,\nthe platform supports the storage and re-use of old versions of workflows, data\nand services; the paper shows how this can be powerfully exploited in order to\nachieve reproduction and re-use.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jun 2014 23:22:59 GMT"}], "update_date": "2014-06-05", "authors_parsed": [["Missier", "Paolo", ""], ["Woodman", "Simon", ""], ["Hiden", "Hugo", ""], ["Watson", "Paul", ""]]}, {"id": "1406.1224", "submitter": "Xuhui Li", "authors": "Xuhui Li, Mengchi Liu, Shanfeng Zhu, Arif Ghafoor", "title": "XTQ: A Declarative Functional XML Query Language", "comments": "65 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Various query languages have been proposed to extract and restructure\ninformation in XML documents. These languages, usually claiming to be\ndeclarative, mainly consider the conjunctive relationships among data elements.\nIn order to present the operations where the hierarchical and the disjunctive\nrelationships need to be considered, such as restructuring hierarchy and\nhandling heterogeneity, the programs in these languages often exhibit a\nprocedural style and thus the declarativeness in them is not so prominent as in\nconventional query languages like SQL.\n  In this paper, we propose a declarative pattern-based functional XML query\nlanguage named XML Tree Query (XTQ). XTQ adopts expressive composite patterns\nto present data extraction, meanwhile establishing the conjunctive, the\ndisjunctive and the hierarchical relationships among data elements. It uses the\nmatching terms, a composite structure of the variables bound to the matched\ndata elements, to present a global sketch of the extracted data, and develops a\ndeductive restructuring mechanism of matching terms to indicate data\ntransformation, especially for restructuring hierarchy and handling\nheterogeneity. Based on matching terms, XTQ employs a coherent approach to\nfunction declaration and invocation to consistently extract and construct\ncomposite data structure, which integrates features of conventional functional\nlanguages and pattern-based query languages. Additionally, XTQ also supports\ndata filtering on composite data structure such as hierarchical data, which is\nseldom deliberately considered in other studies. We demonstrate with various\nexamples that XTQ can declaratively present complex XML queries which are\ncommon in practice.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jun 2014 22:03:20 GMT"}], "update_date": "2014-06-06", "authors_parsed": [["Li", "Xuhui", ""], ["Liu", "Mengchi", ""], ["Zhu", "Shanfeng", ""], ["Ghafoor", "Arif", ""]]}, {"id": "1406.1404", "submitter": "Jan Van den Bussche", "authors": "Xiaowang Zhang, Jan Van den Bussche, Fran\\c{c}ois Picalausa", "title": "On the satisfiability problem for SPARQL patterns", "comments": "Major revision, erroneous polynomial-time claims corrected,\n  NP-completeness result added, detailed proofs added, experimental section\n  added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The satisfiability problem for SPARQL patterns is undecidable in general,\nsince the expressive power of SPARQL 1.0 is comparable with that of the\nrelational algebra. The goal of this paper is to delineate the boundary of\ndecidability of satisfiability in terms of the constraints allowed in filter\nconditions. The classes of constraints considered are bound-constraints,\nnegated bound-constraints, equalities, nonequalities, constant-equalities, and\nconstant-nonequalities. The main result of the paper can be summarized by\nsaying that, as soon as inconsistent filter conditions can be formed,\nsatisfiability is undecidable. The key insight in each case is to find a way to\nemulate the set difference operation. Undecidability can then be obtained from\na known undecidability result for the algebra of binary relations with union,\ncomposition, and set difference. When no inconsistent filter conditions can be\nformed, satisfiability is efficiently decidable by simple checks on bound\nvariables and on the use of literals. The paper also points out that\nsatisfiability for the so-called `well-designed' patterns can be decided by a\ncheck on bound variables and a check for inconsistent filter conditions.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jun 2014 14:48:04 GMT"}, {"version": "v2", "created": "Wed, 1 Jun 2016 21:26:44 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Zhang", "Xiaowang", ""], ["Bussche", "Jan Van den", ""], ["Picalausa", "Fran\u00e7ois", ""]]}, {"id": "1406.1423", "submitter": "Joshua Amavi", "authors": "Joshua Amavi, Jacques Chabin, Mirian Halfeld Ferrari, Pierre R\\'ety", "title": "A ToolBox for Conservative XML Schema Evolution and Document Adaptation", "comments": "15 pages, DEXA'14", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper proposes a set of tools to help dealing with XML database\nevolution. It aims at establishing a multi-system environment where a global\nintegrated system works in harmony with some local original ones, allowing data\ntranslation in both directions and, thus, activities on both levels. To deal\nwith schemas, we propose an algorithm that computes a mapping capable of\nobtaining a global schema which is a conservative extension of original local\nschemas. The role of the obtained mapping is then twofold: it ensures schema\nevolution, via composition and inversion, and it guides the construction of a\ndocument translator, allowing automatic data adaptation w.r.t. type evolution.\nThis paper applies, extends and put together some of our previous\ncontributions.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jun 2014 15:59:42 GMT"}], "update_date": "2014-06-06", "authors_parsed": [["Amavi", "Joshua", ""], ["Chabin", "Jacques", ""], ["Ferrari", "Mirian Halfeld", ""], ["R\u00e9ty", "Pierre", ""]]}, {"id": "1406.1431", "submitter": "Mansouri Ali mr", "authors": "Ali Mansouri and Youssef Amghar", "title": "Int\\'egration des r\\`egles actives dans des documents", "comments": "master's thesis, in French", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The management of technical documentation is an unavoidable activity\ninteresting for the enterprises. Indeed, the need to manage documents during\nall the life cycle is an important issue. For that, the need to enhance the\nability of document management systems is an interesting challenge. Despite\nexisting systems on market (electronic document management systems), they are\nconsidered as non-flexible systems which are based on data models preventing\nany extension or improvement. In addition, those systems do not allow a slight\ndescription of documents elements and propose an insufficient mechanisms for\nboth links and consistency management. LIRIS laboratory has developed research\nin this area and proposed an active system, termed SAGED, whose objectives is\nto manage link and consistency using active rules. However SAGED is based on an\napproach that split rules (for consistency management) and documents\ndescription. The main drawback is the rigidity of such approach which is\nhighlighted whenever documents are moved from one server to another or during\nexchanges of documents. To contribute to solve this problem, we propose to\ndevelop an approach aiming at improve the document management including\nconsistency. This approach is based on the introduction of rules with the XML\ndescription of the documents [BoCP01]. In this context we proposed a\nXML-oriented storage level allowing the storing of documents and rules\nuniformly through a native XML database. We defined an intelligent system\ntermed SIGED according a client/server architecture built around an intelligent\ncomponent for active rules execution. These rules are extracted from XML\ndocument, compiled and executed.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jun 2014 20:08:18 GMT"}], "update_date": "2014-06-06", "authors_parsed": [["Mansouri", "Ali", ""], ["Amghar", "Youssef", ""]]}, {"id": "1406.1998", "submitter": "Paolo Missier", "authors": "Paolo Missier and Jeremy Bryans and Carl Gamble and Vasa Curcin and\n  Roxana Danger", "title": "ProvAbs: model, policy, and tooling for abstracting PROV graphs", "comments": "In Procs. IPAW 2014 (Provenance and Annotations). Koln, Germany:\n  Springer, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Provenance metadata can be valuable in data sharing settings, where it can be\nused to help data consumers form judgements regarding the reliability of the\ndata produced by third parties. However, some parts of provenance may be\nsensitive, requiring access control, or they may need to be simplified for the\nintended audience. Both these issues can be addressed by a single mechanism for\ncreating abstractions over provenance, coupled with a policy model to drive the\nabstraction. Such mechanism, which we refer to as abstraction by grouping,\nsimultaneously achieves partial disclosure of provenance, and facilitates its\nconsumption. In this paper we introduce a formal foundation for this type of\nabstraction, grounded in the W3C PROV model; describe the associated policy\nmodel; and briefly present its implementation, the Provabs tool for interactive\nexperimentation with policies and abstractions.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jun 2014 16:26:53 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Missier", "Paolo", ""], ["Bryans", "Jeremy", ""], ["Gamble", "Carl", ""], ["Curcin", "Vasa", ""], ["Danger", "Roxana", ""]]}, {"id": "1406.2015", "submitter": "Kalyan Veeramachaneni", "authors": "Kalyan Veeramachaneni, Sherif Halawa, Franck Dernoncourt, Una-May\n  O'Reilly, Colin Taylor, Chuong Do", "title": "MOOCdb: Developing Standards and Systems to Support MOOC Data Science", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a shared data model for enabling data science in Massive Open\nOnline Courses (MOOCs). The model captures students interactions with the\nonline platform. The data model is platform agnostic and is based on some basic\ncore actions that students take on an online learning platform. Students\nusually interact with the platform in four different modes: Observing,\nSubmitting, Collaborating and giving feedback. In observing mode students are\nsimply browsing the online platform, watching videos, reading material, reading\nbook or watching forums. In submitting mode, students submit information to the\nplatform. This includes submissions towards quizzes, homeworks, or any\nassessment modules. In collaborating mode students interact with other students\nor instructors on forums, collaboratively editing wiki or chatting on google\nhangout or other hangout venues. With this basic definitions of activities, and\na data model to store events pertaining to these activities, we then create a\ncommon terminology to map Coursera and edX data into this shared data model.\nThis shared data model called MOOCdb becomes the foundation for a number of\ncollaborative frameworks that enable progress in data science without the need\nto share the data.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jun 2014 19:19:45 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Veeramachaneni", "Kalyan", ""], ["Halawa", "Sherif", ""], ["Dernoncourt", "Franck", ""], ["O'Reilly", "Una-May", ""], ["Taylor", "Colin", ""], ["Do", "Chuong", ""]]}, {"id": "1406.2125", "submitter": "Remy Haemmerle", "authors": "Falco Nogatz and Thom Fr\\\"uhwirth", "title": "From XML Schema to JSON Schema: Translation with CHR", "comments": "Part of CHR 2014 proceedings (arXiv:1406.1510)", "journal-ref": null, "doi": null, "report-no": "CHR/2014/2", "categories": "cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite its rising popularity as data format especially for web services, the\nsoftware ecosystem around the JavaScript Object Notation (JSON) is not as\nwidely distributed as that of XML. For both data formats there exist schema\nlanguages to specify the structure of instance documents, but there is\ncurrently no opportunity to translate already existing XML Schema documents\ninto equivalent JSON Schemas.\n  In this paper we introduce an implementation of a language translator. It\ntakes an XML Schema and creates its equivalent JSON Schema document. Our\napproach is based on Prolog and CHR. By unfolding the XML Schema document into\nCHR constraints, it is possible to specify the concrete translation rules in a\ndeclarative way.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jun 2014 10:33:41 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Nogatz", "Falco", ""], ["Fr\u00fchwirth", "Thom", ""]]}, {"id": "1406.2495", "submitter": "Paolo Missier", "authors": "Hugo Firth and Paolo Missier", "title": "ProvGen: generating synthetic PROV graphs with predictable structure", "comments": "IPAW'14 paper, In Procs. IPAW 2014 (Provenance and Annotations).\n  Koln, Germany: Springer, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces provGen, a generator aimed at producing large synthetic\nprovenance graphs with predictable properties and of arbitrary size. Synthetic\nprovenance graphs serve two main purposes. Firstly, they provide a variety of\ncontrolled workloads that can be used to test storage and query capabilities of\nprovenance management systems at scale. Secondly, they provide challenging\ntestbeds for experimenting with graph algorithms for provenance analytics, an\narea of increasing research interest. provGen produces PROV graphs and stores\nthem in a graph DBMS (Neo4J). A key feature is to let users control the\nrelationship makeup and topological features of the graph, by providing a seed\nprovenance pattern along with a set of constraints, expressed using a custom\nDomain Specific Language. We also propose a simple method for evaluating the\nquality of the generated graphs, by measuring how realistically they simulate\nthe structure of real-world patterns.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 10:20:33 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["Firth", "Hugo", ""], ["Missier", "Paolo", ""]]}, {"id": "1406.2644", "submitter": "Ali Elouafiq", "authors": "Ali Elouafiq and Redouan Abid", "title": "Geographical Asynchronous Information Access for Distributed Systems", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Non-relational databases are the common means of data storage in the Cloud,\nand optimizing the data access is of paramount importance into determining the\noverall Cloud system performance. In this paper, we present GAIA, a novel model\nfor retrieving and managing correlated geo-localized data in the cloud\nenvironment. We survey and compare the existing models used mostly in\nGeographical Information Systems (GIS), mainly the Grid model and the\nCoordinates Projection model. Besides, we present a benchmark comparing the\nefficiency of the models. Using extensive experimentation, we show that GAIA\noutperforms the existing models by its high efficiency which is of O(log(n)),\nand this mainly thanks to its combination of projection with cell\ndecomposition. The other models have a linear efficiency of O(n). The presented\nmodel is designed from the ground up to support GIS and is designed to suit\nboth cloud and parallel computing.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 17:33:56 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["Elouafiq", "Ali", ""], ["Abid", "Redouan", ""]]}, {"id": "1406.2963", "submitter": "Ce Zhang", "authors": "Shanan E. Peters, Ce Zhang, Miron Livny, Christopher R\\'e", "title": "A machine-compiled macroevolutionary history of Phanerozoic life", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CL cs.LG q-bio.PE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Many aspects of macroevolutionary theory and our understanding of biotic\nresponses to global environmental change derive from literature-based\ncompilations of palaeontological data. Existing manually assembled databases\nare, however, incomplete and difficult to assess and enhance. Here, we develop\nand validate the quality of a machine reading system, PaleoDeepDive, that\nautomatically locates and extracts data from heterogeneous text, tables, and\nfigures in publications. PaleoDeepDive performs comparably to humans in complex\ndata extraction and inference tasks and generates congruent synthetic\nmacroevolutionary results. Unlike traditional databases, PaleoDeepDive produces\na probabilistic database that systematically improves as information is added.\nWe also show that the system can readily accommodate sophisticated data types,\nsuch as morphological data in biological illustrations and associated textual\ndescriptions. Our machine reading approach to scientific data integration and\nsynthesis brings within reach many questions that are currently underdetermined\nand does so in ways that may stimulate entirely new modes of inquiry.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jun 2014 17:02:14 GMT"}, {"version": "v2", "created": "Sat, 19 Jul 2014 16:40:47 GMT"}], "update_date": "2014-07-23", "authors_parsed": [["Peters", "Shanan E.", ""], ["Zhang", "Ce", ""], ["Livny", "Miron", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1406.3047", "submitter": "Meghyn Bienvenu", "authors": "Meghyn Bienvenu and Stanislav Kikot and Vladimir Podolskii", "title": "Tree-like Queries in OWL 2 QL: Succinctness and Complexity Results", "comments": "This is an extended version of a paper accepted at LICS'15. It\n  contains both succinctness and complexity results and adopts FOL notation.\n  The appendix contains proofs that had to be omitted from the conference\n  version for lack of space. The previous arxiv version (a long version of our\n  DL'14 workshop paper) only contained the succinctness results and used\n  description logic notation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the impact of query topology on the difficulty of\nanswering conjunctive queries in the presence of OWL 2 QL ontologies. Our first\ncontribution is to clarify the worst-case size of positive existential (PE),\nnon-recursive Datalog (NDL), and first-order (FO) rewritings for various\nclasses of tree-like conjunctive queries, ranging from linear queries to\nbounded treewidth queries. Perhaps our most surprising result is a\nsuperpolynomial lower bound on the size of PE-rewritings that holds already for\nlinear queries and ontologies of depth 2. More positively, we show that\npolynomial-size NDL-rewritings always exist for tree-shaped queries with a\nbounded number of leaves (and arbitrary ontologies), and for bounded treewidth\nqueries paired with bounded depth ontologies. For FO-rewritings, we equate the\nexistence of polysize rewritings with well-known problems in Boolean circuit\ncomplexity. As our second contribution, we analyze the computational complexity\nof query answering and establish tractability results (either NL- or\nLOGCFL-completeness) for a range of query-ontology pairs. Combining our new\nresults with those from the literature yields a complete picture of the\nsuccinctness and complexity landscapes for the considered classes of queries\nand ontologies.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jun 2014 20:07:49 GMT"}, {"version": "v2", "created": "Wed, 13 May 2015 12:03:06 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Bienvenu", "Meghyn", ""], ["Kikot", "Stanislav", ""], ["Podolskii", "Vladimir", ""]]}, {"id": "1406.3399", "submitter": "Olaf Hartig", "authors": "Olaf Hartig and Bryan Thompson", "title": "Foundations of an Alternative Approach to Reification in RDF", "comments": "14 pages; fixed Definition 1 and the extension of the SPARQL grammar\n  (Sec.5.1)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document defines extensions of the RDF data model and of the SPARQL\nquery language that capture an alternative approach to represent\nstatement-level metadata. While this alternative approach is backwards\ncompatible with RDF reification as defined by the RDF standard, the approach\naims to address usability and data management shortcomings of RDF reification.\nOne of the great advantages of the proposed approach is that it clarifies a\nmeans to (i) understand sparse matrices, the property graph model, hypergraphs,\nand other data structures with an emphasis on link attributes, (ii) map such\ndata onto RDF, and (iii) query such data using SPARQL. Further, the proposal\ngreatly expands both the freedom that database designers enjoy when creating\nphysical indexing schemes and query plans for graph data annotated with link\nattributes and the interoperability of those database solutions.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jun 2014 01:07:50 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2019 13:48:04 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Hartig", "Olaf", ""], ["Thompson", "Bryan", ""]]}, {"id": "1406.4110", "submitter": "Bernardo Cuenca Grau", "authors": "Bernardo Cuenca Grau, Ian Horrocks, Markus Kr\\\"otzsch, Clemens Kupke,\n  Despoina Magka, Boris Motik, Zhe Wang", "title": "Acyclicity Notions for Existential Rules and Their Application to Query\n  Answering in Ontologies", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 47, pages\n  741-808, 2013", "doi": "10.1613/jair.3949", "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answering conjunctive queries (CQs) over a set of facts extended with\nexistential rules is a prominent problem in knowledge representation and\ndatabases. This problem can be solved using the chase algorithm, which extends\nthe given set of facts with fresh facts in order to satisfy the rules. If the\nchase terminates, then CQs can be evaluated directly in the resulting set of\nfacts. The chase, however, does not terminate necessarily, and checking whether\nthe chase terminates on a given set of rules and facts is undecidable. Numerous\nacyclicity notions were proposed as sufficient conditions for chase\ntermination. In this paper, we present two new acyclicity notions called\nmodel-faithful acyclicity (MFA) and model-summarising acyclicity (MSA).\nFurthermore, we investigate the landscape of the known acyclicity notions and\nestablish a complete taxonomy of all notions known to us. Finally, we show that\nMFA and MSA generalise most of these notions.\n  Existential rules are closely related to the Horn fragments of the OWL 2\nontology language; furthermore, several prominent OWL 2 reasoners implement CQ\nanswering by using the chase to materialise all relevant facts. In order to\navoid termination problems, many of these systems handle only the OWL 2 RL\nprofile of OWL 2; furthermore, some systems go beyond OWL 2 RL, but without any\ntermination guarantees. In this paper we also investigate whether various\nacyclicity notions can provide a principled and practical solution to these\nproblems. On the theoretical side, we show that query answering for acyclic\nontologies is of lower complexity than for general ontologies. On the practical\nside, we show that many of the commonly used OWL 2 ontologies are MSA, and that\nthe number of facts obtained by materialisation is not too large. Our results\nthus suggest that principled development of materialisation-based OWL 2\nreasoners is practically feasible.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 01:44:16 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Grau", "Bernardo Cuenca", ""], ["Horrocks", "Ian", ""], ["Kr\u00f6tzsch", "Markus", ""], ["Kupke", "Clemens", ""], ["Magka", "Despoina", ""], ["Motik", "Boris", ""], ["Wang", "Zhe", ""]]}, {"id": "1406.4737", "submitter": "Sanjay Chakraborty", "authors": "Sanjay Chakraborty and N.K. Nagwani", "title": "Performance Evaluation of Incremental K-means Clustering Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The incremental K-means clustering algorithm has already been proposed and\nanalysed in paper [Chakraborty and Nagwani, 2011]. It is a very innovative\napproach which is applicable in periodically incremental environment and\ndealing with a bulk of updates. In this paper the performance evaluation is\ndone for this incremental K-means clustering algorithm using air pollution\ndatabase. This paper also describes the comparison on the performance\nevaluations between existing K-means clustering and incremental K-means\nclustering using that particular database. It also evaluates that the\nparticular point of change in the database upto which incremental K-means\nclustering performs much better than the existing K-means clustering. That\nparticular point of change in the database is known as \"Threshold value\" or \"%\ndelta change in the database\". This paper also defines the basic methodology\nfor the incremental K-means clustering algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jun 2014 14:34:18 GMT"}], "update_date": "2014-06-19", "authors_parsed": [["Chakraborty", "Sanjay", ""], ["Nagwani", "N. K.", ""]]}, {"id": "1406.4751", "submitter": "Sanjay Chakraborty", "authors": "Sanjay Chakraborty, N.K.Nagwani and Lopamudra Dey", "title": "Performance Comparison of Incremental K-means and Incremental DBSCAN\n  Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incremental K-means and DBSCAN are two very important and popular clustering\ntechniques for today's large dynamic databases (Data warehouses, WWW and so on)\nwhere data are changed at random fashion. The performance of the incremental\nK-means and the incremental DBSCAN are different with each other based on their\ntime analysis characteristics. Both algorithms are efficient compare to their\nexisting algorithms with respect to time, cost and effort. In this paper, the\nperformance evaluation of incremental DBSCAN clustering algorithm is\nimplemented and most importantly it is compared with the performance of\nincremental K-means clustering algorithm and it also explains the\ncharacteristics of these two algorithms based on the changes of the data in the\ndatabase. This paper also explains some logical differences between these two\nmost popular clustering algorithms. This paper uses an air pollution database\nas original database on which the experiment is performed.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jun 2014 15:00:53 GMT"}], "update_date": "2014-06-19", "authors_parsed": [["Chakraborty", "Sanjay", ""], ["Nagwani", "N. K.", ""], ["Dey", "Lopamudra", ""]]}, {"id": "1406.4754", "submitter": "Sanjay Chakraborty", "authors": "Sanjay Chakraborty and N.K.Nagwani", "title": "Analysis and Study of Incremental DBSCAN Clustering Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the incremental behaviours of Density based clustering.\nIt specially focuses on the Density Based Spatial Clustering of Applications\nwith Noise (DBSCAN) algorithm and its incremental approach.DBSCAN relies on a\ndensity based notion of clusters.It discovers clusters of arbitrary shapes in\nspatial databases with noise.In incremental approach, the DBSCAN algorithm is\napplied to a dynamic database where the data may be frequently updated. After\ninsertions or deletions to the dynamic database, the clustering discovered by\nDBSCAN has to be updated. And we measure the new cluster by directly compute\nthe new data entering into the existing clusters instead of rerunning the\nalgorithm.It finally discovers new updated clusters and outliers as well.Thus\nit describes at what percent of delta change in the original database the\nactual and incremental DBSCAN algorithms behave like same.DBSCAN is widely used\nin those situations where large multidimensional databases are maintained such\nas Data Warehouse.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jun 2014 15:03:29 GMT"}], "update_date": "2014-06-19", "authors_parsed": [["Chakraborty", "Sanjay", ""], ["Nagwani", "N. K.", ""]]}, {"id": "1406.4923", "submitter": "Jeremy Kepner", "authors": "Jeremy Kepner, William Arcand, David Bestor, Bill Bergeron, Chansup\n  Byun, Vijay Gadepally, Matthew Hubbell, Peter Michaleas, Julie Mullen, Andrew\n  Prout, Albert Reuther, Antonio Rosa, Charles Yee (MIT)", "title": "Achieving 100,000,000 database inserts per second using Accumulo and D4M", "comments": "6 pages; to appear in IEEE High Performance Extreme Computing (HPEC)\n  2014", "journal-ref": null, "doi": "10.1109/HPEC.2014.7040945", "report-no": null, "categories": "cs.DB astro-ph.IM cs.CE cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Apache Accumulo database is an open source relaxed consistency database\nthat is widely used for government applications. Accumulo is designed to\ndeliver high performance on unstructured data such as graphs of network data.\nThis paper tests the performance of Accumulo using data from the Graph500\nbenchmark. The Dynamic Distributed Dimensional Data Model (D4M) software is\nused to implement the benchmark on a 216-node cluster running the MIT\nSuperCloud software stack. A peak performance of over 100,000,000 database\ninserts per second was achieved which is 100x larger than the highest\npreviously published value for any other database. The performance scales\nlinearly with the number of ingest clients, number of database servers, and\ndata size. The performance was achieved by adapting several supercomputing\ntechniques to this application: distributed arrays, domain decomposition,\nadaptive load balancing, and single-program-multiple-data programming.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jun 2014 00:44:12 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Kepner", "Jeremy", "", "MIT"], ["Arcand", "William", "", "MIT"], ["Bestor", "David", "", "MIT"], ["Bergeron", "Bill", "", "MIT"], ["Byun", "Chansup", "", "MIT"], ["Gadepally", "Vijay", "", "MIT"], ["Hubbell", "Matthew", "", "MIT"], ["Michaleas", "Peter", "", "MIT"], ["Mullen", "Julie", "", "MIT"], ["Prout", "Andrew", "", "MIT"], ["Reuther", "Albert", "", "MIT"], ["Rosa", "Antonio", "", "MIT"], ["Yee", "Charles", "", "MIT"]]}, {"id": "1406.5751", "submitter": "Jeremy Kepner", "authors": "Jeremy Kepner, Vijay Gadepally, Pete Michaleas, Nabil Schear, Mayank\n  Varia, Arkady Yerukhimovich, Robert K. Cunningham (MIT)", "title": "Computing on Masked Data: a High Performance Method for Improving Big\n  Data Veracity", "comments": "to appear in IEEE High Performance Extreme Computing 2014\n  (ieee-hpec.org)", "journal-ref": null, "doi": "10.1109/HPEC.2014.7040946", "report-no": null, "categories": "cs.CR astro-ph.IM cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing gap between data and users calls for innovative tools that\naddress the challenges faced by big data volume, velocity and variety. Along\nwith these standard three V's of big data, an emerging fourth \"V\" is veracity,\nwhich addresses the confidentiality, integrity, and availability of the data.\nTraditional cryptographic techniques that ensure the veracity of data can have\noverheads that are too large to apply to big data. This work introduces a new\ntechnique called Computing on Masked Data (CMD), which improves data veracity\nby allowing computations to be performed directly on masked data and ensuring\nthat only authorized recipients can unmask the data. Using the sparse linear\nalgebra of associative arrays, CMD can be performed with significantly less\noverhead than other approaches while still supporting a wide range of linear\nalgebraic operations on the masked data. Databases with strong support of\nsparse operations, such as SciDB or Apache Accumulo, are ideally suited to this\ntechnique. Examples are shown for the application of CMD to a complex DNA\nmatching algorithm and to database operations over social media data.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jun 2014 19:06:04 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Kepner", "Jeremy", "", "MIT"], ["Gadepally", "Vijay", "", "MIT"], ["Michaleas", "Pete", "", "MIT"], ["Schear", "Nabil", "", "MIT"], ["Varia", "Mayank", "", "MIT"], ["Yerukhimovich", "Arkady", "", "MIT"], ["Cunningham", "Robert K.", "", "MIT"]]}, {"id": "1406.5917", "submitter": "Mohamed Salah  Gouider Dr", "authors": "Abdelwaheb Ferchichi, Mohamed Salah Gouider", "title": "BSTree: an Incremental Indexing Structure for Similarity Search and Real\n  Time Monitoring of Data Streams", "comments": null, "journal-ref": "Future Information Technology Lecture Notes in Electrical\n  Engineering Volume 276, 2014, pp 185-190", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this work, a new indexing technique of data streams called BSTree is\nproposed. This technique uses the method of data discretization, SAX [4], to\nreduce online the dimensionality of data streams. It draws on Btree to build\nthe index and finally uses an LRV (least Recently visited) pruning technique to\nrid the index structure from data whose last visit time exceeds a threshold\nvalue and thus minimizes response time for similarity search queries.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jun 2014 14:21:13 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Ferchichi", "Abdelwaheb", ""], ["Gouider", "Mohamed Salah", ""]]}, {"id": "1406.6268", "submitter": "David Spivak", "authors": "Henrik Forssell, H{\\aa}kon Robbestad Gylterud, David I. Spivak", "title": "Type theoretical databases", "comments": null, "journal-ref": null, "doi": "10.1093/logcom/exaa009", "report-no": null, "categories": "math.LO cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We present a soundness theorem for a dependent type theory with context\nconstants with respect to an indexed category of (finite, abstract) simplical\ncomplexes. The point of interest for computer science is that this category can\nbe seen to represent tables in a natural way. Thus the category is a model for\ndatabases, a single mathematical structure in which all database schemas and\ninstances (of a suitable, but sufficiently general form) are represented. The\ntype theory then allows for the specification of database schemas and\ninstances, the manipulation of the same with the usual type-theoretic\noperations, and the posing of queries.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jun 2014 14:59:15 GMT"}, {"version": "v2", "created": "Wed, 25 Jun 2014 19:33:23 GMT"}, {"version": "v3", "created": "Sat, 18 Oct 2014 00:27:08 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Forssell", "Henrik", ""], ["Gylterud", "H\u00e5kon Robbestad", ""], ["Spivak", "David I.", ""]]}, {"id": "1406.6490", "submitter": "Edith Cohen", "authors": "Edith Cohen", "title": "Variance Competitiveness for Monotone Estimation: Tightening the Bounds", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random samples are extensively used to summarize massive data sets and\nfacilitate scalable analytics. Coordinated sampling, where samples of different\ndata sets \"share\" the randomization, is a powerful method which facilitates\nmore accurate estimation of many aggregates and similarity measures. We\nrecently formulated a model of {\\it Monotone Estimation Problems} (MEP), which\ncan be applied to coordinated sampling, projected on a single item. MEP\nestimators can then be used to estimate sum aggregates, such as distances, over\ncoordinated samples. For MEP, we are interested in estimators that are unbiased\nand nonnegative. We proposed {\\it variance competitiveness} as a quality\nmeasure of estimators: For each data vector, we consider the minimum variance\nattainable on it by an unbiased and nonnegative estimator. We then define the\ncompetitiveness of an estimator as the maximum ratio, over data, of the\nexpectation of the square to the minimum possible. We also presented a general\nconstruction of the L$^*$ estimator, which is defined for any MEP for which a\nnonnegative unbiased estimator exists, and is at most 4-competitive.\n  Our aim here is to obtain tighter bounds on the {\\em universal ratio}, which\nwe define to be the smallest competitive ratio that can be obtained for any\nMEP. We obtain an upper bound of 3.375, improving over the bound of $4$ of the\nL$^*$ estimator. We also establish a lower bound of 1.44. The lower bound is\nobtained by constructing the {\\it optimally competitive} estimator for\nparticular MEPs. The construction is of independent interest, as it facilitates\nestimation with instance-optimal competitiveness.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jun 2014 08:18:39 GMT"}], "update_date": "2014-06-26", "authors_parsed": [["Cohen", "Edith", ""]]}, {"id": "1406.6667", "submitter": "Andrew Crotty", "authors": "Andrew Crotty, Alex Galakatos, Kayhan Dursun, Tim Kraska, Ugur\n  Cetintemel, Stan Zdonik", "title": "Tupleware: Redefining Modern Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a fundamental discrepancy between the targeted and actual users of\ncurrent analytics frameworks. Most systems are designed for the data and\ninfrastructure of the Googles and Facebooks of the world---petabytes of data\ndistributed across large cloud deployments consisting of thousands of cheap\ncommodity machines. Yet, the vast majority of users operate clusters ranging\nfrom a few to a few dozen nodes, analyze relatively small datasets of up to a\nfew terabytes, and perform primarily compute-intensive operations. Targeting\nthese users fundamentally changes the way we should build analytics systems.\n  This paper describes the design of Tupleware, a new system specifically aimed\nat the challenges faced by the typical user. Tupleware's architecture brings\ntogether ideas from the database, compiler, and programming languages\ncommunities to create a powerful end-to-end solution for data analysis. We\npropose novel techniques that consider the data, computations, and hardware\ntogether to achieve maximum performance on a case-by-case basis. Our\nexperimental evaluation quantifies the impact of our novel techniques and shows\norders of magnitude performance improvement over alternative systems.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jun 2014 19:06:15 GMT"}, {"version": "v2", "created": "Wed, 30 Jul 2014 12:49:08 GMT"}], "update_date": "2014-07-31", "authors_parsed": [["Crotty", "Andrew", ""], ["Galakatos", "Alex", ""], ["Dursun", "Kayhan", ""], ["Kraska", "Tim", ""], ["Cetintemel", "Ugur", ""], ["Zdonik", "Stan", ""]]}, {"id": "1406.6778", "submitter": "Chandrakant  MAHOBIYA", "authors": "Chandrakant Mahobiya, M. Kumar", "title": "Performance Comparison of Two Streaming Data Clustering Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The weighted fuzzy c-mean clustering algorithm and weighted fuzzy\nc-mean-adaptive cluster number are extension of traditional fuzzy c-mean\nAlgorithm to stream data clustering algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jun 2014 06:09:26 GMT"}], "update_date": "2014-06-27", "authors_parsed": [["Mahobiya", "Chandrakant", ""], ["Kumar", "M.", ""]]}, {"id": "1406.7288", "submitter": "Wolfgang Gatterbauer", "authors": "Wolfgang Gatterbauer, Stephan G\\\"unnemann, Danai Koutra, Christos\n  Faloutsos", "title": "Linearized and Single-Pass Belief Propagation", "comments": "17 pages, 11 figures, 4 algorithms. Includes following major changes\n  since v1: renaming of \"turbo BP\" to \"single-pass BP\", convergence criteria\n  now give sufficient *and* necessary conditions, more detailed experiments,\n  more detailed comparison with prior BP convergence results, overall improved\n  exposition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we tell when accounts are fake or real in a social network? And how\ncan we tell which accounts belong to liberal, conservative or centrist users?\nOften, we can answer such questions and label nodes in a network based on the\nlabels of their neighbors and appropriate assumptions of homophily (\"birds of a\nfeather flock together\") or heterophily (\"opposites attract\"). One of the most\nwidely used methods for this kind of inference is Belief Propagation (BP) which\niteratively propagates the information from a few nodes with explicit labels\nthroughout a network until convergence. One main problem with BP, however, is\nthat there are no known exact guarantees of convergence in graphs with loops.\n  This paper introduces Linearized Belief Propagation (LinBP), a linearization\nof BP that allows a closed-form solution via intuitive matrix equations and,\nthus, comes with convergence guarantees. It handles homophily, heterophily, and\nmore general cases that arise in multi-class settings. Plus, it allows a\ncompact implementation in SQL. The paper also introduces Single-pass Belief\nPropagation (SBP), a \"localized\" version of LinBP that propagates information\nacross every edge at most once and for which the final class assignments depend\nonly on the nearest labeled neighbors. In addition, SBP allows fast incremental\nupdates in dynamic networks. Our runtime experiments show that LinBP and SBP\nare orders of magnitude faster than standard\n", "versions": [{"version": "v1", "created": "Fri, 27 Jun 2014 19:59:27 GMT"}, {"version": "v2", "created": "Thu, 10 Jul 2014 19:52:31 GMT"}, {"version": "v3", "created": "Thu, 17 Jul 2014 17:37:57 GMT"}, {"version": "v4", "created": "Thu, 16 Oct 2014 19:38:40 GMT"}], "update_date": "2014-10-17", "authors_parsed": [["Gatterbauer", "Wolfgang", ""], ["G\u00fcnnemann", "Stephan", ""], ["Koutra", "Danai", ""], ["Faloutsos", "Christos", ""]]}, {"id": "1406.7367", "submitter": "Qijun Zhu", "authors": "Qijun Zhu, Haibo Hu, Cheng Xu, Jianliang Xu and Wang-Chien Lee", "title": "Geo-Social Group Queries with Minimum Acquaintance Constraint", "comments": "This is the preprint version that is accepted by the Very Large Data\n  Bases Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prosperity of location-based social networking services enables\ngeo-social group queries for group-based activity planning and marketing. This\npaper proposes a new family of geo-social group queries with minimum\nacquaintance constraint (GSGQs), which are more appealing than existing\ngeo-social group queries in terms of producing a cohesive group that guarantees\nthe worst-case acquaintance level. GSGQs, also specified with various spatial\nconstraints, are more complex than conventional spatial queries; particularly,\nthose with a strict $k$NN spatial constraint are proved to be NP-hard. For\nefficient processing of general GSGQ queries on large location-based social\nnetworks, we devise two social-aware index structures, namely SaR-tree and\nSaR*-tree. The latter features a novel clustering technique that considers both\nspatial and social factors. Based on SaR-tree and SaR*-tree, efficient\nalgorithms are developed to process various GSGQs. Extensive experiments on\nreal-world Gowalla and Dianping datasets show that our proposed methods\nsubstantially outperform the baseline algorithms based on R-tree.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jun 2014 07:38:50 GMT"}, {"version": "v2", "created": "Tue, 11 Jul 2017 08:36:22 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Zhu", "Qijun", ""], ["Hu", "Haibo", ""], ["Xu", "Cheng", ""], ["Xu", "Jianliang", ""], ["Lee", "Wang-Chien", ""]]}, {"id": "1406.7371", "submitter": "Paresh Tanna", "authors": "Paresh Tanna, Yogesh Ghodasara", "title": "Using Apriori with WEKA for Frequent Pattern Mining", "comments": "5 Pages, 4 Figures, \"Published with International Journal of\n  Engineering Trends and Technology (IJETT)\"", "journal-ref": "International Journal of Engineering Trends and Technology\n  (IJETT), V12(3), 127-131, June 2014", "doi": "10.14445/22315381/IJETT-V12P223", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge exploration from the large set of data,generated as a result of the\nvarious data processing activities due to data mining only. Frequent Pattern\nMining is a very important undertaking in data mining. Apriori approach applied\nto generate frequent item set generally espouse candidate generation and\npruning techniques for the satisfaction of the desired objective. This paper\nshows how the different approaches achieve the objective of frequent mining\nalong with the complexities required to perform the job. This paper\ndemonstrates the use of WEKA tool for association rule mining using Apriori\nalgorithm.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jun 2014 08:18:57 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Tanna", "Paresh", ""], ["Ghodasara", "Yogesh", ""]]}, {"id": "1406.7685", "submitter": "Pakeeza Batool", "authors": "Mehwish Aziz, Shabnam Nawaz, Pakeeza Batool", "title": "Efficiency Analysis of Materialized views in DataWarehouse Using\n  selfmaintenance", "comments": "Journal based on thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A data warehouse is a large data repository for the purpose of analysis and\ndecision making in organizations. To improve the query performance and to get\nfast access to the data, data is stored as materialized views (MV) in the data\nwarehouse. When data at source gets updated, the materialized views also need\nto be updated. In this paper, we focus on the problem of maintenance of these\nmaterialized views and address the issue of finding such auxiliary views (AV)\nthat together with the materialized views make the data self-maintainable and\ntake minimal space. We propose an algorithm that uses key and referential\nconstraints which reduces the total number of tuples in auxiliary views and\nuses idea of information sharing between these auxiliary views to further\nreduce number of auxiliary views.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 11:56:20 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Aziz", "Mehwish", ""], ["Nawaz", "Shabnam", ""], ["Batool", "Pakeeza", ""]]}, {"id": "1406.7801", "submitter": "Sebastian Rudolph", "authors": "Pierre Bourhis and Markus Kr\\\"otzsch and Sebastian Rudolph", "title": "Query Containment for Highly Expressive Datalog Fragments", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The containment problem of Datalog queries is well known to be undecidable.\nThere are, however, several Datalog fragments for which containment is known to\nbe decidable, most notably monadic Datalog and several \"regular\" query\nlanguages on graphs. Monadically Defined Queries (MQs) have been introduced\nrecently as a joint generalization of these query languages. In this paper, we\nstudy a wide range of Datalog fragments with decidable query containment and\ndetermine exact complexity results for this problem. We generalize MQs to\n(Frontier-)Guarded Queries (GQs), and show that the containment problem is\n3ExpTime-complete in either case, even if we allow arbitrary Datalog in the\nsub-query. If we focus on graph query languages, i.e., fragments of linear\nDatalog, then this complexity is reduced to 2ExpSpace. We also consider nested\nqueries, which gain further expressivity by using predicates that are defined\nby inner queries. We show that nesting leads to an exponentially increasing\nhierarchy for the complexity of query containment, both in the linear and in\nthe general case. Our results settle open problems for (nested) MQs, and they\npaint a comprehensive picture of the state of the art in Datalog query\ncontainment.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 16:30:06 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Bourhis", "Pierre", ""], ["Kr\u00f6tzsch", "Markus", ""], ["Rudolph", "Sebastian", ""]]}]