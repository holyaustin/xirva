[{"id": "1210.0065", "submitter": "Fan Min", "authors": "Fan Min and William Zhu", "title": "Granular association rule mining through parametric rough sets for cold\n  start recommendation", "comments": "The theory part of the paper should be replaced by formal context\n  analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Granular association rules reveal patterns hide in many-to-many relationships\nwhich are common in relational databases. In recommender systems, these rules\nare appropriate for cold start recommendation, where a customer or a product\nhas just entered the system. An example of such rules might be \"40% men like at\nleast 30% kinds of alcohol; 45% customers are men and 6% products are alcohol.\"\nMining such rules is a challenging problem due to pattern explosion. In this\npaper, we propose a new type of parametric rough sets on two universes to study\nthis problem. The model is deliberately defined such that the parameter\ncorresponds to one threshold of rules. With the lower approximation operator in\nthe new parametric rough sets, a backward algorithm is designed for the rule\nmining problem. Experiments on two real world data sets show that the new\nalgorithm is significantly faster than the existing sandwich algorithm. This\nstudy indicates a new application area, namely recommender systems, of\nrelational data mining, granular computing and rough sets.\n", "versions": [{"version": "v1", "created": "Sat, 29 Sep 2012 01:19:33 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2013 11:53:43 GMT"}], "update_date": "2013-07-16", "authors_parsed": [["Min", "Fan", ""], ["Zhu", "William", ""]]}, {"id": "1210.0187", "submitter": "Sandeep Gupta", "authors": "Sandeep Gupta", "title": "External Memory based Distributed Generation of Massive Scale Social\n  Networks on Small Clusters", "comments": "8 pages, 4 pics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Small distributed systems are limited by their main memory to generate\nmassively large graphs. Trivial extension to current graph generators to\nutilize external memory leads to large amount of random I/O hence do not scale\nwith size. In this work we offer a technique to generate massive scale graphs\non small cluster of compute nodes with limited main memory. We develop several\ndistributed and external memory algorithms, primarily, shuffle, relabel,\nredistribute, and, compressed-sparse-row (csr) convert. The algorithms are\nimplemented in MPI/pthread model to help parallelize the operations across\nmulticores within each core. Using our scheme it is feasible to generate a\ngraph of size $2^{38}$ nodes (scale 38) using only 64 compute nodes. This can\nbe compared with the current scheme would require at least 8192 compute node,\nassuming 64GB of main memory.\n  Our work has broader implications for external memory graph libraries such as\nSTXXL and graph processing on SSD-based supercomputers such as Dash and Gordon\n[1][2].\n", "versions": [{"version": "v1", "created": "Sun, 30 Sep 2012 11:14:21 GMT"}], "update_date": "2012-10-02", "authors_parsed": [["Gupta", "Sandeep", ""]]}, {"id": "1210.0481", "submitter": "Todd Veldhuizen", "authors": "Todd L. Veldhuizen", "title": "Leapfrog Triejoin: a worst-case optimal join algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": "LB1201", "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen exciting developments in join algorithms. In 2008,\nAtserias, Grohe and Marx (henceforth AGM) proved a tight bound on the maximum\nresult size of a full conjunctive query, given constraints on the input\nrelation sizes. In 2012, Ngo, Porat, R{\\'e} and Rudra (henceforth NPRR) devised\na join algorithm with worst-case running time proportional to the AGM bound.\nOur commercial Datalog system LogicBlox employs a novel join algorithm,\n\\emph{leapfrog triejoin}, which compared conspicuously well to the NPRR\nalgorithm in preliminary benchmarks. This spurred us to analyze the complexity\nof leapfrog triejoin. In this paper we establish that leapfrog triejoin is also\nworst-case optimal, up to a log factor, in the sense of NPRR. We improve on the\nresults of NPRR by proving that leapfrog triejoin achieves worst-case\noptimality for finer-grained classes of database instances, such as those\ndefined by constraints on projection cardinalities. We show that NPRR is\n\\emph{not} worst-case optimal for such classes, giving a counterexample where\nleapfrog triejoin runs in $O(n \\log n)$ time, compared to $\\Theta(n^{1.375})$\ntime for NPRR. On a practical note, leapfrog triejoin can be implemented using\nconventional data structures such as B-trees, and extends naturally to\n$\\exists_1$ queries. We believe our algorithm offers a useful addition to the\nexisting toolbox of join algorithms, being easy to absorb, simple to implement,\nand having a concise optimality proof.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2012 17:54:13 GMT"}, {"version": "v2", "created": "Thu, 4 Oct 2012 14:12:27 GMT"}, {"version": "v3", "created": "Fri, 22 Mar 2013 00:10:44 GMT"}, {"version": "v4", "created": "Sat, 7 Sep 2013 13:43:24 GMT"}, {"version": "v5", "created": "Fri, 20 Dec 2013 20:21:03 GMT"}], "update_date": "2013-12-23", "authors_parsed": [["Veldhuizen", "Todd L.", ""]]}, {"id": "1210.0595", "submitter": "Amir Hosein Asiaee", "authors": "Amir H. Asiaee, Prashant Doshi, Todd Minning, Satya Sahoo, Priti\n  Parikh, Amit Sheth, Rick L. Tarleton", "title": "From Questions to Effective Answers: On the Utility of Knowledge-Driven\n  Querying Systems for Life Sciences Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We compare two distinct approaches for querying data in the context of the\nlife sciences. The first approach utilizes conventional databases to store the\ndata and intuitive form-based interfaces to facilitate easy querying of the\ndata. These interfaces could be seen as implementing a set of \"pre-canned\"\nqueries commonly used by the life science researchers that we study. The second\napproach is based on semantic Web technologies and is knowledge (model) driven.\nIt utilizes a large OWL ontology and same datasets as before but associated as\nRDF instances of the ontology concepts. An intuitive interface is provided that\nallows the formulation of RDF triples-based queries. Both these approaches are\nbeing used in parallel by a team of cell biologists in their daily research\nactivities, with the objective of gradually replacing the conventional approach\nwith the knowledge-driven one. This provides us with a valuable opportunity to\ncompare and qualitatively evaluate the two approaches. We describe several\nbenefits of the knowledge-driven approach in comparison to the traditional way\nof accessing data, and highlight a few limitations as well. We believe that our\nanalysis not only explicitly highlights the specific benefits and limitations\nof semantic Web technologies in our context but also contributes toward\neffective ways of translating a question in a researcher's mind into precise\ncomputational queries with the intent of obtaining effective answers from the\ndata. While researchers often assume the benefits of semantic Web technologies,\nwe explicitly illustrate these in practice.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2012 22:10:30 GMT"}], "update_date": "2012-10-03", "authors_parsed": [["Asiaee", "Amir H.", ""], ["Doshi", "Prashant", ""], ["Minning", "Todd", ""], ["Sahoo", "Satya", ""], ["Parikh", "Priti", ""], ["Sheth", "Amit", ""], ["Tarleton", "Rick L.", ""]]}, {"id": "1210.0660", "submitter": "Tien Tuan Anh Dinh", "authors": "Tien Tuan Anh Dinh, Anwitaman Datta", "title": "Stream on the Sky: Outsourcing Access Control Enforcement for Stream\n  Data to the Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increasing trend for businesses to migrate their systems towards\nthe cloud. Security concerns that arise when outsourcing data and computation\nto the cloud include data confidentiality and privacy. Given that a tremendous\namount of data is being generated everyday from plethora of devices equipped\nwith sensing capabilities, we focus on the problem of access controls over live\nstreams of data based on triggers or sliding windows, which is a distinct and\nmore challenging problem than access control over archival data. Specifically,\nwe investigate secure mechanisms for outsourcing access control enforcement for\nstream data to the cloud. We devise a system that allows data owners to specify\nfine-grained policies associated with their data streams, then to encrypt the\nstreams and relay them to the cloud for live processing and storage for future\nuse. The access control policies are enforced by the cloud, without the latter\nlearning about the data, while ensuring that unauthorized access is not\nfeasible. To realize these ends, we employ a novel cryptographic primitive,\nnamely proxy-based attribute-based encryption, which not only provides security\nbut also allows the cloud to perform expensive computations on behalf of the\nusers. Our approach is holistic, in that these controls are integrated with an\nXML based framework (XACML) for high-level management of policies. Experiments\nwith our prototype demonstrate the feasibility of such mechanisms, and early\nevaluations suggest graceful scalability with increasing numbers of policies,\ndata streams and users.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2012 06:10:50 GMT"}], "update_date": "2012-10-03", "authors_parsed": [["Dinh", "Tien Tuan Anh", ""], ["Datta", "Anwitaman", ""]]}, {"id": "1210.0748", "submitter": "Yongming Luo", "authors": "Yongming Luo, George H. L. Fletcher, Jan Hidders, Yuqing Wu and Paul\n  De Bra", "title": "External memory bisimulation reduction of big graphs", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present, to our knowledge, the first known I/O efficient\nsolutions for computing the k-bisimulation partition of a massive directed\ngraph, and performing maintenance of such a partition upon updates to the\nunderlying graph. Ubiquitous in the theory and application of graph data,\nbisimulation is a robust notion of node equivalence which intuitively groups\ntogether nodes in a graph which share fundamental structural features.\nk-bisimulation is the standard variant of bisimulation where the topological\nfeatures of nodes are only considered within a local neighborhood of radius\n$k\\geqslant 0$.\n  The I/O cost of our partition construction algorithm is bounded by $O(k\\cdot\n\\mathit{sort}(|\\et|) + k\\cdot scan(|\\nt|) + \\mathit{sort}(|\\nt|))$, while our\nmaintenance algorithms are bounded by $O(k\\cdot \\mathit{sort}(|\\et|) + k\\cdot\n\\mathit{sort}(|\\nt|))$. The space complexity bounds are $O(|\\nt|+|\\et|)$ and\n$O(k\\cdot|\\nt|+k\\cdot|\\et|)$, resp. Here, $|\\et|$ and $|\\nt|$ are the number of\ndisk pages occupied by the input graph's edge set and node set, resp., and\n$\\mathit{sort}(n)$ and $\\mathit{scan}(n)$ are the cost of sorting and scanning,\nresp., a file occupying $n$ pages in external memory. Empirical analysis on a\nvariety of massive real-world and synthetic graph datasets shows that our\nalgorithms perform efficiently in practice, scaling gracefully as graphs grow\nin size.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2012 12:30:15 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2012 09:26:03 GMT"}, {"version": "v3", "created": "Thu, 2 May 2013 08:23:28 GMT"}], "update_date": "2013-05-03", "authors_parsed": [["Luo", "Yongming", ""], ["Fletcher", "George H. L.", ""], ["Hidders", "Jan", ""], ["Wu", "Yuqing", ""], ["De Bra", "Paul", ""]]}, {"id": "1210.1040", "submitter": "Reena Philips Mrs", "authors": "Sharon Christa, K. Lakshmi Madhuri and V. Suma", "title": "A Comparative Analysis of Data Mining Tools in Agent Based Systems", "comments": "6 Pages, 2 Figures, 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  World wide technological advancement has brought in a widespread change in\nadoption and utilization of open source tools. Since, most of the organizations\nacross the globe deal with a large amount of data to be updated online and\ntransactions are made every second, managing, mining and processing this\ndynamic data is very complex. Successful implementation of the data mining\ntechnique requires a careful assessment of the various tools and algorithms\navailable to mining experts. This paper provides a comparative study of open\nsource data mining tools available to the professionals. Parameters influencing\nthe choice of apt tools in addition to the real time challenges are discussed.\nHowever, it is well proven that agents aid in improving the performance of data\nmining tools. This paper provides information on an agent-based framework for\ndata preprocessing with implementation details for the development of better\ntool in the market. An integration of open source data mining tools with agent\nsimulation enable one to implement an effective data pre processing\narchitecture thereby providing robust capabilities of the application which can\nbe upgraded using a minimum of pre planning requirement from the application\ndeveloper.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2012 09:19:19 GMT"}], "update_date": "2012-10-04", "authors_parsed": [["Christa", "Sharon", ""], ["Madhuri", "K. Lakshmi", ""], ["Suma", "V.", ""]]}, {"id": "1210.1745", "submitter": "Golnoosh Keshani", "authors": "Arash Ghorbannia Delavar, Golnoosh Keshani", "title": "Providing an Object Allocation Algorithm in Distributed Databases Using\n  Efficient Factors", "comments": "IJCSI International Journal of Computer Science Issues, Vol. 9, Issue\n  4, No 3, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data replication is a common method used to improve the performance of data\naccess in distributed database systems. In this paper, we present an object\nreplication algorithm in distributed database systems (ORAD). We optimize the\ncreated replicated data in distributed database systems by using activity\nfunctions of previous algorithms, changing them with new technical ways and\napplying ORAD algorithm for making decisions. We propose ORAD algorithm with\nusing effective factors and observe its results in several valid situations.\nOur objective is to propose an optimum method that replies read and write\nrequests with less cost in distributed database systems. Finally, we implement\nORAD and ADRW algorithms in a PC based network system and demonstrate that ORAD\nalgorithm is superior to ADRW algorithm in the field of average request\nservicing cost.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2012 13:05:29 GMT"}], "update_date": "2012-10-08", "authors_parsed": [["Delavar", "Arash Ghorbannia", ""], ["Keshani", "Golnoosh", ""]]}, {"id": "1210.2453", "submitter": "EPTCS", "authors": "Alessandro Solimando (University of Genova, Italy), Giorgio Delzanno\n  (University of Genova, Italy), Giovanna Guerrini (University of Genova,\n  Italy)", "title": "Automata-based Static Analysis of XML Document Adaptation", "comments": "In Proceedings GandALF 2012, arXiv:1210.2028", "journal-ref": "EPTCS 96, 2012, pp. 85-98", "doi": "10.4204/EPTCS.96.7", "report-no": null, "categories": "cs.DB cs.DS cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The structure of an XML document can be optionally specified by means of XML\nSchema, thus enabling the exploitation of structural information for efficient\ndocument handling. Upon schema evolution, or when exchanging documents among\ndifferent collections exploiting related but not identical schemas, the need\nmay arise of adapting a document, known to be valid for a given schema S, to a\ntarget schema S'. The adaptation may require knowledge of the element semantics\nand cannot always be automatically derived. In this paper, we present an\nautomata-based method for the static analysis of user-defined XML document\nadaptations, expressed as sequences of XQuery Update update primitives. The key\nfeature of the method is the use of an automatic inference method for\nextracting the type, expressed as a Hedge Automaton, of a sequence of document\nupdates. The type is computed starting from the original schema S and from\nrewriting rules that formally define the operational semantics of a sequence of\ndocument updates. Type inclusion can then be used as conformance test w.r.t.\nthe type extracted from the target schema S'.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2012 00:53:46 GMT"}], "update_date": "2012-10-10", "authors_parsed": [["Solimando", "Alessandro", "", "University of Genova, Italy"], ["Delzanno", "Giorgio", "", "University of Genova, Italy"], ["Guerrini", "Giovanna", "", "University of Genova,\n  Italy"]]}, {"id": "1210.2688", "submitter": "George  Fletcher", "authors": "George H. L. Fletcher, Marc Gyssens, Dirk Leinders, Jan Van den\n  Bussche, Dirk Van Gucht, Stijn Vansummeren", "title": "Similarity and bisimilarity notions appropriate for characterizing\n  indistinguishability in fragments of the calculus of relations", "comments": "36 pages, Journal of Logic and Computation 2014", "journal-ref": null, "doi": "10.1093/logcom/exu018", "report-no": null, "categories": "cs.LO cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications in databases, this paper considers various\nfragments of the calculus of binary relations. The fragments are obtained by\nleaving out, or keeping in, some of the standard operators, along with some\nderived operators such as set difference, projection, coprojection, and\nresiduation. For each considered fragment, a characterization is obtained for\nwhen two given binary relational structures are indistinguishable by\nexpressions in that fragment. The characterizations are based on appropriately\nadapted notions of simulation and bisimulation.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2012 18:48:08 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2013 13:47:27 GMT"}, {"version": "v3", "created": "Fri, 28 Mar 2014 15:04:11 GMT"}], "update_date": "2014-03-31", "authors_parsed": [["Fletcher", "George H. L.", ""], ["Gyssens", "Marc", ""], ["Leinders", "Dirk", ""], ["Bussche", "Jan Van den", ""], ["Van Gucht", "Dirk", ""], ["Vansummeren", "Stijn", ""]]}, {"id": "1210.2872", "submitter": "Kulthida Tuamsuk", "authors": "Tipawan Silwattananusarn and Kulthida Tuamsuk", "title": "Data Mining and Its Applications for Knowledge Management: A Literature\n  Review from 2007 to 2012", "comments": "12 pages, 4 figures", "journal-ref": "International Journal of Data Mining & Knowledge Management\n  Process (IJDKP) Vol.2, No.5, 2012, pp. 13-24", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data mining is one of the most important steps of the knowledge discovery in\ndatabases process and is considered as significant subfield in knowledge\nmanagement. Research in data mining continues growing in business and in\nlearning organization over coming decades. This review paper explores the\napplications of data mining techniques which have been developed to support\nknowledge management process. The journal articles indexed in ScienceDirect\nDatabase from 2007 to 2012 are analyzed and classified. The discussion on the\nfindings is divided into 4 topics: (i) knowledge resource; (ii) knowledge types\nand/or knowledge datasets; (iii) data mining tasks; and (iv) data mining\ntechniques and applications used in knowledge management. The article first\nbriefly describes the definition of data mining and data mining functionality.\nThen the knowledge management rationale and major knowledge management tools\nintegrated in knowledge management cycle are described. Finally, the\napplications of data mining techniques in the process of knowledge management\nare summarized and discussed.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2012 11:12:13 GMT"}], "update_date": "2012-10-11", "authors_parsed": [["Silwattananusarn", "Tipawan", ""], ["Tuamsuk", "Kulthida", ""]]}, {"id": "1210.2984", "submitter": "Francesca A. Lisi", "authors": "Francesca A. Lisi", "title": "Learning Onto-Relational Rules with Inductive Logic Programming", "comments": "18 pages. arXiv admin note: text overlap with arXiv:1003.2586", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rules complement and extend ontologies on the Semantic Web. We refer to these\nrules as onto-relational since they combine DL-based ontology languages and\nKnowledge Representation formalisms supporting the relational data model within\nthe tradition of Logic Programming and Deductive Databases. Rule authoring is a\nvery demanding Knowledge Engineering task which can be automated though\npartially by applying Machine Learning algorithms. In this chapter we show how\nInductive Logic Programming (ILP), born at the intersection of Machine Learning\nand Logic Programming and considered as a major approach to Relational\nLearning, can be adapted to Onto-Relational Learning. For the sake of\nillustration, we provide details of a specific Onto-Relational Learning\nsolution to the problem of learning rule-based definitions of DL concepts and\nroles with ILP.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2012 16:56:41 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2012 18:25:34 GMT"}], "update_date": "2012-10-30", "authors_parsed": [["Lisi", "Francesca A.", ""]]}, {"id": "1210.3139", "submitter": "Pardeep Kumar", "authors": "Pardeep Kumar, Nitin, Vivek Kumar Sehgal and Durg Singh Chauhan", "title": "A Benchmark to Select Data Mining Based Classification Algorithms For\n  Business Intelligence And Decision Support Systems", "comments": "18 Pages, 11 Figures, 6 Tables, Journal", "journal-ref": "International Journal of Data Mining and Knowledge Discovery\n  Process, September 2012, ISSN: 2230-9608", "doi": "10.5121/ijdkp.2012.2503", "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DSS serve the management, operations, and planning levels of an organization\nand help to make decisions, which may be rapidly changing and not easily\nspecified in advance. Data mining has a vital role to extract important\ninformation to help in decision making of a decision support system.\nIntegration of data mining and decision support systems (DSS) can lead to the\nimproved performance and can enable the tackling of new types of problems.\nArtificial Intelligence methods are improving the quality of decision support,\nand have become embedded in many applications ranges from ant locking\nautomobile brakes to these days interactive search engines. It provides various\nmachine learning techniques to support data mining. The classification is one\nof the main and valuable tasks of data mining. Several types of classification\nalgorithms have been suggested, tested and compared to determine the future\ntrends based on unseen data. There has been no single algorithm found to be\nsuperior over all others for all data sets. The objective of this paper is to\ncompare various classification algorithms that have been frequently used in\ndata mining for decision support systems. Three decision trees based\nalgorithms, one artificial neural network, one statistical, one support vector\nmachines with and without ada boost and one clustering algorithm are tested and\ncompared on four data sets from different domains in terms of predictive\naccuracy, error rate, classification index, comprehensibility and training\ntime. Experimental results demonstrate that Genetic Algorithm (GA) and support\nvector machines based algorithms are better in terms of predictive accuracy.\nSVM without adaboost shall be the first choice in context of speed and\npredictive accuracy. Adaboost improves the accuracy of SVM but on the cost of\nlarge training time.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2012 06:43:56 GMT"}], "update_date": "2012-10-12", "authors_parsed": [["Kumar", "Pardeep", ""], ["Nitin", "", ""], ["Sehgal", "Vivek Kumar", ""], ["Chauhan", "Durg Singh", ""]]}, {"id": "1210.3307", "submitter": "Saeid Pashazadeh", "authors": "Saeid Pashazadeh and Maryam Pashazadeh", "title": "Modelling an Automatic Proof Generator for Functional Dependency Rules\n  Using Colored Petri Net", "comments": "17 pages, 4 figures", "journal-ref": "International Journal in Foundations of Computer Science &\n  Technology (IJFCST) 2 (2012) 31-47", "doi": "10.5121/ijfcst.2012.2504", "report-no": null, "categories": "cs.DB cs.FL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Database administrators need to compute closure of functional dependencies\n(FDs) for normalization of database systems and enforcing integrity rules.\nColored Petri net (CPN) is a powerful formal method for modelling and\nverification of various systems. In this paper, we modelled Armstrong's axioms\nfor automatic proof generation of a new FD rule from initial FD rules using\nCPN. For this purpose, a CPN model of Armstrong's axioms presents and initial\nFDs considered in the model as initial color set. Then we search required FD in\nthe state space of the model via model checking. If it exists in the state\nspace, then a recursive ML code extracts the proof of this FD rule using\nfurther searches in the state space of the model.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2012 17:55:51 GMT"}], "update_date": "2012-10-12", "authors_parsed": [["Pashazadeh", "Saeid", ""], ["Pashazadeh", "Maryam", ""]]}, {"id": "1210.4130", "submitter": "Fangkai Yang", "authors": "Vladimir Lifschitz, Karl Pichotta, and Fangkai Yang", "title": "Relational Theories with Null Values and Non-Herbrand Stable Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized relational theories with null values in the sense of Reiter are\nfirst-order theories that provide a semantics for relational databases with\nincomplete information. In this paper we show that any such theory can be\nturned into an equivalent logic program, so that models of the theory can be\ngenerated using computational methods of answer set programming. As a step\ntowards this goal, we develop a general method for calculating stable models\nunder the domain closure assumption but without the unique name assumption.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2012 18:10:13 GMT"}], "update_date": "2012-10-16", "authors_parsed": [["Lifschitz", "Vladimir", ""], ["Pichotta", "Karl", ""], ["Yang", "Fangkai", ""]]}, {"id": "1210.4405", "submitter": "Hong Sun", "authors": "Hong Sun, Kristof Depraetere, Jos De Roo, Boris De Vloed, Giovanni\n  Mels, Dirk Colaert", "title": "Semantic integration and analysis of clinical data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing need to semantically process and integrate clinical data\nfrom different sources for Clinical Data Management and Clinical Decision\nSupport in the healthcare IT industry. In the clinical practice domain, the\nsemantic gap between clinical information systems and domain ontologies is\nquite often difficult to bridge in one step. In this paper, we report our\nexperience in using a two-step formalization approach to formalize clinical\ndata, i.e. from database schemas to local formalisms and from local formalisms\nto domain (unifying) formalisms. We use N3 rules to explicitly and formally\nstate the mapping from local ontologies to domain ontologies. The resulting\ndata expressed in domain formalisms can be integrated and analyzed, though\noriginating from very distinct sources. Practices of applying the two-step\napproach in the infectious disorders and cancer domains are introduced.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2012 09:49:35 GMT"}, {"version": "v2", "created": "Wed, 24 Oct 2012 11:51:10 GMT"}], "update_date": "2012-10-25", "authors_parsed": [["Sun", "Hong", ""], ["Depraetere", "Kristof", ""], ["De Roo", "Jos", ""], ["De Vloed", "Boris", ""], ["Mels", "Giovanni", ""], ["Colaert", "Dirk", ""]]}, {"id": "1210.4663", "submitter": "Zhijie Wang", "authors": "Jack Wang", "title": "A PRQ Search Method for Probabilistic Objects", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proposes an PQR search method for probabilistic objects. The\nmain idea of our method is to use a strategy called \\textit{pre-approximation}\nthat can reduce the initial problem to a highly simplified version, implying\nthat it makes the rest of steps easy to tackle. In particular, this strategy\nitself is pretty simple and easy to implement. Furthermore, motivated by the\ncost analysis, we further optimize our solution. The optimizations are mainly\nbased on two insights: (\\romannumeral 1) the number of \\textit{effective\nsubdivision}s is no more than 1; and (\\romannumeral 2) an entity with the\nlarger \\textit{span} is more likely to subdivide a single region. We\ndemonstrate the effectiveness and efficiency of our proposed approaches through\nextensive experiments under various experimental settings.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2012 08:14:50 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2013 17:57:50 GMT"}, {"version": "v3", "created": "Tue, 26 Mar 2013 15:19:49 GMT"}, {"version": "v4", "created": "Sun, 28 Apr 2013 23:13:19 GMT"}, {"version": "v5", "created": "Thu, 4 Jul 2013 07:09:45 GMT"}, {"version": "v6", "created": "Tue, 22 Oct 2013 17:59:49 GMT"}, {"version": "v7", "created": "Tue, 3 Jul 2018 13:38:39 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Wang", "Jack", ""]]}, {"id": "1210.4891", "submitter": "Sergiy Matusevych", "authors": "Sergiy Matusevych, Alex Smola, Amr Ahmed", "title": "Hokusai - Sketching Streams in Real Time", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-594-603", "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe Hokusai, a real time system which is able to capture frequency\ninformation for streams of arbitrary sequences of symbols. The algorithm uses\nthe CountMin sketch as its basis and exploits the fact that sketching is\nlinear. It provides real time statistics of arbitrary events, e.g. streams of\nqueries as a function of time. We use a factorizing approximation to provide\npoint estimates at arbitrary (time, item) combinations. Queries can be answered\nin constant time.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:46:50 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Matusevych", "Sergiy", ""], ["Smola", "Alex", ""], ["Ahmed", "Amr", ""]]}, {"id": "1210.5403", "submitter": "Michael Schmidt", "authors": "Andreas Schwarte, Peter Haase, Michael Schmidt, Katja Hose, Ralf\n  Schenkel", "title": "An Experience Report of Large Scale Federations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an experimental study of large-scale RDF federations on top of the\nBio2RDF data sources, involving 29 data sets with more than four billion RDF\ntriples deployed in a local federation. Our federation is driven by FedX, a\nhighly optimized federation mediator for Linked Data. We discuss design\ndecisions, technical aspects, and experiences made in setting up and optimizing\nthe Bio2RDF federation, and present an exhaustive experimental evaluation of\nthe federation scenario. In addition to a controlled setting with local\nfederation members, we study implications arising in a hybrid setting, where\nlocal federation members interact with remote federation members exhibiting\nhigher network latency. The outcome demonstrates the feasibility of federated\nsemantic data management in general and indicates remaining bottlenecks and\nresearch opportunities that shall serve as a guideline for future work in the\narea of federated semantic data processing.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 12:39:17 GMT"}], "update_date": "2012-10-22", "authors_parsed": [["Schwarte", "Andreas", ""], ["Haase", "Peter", ""], ["Schmidt", "Michael", ""], ["Hose", "Katja", ""], ["Schenkel", "Ralf", ""]]}, {"id": "1210.5980", "submitter": "Christian Schallhart", "authors": "Tim Furche, Georg Gottlob, Giovanni Grasso, Xiaonan Guo, Giorgio Orsi,\n  Christian Schallhart", "title": "The Ontological Key: Automatically Understanding and Integrating Forms\n  to Access the Deep Web", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forms are our gates to the web. They enable us to access the deep content of\nweb sites. Automatic form understanding provides applications, ranging from\ncrawlers over meta-search engines to service integrators, with a key to this\ncontent. Yet, it has received little attention other than as component in\nspecific applications such as crawlers or meta-search engines. No comprehensive\napproach to form understanding exists, let alone one that produces rich models\nfor semantic services or integration with linked open data.\n  In this paper, we present OPAL, the first comprehensive approach to form\nunderstanding and integration. We identify form labeling and form\ninterpretation as the two main tasks involved in form understanding. On both\nproblems OPAL pushes the state of the art: For form labeling, it combines\nfeatures from the text, structure, and visual rendering of a web page. In\nextensive experiments on the ICQ and TEL-8 benchmarks and a set of 200 modern\nweb forms OPAL outperforms previous approaches for form labeling by a\nsignificant margin. For form interpretation, OPAL uses a schema (or ontology)\nof forms in a given domain. Thanks to this domain schema, it is able to produce\nnearly perfect (more than 97 percent accuracy in the evaluation domains) form\ninterpretations. Yet, the effort to produce a domain schema is very low, as we\nprovide a Datalog-based template language that eases the specification of such\nschemata and a methodology for deriving a domain schema largely automatically\nfrom an existing domain ontology. We demonstrate the value of the form\ninterpretations in OPAL through a light-weight form integration system that\nsuccessfully translates and distributes master queries to hundreds of forms\nwith no error, yet is implemented with only a handful translation rules.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2012 17:38:29 GMT"}], "update_date": "2012-10-23", "authors_parsed": [["Furche", "Tim", ""], ["Gottlob", "Georg", ""], ["Grasso", "Giovanni", ""], ["Guo", "Xiaonan", ""], ["Orsi", "Giorgio", ""], ["Schallhart", "Christian", ""]]}, {"id": "1210.5984", "submitter": "Christian Schallhart", "authors": "Tim Furche, Georg Gottlob, Giovanni Grasso, Giorgio Orsi, Christian\n  Schallhart, Cheng Wang", "title": "AMBER: Automatic Supervision for Multi-Attribute Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extraction of multi-attribute objects from the deep web is the bridge\nbetween the unstructured web and structured data. Existing approaches either\ninduce wrappers from a set of human-annotated pages or leverage repeated\nstructures on the page without supervision. What the former lack in automation,\nthe latter lack in accuracy. Thus accurate, automatic multi-attribute object\nextraction has remained an open challenge.\n  AMBER overcomes both limitations through mutual supervision between the\nrepeated structure and automatically produced annotations. Previous approaches\nbased on automatic annotations have suffered from low quality due to the\ninherent noise in the annotations and have attempted to compensate by exploring\nmultiple candidate wrappers. In contrast, AMBER compensates for this noise by\nintegrating repeated structure analysis with annotation-based induction: The\nrepeated structure limits the search space for wrapper induction, and\nconversely, annotations allow the repeated structure analysis to distinguish\nnoise from relevant data. Both, low recall and low precision in the annotations\nare mitigated to achieve almost human quality (more than 98 percent)\nmulti-attribute object extraction.\n  To achieve this accuracy, AMBER needs to be trained once for an entire\ndomain. AMBER bootstraps its training from a small, possibly noisy set of\nattribute instances and a few unannotated sites of the domain.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2012 17:58:07 GMT"}], "update_date": "2012-10-23", "authors_parsed": [["Furche", "Tim", ""], ["Gottlob", "Georg", ""], ["Grasso", "Giovanni", ""], ["Orsi", "Giorgio", ""], ["Schallhart", "Christian", ""], ["Wang", "Cheng", ""]]}, {"id": "1210.6242", "submitter": "Lena Wiese", "authors": "Lena Wiese", "title": "Enhancing Algebraic Query Relaxation with Semantic Similarity", "comments": "Appeared in Proceedings of IADIS Information Systems 2012 (10-12\n  March 2012, Berlin, Germany)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cooperative database systems support a database user by searching for answers\nthat are closely related to his query and hence are informative answers. Common\noperators to relax the user query are Dropping Condition, Anti-Instantiation\nand Goal Replacement. In this article, we provide an algebraic version of these\noperators. Moreover we propose some heuristics to assign a degree of similarity\nto each tuple of an answer table; this degree can help the user to determine\nwhether this answer is relevant for him or not.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2012 14:22:02 GMT"}], "update_date": "2012-10-24", "authors_parsed": [["Wiese", "Lena", ""]]}, {"id": "1210.6272", "submitter": "Rebeca Schroeder", "authors": "Rebeca Schroeder and Ronaldo Santos Mello and Carmem Satie Hara", "title": "Affinity-based XML Fragmentation", "comments": "This paper has been withdrawn by the author due to its recent\n  publication in the conference site", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this paper we tackle the fragmentation problem for highly distributed\ndatabases. In such an environment, a suitable fragmentation strategy may\nprovide scalability and availability by minimizing distributed transactions. We\npropose an approach for XML fragmentation that takes as input both the\napplication's expected workload and a storage threshold, and produces as output\nan XML fragmentation schema. Our workload-aware method aims to minimize the\nexecution of distributed transactions by packing up related data in a small set\nof fragments. We present experiments that compare alternative fragmentation\nschemas, showing that the one produced by our technique provides a\nfiner-grained result and better system throughput.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2012 15:46:23 GMT"}, {"version": "v2", "created": "Wed, 24 Apr 2013 18:45:47 GMT"}], "update_date": "2013-04-25", "authors_parsed": [["Schroeder", "Rebeca", ""], ["Mello", "Ronaldo Santos", ""], ["Hara", "Carmem Satie", ""]]}, {"id": "1210.6284", "submitter": "Paolo Giosu\\'e Giarrusso", "authors": "Paolo G. Giarrusso, Klaus Ostermann, Michael Eichberg, Ralf Mitschke,\n  Tillmann Rendel, Christian K\\\"astner", "title": "Reify Your Collection Queries for Modularity and Speed!", "comments": "20 pages", "journal-ref": "Proceedings of the 12th annual international conference on\n  Aspect-oriented software development (AOSD '13), 2013. ACM, New York, NY,\n  USA, 1-12", "doi": "10.1145/2451436.2451438", "report-no": null, "categories": "cs.PL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modularity and efficiency are often contradicting requirements, such that\nprogramers have to trade one for the other. We analyze this dilemma in the\ncontext of programs operating on collections. Performance-critical code using\ncollections need often to be hand-optimized, leading to non-modular, brittle,\nand redundant code. In principle, this dilemma could be avoided by automatic\ncollection-specific optimizations, such as fusion of collection traversals,\nusage of indexing, or reordering of filters. Unfortunately, it is not obvious\nhow to encode such optimizations in terms of ordinary collection APIs, because\nthe program operating on the collections is not reified and hence cannot be\nanalyzed.\n  We propose SQuOpt, the Scala Query Optimizer--a deep embedding of the Scala\ncollections API that allows such analyses and optimizations to be defined and\nexecuted within Scala, without relying on external tools or compiler\nextensions. SQuOpt provides the same \"look and feel\" (syntax and static typing\nguarantees) as the standard collections API. We evaluate SQuOpt by\nre-implementing several code analyses of the Findbugs tool using SQuOpt, show\naverage speedups of 12x with a maximum of 12800x and hence demonstrate that\nSQuOpt can reconcile modularity and efficiency in real-world applications.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2012 16:32:13 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Giarrusso", "Paolo G.", ""], ["Ostermann", "Klaus", ""], ["Eichberg", "Michael", ""], ["Mitschke", "Ralf", ""], ["Rendel", "Tillmann", ""], ["K\u00e4stner", "Christian", ""]]}, {"id": "1210.6746", "submitter": "Mohammed Eunus Ali Dr", "authors": "Hossain Mahmud, Ashfaq Mahmood Amin, Mohammed Eunus Ali and Tanzima\n  Hashem", "title": "Shared Execution of Path Queries on Road Networks", "comments": "20 pages, 7 figures", "journal-ref": "Lecture Notes in Computer Science book series (LNCS, 2013, volume\n  8098)", "doi": "10.1007/978-3-642-40235-7_21", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advancement of mobile technologies and the proliferation of map-based\napplications have enabled a user to access a wide variety of services that\nrange from information queries to navigation systems. Due to the popularity of\nmap-based applications among the users, the service provider often requires to\nanswer a large number of simultaneous queries. Thus, processing queries\nefficiently on spatial networks (i.e., road networks) have become an important\nresearch area in recent years. In this paper, we focus on path queries that\nfind the shortest path between a source and a destination of the user. In\nparticular, we address the problem of finding the shortest paths for a large\nnumber of simultaneous path queries in road networks. Traditional systems that\nconsider one query at a time are not suitable for many applications due to high\ncomputational and service costs. These systems cannot guarantee required\nresponse time in high load conditions. We propose an efficient group based\napproach that provides a practical solution with reduced cost. The key concept\nfor our approach is to group queries that share a common travel path and then\ncompute the shortest path for the group. Experimental results show that our\napproach is on an average ten times faster than the traditional approach in\nreturn of sacrificing the accuracy by 0.5% in the worst case, which is\nacceptable for most of the users.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2012 06:30:51 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Mahmud", "Hossain", ""], ["Amin", "Ashfaq Mahmood", ""], ["Ali", "Mohammed Eunus", ""], ["Hashem", "Tanzima", ""]]}, {"id": "1210.7350", "submitter": "Jimmy Lin", "authors": "Gilad Mishne, Jeff Dalton, Zhenghua Li, Aneesh Sharma, Jimmy Lin", "title": "Fast Data in the Era of Big Data: Twitter's Real-Time Related Query\n  Suggestion Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the architecture behind Twitter's real-time related query\nsuggestion and spelling correction service. Although these tasks have received\nmuch attention in the web search literature, the Twitter context introduces a\nreal-time \"twist\": after significant breaking news events, we aim to provide\nrelevant results within minutes. This paper provides a case study illustrating\nthe challenges of real-time data processing in the era of \"big data\". We tell\nthe story of how our system was built twice: our first implementation was built\non a typical Hadoop-based analytics stack, but was later replaced because it\ndid not meet the latency requirements necessary to generate meaningful\nreal-time results. The second implementation, which is the system deployed in\nproduction, is a custom in-memory processing engine specifically designed for\nthe task. This experience taught us that the current typical usage of Hadoop as\na \"big data\" platform, while great for experimentation, is not well suited to\nlow-latency processing, and points the way to future work on data analytics\nplatforms that can handle \"big\" as well as \"fast\" data.\n", "versions": [{"version": "v1", "created": "Sat, 27 Oct 2012 17:20:42 GMT"}], "update_date": "2012-10-30", "authors_parsed": [["Mishne", "Gilad", ""], ["Dalton", "Jeff", ""], ["Li", "Zhenghua", ""], ["Sharma", "Aneesh", ""], ["Lin", "Jimmy", ""]]}, {"id": "1210.8229", "submitter": "Jnanamurthy Hk", "authors": "Jnanamurthy H. K., Vishesh H. V., Vishruth Jain, Preetham Kumar,\n  Radhika M. Pai", "title": "Top Down Approach to find Maximal Frequent Item Sets using Subset\n  Creation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Association rule has been an area of active research in the field of\nknowledge discovery. Data mining researchers had improved upon the quality of\nassociation rule mining for business development by incorporating influential\nfactors like value (utility), quantity of items sold (weight) and more for the\nmining of association patterns. In this paper, we propose an efficient approach\nto find maximal frequent itemset first. Most of the algorithms in literature\nused to find minimal frequent item first, then with the help of minimal\nfrequent itemsets derive the maximal frequent itemsets. These methods consume\nmore time to find maximal frequent itemsets. To overcome this problem, we\npropose a navel approach to find maximal frequent itemset directly using the\nconcepts of subsets. The proposed method is found to be efficient in finding\nmaximal frequent itemsets.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2012 05:09:32 GMT"}], "update_date": "2012-11-01", "authors_parsed": [["K.", "Jnanamurthy H.", ""], ["V.", "Vishesh H.", ""], ["Jain", "Vishruth", ""], ["Kumar", "Preetham", ""], ["Pai", "Radhika M.", ""]]}, {"id": "1210.8242", "submitter": "Sandeep Gupta", "authors": "Sandeep Gupta", "title": "Pipelined Workflow in Hybrid MPI/Pthread runtime for External Memory\n  Graph Construction", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph construction from a given set of edges is a data-intensive operator\nthat appears in social network analysis, ontology enabled databases, and, other\nanalytics processing. The operator represents an edge list to compressed sparse\nrow (CSR) representation (or sometimes in adjacency list, or as clustered\nB-Tree storage). In this work, we show how to scale CSR construction to massive\nscale on SSD-enabled supercomputers such as Gordon using pipelined processing.\nWe develop several abstraction and operations for external memory and parallel\nedge list and integer array processing that are utilized towards building a\nscalable algorithm for creating CSR representation.\n  Our experiments demonstrate that this scheme is four to six times faster than\ncurrently available implementation. Moreover, our scheme can handle up to 8\nbillion edges (128GB) by using external memory as compared to prior schemes\nwhere performance degrades considerably for edge list size 26 million and\nbeyond.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2012 06:29:10 GMT"}], "update_date": "2012-11-01", "authors_parsed": [["Gupta", "Sandeep", ""]]}]