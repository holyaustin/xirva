[{"id": "1202.0018", "submitter": "Houari Mahfoud", "authors": "Houari Mahfoud (INRIA Lorraine - LORIA / LIFC), Abdessamad Imine\n  (INRIA Lorraine - LORIA / LIFC)", "title": "A General Approach for Securely Querying and Updating XML Data", "comments": "No. RR-7870 (2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past years several works have proposed access control models for XML\ndata where only read-access rights over non-recursive DTDs are considered. A\nfew amount of works have studied the access rights for updates. In this paper,\nwe present a general model for specifying access control on XML data in the\npresence of update operations of W3C XQuery Update Facility. Our approach for\nenforcing such updates specifications is based on the notion of query rewriting\nwhere each update operation defined over arbitrary DTD (recursive or not) is\nrewritten to a safe one in order to be evaluated only over XML data which can\nbe updated by the user. We investigate in the second part of this report the\nsecure of XML updating in the presence of read-access rights specified by a\nsecurity views. For an XML document, a security view represents for each class\nof users all and only the parts of the document these users are able to see. We\nshow that an update operation defined over a security view can cause disclosure\nof sensitive data hidden by this view if it is not thoroughly rewritten with\nrespect to both read and update access rights. Finally, we propose a security\nview based approach for securely updating XML in order to preserve the\nconfidentiality and integrity of XML data.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2012 21:08:19 GMT"}], "update_date": "2012-02-02", "authors_parsed": [["Mahfoud", "Houari", "", "INRIA Lorraine - LORIA / LIFC"], ["Imine", "Abdessamad", "", "INRIA Lorraine - LORIA / LIFC"]]}, {"id": "1202.0242", "submitter": "Daniel Zinn", "authors": "Daniel Zinn", "title": "Weak Forms of Monotonicity and Coordination-Freeness", "comments": "Early Research Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our earlier work titled: \"Win-move is Coordination-Free (Sometimes)\" has\nshown that the classes of queries that can be distributedly computed in a\ncoordination-free manner form a strict hierarchy depending on the assumptions\nof the model for distributed computations. In this paper, we further\ncharacterize these classes by revealing a tight relationship between them and\nnovel weakened forms of monotonicity.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2012 18:46:41 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Zinn", "Daniel", ""]]}, {"id": "1202.0474", "submitter": "M. H. van Emden", "authors": "Philip Kelly and M. H. van Emden", "title": "Relational Semantics for Databases and Predicate Calculus", "comments": "18 pages, 8 figures. arXiv admin note: text overlap with\n  arXiv:cs/0607039", "journal-ref": null, "doi": null, "report-no": "University of Victoria Department of Computer Science report number\n  DCS-343-IR", "categories": "cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The relational data model requires a theory of relations in which tuples are\nnot only many-sorted, but can also have indexes that are not necessarily\nnumerical. In this paper we develop such a theory and define operations on\nrelations that are adequate for database use. The operations are similar to\nthose of Codd's relational algebra, but differ in being based on a\nmathematically adequate theory of relations. The semantics of predicate\ncalculus, being oriented toward the concept of satisfiability, is not suitable\nfor relational databases. We develop an alternative semantics that assigns\nrelations as meaning to formulas with free variables. This semantics makes the\nclassical predicate calculus suitable as a query language for relational\ndatabases.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2012 16:13:17 GMT"}, {"version": "v2", "created": "Sun, 5 Feb 2012 06:58:20 GMT"}, {"version": "v3", "created": "Tue, 7 Feb 2012 16:57:13 GMT"}], "update_date": "2012-02-08", "authors_parsed": [["Kelly", "Philip", ""], ["van Emden", "M. H.", ""]]}, {"id": "1202.2335", "submitter": "Katherine Trushkowsky", "authors": "Beth Trushkowsky, Tim Kraska, Michael J. Franklin, Purnamrita Sarkar", "title": "Getting It All from the Crowd", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hybrid human/computer systems promise to greatly expand the usefulness of\nquery processing by incorporating the crowd for data gathering and other tasks.\nSuch systems raise many database system implementation questions. Perhaps most\nfundamental is that the closed world assumption underlying relational query\nsemantics does not hold in such systems. As a consequence the meaning of even\nsimple queries can be called into question. Furthermore query progress\nmonitoring becomes difficult due to non-uniformities in the arrival of\ncrowdsourced data and peculiarities of how people work in crowdsourcing\nsystems. To address these issues, we develop statistical tools that enable\nusers and systems developers to reason about tradeoffs between time/cost and\ncompleteness. These tools can also help drive query execution and crowdsourcing\nstrategies. We evaluate our techniques using experiments on a popular\ncrowdsourcing platform.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2012 19:47:57 GMT"}], "update_date": "2012-02-13", "authors_parsed": [["Trushkowsky", "Beth", ""], ["Kraska", "Tim", ""], ["Franklin", "Michael J.", ""], ["Sarkar", "Purnamrita", ""]]}, {"id": "1202.2591", "submitter": "David Spivak", "authors": "David I. Spivak", "title": "Database queries and constraints via lifting problems", "comments": null, "journal-ref": "Math. Struct. Comp. Sci. 24 (2014) e240602", "doi": "10.1017/S0960129513000479", "report-no": null, "categories": "math.CT cs.DB math.AT", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Previous work has demonstrated that categories are useful and expressive\nmodels for databases. In the present paper we build on that model, showing that\ncertain queries and constraints correspond to lifting problems, as found in\nmodern approaches to algebraic topology. In our formulation, each so-called\nSPARQL graph pattern query corresponds to a category-theoretic lifting problem,\nwhereby the set of solutions to the query is precisely the set of lifts. We\ninterpret constraints within the same formalism and then investigate some basic\nproperties of queries and constraints. In particular, to any database $\\pi$ we\ncan associate a certain derived database $\\Qry(\\pi)$ of queries on $\\pi$. As an\napplication, we explain how giving users access to certain parts of\n$\\Qry(\\pi)$, rather than direct access to $\\pi$, improves ones ability to\nmanage the impact of schema evolution.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2012 00:07:02 GMT"}, {"version": "v2", "created": "Thu, 12 Jul 2012 19:59:56 GMT"}, {"version": "v3", "created": "Mon, 13 May 2013 19:41:28 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Spivak", "David I.", ""]]}, {"id": "1202.2926", "submitter": "Mala Dutta Ms.", "authors": "Anjana K. Mahanta and Mala Dutta", "title": "Detection of Calendar-Based Periodicities of Interval-Based Temporal\n  Patterns", "comments": null, "journal-ref": "International Journal of Data Mining & Knowledge Management\n  Process (IJDKP) Vol.2, No.1, pp. 17-31,January 2012", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel technique to identify calendar-based (annual, monthly and\ndaily) periodicities of an interval-based temporal pattern. An interval-based\ntemporal pattern is a pattern that occurs across a time-interval, then\ndisappears for some time, again recurs across another time-interval and so on\nand so forth. Given the sequence of time-intervals in which an interval-based\ntemporal pattern has occurred, we propose a method for identifying the extent\nto which the pattern is periodic with respect to a calendar cycle. In\ncomparison to previous work, our method is asymptotically faster. We also show\nan interesting relationship between periodicities across different levels of\nany hierarchical timestamp (year/month/day, hour/minute/second etc.).\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 03:13:10 GMT"}], "update_date": "2012-02-15", "authors_parsed": [["Mahanta", "Anjana K.", ""], ["Dutta", "Mala", ""]]}, {"id": "1202.3179", "submitter": "Chao Han", "authors": "Ke Wang (1), Chao Han (1), Ada Waichee Fu (2) ((1) School of Computing\n  Science Simon Fraser University, (2) Department of Computer Science and\n  Engineering Chinese University of Hong Kong)", "title": "Randomization Resilient To Sensitive Reconstruction", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the randomization approach, sensitive data items of records are\nrandomized to protect privacy of individuals while allowing the distribution\ninformation to be reconstructed for data analysis. In this paper, we\ndistinguish between reconstruction that has potential privacy risk, called\nmicro reconstruction, and reconstruction that does not, called aggregate\nreconstruction. We show that the former could disclose sensitive information\nabout a target individual, whereas the latter is more useful for data analysis\nthan for privacy breaches. To limit the privacy risk of micro reconstruction,\nwe propose a privacy definition, called (epsilon,delta)-reconstruction-privacy.\nIntuitively, this privacy notion requires that micro reconstruction has a large\nerror with a large probability. The promise of this approach is that micro\nreconstruction is more sensitive to the number of independent trials in the\nrandomization process than aggregate reconstruction is; therefore, reducing the\nnumber of independent trials helps achieve\n(epsilon,delta)-reconstruction-privacy while preserving the accuracy of\naggregate reconstruction. We present an algorithm based on this idea and\nevaluate the effectiveness of this approach using real life data sets.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2012 00:23:24 GMT"}], "update_date": "2012-02-16", "authors_parsed": [["Wang", "Ke", ""], ["Han", "Chao", ""], ["Fu", "Ada Waichee", ""]]}, {"id": "1202.3215", "submitter": "Malar", "authors": "J. Malar Vizhi and T. Bhuvaneswari", "title": "Data quality measurement on categorical data using genetic algorithm", "comments": "10 pages. arXiv admin note: text overlap with arXiv:1011.0328 by\n  other authors", "journal-ref": "International Journal of Data Mining & Knowledge Management\n  Process (IJDKP) Vol.2, No.1, January 2012", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Data quality on categorical attribute is a difficult problem that has not\nreceived as much attention as numerical counterpart. Our basic idea is to\nemploy association rule for the purpose of data quality measurement. Strong\nrule generation is an important area of data mining. Association rule mining\nproblems can be considered as a multi objective problem rather than as a single\nobjective one. The main area of concentration was the rules generated by\nassociation rule mining using genetic algorithm. The advantage of using genetic\nalgorithm is to discover high level prediction rules is that they perform a\nglobal search and cope better with attribute interaction than the greedy rule\ninduction algorithm often used in data mining. Genetic algorithm based approach\nutilizes the linkage between association rule and feature selection. In this\npaper, we put forward a Multi objective genetic algorithm approach for data\nquality on categorical attributes. The result shows that our approach is\noutperformed by the objectives like accuracy, completeness, comprehensibility\nand interestingness.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2012 06:32:26 GMT"}], "update_date": "2012-02-16", "authors_parsed": [["Vizhi", "J. Malar", ""], ["Bhuvaneswari", "T.", ""]]}, {"id": "1202.3253", "submitter": "Jia Wang", "authors": "Ada Wai-Chee Fu, Jia Wang, Ke Wang, Raymond Chi-Wing Wong", "title": "Small Count Privacy and Large Count Utility in Data Publishing", "comments": "12 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the introduction of differential privacy has been a major breakthrough\nin the study of privacy preserving data publication, some recent work has\npointed out a number of cases where it is not possible to limit inference about\nindividuals. The dilemma that is intrinsic in the problem is the simultaneous\nrequirement of data utility in the published data. Differential privacy does\nnot aim to protect information about an individual that can be uncovered even\nwithout the participation of the individual. However, this lack of coverage may\nviolate the principle of individual privacy. Here we propose a solution by\nproviding protection to sensitive information, by which we refer to the answers\nfor aggregate queries with small counts. Previous works based on\n$\\ell$-diversity can be seen as providing a special form of this kind of\nprotection. Our method is developed with another goal which is to provide\ndifferential privacy guarantee, and for that we introduce a more refined form\nof differential privacy to deal with certain practical issues. Our empirical\nstudies show that our method can preserve better utilities than a number of\nstate-of-the-art methods although these methods do not provide the protections\nthat we provide.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2012 10:04:25 GMT"}], "update_date": "2012-02-16", "authors_parsed": [["Fu", "Ada Wai-Chee", ""], ["Wang", "Jia", ""], ["Wang", "Ke", ""], ["Wong", "Raymond Chi-Wing", ""]]}, {"id": "1202.3255", "submitter": "Toni Stojanovski", "authors": "Toni Stojanovski, Ivan Velinov, Marko Vu\\v{c}kovik", "title": "Scalability of Data Binding in ASP.NET Web Applications", "comments": "CITYR Conference, September 2012, Skopje, Macedonia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ASP.NET web applications typically employ server controls to provide dynamic\nweb pages, and data-bound server controls to display and maintain database\ndata. Most developers use default properties of ASP.NET server controls when\ndeveloping web applications, which allows for rapid development of workable\napplications. However, creating a high-performance, multi-user, and scalable\nweb application requires enhancement of server controls using custom-made code.\nIn this empirical study we evaluate the impact of various technical approaches\nfor paging and sorting functionality in data-driven ASP.NET web applications:\nautomatic data paging and sorting in web server controls on web server; paging\nand sorting on database server; indexed and non-indexed database columns;\nclustered vs. non-clustered indices. We observed significant performance\nimprovements when custom paging based on SQL stored procedure and clustered\nindex is used.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2012 10:08:20 GMT"}, {"version": "v2", "created": "Fri, 26 Oct 2012 22:50:03 GMT"}, {"version": "v3", "created": "Mon, 5 Nov 2012 11:21:11 GMT"}], "update_date": "2012-11-06", "authors_parsed": [["Stojanovski", "Toni", ""], ["Velinov", "Ivan", ""], ["Vu\u010dkovik", "Marko", ""]]}, {"id": "1202.3399", "submitter": "Chao Li", "authors": "Chao Li and Gerome Miklau", "title": "Optimal error of query sets under the differentially-private matrix\n  mechanism", "comments": "35 pages; Short version to appear in the 16th International\n  Conference on Database Theory (ICDT), 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common goal of privacy research is to release synthetic data that satisfies\na formal privacy guarantee and can be used by an analyst in place of the\noriginal data. To achieve reasonable accuracy, a synthetic data set must be\ntuned to support a specified set of queries accurately, sacrificing fidelity\nfor other queries.\n  This work considers methods for producing synthetic data under differential\nprivacy and investigates what makes a set of queries \"easy\" or \"hard\" to\nanswer. We consider answering sets of linear counting queries using the matrix\nmechanism, a recent differentially-private mechanism that can reduce error by\nadding complex correlated noise adapted to a specified workload.\n  Our main result is a novel lower bound on the minimum total error required to\nsimultaneously release answers to a set of workload queries. The bound reveals\nthat the hardness of a query workload is related to the spectral properties of\nthe workload when it is represented in matrix form. The bound is most\ninformative for $(\\epsilon,\\delta)$-differential privacy but also applies to\n$\\epsilon$-differential privacy.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2012 18:24:04 GMT"}, {"version": "v2", "created": "Tue, 21 Feb 2012 03:19:51 GMT"}, {"version": "v3", "created": "Wed, 29 Aug 2012 13:34:14 GMT"}, {"version": "v4", "created": "Mon, 17 Dec 2012 23:48:42 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Li", "Chao", ""], ["Miklau", "Gerome", ""]]}, {"id": "1202.3461", "submitter": "Liyue Fan", "authors": "Liyue Fan, Li Xiong", "title": "Adaptively Sharing Time-Series with Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sharing real-time aggregate statistics of private data is of great value to\nthe public to perform data mining for understanding important phenomena, such\nas Influenza outbreaks and traffic congestion. However, releasing time-series\ndata with standard differential privacy mechanism has limited utility due to\nhigh correlation between data values. We propose FAST, a novel framework to\nrelease real-time aggregate statistics under differential privacy based on\nfiltering and adaptive sampling. To minimize the overall privacy cost, FAST\nadaptively samples long time-series according to the detected data dynamics. To\nimprove the accuracy of data release per time stamp, FAST predicts data values\nat non-sampling points and corrects noisy observations at sampling points. Our\nexperiments with real-world as well as synthetic data sets confirm that FAST\nimproves the accuracy of released aggregates even under small privacy cost and\ncan be used to enable a wide range of monitoring applications.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2012 22:14:31 GMT"}, {"version": "v2", "created": "Sat, 5 Jan 2013 01:38:57 GMT"}], "update_date": "2013-01-08", "authors_parsed": [["Fan", "Liyue", ""], ["Xiong", "Li", ""]]}, {"id": "1202.3667", "submitter": "Marcelo Arenas", "authors": "Juan F. Sequeda, Marcelo Arenas and Daniel P. Miranker", "title": "On Directly Mapping Relational Databases to RDF and OWL (Extended\n  Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mapping relational databases to RDF is a fundamental problem for the\ndevelopment of the Semantic Web. We present a solution, inspired by draft\nmethods defined by the W3C where relational databases are directly mapped to\nRDF and OWL. Given a relational database schema and its integrity constraints,\nthis direct mapping produces an OWL ontology, which, provides the basis for\ngenerating RDF instances. The semantics of this mapping is defined using\nDatalog. Two fundamental properties are information preservation and query\npreservation. We prove that our mapping satisfies both conditions, even for\nrelational databases that contain null values. We also consider two desirable\nproperties: monotonicity and semantics preservation. We prove that our mapping\nis monotone and also prove that no monotone mapping, including ours, is\nsemantic preserving. We realize that monotonicity is an obstacle for semantic\npreservation and thus present a non-monotone direct mapping that is semantics\npreserving.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2012 19:04:49 GMT"}, {"version": "v2", "created": "Fri, 17 Feb 2012 17:22:40 GMT"}, {"version": "v3", "created": "Wed, 22 Feb 2012 18:42:41 GMT"}], "update_date": "2012-02-23", "authors_parsed": [["Sequeda", "Juan F.", ""], ["Arenas", "Marcelo", ""], ["Miranker", "Daniel P.", ""]]}, {"id": "1202.3686", "submitter": "Peng Wang", "authors": "Ke Wang, Peng Wang, Ada Waichee Fu, Raywong Chi-Wing Wong", "title": "Inferential or Differential: Privacy Laws Dictate", "comments": "13 pages and 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  So far, privacy models follow two paradigms. The first paradigm, termed\ninferential privacy in this paper, focuses on the risk due to statistical\ninference of sensitive information about a target record from other records in\nthe database. The second paradigm, known as differential privacy, focuses on\nthe risk to an individual when included in, versus when not included in, the\ndatabase. The contribution of this paper consists of two parts. The first part\npresents a critical analysis on differential privacy with two results: (i) the\ndifferential privacy mechanism does not provide inferential privacy, (ii) the\nimpossibility result about achieving Dalenius's privacy goal [5] is based on an\nadversary simulated by a Turing machine, but a human adversary may behave\ndifferently; consequently, the practical implication of the impossibility\nresult remains unclear. The second part of this work is devoted to a solution\naddressing three major drawbacks in previous approaches to inferential privacy:\nlack of flexibility for handling variable sensitivity, poor utility, and\nvulnerability to auxiliary information.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2012 20:18:48 GMT"}], "update_date": "2012-02-17", "authors_parsed": [["Wang", "Ke", ""], ["Wang", "Peng", ""], ["Fu", "Ada Waichee", ""], ["Wong", "Raywong Chi-Wing", ""]]}, {"id": "1202.3807", "submitter": "Chao Li", "authors": "Chao Li and Gerome Miklau", "title": "An Adaptive Mechanism for Accurate Query Answering under Differential\n  Privacy", "comments": "VLDB2012. arXiv admin note: substantial text overlap with\n  arXiv:1103.1367", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel mechanism for answering sets of count- ing queries under\ndifferential privacy. Given a workload of counting queries, the mechanism\nautomatically selects a different set of \"strategy\" queries to answer\nprivately, using those answers to derive answers to the workload. The main\nalgorithm proposed in this paper approximates the optimal strategy for any\nworkload of linear counting queries. With no cost to the privacy guarantee, the\nmechanism improves significantly on prior approaches and achieves near-optimal\nerror for many workloads, when applied under (\\epsilon, \\delta)-differential\nprivacy. The result is an adaptive mechanism which can help users achieve good\nutility without requiring that they reason carefully about the best formulation\nof their task.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2012 22:00:09 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Li", "Chao", ""], ["Miklau", "Gerome", ""]]}, {"id": "1202.3957", "submitter": "Diego Figueira", "authors": "Diego Figueira (INRIA & ENS Cachan, LSV)", "title": "Alternating register automata on finite words and trees", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 8, Issue 1 (March 9,\n  2012) lmcs:907", "doi": "10.2168/LMCS-8(1:22)2012", "report-no": null, "categories": "cs.DB cs.FL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study alternating register automata on data words and data trees in\nrelation to logics. A data word (resp. data tree) is a word (resp. tree) whose\nevery position carries a label from a finite alphabet and a data value from an\ninfinite domain. We investigate one-way automata with alternating control over\ndata words or trees, with one register for storing data and comparing them for\nequality. This is a continuation of the study started by Demri, Lazic and\nJurdzinski. From the standpoint of register automata models, this work aims at\ntwo objectives: (1) simplifying the existent decidability proofs for the\nemptiness problem for alternating register automata; and (2) exhibiting\ndecidable extensions for these models. From the logical perspective, we show\nthat (a) in the case of data words, satisfiability of LTL with one register and\nquantification over data values is decidable; and (b) the satisfiability\nproblem for the so-called forward fragment of XPath on XML documents is\ndecidable, even in the presence of DTDs and even of key constraints. The\ndecidability is obtained through a reduction to the automata model introduced.\nThis fragment contains the child, descendant, next-sibling and\nfollowing-sibling axes, as well as data equality and inequality tests.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2012 16:38:10 GMT"}, {"version": "v2", "created": "Wed, 7 Mar 2012 23:57:49 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Figueira", "Diego", "", "INRIA & ENS Cachan, LSV"]]}, {"id": "1202.4532", "submitter": "Anirban Sarkar", "authors": "Anirban Sarkar", "title": "Conceptual Level Design of Semi-structured Database System:\n  Graph-semantic Based Approach", "comments": "10 Pages, 3 Tables, 12 Figures", "journal-ref": "International Journal of Advanced Computer Science and\n  Applications, The SAI Pubs., USA, Vol. 2 (10), 112 - 121, November, 2011.\n  [ISSN: 2156-5570(Online) & ISSN : 2158-107X(Print)]", "doi": null, "report-no": null, "categories": "cs.SE cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper has proposed a Graph - semantic based conceptual model for\nsemi-structured database system, called GOOSSDM, to conceptualize the different\nfacets of such system in object oriented paradigm. The model defines a set of\ngraph based formal constructs, variety of relationship types with participation\nconstraints and rich set of graphical notations to specify the conceptual level\ndesign of semi-structured database system. The proposed design approach\nfacilitates modeling of irregular, heterogeneous, hierarchical and\nnon-hierarchical semi-structured data at the conceptual level. Moreover, the\nproposed GOOSSDM is capable to model XML document at conceptual level with the\nfacility of document-centric design, ordering and disjunction characteristic. A\nrule based transformation mechanism of GOOSSDM schema into the equivalent XML\nSchema Definition (XSD) also has been proposed in this paper. The concepts of\nthe proposed conceptual model have been implemented using Generic Modeling\nEnvironment (GME).\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2012 05:46:28 GMT"}], "update_date": "2012-02-22", "authors_parsed": [["Sarkar", "Anirban", ""]]}, {"id": "1202.4815", "submitter": "Saurabh  Pal", "authors": "Surjeet Kumar Yadav, Brijesh Bharadwaj and Saurabh Pal", "title": "Data Mining Applications: A comparative Study for Predicting Student's\n  performance", "comments": "7 pages, 2 figures. arXiv admin note: text overlap with\n  arXiv:1201.3417 and arXiv:1201.3418", "journal-ref": "International Journal of Innovative Technology and Creative\n  Engineering, Vol.1 No.12 (2011) 13-19", "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Discovery and Data Mining (KDD) is a multidisciplinary area\nfocusing upon methodologies for extracting useful knowledge from data and there\nare several useful KDD tools to extracting the knowledge. This knowledge can be\nused to increase the quality of education. But educational institution does not\nuse any knowledge discovery process approach on these data. Data mining can be\nused for decision making in educational system. A decision tree classifier is\none of the most widely used supervised learning methods used for data\nexploration based on divide & conquer technique. This paper discusses use of\ndecision trees in educational data mining. Decision tree algorithms are applied\non students' past performance data to generate the model and this model can be\nused to predict the students' performance. It helps earlier in identifying the\ndropouts and students who need special attention and allow the teacher to\nprovide appropriate advising/counseling.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2012 04:15:54 GMT"}, {"version": "v2", "created": "Thu, 23 Feb 2012 15:52:32 GMT"}], "update_date": "2012-02-24", "authors_parsed": [["Yadav", "Surjeet Kumar", ""], ["Bharadwaj", "Brijesh", ""], ["Pal", "Saurabh", ""]]}, {"id": "1202.4818", "submitter": "Sanober Shaikh", "authors": "Sanober Shaikh, Madhuri rao", "title": "Association Rule Mining Based On Trade List", "comments": "15 pages", "journal-ref": "http://www.airccj.org/ijdkp/ijdkp2011.html", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a new mining algorithm is defined based on frequent item set.\nApriori Algorithm scans the database every time when it finds the frequent item\nset so it is very time consuming and at each step it generates candidate item\nset. So for large databases it takes lots of space to store candidate item set\n.In undirected item set graph, it is improvement on apriori but it takes time\nand space for tree generation. The defined algorithm scans the database at the\nstart only once and then from that scanned data base it generates the Trade\nList. It contains the information of whole database. By considering minimum\nsupport it finds the frequent item set and by considering the minimum\nconfidence it generates the association rule. If database and minimum support\nis changed, the new algorithm finds the new frequent items by scanning Trade\nList. That is why it's executing efficiency is improved distinctly compared to\ntraditional algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2012 05:07:44 GMT"}], "update_date": "2012-02-23", "authors_parsed": [["Shaikh", "Sanober", ""], ["rao", "Madhuri", ""]]}, {"id": "1202.4910", "submitter": "Justin Hsu", "authors": "Justin Hsu, Sanjeev Khanna, Aaron Roth", "title": "Distributed Private Heavy Hitters", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-642-31594-7_39", "report-no": null, "categories": "cs.DS cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we give efficient algorithms and lower bounds for solving the\nheavy hitters problem while preserving differential privacy in the fully\ndistributed local model. In this model, there are n parties, each of which\npossesses a single element from a universe of size N. The heavy hitters problem\nis to find the identity of the most common element shared amongst the n\nparties. In the local model, there is no trusted database administrator, and so\nthe algorithm must interact with each of the $n$ parties separately, using a\ndifferentially private protocol. We give tight information-theoretic upper and\nlower bounds on the accuracy to which this problem can be solved in the local\nmodel (giving a separation between the local model and the more common\ncentralized model of privacy), as well as computationally efficient algorithms\neven in the case where the data universe N may be exponentially large.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2012 14:01:57 GMT"}, {"version": "v2", "created": "Tue, 8 May 2012 03:16:31 GMT"}, {"version": "v3", "created": "Thu, 6 Nov 2014 22:24:26 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Hsu", "Justin", ""], ["Khanna", "Sanjeev", ""], ["Roth", "Aaron", ""]]}, {"id": "1202.4961", "submitter": "Daniel Lemire", "authors": "Owen Kaser and Daniel Lemire", "title": "Strongly universal string hashing is fast", "comments": "Software is available at\n  http://code.google.com/p/variablelengthstringhashing/ and\n  https://github.com/lemire/StronglyUniversalStringHashing", "journal-ref": "Computer Journal (2014) 57 (11): 1624-1638", "doi": "10.1093/comjnl/bxt070", "report-no": null, "categories": "cs.DB cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present fast strongly universal string hashing families: they can process\ndata at a rate of 0.2 CPU cycle per byte. Maybe surprisingly, we find that\nthese families---though they require a large buffer of random numbers---are\noften faster than popular hash functions with weaker theoretical guarantees.\nMoreover, conventional wisdom is that hash functions with fewer multiplications\nare faster. Yet we find that they may fail to be faster due to operation\npipelining. We present experimental results on several processors including\nlow-powered processors. Our tests include hash functions designed for\nprocessors with the Carry-Less Multiplication (CLMUL) instruction set. We also\nprove, using accessible proofs, the strong universality of our families.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2012 16:34:24 GMT"}, {"version": "v2", "created": "Mon, 18 Feb 2013 22:17:05 GMT"}, {"version": "v3", "created": "Wed, 15 May 2013 01:16:04 GMT"}, {"version": "v4", "created": "Wed, 22 May 2013 17:36:50 GMT"}, {"version": "v5", "created": "Thu, 15 May 2014 14:43:50 GMT"}, {"version": "v6", "created": "Thu, 20 Sep 2018 19:33:55 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Kaser", "Owen", ""], ["Lemire", "Daniel", ""]]}, {"id": "1202.5358", "submitter": "Yonghui Xiao", "authors": "Yonghui Xiao, Li Xiong, Liyue Fan and Slawomir Goryczka", "title": "DPCube: Differentially Private Histogram Release through\n  Multidimensional Partitioning", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy is a strong notion for protecting individual privacy in\nprivacy preserving data analysis or publishing. In this paper, we study the\nproblem of differentially private histogram release for random workloads. We\nstudy two multidimensional partitioning strategies including: 1) a baseline\ncell-based partitioning strategy for releasing an equi-width cell histogram,\nand 2) an innovative 2-phase kd-tree based partitioning strategy for releasing\na v-optimal histogram. We formally analyze the utility of the released\nhistograms and quantify the errors for answering linear queries such as\ncounting queries. We formally characterize the property of the input data that\nwill guarantee the optimality of the algorithm. Finally, we implement and\nexperimentally evaluate several applications using the released histograms,\nincluding counting queries, classification, and blocking for record linkage and\nshow the benefit of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2012 01:29:34 GMT"}], "update_date": "2012-02-27", "authors_parsed": [["Xiao", "Yonghui", ""], ["Xiong", "Li", ""], ["Fan", "Liyue", ""], ["Goryczka", "Slawomir", ""]]}, {"id": "1202.5517", "submitter": "Richard McClatchey", "authors": "Ashiq Anjum, Peter Bloodsworth, Andrew Branson, Irfan Habib, Richard\n  McClatchey, Tony Solomonides and the neuGRID Consortium", "title": "Research Traceability using Provenance Services for Biomedical Analysis", "comments": "9 pages; 5 figures. Proceedings of the 8th HealthGrid Int. Conference\n  (HG'10). Paris, France. June 2010", "journal-ref": "Studies in Health Technology & Informatics Vol 159, pp 88-99 ISBN\n  978-1-60750-582-2 IOS Press. 2010", "doi": null, "report-no": null, "categories": "cs.DB cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We outline the approach being developed in the neuGRID project to use\nprovenance management techniques for the purposes of capturing and preserving\nthe provenance data that emerges in the specification and execution of\nworkflows in biomedical analyses. In the neuGRID project a provenance service\nhas been designed and implemented that is intended to capture, store, retrieve\nand reconstruct the workflow information needed to facilitate users in\nconducting user analyses. We describe the architecture of the neuGRID\nprovenance service and discuss how the CRISTAL system from CERN is being\nadapted to address the requirements of the project and then consider how a\ngeneralised approach for provenance management could emerge for more generic\napplication to the (Health)Grid community.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2012 18:11:46 GMT"}], "update_date": "2012-02-27", "authors_parsed": [["Anjum", "Ashiq", ""], ["Bloodsworth", "Peter", ""], ["Branson", "Andrew", ""], ["Habib", "Irfan", ""], ["McClatchey", "Richard", ""], ["Solomonides", "Tony", ""], ["Consortium", "the neuGRID", ""]]}, {"id": "1202.6677", "submitter": "Kevin Zhao", "authors": "Alin Deutsch, Richard Hull, Avinash Vyas, Kevin Keliang Zhao", "title": "Trajectory and Policy Aware Sender Anonymity in Location Based Services", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Location-based Service (LBS) settings, where a LBS provider logs\nthe requests sent by mobile device users over a period of time and later wants\nto publish/share these logs. Log sharing can be extremely valuable for\nadvertising, data mining research and network management, but it poses a\nserious threat to the privacy of LBS users. Sender anonymity solutions prevent\na malicious attacker from inferring the interests of LBS users by associating\nthem with their service requests after gaining access to the anonymized logs.\nWith the fast-increasing adoption of smartphones and the concern that historic\nuser trajectories are becoming more accessible, it becomes necessary for any\nsender anonymity solution to protect against attackers that are\ntrajectory-aware (i.e. have access to historic user trajectories) as well as\npolicy-aware (i.e they know the log anonymization policy). We call such\nattackers TP-aware.\n  This paper introduces a first privacy guarantee against TP-aware attackers,\ncalled TP-aware sender k-anonymity. It turns out that there are many possible\nTP-aware anonymizations for the same LBS log, each with a different utility to\nthe consumer of the anonymized log. The problem of finding the optimal TP-aware\nanonymization is investigated. We show that trajectory-awareness renders the\nproblem computationally harder than the trajectory-unaware variants found in\nthe literature (NP-complete in the size of the log, versus PTIME). We describe\na PTIME l-approximation algorithm for trajectories of length l and empirically\nshow that it scales to large LBS logs (up to 2 million users).\n", "versions": [{"version": "v1", "created": "Wed, 29 Feb 2012 20:37:33 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Deutsch", "Alin", ""], ["Hull", "Richard", ""], ["Vyas", "Avinash", ""], ["Zhao", "Kevin Keliang", ""]]}]