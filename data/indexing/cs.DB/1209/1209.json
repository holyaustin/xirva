[{"id": "1209.0378", "submitter": "Carlos Viegas Dam\\'asio", "authors": "C. V. Dam\\'asio, A. Analyti and G. Antoniou", "title": "Provenance for SPARQL queries", "comments": "22 pages, extended version of the ISWC 2012 paper including proofs", "journal-ref": "The Semantic Web - ISWC 2012 - 11th International Semantic Web\n  Conference, Part I, LNCS, Vol. 7649, ISBN 978-3-642-35175-4, pp. 625-640,\n  Springer Berlin Heidelberg, November 2012", "doi": "10.1007/978-3-642-35176-1_39", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining trust of data available in the Semantic Web is fundamental for\napplications and users, in particular for linked open data obtained from SPARQL\nendpoints. There exist several proposals in the literature to annotate SPARQL\nquery results with values from abstract models, adapting the seminal works on\nprovenance for annotated relational databases. We provide an approach capable\nof providing provenance information for a large and significant fragment of\nSPARQL 1.1, including for the first time the major non-monotonic constructs\nunder multiset semantics. The approach is based on the translation of SPARQL\ninto relational queries over annotated relations with values of the most\ngeneral m-semiring, and in this way also refuting a claim in the literature\nthat the OPTIONAL construct of SPARQL cannot be captured appropriately with the\nknown abstract models.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2012 15:04:55 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2013 18:57:22 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Dam\u00e1sio", "C. V.", ""], ["Analyti", "A.", ""], ["Antoniou", "G.", ""]]}, {"id": "1209.0410", "submitter": "George Teodoro", "authors": "George Teodoro, Eduardo Valle, Nathan Mariano, Ricardo Torres, Wagner\n  Meira Jr, Joel H. Saltz", "title": "Approximate Similarity Search for Online Multimedia Services on\n  Distributed CPU-GPU Platforms", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.DB cs.DC", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Similarity search in high-dimentional spaces is a pivotal operation found a\nvariety of database applications. Recently, there has been an increase interest\nin similarity search for online content-based multimedia services. Those\nservices, however, introduce new challenges with respect to the very large\nvolumes of data that have to be indexed/searched, and the need to minimize\nresponse times observed by the end-users. Additionally, those users dynamically\ninteract with the systems creating fluctuating query request rates, requiring\nthe search algorithm to adapt in order to better utilize the underline hardware\nto reduce response times. In order to address these challenges, we introduce\nhypercurves, a flexible framework for answering approximate k-nearest neighbor\n(kNN) queries for very large multimedia databases, aiming at online\ncontent-based multimedia services. Hypercurves executes on hybrid CPU--GPU\nenvironments, and is able to employ those devices cooperatively to support\nmassive query request rates. In order to keep the response times optimal as the\nrequest rates vary, it employs a novel dynamic scheduler to partition the work\nbetween CPU and GPU. Hypercurves was throughly evaluated using a large database\nof multimedia descriptors. Its cooperative CPU--GPU execution achieved\nperformance improvements of up to 30x when compared to the single CPU-core\nversion. The dynamic work partition mechanism reduces the observed query\nresponse times in about 50% when compared to the best static CPU--GPU task\npartition configuration. In addition, Hypercurves achieves superlinear\nscalability in distributed (multi-node) executions, while keeping a high\nguarantee of equivalence with its sequential version --- thanks to the proof of\nprobabilistic equivalence, which supported its aggressive parallelization\ndesign.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2012 17:12:59 GMT"}], "update_date": "2012-09-04", "authors_parsed": [["Teodoro", "George", ""], ["Valle", "Eduardo", ""], ["Mariano", "Nathan", ""], ["Torres", "Ricardo", ""], ["Meira", "Wagner", "Jr"], ["Saltz", "Joel H.", ""]]}, {"id": "1209.1011", "submitter": "David Spivak", "authors": "David I. Spivak", "title": "Kleisli Database Instances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB math.CT", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We use monads to relax the atomicity requirement for data in a database.\nDepending on the choice of monad, the database fields may contain generalized\nvalues such as lists or sets of values, or they may contain exceptions such as\nvarious types of nulls. The return operation for monads ensures that any\nordinary database instance will count as one of these generalized instances,\nand the bind operation ensures that generalized values behave well under joins\nof foreign key sequences. Different monads allow for vastly different types of\ninformation to be stored in the database. For example, we show that classical\nconcepts like Markov chains, graphs, and finite state automata are each\nperfectly captured by a different monad on the same schema.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2012 15:18:35 GMT"}], "update_date": "2012-09-06", "authors_parsed": [["Spivak", "David I.", ""]]}, {"id": "1209.1322", "submitter": "Wahbeh Qardaji", "authors": "Wahbeh Qardaji, Weining Yang, Ninghui Li", "title": "Differentially Private Grids for Geospatial Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle the problem of constructing a differentially private\nsynopsis for two-dimensional datasets such as geospatial datasets. The current\nstate-of-the-art methods work by performing recursive binary partitioning of\nthe data domains, and constructing a hierarchy of partitions. We show that the\nkey challenge in partition-based synopsis methods lies in choosing the right\npartition granularity to balance the noise error and the non-uniformity error.\nWe study the uniform-grid approach, which applies an equi-width grid of a\ncertain size over the data domain and then issues independent count queries on\nthe grid cells. This method has received no attention in the literature,\nprobably due to the fact that no good method for choosing a grid size was\nknown. Based on an analysis of the two kinds of errors, we propose a method for\nchoosing the grid size. Experimental results validate our method, and show that\nthis approach performs as well as, and often times better than, the\nstate-of-the-art methods. We further introduce a novel adaptive-grid method.\nThe adaptive grid method lays a coarse-grained grid over the dataset, and then\nfurther partitions each cell according to its noisy count. Both levels of\npartitions are then used in answering queries over the dataset. This method\nexploits the need to have finer granularity partitioning over dense regions\nand, at the same time, coarse partitioning over sparse regions. Through\nextensive experiments on real-world datasets, we show that this approach\nconsistently and significantly outperforms the uniform-grid method and other\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2012 15:47:45 GMT"}], "update_date": "2012-09-07", "authors_parsed": [["Qardaji", "Wahbeh", ""], ["Yang", "Weining", ""], ["Li", "Ninghui", ""]]}, {"id": "1209.1425", "submitter": "Reynold Xin", "authors": "Reynold S. Xin", "title": "The End of an Architectural Era for Analytical Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Traditional enterprise warehouse solutions center around an analytical\ndatabase system that is monolithic and inflexible: data needs to be extracted,\ntransformed, and loaded into the rigid relational form before analysis. It\ntakes years of sophisticated planning to provision and deploy a warehouse;\nadding new hardware resources to an existing warehouse is an equally lengthy\nand daunting task.\n  Additionally, modern data analysis employs statistical methods that go well\nbeyond the typical roll-up and drill-down capabilities provided by warehouse\nsystems. Although it is possible to implement such methods using a combination\nof SQL and UDFs, query engines in relational databases are ill-suited for\nthese.\n  The Hadoop ecosystem introduces a suite of tools for data analytics that\novercome some of the problems of traditional solutions. These systems, however,\nforgo years of warehouse research. Memory is significantly underutilized in\nHadoop clusters, and execution engine is naive compared with its relational\ncounterparts.\n  It is time to rethink the design of data warehouse systems and take the best\nfrom both worlds. The new generation of warehouse systems should be modular,\nhigh performance, fault-tolerant, easy to provision, and designed to support\nboth SQL query processing and machine learning applications.\n  This paper references the Shark system developed at Berkeley as an initial\nattempt.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2012 23:15:46 GMT"}], "update_date": "2012-09-10", "authors_parsed": [["Xin", "Reynold S.", ""]]}, {"id": "1209.1794", "submitter": "Mohamed Salah  Gouider Dr", "authors": "Saida Aissa and Mohamed Salah Gouider", "title": "A New Similairty Measure For Spatial Personalization", "comments": null, "journal-ref": "International Journal of Database Management Systems ( IJDMS )\n  Vol.4, No.4, 2012", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting the relevant information by exploiting the spatial data warehouse\nbecomes increasingly hard. In fact, because of the enormous amount of data\nstored in the spatial data warehouse, the user, usually, don't know what part\nof the cube contain the relevant information and what the forthcoming query\nshould be. As a solution, we propose to study the similarity between the\nbehaviors of the users, in term of the spatial MDX queries launched on the\nsystem, as a basis to recommend the next relevant MDX query to the current\nuser. This paper introduces a new similarity measure for comparing spatial MDX\nqueries. The proposed similarity measure could directly support the development\nof spatial personalization approaches. The proposed similarity measure takes\ninto account the basic components of the similarity assessment models: the\ntopology, the direction and the distance.\n", "versions": [{"version": "v1", "created": "Sun, 9 Sep 2012 11:54:39 GMT"}], "update_date": "2012-09-11", "authors_parsed": [["Aissa", "Saida", ""], ["Gouider", "Mohamed Salah", ""]]}, {"id": "1209.2137", "submitter": "Daniel Lemire", "authors": "Daniel Lemire and Leonid Boytsov", "title": "Decoding billions of integers per second through vectorization", "comments": "For software, see https://github.com/lemire/FastPFor, For data, see\n  http://boytsov.info/datasets/clueweb09gap/", "journal-ref": "Software: Practice & Experience 45 (1), 2015", "doi": "10.1002/spe.2203", "report-no": null, "categories": "cs.IR cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In many important applications -- such as search engines and relational\ndatabase systems -- data is stored in the form of arrays of integers. Encoding\nand, most importantly, decoding of these arrays consumes considerable CPU time.\nTherefore, substantial effort has been made to reduce costs associated with\ncompression and decompression. In particular, researchers have exploited the\nsuperscalar nature of modern processors and SIMD instructions. Nevertheless, we\nintroduce a novel vectorized scheme called SIMD-BP128 that improves over\npreviously proposed vectorized approaches. It is nearly twice as fast as the\npreviously fastest schemes on desktop processors (varint-G8IU and PFOR). At the\nsame time, SIMD-BP128 saves up to 2 bits per integer. For even better\ncompression, we propose another new vectorized scheme (SIMD-FastPFOR) that has\na compression ratio within 10% of a state-of-the-art scheme (Simple-8b) while\nbeing two times faster during decoding.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2012 20:08:03 GMT"}, {"version": "v2", "created": "Wed, 12 Sep 2012 15:01:28 GMT"}, {"version": "v3", "created": "Tue, 16 Oct 2012 14:18:36 GMT"}, {"version": "v4", "created": "Tue, 19 Feb 2013 21:16:35 GMT"}, {"version": "v5", "created": "Fri, 19 Apr 2013 02:27:52 GMT"}, {"version": "v6", "created": "Thu, 15 May 2014 15:02:22 GMT"}, {"version": "v7", "created": "Sat, 30 Jan 2021 18:23:38 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Lemire", "Daniel", ""], ["Boytsov", "Leonid", ""]]}, {"id": "1209.2178", "submitter": "Sutanay Choudhury", "authors": "Sutanay Choudhury, Lawrence B. Holder, Abhik Ray, George Chin Jr.,\n  John T. Feo", "title": "Continuous Queries for Multi-Relational Graphs", "comments": "Withdrawn because for information disclosure considerations", "journal-ref": null, "doi": null, "report-no": "PNNL-SA-90326", "categories": "cs.DB cs.SI", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Acting on time-critical events by processing ever growing social media or\nnews streams is a major technical challenge. Many of these data sources can be\nmodeled as multi-relational graphs. Continuous queries or techniques to search\nfor rare events that typically arise in monitoring applications have been\nstudied extensively for relational databases. This work is dedicated to answer\nthe question that emerges naturally: how can we efficiently execute a\ncontinuous query on a dynamic graph? This paper presents an exact subgraph\nsearch algorithm that exploits the temporal characteristics of representative\nqueries for online news or social media monitoring. The algorithm is based on a\nnovel data structure called the Subgraph Join Tree (SJ-Tree) that leverages the\nstructural and semantic characteristics of the underlying multi-relational\ngraph. The paper concludes with extensive experimentation on several real-world\ndatasets that demonstrates the validity of this approach.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2012 23:23:16 GMT"}, {"version": "v2", "created": "Sat, 9 Mar 2013 00:28:38 GMT"}], "update_date": "2013-03-12", "authors_parsed": [["Choudhury", "Sutanay", ""], ["Holder", "Lawrence B.", ""], ["Ray", "Abhik", ""], ["Chin", "George", "Jr."], ["Feo", "John T.", ""]]}, {"id": "1209.2191", "submitter": "Jimmy Lin", "authors": "Jimmy Lin", "title": "MapReduce is Good Enough? If All You Have is a Hammer, Throw Away\n  Everything That's Not a Nail!", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hadoop is currently the large-scale data analysis \"hammer\" of choice, but\nthere exist classes of algorithms that aren't \"nails\", in the sense that they\nare not particularly amenable to the MapReduce programming model. To address\nthis, researchers have proposed MapReduce extensions or alternative programming\nmodels in which these algorithms can be elegantly expressed. This essay\nespouses a very different position: that MapReduce is \"good enough\", and that\ninstead of trying to invent screwdrivers, we should simply get rid of\neverything that's not a nail. To be more specific, much discussion in the\nliterature surrounds the fact that iterative algorithms are a poor fit for\nMapReduce: the simple solution is to find alternative non-iterative algorithms\nthat solve the same problem. This essay captures my personal experiences as an\nacademic researcher as well as a software engineer in a \"real-world\" production\nanalytics environment. From this combined perspective I reflect on the current\nstate and future of \"big data\" research.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2012 01:04:38 GMT"}], "update_date": "2012-09-12", "authors_parsed": [["Lin", "Jimmy", ""]]}, {"id": "1209.2647", "submitter": "Jason  Liu", "authors": "Jason T. Liu", "title": "Shadow Theory, data model design for data integration", "comments": "85 pages, 31 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For data integration in information ecosystems, semantic heterogeneity is a\nknown difficulty. In this paper, we propose Shadow Theory as the philosophical\nfoundation to address this issue. It is based on the notion of shadows in\nPlato's Allegory of the Cave. What we can observe are just shadows, and\nmeanings of shadows are mental entities that only exist in viewers' cognitive\nstructures. With enterprise customer data integration example, we proposed six\ndesign principles and algebra to support required operations.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2012 15:48:33 GMT"}], "update_date": "2012-09-13", "authors_parsed": [["Liu", "Jason T.", ""]]}, {"id": "1209.2794", "submitter": "Hakik Paci", "authors": "Hakik Paci, Elinda Kajo Mece, Aleksander Xhuvani", "title": "Protecting oracle pl/sql source code from a dba user", "comments": null, "journal-ref": "International Journal of Database Management Systems, 4, (2012)\n  43-52", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we are presenting a new way to disable DDL statements on some\nspecific PL/SQL procedures to a dba user in the Oracle database. Nowadays dba\nusers have access to a lot of data and source code even if they do not have\nlegal permissions to see or modify them. With this method we can disable the\nability to execute DDL and DML statements on some specific pl/sql procedures\nfrom every Oracle database user even if it has a dba role. Oracle gives to\ndeveloper the possibility to wrap the pl/sql procedures, functions and packages\nbut those wrapped scripts can be unwrapped by using third party tools. The\nscripts that we have developed analyzes all database sessions, and if they\ndetect a DML or a DDL statement from an unauthorized user to procedure,\nfunction or package which should be protected then the execution of the\nstatement is denied. Furthermore, these scripts do not allow a dba user to drop\nor disable the scripts themselves. In other words by managing sessions prior to\nthe execution of an eventual statement from a dba user, we can prevent the\nexecution of eventual statements which target our scripts.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2012 07:07:24 GMT"}], "update_date": "2012-09-17", "authors_parsed": [["Paci", "Hakik", ""], ["Mece", "Elinda Kajo", ""], ["Xhuvani", "Aleksander", ""]]}, {"id": "1209.3054", "submitter": "Robert Kent", "authors": "Robert E. Kent", "title": "Database Semantics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper, the first step to connect relational databases with systems\nconsequence (Kent: \"System Consequence\" 2009), is concerned with the semantics\nof relational databases. It aims to to study system consequence in the\nlogical/semantic system of relational databases. The paper, which was inspired\nby and which extends a recent set of papers on the theory of relational\ndatabase systems (Spivak: \"Functorial Data Migration\" 2012), is linked with\nwork on the Information Flow Framework (IFF) [http://suo.ieee.org/IFF/]\nconnected with the ontology standards effort (SUO), since relational databases\nnaturally embed into first order logic. The database semantics discussed here\nis concerned with the conceptual level of database architecture. We offer both\nan intuitive and technical discussion. Corresponding to the notions of primary\nand foreign keys, relational database semantics takes two forms: a\ndistinguished form where entities are distinguished from relations, and a\nunified form where relations and entities coincide. The distinguished form\ncorresponds to the theory presented in (Spivak: \"Simplicial databases\"\n2009)[arXiv:0904.2012]. The unified form, a special case of the distinguished\nform, corresponds to the theory presented in (Spivak: \"Functorial Data\nMigration\" 2012). A later paper will discuss various formalisms of relational\ndatabases, such as relational algebra and first order logic, and will complete\nthe description of the relational database logical environment.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2012 22:30:34 GMT"}], "update_date": "2012-09-17", "authors_parsed": [["Kent", "Robert E.", ""]]}, {"id": "1209.3089", "submitter": "Lei Wu Dr.", "authors": "Mehdi Adda, Lei Wu, Sharon White, Yi Feng", "title": "Pattern Detection with Rare Item-set Mining", "comments": "17 pages, 5 figures, International Journal on Soft Computing,\n  Artificial Intelligence and Applications (IJSCAI), Vol.1, No.1, August 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discovery of new and interesting patterns in large datasets, known as\ndata mining, draws more and more interest as the quantities of available data\nare exploding. Data mining techniques may be applied to different domains and\nfields such as computer science, health sector, insurances, homeland security,\nbanking and finance, etc. In this paper we are interested by the discovery of a\nspecific category of patterns, known as rare and non-present patterns. We\npresent a novel approach towards the discovery of non-present patterns using\nrare item-set mining.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2012 04:25:56 GMT"}], "update_date": "2012-09-17", "authors_parsed": [["Adda", "Mehdi", ""], ["Wu", "Lei", ""], ["White", "Sharon", ""], ["Feng", "Yi", ""]]}, {"id": "1209.3686", "submitter": "Barzan Mozafari", "authors": "Barzan Mozafari, Purnamrita Sarkar, Michael J. Franklin, Michael I.\n  Jordan, Samuel Madden", "title": "Active Learning for Crowd-Sourced Databases", "comments": "A shorter version of this manuscript has been published in\n  Proceedings of Very Large Data Bases 2015, entitled \"Scaling Up\n  Crowd-Sourcing to Very Large Datasets: A Case for Active Learning\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd-sourcing has become a popular means of acquiring labeled data for a\nwide variety of tasks where humans are more accurate than computers, e.g.,\nlabeling images, matching objects, or analyzing sentiment. However, relying\nsolely on the crowd is often impractical even for data sets with thousands of\nitems, due to time and cost constraints of acquiring human input (which cost\npennies and minutes per label). In this paper, we propose algorithms for\nintegrating machine learning into crowd-sourced databases, with the goal of\nallowing crowd-sourcing applications to scale, i.e., to handle larger datasets\nat lower costs. The key observation is that, in many of the above tasks, humans\nand machine learning algorithms can be complementary, as humans are often more\naccurate but slow and expensive, while algorithms are usually less accurate,\nbut faster and cheaper.\n  Based on this observation, we present two new active learning algorithms to\ncombine humans and algorithms together in a crowd-sourced database. Our\nalgorithms are based on the theory of non-parametric bootstrap, which makes our\nresults applicable to a broad class of machine learning models. Our results, on\nthree real-life datasets collected with Amazon's Mechanical Turk, and on 15\nwell-known UCI data sets, show that our methods on average ask humans to label\none to two orders of magnitude fewer items to achieve the same accuracy as a\nbaseline that labels random images, and two to eight times fewer questions than\nprevious active learning schemes.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2012 15:21:06 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2012 15:45:55 GMT"}, {"version": "v3", "created": "Thu, 13 Dec 2012 18:20:04 GMT"}, {"version": "v4", "created": "Sat, 20 Dec 2014 08:56:15 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Mozafari", "Barzan", ""], ["Sarkar", "Purnamrita", ""], ["Franklin", "Michael J.", ""], ["Jordan", "Michael I.", ""], ["Madden", "Samuel", ""]]}, {"id": "1209.3756", "submitter": "Charalampos Nikolaou", "authors": "Charalampos Nikolaou and Manolis Koubarakis", "title": "Incomplete Information in RDF", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend RDF with the ability to represent property values that exist, but\nare unknown or partially known, using constraints. Following ideas from the\nincomplete information literature, we develop a semantics for this extension of\nRDF, called RDFi, and study SPARQL query evaluation in this framework.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2012 19:10:38 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2012 11:00:44 GMT"}, {"version": "v3", "created": "Tue, 18 Dec 2012 16:36:22 GMT"}], "update_date": "2012-12-19", "authors_parsed": [["Nikolaou", "Charalampos", ""], ["Koubarakis", "Manolis", ""]]}, {"id": "1209.3913", "submitter": "M\\'arton Trencs\\'eni", "authors": "M\\'arton Trencs\\'eni, Attila Gazs\\'o", "title": "Keyspace: A Consistently Replicated, Highly-Available Key-Value Store", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the design and architecture of Keyspace, a distributed\nkey-value store offering strong consistency, fault-tolerance and high\navailability. The source code is available under the open-source AGPL license\nfor Linux, Windows and BSD-like platforms. As of 2012, Keyspace is no longer\nundergoing active development.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2012 11:35:57 GMT"}], "update_date": "2012-09-19", "authors_parsed": [["Trencs\u00e9ni", "M\u00e1rton", ""], ["Gazs\u00f3", "Attila", ""]]}, {"id": "1209.3943", "submitter": "Eya Ben Ahmed", "authors": "Wafa Tebourski Ourida Ben Boubaker Saidi", "title": "Formal Concept Analysis Based Association Rules Extraction", "comments": "8", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 8,\n  Issue 4, No. 2, July 2011", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating a huge number of association rules reduces their utility in the\ndecision making process, done by domain experts. In this context, based on the\ntheory of Formal Concept Analysis, we propose to extend the notion of Formal\nConcept through the generalization of the notion of itemset in order to\nconsider the itemset as an intent, its support as the cardinality of the extent\nand its relevance which is related to the confidence of rule. Accordingly, we\npropose a new approach to extract interesting itemsets through the concept\ncoverage. This approach uses a new quality-criteria of a rule: the relevance\nbringing a semantic added value to formal concept analysis approach to discover\nassociation rules.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2012 13:04:01 GMT"}], "update_date": "2012-09-19", "authors_parsed": [["Saidi", "Wafa Tebourski Ourida Ben Boubaker", ""]]}, {"id": "1209.3944", "submitter": "Eya Ben Ahmed", "authors": "Wafa Tebourski Wahiba Ben Abdessalem Karaa", "title": "Cyclic Association Rules Mining under Constraints", "comments": "8", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several researchers have explored the temporal aspect of association rules\nmining. In this paper, we focus on the cyclic association rules, in order to\ndiscover correlations among items characterized by regular cyclic variation\novertime. The overview of the state of the art has revealed the drawbacks of\nproposed algorithm literatures, namely the excessive number of generated rules\nwhich are not meeting the expert's expectations. To overcome these\nrestrictions, we have introduced our approach dedicated to generate the cyclic\nassociation rules under constraints through a new method called\nConstraint-Based Cyclic Association Rules CBCAR. The carried out experiments\nunderline the usefulness and the performance of our new approach.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2012 13:09:24 GMT"}], "update_date": "2012-09-19", "authors_parsed": [["Karaa", "Wafa Tebourski Wahiba Ben Abdessalem", ""]]}, {"id": "1209.4169", "submitter": "Doreswamy", "authors": "Doreswamy, Hemanth K. S", "title": "Hybrid Data Mining Technique for Knowledge Discovery from Engineering\n  Materials' Data sets", "comments": "12 pages, 8 figures; International Journal of Database Management\n  Systems (IJDMS), Vol.3, No.1, February 2011. arXiv admin note: text overlap\n  with arXiv:1206.3078 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studying materials informatics from a data mining perspective can be\nbeneficial for manufacturing and other industrial engineering applications.\nPredictive data mining technique and machine learning algorithm are combined to\ndesign a knowledge discovery system for the selection of engineering materials\nthat meet the design specifications. Predictive method-Naive Bayesian\nclassifier and Machine learning Algorithm - Pearson correlation coefficient\nmethod were implemented respectively for materials classification and\nselection. The knowledge extracted from the engineering materials data sets is\nproposed for effective decision making in advanced engineering materials design\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2012 07:23:02 GMT"}], "update_date": "2012-09-20", "authors_parsed": [["Doreswamy", "", ""], ["S", "Hemanth K.", ""]]}, {"id": "1209.4187", "submitter": "M\\'arton Trencs\\'eni", "authors": "M\\'arton Trencs\\'eni, Attila Gazs\\'o, Holger Reinhardt", "title": "PaxosLease: Diskless Paxos for Leases", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes PaxosLease, a distributed algorithm for lease\nnegotiation. PaxosLease is based on Paxos, but does not require disk writes or\nclock synchrony. PaxosLease is used for master lease negotation in the\nopen-source Keyspace and ScalienDB replicated key-value stores.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2012 08:58:52 GMT"}], "update_date": "2012-09-20", "authors_parsed": [["Trencs\u00e9ni", "M\u00e1rton", ""], ["Gazs\u00f3", "Attila", ""], ["Reinhardt", "Holger", ""]]}, {"id": "1209.4257", "submitter": "Dang Hoan Tran", "authors": "Dang-Hoan Tran", "title": "Communication-Efficient and Exact Clustering Distributed Streaming Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A widely used approach to clustering a single data stream is the two-phased\napproach in which the online phase creates and maintains micro-clusters while\nthe off-line phase generates the macro-clustering from the micro-clusters. We\nuse this approach to propose a distributed framework for clustering streaming\ndata. Our proposed framework consists of fundamen- tal processes: one\ncoordinator-site process and many remote-site processes. Remote-site processes\ncan directly communicate with the coordinator-process but cannot communicate\nthe other remote site processes. Every remote-site process generates and\nmaintains micro- clusters that represent cluster information summary, from its\nlocal data stream. Remote sites send the local micro-clusterings to the\ncoordinator by the serialization technique, or the coordinator invokes the\nremote methods in order to get the local micro-clusterings from the remote\nsites. After the coordinator receives all the local micro-clusterings from the\nremote sites, it generates the global clustering by the macro-clustering\nmethod. Our theoretical and empirical results show that, the global clustering\ngenerated by our distributed framework is similar to the clustering generated\nby the underlying centralized algorithm on the same data set. By using the\nlocal micro-clustering approach, our framework achieves high scalability, and\ncommunication-efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2012 14:31:09 GMT"}], "update_date": "2012-09-20", "authors_parsed": [["Tran", "Dang-Hoan", ""]]}, {"id": "1209.5244", "submitter": "Srikantaiah K C", "authors": "K. C. Srikantaiah, P. L. Srikanth, V. Tejaswi, K. Shaila, K. R.\n  Venugopal and L. M. Patnaik", "title": "Ranking Search Engine Result Pages based on Trustworthiness of Websites", "comments": "10 pages; IJCSI International Journal of Computer Science Issues,\n  Vol. 9, Issue 4, No 2, July 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The World Wide Web (WWW) is the repository of large number of web pages which\ncan be accessed via Internet by multiple users at the same time and therefore\nit is Ubiquitous in nature. The search engine is a key application used to\nsearch the web pages from this huge repository, which uses the link analysis\nfor ranking the web pages without considering the facts provided by them. A new\nalgorithm called Probability of Correctness of Facts(PCF)-Engine is proposed to\nfind the accuracy of the facts provided by the web pages. It uses the\nProbability based similarity function (SIM) which performs the string matching\nbetween the true facts and the facts of web pages to find their probability of\ncorrectness. The existing semantic search engines, may give the relevant result\nto the user query but may not be 100% accurate. Our algorithm computes\ntrustworthiness of websites to rank the web pages. Simulation results show that\nour approach is efficient when compared with existing Voting and Truthfinder[1]\nalgorithms with respect to the trustworthiness of the websites.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2012 12:24:49 GMT"}], "update_date": "2012-09-25", "authors_parsed": [["Srikantaiah", "K. C.", ""], ["Srikanth", "P. L.", ""], ["Tejaswi", "V.", ""], ["Shaila", "K.", ""], ["Venugopal", "K. R.", ""], ["Patnaik", "L. M.", ""]]}, {"id": "1209.5426", "submitter": "Tanvir Ahmed", "authors": "Tanvir Ahmed, Mohammad Saiedur Rahaman, Mohammad Saidur Rahman, Manzur\n  H. Khan", "title": "A Coherent Distributed Grid Service for Assimilation and Unification of\n  Heterogeneous Data Source", "comments": "9 pages; ISSN 1608-3679", "journal-ref": "AIUB Journal of Science And Engineering (AJSE) Vol. 9, No. 1, PP\n  47-55, 2010", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grid services are heavily used for handling large distributed computations.\nThey are also very useful to handle heavy data intensive applications where\ndata are distributed in different sites. Most of the data grid services used in\nsuch situations are meant for homogeneous data source. In case of Heterogeneous\ndata sources, most of the grid services that are available are designed such a\nway that they must be identical in schema definition for their smooth\noperation. But there can be situations where the grid site databases are\nheterogeneous and their schema definition is different from the central schema\ndefinition. In this paper we propose a light weight coherent grid service for\nheterogeneous data sources that is very easily install. It can map and convert\nthe central SQL schema into that of the grid members and send queries to get\naccording results from heterogeneous data sources.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2012 21:17:09 GMT"}, {"version": "v2", "created": "Fri, 12 Oct 2012 22:30:21 GMT"}], "update_date": "2012-10-16", "authors_parsed": [["Ahmed", "Tanvir", ""], ["Rahaman", "Mohammad Saiedur", ""], ["Rahman", "Mohammad Saidur", ""], ["Khan", "Manzur H.", ""]]}, {"id": "1209.5430", "submitter": "Spyros Sioutas SS", "authors": "Spyros Sioutas, Alexandros Panaretos, Ioannis Karydis, Dimitrios\n  Tsoumakos, Giannis Tzimas and Dimitrios Tsolis", "title": "SART: Speeding up Query Processing in Sensor Networks with an Autonomous\n  Range Tree Structure", "comments": "11 pages, 23 figures, 5 algorithms or operations", "journal-ref": "ACM Applied Computing Review (ACR), Vol. 12, No.3, 2012, pp.60-74", "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of constructing efficient P2P overlays for sensornets\nproviding \"Energy-Level Application and Services\". The method presented in\n\\cite{SOPXM09} presents a novel P2P overlay for Energy Level discovery in a\nsensornet. However, this solution is not dynamic, since requires periodical\nrestructuring. In particular, it is not able to support neither join of\nsensor\\_nodes with energy level out of the ranges supported by the existing p2p\noverlay nor leave of \\emph{empty} overlay\\_peers to which no sensor\\_nodes are\ncurrently associated. On this purpose and based on the efficient P2P method\npresented in \\cite{SPSTMT10}, we design a dynamic P2P overlay for Energy Level\ndiscovery in a sensornet, the so-called SART (Sensors' Autonomous Range Tree).\nThe adaptation of the P2P index presented in \\cite{SPSTMT10} guarantees the\nbest-known dynamic query performance of the above operation. We experimentally\nverify this performance, via the D-P2P-Sim simulator (D-P2P-Sim is publicly\navailable at http://code.google.com/p/d-p2p-sim/).\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2012 21:24:36 GMT"}], "update_date": "2012-09-26", "authors_parsed": [["Sioutas", "Spyros", ""], ["Panaretos", "Alexandros", ""], ["Karydis", "Ioannis", ""], ["Tsoumakos", "Dimitrios", ""], ["Tzimas", "Giannis", ""], ["Tsolis", "Dimitrios", ""]]}, {"id": "1209.5598", "submitter": "Fan Min", "authors": "Fan Min", "title": "Granular association rules on two universes with four measures", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relational association rules reveal patterns hide in multiple tables.\nExisting rules are usually evaluated through two measures, namely support and\nconfidence. However, these two measures may not be enough to describe the\nstrength of a rule. In this paper, we introduce granular association rules with\nfour measures to reveal connections between granules in two universes, and\npropose three algorithms for rule mining. An example of such a rule might be\n\"40% men like at least 30% kinds of alcohol; 45% customers are men and 6%\nproducts are alcohol.\" Here 45%, 6%, 40%, and 30% are the source coverage, the\ntarget coverage, the source confidence, and the target confidence,\nrespectively. With these measures, our rules are semantically richer than\nexisting ones. Three subtypes of rules are obtained through considering special\nrequirements on the source/target confidence. Then we define a rule mining\nproblem, and design a sandwich algorithm with different rule checking\napproaches for different subtypes. Experiments on a real world dataset show\nthat the approaches dedicated to three subtypes are 2-3 orders of magnitudes\nfaster than the one for the general case. A forward algorithm and a backward\nalgorithm for one particular subtype can speed up the mining process further.\nThis work opens a new research trend concerning relational association rule\nmining, granular computing and rough sets.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2012 13:13:11 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2013 02:24:12 GMT"}, {"version": "v3", "created": "Thu, 28 Jan 2016 02:23:32 GMT"}], "update_date": "2016-01-29", "authors_parsed": [["Min", "Fan", ""]]}, {"id": "1209.5625", "submitter": "Robert Smith", "authors": "Robert Smith", "title": "Managing Complex Structured Data In a Fast Evolving Environment", "comments": "Accepted for the International Lisp Conference 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Criminal data comes in a variety of formats, mandated by state, federal, and\ninternational standards. Specifying the data in a unified fashion is necessary\nfor any system that intends to integrate with state, federal, and international\nlaw enforcement agencies. However, the contents, format, and structure of the\ndata is highly inconsistent across jurisdictions, and each datum requires\ndifferent ways of being printed, transmitted, and displayed. The goal was to\ndesign a system that is unified in its approach to specify data, and is\namenable to future \"unknown unknowns\". We have developed a domain-specific\nlanguage in Common Lisp which allows the specification of complex data with\nevolving formats and structure, and is inter-operable with the Common Lisp\nlanguage. The resultant system has enabled the easy handling of complex\nevolving information in the general criminal data environment and has made it\npossible to manage and extend the system in a high-paced market. The language\nhas allowed the principal product of Secure Outcomes Inc. to enjoy success with\nover 50 users throughout the United States.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2012 14:37:48 GMT"}], "update_date": "2012-09-26", "authors_parsed": [["Smith", "Robert", ""]]}, {"id": "1209.5922", "submitter": "Satrajit Ghosh", "authors": "D. B. Keator, K. Helmer, J. Steffener, J. A. Turner, T. G. M. Van Erp,\n  S. Gadde, N. Ashish, G. A. Burns, B. N. Nichols, S. S. Ghosh", "title": "Towards structured sharing of raw and derived neuroimaging data across\n  existing resources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data sharing efforts increasingly contribute to the acceleration of\nscientific discovery. Neuroimaging data is accumulating in distributed\ndomain-specific databases and there is currently no integrated access mechanism\nnor an accepted format for the critically important meta-data that is necessary\nfor making use of the combined, available neuroimaging data. In this\nmanuscript, we present work from the Derived Data Working Group, an open-access\ngroup sponsored by the Biomedical Informatics Research Network (BIRN) and the\nInternational Neuroimaging Coordinating Facility (INCF) focused on practical\ntools for distributed access to neuroimaging data. The working group develops\nmodels and tools facilitating the structured interchange of neuroimaging\nmeta-data and is making progress towards a unified set of tools for such data\nand meta-data exchange. We report on the key components required for integrated\naccess to raw and derived neuroimaging data as well as associated meta-data and\nprovenance across neuroimaging resources. The components include (1) a\nstructured terminology that provides semantic context to data, (2) a formal\ndata model for neuroimaging with robust tracking of data provenance, (3) a web\nservice-based application programming interface (API) that provides a\nconsistent mechanism to access and query the data model, and (4) a provenance\nlibrary that can be used for the extraction of provenance data by image\nanalysts and imaging software developers. We believe that the framework and set\nof tools outlined in this manuscript have great potential for solving many of\nthe issues the neuroimaging community faces when sharing raw and derived\nneuroimaging data across the various existing database systems for the purpose\nof accelerating scientific discovery.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2012 13:12:49 GMT"}, {"version": "v2", "created": "Wed, 2 Jan 2013 01:15:36 GMT"}, {"version": "v3", "created": "Thu, 7 Mar 2013 01:56:51 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Keator", "D. B.", ""], ["Helmer", "K.", ""], ["Steffener", "J.", ""], ["Turner", "J. A.", ""], ["Van Erp", "T. G. M.", ""], ["Gadde", "S.", ""], ["Ashish", "N.", ""], ["Burns", "G. A.", ""], ["Nichols", "B. N.", ""], ["Ghosh", "S. S.", ""]]}, {"id": "1209.6070", "submitter": "Tanvir Ahmed", "authors": "Khalid Ibnal Asad, Tanvir Ahmed, Md. Saiedur Rahman", "title": "Movie Popularity Classification based on Inherent Movie Attributes using\n  C4.5,PART and Correlation Coefficient", "comments": "6 pages", "journal-ref": "IEEE/OSA/IAPR International Conference on Informatics, Electronics\n  & Vision (ICIEV2012), pp. 747-752, May 2012", "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abundance of movie data across the internet makes it an obvious candidate for\nmachine learning and knowledge discovery. But most researches are directed\ntowards bi-polar classification of movie or generation of a movie\nrecommendation system based on reviews given by viewers on various internet\nsites. Classification of movie popularity based solely on attributes of a movie\ni.e. actor, actress, director rating, language, country and budget etc. has\nbeen less highlighted due to large number of attributes that are associated\nwith each movie and their differences in dimensions. In this paper, we propose\nclassification scheme of pre-release movie popularity based on inherent\nattributes using C4.5 and PART classifier algorithm and define the relation\nbetween attributes of post release movies using correlation coefficient.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2012 20:30:02 GMT"}], "update_date": "2012-09-28", "authors_parsed": [["Asad", "Khalid Ibnal", ""], ["Ahmed", "Tanvir", ""], ["Rahman", "Md. Saiedur", ""]]}, {"id": "1209.6297", "submitter": "Pratima Gautam", "authors": "Pratima Gautam, Rahul Shukla", "title": "An Efficient Algorithm for Mining Multilevel Association Rule Based on\n  Pincer Search", "comments": null, "journal-ref": "ijcsi international journal vol-9,issue-4,2012", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering frequent itemset is a key difficulty in significant data mining\napplications, such as the discovery of association rules, strong rules,\nepisodes, and minimal keys. The problem of developing models and algorithms for\nmultilevel association mining poses for new challenges for mathematics and\ncomputer science. In this paper, we present a model of mining multilevel\nassociation rules which satisfies the different minimum support at each level,\nwe have employed princer search concepts, multilevel taxonomy and different\nminimum supports to find multilevel association rules in a given transaction\ndata set. This search is used only for maintaining and updating a new data\nstructure. It is used to prune early candidates that would normally encounter\nin the top-down search. A main characteristic of the algorithms is that it does\nnot require explicit examination of every frequent itemsets, an example is also\ngiven to demonstrate and support that the proposed mining algorithm can derive\nthe multiple-level association rules under different supports in a simple and\neffective manner\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2012 17:29:58 GMT"}], "update_date": "2012-09-28", "authors_parsed": [["Gautam", "Pratima", ""], ["Shukla", "Rahul", ""]]}, {"id": "1209.6396", "submitter": "Jeff M Phillips", "authors": "Jeff M. Phillips", "title": "Chernoff-Hoeffding Inequality and Applications", "comments": "Expository document hopefully at the level of an advanced undergrad\n  or beginning graduate student. The update corrects a missing bound on a\n  parameter in one form of the main theorem", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When dealing with modern big data sets, a very common theme is reducing the\nset through a random process. These generally work by making \"many simple\nestimates\" of the full data set, and then judging them as a whole. Perhaps\nmagically, these \"many simple estimates\" can provide a very accurate and small\nrepresentation of the large data set. The key tool in showing how many of these\nsimple estimates are needed for a fixed accuracy trade-off is the\nChernoff-Hoeffding inequality[Che52,Hoe63]. This document provides a simple\nform of this bound, and two examples of its use.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2012 23:41:52 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2013 05:25:19 GMT"}], "update_date": "2013-02-20", "authors_parsed": [["Phillips", "Jeff M.", ""]]}, {"id": "1209.6490", "submitter": "M\\'arton Trencs\\'eni", "authors": "Istv\\'an Csabai, M\\'arton Trencs\\'eni, G\\'eza Herczegh, L\\'aszl\\'o\n  Dobos, P\\'eter J\\'ozsa, Norbert Purger, Tam\\'as Budav\\'ari, Alexander Szalay", "title": "Spatial Indexing of Large Multidimensional Databases", "comments": "12 pages, 16 figures; CIDR 2007", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific endeavors such as large astronomical surveys generate databases on\nthe terabyte scale. These, usually multidimensional databases must be\nvisualized and mined in order to find interesting objects or to extract\nmeaningful and qualitatively new relationships. Many statistical algorithms\nrequired for these tasks run reasonably fast when operating on small sets of\nin-memory data, but take noticeable performance hits when operating on large\ndatabases that do not fit into memory. We utilize new software technologies to\ndevelop and evaluate fast multidimensional indexing schemes that inherently\nfollow the underlying, highly non-uniform distribution of the data: they are\nlayered uniform grid indices, hierarchical binary space partitioning, and\nsampled flat Voronoi tessellation of the data. Our working database is the\n5-dimensional magnitude space of the Sloan Digital Sky Survey with more than\n270 million data points, where we show that these techniques can dramatically\nspeed up data mining operations such as finding similar objects by example,\nclassifying objects or comparing extensive simulation sets with observations.\nWe are also developing tools to interact with the multidimensional database and\nvisualize the data at multiple resolutions in an adaptive manner.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2012 11:47:00 GMT"}], "update_date": "2012-10-01", "authors_parsed": [["Csabai", "Istv\u00e1n", ""], ["Trencs\u00e9ni", "M\u00e1rton", ""], ["Herczegh", "G\u00e9za", ""], ["Dobos", "L\u00e1szl\u00f3", ""], ["J\u00f3zsa", "P\u00e9ter", ""], ["Purger", "Norbert", ""], ["Budav\u00e1ri", "Tam\u00e1s", ""], ["Szalay", "Alexander", ""]]}, {"id": "1209.6580", "submitter": "Jo\\~ao Eugenio Marynowski", "authors": "Jo\\~ao Eugenio Marynowski, Michel Albonico, Eduardo Cunha de Almeida,\n  Gerson Suny\\'e", "title": "Testing MapReduce-Based Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MapReduce (MR) is the most popular solution to build applications for\nlarge-scale data processing. These applications are often deployed on large\nclusters of commodity machines, where failures happen constantly due to bugs,\nhardware problems, and outages. Testing MR-based systems is hard, since it is\nneeded a great effort of test harness to execute distributed test cases upon\nfailures. In this paper, we present a novel testing solution to tackle this\nissue called HadoopTest. This solution is based on a scalable harness approach,\nwhere distributed tester components are hung around each map and reduce worker\n(i.e., node). Testers are allowed to stimulate each worker to inject failures\non them, monitor their behavior, and validate testing results. HadoopTest was\nused to test two applications bundled into Hadoop, the Apache open source\nMapReduce implementation. Our initial implementation demonstrates promising\nresults, with HadoopTest coordinating test cases across distributed MapReduce\nworkers, and finding bugs.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2012 17:47:31 GMT"}, {"version": "v2", "created": "Fri, 8 Feb 2013 02:35:01 GMT"}], "update_date": "2013-02-11", "authors_parsed": [["Marynowski", "Jo\u00e3o Eugenio", ""], ["Albonico", "Michel", ""], ["de Almeida", "Eduardo Cunha", ""], ["Suny\u00e9", "Gerson", ""]]}]