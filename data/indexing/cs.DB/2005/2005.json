[{"id": "2005.00044", "submitter": "David Lomet", "authors": "David Lomet (Microsoft Research, Redmond, WA) and Chen Luo (UC Irvine,\n  Irvine, CA)", "title": "Efficiently Reclaiming Space in a Log Structured Store", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A log structured store uses a single write I/O for a number of diverse and\nnon-contiguous pages within a large buffer instead of using a write I/O for\neach page separately. This requires that pages be relocated on every write,\nbecause pages are never updated in place. Instead, pages are dynamically\nremapped on every write. Log structuring was invented for and used initially in\nfile systems. Today, a form of log structuring is used in SSD controllers\nbecause an SSD requires the erasure of a large block of pages before flash\nstorage can be reused. No update-in-place requires that the storage for\nout-of-date pages be reclaimed (garbage collected or \"cleaned\"). We analyze\ncleaning performance and introduce a cleaning strategy that uses a new way to\nprioritize the order in which stale pages are garbage collected. Our cleaning\nstrategy approximates an \"optimal cleaning strategy\". Simulation studies\nconfirm the results of the analysis. This strategy is a significant improvement\nover previous cleaning strategies.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 18:29:42 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Lomet", "David", "", "Microsoft Research, Redmond, WA"], ["Luo", "Chen", "", "UC Irvine,\n  Irvine, CA"]]}, {"id": "2005.00186", "submitter": "Yang Cao", "authors": "Yang Cao, Shun Takagi, Yonghui Xiao, Li Xiong, Masatoshi Yoshikawa", "title": "PANDA: Policy-aware Location Privacy for Epidemic Surveillance", "comments": "Accepted in the 46th International Conference on Very Large Data\n  Bases (VLDB 2020) demonstration track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this demonstration, we present a privacy-preserving epidemic surveillance\nsystem. Recently, many countries that suffer from coronavirus crises attempt to\naccess citizen's location data to eliminate the outbreak. However, it raises\nprivacy concerns and may open the doors to more invasive forms of surveillance\nin the name of public health. It also brings a challenge for privacy protection\ntechniques: how can we leverage people's mobile data to help combat the\npandemic without scarifying our location privacy. We demonstrate that we can\nhave the best of the two worlds by implementing policy-based location privacy\nfor epidemic surveillance. Specifically, we formalize the privacy policy using\ngraphs in light of differential privacy, called policy graph. Our system has\nthree primary functions for epidemic surveillance: location monitoring,\nepidemic analysis, and contact tracing. We provide an interactive tool allowing\nthe attendees to explore and examine the usability of our system: (1) the\nutility of location monitor and disease transmission model estimation, (2) the\nprocedure of contact tracing in our systems, and (3) the privacy-utility\ntrade-offs w.r.t. different policy graphs. The attendees can find that it is\npossible to have the full functionality of epidemic surveillance while\npreserving location privacy.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 02:47:18 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2020 06:12:34 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Cao", "Yang", ""], ["Takagi", "Shun", ""], ["Xiao", "Yonghui", ""], ["Xiong", "Li", ""], ["Yoshikawa", "Masatoshi", ""]]}, {"id": "2005.00448", "submitter": "Nikolaos Tziavelis", "authors": "Nikolaos Tziavelis, Wolfgang Gatterbauer, Mirek Riedewald", "title": "Optimal Join Algorithms Meet Top-k", "comments": "To be published in Proceedings ofthe 2020 ACM SIGMOD International\n  Conference on Management of Data (SIGMOD'20), June 14-19, 2020, Portland, OR,\n  USA, 7 pages", "journal-ref": null, "doi": "10.1145/3318464.3383132", "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Top-k queries have been studied intensively in the database community and\nthey are an important means to reduce query cost when only the \"best\" or \"most\ninteresting\" results are needed instead of the full output. While some\noptimality results exist, e.g., the famous Threshold Algorithm, they hold only\nin a fairly limited model of computation that does not account for the cost\nincurred by large intermediate results and hence is not aligned with typical\ndatabase-optimizer cost models. On the other hand, the idea of avoiding large\nintermediate results is arguably the main goal of recent work on optimal join\nalgorithms, which uses the standard RAM model of computation to determine\nalgorithm complexity. This research has created a lot of excitement due to its\npromise of reducing the time complexity of join queries with cycles, but it has\nmostly focused on full-output computation. We argue that the two areas can and\nshould be studied from a unified point of view in order to achieve optimality\nin the common model of computation for a very general class of top-k-style join\nqueries. This tutorial has two main objectives. First, we will explore and\ncontrast the main assumptions, concepts, and algorithmic achievements of the\ntwo research areas. Second, we will cover recent, as well as some older,\napproaches that emerged at the intersection to support efficient ranked\nenumeration of join-query results. These are related to classic work on\nk-shortest path algorithms and more general optimization problems, some of\nwhich dates back to the 1950s. We demonstrate that this line of research\nwarrants renewed attention in the challenging context of ranked enumeration for\ngeneral join queries.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 15:33:23 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Tziavelis", "Nikolaos", ""], ["Gatterbauer", "Wolfgang", ""], ["Riedewald", "Mirek", ""]]}, {"id": "2005.00658", "submitter": "Gokhan Sagirlar Dr", "authors": "Gokhan Sagirlar and John D. Sheehan and Emanuele Ragnoli", "title": "On the Design of Co-operating Blockchains for IoT", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enabling blockchain technology into IoT can help to achieve a proper\ndistributed consensus based IoT system that overcomes disadvantages of today's\ncentralized infrastructures, such as, among others, high cloud server\nmaintenance costs, weakness for supporting time-critical IoT applications,\nsecurity and trust issues. However, meeting requirements posed by IoT in\nblockchain domain is not an easy endeavour. [1] proposes Hybrid-IoT, as a step\ntowards decentralizing IoT with the help of blockchain technology. Hybrid-IoT\nconsists of multiple PoW sub-blockchains to achieve distributed consensus among\nIoT devices and an inter-connector framework, to execute transactions between\nsub-blockchains. In this paper, we take the first step towards designing an\ninter-connector for multiple blockchains for IoT that is specifically tailored\nfor the Hybrid-IoT architecture. We also provide a detailed security\ndiscussion, in order to identify threats and we provide discussion on how to\ncope with threats.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 23:56:09 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Sagirlar", "Gokhan", ""], ["Sheehan", "John D.", ""], ["Ragnoli", "Emanuele", ""]]}, {"id": "2005.00993", "submitter": "Xiuwen Zheng", "authors": "Xiuwen Zheng and Amarnath Gupta", "title": "An Algebraic Approach for High-level Text Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text analytical tasks like word embedding, phrase mining, and topic modeling,\nare placing increasing demands as well as challenges to existing database\nmanagement systems.\n  In this paper, we provide a novel algebraic approach based on associative\narrays. Our data model and algebra can bring together relational operators and\ntext operators, which enables interesting optimization opportunities for hybrid\ndata sources that have both relational and textual data. We demonstrate its\nexpressive power in text analytics using several real-world tasks.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 05:41:36 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Zheng", "Xiuwen", ""], ["Gupta", "Amarnath", ""]]}, {"id": "2005.01038", "submitter": "Mohammad Javad Amiri", "authors": "Mohammad Javad Amiri, Joris Dugu\\'ep\\'eroux, Tristan Allard, Divyakant\n  Agrawal, Amr El Abbadi", "title": "SEPAR: Towards Regulating Future of Work Multi-Platform Crowdworking\n  Environments with Privacy Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdworking platforms provide the opportunity for diverse workers to execute\ntasks for different requesters. The popularity of the \"gig\" economy has given\nrise to independent platforms that provide competing and complementary\nservices. Workers as well as requesters with specific tasks may need to work\nfor or avail from the services of multiple platforms resulting in the rise of\nmulti-platform crowdworking systems. Recently, there has been increasing\ninterest by governmental, legal and social institutions to enforce regulations,\nsuch as minimal and maximal work hours, on crowdworking platforms. Platforms\nwithin multi-platform crowdworking systems, therefore, need to collaborate to\nenforce cross-platform regulations. While collaborating to enforce global\nregulations requires the transparent sharing of information about tasks and\ntheir participants, the privacy of all participants needs to be preserved. In\nthis paper, we propose an overall vision exploring the regulation, privacy, and\narchitecture dimensions for the future of work multi-platform crowdworking\nenvironments. We then present SEPAR, a multi-platform crowdworking system that\nenforces a large sub-space of practical global regulations on a set of\ndistributed independent platforms in a privacy-preserving manner. SEPAR,\nenforces privacy using lightweight and anonymous tokens, while transparency is\nachieved using fault-tolerant blockchains shared across multiple platforms. The\nprivacy guarantees of SEPAR against covert adversaries are formalized and\nthoroughly demonstrated, while the experiments reveal the efficiency of SEPAR\nin terms of performance and scalability.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 10:23:32 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 21:26:33 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Amiri", "Mohammad Javad", ""], ["Dugu\u00e9p\u00e9roux", "Joris", ""], ["Allard", "Tristan", ""], ["Agrawal", "Divyakant", ""], ["Abbadi", "Amr El", ""]]}, {"id": "2005.01389", "submitter": "Elwin Huaman", "authors": "Elwin Huaman, Elias K\\\"arle, Dieter Fensel", "title": "Knowledge Graph Validation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graphs (KGs) have shown to be an important asset of large companies\nlike Google and Microsoft. KGs play an important role in providing structured\nand semantically rich information, making them available to people and\nmachines, and supplying accurate, correct and reliable knowledge. To do so a\ncritical task is knowledge validation, which measures whether statements from\nKGs are semantically correct and correspond to the so-called \"real\" world. In\nthis paper, we provide an overview and review of the state-of-the-art\napproaches, methods and tools on knowledge validation for KGs, as well as an\nevaluation of them. As a result, we demonstrate a lack of reproducibility of\ntools results, give insights, and state our future research direction.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 11:09:11 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Huaman", "Elwin", ""], ["K\u00e4rle", "Elias", ""], ["Fensel", "Dieter", ""]]}, {"id": "2005.01393", "submitter": "Salma Mouline", "authors": "Souad Amghar, Safae Cherdal, Salma Mouline", "title": "Storing, preprocessing and analyzing Tweets: Finding the suitable NoSQL\n  system", "comments": "Preprint, 18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NoSQL systems are a new generation of databases that aim to handle a large\nvolume of data. However there is a large set of NoSQL systems, each has its own\ncharacteristics. Consequently choosing the suitable NoSQL system to handle\nTweets is challenging. Based on these motivations, this work is carried out to\nfind the suitable NoSQL system to store, preprocess and analyze Tweets.\n  This paper presents the requirements of managing Tweets and provides a\ndetailed comparison of five of the most popular NoSQL systems namely, Redis,\nCassandra, MongoDB, Couchbase and Neo4j regarding to these requirements. The\nresults of this work show that for Tweets storing, preprocessing and analyzing,\nMongoDB and Couchbase are the most suitable NoSQL systems. Unlike related\nworks, this work compares five NoSQL systems from different types in a real\nscenario which is Tweet storing, preprocessing and analyzing. The chosen\nscenario enables to evaluate not only the performance of read and write\noperations, but also other requirements related to Tweets management such as\nscalability, analysis tools support and analysis languages support.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 11:16:48 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Amghar", "Souad", ""], ["Cherdal", "Safae", ""], ["Mouline", "Salma", ""]]}, {"id": "2005.01511", "submitter": "Lekshmi Beena Gopalakrishnan Nair", "authors": "Lekshmi B.G. and Andreas Becher and Klaus Meyer-Wegener", "title": "The ReProVide Query-Sequence Optimization in a Hardware-Accelerated DBMS", "comments": "arXiv admin note: substantial text overlap with arXiv:2001.10719", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hardware acceleration of database query processing can be done with the help\nof FPGAs. In particular, they are partially reconfigurable at runtime, which\nallows for the runtime adaption of the hardware to a variety of queries.\nReconfiguration itself, however, takes some time. As the affected area of the\nFPGA is not available for computations during the reconfiguration, avoiding\nsome of the reconfigurations can improve overall performance. This paper\npresents optimizations based on query sequences, which reduces the impact of\nthe reconfigurations. Knowledge of upcoming queries is used to (I)\nspeculatively start reconfiguration already when a query is still running and\n(II) avoid overwriting of reconfigurable regions that will be used again in\nsubsequent queries. We evaluate our optimizations with a calibrated model and\nmeasurements for various parameter values. Improvements in execution time of up\nto 28% can be obtained even with sequences of only two queries.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 11:32:23 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["G.", "Lekshmi B.", ""], ["Becher", "Andreas", ""], ["Meyer-Wegener", "Klaus", ""]]}, {"id": "2005.01520", "submitter": "Doris Xin", "authors": "Angela Lee, Doris Xin, Doris Lee, Aditya Parameswaran", "title": "Demystifying a Dark Art: Understanding Real-World Machine Learning Model\n  Development", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well-known that the process of developing machine learning (ML)\nworkflows is a dark-art; even experts struggle to find an optimal workflow\nleading to a high accuracy model. Users currently rely on empirical\ntrial-and-error to obtain their own set of battle-tested guidelines to inform\ntheir modeling decisions. In this study, we aim to demystify this dark art by\nunderstanding how people iterate on ML workflows in practice. We analyze over\n475k user-generated workflows on OpenML, an open-source platform for tracking\nand sharing ML workflows. We find that users often adopt a manual, automated,\nor mixed approach when iterating on their workflows. We observe that manual\napproaches result in fewer wasted iterations compared to automated approaches.\nYet, automated approaches often involve more preprocessing and hyperparameter\noptions explored, resulting in higher performance overall--suggesting potential\nbenefits for a human-in-the-loop ML system that appropriately recommends a\nclever combination of the two strategies.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 14:33:39 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Lee", "Angela", ""], ["Xin", "Doris", ""], ["Lee", "Doris", ""], ["Parameswaran", "Aditya", ""]]}, {"id": "2005.01615", "submitter": "Amir Neshastegaran", "authors": "Amir Neshastegaran, Ali Norouzifar and Iman Izadi", "title": "A Framework for Plant Topology Extraction Using Process Mining and Alarm\n  Data", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SY eess.SP eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Industrial plants are prone to faults. To notify the operator of a fault\noccurrence, alarms are utilized as a basic part of modern computer-controlled\nplants. However, due to the interconnections of different parts of a plant, a\nsingle fault often propagates through the plant and triggers a (sometimes\nlarge) number of alarms. A graphical plant topology can help operators, process\nengineers and maintenance experts find the root cause of a plant upset or\ndiscover the propagation path of a fault. In this paper, a method is developed\nto extract plant topology form alarm data. The method is based on process\nmining, a collection of concepts and algorithms that model a process (not\nnecessarily an engineering one) based on recorded events. The event based\nnature of alarm data as well as the chronological order of recorded alarms make\nthem suitable for process mining. The methodology developed in this paper is\nbased on preparing alarm data for process mining and then using the suitable\nprocess mining algorithms to extract plant topology. The extracted topology is\nrepresented by the familiar Petri net which can be used for root cause analysis\nand discovering fault propagation paths. The methods to evaluate the extracted\ntopology are also discussed. A case study on the well-known Tennessee Eastman\nprocess demonstrates the utility of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 20:05:34 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Neshastegaran", "Amir", ""], ["Norouzifar", "Ali", ""], ["Izadi", "Iman", ""]]}, {"id": "2005.02239", "submitter": "Ruben Verborgh", "authors": "Ruben Verborgh and Ruben Taelman", "title": "Guided Link-Traversal-Based Query Processing", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Link-Traversal-Based Query Processing (LTBQP) is a technique for evaluating\nqueries over a web of data by starting with a set of seed documents that is\ndynamically expanded through following hyperlinks. Compared to query evaluation\nover a static set of sources, LTBQP is significantly slower because of the\nnumber of needed network requests. Furthermore, there are concerns regarding\nrelevance and trustworthiness of results, given that sources are selected\ndynamically. To address both issues, we propose guided LTBQP, a technique in\nwhich information about document linking structure and content policies is\npassed to a query processor. Thereby, the processor can prune the search tree\nof documents by only following relevant links, and restrict the result set to\ndesired results by limiting which documents are considered for what kinds of\ncontent. In this exploratory paper, we describe the technique at a high level\nand sketch some of its applications. We argue that such guidance can make LTBQP\na valuable query strategy in decentralized environments, where data is spread\nacross documents with varying levels of user trust.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 22:35:51 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Verborgh", "Ruben", ""], ["Taelman", "Ruben", ""]]}, {"id": "2005.02510", "submitter": "Shantanu Sharma", "authors": "Peeyush Gupta, Sharad Mehrotra, Nisha Panwar, Shantanu Sharma, Nalini\n  Venkatasubramanian, Guoxi Wang", "title": "Quest: Practical and Oblivious Mitigation Strategies for COVID-19 using\n  WiFi Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contact tracing has emerged as one of the main mitigation strategies to\nprevent the spread of pandemics such as COVID-19. Recently, several efforts\nhave been initiated to track individuals, their movements, and interactions\nusing technologies, e.g., Bluetooth beacons, cellular data records, and\nsmartphone applications. Such solutions are often intrusive, potentially\nviolating individual privacy rights and are often subject to regulations (e.g.,\nGDPR and CCPR) that mandate the need for opt-in policies to gather and use\npersonal information. In this paper, we introduce Quest, a system that empowers\norganizations to observe individuals and spaces to implement policies for\nsocial distancing and contact tracing using WiFi connectivity data in a passive\nand privacy-preserving manner. The goal is to ensure the safety of employees\nand occupants at an organization, while protecting the privacy of all parties.\nQuest incorporates computationally- and information-theoretically-secure\nprotocols that prevent adversaries from gaining knowledge of an individual's\nlocation history (based on WiFi data); it includes support for accurately\nidentifying users who were in the vicinity of a confirmed patient, and then\ninforming them via opt-in mechanisms. Quest supports a range of privacy-enabled\napplications to ensure adherence to social distancing, monitor the flow of\npeople through spaces, identify potentially impacted regions, and raise\nexposure alerts. We describe the architecture, design choices, and\nimplementation of the proposed security/privacy techniques in Quest. We, also,\nvalidate the practicality of Quest and evaluate it thoroughly via an actual\ncampus-scale deployment at UC Irvine over a very large dataset of over 50M\ntuples.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 21:39:38 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Gupta", "Peeyush", ""], ["Mehrotra", "Sharad", ""], ["Panwar", "Nisha", ""], ["Sharma", "Shantanu", ""], ["Venkatasubramanian", "Nalini", ""], ["Wang", "Guoxi", ""]]}, {"id": "2005.02537", "submitter": "Rick Cole", "authors": "Daniel Ting, Rick Cole", "title": "Conditional Cuckoo Filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bloom filters, cuckoo filters, and other approximate set membership sketches\nhave a wide range of applications. Oftentimes, expensive operations can be\nskipped if an item is not in a data set. These filters provide an inexpensive,\nmemory efficient way to test if an item is in a set and avoid unnecessary\noperations. Existing sketches only allow membership testing for single set.\nHowever, in some applications such as join processing, the relevant set is not\nfixed and is determined by a set of predicates.\n  We propose the Conditional Cuckoo Filter, a simple modification of the cuckoo\nfilter that allows for set membership testing given predicates on a\npre-computed sketch. This filter also introduces a novel chaining technique\nthat enables cuckoo filters to handle insertion of duplicate keys. We evaluate\nour methods on a join processing application and show that they significantly\nreduce the number of tuples that a join must process.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 23:46:08 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Ting", "Daniel", ""], ["Cole", "Rick", ""]]}, {"id": "2005.02829", "submitter": "Aleem Akhtar Asif", "authors": "Aleem Akhtar", "title": "Role of Apache Software Foundation in Big Data Projects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increase in amount of Big Data being generated each year, tools and\ntechnologies developed and used for the purpose of storing, processing and\nanalyzing Big Data has also improved. Open-Source software has been an\nimportant factor in the success and innovation in the field of Big Data while\nApache Software Foundation (ASF) has played a crucial role in this success and\ninnovation by providing a number of state-of-the-art projects, free and open to\nthe public. ASF has classified its project in different categories. In this\nreport, projects listed under Big Data category are deeply analyzed and\ndiscussed with reference to one-of-the seven sub-categories defined. Our\ninvestigation has shown that many of the Apache Big Data projects are\nautonomous but some are built based on other Apache projects and some work in\nconjunction with other projects to improve and ease development in Big Data\nspace.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 11:12:59 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Akhtar", "Aleem", ""]]}, {"id": "2005.03156", "submitter": "Jeremy Kepner", "authors": "Jeremy Kepner, Andreas Kipf, Darren Engwirda, Navin Vembar, Michael\n  Jones, Lauren Milechin, Vijay Gadepally, Chris Hill, Tim Kraska, William\n  Arcand, David Bestor, William Bergeron, Chansup Byun, Matthew Hubbell,\n  Michael Houle, Andrew Kirby, Anna Klein, Julie Mullen, Andrew Prout, Albert\n  Reuther, Antonio Rosa, Sid Samsi, Charles Yee, Peter Michaleas", "title": "Fast Mapping onto Census Blocks", "comments": "8 pages, 7 figures, 55 references; accepted to IEEE HPEC 2020", "journal-ref": null, "doi": "10.1109/HPEC43674.2020.9286157", "report-no": null, "categories": "cs.DC cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pandemic measures such as social distancing and contact tracing can be\nenhanced by rapidly integrating dynamic location data and demographic data.\nProjecting billions of longitude and latitude locations onto hundreds of\nthousands of highly irregular demographic census block polygons is\ncomputationally challenging in both research and deployment contexts. This\npaper describes two approaches labeled \"simple\" and \"fast\". The simple approach\ncan be implemented in any scripting language (Matlab/Octave, Python, Julia, R)\nand is easily integrated and customized to a variety of research goals. This\nsimple approach uses a novel combination of hierarchy, sparse bounding boxes,\npolygon crossing-number, vectorization, and parallel processing to achieve\n100,000,000+ projections per second on 100 servers. The simple approach is\ncompact, does not increase data storage requirements, and is applicable to any\ncountry or region. The fast approach exploits the thread, vector, and memory\noptimizations that are possible using a low-level language (C++) and achieves\nsimilar performance on a single server. This paper details these approaches\nwith the goal of enabling the broader community to quickly integrate location\nand demographic data.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 22:07:05 GMT"}, {"version": "v2", "created": "Sat, 1 Aug 2020 17:19:44 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Kepner", "Jeremy", ""], ["Kipf", "Andreas", ""], ["Engwirda", "Darren", ""], ["Vembar", "Navin", ""], ["Jones", "Michael", ""], ["Milechin", "Lauren", ""], ["Gadepally", "Vijay", ""], ["Hill", "Chris", ""], ["Kraska", "Tim", ""], ["Arcand", "William", ""], ["Bestor", "David", ""], ["Bergeron", "William", ""], ["Byun", "Chansup", ""], ["Hubbell", "Matthew", ""], ["Houle", "Michael", ""], ["Kirby", "Andrew", ""], ["Klein", "Anna", ""], ["Mullen", "Julie", ""], ["Prout", "Andrew", ""], ["Reuther", "Albert", ""], ["Rosa", "Antonio", ""], ["Samsi", "Sid", ""], ["Yee", "Charles", ""], ["Michaleas", "Peter", ""]]}, {"id": "2005.03314", "submitter": "Fei Song", "authors": "Fei Song, Khaled Zaouk, Chenghao Lyu, Arnab Sinha, Qi Fan, Yanlei\n  Diao, Prashant Shenoy", "title": "Boosting Cloud Data Analytics using Multi-Objective Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data analytics in the cloud has become an integral part of enterprise\nbusinesses. Big data analytics systems, however, still lack the ability to take\nuser performance goals and budgetary constraints for a task, collectively\nreferred to as task objectives, and automatically configure an analytic job to\nachieve these objectives. This paper presents a data analytics optimizer that\ncan automatically determine a cluster configuration with a suitable number of\ncores as well as other system parameters that best meet the task objectives. At\na core of our work is a principled multi-objective optimization (MOO) approach\nthat computes a Pareto optimal set of job configurations to reveal tradeoffs\nbetween different user objectives, recommends a new job configuration that best\nexplores such tradeoffs, and employs novel optimizations to enable such\nrecommendations within a few seconds. We present efficient incremental\nalgorithms based on the notion of a Progressive Frontier for realizing our MOO\napproach and implement them into a Spark-based prototype. Detailed experiments\nusing benchmark workloads show that our MOO techniques provide a 2-50x speedup\nover existing MOO methods, while offering good coverage of the Pareto frontier.\nWhen compared to Ottertune, a state-of-the-art performance tuning system, our\napproach recommends configurations that yield 26\\%-49\\% reduction of running\ntime of the TPCx-BB benchmark while adapting to different application\npreferences on multiple objectives.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 08:21:31 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Song", "Fei", ""], ["Zaouk", "Khaled", ""], ["Lyu", "Chenghao", ""], ["Sinha", "Arnab", ""], ["Fan", "Qi", ""], ["Diao", "Yanlei", ""], ["Shenoy", "Prashant", ""]]}, {"id": "2005.03328", "submitter": "Bailu Ding", "authors": "Bailu Ding, Surajit Chaudhuri, Vivek Narasayya", "title": "Bitvector-aware Query Optimization for Decision Support Queries\n  (extended version)", "comments": "This technical report is an extended version of the ACM SIGMOD 2020\n  paper Bitvector-aware Query Optimization for Decision Support Queries", "journal-ref": null, "doi": null, "report-no": "MSR-TR-2020-8", "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bitvector filtering is an important query processing technique that can\nsignificantly reduce the cost of execution, especially for complex decision\nsupport queries with multiple joins. Despite its wide application, however, its\nimplication to query optimization is not well understood.\n  In this work, we study how bitvector filters impact query optimization. We\nshow that incorporating bitvector filters into query optimization\nstraightforwardly can increase the plan space complexity by an exponential\nfactor in the number of relations in the query. We analyze the plans with\nbitvector filters for star and snowflake queries in the plan space of right\ndeep trees without cross products. Surprisingly, with some simplifying\nassumptions, we prove that, the plan of the minimal cost with bitvector filters\ncan be found from a linear number of plans in the number of relations in the\nquery. This greatly reduces the plan space complexity for such queries from\nexponential to linear.\n  Motivated by our analysis, we propose an algorithm that accounts for the\nimpact of bitvector filters in query optimization. Our algorithm optimizes the\njoin order for an arbitrary decision support query by choosing from a linear\nnumber of candidate plans in the number of relations in the query. We implement\nour algorithm in Microsoft SQL Server as a transformation rule. Our evaluation\non both industry standard benchmarks and customer workload shows that, compared\nwith the original Microsoft SQL Server, our technique reduces the total CPU\nexecution time by 22%-64% for the workloads, with up to two orders of magnitude\nreduction in CPU execution time for individual queries.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 08:56:53 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Ding", "Bailu", ""], ["Chaudhuri", "Surajit", ""], ["Narasayya", "Vivek", ""]]}, {"id": "2005.03468", "submitter": "Lu Chen", "authors": "Lu Chen, Yunjun Gao, Xuan Song, Zheng Li, Xiaoye Miao, and Christian\n  S. Jensen", "title": "Indexing Metric Spaces for Exact Similarity Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the continued digitalization of societal processes, we are seeing an\nexplosion in available data. This is referred to as big data. In a research\nsetting, three aspects of the data are often viewed as the main sources of\nchallenges when attempting to enable value creation from big data: volume,\nvelocity and variety. Many studies address volume or velocity, while much fewer\nstudies concern the variety. Metric space is ideal for addressing variety\nbecause it can accommodate any type of data as long as its associated distance\nnotion satisfies the triangle inequality. To accelerate search in metric space,\na collection of indexing techniques for metric data have been proposed.\nHowever, existing surveys each offers only a narrow coverage, and no\ncomprehensive empirical study of those techniques exists. We offer a survey of\nall the existing metric indexes that can support exact similarity search, by i)\nsummarizing all the existing partitioning, pruning and validation techniques\nused for metric indexes, ii) providing the time and storage complexity analysis\non the index construction, and iii) report on a comprehensive empirical\ncomparison of their similarity query processing performance. Here, empirical\ncomparisons are used to evaluate the index performance during search as it is\nhard to see the complexity analysis differences on the similarity query\nprocessing and the query performance depends on the pruning and validation\nabilities related to the data distribution. This article aims at revealing\ndifferent strengths and weaknesses of different indexing techniques in order to\noffer guidance on selecting an appropriate indexing technique for a given\nsetting, and directing the future research for metric indexes.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 13:41:13 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Chen", "Lu", ""], ["Gao", "Yunjun", ""], ["Song", "Xuan", ""], ["Li", "Zheng", ""], ["Miao", "Xiaoye", ""], ["Jensen", "Christian S.", ""]]}, {"id": "2005.03529", "submitter": "Shrestha Ghosh", "authors": "Shrestha Ghosh, Simon Razniewski, Gerhard Weikum", "title": "CounQER: A System for Discovering and Linking Count Information in\n  Knowledge Bases", "comments": "Accepted at ESWC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicate constraints of general-purpose knowledge bases (KBs) like Wikidata,\nDBpedia and Freebase are often limited to subproperty, domain and range\nconstraints. In this demo we showcase CounQER, a system that illustrates the\nalignment of counting predicates, like staffSize, and enumerating predicates,\nlike workInstitution^{-1} . In the demonstration session, attendees can inspect\nthese alignments, and will learn about the importance of these alignments for\nKB question answering and curation. CounQER is available at\nhttps://counqer.mpi-inf.mpg.de/spo.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 14:53:11 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Ghosh", "Shrestha", ""], ["Razniewski", "Simon", ""], ["Weikum", "Gerhard", ""]]}, {"id": "2005.03640", "submitter": "Tommaso Soru", "authors": "Tommaso Soru, Edgard Marx, Andr\\'e Valdestilhas, Diego Moussallem,\n  Gustavo Publio, and Muhammad Saleem", "title": "Where is Linked Data in Question Answering over Linked Data?", "comments": "Position paper, THE Workshop @ ISWC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We argue that \"Question Answering with Knowledge Base\" and \"Question\nAnswering over Linked Data\" are currently two instances of the same problem,\ndespite one explicitly declares to deal with Linked Data. We point out the lack\nof existing methods to evaluate question answering on datasets which exploit\nexternal links to the rest of the cloud or share common schema. To this end, we\npropose the creation of new evaluation settings to leverage the advantages of\nthe Semantic Web to achieve AI-complete question answering.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 17:42:13 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Soru", "Tommaso", ""], ["Marx", "Edgard", ""], ["Valdestilhas", "Andr\u00e9", ""], ["Moussallem", "Diego", ""], ["Publio", "Gustavo", ""], ["Saleem", "Muhammad", ""]]}, {"id": "2005.03787", "submitter": "Narjes Hachani", "authors": "Narjes Hachani Gharbi", "title": "D\\'etermination Automatique des Fonctions d'Appartenance et\n  Interrogation Flexible et Coop\\'erative des Bases de Donn\\'ees", "comments": "133 pages, in French. 22 figures PhD thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flexible querying of DB allows to extend DBMS in order to support imprecision\nand flexibility in queries. Flexible queries use vague and imprecise terms\nwhich have been defined as fuzzy sets. However, there is no consensus on\nmemberships functions generation. Most of the proposed methods require expert\nintervention. This thesis is devised in two parts. In the first part, we\npropose a clustering based approach for automatic and incremental membership\nfunctions generation. We have proposed the clustering method CLUSTERDB* which\nevaluates clustering quality underway clusters generation. Moreover, we propose\nincremental updates of partitions and membership functions after insertion or\ndeletion of a new object. The second part of this thesis uses these functions\nand Formal Concepts Analysis in flexible and cooperative querying. In case of\nempty answers, we formally detect the failure reasons and we generate\napproximative queries with their answers. These queries help the user to\nformulate new queries having answers. The different proposed approaches are\nimplemented and experimented with several databases. The experimentation\nresults are encouraging.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 22:33:18 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Gharbi", "Narjes Hachani", ""]]}, {"id": "2005.05036", "submitter": "Sally Elghamrawy Prof", "authors": "Manar A. Elmeiligy, Ali I. El Desouky, Sally M. Elghamrawy", "title": "A Multi-Dimensional Big Data Storing System for Generated COVID-19\n  Large-Scale Data using Apache Spark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ongoing outbreak of coronavirus disease (COVID-19) had burst out in Wuhan\nChina, specifically in December 2019. COVID-19 has caused by a new virus that\nhad not been identified in human previously. This was followed by a widespread\nand rapid spread of this epidemic throughout the world. Daily, the number of\nthe confirmed cases are increasing rapidly, number of the suspect increases,\nbased on the symptoms that accompany this disease, and unfortunately number of\nthe deaths also increase. Therefore, with these increases in number of cases\naround the world, it becomes hard to manage all these cases information with\ndifferent situations; if the patient either injured or suspect with which\nsymptoms that appeared on the patient. Therefore, there is a critical need to\nconstruct a multi-dimensional system to store and analyze the generated\nlarge-scale data. In this paper, a Comprehensive Storing System for COVID-19\ndata using Apache Spark (CSS-COVID) is proposed, to handle and manage the\nproblem caused by increasing the number of COVID-19 daily. CSS-COVID helps in\ndecreasing the processing time for querying and storing COVID-19 daily data.\nCSS-COVID consists of three stages, namely, inserting and indexing, storing,\nand querying stage. In the inserting stage, data is divided into subsets and\nthen index each subset separately. The storing stage uses set of storing-nodes\nto store data, while querying stage is responsible for handling the querying\nprocesses. Using Apache Spark in CSS-COVID leverages the performance of dealing\nwith large-scale data of the coronavirus disease injured whom increase daily. A\nset of experiments are applied, using real COVID-19 Datasets, to prove the\nefficiency of CSS-COVID in indexing large-scale data.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 19:42:07 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Elmeiligy", "Manar A.", ""], ["Desouky", "Ali I. El", ""], ["Elghamrawy", "Sally M.", ""]]}, {"id": "2005.05079", "submitter": "Zhicheng Liu", "authors": "Zhicheng Liu, Aoqian Zhang", "title": "A Survey on Sampling and Profiling over Big Data (Technical Report)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the development of internet technology and computer science, data is\nexploding at an exponential rate. Big data brings us new opportunities and\nchallenges. On the one hand, we can analyze and mine big data to discover\nhidden information and get more potential value. On the other hand, the 5V\ncharacteristic of big data, especially Volume which means large amount of data,\nbrings challenges to storage and processing. For some traditional data mining\nalgorithms, machine learning algorithms and data profiling tasks, it is very\ndifficult to handle such a large amount of data. The large amount of data is\nhighly demanding hardware resources and time consuming. Sampling methods can\neffectively reduce the amount of data and help speed up data processing. Hence,\nsampling technology has been widely studied and used in big data context, e.g.,\nmethods for determining sample size, combining sampling with big data\nprocessing frameworks. Data profiling is the activity that finds metadata of\ndata set and has many use cases, e.g., performing data profiling tasks on\nrelational data, graph data, and time series data for anomaly detection and\ndata repair. However, data profiling is computationally expensive, especially\nfor large data sets. Therefore, this paper focuses on researching sampling and\nprofiling in big data context and investigates the application of sampling in\ndifferent categories of data profiling tasks. From the experimental results of\nthese studies, the results got from the sampled data are close to or even\nexceed the results of the full amount of data. Therefore, sampling technology\nplays an important role in the era of big data, and we also have reason to\nbelieve that sampling technology will become an indispensable step in big data\nprocessing in the future.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 02:54:07 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Liu", "Zhicheng", ""], ["Zhang", "Aoqian", ""]]}, {"id": "2005.05117", "submitter": "Bojan Karla\\v{s}", "authors": "Bojan Karla\\v{s}, Peng Li, Renzhi Wu, Nezihe Merve G\\\"urel, Xu Chu,\n  Wentao Wu, Ce Zhang", "title": "Nearest Neighbor Classifiers over Incomplete Information: From Certain\n  Answers to Certain Predictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) applications have been thriving recently, largely\nattributed to the increasing availability of data. However, inconsistency and\nincomplete information are ubiquitous in real-world datasets, and their impact\non ML applications remains elusive. In this paper, we present a formal study of\nthis impact by extending the notion of Certain Answers for Codd tables, which\nhas been explored by the database research community for decades, into the\nfield of machine learning. Specifically, we focus on classification problems\nand propose the notion of \"Certain Predictions\" (CP) -- a test data example can\nbe certainly predicted (CP'ed) if all possible classifiers trained on top of\nall possible worlds induced by the incompleteness of data would yield the same\nprediction.\n  We study two fundamental CP queries: (Q1) checking query that determines\nwhether a data example can be CP'ed; and (Q2) counting query that computes the\nnumber of classifiers that support a particular prediction (i.e., label). Given\nthat general solutions to CP queries are, not surprisingly, hard without\nassumption over the type of classifier, we further present a case study in the\ncontext of nearest neighbor (NN) classifiers, where efficient solutions to CP\nqueries can be developed -- we show that it is possible to answer both queries\nin linear or polynomial time over exponentially many possible worlds.\n  We demonstrate one example use case of CP in the important application of\n\"data cleaning for machine learning (DC for ML).\" We show that our proposed\nCPClean approach built based on CP can often significantly outperform existing\ntechniques in terms of classification accuracy with mild manual cleaning\neffort.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 13:58:52 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 10:46:33 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Karla\u0161", "Bojan", ""], ["Li", "Peng", ""], ["Wu", "Renzhi", ""], ["G\u00fcrel", "Nezihe Merve", ""], ["Chu", "Xu", ""], ["Wu", "Wentao", ""], ["Zhang", "Ce", ""]]}, {"id": "2005.05227", "submitter": "Jonathan Karr", "authors": "Jonathan R. Karr, Wolfram Liebermeister, Arthur P. Goldberg, John A.\n  P. Sekar and Bilal Shaikh", "title": "ObjTables: structured spreadsheets that promote data quality, reuse, and\n  integration", "comments": "5 pages, 1 figures, 18 pages of supplementary information, 3\n  supplementary datasets", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A central challenge in science is to understand how systems behaviors emerge\nfrom complex networks. This often requires aggregating, reusing, and\nintegrating heterogeneous information. Supplementary spreadsheets to articles\nare a key data source. Spreadsheets are popular because they are easy to read\nand write. However, spreadsheets are often difficult to reanalyze because they\ncapture data ad hoc without schemas that define the objects, relationships, and\nattributes that they represent. To help researchers reuse and compose\nspreadsheets, we developed ObjTables, a toolkit that makes spreadsheets human-\nand machine-readable by combining spreadsheets with schemas and an\nobject-relational mapping system. ObjTables includes a format for schemas;\nmarkup for indicating the class and attribute represented by each spreadsheet\nand column; numerous data types for scientific information; and high-level\nsoftware for using schemas to read, write, validate, compare, merge, revision,\nand analyze spreadsheets. By making spreadsheets easier to reuse, ObjTables\ncould enable unprecedented secondary meta-analyses. By making it easy to build\nnew formats and associated software for new types of data, ObjTables can also\naccelerate emerging scientific fields.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 16:18:52 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2020 17:29:46 GMT"}, {"version": "v3", "created": "Thu, 6 Aug 2020 20:43:50 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Karr", "Jonathan R.", ""], ["Liebermeister", "Wolfram", ""], ["Goldberg", "Arthur P.", ""], ["Sekar", "John A. P.", ""], ["Shaikh", "Bilal", ""]]}, {"id": "2005.05886", "submitter": "Julien Corman", "authors": "Diego Calvanese and Julien Corman and Davide Lanti and Simon\n  Razniewski", "title": "Counting Query Answers over a DL-Lite Knowledge Base (extended version)", "comments": "Extended version of an article published at IJCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Counting answers to a query is an operation supported by virtually all\ndatabase management systems. In this paper we focus on counting answers over a\nKnowledge Base (KB), which may be viewed as a database enriched with background\nknowledge about the domain under consideration. In particular, we place our\nwork in the context of Ontology-Mediated Query Answering/Ontology-based Data\nAccess (OMQA/OBDA), where the language used for the ontology is a member of the\nDL-Lite family and the data is a (usually virtual) set of assertions. We study\nthe data complexity of query answering, for different members of the DL-Lite\nfamily that include number restrictions, and for variants of conjunctive\nqueries with counting that differ with respect to their shape (connected,\nbranching, rooted). We improve upon existing results by providing a PTIME and\ncoNP lower bounds, and upper bounds in PTIME and LOGSPACE. For the latter case,\nwe define a novel query rewriting technique into first-order logic with\ncounting.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 16:01:09 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 17:39:27 GMT"}, {"version": "v3", "created": "Fri, 17 Jul 2020 05:23:03 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Calvanese", "Diego", ""], ["Corman", "Julien", ""], ["Lanti", "Davide", ""], ["Razniewski", "Simon", ""]]}, {"id": "2005.06133", "submitter": "Sainyam Galhotra Mr", "authors": "Sainyam Galhotra, Behzad Golshan and Wang-Chiew Tan", "title": "Adaptive Rule Discovery for Labeling Text Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Creating and collecting labeled data is one of the major bottlenecks in\nmachine learning pipelines and the emergence of automated feature generation\ntechniques such as deep learning, which typically requires a lot of training\ndata, has further exacerbated the problem. While weak-supervision techniques\nhave circumvented this bottleneck, existing frameworks either require users to\nwrite a set of diverse, high-quality rules to label data (e.g., Snorkel), or\nrequire a labeled subset of the data to automatically mine rules (e.g., Snuba).\nThe process of manually writing rules can be tedious and time consuming. At the\nsame time, creating a labeled subset of the data can be costly and even\ninfeasible in imbalanced settings. This is due to the fact that a random sample\nin imbalanced settings often contains only a few positive instances.\n  To address these shortcomings, we present Darwin, an interactive system\ndesigned to alleviate the task of writing rules for labeling text data in\nweakly-supervised settings. Given an initial labeling rule, Darwin\nautomatically generates a set of candidate rules for the labeling task at hand,\nand utilizes the annotator's feedback to adapt the candidate rules. We describe\nhow Darwin is scalable and versatile. It can operate over large text corpora\n(i.e., more than 1 million sentences) and supports a wide range of labeling\nfunctions (i.e., any function that can be specified using a context free\ngrammar). Finally, we demonstrate with a suite of experiments over five\nreal-world datasets that Darwin enables annotators to generate\nweakly-supervised labels efficiently and with a small cost. In fact, our\nexperiments show that rules discovered by Darwin on average identify 40% more\npositive instances compared to Snuba even when it is provided with 1000 labeled\ninstances.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 03:29:12 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Galhotra", "Sainyam", ""], ["Golshan", "Behzad", ""], ["Tan", "Wang-Chiew", ""]]}, {"id": "2005.06154", "submitter": "Shantanu Sharma", "authors": "Sharad Mehrotra, Shantanu Sharma, Jeffrey D. Ullman, Dhrubajyoti\n  Ghosh, Peeyush Gupta", "title": "Panda: Partitioned Data Security on Outsourced Sensitive and\n  Non-sensitive Data", "comments": "This version has been accepted in ACM Transactions on Management\n  Information Systems. The final published version of this paper may differ\n  from this accepted version. A preliminary version of this paper\n  [arXiv:1812.09233] was accepted and presented in IEEE ICDE 2019", "journal-ref": null, "doi": "10.1145/3397521", "report-no": null, "categories": "cs.DB cs.CR cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite extensive research on cryptography, secure and efficient query\nprocessing over outsourced data remains an open challenge. This paper continues\nalong with the emerging trend in secure data processing that recognizes that\nthe entire dataset may not be sensitive, and hence, non-sensitivity of data can\nbe exploited to overcome limitations of existing encryption-based approaches.\nWe, first, provide a new security definition, entitled partitioned data\nsecurity for guaranteeing that the joint processing of non-sensitive data (in\ncleartext) and sensitive data (in encrypted form) does not lead to any leakage.\nThen, this paper proposes a new secure approach, entitled query binning (QB)\nthat allows secure execution of queries over non-sensitive and sensitive parts\nof the data. QB maps a query to a set of queries over the sensitive and\nnon-sensitive data in a way that no leakage will occur due to the joint\nprocessing over sensitive and non-sensitive data. In particular, we propose\nsecure algorithms for selection, range, and join queries to be executed over\nencrypted sensitive and cleartext non-sensitive datasets. Interestingly, in\naddition to improving performance, we show that QB actually strengthens the\nsecurity of the underlying cryptographic technique by preventing size,\nfrequency-count, and workload-skew attacks.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 05:27:18 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Mehrotra", "Sharad", ""], ["Sharma", "Shantanu", ""], ["Ullman", "Jeffrey D.", ""], ["Ghosh", "Dhrubajyoti", ""], ["Gupta", "Peeyush", ""]]}, {"id": "2005.06293", "submitter": "Anastasia Dimou", "authors": "Anastasia Dimou", "title": "R2RML and RML Comparison for RDF Generation, their Rules Validation and\n  Inconsistency Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an overview of the state of the art on knowledge graph\ngeneration is provided, with focus on the two prevalent mapping languages: the\nW3C recommended R2RML and its generalisation RML. We look into details on their\ndifferences and explain how knowledge graphs, in the form of RDF graphs, can be\ngenerated with each one of the two mapping languages. Then we assess if the\nvocabulary terms were properly applied to the data and no violations occurred\non their use, either using R2RML or RML to generate the desired knowledge\ngraph.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 12:53:04 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Dimou", "Anastasia", ""]]}, {"id": "2005.06437", "submitter": "Siddhant Arora", "authors": "Siddhant Arora, Srikanta Bedathur", "title": "On Embeddings in Relational Databases", "comments": "9 pages, 6 Figures, Proceedings of Knowledge Representation &\n  Reasoning Meets Machine Learning Workshop, NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of learning a distributed representation of entities\nin a relational database using a low-dimensional embedding. Low-dimensional\nembeddings aim to encapsulate a concise vector representation for an underlying\ndataset with minimum loss of information. Embeddings across entities in a\nrelational database have been less explored due to the intricate data relations\nand representation complexity involved. Relational databases are an\ninter-weaved collection of relations that not only model relationships between\nentities but also record complex domain-specific quantitative and temporal\nattributes of data defining complex relationships among entities. Recent\nmethods for learning an embedding constitute of a naive approach to consider\ncomplete denormalization of the database by materializing the full join of all\ntables and representing as a knowledge graph. This popular approach has certain\nlimitations as it fails to capture the inter-row relationships and additional\nsemantics encoded in the relational databases. In this paper we demonstrate; a\nbetter methodology for learning representations by exploiting the underlying\nsemantics of columns in a table while using the relation joins and the latent\ninter-row relationships. Empirical results over a real-world database with\nevaluations on similarity join and table completion tasks support our\nproposition.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 17:21:27 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Arora", "Siddhant", ""], ["Bedathur", "Srikanta", ""]]}, {"id": "2005.06469", "submitter": "Erich Bremer", "authors": "Erich Bremer, Jonas Almeida, Joel Saltz", "title": "Representing Whole Slide Cancer Image Features with Hilbert Curves", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.DB cs.IR q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regions of Interest (ROI) contain morphological features in pathology whole\nslide images (WSI) are delimited with polygons[1]. These polygons are often\nrepresented in either a textual notation (with the array of edges) or in a\nbinary mask form. Textual notations have an advantage of human readability and\nportability, whereas, binary mask representations are more useful as the input\nand output of feature-extraction pipelines that employ deep learning\nmethodologies. For any given whole slide image, more than a million cellular\nfeatures can be segmented generating a corresponding number of polygons. The\ncorpus of these segmentations for all processed whole slide images creates\nvarious challenges for filtering specific areas of data for use in interactive\nreal-time and multi-scale displays and analysis. Simple range queries of image\nlocations do not scale and, instead, spatial indexing schemes are required. In\nthis paper we propose using Hilbert Curves simultaneously for spatial indexing\nand as a polygonal ROI representation. This is achieved by using a series of\nHilbert Curves[2] creating an efficient and inherently spatially-indexed\nmachine-usable form. The distinctive property of Hilbert curves that enables\nboth mask and polygon delimitation of ROIs is that the elements of the vector\nextracted ro describe morphological features maintain their relative positions\nfor different scales of the same image.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 16:38:24 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Bremer", "Erich", ""], ["Almeida", "Jonas", ""], ["Saltz", "Joel", ""]]}, {"id": "2005.06503", "submitter": "Pierre Pradic", "authors": "Michael Benedikt and Pierre Pradic", "title": "Generating collection transformations from proofs", "comments": null, "journal-ref": null, "doi": "10.1145/3434295", "report-no": null, "categories": "cs.LO cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nested relations, built up from atomic types via product and set types, form\na rich data model. Over the last decades the nested relational calculus, NRC,\nhas emerged as a standard language for defining transformations on nested\ncollections. NRC is a strongly-typed functional language which allows building\nup transformations using tupling and projections, a singleton-former, and a map\noperation that lifts transformations on tuples to transformations on sets.\n  In this work we describe an alternative declarative method of describing\ntransformations in logic. A formula with distinguished inputs and outputs gives\nan implicit definition if one can prove that for each input there is only one\noutput that satisfies it. Our main result shows that one can synthesize\ntransformations from proofs that a formula provides an implicit definition,\nwhere the proof is in an intuitionistic calculus that captures a natural style\nof reasoning about nested collections. Our polynomial time synthesis procedure\nis based on an analog of Craig's interpolation lemma, starting with a provable\ncontainment between terms representing nested collections and generating an NRC\nexpression that interpolates between them.\n  We further show that NRC expressions that implement an implicit definition\ncan be found when there is a classical proof of functionality, not just when\nthere is an intuitionistic one. That is, whenever a formula implicitly defines\na transformation, there is an NRC expression that implements it.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 18:18:03 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 16:23:50 GMT"}, {"version": "v3", "created": "Mon, 29 Jun 2020 15:37:55 GMT"}, {"version": "v4", "created": "Mon, 6 Jul 2020 20:57:46 GMT"}, {"version": "v5", "created": "Wed, 8 Jul 2020 09:42:58 GMT"}, {"version": "v6", "created": "Wed, 11 Nov 2020 13:51:27 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Benedikt", "Michael", ""], ["Pradic", "Pierre", ""]]}, {"id": "2005.07544", "submitter": "Frances Skinner", "authors": "Frances M. Skinner, Iouli E. Gordon, Christian Hill, Robert J.\n  Hargreaves, Kelly E. Lockhart and Laurence S. Rothman", "title": "Referencing Sources of Molecular Spectroscopic Data in the Era of Data\n  Science: Application to the HITRAN and AMBDAS Databases", "comments": "11 pages, 5 figures, already published online at\n  https://doi.org/10.3390/atoms8020016", "journal-ref": "Journal: Atoms Special Issue Development and Perspectives of\n  Atomic and Molecular Databases, Volume 8, Issue 2, 30 April 2020", "doi": "10.3390/atoms8020016", "report-no": null, "categories": "cs.DL cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The application described has been designed to create bibliographic entries\nin large databases with diverse sources automatically, which reduces both the\nfrequency of mistakes and the workload for the administrators. This new system\nuniquely identifies each reference from its digital object identifier (DOI) and\nretrieves the corresponding bibliographic information from any of several\nonline services, including the SAO/NASA Astrophysics Data Systems (ADS) and\nCrossRef APIs. Once parsed into a relational database, the software is able to\nproduce bibliographies in any of several formats, including HTML and BibTeX,\nfor use on websites or printed articles. The application is provided\nfree-of-charge for general use by any scientific database. The power of this\napplication is demonstrated when used to populate reference data for the HITRAN\nand AMBDAS databases as test cases. HITRAN contains data that is provided by\nresearchers and collaborators throughout the spectroscopic community. These\ncontributors are accredited for their contributions through the bibliography\nproduced alongside the data returned by an online search in HITRAN. Prior to\nthe work presented here, HITRAN and AMBDAS created these bibliographies\nmanually, which is a tedious, time-consuming and error-prone process. The\ncomplete code for the new referencing system can be found at\n\\url{https://github.com/hitranonline/refs}.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 13:48:37 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Skinner", "Frances M.", ""], ["Gordon", "Iouli E.", ""], ["Hill", "Christian", ""], ["Hargreaves", "Robert J.", ""], ["Lockhart", "Kelly E.", ""], ["Rothman", "Laurence S.", ""]]}, {"id": "2005.07658", "submitter": "Yinjun Wu", "authors": "Yinjun Wu, Kwanghyun Park, Rathijit Sen, Brian Kroth, Jaeyoung Do", "title": "Lessons learned from the early performance evaluation of Intel Optane DC\n  Persistent Memory in DBMS", "comments": null, "journal-ref": "DAMON 2020", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-volatile memory (NVM) is an emerging technology, which has the\npersistence characteristics of large capacity storage devices(e.g., HDDs and\nSSDs), while providing the low access latency and byte-addressablity of\ntraditional DRAM memory. This unique combination of features open up several\nnew design considerations when building database management systems (DBMSs),\nsuch as replacing DRAM (as the main working space memory) or block devices (as\nthe persistent storage), or complementing both at the same time for several\nDBMS components (such as access methods,storage engine, buffer management,\nlogging/recovery, etc).\n  However, interacting with NVM requires changes to application software to\nbest use the device (e.g. mmap and clflush of small cache lines instead of\nwrite and fsync of large page buffers). Before introducing (potentially major)\ncode changes to the DBMS for NVM, developers need a clear understanding of NVM\nperformance in various conditions to help make better design choices.\n  In this paper, we provide extensive performance evaluations conducted with a\nrecently released NVM device, Intel Optane DC Persistent Memory (PMem), under\ndifferent configurations with several micro-benchmark tools. Further, we\nevaluate OLTP and OLAP database workloads (i.e., TPC-C and TPC-H) with\nMicrosoft SQL Server 2019 when using the NVM device as an in-memory buffer pool\nor persistent storage. From the lessons learned we share some recommendations\nfor future DBMS design with PMem, e.g.simple hardware or software changes are\nnot enough for the best use of PMem in DBMSs.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 17:18:50 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Wu", "Yinjun", ""], ["Park", "Kwanghyun", ""], ["Sen", "Rathijit", ""], ["Kroth", "Brian", ""], ["Do", "Jaeyoung", ""]]}, {"id": "2005.07778", "submitter": "Pietro Ferraro", "authors": "Andrew Cullen, Pietro Ferraro, William Sanders, Luigi Vigneri and\n  Robert Shorten", "title": "Access Control for Distributed Ledgers in the Internet of Things: A\n  Networking Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Internet of Things (IoT) domain, devices need a platform to transact\nseamlessly without a trusted intermediary. Although Distributed Ledger\nTechnologies (DLTs) could provide such a platform, blockchains, such as\nBitcoin, were not designed with IoT networks in mind, hence are often\nunsuitable for such applications: they offer poor transaction throughput and\nconfirmation times, put stress on constrained computing and storage resources,\nand require high transaction fees. In this work, we consider a class of\nIoT-friendly DLTs based on directed acyclic graphs, rather than a blockchain,\nand with a reputation system in the place of Proof of Work (PoW). However,\nwithout PoW, implementation of these DLTs requires an access control algorithm\nto manage the rate at which nodes can add new transactions to the ledger. We\nmodel the access control problem and present an algorithm that is fair,\nefficient and secure. Our algorithm represents a new design paradigm for DLTs\nin which concepts from networking are applied to the DLT setting for the first\ntime. For example, our algorithm uses distributed rate setting which is similar\nin nature to transmission control used in the Internet. However, our solution\nfeatures novel adaptations to cope with the adversarial environment of DLTs in\nwhich no individual agent can be trusted. Our algorithm guarantees utilisation\nof resources, consistency, fairness, and resilience against attackers. All of\nthis is achieved efficiently and with regard for the limitations of IoT\ndevices. We perform extensive simulations to validate these claims.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 20:58:29 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 00:33:49 GMT"}, {"version": "v3", "created": "Wed, 14 Jul 2021 13:15:39 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Cullen", "Andrew", ""], ["Ferraro", "Pietro", ""], ["Sanders", "William", ""], ["Vigneri", "Luigi", ""], ["Shorten", "Robert", ""]]}, {"id": "2005.07798", "submitter": "Amir Behrouzi-Far", "authors": "Amir Behrouzi-Far, Emina Soljanin and Roy D. Yates", "title": "Data Freshness in Leader-Based Replicated Storage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leader-based data replication improves consistency in highly available\ndistributed storage systems via sequential writes to the leader nodes. After a\nwrite has been committed by the leaders, follower nodes are written by a\nmulticast mechanism and are only guaranteed to be eventually consistent. With\nAge of Information (AoI) as the freshness metric, we characterize how the\nnumber of leaders affects the freshness of the data retrieved by an\ninstantaneous read query. In particular, we derive the average age of a read\nquery for a deterministic model for the leader writing time and a probabilistic\nmodel for the follower writing time. We obtain a closed-form expression for the\naverage age for exponentially distributed follower writing time. Our numerical\nresults show that, depending on the relative speed of the write operation to\nthe two groups of nodes, there exists an optimal number of leaders which\nminimizes the average age of the retrieved data, and that this number increases\nas the relative speed of writing on leaders increases.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 21:56:53 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Behrouzi-Far", "Amir", ""], ["Soljanin", "Emina", ""], ["Yates", "Roy D.", ""]]}, {"id": "2005.07992", "submitter": "George Chernishev", "authors": "Nikita Bobrov (1), Kirill Smirnov (1), George Chernishev (1) ((1)\n  JetBrains Research)", "title": "Extending Databases to Support Data Manipulation with Functional\n  Dependencies: a Vision Paper", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In the current paper, we propose to fuse together stored data (tables) and\ntheir functional dependencies (FDs) inside a DBMS. We aim to make FDs\nfirst-class citizens: objects which can be queried and used to query data. Our\nidea is to allow analysts to explore both data and functional dependencies\nusing the database interface. For example, an analyst may be interested in such\ntasks as: \"find all rows which prevent a given functional dependency from\nholding\", \"for a given table, find all functional dependencies that involve a\ngiven attribute\", \"project all attributes that functionally determine a\nspecified attribute\".\n  For this purpose, we propose: (1) an SQL-based query language for querying a\ncollection of functional dependencies (2) an extension of the SQL SELECT clause\nfor supporting FD-based predicates, including approximate ones (3) a special\ndata structure intended for containing mined FDs and acting as a mediator\nbetween user queries and underlying data. We describe the proposed extensions,\ndemonstrate their use-cases, and finally, discuss implementation details and\ntheir impact on query processing.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 13:46:37 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Bobrov", "Nikita", ""], ["Smirnov", "Kirill", ""], ["Chernishev", "George", ""]]}, {"id": "2005.08111", "submitter": "Liam Steadman", "authors": "Liam Steadman, Nathan Griffiths, Stephen Jarvis, Mark Bell, Shaun\n  Helman, Caroline Wallbank", "title": "kD-STR: A Method for Spatio-Temporal Data Reduction and Modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysing and learning from spatio-temporal datasets is an important process\nin many domains, including transportation, healthcare and meteorology. In\nparticular, data collected by sensors in the environment allows us to\nunderstand and model the processes acting within the environment. Recently, the\nvolume of spatio-temporal data collected has increased significantly,\npresenting several challenges for data scientists. Methods are therefore needed\nto reduce the quantity of data that needs to be processed in order to analyse\nand learn from spatio-temporal datasets. In this paper, we present the\nk-Dimensional Spatio-Temporal Reduction method (kD-STR) for reducing the\nquantity of data used to store a dataset whilst enabling multiple types of\nanalysis on the reduced dataset. kD-STR uses hierarchical partitioning to find\nspatio-temporal regions of similar instances and models the instances within\neach region to summarise the dataset. We demonstrate the generality of kD-STR\nwith 3 datasets exhibiting different spatio-temporal characteristics and\npresent results for a range of data modelling techniques. Finally, we compare\nkD-STR with other techniques for reducing the volume of spatio-temporal data.\nOur results demonstrate that kD-STR is effective in reducing spatio-temporal\ndata and generalises to datasets that exhibit different properties.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 21:43:11 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Steadman", "Liam", ""], ["Griffiths", "Nathan", ""], ["Jarvis", "Stephen", ""], ["Bell", "Mark", ""], ["Helman", "Shaun", ""], ["Wallbank", "Caroline", ""]]}, {"id": "2005.08190", "submitter": "Shunsuke Inenaga", "authors": "Akihiro Nishi, Yuto Nakashima, Shunsuke Inenaga, Hideo Bannai,\n  Masayuki Takeda", "title": "Towards Efficient Interactive Computation of Dynamic Time Warping\n  Distance", "comments": "Accepted for SPIRE 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dynamic time warping (DTW) is a widely-used method that allows us to\nefficiently compare two time series that can vary in speed. Given two strings\n$A$ and $B$ of respective lengths $m$ and $n$, there is a fundamental dynamic\nprogramming algorithm that computes the DTW distance for $A$ and $B$ together\nwith an optimal alignment in $\\Theta(mn)$ time and space. In this paper, we\ntackle the problem of interactive computation of the DTW distance for dynamic\nstrings, denoted $\\mathrm{D^2TW}$, where character-wise edit operation\n(insertion, deletion, substitution) can be performed at an arbitrary position\nof the strings. Let $M$ and $N$ be the sizes of the run-length encoding (RLE)\nof $A$ and $B$, respectively. We present an algorithm for $\\mathrm{D^2TW}$ that\noccupies $\\Theta(mN+nM)$ space and uses $O(m+n+\\#_{\\mathrm{chg}}) \\subseteq\nO(mN + nM)$ time to update a compact differential representation $\\mathit{DS}$\nof the DP table per edit operation, where $\\#_{\\mathrm{chg}}$ denotes the\nnumber of cells in $\\mathit{DS}$ whose values change after the edit operation.\nOur method is at least as efficient as the algorithm recently proposed by\nFroese et al. running in $\\Theta(mN + nM)$ time, and is faster when\n$\\#_{\\mathrm{chg}}$ is smaller than $O(mN + nM)$ which, as our preliminary\nexperiments suggest, is likely to be the case in the majority of instances.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 08:14:43 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 03:15:34 GMT"}, {"version": "v3", "created": "Wed, 29 Jul 2020 14:04:42 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Nishi", "Akihiro", ""], ["Nakashima", "Yuto", ""], ["Inenaga", "Shunsuke", ""], ["Bannai", "Hideo", ""], ["Takeda", "Masayuki", ""]]}, {"id": "2005.08439", "submitter": "Zhiwei Fan", "authors": "Zhiwei Fan, Rathijit Sen, Paraschos Koutris, Aws Albarghouthi", "title": "A Comparative Exploration of ML Techniques for Tuning Query Degree of\n  Parallelism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a large body of recent work applying machine learning (ML)\ntechniques to query optimization and query performance prediction in relational\ndatabase management systems (RDBMSs). However, these works typically ignore the\neffect of \\textit{intra-parallelism} -- a key component used to boost the\nperformance of OLAP queries in practice -- on query performance prediction. In\nthis paper, we take a first step towards filling this gap by studying the\nproblem of \\textit{tuning the degree of parallelism (DOP) via ML techniques} in\nMicrosoft SQL Server, a popular commercial RDBMS that allows an individual\nquery to execute using multiple cores.\n  In our study, we cast the problem of DOP tuning as a {\\em regression} task,\nand examine how several popular ML models can help with query performance\nprediction in a multi-core setting. We explore the design space and perform an\nextensive experimental study comparing different models against a list of\nperformance metrics, testing how well they generalize in different settings:\n$(i)$ to queries from the same template, $(ii)$ to queries from a new template,\n$(iii)$ to instances of different scale, and $(iv)$ to different instances and\nqueries. Our experimental results show that a simple featurization of the input\nquery plan that ignores cost model estimations can accurately predict query\nperformance, capture the speedup trend with respect to the available\nparallelism, as well as help with automatically choosing an optimal per-query\nDOP.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 03:32:32 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 15:37:16 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Fan", "Zhiwei", ""], ["Sen", "Rathijit", ""], ["Koutris", "Paraschos", ""], ["Albarghouthi", "Aws", ""]]}, {"id": "2005.08483", "submitter": "Yang Li", "authors": "Yang Li, Gang Liu, Mingyuan Bai, Junbin Gao, Lixin Ye, Zi Ming", "title": "CSD: Discriminance with Conic Section for Improving Reverse k Nearest\n  Neighbors Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reverse $k$ nearest neighbor (R$k$NN) query finds all points that have\nthe query point as one of their $k$ nearest neighbors ($k$NN), where the $k$NN\nquery finds the $k$ closest points to its query point. Based on the\ncharacteristics of conic section, we propose a discriminance, named CSD (Conic\nSection Discriminance), to determine points whether belong to the R$k$NN set\nwithout issuing any queries with non-constant computational complexity. By\nusing CSD, we also implement an efficient R$k$NN algorithm CSD-R$k$NN with a\ncomputational complexity at $O(k^{1.5}\\cdot log\\,k)$. The comparative\nexperiments are conducted between CSD-R$k$NN and other two state-of-the-art\nRkNN algorithms, SLICE and VR-R$k$NN. The experimental results indicate that\nthe efficiency of CSD-R$k$NN is significantly higher than its competitors.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 06:48:45 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Li", "Yang", ""], ["Liu", "Gang", ""], ["Bai", "Mingyuan", ""], ["Gao", "Junbin", ""], ["Ye", "Lixin", ""], ["Ming", "Zi", ""]]}, {"id": "2005.08540", "submitter": "Ester Livshits", "authors": "Ester Livshits, Alireza Heidari, Ihab F. Ilyas, and Benny Kimelfeld", "title": "Approximate Denial Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of mining integrity constraints from data has been extensively\nstudied over the past two decades for commonly used types of constraints\nincluding the classic Functional Dependencies (FDs) and the more general Denial\nConstraints (DCs). In this paper, we investigate the problem of mining\napproximate DCs (i.e., DCs that are \"almost\" satisfied) from data. Considering\napproximate constraints allows us to discover more accurate constraints in\ninconsistent databases, detect rules that are generally correct but may have a\nfew exceptions, as well as avoid overfitting and obtain more general and less\ncontrived constraints. We introduce the algorithm ADCMiner for mining\napproximate DCs. An important feature of this algorithm is that it does not\nassume any specific definition of an approximate DC, but takes the semantics as\ninput. Since there is more than one way to define an approximate DC and\ndifferent definitions may produce very different results, we do not focus on\none definition, but rather on a general family of approximation functions that\nsatisfies some natural axioms defined in this paper and captures commonly used\ndefinitions of approximate constraints. We also show how our algorithm can be\ncombined with sampling to return results with high accuracy while significantly\nreducing the running time.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 09:06:29 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Livshits", "Ester", ""], ["Heidari", "Alireza", ""], ["Ilyas", "Ihab F.", ""], ["Kimelfeld", "Benny", ""]]}, {"id": "2005.08600", "submitter": "Panagiotis Bouros", "authors": "Dimitrios Tsitsigkos, Konstantinos Lampropoulos, Panagiotis Bouros,\n  Nikos Mamoulis, Manolis Terrovitis", "title": "A Two-level Spatial In-Memory Index", "comments": null, "journal-ref": "Preliminary version of the \"A Two-layer Partitioning for Non-point\n  Spatial Data'' paper at the 37th IEEE International Conference on Data\n  Engineering (ICDE), Chania, Crete, Greece, April 19-22, 2021", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Very large volumes of spatial data increasingly become available and demand\neffective management. While there has been decades of research on spatial data\nmanagement, few works consider the current state of commodity hardware, having\nrelatively large memory and the ability of parallel multi-core processing. In\nthis work, we re-consider the design of spatial indexing under this new\nreality. Specifically, we propose a main-memory indexing approach for objects\nwith spatial extent, which is based on a classic regular space partitioning\ninto disjoint tiles. The novelty of our index is that the contents of each tile\nare further partitioned into four classes. This second-level partitioning not\nonly reduces the number of comparisons required to compute the results, but\nalso avoids the generation and elimination of duplicate results, which is an\ninherent problem of spatial indexes based on disjoint space partitioning. The\nspatial partitions defined by our indexing scheme are totally independent,\nfacilitating effortless parallel evaluation, as no synchronization or\ncommunication between the partitions is necessary. We show how our index can be\nused to efficiently process spatial range queries and drastically reduce the\ncost of the refinement step of the queries. In addition, we study the efficient\nprocessing of numerous range queries in batch and in parallel. Extensive\nexperiments on real datasets confirm the efficiency of our approaches.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 11:30:45 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 16:29:28 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Tsitsigkos", "Dimitrios", ""], ["Lampropoulos", "Konstantinos", ""], ["Bouros", "Panagiotis", ""], ["Mamoulis", "Nikos", ""], ["Terrovitis", "Manolis", ""]]}, {"id": "2005.08708", "submitter": "Istv\\'an Koren", "authors": "Dominik Adam Kus, Istv\\'an Koren, Ralf Klamma", "title": "A Link Generator for Increasing the Utility of OpenAPI-to-GraphQL\n  Translations", "comments": "WWW2020 Developer Track", "journal-ref": null, "doi": "10.13140/RG.2.2.33982.92488", "report-no": null, "categories": "cs.DC cs.DB cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Standardized interfaces are the connecting link of today's distributed\nsystems, facilitating access to data services in the cloud. REST APIs have been\nprevalent over the last years, despite several issues like over- and\nunderfetching of resources. GraphQL enjoys rapid adoption, resolving these\nproblems by using statically typed queries. However, the redevelopment of\nservices to the new paradigm is costly. Therefore, several approaches for the\nsuccessive migration from REST to GraphQL have been proposed, many leveraging\nOpenAPI service descriptions. In this article, we present the findings of our\nempirical evaluation on the APIs.guru directory and identify several schema\ntranslation challenges. These include less expressive schema types in GraphQL,\nas well as missing meta information about related resources in OpenAPI. To this\nend, we developed the open source Link Generator, that analyzes OpenAPI\ndocuments and automatically adds links to increase translation utility. This\nfundamentally benefits around 34% of APIs in the APIs.guru directory. Our\nfindings and tool support contribute to the ongoing discussion about the\nmigration of REST APIs to GraphQL, and provide developers with valuable\ninsights into common pitfalls, to reduce friction during API transformation.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 13:35:22 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Kus", "Dominik Adam", ""], ["Koren", "Istv\u00e1n", ""], ["Klamma", "Ralf", ""]]}, {"id": "2005.09141", "submitter": "Oscar Moll", "authors": "Oscar Moll, Favyen Bastani, Sam Madden, Mike Stonebraker, Vijay\n  Gadepally, Tim Kraska", "title": "ExSample: Efficient Searches on Video Repositories through Adaptive\n  Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capturing and processing video is increasingly common as cameras become\ncheaper to deploy. At the same time, rich video understanding methods have\nprogressed greatly in the last decade. As a result, many organizations now have\nmassive repositories of video data, with applications in mapping, navigation,\nautonomous driving, and other areas. Because state-of-the-art object detection\nmethods are slow and expensive, our ability to process even simple ad-hoc\nobject search queries ('find 100 traffic lights in dashcam video') over this\naccumulated data lags far behind our ability to collect it. Processing video at\nreduced sampling rates is a reasonable default strategy for these types of\nqueries, however, the ideal sampling rate is both data and query dependent. We\nintroduce ExSample, a low cost framework for object search over unindexed video\nthat quickly processes search queries by adapting the amount and location of\nsampled frames to the particular data and query being processed. ExSample\nprioritizes the processing of frames in a video repository so that processing\nis focused in portions of video that most likely contain objects of interest.\nIt continually re-prioritizes processing based on feedback from previously\nprocessed frames. On large, real-world datasets, ExSample reduces processing\ntime by up to 6x over an efficient random sampling baseline and by several\norders of magnitude over state-of-the-art methods that train specialized\nper-query surrogate models. ExSample is thus a key component in building\ncost-efficient video data management systems.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 00:07:58 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2020 04:28:42 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Moll", "Oscar", ""], ["Bastani", "Favyen", ""], ["Madden", "Sam", ""], ["Stonebraker", "Mike", ""], ["Gadepally", "Vijay", ""], ["Kraska", "Tim", ""]]}, {"id": "2005.09169", "submitter": "Shunsuke Inenaga", "authors": "Yoshifumi Sakai and Shunsuke Inenaga", "title": "A faster reduction of the dynamic time warping distance to the longest\n  increasing subsequence length", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The similarity between a pair of time series, i.e., sequences of indexed\nvalues in time order, is often estimated by the dynamic time warping (DTW)\ndistance, instead of any in the well-studied family of measures including the\nlongest common subsequence (LCS) length and the edit distance. Although it may\nseem as if the DTW and the LCS(-like) measures are essentially different, we\nreveal that the DTW distance can be represented by the longest increasing\nsubsequence (LIS) length of a sequence of integers, which is the LCS length\nbetween the integer sequence and itself sorted. For a given pair of time series\nof length $n$ such that the dissimilarity between any elements is an integer\nbetween zero and $c$, we propose an integer sequence that represents any\nsubstring-substring DTW distance as its band-substring LIS length. The length\nof the produced integer sequence is $O(c n^2)$, which can be translated to\n$O(n^2)$ for constant dissimilarity functions. To demonstrate that techniques\ndeveloped under the LCS(-like) measures are directly applicable to analysis of\ntime series via our reduction of DTW to LIS, we present time-efficient\nalgorithms for DTW-related problems utilizing the semi-local sequence\ncomparison technique developed for LCS-related problems.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 02:21:18 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 11:39:40 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Sakai", "Yoshifumi", ""], ["Inenaga", "Shunsuke", ""]]}, {"id": "2005.09367", "submitter": "Lucas Woltmann", "authors": "Lucas Woltmann (1), Claudio Hartmann (1), Dirk Habich (1), Wolfgang\n  Lehner (1) ((1) TU Dresden)", "title": "Machine Learning-based Cardinality Estimation in DBMS on Pre-Aggregated\n  Data", "comments": "10 pages, technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardinality estimation is a fundamental task in database query processing and\noptimization. As shown in recent papers, machine learning (ML)-based approaches\ncan deliver more accurate cardinality estimations than traditional approaches.\nHowever, a lot of example queries have to be executed during the model training\nphase to learn a data-dependent ML model leading to a very time-consuming\ntraining phase. Many of those example queries use the same base data, have the\nsame query structure, and only differ in their predicates. Thus, index\nstructures appear to be an ideal optimization technique at first glance.\nHowever, their benefit is limited. To speed up this model training phase, our\ncore idea is to determine a predicate-independent pre-aggregation of the base\ndata and to execute the example queries over this pre-aggregated data. Based on\nthis idea, we present a specific aggregate-enabled training phase for ML-based\ncardinality estimation approaches in this paper. As we are going to show with\ndifferent workloads in our evaluation, we are able to achieve an average\nspeedup of 63 with our aggregate-enabled training phase.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 11:24:36 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Woltmann", "Lucas", "", "TU Dresden"], ["Hartmann", "Claudio", "", "TU Dresden"], ["Habich", "Dirk", "", "TU Dresden"], ["Lehner", "Wolfgang", "", "TU Dresden"]]}, {"id": "2005.09399", "submitter": "Vasilis Efthymiou", "authors": "Vasilis Efthymiou, Kostas Stefanidis, Vassilis Christophides", "title": "Benchmarking Blocking Algorithms for Web Entities", "comments": "accepted at IEEE Transactions on Big Data journal", "journal-ref": null, "doi": "10.1109/TBDATA.2016.2576463", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An increasing number of entities are described by interlinked data rather\nthan documents on the Web. Entity Resolution (ER) aims to identify descriptions\nof the same real-world entity within one or across knowledge bases in the Web\nof data. To reduce the required number of pairwise comparisons among\ndescriptions, ER methods typically perform a pre-processing step, called\n\\emph{blocking}, which places similar entity descriptions into blocks and thus\nonly compare descriptions within the same block. We experimentally evaluate\nseveral blocking methods proposed for the Web of data using real datasets,\nwhose characteristics significantly impact their effectiveness and efficiency.\nThe proposed experimental evaluation framework allows us to better understand\nthe characteristics of the missed matching entity descriptions and contrast\nthem with ground truth obtained from different kinds of relatedness links.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 12:48:53 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Efthymiou", "Vasilis", ""], ["Stefanidis", "Kostas", ""], ["Christophides", "Vassilis", ""]]}, {"id": "2005.09617", "submitter": "Rajesh Bordawekar", "authors": "Apoorva Nitsure and Rajesh Bordawekar and Jose Neves", "title": "Unlocking New York City Crime Insights using Relational Database\n  Embeddings", "comments": "arXiv admin note: This version withdrawn by arXiv administrators\n  because the author did not have the right to agree to our license at the time\n  of submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This version withdrawn by arXiv administrators because the author did not\nhave the right to agree to our license at the time of submission.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 17:46:34 GMT"}, {"version": "v2", "created": "Wed, 20 May 2020 14:09:11 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Nitsure", "Apoorva", ""], ["Bordawekar", "Rajesh", ""], ["Neves", "Jose", ""]]}, {"id": "2005.09900", "submitter": "Deepak P", "authors": "Deepak P and Savitha Sam Abraham", "title": "Fair Outlier Detection", "comments": "In Proceedings of The 21th International Conference on Web\n  Information Systems Engineering (WISE 2020), Amsterdam and Leiden, The\n  Netherlands", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An outlier detection method may be considered fair over specified sensitive\nattributes if the results of outlier detection are not skewed towards\nparticular groups defined on such sensitive attributes. In this task, we\nconsider, for the first time to our best knowledge, the task of fair outlier\ndetection. In this work, we consider the task of fair outlier detection over\nmultiple multi-valued sensitive attributes (e.g., gender, race, religion,\nnationality, marital status etc.). We propose a fair outlier detection method,\nFairLOF, that is inspired by the popular LOF formulation for neighborhood-based\noutlier detection. We outline ways in which unfairness could be induced within\nLOF and develop three heuristic principles to enhance fairness, which form the\nbasis of the FairLOF method. Being a novel task, we develop an evaluation\nframework for fair outlier detection, and use that to benchmark FairLOF on\nquality and fairness of results. Through an extensive empirical evaluation over\nreal-world datasets, we illustrate that FairLOF is able to achieve significant\nimprovements in fairness at sometimes marginal degradations on result quality\nas measured against the fairness-agnostic LOF method.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 08:02:41 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 20:18:41 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["P", "Deepak", ""], ["Abraham", "Savitha Sam", ""]]}, {"id": "2005.09908", "submitter": "Chuan Xiao", "authors": "Yaoshu Wang, Chuan Xiao, Jianbin Qin, Rui Mao, Onizuka Makoto, Wei\n  Wang, Rui Zhang, Yoshiharu Ishikawa", "title": "Consistent and Flexible Selectivity Estimation for High-Dimensional Data", "comments": "Published at ACM SIGMOD Conference 2021", "journal-ref": null, "doi": "10.1145/3448016.3452772", "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selectivity estimation aims at estimating the number of database objects that\nsatisfy a selection criterion. Answering this problem accurately and\nefficiently is essential to many applications, such as density estimation,\noutlier detection, query optimization, and data integration. The estimation\nproblem is especially challenging for large-scale high-dimensional data due to\nthe curse of dimensionality, the large variance of selectivity across different\nqueries, and the need to make the estimator consistent (i.e., the selectivity\nis non-decreasing in the threshold). We propose a new deep learning-based model\nthat learns a query-dependent piecewise linear function as selectivity\nestimator, which is flexible to fit the selectivity curve of any distance\nfunction and query object, while guaranteeing that the output is non-decreasing\nin the threshold. To improve the accuracy for large datasets, we propose to\npartition the dataset into multiple disjoint subsets and build a local model on\neach of them. We perform experiments on real datasets and show that the\nproposed model consistently outperforms state-of-the-art models in accuracy in\nan efficient way and is useful for real applications.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 08:24:53 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 07:19:26 GMT"}, {"version": "v3", "created": "Sun, 7 Mar 2021 17:41:37 GMT"}, {"version": "v4", "created": "Thu, 27 May 2021 15:14:51 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Wang", "Yaoshu", ""], ["Xiao", "Chuan", ""], ["Qin", "Jianbin", ""], ["Mao", "Rui", ""], ["Makoto", "Onizuka", ""], ["Wang", "Wei", ""], ["Zhang", "Rui", ""], ["Ishikawa", "Yoshiharu", ""]]}, {"id": "2005.09998", "submitter": "Bram Aerts", "authors": "Bram Aerts, Simon Vandevelde, and Joost Vennekens", "title": "Tackling the DMN Challenges with cDMN: a Tight Integration of DMN and\n  constraint reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes an extension to the DMN standard, called cDMN. It aims\nto enlarge the expressivity of DMN in order to solve more complex problems,\nwhile retaining DMN's goal of being readable by domain experts. We test cDMN by\nsolving the most complex challenges posted on the DM Community website. We\ncompare our own cDMN solutions to the solutions that have been submitted to the\nwebsite and find that our approach is competitive, both in readability and\ncompactness. Moreover, cDMN is able to solve more challenges than any other\napproach.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 14:50:34 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Aerts", "Bram", ""], ["Vandevelde", "Simon", ""], ["Vennekens", "Joost", ""]]}, {"id": "2005.10486", "submitter": "Mahawaga Arachchige Pathum Chamikara", "authors": "M.A.P. Chamikara, P. Bertok, I. Khalil, D. Liu, S. Camtepe", "title": "Privacy Preserving Face Recognition Utilizing Differential Privacy", "comments": null, "journal-ref": null, "doi": "10.1016/j.cose.2020.101951", "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial recognition technologies are implemented in many areas, including but\nnot limited to, citizen surveillance, crime control, activity monitoring, and\nfacial expression evaluation. However, processing biometric information is a\nresource-intensive task that often involves third-party servers, which can be\naccessed by adversaries with malicious intent. Biometric information delivered\nto untrusted third-party servers in an uncontrolled manner can be considered a\nsignificant privacy leak (i.e. uncontrolled information release) as biometrics\ncan be correlated with sensitive data such as healthcare or financial records.\nIn this paper, we propose a privacy-preserving technique for \"controlled\ninformation release\", where we disguise an original face image and prevent\nleakage of the biometric features while identifying a person. We introduce a\nnew privacy-preserving face recognition protocol named PEEP (Privacy using\nEigEnface Perturbation) that utilizes local differential privacy. PEEP applies\nperturbation to Eigenfaces utilizing differential privacy and stores only the\nperturbed data in the third-party servers to run a standard Eigenface\nrecognition algorithm. As a result, the trained model will not be vulnerable to\nprivacy attacks such as membership inference and model memorization attacks.\nOur experiments show that PEEP exhibits a classification accuracy of around 70%\n- 90% under standard privacy settings.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 06:59:33 GMT"}, {"version": "v2", "created": "Sat, 4 Jul 2020 14:04:25 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Chamikara", "M. A. P.", ""], ["Bertok", "P.", ""], ["Khalil", "I.", ""], ["Liu", "D.", ""], ["Camtepe", "S.", ""]]}, {"id": "2005.10917", "submitter": "Shunsuke Kanda", "authors": "Shunsuke Kanda, Koh Takeuchi, Keisuke Fujii and Yasuo Tabei", "title": "Succinct Trit-array Trie for Scalable Trajectory Similarity Search", "comments": "Accepted by ACM SIGSPATIAL 2020 as a full paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive datasets of spatial trajectories representing the mobility of a\ndiversity of moving objects are ubiquitous in research and industry. Similarity\nsearch of a large collection of trajectories is indispensable for turning these\ndatasets into knowledge. Locality sensitive hashing (LSH) is a powerful\ntechnique for fast similarity searches. Recent methods employ LSH and attempt\nto realize an efficient similarity search of trajectories; however, those\nmethods are inefficient in terms of search time and memory when applied to\nmassive datasets. To address this problem, we present the trajectory-indexing\nsuccinct trit-array trie (tSTAT), which is a scalable method leveraging LSH for\ntrajectory similarity searches. tSTAT quickly performs the search on a tree\ndata structure called trie. We also present two novel techniques that enable to\ndramatically enhance the memory efficiency of tSTAT. One is a node reduction\ntechnique that substantially omits redundant trie nodes while maintaining the\ntime performance. The other is a space-efficient representation that leverages\nthe idea behind succinct data structures (i.e., a compressed data structure\nsupporting fast data operations). We experimentally test tSTAT on its ability\nto retrieve similar trajectories for a query from large collections of\ntrajectories and show that tSTAT performs superiorly in comparison to\nstate-of-the-art similarity search methods.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 21:42:30 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 21:01:47 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Kanda", "Shunsuke", ""], ["Takeuchi", "Koh", ""], ["Fujii", "Keisuke", ""], ["Tabei", "Yasuo", ""]]}, {"id": "2005.11259", "submitter": "Anna Queralt", "authors": "Rizkallah Touma, Anna Queralt, Toni Cortes", "title": "CAPre: Code-Analysis based Prefetching for Persistent Object Stores", "comments": null, "journal-ref": null, "doi": "10.1016/j.future.2019.10.023", "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data prefetching aims to improve access times to data storage systems by\npredicting data records that are likely to be accessed by subsequent requests\nand retrieving them into a memory cache before they are needed. In the case of\nPersistent Object Stores, previous approaches to prefetching have been based on\npredictions made through analysis of the store's schema, which generates rigid\npredictions, or monitoring access patterns to the store while applications are\nexecuted, which introduces memory and/or computation overhead. In this paper,\nwe present CAPre, a novel prefetching system for Persistent Object Stores based\non static code analysis of object-oriented applications. CAPre generates the\npredictions at compile-time and does not introduce any overhead to the\napplication execution. Moreover, CAPre is able to predict large amounts of\nobjects that will be accessed in the near future, thus enabling the object\nstore to perform parallel prefetching if the objects are distributed, in a much\nmore aggressive way than in schema-based prediction algorithms. We integrate\nCAPre into a distributed Persistent Object Store and run a series of\nexperiments that show that it can reduce the execution time of applications\nfrom 9% to over 50%, depending on the nature of the application and its\npersistent data model.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 16:16:40 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 17:51:32 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Touma", "Rizkallah", ""], ["Queralt", "Anna", ""], ["Cortes", "Toni", ""]]}, {"id": "2005.11264", "submitter": "Konstantina Bereta", "authors": "Konstantina Bereta, George Papadakis, Manolis Koubarakis", "title": "OBDA for the Web: Creating Virtual RDF Graphs On Top of Web Data Sources", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to Variety, Web data come in many different structures and formats, with\nHTML tables and REST APIs (e.g., social media APIs) being among the most\npopular ones. A big subset of Web data is also characterised by Velocity, as\ndata gets frequently updated so that consumers can obtain the most up-to-date\nversion of the respective datasets. At the moment, though, these data sources\nare not effectively supported by Semantic Web tools. To address variety and\nvelocity, we propose Ontop4theWeb, a system that maps Web data of various\nformats into virtual RDF triples, thus allowing for querying them on-the-fly\nwithout materializing them as RDF. We demonstrate how Ontop4theWeb can use\nSPARQL to uniformly query popular, but heterogeneous Web data sources, like\nHTML tables and Web APIs. We showcase our approach in a number of use cases,\nsuch as Twitter, Foursquare, Yelp and HTML tables. We carried out a thorough\nexperimental evaluation which verifies the high efficiency of our framework,\nwhich goes beyond the current state-of-the-art in this area, in terms of both\nfunctionality and performance.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 16:29:25 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Bereta", "Konstantina", ""], ["Papadakis", "George", ""], ["Koubarakis", "Manolis", ""]]}, {"id": "2005.11317", "submitter": "Mohsen Amini Salehi", "authors": "SM Zobaed, Raju Gottmukkala, Mohsen Amini Salehi", "title": "Privacy-Preserving Clustering of Unstructured Big Data for Cloud-Based\n  Enterprise Search Solutions", "comments": "arXiv admin note: text overlap with arXiv:1908.04960", "journal-ref": "ACM Transactions on Information Technology (ACM TOIT), May 2020", "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud-based enterprise search services (e.g., Amazon Kendra) are enchanting\nto big data owners by providing them with convenient search solutions over\ntheir enterprise big datasets. However, individuals and businesses that deal\nwith confidential big data (eg, credential documents) are reluctant to fully\nembrace such services, due to valid concerns about data privacy. Solutions\nbased on client-side encryption have been explored to mitigate privacy\nconcerns. Nonetheless, such solutions hinder data processing, specifically\nclustering, which is pivotal in dealing with different forms of big data. For\ninstance, clustering is critical to limit the search space and perform\nreal-time search operations on big datasets. To overcome the hindrance in\nclustering encrypted big data, we propose privacy-preserving clustering schemes\nfor three forms of unstructured encrypted big datasets, namely static,\nsemi-dynamic, and dynamic datasets. To preserve data privacy, the proposed\nclustering schemes function based on statistical characteristics of the data\nand determine (A) the suitable number of clusters and (B) appropriate content\nfor each cluster. Experimental results obtained from evaluating the clustering\nschemes on three different datasets demonstrate between 30% to 60% improvement\non the clusters' coherency compared to other clustering schemes for encrypted\ndata. Employing the clustering schemes in a privacy-preserving enterprise\nsearch system decreases its search time by up to 78%, while increases the\nsearch accuracy by up to 35%.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 08:42:19 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Zobaed", "SM", ""], ["Gottmukkala", "Raju", ""], ["Salehi", "Mohsen Amini", ""]]}, {"id": "2005.12271", "submitter": "Sally Elghamrawy Prof", "authors": "Eman A. Khashan, Ali I. Eldesouky, M. Fadel and Sally M. Elghamrawy", "title": "A Big Data Based Framework for Executing Complex Query Over COVID-19\n  Datasets (COVID-QF)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  COVID-19's rapid global spread has driven innovative tools for Big Data\nAnalytics. These have guided organizations in all fields of the health industry\nto track and minimized the effects of virus. Researchers are required to detect\ncoronaviruses through artificial intelligence, machine learning, and natural\nlanguage processing, and to gain a complete understanding of the disease.\nCOVID-19 takes place in different countries in the world, with which only big\ndata application and the work of NOSQL databases are suitable. There is a great\nnumber of platforms used for processing NOSQL Databases model like: Spark, H2O\nand Hadoop HDFS/MapReduce, which are proper to control and manage the enormous\namount of data. Many challenges faced by large applications programmers,\nespecially those that work on the COVID-19 databases through hybrid data models\nthrough different APIs and query. In this context, this paper proposes a\nstorage framework to handle both SQL and NOSQL databases named (COVID-QF) for\nCOVID-19 datasets in order to treat and handle the problems caused by virus\nspreading worldwide by reducing treatment times. In case of NoSQL database,\nCOVID-QF uses Hadoop HDFS/Map Reduce and Apache Spark. The COVID-QF consists of\nthree Layers: data collection layer, storage layer, and query Processing layer.\nThe data is collected in the data collection layer. The storage layer divides\ndata into collection of data-saving and processing blocks, and it connects the\nConnector of the spark with different databases engine to reduce time of saving\nand retrieving. While the Processing layer executes the request query and sends\nresults. The proposed framework used three datasets increased for time for\nCOVID-19 data (COVID-19-Merging, COVID-19-inside-Hubei and COVID-19-ex-Hubei)\nto test experiments of this study. The results obtained insure the superiority\nof the COVID-QF framework.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 03:13:00 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Khashan", "Eman A.", ""], ["Eldesouky", "Ali I.", ""], ["Fadel", "M.", ""], ["Elghamrawy", "Sally M.", ""]]}, {"id": "2005.12375", "submitter": "Sebastian Baumbach", "authors": "Sebastian Baumbach, Jahanzeb Khan, Sheraz Ahmed, and Andreas Dengel", "title": "QuViS -- The Question of Visual Site Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper present QuViS, which is an interactive platform for visualization\nand exploratory data analysis of site selection. The aim of QuViS is to support\ndecision makers and experts during the process of site selection. In addition\nto visualization engine for exploratory analysis, QuViS is also integrated with\nour automatic site selection method (QuIS), which recommend different sites\nautomatically based on the selected location factors by economists and experts.\nTo show the potential and highlight the visualization and exploration\ncapabilities of QuViS, a case study on 1,556 German supermarket site selection\nis performed. The real publicly available dataset contains 450 location factors\nfor all 11,162 multiplicities in Germany, covering the last 10-15 years. Case\nstudy results shows that QuViS provides an easy and intuitive way for\nexploratory analysis of geospatial multidimensional data.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 08:58:04 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Baumbach", "Sebastian", ""], ["Khan", "Jahanzeb", ""], ["Ahmed", "Sheraz", ""], ["Dengel", "Andreas", ""]]}, {"id": "2005.12489", "submitter": "Mengyu Ma", "authors": "Mengyu Ma, Ye Wu, Xue Ouyang, Luo Chen, Jun Li, Ning Jing", "title": "HiVision: Rapid Visualization of Large-Scale Spatial Vector Data", "comments": "19 pages 16 figures", "journal-ref": "Computers & Geosciences,2021,147(104665)", "doi": "10.1016/j.cageo.2020.104665", "report-no": null, "categories": "cs.GR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid visualization of large-scale spatial vector data is a long-standing\nchallenge in Geographic Information Science. In existing methods, the\ncomputation overheads grow rapidly with data volumes, leading to the\nincapability of providing real-time visualization for large-scale spatial\nvector data, even with parallel acceleration technologies. To fill the gap, we\npresent HiVision, a display-driven visualization model for large-scale spatial\nvector data. Different from traditional data-driven methods, the computing\nunits in HiVision are pixels rather than spatial objects to achieve real-time\nperformance, and efficient spatial-index-based strategies are introduced to\nestimate the topological relationships between pixels and spatial objects.\nHiVision can maintain exceedingly good performance regardless of the data\nvolume due to the stable pixel number for display. In addition, an optimized\nparallel computing architecture is proposed in HiVision to ensure the ability\nof real-time visualization. Experiments show that our approach outperforms\ntraditional methods in rendering speed and visual effects while dealing with\nlarge-scale spatial vector data, and can provide interactive visualization of\ndatasets with billion-scale points/segments/edges in real-time with flexible\nrendering styles. The HiVision code is open-sourced at\nhttps://github.com/MemoryMmy/HiVision with an online demonstration.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 02:48:40 GMT"}, {"version": "v2", "created": "Sun, 24 Jan 2021 05:53:02 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Ma", "Mengyu", ""], ["Wu", "Ye", ""], ["Ouyang", "Xue", ""], ["Chen", "Luo", ""], ["Li", "Jun", ""], ["Jing", "Ning", ""]]}, {"id": "2005.12873", "submitter": "Miyuru Dayarathna", "authors": "Miyuru Dayarathna and Toyotaro Suzumura", "title": "Benchmarking Graph Data Management and Processing Systems: A Survey", "comments": "26 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.PF cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of scalable, representative, and widely adopted benchmarks\nfor graph data systems have been a question for which answers has been sought\nfor decades. We conduct an in-depth study of the existing literature on\nbenchmarks for graph data management and processing, covering 20 different\nbenchmarks developed during the last 15 years. We categorize the benchmarks\ninto three areas focusing on benchmarks for graph processing systems, graph\ndatabase benchmarks, and bigdata benchmarks with graph processing workloads.\nThis systematic approach allows us to identify multiple issues existing in this\narea, including i) few benchmarks exist which can produce high workload\nscenarios, ii) no significant work done on benchmarking graph stream processing\nas well as graph based machine learning, iii) benchmarks tend to use\nconventional metrics despite new meaningful metrics have been around for years,\niv) increasing number of big data benchmarks appear with graph processing\nworkloads. Following these observations, we conclude the survey by describing\nkey challenges for future research on graph data systems benchmarking.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 17:07:29 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2020 17:35:16 GMT"}, {"version": "v3", "created": "Sun, 7 Jun 2020 17:17:53 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Dayarathna", "Miyuru", ""], ["Suzumura", "Toyotaro", ""]]}, {"id": "2005.13762", "submitter": "Maofan Yin", "authors": "Maofan Yin, Hongbo Zhang, Robbert van Renesse, Emin G\\\"un Sirer", "title": "CedrusDB: Persistent Key-Value Store with Memory-Mapped Lazy-Trie", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a result of RAM becoming cheaper, there has been a trend in key-value\nstore design towards maintaining a fast in-memory index (such as a hash table)\nwhile logging user operations to disk, allowing high performance under\nfailure-free conditions while still being able to recover from failures. This\ndesign, however, comes at the cost of long recovery times or expensive\ncheckpoint operations. This paper presents a new in-memory index that is also\nstorage-friendly. A \"lazy-trie\" is a variant of the hash-trie data structure\nthat achieves near-optimal height, has practical storage overhead, and can be\nmaintained on-disk with standard write-ahead logging.\n  We implemented CedrusDB, persistent key-value store based on a lazy-trie. The\nlazy-trie is kept on disk while made available in memory using standard\nmemory-mapping. The lazy-trie organization in virtual memory allows CedrusDB to\nbetter leverage concurrent processing than other on-disk index schemes (LSMs,\nB+-trees). CedrusDB achieves comparable or superior performance to recent\nlog-based in-memory key-value stores in mixed workloads while being able to\nrecover quickly from failures.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 03:26:02 GMT"}, {"version": "v2", "created": "Sat, 12 Dec 2020 04:49:58 GMT"}, {"version": "v3", "created": "Wed, 21 Jul 2021 20:13:46 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Yin", "Maofan", ""], ["Zhang", "Hongbo", ""], ["van Renesse", "Robbert", ""], ["Sirer", "Emin G\u00fcn", ""]]}, {"id": "2005.14068", "submitter": "Reza Karegar", "authors": "Reza Karegar, Melicaalsadat Mirsafian, Parke Godfrey, Lukasz Golab,\n  Mehdi Kargar, Divesh Srivastava, Jaroslaw Szlichta", "title": "Discovering Domain Orders through Order Dependencies", "comments": "Revisions based on reviewer feedback", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much real-world data come with explicitly defined domain orders; e.g.,\nlexicographic order for strings, numeric for integers, and chronological for\ntime. Our goal is to discover implicit domain orders that we do not already\nknow; for instance, that the order of months in the Chinese Lunar calendar is\nCorner < Apricot < Peach. To do so, we enhance data profiling methods by\ndiscovering implicit domain orders in data through order dependencies. We\nenumerate tractable special cases and proceed towards the most general case,\nwhich we prove is NP-complete. We show that the general case nevertheless can\nbe effectively handled by a SAT solver. We also devise an interestingness\nmeasure to rank the discovered implicit domain orders, which we validate with a\nuser study. Based on an extensive suite of experiments with real-world data, we\nestablish the efficacy of our algorithms, and the utility of the domain orders\ndiscovered by demonstrating significant added value in three applications (data\nprofiling, query optimization, and data mining).\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 15:12:44 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 00:06:15 GMT"}, {"version": "v3", "created": "Mon, 12 Oct 2020 17:35:00 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Karegar", "Reza", ""], ["Mirsafian", "Melicaalsadat", ""], ["Godfrey", "Parke", ""], ["Golab", "Lukasz", ""], ["Kargar", "Mehdi", ""], ["Srivastava", "Divesh", ""], ["Szlichta", "Jaroslaw", ""]]}, {"id": "2005.14213", "submitter": "Yien Xu", "authors": "Yifan Dai, Yien Xu, Aishwarya Ganesan, Ramnatthan Alagappan, Brian\n  Kroth, Andrea C. Arpaci-Dusseau, and Remzi H. Arpaci-Dusseau", "title": "From WiscKey to Bourbon: A Learned Index for Log-Structured Merge Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce BOURBON, a log-structured merge (LSM) tree that utilizes machine\nlearning to provide fast lookups. We base the design and implementation of\nBOURBON on empirically-grounded principles that we derive through careful\nanalysis of LSM design. BOURBON employs greedy piecewise linear regression to\nlearn key distributions, enabling fast lookup with minimal computation, and\napplies a cost-benefit strategy to decide when learning will be worthwhile.\nThrough a series of experiments on both synthetic and real-world datasets, we\nshow that BOURBON improves lookup performance by 1.23x-1.78x as compared to\nstate-of-the-art production LSMs.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 18:05:46 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 18:09:20 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Dai", "Yifan", ""], ["Xu", "Yien", ""], ["Ganesan", "Aishwarya", ""], ["Alagappan", "Ramnatthan", ""], ["Kroth", "Brian", ""], ["Arpaci-Dusseau", "Andrea C.", ""], ["Arpaci-Dusseau", "Remzi H.", ""]]}, {"id": "2005.14326", "submitter": "Donatella Firmani", "authors": "Sainyam Galhotra, Donatella Firmani, Barna Saha, Divesh Srivastava", "title": "Efficient and Effective ER with Progressive Blocking", "comments": "Galhotra, S., Firmani, D., Saha, B. et al. Efficient and effective ER\n  with progressive blocking. The VLDB Journal (2021)", "journal-ref": null, "doi": "10.1007/s00778-021-00656-7", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blocking is a mechanism to improve the efficiency of Entity Resolution (ER)\nwhich aims to quickly prune out all non-matching record pairs. However,\ndepending on the distributions of entity cluster sizes, existing techniques can\nbe either (a) too aggressive, such that they help scale but can adversely\naffect the ER effectiveness, or (b) too permissive, potentially harming ER\nefficiency. In this paper, we propose a new methodology of progressive blocking\n(pBlocking) to enable both efficient and effective ER, which works seamlessly\nacross different entity cluster size distributions. pBlocking is based on the\ninsight that the effectiveness-efficiency trade-off is revealed only when the\noutput of ER starts to be available. Hence, pBlocking leverages partial ER\noutput in a feedback loop to refine the blocking result in a data-driven\nfashion. Specifically, we bootstrap pBlocking with traditional blocking methods\nand progressively improve the building and scoring of blocks until we get the\ndesired trade-off, leveraging a limited amount of ER results as a guidance at\nevery round. We formally prove that pBlocking converges efficiently ($O(n log^2\nn)$ time complexity, where n is the total number of records). Our experiments\nshow that incorporating partial ER output in a feedback loop can improve the\nefficiency and effectiveness of blocking by 5x and 60% respectively, improving\nthe overall F-score of the entire ER process up to 60%.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 22:30:23 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 08:31:30 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Galhotra", "Sainyam", ""], ["Firmani", "Donatella", ""], ["Saha", "Barna", ""], ["Srivastava", "Divesh", ""]]}, {"id": "2005.14493", "submitter": "Yanhao Wang", "authors": "Yanhao Wang, Yuchen Li, Raymond Chi-Wing Wong, Kian-Lee Tan", "title": "A Fully Dynamic Algorithm for k-Regret Minimizing Sets", "comments": "15 pages, 11 figures; to appear in ICDE 2021", "journal-ref": null, "doi": "10.1109/ICDE51399.2021.00144", "report-no": null, "categories": "cs.DB cs.DS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Selecting a small set of representatives from a large database is important\nin many applications such as multi-criteria decision making, web search, and\nrecommendation. The $k$-regret minimizing set ($k$-RMS) problem was recently\nproposed for representative tuple discovery. Specifically, for a large database\n$P$ of tuples with multiple numerical attributes, the $k$-RMS problem returns a\nsize-$r$ subset $Q$ of $P$ such that, for any possible ranking function, the\nscore of the top-ranked tuple in $Q$ is not much worse than the score of the\n$k$\\textsuperscript{th}-ranked tuple in $P$. Although the $k$-RMS problem has\nbeen extensively studied in the literature, existing methods are designed for\nthe static setting and cannot maintain the result efficiently when the database\nis updated. To address this issue, we propose the first fully-dynamic algorithm\nfor the $k$-RMS problem that can efficiently provide the up-to-date result\nw.r.t.~any insertion and deletion in the database with a provable guarantee.\nExperimental results on several real-world and synthetic datasets demonstrate\nthat our algorithm runs up to four orders of magnitude faster than existing\n$k$-RMS algorithms while returning results of nearly equal quality.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 10:36:15 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 06:21:27 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2020 09:02:20 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Wang", "Yanhao", ""], ["Li", "Yuchen", ""], ["Wong", "Raymond Chi-Wing", ""], ["Tan", "Kian-Lee", ""]]}, {"id": "2005.14552", "submitter": "Dirk Fahland", "authors": "Stefan Esser, Dirk Fahland", "title": "Multi-Dimensional Event Data in Graph Databases", "comments": "Accepted at Journal of Data Semantics. Final reviewed manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process event data is usually stored either in a sequential process event log\nor in a relational database. While the sequential, single-dimensional nature of\nevent logs aids querying for (sub)sequences of events based on temporal\nrelations such as \"directly/eventually-follows\", it does not support querying\nmulti-dimensional event data of multiple related entities. Relational databases\nallow storing multi-dimensional event data but existing query languages do not\nsupport querying for sequences or paths of events in terms of temporal\nrelations. In this paper, we propose a general data model for multi-dimensional\nevent data based on labeled property graphs that allows storing structural and\ntemporal relations in a single, integrated graph-based data structure in a\nsystematic way. We provide semantics for all concepts of our data model, and\ngeneric queries for modeling event data over multiple entities that interact\nsynchronously and asynchronously . The queries allow for efficiently converting\nlarge real-life event data sets into our data model and we provide 5 converted\ndata sets for further research. We show that typical and advanced queries for\nretrieving and aggregating such multidimensional event data can be formulated\nand executed efficiently in the existing query language Cypher, giving rise to\nseveral new research questions. Specifically aggregation queries on our data\nmodel enable process mining over multiple interrelated entities using\noff-the-shelf technology.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 12:59:09 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2021 09:36:33 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Esser", "Stefan", ""], ["Fahland", "Dirk", ""]]}]