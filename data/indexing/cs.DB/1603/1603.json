[{"id": "1603.00213", "submitter": "Arnab Bhattacharyya", "authors": "Arnab Bhattacharyya and Palash Dey and David P. Woodruff", "title": "An Optimal Algorithm for l1-Heavy Hitters in Insertion Streams and\n  Related Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the first optimal bounds for returning the $\\ell_1$-heavy hitters in\na data stream of insertions, together with their approximate frequencies,\nclosing a long line of work on this problem. For a stream of $m$ items in $\\{1,\n2, \\dots, n\\}$ and parameters $0 < \\epsilon < \\phi \\leq 1$, let $f_i$ denote\nthe frequency of item $i$, i.e., the number of times item $i$ occurs in the\nstream. With arbitrarily large constant probability, our algorithm returns all\nitems $i$ for which $f_i \\geq \\phi m$, returns no items $j$ for which $f_j \\leq\n(\\phi -\\epsilon)m$, and returns approximations $\\tilde{f}_i$ with $|\\tilde{f}_i\n- f_i| \\leq \\epsilon m$ for each item $i$ that it returns. Our algorithm uses\n$O(\\epsilon^{-1} \\log\\phi^{-1} + \\phi^{-1} \\log n + \\log \\log m)$ bits of\nspace, processes each stream update in $O(1)$ worst-case time, and can report\nits output in time linear in the output size. We also prove a lower bound,\nwhich implies that our algorithm is optimal up to a constant factor in its\nspace complexity. A modification of our algorithm can be used to estimate the\nmaximum frequency up to an additive $\\epsilon m$ error in the above amount of\nspace, resolving Question 3 in the IITK 2006 Workshop on Algorithms for Data\nStreams for the case of $\\ell_1$-heavy hitters. We also introduce several\nvariants of the heavy hitters and maximum frequency problems, inspired by rank\naggregation and voting schemes, and show how our techniques can be applied in\nsuch settings. Unlike the traditional heavy hitters problem, some of these\nvariants look at comparisons between items rather than numerical values to\ndetermine the frequency of an item.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 10:18:20 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Bhattacharyya", "Arnab", ""], ["Dey", "Palash", ""], ["Woodruff", "David P.", ""]]}, {"id": "1603.00400", "submitter": "Immanuel Trummer Mr.", "authors": "Immanuel Trummer and Christoph Koch", "title": "A Fast Randomized Algorithm for Multi-Objective Query Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query plans are compared according to multiple cost metrics in\nmulti-objective query optimization. The goal is to find the set of Pareto plans\nrealizing optimal cost tradeoffs for a given query. So far, only algorithms\nwith exponential complexity in the number of query tables have been proposed\nfor multi-objective query optimization. In this work, we present the first\nalgorithm with polynomial complexity in the query size.\n  Our algorithm is randomized and iterative. It improves query plans via a\nmulti-objective version of hill climbing that applies multiple transformations\nin each climbing step for maximal efficiency. Based on a locally optimal plan,\nwe approximate the Pareto plan set within the restricted space of plans with\nsimilar join orders. We maintain a cache of Pareto-optimal plans for each\npotentially useful intermediate result to share partial plans that were\ndiscovered in different iterations. We show that each iteration of our\nalgorithm performs in expected polynomial time based on an analysis of the\nexpected path length between a random plan and local optima reached by hill\nclimbing. We experimentally show that our algorithm can optimize queries with\nhundreds of tables and outperforms other randomized algorithms such as the\nNSGA-II genetic algorithm over a wide range of scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 18:55:04 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Trummer", "Immanuel", ""], ["Koch", "Christoph", ""]]}, {"id": "1603.00542", "submitter": "Mohammad Dashti", "authors": "Mohammad Dashti, Sachin Basil John, Amir Shaikhha, Christoph Koch", "title": "Repairing Conflicts among MVCC Transactions", "comments": "12 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The optimistic variants of MVCC (Multi-Version Concurrency Control) avoid\nblocking concurrent transactions at the cost of having a validation phase. Upon\nfailure in the validation phase, the transaction is usually aborted and\nrestarted from scratch. The \"abort and restart\" approach becomes a performance\nbottleneck for the use cases with high contention objects or long running\ntransactions. In addition, restarting from scratch creates a negative feedback\nloop in the system, because the system incurs additional overhead that may\ncreate even further conflicts.\n  In this paper, we propose a novel approach for conflict resolution in MVCC\nfor in-memory databases. This low overhead approach summarizes the transaction\nprograms in the form of a dependency graph. The dependency graph also contains\nthe constructs used in the validation phase of the MVCC algorithm. Then, in the\ncase of encountering conflicts among transactions, the conflict locations in\nthe program are quickly detected, and the conflicting transactions are\npartially re-executed. This approach maximizes the reuse of the computations\ndone in the initial execution round, and increases the transaction processing\nthroughput.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 01:18:03 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Dashti", "Mohammad", ""], ["John", "Sachin Basil", ""], ["Shaikhha", "Amir", ""], ["Koch", "Christoph", ""]]}, {"id": "1603.00567", "submitter": "Peter Bailis", "authors": "Peter Bailis and Edward Gan and Samuel Madden and Deepak Narayanan and\n  Kexin Rong and Sahaana Suri", "title": "MacroBase: Prioritizing Attention in Fast Data", "comments": "SIGMOD 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  As data volumes continue to rise, manual inspection is becoming increasingly\nuntenable. In response, we present MacroBase, a data analytics engine that\nprioritizes end-user attention in high-volume fast data streams. MacroBase\nenables efficient, accurate, and modular analyses that highlight and aggregate\nimportant and unusual behavior, acting as a search engine for fast data.\nMacroBase is able to deliver order-of-magnitude speedups over alternatives by\noptimizing the combination of explanation and classification tasks and by\nleveraging a new reservoir sampler and heavy-hitters sketch specialized for\nfast data streams. As a result, MacroBase delivers accurate results at speeds\nof up to 2M events per second per query on a single core. The system has\ndelivered meaningful results in production, including at a telematics company\nmonitoring hundreds of thousands of vehicles.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 03:40:41 GMT"}, {"version": "v2", "created": "Thu, 17 Mar 2016 04:25:19 GMT"}, {"version": "v3", "created": "Sat, 23 Jul 2016 01:38:00 GMT"}, {"version": "v4", "created": "Sat, 25 Mar 2017 00:11:18 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Bailis", "Peter", ""], ["Gan", "Edward", ""], ["Madden", "Samuel", ""], ["Narayanan", "Deepak", ""], ["Rong", "Kexin", ""], ["Suri", "Sahaana", ""]]}, {"id": "1603.00658", "submitter": "Manjunatha Praveen", "authors": "M. Praveen, B. Srivathsan", "title": "Nesting Depth of Operators in Graph Database Queries: Expressiveness Vs.\n  Evaluation Complexity", "comments": "Improvements from ICALP 2016 review comments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DB cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing query languages for graph structured data is an active field of\nresearch, where expressiveness and efficient algorithms for query evaluation\nare conflicting goals. To better handle dynamically changing data, recent work\nhas been done on designing query languages that can compare values stored in\nthe graph database, without hard coding the values in the query. The main idea\nis to allow variables in the query and bind the variables to values when\nevaluating the query. For query languages that bind variables only once, query\nevaluation is usually NP-complete. There are query languages that allow binding\ninside the scope of Kleene star operators, which can themselves be in the scope\nof bindings and so on. Uncontrolled nesting of binding and iteration within one\nanother results in query evaluation being PSPACE-complete.\n  We define a way to syntactically control the nesting depth of iterated\nbindings, and study how this affects expressiveness and efficiency of query\nevaluation. The result is an infinite, syntactically defined hierarchy of\nexpressions. We prove that the corresponding language hierarchy is strict.\nGiven an expression in the hierarchy, we prove that it is undecidable to check\nif there is a language equivalent expression at lower levels. We prove that\nevaluating a query based on an expression at level i can be done in $\\Sigma_i$\nin the polynomial time hierarchy. Satisfiability of quantified Boolean formulas\ncan be reduced to query evaluation; we study the relationship between\nalternations in Boolean quantifiers and the depth of nesting of iterated\nbindings.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 11:16:37 GMT"}, {"version": "v2", "created": "Thu, 5 May 2016 06:50:56 GMT"}], "update_date": "2016-05-06", "authors_parsed": [["Praveen", "M.", ""], ["Srivathsan", "B.", ""]]}, {"id": "1603.00893", "submitter": "Boxiang Dong", "authors": "Boxiang Dong, Hui Wendy Wang", "title": "Frequency-hiding Dependency-preserving Encryption for Outsourced\n  Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cloud paradigm enables users to outsource their data to computationally\npowerful third-party service providers for data management. Many data\nmanagement tasks rely on the data dependencies in the outsourced data. This\nraises an important issue of how the data owner can protect the sensitive\ninformation in the outsourced data while preserving the data dependencies. In\nthis paper, we consider functional dependency FD, an important type of data\ndependency. We design a FD-preserving encryption scheme, named F2, that enables\nthe service provider to discover the FDs from the encrypted dataset. We\nconsider the frequency analysis attack, and show that the F2 encryption scheme\ncan defend against the attack under Kerckhoff's principle with provable\nguarantee. Our empirical study demonstrates the efficiency and effectiveness of\nF2.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 21:20:16 GMT"}, {"version": "v2", "created": "Tue, 8 Mar 2016 15:16:00 GMT"}], "update_date": "2016-03-09", "authors_parsed": [["Dong", "Boxiang", ""], ["Wang", "Hui Wendy", ""]]}, {"id": "1603.01102", "submitter": "Ivan Ponomarev", "authors": "I. N. Ponomarev (Moscow Institute of Physics and Technology)", "title": "Implementation of the fast table grid user interface element for working\n  with large database tables", "comments": "10 pages, 3 figures, in Russian", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Table grid user interface element with a vertical scrollbar is a standard way\nof working with database table records. There are two basic operations each\ngrid should support: scrolling records with vertical scrollbar and positioning\nto a record with a given primary key. This paper addresses the case when the\nnumber of records is so large that it is not feasible to load them all into\nmemory, and database functions like \"select..offset\" work insufficiently fast\nand put undue load on RDBMS. Our main idea is to use only queries that involve\nindex lookup (a O(Log(N)-fast operation) and to use statistic properties of\nhypergeometric distribution to \"guess\" primary keys of records given their\nordinal numbers. The proposed method allows us to implement a grid with\nO(Log(N))-fast scrolling and positioning performance.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 14:01:04 GMT"}], "update_date": "2016-03-04", "authors_parsed": [["Ponomarev", "I. N.", "", "Moscow Institute of Physics and Technology"]]}, {"id": "1603.01113", "submitter": "Sabri Pllana", "authors": "Sabri Pllana, Ivan Janciak, Peter Brezany, Alexander W\\\"ohrer", "title": "A Survey of the State of the Art in Data Mining and Integration Query\n  Languages", "comments": "2011 International Conference on Network-Based Information Systems", "journal-ref": null, "doi": "10.1109/NBiS.2011.58", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The major aim of this survey is to identify the strengths and weaknesses of a\nrepresentative set of Data-Mining and Integration (DMI) query languages. We\ndescribe a set of properties of DMI-related languages that we use for a\nsystematic evaluation of these languages. In addition, we introduce a scoring\nsystem that we use to quantify our opinion on how well a DMI-related language\nsupports a property. The languages surveyed in this paper include: DMQL,\nMineSQL, MSQL, M2MQL, dmFSQL, OLEDB for DM, MINE RULE, and Oracle Data Mining.\nThis survey may help researchers to propose a DMI language that is beyond the\nstate-of-the-art, or it may help practitioners to select an existing language\nthat fits well a purpose.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 14:44:07 GMT"}], "update_date": "2016-03-04", "authors_parsed": [["Pllana", "Sabri", ""], ["Janciak", "Ivan", ""], ["Brezany", "Peter", ""], ["W\u00f6hrer", "Alexander", ""]]}, {"id": "1603.01682", "submitter": "Rameshwar Pratap", "authors": "Debajyoti Bera, Rameshwar Pratap", "title": "Frequent-Itemset Mining using Locality-Sensitive Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Apriori algorithm is a classical algorithm for the frequent itemset\nmining problem. A significant bottleneck in Apriori is the number of I/O\noperation involved, and the number of candidates it generates. We investigate\nthe role of LSH techniques to overcome these problems, without adding much\ncomputational overhead. We propose randomized variations of Apriori that are\nbased on asymmetric LSH defined over Hamming distance and Jaccard similarity.\n", "versions": [{"version": "v1", "created": "Sat, 5 Mar 2016 05:39:46 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Bera", "Debajyoti", ""], ["Pratap", "Rameshwar", ""]]}, {"id": "1603.01820", "submitter": "Liat Peterfreund", "authors": "Benny Kimelfeld, Ester Livshits, Liat Peterfreund", "title": "Unambiguous Prioritized Repairing of Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In its traditional definition, a repair of an inconsistent database is a\nconsistent database that differs from the inconsistent one in a \"minimal way\".\nOften, repairs are not equally legitimate, as it is desired to prefer one over\nanother; for example, one fact is regarded more reliable than another, or a\nmore recent fact should be preferred to an earlier one. Motivated by these\nconsiderations, researchers have introduced and investigated the framework of\npreferred repairs, in the context of denial constraints and subset repairs.\nThere, a priority relation between facts is lifted towards a priority relation\nbetween consistent databases, and repairs are restricted to the ones that are\noptimal in the lifted sense. Three notions of lifting (and optimal repairs)\nhave been proposed: Pareto, global, and completion.\n  In this paper we investigate the complexity of deciding whether the priority\nrelation suffices to clean the database unambiguously, or in other words,\nwhether there is exactly one optimal repair. We show that the different lifting\nsemantics entail highly different complexities. Under Pareto optimality, the\nproblem is coNP-complete, in data complexity, for every set of functional\ndependencies (FDs), except for the tractable case of (equivalence to) one FD\nper relation. Under global optimality, one FD per relation is still tractable,\nbut we establish $\\Pi^{p}_{2}$-completeness for a relation with two FDs. In\ncontrast, under completion optimality the problem is solvable in polynomial\ntime for every set of FDs. In fact, we present a polynomial-time algorithm for\narbitrary conflict hypergraphs. We further show that under a general assumption\nof transitivity, this algorithm solves the problem even for global optimality.\nThe algorithm is extremely simple, but its proof of correctness is quite\nintricate.\n", "versions": [{"version": "v1", "created": "Sun, 6 Mar 2016 12:44:54 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Kimelfeld", "Benny", ""], ["Livshits", "Ester", ""], ["Peterfreund", "Liat", ""]]}, {"id": "1603.02056", "submitter": "Wenqiang Liu", "authors": "Wenqiang Liu, Jun Liu, Jian Zhang, Haimeng Duan, Bifan Wei", "title": "TruthDiscover: Resolving Object Conflicts on Massive Linked Data", "comments": "This paper had been accepted by Proceedings of the 26th International\n  Conference on World Wide Web Companion. International World Wide Web\n  Conferences Steering Committee, 2017, WWW2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Considerable effort has been made to increase the scale of Linked Data.\nHowever, because of the openness of the Semantic Web and the ease of extracting\nLinked Data from semi-structured sources (e.g., Wikipedia) and unstructured\nsources, many Linked Data sources often provide conflicting objects for a\ncertain predicate of a real-world entity. Existing methods cannot be trivially\nextended to resolve conflicts in Linked Data because Linked Data has a\nscale-free property. In this demonstration, we present a novel system called\nTruthDiscover, to identify the truth in Linked Data with a scale-free property.\nFirst, TruthDiscover leverages the topological properties of the Source Belief\nGraph to estimate the priori beliefs of sources, which are utilized to smooth\nthe trustworthiness of sources. Second, the Hidden Markov Random Field is\nutilized to model interdependencies among objects for estimating the trust\nvalues of objects accurately. TruthDiscover can visualize the process of\nresolving conflicts in Linked Data. Experiments results on four datasets show\nthat TruthDiscover exhibits satisfactory accuracy when confronted with data\nhaving a scale-free property.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 13:34:36 GMT"}, {"version": "v2", "created": "Fri, 21 Apr 2017 22:46:20 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Liu", "Wenqiang", ""], ["Liu", "Jun", ""], ["Zhang", "Jian", ""], ["Duan", "Haimeng", ""], ["Wei", "Bifan", ""]]}, {"id": "1603.02371", "submitter": "Minsuk Kahng", "authors": "Minsuk Kahng, Shamkant B. Navathe, John T. Stasko, Duen Horng Chau", "title": "Interactive Browsing and Navigation in Relational Databases", "comments": "VLDB 2016", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 9, No. 12, pp.\n  1017-1028, 2016", "doi": "10.14778/2994509.2994520", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although researchers have devoted considerable attention to helping database\nusers formulate queries, many users still find it challenging to specify\nqueries that involve joining tables. To help users construct join queries for\nexploring relational databases, we propose ETable, a novel presentation data\nmodel that provides users with a presentation-level interactive view. This view\ncompactly presents one-to-many and many-to-many relationships within a single\nenriched table by allowing a cell to contain a set of entity references. Users\ncan directly interact with this enriched table to incrementally construct\ncomplex queries and navigate databases on a conceptual entity-relationship\nlevel. In a user study, participants performed a range of database querying\ntasks faster with ETable than with a commercial graphical query builder.\nSubjective feedback about ETable was also positive. All participants found that\nETable was easier to learn and helpful for exploring databases.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 03:41:34 GMT"}, {"version": "v2", "created": "Sun, 21 Aug 2016 02:18:33 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Kahng", "Minsuk", ""], ["Navathe", "Shamkant B.", ""], ["Stasko", "John T.", ""], ["Chau", "Duen Horng", ""]]}, {"id": "1603.02705", "submitter": "Babak Salimi", "authors": "Babak Salimi, Leopoldo Bertossi, Dan Suciu and Guy Van den Broeck", "title": "Quantifying Causal Effects on Query Answering in Databases", "comments": "To appear in Proc. TAPP'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of actual causation, as formalized by Halpern and Pearl, has been\nrecently applied to relational databases, to characterize and compute actual\ncauses for possibly unexpected answers to monotone queries. Causes take the\nform of database tuples, and can be ranked according to their causal\nresponsibility, a numerical measure of their relevance as a cause to the query\nanswer. In this work we revisit this notion, introducing and making a case for\nan alternative measure of causal contribution, that of causal effect. The\nmeasure generalizes actual causes, and can be applied beyond monotone queries.\nWe show that causal effect provides intuitive and intended results.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 21:11:25 GMT"}, {"version": "v2", "created": "Mon, 25 Apr 2016 00:29:19 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Salimi", "Babak", ""], ["Bertossi", "Leopoldo", ""], ["Suciu", "Dan", ""], ["Broeck", "Guy Van den", ""]]}, {"id": "1603.02727", "submitter": "Boxiang Dong", "authors": "Boxiang Dong, Hui Wang", "title": "Efficient Authentication of Outsourced String Similarity Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing enables the outsourcing of big data analytics, where a third\nparty server is responsible for data storage and processing. In this paper, we\nconsider the outsourcing model that provides string similarity search as the\nservice. In particular, given a similarity search query, the service provider\nreturns all strings from the outsourced dataset that are similar to the query\nstring. A major security concern of the outsourcing paradigm is to authenticate\nwhether the service provider returns sound and complete search results. In this\npaper, we design AutoS3, an authentication mechanism of outsourced string\nsimilarity search. The key idea of AutoS3 is that the server returns a\nverification object VO to prove the result correctness. First, we design an\nauthenticated string indexing structure named MBtree for VO construction.\nSecond, we design two lightweight authentication methods named VS2 and EVS2\nthat can catch the service provider various cheating behaviors with cheap\nverification cost. Moreover, we generalize our solution for top k string\nsimilarity search. We perform an extensive set of experiment results on real\nworld datasets to demonstrate the efficiency of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 22:40:41 GMT"}], "update_date": "2016-04-04", "authors_parsed": [["Dong", "Boxiang", ""], ["Wang", "Hui", ""]]}, {"id": "1603.03281", "submitter": "UshaRani Yelipe", "authors": "Yelipe UshaRani, P. Sammulal", "title": "An Innovative Imputation and Classification Approach for Accurate\n  Disease Prediction", "comments": "Special Issue of Journal IJCSIS indexed in Web of Science and Thomson\n  Reuters ISI. https://sites.google.com/site/ijcsis/vol-14-s1-feb-2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Imputation of missing attribute values in medical datasets for extracting\nhidden knowledge from medical datasets is an interesting research topic of\ninterest which is very challenging. One cannot eliminate missing values in\nmedical records. The reason may be because some tests may not been conducted as\nthey are cost effective, values missed when conducting clinical trials, values\nmay not have been recorded to name some of the reasons. Data mining researchers\nhave been proposing various approaches to find and impute missing values to\nincrease classification accuracies so that disease may be predicted accurately.\nIn this paper, we propose a novel imputation approach for imputation of missing\nvalues and performing classification after fixing missing values. The approach\nis based on clustering concept and aims at dimensionality reduction of the\nrecords. The case study discussed shows that missing values can be fixed and\nimputed efficiently by achieving dimensionality reduction. The importance of\nproposed approach for classification is visible in the case study which assigns\nsingle class label in contrary to multi-label assignment if dimensionality\nreduction is not performed.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 14:31:33 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["UshaRani", "Yelipe", ""], ["Sammulal", "P.", ""]]}, {"id": "1603.03884", "submitter": "Torsten Schaub", "authors": "Martin Gebser and Roland Kaminski and Torsten Schaub", "title": "Grounding Recursive Aggregates: Preliminary Report", "comments": "21 pages, 7 figures, preliminary version appeared at GTTV'15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Problem solving in Answer Set Programming consists of two steps, a first\ngrounding phase, systematically replacing all variables by terms, and a second\nsolving phase computing the stable models of the obtained ground program. An\nintricate part of both phases is the treatment of aggregates, which are popular\nlanguage constructs that allow for expressing properties over sets. In this\npaper, we elaborate upon the treatment of aggregates during grounding in Gringo\nseries 4. Consequently, our approach is applicable to grounding based on\nsemi-naive database evaluation techniques. In particular, we provide a series\nof algorithms detailing the treatment of recursive aggregates and illustrate\nthis by a running example.\n", "versions": [{"version": "v1", "created": "Sat, 12 Mar 2016 10:22:13 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Gebser", "Martin", ""], ["Kaminski", "Roland", ""], ["Schaub", "Torsten", ""]]}, {"id": "1603.04068", "submitter": "Benjamin McCamish", "authors": "Ben McCamish, Vahid Ghadakchi, Arash Termehchy and Behrouz Touri", "title": "A Signaling Game Approach to Databases Querying and Interaction", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As most users do not precisely know the structure and/or the content of\ndatabases, their queries do not exactly reflect their information needs. The\ndatabase management systems (DBMS) may interact with users and use their\nfeedback on the returned results to learn the information needs behind their\nqueries. Current query interfaces assume that users do not learn and modify the\nway way they express their information needs in form of queries during their\ninteraction with the DBMS. Using a real-world interaction workload, we show\nthat users learn and modify how to express their information needs during their\ninteractions with the DBMS and their learning is accurately modeled by a\nwell-known reinforcement learning mechanism. As current data interaction\nsystems assume that users do not modify their strategies, they cannot discover\nthe information needs behind users' queries effectively. We model the\ninteraction between users and DBMS as a game with identical interest between\ntwo rational agents whose goal is to establish a common language for\nrepresenting information needs in form of queries. We propose a reinforcement\nlearning method that learns and answers the information needs behind queries\nand adapts to the changes in users' strategies and prove that it improves the\neffectiveness of answering queries stochastically speaking. We propose two\nefficient implementation of this method over large relational databases. Our\nextensive empirical studies over real-world query workloads indicate that our\nalgorithms are efficient and effective.\n", "versions": [{"version": "v1", "created": "Sun, 13 Mar 2016 19:28:22 GMT"}, {"version": "v2", "created": "Wed, 22 Jun 2016 14:18:52 GMT"}, {"version": "v3", "created": "Sat, 27 May 2017 22:04:56 GMT"}, {"version": "v4", "created": "Thu, 22 Jun 2017 17:04:16 GMT"}, {"version": "v5", "created": "Fri, 4 May 2018 21:33:26 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["McCamish", "Ben", ""], ["Ghadakchi", "Vahid", ""], ["Termehchy", "Arash", ""], ["Touri", "Behrouz", ""]]}, {"id": "1603.04626", "submitter": "Hugo Firth", "authors": "Hugo Firth and Paolo Missier", "title": "TAPER: query-aware, partition-enhancement for large, heterogenous,\n  graphs", "comments": "12 pages, 11 figures, unpublished", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph partitioning has long been seen as a viable approach to address Graph\nDBMS scalability. A partitioning, however, may introduce extra query processing\nlatency unless it is sensitive to a specific query workload, and optimised to\nminimise inter-partition traversals for that workload. Additionally, it should\nalso be possible to incrementally adjust the partitioning in reaction to\nchanges in the graph topology, the query workload, or both. Because of their\ncomplexity, current partitioning algorithms fall short of one or both of these\nrequirements, as they are designed for offline use and as one-off operations.\nThe TAPER system aims to address both requirements, whilst leveraging existing\npartitioning algorithms. TAPER takes any given initial partitioning as a\nstarting point, and iteratively adjusts it by swapping chosen vertices across\npartitions, heuristically reducing the probability of inter-partition\ntraversals for a given pattern matching queries workload. Iterations are\ninexpensive thanks to time and space optimisations in the underlying support\ndata structures. We evaluate TAPER on two different large test graphs and over\nrealistic query workloads. Our results indicate that, given a hash-based\npartitioning, TAPER reduces the number of inter-partition traversals by around\n80%; given an unweighted METIS partitioning, by around 30%. These reductions\nare achieved within 8 iterations and with the additional advantage of being\nworkload-aware and usable online.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 10:41:59 GMT"}, {"version": "v2", "created": "Thu, 23 Jun 2016 11:38:20 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Firth", "Hugo", ""], ["Missier", "Paolo", ""]]}, {"id": "1603.04792", "submitter": "Martin Kirchgessner", "authors": "Martin Kirchgessner, Vincent Leroy, Sihem Amer-Yahia, Shashwat Mishra\n  (Intermarch\\'e Alimentaire International - STIME)", "title": "Testing Interestingness Measures in Practice: A Large-Scale Analysis of\n  Buying Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding customer buying patterns is of great interest to the retail\nindustry and has shown to benefit a wide variety of goals ranging from managing\nstocks to implementing loyalty programs. Association rule mining is a common\ntechnique for extracting correlations such as \"people in the South of France\nbuy ros\\'e wine\" or \"customers who buy pat\\'e also buy salted butter and sour\nbread.\" Unfortunately, sifting through a high number of buying patterns is not\nuseful in practice, because of the predominance of popular products in the top\nrules. As a result, a number of \"interestingness\" measures (over 30) have been\nproposed to rank rules. However, there is no agreement on which measures are\nmore appropriate for retail data. Moreover, since pattern mining algorithms\noutput thousands of association rules for each product, the ability for an\nanalyst to rely on ranking measures to identify the most interesting ones is\ncrucial. In this paper, we develop CAPA (Comparative Analysis of PAtterns), a\nframework that provides analysts with the ability to compare the outcome of\ninterestingness measures applied to buying patterns in the retail industry. We\nreport on how we used CAPA to compare 34 measures applied to over 1,800 stores\nof Intermarch\\'e, one of the largest food retailers in France.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 18:11:39 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Kirchgessner", "Martin", "", "Intermarch\u00e9 Alimentaire International - STIME"], ["Leroy", "Vincent", "", "Intermarch\u00e9 Alimentaire International - STIME"], ["Amer-Yahia", "Sihem", "", "Intermarch\u00e9 Alimentaire International - STIME"], ["Mishra", "Shashwat", "", "Intermarch\u00e9 Alimentaire International - STIME"]]}, {"id": "1603.05355", "submitter": "Yuhan Sun", "authors": "Yuhan Sun, Mohamed Sarwat", "title": "GeoReach: An Efficient Approach for Evaluating Graph Reachability\n  Queries with Spatial Range Predicates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs are widely used to model data in many application domains. Thanks to\nthe wide spread use of GPS-enabled devices, many applications assign a spatial\nattribute to graph vertices (e.g., geo-tagged social media). Users may issue a\nReachability Query with Spatial Range Predicate (abbr. RangeReach). RangeReach\nfinds whether an input vertex can reach any spatial vertex that lies within an\ninput spatial range. An example of a RangeReach query is: Given a social graph,\nfind whether Alice can reach any of the venues located within the geographical\narea of Arizona State University. The paper proposes GeoReach an approach that\nadds spatial data awareness to a graph database management system (GDBMS).\nGeoReach allows efficient execution of RangeReach queries, yet without\ncompromising a lot on the overall system scalability (measured in terms of\nstorage size and initialization/maintenance time). To achieve that, GeoReach is\nequipped with a light-weight data structure, namely SPA-Graph, that augments\nthe underlying graph data with spatial indexing directories. When a RangeReach\nquery is issued, the system employs a pruned-graph traversal approach.\nExperiments based on real system implementation inside Neo4j proves that\nGEOREACH exhibits up to two orders of magnitude better query response time and\nup to four times less storage than the state-of-the-art spatial and\nreachability indexing approaches.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 05:06:52 GMT"}], "update_date": "2016-03-18", "authors_parsed": [["Sun", "Yuhan", ""], ["Sarwat", "Mohamed", ""]]}, {"id": "1603.05422", "submitter": "Panagiotis Bouros", "authors": "Panagiotis Bouros, Nikos Mamoulis, Shen Ge and Manolis Terrovitis", "title": "Set Containment Join Revisited", "comments": "To appear at the Knowledge and Information Systems journal (KAIS)", "journal-ref": null, "doi": "10.1007/s10115-015-0895-7", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given two collections of set objects $R$ and $S$, the $R \\bowtie_{\\subseteq}\nS$ set containment join returns all object pairs $(r, s) \\in R \\times S$ such\nthat $r \\subseteq s$. Besides being a basic operator in all modern data\nmanagement systems with a wide range of applications, the join can be used to\nevaluate complex SQL queries based on relational division and as a module of\ndata mining algorithms. The state-of-the-art algorithm for set containment\njoins (PRETTI) builds an inverted index on the right-hand collection $S$ and a\nprefix tree on the left-hand collection $R$ that groups set objects with common\nprefixes and thus, avoids redundant processing. In this paper, we present a\nframework which improves PRETTI in two directions. First, we limit the prefix\ntree construction by proposing an adaptive methodology based on a cost model;\nthis way, we can greatly reduce the space and time cost of the join. Second, we\npartition the objects of each collection based on their first contained item,\nassuming that the set objects are internally sorted. We show that we can\nprocess the partitions and evaluate the join while building the prefix tree and\nthe inverted index progressively. This allows us to significantly reduce not\nonly the join cost, but also the maximum memory requirements during the join.\nAn experimental evaluation using both real and synthetic datasets shows that\nour framework outperforms PRETTI by a wide margin.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 10:47:48 GMT"}], "update_date": "2016-03-18", "authors_parsed": [["Bouros", "Panagiotis", ""], ["Mamoulis", "Nikos", ""], ["Ge", "Shen", ""], ["Terrovitis", "Manolis", ""]]}, {"id": "1603.06053", "submitter": "Renzo Angles", "authors": "Renzo Angles and Claudio Gutierrez", "title": "Negation in SPARQL", "comments": "Proc. of the Alberto Mendelzon International Workshop on Foundations\n  of Data Management (AMW'2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a thorough study of negation in SPARQL. The types of\nnegation supported in SPARQL are identified and their main features discussed.\nThen, we study the expressive power of the corresponding negation operators. At\nthis point, we identify a core SPARQL algebra which could be used instead of\nthe W3C SPARQL algebra. Finally, we analyze the negation operators in terms of\ntheir compliance with elementary axioms of set theory.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 06:37:28 GMT"}, {"version": "v2", "created": "Tue, 22 Mar 2016 09:41:37 GMT"}, {"version": "v3", "created": "Tue, 10 May 2016 02:08:34 GMT"}, {"version": "v4", "created": "Mon, 6 Jun 2016 11:38:47 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Angles", "Renzo", ""], ["Gutierrez", "Claudio", ""]]}, {"id": "1603.06068", "submitter": "Thomas Gottron", "authors": "Thomas Gottron", "title": "Measuring the Accuracy of Linked Data Indices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Being based on Web technologies, Linked Data is distributed and decentralised\nin its nature. Hence, for the purpose of finding relevant Linked Data on the\nWeb, search indices play an important role. Also for avoiding network\ncommunication overhead and latency, applications rely on indices or caches over\nLinked Data. These indices and caches are based on local copies of the original\ndata and, thereby, introduce redundancy. Furthermore, as changes at the\noriginal Linked Data sources are not automatically propagated to the local\ncopies, there is a risk of having inaccurate indices and caches due to outdated\ninformation. In this paper I discuss and compare methods for measuring the\naccuracy of indices. I will present different measures which have been used in\nrelated work and evaluate their advantages and disadvantages from a theoretic\npoint of view as well as from a practical point of view by analysing their\nbehaviour on real world data in an empirical experiment.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 09:19:19 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Gottron", "Thomas", ""]]}, {"id": "1603.06549", "submitter": "Daniel Lemire", "authors": "Daniel Lemire, Gregory Ssi-Yan-Kai, Owen Kaser", "title": "Consistently faster and smaller compressed bitmaps with Roaring", "comments": null, "journal-ref": "Software: Practice and Experience Volume 46, Issue 11, pages\n  1547-1569, November 2016", "doi": "10.1002/spe.2402", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Compressed bitmap indexes are used in databases and search engines. Many\nbitmap compression techniques have been proposed, almost all relying primarily\non run-length encoding (RLE). However, on unsorted data, we can get superior\nperformance with a hybrid compression technique that uses both uncompressed\nbitmaps and packed arrays inside a two-level tree. An instance of this\ntechnique, Roaring, has recently been proposed. Due to its good performance, it\nhas been adopted by several production platforms (e.g., Apache Lucene, Apache\nSpark, Apache Kylin and Druid).\n  Yet there are cases where run-length encoded bitmaps are smaller than the\noriginal Roaring bitmaps---typically when the data is sorted so that the\nbitmaps contain long compressible runs. To better handle these cases, we build\na new Roaring hybrid that combines uncompressed bitmaps, packed arrays and RLE\ncompressed segments. The result is a new Roaring format that compresses better.\n  Overall, our new implementation of Roaring can be several times faster (up to\ntwo orders of magnitude) than the implementations of traditional RLE-based\nalternatives (WAH, Concise, EWAH) while compressing better. We review the\ndesign choices and optimizations that make these good results possible.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 19:30:53 GMT"}, {"version": "v2", "created": "Tue, 19 Apr 2016 14:19:05 GMT"}, {"version": "v3", "created": "Sat, 8 Oct 2016 16:31:17 GMT"}, {"version": "v4", "created": "Fri, 2 Mar 2018 18:35:46 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Lemire", "Daniel", ""], ["Ssi-Yan-Kai", "Gregory", ""], ["Kaser", "Owen", ""]]}, {"id": "1603.06729", "submitter": "Xiaowang Zhang", "authors": "Xingwang Han and Zhiyong Feng and Xiaowang Zhang and Xin Wang and\n  Guozheng Rao and Shuo Jiang", "title": "On the Statistical Analysis of Practical SPARQL Queries", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyze some basic features of SPARQL queries coming from\nour practical world in a statistical way. These features include three\nstatistic features such as the occurrence frequency of triple patterns,\nfragments, well-designed patterns and four semantic features such as\nmonotonicity, non-monotonicity, weak monotonicity (old solutions are still\nserved as parts of new solutions when some new triples are added) and\nsatisfiability. All these features contribute to characterize SPARQL queries in\ndifferent dimensions. We hope that this statistical analysis would provide some\nuseful observation for researchers and engineers who are interested in what\npractical SPARQL queries look like, so that they could develop some practical\nheuristics for processing SPARQL queries and build SPARQL query processing\nengines and benchmarks. Besides, they can narrow the scope of their problems by\navoiding those cases that do possibly not happen in our practical world.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 10:44:02 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Han", "Xingwang", ""], ["Feng", "Zhiyong", ""], ["Zhang", "Xiaowang", ""], ["Wang", "Xin", ""], ["Rao", "Guozheng", ""], ["Jiang", "Shuo", ""]]}, {"id": "1603.06732", "submitter": "Xiaowang Zhang", "authors": "Zhenyu Song and Zhiyong Feng and Xiaowang Zhang and Xin Wang and\n  Guozheng Rao", "title": "Efficient Approximation of Well-Designed SPARQL Queries", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query response time often influences user experience in the real world.\nHowever, it possibly takes more time to answer a query with its all exact\nsolutions, especially when it contains the OPT operations since the OPT\noperation is the least conventional operator in SPARQL. So it becomes essential\nto make a trade-off between the query response time and the accuracy of their\nsolutions. In this paper, based on the depth of the OPT operation occurring in\na query, we propose an approach to obtain its all approximate queries with less\ndepth of the OPT operation. This paper mainly discusses those queries with\nwell-designed patterns since the OPT operation in a well-designed pattern is\nreally \"optional\". Firstly, we transform a well-designed pattern in OPT normal\nform into a well-designed tree, whose inner nodes are labeled by OPT operation\nand leaf nodes are labeled by patterns containing other operations such as the\nAND operation and the FILTER operation. Secondly, based on this well-designed\ntree, we remove \"optional\" well-designed subtrees with less depth of the OPT\noperation and then obtain approximate queries with different depths of the OPT\noperation. Finally, we evaluate the approximate query efficiency with the\ndegree of approximation.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 10:51:01 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Song", "Zhenyu", ""], ["Feng", "Zhiyong", ""], ["Zhang", "Xiaowang", ""], ["Wang", "Xin", ""], ["Rao", "Guozheng", ""]]}, {"id": "1603.07185", "submitter": "Rajesh Bordawekar", "authors": "Rajesh Bordawekar and Oded Shmueli", "title": "Enabling Cognitive Intelligence Queries in Relational Databases using\n  Low-dimensional Word Embeddings", "comments": "Submitted to VLDB", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply distributed language embedding methods from Natural Language\nProcessing to assign a vector to each database entity associated token (for\nexample, a token may be a word occurring in a table row, or the name of a\ncolumn). These vectors, of typical dimension 200, capture the meaning of tokens\nbased on the contexts in which the tokens appear together. To form vectors, we\napply a learning method to a token sequence derived from the database. We\ndescribe various techniques for extracting token sequences from a database. The\ntechniques differ in complexity, in the token sequences they output and in the\ndatabase information used (e.g., foreign keys). The vectors can be used to\nalgebraically quantify semantic relationships between the tokens such as\nsimilarities and analogies. Vectors enable a dual view of the data: relational\nand (meaningful rather than purely syntactical) text. We introduce and explore\na new class of queries called cognitive intelligence (CI) queries that extract\ninformation from the database based, in part, on the relationships encoded by\nvectors. We have implemented a prototype system on top of Spark to exhibit the\npower of CI queries. Here, CI queries are realized via SQL UDFs. This power\ngoes far beyond text extensions to relational systems due to the information\nencoded in vectors. We also consider various extensions to the basic scheme,\nincluding using a collection of views derived from the database to focus on a\ndomain of interest, utilizing vectors and/or text from external sources,\nmaintaining vectors as the database evolves and exploring a database without\nutilizing its schema. For the latter, we consider minimal extensions to SQL to\nvastly improve query expressiveness.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 13:57:33 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Bordawekar", "Rajesh", ""], ["Shmueli", "Oded", ""]]}, {"id": "1603.07410", "submitter": "Erkang Zhu", "authors": "Erkang Zhu, Fatemeh Nargesian, Ken Q. Pu, Ren\\'ee J. Miller", "title": "LSH Ensemble: Internet-Scale Domain Search", "comments": "To appear in VLDB 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of domain search where a domain is a set of distinct\nvalues from an unspecified universe. We use Jaccard set containment, defined as\n$|Q \\cap X|/|Q|$, as the relevance measure of a domain $X$ to a query domain\n$Q$. Our choice of Jaccard set containment over Jaccard similarity makes our\nwork particularly suitable for searching Open Data and data on the web, as\nJaccard similarity is known to have poor performance over sets with large\ndifferences in their domain sizes. We demonstrate that the domains found in\nseveral real-life Open Data and web data repositories show a power-law\ndistribution over their domain sizes.\n  We present a new index structure, Locality Sensitive Hashing (LSH) Ensemble,\nthat solves the domain search problem using set containment at Internet scale.\nOur index structure and search algorithm cope with the data volume and skew by\nmeans of data sketches (MinHash) and domain partitioning. Our index structure\ndoes not assume a prescribed set of values. We construct a cost model that\ndescribes the accuracy of LSH Ensemble with any given partitioning. This allows\nus to formulate the partitioning for LSH Ensemble as an optimization problem.\nWe prove that there exists an optimal partitioning for any distribution.\nFurthermore, for datasets following a power-law distribution, as observed in\nOpen Data and Web data corpora, we show that the optimal partitioning can be\napproximated using equi-depth, making it efficient to use in practice.\n  We evaluate our algorithm using real data (Canadian Open Data and WDC Web\nTables) containing up over 262 M domains. The experiments demonstrate that our\nindex consistently outperforms other leading alternatives in accuracy and\nperformance. The improvements are most dramatic for data with large skew in the\ndomain sizes. Even at 262 M domains, our index sustains query performance with\nunder 3 seconds response time.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 01:43:28 GMT"}, {"version": "v2", "created": "Wed, 30 Mar 2016 00:52:45 GMT"}, {"version": "v3", "created": "Mon, 4 Apr 2016 18:54:13 GMT"}, {"version": "v4", "created": "Sat, 23 Jul 2016 04:47:58 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Zhu", "Erkang", ""], ["Nargesian", "Fatemeh", ""], ["Pu", "Ken Q.", ""], ["Miller", "Ren\u00e9e J.", ""]]}, {"id": "1603.07534", "submitter": "Eiko Yoneki", "authors": "Juan M. Tirado, Ovidiu Serban, Qiang Guo, Eiko Yoneki", "title": "Web Data Knowledge Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A constantly growing amount of information is available through the web.\nUnfortunately, extracting useful content from this massive amount of data still\nremains an open issue. The lack of standard data models and structures forces\ndevelopers to create adhoc solutions from the scratch. The figure of the expert\nis still needed in many situations where developers do not have the correct\nbackground knowledge. This forces developers to spend time acquiring the needed\nbackground from the expert. In other directions, there are promising solutions\nemploying machine learning techniques. However, increasing accuracy requires an\nincrease in system complexity that cannot be endured in many projects. In this\nwork, we approach the web knowledge extraction problem using an expertcentric\nmethodology. This methodology defines a set of configurable, extendible and\nindependent components that permit the reutilisation of large pieces of code\namong projects. Our methodology differs from similar solutions in its\nexpert-driven design. This design, makes it possible for subject-matter expert\nto drive the knowledge extraction for a given set of documents. Additionally,\nwe propose the utilization of machine assisted solutions that guide the expert\nduring this process. To demonstrate the capabilities of our methodology, we\npresent a real use case scenario in which public procurement data is extracted\nfrom the web-based repositories of several public institutions across Europe.\nWe provide insightful details about the challenges we had to deal with in this\nuse case and additional discussions about how to apply our methodology.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 11:43:34 GMT"}], "update_date": "2016-03-25", "authors_parsed": [["Tirado", "Juan M.", ""], ["Serban", "Ovidiu", ""], ["Guo", "Qiang", ""], ["Yoneki", "Eiko", ""]]}, {"id": "1603.07641", "submitter": "Prithu Banerjee", "authors": "Prithu Banerjee, Sayan Ranu, Sriram Raghavan", "title": "Inferring Uncertain Trajectories from Partial Observations", "comments": null, "journal-ref": null, "doi": "10.1109/ICDM.2014.41", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The explosion in the availability of GPS-enabled devices has resulted in an\nabundance of trajectory data. In reality, however, majority of these\ntrajectories are collected at a low sampling rate and only provide partial\nobservations on their actually traversed routes. Consequently, they are mired\nwith uncertainty. In this paper, we develop a technique called InferTra to\ninfer uncertain trajectories from network-constrained partial observations.\nRather than predicting the most likely route, the inferred uncertain trajectory\ntakes the form of an edge-weighted graph and summarizes all probable routes in\na holistic manner. For trajectory inference, InferTra employs Gibbs sampling by\nlearning a Network Mobility Model (NMM) from a database of historical\ntrajectories. Extensive experiments on real trajectory databases show that the\ngraph-based approach of InferTra is up to 50% more accurate, 20 times faster,\nand immensely more versatile than state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 16:27:42 GMT"}], "update_date": "2016-03-25", "authors_parsed": [["Banerjee", "Prithu", ""], ["Ranu", "Sayan", ""], ["Raghavan", "Sriram", ""]]}, {"id": "1603.08390", "submitter": "Jingbo Zhou", "authors": "Jingbo Zhou, Qi Guo, H. V. Jagadish, Lubo\\v{s} Kr\\v{c}\\'al, Siyuan\n  Liu, Wenhao Luan, Anthony K. H. Tung, Yueji Yang, Yuxin Zheng", "title": "A Generic Inverted Index Framework for Similarity Search on the GPU -\n  Technical Report", "comments": "18 pages, technical report for the ICDE 2018 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CV cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel generic inverted index framework on the GPU (called\nGENIE), aiming to reduce the programming complexity of the GPU for parallel\nsimilarity search of different data types. Not every data type and similarity\nmeasure are supported by GENIE, but many popular ones are. We present the\nsystem design of GENIE, and demonstrate similarity search with GENIE on several\ndata types along with a theoretical analysis of search results. A new concept\nof locality sensitive hashing (LSH) named $\\tau$-ANN search, and a novel data\nstructure c-PQ on the GPU are also proposed for achieving this purpose.\nExtensive experiments on different real-life datasets demonstrate the\nefficiency and effectiveness of our framework. The implemented system has been\nreleased as open source.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 14:44:34 GMT"}, {"version": "v2", "created": "Sun, 25 Feb 2018 06:05:25 GMT"}, {"version": "v3", "created": "Tue, 14 Aug 2018 08:49:16 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Zhou", "Jingbo", ""], ["Guo", "Qi", ""], ["Jagadish", "H. V.", ""], ["Kr\u010d\u00e1l", "Lubo\u0161", ""], ["Liu", "Siyuan", ""], ["Luan", "Wenhao", ""], ["Tung", "Anthony K. H.", ""], ["Yang", "Yueji", ""], ["Zheng", "Yuxin", ""]]}, {"id": "1603.08514", "submitter": "Girish Sundaram", "authors": "Girish Sundaram, Mudit Bachhawat", "title": "Deriving Application Level Relationships by Analysing the Run Time\n  Behaviour of Simple and Complex SQL Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a unique approach to perform application behavioral\nanalysis for identifying how tables might be related to each other. The\nanalysis techniques are based on the properties of primary and foreign keys and\nalso the data present in their respective columns. We have also implemented the\nidea using JAVA and presented experimental results in Demo Section.This paper\nintroduces a unique approach to predict the possible application level\nrelationships in databases with the help of the application relationship\nanalysis of simple and complex SQL queries. Complex SQL queries are those which\ncontain multiple constraints at different levels of the database. In the\nprocess of deriving relations, we first parse the SQL statements and then\nanalyse the parsed information to extract related information.\n", "versions": [{"version": "v1", "created": "Sat, 26 Mar 2016 14:33:30 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Sundaram", "Girish", ""], ["Bachhawat", "Mudit", ""]]}, {"id": "1603.09291", "submitter": "Elena Botoeva", "authors": "Elena Botoeva, Diego Calvanese, Benjamin Cogrel, and Guohui Xiao", "title": "Expressivity and Complexity of MongoDB (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A significant number of novel database architectures and data models have\nbeen proposed during the last decade. While some of these new systems have\ngained in popularity, they lack a proper formalization, and a precise\nunderstanding of the expressivity and the computational properties of the\nassociated query languages. In this paper, we aim at filling this gap, and we\ndo so by considering MongoDB, a widely adopted document database managing\ncomplex (tree structured) values represented in a JSON-based data model,\nequipped with a powerful query mechanism. We provide a formalization of the\nMongoDB data model, and of a core fragment, called MQuery, of the MongoDB query\nlanguage. We study the expressivity of MQuery, showing its equivalence with\nnested relational algebra. We further investigate the computational complexity\nof significant fragments of it, obtaining several (tight) bounds in combined\ncomplexity, which range from LOGSPACE to alternating exponential-time with a\npolynomial number of alternations. As a consequence, we obtain also a\ncharacterization of the combined complexity of nested relational algebra query\nevaluation.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 17:47:06 GMT"}, {"version": "v2", "created": "Fri, 17 Feb 2017 14:27:23 GMT"}, {"version": "v3", "created": "Tue, 25 Apr 2017 15:25:59 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Botoeva", "Elena", ""], ["Calvanese", "Diego", ""], ["Cogrel", "Benjamin", ""], ["Xiao", "Guohui", ""]]}, {"id": "1603.09434", "submitter": "Ibrahim AlShourbaji H", "authors": "Ibrahim AlShourbaji, Samaher Al-Janabi and Ahmed Patel", "title": "Document Selection in a Distributed Search Engine Architecture", "comments": "8 pages, 6 figures in Middle-East Journal of Scientific Research,\n  IDOSI Publications, 2015", "journal-ref": null, "doi": "10.5829/idosi.mejsr.2015.23.07.22398", "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed Search Engine Architecture (DSEA) hosts numerous independent\ntopic-specific search engines and selects a subset of the databases to search\nwithin the architecture. The objective of this approach is to reduce the amount\nof space needed to perform a search by querying only a subset of the total data\navailable. In order to manipulate data across many databases, it is most\nefficient to identify a smaller subset of databases that would be most likely\nto return the data of specific interest that can then be examined in greater\ndetail. The selection index has been most commonly used as a method for\nchoosing the most applicable databases as it captures broad information about\neach database and its indexed documents. Employing this type of database allows\nthe researcher to find information more quickly, not only with less cost, but\nit also minimizes the potential for biases. This paper investigates the\neffectiveness of different databases selected within the framework and scope of\nthe distributed search engine architecture. The purpose of the study is to\nimprove the quality of distributed information retrieval.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 01:19:21 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["AlShourbaji", "Ibrahim", ""], ["Al-Janabi", "Samaher", ""], ["Patel", "Ahmed", ""]]}, {"id": "1603.09617", "submitter": "Francesco Scarcello", "authors": "Gianluigi Greco and Francesco Scarcello", "title": "Greedy Strategies and Larger Islands of Tractability for Conjunctive\n  Queries and Constraint Satisfaction Problems", "comments": "arXiv admin note: substantial text overlap with arXiv:1205.3321", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural decomposition methods have been developed for identifying\ntractable classes of instances of fundamental problems in databases, such as\nconjunctive queries and query containment, of the constraint satisfaction\nproblem in artificial intelligence, or more generally of the homomorphism\nproblem over relational structures. Most structural decomposition methods can\nbe characterized through hypergraph games that are variations of the Robber and\nCops graph game that characterizes the notion of treewidth. In particular,\ndecomposition trees somehow correspond to monotone winning strategies, where\nthe escape space of the robber on the hypergraph is shrunk monotonically by the\ncops. In fact, unlike the treewidth case, there are hypergraphs where monotonic\nstrategies do not exist, while the robber can be captured by means of more\ncomplex non-monotonic strategies. However, these powerful strategies do not\ncorrespond in general to valid decompositions. The paper provides a general way\nto exploit the power of non-monotonic strategies, by allowing a \"disciplined\"\nform of non-monotonicity, characteristic of cops playing in a greedy way. It is\nshown that deciding the existence of a (non-monotone) greedy winning strategy\n(and compute one, if any) is tractable. Moreover, despite their\nnon-monotonicity, such strategies always induce valid decomposition trees,\nwhich can be computed efficiently based on them. As a consequence, greedy\nstrategies allow us to define new islands of tractability for the considered\nproblems properly including all previously known classes of tractable\ninstances.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 14:57:33 GMT"}, {"version": "v2", "created": "Mon, 4 Jul 2016 21:20:28 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Greco", "Gianluigi", ""], ["Scarcello", "Francesco", ""]]}]