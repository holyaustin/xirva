[{"id": "1906.00140", "submitter": "Xin Huang", "authors": "Soroush Ebadian, Xin Huang", "title": "Fast Algorithm for K-Truss Discovery on Public-Private Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In public-private graphs, users share one public graph and have their own\nprivate graphs. A private graph consists of personal private contacts that only\ncan be visible to its owner, e.g., hidden friend lists on Facebook and secret\nfollowing on Sina Weibo. However, existing public-private analytic algorithms\nhave not yet investigated the dense subgraph discovery of k-truss, where each\nedge is contained in at least k-2 triangles. This paper aims at finding k-truss\nefficiently in public-private graphs. The core of our solution is a novel\nalgorithm to update k-truss with node insertions. We develop a\nclassification-based hybrid strategy of node insertions and edge insertions to\nincrementally compute k-truss in public-private graphs. Extensive experiments\nvalidate the superiority of our proposed algorithms against state-of-the-art\nmethods on real-world datasets.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 03:31:06 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Ebadian", "Soroush", ""], ["Huang", "Xin", ""]]}, {"id": "1906.00179", "submitter": "Diego Calvanese", "authors": "Diego Calvanese, Davide Lanti, Ana Ozaki, Rafael Penaloza, Guohui Xiao", "title": "Enriching Ontology-based Data Access with Provenance (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ontology-based data access (OBDA) is a popular paradigm for querying\nheterogeneous data sources by connecting them through mappings to an ontology.\nIn OBDA, it is often difficult to reconstruct why a tuple occurs in the answer\nof a query. We address this challenge by enriching OBDA with provenance\nsemirings, taking inspiration from database theory. In particular, we\ninvestigate the problems of (i) deciding whether a provenance annotated OBDA\ninstance entails a provenance annotated conjunctive query, and (ii) computing a\npolynomial representing the provenance of a query entailed by a provenance\nannotated OBDA instance. Differently from pure databases, in our case these\npolynomials may be infinite. To regain finiteness, we consider idempotent\nsemirings, and study the complexity in the case of DL-Lite ontologies. We\nimplement Task (ii) in a state-of-the-art OBDA system and show the practical\nfeasibility of the approach through an extensive evaluation against two popular\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 08:15:48 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Calvanese", "Diego", ""], ["Lanti", "Davide", ""], ["Ozaki", "Ana", ""], ["Penaloza", "Rafael", ""], ["Xiao", "Guohui", ""]]}, {"id": "1906.00228", "submitter": "Myeongjae Jeon", "authors": "Gangmuk Lim, Mohamed Hassan, Ze Jin, Stavros Volos, Myeongjae Jeon", "title": "Approximate Quantiles for Datacenter Telemetry Monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Datacenter systems require efficient troubleshooting and effective resource\nscheduling so as to minimize downtimes and to efficiently utilize limited\nresources. In doing so, datacenter operators employ streaming analytics for\ncollecting and processing datacenter telemetry over a temporal window. The\nquantile operator is key to these systems as it can summarize the typical and\nabnormal behavior of the monitored system. Computing quantiles in real-time is\nresource-intensive as it requires processing hundreds of millions of events in\nseconds while providing high accuracy.\n  We overcome these challenges in real-time quantile computation through\nworkload-driven approximation, motivated by three insights in our study: (i)\nvalues are dominated by a set of recurring small values, (ii) distribution of\nsmall values is consistent across different time scales, and (iii) tail values\nare dominated by a small set of large values. That is, we propose AOMG, an\nefficient and accurate quantile approximation algorithm that capitalizes on\nthese insights. AOMG minimizes memory footprint of the quantile operator via\ncompression and frequency-based summarization of small values. While these\nsummaries are stored and processed at sub-window granularity for memory\nefficiency, they can extend to compute quantiles on user-defined temporal\nwindows. Low value error for tail quantiles is achieved by retaining a few tail\nvalues per subwindow. AOMG estimates quantiles with high throughput and less\nthan 5% relative value error across a wide range of use cases while\nstate-of-the-art algorithms either have a high relative value error\n(9.3-137.0%) or deliver lower throughput (15-92%).\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 14:18:37 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 14:14:17 GMT"}, {"version": "v3", "created": "Thu, 17 Oct 2019 15:53:48 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Lim", "Gangmuk", ""], ["Hassan", "Mohamed", ""], ["Jin", "Ze", ""], ["Volos", "Stavros", ""], ["Jeon", "Myeongjae", ""]]}, {"id": "1906.00239", "submitter": "HMN Dilum Bandara", "authors": "HMN Dilum Bandara and Xiwei Xu and Ingo Weber", "title": "Patterns for Blockchain Data Migration", "comments": "40 pages, 13 figures, 1 table", "journal-ref": null, "doi": "10.1145/3424771.3424796", "report-no": null, "categories": "cs.DC cs.DB cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the rapid evolution of technological, economic, and regulatory\nlandscapes, contemporary blockchain platforms are all but certain to undergo\nmajor changes. Therefore, the applications that rely on them will eventually\nneed to migrate from one blockchain instance to another to remain competitive\nand secure, as well as to enhance the business process, performance, cost\nefficiency, privacy, and regulatory compliance. However, the differences in\ndata and smart contract representations, modes of hosting, transaction fees, as\nwell as the need to preserve consistency, immutability, and data provenance\nintroduce unique challenges over database migration. We first present a set of\nblockchain migration scenarios and data fidelity levels using an illustrative\nexample. We then present a set of migration patterns to address those scenarios\nand the above data management challenges. Finally, we demonstrate how the\neffort, cost, and risk of migration could be minimized by choosing a suitable\nset of data migration patterns, data fidelity level, and proactive system\ndesign. Practical considerations and research challenges are also highlighted.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 15:10:21 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 13:05:33 GMT"}, {"version": "v3", "created": "Wed, 26 May 2021 00:42:20 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Bandara", "HMN Dilum", ""], ["Xu", "Xiwei", ""], ["Weber", "Ingo", ""]]}, {"id": "1906.00299", "submitter": "Wentao Wu", "authors": "Frances Ann Hubis, Wentao Wu, Ce Zhang", "title": "Quantitative Overfitting Management for Human-in-the-loop ML Application\n  Development with ease.ml/meter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simplifying machine learning (ML) application development, including\ndistributed computation, programming interface, resource management, model\nselection, etc, has attracted intensive interests recently. These research\nefforts have significantly improved the efficiency and the degree of automation\nof developing ML models. In this paper, we take a first step in an orthogonal\ndirection towards automated quality management for human-in-the-loop ML\napplication development. We build ease. ml/meter, a system that can\nautomatically detect and measure the degree of overfitting during the whole\nlifecycle of ML application development. ease. ml/meter returns overfitting\nsignals with strong probabilistic guarantees, based on which developers can\ntake appropriate actions. In particular, ease. ml/meter provides principled\nguidelines to simple yet nontrivial questions regarding desired validation and\ntest data sizes, which are among commonest questions raised by developers. The\nfact that ML application development is typically a continuous procedure\nfurther worsens the situation: The validation and test data sets can lose their\nstatistical power quickly due to multiple accesses, especially in the presence\nof adaptive analysis. ease. ml/meter addresses these challenges by leveraging a\ncollection of novel techniques and optimizations, resulting in practically\ntractable data sizes without compromising the probabilistic guarantees. We\npresent the design and implementation details of ease. ml/meter, as well as\ndetailed theoretical analysis and empirical evaluation of its effectiveness.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 21:54:05 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 04:47:26 GMT"}, {"version": "v3", "created": "Thu, 6 Jun 2019 03:58:36 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Hubis", "Frances Ann", ""], ["Wu", "Wentao", ""], ["Zhang", "Ce", ""]]}, {"id": "1906.00341", "submitter": "Kaiqiang Yu", "authors": "Yixiang Fang, Kaiqiang Yu, Reynold Cheng, Laks V.S. Lakshmanan, Xuemin\n  Lin", "title": "Efficient Algorithms for Densest Subgraph Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Densest subgraph discovery (DSD) is a fundamental problem in graph mining. It\nhas been studied for decades, and is widely used in various areas, including\nnetwork science, biological analysis, and graph databases. Given a graph G, DSD\naims to find a subgraph D of G with the highest density (e.g., the number of\nedges over the number of vertices in D). Because DSD is difficult to solve, we\npropose a new solution paradigm in this paper. Our main observation is that a\ndensest subgraph can be accurately found through a k-core (a kind of dense\nsubgraph of G), with theoretical guarantees. Based on this intuition, we\ndevelop efficient exact and approximation solutions for DSD. Moreover, our\nsolutions are able to find the densest subgraphs for a wide range of graph\ndensity definitions, including clique-based and general pattern-based density.\nWe have performed extensive experimental evaluation on eleven real datasets.\nOur results show that our algorithms are up to four orders of magnitude faster\nthan existing approaches.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jun 2019 04:28:06 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2019 11:43:39 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Fang", "Yixiang", ""], ["Yu", "Kaiqiang", ""], ["Cheng", "Reynold", ""], ["Lakshmanan", "Laks V. S.", ""], ["Lin", "Xuemin", ""]]}, {"id": "1906.00624", "submitter": "Louis Jachiet", "authors": "Michael Benedikt, Pierre Bourhis (CRIStAL, CNRS, SPIRALS), Louis\n  Jachiet (CRIStAL, CNRS, SPIRALS), Micha\\\"el Thomazo (DI-ENS, ENS Paris, CNRS,\n  PSL, VALDA )", "title": "Reasoning about disclosure in data integration in the presence of source\n  constraints", "comments": null, "journal-ref": "28th International Joint Conference on Artificial Intelligence\n  (IJCAI-19), Aug 2019, Macau, China", "doi": "10.24963/ijcai.2019/215", "report-no": null, "categories": "cs.LO cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data integration systems allow users to access data sitting in multiple\nsources by means of queries over a global schema, related to the sources via\nmappings. Data sources often contain sensitive information, and thus an\nanalysis is needed to verify that a schema satisfies a privacy policy, given as\na set of queries whose answers should not be accessible to users. Such an\nanalysis should take into account not only knowledge that an attacker may have\nabout the mappings, but also what they may know about the semantics of the\nsources. In this paper, we show that source constraints can have a dramatic\nimpact on disclosure analysis. We study the problem of determining whether a\ngiven data integration system discloses a source query to an attacker in the\npresence of constraints, providing both lower and upper bounds on source-aware\ndisclosure analysis.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 08:18:12 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 12:50:52 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Benedikt", "Michael", "", "CRIStAL, CNRS, SPIRALS"], ["Bourhis", "Pierre", "", "CRIStAL, CNRS, SPIRALS"], ["Jachiet", "Louis", "", "CRIStAL, CNRS, SPIRALS"], ["Thomazo", "Micha\u00ebl", "", "DI-ENS, ENS Paris, CNRS,\n  PSL, VALDA"]]}, {"id": "1906.00781", "submitter": "Jiaoyan Chen", "authors": "Jiaoyan Chen and Ernesto Jimenez-Ruiz and Ian Horrocks and Charles\n  Sutton", "title": "Learning Semantic Annotations for Tabular Data", "comments": "7 pages", "journal-ref": "IJCAI 2019", "doi": null, "report-no": null, "categories": "cs.DB cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The usefulness of tabular data such as web tables critically depends on\nunderstanding their semantics. This study focuses on column type prediction for\ntables without any meta data. Unlike traditional lexical matching-based\nmethods, we propose a deep prediction model that can fully exploit a table's\ncontextual semantics, including table locality features learned by a Hybrid\nNeural Network (HNN), and inter-column semantics features learned by a\nknowledge base (KB) lookup and query answering algorithm.It exhibits good\nperformance not only on individual table sets, but also when transferring from\none table set to another.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 20:10:14 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Chen", "Jiaoyan", ""], ["Jimenez-Ruiz", "Ernesto", ""], ["Horrocks", "Ian", ""], ["Sutton", "Charles", ""]]}, {"id": "1906.01599", "submitter": "Marco Bressan", "authors": "Marco Bressan, Stefano Leucci, Alessandro Panconesi", "title": "Motivo: fast motif counting via succinct color coding and adaptive\n  sampling", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The randomized technique of color coding is behind state-of-the-art\nalgorithms for estimating graph motif counts. Those algorithms, however, are\nnot yet capable of scaling well to very large graphs with billions of edges. In\nthis paper we develop novel tools for the `motif counting via color coding'\nframework. As a result, our new algorithm, Motivo, is able to scale well to\nlarger graphs while at the same time provide more accurate graphlet counts than\never before. This is achieved thanks to two types of improvements. First, we\ndesign new succinct data structures that support fast common color coding\noperations, and a biased coloring trick that trades accuracy versus running\ntime and memory usage. These adaptations drastically reduce the time and memory\nrequirements of color coding. Second, we develop an adaptive graphlet sampling\nstrategy, based on a fractional set cover problem, that breaks the additive\napproximation barrier of standard sampling. This strategy gives multiplicative\napproximations for all graphlets at once, allowing us to count not only the\nmost frequent graphlets but also extremely rare ones.\n  To give an idea of the improvements, in $40$ minutes Motivo counts $7$-nodes\nmotifs on a graph with $65$M nodes and $1.8$B edges; this is $30$ and $500$\ntimes larger than the state of the art, respectively in terms of nodes and\nedges. On the accuracy side, in one hour Motivo produces accurate counts of\n$\\approx \\! 10.000$ distinct $8$-node motifs on graphs where state-of-the-art\nalgorithms fail even to find the second most frequent motif. Our method\nrequires just a high-end desktop machine. These results show how color coding\ncan bring motif mining to the realm of truly massive graphs using only ordinary\nhardware.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 17:22:07 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Bressan", "Marco", ""], ["Leucci", "Stefano", ""], ["Panconesi", "Alessandro", ""]]}, {"id": "1906.01933", "submitter": "Theofilos Ioannidis", "authors": "Theofilos Ioannidis, George Garbis, Kostis Kyzirakos, Konstantina\n  Bereta, Manolis Koubarakis", "title": "Evaluating Geospatial RDF stores Using the Benchmark Geographica 2", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Since 2007, geospatial extensions of SPARQL, like GeoSPARQL and stSPARQL,\nhave been defined and corresponding geospatial RDF stores have been\nimplemented. In addition, some work on developing benchmarks for evaluating\ngeospatial RDF stores has been carried out. In this paper, we revisit the\nGeographica benchmark defined by our group in 2013 which uses both real world\nand synthetic data to test the performance and functionality of geospatial RDF\nstores. We present Geographica 2, a new version of the benchmark which extends\nGeographica by adding one more workload, extending our existing workloads and\nevaluating 5 more RDF stores. Using three different real workloads, Geographica\n2 tests the efficiency of primitive spatial functions in RDF stores and the\nperformance of the RDF stores in real use case scenarios, a more detailed\nevaluation is performed using a synthetic workload and the scalability of the\nRDF stores is stressed with the scalability workload. In total eight systems\nare evaluated out of which six adequately support GeoSPARQL and two offer\nlimited spatial support.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 10:51:25 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Ioannidis", "Theofilos", ""], ["Garbis", "George", ""], ["Kyzirakos", "Kostis", ""], ["Bereta", "Konstantina", ""], ["Koubarakis", "Manolis", ""]]}, {"id": "1906.01950", "submitter": "Tarcisio Mendes de Farias", "authors": "Tarcisio Mendes de Farias and Kurt Stockinger and Christophe Dessimoz", "title": "VoIDext: Vocabulary and Patterns for Enhancing Interoperable Datasets\n  with Virtual Links", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semantic heterogeneity remains a problem when interoperating with data from\nsources of different scopes and knowledge domains. Causes for this challenge\nare context-specific requirements (i.e. no \"one model fits all\"), different\ndata modelling decisions, domain-specific purposes, and technical constraints.\nMoreover, even if the problem of semantic heterogeneity among different RDF\npublishers and knowledge domains is solved, querying and accessing the data of\ndistributed RDF datasets on the Web is not straightforward. This is because of\nthe complex and fastidious process needed to understand how these datasets can\nbe related or linked, and consequently, queried. To address this issue, we\npropose to extend the existing Vocabulary of Interlinked Datasets (VoID) by\nintroducing new terms such as the Virtual Link Set concept and data model\npatterns. A virtual link is a connection between resources such as literals and\nIRIs (Internationalized Resource Identifier) with some commonality where each\nof these resources is from a different RDF dataset. The links are required in\norder to understand how to semantically relate datasets. In addition, we\ndescribe several benefits of using virtual links to improve interoperability\nbetween heterogenous and independent datasets. Finally, we exemplify and apply\nour approach to multiple world-wide used RDF datasets.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 11:29:01 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 07:02:08 GMT"}, {"version": "v3", "created": "Sun, 8 Sep 2019 07:56:58 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["de Farias", "Tarcisio Mendes", ""], ["Stockinger", "Kurt", ""], ["Dessimoz", "Christophe", ""]]}, {"id": "1906.01974", "submitter": "Peter Kraft", "authors": "Peter Kraft, Daniel Kang, Deepak Narayanan, Shoumik Palkar, Peter\n  Bailis, Matei Zaharia", "title": "Willump: A Statistically-Aware End-to-end Optimizer for Machine Learning\n  Inference", "comments": null, "journal-ref": "Proceedings of Machine Learning and Systems 2020, 147-159", "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systems for ML inference are widely deployed today, but they typically\noptimize ML inference workloads using techniques designed for conventional data\nserving workloads and miss critical opportunities to leverage the statistical\nnature of ML. In this paper, we present Willump, an optimizer for ML inference\nthat introduces two statistically-motivated optimizations targeting ML\napplications whose performance bottleneck is feature computation. First,\nWillump automatically cascades feature computation for classification queries:\nWillump classifies most data inputs using only high-value, low-cost features\nselected through empirical observations of ML model performance, improving\nquery performance by up to 5x without statistically significant accuracy loss.\nSecond, Willump accurately approximates ML top-K queries, discarding\nlow-scoring inputs with an automatically constructed approximate model and then\nranking the remainder with a more powerful model, improving query performance\nby up to 10x with minimal accuracy loss. Willump automatically tunes these\noptimizations' parameters to maximize query performance while meeting an\naccuracy target. Moreover, Willump complements these statistical optimizations\nwith compiler optimizations to automatically generate fast inference code for\nML applications. We show that Willump improves the end-to-end performance of\nreal-world ML inference pipelines curated from major data science competitions\nby up to 16x without statistically significant loss of accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 22:43:00 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 18:44:27 GMT"}, {"version": "v3", "created": "Thu, 5 Mar 2020 16:58:35 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Kraft", "Peter", ""], ["Kang", "Daniel", ""], ["Narayanan", "Deepak", ""], ["Palkar", "Shoumik", ""], ["Bailis", "Peter", ""], ["Zaharia", "Matei", ""]]}, {"id": "1906.02074", "submitter": "Yeting Li", "authors": "Yeting Li, Haiming Chen, Xiaolan Zhang, Lingqi Zhang", "title": "An Effective Algorithm for Learning Single Occurrence Regular\n  Expressions with Interleaving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advantages offered by the presence of a schema are numerous. However,\nmany XML documents in practice are not accompanied by a (valid) schema, making\nschema inference an attractive research problem. The fundamental task in XML\nschema learning is inferring restricted subclasses of regular expressions. Most\nprevious work either lacks support for interleaving or only has limited support\nfor interleaving. In this paper, we first propose a new subclass Single\nOccurrence Regular Expressions with Interleaving (SOIRE), which has\nunrestricted support for interleaving. Then, based on single occurrence\nautomaton and maximum independent set, we propose an algorithm iSOIRE to infer\nSOIREs. Finally, we further conduct a series of experiments on real datasets to\nevaluate the effectiveness of our work, comparing with both ongoing learning\nalgorithms in academia and industrial tools in real-world. The results reveal\nthe practicability of SOIRE and the effectiveness of iSOIRE, showing the high\npreciseness and conciseness of our work.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 15:32:22 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Li", "Yeting", ""], ["Chen", "Haiming", ""], ["Zhang", "Xiaolan", ""], ["Zhang", "Lingqi", ""]]}, {"id": "1906.02560", "submitter": "Ji Sun", "authors": "Ji Sun, Guoliang Li", "title": "An End-to-End Learning-based Cost Estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cost and cardinality estimation is vital to query optimizer, which can guide\nthe plan selection. However traditional empirical cost and cardinality\nestimation techniques cannot provide high-quality estimation, because they\ncannot capture the correlation between multiple columns. Recently the database\ncommunity shows that the learning-based cardinality estimation is better than\nthe empirical methods. However, existing learning-based methods have several\nlimitations. Firstly, they can only estimate the cardinality, but cannot\nestimate the cost. Secondly, convolutional neural network (CNN) with average\npooling is hard to represent complicated structures, e.g., complex predicates,\nand the model is hard to be generalized.\n  To address these challenges, we propose an effective end-to-end\nlearning-based cost estimation framework based on a tree-structured model,\nwhich can estimate both cost and cardinality simultaneously. To the best of our\nknowledge, this is the first end-to-end cost estimator based on deep learning.\nWe propose effective feature extraction and encoding techniques, which consider\nboth queries and physical operations in feature extraction. We embed these\nfeatures into our tree-structured model. We propose an effective method to\nencode string values, which can improve the generalization ability for\npredicate matching. As it is prohibitively expensive to enumerate all string\nvalues, we design a patten-based method, which selects patterns to cover string\nvalues and utilizes the patterns to embed string values. We conducted\nexperiments on real-world datasets and experimental results showed that our\nmethod outperformed baselines.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 13:01:06 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Sun", "Ji", ""], ["Li", "Guoliang", ""]]}, {"id": "1906.03053", "submitter": "Maurice Tchoup\\'e Tchendji", "authors": "Maurice Tchoup\\'e Tchendji, Lionel Tadonfouet, Thomas T\\'ebougang\n  Tchendji", "title": "A Tree Pattern Matching Algorithm for XML Queries with Structural\n  Preferences", "comments": null, "journal-ref": "Journal of Computer and Communications, 2019, 7, 61-83", "doi": "10.4236/jcc.2019.71006", "report-no": null, "categories": "cs.DB cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In the XML community, exact queries allow users to specify exactly what they\nwant to check and/or retrieve in an XML document. When they are applied to a\nsemi-structured document or to a document with an overly complex model, the\nlack or the ignorance of the explicit document model (DTD-Document Type\nDefinition, Schema, etc.) increases the risk of ob-taining an empty result set\nwhen the query is too specific, or, too large result set when it is too vague\n(e.g. it contains wildcards such as \"*\"). The reason is that in both cases,\nusers write queries according to the document model they have in mind; this can\nbe very far from the one that can actually be extracted from the document.\nOpposed to exact queries, preference queries are more flexible and can be\nrelaxed to expand the search space during their evalua-tions. Indeed, during\ntheir evaluation, certain constraints (the preferences they contain) can be\nrelaxed if necessary to avoid precisely empty results; moreover, the returned\nanswers can be filtered to retain only the best ones. This paper presents an\nalgorithm for evaluating such queries inspired by the TreeMatch algorithm\nproposed by Yao et al. for exact queries. In the pro-posed algorithm, the best\nanswers are obtained by using an adaptation of the Skyline operator (defined in\nrelational databases) in the context of documents (trees) to incrementally\nfilter into the partial solutions set, those which satisfy the maximum of\npreferential constraints. The only restriction imposed on documents is\nNo-Self-Containment.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 12:46:40 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Tchendji", "Maurice Tchoup\u00e9", ""], ["Tadonfouet", "Lionel", ""], ["Tchendji", "Thomas T\u00e9bougang", ""]]}, {"id": "1906.03345", "submitter": "Qinbo Li", "authors": "Qinbo Li, Adam G. D'Souza, Cason Schmit, Hye-Chung Kum", "title": "Increasing Transparent and Accountable Use of Data by Quantifying the\n  Actual Privacy Risk in Interactive Record Linkage", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Record linkage refers to the task of integrating data from two or more\ndatabases without a common identifier. MINDFIRL (MInimum Necessary Disclosure\nFor Interactive Record Linkage) is a software system that demonstrates the\ntradeoff between utility and privacy in interactive record linkage. Due to the\nneed to access personally identifiable information (PII) to accurately assess\nwhether different records refer to the same person in heterogeneous databases,\nprivacy is a major concern in interactive record linkage. MINDFIRL supports\ninteractive record linkage while minimizing the privacy risk by (1) using\npseudonyms to separate the identifying information from the sensitive\ninformation, (2) dynamically disclosing only the minimum necessary information\nincrementally, as needed on-demand at the point of decision, and (3) quantifies\nthe risk due to the needed information disclosure to support transparency, the\nreasoning, communication, and decisions on the privacy and utility trade off.\nIn this paper we present an overview of the MINDFIRL system and the\nk-Anonymized Privacy Risk (KAPR) score used to measure the privacy risk based\non the disclosed information. We prove that KAPR score is a norm meeting all\nthe desirable properties for a risk score for interactive record linkage.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 22:00:08 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Li", "Qinbo", ""], ["D'Souza", "Adam G.", ""], ["Schmit", "Cason", ""], ["Kum", "Hye-Chung", ""]]}, {"id": "1906.03418", "submitter": "Hendrik M\\\"older", "authors": "Hendrik M\\\"older", "title": "A Component-Based Approach to Traffic Data Wrangling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We produce an increasing amount of data. This is positive as it allows us to\nmake better informed decisions if we can base them on a lot of data. However,\nin many domains the `raw' data that is produced, is not usable for analysis due\nto unreadable format, errors, noise, inconsistencies or other factors. An\nexample of such domain is traffic -- traffic data can be used for impactful\ndecision-making from short-term problems to large-scale infrastructure\nprojects.\n  We call the process of preparing data for consumption Data Wrangling. Several\ndata wrangling tools exist that are easy to use and provide general\nfunctionality. However, no one tool is capable of performing complex\ndomain-specific data wrangling operations.\n  The author of this project has chosen two popular programming languages for\ndata science -- R and Python -- for implementing traffic data wrangling\noperators as web services. These web services expose HTTP (Hypertext Transfer\nProtocol) REST (Representational State Transfer) APIs (Application Programming\nInterfaces), which can be used for integrating the services into another\nsystem. As traffic data analysts often lack the necessary programming skills\nrequired for working with complex services, an abstraction layer was designed\nby the author. In the abstraction layer, the author wrapped the data wrangling\nservices inside Taverna components -- this made the services usable via an\neasy-to-use GUI (Graphical User Interface) provided by Taverna Workbench, which\nis a program suitable for carrying out data wrangling tasks. This also enables\nreuse of the components in other workflows.\n  The data wrangling components were tested and validated by using them for two\ncommon traffic data wrangling requests. Datasets from Transport for Greater\nManchester (TfGM) and the Met Office were used to carry out the experiments.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jun 2019 08:41:08 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["M\u00f6lder", "Hendrik", ""]]}, {"id": "1906.03420", "submitter": "Li Zeng", "authors": "Li Zeng, Lei Zou, M. Tamer \\\"Ozsu, Lin Hu, Fan Zhang", "title": "GSI: GPU-friendly Subgraph Isomorphism", "comments": "15 pages, 17 figures, conference", "journal-ref": "IEEE International Conference on Data Engineering 2020", "doi": "10.1109/ICDE48307.2020.00112", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subgraph isomorphism is a well-known NP-hard problem that is widely used in\nmany applications, such as social network analysis and query over the knowledge\ngraph. Due to the inherent hardness, its performance is often a bottleneck in\nvarious real-world applications. Therefore, we address this by designing an\nefficient subgraph isomorphism algorithm leveraging features of GPU\narchitecture, such as massive parallelism and memory hierarchy. Existing\nGPU-based solutions adopt a two-step output scheme, performing the same join\nprocess twice in order to write intermediate results concurrently. They also\nlack GPU architecture-aware optimizations that allow scaling to large graphs.\nIn this paper, we propose a GPU-friendly subgraph isomorphism algorithm, GSI.\nDifferent from existing edge join-based GPU solutions, we propose a\nPrealloc-Combine strategy based on the vertex-oriented framework, which avoids\njoining-twice in existing solutions. Also, a GPU-friendly data structure\n(called PCSR) is proposed to represent an edge-labeled graph. Extensive\nexperiments on both synthetic and real graphs show that GSI outperforms the\nstate-of-the-art algorithms by up to several orders of magnitude and has good\nscalability with graph size scaling to hundreds of millions of edges.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jun 2019 08:47:08 GMT"}, {"version": "v2", "created": "Sun, 16 Jun 2019 18:12:29 GMT"}, {"version": "v3", "created": "Sat, 14 Sep 2019 16:17:32 GMT"}, {"version": "v4", "created": "Tue, 20 Apr 2021 16:52:47 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Zeng", "Li", ""], ["Zou", "Lei", ""], ["\u00d6zsu", "M. Tamer", ""], ["Hu", "Lin", ""], ["Zhang", "Fan", ""]]}, {"id": "1906.04240", "submitter": "Yingbing Hua", "authors": "Yingbing Hua, Bj\\\"orn Hein", "title": "Interpreting OWL Complex Classes in AutomationML based on Bidirectional\n  Translation", "comments": "As accepted to IEEE 24th International Conference on Emerging\n  Technologies and Factory Automation (ETFA 2019)", "journal-ref": null, "doi": "10.1109/ETFA.2019.8869456", "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The World Wide Web Consortium (W3C) has published several recommendations for\nbuilding and storing ontologies, including the most recent OWL 2 Web Ontology\nLanguage (OWL). These initiatives have been followed by practical\nimplementations that popularize OWL in various domains. For example, OWL has\nbeen used for conceptual modeling in industrial engineering, and its reasoning\nfacilities are used to provide a wealth of services, e.g. model diagnosis,\nautomated code generation, and semantic integration. More specifically, recent\nstudies have shown that OWL is well suited for harmonizing information of\nengineering tools stored as AutomationML (AML) files. However, OWL and its\ntools can be cumbersome for direct use by engineers such that an ontology\nexpert is often required in practice. Although much attention has been paid in\nthe literature to overcome this issue by transforming OWL ontologies from/to\nAML models automatically, dealing with OWL complex classes remains an open\nresearch question. In this paper, we introduce the AML concept models for\nrepresenting OWL complex classes in AutomationML, and present algorithms for\nthe bidirectional translation between OWL complex classes and their\ncorresponding AML concept models. We show that this approach provides an\nefficient and intuitive interface for nonexperts to visualize, modify, and\ncreate OWL complex classes.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 09:04:48 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Hua", "Yingbing", ""], ["Hein", "Bj\u00f6rn", ""]]}, {"id": "1906.04304", "submitter": "Jack Rae", "authors": "Jack W Rae, Sergey Bartunov, Timothy P Lillicrap", "title": "Meta-Learning Neural Bloom Filters", "comments": "International Conference on Machine Learning 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a recent trend in training neural networks to replace data\nstructures that have been crafted by hand, with an aim for faster execution,\nbetter accuracy, or greater compression. In this setting, a neural data\nstructure is instantiated by training a network over many epochs of its inputs\nuntil convergence. In applications where inputs arrive at high throughput, or\nare ephemeral, training a network from scratch is not practical. This motivates\nthe need for few-shot neural data structures. In this paper we explore the\nlearning of approximate set membership over a set of data in one-shot via\nmeta-learning. We propose a novel memory architecture, the Neural Bloom Filter,\nwhich is able to achieve significant compression gains over classical Bloom\nFilters and existing memory-augmented neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 22:23:14 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Rae", "Jack W", ""], ["Bartunov", "Sergey", ""], ["Lillicrap", "Timothy P", ""]]}, {"id": "1906.05162", "submitter": "Joana M. F. Da Trindade", "authors": "Joana M. F. da Trindade, Konstantinos Karanasos, Carlo Curino, Samuel\n  Madden, Julian Shun", "title": "Kaskade: Graph Views for Efficient Graph Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs are an increasingly popular way to model real-world entities and\nrelationships between them, ranging from social networks to data lineage graphs\nand biological datasets. Queries over these large graphs often involve\nexpensive subgraph traversals and complex analytical computations. These\nreal-world graphs are often substantially more structured than a generic\nvertex-and-edge model would suggest, but this insight has remained mostly\nunexplored by existing graph engines for graph query optimization purposes.\nTherefore, in this work, we focus on leveraging structural properties of graphs\nand queries to automatically derive materialized graph views that can\ndramatically speed up query evaluation. We present KASKADE, the first graph\nquery optimization framework to exploit materialized graph views for query\noptimization purposes. KASKADE employs a novel constraint-based view\nenumeration technique that mines constraints from query workloads and graph\nschemas, and injects them during view enumeration to significantly reduce the\nsearch space of views to be considered. Moreover, it introduces a graph view\nsize estimator to pick the most beneficial views to materialize given a query\nset and to select the best query evaluation plan given a set of materialized\nviews. We evaluate its performance over real-world graphs, including the\nprovenance graph that we maintain at Microsoft to enable auditing, service\nanalytics, and advanced system optimizations. Our results show that KASKADE\nsubstantially reduces the effective graph size and yields significant\nperformance speedups (up to 50X), in some cases making otherwise intractable\nqueries possible.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 14:21:14 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["da Trindade", "Joana M. F.", ""], ["Karanasos", "Konstantinos", ""], ["Curino", "Carlo", ""], ["Madden", "Samuel", ""], ["Shun", "Julian", ""]]}, {"id": "1906.05366", "submitter": "Amit Kirschenbaum", "authors": "Christian Zinke-Wehlmann and Amit Kirschenbaum", "title": "Geo-L: Linking Geospatial Data Made Easy", "comments": "18 pages, 10 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geospatial Linked Data is an emerging domain with growing interest in\nresearch and industry. There is an increasing number of publicly available\ngeospatial Linked Data resources and they need to be interlinked and easily\nintegrated with private and industrial Linked Data on the Web. The present\npaper introduces Geo-L, a system for discovery of RDF spatial links based on\ntopological relations. Experiments show that the proposed system improves\nstate-of-the-art spatial linking processes in terms of mapping-time and\n-accuracy, as well as concerning resources retrieval efficiency and robustness.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 20:10:29 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2020 17:19:13 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Zinke-Wehlmann", "Christian", ""], ["Kirschenbaum", "Amit", ""]]}, {"id": "1906.05409", "submitter": "Sobhan Moosavi", "authors": "Sobhan Moosavi, Mohammad Hossein Samavatian, Srinivasan Parthasarathy,\n  and Rajiv Ramnath", "title": "A Countrywide Traffic Accident Dataset", "comments": "New preprint, 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reducing traffic accidents is an important public safety challenge. However,\nthe majority of studies on traffic accident analysis and prediction have used\nsmall-scale datasets with limited coverage, which limits their impact and\napplicability; and existing large-scale datasets are either private, old, or do\nnot include important contextual information such as environmental stimuli\n(weather, points-of-interest, etc.). In order to help the research community\naddress these shortcomings we have - through a comprehensive process of data\ncollection, integration, and augmentation - created a large-scale publicly\navailable database of accident information named US-Accidents. US-Accidents\ncurrently contains data about $2.25$ million instances of traffic accidents\nthat took place within the contiguous United States, and over the last three\nyears. Each accident record consists of a variety of intrinsic and contextual\nattributes such as location, time, natural language description, weather,\nperiod-of-day, and points-of-interest. We present this dataset in this paper,\nalong with a wide range of insights gleaned from this dataset with respect to\nthe spatiotemporal characteristics of accidents. The dataset is publicly\navailable at https://smoosavi.org/datasets/us_accidents.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 22:26:06 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Moosavi", "Sobhan", ""], ["Samavatian", "Mohammad Hossein", ""], ["Parthasarathy", "Srinivasan", ""], ["Ramnath", "Rajiv", ""]]}, {"id": "1906.05457", "submitter": "Shuyuan Zheng", "authors": "Shuyuan Zheng and Yang Cao and Masatoshi Yoshikawa", "title": "Trading Location Data with Bounded Personalized Privacy Loss", "comments": "Proceedings of the Third Workshop on Software Foundations for Data\n  Interoperability (SFDI2019+), October 28, 2019, Fukuoka, Japan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As personal data have been the new oil of the digital era, there is a growing\ntrend perceiving personal data as a commodity. Although some people are willing\nto trade their personal data for money, they might still expect limited privacy\nloss, and the maximum tolerable privacy loss varies with each individual. In\nthis paper, we propose a framework that enables individuals to trade their\npersonal data with bounded personalized privacy loss, which raises technical\nchallenges in the aspects of budget allocation and arbitrage-freeness. To deal\nwith those challenges,we propose two arbitrage-free trading mechanisms with\ndifferent advantages.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 02:22:32 GMT"}, {"version": "v2", "created": "Sat, 29 Jun 2019 16:58:17 GMT"}, {"version": "v3", "created": "Thu, 24 Oct 2019 07:51:19 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Zheng", "Shuyuan", ""], ["Cao", "Yang", ""], ["Yoshikawa", "Masatoshi", ""]]}, {"id": "1906.05505", "submitter": "Xiuwen Zheng", "authors": "Xiuwen Zheng and Qiyu Liu and Amarnath Gupta", "title": "Scalable Community Detection over Geo-Social Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a community finding problem called Co-located Community Detection\n(CCD) over geo-social networks, which retrieves communities that satisfy both\nhigh structural tightness and spatial closeness constraints. To provide a\nsolution that benefits from existing studies on community detection, we\ndecouple the spatial constraint from graph structural constraint and propose a\nuniform CCD framework which gives users the freedom to choose customized\nmeasurements for social cohesiveness (e.g., $k$-core or $k$-truss). For the\nspatial closeness constraint, we apply the bounded radius spatial constraint\nand develop an exact algorithm together with effective pruning rules. To\nfurther improve the efficiency and make our framework scale to a very large\nscale of data, we propose a near-linear time approximation algorithm with a\nconstant approximation ratio ($\\sqrt{2}$). We conduct extensive experiments on\nboth synthetic and real-world datasets to demonstrate the efficiency and\neffectiveness of our algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 06:50:02 GMT"}, {"version": "v2", "created": "Fri, 14 Jun 2019 06:07:39 GMT"}, {"version": "v3", "created": "Fri, 23 Aug 2019 20:35:59 GMT"}, {"version": "v4", "created": "Fri, 30 Aug 2019 18:00:52 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Zheng", "Xiuwen", ""], ["Liu", "Qiyu", ""], ["Gupta", "Amarnath", ""]]}, {"id": "1906.05677", "submitter": "Yuanyuan Tian", "authors": "Brian Hentschel, Peter J. Haas, Yuanyuan Tian", "title": "Temporally-Biased Sampling Schemes for Online Model Management", "comments": "49 pages, 18 figures. arXiv admin note: substantial text overlap with\n  arXiv:1801.09709", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To maintain the accuracy of supervised learning models in the presence of\nevolving data streams, we provide temporally-biased sampling schemes that\nweight recent data most heavily, with inclusion probabilities for a given data\nitem decaying over time according to a specified \"decay function\". We then\nperiodically retrain the models on the current sample. This approach speeds up\nthe training process relative to training on all of the data. Moreover,\ntime-biasing lets the models adapt to recent changes in the data while---unlike\nin a sliding-window approach---still keeping some old data to ensure robustness\nin the face of temporary fluctuations and periodicities in the data values. In\naddition, the sampling-based approach allows existing analytic algorithms for\nstatic data to be applied to dynamic streaming data essentially without change.\nWe provide and analyze both a simple sampling scheme (T-TBS) that\nprobabilistically maintains a target sample size and a novel reservoir-based\nscheme (R-TBS) that is the first to provide both control over the decay rate\nand a guaranteed upper bound on the sample size. If the decay function is\nexponential, then control over the decay rate is complete, and R-TBS maximizes\nboth expected sample size and sample-size stability. For general decay\nfunctions, the actual item inclusion probabilities can be made arbitrarily\nclose to the nominal probabilities, and we provide a scheme that allows a\ntrade-off between sample footprint and sample-size stability. The R-TBS and\nT-TBS schemes are of independent interest, extending the known set of\nunequal-probability sampling schemes. We discuss distributed implementation\nstrategies; experiments in Spark illuminate the performance and scalability of\nthe algorithms, and show that our approach can increase machine learning\nrobustness in the face of evolving data.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 21:02:38 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Hentschel", "Brian", ""], ["Haas", "Peter J.", ""], ["Tian", "Yuanyuan", ""]]}, {"id": "1906.05745", "submitter": "Konstantinos Xirogiannopoulos", "authors": "Konstantinos Xirogiannopoulos, Amol Deshpande", "title": "Memory-Efficient Group-by Aggregates over Multi-Way Joins", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aggregate computation in relational databases has long been done using the\nstandard unary aggregation and binary join operators. These implement the\nclassical model of computing joins between relations two at a time,\nmaterializing the intermediate results, and then proceeding to do either\nsort-based or hash-based aggregate computation to derive the final result. This\napproach, however, can be dramatically sub-optimal in the case of\nlow-selectivity joins, and often ends up generating large intermediate results\neven if the relations involved as well as the final result sets themselves are\nquite small. Moreover, many of the computed intermediate results may never be\npart of the final result. In this paper we propose a novel aggregate query\nprocessing technique that leverages graph data structures towards efficiently\nanswering aggregate queries over joins, without materializing intermediate join\nresults. We wrap this technique inside a multi-way composite database operator\ncalled JOIN-AGG that combines Join and Aggregation. We provide a general\nframework for executing aggregation queries over arbitrary acyclic joins,\ninvolving any number of group-by attributes from any relation. We also present\na thorough experimental evaluation on both real world and synthetic datasets.\nOur experiments show that our operators can achieve orders of magnitude lower\nquery times and memory requirements compared to the traditional approach, even\nwhen implemented outside of the database system.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 15:15:20 GMT"}, {"version": "v2", "created": "Mon, 17 Jun 2019 14:18:18 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Xirogiannopoulos", "Konstantinos", ""], ["Deshpande", "Amol", ""]]}, {"id": "1906.05937", "submitter": "EPTCS", "authors": "Antonin Delpeuch (University of Oxford)", "title": "A Complete Language for Faceted Dataflow Programs", "comments": "In Proceedings ACT 2019, arXiv:2009.06334", "journal-ref": "EPTCS 323, 2020, pp. 1-14", "doi": "10.4204/EPTCS.323.1", "report-no": null, "categories": "cs.LO cs.DB math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a complete categorical axiomatization of a wide class of dataflow\nprograms. This gives a three-dimensional diagrammatic language for workflows,\nmore expressive than the directed acyclic graphs generally used for this\npurpose. This calls for an implementation of these representations in data\ntransformation tools.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 21:35:19 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 02:12:39 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Delpeuch", "Antonin", "", "University of Oxford"]]}, {"id": "1906.06085", "submitter": "Dimitri Vorona", "authors": "Dimitri Vorona, Andreas Kipf, Thomas Neumann, Alfons Kemper", "title": "DeepSPACE: Approximate Geospatial Query Processing with Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The amount of the available geospatial data grows at an ever faster pace.\nThis leads to the constantly increasing demand for processing power and storage\nin order to provide data analysis in a timely manner. At the same time, a lot\nof geospatial processing is visual and exploratory in nature, thus having\nbounded precision requirements. We present DeepSPACE, a deep learning-based\napproximate geospatial query processing engine which combines modest hardware\nrequirements with the ability to answer flexible aggregation queries while\nkeeping the required state to a few hundred KiBs.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 09:16:16 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Vorona", "Dimitri", ""], ["Kipf", "Andreas", ""], ["Neumann", "Thomas", ""], ["Kemper", "Alfons", ""]]}, {"id": "1906.06314", "submitter": "Jinfei Liu", "authors": "Jinfei Liu, Li Xiong, Qiuchen Zhang, Jian Pei, and Jun Luo", "title": "Eclipse: Generalizing kNN and Skyline", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  $k$ nearest neighbor ($k$NN) queries and skyline queries are important\noperators on multi-dimensional data points. Given a query point, $k$NN query\nreturns the $k$ nearest neighbors based on a scoring function such as a\nweighted sum of the attributes, which requires predefined attribute weights (or\npreferences). Skyline query returns all possible nearest neighbors for any\nmonotonic scoring functions without requiring attribute weights but the number\nof returned points can be prohibitively large. We observe that both $k$NN and\nskyline are inflexible and cannot be easily customized.\n  In this paper, we propose a novel \\emph{eclipse} operator that generalizes\nthe classic $1$NN and skyline queries and provides a more flexible and\ncustomizable query solution for users. In eclipse, users can specify rough and\ncustomizable attribute preferences and control the number of returned points.\nWe show that both $1$NN and skyline are instantiations of eclipse. To process\neclipse queries, we propose a baseline algorithm with time complexity\n$O(n^22^{d-1})$, and an improved $O(n\\log ^{d-1}n)$ time transformation-based\nalgorithm, where $n$ is the number of points and $d$ is the number of\ndimensions. Furthermore, we propose a novel index-based algorithm utilizing\nduality transform with much better efficiency. The experimental results on the\nreal NBA dataset and the synthetic datasets demonstrate the effectiveness of\nthe eclipse operator and the efficiency of our eclipse algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 17:43:07 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Liu", "Jinfei", ""], ["Xiong", "Li", ""], ["Zhang", "Qiuchen", ""], ["Pei", "Jian", ""], ["Luo", "Jun", ""]]}, {"id": "1906.06331", "submitter": "Nima Miryeganeh", "authors": "Nima Miryeganeh, Mehdi Amoui and Hadi Hemmati", "title": "An IR-based Approach Towards Automated Integration of Geo-spatial\n  Datasets in Map-based Software Systems", "comments": "ESEC/FSE 2019 - Industry track", "journal-ref": null, "doi": "10.1145/3338906.3340454", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Data is arguably the most valuable asset of the modern world. In this era,\nthe success of any data-intensive solution relies on the quality of data that\ndrives it. Among vast amount of data that are captured, managed, and analyzed\neveryday, geospatial data are one of the most interesting class of data that\nhold geographical information of real-world phenomena and can be visualized as\ndigital maps. Geo-spatial data is the source of many enterprise solutions that\nprovide local information and insights. In order to increase the quality of\nsuch solutions, companies continuously aggregate geospatial datasets from\nvarious sources. However, lack of a global standard model for geospatial\ndatasets makes the task of merging and integrating datasets difficult and\nerror-prone. Traditionally, domain experts manually validate the data\nintegration process by merging new data sources and/or new versions of previous\ndata against conflicts and other requirement violations. However, this approach\nis not scalable and is hinder toward rapid release, when dealing with\nfrequently changing big datasets. Thus more automated approaches with limited\ninteraction with domain experts is required. As a first step to tackle this\nproblem, in this paper, we leverage Information Retrieval (IR) and geospatial\nsearch techniques to propose a systematic and automated conflict identification\napproach. To evaluate our approach, we conduct a case study in which we measure\nthe accuracy of our approach in several real-world scenarios and we interview\nwith software developers at Localintel Inc. (our industry partner) to get their\nfeedbacks.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 22:26:38 GMT"}, {"version": "v2", "created": "Thu, 27 Jun 2019 16:53:39 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Miryeganeh", "Nima", ""], ["Amoui", "Mehdi", ""], ["Hemmati", "Hadi", ""]]}, {"id": "1906.06492", "submitter": "Umutcan \\c{S}im\\c{s}ek", "authors": "Umutcan \\c{S}im\\c{s}ek, Kevin Angele, Elias K\\\"arle, Oleksandra\n  Panasiuk and Dieter Fensel", "title": "A formal approach for customization of schema.org based on SHACL", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Schema.org is a widely adopted vocabulary for semantic annotation of content\nand data. However, its generic nature makes it complicated for data publishers\nto pick right types and properties for a specific domain and task. In this\npaper we propose a formal approach, a domain specification process that\ngenerates domain specific patterns by applying operators implemented in SHACL\nto the schema.org vocabulary. These patterns can support knowledge generation\nand assessment processes for specific domains and tasks. We demonstrated our\napproach with use cases in tourism domain.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 08:09:19 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["\u015eim\u015fek", "Umutcan", ""], ["Angele", "Kevin", ""], ["K\u00e4rle", "Elias", ""], ["Panasiuk", "Oleksandra", ""], ["Fensel", "Dieter", ""]]}, {"id": "1906.06542", "submitter": "Jing Chen", "authors": "Xinyu Wei, Jiahui Chen, Jing Chen, Bernie Liu", "title": "A Books Recommendation Approach Based on Online Bookstore Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the era of information explosion, facing complex information, it is\ndifficult for users to choose the information of interest, and businesses also\nneed detailed information on ways to let the ad stand out. By this time, it is\nrecommended that a good way. We firstly by using random interviews,\nsimulations, asking experts, summarizes methods outlined the main factors\naffecting the scores of books that users drew. In order to further illustrate\nthe impact of these factors, we also by combining the AHP consistency test,\nthen fuzzy evaluation method, empowered each factor, influencing factors and\nthe degree of influence come. For the second question, predict user evaluation\nof the listed books from the predict annex. First, given the books Annex\nlabels, user data extraction scorebooks and mathematical analysis of data\nobtained from SPSS user preferences and then use software to nearest neighbor\nanalysis to result in predicted value.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 12:32:17 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Wei", "Xinyu", ""], ["Chen", "Jiahui", ""], ["Chen", "Jing", ""], ["Liu", "Bernie", ""]]}, {"id": "1906.06574", "submitter": "Ji Sun", "authors": "Ji Sun, Dong Deng, Ihab Ilyas, Guoliang Li, Samuel Madden, Mourad\n  Ouzzani, Michael Stonebraker, Nan Tang", "title": "Technical Report: Optimizing Human Involvement for Entity Matching and\n  Consolidation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An end-to-end data integration system requires human feedback in several\nphases, including collecting training data for entity matching, debugging the\nresulting clusters, confirming transformations applied on these clusters for\ndata standardization, and finally, reducing each cluster to a single, canonical\nrepresentation (or \"golden record\"). The traditional wisdom is to sequentially\napply the human feedback, obtained by asking specific questions, within some\nbudget in each phase. However, these questions are highly correlated; the\nanswer to one can influence the outcome of any of the phases of the pipeline.\nHence, interleaving them has the potential to offer significant benefits.\n  In this paper, we propose a human-in-the-loop framework that interleaves\ndifferent types of questions to optimize human involvement. We propose benefit\nmodels to measure the quality improvement from asking a question, and cost\nmodels to measure the human time it takes to answer a question. We develop a\nquestion scheduling framework that judiciously selects questions to maximize\nthe accuracy of the final golden records. Experimental results on three\nreal-world datasets show that our holistic method significantly improves the\nquality of golden records from 70% to 90%, compared with the state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 14:53:30 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Sun", "Ji", ""], ["Deng", "Dong", ""], ["Ilyas", "Ihab", ""], ["Li", "Guoliang", ""], ["Madden", "Samuel", ""], ["Ouzzani", "Mourad", ""], ["Stonebraker", "Michael", ""], ["Tang", "Nan", ""]]}, {"id": "1906.06590", "submitter": "Alekh Jindal", "authors": "Alekh Jindal, Lalitha Viswanathan, Konstantinos Karanasos", "title": "Query and Resource Optimizations: A Case for Breaking the Wall in Big\n  Data Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern big data systems run on cloud environments where resources are shared\namongst several users and applications. As a result, declarative user queries\nin these environments need to be optimized and executed over resources that\nconstantly change and are provisioned on demand for each job. This requires us\nto rethink traditional query optimizers designed for systems that run on\ndedicated resources. In this paper, we show evidence that the choice of query\nplans depends heavily on the available resources, and the current practice of\nchoosing query plans before picking the resources could lead to significant\nperformance loss in two popular big data systems, namely Hive and SparkSQL.\nTherefore, we make a case for Resource and Query Optimization (or RAQO), i.e.,\nchoosing both the query plan and the resource configuration at the same time.\nWe describe rule-based RAQO and present alternate decisions trees to make\nresource-aware query planning in Hive and Spark. We further present cost-based\nRAQO that integrates resource planning within a query planner, and show\ntechniques to significantly reduce the resource planning overheads. We evaluate\ncost-based RAQO using state-of-the-art System R query planner as well as a\nrecently proposed multi-objective query planner. Our evaluation on TPC-H and\nrandomly generated schemas show that: (i) we can reduce the resource planning\noverhead by up to 16x, and (ii) RAQO can scale to schemas as large as 100 table\njoins as well as clusters as big as 100K containers with 100GB each.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 16:41:16 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Jindal", "Alekh", ""], ["Viswanathan", "Lalitha", ""], ["Karanasos", "Konstantinos", ""]]}, {"id": "1906.06956", "submitter": "Panagiotis Tampakis", "authors": "Panagiotis Tampakis, Nikos Pelekis, Christos Doulkeridis and Yannis\n  Theodoridis", "title": "Scalable Distributed Subtrajectory Clustering", "comments": null, "journal-ref": "2019 IEEE International Conference on Big Data (Big Data)", "doi": "10.1109/BigData47090.2019.9005563", "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trajectory clustering is an important operation of knowledge discovery from\nmobility data. Especially nowadays, the need for performing advanced analytic\noperations over massively produced data, such as mobility traces, in efficient\nand scalable ways is imperative. However, discovering clusters of complete\ntrajectories can overlook significant patterns that exist only for a small\nportion of their lifespan. In this paper, we address the problem of Distributed\nSubtrajectory Clustering in an efficient and highly scalable way. The problem\nis challenging because the subtrajectories to be clustered are not known in\nadvance, but they need to be discovered dynamically based on adjacent\nsubtrajectories in space and time. Towards this objective, we split the\noriginal problem to three sub-problems, namely Subtrajectory Join, Trajectory\nSegmentation and Clustering and Outlier Detection, and deal with each one in a\ndistributed fashion by utilizing the MapReduce programming model. The\nefficiency and the effectiveness of our solution is demonstrated experimentally\nover a synthetic and two large real datasets from the maritime and urban\ndomains and through comparison with two state of the art subtrajectory\nclustering algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 11:15:21 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2019 07:32:22 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Tampakis", "Panagiotis", ""], ["Pelekis", "Nikos", ""], ["Doulkeridis", "Christos", ""], ["Theodoridis", "Yannis", ""]]}, {"id": "1906.07850", "submitter": "Mohammad Javad Amiri", "authors": "Mohammad Javad Amiri, Sujaya Maiyya, Divyakant Agrawal, Amr El Abbadi", "title": "SeeMoRe: A Fault-Tolerant Protocol for Hybrid Cloud Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale data management systems utilize State Machine Replication to\nprovide fault tolerance and to enhance performance. Fault-tolerant protocols\nare extensively used in the distributed database infrastructure of large\nenterprises such as Google, Amazon, and Facebook, as well as permissioned\nblockchain systems like IBM's Hyperledger Fabric. However, and in spite of\nyears of intensive research, existing fault-tolerant protocols do not\nadequately address all the characteristics of distributed system applications.\nIn particular, hybrid cloud environments consisting of private and public\nclouds are widely used by enterprises. However, fault-tolerant protocols have\nnot been adapted for such environments. In this paper, we introduce SeeMoRe, a\nhybrid State Machine Replication protocol to handle both crash and malicious\nfailures in a public/private cloud environment. SeeMoRe considers a private\ncloud consisting of nonmalicious nodes (either correct or crash) and a public\ncloud with both Byzantine faulty and correct nodes. SeeMoRe has three different\nmodes which can be used depending on the private cloud load and the\ncommunication latency between the public and the private cloud. We also\nintroduce a dynamic mode switching technique to transition from one mode to\nanother. Furthermore, we evaluate SeeMoRe using a series of benchmarks. The\nexperiments reveal that SeeMoRe's performance is close to the state of the art\ncrash fault-tolerant protocols while tolerating malicious failures.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 23:45:35 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Amiri", "Mohammad Javad", ""], ["Maiyya", "Sujaya", ""], ["Agrawal", "Divyakant", ""], ["Abbadi", "Amr El", ""]]}, {"id": "1906.08042", "submitter": "Kun Qian", "authors": "Jungo Kasai, Kun Qian, Sairam Gurajada, Yunyao Li, Lucian Popa", "title": "Low-resource Deep Entity Resolution with Transfer and Active Learning", "comments": "This paper is accepted by ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity resolution (ER) is the task of identifying different representations\nof the same real-world entities across databases. It is a key step for\nknowledge base creation and text mining. Recent adaptation of deep learning\nmethods for ER mitigates the need for dataset-specific feature engineering by\nconstructing distributed representations of entity records. While these methods\nachieve state-of-the-art performance over benchmark data, they require large\namounts of labeled data, which are typically unavailable in realistic ER\napplications. In this paper, we develop a deep learning-based method that\ntargets low-resource settings for ER through a novel combination of transfer\nlearning and active learning. We design an architecture that allows us to learn\na transferable model from a high-resource setting to a low-resource one. To\nfurther adapt to the target dataset, we incorporate active learning that\ncarefully selects a few informative examples to fine-tune the transferred\nmodel. Empirical evaluation demonstrates that our method achieves comparable,\nif not better, performance compared to state-of-the-art learning-based methods\nwhile using an order of magnitude fewer labels.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 20:33:24 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Kasai", "Jungo", ""], ["Qian", "Kun", ""], ["Gurajada", "Sairam", ""], ["Li", "Yunyao", ""], ["Popa", "Lucian", ""]]}, {"id": "1906.08092", "submitter": "Antonin Delpeuch", "authors": "Antonin Delpeuch", "title": "A survey of OpenRefine reconciliation services", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We review the services implementing the OpenRefine reconciliation API,\ncomparing their design to the state of the art in record linkage. Due to the\ndesign of the API, the matching scores returned by the services are of little\nhelp to guide matching decisions. This suggests possible improvements to the\nspecifications of the API, which could improve user workflows by giving more\ncontrol over the scoring mechanism to the client.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 13:35:41 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 13:23:17 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Delpeuch", "Antonin", ""]]}, {"id": "1906.08097", "submitter": "Luigi Asprino", "authors": "Luigi Asprino, Wouter Beek, Paolo Ciancarini, Frank van Harmelen and\n  Valentina Presutti", "title": "Observing LOD using Equivalent Set Graphs: it is mostly flat and\n  sparsely linked", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents an empirical study aiming at understanding the modeling\nstyle and the overall semantic structure of Linked Open Data. We observe how\nclasses, properties and individuals are used in practice. We also investigate\nhow hierarchies of concepts are structured, and how much they are linked. In\naddition to discussing the results, this paper contributes (i) a conceptual\nframework, including a set of metrics, which generalises over the observable\nconstructs; (ii) an open source implementation that facilitates its application\nto other Linked Data knowledge graphs.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 13:48:08 GMT"}, {"version": "v2", "created": "Mon, 8 Jul 2019 16:49:46 GMT"}, {"version": "v3", "created": "Thu, 18 Jul 2019 16:48:47 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Asprino", "Luigi", ""], ["Beek", "Wouter", ""], ["Ciancarini", "Paolo", ""], ["van Harmelen", "Frank", ""], ["Presutti", "Valentina", ""]]}, {"id": "1906.08149", "submitter": "Mahawaga Arachchige Pathum Chamikara", "authors": "M.A.P. Chamikara, P. Bertok, D. Liu, S. Camtepe, I. Khalil", "title": "Efficient privacy preservation of big data for accurate data mining", "comments": "Information Sciences", "journal-ref": null, "doi": "10.1016/j.ins.2019.05.053", "report-no": null, "categories": "cs.DB cs.CR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Computing technologies pervade physical spaces and human lives, and produce a\nvast amount of data that is available for analysis. However, there is a growing\nconcern that potentially sensitive data may become public if the collected data\nare not appropriately sanitized before being released for investigation.\nAlthough there are more than a few privacy-preserving methods available, they\nare not efficient, scalable or have problems with data utility, and/or privacy.\nThis paper addresses these issues by proposing an efficient and scalable\nnonreversible perturbation algorithm, PABIDOT, for privacy preservation of big\ndata via optimal geometric transformations. PABIDOT was tested for efficiency,\nscalability, resistance, and accuracy using nine datasets and five\nclassification algorithms. Experiments show that PABIDOT excels in execution\nspeed, scalability, attack resistance and accuracy in large-scale\nprivacy-preserving data classification when compared with two other, related\nprivacy-preserving algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 15:23:41 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Chamikara", "M. A. P.", ""], ["Bertok", "P.", ""], ["Liu", "D.", ""], ["Camtepe", "S.", ""], ["Khalil", "I.", ""]]}, {"id": "1906.08231", "submitter": "Maurice Tchoup\\'e Tchendji", "authors": "Maurice Tchoup\\'e Tchendji, Adolphe Gaius Nkuefone, and Thomas\n  T\\'ebougang Tchendji", "title": "Holistic evaluation of XML queries with structural preferences on an\n  annotated strong dataguide", "comments": null, "journal-ref": "International Journal of Software Engineering & Applications\n  (IJSEA), Vol.10, No.3, May 2019", "doi": "10.5121/ijsea.2019.10304", "report-no": null, "categories": "cs.DB cs.IR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  With the emergence of XML as de facto format for storing and exchanging\ninformation over the Internet, the search for ever more innovative and\neffective techniques for their querying is a major and current concern of the\nXML database community. Several studies carried out to help solve this problem\nare mostly oriented towards the evaluation of so-called exact queries which,\nunfortunately, are likely (especially in the case of semi-structured documents)\nto yield abundant results (in the case of vague queries) or empty results (in\nthe case of very precise queries). From the observation that users who make\nrequests are not necessarily interested in all possible solutions, but rather\nin those that are closest to their needs, an important field of research has\nbeen opened on the evaluation of preferences queries. In this paper, we propose\nan approach for the evaluation of such queries, in case the preferences concern\nthe structure of the document. The solution investigated revolves around the\nproposal of an evaluation plan in three phases: rewriting-evaluation-merge. The\nrewriting phase makes it possible to obtain, from a partitioning\n-transformation operation of the initial query, a hierarchical set of\npreferences path queries which are holistically evaluated in the second phase\nby an instrumented version of the algorithm TwigStack. The merge phase is the\nsynthesis of the best results.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 11:23:18 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Tchendji", "Maurice Tchoup\u00e9", ""], ["Nkuefone", "Adolphe Gaius", ""], ["Tchendji", "Thomas T\u00e9bougang", ""]]}, {"id": "1906.08510", "submitter": "Canchen Li", "authors": "Canchen Li", "title": "Preprocessing Methods and Pipelines of Data Mining: An Overview", "comments": "7 pages, 3 figures, IEEE conference format", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data mining is about obtaining new knowledge from existing datasets. However,\nthe data in the existing datasets can be scattered, noisy, and even incomplete.\nAlthough lots of effort is spent on developing or fine-tuning data mining\nmodels to make them more robust to the noise of the input data, their qualities\nstill strongly depend on the quality of it. The article starts with an overview\nof the data mining pipeline, where the procedures in a data mining task are\nbriefly introduced. Then an overview of the data preprocessing techniques which\nare categorized as the data cleaning, data transformation and data\npreprocessing is given. Detailed preprocessing methods, as well as their\ninfluenced on the data mining models, are covered in this article.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 09:11:04 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Li", "Canchen", ""]]}, {"id": "1906.08574", "submitter": "Patricia Serrano-Alvarado Mme", "authors": "Nassopoulos Georges and Serrano-Alvarado Patricia and Molli Pascal and\n  Desmontils Emmanuel", "title": "Extracting Basic Graph Patterns from Triple Pattern Fragment Logs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Triple Pattern Fragment (TPF) approach is de-facto a new way to publish\nLinked Data at low cost and with high server availability. However, data\nproviders hosting TPF servers are not able to analyze the SPARQL queries they\nexecute because they only receive and evaluate queries with one triple pattern.\nIn this paper, we propose LIFT: an algorithm to extract Basic Graph Patterns\n(BGPs) of executed queries from TPF server logs. Experiments show that LIFT\nextracts BGPs with good precision and good recall generating limited noise.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 12:22:00 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Georges", "Nassopoulos", ""], ["Patricia", "Serrano-Alvarado", ""], ["Pascal", "Molli", ""], ["Emmanuel", "Desmontils", ""]]}, {"id": "1906.08687", "submitter": "Maximilian Schleich", "authors": "Maximilian Schleich and Dan Olteanu and Mahmoud Abo Khamis and Hung Q.\n  Ngo and XuanLong Nguyen", "title": "A Layered Aggregate Engine for Analytics Workloads", "comments": "18 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces LMFAO (Layered Multiple Functional Aggregate\nOptimization), an in-memory optimization and execution engine for batches of\naggregates over the input database. The primary motivation for this work stems\nfrom the observation that for a variety of analytics over databases, their\ndata-intensive tasks can be decomposed into group-by aggregates over the join\nof the input database relations. We exemplify the versatility and\ncompetitiveness of LMFAO for a handful of widely used analytics: learning ridge\nlinear regression, classification trees, regression trees, and the structure of\nBayesian networks using Chow-Liu trees; and data cubes used for exploration in\ndata warehousing.\n  LMFAO consists of several layers of logical and code optimizations that\nsystematically exploit sharing of computation, parallelism, and code\nspecialization.\n  We conducted two types of performance benchmarks. In experiments with four\ndatasets, LMFAO outperforms by several orders of magnitude on one hand, a\ncommercial database system and MonetDB for computing batches of aggregates, and\non the other hand, TensorFlow, Scikit, R, and AC/DC for learning a variety of\nmodels over databases.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 15:20:20 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Schleich", "Maximilian", ""], ["Olteanu", "Dan", ""], ["Khamis", "Mahmoud Abo", ""], ["Ngo", "Hung Q.", ""], ["Nguyen", "XuanLong", ""]]}, {"id": "1906.08724", "submitter": "Fabian Neuhaus", "authors": "Bernd Krieg-Br\\\"uckner and Till Mossakowski and Fabian Neuhaus", "title": "Generic Ontology Design Patterns at Work", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generic Ontology Design Patterns, GODPs, are defined in Generic DOL, an\nextension of DOL, the Distributed Ontology, Model and Specification Language,\nand implemented using Heterogeneous Tool Set.\n  Parameters such as classes, properties, individuals, or whole ontologies may\nbe instantiated with arguments in a host ontology. The potential of Generic DOL\nis illustrated with GODPs for an example from the literature, namely the Role\ndesign pattern. We also discuss how larger GODPs may be composed by\ninstantiating smaller GODPs.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 16:12:12 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Krieg-Br\u00fcckner", "Bernd", ""], ["Mossakowski", "Till", ""], ["Neuhaus", "Fabian", ""]]}, {"id": "1906.08874", "submitter": "Sotiris Moschoyiannis Dr", "authors": "Matthew R Karlsen and Sotiris K. Moschoyiannis", "title": "Customer Segmentation of Wireless Trajectory Data", "comments": "Technical Report, University of Surrey, UK, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wireless trajectory data consists of a number of (time, point) entries where\neach point is associated with a particular wireless device (WAP or BLE beacon)\ntied to a location identifier, such as a place name. A trajectory relates to a\nparticular mobile device. Such data can be clustered `semantically' to identify\nsimilar trajectories, where similarity relates to non-geographic\ncharacteristics such as the type of location visited. Here we present a new\napproach to semantic trajectory clustering for such data. The approach is\napplicable to interpreting data that does not contain geographical coordinates,\nand thus contributes to the current literature on semantic trajectory\nclustering. The literature does not appear to provide such an approach, instead\nfocusing on trajectory data where latitude and longitude data is available.\n  We apply the techniques developed above in the context of the Onward Journey\nPlanner Application, with the motivation of providing on-line recommendations\nfor onward journey options in a context-specific manner. The trajectories\nanalysed indicate commute patterns on the London Underground. Points are only\nrecorded for communication with WAP and BLE beacons within the rail network.\nThis context presents additional challenge since the trajectories are\n`truncated', with no true origin and destination details.\n  In the above context we find that there are a range of travel patterns in the\ndata, without the existence of distinct clusters. Suggestions are made\nconcerning how to approach the problem of provision of on-line recommendations\nwith such a data set. Thoughts concerning the related problem of prediction of\njourney route and destination are also provided.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 21:32:33 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Karlsen", "Matthew R", ""], ["Moschoyiannis", "Sotiris K.", ""]]}, {"id": "1906.08986", "submitter": "Wei Wang", "authors": "Wei Wang and Meihui Zhang and Gang Chen and H. V. Jagadish and Beng\n  Chin Ooi and Kian-Lee Tan", "title": "Database Meets Deep Learning: Challenges and Opportunities", "comments": "The first version of this paper has appeared in SIGMOD Record. In\n  this (third) version, we extend it to include the recent developments in this\n  field and references to recent work (especially for section 3.2 and section\n  4.2)", "journal-ref": "ACM SIGMOD Record, Volume 45 Issue 2, June 2016, Pages 17-22", "doi": "10.1145/3003665.3003669", "report-no": null, "categories": "cs.DB cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has recently become very popular on account of its incredible\nsuccess in many complex data-driven applications, such as image classification\nand speech recognition. The database community has worked on data-driven\napplications for many years, and therefore should be playing a lead role in\nsupporting this new wave. However, databases and deep learning are different in\nterms of both techniques and applications. In this paper, we discuss research\nproblems at the intersection of the two fields. In particular, we discuss\npossible improvements for deep learning systems from a database perspective,\nand analyze database applications that may benefit from deep learning\ntechniques.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 07:26:31 GMT"}, {"version": "v2", "created": "Sun, 19 Jan 2020 03:34:25 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Wang", "Wei", ""], ["Zhang", "Meihui", ""], ["Chen", "Gang", ""], ["Jagadish", "H. V.", ""], ["Ooi", "Beng Chin", ""], ["Tan", "Kian-Lee", ""]]}, {"id": "1906.08990", "submitter": "Kurt Stockinger", "authors": "Katrin Affolter, Kurt Stockinger, Abraham Bernstein", "title": "A Comparative Survey of Recent Natural Language Interfaces for Databases", "comments": null, "journal-ref": "VLDB Journal 2019", "doi": "10.1007/s00778-019-00567-8", "report-no": null, "categories": "cs.DB cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over the last few years natural language interfaces (NLI) for databases have\ngained significant traction both in academia and industry. These systems use\nvery different approaches as described in recent survey papers. However, these\nsystems have not been systematically compared against a set of benchmark\nquestions in order to rigorously evaluate their functionalities and expressive\npower.\n  In this paper, we give an overview over 24 recently developed NLIs for\ndatabases. Each of the systems is evaluated using a curated list of ten sample\nquestions to show their strengths and weaknesses. We categorize the NLIs into\nfour groups based on the methodology they are using: keyword-, pattern-,\nparsing-, and grammar-based NLI. Overall, we learned that keyword-based systems\nare enough to answer simple questions. To solve more complex questions\ninvolving subqueries, the system needs to apply some sort of parsing to\nidentify structural dependencies. Grammar-based systems are overall the most\npowerful ones, but are highly dependent on their manually designed rules. In\naddition to providing a systematic analysis of the major systems, we derive\nlessons learned that are vital for designing NLIs that can answer a wide range\nof user questions.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 07:49:36 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Affolter", "Katrin", ""], ["Stockinger", "Kurt", ""], ["Bernstein", "Abraham", ""]]}, {"id": "1906.09198", "submitter": "Paolo Papotti", "authors": "Naser Ahmadi, Joohyung Lee, Paolo Papotti, Mohammed Saeed", "title": "Explainable Fact Checking with Probabilistic Answer Set Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One challenge in fact checking is the ability to improve the transparency of\nthe decision. We present a fact checking method that uses reference information\nin knowledge graphs (KGs) to assess claims and explain its decisions. KGs\ncontain a formal representation of knowledge with semantic descriptions of\nentities and their relationships. We exploit such rich semantics to produce\ninterpretable explanations for the fact checking output. As information in a KG\nis inevitably incomplete, we rely on logical rule discovery and on Web text\nmining to gather the evidence to assess a given claim. Uncertain rules and\nfacts are turned into logical programs and the checking task is modeled as an\ninference problem in a probabilistic extension of answer set programs.\nExperiments show that the probabilistic inference enables the efficient\nlabeling of claims with interpretable explanations, and the quality of the\nresults is higher than state of the art baselines.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 15:35:03 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Ahmadi", "Naser", ""], ["Lee", "Joohyung", ""], ["Papotti", "Paolo", ""], ["Saeed", "Mohammed", ""]]}, {"id": "1906.09335", "submitter": "Stavros Sintos", "authors": "Brett Walenz, Stavros Sintos, Sudeepa Roy, Jun Yang", "title": "Learning to Sample: Counting with Complex Queries", "comments": null, "journal-ref": null, "doi": "10.14778/3368289.3368302", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of efficiently estimating counts for queries involving\ncomplex filters, such as user-defined functions, or predicates involving\nself-joins and correlated subqueries. For such queries, traditional sampling\ntechniques may not be applicable due to the complexity of the filter preventing\nsampling over joins, and sampling after the join may not be feasible due to the\ncost of computing the full join. The other natural approach of training and\nusing an inexpensive classifier to estimate the count instead of the expensive\npredicate suffers from the difficulties in training a good classifier and\ngiving meaningful confidence intervals. In this paper we propose a new method\nof learning to sample where we combine the best of both worlds by using\nsampling in two phases. First, we use samples to learn a probabilistic\nclassifier, and then use the classifier to design a stratified sampling method\nto obtain the final estimates. We theoretically analyze algorithms for\nobtaining an optimal stratification, and compare our approach with a suite of\nnatural alternatives like quantification learning, weighted and stratified\nsampling, and other techniques from the literature. We also provide extensive\nexperiments in diverse use cases using multiple real and synthetic datasets to\nevaluate the quality, efficiency, and robustness of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 21:29:25 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 17:52:35 GMT"}, {"version": "v3", "created": "Thu, 19 Sep 2019 03:03:17 GMT"}, {"version": "v4", "created": "Sun, 29 Dec 2019 14:45:54 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Walenz", "Brett", ""], ["Sintos", "Stavros", ""], ["Roy", "Sudeepa", ""], ["Yang", "Jun", ""]]}, {"id": "1906.09353", "submitter": "Ian Schmutte", "authors": "John M. Abowd and Ian M. Schmutte and William Sexton and Lars Vilhuber", "title": "Suboptimal Provision of Privacy and Statistical Accuracy When They are\n  Public Goods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.TH cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With vast databases at their disposal, private tech companies can compete\nwith public statistical agencies to provide population statistics. However,\nprivate companies face different incentives to provide high-quality statistics\nand to protect the privacy of the people whose data are used. When both privacy\nprotection and statistical accuracy are public goods, private providers tend to\nproduce at least one suboptimally, but it is not clear which. We model a firm\nthat publishes statistics under a guarantee of differential privacy. We prove\nthat provision by the private firm results in inefficiently low data quality in\nthis framework.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 23:42:46 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Abowd", "John M.", ""], ["Schmutte", "Ian M.", ""], ["Sexton", "William", ""], ["Vilhuber", "Lars", ""]]}, {"id": "1906.09667", "submitter": "Chen Luo", "authors": "Chen Luo, Michael J. Carey", "title": "On Performance Stability in LSM-based Storage Systems (Extended Version)", "comments": "This is the extended version of a paper published at VLDB 2020. The\n  published version is available at https://doi.org/10.14778/3372716.3372719", "journal-ref": null, "doi": "10.14778/3372716.3372719", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Log-Structured Merge-Tree (LSM-tree) has been widely adopted for use in\nmodern NoSQL systems for its superior write performance. Despite the popularity\nof LSM-trees, they have been criticized for suffering from write stalls and\nlarge performance variances due to the inherent mismatch between their fast\nin-memory writes and slow background I/O operations. In this paper, we use a\nsimple yet effective two-phase experimental approach to evaluate write stalls\nfor various LSM-tree designs. We further explore the design choices of LSM\nmerge schedulers to minimize write stalls given an I/O bandwidth budget. We\nhave conducted extensive experiments in the context of the Apache AsterixDB\nsystem and we present the results here.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jun 2019 22:57:55 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 17:50:26 GMT"}, {"version": "v3", "created": "Wed, 27 Nov 2019 17:17:17 GMT"}, {"version": "v4", "created": "Sun, 1 Dec 2019 18:33:51 GMT"}, {"version": "v5", "created": "Tue, 10 Dec 2019 06:27:37 GMT"}, {"version": "v6", "created": "Sat, 11 Apr 2020 05:36:26 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Luo", "Chen", ""], ["Carey", "Michael J.", ""]]}, {"id": "1906.09727", "submitter": "Mahmoud Abo Khamis", "authors": "Mahmoud Abo Khamis, Phokion G. Kolaitis, Hung Q. Ngo, Dan Suciu", "title": "Bag Query Containment and Information Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The query containment problem is a fundamental algorithmic problem in data\nmanagement. While this problem is well understood under set semantics, it is by\nfar less understood under bag semantics. In particular, it is a long-standing\nopen question whether or not the conjunctive query containment problem under\nbag semantics is decidable. We unveil tight connections between information\ntheory and the conjunctive query containment under bag semantics. These\nconnections are established using information inequalities, which are\nconsidered to be the laws of information theory. Our first main result asserts\nthat deciding the validity of maxima of information inequalities is many-one\nequivalent to the restricted case of conjunctive query containment in which the\ncontaining query is acyclic; thus, either both these problems are decidable or\nboth are undecidable. Our second main result identifies a new decidable case of\nthe conjunctive query containment problem under bag semantics. Specifically, we\ngive an exponential time algorithm for conjunctive query containment under bag\nsemantics, provided the containing query is chordal and admits a simple\njunction tree.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 05:20:24 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 05:57:34 GMT"}, {"version": "v3", "created": "Mon, 5 Jul 2021 04:24:53 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Khamis", "Mahmoud Abo", ""], ["Kolaitis", "Phokion G.", ""], ["Ngo", "Hung Q.", ""], ["Suciu", "Dan", ""]]}, {"id": "1906.10261", "submitter": "Temitope Ajileye", "authors": "Temitope Ajileye, Boris Motik, Ian Horrocks", "title": "Datalog Materialisation in Distributed RDF Stores with Dynamic Data\n  Exchange", "comments": "16 pages, ISWC conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several centralised RDF systems support datalog reasoning by precomputing and\nstoring all logically implied triples using the wellknown seminaive algorithm.\nLarge RDF datasets often exceed the capacity of centralised RDF systems, and a\ncommon solution is to distribute the datasets in a cluster of shared-nothing\nservers. While numerous distributed query answering techniques are known,\ndistributed seminaive evaluation of arbitrary datalog rules is less understood.\nIn fact, most distributed RDF stores either support no reasoning or can handle\nonly limited datalog fragments. In this paper we extend the dynamic data\nexchange approach for distributed query answering by Potter et al. [12] to a\nreasoning algorithm that can handle arbitrary rules while preserving important\nproperties such as nonrepetition of inferences. We also show empirically that\nour algorithm scales well to very large RDF datasets\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 22:52:09 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Ajileye", "Temitope", ""], ["Motik", "Boris", ""], ["Horrocks", "Ian", ""]]}, {"id": "1906.10322", "submitter": "Anna Fariha", "authors": "Anna Fariha, Alexandra Meliou", "title": "Example-Driven Query Intent Discovery: Abductive Reasoning using\n  Semantic Similarity", "comments": "SQuID Technical Report, 18 pages. [PVLDB 2019, Volume 12, No 10]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional relational data interfaces require precise structured queries\nover potentially complex schemas. These rigid data retrieval mechanisms pose\nhurdles for non-expert users, who typically lack language expertise and are\nunfamiliar with the details of the schema. Query by Example (QBE) methods offer\nan alternative mechanism: users provide examples of their intended query output\nand the QBE system needs to infer the intended query. However, these approaches\nfocus on the structural similarity of the examples and ignore the richer\ncontext present in the data. As a result, they typically produce queries that\nare too general, and fail to capture the user's intent effectively. In this\npaper, we present SQuID, a system that performs semantic similarity-aware query\nintent discovery. Our work makes the following contributions: (1) We design an\nend-to-end system that automatically formulates select-project-join queries in\nan open-world setting, with optional group-by aggregation and intersection\noperators; a much larger class than prior QBE techniques. (2) We express the\nproblem of query intent discovery using a probabilistic abduction model, that\ninfers a query as the most likely explanation of the provided examples. (3) We\nintroduce the notion of an abduction-ready database, which precomputes semantic\nproperties and related statistics, allowing SQuID to achieve real-time\nperformance. (4) We present an extensive empirical evaluation on three\nreal-world datasets, including user-intent case studies, demonstrating that\nSQuID is efficient and effective, and outperforms machine learning methods, as\nwell as the state-of-the-art in the related query reverse engineering problem.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 05:00:06 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Fariha", "Anna", ""], ["Meliou", "Alexandra", ""]]}, {"id": "1906.10425", "submitter": "Dmitriy Kostunin", "authors": "P. Bezyazeekov, N. Budnev, O. Fedorov, O. Gress, O. Grishin, A.\n  Haungs, T. Huege, Y. Kazarina, M. Kleifges, D. Kostunin, E. Korosteleva, L.\n  Kuzmichev, V. Lenok, N. Lubsandorzhiev, S. Malakhov, T. Marshalkina, R.\n  Monkhoev, E. Osipova, A. Pakhorukov, L. Pankov, V. Prosin, F. G. Schr\\\"oder,\n  D. Shipilov, A. Zagorodnikov", "title": "Towards the Tunka-Rex Virtual Observatory", "comments": "Proceedings of the 3rd International Workshop on Data Life Cycle in\n  Physics, Irkutsk, Russia, April 2-7, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Tunka Radio Extension (Tunka-Rex) is a cosmic-ray detector operating\nsince 2012. The detection principle of Tunka-Rex is based on the radio\ntechnique, which impacts data acquisition and storage. In this paper we give a\nfirst detailed overview of the concept of the Tunka-Rex Virtual Observatory\n(TRVO), a framework for open access to the Tunka-Rex data, which currently is\nunder active development and testing. We describe the structure of the data,\nmain features of the interface and possible applications of the TRVO.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 09:56:46 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Bezyazeekov", "P.", ""], ["Budnev", "N.", ""], ["Fedorov", "O.", ""], ["Gress", "O.", ""], ["Grishin", "O.", ""], ["Haungs", "A.", ""], ["Huege", "T.", ""], ["Kazarina", "Y.", ""], ["Kleifges", "M.", ""], ["Kostunin", "D.", ""], ["Korosteleva", "E.", ""], ["Kuzmichev", "L.", ""], ["Lenok", "V.", ""], ["Lubsandorzhiev", "N.", ""], ["Malakhov", "S.", ""], ["Marshalkina", "T.", ""], ["Monkhoev", "R.", ""], ["Osipova", "E.", ""], ["Pakhorukov", "A.", ""], ["Pankov", "L.", ""], ["Prosin", "V.", ""], ["Schr\u00f6der", "F. G.", ""], ["Shipilov", "D.", ""], ["Zagorodnikov", "A.", ""]]}, {"id": "1906.10606", "submitter": "Gabriel Meseguer-Brocal", "authors": "Gabriel Meseguer-Brocal, Alice Cohen-Hadria and Geoffroy Peeters", "title": "DALI: a large Dataset of synchronized Audio, LyrIcs and notes,\n  automatically created using teacher-student machine learning paradigm", "comments": null, "journal-ref": "Proceedings of the 19th International Society for Music\n  Information Retrieval Conference, ISMIR, Paris, France, pp. 431-437, 2018", "doi": "10.5281/zenodo.1492443", "report-no": null, "categories": "eess.AS cs.DB cs.LG cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The goal of this paper is twofold. First, we introduce DALI, a large and rich\nmultimodal dataset containing 5358 audio tracks with their time-aligned vocal\nmelody notes and lyrics at four levels of granularity. The second goal is to\nexplain our methodology where dataset creation and learning models interact\nusing a teacher-student machine learning paradigm that benefits each other. We\nstart with a set of manual annotations of draft time-aligned lyrics and notes\nmade by non-expert users of Karaoke games. This set comes without audio.\nTherefore, we need to find the corresponding audio and adapt the annotations to\nit. To that end, we retrieve audio candidates from the Web. Each candidate is\nthen turned into a singing-voice probability over time using a teacher, a deep\nconvolutional neural network singing-voice detection system (SVD), trained on\ncleaned data. Comparing the time-aligned lyrics and the singing-voice\nprobability, we detect matches and update the time-alignment lyrics\naccordingly. From this, we obtain new audio sets. They are then used to train\nnew SVD students used to perform again the above comparison. The process could\nbe repeated iteratively. We show that this allows to progressively improve the\nperformances of our SVD and get better audio-matching and alignment.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 15:30:07 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Meseguer-Brocal", "Gabriel", ""], ["Cohen-Hadria", "Alice", ""], ["Peeters", "Geoffroy", ""]]}, {"id": "1906.11062", "submitter": "Thomas Heinis", "authors": "Thomas Heinis", "title": "Survey of Information Encoding Techniques for DNA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.DB cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Key to DNA storage is encoding the information to a sequence of nucleotides\nbefore it can be synthesised for storage. Definition of such an encoding or\nmapping must adhere to multiple design restrictions. First, not all possible\nsequences of nucleotides can be synthesised. Homopolymers, e.g., sequences of\nthe same nucleotide, of a length of more than two, for example, cannot be\nsynthesised without potential errors. Similarly, the G-C content of the\nresulting sequences should be higher than 50\\%. Second, given that synthesis is\nexpensive, the encoding must map as many bits as possible to one nucleotide.\nThird, the synthesis (as well as the sequencing) is error prone, leading to\nsubstitutions, deletions and insertions. An encoding must therefore be designed\nto be resilient to errors through error correction codes or replication.\nFourth, for the purpose of computation and selective retrieval, encodings\nshould result in substantially different sequences across all data, even for\nvery similar data. In the following we discuss the history and evolution of\nencodings.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 18:57:57 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Heinis", "Thomas", ""]]}, {"id": "1906.11078", "submitter": "Peter Mell", "authors": "Dylan Yaga, Peter Mell, Nik Roby, Karen Scarfone", "title": "Blockchain Technology Overview", "comments": "68 pages, National Institute of Standards and Technology Internal\n  Report", "journal-ref": null, "doi": "10.6028/NIST.IR.8202", "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchains are tamper evident and tamper resistant digital ledgers\nimplemented in a distributed fashion (i.e., without a central repository) and\nusually without a central authority (i.e., a bank, company, or government). At\ntheir basic level, they enable a community of users to record transactions in a\nshared ledger within that community, such that under normal operation of the\nblockchain network no transaction can be changed once published. This document\nprovides a high-level technical overview of blockchain technology. The purpose\nis to help readers understand how blockchain technology works.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 13:20:44 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Yaga", "Dylan", ""], ["Mell", "Peter", ""], ["Roby", "Nik", ""], ["Scarfone", "Karen", ""]]}, {"id": "1906.11327", "submitter": "Omri Ben-Eliezer", "authors": "Omri Ben-Eliezer and Eylon Yogev", "title": "The Adversarial Robustness of Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.CR cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random sampling is a fundamental primitive in modern algorithms, statistics,\nand machine learning, used as a generic method to obtain a small yet\n\"representative\" subset of the data. In this work, we investigate the\nrobustness of sampling against adaptive adversarial attacks in a streaming\nsetting: An adversary sends a stream of elements from a universe $U$ to a\nsampling algorithm (e.g., Bernoulli sampling or reservoir sampling), with the\ngoal of making the sample \"very unrepresentative\" of the underlying data\nstream. The adversary is fully adaptive in the sense that it knows the exact\ncontent of the sample at any given point along the stream, and can choose which\nelement to send next accordingly, in an online manner.\n  Well-known results in the static setting indicate that if the full stream is\nchosen in advance (non-adaptively), then a random sample of size $\\Omega(d /\n\\varepsilon^2)$ is an $\\varepsilon$-approximation of the full data with good\nprobability, where $d$ is the VC-dimension of the underlying set system\n$(U,R)$. Does this sample size suffice for robustness against an adaptive\nadversary? The simplistic answer is \\emph{negative}: We demonstrate a set\nsystem where a constant sample size (corresponding to VC-dimension $1$)\nsuffices in the static setting, yet an adaptive adversary can make the sample\nvery unrepresentative, as long as the sample size is (strongly) sublinear in\nthe stream length, using a simple and easy-to-implement attack.\n  However, this attack is \"theoretical only\", requiring the set system size to\n(essentially) be exponential in the stream length. This is not a coincidence:\nWe show that to make Bernoulli or reservoir sampling robust against adaptive\nadversaries, the modification required is solely to replace the VC-dimension\nterm $d$ in the sample size with the cardinality term $\\log |R|$. This nearly\nmatches the bound imposed by the attack.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 20:15:54 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Ben-Eliezer", "Omri", ""], ["Yogev", "Eylon", ""]]}, {"id": "1906.11441", "submitter": "Lin Sun", "authors": "Lin Sun, Jun Zhao, Xiaojun Ye", "title": "Distributed Clustering in the Anonymized Space with Local Differential\n  Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering and analyzing on collected data can improve user experiences and\nquality of services in big data, IoT applications. However, directly releasing\noriginal data brings potential privacy concerns, which raises challenges and\nopportunities for privacy-preserving clustering. In this paper, we study the\nproblem of non-interactive clustering in distributed setting under the\nframework of local differential privacy. We first extend the Bit Vector, a\nnovel anonymization mechanism to be functionality-capable and\nprivacy-preserving. Based on the modified encoding mechanism, we propose\nkCluster algorithm that can be used for clustering in the anonymized space. We\nshow the modified encoding mechanism can be easily implemented in existing\nclustering algorithms that only rely on distance information, such as DBSCAN.\nTheoretical analysis and experimental results validate the effectiveness of the\nproposed schemes.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 05:38:40 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Sun", "Lin", ""], ["Zhao", "Jun", ""], ["Ye", "Xiaojun", ""]]}, {"id": "1906.11518", "submitter": "Longbin Lai", "authors": "Longbin Lai, Zhu Qing, Zhengyi Yang, Xin Jin, Zhengmin Lai, Ran Wang,\n  Kongzhang Hao, Xuemin Lin, Lu Qin, Wenjie Zhang, Ying Zhang, Zhengping Qian\n  and Jingren Zhou", "title": "A Survey and Experimental Analysis of Distributed Subgraph Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently there emerge many distributed algorithms that aim at solving\nsubgraph matching at scale. Existing algorithm-level comparisons failed to\nprovide a systematic view to the pros and cons of each algorithm mainly due to\nthe intertwining of strategy and optimization. In this paper, we identify four\nstrategies and three general-purpose optimizations from representative\nstate-of-the-art works. We implement the four strategies with the optimizations\nbased on the common Timely dataflow system for systematic strategy-level\ncomparison. Our implementation covers all representation algorithms. We conduct\nextensive experiments for both unlabelled matching and labelled matching to\nanalyze the performance of distributed subgraph matching under various\nsettings, which is finally summarized as a practical guide.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 09:38:46 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Lai", "Longbin", ""], ["Qing", "Zhu", ""], ["Yang", "Zhengyi", ""], ["Jin", "Xin", ""], ["Lai", "Zhengmin", ""], ["Wang", "Ran", ""], ["Hao", "Kongzhang", ""], ["Lin", "Xuemin", ""], ["Qin", "Lu", ""], ["Zhang", "Wenjie", ""], ["Zhang", "Ying", ""], ["Qian", "Zhengping", ""], ["Zhou", "Jingren", ""]]}, {"id": "1906.12018", "submitter": "Ruoming Jin", "authors": "Ruoming Jin, Zhen Peng, Wendell Wu, Feodor Dragan, Gagan Agrawal, Bin\n  Ren", "title": "Pruned Landmark Labeling Meets Vertex Centric Computation: A\n  Surprisingly Happy Marriage!", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study how the Pruned Landmark Labeling (PPL) algorithm can\nbe parallelized in a scalable fashion, producing the same results as the\nsequential algorithm. More specifically, we parallelize using a Vertex-Centric\n(VC) computational model on a modern SIMD powered multicore architecture. We\ndesign a new VC-PLL algorithm that resolves the apparent mismatch between the\ninherent sequential dependence of the PLL algorithm and the Vertex- Centric\n(VC) computing model. Furthermore, we introduce a novel batch execution model\nfor VC computation and the BVC-PLL algorithm to reduce the computational\ninefficiency in VC-PLL. Quite surprisingly, the theoretical analysis reveals\nthat under a reasonable assumption, BVC-PLL has lower computational and memory\naccess costs than PLL and indicates it may run faster than PLL as a sequential\nalgorithm. We also demonstrate how BVC-PLL algorithm can be extended to handle\ndirected graphs and weighted graphs and how it can utilize the hierarchical\nparallelism on a modern parallel computing architecture. Extensive experiments\non real-world graphs not only show the sequential BVC-PLL can run more than two\ntimes faster than the original PLL, but also demonstrates its parallel\nefficiency and scalability.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 02:19:19 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Jin", "Ruoming", ""], ["Peng", "Zhen", ""], ["Wu", "Wendell", ""], ["Dragan", "Feodor", ""], ["Agrawal", "Gagan", ""], ["Ren", "Bin", ""]]}, {"id": "1906.12089", "submitter": "Nicolas Heist", "authors": "Nicolas Heist and Heiko Paulheim", "title": "Uncovering the Semantics of Wikipedia Categories", "comments": "Preprint of a research track paper at the International Semantic Web\n  Conference (ISWC) 2019, Auckland, NZ", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Wikipedia category graph serves as the taxonomic backbone for large-scale\nknowledge graphs like YAGO or Probase, and has been used extensively for tasks\nlike entity disambiguation or semantic similarity estimation. Wikipedia's\ncategories are a rich source of taxonomic as well as non-taxonomic information.\nThe category 'German science fiction writers', for example, encodes the type of\nits resources (Writer), as well as their nationality (German) and genre\n(Science Fiction). Several approaches in the literature make use of fractions\nof this encoded information without exploiting its full potential. In this\npaper, we introduce an approach for the discovery of category axioms that uses\ninformation from the category network, category instances, and their\nlexicalisations. With DBpedia as background knowledge, we discover 703k axioms\ncovering 502k of Wikipedia's categories and populate the DBpedia knowledge\ngraph with additional 4.4M relation assertions and 3.3M type assertions at more\nthan 87% and 90% precision, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 08:32:46 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Heist", "Nicolas", ""], ["Paulheim", "Heiko", ""]]}, {"id": "1906.12335", "submitter": "Xiaoyang Wang", "authors": "Wenjie Zhu, Mengqi Zhang, Chen Chen, Xiaoyang Wang, Fan Zhang, Xuemin\n  Lin", "title": "Critical Edge Identification: A K-Truss Based Model", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In a social network, the strength of relationships between users can\nsignificantly affect the stability of the network. In this paper, we use the\nk-truss model to measure the stability of a social network. To identify\ncritical connections, we propose a novel problem, named k-truss minimization.\nGiven a social network G and a budget b, it aims to find b edges for deletion\nwhich can lead to the maximum number of edge breaks in the k-truss of G. We\nshow that the problem is NP-hard. To accelerate the computation, novel pruning\nrules are developed to reduce the candidate size. In addition, we propose an\nupper bound based strategy to further reduce the searching space. Comprehensive\nexperiments are conducted over real social networks to demonstrate the\nefficiency and effectiveness of the proposed techniques.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 13:51:30 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Zhu", "Wenjie", ""], ["Zhang", "Mengqi", ""], ["Chen", "Chen", ""], ["Wang", "Xiaoyang", ""], ["Zhang", "Fan", ""], ["Lin", "Xuemin", ""]]}]