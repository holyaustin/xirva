[{"id": "1907.00050", "submitter": "Youry Khmelevsky", "authors": "Bernd Amann, Youry Khmelevsky and Gaetan Hains", "title": "State-of-the-Art on Query & Transaction Processing Acceleration", "comments": "7 pages, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vast amount of processing power and memory bandwidth provided by modern\nGraphics Processing Units (GPUs) make them a platform for data-intensive\napplications. The database community identified GPUs as effective co-processors\nfor data processing. In the past years, there were many approaches to make use\nof GPUs at different levels of a database system. In this Internal Technical\nReport, based on the [1] and some other research papers, we identify possible\nresearch areas at LIP6 for GPU-accelerated database management systems. We\ndescribe some key properties, typical challenges of GPU-aware database\narchitectures, and identify major open challenges.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 01:46:35 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Amann", "Bernd", ""], ["Khmelevsky", "Youry", ""], ["Hains", "Gaetan", ""]]}, {"id": "1907.00062", "submitter": "Yifan Wu", "authors": "Yifan Wu, Remco Chang, Eugene Wu, Joseph M. Hellerstein", "title": "DIEL: Transparent Scaling for Interactive Visualization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We live in an era of big data and rich data visualization. As data sets\nincrease in size, browser-based interactive visualizations eventually hit\nlimits in storage and processing capacity. In order to provide interactivity\nover large datasets, visualization applications typically need to be\nextensively rewritten to make use of powerful back-end services. It would be\nfar preferable if front-end developers could write visualizations once in a\nnatural way, and have a framework take responsibility for transparently scaling\nup the visualization to use back-end services as needed. Achieving this goal\nrequires rethinking how communication and state are managed by the framework:\nthe mapping of interaction logic to server APIs or database queries, handling\nof results arriving asynchronously over the network, as well as basic\ncross-layer performance optimizations like caching.\n  In this paper, we present DIEL, a framework that achieves this cross-layer\nautoscaling transparently under a simple, declarative interface. DIEL treats UI\nevents as a stream of data that is captured in an event history for reuse.\nDevelopers declare what the state of the interface should be after the arrival\nof events. DIEL compiles these declarative specifications into relational\nqueries over both event history and the data to be visualized. In doing so,\nDIEL makes it easier to develop visualizations that are robust against changes\nto the size and location of data. To evaluate the DIEL framework, we developed\na prototype implementation and confirmed that DIEL supports a range of\nvisualization and interaction designs. Visualizations written using DIEL can\ntransparently and seamlessly scale to use back-end services with little\nintervention from the developer.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 20:24:40 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Wu", "Yifan", ""], ["Chang", "Remco", ""], ["Wu", "Eugene", ""], ["Hellerstein", "Joseph M.", ""]]}, {"id": "1907.00075", "submitter": "Yifan Wu", "authors": "Yifan Wu, Remco Chang, Eugene Wu, Joe Hellerstein", "title": "Programming with Timespans in Interactive Visualizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Modern interactive visualizations are akin to distributed systems, where user\ninteractions, background data processing, remote requests, and streaming data\nread and modify the interface at the same time. This concurrency is crucial to\nprovide an interactive user experience---forbidding it can cripple\nresponsiveness. However, it is notoriously challenging to program distributed\nsystems, and concurrency can easily lead to ambiguous or confusing interface\nbehaviors. In this paper, we present DIEL, a declarative programming model to\nhelp developers reason about and reconcile concurrency-related issues. Using\nDIEL, developers no longer need to procedurally describe how the interface\nshould update based on different input events, but rather declaratively specify\nwhat the state of the interface should be as queries over event history. We\nshow that resolving conflicts from concurrent processes in real-world\ninteractive visualizations can be done in a few lines of DIEL code.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 20:53:51 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Wu", "Yifan", ""], ["Chang", "Remco", ""], ["Wu", "Eugene", ""], ["Hellerstein", "Joe", ""]]}, {"id": "1907.00083", "submitter": "Benno Kruit", "authors": "Benno Kruit and Peter Boncz and Jacopo Urbani", "title": "Extracting Novel Facts from Tables for Knowledge Graph Completion\n  (Extended version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new end-to-end method for extending a Knowledge Graph (KG) from\ntables. Existing techniques tend to interpret tables by focusing on information\nthat is already in the KG, and therefore tend to extract many redundant facts.\nOur method aims to find more novel facts. We introduce a new technique for\ntable interpretation based on a scalable graphical model using entity\nsimilarities. Our method further disambiguates cell values using KG embeddings\nas additional ranking method. Other distinctive features are the lack of\nassumptions about the underlying KG and the enabling of a fine-grained tuning\nof the precision/recall trade-off of extracted facts. Our experiments show that\nour approach has a higher recall during the interpretation process than the\nstate-of-the-art, and is more resistant against the bias observed in extracting\nmostly redundant facts since it produces more novel extractions.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 21:11:32 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2019 08:33:39 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Kruit", "Benno", ""], ["Boncz", "Peter", ""], ["Urbani", "Jacopo", ""]]}, {"id": "1907.00146", "submitter": "Christan Grant", "authors": "Elena Montes, Monique Shotande, Daniel Helm, Christan Grant", "title": "DataPop: Knowledge Base Population using Distributed Voice Enabled\n  Devices", "comments": "7 pages, 2 references, unsubmitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data scientists are constantly creating methods to efficiently and accurately\npopulate big data sets for use in large-scale applications. Many recent efforts\nutilize crowd-sourcing and textual interfaces. In this paper, we propose a new\nmethod of curating data; namely, creating a multi-device Amazon Alexa Skill in\nthe form of a research trivia game. Users experience a synchronized gaming\nexperience with other Amazon Echo users, competing against one another while\nfilling in gaps of a connected knowledge base. This allows for full\nexploitation of the speed improvement offered by voice interface technology in\na game-based format.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 04:45:37 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Montes", "Elena", ""], ["Shotande", "Monique", ""], ["Helm", "Daniel", ""], ["Grant", "Christan", ""]]}, {"id": "1907.00236", "submitter": "Nikita Ivkin", "authors": "Nikita Ivkin, Edo Liberty, Kevin Lang, Zohar Karnin and Vladimir\n  Braverman", "title": "Streaming Quantiles Algorithms with Small Space and Update Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximating quantiles and distributions over streaming data has been\nstudied for roughly two decades now. Recently, Karnin, Lang, and Liberty\nproposed the first asymptotically optimal algorithm for doing so. This\nmanuscript complements their theoretical result by providing a practical\nvariants of their algorithm with improved constants. For a given sketch size,\nour techniques provably reduce the upper bound on the sketch error by a factor\nof two. These improvements are verified experimentally. Our modified quantile\nsketch improves the latency as well by reducing the worst case update time from\n$O(1/\\varepsilon)$ down to $O(\\log (1/\\varepsilon))$. We also suggest two\nalgorithms for weighted item streams which offer improved asymptotic update\ntimes compared to na\\\"ive extensions. Finally, we provide a specialized data\nstructure for these sketches which reduces both their memory footprints and\nupdate times.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 16:37:33 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Ivkin", "Nikita", ""], ["Liberty", "Edo", ""], ["Lang", "Kevin", ""], ["Karnin", "Zohar", ""], ["Braverman", "Vladimir", ""]]}, {"id": "1907.00782", "submitter": "Jun Zhao", "authors": "Ning Wang, Xiaokui Xiao, Yin Yang, Jun Zhao, Siu Cheung Hui, Hyejin\n  Shin, Junbum Shin, Ge Yu", "title": "Collecting and Analyzing Multidimensional Data with Local Differential\n  Privacy", "comments": "12-Page Full Paper in Proceedings of the 2019 IEEE International\n  Conference on Data Engineering (ICDE). arXiv admin note: text overlap with\n  arXiv:1606.05053", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local differential privacy (LDP) is a recently proposed privacy standard for\ncollecting and analyzing data, which has been used, e.g., in the Chrome\nbrowser, iOS and macOS. In LDP, each user perturbs her information locally, and\nonly sends the randomized version to an aggregator who performs analyses, which\nprotects both the users and the aggregator against private information leaks.\nAlthough LDP has attracted much research attention in recent years, the\nmajority of existing work focuses on applying LDP to complex data and/or\nanalysis tasks. In this paper, we point out that the fundamental problem of\ncollecting multidimensional data under LDP has not been addressed sufficiently,\nand there remains much room for improvement even for basic tasks such as\ncomputing the mean value over a single numeric attribute under LDP. Motivated\nby this, we first propose novel LDP mechanisms for collecting a numeric\nattribute, whose accuracy is at least no worse (and usually better) than\nexisting solutions in terms of worst-case noise variance. Then, we extend these\nmechanisms to multidimensional data that can contain both numeric and\ncategorical attributes, where our mechanisms always outperform existing\nsolutions regarding worst-case noise variance. As a case study, we apply our\nsolutions to build an LDP-compliant stochastic gradient descent algorithm\n(SGD), which powers many important machine learning tasks. Experiments using\nreal datasets confirm the effectiveness of our methods, and their advantages\nover existing solutions.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 13:33:43 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Wang", "Ning", ""], ["Xiao", "Xiaokui", ""], ["Yang", "Yin", ""], ["Zhao", "Jun", ""], ["Hui", "Siu Cheung", ""], ["Shin", "Hyejin", ""], ["Shin", "Junbum", ""], ["Yu", "Ge", ""]]}, {"id": "1907.01129", "submitter": "Cibele Freire", "authors": "Cibele Freire, Wolfgang Gatterbauer, Neil Immerman, Alexandra Meliou", "title": "New Results for the Complexity of Resilience for Binary Conjunctive\n  Queries with Self-Joins", "comments": "23 pages, 19 figures, included a new section", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The resilience of a Boolean query is the minimum number of tuples that need\nto be deleted from the input tables in order to make the query false. A\nsolution to this problem immediately translates into a solution for the more\nwidely known problem of deletion propagation with source-side effects. In this\npaper, we give several novel results on the hardness of the resilience problem\nfor $\\textit{binary conjunctive queries with self-joins}$ (i.e. conjunctive\nqueries with relations of maximal arity 2) with one repeated relation. Unlike\nin the self-join free case, the concept of triad is not enough to fully\ncharacterize the complexity of resilience. We identify new structural\nproperties, namely chains, confluences and permutations, which lead to various\n$NP$-hardness results. We also give novel involved reductions to network flow\nto show certain cases are in $P$. Overall, we give a dichotomy result for the\nrestricted setting when one relation is repeated at most 2 times, and we cover\nmany of the cases for 3. Although restricted, our results provide important\ninsights into the problem of self-joins that we hope can help solve the general\ncase of all conjunctive queries with self-joins in the future.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 02:34:59 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 23:29:34 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Freire", "Cibele", ""], ["Gatterbauer", "Wolfgang", ""], ["Immerman", "Neil", ""], ["Meliou", "Alexandra", ""]]}, {"id": "1907.01183", "submitter": "Gong Cheng", "authors": "Xiaxia Wang, Jinchi Chen, Shuxin Li, Gong Cheng, Jeff Z. Pan, Evgeny\n  Kharlamov, Yuzhong Qu", "title": "A Framework for Evaluating Snippet Generation for Dataset Search", "comments": "17 pages, to appear at the research track of the 18th International\n  Semantic Web Conference (ISWC 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reusing existing datasets is of considerable significance to researchers and\ndevelopers. Dataset search engines help a user find relevant datasets for\nreuse. They can present a snippet for each retrieved dataset to explain its\nrelevance to the user's data needs. This emerging problem of snippet generation\nfor dataset search has not received much research attention. To provide a basis\nfor future research, we introduce a framework for quantitatively evaluating the\nquality of a dataset snippet. The proposed metrics assess the extent to which a\nsnippet matches the query intent and covers the main content of the dataset. To\nestablish a baseline, we adapt four state-of-the-art methods from related\nfields to our problem, and perform an empirical evaluation based on real-world\ndatasets and queries. We also conduct a user study to verify our findings. The\nresults demonstrate the effectiveness of our evaluation framework, and suggest\ndirections for future research.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 05:58:18 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Wang", "Xiaxia", ""], ["Chen", "Jinchi", ""], ["Li", "Shuxin", ""], ["Cheng", "Gong", ""], ["Pan", "Jeff Z.", ""], ["Kharlamov", "Evgeny", ""], ["Qu", "Yuzhong", ""]]}, {"id": "1907.01627", "submitter": "Paolo Pareti Dr.", "authors": "Paolo Pareti and George Konstantinidis and Timothy J. Norman and Murat\n  \\c{S}ensoy", "title": "Rule Applicability on RDF Triplestore Schemas", "comments": "AI for Internet of Things Workshop, co-located with the 28th\n  International Joint Conference on Artificial Intelligence (IJCAI-19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Rule-based systems play a critical role in health and safety, where policies\ncreated by experts are usually formalised as rules. When dealing with\nincreasingly large and dynamic sources of data, as in the case of Internet of\nThings (IoT) applications, it becomes important not only to efficiently apply\nrules, but also to reason about their applicability on datasets confined by a\ncertain schema. In this paper we define the notion of a triplestore schema\nwhich models a set of RDF graphs. Given a set of rules and such a schema as\ninput we propose a method to determine rule applicability and produce output\nschemas. Output schemas model the graphs that would be obtained by running the\nrules on the graph models of the input schema. We present two approaches: one\nbased on computing a canonical (critical) instance of the schema, and a novel\napproach based on query rewriting. We provide theoretical, complexity and\nevaluation results that show the superior efficiency of our rewriting approach.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 20:50:01 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Pareti", "Paolo", ""], ["Konstantinidis", "George", ""], ["Norman", "Timothy J.", ""], ["\u015eensoy", "Murat", ""]]}, {"id": "1907.01831", "submitter": "Yixin Xu", "authors": "Yixin Xu, Jianzhong Qi, Renata Borovica-Gajic, Lars Kulik", "title": "GeoPrune: Efficiently Finding Shareable Vehicles Based on Geometric\n  Properties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On-demand ride-sharing is rapidly growing.Matching trip requests to vehicles\nefficiently is critical for the service quality of ride-sharing. To match trip\nrequests with vehicles, a prune-and-select scheme is commonly used. The pruning\nstage identifies feasible vehicles that can satisfy the trip constraints (e.g.,\ntrip time). The selection stage selects the optimal one(s) from the feasible\nvehicles. The pruning stage is crucial to reduce the complexity of the\nselection stage and to achieve efficient matching. We propose an effective and\nefficient pruning algorithm called GeoPrune. GeoPrune represents the time\nconstraints of trip requests using circles and ellipses, which can be computed\nand updated efficiently. Experiments on real-world datasets show that GeoPrune\nreduces the number of vehicle candidates in nearly all cases by an order of\nmagnitude and the update cost by two to three orders of magnitude compared to\nthe state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 10:15:07 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 13:48:28 GMT"}, {"version": "v3", "created": "Sat, 19 Oct 2019 22:56:13 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Xu", "Yixin", ""], ["Qi", "Jianzhong", ""], ["Borovica-Gajic", "Renata", ""], ["Kulik", "Lars", ""]]}, {"id": "1907.01882", "submitter": "Fangcheng Fu", "authors": "Fangcheng Fu, Jiawei Jiang, Yingxia Shao, Bin Cui", "title": "An Experimental Evaluation of Large Scale GBDT Systems", "comments": "Technical Report for paper to appear in VLDB 2019", "journal-ref": null, "doi": "10.14778/3342263.3342273", "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient boosting decision tree (GBDT) is a widely-used machine learning\nalgorithm in both data analytic competitions and real-world industrial\napplications. Further, driven by the rapid increase in data volume, efforts\nhave been made to train GBDT in a distributed setting to support large-scale\nworkloads. However, we find it surprising that the existing systems manage the\ntraining dataset in different ways, but none of them have studied the impact of\ndata management. To that end, this paper aims to study the pros and cons of\ndifferent data management methods regarding the performance of distributed\nGBDT. We first introduce a quadrant categorization of data management policies\nbased on data partitioning and data storage. Then we conduct an in-depth\nsystematic analysis and summarize the advantageous scenarios of the quadrants.\nBased on the analysis, we further propose a novel distributed GBDT system named\nVero, which adopts the unexplored composition of vertical partitioning and\nrow-store and suits for many large-scale cases. To validate our analysis\nempirically, we implement different quadrants in the same code base and compare\nthem under extensive workloads, and finally compare Vero with other\nstate-of-the-art systems over a wide range of datasets. Our theoretical and\nexperimental results provide a guideline on choosing a proper data management\npolicy for a given workload.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 12:26:38 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2019 15:51:47 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Fu", "Fangcheng", ""], ["Jiang", "Jiawei", ""], ["Shao", "Yingxia", ""], ["Cui", "Bin", ""]]}, {"id": "1907.01885", "submitter": "Matth\\\"aus Zloch", "authors": "Matth\\\"aus Zloch, Maribel Acosta, Daniel Hienert, Stefan Dietze,\n  Stefan Conrad", "title": "A Software Framework and Datasets for the Analysis of Graph Measures on\n  RDF Graphs", "comments": "Submitted at ESWC 2019, Resources Track. 15 pages, 5 figures, 2\n  tables", "journal-ref": null, "doi": "10.5281/zenodo.2109469, 10.5281/zenodo.1214433", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the availability and the inter-connectivity of RDF datasets grow, so does\nthe necessity to understand the structure of the data. Understanding the\ntopology of RDF graphs can guide and inform the development of, e.g. synthetic\ndataset generators, sampling methods, index structures, or query optimizers. In\nthis work, we propose two resources: (i) a software framework able to acquire,\nprepare, and perform a graph-based analysis on the topology of large RDF\ngraphs, and (ii) results on a graph-based analysis of 280 datasets from the LOD\nCloud with values for 28 graph measures computed with the framework. We present\na preliminary analysis based on the proposed resources and point out\nimplications for synthetic dataset generators. Finally, we identify a set of\nmeasures, that can be used to characterize graphs in the Semantic Web.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 12:32:50 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Zloch", "Matth\u00e4us", ""], ["Acosta", "Maribel", ""], ["Hienert", "Daniel", ""], ["Dietze", "Stefan", ""], ["Conrad", "Stefan", ""]]}, {"id": "1907.01988", "submitter": "Ahmet Kara", "authors": "Ahmet Kara, Milos Nikolic, Dan Olteanu, Haozhe Zhang", "title": "Trade-offs in Static and Dynamic Evaluation of Hierarchical Queries", "comments": "Technical Report; 52 pages. The updated version contains: new\n  diagrams and plots summarizing known results and putting the results of the\n  paper into context; introduction of delta_i-hieararchical queries, for any\n  non-negative integer i; optimality results for delta_0- and\n  delta_1-hieararchical queries", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate trade-offs in static and dynamic evaluation of hierarchical\nqueries with arbitrary free variables. In the static setting, the trade-off is\nbetween the time to partially compute the query result and the delay needed to\nenumerate its tuples. In the dynamic setting, we additionally consider the time\nneeded to update the query result in the presence of single-tuple inserts and\ndeletes to the input database.\n  Our approach observes the degree of values in the database and uses different\ncomputation and maintenance strategies for high-degree and low-degree values.\nFor the latter it partially computes the result, while for the former it\ncomputes enough information to allow for on-the-fly enumeration.\n  The main result of this work defines the preprocessing time, the update time,\nand the enumeration delay as functions of the light/heavy threshold and of the\nfactorization width of the hierarchical query. By conveniently choosing this\nthreshold, our approach can recover a number of prior results when restricted\nto hierarchical queries.\n  For a restricted class of hierarchical queries, our approach can achieve\nworst-case optimal update time and enumeration delay conditioned on the Online\nMatrix-Vector Multiplication Conjecture.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 15:21:47 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2020 12:26:07 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Kara", "Ahmet", ""], ["Nikolic", "Milos", ""], ["Olteanu", "Dan", ""], ["Zhang", "Haozhe", ""]]}, {"id": "1907.02790", "submitter": "Fabrizio Orlandi", "authors": "Fabrizio Orlandi, Alan Meehan, Murhaf Hossari, Soumyabrata Dev, Declan\n  O'Sullivan, Tarek AlSkaif", "title": "Interlinking Heterogeneous Data for Smart Energy Systems", "comments": "Published in Proc. International Conference on Smart Energy Systems\n  and Technologies (SEST), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart energy systems in general, and solar energy analysis in particular,\nhave recently gained increasing interest. This is mainly due to stronger focus\non smart energy saving solutions and recent developments in photovoltaic (PV)\ncells. Various data-driven and machine-learning frameworks are being proposed\nby the research community. However, these frameworks perform their analysis -\nand are designed on - specific, heterogeneous and isolated datasets,\ndistributed across different sites and sources, making it hard to compare\nresults and reproduce the analysis on similar data. We propose an approach\nbased on Web (W3C) standards and Linked Data technologies for representing and\nconverting PV and weather records into an Resource Description Framework (RDF)\ngraph-based data format. This format, and the presented approach, is ideal in a\ndata integration scenario where data needs to be converted into homogeneous\nform and different datasets could be interlinked for distributed analysis.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 12:16:30 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Orlandi", "Fabrizio", ""], ["Meehan", "Alan", ""], ["Hossari", "Murhaf", ""], ["Dev", "Soumyabrata", ""], ["O'Sullivan", "Declan", ""], ["AlSkaif", "Tarek", ""]]}, {"id": "1907.02900", "submitter": "Oded Green", "authors": "Oded Green", "title": "HashGraph -- Scalable Hash Tables Using A Sparse Graph Data Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hash tables are ubiquitous and used in a wide range of applications for\nefficient probing of large and unsorted data. If designed properly, hash-tables\ncan enable efficients look ups in a constant number of operations or commonly\nreferred to as O(1) operations. As data sizes continue to grow and data becomes\nless structured (as is common for big-data applications), the need for\nefficient and scalable hash table also grows. In this paper we introduce\nHashGraph, a new scalable approach for building hash tables that uses concepts\ntaken from sparse graph representations--hence the name HashGraph. We show two\ndifferent variants of HashGraph, a simple algorithm that outlines the method to\ncreate the hash-table and an advanced method that creates the hash table in a\nmore efficient manner (with an improved memory access pattern). HashGraph shows\na new way to deal with hash-collisions that does not use \"open-addressing\" or\n\"chaining\", yet has all the benefits of both these approaches. HashGraph\ncurrently works for static inputs, though recent progress with dynamic graph\ndata structures suggest that HashGraph might be extended to dynamic inputs as\nwell. We show that HashGraph can deal with a large number of hash-values per\nentry without loss of performance as most open-addressing and chaining\napproaches have. Further, we show that HashGraph is indifferent to the\nload-factor. Lastly, we show a new probing algorithm for the second phase of\nvalue lookups. Given the above, HashGraph is extremely fast and outperforms\nseveral state of the art hash-table implementations. The implementation of\nHashGraph in this paper is for NVIDIA GPUs, though HashGraph is not\narchitecture dependent. Using a NVIDIA GV100 GPU, HashGraph is anywhere from\n2X-8X faster than cuDPP, WarpDrive, and cuDF. HashGraph is able to build a\nhash-table at a rate of 2.5 billion keys per second and can probe at nearly the\nsame rate.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 15:44:27 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Green", "Oded", ""]]}, {"id": "1907.02990", "submitter": "Oliver Bra\\v{c}evac", "authors": "Oliver Bra\\v{c}evac, Guido Salvaneschi, Sebastian Erdweg, Mira Mezini", "title": "Type-safe, Polyvariadic Event Correlation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The pivotal role that event correlation technology plays in todays\napplications has lead to the emergence of different families of event\ncorrelation approaches with a multitude of specialized correlation semantics,\nincluding computation models that support the composition and extension of\ndifferent semantics. However, type-safe embeddings of extensible and composable\nevent patterns into statically-typed general-purpose programming languages have\nnot been systematically explored so far. Event correlation technology has often\nadopted well-known and intuitive notations from database queries, for which\napproaches to type-safe embedding do exist. However, we argue in the paper that\nthese approaches, which are essentially descendants of the work on monadic\ncomprehensions, are not well-suited for event correlations and, thus, cannot\nwithout further ado be reused/re-purposed for embedding event patterns. To\nclose this gap we propose PolyJoin, a novel approach to type-safe embedding for\nfully polyvariadic event patterns with polymorphic correlation semantics. Our\napproach is based on a tagless final encoding with uncurried higher-order\nabstract syntax (HOAS) representation of event patterns with n variables, for\narbitrary $n \\in \\mathbb{N}$. Thus, our embedding is defined in terms of the\nhost language without code generation and exploits the host language type\nsystem to model and type check the type system of the pattern language. Hence,\nby construction it impossible to define ill-typed patterns. We show that it is\npossible to have a purely library-level embedding of event patterns, in the\nfamiliar join query notation, which is not restricted to monads. PolyJoin is\npractical, type-safe and extensible. An implementation of it in pure multicore\nOCaml is readily usable.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 18:27:36 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Bra\u010devac", "Oliver", ""], ["Salvaneschi", "Guido", ""], ["Erdweg", "Sebastian", ""], ["Mezini", "Mira", ""]]}, {"id": "1907.03191", "submitter": "Saeid Hosseini", "authors": "Saeid Hosseini, Saeed Najafipour, Ngai-Man Cheung, Hongzhi Yin,\n  Mohammad Reza Kangavari, and Xiaofang Zhou", "title": "TEAGS: Time-aware Text Embedding Approach to Generate Subgraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contagions (e.g. virus, gossip) spread over the nodes in propagation graphs.\nWe can use the temporal and textual data of the nodes to compute the edge\nweights and then generate subgraphs with highly relevant nodes. This is\nbeneficial to many applications. Yet, challenges abound. First, the propagation\npattern between each pair of nodes may change by time. Second, not always the\nsame contagion propagates. Hence, the state-of-the-art text mining approaches\nincluding topic-modeling cannot effectively compute the edge weights. Third,\nsince the propagation is affected by time, the word-word co-occurrence patterns\nmay differ in various temporal dimensions, that can decrease the effectiveness\nof word embedding approaches. We argue that multi-aspect temporal dimensions\n(hour, day, etc) should be considered to better calculate the correlation\nweights between the nodes. In this work, we devise a novel framework that on\nthe one hand, integrates a neural network based time-aware word embedding\ncomponent to construct the word vectors through multiple temporal facets, and\non the other hand, uses a temporal generative model to compute the weights.\nSubsequently, we propose a Max-Heap Graph cutting algorithm to generate\nsubgraphs. We validate our model through comprehensive experiments on\nreal-world datasets. The results show that our model can retrieve the subgraphs\nmore effective than other rivals and the temporal dynamics should be noticed\nboth in word embedding and propagation processes.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 21:26:22 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 13:28:23 GMT"}, {"version": "v3", "created": "Sat, 24 Aug 2019 11:40:41 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Hosseini", "Saeid", ""], ["Najafipour", "Saeed", ""], ["Cheung", "Ngai-Man", ""], ["Yin", "Hongzhi", ""], ["Kangavari", "Mohammad Reza", ""], ["Zhou", "Xiaofang", ""]]}, {"id": "1907.03335", "submitter": "Disa Mhembere", "authors": "Disa Mhembere, Da Zheng, Carey E. Priebe, Joshua T. Vogelstein and\n  Randal Burns", "title": "Graphyti: A Semi-External Memory Graph Library for FlashGraph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph datasets exceed the in-memory capacity of most standalone machines.\nTraditionally, graph frameworks have overcome memory limitations through\nscale-out, distributing computing. Emerging frameworks avoid the network\nbottleneck of distributed data with Semi-External Memory (SEM) that uses a\nsingle multicore node and operates on graphs larger than memory. In SEM,\n$\\mathcal{O}(m)$ data resides on disk and $\\mathcal{O}(n)$ data in memory, for\na graph with $n$ vertices and $m$ edges. For developers, this adds complexity\nbecause they must explicitly encode I/O within applications. We present\nprinciples that are critical for application developers to adopt in order to\nachieve state-of-the-art performance, while minimizing I/O and memory for\nalgorithms in SEM. We present them in Graphyti, an extensible parallel SEM\ngraph library built on FlashGraph and available in Python via pip. In SEM,\nGraphyti achieves 80% of the performance of in-memory execution and retains the\nperformance of FlashGraph, which outperforms distributed engines, such as\nPowerGraph and Galois.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2019 19:13:24 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Mhembere", "Disa", ""], ["Zheng", "Da", ""], ["Priebe", "Carey E.", ""], ["Vogelstein", "Joshua T.", ""], ["Burns", "Randal", ""]]}, {"id": "1907.03736", "submitter": "Mingjie Tang", "authors": "Mingjie Tang, Yongyang Yu, Walid G. Aref, Ahmed R. Mahmood, Qutaibah\n  M. Malluhi, Mourad Ouzzani", "title": "LocationSpark: In-memory Distributed Spatial Query Processing and\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the ubiquity of spatial data applications and the large amounts of\nspatial data that these applications generate and process, there is a pressing\nneed for scalable spatial query processing. In this paper, we present new\ntechniques for spatial query processing and optimization in an in-memory and\ndistributed setup to address scalability. More specifically, we introduce new\ntechniques for handling query skew, which is common in practice, and optimize\ncommunication costs accordingly. We propose a distributed query scheduler that\nuse a new cost model to optimize the cost of spatial query processing. The\nscheduler generates query execution plans that minimize the effect of query\nskew. The query scheduler employs new spatial indexing techniques based on\nbitmap filters to forward queries to the appropriate local nodes. Each local\ncomputation node is responsible for optimizing and selecting its best local\nquery execution plan based on the indexes and the nature of the spatial queries\nin that node. All the proposed spatial query processing and optimization\ntechniques are prototyped inside Spark, a distributed memory-based computation\nsystem. The experimental study is based on real datasets and demonstrates that\ndistributed spatial query processing can be enhanced by up to an order of\nmagnitude over existing in-memory and distributed spatial systems.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 17:36:40 GMT"}, {"version": "v2", "created": "Tue, 16 Jul 2019 17:38:23 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Tang", "Mingjie", ""], ["Yu", "Yongyang", ""], ["Aref", "Walid G.", ""], ["Mahmood", "Ahmed R.", ""], ["Malluhi", "Qutaibah M.", ""], ["Ouzzani", "Mourad", ""]]}, {"id": "1907.03755", "submitter": "Ahmed BaniMustafa", "authors": "Ahmed BaniMustafa and Nigel Hardy", "title": "Applications of a Novel Knowledge Discovery and Data Mining Process\n  Model for Metabolomics", "comments": "references information updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.DB cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work demonstrates the execution of a novel process model for knowledge\ndiscovery and data mining for metabolomics (MeKDDaM). It aims to illustrate\nMeKDDaM process model applicability using four different real-world\napplications and to highlight its strengths and unique features. The\ndemonstrated applications provide coverage for metabolite profiling, target\nanalysis, and metabolic fingerprinting. The data analysed in these applications\nwere captured by chromatographic separation and mass spectrometry technique\n(LC-MS), Fourier transform infrared spectroscopy (FT-IR), and nuclear magnetic\nresonance spectroscopy (NMR) and involve the analysis of plant, animal, and\nhuman samples. The process was executed using both data-driven and\nhypothesis-driven data mining approaches in order to perform various data\nmining goals and tasks by applying a number of data mining techniques. The\napplications were selected to achieve a range of analytical goals and research\nquestions and to provide coverage for metabolite profiling, target analysis,\nand metabolic fingerprinting using datasets that were captured by NMR, LC-MS,\nand FT-IR using samples of a plant, animal, and human origin. The process was\napplied using an implementation environment which was created in order to\nprovide a computer-aided realisation of the process model execution.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 01:14:55 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2019 07:57:31 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["BaniMustafa", "Ahmed", ""], ["Hardy", "Nigel", ""]]}, {"id": "1907.03936", "submitter": "Hirokazu Chiba", "authors": "Hirokazu Chiba, Ryota Yamanaka, Shota Matsumoto", "title": "Property Graph Exchange Format", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a variety of database implementations adopting the property graph\nmodel have emerged. However, interoperable management of graph data on these\nimplementations is challenging due to the differences in data models and\nformats. Here, we redefine the property graph model incorporating the\ndifferences in the existing models and propose interoperable serialization\nformats for property graphs. The model is independent of specific\nimplementations and provides a basis of interoperable management of property\ngraph data. The proposed serialization is not only general but also intuitive,\nthus it is useful for creating and maintaining graph data. To demonstrate the\npractical use of our model and serialization, we implemented converters from\nour serialization into existing formats, which can then be loaded into various\ngraph databases. This work provides a basis of an interoperable platform for\ncreating, exchanging, and utilizing property graph data.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 01:43:54 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Chiba", "Hirokazu", ""], ["Yamanaka", "Ryota", ""], ["Matsumoto", "Shota", ""]]}, {"id": "1907.04028", "submitter": "Bin Yang", "authors": "Sean Bin Yang, Bin Yang", "title": "PathRank: A Multi-Task Learning Framework to Rank Paths in Spatial\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern navigation services often provide multiple paths connecting the same\nsource and destination for users to select. Hence, ranking such paths becomes\nincreasingly important, which directly affects the service quality. We present\nPathRank, a data-driven framework for ranking paths based on historical\ntrajectories using multi-task learning. If a trajectory used path P from source\ns to destination d, PathRank considers this as an evidence that P is preferred\nover all other paths from s to d. Thus, a path that is similar to P should have\na larger ranking score than a path that is dissimilar to P. Based on this\nintuition, PathRank models path ranking as a regression problem, where each\npath is associated with a ranking score.\n  To enable PathRank, we first propose an effective method to generate a\ncompact set of training data: for each trajectory, we generate a small set of\ndiversified paths. Next, we propose a multi-task learning framework to solve\nthe regression problem. In particular, a spatial network embedding is proposed\nto embed each vertex to a feature vector by considering both road network\ntopology and spatial properties, such as distances and travel times. Since a\npath is represented by a sequence of vertices, which is now a sequence of\nfeature vectors after embedding, recurrent neural network is applied to model\nthe sequence. The objective function is designed to consider errors on both\nranking scores and spatial properties, making the framework a multi-task\nlearning framework. Empirical studies on a substantial trajectory data set\noffer insight into the designed properties of the proposed framework and\nindicating that it is effective and practical.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 07:45:55 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Yang", "Sean Bin", ""], ["Yang", "Bin", ""]]}, {"id": "1907.04217", "submitter": "Jeremy Kepner", "authors": "Jeremy Kepner, Vijay Gadepally, Lauren Milechin, Siddharth Samsi,\n  William Arcand, David Bestor, William Bergeron, Chansup Byun, Matthew\n  Hubbell, Michael Houle, Michael Jones, Anne Klein, Peter Michaleas, Julie\n  Mullen, Andrew Prout, Antonio Rosa, Charles Yee, Albert Reuther", "title": "Streaming 1.9 Billion Hypersparse Network Updates per Second with D4M", "comments": "6 pages; 6 figures; accepted to IEEE High Performance Extreme\n  Computing (HPEC) Conference 2019. arXiv admin note: text overlap with\n  arXiv:1807.05308, arXiv:1902.00846", "journal-ref": null, "doi": "10.1109/HPEC.2019.8916508", "report-no": null, "categories": "cs.DC cs.DB cs.DS cs.IR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Dynamic Distributed Dimensional Data Model (D4M) library implements\nassociative arrays in a variety of languages (Python, Julia, and Matlab/Octave)\nand provides a lightweight in-memory database implementation of hypersparse\narrays that are ideal for analyzing many types of network data. D4M relies on\nassociative arrays which combine properties of spreadsheets, databases,\nmatrices, graphs, and networks, while providing rigorous mathematical\nguarantees, such as linearity. Streaming updates of D4M associative arrays put\nenormous pressure on the memory hierarchy. This work describes the design and\nperformance optimization of an implementation of hierarchical associative\narrays that reduces memory pressure and dramatically increases the update rate\ninto an associative array. The parameters of hierarchical associative arrays\nrely on controlling the number of entries in each level in the hierarchy before\nan update is cascaded. The parameters are easily tunable to achieve optimal\nperformance for a variety of applications. Hierarchical arrays achieve over\n40,000 updates per second in a single instance. Scaling to 34,000 instances of\nhierarchical D4M associative arrays on 1,100 server nodes on the MIT SuperCloud\nachieved a sustained update rate of 1,900,000,000 updates per second. This\ncapability allows the MIT SuperCloud to analyze extremely large streaming\nnetwork data sets.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 20:55:04 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Kepner", "Jeremy", ""], ["Gadepally", "Vijay", ""], ["Milechin", "Lauren", ""], ["Samsi", "Siddharth", ""], ["Arcand", "William", ""], ["Bestor", "David", ""], ["Bergeron", "William", ""], ["Byun", "Chansup", ""], ["Hubbell", "Matthew", ""], ["Houle", "Michael", ""], ["Jones", "Michael", ""], ["Klein", "Anne", ""], ["Michaleas", "Peter", ""], ["Mullen", "Julie", ""], ["Prout", "Andrew", ""], ["Rosa", "Antonio", ""], ["Yee", "Charles", ""], ["Reuther", "Albert", ""]]}, {"id": "1907.04318", "submitter": "Ahmed BaniMustafa Dr.", "authors": "Ahmed BaniMustafa and Nigel Hardy", "title": "Computer-Aided Data Mining: Automating a Novel Knowledge Discovery and\n  Data Mining Process Model for Metabolomics", "comments": "arXiv admin note: text overlap with arXiv:1907.03755", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.DB cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents MeKDDaM-SAGA, computer-aided automation software for\nimplementing a novel knowledge discovery and data mining process model that was\ndesigned for performing justifiable, traceable and reproducible metabolomics\ndata analysis. The process model focuses on achieving metabolomics analytical\nobjectives and on considering the nature of its involved data. MeKDDaM-SAGA was\nsuccessfully used for guiding the process model execution in a number of\nmetabolomics applications. It satisfies the requirements of the proposed\nprocess model design and execution. The software realises the process model\nlayout, structure and flow and it enables its execution externally using\nvarious data mining and machine learning tools or internally using a number of\nembedded facilities that were built for performing a number of automated\nactivities such as data preprocessing, data exploration, data acclimatization,\nmodelling, evaluation and visualization. MeKDDaM-SAGA was developed using\nobject-oriented software engineering methodology and was constructed in Java.\nIt consists of 241 design classes that were designed to implement 27 use-cases.\nThe software uses an XML database to guarantee portability and uses a GUI\ninterface to ensure its user-friendliness. It implements an internal embedded\nversion control system that is used to realise and manage the process flow,\nfeedback and iterations and to enable undoing and redoing the execution of the\nprocess phases, activities, and the internal tasks within its phases.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 01:14:53 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["BaniMustafa", "Ahmed", ""], ["Hardy", "Nigel", ""]]}, {"id": "1907.04643", "submitter": "Julie Bu Daher", "authors": "Julie Bu Daher, Armelle Brun, Anne Boyer", "title": "Multi-source Relations for Contextual Data Mining in Learning Analytics", "comments": "Learning and Student Analytics Conference 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goals of Learning Analytics (LA) are manifold, among which helping\nstudents to understand their academic progress and improving their learning\nprocess, which are at the core of our work. To reach this goal, LA relies on\neducational data: students' traces of activities on VLE, or academic,\nsocio-demographic information, information about teachers, pedagogical\nresources, curricula, etc. The data sources that contain such information are\nmultiple and diverse. Data mining, specifically pattern mining, aims at\nextracting valuable and understandable information from large datasets. In our\nwork, we assume that multiple educational data sources form a rich dataset that\ncan result in valuable patterns. Mining such data is thus a promising way to\nreach the goal of helping students. However, heterogeneity and interdependency\nwithin data lead to high computational complexity. We thus aim at designing low\ncomplex pattern mining algorithms that mine multi-source data, taking into\nconsideration the dependency and heterogeneity among sources. The patterns\nformed are meaningful and interpretable, they can thus be directly used for\nstudents.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 12:26:09 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Daher", "Julie Bu", ""], ["Brun", "Armelle", ""], ["Boyer", "Anne", ""]]}, {"id": "1907.05014", "submitter": "Lin Sun", "authors": "Lin Sun, Jun Zhao, Xiaojun Ye, Shuo Feng, Teng Wang, Tao Bai", "title": "Conditional Analysis for Key-Value Data with Local Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local differential privacy (LDP) has been deemed as the de facto measure for\nprivacy-preserving distributed data collection and analysis. Recently,\nresearchers have extended LDP to the basic data type in NoSQL systems: the\nkey-value data, and show its feasibilities in mean estimation and frequency\nestimation. In this paper, we develop a set of new perturbation mechanisms for\nkey-value data collection and analysis under the strong model of local\ndifferential privacy. Since many modern machine learning tasks rely on the\navailability of conditional probability or the marginal statistics, we then\npropose the conditional frequency estimation method for key analysis and the\nconditional mean estimation for value analysis in key-value data. The released\nstatistics with conditions can further be used in learning tasks. Extensive\nexperiments of frequency and mean estimation on both synthetic and real-world\ndatasets validate the effectiveness and accuracy of the proposed key-value\nperturbation mechanisms against the state-of-art competitors.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 06:34:02 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Sun", "Lin", ""], ["Zhao", "Jun", ""], ["Ye", "Xiaojun", ""], ["Feng", "Shuo", ""], ["Wang", "Teng", ""], ["Bai", "Tao", ""]]}, {"id": "1907.05443", "submitter": "Stratos Idreos", "authors": "Stratos Idreos, Niv Dayan, Wilson Qin, Mali Akmanalp, Sophie Hilgard,\n  Andrew Ross, James Lennon, Varun Jain, Harshita Gupta, David Li, Zichen Zhu", "title": "Learning Key-Value Store Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the concept of design continuums for the data layout of\nkey-value stores. A design continuum unifies major distinct data structure\ndesigns under the same model. The critical insight and potential long-term\nimpact is that such unifying models 1) render what we consider up to now as\nfundamentally different data structures to be seen as views of the very same\noverall design space, and 2) allow seeing new data structure designs with\nperformance properties that are not feasible by existing designs. The core\nintuition behind the construction of design continuums is that all data\nstructures arise from the very same set of fundamental design principles, i.e.,\na small set of data layout design concepts out of which we can synthesize any\ndesign that exists in the literature as well as new ones. We show how to\nconstruct, evaluate, and expand, design continuums and we also present the\nfirst continuum that unifies major data structure designs, i.e., B+tree,\nB-epsilon-tree, LSM-tree, and LSH-table.\n  The practical benefit of a design continuum is that it creates a fast\ninference engine for the design of data structures. For example, we can predict\nnear instantly how a specific design change in the underlying storage of a data\nsystem would affect performance, or reversely what would be the optimal data\nstructure (from a given set of designs) given workload characteristics and a\nmemory budget. In turn, these properties allow us to envision a new class of\nself-designing key-value stores with a substantially improved ability to adapt\nto workload and hardware changes by transitioning between drastically different\ndata structure designs to assume a diverse set of performance properties at\nwill.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 18:35:39 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Idreos", "Stratos", ""], ["Dayan", "Niv", ""], ["Qin", "Wilson", ""], ["Akmanalp", "Mali", ""], ["Hilgard", "Sophie", ""], ["Ross", "Andrew", ""], ["Lennon", "James", ""], ["Jain", "Varun", ""], ["Gupta", "Harshita", ""], ["Li", "David", ""], ["Zhu", "Zichen", ""]]}, {"id": "1907.05458", "submitter": "Matthew Malloy", "authors": "Swapnil Shinde, Jukka Ranta, Paul Deitrick, Matthew Malloy", "title": "Scalable Panel Fusion Using Distributed Min Cost Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern audience measurement requires combining observations from disparate\npanel datasets. Connecting and relating such panel datasets is a process termed\npanel fusion. This paper formalizes the panel fusion problem and presents a\nnovel approach to solve it. We cast the panel fusion as a network flow problem,\nallowing the application of a rich body of research. In the context of digital\naudience measurement, where panel sizes can grow into the tens of millions, we\npropose an efficient algorithm to partition the network into sub-problems.\nWhile the algorithm solves a relaxed version of the original problem, we\nprovide conditions under which it guarantees optimality. We demonstrate our\napproach by fusing two real-world panel datasets in a distributed computing\nenvironment.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 19:27:37 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Shinde", "Swapnil", ""], ["Ranta", "Jukka", ""], ["Deitrick", "Paul", ""], ["Malloy", "Matthew", ""]]}, {"id": "1907.05618", "submitter": "Patrick Marcel", "authors": "Veronika Peralta and Patrick Marcel and Willeme Verdeaux and Aboubakar\n  Sidikhy Diakhaby", "title": "Detecting coherent explorations in SQL workloads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a proposal aiming at better understanding a workload of\nSQL queries and detecting coherent explorations hidden within the workload. In\nparticular, our work investigates SQLShare [11], a database-as-a-service\nplatform targeting scientists and data scientists with minimal database\nexperience, whose workload was made available to the research community.\nAccording to the authors of [11], this workload is the only one containing\nprimarily ad-hoc hand-written queries over user-uploaded datasets. We analyzed\nthis workload by extracting features that characterize SQL queries and we show\nhow to use these features to separate sequences of SQL queries into meaningful\nexplorations. We ran several tests over various query workloads to validate\nempirically our approach.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 08:35:23 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Peralta", "Veronika", ""], ["Marcel", "Patrick", ""], ["Verdeaux", "Willeme", ""], ["Diakhaby", "Aboubakar Sidikhy", ""]]}, {"id": "1907.05991", "submitter": "Yusuke Kawamoto", "authors": "Yusuke Kawamoto and Takao Murakami", "title": "Local Distribution Obfuscation via Probability Coupling", "comments": "Full version of Allerton 2019 paper (This paper extends some part of\n  the unpublished v3 of arXiv:1812.00939, while v4 of arXiv:1812.00939 extends\n  the other part and is published in ESORICS'19.)", "journal-ref": null, "doi": "10.1109/ALLERTON.2019.8919803", "report-no": null, "categories": "cs.CR cs.DB cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a general model for the local obfuscation of probability\ndistributions by probabilistic perturbation, e.g., by adding differentially\nprivate noise, and investigate its theoretical properties. Specifically, we\nrelax a notion of distribution privacy (DistP) by generalizing it to\ndivergence, and propose local obfuscation mechanisms that provide divergence\ndistribution privacy. To provide f-divergence distribution privacy, we prove\nthat probabilistic perturbation noise should be added proportionally to the\nEarth mover's distance between the probability distributions that we want to\nmake indistinguishable. Furthermore, we introduce a local obfuscation\nmechanism, which we call a coupling mechanism, that provides divergence\ndistribution privacy while optimizing the utility of obfuscated data by using\nexact/approximate auxiliary information on the input distributions we want to\nprotect.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 01:13:40 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2019 14:32:11 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Kawamoto", "Yusuke", ""], ["Murakami", "Takao", ""]]}, {"id": "1907.06146", "submitter": "Cong Fu", "authors": "Cong Fu, Changxu Wang, Deng Cai", "title": "High Dimensional Similarity Search with Satellite System Graph:\n  Efficiency, Scalability, and Unindexed Query Compatibility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Nearest Neighbor Search (ANNS) in high dimensional space is\nessential in database and information retrieval. Recently, there has been a\nsurge of interest in exploring efficient graph-based indices for the ANNS\nproblem. Among them, Navigating Spreading-out Graph (NSG) provides fine\ntheoretical analysis and achieves state-of-the-art performance. However, we\nfind there are several limitations with NSG: 1) NSG has no theoretical\nguarantee on nearest neighbor search when the query is not indexed in the\ndatabase; 2) NSG is too sparse which harms the search performance. In addition,\nNSG suffers from high indexing complexity. To address the above problems, we\npropose the Satellite System Graphs (SSG) and a practical variant NSSG.\nSpecifically, we propose a novel pruning strategy to produce SSGs from the\ncomplete graph. SSGs define a new family of MSNETs in which the out-edges of\neach node are distributed evenly in all directions. Each node in the graph\nbuilds effective connections to its neighborhood omnidirectionally, whereupon\nwe derive SSG's excellent theoretical properties for both indexed and unindexed\nqueries. We can adaptively adjust the sparsity of an SSG with a hyper-parameter\nto optimize the search performance. Further, NSSG is proposed to reduce the\nindexing complexity of the SSG for large-scale applications. Both theoretical\nand extensive experimental analyses are provided to demonstrate the strengths\nof the proposed approach over the existing representative algorithms. Our code\nhas been released at https://github.com/ZJULearning/SSG.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 23:17:19 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2020 09:05:26 GMT"}, {"version": "v3", "created": "Thu, 18 Mar 2021 06:19:44 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Fu", "Cong", ""], ["Wang", "Changxu", ""], ["Cai", "Deng", ""]]}, {"id": "1907.06250", "submitter": "Boris Novikov", "authors": "Artem Trofimov, Igor E. Kuralenok, Nikita Marshalkin, Boris Novikov", "title": "Delivery, consistency, and determinism: rethinking guarantees in\n  distributed stream processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consistency requirements for state-of-the-art stream processing systems are\ndefined in terms of delivery guarantees. Exactly-once is the strongest one and\nthe most desirable for end-user. However, there are several issues regarding\nthis concept. Commonly used techniques that enforce exactly-once produce\nsignificant performance overhead. Besides, the notion of exactly-once is not\nformally defined and does not capture all properties that provide stream\nprocessing systems supporting this guarantee. In this paper, we introduce a\nformal framework that allows us to define streaming guarantees more regularly.\nWe demonstrate that the properties of delivery, consistency, and determinism\nare tightly connected within distributed stream processing. We also show that\nhaving lightweight determinism, it is possible to provide exactly-once with\nalmost no performance overhead. Experiments show that the proposed approach can\nsignificantly outperform alternative industrial solutions.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2019 17:24:39 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Trofimov", "Artem", ""], ["Kuralenok", "Igor E.", ""], ["Marshalkin", "Nikita", ""], ["Novikov", "Boris", ""]]}, {"id": "1907.06295", "submitter": "Max Halford", "authors": "Max Halford, Philippe Saint-Pierre and Frank Morvan", "title": "An Approach Based on Bayesian Networks for Query Selectivity Estimation", "comments": null, "journal-ref": "Lecture Notes in Computer Science, volume 11447, 2019, pages 3-19", "doi": "10.1007/978-3-030-18579-4_1", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The efficiency of a query execution plan depends on the accuracy of the\nselectivity estimates given to the query optimiser by the cost model. The cost\nmodel makes simplifying assumptions in order to produce said estimates in a\ntimely manner. These assumptions lead to selectivity estimation errors that\nhave dramatic effects on the quality of the resulting query execution plans. A\nconvenient assumption that is ubiquitous among current cost models is to assume\nthat attributes are independent with each other. However, it ignores potential\ncorrelations which can have a huge negative impact on the accuracy of the cost\nmodel. In this paper we attempt to relax the attribute value independence\nassumption without unreasonably deteriorating the accuracy of the cost model.\nWe propose a novel approach based on a particular type of Bayesian networks\ncalled Chow-Liu trees to approximate the distribution of attribute values\ninside each relation of a database. Our results on the TPC-DS benchmark show\nthat our method is an order of magnitude more precise than other approaches\nwhilst remaining reasonably efficient in terms of time and space.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2019 23:13:19 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Halford", "Max", ""], ["Saint-Pierre", "Philippe", ""], ["Morvan", "Frank", ""]]}, {"id": "1907.06417", "submitter": "Raul Fernandez", "authors": "Raul Fernandez-Fernandez, Juan G. Victores, David Estevez and Carlos\n  Balaguer", "title": "Quick, Stat!: A Statistical Analysis of the Quick, Draw! Dataset", "comments": "12 pages, Eurosim 2019", "journal-ref": null, "doi": "10.11128/arep.58", "report-no": null, "categories": "cs.CV cs.DB eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The Quick, Draw! Dataset is a Google dataset with a collection of 50 million\ndrawings, divided in 345 categories, collected from the users of the game\nQuick, Draw!. In contrast with most of the existing image datasets, in the\nQuick, Draw! Dataset, drawings are stored as time series of pencil positions\ninstead of a bitmap matrix composed by pixels. This aspect makes this dataset\nthe largest doodle dataset available at the time. The Quick, Draw! Dataset is\npresented as a great opportunity to researchers for developing and studying\nmachine learning techniques. Due to the size of this dataset and the nature of\nits source, there is a scarce of information about the quality of the drawings\ncontained. In this paper, a statistical analysis of three of the classes\ncontained in the Quick, Draw! Dataset is depicted: mountain, book and whale.\nThe goal is to give to the reader a first impression of the data collected in\nthis dataset. For the analysis of the quality of the drawings, a Classification\nNeural Network was trained to obtain a classification score. Using this\nclassification score and the parameters provided by the dataset, a statistical\nanalysis of the quality and nature of the drawings contained in this dataset is\nprovided.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 10:28:34 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 09:07:23 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Fernandez-Fernandez", "Raul", ""], ["Victores", "Juan G.", ""], ["Estevez", "David", ""], ["Balaguer", "Carlos", ""]]}, {"id": "1907.06723", "submitter": "Gustavo Machado", "authors": "Gustavo V. Machado, \\'Italo Cunha, Adriano C. M. Pereira, Leonardo B.\n  Oliveira", "title": "DOD-ETL: Distributed On-Demand ETL for Near Real-Time Business\n  Intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The competitive dynamics of the globalized market demand information on the\ninternal and external reality of corporations. Information is a precious asset\nand is responsible for establishing key advantages to enable companies to\nmaintain their leadership. However, reliable, rich information is no longer the\nonly goal. The time frame to extract information from data determines its\nusefulness. This work proposes DOD-ETL, a tool that addresses, in an innovative\nmanner, the main bottleneck in Business Intelligence solutions, the Extract\nTransform Load process (ETL), providing it in near real-time. DODETL achieves\nthis by combining an on-demand data stream pipeline with a distributed,\nparallel and technology-independent architecture with in-memory caching and\nefficient data partitioning. We compared DOD-ETL with other Stream Processing\nframeworks used to perform near real-time ETL and found DOD-ETL executes\nworkloads up to 10 times faster. We have deployed it in a large steelworks as a\nreplacement for its previous ETL solution, enabling near real-time reports\npreviously unavailable.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 20:06:03 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Machado", "Gustavo V.", ""], ["Cunha", "\u00cdtalo", ""], ["Pereira", "Adriano C. M.", ""], ["Oliveira", "Leonardo B.", ""]]}, {"id": "1907.06775", "submitter": "Lukas Iffl\\\"ander", "authors": "Lukas Iffl\\\"ander, Alexandra Dmitrienko, Christoph Hagen, Michael\n  Jobst, Samuel Kounev", "title": "Hands Off my Database: Ransomware Detection in Databases through Dynamic\n  Analysis of Query Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ransomware is an emerging threat which imposed a \\$ 5 billion loss in 2017\nand is predicted to hit \\$ 11.5 billion in 2019. While initially targeting PC\n(client) platforms, ransomware recently made the leap to server-side databases\n- starting in January 2017 with the MongoDB Apocalypse attack, followed by\nother attack waves targeting a wide range of DB types such as MongoDB, MySQL,\nElasticSearch, Cassandra, Hadoop, and CouchDB. While previous research has\ndeveloped countermeasures against client-side ransomware (e.g., CryptoDrop and\nShieldFS), the problem of server-side ransomware has received zero attention so\nfar.\n  In our work, we aim to bridge this gap and present DIMAQS (Dynamic\nIdentification of Malicious Query Sequences), a novel anti-ransomware solution\nfor databases. DIMAQS performs runtime monitoring of incoming queries and\npattern matching using Colored Petri Nets (CPNs) for attack detection. Our\nsystem design exhibits several novel techniques to enable efficient detection\nof malicious query sequences globally (i.e., without limiting detection to\ndistinct user connections). Our proof-of-concept implementation targets MySQL\nservers. The evaluation shows high efficiency with no false positives and no\nfalse negatives and very moderate performance overhead of under 5%. We will\npublish our data sets and implementation allowing the community to reproduce\nour tests and compare to our results.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 22:22:38 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Iffl\u00e4nder", "Lukas", ""], ["Dmitrienko", "Alexandra", ""], ["Hagen", "Christoph", ""], ["Jobst", "Michael", ""], ["Kounev", "Samuel", ""]]}, {"id": "1907.06946", "submitter": "Patrick Marcel", "authors": "Alexandre Chanson and Ben Crulis and Nicolas Labroche and Patrick\n  Marcel", "title": "A Subjective Interestingness measure for Business Intelligence\n  explorations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of defining a subjective interestingness\nmeasure for BI exploration. Such a measure involves prior modeling of the\nbelief of the user. The complexity of this problem lies in the impossibility to\nask the user about the degree of belief in each element composing their\nknowledge prior to the writing of a query. To this aim, we propose to\nautomatically infer this user belief based on the user's past interactions over\na data cube, the cube schema and other users past activities. We express the\nbelief under the form of a probability distribution over all the query parts\npotentially accessible to the user, and use a random walk to learn this\ndistribution. This belief is then used to define a first Subjective\nInterestingness measure over multidimensional queries. Experiments conducted on\nsimulated and real explorations show how this new subjective interestingness\nmeasure relates to prototypical and real user behaviors, and that query parts\noffer a reasonable proxy to infer user belief.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 11:43:59 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Chanson", "Alexandre", ""], ["Crulis", "Ben", ""], ["Labroche", "Nicolas", ""], ["Marcel", "Patrick", ""]]}, {"id": "1907.07303", "submitter": "Shuaicheng Ma", "authors": "Shuaicheng Ma, Yang Cao, Li Xiong", "title": "Effcient logging and querying for Blockchain-based cross-site genomic\n  dataset access audit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Genomic data have been collected by different institutions and\ncompanies and need to be shared for broader use. In a cross-site genomic data\nsharing system, a secure and transparent access control audit module plays an\nessential role in ensuring the accountability. The 2018 iDASH competition first\ntrack provides us with an opportunity to design efficient logging and querying\nsystem for cross-site genomic dataset access audit. We designed a\nblockchain-based log system which can provide a light-weight and widely\ncompatible module for existing blockchain platforms. The submitted solution won\nthe third place of the competition. In this paper, we report the technical\ndetails in our system. Methods: We present two methods: baseline method and\nenhanced method. We started with the baseline method and then adjusted our\nimplementation based on the competition evaluation criteria and characteristics\nof the log system. To overcome obstacles of indexing on the immutable\nBlockchain system, we designed a hierarchical timestamp structure which\nsupports efficient range queries on the timestamp field. Results: We\nimplemented our methods in Python3, tested the scalability, and compared the\nperformance using the test data supplied by competition organizer. We\nsuccessfully boosted the log retrieval speed for complex AND queries that\ncontain multiple predicates. For the range query, we boosted the speed for at\nleast one order of magnitude. The storage usage is reduced by 25%. Conclusion:\nWe demonstrate that Blockchain can be used to build a time and space efficient\nlog and query genomic dataset audit trail. Therefore, it provides a promising\nsolution for sharing genomic data with accountability requirement across\nmultiple sites.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 02:11:53 GMT"}, {"version": "v2", "created": "Fri, 26 Jul 2019 08:04:08 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Ma", "Shuaicheng", ""], ["Cao", "Yang", ""], ["Xiong", "Li", ""]]}, {"id": "1907.07378", "submitter": "C. Maria Keet", "authors": "C. Maria Keet, Zola Mahlaza, Mary-Jane Antia", "title": "CLaRO: a Data-driven CNL for Specifying Competency Questions", "comments": "24 pages, 3 figures, extended technical report of a shorter paper\n  submitted to an international conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Competency Questions (CQs) for an ontology and similar artefacts aim to\nprovide insights into the contents of an ontology and to demarcate its scope.\nThe absence of a controlled natural language, tooling and automation to support\nthe authoring of CQs has hampered their effective use in ontology development\nand evaluation. The few question templates that exists are based on informal\nanalyses of a small number of CQs and have limited coverage of question types\nand sentence constructions. We aim to fill this gap by proposing a\ntemplate-based CNL to author CQs, called CLaRO. For its design, we exploited a\nnew dataset of 234 CQs that had been processed automatically into 106 patterns,\nwhich we analysed and used to design a template-based CNL, with an additional\nCNL model and XML serialisation. The CNL was evaluated with a subset of\nquestions from the original dataset and with two sets of newly sourced CQs. The\ncoverage of CLaRO, with its 93 main templates and 41 linguistic variants, is\nabout 90% for unseen questions. CLaRO has the potential to facilitate\nstreamlining formalising ontology content requirements and, given that about\none third of the competency questions in the test sets turned out to be invalid\nquestions, assist in writing good questions.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 08:18:36 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Keet", "C. Maria", ""], ["Mahlaza", "Zola", ""], ["Antia", "Mary-Jane", ""]]}, {"id": "1907.07387", "submitter": "Martin Aum\\\"uller", "authors": "Martin Aum\\\"uller and Matteo Ceccarello", "title": "The Role of Local Intrinsic Dimensionality in Benchmarking Nearest\n  Neighbor Search", "comments": "Preprint of the paper accepted at SISAP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reconsiders common benchmarking approaches to nearest neighbor\nsearch. It is shown that the concept of local intrinsic dimensionality (LID)\nallows to choose query sets of a wide range of difficulty for real-world\ndatasets. Moreover, the effect of different LID distributions on the running\ntime performance of implementations is empirically studied. To this end,\ndifferent visualization concepts are introduced that allow to get a more\nfine-grained overview of the inner workings of nearest neighbor search\nprinciples. The paper closes with remarks about the diversity of datasets\ncommonly used for nearest neighbor search benchmarking. It is shown that such\nreal-world datasets are not diverse: results on a single dataset predict\nresults on all other datasets well.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 08:39:53 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Aum\u00fcller", "Martin", ""], ["Ceccarello", "Matteo", ""]]}, {"id": "1907.07405", "submitter": "Florin Rusu", "authors": "Florin Rusu and Zhiyi Huang", "title": "In-Depth Benchmarking of Graph Database Systems with the Linked Data\n  Benchmark Council (LDBC) Social Network Benchmark (SNB)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we present the first results of a complete implementation of\nthe LDBC SNB benchmark -- interactive short, interactive complex, and business\nintelligence -- in two native graph database systems---Neo4j and TigerGraph. In\naddition to thoroughly evaluating the performance of all of the 46 queries in\nthe benchmark on four scale factors -- SF-1, SF-10, SF-100, and SF-1000 -- and\nthree computing architectures -- on premise and in the cloud -- we also measure\nthe bulk loading time and storage size. Our results show that TigerGraph is\nconsistently outperforming Neo4j on the majority of the queries---by two or\nmore orders of magnitude (100X factor) on certain interactive complex and\nbusiness intelligence queries. The gap increases with the size of the data\nsince only TigerGraph is able to scale to SF-1000---Neo4j finishes only 12 of\nthe 25 business intelligence queries in reasonable time. Nonetheless, Neo4j is\ngenerally faster at bulk loading graph data up to SF-100. A key to our study is\nthe active involvement of the vendors in the tuning of their platforms. In\norder to encourage reproducibility, we make all the code, scripts, and\nconfiguration parameters publicly available online.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 09:17:27 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Rusu", "Florin", ""], ["Huang", "Zhiyi", ""]]}, {"id": "1907.08038", "submitter": "Soheila Ghane Ezabadi", "authors": "Soheila Ghane, Lars Kulik, Kotagiri Ramamohanarao", "title": "A Differentially Private Algorithm for Range Queries on Trajectories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel algorithm to ensure $\\epsilon$-differential privacy for\nanswering range queries on trajectory data. In order to guarantee privacy,\ndifferential privacy mechanisms add noise to either data or query, thus\nintroducing errors to queries made and potentially decreasing the utility of\ninformation. In contrast to the state-of-the-art, our method achieves\nsignificantly lower error as it is the first data- and query-aware approach for\nsuch queries. The key challenge for answering range queries on trajectory data\nprivately is to ensure an accurate count. Simply representing a trajectory as a\nset instead of \\emph{sequence} of points will generally lead to highly\ninaccurate query answers as it ignores the sequential dependency of location\npoints in trajectories, i.e., will violate the consistency of trajectory data.\nFurthermore, trajectories are generally unevenly distributed across a city and\nadding noise uniformly will generally lead to a poor utility. To achieve\ndifferential privacy, our algorithm adaptively adds noise to the input data\naccording to the given query set. It first privately partitions the data space\ninto uniform regions and computes the traffic density of each region. The\nregions and their densities, in addition to the given query set, are then used\nto estimate the distribution of trajectories over the queried space, which\nensures high accuracy for the given query set. We show the accuracy and\nefficiency of our algorithm using extensive empirical evaluations on real and\nsynthetic data sets.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 13:12:02 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Ghane", "Soheila", ""], ["Kulik", "Lars", ""], ["Ramamohanarao", "Kotagiri", ""]]}, {"id": "1907.08138", "submitter": "Lisa Ehrlinger", "authors": "Lisa Ehrlinger, Elisa Rusz, Wolfram W\\\"o{\\ss}", "title": "A Survey of Data Quality Measurement and Monitoring Tools", "comments": "35 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-quality data is key to interpretable and trustworthy data analytics and\nthe basis for meaningful data-driven decisions. In practical scenarios, data\nquality is typically associated with data preprocessing, profiling, and\ncleansing for subsequent tasks like data integration or data analytics.\nHowever, from a scientific perspective, a lot of research has been published\nabout the measurement (i.e., the detection) of data quality issues and\ndifferent generally applicable data quality dimensions and metrics have been\ndiscussed. In this work, we close the gap between research into data quality\nmeasurement and practical implementations by investigating the functional scope\nof current data quality tools. With a systematic search, we identified 667\nsoftware tools dedicated to \"data quality\", from which we evaluated 13 tools\nwith respect to three functionality areas: (1) data profiling, (2) data quality\nmeasurement in terms of metrics, and (3) continuous data quality monitoring. We\nselected the evaluated tools with regard to pre-defined exclusion criteria to\nensure that they are domain-independent, provide the investigated functions,\nand are evaluable freely or as trial. This survey aims at a comprehensive\noverview on state-of-the-art data quality tools and reveals potential for their\nfunctional enhancement. Additionally, the results allow a critical discussion\non concepts, which are widely accepted in research, but hardly implemented in\nany tool observed, for example, generally applicable data quality metrics.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 16:19:52 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Ehrlinger", "Lisa", ""], ["Rusz", "Elisa", ""], ["W\u00f6\u00df", "Wolfram", ""]]}, {"id": "1907.08470", "submitter": "Erich Gr\\\"adel", "authors": "Erich Gr\\\"adel and Val Tannen", "title": "Provenance Analysis for Logic and Games", "comments": null, "journal-ref": "Moscow J. Comb. Number Th. 9 (2020) 203-228", "doi": "10.2140/moscow.2020.9.203", "report-no": null, "categories": "cs.LO cs.DB math.LO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  A model checking computation checks whether a given logical sentence is true\nin a given finite structure. Provenance analysis abstracts from such a\ncomputation mathematical information on how the result depends on the atomic\ndata that describe the structure. In database theory, provenance analysis by\ninterpretations in commutative semirings has been rather succesful for positive\nquery languages (such a unions of conjunctive queries, positive relational\nalgebra, or datalog). However, it did not really offer an adequate treatment of\nnegation or missing information.\n  Here we propose a new approach for the provenance analysis of logics with\nnegation, such as first-order logic and fixed-point logics. It is closely\nrelated to a provenance analysis of the associated model-checking games, and\nbased on new semirings of dual-indeterminate polynomials or dual-indeterminate\nformal power series. These are obtained by taking quotients of traditional\nprovenance semirings by congruences that are generated by products of positive\nand negative provenance tokens. Beyond the use for model-checking problems in\nlogics, provenance analysis of games is of independent interest. Provenance\nvalues in games provide detailed information about the number and properties of\nthe strategies of the players, far beyond the question whether or not a player\nhas a winning strategy from a given position.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 11:43:41 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 12:06:00 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Gr\u00e4del", "Erich", ""], ["Tannen", "Val", ""]]}, {"id": "1907.08667", "submitter": "Katsiaryna Mirylenka", "authors": "Thomas Gschwind, Christoph Miksovic, Julian Minder, Katsiaryna\n  Mirylenka, Paolo Scotton", "title": "Fast Record Linkage for Company Entities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Record linkage is an essential part of nearly all real-world systems that\nconsume structured and unstructured data coming from different sources.\nTypically no common key is available for connecting records. Massive data\ncleaning and data integration processes often have to be completed before any\ndata analytics and further processing can be performed. Although record linkage\nis frequently regarded as a somewhat tedious but necessary step, it reveals\nvaluable insights into the data at hand. These insights guide further analytic\napproaches to the data and support data visualization.\n  In this work we focus on company entity matching, where company name,\nlocation and industry are taken into account. Our contribution is an\nend-to-end, highly scalable, enterprise-grade system that uses rule-based\nlinkage algorithms extended with a machine learning approach to account for\nshort company names. Linkage time is greatly reduced by efficient decomposition\nof the search space using MinHash. High linkage accuracy is achieved by the\nproposed thorough scoring process of the matching candidates.\n  Based on real-world ground truth datasets, we show that our approach reaches\na recall of 91% compared to 73% for baseline approaches. These results are\nachieved while scaling linearly with the number of nodes used in the system.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 20:00:41 GMT"}, {"version": "v2", "created": "Fri, 23 Aug 2019 12:29:37 GMT"}, {"version": "v3", "created": "Fri, 27 Sep 2019 15:22:35 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Gschwind", "Thomas", ""], ["Miksovic", "Christoph", ""], ["Minder", "Julian", ""], ["Mirylenka", "Katsiaryna", ""], ["Scotton", "Paolo", ""]]}, {"id": "1907.08671", "submitter": "Michael F\\\"arber", "authors": "Michael F\\\"arber", "title": "Linked Crunchbase: A Linked Data API and RDF Data Set About Innovative\n  Companies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crunchbase is an online platform collecting information about startups and\ntechnology companies, including attributes and relations of companies, people,\nand investments. Data contained in Crunchbase is, to a large extent, not\navailable elsewhere, making Crunchbase to a unique data source. In this paper,\nwe present how to bring Crunchbase to the Web of Data so that its data can be\nused in the machine-readable RDF format by anyone on the Web. First, we give\ninsights into how we developed and hosted a Linked Data API for Crunchbase and\nhow sameAs links to other data sources are integrated. Then, we present our\nmethod for crawling RDF data based on this API to build a custom Crunchbase RDF\nknowledge graph. We created an RDF data set with over 347 million triples,\nincluding 781k people, 659k organizations, and 343k investments. Our Crunchbase\nLinked Data API is available online at http://linked-crunchbase.org.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 20:08:47 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["F\u00e4rber", "Michael", ""]]}, {"id": "1907.09387", "submitter": "Guenter Hesse", "authors": "Guenter Hesse, Werner Sinzig, Christoph Matthies, Matthias Uflacker", "title": "Application of Data Stream Processing Technologies in Industry 4.0 --\n  What is Missing?", "comments": "Accepted at 2019 International Conference on Data Science, Technology\n  and Applications (DATA). The final authenticated version will be available\n  online in the conference proceedings - ISBN: 978-989-758-377-3, ISSN:\n  2184-285X", "journal-ref": "Proceedings of the 8th International Conference on Data Science,\n  Technology and Applications (DATA), 2019, pages 304-310", "doi": "10.5220/0007950203040310", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Industry 4.0 is becoming more and more important for manufacturers as the\ndevelopments in the area of Internet of Things advance. Another technology\ngaining more attention is data stream processing systems. Although such\nstreaming frameworks seem to be a natural fit for Industry 4.0 scenarios, their\napplication in this context is still low. The contributions in this paper are\nthreefold. Firstly, we present industry findings that we derived from site\ninspections with a focus on Industry 4.0. Moreover, our view on Industry 4.0\nand important related aspects is elaborated. As a third contribution, we\nillustrate our opinion on why data stream processing technologies could act as\nan enabler for Industry 4.0 and point out possible obstacles on this way.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 16:01:21 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Hesse", "Guenter", ""], ["Sinzig", "Werner", ""], ["Matthies", "Christoph", ""], ["Uflacker", "Matthias", ""]]}, {"id": "1907.09535", "submitter": "Niels M\\\"undler", "authors": "Niels M\\\"undler", "title": "Association rule mining and itemset-correlation based variants", "comments": "IEEE format, 6 pages, 4 figures, seminar paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Association rules express implication formed relations among attributes in\ndatabases of itemsets. The apriori algorithm is presented, the basis for most\nassociation rule mining algorithms. It works by pruning away rules that need\nnot be evaluated based on the user specified minimum support confidence.\nAdditionally, variations of the algorithm are presented that enable it to\nhandle quantitative attributes and to extract rules about generalizations of\nitems, but preserve the downward closure property that enables pruning.\nIntertransformation of the extensions is proposed for special cases.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 19:11:49 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["M\u00fcndler", "Niels", ""]]}, {"id": "1907.09657", "submitter": "Junyang Gao", "authors": "Junyang Gao, Xian Li, Yifan Ethan Xu, Bunyamin Sisman, Xin Luna Dong,\n  Jun Yang", "title": "Efficient Knowledge Graph Accuracy Evaluation", "comments": "in VLDB 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Estimation of the accuracy of a large-scale knowledge graph (KG) often\nrequires humans to annotate samples from the graph. How to obtain statistically\nmeaningful estimates for accuracy evaluation while keeping human annotation\ncosts low is a problem critical to the development cycle of a KG and its\npractical applications. Surprisingly, this challenging problem has largely been\nignored in prior research. To address the problem, this paper proposes an\nefficient sampling and evaluation framework, which aims to provide quality\naccuracy evaluation with strong statistical guarantee while minimizing human\nefforts. Motivated by the properties of the annotation cost function observed\nin practice, we propose the use of cluster sampling to reduce the overall cost.\nWe further apply weighted and two-stage sampling as well as stratification for\nbetter sampling designs. We also extend our framework to enable efficient\nincremental evaluation on evolving KG, introducing two solutions based on\nstratified sampling and a weighted variant of reservoir sampling. Extensive\nexperiments on real-world datasets demonstrate the effectiveness and efficiency\nof our proposed solution. Compared to baseline approaches, our best solutions\ncan provide up to 60% cost reduction on static KG evaluation and up to 80% cost\nreduction on evolving KG evaluation, without loss of evaluation quality.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 02:06:48 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Gao", "Junyang", ""], ["Li", "Xian", ""], ["Xu", "Yifan Ethan", ""], ["Sisman", "Bunyamin", ""], ["Dong", "Xin Luna", ""], ["Yang", "Jun", ""]]}, {"id": "1907.09693", "submitter": "Qinbin Li", "authors": "Qinbin Li, Zeyi Wen, Zhaomin Wu, Sixu Hu, Naibo Wang, Yuan Li, Xu Liu,\n  Bingsheng He", "title": "A Survey on Federated Learning Systems: Vision, Hype and Reality for\n  Data Privacy and Protection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning has been a hot research topic in enabling the\ncollaborative training of machine learning models among different organizations\nunder the privacy restrictions. As researchers try to support more machine\nlearning models with different privacy-preserving approaches, there is a\nrequirement in developing systems and infrastructures to ease the development\nof various federated learning algorithms. Similar to deep learning systems such\nas PyTorch and TensorFlow that boost the development of deep learning,\nfederated learning systems (FLSs) are equivalently important, and face\nchallenges from various aspects such as effectiveness, efficiency, and privacy.\nIn this survey, we conduct a comprehensive review on federated learning\nsystems. To achieve smooth flow and guide future research, we introduce the\ndefinition of federated learning systems and analyze the system components.\nMoreover, we provide a thorough categorization for federated learning systems\naccording to six different aspects, including data distribution, machine\nlearning model, privacy mechanism, communication architecture, scale of\nfederation and motivation of federation. The categorization can help the design\nof federated learning systems as shown in our case studies. By systematically\nsummarizing the existing federated learning systems, we present the design\nfactors, case studies, and future research opportunities.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 04:30:50 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 08:06:54 GMT"}, {"version": "v3", "created": "Tue, 3 Dec 2019 13:34:19 GMT"}, {"version": "v4", "created": "Wed, 1 Apr 2020 13:33:21 GMT"}, {"version": "v5", "created": "Wed, 13 Jan 2021 11:30:07 GMT"}, {"version": "v6", "created": "Thu, 1 Jul 2021 04:04:38 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Li", "Qinbin", ""], ["Wen", "Zeyi", ""], ["Wu", "Zhaomin", ""], ["Hu", "Sixu", ""], ["Wang", "Naibo", ""], ["Li", "Yuan", ""], ["Liu", "Xu", ""], ["He", "Bingsheng", ""]]}, {"id": "1907.09820", "submitter": "Angelos Charalambidis", "authors": "Angelos Charalambidis, Christos Nomikos, Panos Rondogiannis", "title": "The Expressive Power of Higher-Order Datalog", "comments": "Paper presented at the 35th International Conference on Logic\n  Programming (ICLP 2019), Las Cruces, New Mexico, USA, 20-25 September 2019,\n  24 pages, LaTeX", "journal-ref": "Theory and Practice of Logic Programming 19 (2019) 925-940", "doi": "10.1017/S1471068419000279", "report-no": null, "categories": "cs.PL cs.CC cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A classical result in descriptive complexity theory states that Datalog\nexpresses exactly the class of polynomially computable queries on ordered\ndatabases. In this paper we extend this result to the case of higher-order\nDatalog. In particular, we demonstrate that on ordered databases, for all\n$k\\geq2$, $k$-order Datalog captures $(k-1)$-EXPTIME. This result suggests that\nhigher-order extensions of Datalog possess superior expressive power and they\nare worthwhile of further investigation both in theory and in practice. This\npaper is under consideration for acceptance in TPLP.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 11:21:49 GMT"}, {"version": "v2", "created": "Wed, 24 Jul 2019 14:03:23 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Charalambidis", "Angelos", ""], ["Nomikos", "Christos", ""], ["Rondogiannis", "Panos", ""]]}, {"id": "1907.10125", "submitter": "Sudeepa Roy", "authors": "Debmalya Panigrahi, Shweta Patwa, and Sudeepa Roy", "title": "Generalized Deletion Propagation on Counting Conjunctive Query Answers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the computational complexity of minimizing the source\nside-effect in order to remove a given number of tuples from the output of a\nconjunctive query. In particular, given a multi-relational database $D$, a\nconjunctive query $Q$, and a positive integer $k$ as input, the goal is to find\na minimum subset of input tuples to remove from D that would eliminate at least\n$k$ output tuples from $Q(D)$. This problem generalizes the well-studied\ndeletion propagation problem in databases. In addition, it encapsulates the\nnotion of intervention for aggregate queries used in data analysis with\napplications to explaining interesting observations on the output. We show a\ndichotomy in the complexity of this problem for the class of full conjunctive\nqueries without self-joins by giving a characterization on the structure of $Q$\nthat makes the problem either polynomial-time solvable or NP-hard. Our proof of\nthis dichotomy result already gives an exact algorithm in the easy cases; we\ncomplement this by giving an approximation algorithm for the hard cases of the\nproblem.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 20:56:00 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Panigrahi", "Debmalya", ""], ["Patwa", "Shweta", ""], ["Roy", "Sudeepa", ""]]}, {"id": "1907.10278", "submitter": "Ariyam Das", "authors": "Ariyam Das and Carlo Zaniolo", "title": "A Case for Stale Synchronous Distributed Model for Declarative Recursive\n  Computation", "comments": "Paper presented at the 35th International Conference on Logic\n  Programming (ICLP 2019), Las Cruces, New Mexico, USA, 20-25 September 2019,\n  16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DB cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large class of traditional graph and data mining algorithms can be\nconcisely expressed in Datalog, and other Logic-based languages, once\naggregates are allowed in recursion. In fact, for most BigData algorithms, the\ndifficult semantic issues raised by the use of non-monotonic aggregates in\nrecursion are solved by Pre-Mappability (PreM), a property that assures that\nfor a program with aggregates in recursion there is an equivalent\naggregate-stratified program. In this paper we show that, by bringing together\nthe formal abstract semantics of stratified programs with the efficient\noperational one of unstratified programs, PreM can also facilitate and improve\ntheir parallel execution. We prove that PreM-optimized lock-free and\ndecomposable parallel semi-naive evaluations produce the same results as the\nsingle executor programs. Therefore, PreM can be assimilated into the\ndata-parallel computation plans of different distributed systems, irrespective\nof whether these follow bulk synchronous parallel (BSP) or asynchronous\ncomputing models. In addition, we show that non-linear recursive queries can be\nevaluated using a hybrid stale synchronous parallel (SSP) model on distributed\nenvironments. After providing a formal correctness proof for the recursive\nquery evaluation with PreM under this relaxed synchronization model, we present\nexperimental evidence of its benefits. This paper is under consideration for\nacceptance in Theory and Practice of Logic Programming (TPLP).\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 07:35:18 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Das", "Ariyam", ""], ["Zaniolo", "Carlo", ""]]}, {"id": "1907.10492", "submitter": "EPTCS", "authors": "Francesco Belardinelli (Department of Computing, Imperial College\n  London, UK and Laboratoire IBISC, University of Evry, France), Umberto Grandi\n  (IRIT, University of Toulouse, France)", "title": "Social Choice Methods for Database Aggregation", "comments": "In Proceedings TARK 2019, arXiv:1907.08335. arXiv admin note:\n  substantial text overlap with arXiv:1802.08586", "journal-ref": "EPTCS 297, 2019, pp. 50-67", "doi": "10.4204/EPTCS.297.4", "report-no": null, "categories": "cs.LO cs.AI cs.DB cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge can be represented compactly in multiple ways, from a set of\npropositional formulas, to a Kripke model, to a database. In this paper we\nstudy the aggregation of information coming from multiple sources, each source\nsubmitting a database modelled as a first-order relational structure. In the\npresence of integrity constraints, we identify classes of aggregators that\nrespect them in the aggregated database, provided these are satisfied in all\nindividual databases. We also characterise languages for first-order queries on\nwhich the answer to a query on the aggregated database coincides with the\naggregation of the answers to the query obtained on each individual database.\nThis contribution is meant to be a first step on the application of techniques\nfrom social choice theory to knowledge representation in databases.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 03:13:22 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Belardinelli", "Francesco", "", "Department of Computing, Imperial College\n  London, UK and Laboratoire IBISC, University of Evry, France"], ["Grandi", "Umberto", "", "IRIT, University of Toulouse, France"]]}, {"id": "1907.10528", "submitter": "Joe Raad", "authors": "Joe Raad, Nathalie Pernelle, Fatiha Sa\\\"is, Wouter Beek and Frank van\n  Harmelen", "title": "The sameAs Problem: A Survey on Identity Management in the Web of Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a decentralised knowledge representation system such as the Web of Data,\nit is common and indeed desirable for different knowledge graphs to overlap.\nWhenever multiple names are used to denote the same thing, owl:sameAs\nstatements are needed in order to link the data and foster reuse. Whilst the\ndeductive value of such identity statements can be extremely useful in\nenhancing various knowledge-based systems, incorrect use of identity can have\nwide-ranging effects in a global knowledge space like the Web of Data. With\nseveral works already proven that identity in the Web is broken, this survey\ninvestigates the current state of this \"sameAs problem\". An open discussion\nhighlights the main weaknesses suffered by solutions in the literature, and\ndraws open challenges to be faced in the future.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 15:42:22 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Raad", "Joe", ""], ["Pernelle", "Nathalie", ""], ["Sa\u00efs", "Fatiha", ""], ["Beek", "Wouter", ""], ["van Harmelen", "Frank", ""]]}, {"id": "1907.10603", "submitter": "Iovka Boneva", "authors": "Iovka Boneva, J\\'er\\'emie Dusart, Daniel Fern\\'andez \\'Alvarez, Jose\n  Emilio Labra Gayo", "title": "Semi Automatic Construction of ShEx and SHACL Schemas", "comments": "Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a method for the construction of SHACL or ShEx constraints for an\nexisting RDF dataset. It has two components that are used conjointly: an\nalgorithm for automatic schema construction, and an interactive workflow for\nediting the schema. The schema construction algorithm takes as input sets of\nsample nodes and constructs a shape constraint for every sample set. It can be\nparametrized by a schema pattern that defines structural requirements for the\nschema to be constructed. Schema patterns are also used to feed the algorithm\nwith relevant information about the dataset coming from a domain expert or from\nsome ontology. The interactive workflow provides useful information about the\ndataset, shows validation results w.r.t. the schema under construction, and\noffers schema editing operations that combined with the schema construction\nalgorithm allow to build a complex ShEx or SHACL schema.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 10:47:21 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Boneva", "Iovka", ""], ["Dusart", "J\u00e9r\u00e9mie", ""], ["\u00c1lvarez", "Daniel Fern\u00e1ndez", ""], ["Gayo", "Jose Emilio Labra", ""]]}, {"id": "1907.10814", "submitter": "Yang Cao", "authors": "Yang Cao, Yonghui Xiao, Li Xiong, Liquan Bai, and Masatoshi Yoshikawa", "title": "Protecting Spatiotemporal Event Privacy in Continuous Location-Based\n  Services", "comments": "accepted in TKDE. arXiv admin note: substantial text overlap with\n  arXiv:1810.09152", "journal-ref": "IEEE Transactions on Knowledge and Data Engineering (TKDE) 2020", "doi": "10.1109/TKDE.2019.2963312", "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Location privacy-preserving mechanisms (LPPMs) have been extensively studied\nfor protecting users' location privacy by releasing a perturbed location to\nthird parties such as location-based service providers. However, when a user's\nperturbed locations are released continuously, existing LPPMs may not protect\nthe sensitive information about the user's spatiotemporal activities, such as\n\"visited hospital in the last week\" or \"regularly commuting between Address 1\nand Address 2\" (it is easy to infer that Addresses 1 and 2 may be home and\noffice), which we call it \\textit{spatiotemporal event}. In this paper, we\nfirst formally define {spatiotemporal event} as Boolean expressions between\nlocation and time predicates, and then we define $ \\epsilon\n$-\\textit{spatiotemporal event privacy} by extending the notion of differential\nprivacy. Second, to understand how much spatiotemporal event privacy that\nexisting LPPMs can provide, we design computationally efficient algorithms to\nquantify the privacy leakage of state-of-the-art LPPMs when an adversary has\nprior knowledge of the user's initial probability over possible locations. It\nturns out that the existing LPPMs cannot adequately protect spatiotemporal\nevent privacy. Third, we propose a framework, PriSTE, to transform an existing\nLPPM into one protecting spatiotemporal event privacy against adversaries with\n\\textit{any} prior knowledge. Our experiments on real-life and synthetic data\nverified that the proposed method is effective and efficient.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 03:17:41 GMT"}, {"version": "v2", "created": "Sat, 16 May 2020 09:30:23 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Cao", "Yang", ""], ["Xiao", "Yonghui", ""], ["Xiong", "Li", ""], ["Bai", "Liquan", ""], ["Yoshikawa", "Masatoshi", ""]]}, {"id": "1907.10914", "submitter": "Fernando S\\'aenz-P\\'erez", "authors": "Fernando S\\'aenz-P\\'erez", "title": "Applying Constraint Logic Programming to SQL Semantic Analysis", "comments": "Paper presented at the 35th International Conference on Logic\n  Programming (ICLP 2019), Las Cruces, New Mexico, USA, 20-25 September 2019,\n  16 pages", "journal-ref": "Theory and Practice of Logic Programming 19 (2019) 808-825", "doi": "10.1017/S1471068419000206", "report-no": null, "categories": "cs.DB cs.AI cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes the use of Constraint Logic Programming (CLP) to model\nSQL queries in a data-independent abstract layer by focusing on some semantic\nproperties for signalling possible errors in such queries. First, we define a\ntranslation from SQL to Datalog, and from Datalog to CLP, so that solving this\nCLP program will give information about inconsistency, tautology, and possible\nsimplifications. We use different constraint domains which are mapped to SQL\ntypes, and propose them to cooperate for improving accuracy. Our approach\nleverages a deductive system that includes SQL and Datalog, and we present an\nimplementation in this system which is currently being tested in classroom,\nshowing its advantages and differences with respect to other approaches, as\nwell as some performance data. This paper is under consideration for acceptance\nin TPLP.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 09:19:30 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["S\u00e1enz-P\u00e9rez", "Fernando", ""]]}, {"id": "1907.10984", "submitter": "Sankardeep Chakraborty", "authors": "Kentaro Sumigawa, Sankardeep Chakraborty, Kunihiko Sadakane, Srinivasa\n  Rao Satti", "title": "Enumerating Range Modes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the range mode problem where given a sequence and a query range\nin it, we want to find items with maximum frequency in the range. We give time-\nand space- efficient algorithms for this problem. Our algorithms are efficient\nfor small maximum frequency cases. We also consider a natural generalization of\nthe problem: the range mode enumeration problem, for which there has been no\nknown efficient algorithms. Our algorithms have query time complexities which\nis linear to the output size plus small terms.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 11:48:57 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Sumigawa", "Kentaro", ""], ["Chakraborty", "Sankardeep", ""], ["Sadakane", "Kunihiko", ""], ["Satti", "Srinivasa Rao", ""]]}, {"id": "1907.11651", "submitter": "Mohsen Rakhshandehroo", "authors": "Mohsen Rakhshandehroo, Mohammad Rajabdorri", "title": "Time Series Analysis of Electricity Price and Demand to Find\n  Cyber-attacks using Stationary Analysis", "comments": "9pages, 13 figs, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CR cs.DB cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With developing of computation tools in the last years, data analysis methods\nto find insightful information are becoming more common among industries and\nresearchers. This paper is the first part of the times series analysis of New\nEngland electricity price and demand to find anomaly in the data. In this paper\ntime-series stationary criteria to prepare data for further times-series\nrelated analysis is investigated. Three main analysis are conducted in this\npaper, including moving average, moving standard deviation and augmented\nDickey-Fuller test. The data used in this paper is New England big data from 9\ndifferent operational zones. For each zone, 4 different variables including\nday-ahead (DA) electricity demand, price and real-time (RT) electricity demand\nprice are considered.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 18:11:05 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 15:15:11 GMT"}, {"version": "v3", "created": "Sun, 29 Sep 2019 19:21:14 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Rakhshandehroo", "Mohsen", ""], ["Rajabdorri", "Mohammad", ""]]}, {"id": "1907.11743", "submitter": "Doris Jung-Lin Lee", "authors": "Doris Jung-Lin Lee, Jaewoo Kim, Renxuan Wang, Aditya Parameswaran", "title": "SCATTERSEARCH: Visual Querying of Scatterplot Visualizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scatterplots are one of the simplest and most commonly-used visualizations\nfor understanding quantitative, multidimensional data. However, since\nscatterplots only depict two attributes at a time, analysts often need to\nmanually generate and inspect large numbers of scatterplots to make sense of\nlarge datasets with many attributes. We present a visual query system for\nscatterplots, SCATTERSEARCH, that enables users to visually search and browse\nthrough large collections of scatterplots. Users can query for other\nvisualizations based on a region of interest or find other scatterplots that\n\"look similar'' to a selected one. We present two demo scenarios, provide a\nsystem overview of SCATTERSEARCH, and outline future directions.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 18:33:45 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Lee", "Doris Jung-Lin", ""], ["Kim", "Jaewoo", ""], ["Wang", "Renxuan", ""], ["Parameswaran", "Aditya", ""]]}, {"id": "1907.11803", "submitter": "Omid Jafari", "authors": "Omid Jafari, John Ossorgin, Parth Nagarkar", "title": "qwLSH: Cache-conscious Indexing for Processing Similarity Search Query\n  Workloads in High-Dimensional Spaces", "comments": "Extended version of the published work", "journal-ref": "In Proceedings of ICMR '19, pp. 329-333. ACM, 2019", "doi": "10.1145/3323873.3325048", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity search queries in high-dimensional spaces are an important type of\nqueries in many domains such as image processing, machine learning, etc. Since\nexact similarity search indexing techniques suffer from the well-known curse of\ndimensionality in high-dimensional spaces, approximate search techniques are\noften utilized instead. Locality Sensitive Hashing (LSH) has been shown to be\nan effective approximate search method for solving similarity search queries in\nhigh-dimensional spaces. Often times, queries in real-world settings arrive as\npart of a query workload. LSH and its variants are particularly designed to\nsolve single queries effectively. They suffer from one major drawback while\nexecuting query workloads: they do not take into consideration important data\ncharacteristics for effective cache utilization while designing the index\nstructures. In this paper, we present qwLSH, an index structure for efficiently\nprocessing similarity search query workloads in high-dimensional spaces. We\nintelligently divide a given cache during processing of a query workload by\nusing novel cost models. Experimental results show that, given a query\nworkload, qwLSH is able to perform faster than existing techniques due to its\nunique cost models and strategies.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 22:17:07 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Jafari", "Omid", ""], ["Ossorgin", "John", ""], ["Nagarkar", "Parth", ""]]}, {"id": "1907.12415", "submitter": "Nantia Makrynioti", "authors": "Nantia Makrynioti, Ruy Ley-Wild, Vasilis Vassalos", "title": "sql4ml A declarative end-to-end workflow for machine learning", "comments": "14 pages, 9 figures, replaced code repository link", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present sql4ml, a system for expressing supervised machine learning (ML)\nmodels in SQL and automatically training them in TensorFlow. The primary\nmotivation for this work stems from the observation that in many data science\ntasks there is a back-and-forth between a relational database that stores the\ndata and a machine learning framework. Data preprocessing and feature\nengineering typically happen in a database, whereas learning is usually\nexecuted in separate ML libraries. This fragmented workflow requires from the\nusers to juggle between different programming paradigms and software systems.\nWith sql4ml the user can express both feature engineering and ML algorithms in\nSQL, while the system translates this code to an appropriate representation for\ntraining inside a machine learning framework. We describe our translation\nmethod, present experimental results from applying it on three well-known ML\nalgorithms and discuss the usability benefits from concentrating the entire\nworkflow on the database side.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 13:28:59 GMT"}, {"version": "v2", "created": "Fri, 2 Aug 2019 16:18:27 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Makrynioti", "Nantia", ""], ["Ley-Wild", "Ruy", ""], ["Vassalos", "Vasilis", ""]]}, {"id": "1907.12817", "submitter": "Alessandro Berti Mr", "authors": "Alessandro Berti", "title": "Increasing Scalability of Process Mining using Event Dataframes: How\n  Data Structure Matters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Process Mining is a branch of Data Science that aims to extract\nprocess-related information from event data contained in information systems,\nthat is steadily increasing in amount. Many algorithms, and a general-purpose\nopen source framework (ProM 6), have been developed in the last years for\nprocess discovery, conformance checking, machine learning on event data.\nHowever, in very few cases scalability has been a target, prioritizing the\nquality of the output over the execution speed and the optimization of\nresources. This is making progressively more difficult to apply process mining\nwith mainstream workstations on real-life event data with any open source\nprocess mining framework. Hence, exploring more scalable storage techniques,\nin-memory data structures, more performant algorithms is a strictly incumbent\nneed. In this paper, we propose the usage of mainstream columnar storages and\ndataframes to increase the scalability of process mining. These can replace the\nclassic event log structures in most tasks, but require completely different\nimplementations with regards to mainstream process mining algorithms.\nDataframes will be defined, some algorithms on such structures will be\npresented and their complexity will be calculated.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 10:00:52 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 07:14:32 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Berti", "Alessandro", ""]]}, {"id": "1907.13060", "submitter": "William O'Mullane", "authors": "William O'Mullane, Niall Gaffney, Frossie Economou, Arfon M. Smith, J.\n  Ross Thomson, Tim Jenness", "title": "The demise of the filesystem and multi level service architecture", "comments": "Submitted as decadal APC paper 2019. arXiv admin note: text overlap\n  with arXiv:1905.05116", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many astronomy data centres still work on filesystems. Industry has moved on;\ncurrent practice in computing infrastructure is to achieve Big Data scalability\nusing object stores rather than POSIX file systems. This presents us with\nopportunities for portability and reuse of software underlying processing and\narchive systems but it also causes problems for legacy implementations in\ncurrent data centers.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 22:59:49 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 16:48:54 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["O'Mullane", "William", ""], ["Gaffney", "Niall", ""], ["Economou", "Frossie", ""], ["Smith", "Arfon M.", ""], ["Thomson", "J. Ross", ""], ["Jenness", "Tim", ""]]}, {"id": "1907.13218", "submitter": "Ghareeb Falazi", "authors": "Ghareeb Falazi, Vikas Khinchi, Uwe Breitenb\\\"ucher, Frank Leymann", "title": "Transactional Properties of Permissioned Blockchains", "comments": "12 pages. This is a pre-print of an article published in SICS\n  Software-Intensive Cyber-Physical Systems. The final authenticated version is\n  available online at: https://doi.org/10.1007/s00450-019-00411-y", "journal-ref": null, "doi": "10.1007/s00450-019-00411-y", "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional distributed transaction processing (TP) systems, such as\nreplicated databases, faced difficulties in getting wide adoption for scenarios\nof enterprise integration due to the level of mutual trust required.\nIronically, public blockchains, which promised to solve the problem of mutual\ntrust in collaborative processes, suffer from issues like scalability,\nprobabilistic transaction finality, and lack of data confidentiality. To tackle\nthese issues, permissioned blockchains were introduced as an alternative\napproach combining the positives of the two worlds and avoiding their\ndrawbacks. However, no sufficient analysis has been done to emphasize their\nactual capabilities regarding TP. In this paper, we identify a suitable\ncollection of TP criteria to analyze permissioned blockchains and apply them to\na prominent set of these systems. Finally, we compare the derived properties\nand provide general conclusions.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 20:53:31 GMT"}, {"version": "v2", "created": "Wed, 28 Aug 2019 11:22:20 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Falazi", "Ghareeb", ""], ["Khinchi", "Vikas", ""], ["Breitenb\u00fccher", "Uwe", ""], ["Leymann", "Frank", ""]]}, {"id": "1907.13264", "submitter": "Shaikh Arifuzzaman", "authors": "Janak Dahal, Elias Ioup, Shaikh Arifuzzaman, Mahdi Abdelguerfi", "title": "Distributed Streaming Analytics on Large-scale Oceanographic Data using\n  Apache Spark", "comments": "Preprint, 12 pages, Big Data and Scalable Computing (BDSC) research\n  group, Computer Science, University of New Orleans", "journal-ref": null, "doi": null, "report-no": "BDSC-19-01-01", "categories": "cs.DC cs.DB cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Real-world data from diverse domains require real-time scalable analysis.\nLarge-scale data processing frameworks or engines such as Hadoop fall short\nwhen results are needed on-the-fly. Apache Spark's streaming library is\nincreasingly becoming a popular choice as it can stream and analyze a\nsignificant amount of data. In this paper, we analyze large-scale geo-temporal\ndata collected from the USGODAE (United States Global Ocean Data Assimilation\nExperiment) data catalog, and showcase and assess the ability of Spark stream\nprocessing. We measure the latency of streaming and monitor scalability by\nadding and removing nodes in the middle of a streaming job. We also verify the\nfault tolerance by stopping nodes in the middle of a job and making sure that\nthe job is rescheduled and completed on other nodes. We design a full-stack\napplication that automates data collection, data processing and visualizing the\nresults. We also use Google Maps API to visualize results by color coding the\nworld map with values from various analytics.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 00:00:53 GMT"}, {"version": "v2", "created": "Thu, 1 Aug 2019 01:18:03 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Dahal", "Janak", ""], ["Ioup", "Elias", ""], ["Arifuzzaman", "Shaikh", ""], ["Abdelguerfi", "Mahdi", ""]]}, {"id": "1907.13498", "submitter": "Mahawaga Arachchige Pathum Chamikara", "authors": "M.A.P. Chamikara, P. Bertok, D. Liu, S. Camtepe, I. Khalil", "title": "An Efficient and Scalable Privacy Preserving Algorithm for Big Data and\n  Data Streams", "comments": null, "journal-ref": "Computers & Security, 101570 (2019)", "doi": "10.1016/j.cose.2019.101570", "report-no": null, "categories": "cs.CR cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A vast amount of valuable data is produced and is becoming available for\nanalysis as a result of advancements in smart cyber-physical systems. The data\ncomes from various sources, such as healthcare, smart homes, smart vehicles,\nand often includes private, potentially sensitive information that needs\nappropriate sanitization before being released for analysis. The incremental\nand fast nature of data generation in these systems necessitates scalable\nprivacy-preserving mechanisms with high privacy and utility. However, privacy\npreservation often comes at the expense of data utility. We propose a new data\nperturbation algorithm, SEAL (Secure and Efficient data perturbation Algorithm\nutilizing Local differential privacy), based on Chebyshev interpolation and\nLaplacian noise, which provides a good balance between privacy and utility with\nhigh efficiency and scalability. Empirical comparisons with existing\nprivacy-preserving algorithms show that SEAL excels in execution speed,\nscalability, accuracy, and attack resistance. SEAL provides flexibility in\nchoosing the best possible privacy parameters, such as the amount of added\nnoise, which can be tailored to the domain and dataset.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 13:41:31 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Chamikara", "M. A. P.", ""], ["Bertok", "P.", ""], ["Liu", "D.", ""], ["Camtepe", "S.", ""], ["Khalil", "I.", ""]]}]