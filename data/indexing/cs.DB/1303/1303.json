[{"id": "1303.0418", "submitter": "Dr Anwar Pasha Deshmukh", "authors": "Dr. Anwar Pasha Deshmukh and Dr. Riyazuddin Qureshi", "title": "Transparent Data Encryption -- Solution for Security of Database\n  Contents", "comments": "4 Pages 2 figures", "journal-ref": "International Journal of Advanced Computer Science and\n  Applications, Volume 2 No. 3, March 2011, pp 25-28. ISSN: 2156-5570(Online) &\n  ISSN: 2158-107X(Print)", "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present study deals with Transparent Data Encryption which is a\ntechnology used to solve the problems of security of data. Transparent Data\nEncryption means encrypting databases on hard disk and on any backup media.\nPresent day global business environment presents numerous security threats and\ncompliance challenges. To protect against data thefts and frauds we require\nsecurity solutions that are transparent by design.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2013 19:15:38 GMT"}], "update_date": "2013-03-05", "authors_parsed": [["Deshmukh", "Dr. Anwar Pasha", ""], ["Qureshi", "Dr. Riyazuddin", ""]]}, {"id": "1303.0447", "submitter": "Kanagavalli V R", "authors": "V. R. Kanagavalli and K. Raja", "title": "A Study on Application of Spatial Data Mining Techniques for Rural\n  Progress", "comments": "International Conference on Innovative Computing, information and\n  communication technology ICICT09; souvenir pp no 64", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the application of Spatial Data mining Techniques to\nefficiently manage the challenges faced by peripheral rural areas in analyzing\nand predicting market scenario and better manage their economy. Spatial data\nmining is the task of unfolding the implicit knowledge hidden in the spatial\ndatabases. The spatial Databases contain both spatial and non-spatial\nattributes of the areas under study. Finding implicit regularities, rules or\npatterns hidden in spatial databases is an important task, e.g. for\ngeo-marketing, traffic control or environmental studies. In this paper the\nfocus is on the effective use of Spatial Data Mining Techniques in the field of\nEconomic Geography constrained to the rural areas\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2013 01:45:37 GMT"}], "update_date": "2013-03-05", "authors_parsed": [["Kanagavalli", "V. R.", ""], ["Raja", "K.", ""]]}, {"id": "1303.0866", "submitter": "David LeJeune Jr.", "authors": "David W. LeJeune Jr", "title": "Adaptive Partitioning and its Applicability to a Highly Scalable and\n  Available Geo-Spatial Indexing Solution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Satellite Tracking of People (STOP) tracks thousands of GPS-enabled devices\n24 hours a day and 365 days a year. With locations captured for each device\nevery minute, STOP servers receive tens of millions of points each day. In\naddition to cataloging these points in real-time, STOP must also respond to\nquestions from customers such as, \"What devices of mine were at this location\ntwo months ago?\" They often then broaden their question to one such as, \"Which\nof my devices have ever been at this location?\" The processing requirements\nnecessary to answer these questions while continuing to process inbound data in\nreal-time is non-trivial.\n  To meet this demand, STOP developed Adaptive Partitioning to provide a\ncost-effective and highly available hardware platform for the geographical and\ntime-spatial indexing capabilities necessary for responding to customer data\nrequests while continuing to catalog inbound data in real-time.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2013 21:32:18 GMT"}], "update_date": "2013-03-06", "authors_parsed": [["LeJeune", "David W.", "Jr"]]}, {"id": "1303.1716", "submitter": "Kim Nguyen", "authors": "V\\'eronique Benzaken (LRI), Giuseppe Castagna (PPS), Kim Nguy\\~\\^en\n  (LRI), J\\'er\\^ome Sim\\'eon", "title": "Static and dynamic semantics of NoSQL languages", "comments": null, "journal-ref": "POPL, Rome : Italy (2013)", "doi": "10.1145/2429069.2429083", "report-no": null, "categories": "cs.PL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a calculus for processing semistructured data that spans\ndifferences of application area among several novel query languages, broadly\ncategorized as \"NoSQL\". This calculus lets users define their own operators,\ncapturing a wider range of data processing capabilities, whilst providing a\ntyping precision so far typical only of primitive hard-coded operators. The\ntype inference algorithm is based on semantic type checking, resulting in type\ninformation that is both precise, and flexible enough to handle structured and\nsemistructured data. We illustrate the use of this calculus by encoding a large\nfragment of Jaql, including operations and iterators over JSON, embedded SQL\nexpressions, and co-grouping, and show how the encoding directly yields a\ntyping discipline for Jaql as it is, namely without the addition of any type\ndefinition or type annotation in the code.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2013 15:19:21 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Benzaken", "V\u00e9ronique", "", "LRI"], ["Castagna", "Giuseppe", "", "PPS"], ["Nguy\\~\u00ean", "Kim", "", "LRI"], ["Sim\u00e9on", "J\u00e9r\u00f4me", ""]]}, {"id": "1303.1950", "submitter": "Alexandre Vaniachine", "authors": "A.V. Vaniachine (on behalf of the ATLAS and CMS Collaborations)", "title": "Advancements in Big Data Processing in the ATLAS and CMS Experiments", "comments": "7 pages, 7 figures", "journal-ref": "In: Proc. of the Fifth International Conference \"Distributed\n  computing and Grid-technologies in Science and Education\" (Dubna, July 16-21,\n  2012), Dubna, JINR, 2012, p. 224", "doi": null, "report-no": "ANL-HEP-CP-12-77; ATL-SOFT-PROC-2012-068", "categories": "cs.DC cs.DB hep-ex", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  The ever-increasing volumes of scientific data present new challenges for\ndistributed computing and Grid technologies. The emerging Big Data revolution\ndrives exploration in scientific fields including nanotechnology, astrophysics,\nhigh-energy physics, biology and medicine. New initiatives are transforming\ndata-driven scientific fields enabling massive data analysis in new ways. In\npetascale data processing scientists deal with datasets, not individual files.\nAs a result, a task (comprised of many jobs) became a unit of petascale data\nprocessing on the Grid. Splitting of a large data processing task into jobs\nenabled fine-granularity checkpointing analogous to the splitting of a large\nfile into smaller TCP/IP packets during data transfers. Transferring large data\nin small packets achieves reliability through automatic re-sending of the\ndropped TCP/IP packets. Similarly, transient job failures on the Grid can be\nrecovered by automatic re-tries to achieve reliable six sigma production\nquality in petascale data processing on the Grid. The computing experience of\nthe ATLAS and CMS experiments provides foundation for reliability engineering\nscaling up Grid technologies for data processing beyond the petascale.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2013 11:06:44 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Vaniachine", "A. V.", "", "on behalf of the ATLAS and CMS Collaborations"]]}, {"id": "1303.1951", "submitter": "Mohammed Otair Otair", "authors": "Dr. Mohammed Otair", "title": "Approximate k-nearest neighbour based spatial clustering using k-d tree", "comments": null, "journal-ref": null, "doi": null, "report-no": "Vol.5, No.1, February 2013", "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Different spatial objects that vary in their characteristics, such as\nmolecular biology and geography, are presented in spatial areas. Methods to\norganize, manage, and maintain those objects in a structured manner are\nrequired. Data mining raised different techniques to overcome these\nrequirements. There are many major tasks of data mining, but the mostly used\ntask is clustering. Data set within the same cluster share common features that\ngive each cluster its characteristics. In this paper, an implementation of\nApproximate kNN-based spatial clustering algorithm using the K-d tree is\nproposed. The major contribution achieved by this research is the use of the\nk-d tree data structure for spatial clustering, and comparing its performance\nto the brute-force approach. The results of the work performed in this paper\nrevealed better performance using the k-d tree, compared to the traditional\nbrute-force approach.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2013 11:13:02 GMT"}], "update_date": "2013-03-11", "authors_parsed": [["Otair", "Dr. Mohammed", ""]]}, {"id": "1303.2310", "submitter": "Vaida Ceikute", "authors": "Xiaohui Li, Vaida Ceikute, Christian S. Jensen and Kian-Lee Tan", "title": "Trajectory Based Optimal Segment Computation in Road Network Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding a location for a new facility such that the facility attracts the\nmaximal number of customers is a challenging problem. Existing studies either\nmodel customers as static sites and thus do not consider customer movement, or\nthey focus on theoretical aspects and do not provide solutions that are shown\nempirically to be scalable. Given a road network, a set of existing facilities,\nand a collection of customer route traversals, an optimal segment query returns\nthe optimal road network segment(s) for a new facility. We propose a practical\nframework for computing this query, where each route traversal is assigned a\nscore that is distributed among the road segments covered by the route\naccording to a score distribution model. The query returns the road segment(s)\nwith the highest score. To achieve low latency, it is essential to prune the\nvery large search space. We propose two algorithms that adopt different\napproaches to computing the query. Algorithm AUG uses graph augmentation, and\nITE uses iterative road-network partitioning. Empirical studies with real data\nsets demonstrate that the algorithms are capable of offering high performance\nin realistic settings.\n", "versions": [{"version": "v1", "created": "Sun, 10 Mar 2013 11:49:41 GMT"}], "update_date": "2013-03-12", "authors_parsed": [["Li", "Xiaohui", ""], ["Ceikute", "Vaida", ""], ["Jensen", "Christian S.", ""], ["Tan", "Kian-Lee", ""]]}, {"id": "1303.2595", "submitter": "Norbert Paul", "authors": "Norbert Paul and Patrick Erik Bradley and Martin Breunig", "title": "Integrating Space, Time, Version and Scale Using Alexandrov Topologies", "comments": "International Symposium on Spatial and Temporal Databases SSTD 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces a novel approach to spatial database design. Instead\nof extending the canonical Solid-Face-Edge-Vertex schema by, say, \"hypersolids\"\nthese classes are generalised to a common type SpatialEntity, and the\nindividual BoundedBy relations between two consecutive classes are generalised\nto one BoundedBy relation on SpatialEntity instances. Then the pair\n(SpatialEntity, BoundedBy) is a so-called incidence graph.\n  The novelty about this approach uses the observation that an incidence graph\nrepresents a topological space of SpatialEntity instances because the\nBoundedBy-relation defines a so-called Alexandrov topology for them turning\nthem into a topological space. So spatial data becomes part of mathematical\ntopology and topology can be immediately applied to spatial data. For example,\ncontinuous functions between two instances of spatial data allow the consistent\nmodelling of generalisation. Further, it is also possible to establish a formal\ntopological definition of spatial data dimension, and every topological data\nmodel of arbitrary dimension gets a simple uniform data model. This model\ncovers space-time, and the version history of a spatial model can be\nrepresented by an Alexandrov topology, too. By integrating space, time,\nversion, and scale into one single schema, topological queries across those\naspects are enabled through topological constructions. In fact, the topological\nconstructions cover a relationally complete query language for spaces and can\nbe redefined to operate accordingly on their graph representations.\n  With these observations a relational database schema for a spatial data model\nof dimension 6 and more is developed. The schema seamlessly integrates 4D\nspace-time, levels of detail and version history, and it can be easily expanded\nto also contain non-spatial information or be linked to other data sources.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2013 17:46:13 GMT"}], "update_date": "2013-03-12", "authors_parsed": [["Paul", "Norbert", ""], ["Bradley", "Patrick Erik", ""], ["Breunig", "Martin", ""]]}, {"id": "1303.3047", "submitter": "Miroslav Stampar", "authors": "Miroslav Stampar", "title": "Data Retrieval over DNS in SQL Injection Attacks", "comments": "7 pages, 3 figures, 1 table. Presented at PHDays 2012 security\n  conference, Moscow, Russia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes an advanced SQL injection technique where DNS resolution\nprocess is exploited for retrieval of malicious SQL query results. Resulting\nDNS requests are intercepted by attackers themselves at the controlled remote\nname server extracting valuable data. Open source SQL injection tool sqlmap has\nbeen adjusted to automate this task. With modifications done, attackers are\nable to use this technique for fast and low profile data retrieval, especially\nin cases where other standard ones fail.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2013 22:09:05 GMT"}], "update_date": "2013-03-14", "authors_parsed": [["Stampar", "Miroslav", ""]]}, {"id": "1303.3233", "submitter": "Francesco Parisi", "authors": "Sergio Flesca, Filippo Furfaro, Francesco Parisi", "title": "Consistency Checking and Querying in Probabilistic Databases under\n  Integrity Constraints", "comments": "Probabilistic databases, Integrity constraints, Consistency checking", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the issue of incorporating a particular yet expressive form of\nintegrity constraints (namely, denial constraints) into probabilistic\ndatabases. To this aim, we move away from the common way of giving semantics to\nprobabilistic databases, which relies on considering a unique interpretation of\nthe data, and address two fundamental problems: consistency checking and query\nevaluation. The former consists in verifying whether there is an interpretation\nwhich conforms to both the marginal probabilities of the tuples and the\nintegrity constraints. The latter is the problem of answering queries under a\n\"cautious\" paradigm, taking into account all interpretations of the data in\naccordance with the constraints. In this setting, we investigate the complexity\nof the above-mentioned problems, and identify several tractable cases of\npractical relevance.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 17:53:15 GMT"}], "update_date": "2013-03-14", "authors_parsed": [["Flesca", "Sergio", ""], ["Furfaro", "Filippo", ""], ["Parisi", "Francesco", ""]]}, {"id": "1303.3517", "submitter": "Yingyi Bu Yingyi Bu", "authors": "Joshua Rosen, Neoklis Polyzotis, Vinayak Borkar, Yingyi Bu, Michael J.\n  Carey, Markus Weimer, Tyson Condie, Raghu Ramakrishnan", "title": "Iterative MapReduce for Large Scale Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large datasets (\"Big Data\") are becoming ubiquitous because the potential\nvalue in deriving insights from data, across a wide range of business and\nscientific applications, is increasingly recognized. In particular, machine\nlearning - one of the foundational disciplines for data analysis, summarization\nand inference - on Big Data has become routine at most organizations that\noperate large clouds, usually based on systems such as Hadoop that support the\nMapReduce programming paradigm. It is now widely recognized that while\nMapReduce is highly scalable, it suffers from a critical weakness for machine\nlearning: it does not support iteration. Consequently, one has to program\naround this limitation, leading to fragile, inefficient code. Further, reliance\non the programmer is inherently flawed in a multi-tenanted cloud environment,\nsince the programmer does not have visibility into the state of the system when\nhis or her program executes. Prior work has sought to address this problem by\neither developing specialized systems aimed at stylized applications, or by\naugmenting MapReduce with ad hoc support for saving state across iterations\n(driven by an external loop). In this paper, we advocate support for looping as\na first-class construct, and propose an extension of the MapReduce programming\nparadigm called {\\em Iterative MapReduce}. We then develop an optimizer for a\nclass of Iterative MapReduce programs that cover most machine learning\ntechniques, provide theoretical justifications for the key optimization steps,\nand empirically demonstrate that system-optimized programs for significant\nmachine learning tasks are competitive with state-of-the-art specialized\nsolutions.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 04:24:12 GMT"}], "update_date": "2013-03-15", "authors_parsed": [["Rosen", "Joshua", ""], ["Polyzotis", "Neoklis", ""], ["Borkar", "Vinayak", ""], ["Bu", "Yingyi", ""], ["Carey", "Michael J.", ""], ["Weimer", "Markus", ""], ["Condie", "Tyson", ""], ["Ramakrishnan", "Raghu", ""]]}, {"id": "1303.4277", "submitter": "Radu Ciucanu", "authors": "Iovka Boneva, Radu Ciucanu, Slawek Staworko", "title": "Simple Schemas for Unordered XML", "comments": "16th International Workshop on the Web and Databases (WebDB 2013)\n  http://webdb2013.lille.inria.fr/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider unordered XML, where the relative order among siblings is\nignored, and propose two simple yet practical schema formalisms: disjunctive\nmultiplicity schemas (DMS), and its restriction, disjunction-free multiplicity\nschemas (MS). We investigate their computational properties and characterize\nthe complexity of the following static analysis problems: schema\nsatisfiability, membership of a tree to the language of a schema, schema\ncontainment, twig query satisfiability, implication, and containment in the\npresence of schema. Our research indicates that the proposed formalisms retain\nmuch of the expressiveness of DTDs without an increase in computational\ncomplexity.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2013 15:03:43 GMT"}, {"version": "v2", "created": "Fri, 22 Mar 2013 18:55:16 GMT"}, {"version": "v3", "created": "Fri, 7 Jun 2013 11:35:08 GMT"}, {"version": "v4", "created": "Thu, 20 Jun 2013 08:23:49 GMT"}], "update_date": "2013-06-21", "authors_parsed": [["Boneva", "Iovka", ""], ["Ciucanu", "Radu", ""], ["Staworko", "Slawek", ""]]}, {"id": "1303.4471", "submitter": "Oliver Kennedy", "authors": "Oliver Kennedy, Lukasz Ziarek", "title": "BarQL: Collaborating Through Change", "comments": "BarQL reference document", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications such as Google Docs, Office 365, and Dropbox show a growing\ntrend towards incorporating multi-user live collaboration functionality into\nweb applications. These collaborative applications share a need to efficiently\nexpress shared state, and a common strategy for doing so is a shared log\nabstraction. Extensive research efforts on log abstractions by the database,\nprogramming languages, and distributed systems communities have identified a\nvariety of optimization techniques based on the algebraic properties of updates\n(i.e., pairwise commutativity, subsumption, and idempotence). Although these\ntechniques have been applied to specific applications and use-cases, to the\nbest of our knowledge, no attempt has been made to create a general framework\nfor such optimizations in the context of a non-trivial update language. In this\npaper, we introduce mutation languages, a low-level framework for reasoning\nabout the algebraic properties of state updates, or mutations. We define BarQL,\na general purpose state-update language, and show how mutation languages allow\nus to reason about the algebraic properties of updates expressed in BarQ L .\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2013 02:15:24 GMT"}], "update_date": "2013-03-20", "authors_parsed": [["Kennedy", "Oliver", ""], ["Ziarek", "Lukasz", ""]]}, {"id": "1303.4869", "submitter": "Raik Niemann M.Sc.", "authors": "Raik Niemann, Nikolaos Korfiatis, Roberto Zicari, Richard G\\\"obel", "title": "Does query performance optimization lead to energy efficiency? A\n  comparative analysis of energy efficiency of database operations under\n  different workload scenarios", "comments": "10 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the continuous increase of online services as well as energy costs,\nenergy consumption becomes a significant cost factor for the evaluation of data\ncenter operations. A significant contributor to that is the performance of\ndatabase servers which are found to constitute the backbone of online services.\nFrom a software approach, while a set of novel data management technologies\nappear in the market e.g. key-value based or in-memory databases, classic\nrelational database management systems (RDBMS) are still widely used. In\naddition from a hardware perspective, the majority of database servers is still\nusing standard magnetic hard drives (HDDs) instead of solid state drives (SSDs)\ndue to lower cost of storage per gigabyte, disregarding the performance boost\nthat might be given due to high cost.\n  In this study we focus on a software based assessment of the energy\nconsumption of a database server by running three different and complete\ndatabase workloads namely TCP-H, Star Schema Benchmark -SSB as well a modified\nbenchmark we have derived for this study called W22. We profile the energy\ndistribution among the ost important server components and by using different\nresource allocation we assess the energy consumption of a typical open source\nRDBMS (PostgreSQL) on a standard server in relation with its performance\n(measured by query time).\n  Results confirm the well-known fact that even for complete workloads,\noptimization of the RDBMS results to lower energy consumption.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 08:22:11 GMT"}], "update_date": "2013-03-21", "authors_parsed": [["Niemann", "Raik", ""], ["Korfiatis", "Nikolaos", ""], ["Zicari", "Roberto", ""], ["G\u00f6bel", "Richard", ""]]}, {"id": "1303.5175", "submitter": "Dmitry Namiot", "authors": "Dmitry Namiot", "title": "Discovery of Convoys in Network Proximity Log", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes an algorithm for discovery of convoys in database with\nproximity log. Traditionally, discovery of convoys covers trajectories\ndatabases. This paper presents a model for context-aware browsing application\nbased on the network proximity. Our model uses mobile phone as proximity sensor\nand proximity data replaces location information. As per our concept, any\nexisting or even especially created wireless network node could be used as\npresence sensor that can discover access to some dynamic or user-generated\ncontent. Content revelation in this model depends on rules based on the\nproximity. Discovery of convoys in historical user's logs provides a new class\nof rules for delivering local content to mobile subscribers.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2013 06:14:05 GMT"}], "update_date": "2013-03-22", "authors_parsed": [["Namiot", "Dmitry", ""]]}, {"id": "1303.5313", "submitter": "Todd Veldhuizen", "authors": "Todd L. Veldhuizen", "title": "Incremental Maintenance for Leapfrog Triejoin", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an incremental maintenance algorithm for leapfrog triejoin. The\nalgorithm maintains rules in time proportional (modulo log factors) to the edit\ndistance between leapfrog triejoin traces.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2013 15:56:54 GMT"}], "update_date": "2013-03-22", "authors_parsed": [["Veldhuizen", "Todd L.", ""]]}, {"id": "1303.5420", "submitter": "Raymond T. Ng", "authors": "Raymond T. Ng, V. S. Subrahmanian", "title": "Empirical Probabilities in Monadic Deductive Databases", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-215-222", "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of supporting empirical probabilities in monadic logic\ndatabases. Though the semantics of multivalued logic programs has been studied\nextensively, the treatment of probabilities as results of statistical findings\nhas not been studied in logic programming/deductive databases. We develop a\nmodel-theoretic characterization of logic databases that facilitates such a\ntreatment. We present an algorithm for checking consistency of such databases\nand prove its total correctness. We develop a sound and complete query\nprocessing procedure for handling queries to such databases.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:54:16 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Ng", "Raymond T.", ""], ["Subrahmanian", "V. S.", ""]]}, {"id": "1303.5867", "submitter": "Srikantaiah K C", "authors": "Srikantaiah K C, Suraj M, Venugopal K R, L M Patnaik", "title": "Similarity based Dynamic Web Data Extraction and Integration System from\n  Search Engine Result Pages for Web Content Mining", "comments": "8 pages", "journal-ref": "ACEEE International Journal on Information Technology, Volume 3,\n  Issue 1, 2013", "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an explosive growth of information in the World Wide Web thus posing\na challenge to Web users to extract essential knowledge from the Web. Search\nengines help us to narrow down the search in the form of Search Engine Result\nPages (SERP). Web Content Mining is one of the techniques that help users to\nextract useful information from these SERPs. In this paper, we propose two\nsimilarity based mechanisms; WDES, to extract desired SERPs and store them in\nthe local depository for offline browsing and WDICS, to integrate the requested\ncontents and enable the user to perform the intended analysis and extract the\ndesired information. Our experimental results show that WDES and WDICS\noutperform DEPTA [1] in terms of Precision and Recall.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2013 17:40:32 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["C", "Srikantaiah K", ""], ["M", "Suraj", ""], ["R", "Venugopal K", ""], ["Patnaik", "L M", ""]]}, {"id": "1303.6609", "submitter": "Jagan Sankaranarayanan", "authors": "Jeff LeFevre, Jagan Sankaranarayanan, Hakan Hacigumus, Junichi\n  Tatemura, Neoklis Polyzotis, Michael J. Carey", "title": "Exploiting Opportunistic Physical Design in Large-scale Data Analytics", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Large-scale systems, such as MapReduce and Hadoop, perform aggressive\nmaterialization of intermediate job results in order to support fault\ntolerance. When jobs correspond to exploratory queries submitted by data\nanalysts, these materializations yield a large set of materialized views that\ntypically capture common computation among successive queries from the same\nanalyst, or even across queries of different analysts who test similar\nhypotheses. We propose to treat these views as an opportunistic physical design\nand use them for the purpose of query optimization. We develop a novel\nquery-rewrite algorithm that addresses the two main challenges in this context:\nhow to search the large space of rewrites, and how to reason about views that\ncontain UDFs (a common feature in large-scale data analytics). The algorithm,\nwhich provably finds the minimum-cost rewrite, is inspired by nearest-neighbor\nsearches in non-metric spaces. We present an extensive experimental study on\nreal-world datasets with a prototype data-analytics system based on Hive. The\nresults demonstrate that our approach can result in dramatic performance\nimprovements on complex data-analysis queries, reducing total execution time by\nan average of 61% and up to two orders of magnitude.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2013 19:08:55 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2013 17:35:09 GMT"}], "update_date": "2013-12-11", "authors_parsed": [["LeFevre", "Jeff", ""], ["Sankaranarayanan", "Jagan", ""], ["Hacigumus", "Hakan", ""], ["Tatemura", "Junichi", ""], ["Polyzotis", "Neoklis", ""], ["Carey", "Michael J.", ""]]}, {"id": "1303.6682", "submitter": "Gosta Grahne", "authors": "Gosta Grahne, Adrian Onet", "title": "Anatomy of the chase", "comments": "18 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  A lot of research activity has recently taken place around the chase\nprocedure, due to its usefulness in data integration, data exchange, query\noptimization, peer data exchange and data correspondence, to mention a few. As\nthe chase has been investigated and further developed by a number of research\ngroups and authors, many variants of the chase have emerged and associated\nresults obtained. Due to the heterogeneous nature of the area it is frequently\ndifficult to verify the scope of each result. In this paper we take closer look\nat recent developments, and provide additional results. Our analysis allows us\ncreate a taxonomy of the chase variations and the properties they satisfy.\n  Two of the most central problems regarding the chase is termination, and\ndiscovery of restricted classes of sets of dependencies that guarantee\ntermination of the chase. The search for the restricted classes has been\nmotivated by a fairly recent result that shows that it is undecidable to\ndetermine whether the chase with a given dependency set will terminate on a\ngiven instance. There is a small dissonance here, since the quest has been for\nclasses of sets of dependencies guaranteeing termination of the chase on all\ninstances, even though the latter problem was not known to be undecidable. We\nresolve the dissonance in this paper by showing that determining whether the\nchase with a given set of dependencies terminates on all instances is\ncoRE-complete. Our reduction also gives us the aforementioned\ninstance-dependent RE-completeness result as a byproduct. For one of the\nrestricted classes, the stratified sets dependencies, we provide new complexity\nresults for the problem of testing whether a given set of dependencies belongs\nto it. These results rectify some previous claims that have occurred in the\nliterature.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2013 22:10:15 GMT"}], "update_date": "2013-03-28", "authors_parsed": [["Grahne", "Gosta", ""], ["Onet", "Adrian", ""]]}, {"id": "1303.7430", "submitter": "Giorgio Stefanoni", "authors": "Giorgio Stefanoni, Boris Motik, Ian Horrocks", "title": "Introducing Nominals to the Combined Query Answering Approaches for EL", "comments": "Extended version of a paper to appear on AAAI-13", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  So-called combined approaches answer a conjunctive query over a description\nlogic ontology in three steps: first, they materialise certain consequences of\nthe ontology and the data; second, they evaluate the query over the data; and\nthird, they filter the result of the second phase to eliminate unsound answers.\nSuch approaches were developed for various members of the DL-Lite and the EL\nfamilies of languages, but none of them can handle ontologies containing\nnominals. In our work, we bridge this gap and present a combined query\nanswering approach for ELHO---a logic that contains all features of the OWL 2\nEL standard apart from transitive roles and complex role inclusions. This\nextension is nontrivial because nominals require equality reasoning, which\nintroduces complexity into the first and the third step. Our empirical\nevaluation suggests that our technique is suitable for practical application,\nand so it provides a practical basis for conjunctive query answering in a large\nfragment of OWL 2 EL.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2013 16:07:12 GMT"}, {"version": "v2", "created": "Mon, 1 Apr 2013 17:08:32 GMT"}], "update_date": "2013-04-02", "authors_parsed": [["Stefanoni", "Giorgio", ""], ["Motik", "Boris", ""], ["Horrocks", "Ian", ""]]}]