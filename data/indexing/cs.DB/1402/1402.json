[{"id": "1402.0282", "submitter": "Benjamin Rubinstein", "authors": "Duo Zhang and Benjamin I. P. Rubinstein and Jim Gemmell", "title": "Principled Graph Matching Algorithms for Integrating Multiple Data\n  Sources", "comments": "14 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores combinatorial optimization for problems of max-weight\ngraph matching on multi-partite graphs, which arise in integrating multiple\ndata sources. Entity resolution-the data integration problem of performing\nnoisy joins on structured data-typically proceeds by first hashing each record\ninto zero or more blocks, scoring pairs of records that are co-blocked for\nsimilarity, and then matching pairs of sufficient similarity. In the most\ncommon case of matching two sources, it is often desirable for the final\nmatching to be one-to-one (a record may be matched with at most one other);\nmembers of the database and statistical record linkage communities accomplish\nsuch matchings in the final stage by weighted bipartite graph matching on\nsimilarity scores. Such matchings are intuitively appealing: they leverage a\nnatural global property of many real-world entity stores-that of being nearly\ndeduped-and are known to provide significant improvements to precision and\nrecall. Unfortunately unlike the bipartite case, exact max-weight matching on\nmulti-partite graphs is known to be NP-hard. Our two-fold algorithmic\ncontributions approximate multi-partite max-weight matching: our first\nalgorithm borrows optimization techniques common to Bayesian probabilistic\ninference; our second is a greedy approximation algorithm. In addition to a\ntheoretical guarantee on the latter, we present comparisons on a real-world ER\nproblem from Bing significantly larger than typically found in the literature,\npublication data, and on a series of synthetic problems. Our results quantify\nsignificant improvements due to exploiting multiple sources, which are made\npossible by global one-to-one constraints linking otherwise independent\nmatching sub-problems. We also discover that our algorithms are complementary:\none being much more robust under noise, and the other being simple to implement\nand very fast to run.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2014 04:56:58 GMT"}], "update_date": "2014-02-04", "authors_parsed": [["Zhang", "Duo", ""], ["Rubinstein", "Benjamin I. P.", ""], ["Gemmell", "Jim", ""]]}, {"id": "1402.0576", "submitter": "Ilianna  Kollia", "authors": "Ilianna Kollia, Birte Glimm", "title": "Optimizing SPARQL Query Answering over OWL Ontologies", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 48, pages\n  253-303, 2013", "doi": "10.1613/jair.3872", "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The SPARQL query language is currently being extended by the World Wide Web\nConsortium (W3C) with so-called entailment regimes. An entailment regime\ndefines how queries are evaluated under more expressive semantics than SPARQLs\nstandard simple entailment, which is based on subgraph matching. The queries\nare very expressive since variables can occur within complex concepts and can\nalso bind to concept or role names. In this paper, we describe a sound and\ncomplete algorithm for the OWL Direct Semantics entailment regime. We further\npropose several novel optimizations such as strategies for determining a good\nquery execution order, query rewriting techniques, and show how specialized OWL\nreasoning tasks and the concept and role hierarchy can be used to reduce the\nquery execution time. For determining a good execution order, we propose a\ncost-based model, where the costs are based on information about the instances\nof concepts and roles that are extracted from a model abstraction built by an\nOWL reasoner. We present two ordering strategies: a static and a dynamic one.\nFor the dynamic case, we improve the performance by exploiting an individual\nclustering approach that allows for computing the cost functions based on one\nindividual sample from a cluster. We provide a prototypical implementation and\nevaluate the efficiency of the proposed optimizations. Our experimental study\nshows that the static ordering usually outperforms the dynamic one when\naccurate statistics are available. This changes, however, when the statistics\nare less accurate, e.g., due to nondeterministic reasoning decisions. For\nqueries that go beyond conjunctive instance queries we observe an improvement\nof up to three orders of magnitude due to the proposed optimizations.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 01:39:51 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Kollia", "Ilianna", ""], ["Glimm", "Birte", ""]]}, {"id": "1402.1257", "submitter": "Nishant Vadnere", "authors": "Nishant Vadnere, R.G.Mehta, D.P.Rana, N.J.Mistry and M.M.Raghuwanshi", "title": "Incremental classification using Feature Tree", "comments": "5 Pages, 1 figure, International conference on recent trends and\n  innovations in engineering and technology - 2013 (ICRTIET-2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, stream data have become an immensely growing area of\nresearch for the database, computer science and data mining communities. Stream\ndata is an ordered sequence of instances. In many applications of data stream\nmining data can be read only once or a small number of times using limited\ncomputing and storage capabilities. Some of the issues occurred in classifying\nstream data that have significant impact in algorithm development are size of\ndatabase, online streaming, high dimensionality and concept drift. The concept\ndrift occurs when the properties of the historical data and target variable\nchange over time abruptly in such a case that the predictions will become\ninaccurate as time passes. In this paper the framework of incremental\nclassification is proposed to solve the issues for the classification of stream\ndata. The Trie structure based incremental feature tree, Trie structure based\nincremental FP (Frequent Pattern) growth tree and tree based incremental\nclassification algorithm are introduced in the proposed framework.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2014 06:17:18 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2014 17:34:51 GMT"}], "update_date": "2014-02-10", "authors_parsed": [["Vadnere", "Nishant", ""], ["Mehta", "R. G.", ""], ["Rana", "D. P.", ""], ["Mistry", "N. J.", ""], ["Raghuwanshi", "M. M.", ""]]}, {"id": "1402.1258", "submitter": "Vishal Verma", "authors": "Mohit Kumar Gupta, Vishal Verma, Megha Singh Verma", "title": "In-Memory Database Systems - A Paradigm Shift", "comments": "Pages-4, Figures-1, Published with International Journal of\n  Engineering Trends and Technology (IJETT)", "journal-ref": "International Journal of Engineering Trends and Technology (IJETT)\n  6(6):333-336, Dec. 2013. Published by Seventh Sense Research Group", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In today world, organizations like Google, Yahoo, Amazon, Facebook etc. are\nfacing drastic increase in data. This leads to the problem of capturing,\nstoring, managing and analyzing terabytes or petabytes of data, stored in\nmultiple formats, from different internal and external sources. Moreover, new\napplications scenarios like weather forecasting, trading, artificial\nintelligence etc. need huge data processing in real time. These requirements\nexceed the processing capacity of traditional on-disk database management\nsystems to manage this data and to give speedy real time results. Therefore,\ndata management needs new solutions for coping with the challenges of data\nvolumes and processing data in real-time. An in-memory database system (IMDS)\nis a latest breed of database management system which is becoming answer to\nabove challenges with other supporting technologies. IMDS is capable to process\nmassive data distinctly faster. This paper explores IMDS approach and its\nassociated design issues and challenges. It also investigates some famous\ncommercial and open-source IMDS solutions available in the market.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2014 06:17:51 GMT"}], "update_date": "2014-02-07", "authors_parsed": [["Gupta", "Mohit Kumar", ""], ["Verma", "Vishal", ""], ["Verma", "Megha Singh", ""]]}, {"id": "1402.1327", "submitter": "Rushirajsinh Zala", "authors": "Mr.Rushirajsinh L. Zala, Mr.Brijesh B. Mehta, Mr.Mahipalsinh R. Zala", "title": "A Survey on Spatial Co-location Patterns Discovery from Spatial Datasets", "comments": "6 pages,8 figures", "journal-ref": "IJCTT 7(3):137-142, January 2014", "doi": "10.14445/22312803/IJCTT-V7P140", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Spatial data mining or Knowledge discovery in spatial database is the\nextraction of implicit knowledge, spatial relations and spatial patterns that\nare not explicitly stored in databases. Co-location patterns discovery is the\nprocess of finding the subsets of features that are frequently located together\nin the same geographic area. In this paper, we discuss the different approaches\nlike Rule based approach, Join-less approach, Partial Join approach and\nConstraint neighborhood based approach for finding co-location patterns.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2014 11:37:45 GMT"}], "update_date": "2014-02-07", "authors_parsed": [["Zala", "Mr. Rushirajsinh L.", ""], ["Mehta", "Mr. Brijesh B.", ""], ["Zala", "Mr. Mahipalsinh R.", ""]]}, {"id": "1402.1469", "submitter": "Evgeny Nikulchev", "authors": "Evgeniy Pluzhnik, Evgeny Nikulchev", "title": "Use of Dynamical Systems Modeling to Hybrid Cloud Database", "comments": "8 pages", "journal-ref": "International Journal of Communications, Network and System\n  Sciences.2013.V.6. N.12. PP. 505-512", "doi": "10.4236/ijcns.2013.612054", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the article, an experiment is aimed at clarifying the transfer efficiency\nof the database in the cloud infrastructure. The system was added to the\ncontrol unit, which has guided the database search in the local part or in the\ncloud. It is shown that the time data acquisition remains unchanged as a result\nof modification. Suggestions have been made about the use of the theory of\ndynamic systems to hybrid cloud database. The present work is aimed at\nattracting the attention of spe-cialists in the field of cloud database to the\napparatus control theory. The experiment presented in this article allows the\nuse of the description of the known methods for solving important practical\nproblems.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2014 19:59:14 GMT"}], "update_date": "2014-02-07", "authors_parsed": [["Pluzhnik", "Evgeniy", ""], ["Nikulchev", "Evgeny", ""]]}, {"id": "1402.1526", "submitter": "Justin Hsu", "authors": "Marco Gaboardi, Emilio Jes\\'us Gallego Arias, Justin Hsu, Aaron Roth,\n  Zhiwei Steven Wu", "title": "Dual Query: Practical Private Query Release for High Dimensional Data", "comments": null, "journal-ref": "Journal of Privacy and Confidentiality 7(2) 53--77 (2017)", "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a practical, differentially private algorithm for answering a\nlarge number of queries on high dimensional datasets. Like all algorithms for\nthis task, ours necessarily has worst-case complexity exponential in the\ndimension of the data. However, our algorithm packages the computationally hard\nstep into a concisely defined integer program, which can be solved\nnon-privately using standard solvers. We prove accuracy and privacy theorems\nfor our algorithm, and then demonstrate experimentally that our algorithm\nperforms well in practice. For example, our algorithm can efficiently and\naccurately answer millions of queries on the Netflix dataset, which has over\n17,000 attributes; this is an improvement on the state of the art by multiple\norders of magnitude.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2014 23:20:43 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2015 04:36:00 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Gaboardi", "Marco", ""], ["Arias", "Emilio Jes\u00fas Gallego", ""], ["Hsu", "Justin", ""], ["Roth", "Aaron", ""], ["Wu", "Zhiwei Steven", ""]]}, {"id": "1402.1546", "submitter": "Weiwei Sun", "authors": "Renchu Song, Weiwei Sun, Baihua Zheng, Yu Zheng", "title": "PRESS: A Novel Framework of Trajectory Compression in Road Networks", "comments": "27 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Location data becomes more and more important. In this paper, we focus on the\ntrajectory data, and propose a new framework, namely PRESS (Paralleled\nRoad-Network-Based Trajectory Compression), to effectively compress trajectory\ndata under road network constraints. Different from existing work, PRESS\nproposes a novel representation for trajectories to separate the spatial\nrepresentation of a trajectory from the temporal representation, and proposes a\nHybrid Spatial Compression (HSC) algorithm and error Bounded Temporal\nCompression (BTC) algorithm to compress the spatial and temporal information of\ntrajectories respectively. PRESS also supports common spatial-temporal queries\nwithout fully decompressing the data. Through an extensive experimental study\non real trajectory dataset, PRESS significantly outperforms existing approaches\nin terms of saving storage cost of trajectory data with bounded errors.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2014 03:29:08 GMT"}], "update_date": "2014-02-10", "authors_parsed": [["Song", "Renchu", ""], ["Sun", "Weiwei", ""], ["Zheng", "Baihua", ""], ["Zheng", "Yu", ""]]}, {"id": "1402.1814", "submitter": "Paresh Tanna", "authors": "Prof. Paresh Tanna, Dr. Yogesh Ghodasara", "title": "Foundation for Frequent Pattern Mining Algorithms Implementation", "comments": "5 pages, Published with International Journal of Computer Trends and\n  Technology (IJCTT)", "journal-ref": "International Journal of Computer Trends and Technology (IJCTT)\n  4(7):2159-2163, July 2013", "doi": "10.14445/2231-2803", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As with the development of the IT technologies, the amount of accumulated\ndata is also increasing. Thus the role of data mining comes into picture.\nAssociation rule mining becomes one of the significant responsibilities of\ndescriptive technique which can be defined as discovering meaningful patterns\nfrom large collection of data. The frequent pattern mining algorithms determine\nthe frequent patterns from a database. Mining frequent itemset is very\nfundamental part of association rule mining. Many algorithms have been proposed\nfrom last many decades including majors are Apriori, Direct Hashing and\nPruning, FP-Growth, ECLAT etc. The aim of this study is to analyze the existing\ntechniques for mining frequent patterns and evaluate the performance of them by\ncomparing Apriori and DHP algorithms in terms of candidate generation, database\nand transaction pruning. This creates a foundation to develop newer algorithm\nfor frequent pattern mining.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2014 03:25:57 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Tanna", "Prof. Paresh", ""], ["Ghodasara", "Dr. Yogesh", ""]]}, {"id": "1402.1992", "submitter": "Mingmin  Chen", "authors": "Mingmin Chen, Shizhuo Yu, Nico Franz, Shawn Bowers, Bertram Lu\\''asher", "title": "Euler/X: A Toolkit for Logic-based Taxonomy Integration", "comments": "8 pages, 14 figures, WFLP 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Euler/X, a toolkit for logic-based taxonomy integration. Given\ntwo taxonomies and a set of alignment constraints between them, Euler/X\nprovides tools for detecting, explaining, and reconciling inconsistencies;\nfinding all possible merges between (consistent) taxonomies; and visualizing\nmerge results. Euler/X employs a number of different underlying reasoning\nsystems, including first-order reasoners (Prover9 and Mace4), answer set\nprogramming (DLV and Potassco), and RCC reasoners (PyRCC8). We demonstrate the\nfeatures of Euler/X and provide experimental results showing its feasibility on\nvarious synthetic and real-world examples.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2014 21:13:30 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Chen", "Mingmin", ""], ["Yu", "Shizhuo", ""], ["Franz", "Nico", ""], ["Bowers", "Shawn", ""], ["Lu\\''asher", "Bertram", ""]]}, {"id": "1402.2071", "submitter": "Radim Belohlavek", "authors": "Radim Belohlavek, Vilem Vychodil", "title": "Attribute Dependencies for Data with Grades", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines attribute dependencies in data that involve grades, such\nas a grade to which an object is red or a grade to which two objects are\nsimilar. We thus extend the classical agenda by allowing graded, or fuzzy,\nattributes instead of Boolean attributes in case of attribute implications, and\nallowing approximate match based on degrees of similarity instead of exact\nmatch in case of functional dependencies. In a sense, we move from bivalence,\ninherently present in the now-available theories of dependencies, to a more\nflexible setting that involves grades. Such a shift has far-reaching\nconsequences. We argue that a reasonable theory of dependencies may be\ndeveloped by making use of mathematical fuzzy logic. Namely, the theory of\ndependencies is then based on a solid logic calculus the same way the classical\ndependencies are based on classical logic. For instance, rather than handling\ndegrees of similarity in an ad hoc manner, we consistently treat them as truth\nvalues, the same way as true (match) and false (mismatch) are treated in\nclassical theories. In addition, several notions intuitively embraced in the\npresence of grades, such as a degree of validity of a particular dependence or\na degree of entailment, naturally emerge and receive a conceptually clean\ntreatment in the presented approach. In the paper, we discuss motivations,\nprovide basic notions of syntax and semantics, and develop basic results which\ninclude entailment of dependencies, associated closure structures, a logic of\ndependencies with two versions of completeness theorem, results and algorithms\nregarding complete non-redundant sets of dependencies, relationship to and a\npossible reductionist interface to classical dependencies, and relationship to\nfunctional dependencies over domains with similarity.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2014 09:08:18 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Belohlavek", "Radim", ""], ["Vychodil", "Vilem", ""]]}, {"id": "1402.2237", "submitter": "Peter Bailis", "authors": "Peter Bailis, Alan Fekete, Michael J. Franklin, Ali Ghodsi, Joseph M.\n  Hellerstein, Ion Stoica", "title": "Coordination Avoidance in Database Systems (Extended Version)", "comments": "Extended version of paper appearing in PVLDB Vol. 8, No. 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimizing coordination, or blocking communication between concurrently\nexecuting operations, is key to maximizing scalability, availability, and high\nperformance in database systems. However, uninhibited coordination-free\nexecution can compromise application correctness, or consistency. When is\ncoordination necessary for correctness? The classic use of serializable\ntransactions is sufficient to maintain correctness but is not necessary for all\napplications, sacrificing potential scalability. In this paper, we develop a\nformal framework, invariant confluence, that determines whether an application\nrequires coordination for correct execution. By operating on application-level\ninvariants over database states (e.g., integrity constraints), invariant\nconfluence analysis provides a necessary and sufficient condition for safe,\ncoordination-free execution. When programmers specify their application\ninvariants, this analysis allows databases to coordinate only when anomalies\nthat might violate invariants are possible. We analyze the invariant confluence\nof common invariants and operations from real-world database systems (i.e.,\nintegrity constraints) and applications and show that many are invariant\nconfluent and therefore achievable without coordination. We apply these results\nto a proof-of-concept coordination-avoiding database prototype and demonstrate\nsizable performance gains compared to serializable execution, notably a 25-fold\nimprovement over prior TPC-C New-Order performance on a 200 server cluster.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2014 19:01:33 GMT"}, {"version": "v2", "created": "Tue, 11 Feb 2014 10:50:06 GMT"}, {"version": "v3", "created": "Fri, 4 Apr 2014 19:07:20 GMT"}, {"version": "v4", "created": "Thu, 30 Oct 2014 06:15:25 GMT"}], "update_date": "2014-10-31", "authors_parsed": [["Bailis", "Peter", ""], ["Fekete", "Alan", ""], ["Franklin", "Michael J.", ""], ["Ghodsi", "Ali", ""], ["Hellerstein", "Joseph M.", ""], ["Stoica", "Ion", ""]]}, {"id": "1402.2394", "submitter": "Joseph Gonzalez", "authors": "Reynold S. Xin, Daniel Crankshaw, Ankur Dave, Joseph E. Gonzalez,\n  Michael J. Franklin, Ion Stoica", "title": "GraphX: Unifying Data-Parallel and Graph-Parallel Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From social networks to language modeling, the growing scale and importance\nof graph data has driven the development of numerous new graph-parallel systems\n(e.g., Pregel, GraphLab). By restricting the computation that can be expressed\nand introducing new techniques to partition and distribute the graph, these\nsystems can efficiently execute iterative graph algorithms orders of magnitude\nfaster than more general data-parallel systems. However, the same restrictions\nthat enable the performance gains also make it difficult to express many of the\nimportant stages in a typical graph-analytics pipeline: constructing the graph,\nmodifying its structure, or expressing computation that spans multiple graphs.\nAs a consequence, existing graph analytics pipelines compose graph-parallel and\ndata-parallel systems using external storage systems, leading to extensive data\nmovement and complicated programming model.\n  To address these challenges we introduce GraphX, a distributed graph\ncomputation framework that unifies graph-parallel and data-parallel\ncomputation. GraphX provides a small, core set of graph-parallel operators\nexpressive enough to implement the Pregel and PowerGraph abstractions, yet\nsimple enough to be cast in relational algebra. GraphX uses a collection of\nquery optimization techniques such as automatic join rewrites to efficiently\nimplement these graph-parallel operators. We evaluate GraphX on real-world\ngraphs and workloads and demonstrate that GraphX achieves comparable\nperformance as specialized graph computation systems, while outperforming them\nin end-to-end graph pipelines. Moreover, GraphX achieves a balance between\nexpressiveness, performance, and ease of use.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2014 08:23:49 GMT"}], "update_date": "2014-02-12", "authors_parsed": [["Xin", "Reynold S.", ""], ["Crankshaw", "Daniel", ""], ["Dave", "Ankur", ""], ["Gonzalez", "Joseph E.", ""], ["Franklin", "Michael J.", ""], ["Stoica", "Ion", ""]]}, {"id": "1402.2487", "submitter": "Soumya Sen", "authors": "Partha Ghosh, Soumya Sen", "title": "Materialized View Replacement using Markovs Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Materialized view is used in large data centric applications to expedite\nquery processing. The efficiency of materialized view depends on degree of\nresult found against the queries over the existing materialized views.\nMaterialized views are constructed following different methodologies. Thus the\nefficacy of the materialized views depends on the methodology based on which\nthese are formed. Construction of materialized views are often time consuming\nand moreover after a certain time the performance of the materialized views\ndegrade when the nature of queries change. In this situation either new\nmaterialized views could be constructed from scratch or the existing views\ncould be upgraded. Fresh construction of materialized views has higher time\ncomplexity hence the modification of the existing views is a better\nsolution.Modification process of materialized view is classified under\nmaterialized view maintenance scheme. Materialized view maintenance is a\ncontinuous process and the system could be tuned to ensure a constant rate of\nperformance. If a materialized view construction process is not supported by\nmaterialized view maintenance scheme that system would suffer from performance\ndegradation. In this paper a new materialized view maintenance scheme is\nproposed using markovs analysis to ensure consistent performance. Markovs\nanalysis is chosen here to predict steady state probability over initial\nprobability.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2014 13:46:30 GMT"}], "update_date": "2014-02-12", "authors_parsed": [["Ghosh", "Partha", ""], ["Sen", "Soumya", ""]]}, {"id": "1402.2807", "submitter": "Rui Zhou", "authors": "Rui Zhou, Chengfei Liu, Jeffrey Xu Yu, Weifa Liang and Yanchun Zhang", "title": "Efficient Truss Maintenance in Evolving Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Truss was proposed to study social network data represented by graphs. A\nk-truss of a graph is a cohesive subgraph, in which each edge is contained in\nat least k-2 triangles within the subgraph. While truss has been demonstrated\nas superior to model the close relationship in social networks and efficient\nalgorithms for finding trusses have been extensively studied, very little\nattention has been paid to truss maintenance. However, most social networks are\nevolving networks. It may be infeasible to recompute trusses from scratch from\ntime to time in order to find the up-to-date $k$-trusses in the evolving\nnetworks. In this paper, we discuss how to maintain trusses in a graph with\ndynamic updates. We first discuss a set of properties on maintaining trusses,\nthen propose algorithms on maintaining trusses on edge deletions and\ninsertions, finally, we discuss truss index maintenance. We test the proposed\ntechniques on real datasets. The experiment results show the promise of our\nwork.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2014 12:57:06 GMT"}], "update_date": "2014-02-13", "authors_parsed": [["Zhou", "Rui", ""], ["Liu", "Chengfei", ""], ["Yu", "Jeffrey Xu", ""], ["Liang", "Weifa", ""], ["Zhang", "Yanchun", ""]]}, {"id": "1402.2892", "submitter": "Thabet Slimani", "authors": "Thabet Slimani, Amor Lazzez", "title": "Efficient Analysis of Pattern and Association Rule Mining Approaches", "comments": "14 pages, 3 figures. arXiv admin note: text overlap with\n  arXiv:1312.4800; and with arXiv:1109.2427 by other authors", "journal-ref": "International Journal of Information Technology and Computer\n  Science (IJITCS), vol.6, no.3, pp.70-81, 2014", "doi": "10.5815/ijitcs.2014.03.09", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The process of data mining produces various patterns from a given data\nsource. The most recognized data mining tasks are the process of discovering\nfrequent itemsets, frequent sequential patterns, frequent sequential rules and\nfrequent association rules. Numerous efficient algorithms have been proposed to\ndo the above processes. Frequent pattern mining has been a focused topic in\ndata mining research with a good number of references in literature and for\nthat reason an important progress has been made, varying from performant\nalgorithms for frequent itemset mining in transaction databases to complex\nalgorithms, such as sequential pattern mining, structured pattern mining,\ncorrelation mining. Association Rule mining (ARM) is one of the utmost current\ndata mining techniques designed to group objects together from large databases\naiming to extract the interesting correlation and relation among huge amount of\ndata. In this article, we provide a brief review and analysis of the current\nstatus of frequent pattern mining and discuss some promising research\ndirections. Additionally, this paper includes a comparative study between the\nperformance of the described approaches.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2014 16:45:54 GMT"}], "update_date": "2014-02-13", "authors_parsed": [["Slimani", "Thabet", ""], ["Lazzez", "Amor", ""]]}, {"id": "1402.3329", "submitter": "Justin Hsu", "authors": "Justin Hsu and Marco Gaboardi and Andreas Haeberlen and Sanjeev Khanna\n  and Arjun Narayan and Benjamin C. Pierce and Aaron Roth", "title": "Differential Privacy: An Economic Method for Choosing Epsilon", "comments": null, "journal-ref": null, "doi": "10.1109/CSF.2014.35", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy is becoming a gold standard for privacy research; it\noffers a guaranteed bound on loss of privacy due to release of query results,\neven under worst-case assumptions. The theory of differential privacy is an\nactive research area, and there are now differentially private algorithms for a\nwide range of interesting problems.\n  However, the question of when differential privacy works in practice has\nreceived relatively little attention. In particular, there is still no rigorous\nmethod for choosing the key parameter $\\epsilon$, which controls the crucial\ntradeoff between the strength of the privacy guarantee and the accuracy of the\npublished results.\n  In this paper, we examine the role that these parameters play in concrete\napplications, identifying the key questions that must be addressed when\nchoosing specific values. This choice requires balancing the interests of two\ndifferent parties: the data analyst and the prospective participant, who must\ndecide whether to allow their data to be included in the analysis. We propose a\nsimple model that expresses this balance as formulas over a handful of\nparameters, and we use our model to choose $\\epsilon$ on a series of simple\nstatistical studies. We also explore a surprising insight: in some\ncircumstances, a differentially private study can be more accurate than a\nnon-private study for the same cost, under our model. Finally, we discuss the\nsimplifying assumptions in our model and outline a research agenda for possible\nrefinements.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2014 22:47:13 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Hsu", "Justin", ""], ["Gaboardi", "Marco", ""], ["Haeberlen", "Andreas", ""], ["Khanna", "Sanjeev", ""], ["Narayan", "Arjun", ""], ["Pierce", "Benjamin C.", ""], ["Roth", "Aaron", ""]]}, {"id": "1402.3384", "submitter": "Weina Wang", "authors": "Weina Wang, Lei Ying and Junshan Zhang", "title": "A Minimax Distortion View of Differentially Private Query Release", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of differentially private query release through a\nsynthetic database approach. Departing from the existing approaches that\nrequire the query set to be specified in advance, we advocate to devise\nquery-set independent mechanisms, with an ambitious goal of providing accurate\nanswers, while meeting the privacy constraints, for all queries in a general\nquery class. Specifically, a differentially private mechanism is constructed to\n\"encode\" rich stochastic structure into the synthetic database, and\n\"customized\" companion estimators are then derived to provide accurate answers\nby making use of all available information, including the mechanism (which is\npublic information) and the query functions. Accordingly, the distortion under\nthe best of this kind of mechanisms at the worst-case query in a general query\nclass, so called the minimax distortion, provides a fundamental\ncharacterization of differentially private query release.\n  For the general class of statistical queries, we prove that with the\nsquared-error distortion measure, the minimax distortion is $O(1/n)$ by\nderiving asymptotically tight upper and lower bounds in the regime that the\ndatabase size $n$ goes to infinity. The upper bound is achievable by a\nmechanism $\\mathcal{E}$ and its corresponding companion estimators, which\npoints directly to the feasibility of the proposed approach in large databases.\nWe further evaluate the mechanism $\\mathcal{E}$ and the companion estimators\nthrough experiments on real datasets from Netflix and Facebook. Experimental\nresults show improvement over the state-of-art MWEM algorithm and verify the\nscaling behavior $O(1/n)$ of the minimax distortion.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2014 06:54:49 GMT"}, {"version": "v2", "created": "Mon, 1 Dec 2014 07:01:08 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Wang", "Weina", ""], ["Ying", "Lei", ""], ["Zhang", "Junshan", ""]]}, {"id": "1402.4073", "submitter": "Daniel Lemire", "authors": "Owen Kaser and Daniel Lemire", "title": "Threshold and Symmetric Functions over Bitmaps", "comments": "This paper uses small fonts and colours and is only intended for\n  electronic viewing", "journal-ref": null, "doi": null, "report-no": "TR-14-001", "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bitmap indexes are routinely used to speed up simple aggregate queries in\ndatabases. Set operations such as intersections, unions and complements can be\nrepresented as logical operations (AND, OR, NOT). However, less is known about\nthe application of bitmap indexes to more advanced queries. We want to extend\nthe applicability of bitmap indexes. As a starting point, we consider symmetric\nBoolean queries (e.g., threshold functions). For example, we might consider\nstores as sets of products, and ask for products that are on sale in 2 to 10\nstores. Such symmetric Boolean queries generalize intersection, union, and\nT-occurrence queries.\n  It may not be immediately obvious to an engineer how to use bitmap indexes\nfor symmetric Boolean queries. Yet, maybe surprisingly, we find that the best\nof our bitmap-based algorithms are competitive with the state-of-the-art\nalgorithms for important special cases (e.g., MergeOpt, MergeSkip, DivideSkip,\nScanCount). Moreover, unlike the competing algorithms, the result of our\ncomputation is again a bitmap which can be further processed within a bitmap\nindex.\n  We review algorithmic design issues such as the aggregation of many\ncompressed bitmaps. We conclude with a discussion on other advanced queries\nthat bitmap indexes might be able to support efficiently.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2014 17:22:05 GMT"}, {"version": "v2", "created": "Tue, 15 Nov 2016 01:59:54 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Kaser", "Owen", ""], ["Lemire", "Daniel", ""]]}, {"id": "1402.4283", "submitter": "Dipti Rana Mrs.", "authors": "P. Chaudhari, D. P. Rana, R. G. Mehta, N. J. Mistry, M. M. Raghuwanshi", "title": "Discretization of Temporal Data: A Survey", "comments": "4 pages, 1 Table", "journal-ref": "International Journal of Computer Science and Information Security\n  (IJCSIS), ISSN:1947-5500, Vol. 11, No. 2", "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real world, the huge amount of temporal data is to be processed in many\napplication areas such as scientific, financial, network monitoring, sensor\ndata analysis. Data mining techniques are primarily oriented to handle discrete\nfeatures. In the case of temporal data the time plays an important role on the\ncharacteristics of data. To consider this effect, the data discretization\ntechniques have to consider the time while processing to resolve the issue by\nfinding the intervals of data which are more concise and precise with respect\nto time. Here, this research is reviewing different data discretization\ntechniques used in temporal data applications according to the inclusion or\nexclusion of: class label, temporal order of the data and handling of stream\ndata to open the research direction for temporal data discretization to improve\nthe performance of data mining technique.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2014 10:44:01 GMT"}], "update_date": "2014-02-19", "authors_parsed": [["Chaudhari", "P.", ""], ["Rana", "D. P.", ""], ["Mehta", "R. G.", ""], ["Mistry", "N. J.", ""], ["Raghuwanshi", "M. M.", ""]]}, {"id": "1402.4417", "submitter": "Pankaj Malhotra", "authors": "Pankaj Malhotra, Puneet Agarwal, Gautam Shroff", "title": "Incremental Entity Resolution from Linked Documents", "comments": "15 pages, 8 figures, patented work", "journal-ref": null, "doi": null, "report-no": "TR-DAIF-2014-1", "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many government applications we often find that information about\nentities, such as persons, are available in disparate data sources such as\npassports, driving licences, bank accounts, and income tax records. Similar\nscenarios are commonplace in large enterprises having multiple customer,\nsupplier, or partner databases. Each data source maintains different aspects of\nan entity, and resolving entities based on these attributes is a well-studied\nproblem. However, in many cases documents in one source reference those in\nothers; e.g., a person may provide his driving-licence number while applying\nfor a passport, or vice-versa. These links define relationships between\ndocuments of the same entity (as opposed to inter-entity relationships, which\nare also often used for resolution). In this paper we describe an algorithm to\ncluster documents that are highly likely to belong to the same entity by\nexploiting inter-document references in addition to attribute similarity. Our\ntechnique uses a combination of iterative graph-traversal, locality-sensitive\nhashing, iterative match-merge, and graph-clustering to discover unique\nentities based on a document corpus. A unique feature of our technique is that\nnew sets of documents can be added incrementally while having to re-resolve\nonly a small subset of a previously resolved entity-document collection. We\npresent performance and quality results on two data-sets: a real-world database\nof companies and a large synthetically generated `population' database. We also\ndemonstrate benefit of using inter-document references for clustering in the\nform of enhanced recall of documents for resolution.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2014 17:45:31 GMT"}], "update_date": "2014-02-19", "authors_parsed": [["Malhotra", "Pankaj", ""], ["Agarwal", "Puneet", ""], ["Shroff", "Gautam", ""]]}, {"id": "1402.4466", "submitter": "Daniel Lemire", "authors": "Owen Kaser and Daniel Lemire", "title": "Compressed bitmap indexes: beyond unions and intersections", "comments": "Accepted for publication in Software: Practice and Experience on\n  August 14th 2014. Note that arXiv:1402.4073 [cs:DB] is a companion to this\n  paper; while they share some text, each contains many results not in the\n  other", "journal-ref": "Software: Practice & Experience 46 (2), 2016", "doi": "10.1002/spe.2289", "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed bitmap indexes are used to speed up simple aggregate queries in\ndatabases. Indeed, set operations like intersections, unions and complements\ncan be represented as logical operations (AND,OR,NOT) that are ideally suited\nfor bitmaps. However, it is less obvious how to apply bitmaps to more advanced\nqueries. For example, we might seek products in a store that meet some, but\nmaybe not all, criteria. Such threshold queries generalize intersections and\nunions; they are often used in information-retrieval and data-mining\napplications. We introduce new algorithms that are sometimes three orders of\nmagnitude faster than a naive approach. Our work shows that bitmap indexes are\nmore broadly applicable than is commonly believed.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2014 20:41:33 GMT"}, {"version": "v2", "created": "Thu, 15 May 2014 18:28:14 GMT"}, {"version": "v3", "created": "Fri, 15 Aug 2014 13:45:47 GMT"}], "update_date": "2016-01-11", "authors_parsed": [["Kaser", "Owen", ""], ["Lemire", "Daniel", ""]]}, {"id": "1402.4742", "submitter": "Sarah Emery Bunn", "authors": "Markus Demleitner, Patrick Dowler, Ray Plante, Guy Rixon and Mark\n  Taylor", "title": "IVOA Recommendation: TAPRegExt: a VOResource Schema Extension for\n  Describing TAP Services", "comments": null, "journal-ref": null, "doi": "10.5479/ADS/bib/2012ivoa.spec.0827D", "report-no": "REC-TAPRegExt-1.0-20120827", "categories": "astro-ph.IM cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document describes an XML encoding standard for metadata about services\nimplementing the table access protocol TAP [TAP], referred to as TAPRegExt.\nInstance documents are part of the service's registry record or can be obtained\nfrom the service itself. They deliver information to both humans and software\non the languages, output formats, and upload methods supported by the service,\nas well as data models implemented by the exposed tables, optional language\nfeatures, and certain limits enforced by the service.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2014 17:52:34 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Demleitner", "Markus", ""], ["Dowler", "Patrick", ""], ["Plante", "Ray", ""], ["Rixon", "Guy", ""], ["Taylor", "Mark", ""]]}, {"id": "1402.5194", "submitter": "Rui Han", "authors": "Rui Han, Xiaoyi Lu", "title": "On Big Data Benchmarking", "comments": "7 pages, 4 figures, 2 tables, accepted in BPOE-04\n  (http://prof.ict.ac.cn/bpoe_4_asplos/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data systems address the challenges of capturing, storing, managing,\nanalyzing, and visualizing big data. Within this context, developing benchmarks\nto evaluate and compare big data systems has become an active topic for both\nresearch and industry communities. To date, most of the state-of-the-art big\ndata benchmarks are designed for specific types of systems. Based on our\nexperience, however, we argue that considering the complexity, diversity, and\nrapid evolution of big data systems, for the sake of fairness, big data\nbenchmarks must include diversity of data and workloads. Given this motivation,\nin this paper, we first propose the key requirements and challenges in\ndeveloping big data benchmarks from the perspectives of generating data with 4V\nproperties (i.e. volume, velocity, variety and veracity) of big data, as well\nas generating tests with comprehensive workloads for big data systems. We then\npresent the methodology on big data benchmarking designed to address these\nchallenges. Next, the state-of-the-art are summarized and compared, following\nby our vision for future research directions.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2014 03:03:16 GMT"}], "update_date": "2014-02-24", "authors_parsed": [["Han", "Rui", ""], ["Lu", "Xiaoyi", ""]]}, {"id": "1402.5742", "submitter": "Ismail Toroslu", "authors": "Ugur Turan, Ismail Hakki Toroslu", "title": "Secure Logical Schema and Decomposition Algorithm for Proactive Context\n  Dependent Attribute Based Access Control", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional database access control mechanisms use role based methods, with\ngenerally row based and attribute based constraints for granularity, and\nprivacy is achieved mainly by using views. However if only a set of views\naccording to policy are made accessible to users, then this set should be\nchecked against the policy for the whole probable query history. The aim of\nthis work is to define a proactive decomposition algorithm according to the\nattribute based policy rules and build a secure logical schema in which\nrelations are decomposed into several ones in order to inhibit joins or\ninferences that may violate predefined privacy constraints. The attributes\nwhose association should not be inferred, are defined as having security\ndependency among them and they form a new kind of context dependent attribute\nbased policy rule named as security dependent set. The decomposition algorithm\nworks on a logical schema with given security dependent sets and aims to\nprohibit the inference of the association among the elements of these sets. It\nis also proven that the decomposition technique generates a secure logical\nschema that is in compliance with the given security dependent set constraints.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2014 07:55:17 GMT"}, {"version": "v2", "created": "Thu, 17 Jul 2014 07:12:46 GMT"}], "update_date": "2014-07-18", "authors_parsed": [["Turan", "Ugur", ""], ["Toroslu", "Ismail Hakki", ""]]}, {"id": "1402.5773", "submitter": "Richard McClatchey", "authors": "Richard McClatchey", "title": "Data Management Challenges in Paediatric Information Systems", "comments": "16 pages, 4 figures, 1 table. arXiv admin note: substantial text\n  overlap with arXiv:0812.2874, arXiv:cs/0603036, arXiv:0707.0763", "journal-ref": "Chapter in From Physics into Daily Life - How Knowledge Transfer\n  Changed Biology, Medicine and Healthcare. Ed B. Bressan. Wiley Publishers,\n  2014", "doi": null, "report-no": null, "categories": "cs.DB cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a compelling demand for the data integration and exploitation of\nheterogeneous biomedical information for improved clinical practice, medical\nresearch, and personalised healthcare across the EU. The area of paediatric\ninformation integration is particularly challenging since the patients\nphysiology changes with growth and different aspects of health being regularly\nmonitored over extended periods of time. Paediatricians require access to\nheterogeneous data sets, often collected in different locations with different\napparatus and over extended timescales. Using a Grid platform originally\ndeveloped for physics at CERN and a novel integrated semantic data model the\nHealth-e-Child project has developed an integrated healthcare platform for\nEuropean paediatrics, providing seamless integration of traditional and\nemerging sources of biomedical data. The long-term goal of the project was to\nprovide uninhibited access to universal biomedical knowledge repositories for\npersonalised and preventive healthcare, large-scale information-based\nbiomedical research and training, and informed policy making. The project built\na Grid-enabled european network of leading clinical centres that can share and\nannotate paediatric data, can validate systems clinically, and diffuse clinical\nexcellence across Europe by setting up new technologies, clinical workflows,\nand standards. The Health-e-Child project highlights data management challenges\nfor the future of European paediatric healthcare and is the subject of this\nchapter.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2014 10:21:01 GMT"}], "update_date": "2014-02-25", "authors_parsed": [["McClatchey", "Richard", ""]]}, {"id": "1402.5781", "submitter": "Karthik Ramachandra", "authors": "Karthik Ramachandra and Mahendra Chavan and Ravindra Guravannavar and\n  S Sudarshan", "title": "Program Transformations for Asynchronous and Batched Query Submission", "comments": "14 pages", "journal-ref": null, "doi": "10.1109/TKDE.2014.2334302", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of database/Web-service backed applications can be\nsignificantly improved by asynchronous submission of queries/requests well\nahead of the point where the results are needed, so that results are likely to\nhave been fetched already when they are actually needed. However, manually\nwriting applications to exploit asynchronous query submission is tedious and\nerror-prone. In this paper we address the issue of automatically transforming a\nprogram written assuming synchronous query submission, to one that exploits\nasynchronous query submission. Our program transformation method is based on\ndata flow analysis and is framed as a set of transformation rules. Our rules\ncan handle query executions within loops, unlike some of the earlier work in\nthis area. We also present a novel approach that, at runtime, can combine\nmultiple asynchronous requests into batches, thereby achieving the benefits of\nbatching in addition to that of asynchronous submission. We have built a tool\nthat implements our transformation techniques on Java programs that use JDBC\ncalls; our tool can be extended to handle Web service calls. We have carried\nout a detailed experimental study on several real-life applications, which\nshows the effectiveness of the proposed rewrite techniques, both in terms of\ntheir applicability and the performance gains achieved.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2014 10:39:44 GMT"}], "update_date": "2014-07-29", "authors_parsed": [["Ramachandra", "Karthik", ""], ["Chavan", "Mahendra", ""], ["Guravannavar", "Ravindra", ""], ["Sudarshan", "S", ""]]}, {"id": "1402.6067", "submitter": "Zhilin Wu", "authors": "Zhilin Wu", "title": "Regular path queries on graphs with data: A rigid approach", "comments": "25 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DB cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regular path queries (RPQ) is a classical navigational query formalism for\ngraph databases to specify constraints on labeled paths. Recently, RPQs have\nbeen extended by Libkin and Vrgo$\\rm \\check{c}$ to incorporate data value\ncomparisons among different nodes on paths, called regular path queries with\ndata (RDPQ). It has been shown that the evaluation problem of RDPQs is\nPSPACE-complete and NLOGSPACE-complete in data complexity. On the other hand,\nthe containment problem of RDPQs is in general undecidable. In this paper, we\npropose a novel approach to extend regular path queries with data value\ncomparisons, called rigid regular path queries with data (RRDPQ). The main\ningredient of this approach is an automata model called nondeterministic rigid\nregister automata (NRRA), in which the data value comparisons are \\emph{rigid},\nin the sense that if the data value in the current position $x$ is compared to\na data value in some other position $y$, then by only using the labels (but not\ndata values), the position $y$ can be uniquely determined from $x$. We show\nthat NRRAs are robust in the sense that nondeterministic, deterministic and\ntwo-way variant of NRRAs, as well as an extension of regular expressions, are\nall of the same expressivity. We then argue that the expressive power of RDPQs\nare reasonable by demonstrating that for every graph database, there is a\nlocalized transformation of the graph database so that every RDPQ in the\noriginal graph database can be turned into an equivalent RRDPQ over the\ntransformed one. Finally, we investigate the computational properties of RRDPQs\nand conjunctive RRDPQs (CRRDPQ). In particular, we show that the containment of\nCRRDPQs (and RRDPQs) can be decided in 2EXPSPACE.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2014 06:54:08 GMT"}], "update_date": "2014-02-26", "authors_parsed": [["Wu", "Zhilin", ""]]}, {"id": "1402.6124", "submitter": "Oliver Mason", "authors": "Naoise Holohan, Douglas Leith and Oliver Mason", "title": "Differential Privacy in Metric Spaces: Numerical, Categorical and\n  Functional Data Under the One Roof", "comments": "18 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IT math.IT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study Differential Privacy in the abstract setting of Probability on\nmetric spaces. Numerical, categorical and functional data can be handled in a\nuniform manner in this setting. We demonstrate how mechanisms based on data\nsanitisation and those that rely on adding noise to query responses fit within\nthis framework. We prove that once the sanitisation is differentially private,\nthen so is the query response for any query. We show how to construct\nsanitisations for high-dimensional databases using simple 1-dimensional\nmechanisms. We also provide lower bounds on the expected error for\ndifferentially private sanitisations in the general metric space setting.\nFinally, we consider the question of sufficient sets for differential privacy\nand show that for relaxed differential privacy, any algebra generating the\nBorel $\\sigma$-algebra is a sufficient set for relaxed differential privacy.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2014 10:47:07 GMT"}], "update_date": "2014-02-26", "authors_parsed": [["Holohan", "Naoise", ""], ["Leith", "Douglas", ""], ["Mason", "Oliver", ""]]}, {"id": "1402.6407", "submitter": "Daniel Lemire", "authors": "Samy Chambi, Daniel Lemire, Owen Kaser, Robert Godin", "title": "Better bitmap performance with Roaring bitmaps", "comments": null, "journal-ref": "Software: Practice and Experience Volume 46, Issue 5, pages\n  709-719, May 2016", "doi": "10.1002/spe.2325", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bitmap indexes are commonly used in databases and search engines. By\nexploiting bit-level parallelism, they can significantly accelerate queries.\nHowever, they can use much memory, and thus we might prefer compressed bitmap\nindexes. Following Oracle's lead, bitmaps are often compressed using run-length\nencoding (RLE). Building on prior work, we introduce the Roaring compressed\nbitmap format: it uses packed arrays for compression instead of RLE. We compare\nit to two high-performance RLE-based bitmap encoding techniques: WAH (Word\nAligned Hybrid compression scheme) and Concise (Compressed `n' Composable\nInteger Set). On synthetic and real data, we find that Roaring bitmaps (1)\noften compress significantly better (e.g., 2 times) and (2) are faster than the\ncompressed alternatives (up to 900 times faster for intersections). Our results\nchallenge the view that RLE-based bitmap compression is best.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2014 04:38:22 GMT"}, {"version": "v10", "created": "Tue, 15 Mar 2016 19:31:53 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2014 23:34:18 GMT"}, {"version": "v3", "created": "Tue, 4 Mar 2014 22:45:23 GMT"}, {"version": "v4", "created": "Fri, 6 Jun 2014 17:55:23 GMT"}, {"version": "v5", "created": "Wed, 19 Nov 2014 18:25:55 GMT"}, {"version": "v6", "created": "Tue, 2 Dec 2014 19:17:31 GMT"}, {"version": "v7", "created": "Wed, 3 Dec 2014 15:36:12 GMT"}, {"version": "v8", "created": "Wed, 11 Mar 2015 17:50:04 GMT"}, {"version": "v9", "created": "Sat, 28 Mar 2015 14:45:00 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Chambi", "Samy", ""], ["Lemire", "Daniel", ""], ["Kaser", "Owen", ""], ["Godin", "Robert", ""]]}, {"id": "1402.6742", "submitter": "Richard McClatchey", "authors": "Jetendr Shamdasani, Andrew Branson, Richard McClatchey, Coralie Blanc,\n  Florent Martin, Pierre Bornand, Sandra Massonnat, Olivier Gattaz and Patrick\n  Emin", "title": "CRISTAL-ISE : Provenance Applied in Industry", "comments": "6 pages, 3 figures; Presented at the 16th International Conference on\n  Enterprise Information Systems (ICEIS 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the CRISTAL-iSE project as a framework for the management\nof provenance information in industry. The project itself is a research\ncollaboration between academia and industry. A key factor in the project is the\nuse of a system known as CRISTAL which is a mature system based on proven\ndescription driven principles. A crucial element in the description driven\napproach is that the fact that objects (Items) are described at runtime\nenabling managed systems to be both dynamic and flexible. Another factor is the\nnotion that all Items in CRISTAL are stored and versioned, therefore enabling a\nprovenance collection system. In this paper a concrete application, called\nAgilium, is briefly described and a future application CIMAG-RA is presented\nwhich will harness the power of both CRISTAL and Agilium.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2014 23:17:58 GMT"}], "update_date": "2014-02-28", "authors_parsed": [["Shamdasani", "Jetendr", ""], ["Branson", "Andrew", ""], ["McClatchey", "Richard", ""], ["Blanc", "Coralie", ""], ["Martin", "Florent", ""], ["Bornand", "Pierre", ""], ["Massonnat", "Sandra", ""], ["Gattaz", "Olivier", ""], ["Emin", "Patrick", ""]]}, {"id": "1402.6859", "submitter": "Ahmed Ibrahim Taloba", "authors": "M. H. Marghny, Ahmed I. Taloba", "title": "Outlier Detection using Improved Genetic K-means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The outlier detection problem in some cases is similar to the classification\nproblem. For example, the main concern of clustering-based outlier detection\nalgorithms is to find clusters and outliers, which are often regarded as noise\nthat should be removed in order to make more reliable clustering. In this\narticle, we present an algorithm that provides outlier detection and data\nclustering simultaneously. The algorithmimprovesthe estimation of centroids of\nthe generative distribution during the process of clustering and outlier\ndiscovery. The proposed algorithm consists of two stages. The first stage\nconsists of improved genetic k-means algorithm (IGK) process, while the second\nstage iteratively removes the vectors which are far from their cluster\ncentroids.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2014 11:07:00 GMT"}], "update_date": "2014-05-25", "authors_parsed": [["Marghny", "M. H.", ""], ["Taloba", "Ahmed I.", ""]]}, {"id": "1402.7063", "submitter": "Spyros Sioutas SS", "authors": "Nikolaos Nodarakis, Spyros Sioutas, Dimitrios Tsoumakos, Giannis\n  Tzimas and Evaggelia Pitoura", "title": "Rapid AkNN Query Processing for Fast Classification of Multidimensional\n  Data in the Cloud", "comments": "12 pages, 14 figures, 4 tables (it will be submitted to DEXA 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A $k$-nearest neighbor ($k$NN) query determines the $k$ nearest points, using\ndistance metrics, from a specific location. An all $k$-nearest neighbor\n(A$k$NN) query constitutes a variation of a $k$NN query and retrieves the $k$\nnearest points for each point inside a database. Their main usage resonates in\nspatial databases and they consist the backbone of many location-based\napplications and not only (i.e. $k$NN joins in databases, classification in\ndata mining). So, it is very crucial to develop methods that answer them\nefficiently. In this work, we propose a novel method for classifying\nmultidimensional data using an A$k$NN algorithm in the MapReduce framework. Our\napproach exploits space decomposition techniques for processing the\nclassification procedure in a parallel and distributed manner. To our\nknowledge, we are the first to study the classification of multidimensional\nobjects under this perspective. Through an extensive experimental evaluation we\nprove that our solution is efficient and scalable in processing the given\nqueries. We investigate many different perspectives that can affect the total\ncomputational cost, such as different dataset distributions, number of\ndimensions, growth of $k$ value and granularity of space decomposition and\nprove that our system is efficient, robust and scalable.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2014 20:46:09 GMT"}], "update_date": "2014-02-28", "authors_parsed": [["Nodarakis", "Nikolaos", ""], ["Sioutas", "Spyros", ""], ["Tsoumakos", "Dimitrios", ""], ["Tzimas", "Giannis", ""], ["Pitoura", "Evaggelia", ""]]}, {"id": "1402.7122", "submitter": "Anonymous Anonymous Mr.", "authors": "Meghyn Bienvenu, Diego Calvanese, Magdalena Ortiz, Mantas Simkus", "title": "Nested Regular Path Queries in Description Logics", "comments": "added Figure 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-way regular path queries (2RPQs) have received increased attention\nrecently due to their ability to relate pairs of objects by flexibly navigating\ngraph-structured data. They are present in property paths in SPARQL 1.1, the\nnew standard RDF query language, and in the XML query language XPath. In line\nwith XPath, we consider the extension of 2RPQs with nesting, which allows one\nto require that objects along a path satisfy complex conditions, in turn\nexpressed through (nested) 2RPQs. We study the computational complexity of\nanswering nested 2RPQs and conjunctions thereof (CN2RPQs) in the presence of\ndomain knowledge expressed in description logics (DLs). We establish tight\ncomplexity bounds in data and combined complexity for a variety of DLs, ranging\nfrom lightweight DLs (DL-Lite, EL) up to highly expressive ones. Interestingly,\nwe are able to show that adding nesting to (C)2RPQs does not affect worst-case\ndata complexity of query answering for any of the considered DLs. However, in\nthe case of lightweight DLs, adding nesting to 2RPQs leads to a surprising jump\nin combined complexity, from P-complete to Exp-complete.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2014 02:52:57 GMT"}, {"version": "v2", "created": "Tue, 4 Mar 2014 18:18:00 GMT"}], "update_date": "2014-03-05", "authors_parsed": [["Bienvenu", "Meghyn", ""], ["Calvanese", "Diego", ""], ["Ortiz", "Magdalena", ""], ["Simkus", "Mantas", ""]]}, {"id": "1402.7190", "submitter": "Venugopal K r", "authors": "S kumarasawamy, Srikanth P L, Manjula S H, K R Venugopal, L M Patnaik", "title": "Two Stage Prediction Process with Gradient Descent Methods Aligning with\n  the Data Privacy Preservation", "comments": "14 pages", "journal-ref": "International Journal of Information Processing, 7(3), 68-82, 2013", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy preservation emphasize on authorization of data, which signifies that\ndata should be accessed only by authorized users. Ensuring the privacy of data\nis considered as one of the challenging task in data management. The\ngeneralization of data with varying concept hierarchies seems to be interesting\nsolution. This paper proposes two stage prediction processes on privacy\npreserved data. The privacy is preserved using generalization and betraying\nother communicating parties by disguising generalized data which adds another\nlevel of privacy. The generalization with betraying is performed in first stage\nto define the knowledge or hypothesis and which is further optimized using\ngradient descent method in second stage prediction for accurate prediction of\ndata. The experiment carried with both batch and stochastic gradient methods\nand it is shown that bulk operation performed by batch takes long time and more\niterations than stochastic to give more accurate solution.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2014 10:36:16 GMT"}], "update_date": "2014-03-03", "authors_parsed": [["kumarasawamy", "S", ""], ["L", "Srikanth P", ""], ["H", "Manjula S", ""], ["Venugopal", "K R", ""], ["Patnaik", "L M", ""]]}, {"id": "1402.7223", "submitter": "Henning Hasemann", "authors": "Dennis Boldt, Henning Hasemann, Alexander Kr\\\"oller, Marcel Karnstedt,\n  Christian von der Weth", "title": "SPARQL for Networks of Embedded Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Semantic Web (or Web of Data) represents the successful efforts towards\nlinking and sharing data over the Web. The cornerstones of the Web of Data are\nRDF as data format and SPARQL as de-facto standard query language. Recent\ntrends show the evolution of the Web of Data towards the Web of Things,\nintegrating embedded devices and smart objects. Data stemming from such devices\ndo not share a common format, making the integration and querying impossible.\nTo overcome this problem, we present our approach to make embedded systems\nfirst-class citizens of the Web of Things. Our framework abstracts from\nindividual deployments to represent them as common data sources in line with\nthe ideas behind the Semantic Web. This includes the execution of arbitrary\nSPARQL queries over the data from a pool of embedded devices and/or external\ndata sources. Handling verbose RDF data and executing SPARQL queries in an\nembedded network poses major challenges to minimize the involved processing and\ncommunication cost. We therefore present an in-network query processor aiming\nto push processing steps onto devices. We demonstrate the practical application\nand the potential benefits of our framework in a comprehensive evaluation using\na real-world deployment and a range of SPARQL queries stemming from a common\nuse case of the Web of Things.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2014 12:36:12 GMT"}], "update_date": "2014-03-03", "authors_parsed": [["Boldt", "Dennis", ""], ["Hasemann", "Henning", ""], ["Kr\u00f6ller", "Alexander", ""], ["Karnstedt", "Marcel", ""], ["von der Weth", "Christian", ""]]}, {"id": "1402.7228", "submitter": "Henning Hasemann", "authors": "Henning Hasemann, Alexander Kr\\\"oller, Max Pagel", "title": "The Wiselib TupleStore: A Modular RDF Database for the Internet of\n  Things", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet of Things movement provides self-configuring and universally\ninteroperable devices. While such devices are often built with a specific\napplication in mind, they often turn out to be useful in other contexts as\nwell. We claim that by describing the devices' knowledge in a universal way,\nIoT devices can become first-class citizens in the Internet. They can then\nexchange data between heterogeneous hardware, different applications and large\ndata sources on the Web. Our key idea --- in contrast to most existing\napproaches --- is to not restrict the domain of knowledge that can be expressed\non the device in any way and, at the same time, allow this knowledge to be\nmachine-understandable and linkable across different locations.\n  We propose an architecture that allows to connect embedded devices to the\nSemantic Web by expressing their knowledge in the Resource Description\nFramework (RDF). We present the Wiselib TupleStore, a modular embedded database\ntailored specifically for the storage of RDF. The Wiselib TupleStore is\nportable to many platforms including Contiki and TinyOS and allows a variety of\ntrade-offs, making it able to scale to a large variety of hardware scenarios.\nWe discuss the applicability of RDF to heterogeneous resource-constrained\ndevices and compare our system to the existing embedded tuple stores Antelope\nand TeenyLIME.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2014 12:52:47 GMT"}], "update_date": "2014-03-03", "authors_parsed": [["Hasemann", "Henning", ""], ["Kr\u00f6ller", "Alexander", ""], ["Pagel", "Max", ""]]}, {"id": "1402.7341", "submitter": "Brijesh Mehta", "authors": "Brijesh B. Mehta, Udai Pratap Rao", "title": "A Novel approach as Multi-place Watermarking for Security in Database", "comments": "5 pages, 3 figures, Int'l Conf. Security and Management, SAM'11", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital multimedia watermarking technology had suggested in the last decade\nto embed copyright information in digital objects such as images, audio and\nvideo. However, the increasing use of relational database systems in many\nreal-life applications created an ever-increasing need for watermarking\ndatabase systems. As a result, watermarking relational database system is now\nemerging as a research area that deals with the legal issue of copyright\nprotection of database systems. The main goal of database watermarking is to\ngenerate robust and impersistent watermark for database. In this paper we\npropose a method, based on image as watermark and this watermark is embedded\nover the database at two different attribute of tuple, one in the numeric\nattribute of tuple and another in the date attribute's time (seconds) field.\nOur approach can be applied for numerical and categorical database.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2014 18:43:29 GMT"}], "update_date": "2014-03-03", "authors_parsed": [["Mehta", "Brijesh B.", ""], ["Rao", "Udai Pratap", ""]]}]