[{"id": "1908.00928", "submitter": "Benjamin V\\\"olker", "authors": "Philipp M. Scholl, Benjamin V\\\"olker, Bernd Becker, Kristof Van\n  Laerhoven", "title": "A Multi-Media Exchange Format for Time-Series Dataset Curation", "comments": "HASCA Book Contribution, Keywords: Data Curation; Activity\n  Recognition; Multi-Media Format; Data Storage; Comma-Separated-Values", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Exchanging data as character-separated values (CSV) is slow, cumbersome and\nerror-prone. Especially for time-series data, which is common in Activity\nRecognition, synchronizing several independently recorded sensors is\nchallenging. Adding second level evidence, like video recordings from multiple\nangles and time-coded annotations, further complicates the matter of curating\nsuch data. A possible alternative is to make use of standardized multi-media\nformats. Sensor data can be encoded in audio format, and time-coded\ninformation, like annotations, as subtitles. Video data can be added easily.\nAll this media can be merged into a single container file, which makes the\nissue of synchronization explicit. The incurred performance overhead by this\nencoding is shown to be negligible and compression can be applied to optimize\nstorage and transmission overhead.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 13:54:15 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Scholl", "Philipp M.", ""], ["V\u00f6lker", "Benjamin", ""], ["Becker", "Bernd", ""], ["Van Laerhoven", "Kristof", ""]]}, {"id": "1908.01338", "submitter": "Sudhakar Singh", "authors": "Pankaj Singh, Sudhakar Singh, P. K. Mishra, Rakhi Garg", "title": "A Data Structure Perspective to the RDD-based Apriori Algorithm on Spark", "comments": "14 pages", "journal-ref": "International Journal of Information Technology, Springer, 2019", "doi": "10.1007/s41870-019-00337-3", "report-no": "BJIT-D-18-00717", "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the recent years, a number of efficient and scalable frequent itemset\nmining algorithms for big data analytics have been proposed by many\nresearchers. Initially, MapReduce-based frequent itemset mining algorithms on\nHadoop cluster were proposed. Although, Hadoop has been developed as a cluster\ncomputing system for handling and processing big data, but the performance of\nHadoop does not meet the expectation for the iterative algorithms of data\nmining, due to its high I/O, and writing and then reading intermediate results\nin the disk. Consequently, Spark has been developed as another cluster\ncomputing infrastructure which is much faster than Hadoop due to its in-memory\ncomputation. It is highly suitable for iterative algorithms and supports batch,\ninteractive, iterative, and stream processing of data. Many frequent itemset\nmining algorithms have been re-designed on the Spark, and most of them are\nApriori-based. All these Spark-based Apriori algorithms use Hash Tree as the\nunderlying data structure. This paper investigates the efficiency of various\ndata structures for the Spark-based Apriori. Although, the data structure\nperspective has been investigated previously, but for MapReduce-based Apriori,\nand it must be re-investigated in the distributed computing environment of\nSpark. The considered underlying data structures are Hash Tree, Trie, and Hash\nTable Trie. The experimental results on the benchmark datasets show that the\nperformance of Spark-based Apriori with Trie and Hash Table Trie are almost\nsimilar but both perform many times better than Hash Tree in the distributed\ncomputing environment of Spark.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 13:13:28 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Singh", "Pankaj", ""], ["Singh", "Sudhakar", ""], ["Mishra", "P. K.", ""], ["Garg", "Rakhi", ""]]}, {"id": "1908.01458", "submitter": "Suyash Gupta", "authors": "Suyash Gupta, Jelle Hellings, Mohammad Sadoghi", "title": "Revisiting consensus protocols through wait-free parallelization", "comments": "Brief Announcement at DISC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent surge of blockchain systems has renewed the interest in\ntraditional Byzantine fault-tolerant consensus protocols. Many such consensus\nprotocols have a primary-backup design in which an assigned replica, the\nprimary, is responsible for coordinating the consensus protocol. Although the\nprimary-backup design leads to relatively simple and high performance consensus\nprotocols, it places an unreasonable burden on a good primary and allows\nmalicious primaries to substantially affect the system performance. In this\npaper, we propose a protocol-agnostic approach to improve the design of primary\nbackup consensus protocols. At the core of our approach is a novel wait-free\napproach of running several instances of the underlying consensus protocol in\nparallel. To yield a high performance parallelized design, we present\ncoordination-free techniques to order operations across parallel instances,\ndeal with instance failures, and assign clients to specific instances.\nConsequently, the design we present is able to reduce the load on individual\ninstances and primaries, while also reducing the adverse effects of any\nmalicious replicas.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 04:07:21 GMT"}, {"version": "v2", "created": "Tue, 6 Aug 2019 02:14:40 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Gupta", "Suyash", ""], ["Hellings", "Jelle", ""], ["Sadoghi", "Mohammad", ""]]}, {"id": "1908.01528", "submitter": "Till Blume", "authors": "Till Blume, David Richerby and Ansgar Scherp", "title": "FLUID: A Common Model for Semantic Structural Graph Summaries Based on\n  Equivalence Relations", "comments": "Accepted author manuscript to appear in Theoretical Computer Science", "journal-ref": null, "doi": "10.1016/j.tcs.2020.12.019", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Summarization is a widespread method for handling very large graphs. The task\nof structural graph summarization is to compute a concise but meaningful\nsynopsis of the key structural information of a graph. As summaries may be used\nfor many different purposes, there is no single concept or model of graph\nsummaries. We have studied existing structural graph summaries for large-scale\n(semantic) graphs. Despite their different concepts and purposes, we found\ncommonalities in the graph structures they capture. We use these commonalities\nto provide for the first time a formally defined common model, FLUID (FLexible\ngraph sUmmarIes for Data graphs), that allows us to flexibly define structural\ngraph summaries. FLUID allows graph summaries to be quickly defined, adapted,\nand compared for different purposes and datasets. To this end, FLUID provides\nfeatures of structural summarization based on equivalence relations such as\ndistinction of types and properties, direction of edges, bisimulation, and\ninference. We conduct a detailed complexity analysis of the features provided\nby FLUID. We show that graph summaries defined with FLUID can be computed in\nthe worst case in time $\\mathcal{O}(n^2)$ w.r.t. $n$, the number of edges in\nthe data graph. An empirical analysis of large-scale web graphs with billions\nof edges indicates a typical running time of $\\Theta(n)$. Based on the formal\nFLUID model, one can quickly define and modify various structural graph\nsummaries from the literature and beyond.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 09:14:54 GMT"}, {"version": "v2", "created": "Sun, 16 Aug 2020 15:20:07 GMT"}, {"version": "v3", "created": "Mon, 4 Jan 2021 11:40:52 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Blume", "Till", ""], ["Richerby", "David", ""], ["Scherp", "Ansgar", ""]]}, {"id": "1908.01812", "submitter": "Javiel Rojas-Ledesma", "authors": "Gonzalo Navarro and Juan L. Reutter and Javiel Rojas-Ledesma", "title": "Optimal Joins using Compact Data Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Worst-case optimal join algorithms have gained a lot of attention in the\ndatabase literature. We now count with several algorithms that are optimal in\nthe worst case, and many of them have been implemented and validated in\npractice. However, the implementation of these algorithms often requires an\nenhanced indexing structure: to achieve optimality we either need to build\ncompletely new indexes, or we must populate the database with several\ninstantiations of indexes such as B$+$-trees. Either way, this means spending\nan extra amount of storage space that may be non-negligible.\n  We show that optimal algorithms can be obtained directly from a\nrepresentation that regards the relations as point sets in variable-dimensional\ngrids, without the need of extra storage. Our representation is a compact quad\ntree for the static indexes, and a dynamic quadtree sharing subtrees (which we\ndub a qdag) for intermediate results. We develop a compositional algorithm to\nprocess full join queries under this representation, and show that the running\ntime of this algorithm is worst-case optimal in data complexity. Remarkably, we\ncan extend our framework to evaluate more expressive queries from relational\nalgebra by introducing a lazy version of qdags (lqdags). Once again, we can\nshow that the running time of our algorithms is worst-case optimal.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 19:25:57 GMT"}, {"version": "v2", "created": "Thu, 9 Jan 2020 15:28:33 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Navarro", "Gonzalo", ""], ["Reutter", "Juan L.", ""], ["Rojas-Ledesma", "Javiel", ""]]}, {"id": "1908.01860", "submitter": "Puya Memarzia", "authors": "Puya Memarzia, Suprio Ray, Virendra C Bhavsar", "title": "Toward Efficient In-memory Data Analytics on NUMA Systems", "comments": "15 pages, 9 figures Rev 3: fixed minor typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data analytics systems commonly utilize in-memory query processing techniques\nto achieve better throughput and lower latency. Modern computers increasingly\nrely on Non-Uniform Memory Access (NUMA) architectures in order to achieve\nscalability. A key drawback of NUMA architectures is that many existing\nsoftware solutions are not aware of the underlying NUMA topology and thus do\nnot take full advantage of the hardware. Modern operating systems are designed\nto provide basic support for NUMA systems. However, default system\nconfigurations are typically sub-optimal for large data analytics applications.\nAdditionally, achieving NUMA-awareness by rewriting the application from the\nground up is not always feasible.\n  In this work, we evaluate a variety of strategies that aim to accelerate\nmemory-intensive data analytics workloads on NUMA systems. We analyze the\nimpact of different memory allocators, memory placement strategies, thread\nplacement, and kernel-level load balancing and memory management mechanisms.\nWith extensive experimental evaluation we demonstrate that methodical\napplication of these techniques can be used to obtain significant speedups in\nfour commonplace in-memory data analytics workloads, on three different\nhardware architectures. Furthermore, we show that these strategies can speed up\ntwo popular database systems running a TPC-H workload.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 21:03:00 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2019 01:34:48 GMT"}, {"version": "v3", "created": "Sat, 25 Jan 2020 06:05:31 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Memarzia", "Puya", ""], ["Ray", "Suprio", ""], ["Bhavsar", "Virendra C", ""]]}, {"id": "1908.01908", "submitter": "Yingjun Wu", "authors": "Ronald Barber, Christian Garcia-Arellano, Ronen Grosman, Guy Lohman,\n  C. Mohan, Rene Muller, Hamid Pirahesh, Vijayshankar Raman, Richard Sidle,\n  Adam Storm, Yuanyuan Tian, Pinar Tozun, Yingjun Wu", "title": "WiSer: A Highly Available HTAP DBMS for IoT Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a classic transactional distributed database management system (DBMS),\nwrite transactions invariably synchronize with a coordinator before final\ncommitment. While enforcing serializability, this model has long been\ncriticized for not satisfying the applications' availability requirements. When\nentering the era of Internet of Things (IoT), this problem has become more\nsevere, as an increasing number of applications call for the capability of\nhybrid transactional and analytical processing (HTAP), where aggregation\nconstraints need to be enforced as part of transactions. Current systems work\naround this by creating escrows, allowing occasional overshoots of constraints,\nwhich are handled via compensating application logic.\n  The WiSer DBMS targets consistency with availability, by splitting the\ndatabase commit into two steps. First, a PROMISE step that corresponds to what\nhumans are used to as commitment, and runs without talking to a coordinator.\nSecond, a SERIALIZE step, that fixes transactions' positions in the\nserializable order, via a consensus procedure. We achieve this split via a\nnovel data representation that embeds read-sets into transaction deltas, and\nserialization sequence numbers into table rows. WiSer does no sharding (all\nnodes can run transactions that modify the entire database), and yet enforces\naggregation constraints. Both readwrite conflicts and aggregation constraint\nviolations are resolved lazily in the serialized data. WiSer also covers node\njoins and departures as database tables, thus simplifying correctness and\nfailure handling. We present the design of WiSer as well as experiments\nsuggesting this approach has promise.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 00:03:57 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Barber", "Ronald", ""], ["Garcia-Arellano", "Christian", ""], ["Grosman", "Ronen", ""], ["Lohman", "Guy", ""], ["Mohan", "C.", ""], ["Muller", "Rene", ""], ["Pirahesh", "Hamid", ""], ["Raman", "Vijayshankar", ""], ["Sidle", "Richard", ""], ["Storm", "Adam", ""], ["Tian", "Yuanyuan", ""], ["Tozun", "Pinar", ""], ["Wu", "Yingjun", ""]]}, {"id": "1908.02005", "submitter": "Honghui Mei Mr.", "authors": "Honghui Mei, Wei Chen, Yating Wei, Yuanzhe Hu, Shuyue Zhou, Bingru\n  Lin, Ying Zhao, Jiazhi Xia", "title": "RSATree: Distribution-Aware Data Representation of Large-Scale Tabular\n  Datasets for Flexible Visual Query", "comments": "VIS 2019 (InfoVis) accepted", "journal-ref": null, "doi": "10.1109/TVCG.2019.2934800", "report-no": null, "categories": "cs.DB cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysts commonly investigate the data distributions derived from statistical\naggregations of data that are represented by charts, such as histograms and\nbinned scatterplots, to visualize and analyze a large-scale dataset. Aggregate\nqueries are implicitly executed through such a process. Datasets are constantly\nextremely large; thus, the response time should be accelerated by calculating\npredefined data cubes. However, the queries are limited to the predefined\nbinning schema of preprocessed data cubes. Such limitation hinders analysts'\nflexible adjustment of visual specifications to investigate the implicit\npatterns in the data effectively. Particularly, RSATree enables arbitrary\nqueries and flexible binning strategies by leveraging three schemes, namely, an\nR-tree-based space partitioning scheme to catch the data distribution, a\nlocality-sensitive hashing technique to achieve locality-preserving random\naccess to data items, and a summed area table scheme to support interactive\nquery of aggregated values with a linear computational complexity. This study\npresents and implements a web-based visual query system that supports visual\nspecification, query, and exploration of large-scale tabular data with\nuser-adjustable granularities. We demonstrate the efficiency and utility of our\napproach by performing various experiments on real-world datasets and analyzing\ntime and space complexity.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 08:17:43 GMT"}, {"version": "v2", "created": "Fri, 11 Oct 2019 01:17:08 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Mei", "Honghui", ""], ["Chen", "Wei", ""], ["Wei", "Yating", ""], ["Hu", "Yuanzhe", ""], ["Zhou", "Shuyue", ""], ["Lin", "Bingru", ""], ["Zhao", "Ying", ""], ["Xia", "Jiazhi", ""]]}, {"id": "1908.02287", "submitter": "Visara Urovi", "authors": "Andine Havelange, Michel Dumontier, Birgit Wouters, Jona Linde, David\n  Townend, Arno Riedl and Visara Urovi", "title": "LUCE: A Blockchain Solution for monitoring data License accoUntability\n  and CompliancE", "comments": "14 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present our preliminary work on monitoring data License\naccoUntability and CompliancE (LUCE). LUCE is a blockchain platform solution\ndesigned to stimulate data sharing and reuse, by facilitating compliance with\nlicensing terms. The platform enables data accountability by recording the use\nof data and their purpose on a blockchain-supported platform. LUCE allows for\nindividual data to be rectified and erased. In doing so LUCE can ensure\nsubjects' General Data Protection Regulation's (GDPR) rights to access,\nrectification and erasure. Our contribution is to provide a distributed\nsolution for the automatic management of data accountability and their license\nterms.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 10:40:19 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Havelange", "Andine", ""], ["Dumontier", "Michel", ""], ["Wouters", "Birgit", ""], ["Linde", "Jona", ""], ["Townend", "David", ""], ["Riedl", "Arno", ""], ["Urovi", "Visara", ""]]}, {"id": "1908.03201", "submitter": "Vikram Ravindra", "authors": "Vikram Ravindra and Huda Nassar and David F. Gleich and Ananth Grama", "title": "Rigid Graph Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph databases have been the subject of significant research and\ndevelopment. Problems such as modularity, centrality, alignment, and clustering\nhave been formalized and solved in various application contexts. In this paper,\nwe focus on databases for applications in which graphs have a spatial basis,\nwhich we refer to as rigid graphs. Nodes in such graphs have preferred\npositions relative to their graph neighbors. Examples of such graphs include\nabstractions of large biomolecules, functional connectomes of the human brain,\nand mobile device/ sensor communication logs. When analyzing such networks it\nis important to consider edge lengths; e.g., when identifying conserved\npatterns through graph alignment, it is important for conserved edges to have\ncorrelated lengths, in addition to topological similarity. In contrast to a\nlarge body of work on topological graph alignment, rigid graph alignment\nsimultaneously aligns the network, as well as the underlying structure as\ncharacterized by edge lengths. We formulate the problem and present a\nmeta-algorithm based on expectation-maximization that alternately aligns the\nnetwork and the structure. We demonstrate that our meta-algorithm significantly\nimproves the quality of alignments in target applications, compared to\ntopological or structural aligners alone. We apply rigid graph alignment to\nfunctional brain networks derived from 20 subjects drawn from the Human\nConnectome Project (HCP) database, and show over a two-fold increase in quality\nof alignment over state of the art topological aligners. We evaluate the impact\nof various parameters associated with input datasets through a study on\nsynthetic graphs, where we fully characterize the performance of our method.\nOur results are broadly applicable to other applications and abstracted\nnetworks that can be embedded in metric spaces -- e.g., through spectral\nembeddings.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 02:24:29 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Ravindra", "Vikram", ""], ["Nassar", "Huda", ""], ["Gleich", "David F.", ""], ["Grama", "Ananth", ""]]}, {"id": "1908.03206", "submitter": "Sebastian Frischbier", "authors": "Sebastian Frischbier, Mario Paic, Alexander Echler, Christian Roth", "title": "Managing the Complexity of Processing Financial Data at Scale -- an\n  Experience Report", "comments": "12 pages, 2 figures, to be published in the proceedings of the 10th\n  Complex Systems Design & Management conference (CSD&M'19) by Springer", "journal-ref": null, "doi": "10.1007/978-3-030-34843-4_2", "report-no": null, "categories": "cs.DB cs.DC q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Financial markets are extremely data-driven and regulated. Participants rely\non notifications about significant events and background information that meet\ntheir requirements regarding timeliness, accuracy, and completeness. As one of\nEurope's leading providers of financial data and regulatory solutions vwd\nprocesses a daily average of 18 billion notifications from 500+ data sources\nfor 30 million symbols. Our large-scale geo-distributed systems handle daily\npeak rates of 1+ million notifications/sec. In this paper we give practical\ninsights about the different types of complexity we face regarding the data we\nprocess, the systems we operate, and the regulatory constraints we must comply\nwith. We describe the volume, variety, velocity, and veracity of the data we\nprocess, the infrastructure we operate, and the architecture we apply. We\nillustrate the load patterns created by trading and how the markets' attention\nto the Brexit vote and similar events stressed our systems.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 21:27:35 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Frischbier", "Sebastian", ""], ["Paic", "Mario", ""], ["Echler", "Alexander", ""], ["Roth", "Christian", ""]]}, {"id": "1908.03715", "submitter": "Zhili Chen Prof.", "authors": "Zhili Chen, Xiaoli Kan, Shun Zhang, Lin Chen, Yan Xu, Hong Zhong", "title": "Differentially Private Aggregated Mobility Data Publication Using Moving\n  Characteristics", "comments": "13 pages,12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of GPS enabled devices (smartphones) and\nlocation-based applications, location privacy is increasingly concerned.\nIntuitively, it is widely believed that location privacy can be preserved by\npublishing aggregated mobility data, such as the number of users in an area at\nsome time. However, a recent attack shows that these aggregated mobility data\ncan be exploited to recover individual trajectories. In this paper, we first\npropose two differentially private basic schemes for aggregated mobility data\npublication, namely direct perturbation and threshold perturbation, which\npreserve location privacy of users and especially resist the trajectory\nrecovery attack. Then, we explore the moving characteristics of mobile users,\nand design an improved scheme named static hybrid perturbation by combining the\ntwo basic schemes according to the moving characteristics. Since static hybrid\nperturbation works only for static data, which are entirely available before\npublishing, we further adapt the static hybrid perturbation by combining it\nwith linear regression, and yield another improved scheme named dynamic hybrid\nperturbation. The dynamic hybrid perturbation works also for dynamic data,\nwhich are generated on the fly during publication. Privacy analysis shows that\nthe proposed schemes achieve differential privacy. Extensive experiments on\nboth simulated and real datasets demonstrate that all proposed schemes resist\nthe trajectory recovery attack well, and the improved schemes significantly\noutperform the basic schemes.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2019 09:19:44 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Chen", "Zhili", ""], ["Kan", "Xiaoli", ""], ["Zhang", "Shun", ""], ["Chen", "Lin", ""], ["Xu", "Yan", ""], ["Zhong", "Hong", ""]]}, {"id": "1908.03830", "submitter": "Kiran Byadarhaly", "authors": "Harish Kashyap K, Kiran Byadarhaly and Saumya Shah", "title": "Supervised Negative Binomial Classifier for Probabilistic Record Linkage", "comments": null, "journal-ref": null, "doi": null, "report-no": "03a", "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the need of the linking records across various databases, we\npropose a novel graphical model based classifier that uses a mixture of Poisson\ndistributions with latent variables. The idea is to derive insight into each\npair of hypothesis records that match by inferring its underlying latent rate\nof error using Bayesian Modeling techniques. The novel approach of using gamma\npriors for learning the latent variables along with supervised labels is unique\nand allows for active learning. The naive assumption is made deliberately as to\nthe independence of the fields to propose a generalized theory for this class\nof problems and not to undermine the hierarchical dependencies that could be\npresent in different scenarios. This classifier is able to work with sparse and\nstreaming data. The application to record linkage is able to meet several\nchallenges of sparsity, data streams and varying nature of the data-sets.\n", "versions": [{"version": "v1", "created": "Sun, 11 Aug 2019 00:25:13 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["K", "Harish Kashyap", ""], ["Byadarhaly", "Kiran", ""], ["Shah", "Saumya", ""]]}, {"id": "1908.04083", "submitter": "Dominique Li", "authors": "Rui Liu and Dominique Li", "title": "An Efficient Skyline Computation Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skyline computation aims at looking for the set of tuples that are not worse\nthan any other tuples in all dimensions from a multidimensional database. In\nthis paper, we present SDI (Skyline on Dimension Index), a dimension indexing\nconducted general framework to skyline computation. We prove that to determine\nwhether a tuple belongs to the skyline, it is enough to compare this tuple with\na bounded subset of skyline tuples in an arbitrary dimensional index, but not\nwith all existing skyline tuples. Base on SDI, we also show that any skyline\ntuple can be used to stop the whole skyline computation process with outputting\nthe complete set of all skyline tuples. We develop an efficient algorithm\nSDI-RS that significantly reduces the skyline computation time, of which the\nspace and time complexity can be guaranteed. Our experimental evaluation shows\nthat SDI-RS outperforms the baseline algorithms in general and is especially\nvery efficient on high-dimensional data.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 10:57:01 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Liu", "Rui", ""], ["Li", "Dominique", ""]]}, {"id": "1908.04464", "submitter": "Selasi Kwashie", "authors": "Jixue Liu, Selasi Kwashie, Jiuyong Li, Lin Liu and Michael Bewong", "title": "Linking Graph Entities with Multiplicity and Provenance", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity linking and resolution is a fundamental database problem with\napplications in data integration, data cleansing, information retrieval,\nknowledge fusion, and knowledge-base population. It is the task of accurately\nidentifying multiple, differing, and possibly contradicting representations of\nthe same real-world entity in data. In this work, we propose an entity linking\nand resolution system capable of linking entities across different databases\nand mentioned-entities extracted from text data. Our entity linking/resolution\nsolution, called Certus, uses a graph model to represent the profiles of\nentities. The graph model is versatile, thus, it is capable of handling\nmultiple values for an attribute or a relationship, as well as the provenance\ndescriptions of the values. Provenance descriptions of a value provide the\nsettings of the value, such as validity periods, sources, security\nrequirements, etc. This paper presents the architecture for the entity linking\nsystem, the logical, physical, and indexing models used in the system, and the\ngeneral linking process. Furthermore, we demonstrate the performance of update\noperations of the physical storage models when the system is implemented in two\nstate-of-the-art database management systems, HBase and Postgres.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 02:28:37 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 05:20:51 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Liu", "Jixue", ""], ["Kwashie", "Selasi", ""], ["Li", "Jiuyong", ""], ["Liu", "Lin", ""], ["Bewong", "Michael", ""]]}, {"id": "1908.04509", "submitter": "Constantin Enea", "authors": "Ranadeep Biswas and Constantin Enea", "title": "On the Complexity of Checking Transactional Consistency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transactions simplify concurrent programming by enabling computations on\nshared data that are isolated from other concurrent computations and are\nresilient to failures. Modern databases provide different consistency models\nfor transactions corresponding to different tradeoffs between consistency and\navailability. In this work, we investigate the problem of checking whether a\ngiven execution of a transactional database adheres to some consistency model.\nWe show that consistency models like read committed, read atomic, and causal\nconsistency are polynomial time checkable while prefix consistency and snapshot\nisolation are NP-complete in general. These results complement a previous\nNP-completeness result concerning serializability. Moreover, in the context of\nNP-complete consistency models, we devise algorithms that are polynomial time\nassuming that certain parameters in the input executions, e.g., the number of\nsessions, are fixed. We evaluate the scalability of these algorithms in the\ncontext of several production databases.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 06:19:26 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Biswas", "Ranadeep", ""], ["Enea", "Constantin", ""]]}, {"id": "1908.04517", "submitter": "Zhi-Hong Deng", "authors": "Zhi-Hong Deng", "title": "Beyond the Inverted Index", "comments": "9 pages, 7 figures, and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new data structure named group-list is proposed. The\ngroup-list is as simple as the inverted index. However, the group-list divides\ndocument identifiers in an inverted index into groups, which makes it more\nefficient when it is used to perform the intersection or union operation on\ndocument identifiers. The experimental results on a synthetic dataset show that\nthe group-list outperforms the inverted index.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 07:14:01 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Deng", "Zhi-Hong", ""]]}, {"id": "1908.04718", "submitter": "Utku Sirin", "authors": "Utku Sirin, Anastasia Ailamaki", "title": "Micro-architectural Analysis of OLAP: Limitations and Opportunities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding micro-architectural behavior is profound in efficiently using\nhardware resources. Recent work has shown that, despite being aggressively\noptimized for modern hardware, in-memory online transaction processing (OLTP)\nsystems severely underutilize their core micro-architecture resources [25].\nOnline analytical processing (OLAP) workloads, on the other hand, exhibit a\ncompletely different computing pattern. OLAP workloads are read-only,\nbandwidth-intensive and include various data access patterns including both\nsequential and random data accesses. In addition, with the rise of\ncolumn-stores, they run on high performance engines that are tightly optimized\nfor the efficient use of modern hardware. Hence, the micro-architectural\nbehavior of modern OLAP systems remains unclear.\n  This work presents the micro-architectural analysis of a breadth of OLAP\nsystems. We examine CPU cycles and memory bandwidth utilization. The results\nshow that, unlike the traditional, commercial OLTP systems, traditional,\ncommercial OLAP systems do not suffer from instruction cache misses.\nNevertheless, they suffer from their large instruction footprint resulting in\nslow response times. High performance OLAP engines execute tight instruction\nstreams; however, they spend 25 to 82% of the CPU cycles on stalls regardless\nof the workload being sequential- or random-access-heavy. In addition, high\nperformance OLAP engines underutilize the multi-core CPU or memory bandwidth\nresources due to their disproportional compute and memory demands. Hence,\nanalytical processing engines should carefully assign their compute and memory\nresources for efficient multi-core micro-architectural utilization.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 16:18:00 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Sirin", "Utku", ""], ["Ailamaki", "Anastasia", ""]]}, {"id": "1908.04772", "submitter": "Fotis Savva", "authors": "Fotis Savva, Christos Anagnostopoulos, Peter Triantafillou", "title": "Adaptive Learning of Aggregate Analytics under Dynamic Workloads", "comments": "12 pages, 9 figures", "journal-ref": null, "doi": "10.1109/BigData47090.2019.9006267", "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large organizations have seamlessly incorporated data-driven decision making\nin their operations. However, as data volumes increase, expensive big data\ninfrastructures are called to rescue. In this setting, analytics tasks become\nvery costly in terms of query response time, resource consumption, and money in\ncloud deployments, especially when base data are stored across geographically\ndistributed data centers. Therefore, we introduce an adaptive Machine Learning\nmechanism which is light-weight, stored client-side, can estimate the answers\nof a variety of aggregate queries and can avoid the big data backend. The\nestimations are performed in milliseconds are inexpensive and accurate as the\nmechanism learns from past analytical-query patterns. However, as analytic\nqueries are ad-hoc and analysts' interests change over time we develop\nsolutions that can swiftly and accurately detect such changes and adapt to new\nquery patterns. The capabilities of our approach are demonstrated using\nextensive evaluation with real and synthetic datasets.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 17:32:39 GMT"}, {"version": "v2", "created": "Sat, 14 Mar 2020 12:01:17 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Savva", "Fotis", ""], ["Anagnostopoulos", "Christos", ""], ["Triantafillou", "Peter", ""]]}, {"id": "1908.05855", "submitter": "Masatoshi Hanai", "authors": "Masatoshi Hanai, Toyotaro Suzumura, Wen Jun Tan, Elvis Liu, Georgios\n  Theodoropoulos and Wentong Cai", "title": "Distributed Edge Partitioning for Trillion-edge Graphs", "comments": "VLDB 2020, Code in http://www.masahanai.jp/DistributedNE/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Distributed Neighbor Expansion (Distributed NE), a parallel and\ndistributed graph partitioning method that can scale to trillion-edge graphs\nwhile providing high partitioning quality. Distributed NE is based on a new\nheuristic, called parallel expansion, where each partition is constructed in\nparallel by greedily expanding its edge set from a single vertex in such a way\nthat the increase of the vertex cuts becomes local minimal. We theoretically\nprove that the proposed method has the upper bound in the partitioning quality.\nThe empirical evaluation with various graphs shows that the proposed method\nproduces higher-quality partitions than the state-of-the-art distributed graph\npartitioning algorithms. The performance evaluation shows that the space\nefficiency of the proposed method is an order-of-magnitude better than the\nexisting algorithms, keeping its time efficiency comparable. As a result,\nDistributed NE can partition a trillion-edge graph using only 256 machines\nwithin 70 minutes.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 05:52:19 GMT"}, {"version": "v2", "created": "Sat, 21 Sep 2019 14:18:04 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Hanai", "Masatoshi", ""], ["Suzumura", "Toyotaro", ""], ["Tan", "Wen Jun", ""], ["Liu", "Elvis", ""], ["Theodoropoulos", "Georgios", ""], ["Cai", "Wentong", ""]]}, {"id": "1908.06049", "submitter": "Renzhi Wu", "authors": "Renzhi Wu, Sanya Chaba, Saurabh Sawlani, Xu Chu, Saravanan\n  Thirumuruganathan", "title": "ZeroER: Entity Resolution using Zero Labeled Examples", "comments": "Published at 2020 ACM SIGMOD International Conference on Management\n  of Data", "journal-ref": null, "doi": "10.1145/3318464.3389743", "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity resolution (ER) refers to the problem of matching records in one or\nmore relations that refer to the same real-world entity. While supervised\nmachine learning (ML) approaches achieve the state-of-the-art results, they\nrequire a large amount of labeled examples that are expensive to obtain and\noften times infeasible. We investigate an important problem that vexes\npractitioners: is it possible to design an effective algorithm for ER that\nrequires Zero labeled examples, yet can achieve performance comparable to\nsupervised approaches? In this paper, we answer in the affirmative through our\nproposed approach dubbed ZeroER. Our approach is based on a simple observation\n-- the similarity vectors for matches should look different from that of\nunmatches. Operationalizing this insight requires a number of technical\ninnovations. First, we propose a simple yet powerful generative model based on\nGaussian Mixture Models for learning the match and unmatch distributions.\nSecond, we propose an adaptive regularization technique customized for ER that\nameliorates the issue of feature overfitting. Finally, we incorporate the\ntransitivity property into the generative model in a novel way resulting in\nimproved accuracy. On five benchmark ER datasets, we show that ZeroER greatly\noutperforms existing unsupervised approaches and achieves comparable\nperformance to supervised approaches.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 16:30:05 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 08:34:54 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Wu", "Renzhi", ""], ["Chaba", "Sanya", ""], ["Sawlani", "Saurabh", ""], ["Chu", "Xu", ""], ["Thirumuruganathan", "Saravanan", ""]]}, {"id": "1908.06265", "submitter": "Harsh Thakkar", "authors": "Harsh Thakkar, Dharmen Punjani, Soeren Auer, Maria-Esther Vidal", "title": "Towards an Integrated Graph Algebra for Graph Pattern Matching with\n  Gremlin (Extended Version)", "comments": "This is an extended version of an article formally published at DEXA\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Graph data management (also called NoSQL) has revealed beneficial\ncharacteristics in terms of flexibility and scalability by differently\nbalancing between query expressivity and schema flexibility. This peculiar\nadvantage has resulted into an unforeseen race of developing new task-specific\ngraph systems, query languages and data models, such as property graphs,\nkey-value, wide column, resource description framework (RDF), etc. Present-day\ngraph query languages are focused towards flexible graph pattern matching (aka\nsub-graph matching), whereas graph computing frameworks aim towards providing\nfast parallel (distributed) execution of instructions. The consequence of this\nrapid growth in the variety of graph-based data management systems has resulted\nin a lack of standardization. Gremlin, a graph traversal language, and machine\nprovides a common platform for supporting any graph computing system (such as\nan OLTP graph database or OLAP graph processors). We present a formalization of\ngraph pattern matching for Gremlin queries. We also study, discuss and\nconsolidate various existing graph algebra operators into an integrated graph\nalgebra.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2019 09:04:02 GMT"}, {"version": "v2", "created": "Sat, 7 Sep 2019 08:18:50 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Thakkar", "Harsh", ""], ["Punjani", "Dharmen", ""], ["Auer", "Soeren", ""], ["Vidal", "Maria-Esther", ""]]}, {"id": "1908.06309", "submitter": "Felix Neutatz", "authors": "Felix Neutatz and Mohammad Mahdavi and Ziawasch Abedjan", "title": "ED2: Two-stage Active Learning for Error Detection -- Technical Report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional error detection approaches require user-defined parameters and\nrules. Thus, the user has to know both the error detection system and the data.\nHowever, we can also formulate error detection as a semi-supervised\nclassification problem that only requires domain expertise. The challenges for\nsuch an approach are twofold: (1) to represent the data in a way that enables a\nclassification model to identify various kinds of data errors, and (2) to pick\nthe most promising data values for learning. In this paper, we address these\nchallenges with ED2, our new example-driven error detection method. First, we\npresent a new two-dimensional multi-classifier sampling strategy for active\nlearning. Second, we propose novel multi-column features. The combined\napplication of these techniques provides fast convergence of the classification\ntask with high detection accuracy. On several real-world datasets, ED2\nrequires, on average, less than 1% labels to outperform existing error\ndetection approaches. This report extends the peer-reviewed paper \"ED2: A Case\nfor Active Learning in Error Detection\". All source code related to this\nproject is available on GitHub.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2019 15:13:12 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Neutatz", "Felix", ""], ["Mahdavi", "Mohammad", ""], ["Abedjan", "Ziawasch", ""]]}, {"id": "1908.06553", "submitter": "Zjian Ding", "authors": "Zijian Ding, Shan Qiu, Yutong Guo, Jianping Lin, Li Sun, Dapeng Fu,\n  Zhen Yang, Chengquan Li, Yang Yu, Long Meng, Tingting Lv, Dan Li and Ping\n  Zhang", "title": "LabelECG: A Web-based Tool for Distributed Electrocardiogram Annotation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electrocardiography plays an essential role in diagnosing and screening\ncardiovascular diseases in daily healthcare. Deep neural networks have shown\nthe potentials to improve the accuracies of arrhythmia detection based on\nelectrocardiograms (ECGs). However, more ECG records with ground truth are\nneeded to promote the development and progression of deep learning techniques\nin automatic ECG analysis. Here we propose a web-based tool for ECG viewing and\nannotating, LabelECG. With the facilitation of unified data management,\nLabelECG is able to distribute large cohorts of ECGs to dozens of technicians\nand physicians, who can simultaneously make annotations through web-browsers on\nPCs, tablets and cell phones. Along with the doctors from four hospitals in\nChina, we applied LabelECG to support the annotations of about 15,000 12-lead\nresting ECG records in three months. These annotated ECGs have successfully\nsupported the First China ECG intelligent Competition. La-belECG will be freely\naccessible on the Internet to support similar researches, and will also be\nupgraded through future works.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 01:36:52 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Ding", "Zijian", ""], ["Qiu", "Shan", ""], ["Guo", "Yutong", ""], ["Lin", "Jianping", ""], ["Sun", "Li", ""], ["Fu", "Dapeng", ""], ["Yang", "Zhen", ""], ["Li", "Chengquan", ""], ["Yu", "Yang", ""], ["Meng", "Long", ""], ["Lv", "Tingting", ""], ["Li", "Dan", ""], ["Zhang", "Ping", ""]]}, {"id": "1908.06719", "submitter": "Phanwadee Sinthong", "authors": "Phanwadee Sinthong, Michael J. Carey", "title": "AFrame: Extending DataFrames for Large-Scale Modern Data Analysis\n  (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing the increasingly large volumes of data that are available today,\npossibly including the application of custom machine learning models, requires\nthe utilization of distributed frameworks. This can result in serious\nproductivity issues for \"normal\" data scientists. This paper introduces AFrame,\na new scalable data analysis package powered by a Big Data management system\nthat extends the data scientists' familiar DataFrame operations to efficiently\noperate on managed data at scale. AFrame is implemented as a layer on top of\nApache AsterixDB, transparently scaling out the execution of DataFrame\noperations and machine learning model invocation through a parallel,\nshared-nothing big data management system. AFrame incrementally constructs\nSQL++ queries and leverages AsterixDB's semistructured data management\nfacilities, user-defined function support, and live data ingestion support. In\norder to evaluate the proposed approach, this paper also introduces an\nextensible micro-benchmark for use in evaluating DataFrame performance in both\nsingle-node and distributed settings via a collection of representative\nanalytic operations. This paper presents the architecture of AFrame, describes\nthe underlying capabilities of AsterixDB that efficiently support modern data\nanalytic operations, and utilizes the proposed benchmark to evaluate and\ncompare the performance and support for large-scale data analyses provided by\nalternative DataFrame libraries.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 12:00:57 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Sinthong", "Phanwadee", ""], ["Carey", "Michael J.", ""]]}, {"id": "1908.06729", "submitter": "Hongzhi Wang", "authors": "Xi Chen, Hongzhi Wang, Yanjie Wei, Jianzhong Li and Hong Gao", "title": "Autoregressive-Model-Based Methods for Online Time Series Prediction\n  with Missing Values: an Experimental Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series prediction with missing values is an important problem of time\nseries analysis since complete data is usually hard to obtain in many\nreal-world applications. To model the generation of time series, autoregressive\n(AR) model is a basic and widely used one, which assumes that each observation\nin the time series is a noisy linear combination of some previous observations\nalong with a constant shift. To tackle the problem of prediction with missing\nvalues, a number of methods were proposed based on various data models. For\nreal application scenarios, how do these methods perform over different types\nof time series with different levels of data missing remains to be\ninvestigated. In this paper, we focus on online methods for AR-model-based time\nseries prediction with missing values. We adapted five mainstream methods to\nfit in such a scenario. We make detailed discussion on each of them by\nintroducing their core ideas about how to estimate the AR coefficients and\ntheir different strategies to deal with missing values. We also present\nalgorithmic implementations for better understanding. In order to\ncomprehensively evaluate these methods and do the comparison, we conduct\nexperiments with various configurations of relative parameters over both\nsynthetic and real data. From the experimental results, we derived several\nnoteworthy conclusions and shows that imputation is a simple but reliable\nstrategy to handle missing values in online prediction tasks.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2019 13:58:54 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 01:59:09 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Chen", "Xi", ""], ["Wang", "Hongzhi", ""], ["Wei", "Yanjie", ""], ["Li", "Jianzhong", ""], ["Gao", "Hong", ""]]}, {"id": "1908.06801", "submitter": "Yoshitaka Kameya", "authors": "Yoshitaka Kameya", "title": "Towards Efficient Discriminative Pattern Mining in Hybrid Domains", "comments": "This paper is an English version of the paper originally presented in\n  the 17th Forum on Information Technology (FIT 2018), a Japanese domestic\n  conference held during September 19-21, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discriminative pattern mining is a data mining task in which we find patterns\nthat distinguish transactions in the class of interest from those in other\nclasses, and is also called emerging pattern mining or subgroup discovery. One\npractical problem in discriminative pattern mining is how to handle numeric\nvalues in the input dataset. In this paper, we propose an algorithm for\ndiscriminative pattern mining that can deal with a transactional dataset in a\nhybrid domain, i.e. the one that includes both symbolic and numeric values. We\nalso show the execution results of a prototype implementation of the proposed\nalgorithm for two standard benchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 13:28:50 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Kameya", "Yoshitaka", ""]]}, {"id": "1908.07093", "submitter": "Antoine Amarilli", "authors": "Antoine Amarilli and Benny Kimelfeld", "title": "Uniform Reliability of Self-Join-Free Conjunctive Queries", "comments": "27 pages including 17 pages of main text. Integrates all reviewer\n  feedback. Outside of some minor formatting differences and tweaks, this paper\n  is the same as the ICDT'21 paper with the addition of 10 pages of technical\n  appendix", "journal-ref": null, "doi": "10.4230/LIPIcs.ICDT.2021.17", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The reliability of a Boolean Conjunctive Query (CQ) over a tuple-independent\nprobabilistic database is the probability that the CQ is satisfied when the\ntuples of the database are sampled one by one, independently, with their\nassociated probability. For queries without self-joins (repeated relation\nsymbols), the data complexity of this problem is fully characterized in a known\ndichotomy: reliability can be computed in polynomial time for hierarchical\nqueries, and is #P-hard for non-hierarchical queries. Hierarchical queries also\ncharacterize the tractability of queries for other tasks: having read-once\nlineage formulas, supporting insertion/deletion updates to the database in\nconstant time, and having a tractable computation of tuples' Shapley and\nBanzhaf values.\n  In this work, we investigate a fundamental counting problem for CQs without\nself-joins: how many sets of facts from the input database satisfy the query?\nThis is equivalent to the uniform case of the query reliability problem, where\nthe probability of every tuple is required to be 1/2. Of course, for\nhierarchical queries, uniform reliability is in polynomial time, like the\nreliability problem. However, it is an open question whether being hierarchical\nis necessary for the uniform reliability problem to be in polynomial time. In\nfact, the complexity of the problem has been unknown even for the simplest\nnon-hierarchical CQs without self-joins.\n  We solve this open question by showing that uniform reliability is\n#P-complete for every non-hierarchical CQ without self-joins. Hence, we\nestablish that being hierarchical also characterizes the tractability of\nunweighted counting of the satisfying tuple subsets. We also consider the\ngeneralization to query reliability where all tuples of the same relation have\nthe same probability, and give preliminary results on the complexity of this\nproblem.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 22:51:40 GMT"}, {"version": "v2", "created": "Tue, 24 Sep 2019 09:57:09 GMT"}, {"version": "v3", "created": "Thu, 20 Feb 2020 16:45:44 GMT"}, {"version": "v4", "created": "Fri, 25 Sep 2020 08:04:02 GMT"}, {"version": "v5", "created": "Wed, 28 Jul 2021 19:50:18 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Amarilli", "Antoine", ""], ["Kimelfeld", "Benny", ""]]}, {"id": "1908.07431", "submitter": "Lucas Lersch", "authors": "Ismail Oukid and Lucas Lersch", "title": "On the Diversity of Memory and Storage Technologies", "comments": "This is a post-peer-review, pre-copyedit version of an article\n  published in Datenbank-Spektrum. The final authenticated version is available\n  online at: http://dx.doi.org/10.1007/s13222-018-0287-8", "journal-ref": "Datenbank-Spektrum, Volume 18, Issue 2, 2018, Pages 121-127", "doi": "10.1007/s13222-018-0287-8", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last decade has seen tremendous developments in memory and storage\ntechnologies, starting with Flash Memory and continuing with the upcoming\nStorage-Class Memories. Combined with an explosion of data processing, data\nanalytics, and machine learning, this led to a segmentation of the memory and\nstorage market. Consequently, the traditional storage hierarchy, as we know it\ntoday, might be replaced by a multitude of storage hierarchies, with\npotentially different depths, each tailored for specific workloads. In this\ncontext, we explore in this \"Kurz Erkl\\\"art\" the state of memory technologies\nand reflect on their future use with a focus on data management systems.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 15:28:26 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Oukid", "Ismail", ""], ["Lersch", "Lucas", ""]]}, {"id": "1908.07517", "submitter": "Zhongwei Cheng", "authors": "Yuan Liu, Zhongwei Cheng, Jie Liu, Bourhan Yassin, Zhe Nan, Jiebo Luo", "title": "AI for Earth: Rainforest Conservation by Acoustic Surveillance", "comments": "Accepted to KDD2019 Workshop on Data Mining and AI for Conservation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.DB cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Saving rainforests is a key to halting adverse climate changes. In this\npaper, we introduce an innovative solution built on acoustic surveillance and\nmachine learning technologies to help rainforest conservation. In particular,\nWe propose new convolutional neural network (CNN) models for environmental\nsound classification and achieved promising preliminary results on two\ndatasets, including a public audio dataset and our real rainforest sound\ndataset. The proposed audio classification models can be easily extended in an\nautomated machine learning paradigm and integrated in cloud-based services for\nreal world deployment.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 03:50:57 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Liu", "Yuan", ""], ["Cheng", "Zhongwei", ""], ["Liu", "Jie", ""], ["Yassin", "Bourhan", ""], ["Nan", "Zhe", ""], ["Luo", "Jiebo", ""]]}, {"id": "1908.07723", "submitter": "Rojeh Hayek", "authors": "Rojeh Hayek and Oded Shmueli", "title": "Improved Cardinality Estimation by Learning Queries Containment Rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The containment rate of query Q1 in query Q2 over database D is the\npercentage of Q1's result tuples over D that are also in Q2's result over D. We\ndirectly estimate containment rates between pairs of queries over a specific\ndatabase. For this, we use a specialized deep learning scheme, CRN, which is\ntailored to representing pairs of SQL queries. Result-cardinality estimation is\na core component of query optimization. We describe a novel approach for\nestimating queries result-cardinalities using estimated containment rates among\nqueries. This containment rate estimation may rely on CRN or embed, unchanged,\nknown cardinality estimation methods. Experimentally, our novel approach for\nestimating cardinalities, using containment rates between queries, on a\nchallenging real-world database, realizes significant improvements to state of\nthe art cardinality estimation methods.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 07:07:27 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Hayek", "Rojeh", ""], ["Shmueli", "Oded", ""]]}, {"id": "1908.07753", "submitter": "Christian Winter", "authors": "Christian Winter (1), Andreas Kipf (2), Christoph Anneser (1), Eleni\n  Tzirita Zacharatou (3), Thomas Neumann (1), Alfons Kemper (1) ((1) Technische\n  Universit\\\"at M\\\"unchen, (2) MIT CSAIL, (3) Technische Universit\\\"at Berlin)", "title": "GeoBlocks: A Query-Cache Accelerated Data Structure for Spatial\n  Aggregation over Polygons", "comments": "Accepted at EDBT 2021, please cite the EDBT version", "journal-ref": null, "doi": "10.5441/002/edbt.2021.16", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As individual traffic and public transport in cities are changing, city\nauthorities need to analyze urban geospatial data to improve transportation and\ninfrastructure. To that end, they highly rely on spatial aggregation queries\nthat extract summarized information from point data (e.g., Uber rides)\ncontained in a given polygonal region (e.g., a city neighborhood). To support\nsuch queries, current analysis tools either allow only predefined aggregates on\npredefined regions and are thus unsuitable for exploratory analyses, or access\nthe raw data to compute aggregate results on-the-fly, which severely limits the\ninteractivity. At the same time, existing pre-aggregation techniques are\ninadequate since they maintain aggregates over rectangular regions. As a\nresult, when applied over arbitrary polygonal regions, they induce an\napproximation error that cannot be bounded. In this paper, we introduce\nGeoBlocks, a novel pre-aggregating data structure that supports spatial\naggregation over arbitrary polygons. GeoBlocks closely approximate polygons\nusing a set of fine-grained grid cells and, in contrast to prior work, allow to\nbound the approximation error by adjusting the cell size. Furthermore,\nGeoBlocks employ a trie-like cache that caches aggregate results of frequently\nqueried regions, thereby dynamically adapting to the skew inherently present in\nquery workloads and improving performance over time. In summary, GeoBlocks\noutperform on-the-fly aggregation by up to three orders of magnitude, achieving\nthe sub-second query latencies required for interactive exploratory analytics.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 08:58:26 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 15:41:25 GMT"}, {"version": "v3", "created": "Tue, 16 Mar 2021 13:15:10 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Winter", "Christian", ""], ["Kipf", "Andreas", ""], ["Anneser", "Christoph", ""], ["Zacharatou", "Eleni Tzirita", ""], ["Neumann", "Thomas", ""], ["Kemper", "Alfons", ""]]}, {"id": "1908.07918", "submitter": "Sudhakar Singh", "authors": "Sudhakar Singh, Pankaj Singh, Rakhi Garg, P. K. Mishra", "title": "Mining Association Rules in Various Computing Environments: A Survey", "comments": "14 pages", "journal-ref": "International Journal of Applied Engineering Research 2016; 11(8):\n  5629-5640", "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Association Rule Mining (ARM) is one of the well know and most researched\ntechnique of data mining. There are so many ARM algorithms have been designed\nthat their counting is a large number. In this paper we have surveyed the\nvarious ARM algorithms in four computing environments. The considered computing\nenvironments are sequential computing, parallel and distributed computing, grid\ncomputing and cloud computing. With the emergence of new computing paradigm,\nARM algorithms have been designed by many researchers to improve the efficiency\nby utilizing the new paradigm. This paper represents the journey of ARM\nalgorithms started from sequential algorithms, and through parallel and\ndistributed, and grid based algorithms to the current state-of-the-art, along\nwith the motives for adopting new machinery.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 11:13:50 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Singh", "Sudhakar", ""], ["Singh", "Pankaj", ""], ["Garg", "Rakhi", ""], ["Mishra", "P. K.", ""]]}, {"id": "1908.07924", "submitter": "Babak Salimi", "authors": "Babak Salimi, Bill Howe, Dan Suciu", "title": "Data Management for Causal Algorithmic Fairness", "comments": "arXiv admin note: text overlap with arXiv:1902.08283", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fairness is increasingly recognized as a critical component of machine\nlearning systems. However, it is the underlying data on which these systems are\ntrained that often reflects discrimination, suggesting a data management\nproblem. In this paper, we first make a distinction between associational and\ncausal definitions of fairness in the literature and argue that the concept of\nfairness requires causal reasoning. We then review existing works and identify\nfuture opportunities for applying data management techniques to causal\nalgorithmic fairness.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 17:23:00 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 05:24:37 GMT"}, {"version": "v3", "created": "Tue, 1 Oct 2019 02:07:32 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Salimi", "Babak", ""], ["Howe", "Bill", ""], ["Suciu", "Dan", ""]]}, {"id": "1908.08341", "submitter": "Joris Nix", "authors": "Jens Dittrich and Joris Nix", "title": "The Case for Deep Query Optimisation", "comments": "camera ready version for CIDR 2020, renamed Algorithmic Views to\n  Materialised Algorithmic Views, added meta-relational plan properties,\n  revised and extended Materialised Algorithmic Views section, added System\n  Integration section, revised Experiments section, added tables 2, 3, and 5,\n  added figure 4, added references, added appendix, corrected typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query Optimisation (QO) is the most important optimisation problem in\ndatabases. The goal of QO is to compute the best physical plan under a given\ncost model. In that process, physical operators are used as building blocks for\nthe planning and optimisation process. In this paper, we propose to deepen that\nprocess. We present Deep Query Optimisation (DQO). In DQO, we break up the\nabstraction of a 'physical' operator to consider more fine-granular\nsubcomponents. These subcomponents are then used to enumerate (sub-)plans both\noffline and at query time. This idea triggers several exciting research\ndirections: (1) How exactly can DQO help to compute better plans than (shallow)\nQO and at which costs? (2) DQO can be used to precompute and synthesise\ndatabase operators and any other database component as Materialised Algorithmic\nViews (MAVs). (3) We identify the Algorithmic View Selection Problem (AVSP),\ni.e. which MAVs should be materialised when?\n  This paper presents the high-level idea of DQO using an analogy inspired from\nbiology. Then we proceed to question the terms 'physical' and 'physical\noperator'. We present experiments with a 'physical operator' formerly known as\n'hash-based grouping'. We benchmark that operator both independently as well as\nin the context of DQO-enabled dynamic programming. We conclude by sketching a\nDQO research agenda.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 12:54:33 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2019 11:24:38 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Dittrich", "Jens", ""], ["Nix", "Joris", ""]]}, {"id": "1908.08368", "submitter": "Hongzhi Wang", "authors": "Hongzhi Wang, Yijie Yang and Yang Song", "title": "A General Data Renewal Model for Prediction Algorithms in Industrial\n  Data Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In industrial data analytics, one of the fundamental problems is to utilize\nthe temporal correlation of the industrial data to make timely predictions in\nthe production process, such as fault prediction and yield prediction. However,\nthe traditional prediction models are fixed while the conditions of the\nmachines change over time, thus making the errors of predictions increase with\nthe lapse of time. In this paper, we propose a general data renewal model to\ndeal with it. Combined with the similarity function and the loss function, it\nestimates the time of updating the existing prediction model, then updates it\naccording to the evaluation function iteratively and adaptively. We have\napplied the data renewal model to two prediction algorithms. The experiments\ndemonstrate that the data renewal model can effectively identify the changes of\ndata, update and optimize the prediction model so as to improve the accuracy of\nprediction.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 13:38:22 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Wang", "Hongzhi", ""], ["Yang", "Yijie", ""], ["Song", "Yang", ""]]}, {"id": "1908.08433", "submitter": "Deng-Ping Fan", "authors": "Deng-Ping Fan, ShengChuan Zhang, Yu-Huan Wu, Yun Liu, Ming-Ming Cheng,\n  Bo Ren, Paul L. Rosin, Rongrong Ji", "title": "Scoot: A Perceptual Metric for Facial Sketches", "comments": "Code & dataset:http://mmcheng.net/scoot/, 11 pages, ICCV 2019, First\n  one good evaluation metric for facial sketh that consistent with human\n  judgment. arXiv admin note: text overlap with arXiv:1804.02975", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human visual system has the strong ability to quick assess the perceptual\nsimilarity between two facial sketches. However, existing two widely-used\nfacial sketch metrics, e.g., FSIM and SSIM fail to address this perceptual\nsimilarity in this field. Recent study in facial modeling area has verified\nthat the inclusion of both structure and texture has a significant positive\nbenefit for face sketch synthesis (FSS). But which statistics are more\nimportant, and are helpful for their success? In this paper, we design a\nperceptual metric,called Structure Co-Occurrence Texture (Scoot), which\nsimultaneously considers the block-level spatial structure and co-occurrence\ntexture statistics. To test the quality of metrics, we propose three novel\nmeta-measures based on various reliable properties. Extensive experiments\ndemonstrate that our Scoot metric exceeds the performance of prior work.\nBesides, we built the first large scale (152k judgments) human-perception-based\nsketch database that can evaluate how well a metric is consistent with human\nperception. Our results suggest that \"spatial structure\" and \"co-occurrence\ntexture\" are two generally applicable perceptual features in face sketch\nsynthesis.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 11:55:47 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 06:06:31 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Fan", "Deng-Ping", ""], ["Zhang", "ShengChuan", ""], ["Wu", "Yu-Huan", ""], ["Liu", "Yun", ""], ["Cheng", "Ming-Ming", ""], ["Ren", "Bo", ""], ["Rosin", "Paul L.", ""], ["Ji", "Rongrong", ""]]}, {"id": "1908.08654", "submitter": "Weilong Ren", "authors": "Weilong Ren, Xiang Lian, and Kambiz Ghazinour", "title": "Efficient Join Processing Over Incomplete Data Streams (Technical\n  Report)", "comments": "11 pages, 11 figures, accepted conference paper for CIKM19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For decades, the join operator over fast data streams has always drawn much\nattention from the database community, due to its wide spectrum of real-world\napplications, such as online clustering, intrusion detection, sensor data\nmonitoring, and so on. Existing works usually assume that the underlying\nstreams to be joined are complete (without any missing values). However, this\nassumption may not always hold, since objects from streams may contain some\nmissing attributes, due to various reasons such as packet losses, network\ncongestion/failure, and so on. In this paper, we formalize an important\nproblem, namely join over incomplete data streams (Join-iDS), which retrieves\njoining object pairs from incomplete data streams with high confidences. We\ntackle the Join-iDS problem in the style of \"data imputation and query\nprocessing at the same time\". To enable this style, we design an effective and\nefficient cost-model-based imputation method via deferential dependency (DD),\ndevise effective pruning strategies to reduce the Join-iDS search space, and\npropose efficient algorithms via our proposed cost-model-based data\nsynopsis/indexes. Extensive experiments have been conducted to verify the\nefficiency and effectiveness of our proposed Join-iDS approach on both real and\nsynthetic data sets.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 03:39:14 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Ren", "Weilong", ""], ["Lian", "Xiang", ""], ["Ghazinour", "Kambiz", ""]]}, {"id": "1908.08656", "submitter": "Ninh Pham", "authors": "Stephan S. Lorenzen, Ninh Pham", "title": "Revisiting Wedge Sampling for Budgeted Maximum Inner Product Search", "comments": "ECML-PKDD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Top-k maximum inner product search (MIPS) is a central task in many machine\nlearning applications. This paper extends top-k MIPS with a budgeted setting,\nthat asks for the best approximate top-k MIPS given a limit of B computational\noperations. We investigate recent advanced sampling algorithms, including wedge\nand diamond sampling to solve it. Though the design of these sampling schemes\nnaturally supports budgeted top-k MIPS, they suffer from the linear cost from\nscanning all data points to retrieve top-k results and the performance\ndegradation for handling negative inputs.\n  This paper makes two main contributions. First, we show that diamond sampling\nis essentially a combination between wedge sampling and basic sampling for\ntop-k MIPS. Our theoretical analysis and empirical evaluation show that wedge\nis competitive (often superior) to diamond on approximating top-k MIPS\nregarding both efficiency and accuracy. Second, we propose a series of\nalgorithmic engineering techniques to deploy wedge sampling on budgeted top-k\nMIPS. Our novel deterministic wedge-based algorithm runs significantly faster\nthan the state-of-the-art methods for budgeted and exact top-k MIPS while\nmaintaining the top-5 precision at least 80% on standard recommender system\ndata sets.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 04:05:25 GMT"}, {"version": "v2", "created": "Sat, 12 Sep 2020 08:54:28 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Lorenzen", "Stephan S.", ""], ["Pham", "Ninh", ""]]}, {"id": "1908.09205", "submitter": "Paul Kantor", "authors": "Vladimir Menkov, Paul Kantor", "title": "Ontology alignment: A Content-Based Bayesian Approach", "comments": "29 pp. 2 figure. Research Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many legacy databases, and related stores of information that are\nmaintained by distinct organizations, and there are other organizations that\nwould like to be able to access and use those disparate sources. Among the\nexamples of current interest are such things as emergency room records, of\ninterest in tracking and interdicting illicit drugs, or social media public\nposts that indicate preparation and intention for a mass shooting incident. In\nmost cases, this information is discovered too late to be useful. While\nagencies responsible for coordination are aware of the potential value of\ncontemporaneous access to new data, the costs of establishing a connection are\nprohibitive. The problem grown even worse with the proliferation of\n``hash-tagging,'' which permits new labels and ontological relations to spring\nup overnight. While research interest has waned, the need for powerful and\ninexpensive tools enabling prompt access to multiple sources has grown ever\nmore pressing. This paper describes techniques for computing alignment matrix\ncoefficients, which relate the fields or content of one database to those of\nanother, using the Bayesian Ontology Alignment tool (BOA). Particular attention\nis given to formulas that have an easy-to-understand meaning when all cells of\nthe data sources containing values from some small set. These formulas can be\nexpressed in terms of probability estimates. The estimates themselves are given\nby a ``black box'' polytomous logistic regression model (PLRM), and thus can be\neasily generalized to the case of any arbitrary probability-generating model.\nThe specific PLRM model used in this example is the BOXER Bayesian Extensible\nOnline Regression model.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 20:54:27 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Menkov", "Vladimir", ""], ["Kantor", "Paul", ""]]}, {"id": "1908.09333", "submitter": "Aleksandar Janji\\'c", "authors": "Aleksandar Janji\\'c", "title": "Proximity-based equivalence classes in fuzzy relational database model", "comments": "28 pages, 4 figures, 18 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the first attempts to set a solid theoretical foundation for extending\nthe content of relational databases with incomplete information was the fuzzy\nrelational model by Buckles and Petry. This structure was based on two\ngeneralizations of the traditional relational model: (1) A tuple component can\nbe any subset of the corresponding domain, rather than a single element and (2)\nA similarity relation is defined on each domain. This relation satisfies the\nproperties of reflexivity, symmetry and max-min transitivity, thus having the\nequality relation as a special case. This generalization keeps two key\nproperties of the relational model - that no two different tuples represent the\nsame information and that the application of any operation of the relation\nalgebra has a unique result.\n  Shenoi and Melton generalized this model and showed how the existence of\nequivalence classes over the attribute domains can also be preserved with a\nrelation that only satisfies the properties of reflexivity and symmetry\n(proximity relation). The motivation for this generalization is the strictness\nof the max-min transitivity property of similarity relations, which complicates\nthe construction of this relation for some domain types.\n  An important characteristic of the Shenoi-Melton model is the dependence of\nthe equivalence classes upon the current content of the database. This\ncharacteristic, together with the way the equivalence relation is constructed\nby the proximity relation, can lead to the equivalence classes that don't\ncorrespond well with some database query types.\n  Here we will present a different way of forming the equivalence classes over\nthe attribute domains in fuzzy relational databases in which they depend only\non the attribute domain and not on the current database state. We will also\nshow a simple method for automatic construction of proximity relations over\nsome domain types.\n", "versions": [{"version": "v1", "created": "Sun, 25 Aug 2019 14:11:46 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Janji\u0107", "Aleksandar", ""]]}, {"id": "1908.10033", "submitter": "Shantanu Sharma", "authors": "Nisha Panwar, Shantanu Sharma, Guoxi Wang, Sharad Mehrotra, Nalini\n  Venkatasubramanian, Mamadou H. Diallo, Ardalan Amiri Sani", "title": "IoT Notary: Sensor Data Attestation in Smart Environment", "comments": "Accepted in IEEE International Symposium on Network Computing and\n  Applications (NCA), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contemporary IoT environments, such as smart buildings, require end-users to\ntrust data-capturing rules published by the systems. There are several reasons\nwhy such a trust is misplaced --- IoT systems may violate the rules\ndeliberately or IoT devices may transfer user data to a malicious third-party\ndue to cyberattacks, leading to the loss of individuals' privacy or service\nintegrity. To address such concerns, we propose IoT Notary, a framework to\nensure trust in IoT systems and applications. IoT Notary provides secure log\nsealing on live sensor data to produce a verifiable `proof-of-integrity,' based\non which a verifier can attest that captured sensor data adheres to the\npublished data-capturing rules. IoT Notary is an integral part of TIPPERS, a\nsmart space system that has been deployed at UCI to provide various real-time\nlocation-based services in the campus. IoT Notary imposes nominal overheads for\nverification, thereby users can verify their data of one day in less than two\nseconds.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 05:10:04 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Panwar", "Nisha", ""], ["Sharma", "Shantanu", ""], ["Wang", "Guoxi", ""], ["Mehrotra", "Sharad", ""], ["Venkatasubramanian", "Nalini", ""], ["Diallo", "Mamadou H.", ""], ["Sani", "Ardalan Amiri", ""]]}, {"id": "1908.10177", "submitter": "Pan Hu", "authors": "Pan Hu, Jacopo Urbani, Boris Motik, Ian Horrocks", "title": "Datalog Reasoning over Compressed RDF Knowledge Bases", "comments": "CIKM 2019", "journal-ref": null, "doi": "10.1145/3357384.3358147", "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Materialisation is often used in RDF systems as a preprocessing step to\nderive all facts implied by given RDF triples and rules. Although widely used,\nmaterialisation considers all possible rule applications and can use a lot of\nmemory for storing the derived facts, which can hinder performance. We present\na novel materialisation technique that compresses the RDF triples so that the\nrules can sometimes be applied to multiple facts at once, and the derived facts\ncan be represented using structure sharing. Our technique can thus require less\nspace, as well as skip certain rule applications. Our experiments show that our\ntechnique can be very effective: when the rules are relatively simple, our\nsystem is both faster and requires less memory than prominent state-of-the-art\nRDF systems.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 13:12:21 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 12:02:43 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Hu", "Pan", ""], ["Urbani", "Jacopo", ""], ["Motik", "Boris", ""], ["Horrocks", "Ian", ""]]}, {"id": "1908.10268", "submitter": "Yikai Wu", "authors": "Yikai Wu, David Pujol, Ios Kotsogiannis, Ashwin Machanavajjhala", "title": "Answering Summation Queries for Numerical Attributes under Differential\n  Privacy", "comments": "TPDP 2019, 7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we explore the problem of answering a set of sum queries under\nDifferential Privacy. This is a little understood, non-trivial problem\nespecially in the case of numerical domains. We show that traditional\ntechniques from the literature are not always the best choice and a more\nrigorous approach is necessary to develop low error algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 15:22:28 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Wu", "Yikai", ""], ["Pujol", "David", ""], ["Kotsogiannis", "Ios", ""], ["Machanavajjhala", "Ashwin", ""]]}, {"id": "1908.10583", "submitter": "Wenqing Lin", "authors": "Sibo Wang, Renchi Yang, Runhui Wang, Xiaokui Xiao, Zhewei Wei, Wenqing\n  Lin, Yin Yang, Nan Tang", "title": "Efficient Algorithms for Approximate Single-Source Personalized PageRank\n  Queries", "comments": "Accepted in the ACM Transactions on Database Systems (TODS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB cs.DS", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Given a graph $G$, a source node $s$ and a target node $t$, the personalized\nPageRank (PPR) of $t$ with respect to $s$ is the probability that a random walk\nstarting from $s$ terminates at $t$. An important variant of the PPR query is\nsingle-source PPR (SSPPR), which enumerates all nodes in $G$, and returns the\ntop-$k$ nodes with the highest PPR values with respect to a given source $s$.\nPPR in general and SSPPR in particular have important applications in web\nsearch and social networks, e.g., in Twitter's Who-To-Follow recommendation\nservice. However, PPR computation is known to be expensive on large graphs, and\nresistant to indexing. Consequently, previous solutions either use heuristics,\nwhich do not guarantee result quality, or rely on the strong computing power of\nmodern data centers, which is costly.\n  Motivated by this, we propose effective index-free and index-based algorithms\nfor approximate PPR processing, with rigorous guarantees on result quality. We\nfirst present FORA, an approximate SSPPR solution that combines two existing\nmethods Forward Push (which is fast but does not guarantee quality) and Monte\nCarlo Random Walk (accurate but slow) in a simple and yet non-trivial way,\nleading to both high accuracy and efficiency. Further, FORA includes a simple\nand effective indexing scheme, as well as a module for top-$k$ selection with\nhigh pruning power. Extensive experiments demonstrate that the proposed\nsolutions are orders of magnitude more efficient than their respective\ncompetitors. Notably, on a billion-edge Twitter dataset, FORA answers a top-500\napproximate SSPPR query within 1 second, using a single commodity server.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 07:34:08 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Wang", "Sibo", ""], ["Yang", "Renchi", ""], ["Wang", "Runhui", ""], ["Xiao", "Xiaokui", ""], ["Wei", "Zhewei", ""], ["Lin", "Wenqing", ""], ["Yang", "Yin", ""], ["Tang", "Nan", ""]]}, {"id": "1908.10693", "submitter": "Homin Lee", "authors": "Charles Masson and Jee E. Rim and Homin K. Lee", "title": "DDSketch: A fast and fully-mergeable quantile sketch with relative-error\n  guarantees", "comments": "11 pages, 11 figures, VLDB", "journal-ref": "PVLDB, 12(12): 2195-2205, 2019", "doi": "10.14778/3352063.3352135", "report-no": null, "categories": "cs.DB cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Summary statistics such as the mean and variance are easily maintained for\nlarge, distributed data streams, but order statistics (i.e., sample quantiles)\ncan only be approximately summarized. There is extensive literature on\nmaintaining quantile sketches where the emphasis has been on bounding the rank\nerror of the sketch while using little memory. Unfortunately, rank error\nguarantees do not preclude arbitrarily large relative errors, and this often\noccurs in practice when the data is heavily skewed. Given the distributed\nnature of contemporary large-scale systems, another crucial property for\nquantile sketches is mergeablility, i.e., several combined sketches must be as\naccurate as a single sketch of the same data. We present the first\nfully-mergeable, relative-error quantile sketching algorithm with formal\nguarantees. The sketch is extremely fast and accurate, and is currently being\nused by Datadog at a wide-scale.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 12:48:44 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Masson", "Charles", ""], ["Rim", "Jee E.", ""], ["Lee", "Homin K.", ""]]}, {"id": "1908.11366", "submitter": "Karim Banawan", "authors": "Karim Banawan and Batuhan Arasli and Sennur Ulukus", "title": "Improved Storage for Efficient Private Information Retrieval", "comments": "ITW 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.DB math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of private information retrieval from $N$\n\\emph{storage-constrained} databases. In this problem, a user wishes to\nretrieve a single message out of $M$ messages (of size $L$) without revealing\nany information about the identity of the message to individual databases. Each\ndatabase stores $\\mu ML$ symbols, i.e., a $\\mu$ fraction of the entire library,\nwhere $\\frac{1}{N} \\leq \\mu \\leq 1$. Our goal is to characterize the optimal\ntradeoff curve for the storage cost (captured by $\\mu$) and the normalized\ndownload cost ($D/L$). We show that the download cost can be reduced by\nemploying a hybrid storage scheme that combines \\emph{MDS coding} ideas with\n\\emph{uncoded partial replication} ideas. When there is no coding, our scheme\nreduces to Attia-Kumar-Tandon storage scheme, which was initially introduced by\nMaddah-Ali-Niesen in the context of the caching problem, and when there is no\nuncoded partial replication, our scheme reduces to Banawan-Ulukus storage\nscheme; in general, our scheme outperforms both.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 17:52:01 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Banawan", "Karim", ""], ["Arasli", "Batuhan", ""], ["Ulukus", "Sennur", ""]]}, {"id": "1908.11515", "submitter": "Tianhao Wang", "authors": "Tianhao Wang, Bolin Ding, Min Xu, Zhicong Huang, Cheng Hong, Jingren\n  Zhou, Ninghui Li, Somesh Jha", "title": "Improving Utility and Security of the Shuffler-based Differential\n  Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When collecting information, local differential privacy (LDP) alleviates\nprivacy concerns of users because their private information is randomized\nbefore being sent it to the central aggregator. LDP imposes large amount of\nnoise as each user executes the randomization independently. To address this\nissue, recent work introduced an intermediate server with the assumption that\nthis intermediate server does not collude with the aggregator. Under this\nassumption, less noise can be added to achieve the same privacy guarantee as\nLDP, thus improving utility for the data collection task.\n  This paper investigates this multiple-party setting of LDP. We analyze the\nsystem model and identify potential adversaries. We then make two improvements:\na new algorithm that achieves a better privacy-utility tradeoff; and a novel\nprotocol that provides better protection against various attacks. Finally, we\nperform experiments to compare different methods and demonstrate the benefits\nof using our proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 03:02:04 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 03:06:01 GMT"}, {"version": "v3", "created": "Sun, 2 Aug 2020 14:54:55 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Wang", "Tianhao", ""], ["Ding", "Bolin", ""], ["Xu", "Min", ""], ["Huang", "Zhicong", ""], ["Hong", "Cheng", ""], ["Zhou", "Jingren", ""], ["Li", "Ninghui", ""], ["Jha", "Somesh", ""]]}, {"id": "1908.11567", "submitter": "Tobias Skovgaard Jepsen", "authors": "Tobias Skovgaard Jepsen, Christian S. Jensen, Thomas Dyhre Nielsen", "title": "Graph Convolutional Networks for Road Networks", "comments": "Ten-page pre-print version of a four-page ACM SIGSPATIAL 2019 poster\n  paper", "journal-ref": null, "doi": "10.1145/3347146.3359094", "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning techniques for road networks hold the potential to\nfacilitate many important transportation applications. Graph Convolutional\nNetworks (GCNs) are neural networks that are capable of leveraging the\nstructure of a road network by utilizing information of, e.g., adjacent road\nsegments. While state-of-the-art GCNs target node classification tasks in\nsocial, citation, and biological networks, machine learning tasks in road\nnetworks differ substantially from such tasks. In road networks, prediction\ntasks concern edges representing road segments, and many tasks involve\nregression. In addition, road networks differ substantially from the networks\nassumed in the GCN literature in terms of the attribute information available\nand the network characteristics. Many implicit assumptions of GCNs do therefore\nnot apply. We introduce the notion of Relational Fusion Network (RFN), a novel\ntype of GCN designed specifically for machine learning on road networks. In\nparticular, we propose methods that outperform state-of-the-art GCNs on both a\nroad segment regression task and a road segment classification task by 32-40%\nand 21-24%, respectively. In addition, we provide experimental evidence of the\nshort-comings of state-of-the-art GCNs in the context of road networks: unlike\nour method, they cannot effectively leverage the road network structure for\nroad segment classification and fail to outperform a regular multi-layer\nperceptron.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 07:08:51 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 09:44:40 GMT"}, {"version": "v3", "created": "Wed, 22 Jul 2020 14:50:54 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Jepsen", "Tobias Skovgaard", ""], ["Jensen", "Christian S.", ""], ["Nielsen", "Thomas Dyhre", ""]]}, {"id": "1908.11612", "submitter": "Jinbin Huang", "authors": "Jinbin Huang, Xin Huang, Yuanyuan Zhu, Jianliang Xu", "title": "Parameter-free Structural Diversity Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of structural diversity search is to find the top-k vertices with\nthe largest structural diversity in a graph. However, when identifying distinct\nsocial contexts, existing structural diversity models (e.g., t-sized component,\nt-core, and t-brace) are sensitive to an input parameter of t. To address this\ndrawback, we propose a parameter-free structural diversity model. Specifically,\nwe propose a novel notation of discriminative core, which automatically models\nvarious kinds of social contexts without parameter t. Leveraging on\ndiscriminative cores and h-index, the structural diversity score for a vertex\nis calculated. We study the problem of parameter-free structural diversity\nsearch in this paper. An efficient top-k search algorithm with a well-designed\nupper bound for pruning is proposed. Extensive experiment results demonstrate\nthe parameter sensitivity of existing t-core based model and verify the\nsuperiority of our methods.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 09:36:28 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 03:33:38 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Huang", "Jinbin", ""], ["Huang", "Xin", ""], ["Zhu", "Yuanyuan", ""], ["Xu", "Jianliang", ""]]}, {"id": "1908.11642", "submitter": "Johannes Doleschal", "authors": "Johannes Doleschal and Benny Kimelfeld and Wim Martens and Liat\n  Peterfreund", "title": "Weight Annotation in Information Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.FL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The framework of document spanners abstracts the task of information\nextraction from text as a function that maps every document (a string) into a\nrelation over the document's spans (intervals identified by their start and end\nindices). For instance, the regular spanners are the closure under the\nRelational Algebra (RA) of the regular expressions with capture variables, and\nthe expressive power of the regular spanners is precisely captured by the class\nof VSet-automata - a restricted class of transducers that mark the endpoints of\nselected spans.\n  In this work, we embark on the investigation of document spanners that can\nannotate extractions with auxiliary information such as confidence, support,\nand confidentiality measures. To this end, we adopt the abstraction of\nprovenance semirings by Green et al., where tuples of a relation are annotated\nwith the elements of a commutative semiring, and where the annotation\npropagates through the (positive) RA operators via the semiring operators.\nHence, the proposed spanner extension, referred to as an annotator, maps every\nstring into an annotated relation over the spans. As a specific instantiation,\nwe explore weighted VSet-automata that, similarly to weighted automata and\ntransducers, attach semiring elements to transitions. We investigate key\naspects of expressiveness, such as the closure under the positive RA, and key\naspects of computational complexity, such as the enumeration of annotated\nanswers and their ranked enumeration in the case of numeric semirings. For a\nnumber of these problems, fundamental properties of the underlying semiring,\nsuch as positivity, are crucial for establishing tractability.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 10:39:21 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 13:46:59 GMT"}, {"version": "v3", "created": "Fri, 27 Nov 2020 12:11:50 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Doleschal", "Johannes", ""], ["Kimelfeld", "Benny", ""], ["Martens", "Wim", ""], ["Peterfreund", "Liat", ""]]}, {"id": "1908.11682", "submitter": "Panagiotis Mandros", "authors": "Panagiotis Mandros, Mario Boley, Jilles Vreeken", "title": "Discovering Reliable Correlations in Categorical Data", "comments": "Accepted to the IEEE International Conference on Data Mining 2019\n  (ICDM'19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.IT math.IT stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In many scientific tasks we are interested in discovering whether there exist\nany correlations in our data. This raises many questions, such as how to\nreliably and interpretably measure correlation between a multivariate set of\nattributes, how to do so without having to make assumptions on distribution of\nthe data or the type of correlation, and, how to efficiently discover the\ntop-most reliably correlated attribute sets from data. In this paper we answer\nthese questions for discovery tasks in categorical data.\n  In particular, we propose a corrected-for-chance, consistent, and efficient\nestimator for normalized total correlation, by which we obtain a reliable,\nnaturally interpretable, non-parametric measure for correlation over\nmultivariate sets. For the discovery of the top-k correlated sets, we derive an\neffective algorithmic framework based on a tight bounding function. This\nframework offers exact, approximate, and heuristic search. Empirical evaluation\nshows that already for small sample sizes the estimator leads to low-regret\noptimization outcomes, while the algorithms are shown to be highly effective\nfor both large and high-dimensional data. Through two case studies we confirm\nthat our discovery framework identifies interesting and meaningful\ncorrelations.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 12:18:29 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Mandros", "Panagiotis", ""], ["Boley", "Mario", ""], ["Vreeken", "Jilles", ""]]}, {"id": "1908.11740", "submitter": "Panagiotis Bouros", "authors": "Dimitrios Tsitsigkos and Panagiotis Bouros and Nikos Mamoulis and\n  Manolis Terrovitis", "title": "Parallel In-Memory Evaluation of Spatial Joins", "comments": "Extended version of the SIGSPATIAL'19 paper under the same title", "journal-ref": "27th ACM SIGSPATIAL International Conference on Advances in\n  Geographic Information Systems (ACM SIGSPATIAL GIS 2019), Chicago, Illinois,\n  USA, November 5-8, 2019", "doi": "10.1145/3347146.3359343", "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spatial join is a popular operation in spatial database systems and its\nevaluation is a well-studied problem. As main memories become bigger and faster\nand commodity hardware supports parallel processing, there is a need to revamp\nclassic join algorithms which have been designed for I/O-bound processing. In\nview of this, we study the in-memory and parallel evaluation of spatial joins,\nby re-designing a classic partitioning-based algorithm to consider alternative\napproaches for space partitioning. Our study shows that, compared to a\nstraightforward implementation of the algorithm, our tuning can improve\nperformance significantly. We also show how to select appropriate partitioning\nparameters based on data statistics, in order to tune the algorithm for the\ngiven join inputs. Our parallel implementation scales gracefully with the\nnumber of threads reducing the cost of the join to at most one second even for\njoin inputs with tens of millions of rectangles.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 13:48:31 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 08:54:01 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Tsitsigkos", "Dimitrios", ""], ["Bouros", "Panagiotis", ""], ["Mamoulis", "Nikos", ""], ["Terrovitis", "Manolis", ""]]}, {"id": "1908.11788", "submitter": "Longxu Sun", "authors": "Longxu Sun, Xin Huang, Rong-Hua Li and Jianliang Xu", "title": "Fast Algorithms for Intimate-Core Group Search in Weighted Graphs", "comments": "15 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community search that finds query-dependent communities has been studied on\nvarious kinds of graphs. As one instance of community search, intimate-core\ngroup search over a weighted graph is to find a connected $k$-core containing\nall query nodes with the smallest group weight. However, existing\nstate-of-the-art methods start from the maximal $k$-core to refine an answer,\nwhich is practically inefficient for large networks. In this paper, we develop\nan efficient framework, called local exploration k-core search (LEKS), to find\nintimate-core groups in graphs. We propose a small-weighted spanning tree to\nconnect query nodes, and then expand the tree level by level to a connected\n$k$-core, which is finally refined as an intimate-core group. We also design a\nprotection mechanism for critical nodes to avoid the collapsed $k$-core.\nExtensive experiments on real-life networks validate the effectiveness and\nefficiency of our methods.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 15:28:54 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Sun", "Longxu", ""], ["Huang", "Xin", ""], ["Li", "Rong-Hua", ""], ["Xu", "Jianliang", ""]]}]