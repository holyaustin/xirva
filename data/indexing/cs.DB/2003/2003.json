[{"id": "2003.00051", "submitter": "Sepanta Zeighami", "authors": "Sepanta Zeighami, Gabriel Ghinita, Cyrus Shahabi", "title": "Dynamic Skyline Queries on Encrypted Data Using Result Materialization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skyline computation is an increasingly popular query, with broad\napplicability in domains such as healthcare, travel and finance. Given the\nrecent trend to outsource databases and query evaluation, and due to the\nproprietary and sometimes highly sensitivity nature of the data (e.g., in\nhealthcare), it is essential to evaluate skylines on encrypted datasets.\nSeveral research efforts acknowledged the importance of secure skyline\ncomputation, but existing solutions suffer from at least one of the following\nshortcomings: (i) they only provide ad-hoc security; (ii) they are\nprohibitively expensive; or (iii) they rely on unrealistic assumptions, such as\nthe presence of multiple non-colluding parties in the protocol.\n  Inspired from solutions for secure nearest-neighbors (NN) computation, we\nconjecture that the most secure and efficient way to compute skylines is\nthrough result materialization. However, this approach is significantly more\nchallenging for skylines than for NN queries. We exhaustively study and provide\nalgorithms for pre-computation of skyline results, and we perform an in-depth\ntheoretical analysis of this process. We show that pre-computing results while\nminimizing storage overhead is NP-hard, and we provide dynamic programming and\ngreedy heuristics that solve the problem more efficiently, while maintaining\nstorage at reasonable levels. Our algorithms are novel and applicable to\nplain-text skyline computation, but we focus on the encrypted setting where\nmaterialization reduces the cost of skyline computation from hours to seconds.\nExtensive experiments show that we clearly outperform existing work in terms of\nperformance, and our security analysis proves that we obtain a smaller (and\nquantifiable) data leakage than competitors.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 20:27:09 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Zeighami", "Sepanta", ""], ["Ghinita", "Gabriel", ""], ["Shahabi", "Cyrus", ""]]}, {"id": "2003.00054", "submitter": "Stefanie Scherzinger", "authors": "Stefanie Scherzinger and Sebastian Sidortschuck", "title": "An Empirical Study on the Design and Evolution of NoSQL Database Schemas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study how software engineers design and evolve their domain model when\nbuilding applications against NoSQL data stores. Specifically, we target Java\nprojects that use object-NoSQL mappers to interface with schema-free NoSQL data\nstores. Given the source code of ten real-world database applications, we\nextract the implicit NoSQL database schema. We capture the sizes of the\nschemas, and investigate whether the schema is denormalized, as is recommended\npractice in data modeling for NoSQL data stores. Further, we analyze the entire\nproject history, and with it, the evolution history of the NoSQL database\nschema. In doing so, we conduct the so far largest empirical study on NoSQL\nschema design and evolution.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 20:34:09 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Scherzinger", "Stefanie", ""], ["Sidortschuck", "Sebastian", ""]]}, {"id": "2003.00103", "submitter": "Nikos Batsaras", "authors": "Nikos Batsaras, Giorgos Saloustros, Anastasios Papagiannis, Panagiota\n  Fatourou, and Angelos Bilas", "title": "VAT: Asymptotic Cost Analysis for Multi-Level Key-Value Stores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past years, there has been an increasing number of key-value (KV)\nstore designs, each optimizing for a different set of requirements.\nFurthermore, with the advancements of storage technology the design space of KV\nstores has become even more complex. More recent KV-store designs target fast\nstorage devices, such as SSDs and NVM. Most of these designs aim to reduce\namplification during data reorganization by taking advantage of device\ncharacteristics. However, until today most analysis of KV-store designs is\nexperimental and limited to specific design points. This makes it difficult to\ncompare tradeoffs across different designs, find optimal configurations and\nguide future KV-store design. In this paper, we introduce the Variable\nAmplification- Throughput analysis (VAT) to calculate insert-path amplification\nand its impact on multi-level KV-store performance.We use VAT to express the\nbehavior of several existing design points and to explore tradeoffs that are\nnot possible or easy to measure experimentally. VAT indicates that by inserting\nrandomness in the insert-path, KV stores can reduce amplification by more than\n10x for fast storage devices. Techniques, such as key-value separation and\ntiering compaction, reduce amplification by 10x and 5x, respectively.\nAdditionally, VAT predicts that the advancements in device technology towards\nNVM, reduces the benefits from both using key-value separation and tiering.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 22:54:18 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Batsaras", "Nikos", ""], ["Saloustros", "Giorgos", ""], ["Papagiannis", "Anastasios", ""], ["Fatourou", "Panagiota", ""], ["Bilas", "Angelos", ""]]}, {"id": "2003.00584", "submitter": "David Daly", "authors": "David Daly, William Brown, Henrik Ingo, Jim O'Leary, David Bradford", "title": "Change Point Detection in Software Performance Testing", "comments": null, "journal-ref": null, "doi": "10.1145/3358960.3375791", "report-no": null, "categories": "cs.SE cs.DB cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe our process for automatic detection of performance changes for a\nsoftware product in the presence of noise. A large collection of tests run\nperiodically as changes to our software product are committed to our source\nrepository, and we would like to identify the commits responsible for\nperformance regressions. Previously, we relied on manual inspection of time\nseries graphs to identify significant changes. That was later replaced with a\nthreshold-based detection system, but neither system was sufficient for finding\nchanges in performance in a timely manner. This work describes our recent\nimplementation of a change point detection system built upon the E-Divisive\nmeans algorithm. The algorithm produces a list of change points representing\nsignificant changes from a given history of performance results. A human\nreviews the list of change points for actionable changes, which are then\ntriaged for further inspection. Using change point detection has had a dramatic\nimpact on our ability to detect performance changes. Quantitatively, it has\ndramatically dropped our false positive rate for performance changes, while\nqualitatively it has made the entire performance evaluation process easier,\nmore productive (ex. catching smaller regressions), and more timely.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 21:01:25 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Daly", "David", ""], ["Brown", "William", ""], ["Ingo", "Henrik", ""], ["O'Leary", "Jim", ""], ["Bradford", "David", ""]]}, {"id": "2003.00719", "submitter": "Nicolas Heist", "authors": "Nicolas Heist, Sven Hertling, Daniel Ringler and Heiko Paulheim", "title": "Knowledge Graphs on the Web -- an Overview", "comments": "Nicolas Heist, Sven Hertling, Daniel Ringler, Heiko Paulheim:\n  Knowledge Graphs on the Web -- an Overview. In: Ilaria Tiddi, Freddy Lecue,\n  Pascal Hitzler (eds.), Knowledge Graphs for eXplainable AI -- Foundations,\n  Applications and Challenges. Studies on the Semantic Web, IOS Press,\n  Amsterdam, 2020, to appear. [extended version]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Graphs are an emerging form of knowledge representation. While\nGoogle coined the term Knowledge Graph first and promoted it as a means to\nimprove their search results, they are used in many applications today. In a\nknowledge graph, entities in the real world and/or a business domain (e.g.,\npeople, places, or events) are represented as nodes, which are connected by\nedges representing the relations between those entities. While companies such\nas Google, Microsoft, and Facebook have their own, non-public knowledge graphs,\nthere is also a larger body of publicly available knowledge graphs, such as\nDBpedia or Wikidata. In this chapter, we provide an overview and comparison of\nthose publicly available knowledge graphs, and give insights into their\ncontents, size, coverage, and overlap.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 08:58:21 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2020 07:36:55 GMT"}, {"version": "v3", "created": "Thu, 12 Mar 2020 10:31:25 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Heist", "Nicolas", ""], ["Hertling", "Sven", ""], ["Ringler", "Daniel", ""], ["Paulheim", "Heiko", ""]]}, {"id": "2003.00773", "submitter": "Chris Liu", "authors": "Ziliang Lai, Chenxia Han, Chris Liu, Pengfei Zhang, Eric Lo, Ben Kao", "title": "Top-K Deep Video Analytics: A Probabilistic Approach", "comments": "14 pages, 9 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The impressive accuracy of deep neural networks (DNNs) has created great\ndemands on practical analytics over video data. Although efficient and\naccurate, the latest video analytic systems have not supported analytics beyond\nselection and aggregation queries. In data analytics, Top-K is a very important\nanalytical operation that enables analysts to focus on the most important\nentities. In this paper, we present Everest, the first system that supports\nefficient and accurate Top-K video analytics. Everest ranks and identifies the\nmost interesting frames/moments from videos with probabilistic guarantees.\nEverest is a system built with a careful synthesis of deep computer vision\nmodels, uncertain data management, and Top-K query processing. Evaluations on\nreal-world videos and the latest Visual Road benchmark show that Everest\nachieves between 14.3x to 20.6x higher efficiency than baseline approaches with\nhigh result accuracy\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 11:30:35 GMT"}, {"version": "v2", "created": "Sun, 13 Sep 2020 11:16:23 GMT"}, {"version": "v3", "created": "Mon, 29 Mar 2021 01:07:04 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Lai", "Ziliang", ""], ["Han", "Chenxia", ""], ["Liu", "Chris", ""], ["Zhang", "Pengfei", ""], ["Lo", "Eric", ""], ["Kao", "Ben", ""]]}, {"id": "2003.00953", "submitter": "Yueting Chen", "authors": "Yueting Chen and Xiaohui Yu and Nick Koudas", "title": "Evaluating Temporal Queries Over Video Feeds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in Computer Vision and Deep Learning made possible the\nefficient extraction of a schema from frames of streaming video. As such, a\nstream of objects and their associated classes along with unique object\nidentifiers derived via object tracking can be generated, providing unique\nobjects as they are captured across frames. In this paper we initiate a study\nof temporal queries involving objects and their co-occurrences in video feeds.\nFor example, queries that identify video segments during which the same two red\ncars and the same two humans appear jointly for five minutes are of interest to\nmany applications ranging from law enforcement to security and safety. We take\nthe first step and define such queries in a way that they incorporate certain\nphysical aspects of video capture such as object occlusion. We present an\narchitecture consisting of three layers, namely object detection/tracking,\nintermediate data generation and query evaluation. We propose two\ntechniques,MFS and SSG, to organize all detected objects in the intermediate\ndata generation layer, which effectively, given the queries, minimizes the\nnumber of objects and frames that have to be considered during query\nevaluation. We also introduce an algorithm called State Traversal (ST) that\nprocesses incoming frames against the SSG and efficiently prunes objects and\nframes unrelated to query evaluation, while maintaining all states required for\nsuccinct query evaluation. We present the results of a thorough experimental\nevaluation utilizing both real and synthetic data establishing the trade-offs\nbetween MFS and SSG. We stress various parameters of interest in our evaluation\nand demonstrate that the proposed query evaluation methodology coupled with the\nproposed algorithms is capable to evaluate temporal queries over video feeds\nefficiently, achieving orders of magnitude performance benefits.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 14:55:57 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 14:16:18 GMT"}, {"version": "v3", "created": "Thu, 5 Mar 2020 22:22:46 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Chen", "Yueting", ""], ["Yu", "Xiaohui", ""], ["Koudas", "Nick", ""]]}, {"id": "2003.00965", "submitter": "Gaetano Geck", "authors": "Gaetano Geck, Frank Neven, Thomas Schwentick", "title": "Distribution Constraints: The Chase for Distributed Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a declarative framework to specify and reason about\ndistributions of data over computing nodes in a distributed setting. More\nspecifically, it proposes distribution constraints which are tuple and equality\ngenerating dependencies (tgds and egds) extended with node variables ranging\nover computing nodes. In particular, they can express co-partitioning\nconstraints and constraints about range-based data distributions by using\ncomparison atoms. The main technical contribution is the study of the\nimplication problem of distribution constraints. While implication is\nundecidable in general, relevant fragments of so-called data-full constraints\nare exhibited for which the corresponding implication problems are complete for\nEXPTIME, PSPACE and NP. These results yield bounds on deciding\nparallel-correctness for conjunctive queries in the presence of distribution\nconstraints.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 15:25:15 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Geck", "Gaetano", ""], ["Neven", "Frank", ""], ["Schwentick", "Thomas", ""]]}, {"id": "2003.00971", "submitter": "Philip Kulp", "authors": "Philip H. Kulp and Nikki E. Robinson", "title": "Graphing Website Relationships for Risk Prediction: Identifying Derived\n  Threats to Users Based on Known Indicators", "comments": "10 pages, 3 figures, 3 tables", "journal-ref": null, "doi": "10.1007/978-3-030-63089-8", "report-no": null, "categories": "cs.CR cs.DB cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The hypothesis for the study was that the relationship based on referrer\nlinks and the number of hops to a malicious site could indicate the risk to\nanother website. We chose Receiver Operating Characteristics (ROC) analysis as\nthe method of comparing true positive and false positive rates for captured web\ntraffic to test the predictive capabilities of our model. Known threat\nindicators were used as designators, and the Neo4j graph database was leveraged\nto map the relationships between other websites based on referring links. Using\nthe referring traffic, we mapped user visits across websites with a known\nrelationship to track the rate at which users progressed from a non-malicious\nwebsite to a known threat. The results were grouped by the hop distance from\nthe known threat to calculate the predictive rate. The results of the model\nproduced true positive rates between 58.59% and 63.45% and false positive rates\nbetween 7.42% to 37.50%, respectively. The true and false positive rates\nsuggest an improved performance based on the closer proximity from the known\nthreat, while an increased referring distance from the threat resulted in\nhigher rates of false positives.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 15:41:48 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Kulp", "Philip H.", ""], ["Robinson", "Nikki E.", ""]]}, {"id": "2003.01064", "submitter": "Sepanta Zeighami", "authors": "Sepanta Zeighami, Raymond Chi-Wing Wong", "title": "Bridging the Gap Between Theory and Practice on Insertion-Intensive\n  Database", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the prevalence of online platforms, today, data is being generated and\naccessed by users at a very high rate. Besides, applications such as stock\ntrading or high frequency trading require guaranteed low delays for performing\nan operation on a database. It is consequential to design databases that\nguarantee data insertion and query at a consistently high rate without\nintroducing any long delay during insertion. In this paper, we propose Nested\nB-trees (NB-trees), an index that can achieve a consistently high insertion\nrate on large volumes of data, while providing asymptotically optimal query\nperformance that is very efficient in practice. Nested B-trees support\ninsertions at rates higher than LSM-trees, the state-of-the-art index for\ninsertion-intensive workloads, while avoiding their long insertion delays and\nimproving on their query performance. They approach the query performance of\nB-trees when complemented with Bloom filters. In our experiments, NB-trees had\nworst-case delays up to 1000 smaller than LevelDB, RocksDB and bLSM, commonly\nused LSM-tree data-stores, could perform queries more than 4 times faster than\nLevelDB and 1.5 times faster than bLSM and RocksDB, while also outperforming\nthem in terms of average insertion rate.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 17:50:07 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Zeighami", "Sepanta", ""], ["Wong", "Raymond Chi-Wing", ""]]}, {"id": "2003.01075", "submitter": "Christoph Berkholz", "authors": "Christoph Berkholz, Nicole Schweikardt", "title": "Constant delay enumeration with FPT-preprocessing for conjunctive\n  queries of bounded submodular width", "comments": "This is the full version of the conference contribution with the same\n  title that appeared at MFCS 2019", "journal-ref": "Proceedings of the 44th International Symposium on Mathematical\n  Foundations of Computer Science, pp. 58:1-58:15, 2019", "doi": "10.4230/LIPIcs.MFCS.2019.58", "report-no": null, "categories": "cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Marx (STOC~2010, J.~ACM 2013) introduced the notion of submodular width of a\nconjunctive query (CQ) and showed that for any class $\\Phi$ of Boolean CQs of\nbounded submodular width, the model-checking problem for $\\Phi$ on the class of\nall finite structures is fixed-parameter tractable (FPT). Note that for\nnon-Boolean queries, the size of the query result may be far too large to be\ncomputed entirely within FPT time. We investigate the free-connex variant of\nsubmodular width and generalise Marx's result to non-Boolean queries as\nfollows: For every class $\\Phi$ of CQs of bounded free-connex submodular width,\nwithin FPT-preprocessing time we can build a data structure that allows to\nenumerate, without repetition and with constant delay, all tuples of the query\nresult. Our proof builds upon Marx's splitting routine to decompose the query\nresult into a union of results; but we have to tackle the additional technical\ndifficulty to ensure that these can be enumerated efficiently.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 18:09:43 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Berkholz", "Christoph", ""], ["Schweikardt", "Nicole", ""]]}, {"id": "2003.01178", "submitter": "Anil Shanbhag", "authors": "Anil Shanbhag, Samuel Madden, Xiangyao Yu", "title": "A Study of the Fundamental Performance Characteristics of GPUs and CPUs\n  for Database Analytics (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been significant amount of excitement and recent work on GPU-based\ndatabase systems. Previous work has claimed that these systems can perform\norders of magnitude better than CPU-based database systems on analytical\nworkloads such as those found in decision support and business intelligence\napplications. A hardware expert would view these claims with suspicion. Given\nthe general notion that database operators are memory-bandwidth bound, one\nwould expect the maximum gain to be roughly equal to the ratio of the memory\nbandwidth of GPU to that of CPU. In this paper, we adopt a model-based approach\nto understand when and why the performance gains of running queries on GPUs vs\non CPUs vary from the bandwidth ratio (which is roughly 16x on modern\nhardware). We propose Crystal, a library of parallel routines that can be\ncombined together to run full SQL queries on a GPU with minimal materialization\noverhead. We implement individual query operators to show that while the\nspeedups for selection, projection, and sorts are near the bandwidth ratio,\njoins\n  achieve less speedup due to differences in hardware capabilities.\nInterestingly, we show on a popular analytical workload that full query\nperformance gain from running on GPU exceeds the bandwidth ratio despite\nindividual operators having speedup less than bandwidth ratio, as a result of\nlimitations of vectorizing chained operators on CPUs, resulting in a 25x\nspeedup for GPUs over CPUs on the benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 20:30:16 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Shanbhag", "Anil", ""], ["Madden", "Samuel", ""], ["Yu", "Xiangyao", ""]]}, {"id": "2003.01289", "submitter": "Anna Fariha", "authors": "Anna Fariha, Ashish Tiwari, Arjun Radhakrishna, Sumit Gulwani,\n  Alexandra Meliou", "title": "Conformance Constraint Discovery: Measuring Trust in Data-Driven Systems", "comments": "* Technical report for the conference paper to appear in SIGMOD 2021\n  * An earlier version of this paper had a different title: \"Data Invariants:\n  On Trust in Data-Driven Systems\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reliability and proper function of data-driven applications hinge on the\ndata's continued conformance to the applications' initial design. When data\ndeviates from this initial profile, system behavior becomes unpredictable. Data\nprofiling techniques such as functional dependencies and denial constraints\nencode patterns in the data that can be used to detect deviations. But\ntraditional methods typically focus on exact constraints and categorical\nattributes, and are ill-suited for tasks such as determining whether the\nprediction of a machine learning system can be trusted or for quantifying data\ndrift. In this paper, we introduce data invariants, a new data-profiling\nprimitive that models arithmetic relationships involving multiple numerical\nattributes within a (noisy) dataset and which complements the existing\ndata-profiling techniques. We propose a quantitative semantics to measure the\ndegree of violation of a data invariant, and establish that strong data\ninvariants can be constructed from observations with low variance on the given\ndataset. A concrete instance of this principle gives the surprising result that\nlow-variance components of a principal component analysis (PCA), which are\nusually discarded, generate better invariants than the high-variance\ncomponents. We demonstrate the value of data invariants on two applications:\ntrusted machine learning and data drift. We empirically show that data\ninvariants can (1) reliably detect tuples on which the prediction of a\nmachine-learned model should not be trusted, and (2) quantify data drift more\naccurately than the state-of-the-art methods. Additionally, we show four case\nstudies where an intervention-centric explanation tool uses data invariants to\nexplain causes for tuple non-conformance.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 01:40:14 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 18:43:27 GMT"}, {"version": "v3", "created": "Thu, 5 Mar 2020 02:59:03 GMT"}, {"version": "v4", "created": "Mon, 4 Jan 2021 05:22:11 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Fariha", "Anna", ""], ["Tiwari", "Ashish", ""], ["Radhakrishna", "Arjun", ""], ["Gulwani", "Sumit", ""], ["Meliou", "Alexandra", ""]]}, {"id": "2003.01331", "submitter": "Yuepeng Wang", "authors": "Yuepeng Wang, Rushi Shah, Abby Criswell, Rong Pan, Isil Dillig", "title": "Data Migration using Datalog Program Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new technique for migrating data between different\nschemas. Our method expresses the schema mapping as a Datalog program and\nautomatically synthesizes a Datalog program from simple input-output examples\nto perform data migration. This approach can transform data between different\ntypes of schemas (e.g., relational-to-graph, document-to-relational) and\nperforms synthesis efficiently by leveraging the semantics of Datalog. We\nimplement the proposed technique as a tool called Dynamite and show its\neffectiveness by evaluating Dynamite on 28 realistic data migration scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 04:48:40 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Wang", "Yuepeng", ""], ["Shah", "Rushi", ""], ["Criswell", "Abby", ""], ["Pan", "Rong", ""], ["Dillig", "Isil", ""]]}, {"id": "2003.01853", "submitter": "Geon Lee", "authors": "Geon Lee, Jihoon Ko, Kijung Shin", "title": "Hypergraph Motifs: Concepts, Algorithms, and Discoveries", "comments": "to be published in the 46th International Conference on Very Large\n  Data Bases (VLDB '20)", "journal-ref": null, "doi": "10.14778/3407790.3407823", "report-no": null, "categories": "cs.SI cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypergraphs naturally represent group interactions, which are omnipresent in\nmany domains: collaborations of researchers, co-purchases of items, joint\ninteractions of proteins, to name a few. In this work, we propose tools for\nanswering the following questions in a systematic manner: (Q1) what are\nstructural design principles of real-world hypergraphs? (Q2) how can we compare\nlocal structures of hypergraphs of different sizes? (Q3) how can we identify\ndomains which hypergraphs are from? We first define hypergraph motifs\n(h-motifs), which describe the connectivity patterns of three connected\nhyperedges. Then, we define the significance of each h-motif in a hypergraph as\nits occurrences relative to those in properly randomized hypergraphs. Lastly,\nwe define the characteristic profile (CP) as the vector of the normalized\nsignificance of every h-motif. Regarding Q1, we find that h-motifs' occurrences\nin 11 real-world hypergraphs from 5 domains are clearly distinguished from\nthose of randomized hypergraphs. In addition, we demonstrate that CPs capture\nlocal structural patterns unique to each domain, and thus comparing CPs of\nhypergraphs addresses Q2 and Q3. Our algorithmic contribution is to propose\nMoCHy, a family of parallel algorithms for counting h-motifs' occurrences in a\nhypergraph. We theoretically analyze their speed and accuracy, and we show\nempirically that the advanced approximate version MoCHy-A+ is up to 25X more\naccurate and 32X faster than the basic approximate and exact versions,\nrespectively.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 01:40:20 GMT"}, {"version": "v2", "created": "Sun, 19 Jul 2020 21:27:01 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Lee", "Geon", ""], ["Ko", "Jihoon", ""], ["Shin", "Kijung", ""]]}, {"id": "2003.01974", "submitter": "Chrysanthi Kosyfaki", "authors": "Chrysanthi Kosyfaki, Nikos Mamoulis, Evaggelia Pitoura, Panayiotis\n  Tsaparas", "title": "Flow Computation in Temporal Interaction Networks", "comments": "13 pages, 27 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Temporal interaction networks capture the history of activities between\nentities along a timeline. At each interaction, some quantity of data (money,\ninformation, kbytes, etc.) flows from one vertex of the network to another.\nFlow-based analysis can reveal important information. For instance, financial\nintelligent units (FIUs) are interested in finding subgraphs in transactions\nnetworks with significant flow of money transfers. In this paper, we introduce\nthe flow computation problem in an interaction network or a subgraph thereof.\nWe propose and study two models of flow computation, one based on a greedy flow\ntransfer assumption and one that finds the maximum possible flow. We show that\nthe greedy flow computation problem can be easily solved by a single scan of\nthe interactions in time order. For the harder maximum flow problem, we propose\ngraph precomputation and simplification approaches that can greatly reduce its\ncomplexity in practice. As an application of flow computation, we formulate and\nsolve the problem of flow pattern search, where, given a graph pattern, the\nobjective is to find its instances and their flows in a large interaction\nnetwork. We evaluate our algorithms using real datasets. The results show that\nthe techniques proposed in this paper can greatly reduce the cost of flow\ncomputation and pattern enumeration.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 09:52:09 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Kosyfaki", "Chrysanthi", ""], ["Mamoulis", "Nikos", ""], ["Pitoura", "Evaggelia", ""], ["Tsaparas", "Panayiotis", ""]]}, {"id": "2003.02090", "submitter": "Cong Yue", "authors": "Cong Yue, Zhongle Xie, Meihui Zhang, Gang Chen, Beng Chin Ooi, Sheng\n  Wang, Xiaokui Xiao", "title": "Analysis of Indexing Structures for Immutable Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In emerging applications such as blockchains and collaborative data\nanalytics, there are strong demands for data immutability, multi-version\naccesses, and tamper-evident controls. This leads to three new index structures\nfor immutable data, namely Merkle Patricia Trie (MPT), Merkle Bucket Tree\n(MBT), and Pattern-Oriented-Split Tree (POS-Tree). Although these structures\nhave been adopted in real applications, there is no systematic evaluation of\ntheir pros and cons in the literature. This makes it difficult for\npractitioners to choose the right index structure for their applications, as\nthere is only a limited understanding of the characteristics of each index.\n  To alleviate the above deficiency, we present a comprehensive analysis of the\nexisting index structures for immutable data, evaluating both their asymptotic\nand empirical performance. Specifically, we show that MPT, MBT, and POS-Tree\nare all instances of a recently proposed framework, dubbed \\my{Structurally\nInvariant and Reusable Indexes (SIRI)}. We propose to evaluate the SIRI\ninstances based on five essential metrics: their efficiency for four index\noperations (i.e., lookup, update, comparison, and merge), as well as their\n\\my{deduplication ratios} (i.e., the size of the index with deduplication over\nthe size without deduplication). We establish the worst-case guarantees of each\nindex in terms of these five metrics, and we experimentally evaluate all\nindexes in a large variety of settings. Based on our theoretical and empirical\nanalysis, we conclude that POS-Tree is a favorable choice for indexing\nimmutable data.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 14:08:01 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 11:30:28 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Yue", "Cong", ""], ["Xie", "Zhongle", ""], ["Zhang", "Meihui", ""], ["Chen", "Gang", ""], ["Ooi", "Beng Chin", ""], ["Wang", "Sheng", ""], ["Xiao", "Xiaokui", ""]]}, {"id": "2003.02320", "submitter": "Aidan Hogan", "authors": "Aidan Hogan, Eva Blomqvist, Michael Cochez, Claudia d'Amato, Gerard de\n  Melo, Claudio Gutierrez, Jos\\'e Emilio Labra Gayo, Sabrina Kirrane, Sebastian\n  Neumaier, Axel Polleres, Roberto Navigli, Axel-Cyrille Ngonga Ngomo, Sabbir\n  M. Rashid, Anisa Rula, Lukas Schmelzeisen, Juan Sequeda, Steffen Staab,\n  Antoine Zimmermann", "title": "Knowledge Graphs", "comments": "Revision from v4: Correcting minor typos and errata involving\n  entailment discussion (former/latter), figure for query rewriting (swap\n  city/venue for location), add more brittle nodes in connectivity; etc", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide a comprehensive introduction to knowledge graphs,\nwhich have recently garnered significant attention from both industry and\nacademia in scenarios that require exploiting diverse, dynamic, large-scale\ncollections of data. After some opening remarks, we motivate and contrast\nvarious graph-based data models and query languages that are used for knowledge\ngraphs. We discuss the roles of schema, identity, and context in knowledge\ngraphs. We explain how knowledge can be represented and extracted using a\ncombination of deductive and inductive techniques. We summarise methods for the\ncreation, enrichment, quality assessment, refinement, and publication of\nknowledge graphs. We provide an overview of prominent open knowledge graphs and\nenterprise knowledge graphs, their applications, and how they use the\naforementioned techniques. We conclude with high-level future research\ndirections for knowledge graphs.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 20:20:32 GMT"}, {"version": "v2", "created": "Sat, 28 Mar 2020 19:39:38 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2020 00:07:00 GMT"}, {"version": "v4", "created": "Fri, 11 Dec 2020 16:16:28 GMT"}, {"version": "v5", "created": "Sun, 24 Jan 2021 02:06:48 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Hogan", "Aidan", ""], ["Blomqvist", "Eva", ""], ["Cochez", "Michael", ""], ["d'Amato", "Claudia", ""], ["de Melo", "Gerard", ""], ["Gutierrez", "Claudio", ""], ["Gayo", "Jos\u00e9 Emilio Labra", ""], ["Kirrane", "Sabrina", ""], ["Neumaier", "Sebastian", ""], ["Polleres", "Axel", ""], ["Navigli", "Roberto", ""], ["Ngomo", "Axel-Cyrille Ngonga", ""], ["Rashid", "Sabbir M.", ""], ["Rula", "Anisa", ""], ["Schmelzeisen", "Lukas", ""], ["Sequeda", "Juan", ""], ["Staab", "Steffen", ""], ["Zimmermann", "Antoine", ""]]}, {"id": "2003.02391", "submitter": "Huanchen Zhang", "authors": "Huanchen Zhang, Xiaoxuan Liu, David G. Andersen, Michael Kaminsky,\n  Kimberly Keeton, Andrew Pavlo", "title": "Order-Preserving Key Compression for In-Memory Search Trees", "comments": "SIGMOD'20 version + Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the High-speed Order-Preserving Encoder (HOPE) for in-memory\nsearch trees. HOPE is a fast dictionary-based compressor that encodes arbitrary\nkeys while preserving their order. HOPE's approach is to identify common key\npatterns at a fine granularity and exploit the entropy to achieve high\ncompression rates with a small dictionary. We first develop a theoretical model\nto reason about order-preserving dictionary designs. We then select six\nrepresentative compression schemes using this model and implement them in HOPE.\nThese schemes make different trade-offs between compression rate and encoding\nspeed. We evaluate HOPE on five data structures used in databases: SuRF, ART,\nHOT, B+tree, and Prefix B+tree. Our experiments show that using HOPE allows the\nsearch trees to achieve lower query latency (up to 40\\% lower) and better\nmemory efficiency (up to 30\\% smaller) simultaneously for most string key\nworkloads.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 01:02:47 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Zhang", "Huanchen", ""], ["Liu", "Xiaoxuan", ""], ["Andersen", "David G.", ""], ["Kaminsky", "Michael", ""], ["Keeton", "Kimberly", ""], ["Pavlo", "Andrew", ""]]}, {"id": "2003.02446", "submitter": "Hongzhi Wang", "authors": "Meifan Zhang and Hongzhi Wang", "title": "LAQP: Learning-based Approximate Query Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Querying on big data is a challenging task due to the rapid growth of data\namount. Approximate query processing (AQP) is a way to meet the requirement of\nfast response. In this paper, we propose a learning-based AQP method called the\nLAQP. The LAQP builds an error model learned from the historical queries to\npredict the sampling-based estimation error of each new query. It makes a\ncombination of the sampling-based AQP, the pre-computed aggregations and the\nlearned error model to provide high-accurate query estimations with a small\noff-line sample. The experimental results indicate that our LAQP outperforms\nthe sampling-based AQP, the pre-aggregation-based AQP and the most recent\nlearning-based AQP method.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 06:08:25 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Zhang", "Meifan", ""], ["Wang", "Hongzhi", ""]]}, {"id": "2003.02521", "submitter": "Antoine Amarilli", "authors": "Antoine Amarilli, Michael Benedikt", "title": "Finite Open-World Query Answering with Number Restrictions", "comments": "70 pages. Extended journal version of arXiv:1505.04216. This article\n  is the same as what will be published in ToCL, except for publisher-induced\n  changes, minor changes, and reordering of the material (in the ToCL version\n  some detailed proofs are moved from the article body to an appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open-world query answering is the problem of deciding, given a set of facts,\nconjunction of constraints, and query, whether the facts and constraints imply\nthe query. This amounts to reasoning over all instances that include the facts\nand satisfy the constraints. We study finite open-world query answering (FQA),\nwhich assumes that the underlying world is finite and thus only considers the\nfinite completions of the instance. The major known decidable cases of FQA\nderive from the following: the guarded fragment of first-order logic, which can\nexpress referential constraints (data in one place points to data in another)\nbut cannot express number restrictions such as functional dependencies; and the\nguarded fragment with number restrictions but on a signature of arity only two.\nIn this paper, we give the first decidability results for FQA that combine both\nreferential constraints and number restrictions for arbitrary signatures: we\nshow that, for unary inclusion dependencies and functional dependencies, the\nfiniteness assumption of FQA can be lifted up to taking the finite implication\nclosure of the dependencies. Our result relies on new techniques to construct\nfinite universal models of such constraints, for any bound on the maximal query\nsize.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 10:21:04 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Amarilli", "Antoine", ""], ["Benedikt", "Michael", ""]]}, {"id": "2003.02542", "submitter": "Zheng Wang", "authors": "Zheng Wang, Cheng Long, Gao Cong, Yiding Liu", "title": "Efficient and Effective Similar Subtrajectory Search with Deep\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similar trajectory search is a fundamental problem and has been well studied\nover the past two decades. However, the similar subtrajectory search (SimSub)\nproblem, aiming to return a portion of a trajectory (i.e., a subtrajectory)\nwhich is the most similar to a query trajectory, has been mostly disregarded\ndespite that it could capture trajectory similarity in a finer-grained way and\nmany applications take subtrajectories as basic units for analysis. In this\npaper, we study the SimSub problem and develop a suite of algorithms including\nboth exact and approximate ones. Among those approximate algorithms, two that\nare based on deep reinforcement learning stand out and outperform those\nnon-learning based algorithms in terms of effectiveness and efficiency. We\nconduct experiments on real-world trajectory datasets, which verify the\neffectiveness and efficiency of the proposed algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 11:38:21 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 07:34:08 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Wang", "Zheng", ""], ["Long", "Cheng", ""], ["Cong", "Gao", ""], ["Liu", "Yiding", ""]]}, {"id": "2003.02576", "submitter": "Antoine Amarilli", "authors": "Antoine Amarilli, Pierre Bourhis, Stefan Mengel, Matthias Niewerth", "title": "Constant-Delay Enumeration for Nondeterministic Document Spanners", "comments": "29 pages. Extended version of arXiv:1807.09320. Integrates all\n  corrections following reviewer feedback. Outside of some minor formatting\n  differences and tweaks, this paper is the same as the paper to appear in the\n  ACM TODS journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the information extraction framework known as document spanners,\nand study the problem of efficiently computing the results of the extraction\nfrom an input document, where the extraction task is described as a sequential\nvariable-set automaton (VA). We pose this problem in the setting of enumeration\nalgorithms, where we can first run a preprocessing phase and must then produce\nthe results with a small delay between any two consecutive results. Our goal is\nto have an algorithm which is tractable in combined complexity, i.e., in the\nsizes of the input document and the VA; while ensuring the best possible data\ncomplexity bounds in the input document size, i.e., constant delay in the\ndocument size. Several recent works at PODS'18 proposed such algorithms but\nwith linear delay in the document size or with an exponential dependency in\nsize of the (generally nondeterministic) input VA. In particular, Florenzano et\nal. suggest that our desired runtime guarantees cannot be met for general\nsequential VAs. We refute this and show that, given a nondeterministic\nsequential VA and an input document, we can enumerate the mappings of the VA on\nthe document with the following bounds: the preprocessing is linear in the\ndocument size and polynomial in the size of the VA, and the delay is\nindependent of the document and polynomial in the size of the VA. The resulting\nalgorithm thus achieves tractability in combined complexity and the best\npossible data complexity bounds. Moreover, it is rather easy to describe, in\nparticular for the restricted case of so-called extended VAs. Finally, we\nevaluate our algorithm empirically using a prototype implementation.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 12:49:56 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2020 07:48:10 GMT"}, {"version": "v3", "created": "Mon, 7 Dec 2020 13:51:51 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Amarilli", "Antoine", ""], ["Bourhis", "Pierre", ""], ["Mengel", "Stefan", ""], ["Niewerth", "Matthias", ""]]}, {"id": "2003.02615", "submitter": "Imad Afyouni", "authors": "Faizan Ur Rehman, Imad Afyouni, Ahmed Lbath, Saleh Basalamah", "title": "Hadath: From Social Media Mapping to Multi-Resolution Event-Enriched\n  Maps", "comments": "Proceedings of 16th International Conference on Computer Systems and\n  Applications (AICCSA 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Publicly available data is increasing rapidly, and will continue to grow with\nthe advancement of technologies in sensors, smartphones and the Internet of\nThings. Data from multiple sources can improve coverage and provide more\nrelevant knowledge about surrounding events and points of Interest. The\nstrength of one source of data can compensate for the shortcomings of another\nsource by providing supplementary information. Maps are also getting popular\nday-by-day and people are using it to achieve their daily task smoothly and\nefficiently. Starting from paper maps hundred years ago, multiple type of maps\nare available with point of interest, real-time traffic update or displaying\nmicro-blogs from social media. In this paper, we introduce Hadath, a system\nthat displays multi-resolution live events of interest from a variety of\navailable data sources. The system has been designed to be able to handle\nmultiple type of inputs by encapsulating incoming unstructured data into\ngeneric data packets. System extracts local events of interest from generic\ndata packets and identify their spatio-temporal scope to display such events on\na map, so that as a user changes the zoom level, only events of appropriate\nscope are displayed. This allows us to show live events in correspondence to\nthe scale of view - when viewing at a city scale, we see events of higher\nsignificance, while zooming in to a neighbourhood, events of a more local\ninterest are highlighted. The final output creates a unique and dynamic map\nbrowsing experience. Finally, to validate our proposed system, we conducted\nexperiments on social media data.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 13:53:59 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Rehman", "Faizan Ur", ""], ["Afyouni", "Imad", ""], ["Lbath", "Ahmed", ""], ["Basalamah", "Saleh", ""]]}, {"id": "2003.02829", "submitter": "Wolfgang Gatterbauer", "authors": "Krishna Kumar P. and Paul Langton and Wolfgang Gatterbauer", "title": "Factorized Graph Representations for Semi-Supervised Learning from\n  Sparse Data", "comments": "SIGMOD 2020 (Extended version)", "journal-ref": null, "doi": "10.1145/3318464.3380577", "report-no": null, "categories": "cs.LG cs.DB cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Node classification is an important problem in graph data management. It is\ncommonly solved by various label propagation methods that work iteratively\nstarting from a few labeled seed nodes. For graphs with arbitrary\ncompatibilities between classes, these methods crucially depend on knowing the\ncompatibility matrix that must be provided by either domain experts or\nheuristics. Can we instead directly estimate the correct compatibilities from a\nsparsely labeled graph in a principled and scalable way? We answer this\nquestion affirmatively and suggest a method called distant compatibility\nestimation that works even on extremely sparsely labeled graphs (e.g., 1 in\n10,000 nodes is labeled) in a fraction of the time it later takes to label the\nremaining nodes. Our approach first creates multiple factorized graph\nrepresentations (with size independent of the graph) and then performs\nestimation on these smaller graph sketches. We define algebraic amplification\nas the more general idea of leveraging algebraic properties of an algorithm's\nupdate equations to amplify sparse signals. We show that our estimator is by\norders of magnitude faster than an alternative approach and that the end-to-end\nclassification accuracy is comparable to using gold standard compatibilities.\nThis makes it a cheap preprocessing step for any existing label propagation\nmethod and removes the current dependence on heuristics.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 18:57:45 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["P.", "Krishna Kumar", ""], ["Langton", "Paul", ""], ["Gatterbauer", "Wolfgang", ""]]}, {"id": "2003.03079", "submitter": "Yuya Sasaki", "authors": "Yuya Sasaki, George Fletcher, Makoto Onizuka", "title": "Structural Indexing for Conjunctive Path Queries", "comments": "14 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural indexing is an approach to accelerating query evaluation, whereby\ndata objects are partitioned and indexed reflecting the precise expressive\npower of a given query language. Each partition block of the index holds\nexactly those objects that are indistinguishable with respect to queries\nexpressible in the language. Structural indexes have proven successful for XML,\nRDF, and relational data management. In this paper we study structural indexing\nfor conjunctive path queries (CPQ). CPQ forms the core of contemporary graph\nquery languages such as SPARQL, Cypher, PGQL, and G-CORE. CPQ plays the same\nfundamental role with respect to contemporary graph query languages as the\nclassic conjunctive queries play for SQL. We develop the first practical\nstructural indexes for this important query language. In particular, we propose\na structural index based on k-path-bisimulation, tightly coupled to the\nexpressive power of CPQ, and develop algorithms for efficient query processing\nwith our index. Furthermore, we study workload-aware structural indexes to\nreduce both the construction and space costs according to a given workload. We\ndemonstrate through extensive experiments using real and synthetic graphs that\nour methods accelerate query processing by up to multiple orders of magnitude\nover the state-of-the-art methods, without increasing index size.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 08:50:10 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Sasaki", "Yuya", ""], ["Fletcher", "George", ""], ["Onizuka", "Makoto", ""]]}, {"id": "2003.03155", "submitter": "Shrestha Ghosh", "authors": "Shrestha Ghosh, Simon Razniewski, Gerhard Weikum", "title": "Uncovering Hidden Semantics of Set Information in Knowledge Bases", "comments": "This work is under review in the Journal of Web Semantics, Special\n  Issue on Language Technology and Knowledge Graphs. This is a revision draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Bases (KBs) contain a wealth of structured information about\nentities and predicates. This paper focuses on set-valued predicates, i.e., the\nrelationship between an entity and a set of entities. In KBs, this information\nis often represented in two formats: (i) via counting predicates such as\nnumberOfChildren and staffSize, that store aggregated integers, and (ii) via\nenumerating predicates such as parentOf and worksFor, that store individual set\nmemberships. Both formats are typically complementary: unlike enumerating\npredicates, counting predicates do not give away individuals, but are more\nlikely informative towards the true set size, thus this coexistence could\nenable interesting applications in question answering and KB curation.\n  In this paper we aim at uncovering this hidden knowledge. We proceed in two\nsteps. (i) We identify set-valued predicates from a given KB predicates via\nstatistical and embedding-based features. (ii) We link counting predicates and\nenumerating predicates by a combination of co-occurrence, correlation and\ntextual relatedness metrics. We analyze the prevalence of count information in\nfour prominent knowledge bases, and show that our linking method achieves up to\n0.55 F1 score in set predicate identification versus 0.40 F1 score of a random\nselection, and normalized discounted gains of up to 0.84 at position 1 and 0.75\nat position 3 in relevant predicate alignments. Our predicate alignments are\nshowcased in a demonstration system available at\nhttps://counqer.mpi-inf.mpg.de/spo.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 12:26:00 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2020 10:20:33 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Ghosh", "Shrestha", ""], ["Razniewski", "Simon", ""], ["Weikum", "Gerhard", ""]]}, {"id": "2003.03505", "submitter": "Tao Gu", "authors": "Wenwei Xue, Hungkeng Pung, Wenlong Ng, Tao Gu", "title": "Data Management for Context-Aware Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We envisage future context-aware applications will dynamically adapt their\nbehaviors to various context data from sources in wide-area networks, such as\nthe Internet. Facing the changing context and the sheer number of context\nsources, a data management system that supports effective source organization\nand efficient data lookup becomes crucial to the easy development of\ncontext-aware applications. In this paper, we propose the design of a new\ncontext data management system that is equipped with query processing\ncapabilities. We encapsulate the context sources into physical spaces belonging\nto different context spaces and organize them as peers in semantic overlay\nnetworks. Initial evaluation results of an experimental system prototype\ndemonstrate the effectiveness of our design\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 03:34:14 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Xue", "Wenwei", ""], ["Pung", "Hungkeng", ""], ["Ng", "Wenlong", ""], ["Gu", "Tao", ""]]}, {"id": "2003.03734", "submitter": "Gong Cheng", "authors": "Qingxia Liu, Gong Cheng, Kalpa Gunaratna, Yuzhong Qu", "title": "ESBM: An Entity Summarization BenchMark", "comments": "16 pages, accepted to the Resource Track of ESWC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Entity summarization is the problem of computing an optimal compact summary\nfor an entity by selecting a size-constrained subset of triples from RDF data.\nEntity summarization supports a multiplicity of applications and has led to\nfruitful research. However, there is a lack of evaluation efforts that cover\nthe broad spectrum of existing systems. One reason is a lack of benchmarks for\nevaluation. Some benchmarks are no longer available, while others are small and\nhave limitations. In this paper, we create an Entity Summarization BenchMark\n(ESBM) which overcomes the limitations of existing benchmarks and meets\nstandard desiderata for a benchmark. Using this largest available benchmark for\nevaluating general-purpose entity summarizers, we perform the most extensive\nexperiment to date where 9~existing systems are compared. Considering that all\nof these systems are unsupervised, we also implement and evaluate a supervised\nlearning based system for reference.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 07:12:20 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Liu", "Qingxia", ""], ["Cheng", "Gong", ""], ["Gunaratna", "Kalpa", ""], ["Qu", "Yuzhong", ""]]}, {"id": "2003.03736", "submitter": "Gong Cheng", "authors": "Qingxia Liu, Gong Cheng, Yuzhong Qu", "title": "DeepLENS: Deep Learning for Entity Summarization", "comments": "6 pages, submitted to DL4KG 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Entity summarization has been a prominent task over knowledge graphs. While\nexisting methods are mainly unsupervised, we present DeepLENS, a simple yet\neffective deep learning model where we exploit textual semantics for encoding\ntriples and we score each candidate triple based on its interdependence on\nother triples. DeepLENS significantly outperformed existing methods on a public\nbenchmark.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 07:15:48 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Liu", "Qingxia", ""], ["Cheng", "Gong", ""], ["Qu", "Yuzhong", ""]]}, {"id": "2003.03785", "submitter": "Zhangsheng Lai", "authors": "Zhangsheng Lai, Aik Beng Ng, Liang Ze Wong, Simon See, and Shaowei Lin", "title": "Dependently Typed Knowledge Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reasoning over knowledge graphs is traditionally built upon a hierarchy of\nlanguages in the Semantic Web Stack. Starting from the Resource Description\nFramework (RDF) for knowledge graphs, more advanced constructs have been\nintroduced through various syntax extensions to add reasoning capabilities to\nknowledge graphs. In this paper, we show how standardized semantic web\ntechnologies (RDF and its query language SPARQL) can be reproduced in a unified\nmanner with dependent type theory. In addition to providing the basic\nfunctionalities of knowledge graphs, dependent types add expressiveness in\nencoding both entities and queries, explainability in answers to queries\nthrough witnesses, and compositionality and automation in the construction of\nwitnesses. Using the Coq proof assistant, we demonstrate how to build and query\ndependently typed knowledge graphs as a proof of concept for future works in\nthis direction.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 14:04:23 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Lai", "Zhangsheng", ""], ["Ng", "Aik Beng", ""], ["Wong", "Liang Ze", ""], ["See", "Simon", ""], ["Lin", "Shaowei", ""]]}, {"id": "2003.04074", "submitter": "Chaimae Asaad", "authors": "Chaimae Asaad, Karim Ba\\\"ina, Mounir Ghogho", "title": "NoSQL Databases: Yearning for Disambiguation", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The demanding requirements of the new Big Data intensive era raised the need\nfor flexible storage systems capable of handling huge volumes of unstructured\ndata and of tackling the challenges that traditional databases were facing.\nNoSQL Databases, in their heterogeneity, are a powerful and diverse set of\ndatabases tailored to specific industrial and business needs. However, the lack\nof theoretical background creates a lack of consensus even among experts about\nmany NoSQL concepts, leading to ambiguity and confusion. In this paper, we\npresent a survey of NoSQL databases and their classification by data model\ntype. We also conduct a benchmark in order to compare different NoSQL databases\nand distinguish their characteristics. Additionally, we present the major areas\nof ambiguity and confusion around NoSQL databases and their related concepts,\nand attempt to disambiguate them.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 12:35:46 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 12:20:05 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Asaad", "Chaimae", ""], ["Ba\u00efna", "Karim", ""], ["Ghogho", "Mounir", ""]]}, {"id": "2003.04372", "submitter": "Mujahid Sultan", "authors": "Mujahid Sultan", "title": "Probabilistic Partitive Partitioning (PPP)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is a NP-hard problem. Thus, no optimal algorithm exists,\nheuristics are applied to cluster the data. Heuristics can be very\nresource-intensive, if not applied properly. For substantially large data sets\ncomputational efficiencies can be achieved by reducing the input space if a\nminimal loss of information can be achieved. Clustering algorithms, in general,\nface two common problems: 1) these converge to different settings with\ndifferent initial conditions and; 2) the number of clusters has to be\narbitrarily decided beforehand. This problem has become critical in the realm\nof big data. Recently, clustering algorithms have emerged which can speedup\ncomputations using parallel processing over the grid but face the\naforementioned problems. Goals: Our goals are to find methods to cluster data\nwhich: 1) guarantee convergence to the same settings irrespective of the\ninitial conditions; 2) eliminate the need to establish the number of clusters\nbeforehand, and 3) can be applied to cluster large datasets. Methods: We\nintroduce a method that combines probabilistic and combinatorial clustering\nmethods to produce repeatable and compact clusters that are not sensitive to\ninitial conditions. This method harnesses the power of k-means (a combinatorial\nclustering method) to cluster/partition very large dimensional datasets and\nuses the Gaussian Mixture Model (a probabilistic clustering method) to validate\nthe k-means partitions. Results: We show that this method produces very compact\nclusters that are not sensitive to initial conditions. This method can be used\nto identify the most 'separable' set in a dataset which increases the\n'clusterability' of a dataset. This method also eliminates the need to specify\nthe number of clusters in advance.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 19:18:35 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Sultan", "Mujahid", ""]]}, {"id": "2003.04410", "submitter": "Wentao Wu", "authors": "Wentao Wu", "title": "A Note On Operator-Level Query Execution Cost Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  External query execution cost modeling using query execution feedback has\nfound its way in various database applications such as admission control and\nquery scheduling. Existing techniques in general fall into two categories,\nplan-level cost modeling and operator-level cost modeling. It has been shown in\nthe literature that operator-level cost modeling can often significantly\noutperform plan-level cost modeling. In this paper, we study operator-level\ncost modeling from a robustness perspective. We address two main challenges in\npractice regarding limited execution feedback (for certain operators) and mixed\ncost estimates due to the use of multiple cost modeling techniques. We propose\na framework that deals with these issues and present a comprehensive analysis\nof this framework. We further provide a case study to demonstrate the efficacy\nof our framework in the context of index tuning, which is itself a new\napplication of external cost modeling techniques.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 21:03:52 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Wu", "Wentao", ""]]}, {"id": "2003.04411", "submitter": "Wim Martens", "authors": "Diego Figueira and Adwait Godbole and S. Krishna and Wim Martens and\n  Matthias Niewerth and Tina Trautner", "title": "Containment of Simple Regular Path Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing containment of queries is a fundamental reasoning task in knowledge\nrepresentation. We study here the containment problem for Conjunctive Regular\nPath Queries (CRPQs), a navigational query language extensively used in\nontology and graph database querying. While it is known that containment of\nCRPQs is expspace-complete in general, we focus here on severely restricted\nfragments, which are known to be highly relevant in practice according to\nseveral recent studies. We obtain a detailed overview of the complexity of the\ncontainment problem, depending on the features used in the regular expressions\nof the queries, with completeness results for np, pitwo, pspace or expspace.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 21:05:29 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Figueira", "Diego", ""], ["Godbole", "Adwait", ""], ["Krishna", "S.", ""], ["Martens", "Wim", ""], ["Niewerth", "Matthias", ""], ["Trautner", "Tina", ""]]}, {"id": "2003.04470", "submitter": "Vuong M. Ngo", "authors": "V.M. Ngo, N.A. Le-Khac, and M.T. Kechadi", "title": "Data Warehouse and Decision Support on Integrated Crop Big Data", "comments": "13 pages, 11 figures. arXiv admin note: text overlap with\n  arXiv:1905.12411", "journal-ref": "International Journal of Business Process Integration and\n  Management 2020 Vol.10 No.1", "doi": "10.1504/IJBPIM.2020.113115", "report-no": null, "categories": "cs.DB cs.DC cs.LG cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, precision agriculture is becoming very popular. The\nintroduction of modern information and communication technologies for\ncollecting and processing Agricultural data revolutionise the agriculture\npractises. This has started a while ago (early 20th century) and it is driven\nby the low cost of collecting data about everything; from information on fields\nsuch as seed, soil, fertiliser, pest, to weather data, drones and satellites\nimages. Specially, the agricultural data mining today is considered as Big Data\napplication in terms of volume, variety, velocity and veracity. Hence it leads\nto challenges in processing vast amounts of complex and diverse information to\nextract useful knowledge for the farmer, agronomist, and other businesses. It\nis a key foundation to establishing a crop intelligence platform, which will\nenable efficient resource management and high quality agronomy decision making\nand recommendations. In this paper, we designed and implemented a continental\nlevel agricultural data warehouse (ADW). ADW is characterised by its (1)\nflexible schema; (2) data integration from real agricultural multi datasets;\n(3) data science and business intelligent support; (4) high performance; (5)\nhigh storage; (6) security; (7) governance and monitoring; (8) consistency,\navailability and partition tolerant; (9) cloud compatibility. We also evaluate\nthe performance of ADW and present some complex queries to extract and return\nnecessary knowledge about crop management.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 00:10:22 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 08:45:11 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Ngo", "V. M.", ""], ["Le-Khac", "N. A.", ""], ["Kechadi", "M. T.", ""]]}, {"id": "2003.04915", "submitter": "Renan Souza", "authors": "Raphael Thiago, Renan Souza, L. Azevedo, E. Soares, Rodrigo Santos,\n  Wallas Santos, Max De Bayser, M. Cardoso, M. Moreno, and Renato Cerqueira", "title": "Managing Data Lineage of O&G Machine Learning Models: The Sweet Spot for\n  Shale Use Case", "comments": "Author preprint of paper accepted at the 2020 European Association of\n  Geoscientists and Engineers (EAGE) Digitalization Conference and Exhibition", "journal-ref": "2020 European Association of Geoscientists and Engineers (EAGE)\n  Digitalization Conference and Exhibition", "doi": null, "report-no": null, "categories": "cs.DB cs.CY cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning (ML) has increased its role, becoming essential in several\nindustries. However, questions around training data lineage, such as \"where has\nthe dataset used to train this model come from?\"; the introduction of several\nnew data protection legislation; and, the need for data governance\nrequirements, have hindered the adoption of ML models in the real world. In\nthis paper, we discuss how data lineage can be leveraged to benefit the ML\nlifecycle to build ML models to discover sweet-spots for shale oil and gas\nproduction, a major application in the Oil and Gas O&G Industry.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 18:10:16 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Thiago", "Raphael", ""], ["Souza", "Renan", ""], ["Azevedo", "L.", ""], ["Soares", "E.", ""], ["Santos", "Rodrigo", ""], ["Santos", "Wallas", ""], ["De Bayser", "Max", ""], ["Cardoso", "M.", ""], ["Moreno", "M.", ""], ["Cerqueira", "Renato", ""]]}, {"id": "2003.04969", "submitter": "Shantanu Sharma", "authors": "Nisha Panwar, Shantanu Sharma, Peeyush Gupta, Dhrubajyoti Ghosh,\n  Sharad Mehrotra, Nalini Venkatasubramanian", "title": "IoT Expunge: Implementing Verifiable Retention of IoT Data", "comments": "This paper has been accepted in 10th ACM Conference on Data and\n  Application Security and Privacy (CODASPY), 2020", "journal-ref": null, "doi": "10.1145/3374664.3375737", "report-no": null, "categories": "cs.CR cs.DB cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing deployment of Internet of Things (IoT) systems aims to ease the\ndaily life of end-users by providing several value-added services. However, IoT\nsystems may capture and store sensitive, personal data about individuals in the\ncloud, thereby jeopardizing user-privacy. Emerging legislation, such as\nCalifornia's CalOPPA and GDPR in Europe, support strong privacy laws to protect\nan individual's data in the cloud. One such law relates to strict enforcement\nof data retention policies. This paper proposes a framework, entitled IoT\nExpunge that allows sensor data providers to store the data in cloud platforms\nthat will ensure enforcement of retention policies. Additionally, the cloud\nprovider produces verifiable proofs of its adherence to the retention policies.\nExperimental results on a real-world smart building testbed show that IoT\nExpunge imposes minimal overheads to the user to verify the data against data\nretention policies.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 20:55:01 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Panwar", "Nisha", ""], ["Sharma", "Shantanu", ""], ["Gupta", "Peeyush", ""], ["Ghosh", "Dhrubajyoti", ""], ["Mehrotra", "Sharad", ""], ["Venkatasubramanian", "Nalini", ""]]}, {"id": "2003.05043", "submitter": "Vuong M. Ngo", "authors": "Vuong M. Ngo and M-Tahar Kechadi", "title": "Crop Knowledge Discovery Based on Agricultural Big Data Integration", "comments": "5 pages", "journal-ref": "ICMLSC-2020", "doi": null, "report-no": null, "categories": "cs.DB cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, the agricultural data can be generated through various sources,\nsuch as: Internet of Thing (IoT), sensors, satellites, weather stations,\nrobots, farm equipment, agricultural laboratories, farmers, government agencies\nand agribusinesses. The analysis of this big data enables farmers, companies\nand agronomists to extract high business and scientific knowledge, improving\ntheir operational processes and product quality. However, before analysing this\ndata, different data sources need to be normalised, homogenised and integrated\ninto a unified data representation. In this paper, we propose an agricultural\ndata integration method using a constellation schema which is designed to be\nflexible enough to incorporate other datasets and big data models. We also\napply some methods to extract knowledge with the view to improve crop yield;\nthese include finding suitable quantities of soil properties, herbicides and\ninsecticides for both increasing crop yield and protecting the environment.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 00:13:17 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Ngo", "Vuong M.", ""], ["Kechadi", "M-Tahar", ""]]}, {"id": "2003.05113", "submitter": "Senthil Nathan", "authors": "Parth Thakkar, Senthilnathan Natarajan", "title": "Scaling Hyperledger Fabric Using Pipelined Execution and Sparse Peers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Permissioned blockchains are becoming popular as data management systems in\nthe enterprise setting. Compared to traditional distributed databases,\nblockchain platforms provide increased security guarantees but significantly\nlower performance. Further, these platforms are quite expensive to run for the\nlow throughput they provide. The following are two ways to improve performance\nand reduce cost: (1) make the system utilize allocated resources efficiently;\n(2) allow rapid and dynamic scaling of allocated resources based on load. We\nexplore both of these in this work.\n  We first investigate the reasons for the poor performance and scalability of\nthe dominant permissioned blockchain flavor called Execute-Order-Validate\n(EOV). We do this by studying the scaling characteristics of Hyperledger\nFabric, a popular EOV platform, using vertical scaling and horizontal scaling.\nWe find that the transaction throughput scales very poorly with these\ntechniques. At least in the permissioned setting, the real bottleneck is\ntransaction processing, not the consensus protocol. With vertical scaling, the\nallocated vCPUs go under-utilized. In contrast, with horizontal scaling, the\nallocated resources get wasted due to redundant work across nodes within an\norganization.\n  To mitigate the above concerns, we first improve resource efficiency by (a)\nimproving CPU utilization with a pipelined execution of validation & commit\nphases; (b) avoiding redundant work across nodes by introducing a new type of\npeer node called sparse peer that selectively commits transactions. We further\npropose a technique that enables the rapid scaling of resources. Our\nimplementation - SmartFabric, built on top of Hyperledger Fabric demonstrates\n3x higher throughput, 12-26x faster scale-up time, and provides Fabric's\nthroughput at 50% to 87% lower cost.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 05:02:12 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 06:36:48 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Thakkar", "Parth", ""], ["Natarajan", "Senthilnathan", ""]]}, {"id": "2003.05238", "submitter": "Farah Karim Ms", "authors": "Farah Karim and Maria-Esther Vidal and S\\\"oren Auer", "title": "Compacting Frequent Star Patterns in RDF Graphs", "comments": null, "journal-ref": null, "doi": "10.1007/s10844-020-00595-9", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graphs have become a popular formalism for representing entities\nand their properties using a graph data model, e.g., the Resource Description\nFramework (RDF). An RDF graph comprises entities of the same type connected to\nobjects or other entities using labeled edges annotated with properties. RDF\ngraphs usually contain entities that share the same objects in a certain group\nof properties, i.e., they match star patterns composed of these properties and\nobjects. In case the number of these entities or properties in these star\npatterns is large, the size of the RDF graph and query processing are\nnegatively impacted; we refer these star patterns as frequent star patterns. We\naddress the problem of identifying frequent star patterns in RDF graphs and\ndevise the concept of factorized RDF graphs, which denote compact\nrepresentations of RDF graphs where the number of frequent star patterns is\nminimized. We also develop computational methods to identify frequent star\npatterns and generate a factorized RDF graph, where compact RDF molecules\nreplace frequent star patterns. A compact RDF molecule of a frequent star\npattern denotes an RDF subgraph that instantiates the corresponding star\npattern. Instead of having all the entities matching the original frequent star\npattern, a surrogate entity is added and related to the properties of the\nfrequent star pattern; it is linked to the entities that originally match the\nfrequent star pattern. We evaluate the performance of our factorization\ntechniques on several RDF graph benchmarks and compare with a baseline built on\ntop of gSpan, a state-of-the-art algorithm to detect frequent patterns. The\noutcomes evidence the efficiency of proposed approach and show that our\ntechniques are able to reduce execution time of the baseline approach in at\nleast three orders of magnitude reducing the RDF graph size by up to 66.56%.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 11:55:37 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Karim", "Farah", ""], ["Vidal", "Maria-Esther", ""], ["Auer", "S\u00f6ren", ""]]}, {"id": "2003.05575", "submitter": "Sai Vikneshwar Mani Jayaraman", "authors": "Michael Langberg, Shi Li, Sai Vikneshwar Mani Jayaraman, and Atri\n  Rudra", "title": "Topology Dependent Bounds For FAQs", "comments": "A conference version was presented at PODS 2019", "journal-ref": null, "doi": "10.1145/3294052.3319686", "report-no": null, "categories": "cs.DC cs.CC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we prove topology dependent bounds on the number of rounds\nneeded to compute Functional Aggregate Queries (FAQs) studied by Abo Khamis et\nal. [PODS 2016] in a synchronous distributed network under the model considered\nby Chattopadhyay et al. [FOCS 2014, SODA 2017]. Unlike the recent work on\ncomputing database queries in the Massively Parallel Computation model, in the\nmodel of Chattopadhyay et al., nodes can communicate only via private\npoint-to-point channels and we are interested in bounds that work over an {\\em\narbitrary} communication topology. This is the first work to consider more\npractically motivated problems in this distributed model. For the sake of\nexposition, we focus on two special problems in this paper: Boolean Conjunctive\nQuery (BCQ) and computing variable/factor marginals in Probabilistic Graphical\nModels (PGMs). We obtain tight bounds on the number of rounds needed to compute\nsuch queries as long as the underlying hypergraph of the query is\n$O(1)$-degenerate and has $O(1)$-arity. In particular, the $O(1)$-degeneracy\ncondition covers most well-studied queries that are efficiently computable in\nthe centralized computation model like queries with constant treewidth. These\ntight bounds depend on a new notion of `width' (namely internal-node-width) for\nGeneralized Hypertree Decompositions (GHDs) of acyclic hypergraphs, which\nminimizes the number of internal nodes in a sub-class of GHDs. To the best of\nour knowledge, this width has not been studied explicitly in the theoretical\ndatabase literature. Finally, we consider the problem of computing the product\nof a vector with a chain of matrices and prove tight bounds on its round\ncomplexity (over the finite field of two elements) using a novel min-entropy\nbased argument.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 02:23:26 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Langberg", "Michael", ""], ["Li", "Shi", ""], ["Jayaraman", "Sai Vikneshwar Mani", ""], ["Rudra", "Atri", ""]]}, {"id": "2003.05686", "submitter": "Shovanur Haque", "authors": "Shovanur Haque, Kerrie Mengersen", "title": "Assessing the accuracy of individual link with varying block sizes and\n  cut-off values using MaCSim approach", "comments": "24 pages, 6 figures. arXiv admin note: text overlap with\n  arXiv:1901.04779", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DB stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Record linkage is the process of matching together records from different\ndata sources that belong to the same entity. Record linkage is increasingly\nbeing used by many organizations including statistical, health, government etc.\nto link administrative, survey, and other files to create a robust file for\nmore comprehensive analysis. Therefore, it becomes necessary to assess the\nability of a linking method to achieve high accuracy or compare between methods\nwith respect to accuracy. In this paper, we evaluate the accuracy of individual\nlink using varying block sizes and different cut-off values by utilizing a\nMarkov Chain based Monte Carlo simulation approach (MaCSim). MaCSim utilizes\ntwo linked files to create an agreement matrix. The agreement matrix is\nsimulated to generate re-sampled versions of the agreement matrix. A defined\nlinking method is used in each simulation to link the files and the accuracy of\nthe linking method is assessed. The aim of this paper is to facilitate optimal\nchoice of block size and cut-off value to achieve high accuracy in terms of\nminimizing average False Discovery Rate and False Negative Rate. The analyses\nhave been performed using a synthetic dataset provided by the Australian Bureau\nof Statistics (ABS) and indicated promising results.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 10:03:20 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 04:46:31 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2020 22:50:28 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Haque", "Shovanur", ""], ["Mengersen", "Kerrie", ""]]}, {"id": "2003.05687", "submitter": "Mayank Raikwar", "authors": "Mayank Raikwar, Danilo Gligoroski, Goran Velinov", "title": "Trends in Development of Databases and Blockchain", "comments": "Accepted in The Second International Workshop on Blockchain\n  Applications and Theory (BAT 2020)", "journal-ref": "2020 Seventh International Conference on Software Defined Systems\n  (SDS)", "doi": "10.1109/SDS49854.2020.9143893", "report-no": null, "categories": "cs.DC cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is about the mutual influence between two technologies: Databases\nand Blockchain. It addresses two questions: 1. How the database technology has\ninfluenced the development of blockchain technology?, and 2. How blockchain\ntechnology has influenced the introduction of new functionalities in some\nmodern databases? For the first question, we explain how database technology\ncontributes to blockchain technology by unlocking different features such as\nACID (Atomicity, Consistency, Isolation, and Durability) transactional\nconsistency, rich queries, real-time analytics, and low latency. We explain how\nthe CAP (Consistency, Availability, Partition tolerance) theorem known for\ndatabases influenced the DCS (Decentralization, Consistency, Scalability)\ntheorem for the blockchain systems. By using an analogous relaxation approach\nas it was used for the proof of the CAP theorem, we postulate a\n\"DCS-satisfiability conjecture.\" For the second question, we review different\ndatabases that are designed specifically for blockchain and provide most of the\nblockchain functionality like immutability, privacy, censorship resistance,\nalong with database features.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 10:04:41 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Raikwar", "Mayank", ""], ["Gligoroski", "Danilo", ""], ["Velinov", "Goran", ""]]}, {"id": "2003.05746", "submitter": "Camille Bourgaux", "authors": "Meghyn Bienvenu and Camille Bourgaux", "title": "Querying and Repairing Inconsistent Prioritized Knowledge Bases:\n  Complexity Analysis and Links with Abstract Argumentation", "comments": "27 pages. To appear in the 17th International Conference on\n  Principles of Knowledge Representation and Reasoning (KR 2020) without the\n  appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore the issue of inconsistency handling over\nprioritized knowledge bases (KBs), which consist of an ontology, a set of\nfacts, and a priority relation between conflicting facts. In the database\nsetting, a closely related scenario has been studied and led to the definition\nof three different notions of optimal repairs (global, Pareto, and completion)\nof a prioritized inconsistent database. After transferring the notions of\nglobally-, Pareto- and completion-optimal repairs to our setting, we study the\ndata complexity of the core reasoning tasks: query entailment under\ninconsistency-tolerant semantics based upon optimal repairs, existence of a\nunique optimal repair, and enumeration of all optimal repairs. Our results\nprovide a nearly complete picture of the data complexity of these tasks for\nontologies formulated in common DL-Lite dialects. The second contribution of\nour work is to clarify the relationship between optimal repairs and different\nnotions of extensions for (set-based) argumentation frameworks. Among our\nresults, we show that Pareto-optimal repairs correspond precisely to stable\nextensions (and often also to preferred extensions), and we propose a novel\nsemantics for prioritized KBs which is inspired by grounded extensions and\nenjoys favourable computational properties. Our study also yields some results\nof independent interest concerning preference-based argumentation frameworks.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 12:38:37 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 16:15:30 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Bienvenu", "Meghyn", ""], ["Bourgaux", "Camille", ""]]}, {"id": "2003.05809", "submitter": "Heiko Paulheim", "authors": "Jan Portisch, Michael Hladik, Heiko Paulheim", "title": "KGvec2go -- Knowledge Graph Embeddings as a Service", "comments": "to be published in the Proceedings of the International Conference on\n  Language Resources and Evaluation (LREC) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present KGvec2go, a Web API for accessing and consuming\ngraph embeddings in a light-weight fashion in downstream applications.\nCurrently, we serve pre-trained embeddings for four knowledge graphs. We\nintroduce the service and its usage, and we show further that the trained\nmodels have semantic value by evaluating them on multiple semantic benchmarks.\nThe evaluation also reveals that the combination of multiple models can lead to\na better outcome than the best individual model.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 12:57:10 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Portisch", "Jan", ""], ["Hladik", "Michael", ""], ["Paulheim", "Heiko", ""]]}, {"id": "2003.06007", "submitter": "Vikram Sreekanti", "authors": "Vikram Sreekanti, Chenggang Wu, Saurav Chhatrapati, Joseph E.\n  Gonzalez, Joseph M. Hellerstein, Jose M. Faleiro", "title": "A Fault-Tolerance Shim for Serverless Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Serverless computing has grown in popularity in recent years, with an\nincreasing number of applications being built on Functions-as-a-Service (FaaS)\nplatforms. By default, FaaS platforms support retry-based fault tolerance, but\nthis is insufficient for programs that modify shared state, as they can\nunwittingly persist partial sets of updates in case of failures. To address\nthis challenge, we would like atomic visibility of the updates made by a FaaS\napplication.\n  In this paper, we present AFT, an atomic fault tolerance shim for serverless\napplications. AFT interposes between a commodity FaaS platform and storage\nengine and ensures atomic visibility of updates by enforcing the read atomic\nisolation guarantee. AFT supports new protocols to guarantee read atomic\nisolation in the serverless setting. We demonstrate that aft introduces minimal\noverhead relative to existing storage engines and scales smoothly to thousands\nof requests per second, while preventing a significant number of consistency\nanomalies.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 20:28:55 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Sreekanti", "Vikram", ""], ["Wu", "Chenggang", ""], ["Chhatrapati", "Saurav", ""], ["Gonzalez", "Joseph E.", ""], ["Hellerstein", "Joseph M.", ""], ["Faleiro", "Jose M.", ""]]}, {"id": "2003.06291", "submitter": "Shovanur Haque", "authors": "Shovanur Haque, Kerrie Mengersen", "title": "Improved assessment of the accuracy of record linkage via an extended\n  MaCSim approach", "comments": "32 pages, 4 figures. arXiv admin note: text overlap with\n  arXiv:1901.04779", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DB stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Record linkage is the process of bringing together the same entity from\noverlapping data sources while removing duplicates. Huge amounts of data are\nnow being collected by public or private organizations as well as by\nresearchers and individuals. Linking and analysing relevant information from\nthis massive data reservoir can provide new insights into society. However,\nthis increase in the amount of data may also increase the likelihood of\nincorrectly linked records among databases. It has become increasingly\nimportant to have effective and efficient methods for linking data from\ndifferent sources. Therefore, it becomes necessary to assess the ability of a\nlinking method to achieve high accuracy or to compare between methods with\nrespect to accuracy. In this paper, we improve on a Markov Chain based Monte\nCarlo simulation approach (MaCSim) for assessing a linking method. MaCSim\nutilizes two linked files that have been previously linked on similar types of\ndata to create an agreement matrix and then simulates the matrix using a\nproposed algorithm developed to generate re-sampled versions of the agreement\nmatrix. A defined linking method is used in each simulation to link the files\nand the accuracy of the linking method is assessed. The improvement proposed\nhere involves calculation of a similarity weight for every linking variable\nvalue for each record pair, which allows partial agreement of the linking\nvariable values. A threshold is calculated for every linking variable based on\nadjustable parameter \"tolerance\" for that variable. To assess the accuracy of\nlinking method, correctly linked proportions are investigated for each record.\nThe extended MaCSim approach is illustrated using a synthetic dataset provided\nby the Australian Bureau of Statistics (ABS) based on realistic data settings.\nTest results show higher accuracy of the assessment of linkages.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 10:41:21 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 05:23:21 GMT"}, {"version": "v3", "created": "Tue, 13 Oct 2020 01:16:17 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Haque", "Shovanur", ""], ["Mengersen", "Kerrie", ""]]}, {"id": "2003.06415", "submitter": "Omid Jafari", "authors": "Omid Jafari, Parth Nagarkar, Jonathan Monta\\~no", "title": "mmLSH: A Practical and Efficient Technique for Processing Approximate\n  Nearest Neighbor Queries on Multimedia Data", "comments": "Submitted to SISAP 2020", "journal-ref": "SISAP 2020. Lecture Notes in Computer Science, vol 12440.\n  Springer, Cham", "doi": "10.1007/978-3-030-60936-8_4", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many large multimedia applications require efficient processing of nearest\nneighbor queries. Often, multimedia data are represented as a collection of\nimportant high-dimensional feature vectors. Existing Locality Sensitive Hashing\n(LSH) techniques require users to find top-k similar feature vectors for each\nof the feature vectors that represent the query object. This leads to wasted\nand redundant work due to two main reasons: 1) not all feature vectors may\ncontribute equally in finding the top-k similar multimedia objects, and 2)\nfeature vectors are treated independently during query processing.\nAdditionally, there is no theoretical guarantee on the returned multimedia\nresults. In this work, we propose a practical and efficient indexing approach\nfor finding top-k approximate nearest neighbors for multimedia data using LSH\ncalled mmLSH, which can provide theoretical guarantees on the returned\nmultimedia results. Additionally, we present a buffer-conscious strategy to\nspeed up the query processing. Experimental evaluation shows significant gains\nin performance time and accuracy for different real multimedia datasets when\ncompared against state-of-the-art LSH techniques.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 17:57:08 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 16:40:03 GMT"}, {"version": "v3", "created": "Mon, 22 Jun 2020 17:38:26 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Jafari", "Omid", ""], ["Nagarkar", "Parth", ""], ["Monta\u00f1o", "Jonathan", ""]]}, {"id": "2003.06613", "submitter": "Fotis Savva", "authors": "Fotis Savva, Christos Anagnostopoulos, Peter Triantafillou", "title": "ML-AQP: Query-Driven Approximate Query Processing based on Machine\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As more and more organizations rely on data-driven decision making,\nlarge-scale analytics become increasingly important. However, an analyst is\noften stuck waiting for an exact result. As such, organizations turn to Cloud\nproviders that have infrastructure for efficiently analyzing large quantities\nof data. But, with increasing costs, organizations have to optimize their\nusage. Having a cheap alternative that provides speed and efficiency will go a\nlong way. Concretely, we offer a solution that can provide approximate answers\nto aggregate queries, relying on Machine Learning (ML), which is able to work\nalongside Cloud systems. Our developed lightweight ML-led system can be stored\non an analyst's local machine or deployed as a service to instantly answer\nanalytic queries, having low response times and monetary/computational costs\nand energy footprint. To accomplish this we leverage the knowledge obtained by\npreviously answered queries and build ML models that can estimate the result of\nnew queries in an efficient and inexpensive manner. The capabilities of our\nsystem are demonstrated using extensive evaluation with both real and synthetic\ndatasets/workloads and well known benchmarks.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 12:08:06 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Savva", "Fotis", ""], ["Anagnostopoulos", "Christos", ""], ["Triantafillou", "Peter", ""]]}, {"id": "2003.06708", "submitter": "Immanuel Trummer Mr.", "authors": "Georgios Karagiannis and Mohammed Saeed and Paolo Papotti and Immanuel\n  Trummer", "title": "Scrutinizer: A Mixed-Initiative Approach to Large-Scale, Data-Driven\n  Claim Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Organizations such as the International Energy Agency (IEA) spend significant\namounts of time and money to manually fact check text documents summarizing\ndata. The goal of the Scrutinizer system is to reduce verification overheads by\nsupporting human fact checkers in translating text claims into SQL queries on\nan associated database.\n  Scrutinizer coordinates teams of human fact checkers. It reduces verification\ntime by proposing queries or query fragments to the users. Those proposals are\nbased on claim text classifiers, that gradually improve during the verification\nof a large document. In addition, Scrutinizer uses tentative execution of query\ncandidates to narrow down the set of alternatives. The verification process is\ncontrolled by a cost-based optimizer. It optimizes the interaction with users\nand prioritizes claim verifications. For the latter, it considers expected\nverification overheads as well as the expected claim utility as training\nsamples for the classifiers. We evaluate the Scrutinizer system using\nsimulations and a user study, based on actual claims and data and using\nprofessional fact checkers employed by IEA. Our experiments consistently\ndemonstrate significant savings in verification time, without reducing result\naccuracy.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 21:28:43 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Karagiannis", "Georgios", ""], ["Saeed", "Mohammed", ""], ["Papotti", "Paolo", ""], ["Trummer", "Immanuel", ""]]}, {"id": "2003.06826", "submitter": "Peng Cheng", "authors": "Wangze Ni, Han Wu, Peng Cheng, Lei Chen, Xuemin Lin, Lei Chen, Xin\n  Lai, Xiao Zhang", "title": "CoinMagic: A Differential Privacy Framework for Ring Signature Schemes", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By allowing users to obscure their transactions via including \"mixins\" (chaff\ncoins), ring signature schemes have been widely used to protect a sender's\nidentity of a transaction in privacy-preserving blockchain systems, like Monero\nand Bytecoin. However, recent works point out that the existing ring signature\nscheme is vulnerable to the \"chain-reaction\" analysis (i.e., the spent coin in\na given ring signature can be deduced through elimination). Especially, when\nthe diversity of mixins is low, the spent coin will have a high risk to be\ndetected. To overcome the weakness, the ring signature should be consisted of a\nset of mixins with high diversity and produce observations having \"similar\"\ndistributions for any two coins. In this paper, we propose a notion, namely\n$\\epsilon$-coin-indistinguishability ($\\epsilon$-CI), to formally define the\n\"similar\" distribution guaranteed through a differential privacy scheme. Then,\nwe formally define the CI-aware mixins selection problem with disjoint-superset\nconstraint (CIA-MS-DS), which aims to find a mixin set that has maximal\ndiversity and satisfies the constraints of $\\epsilon$-CI and the budget. In\nCIA-MS-DS, each ring signature is either disjoint with or the superset of its\npreceding ring signatures. We prove that CIA-MS-DS is NP-hard and thus\nintractable. To solve the CIA-MS-DS problem, we propose two approximation\nalgorithms, namely the Progressive Algorithm and the Game Theoretic Algorithm,\nwith theoretic guarantees. Through extensive experiments on both real data sets\nand synthetic data sets, we demonstrate the efficiency and the effectiveness of\nour approaches.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 13:23:57 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Ni", "Wangze", ""], ["Wu", "Han", ""], ["Cheng", "Peng", ""], ["Chen", "Lei", ""], ["Lin", "Xuemin", ""], ["Chen", "Lei", ""], ["Lai", "Xin", ""], ["Zhang", "Xiao", ""]]}, {"id": "2003.06868", "submitter": "Maximilian Schleich", "authors": "Leopoldo Bertossi, Jordan Li, Maximilian Schleich, Dan Suciu,\n  Zografoula Vagena", "title": "Causality-based Explanation of Classification Outcomes", "comments": "16 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple definition of an explanation for the outcome of a\nclassifier based on concepts from causality. We compare it with previously\nproposed notions of explanation, and study their complexity. We conduct an\nexperimental evaluation with two real datasets from the financial domain.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 17:00:37 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 04:24:18 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Bertossi", "Leopoldo", ""], ["Li", "Jordan", ""], ["Schleich", "Maximilian", ""], ["Suciu", "Dan", ""], ["Vagena", "Zografoula", ""]]}, {"id": "2003.06875", "submitter": "Dong Wei", "authors": "Dong Wei, Senjuti Basu Roy, Sihem Amer-Yahia", "title": "Recommending Deployment Strategies for Collaborative Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our work contributes to aiding requesters in deploying collaborative tasks in\ncrowdsourcing. We initiate the study of recommending deployment strategies for\ncollaborative tasks to requesters that are consistent with deployment\nparameters they desire: a lower-bound on the quality of the crowd contribution,\nan upper-bound on the latency of task completion, and an upper-bound on the\ncost incurred by paying workers. A deployment strategy is a choice of value for\nthree dimensions: Structure (whether to solicit the workforce sequentially or\nsimultaneously), Organization (to organize it collaboratively or\nindependently), and Style (to rely solely on the crowd or to combine it with\nmachine algorithms). We propose StratRec, an optimization-driven middle layer\nthat recommends deployment strategies and alternative deployment parameters to\nrequesters by accounting for worker availability. Our solutions are grounded in\ndiscrete optimization and computational geometry techniques that produce\nresults with theoretical guarantees. We present extensive experiments on Amazon\nMechanical Turk and conduct synthetic experiments to validate the qualitative\nand scalability aspects of StratRec.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 17:36:55 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Wei", "Dong", ""], ["Roy", "Senjuti Basu", ""], ["Amer-Yahia", "Sihem", ""]]}, {"id": "2003.06880", "submitter": "Liat Peterfreund", "authors": "Liat Peterfreund", "title": "Grammars for Document Spanners", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new grammar-based language for defining information-extractors\nfrom documents (text) that is built upon the well-studied framework of document\nspanners for extracting structured data from text. While previously studied\nformalisms for document spanners are mainly based on regular expressions, we\nuse an extension of context-free grammars, called {extraction grammars}, to\ndefine the new class of context-free spanners. Extraction grammars are simply\ncontext-free grammars extended with variables that capture interval positions\nof the document, namely spans. While regular expressions are efficient for\ntokenizing and tagging, context-free grammars are also efficient for capturing\nstructural properties. Indeed, we show that context-free spanners are strictly\nmore expressive than their regular counterparts. We reason about the expressive\npower of our new class and present a pushdown-automata model that captures it.\nWe show that extraction grammars can be evaluated with polynomial data\ncomplexity. Nevertheless, as the degree of the polynomial depends on the query,\nwe present an enumeration algorithm for unambiguous extraction grammars that,\nafter quintic preprocessing, outputs the results sequentially, without\nrepetitions, with a constant delay between every two consecutive ones.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 17:50:18 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 11:36:38 GMT"}, {"version": "v3", "created": "Mon, 20 Apr 2020 17:00:06 GMT"}, {"version": "v4", "created": "Thu, 12 Nov 2020 11:10:52 GMT"}, {"version": "v5", "created": "Sat, 13 Mar 2021 10:15:15 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Peterfreund", "Liat", ""]]}, {"id": "2003.06984", "submitter": "Haoyue Ping", "authors": "Haoyue Ping, Julia Stoyanovich, Benny Kimelfeld", "title": "Supporting Hard Queries over Probabilistic Preferences", "comments": "This is the technical report of the following paper: Supporting Hard\n  Queries over Probabilistic Preferences. PVLDB, 13(7): 1134-1146, 2019. DOI:\n  https://doi.org/10.14778/3384345.3384359", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preference analysis is widely applied in various domains such as social\nchoice and e-commerce. A recently proposed framework augments the relational\ndatabase with a preference relation that represents uncertain preferences in\nthe form of statistical ranking models, and provides methods to evaluate\nConjunctive Queries (CQs) that express preferences among item attributes. In\nthis paper, we explore the evaluation of queries that are more general and\nharder to compute.\n  The main focus of this paper is on a class of CQs that cannot be evaluated by\nprevious work. These queries are provably hard since relate variables that\nrepresent items being compared. To overcome this hardness, we instantiate these\nvariables with their domain values, rewrite hard CQs as unions of such\ninstantiated queries, and develop several exact and approximate solvers to\nevaluate these unions of queries. We demonstrate that exact solvers that target\nspecific common kinds of queries are far more efficient than general solvers.\nFurther, we demonstrate that sophisticated approximate solvers making use of\nimportance sampling can be orders of magnitude more efficient than exact\nsolvers, while showing good accuracy. In addition to supporting provably hard\nCQs, we also present methods to evaluate an important family of count queries,\nand of top-k queries.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 03:08:34 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Ping", "Haoyue", ""], ["Stoyanovich", "Julia", ""], ["Kimelfeld", "Benny", ""]]}, {"id": "2003.07302", "submitter": "Baotong Lu", "authors": "Baotong Lu, Xiangpeng Hao, Tianzheng Wang, Eric Lo", "title": "Dash: Scalable Hashing on Persistent Memory", "comments": "To appear at VLDB 2020 (PVLDB Vol. 13 Issue 8)", "journal-ref": "PVLDB, 13(8): 1147-1161, 2020", "doi": "10.14778/3389133.3389134", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Byte-addressable persistent memory (PM) brings hash tables the potential of\nlow latency, cheap persistence and instant recovery. The recent advent of Intel\nOptane DC Persistent Memory Modules (DCPMM) further accelerates this trend.\nMany new hash table designs have been proposed, but most of them were based on\nemulation and perform sub-optimally on real PM. They were also piece-wise and\npartial solutions that side-step many important properties, in particular good\nscalability, high load factor and instant recovery. We present Dash, a holistic\napproach to building dynamic and scalable hash tables on real PM hardware with\nall the aforementioned properties. Based on Dash, we adapted two popular\ndynamic hashing schemes (extendible hashing and linear hashing). On a 24-core\nmachine with Intel Optane DCPMM, we show that compared to state-of-the-art,\nDash-enabled hash tables can achieve up to ~3.9X higher performance with up to\nover 90% load factor and an instant recovery time of 57ms regardless of data\nsize.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 16:15:46 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 08:03:53 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Lu", "Baotong", ""], ["Hao", "Xiangpeng", ""], ["Wang", "Tianzheng", ""], ["Lo", "Eric", ""]]}, {"id": "2003.07316", "submitter": "Antoine Amarilli", "authors": "Julien Romero, Nicoleta Preda, Antoine Amarilli, Fabian Suchanek", "title": "Equivalent Rewritings on Path Views with Binding Patterns", "comments": "33 pages including 16 pages of main text. This is the full version of\n  the ESWC'2020 article, which integrates all reviewer feedback, with the same\n  text as the publisher version except minor changes. Several corrections\n  relative to the first version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A view with a binding pattern is a parameterized query on a database. Such\nviews are used, e.g., to model Web services. To answer a query on such views,\nthe views have to be orchestrated together in execution plans. We show how\nqueries can be rewritten into equivalent execution plans, which are guaranteed\nto deliver the same results as the query on all databases. We provide a correct\nand complete algorithm to find these plans for path views and atomic queries.\nFinally, we show that our method can be used to answer queries on real-world\nWeb services.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 16:38:56 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2020 11:30:31 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Romero", "Julien", ""], ["Preda", "Nicoleta", ""], ["Amarilli", "Antoine", ""], ["Suchanek", "Fabian", ""]]}, {"id": "2003.07432", "submitter": "Herodotos Herodotou", "authors": "Michael A. Georgiou, Aristodemos Paphitis, Michael Sirivianos,\n  Herodotos Herodotou", "title": "Hihooi: A Database Replication Middleware for Scaling Transactional\n  Databases Consistently", "comments": "16 pages, 11 figures, 7 tables", "journal-ref": null, "doi": "10.1109/TKDE.2020.2987560", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of the Internet and Internet-connected devices, modern\nbusiness applications can experience rapid increases as well as variability in\ntransactional workloads. Database replication has been employed to scale\nperformance and improve availability of relational databases but past\napproaches have suffered from various issues including limited scalability,\nperformance versus consistency tradeoffs, and requirements for database or\napplication modifications. This paper presents Hihooi, a replication-based\nmiddleware system that is able to achieve workload scalability, strong\nconsistency guarantees, and elasticity for existing transactional databases at\na low cost. A novel replication algorithm enables Hihooi to propagate database\nmodifications asynchronously to all replicas at high speeds, while ensuring\nthat all replicas are consistent. At the same time, a fine-grained routing\nalgorithm is used to load balance incoming transactions to available replicas\nin a consistent way. Our thorough experimental evaluation with several\nwell-established benchmarks shows how Hihooi is able to achieve almost linear\nworkload scalability for transactional databases.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 20:32:20 GMT"}, {"version": "v2", "created": "Sun, 22 Mar 2020 11:06:44 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Georgiou", "Michael A.", ""], ["Paphitis", "Aristodemos", ""], ["Sirivianos", "Michael", ""], ["Herodotou", "Herodotos", ""]]}, {"id": "2003.07438", "submitter": "Christopher Baik", "authors": "Christopher Baik, Zhongjun Jin, Michael Cafarella, H. V. Jagadish", "title": "Duoquest: A Dual-Specification System for Expressive SQL Queries", "comments": "Technical Report, 16 pages. Shorter version to be published in SIGMOD\n  2020", "journal-ref": null, "doi": "10.1145/3318464.3389776", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Querying a relational database is difficult because it requires users to know\nboth the SQL language and be familiar with the schema. On the other hand, many\nusers possess enough domain familiarity or expertise to describe their desired\nqueries by alternative means. For such users, two major alternatives to writing\nSQL are natural language interfaces (NLIs) and programming-by-example (PBE).\nBoth of these alternatives face certain pitfalls: natural language queries\n(NLQs) are often ambiguous, even for human interpreters, while current PBE\napproaches require either low-complexity queries, user schema knowledge, exact\nexample tuples from the user, or a closed-world assumption to be tractable.\nConsequently, we propose dual-specification query synthesis, which consumes\nboth a NLQ and an optional PBE-like table sketch query that enables users to\nexpress varied levels of domain-specific knowledge. We introduce the novel\ndual-specification Duoquest system, which leverages guided partial query\nenumeration to efficiently explore the space of possible queries. We present\nresults from user studies in which Duoquest demonstrates a 62.5% absolute\nincrease in query construction accuracy over a state-of-the-art NLI and\ncomparable accuracy to a PBE system on a more limited workload supported by the\nPBE system. In a simulation study on the prominent Spider benchmark, Duoquest\ndemonstrates a >2x increase in top-1 accuracy over both NLI and PBE.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 20:47:28 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Baik", "Christopher", ""], ["Jin", "Zhongjun", ""], ["Cafarella", "Michael", ""], ["Jagadish", "H. V.", ""]]}, {"id": "2003.07669", "submitter": "Jakob Blomer", "authors": "Jakob Blomer and Philippe Canal and Axel Naumann and Danilo Piparo", "title": "Evolution of the ROOT Tree I/O", "comments": "9 pages, 8 figures, 1 table, submitted to 24th Computing in High\n  Energy and Nuclear Physics (CHEP'19)", "journal-ref": null, "doi": "10.1051/epjconf/202024502030", "report-no": null, "categories": "cs.DB hep-ex", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ROOT TTree data format encodes hundreds of petabytes of High Energy and\nNuclear Physics events. Its columnar layout drives rapid analyses, as only\nthose parts (\"branches\") that are really used in a given analysis need to be\nread from storage. Its unique feature is the seamless C++ integration, which\nallows users to directly store their event classes without explicitly defining\ndata schemas. In this contribution, we present the status and plans of the\nfuture ROOT 7 event I/O. Along with the ROOT 7 interface modernization, we aim\nfor robust, where possible compile-time safe C++ interfaces to read and write\nevent data. On the performance side, we show first benchmarks using ROOT's new\nexperimental I/O subsystem that combines the best of TTrees with recent\nadvances in columnar data formats. A core ingredient is a strong separation of\nthe high-level logical data layout (C++ classes) from the low-level physical\ndata layout (storage backed nested vectors of simple types). We show how the\nnew, optimized physical data layout speeds up serialization and deserialization\nand facilitates parallel, vectorized and bulk operations. This lets ROOT I/O\nrun optimally on the upcoming ultra-fast NVRAM storage devices, as well as\nfile-less storage systems such as object stores.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 12:41:55 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Blomer", "Jakob", ""], ["Canal", "Philippe", ""], ["Naumann", "Axel", ""], ["Piparo", "Danilo", ""]]}, {"id": "2003.07743", "submitter": "Wei Hu", "authors": "Zequn Sun and Qingheng Zhang and Wei Hu and Chengming Wang and Muhao\n  Chen and Farahnaz Akrami and Chengkai Li", "title": "A Benchmarking Study of Embedding-based Entity Alignment for Knowledge\n  Graphs", "comments": "Accepted in the 46th International Conference on Very Large Data\n  Bases (VLDB 2020)", "journal-ref": null, "doi": "10.14778/3407790.3407828", "report-no": null, "categories": "cs.CL cs.AI cs.DB cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity alignment seeks to find entities in different knowledge graphs (KGs)\nthat refer to the same real-world object. Recent advancement in KG embedding\nimpels the advent of embedding-based entity alignment, which encodes entities\nin a continuous embedding space and measures entity similarities based on the\nlearned embeddings. In this paper, we conduct a comprehensive experimental\nstudy of this emerging field. We survey 23 recent embedding-based entity\nalignment approaches and categorize them based on their techniques and\ncharacteristics. We also propose a new KG sampling algorithm, with which we\ngenerate a set of dedicated benchmark datasets with various heterogeneity and\ndistributions for a realistic evaluation. We develop an open-source library\nincluding 12 representative embedding-based entity alignment approaches, and\nextensively evaluate these approaches, to understand their strengths and\nlimitations. Additionally, for several directions that have not been explored\nin current approaches, we perform exploratory experiments and report our\npreliminary findings for future studies. The benchmark datasets, open-source\nlibrary and experimental results are all accessible online and will be duly\nmaintained.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 05:32:06 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 00:47:26 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Sun", "Zequn", ""], ["Zhang", "Qingheng", ""], ["Hu", "Wei", ""], ["Wang", "Chengming", ""], ["Chen", "Muhao", ""], ["Akrami", "Farahnaz", ""], ["Li", "Chengkai", ""]]}, {"id": "2003.07800", "submitter": "Carsten Lutz", "authors": "Pablo Barcelo, Cristina Feier, Carsten Lutz, Andreas Pieris", "title": "When is Ontology-Mediated Querying Efficient?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In ontology-mediated querying, description logic (DL) ontologies are used to\nenrich incomplete data with domain knowledge which results in more complete\nanswers to queries. However, the evaluation of ontology-mediated queries (OMQs)\nover relational databases is computationally hard. This raises the question\nwhen OMQ evaluation is efficient, in the sense of being tractable in combined\ncomplexity or fixed-parameter tractable. We study this question for a range of\nontology-mediated query languages based on several important and widely-used\nDLs, using unions of conjunctive queries as the actual queries. For the DL ELHI\nextended with the bottom concept, we provide a characterization of the classes\nof OMQs that are fixed-parameter tractable. For its fragment EL extended with\ndomain and range restrictions and the bottom concept (which restricts the use\nof inverse roles), we provide a characterization of the classes of OMQs that\nare tractable in combined complexity. Both results are in terms of equivalence\nto OMQs of bounded tree width and rest on a reasonable assumption from\nparameterized complexity theory. They are similar in spirit to Grohe's seminal\ncharacterization of the tractable classes of conjunctive queries over\nrelational databases. We further study the complexity of the meta problem of\ndeciding whether a given OMQ is equivalent to an OMQ of bounded tree width,\nproviding several completeness results that range from NP to 2ExpTime,\ndepending on the DL used. We also consider the DL-Lite family of DLs, including\nmembers that admit functional roles.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 16:32:00 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 15:25:57 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Barcelo", "Pablo", ""], ["Feier", "Cristina", ""], ["Lutz", "Carsten", ""], ["Pieris", "Andreas", ""]]}, {"id": "2003.08031", "submitter": "Zhe Li", "authors": "Zhe Li, Tsz Nam Chan, Man Lung Yiu, Christian S. Jensen", "title": "PolyFit: Polynomial-based Indexing Approach for Fast Approximate Range\n  Aggregate Queries", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Range aggregate queries find frequent application in data analytics. In some\nuse cases, approximate results are preferred over accurate results if they can\nbe computed rapidly and satisfy approximation guarantees. Inspired by a recent\nindexing approach, we provide means of representing a discrete point data set\nby continuous functions that can then serve as compact index structures. More\nspecifically, we develop a polynomial-based indexing approach, called PolyFit,\nfor processing approximate range aggregate queries. PolyFit is capable of\nsupporting multiple types of range aggregate queries, including COUNT, SUM, MIN\nand MAX aggregates, with guaranteed absolute and relative error bounds.\nExperiment results show that PolyFit is faster and more accurate and compact\nthan existing learned index structures.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 03:54:51 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2020 11:37:31 GMT"}, {"version": "v3", "created": "Sat, 21 Mar 2020 06:53:21 GMT"}, {"version": "v4", "created": "Mon, 22 Jun 2020 10:30:50 GMT"}, {"version": "v5", "created": "Tue, 13 Oct 2020 09:26:11 GMT"}, {"version": "v6", "created": "Sat, 19 Dec 2020 13:54:41 GMT"}, {"version": "v7", "created": "Wed, 10 Feb 2021 06:49:31 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Li", "Zhe", ""], ["Chan", "Tsz Nam", ""], ["Yiu", "Man Lung", ""], ["Jensen", "Christian S.", ""]]}, {"id": "2003.08170", "submitter": "Teemu Lehto", "authors": "Teemu Lehto and Markku Hinkka", "title": "Discovering Business Area Effects to Process Mining Analysis Using\n  Clustering and Influence Analysis", "comments": "12 pages. Paper accepted in 23rd International Conference on Business\n  Information Systems (BIS 2020) to be published in a proceedings edition of\n  the Lecture Notes in Business Information Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common challenge for improving business processes in large organizations is\nthat business people in charge of the operations are lacking a fact-based\nunderstanding of the execution details, process variants, and exceptions taking\nplace in business operations. While existing process mining methodologies can\ndiscover these details based on event logs, it is challenging to communicate\nthe process mining findings to business people. In this paper, we present a\nnovel methodology for discovering business areas that have a significant effect\non the process execution details. Our method uses clustering to group similar\ncases based on process flow characteristics and then influence analysis for\ndetecting those business areas that correlate most with the discovered\nclusters. Our analysis serves as a bridge between BPM people and business,\npeople facilitating the knowledge sharing between these groups. We also present\nan example analysis based on publicly available real-life purchase order\nprocess data.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 11:58:01 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Lehto", "Teemu", ""], ["Hinkka", "Markku", ""]]}, {"id": "2003.08215", "submitter": "Md Amiruzzaman", "authors": "Md Amiruzzaman and Suphanut Jamonnak", "title": "Multi-dimensional Skyline Query to Find Best Shopping Mall for Customers", "comments": "This paper is accepted for publication in IEEE CDMA 2020 conference", "journal-ref": "2020 6th Conference on Data Science and Machine Learning\n  Applications (CDMA)", "doi": "10.1109/CDMA47397.2020.00018", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new application for multi-dimensional Skyline query.\nThe idea presented in this paper can be used to find best shopping malls based\non users requirements. A web-based application was used to simulate the problem\nand proposed solution. Also, a mathematical definition was developed to define\nthe problem and show how multi-dimensional Skyline query can be used to solve\ncomplex problems, such as, finding shopping malls using multiple different\ncriteria. The idea of this paper can be used in other fields, where different\ncriteria should be considered.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 02:50:13 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Amiruzzaman", "Md", ""], ["Jamonnak", "Suphanut", ""]]}, {"id": "2003.09009", "submitter": "Yifan Li", "authors": "Yifan Li, Xiaohui Yu, Nick Koudas", "title": "Top-k queries over digital traces", "comments": "Accepted by SIGMOD2019. Proceedings of the 2019 International\n  Conference on Management of Data", "journal-ref": null, "doi": "10.1145/3299869.3319857", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent advances in social and mobile technology have enabled an abundance of\ndigital traces (in the form of mobile check-ins, association of mobile devices\nto specific WiFi hotspots, etc.) revealing the physical presence history of\ndiverse sets of entities (e.g., humans, devices, and vehicles). One challenging\nyet important task is to identify k entities that are most closely associated\nwith a given query entity based on their digital traces. We propose a suite of\nindexing techniques and algorithms to enable fast query processing for this\nproblem at scale. We first define a generic family of functions measuring the\nassociation between entities, and then propose algorithms to transform digital\ntraces into a lower-dimensional space for more efficient computation. We\nsubsequently design a hierarchical indexing structure to organize entities in a\nway that closely associated entities tend to appear together. We then develop\nalgorithms to process top-k queries utilizing the index. We theoretically\nanalyze the pruning effectiveness of the proposed methods based on a mobility\nmodel which we propose and validate in real life situations. Finally, we\nconduct extensive experiments on both synthetic and real datasets at scale,\nevaluating the performance of our techniques both analytically and\nexperimentally, confirming the effectiveness and superiority of our approach\nover other applicable approaches across a variety of parameter settings and\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 20:17:59 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Li", "Yifan", ""], ["Yu", "Xiaohui", ""], ["Koudas", "Nick", ""]]}, {"id": "2003.09074", "submitter": "Ed Upchurch", "authors": "Ed T. Upchurch", "title": "A Migratory Near Memory Processing Architecture Applied to Big Data\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Servers produced by mainstream vendors are inefficient in processing Big Data\nqueries due to bottlenecks inherent in the fundamental architecture of these\nsystems. Current server blades contain multicore processors connected to DRAM\nmemory and disks by an interconnection chipset. The multicore processor chips\nperform all the computations while the DRAM and disks store the data but have\nno processing capability. To perform a database query, data must be moved back\nand forth between DRAM and a small cache as well as between DRAM and disks. For\nBig Data applications this data movement in onerous. Migratory Near Memory\nServers address this bottleneck by placing large numbers of lightweight\nprocessors directly into the memory system. These processors operate directly\non the relations, vertices and edges of Big Data applications in place without\nhaving to shuttle large quantities of data back and forth between DRAM, cache\nand heavyweight multicore processors. This paper addresses the application of\nsuch an architecture to relational database SELECT and JOIN queries.\nPreliminary results indicate end-to-end orders of magnitude speedup.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 02:37:40 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Upchurch", "Ed T.", ""]]}, {"id": "2003.09481", "submitter": "Simeon Krastnikov", "authors": "Simeon Krastnikov, Florian Kerschbaum, Douglas Stebila", "title": "Efficient Oblivious Database Joins", "comments": null, "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), 13(11): 2132-2145, 2020", "doi": "10.14778/3407790.3407814", "report-no": null, "categories": "cs.DB cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major algorithmic challenge in designing applications intended for secure\nremote execution is ensuring that they are oblivious to their inputs, in the\nsense that their memory access patterns do not leak sensitive information to\nthe server. This problem is particularly relevant to cloud databases that wish\nto allow queries over the client's encrypted data. One of the major obstacles\nto such a goal is the join operator, which is non-trivial to implement\nobliviously without resorting to generic but inefficient solutions like\nOblivious RAM (ORAM).\n  We present an oblivious algorithm for equi-joins which (up to a logarithmic\nfactor) matches the optimal $O(n\\log n)$ complexity of the standard non-secure\nsort-merge join (on inputs producing $O(n)$ outputs). We do not use use\nexpensive primitives like ORAM or rely on unrealistic hardware or security\nassumptions. Our approach, which is based on sorting networks and novel\nprovably-oblivious constructions, is conceptually simple, easily verifiable,\nand very efficient in practice. Its data-independent algorithmic structure\nmakes it secure in various different settings for remote computation, even in\nthose that are known to be vulnerable to certain side-channel attacks (such as\nIntel SGX) or with strict requirements for low circuit complexity (like secure\nmultiparty computation). We confirm that our approach is easily realizable\nthrough a compact implementation which matches our expectations for performance\nand is shown, both formally and empirically, to possess the desired security\ncharacteristics.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 19:49:21 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 14:52:56 GMT"}, {"version": "v3", "created": "Tue, 15 Dec 2020 17:05:25 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Krastnikov", "Simeon", ""], ["Kerschbaum", "Florian", ""], ["Stebila", "Douglas", ""]]}, {"id": "2003.09530", "submitter": "Jonathan Harris", "authors": "Jonathan J. Harris, Ching-Hua Chen, Mohammed J. Zaki", "title": "A Framework for Generating Explanations from Temporal Personal Health\n  Data", "comments": "41 pages, 24 figures. To appear in ACM Transactions on Computing for\n  Healthcare", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whereas it has become easier for individuals to track their personal health\ndata (e.g., heart rate, step count, food log), there is still a wide chasm\nbetween the collection of data and the generation of meaningful explanations to\nhelp users better understand what their data means to them. With an increased\ncomprehension of their data, users will be able to act upon the newfound\ninformation and work towards striving closer to their health goals. We aim to\nbridge the gap between data collection and explanation generation by mining the\ndata for interesting behavioral findings that may provide hints about a user's\ntendencies. Our focus is on improving the explainability of temporal personal\nhealth data via a set of informative summary templates, or \"protoforms.\" These\nprotoforms span both evaluation-based summaries that help users evaluate their\nhealth goals and pattern-based summaries that explain their implicit behaviors.\nIn addition to individual users, the protoforms we use are also designed for\npopulation-level summaries. We apply our approach to generate summaries (both\nunivariate and multivariate) from real user data and show that our system can\ngenerate interesting and useful explanations.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 23:32:08 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 00:53:00 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Harris", "Jonathan J.", ""], ["Chen", "Ching-Hua", ""], ["Zaki", "Mohammed J.", ""]]}, {"id": "2003.09537", "submitter": "Sai Vikneshwar Mani Jayaraman", "authors": "Shi Li and Sai Vikneshwar Mani Jayaraman and Atri Rudra", "title": "Covering the Relational Join", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we initiate a theoretical study of what we call the join\ncovering problem. We are given a natural join query instance $Q$ on $n$\nattributes and $m$ relations $(R_i)_{i \\in [m]}$. Let $J_{Q} = \\ \\Join_{i=1}^m\nR_i$ denote the join output of $Q$. In addition to $Q$, we are given a\nparameter $\\Delta: 1\\le \\Delta\\le n$ and our goal is to compute the smallest\nsubset $\\mathcal{T}_{Q, \\Delta} \\subseteq J_{Q}$ such that every tuple in\n$J_{Q}$ is within Hamming distance $\\Delta - 1$ from some tuple in\n$\\mathcal{T}_{Q, \\Delta}$. The join covering problem captures both computing\nthe natural join from database theory and constructing a covering code with\ncovering radius $\\Delta - 1$ from coding theory, as special cases.\n  We consider the combinatorial version of the join covering problem, where our\ngoal is to determine the worst-case $|\\mathcal{T}_{Q, \\Delta}|$ in terms of the\nstructure of $Q$ and value of $\\Delta$. One obvious approach to upper bound\n$|\\mathcal{T}_{Q, \\Delta}|$ is to exploit a distance property (of Hamming\ndistance) from coding theory and combine it with the worst-case bounds on\noutput size of natural joins (AGM bound hereon) due to Atserias, Grohe and Marx\n[SIAM J. of Computing'13]. Somewhat surprisingly, this approach is not tight\neven for the case when the input relations have arity at most two. Instead, we\nshow that using the polymatroid degree-based bound of Abo Khamis, Ngo and Suciu\n[PODS'17] in place of the AGM bound gives us a tight bound (up to constant\nfactors) on the $|\\mathcal{T}_{Q, \\Delta}|$ for the arity two case. We prove\nlower bounds for $|\\mathcal{T}_{Q, \\Delta}|$ using well-known classes of\nerror-correcting codes e.g, Reed-Solomon codes. We can extend our results for\nthe arity two case to general arity with a polynomial gap between our upper and\nlower bounds.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2020 00:24:57 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Li", "Shi", ""], ["Jayaraman", "Sai Vikneshwar Mani", ""], ["Rudra", "Atri", ""]]}, {"id": "2003.09539", "submitter": "Anna Fariha", "authors": "Anna Fariha, Suman Nath, Alexandra Meliou", "title": "Causality-Guided Adaptive Interventional Debugging", "comments": "Technical report of AID (SIGMOD 2020)", "journal-ref": null, "doi": "10.1145/3318464.3389694", "report-no": null, "categories": "cs.DB cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Runtime nondeterminism is a fact of life in modern database applications.\nPrevious research has shown that nondeterminism can cause applications to\nintermittently crash, become unresponsive, or experience data corruption. We\npropose Adaptive Interventional Debugging (AID) for debugging such intermittent\nfailures. AID combines existing statistical debugging, causal analysis, fault\ninjection, and group testing techniques in a novel way to (1) pinpoint the root\ncause of an application's intermittent failure and (2) generate an explanation\nof how the root cause triggers the failure. AID works by first identifying a\nset of runtime behaviors (called predicates) that are strongly correlated to\nthe failure. It then utilizes temporal properties of the predicates to\n(over)-approximate their causal relationships. Finally, it uses fault injection\nto execute a sequence of interventions on the predicates and discover their\ntrue causal relationships. This enables AID to identify the true root cause and\nits causal relationship to the failure. We theoretically analyze how fast AID\ncan converge to the identification. We evaluate AID with six real-world\napplications that intermittently fail under specific inputs. In each case, AID\nwas able to identify the root cause and explain how the root cause triggered\nthe failure, much faster than group testing and more precisely than statistical\ndebugging. We also evaluate AID with many synthetically generated applications\nwith known root causes and confirm that the benefits also hold for them.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2020 00:43:20 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 21:36:40 GMT"}, {"version": "v3", "created": "Thu, 9 Apr 2020 21:01:43 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Fariha", "Anna", ""], ["Nath", "Suman", ""], ["Meliou", "Alexandra", ""]]}, {"id": "2003.09541", "submitter": "Nikos Giatrakos", "authors": "Antonis Kontaxakis, Nikos Giatrakos, Antonios Deligiannakis", "title": "A Synopses Data Engine for Interactive Extreme-Scale Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we detail the design and structure of a Synopses Data Engine\n(SDE) which combines the virtues of parallel processing and stream\nsummarization towards delivering interactive analytics at extreme scale. Our\nSDE is built on top of Apache Flink and implements a synopsis-as-a-service\nparadigm. In that it achieves (a) concurrently maintaining thousands of\nsynopses of various types for thousands of streams on demand, (b) reusing\nmaintained synopses among various concurrent workflows, (c) providing data\nsummarization facilities even for cross-(Big Data) platform workflows, (d)\npluggability of new synopses on-the-fly, (e) increased potential for workflow\nexecution optimization. The proposed SDE is useful for interactive analytics at\nextreme scales because it enables (i) enhanced horizontal scalability, i.e.,\nnot only scaling out the computation to a number of processing units available\nin a computer cluster, but also harnessing the processing load assigned to each\nby operating on carefully-crafted data summaries, (ii) vertical scalability,\ni.e., scaling the computation to very high numbers of processed streams and\n(iii) federated scalability i.e., scaling the computation beyond single\nclusters and clouds by controlling the communication required to answer global\nqueries posed over a number of potentially geo-dispersed clusters.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2020 00:55:26 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 11:04:41 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Kontaxakis", "Antonis", ""], ["Giatrakos", "Nikos", ""], ["Deligiannakis", "Antonios", ""]]}, {"id": "2003.09758", "submitter": "Nadiia Chepurko", "authors": "Nadiia Chepurko, Ryan Marcus, Emanuel Zgraggen, Raul Castro Fernandez,\n  Tim Kraska, David Karger", "title": "ARDA: Automatic Relational Data Augmentation for Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic machine learning (\\AML) is a family of techniques to automate the\nprocess of training predictive models, aiming to both improve performance and\nmake machine learning more accessible. While many recent works have focused on\naspects of the machine learning pipeline like model selection, hyperparameter\ntuning, and feature selection, relatively few works have focused on automatic\ndata augmentation. Automatic data augmentation involves finding new features\nrelevant to the user's predictive task with minimal ``human-in-the-loop''\ninvolvement.\n  We present \\system, an end-to-end system that takes as input a dataset and a\ndata repository, and outputs an augmented data set such that training a\npredictive model on this augmented dataset results in improved performance. Our\nsystem has two distinct components: (1) a framework to search and join data\nwith the input data, based on various attributes of the input, and (2) an\nefficient feature selection algorithm that prunes out noisy or irrelevant\nfeatures from the resulting join. We perform an extensive empirical evaluation\nof different system components and benchmark our feature selection algorithm on\nreal-world datasets.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2020 21:55:22 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Chepurko", "Nadiia", ""], ["Marcus", "Ryan", ""], ["Zgraggen", "Emanuel", ""], ["Fernandez", "Raul Castro", ""], ["Kraska", "Tim", ""], ["Karger", "David", ""]]}, {"id": "2003.09769", "submitter": "Leonidas Fegaras", "authors": "Leonidas Fegaras and Md Hasanuzzaman Noor", "title": "Translation of Array-Based Loops to Distributed Data-Parallel Programs", "comments": "This is the extended version of a paper that will appear at VLDB 2020\n  (PVLDB Vol. 13)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large volumes of data generated by scientific experiments and simulations\ncome in the form of arrays, while programs that analyze these data are\nfrequently expressed in terms of array operations in an imperative, loop-based\nlanguage. But, as datasets grow larger, new frameworks in distributed Big Data\nanalytics have become essential tools to large-scale scientific computing.\nScientists, who are typically comfortable with numerical analysis tools but are\nnot familiar with the intricacies of Big Data analytics, must now learn to\nconvert their loop-based programs to distributed data-parallel programs. We\npresent a novel framework for translating programs expressed as array-based\nloops to distributed data parallel programs that is more general and efficient\nthan related work. Although our translations are over sparse arrays, we extend\nour framework to handle packed arrays, such as tiled matrices, without\nsacrificing performance. We report on a prototype implementation on top of\nSpark and evaluate the performance of our system relative to hand-written\nprograms.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2020 23:40:44 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Fegaras", "Leonidas", ""], ["Noor", "Md Hasanuzzaman", ""]]}, {"id": "2003.09816", "submitter": "Naoto Ohsaka", "authors": "Naoto Ohsaka", "title": "The Solution Distribution of Influence Maximization: A High-level\n  Experimental Study on Three Algorithmic Approaches", "comments": "To appear in SIGMOD 2020", "journal-ref": null, "doi": "10.1145/3318464.3380564", "report-no": null, "categories": "cs.SI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Influence maximization is among the most fundamental algorithmic problems in\nsocial influence analysis. Over the last decade, a great effort has been\ndevoted to developing efficient algorithms for influence maximization, so that\nidentifying the ``best'' algorithm has become a demanding task. In SIGMOD'17,\nArora, Galhotra, and Ranu reported benchmark results on eleven existing\nalgorithms and demonstrated that there is no single state-of-the-art offering\nthe best trade-off between computational efficiency and solution quality.\n  In this paper, we report a high-level experimental study on three\nwell-established algorithmic approaches for influence maximization, referred to\nas Oneshot, Snapshot, and Reverse Influence Sampling (RIS). Different from\nArora et al., our experimental methodology is so designed that we examine the\ndistribution of random solutions, characterize the relation between the sample\nnumber and the actual solution quality, and avoid implementation dependencies.\nOur main findings are as follows: 1. For a sufficiently large sample number, we\nobtain a unique solution regardless of algorithms. 2. The average solution\nquality of Oneshot, Snapshot, and RIS improves at the same rate up to scaling\nof sample number. 3. Oneshot requires more samples than Snapshot, and Snapshot\nrequires fewer but larger samples than RIS. We discuss the time efficiency when\nconditioning Oneshot, Snapshot, and RIS to be of identical accuracy. Our\nconclusion is that Oneshot is suitable only if the size of available memory is\nlimited, and RIS is more efficient than Snapshot for large networks; Snapshot\nis preferable for small, low-probability networks.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 05:50:26 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Ohsaka", "Naoto", ""]]}, {"id": "2003.10064", "submitter": "Pingcheng Ruan", "authors": "Pingcheng Ruan, Dumitrel Loghin, Quang-Trung Ta, Meihui Zhang, Gang\n  Chen, Beng Chin Ooi", "title": "A Transactional Perspective on Execute-order-validate Blockchains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart contracts have enabled blockchain systems to evolve from simple\ncryptocurrency platforms, such as Bitcoin, to general transactional systems,\nsuch as Ethereum. Catering for emerging business requirements, a new\narchitecture called execute-order-validate has been proposed in Hyperledger\nFabric to support parallel transactions and improve the blockchain's\nthroughput. However, this new architecture might render many invalid\ntransactions when serializing them. This problem is further exaggerated as the\nblock formation rate is inherently limited due to other factors beside data\nprocessing, such as cryptography and consensus.\n  In this work, we propose a novel method to enhance the execute-order-validate\narchitecture, by reducing invalid transactions to improve the throughput of\nblockchains. Our method is inspired by state-of-the-art optimistic concurrency\ncontrol techniques in modern database systems. In contrast to existing\nblockchains that adopt database's preventive approaches which might abort\nserializable transactions, our method is theoretically more fine-grained.\nSpecifically, unserializable transactions are aborted before ordering and the\nremaining transactions are guaranteed to be serializable. For evaluation, we\nimplement our method in two blockchains respectively, FabricSharp on top of\nHyperledger Fabric, and FastFabricSharp on top of FastFabric. We compare the\nperformance of FabricSharp with vanilla Fabric and three related systems, two\nof which are respectively implemented with one standard and one\nstate-of-the-art concurrency control techniques from databases. The results\ndemonstrate that FabricSharp achieves 25% higher throughput compared to the\nother systems in nearly all experimental scenarios. Moreover, the\nFastFabricSharp's improvement over FastFabric is up to 66%.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 03:30:45 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Ruan", "Pingcheng", ""], ["Loghin", "Dumitrel", ""], ["Ta", "Quang-Trung", ""], ["Zhang", "Meihui", ""], ["Chen", "Gang", ""], ["Ooi", "Beng Chin", ""]]}, {"id": "2003.10076", "submitter": "Jinfei Liu", "authors": "Jinfei Liu", "title": "Absolute Shapley Value", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shapley value is a concept in cooperative game theory for measuring the\ncontribution of each participant, which was named in honor of Lloyd Shapley.\nShapley value has been recently applied in data marketplaces for compensation\nallocation based on their contribution to the models. Shapley value is the only\nvalue division scheme used for compensation allocation that meets three\ndesirable criteria: group rationality, fairness, and additivity. In cooperative\ngame theory, the marginal contribution of each contributor to each coalition is\na nonnegative value. However, in machine learning model training, the marginal\ncontribution of each contributor (data tuple) to each coalition (a set of data\ntuples) can be a negative value, i.e., the accuracy of the model trained by a\ndataset with an additional data tuple can be lower than the accuracy of the\nmodel trained by the dataset only.\n  In this paper, we investigate the problem of how to handle the negative\nmarginal contribution when computing Shapley value. We explore three\nphilosophies: 1) taking the original value (Original Shapley Value); 2) taking\nthe larger of the original value and zero (Zero Shapley Value); and 3) taking\nthe absolute value of the original value (Absolute Shapley Value). Experiments\non Iris dataset demonstrate that the definition of Absolute Shapley Value\nsignificantly outperforms the other two definitions in terms of evaluating data\nimportance (the contribution of each data tuple to the trained model).\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 04:26:30 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Liu", "Jinfei", ""]]}, {"id": "2003.10534", "submitter": "Somalee Datta", "authors": "Somalee Datta, Jose Posada, Garrick Olson, Wencheng Li, Ciaran\n  O'Reilly, Deepa Balraj, Joseph Mesterhazy, Joseph Pallas, Priyamvada Desai,\n  Nigam Shah", "title": "A new paradigm for accelerating clinical data science at Stanford\n  Medicine", "comments": "Total of 44 pages. Main has total of 18 pages including references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stanford Medicine is building a new data platform for our academic research\ncommunity to do better clinical data science. Hospitals have a large amount of\npatient data and researchers have demonstrated the ability to reuse that data\nand AI approaches to derive novel insights, support patient care, and improve\ncare quality. However, the traditional data warehouse and Honest Broker\napproaches that are in current use, are not scalable. We are establishing a new\nsecure Big Data platform that aims to reduce time to access and analyze data.\nIn this platform, data is anonymized to preserve patient data privacy and made\navailable preparatory to Institutional Review Board (IRB) submission.\nFurthermore, the data is standardized such that analysis done at Stanford can\nbe replicated elsewhere using the same analytical code and clinical concepts.\nFinally, the analytics data warehouse integrates with a secure data science\ncomputational facility to support large scale data analytics. The ecosystem is\ndesigned to bring the modern data science community to highly sensitive\nclinical data in a secure and collaborative big data analytics environment with\na goal to enable bigger, better and faster science.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 16:21:42 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Datta", "Somalee", ""], ["Posada", "Jose", ""], ["Olson", "Garrick", ""], ["Li", "Wencheng", ""], ["O'Reilly", "Ciaran", ""], ["Balraj", "Deepa", ""], ["Mesterhazy", "Joseph", ""], ["Pallas", "Joseph", ""], ["Desai", "Priyamvada", ""], ["Shah", "Nigam", ""]]}, {"id": "2003.10554", "submitter": "Peter Alvaro", "authors": "Kyle Kingsbury and Peter Alvaro", "title": "Elle: Inferring Isolation Anomalies from Experimental Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Users who care about their data store it in databases, which (at least in\nprinciple) guarantee some form of transactional isolation. However, experience\nshows [Kleppmann 2019, Kingsbury and Patella 2019a] that many databases do not\nprovide the isolation guarantees they claim. With the recent proliferation of\nnew distributed databases, demand has grown for checkers that can, by\ngenerating client workloads and injecting faults, produce anomalies that\nwitness a violation of a stated guarantee. An ideal checker would be sound (no\nfalse positives), efficient (polynomial in history length and concurrency),\neffective (finding violations in real databases), general (analyzing many\npatterns of transactions), and informative (justifying the presence of an\nanomaly with understandable counterexamples). Sadly, we are aware of no\ncheckers that satisfy these goals.\n  We present Elle: a novel checker which infers an Adya-style dependency graph\nbetween client-observed transactions. It does so by carefully selecting\ndatabase objects and operations when generating histories, so as to ensure that\nthe results of database reads reveal information about their version history.\nElle can detect every anomaly in Adya et al's formalism [Adya et al. 2000]\n(except for predicates), discriminate between them, and provide concise\nexplanations of each. This paper makes the following contributions: we present\nElle, demonstrate its soundness, measure its efficiency against the current\nstate of the art, and give evidence of its effectiveness via a case study of\nfour real databases.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 21:11:06 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Kingsbury", "Kyle", ""], ["Alvaro", "Peter", ""]]}, {"id": "2003.10588", "submitter": "Alireza Samadian", "authors": "Mahmoud Abo-Khamis and Sungjin Im and Benjamin Moseley and Kirk Pruhs\n  and Alireza Samadian", "title": "Approximate Aggregate Queries Under Additive Inequalities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of evaluating certain types of functional aggregation\nqueries on relational data subject to additive inequalities. Such aggregation\nqueries, with a smallish number of additive inequalities, arise\nnaturally/commonly in many applications, particularly in learning applications.\nWe give a relatively complete categorization of the computational complexity of\nsuch problems. We first show that the problem is NP-hard, even in the case of\none additive inequality. Thus we turn to approximating the query. Our main\nresult is an efficient algorithm for approximating, with arbitrarily small\nrelative error, many natural aggregation queries with one additive inequality.\nWe give examples of natural queries that can be efficiently solved using this\nalgorithm. In contrast, we show that the situation with two additive\ninequalities is quite different, by showing that it is NP-hard to evaluate\nsimple aggregation queries, with two additive inequalities, with any bounded\nrelative error.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 00:24:18 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 20:52:34 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Abo-Khamis", "Mahmoud", ""], ["Im", "Sungjin", ""], ["Moseley", "Benjamin", ""], ["Pruhs", "Kirk", ""], ["Samadian", "Alireza", ""]]}, {"id": "2003.10815", "submitter": "Viktor Varkarakis", "authors": "Viktor Varkarakis, Peter Corcoran", "title": "Dataset Cleaning -- A Cross Validation Methodology for Large Facial\n  Datasets using Face Recognition", "comments": "2020 Twelfth International Conference on Quality of Multimedia\n  Experience (QoMEX)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, large \"in the wild\" face datasets have been released in an\nattempt to facilitate progress in tasks such as face detection, face\nrecognition, and other tasks. Most of these datasets are acquired from webpages\nwith automatic procedures. As a consequence, noisy data are often found.\nFurthermore, in these large face datasets, the annotation of identities is\nimportant as they are used for training face recognition algorithms. But due to\nthe automatic way of gathering these datasets and due to their large size, many\nidentities folder contain mislabeled samples which deteriorates the quality of\nthe datasets. In this work, it is presented a semi-automatic method for\ncleaning the noisy large face datasets with the use of face recognition. This\nmethodology is applied to clean the CelebA dataset and show its effectiveness.\nFurthermore, the list with the mislabelled samples in the CelebA dataset is\nmade available.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 13:01:13 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Varkarakis", "Viktor", ""], ["Corcoran", "Peter", ""]]}, {"id": "2003.11105", "submitter": "Alan Liu", "authors": "Han Liu, Shantao Liu", "title": "EQL -- an extremely easy to learn knowledge graph query language,\n  achieving highspeed and precise search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  EQL, also named as Extremely Simple Query Language, can be widely used in the\nfield of knowledge graph, precise search, strong artificial intelligence,\ndatabase, smart speaker ,patent search and other fields. EQL adopt the\nprinciple of minimalism in design and pursues simplicity and easy to learn so\nthat everyone can master it quickly. EQL language and lambda calculus are\ninterconvertible, that reveals the mathematical nature of EQL language, and\nlays a solid foundation for rigor and logical integrity of EQL language. The\nEQL language and a comprehensive knowledge graph system with the world's\ncommonsense can together form the foundation of strong AI in the future, and\nmake up for the current lack of understanding of world's commonsense by current\nAI system. EQL language can be used not only by humans, but also as a basic\nlanguage for data query and data exchange between robots.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 03:32:04 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Liu", "Han", ""], ["Liu", "Shantao", ""]]}, {"id": "2003.11124", "submitter": "Piero Giacomelli", "authors": "Piero Giacomelli", "title": "Implementing Suffix Array Algorithm Using Apache Big Table Data\n  Implementation", "comments": "Paper prepared for a conference but never submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we will describe a new approach on the well-known suffix-array\nalgorithm using Big Table Data Technology. We will demonstrate how it is\npossible to refactor a well-known algorithm coupled by taking advantage of an\nhigh-performance distributed datastore, to illustrate the advantages of using\ndatastore cloud related technology for storing large text sequences and\nretrieving them. A case study using DNA strings, considered one of the most\ndifficult pattern matching problem, will be described and evaluated to\ndemonstrate the potentiality of this implementation. Further discussion on\nperformances and other big data related issues will be described as well as new\npossible lines of research in big data technology for precise medical\napplications.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 21:38:55 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Giacomelli", "Piero", ""]]}, {"id": "2003.11546", "submitter": "Alfredo Pulvirenti", "authors": "Giovanni Micale, Vincenzo Bonnici, Alfredo Ferro, Dennis Shasha,\n  Rosalba Giugno, Alfredo Pulvirenti", "title": "MultiRI: Fast Subgraph Matching in Labeled Multigraphs", "comments": "Submitted for pubblication on January 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The Subgraph Matching (SM) problem consists of finding all the embeddings of\na given small graph, called the query, into a large graph, called the target.\nThe SM problem has been widely studied for simple graphs, i.e. graphs where\nthere is exactly one edge between two nodes and nodes have single labels, but\nfew approaches have been devised for labeled multigraphs, i.e. graphs having\npossibly multiple labels on nodes in which pair of nodes may have multiple\nlabeled edges between them. Here we present MultiRI, a novel algorithm for the\nSub-Multigraph Matching (SMM) problem, i.e. subgraph matching in labeled\nmultigraphs. MultiRI improves on the state-of-the-art by computing\ncompatibility domains and symmetry breaking conditions on query nodes to filter\nthe search space of possible solutions. Empirically, we show that MultiRI\noutperforms the state-of-the-art method for the SMM problem in both synthetic\nand real graphs, with a multiplicative speedup between five and ten for large\ngraphs, by using a limited amount of memory.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 15:04:05 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Micale", "Giovanni", ""], ["Bonnici", "Vincenzo", ""], ["Ferro", "Alfredo", ""], ["Shasha", "Dennis", ""], ["Giugno", "Rosalba", ""], ["Pulvirenti", "Alfredo", ""]]}, {"id": "2003.11547", "submitter": "Sheng Wang", "authors": "Sheng Wang, Zhifeng Bao, J. Shane Culpepper, Gao Cong", "title": "A Survey on Trajectory Data Management, Analytics, and Learning", "comments": "Accepted to the ACM Computing Surveys", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in sensor and mobile devices have enabled an unprecedented\nincrease in the availability and collection of urban trajectory data, thus\nincreasing the demand for more efficient ways to manage and analyze the data\nbeing produced. In this survey, we comprehensively review recent research\ntrends in trajectory data management, ranging from trajectory pre-processing,\nstorage, common trajectory analytic tools, such as querying spatial-only and\nspatial-textual trajectory data, and trajectory clustering. We also explore\nfour closely related analytical tasks commonly used with trajectory data in\ninteractive or real-time processing. Deep trajectory learning is also reviewed\nfor the first time. Finally, we outline the essential qualities that a\ntrajectory data management system should possess in order to maximize\nflexibility.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 16:19:41 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 16:37:56 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Wang", "Sheng", ""], ["Bao", "Zhifeng", ""], ["Culpepper", "J. Shane", ""], ["Cong", "Gao", ""]]}, {"id": "2003.11580", "submitter": "Chuan Lei", "authors": "Chuan Lei, Rana Alotaibi, Abdul Quamar, Vasilis Efthymiou, and Fatma\n  \\\"Ozcan", "title": "Property Graph Schema Optimization for Domain-Specific Knowledge Graphs", "comments": "To appear in ICDE 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enterprises are creating domain-specific knowledge graphs by curating and\nintegrating their business data from multiple sources. The data in these\nknowledge graphs can be described using ontologies, which provide a semantic\nabstraction to define the content in terms of the entities and the\nrelationships of the domain. The rich semantic relationships in an ontology\ncontain a variety of opportunities to reduce edge traversals and consequently\nimprove the graph query performance. Although there has been a lot of effort to\nbuild systems that enable efficient querying over knowledge graphs, the problem\nof schema optimization for query performance has been largely ignored in the\ngraph setting. In this work, we show that graph schema design has significant\nimpact on query performance, and then propose optimization algorithms that\nexploit the opportunities from the domain ontology to generate efficient\nproperty graph schemas. To the best of our knowledge, we are the first to\npresent an ontology-driven approach for property graph schema optimization. We\nconduct empirical evaluations with two real-world knowledge graphs from medical\nand financial domains. The results show that the schemas produced by the\noptimization algorithms achieve up to 2 orders of magnitude speed-up compared\nto the baseline approach.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 18:35:22 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 06:42:06 GMT"}, {"version": "v3", "created": "Sat, 3 Oct 2020 17:08:58 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Lei", "Chuan", ""], ["Alotaibi", "Rana", ""], ["Quamar", "Abdul", ""], ["Efthymiou", "Vasilis", ""], ["\u00d6zcan", "Fatma", ""]]}, {"id": "2003.12396", "submitter": "Shaoxu Song", "authors": "Aoqian Zhang, Shaoxu Song, Jianmin Wang, Philip S. Yu", "title": "Time Series Data Cleaning: From Anomaly Detection to Anomaly Repairing\n  (Technical Report)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Errors are prevalent in time series data, such as GPS trajectories or sensor\nreadings. Existing methods focus more on anomaly detection but not on repairing\nthe detected anomalies. By simply filtering out the dirty data via anomaly\ndetection, applications could still be unreliable over the incomplete time\nseries. Instead of simply discarding anomalies, we propose to (iteratively)\nrepair them in time series data, by creatively bonding the beauty of temporal\nnature in anomaly detection with the widely considered minimum change principle\nin data repairing. Our major contributions include: (1) a novel framework of\niterative minimum repairing (IMR) over time series data, (2) explicit analysis\non convergence of the proposed iterative minimum repairing, and (3) efficient\nestimation of parameters in each iteration. Remarkably, with incremental\ncomputation, we reduce the complexity of parameter estimation from O(n) to\nO(1). Experiments on real datasets demonstrate the superiority of our proposal\ncompared to the state-of-the-art approaches. In particular, we show that (the\nproposed) repairing indeed improves the time series classification application.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 13:05:11 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Zhang", "Aoqian", ""], ["Song", "Shaoxu", ""], ["Wang", "Jianmin", ""], ["Yu", "Philip S.", ""]]}, {"id": "2003.12590", "submitter": "Martin Grohe", "authors": "Martin Grohe", "title": "word2vec, node2vec, graph2vec, X2vec: Towards a Theory of Vector\n  Embeddings of Structured Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.DM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vector representations of graphs and relational structures, whether\nhand-crafted feature vectors or learned representations, enable us to apply\nstandard data analysis and machine learning techniques to the structures. A\nwide range of methods for generating such embeddings have been studied in the\nmachine learning and knowledge representation literature. However, vector\nembeddings have received relatively little attention from a theoretical point\nof view.\n  Starting with a survey of embedding techniques that have been used in\npractice, in this paper we propose two theoretical approaches that we see as\ncentral for understanding the foundations of vector embeddings. We draw\nconnections between the various approaches and suggest directions for future\nresearch.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 18:23:55 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Grohe", "Martin", ""]]}, {"id": "2003.13083", "submitter": "Hao Xu", "authors": "Hao Xu, Paulo Valente Klaine, Oluwakayode Onireti, Bin Cao, Muhammad\n  Imran, Lei Zhang", "title": "Blockchain-enabled Resource Management and Sharing for 6G Communications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.SY eess.SP eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The sixth generation (6G) network must provide performance superior to\nprevious generations in order to meet the requirements of emerging services and\napplications, such as multi-gigabit transmission rate, even higher reliability,\nsub 1 millisecond latency and ubiquitous connection for Internet of Everything.\nHowever, with the scarcity of spectrum resources, efficient resource management\nand sharing is crucial to achieve all these ambitious requirements. One\npossible technology to enable all of this is blockchain, which has recently\ngained significance and will be of paramount importance to 6G networks and\nbeyond due to its inherent properties. In particular, the integration of\nblockchain in 6G will enable the network to monitor and manage resource\nutilization and sharing efficiently. Hence, in this article, we discuss the\npotentials of blockchain for resource management and sharing in 6G using\nmultiple application scenarios namely, Internet of things, device-to-device\ncommunications, network slicing, and inter-domain blockchain ecosystems.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 17:38:49 GMT"}, {"version": "v2", "created": "Sat, 11 Apr 2020 11:57:16 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Xu", "Hao", ""], ["Klaine", "Paulo Valente", ""], ["Onireti", "Oluwakayode", ""], ["Cao", "Bin", ""], ["Imran", "Muhammad", ""], ["Zhang", "Lei", ""]]}, {"id": "2003.13084", "submitter": "Daniel Garijo", "authors": "Daniel Garijo and Mar\\'ia Poveda-Villal\\'on", "title": "Best Practices for Implementing FAIR Vocabularies and Ontologies on the\n  Web", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.AI cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the adoption of Semantic Web technologies, an increasing number of\nvocabularies and ontologies have been developed in different domains, ranging\nfrom Biology to Agronomy or Geosciences. However, many of these ontologies are\nstill difficult to find, access and understand by researchers due to a lack of\ndocumentation, URI resolving issues, versioning problems, etc. In this chapter\nwe describe guidelines and best practices for creating accessible,\nunderstandable and reusable ontologies on the Web, using standard practices and\npointing to existing tools and frameworks developed by the Semantic Web\ncommunity. We illustrate our guidelines with concrete examples, in order to\nhelp researchers implement these practices in their future vocabularies.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 17:40:04 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Garijo", "Daniel", ""], ["Poveda-Villal\u00f3n", "Mar\u00eda", ""]]}, {"id": "2003.13103", "submitter": "Jinfei Liu", "authors": "Jinfei Liu", "title": "Dealer: End-to-End Data Marketplace with Model-based Pricing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven machine learning (ML) has witnessed great successes across a\nvariety of application domains. Since ML model training are crucially relied on\na large amount of data, there is a growing demand for high quality data to be\ncollected for ML model training. However, from data owners' perspective, it is\nrisky for them to contribute their data. To incentivize data contribution, it\nwould be ideal that their data would be used under their preset restrictions\nand they get paid for their data contribution.\n  In this paper, we take a formal data market perspective and propose the first\nen\\textbf{\\underline{D}}-to-\\textbf{\\underline{e}}nd d\\textbf{\\underline{a}}ta\nmarketp\\textbf{\\underline{l}}ace with mod\\textbf{\\underline{e}}l-based\np\\textbf{\\underline{r}}icing (\\emph{Dealer}) towards answering the question:\n\\emph{How can the broker assign value to data owners based on their\ncontribution to the models to incentivize more data contribution, and determine\npricing for a series of models for various model buyers to maximize the revenue\nwith arbitrage-free guarantee}. For the former, we introduce a Shapley\nvalue-based mechanism to quantify each data owner's value towards all the\nmodels trained out of the contributed data. For the latter, we design a pricing\nmechanism based on models' privacy parameters to maximize the revenue. More\nimportantly, we study how the data owners' data usage restrictions affect\nmarket design, which is a striking difference of our approach with the existing\nmethods. Furthermore, we show a concrete realization DP-\\emph{Dealer} which\nprovably satisfies the desired formal properties. Extensive experiments show\nthat DP-\\emph{Dealer} is efficient and effective.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 18:30:53 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Liu", "Jinfei", ""]]}, {"id": "2003.13114", "submitter": "Venkata Vamsikrishna Meduri", "authors": "Venkata Vamsikrishna Meduri, Lucian Popa, Prithviraj Sen, Mohamed\n  Sarwat", "title": "A Comprehensive Benchmark Framework for Active Learning Methods in\n  Entity Matching", "comments": "accepted for publication in ACM-SIGMOD 2020, 15 pages", "journal-ref": null, "doi": "10.1145/3318464.3380597", "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity Matching (EM) is a core data cleaning task, aiming to identify\ndifferent mentions of the same real-world entity. Active learning is one way to\naddress the challenge of scarce labeled data in practice, by dynamically\ncollecting the necessary examples to be labeled by an Oracle and refining the\nlearned model (classifier) upon them. In this paper, we build a unified active\nlearning benchmark framework for EM that allows users to easily combine\ndifferent learning algorithms with applicable example selection algorithms. The\ngoal of the framework is to enable concrete guidelines for practitioners as to\nwhat active learning combinations will work well for EM. Towards this, we\nperform comprehensive experiments on publicly available EM datasets from\nproduct and publication domains to evaluate active learning methods, using a\nvariety of metrics including EM quality, #labels and example selection\nlatencies. Our most surprising result finds that active learning with fewer\nlabels can learn a classifier of comparable quality as supervised learning. In\nfact, for several of the datasets, we show that there is an active learning\ncombination that beats the state-of-the-art supervised learning result. Our\nframework also includes novel optimizations that improve the quality of the\nlearned model by roughly 9% in terms of F1-score and reduce example selection\nlatencies by up to 10x without affecting the quality of the model.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 19:08:03 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Meduri", "Venkata Vamsikrishna", ""], ["Popa", "Lucian", ""], ["Sen", "Prithviraj", ""], ["Sarwat", "Mohamed", ""]]}, {"id": "2003.13831", "submitter": "S{\\l}awomir Staworko", "authors": "Iovka Boneva, Jose Lozano, S{\\l}awek Staworko", "title": "Consistency and Certain Answers in Relational to RDF Data Exchange with\n  Shape Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the data exchange from relational databases to RDF graphs\ninspired by R2RML with the addition of target shape schemas. We study the\nproblems of consistency i.e., checking that every source instance admits a\nsolution, and certain query answering i.e., finding answers present in every\nsolution. We identify the class of constructive relational to RDF data exchange\nthat uses IRI constructors and full tgds (with no existential variables) in its\nsource to target dependencies. We show that the consistency problem is\ncoNP-complete. We introduce the notion of universal simulation solution that\nallows to compute certain query answers to any class of queries that is robust\nunder simulation. One such class are nested regular expressions (NREs) that are\nforward i.e., do not use the inverse operation. Using universal simulation\nsolution renders tractable the computation of certain answers to forward NREs\n(data-complexity). Finally, we present a number of results that show that\nrelaxing the restrictions of the proposed framework leads to an increase in\ncomplexity.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 21:36:49 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Boneva", "Iovka", ""], ["Lozano", "Jose", ""], ["Staworko", "S\u0142awek", ""]]}, {"id": "2003.13922", "submitter": "Tianhao Wang", "authors": "Aiping Xiong, Tianhao Wang, Ninghui Li, Somesh Jha", "title": "Towards Effective Differential Privacy Communication for Users' Data\n  Sharing Decision and Comprehension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy protects an individual's privacy by perturbing data on\nan aggregated level (DP) or individual level (LDP). We report four online\nhuman-subject experiments investigating the effects of using different\napproaches to communicate differential privacy techniques to laypersons in a\nhealth app data collection setting. Experiments 1 and 2 investigated\nparticipants' data disclosure decisions for low-sensitive and high-sensitive\npersonal information when given different DP or LDP descriptions. Experiments 3\nand 4 uncovered reasons behind participants' data sharing decisions, and\nexamined participants' subjective and objective comprehensions of these DP or\nLDP descriptions. When shown descriptions that explain the implications instead\nof the definition/processes of DP or LDP technique, participants demonstrated\nbetter comprehension and showed more willingness to share information with LDP\nthan with DP, indicating their understanding of LDP's stronger privacy\nguarantee compared with DP.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 02:36:39 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Xiong", "Aiping", ""], ["Wang", "Tianhao", ""], ["Li", "Ninghui", ""], ["Jha", "Somesh", ""]]}, {"id": "2003.13968", "submitter": "Yuliang Li", "authors": "Aaron Feng, Shuwei Chen, Yuliang Li, Hiroshi Matsuda, Hidekazu Tamaki,\n  Wang-Chiew Tan", "title": "Towards Productionizing Subjective Search Systems", "comments": "In Submission to VLDB 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing e-commerce search engines typically support search only over\nobjective attributes, such as price and locations, leaving the more desirable\nsubjective attributes, such as romantic vibe and worklife balance unsearchable.\nWe found that this is also the case for Recruit Group, which operates a wide\nrange of online booking and search services, including jobs, travel, housing,\nbridal, dining, beauty, and where each service is among the biggest in Japan,\nif not internationally. We present our progress towards productionizing a\nrecent subjective search prototype (OpineDB) developed by Megagon Labs for\nRecruit Group. Several components within OpineDB are enhanced to satisfy\nproduction demands, including adding a BERT language model pre-trained on\nmassive hospitality domain review corpora. We also found that the challenges of\nproductionizing the system are beyond enhancing the components. In particular,\nan important requirement in production-quality systems is to instrument a\nproper way of measuring the search quality, which is extremely tricky when the\nsearch results are subjective. This led to the creation of a high-quality\nbenchmark dataset from scratch, involving over 600 queries by user interviews\nand a collection of more than 120,000 query-entity relevancy labels. Also, we\nfound that the existing search algorithms do not meet the search quality\nstandard required by production systems. Consequently, we enhanced the ranking\nmodel by fine-tuning several search algorithms and combining them under a\nlearning-to-rank framework. The model achieves 5%-10% overall precision\nimprovement and 90+% precision on more than half of the benchmark testing\nqueries making these queries ready for AB-testing. While some enhancements can\nbe immediately applied to other verticals, our experience reveals that\nbenchmarking and fine-tuning ranking algorithms are specific to each domain and\ncannot be avoided.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 06:17:49 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Feng", "Aaron", ""], ["Chen", "Shuwei", ""], ["Li", "Yuliang", ""], ["Matsuda", "Hiroshi", ""], ["Tamaki", "Hidekazu", ""], ["Tan", "Wang-Chiew", ""]]}, {"id": "2003.14046", "submitter": "Zhiwu Xie", "authors": "Xinyue Wang, Zhiwu Xie", "title": "The Case For Alternative Web Archival Formats To Expedite The\n  Data-To-Insight Cycle", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": "10.1145/3383583.3398542", "report-no": null, "categories": "cs.DL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The WARC file format is widely used by web archives to preserve collected web\ncontent for future use. With the rapid growth of web archives and the\nincreasing interest to reuse these archives as big data sources for statistical\nand analytical research, the speed to turn these data into insights becomes\ncritical. In this paper we show that the WARC format carries significant\nperformance penalties for batch processing workload. We trace the root cause of\nthese penalties to its data structure, encoding, and addressing method. We then\nrun controlled experiments to illustrate how severe these problems can be.\nIndeed, performance gain of one to two orders of magnitude can be achieved\nsimply by reformatting WARC files into Parquet or Avro formats. While these\nresults do not necessarily constitute an endorsement for Avro or Parquet, the\ntime has come for the web archiving community to consider replacing WARC with\nmore efficient web archival formats.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 09:23:35 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Wang", "Xinyue", ""], ["Xie", "Zhiwu", ""]]}]