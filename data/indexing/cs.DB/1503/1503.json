[{"id": "1503.00244", "submitter": "Kamran Kowsari", "authors": "Nima Bari, Roman Vichr, Kamran Kowsari, Simon Y. Berkovich", "title": "23-bit Metaknowledge Template Towards Big Data Knowledge Discovery and\n  Management", "comments": "IEEE Data Science and Advanced Analytics (DSAA'2014)", "journal-ref": null, "doi": "10.1109/DSAA.2014.7058121", "report-no": null, "categories": "cs.DB cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The global influence of Big Data is not only growing but seemingly endless.\nThe trend is leaning towards knowledge that is attained easily and quickly from\nmassive pools of Big Data. Today we are living in the technological world that\nDr. Usama Fayyad and his distinguished research fellows discussed in the\nintroductory explanations of Knowledge Discovery in Databases (KDD) predicted\nnearly two decades ago. Indeed, they were precise in their outlook on Big Data\nanalytics. In fact, the continued improvement of the interoperability of\nmachine learning, statistics, database building and querying fused to create\nthis increasingly popular science- Data Mining and Knowledge Discovery. The\nnext generation computational theories are geared towards helping to extract\ninsightful knowledge from even larger volumes of data at higher rates of speed.\nAs the trend increases in popularity, the need for a highly adaptive solution\nfor knowledge discovery will be necessary. In this research paper, we are\nintroducing the investigation and development of 23 bit-questions for a\nMetaknowledge template for Big Data Processing and clustering purposes. This\nresearch aims to demonstrate the construction of this methodology and proves\nthe validity and the beneficial utilization that brings Knowledge Discovery\nfrom Big Data.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2015 09:41:11 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Bari", "Nima", ""], ["Vichr", "Roman", ""], ["Kowsari", "Kamran", ""], ["Berkovich", "Simon Y.", ""]]}, {"id": "1503.00245", "submitter": "Kamran Kowsari", "authors": "Nima Bari, Roman Vichr, Kamran Kowsari, Simon Y. Berkovich", "title": "Novel Metaknowledge-based Processing Technique for Multimedia Big Data\n  clustering challenges", "comments": "IEEE Multimedia Big Data (BigMM 2015)", "journal-ref": null, "doi": "10.1109/BigMM.2015.78", "report-no": null, "categories": "cs.DB cs.AI cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Past research has challenged us with the task of showing relational patterns\nbetween text-based data and then clustering for predictive analysis using Golay\nCode technique. We focus on a novel approach to extract metaknowledge in\nmultimedia datasets. Our collaboration has been an on-going task of studying\nthe relational patterns between datapoints based on metafeatures extracted from\nmetaknowledge in multimedia datasets. Those selected are significant to suit\nthe mining technique we applied, Golay Code algorithm. In this research paper\nwe summarize findings in optimization of metaknowledge representation for\n23-bit representation of structured and unstructured multimedia data in order\nto\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2015 09:53:15 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Bari", "Nima", ""], ["Vichr", "Roman", ""], ["Kowsari", "Kamran", ""], ["Berkovich", "Simon Y.", ""]]}, {"id": "1503.00301", "submitter": "Saskia Metzler", "authors": "Saskia Metzler and Pauli Miettinen", "title": "On Defining SPARQL with Boolean Tensor Algebra", "comments": null, "journal-ref": null, "doi": "10.1145/2740908.2742738", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Resource Description Framework (RDF) represents information as\nsubject-predicate-object triples. These triples are commonly interpreted as a\ndirected labelled graph. We propose an alternative approach, interpreting the\ndata as a 3-way Boolean tensor. We show how SPARQL queries - the standard\nqueries for RDF - can be expressed as elementary operations in Boolean algebra,\ngiving us a complete re-interpretation of RDF and SPARQL. We show how the\nBoolean tensor interpretation allows for new optimizations and analyses of the\ncomplexity of SPARQL queries. For example, estimating the size of the results\nfor different join queries becomes much simpler.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2015 16:40:56 GMT"}], "update_date": "2016-09-19", "authors_parsed": [["Metzler", "Saskia", ""], ["Miettinen", "Pauli", ""]]}, {"id": "1503.00302", "submitter": "Xin Luna Dong", "authors": "Xin Luna Dong, Evgeniy Gabrilovich, Geremy Heitz, Wilko Horn, Kevin\n  Murphy, Shaohua Sun, Wei Zhang", "title": "From Data Fusion to Knowledge Fusion", "comments": "VLDB'2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of {\\em data fusion} is to identify the true values of data items\n(eg, the true date of birth for {\\em Tom Cruise}) among multiple observed\nvalues drawn from different sources (eg, Web sites) of varying (and unknown)\nreliability. A recent survey\\cite{LDL+12} has provided a detailed comparison of\nvarious fusion methods on Deep Web data. In this paper, we study the\napplicability and limitations of different fusion techniques on a more\nchallenging problem: {\\em knowledge fusion}. Knowledge fusion identifies true\nsubject-predicate-object triples extracted by multiple information extractors\nfrom multiple information sources. These extractors perform the tasks of entity\nlinkage and schema alignment, thus introducing an additional source of noise\nthat is quite different from that traditionally considered in the data fusion\nliterature, which only focuses on factual errors in the original sources. We\nadapt state-of-the-art data fusion techniques and apply them to a knowledge\nbase with 1.6B unique knowledge triples extracted by 12 extractors from over 1B\nWeb pages, which is three orders of magnitude larger than the data sets used in\nprevious data fusion papers. We show great promise of the data fusion\napproaches in solving the knowledge fusion problem, and suggest interesting\nresearch directions through a detailed error analysis of the methods.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2015 16:46:26 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Dong", "Xin Luna", ""], ["Gabrilovich", "Evgeniy", ""], ["Heitz", "Geremy", ""], ["Horn", "Wilko", ""], ["Murphy", "Kevin", ""], ["Sun", "Shaohua", ""], ["Zhang", "Wei", ""]]}, {"id": "1503.00303", "submitter": "Xin Luna Dong", "authors": "Xian Li and Xin Luna Dong and Kenneth Lyons and Weiyi Meng and Divesh\n  Srivastava", "title": "Truth Finding on the Deep Web: Is the Problem Solved?", "comments": "VLDB'2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The amount of useful information available on the Web has been growing at a\ndramatic pace in recent years and people rely more and more on the Web to\nfulfill their information needs. In this paper, we study truthfulness of Deep\nWeb data in two domains where we believed data are fairly clean and data\nquality is important to people's lives: {\\em Stock} and {\\em Flight}. To our\nsurprise, we observed a large amount of inconsistency on data from different\nsources and also some sources with quite low accuracy. We further applied on\nthese two data sets state-of-the-art {\\em data fusion} methods that aim at\nresolving conflicts and finding the truth, analyzed their strengths and\nlimitations, and suggested promising research directions. We wish our study can\nincrease awareness of the seriousness of conflicting data on the Web and in\nturn inspire more research in our community to tackle this problem.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2015 16:47:30 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Li", "Xian", ""], ["Dong", "Xin Luna", ""], ["Lyons", "Kenneth", ""], ["Meng", "Weiyi", ""], ["Srivastava", "Divesh", ""]]}, {"id": "1503.00306", "submitter": "Xin Luna Dong", "authors": "Ravali Pochampally and Anish Das Sarma and Xin Luna Dong and Alexandra\n  Meliou and Divesh Srivastava", "title": "Fusing Data with Correlations", "comments": "Sigmod'2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications rely on Web data and extraction systems to accomplish\nknowledge-driven tasks. Web information is not curated, so many sources provide\ninaccurate, or conflicting information. Moreover, extraction systems introduce\nadditional noise to the data. We wish to automatically distinguish correct data\nand erroneous data for creating a cleaner set of integrated data. Previous work\nhas shown that a na\\\"ive voting strategy that trusts data provided by the\nmajority or at least a certain number of sources may not work well in the\npresence of copying between the sources. However, correlation between sources\ncan be much broader than copying: sources may provide data from complementary\ndomains (\\emph{negative correlation}), extractors may focus on different types\nof information (\\emph{negative correlation}), and extractors may apply common\nrules in extraction (\\emph{positive correlation, without copying}). In this\npaper we present novel techniques modeling correlations between sources and\napplying it in truth finding.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2015 16:52:55 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Pochampally", "Ravali", ""], ["Sarma", "Anish Das", ""], ["Dong", "Xin Luna", ""], ["Meliou", "Alexandra", ""], ["Srivastava", "Divesh", ""]]}, {"id": "1503.00309", "submitter": "Xin Luna Dong", "authors": "Xian Li and Xin Luna Dong and Kenneth B. Lyons and Weiyi Meng and\n  Divesh Srivastava", "title": "Scaling up Copy Detection", "comments": "ICDE 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research shows that copying is prevalent for Deep-Web data and\nconsidering copying can significantly improve truth finding from conflicting\nvalues. However, existing copy detection techniques do not scale for large\nsizes and numbers of data sources, so truth finding can be slowed down by one\nto two orders of magnitude compared with the corresponding techniques that do\nnot consider copying. In this paper, we study {\\em how to improve scalability\nof copy detection on structured data}.\n  Our algorithm builds an inverted index for each \\emph{shared} value and\nprocesses the index entries in decreasing order of how much the shared value\ncan contribute to the conclusion of copying. We show how we use the index to\nprune the data items we consider for each pair of sources, and to incrementally\nrefine our results in iterative copy detection. We also apply a sampling\nstrategy with which we are able to further reduce copy-detection time while\nstill obtaining very similar results as on the whole data set. Experiments on\nvarious real data sets show that our algorithm can reduce the time for copy\ndetection by two to three orders of magnitude; in other words, truth finding\ncan benefit from copy detection with very little overhead.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2015 17:00:29 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Li", "Xian", ""], ["Dong", "Xin Luna", ""], ["Lyons", "Kenneth B.", ""], ["Meng", "Weiyi", ""], ["Srivastava", "Divesh", ""]]}, {"id": "1503.00310", "submitter": "Xin Luna Dong", "authors": "Xin Luna Dong and Laure Berti-Equille and Divesh Srivastava", "title": "Data Fusion: Resolving Conflicts from Multiple Sources", "comments": "WAIM 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many data management applications, such as setting up Web portals, managing\nenterprise data, managing community data, and sharing scientific data, require\nintegrating data from multiple sources. Each of these sources provides a set of\nvalues and different sources can often provide conflicting values. To present\nquality data to users, it is critical to resolve conflicts and discover values\nthat reflect the real world; this task is called {\\em data fusion}. This paper\ndescribes a novel approach that finds true values from conflicting information\nwhen there are a large number of sources, among which some may copy from\nothers. We present a case study on real-world data showing that the described\nalgorithm can significantly improve accuracy of truth discovery and is scalable\nwhen there are a large number of data sources.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2015 17:02:17 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Dong", "Xin Luna", ""], ["Berti-Equille", "Laure", ""], ["Srivastava", "Divesh", ""]]}, {"id": "1503.00503", "submitter": "Eugene Panferov", "authors": "Eugene Panferov", "title": "A Next-Generation Data Language Proposal", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper analyzes the fatal drawback of the relational calculus not\nallowing relations to be domains of relations, and its consequences entrenched\nin SQL. In order to overcome this obstacle we propose \"multitable index\" - an\neasily implementable upgrade to an existing data storage, which enables a\nrevolutionary change in the field of data languages - the demotion of\nrelational calculus and tables. We propose a new data language with \"pragmatic\ntypisation\" where types represent domain knowledge but not memory management.\nThe language handles sets of tuples as first class data objects and supports\nset operation and tuple (de)composition operations as fluently as basic arith.\nAnd it is equally suitable for building-into a general purpose language as well\nas querying a remote DB (thus removing the ubiquitous gap between SQL and\napplication).\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 12:52:53 GMT"}, {"version": "v2", "created": "Sun, 22 Mar 2015 20:30:36 GMT"}, {"version": "v3", "created": "Sun, 21 Feb 2021 18:11:54 GMT"}, {"version": "v4", "created": "Fri, 26 Feb 2021 10:48:29 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Panferov", "Eugene", ""]]}, {"id": "1503.00604", "submitter": "Pei Li", "authors": "Pei Li, Xin Luna Dong, Songtao Guo, Andrea Maurino, Divesh Srivastava", "title": "Robust Group Linkage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of group linkage: linking records that refer to entities\nin the same group. Applications for group linkage include finding businesses in\nthe same chain, finding conference attendees from the same affiliation, finding\nplayers from the same team, etc. Group linkage faces challenges not present for\ntraditional record linkage. First, although different members in the same group\ncan share some similar global values of an attribute, they represent different\nentities so can also have distinct local values for the same or different\nattributes, requiring a high tolerance for value diversity. Second, groups can\nbe huge (with tens of thousands of records), requiring high scalability even\nafter using good blocking strategies.\n  We present a two-stage algorithm: the first stage identifies cores containing\nrecords that are very likely to belong to the same group, while being robust to\npossible erroneous values; the second stage collects strong evidence from the\ncores and leverages it for merging more records into the same group, while\nbeing tolerant to differences in local values of an attribute. Experimental\nresults show the high effectiveness and efficiency of our algorithm on various\nreal-world data sets.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 16:39:17 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Li", "Pei", ""], ["Dong", "Xin Luna", ""], ["Guo", "Songtao", ""], ["Maurino", "Andrea", ""], ["Srivastava", "Divesh", ""]]}, {"id": "1503.00650", "submitter": "Angelos Vasilakopoulos", "authors": "Foto N. Afrati, Phokion G. Kolaitis, Angelos Vasilakopoulos", "title": "Consistent Answers of Conjunctive Queries on Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the past decade, there has been an extensive investigation of the\ncomputational complexity of the consistent answers of Boolean conjunctive\nqueries under primary key constraints. Much of this investigation has focused\non self-join-free Boolean conjunctive queries. In this paper, we study the\nconsistent answers of Boolean conjunctive queries involving a single binary\nrelation, i.e., we consider arbitrary Boolean conjunctive queries on directed\ngraphs. In the presence of a single key constraint, we show that for each such\nBoolean conjunctive query, either the problem of computing its consistent\nanswers is expressible in first-order logic, or it is polynomial-time solvable,\nbut not expressible in first-order logic.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 18:14:20 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Afrati", "Foto N.", ""], ["Kolaitis", "Phokion G.", ""], ["Vasilakopoulos", "Angelos", ""]]}, {"id": "1503.00849", "submitter": "Sutanay Choudhury", "authors": "Sutanay Choudhury, Lawrence Holder, George Chin, Khushbu Agarwal, John\n  Feo", "title": "A Selectivity based approach to Continuous Pattern Detection in\n  Streaming Graphs", "comments": "in 18th International Conference on Extending Database Technology\n  (EDBT) (2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Cyber security is one of the most significant technical challenges in current\ntimes. Detecting adversarial activities, prevention of theft of intellectual\nproperties and customer data is a high priority for corporations and government\nagencies around the world. Cyber defenders need to analyze massive-scale,\nhigh-resolution network flows to identify, categorize, and mitigate attacks\ninvolving networks spanning institutional and national boundaries. Many of the\ncyber attacks can be described as subgraph patterns, with prominent examples\nbeing insider infiltrations (path queries), denial of service (parallel paths)\nand malicious spreads (tree queries). This motivates us to explore subgraph\nmatching on streaming graphs in a continuous setting. The novelty of our work\nlies in using the subgraph distributional statistics collected from the\nstreaming graph to determine the query processing strategy. We introduce a\n\"Lazy Search\" algorithm where the search strategy is decided on a\nvertex-to-vertex basis depending on the likelihood of a match in the vertex\nneighborhood. We also propose a metric named \"Relative Selectivity\" that is\nused to select between different query processing strategies. Our experiments\nperformed on real online news, network traffic stream and a synthetic social\nnetwork benchmark demonstrate 10-100x speedups over selectivity agnostic\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 08:11:27 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["Choudhury", "Sutanay", ""], ["Holder", "Lawrence", ""], ["Chin", "George", ""], ["Agarwal", "Khushbu", ""], ["Feo", "John", ""]]}, {"id": "1503.00900", "submitter": "Dr. Deepali Virmani", "authors": "Deepali Virmani, Shweta Taneja, Geetika Malhotra", "title": "Normalization based K means Clustering Algorithm", "comments": "5 pages, 4 figures in International Journal of Advanced Engineering\n  Research and Science (IJAERS)-Feb 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  K-means is an effective clustering technique used to separate similar data\ninto groups based on initial centroids of clusters. In this paper,\nNormalization based K-means clustering algorithm(N-K means) is proposed.\nProposed N-K means clustering algorithm applies normalization prior to\nclustering on the available data as well as the proposed approach calculates\ninitial centroids based on weights. Experimental results prove the betterment\nof proposed N-K means clustering algorithm over existing K-means clustering\nalgorithm in terms of complexity and overall performance.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 11:26:27 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["Virmani", "Deepali", ""], ["Taneja", "Shweta", ""], ["Malhotra", "Geetika", ""]]}, {"id": "1503.01143", "submitter": "John Meehan", "authors": "John Meehan, Nesime Tatbul, Stan Zdonik, Cansu Aslantas, Ugur\n  Cetintemel, Jiang Du, Tim Kraska, Samuel Madden, David Maier, Andrew Pavlo,\n  Michael Stonebraker, Kristin Tufte, Hao Wang", "title": "S-Store: Streaming Meets Transaction Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stream processing addresses the needs of real-time applications. Transaction\nprocessing addresses the coordination and safety of short atomic computations.\nHeretofore, these two modes of operation existed in separate, stove-piped\nsystems. In this work, we attempt to fuse the two computational paradigms in a\nsingle system called S-Store. In this way, S-Store can simultaneously\naccommodate OLTP and streaming applications. We present a simple transaction\nmodel for streams that integrates seamlessly with a traditional OLTP system. We\nchose to build S-Store as an extension of H-Store, an open-source, in-memory,\ndistributed OLTP database system. By implementing S-Store in this way, we can\nmake use of the transaction processing facilities that H-Store already\nsupports, and we can concentrate on the additional implementation features that\nare needed to support streaming. Similar implementations could be done using\nother main-memory OLTP platforms. We show that we can actually achieve higher\nthroughput for streaming workloads in S-Store than an equivalent deployment in\nH-Store alone. We also show how this can be achieved within H-Store with the\naddition of a modest amount of new functionality. Furthermore, we compare\nS-Store to two state-of-the-art streaming systems, Spark Streaming and Storm,\nand show how S-Store matches and sometimes exceeds their performance while\nproviding stronger transactional guarantees.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 21:56:17 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2015 17:51:27 GMT"}], "update_date": "2015-03-11", "authors_parsed": [["Meehan", "John", ""], ["Tatbul", "Nesime", ""], ["Zdonik", "Stan", ""], ["Aslantas", "Cansu", ""], ["Cetintemel", "Ugur", ""], ["Du", "Jiang", ""], ["Kraska", "Tim", ""], ["Madden", "Samuel", ""], ["Maier", "David", ""], ["Pavlo", "Andrew", ""], ["Stonebraker", "Michael", ""], ["Tufte", "Kristin", ""], ["Wang", "Hao", ""]]}, {"id": "1503.01707", "submitter": "Jan Van den Bussche", "authors": "Angela Bonifati, Werner Nutt, Riccardo Torlone, Jan Van den Bussche", "title": "Mapping-equivalence and oid-equivalence of single-function\n  object-creating conjunctive queries", "comments": "This revised version has been accepted on 11 January 2016 for\n  publication in The VLDB Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conjunctive database queries have been extended with a mechanism for object\ncreation to capture important applications such as data exchange, data\nintegration, and ontology-based data access. Object creation generates new\nobject identifiers in the result, that do not belong to the set of constants in\nthe source database. The new object identifiers can be also seen as Skolem\nterms. Hence, object-creating conjunctive queries can also be regarded as\nrestricted second-order tuple-generating dependencies (SO tgds), considered in\nthe data exchange literature.\n  In this paper, we focus on the class of single-function object-creating\nconjunctive queries, or sifo CQs for short. We give a new characterization for\noid-equivalence of sifo CQs that is simpler than the one given by Hull and\nYoshikawa and places the problem in the complexity class NP. Our\ncharacterization is based on Cohen's equivalence notions for conjunctive\nqueries with multiplicities. We also solve the logical entailment problem for\nsifo CQs, showing that also this problem belongs to NP. Results by Pichler et\nal. have shown that logical equivalence for more general classes of SO tgds is\neither undecidable or decidable with as yet unknown complexity upper bounds.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 17:47:04 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2016 14:59:40 GMT"}], "update_date": "2016-01-13", "authors_parsed": [["Bonifati", "Angela", ""], ["Nutt", "Werner", ""], ["Torlone", "Riccardo", ""], ["Bussche", "Jan Van den", ""]]}, {"id": "1503.01812", "submitter": "Vanessa Ayala-Rivera", "authors": "Vanessa Ayala-Rivera, Patrick McDonagh, Thomas Cerqueus, Liam Murphy", "title": "Ontology-Based Quality Evaluation of Value Generalization Hierarchies\n  for Data Anonymization", "comments": "18 pages, 7 figures, presented in the Privacy in Statistical\n  Databases Conference 2014 (Ibiza, Spain)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In privacy-preserving data publishing, approaches using Value Generalization\nHierarchies (VGHs) form an important class of anonymization algorithms. VGHs\nplay a key role in the utility of published datasets as they dictate how the\nanonymization of the data occurs. For categorical attributes, it is imperative\nto preserve the semantics of the original data in order to achieve a higher\nutility. Despite this, semantics have not being formally considered in the\nspecification of VGHs. Moreover, there are no methods that allow the users to\nassess the quality of their VGH. In this paper, we propose a measurement\nscheme, based on ontologies, to quantitatively evaluate the quality of VGHs, in\nterms of semantic consistency and taxonomic organization, with the aim of\nproducing higher-quality anonymizations. We demonstrate, through a case study,\nhow our evaluation scheme can be used to compare the quality of multiple VGHs\nand can help to identify faulty VGHs.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 22:58:19 GMT"}], "update_date": "2015-03-09", "authors_parsed": [["Ayala-Rivera", "Vanessa", ""], ["McDonagh", "Patrick", ""], ["Cerqueus", "Thomas", ""], ["Murphy", "Liam", ""]]}, {"id": "1503.02295", "submitter": "Emmanuel Osegi", "authors": "N.E. Osegi, P. Enyindah", "title": "GOptimaEmbed: A SmartSMS-SQLDatabaseManagementSystem for Low-Cost\n  Microcontrollers", "comments": "To be published", "journal-ref": "African Journal of Computing & ICT, 8(2)", "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The era of the Internet of things, machine-to-machine and human to machine\ncomputing has heralded the development of a modern-day smart industry in which\nhumanoids can co-operate,co-exist and interact seamlessly.Currently, there are\nmany projects in this area of smart communication and thus giving rise to an\nindustry electrified by smart things.In this paper we present a novel smart\ndatabase management system (dbms),GOptimaEmbed, for intelligent querying of\ndatabases in device constrained embedded systems. The system uses genetic\nalgorithms as main search engine and simplifies the query process using stored\nin-memory model based on an invented device dependent\nShort-messaging-Structured Query Language SMS SQL schema translator. In\naddition, querying is done over the air using integrated GSM module in the\nsmart space. The system has been applied to querying a plant database and\nresults were quite satisfactory.\n  Keywords. GOptimaEmbed,smart dbms, genetic algorithms, SMS SQL\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2015 16:46:03 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2015 21:41:29 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Osegi", "N. E.", ""], ["Enyindah", "P.", ""]]}, {"id": "1503.02368", "submitter": "Christopher Aberger", "authors": "Christopher R. Aberger, Susan Tu, Kunle Olukotun, and Christopher R\\'e", "title": "EmptyHeaded: A Relational Engine for Graph Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are two types of high-performance graph processing engines: low- and\nhigh-level engines. Low-level engines (Galois, PowerGraph, Snap) provide\noptimized data structures and computation models but require users to write\nlow-level imperative code, hence ensuring that efficiency is the burden of the\nuser. In high-level engines, users write in query languages like datalog\n(SociaLite) or SQL (Grail). High-level engines are easier to use but are orders\nof magnitude slower than the low-level graph engines. We present EmptyHeaded, a\nhigh-level engine that supports a rich datalog-like query language and achieves\nperformance comparable to that of low-level engines. At the core of\nEmptyHeaded's design is a new class of join algorithms that satisfy strong\ntheoretical guarantees but have thus far not achieved performance comparable to\nthat of specialized graph processing engines. To achieve high performance,\nEmptyHeaded introduces a new join engine architecture, including a novel query\noptimizer and data layouts that leverage single-instruction multiple data\n(SIMD) parallelism. With this architecture, EmptyHeaded outperforms high-level\napproaches by up to three orders of magnitude on graph pattern queries,\nPageRank, and Single-Source Shortest Paths (SSSP) and is an order of magnitude\nfaster than many low-level baselines. We validate that EmptyHeaded competes\nwith the best-of-breed low-level engine (Galois), achieving comparable\nperformance on PageRank and at most 3x worse performance on SSSP.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 04:02:36 GMT"}, {"version": "v2", "created": "Sun, 15 Mar 2015 23:15:28 GMT"}, {"version": "v3", "created": "Fri, 3 Apr 2015 02:32:32 GMT"}, {"version": "v4", "created": "Fri, 20 Nov 2015 19:49:10 GMT"}, {"version": "v5", "created": "Tue, 8 Mar 2016 01:55:09 GMT"}, {"version": "v6", "created": "Tue, 12 Apr 2016 19:32:53 GMT"}, {"version": "v7", "created": "Thu, 5 Jan 2017 18:54:06 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Aberger", "Christopher R.", ""], ["Tu", "Susan", ""], ["Olukotun", "Kunle", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1503.02781", "submitter": "Jonathan Tuke", "authors": "Matthew Roughan, Jonathan Tuke", "title": "Unravelling Graph-Exchange File Formats", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  A graph is used to represent data in which the relationships between the\nobjects in the data are at least as important as the objects themselves. Over\nthe last two decades nearly a hundred file formats have been proposed or used\nto provide portable access to such data. This paper seeks to review these\nformats, and provide some insight to both reduce the ongoing creation of\nunnecessary formats, and guide the development of new formats where needed.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 06:23:56 GMT"}], "update_date": "2015-06-29", "authors_parsed": [["Roughan", "Matthew", ""], ["Tuke", "Jonathan", ""]]}, {"id": "1503.02911", "submitter": "Maribel Acosta", "authors": "Maribel Acosta, Elena Simperl, Fabian Fl\\\"ock, Maria-Esther Vidal,\n  Rudi Studer", "title": "RDF-Hunter: Automatically Crowdsourcing the Execution of Queries Against\n  RDF Data Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last years, a large number of RDF data sets has become available on\nthe Web. However, due to the semi-structured nature of RDF data, missing values\naffect answer completeness of queries that are posed against this data. To\novercome this limitation, we propose RDF-Hunter, a novel hybrid query\nprocessing approach that brings together machine and human computation to\nexecute queries against RDF data. We develop a novel quality model and query\nengine in order to enable RDF-Hunter to on the fly decide which parts of a\nquery should be executed through conventional technology or crowd computing. To\nevaluate RDF-Hunter, we created a collection of 50 SPARQL queries against the\nDBpedia data set, executed them using our hybrid query engine, and analyzed the\naccuracy of the outcomes obtained from the crowd. The experiments clearly show\nthat the overall approach is feasible and produces query results that reliably\nand significantly enhance completeness of automatic query processing responses.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 14:02:20 GMT"}], "update_date": "2015-03-11", "authors_parsed": [["Acosta", "Maribel", ""], ["Simperl", "Elena", ""], ["Fl\u00f6ck", "Fabian", ""], ["Vidal", "Maria-Esther", ""], ["Studer", "Rudi", ""]]}, {"id": "1503.02940", "submitter": "Gabriela Montoya", "authors": "Gabriela Montoya, Hala Skaf-Molli, Pascal Molli, Maria-Esther Vidal", "title": "Efficient Query Processing for SPARQL Federations with Replicated\n  Fragments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low reliability and availability of public SPARQL endpoints prevent\nreal-world applications from exploiting all the potential of these querying\ninfras-tructures. Fragmenting data on servers can improve data availability but\ndegrades performance. Replicating fragments can offer new tradeoff between\nperformance and availability. We propose FEDRA, a framework for querying Linked\nData that takes advantage of client-side data replication, and performs a\nsource selection algorithm that aims to reduce the number of selected public\nSPARQL endpoints, execution time, and intermediate results. FEDRA has been\nimplemented on the state-of-the-art query engines ANAPSID and FedX, and\nempirically evaluated on a variety of real-world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 14:57:26 GMT"}], "update_date": "2015-03-11", "authors_parsed": [["Montoya", "Gabriela", ""], ["Skaf-Molli", "Hala", ""], ["Molli", "Pascal", ""], ["Vidal", "Maria-Esther", ""]]}, {"id": "1503.03208", "submitter": "Massoud Vadoodparast Mr", "authors": "M.Vadoodparast, A. Razak Hamdan, Hafiz", "title": "Fraudulent Electronic transaction detection using KDA Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering analysis and Datamining methodologies were applied to the problem\nof identifying illegal and fraud transactions. The researchers independently\ndeveloped model and software using data provided by a bank and using Rapidminer\nmodeling tool. The research objectives are to propose dynamic model and\nmechanism to cover fraud detection system limitations. KDA model as proposed\nmodel can detect 68.75% of fraudulent transactions with online dynamic modeling\nand 81.25% in offline mode and the Fraud Detection System & Decision Support\nSystem. Software propose a good supporting procedure to detect fraudulent\ntransaction dynamically.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 08:13:51 GMT"}], "update_date": "2015-03-12", "authors_parsed": [["Vadoodparast", "M.", ""], ["Hamdan", "A. Razak", ""], ["Hafiz", "", ""]]}, {"id": "1503.03571", "submitter": "Ryan Wisnesky", "authors": "Patrick Schultz, Ryan Wisnesky", "title": "Algebraic Data Integration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop an algebraic approach to data integration by\ncombining techniques from functional programming, category theory, and database\ntheory. In our formalism, database schemas and instances are algebraic\n(multi-sorted equational) theories of a certain form. Schemas denote\ncategories, and instances denote their initial (term) algebras. The instances\non a schema S form a category, S-Inst, and a morphism of schemas F : S -> T\ninduces three adjoint data migration functors: Sigma_F : S-Inst -> T-Inst,\ndefined by substitution along F, which has a right adjoint Delta_F : T-Inst ->\nS-Inst, which in turn has a right adjoint Pi_F : S-Inst -> T-Inst. We present a\nquery language based on for/where/return syntax where each query denotes a\nsequence of data migration functors; a pushout-based design pattern for\nperforming data integration using our formalism; and describe the\nimplementation of our formalism in a tool we call AQL.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 03:10:25 GMT"}, {"version": "v2", "created": "Tue, 26 Apr 2016 03:44:57 GMT"}, {"version": "v3", "created": "Thu, 28 Apr 2016 02:35:28 GMT"}, {"version": "v4", "created": "Fri, 30 Dec 2016 22:41:06 GMT"}, {"version": "v5", "created": "Mon, 19 Jun 2017 23:21:34 GMT"}, {"version": "v6", "created": "Fri, 29 Sep 2017 03:32:06 GMT"}, {"version": "v7", "created": "Tue, 12 Dec 2017 05:13:53 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Schultz", "Patrick", ""], ["Wisnesky", "Ryan", ""]]}, {"id": "1503.03642", "submitter": "Chang Yao", "authors": "Chang Yao, Divyakant Agrawal, Pengfei Chang, Gang Chen, Beng Chin Ooi,\n  Weng-Fai Wong, Meihui Zhang", "title": "DGCC:A New Dependency Graph based Concurrency Control Protocol for\n  Multicore Database Systems", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multicore CPUs and large memories are increasingly becoming the norm in\nmodern computer systems. However, current database management systems (DBMSs)\nare generally ineffective in exploiting the parallelism of such systems. In\nparticular, contention can lead to a dramatic fall in performance. In this\npaper, we propose a new concurrency control protocol called DGCC (Dependency\nGraph based Concurrency Control) that separates concurrency control from\nexecution. DGCC builds dependency graphs for batched transactions before\nexecuting them. Using these graphs, contentions within the same batch of\ntransactions are resolved before execution. As a result, the execution of the\ntransactions does not need to deal with contention while maintaining full\nequivalence to that of serialized execution. This better exploits multicore\nhardware and achieves higher level of parallelism. To facilitate DGCC, we have\nalso proposed a system architecture that does not have certain centralized\ncontrol components yielding better scalability, as well as supports a more\nefficient recovery mechanism. Our extensive experimental study shows that DGCC\nachieves up to four times higher throughput compared to that of\nstate-of-the-art concurrency control protocols for high contention workloads.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 09:32:02 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Yao", "Chang", ""], ["Agrawal", "Divyakant", ""], ["Chang", "Pengfei", ""], ["Chen", "Gang", ""], ["Ooi", "Beng Chin", ""], ["Wong", "Weng-Fai", ""], ["Zhang", "Meihui", ""]]}, {"id": "1503.03650", "submitter": "Weiqing Wang", "authors": "Weiqing Wang, Hongzhi Yin, Ling Chen, Yizhou Sun, Shazia Sadiq,\n  Xiaofang Zhou", "title": "Geo-SAGE: A Geographical Sparse Additive Generative Model for Spatial\n  Item Recommendation", "comments": "10 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of location-based social networks (LBSNs), spatial\nitem recommendation has become an important means to help people discover\nattractive and interesting venues and events, especially when users travel out\nof town. However, this recommendation is very challenging compared to the\ntraditional recommender systems. A user can visit only a limited number of\nspatial items, leading to a very sparse user-item matrix. Most of the items\nvisited by a user are located within a short distance from where he/she lives,\nwhich makes it hard to recommend items when the user travels to a far away\nplace. Moreover, user interests and behavior patterns may vary dramatically\nacross different geographical regions. In light of this, we propose Geo-SAGE, a\ngeographical sparse additive generative model for spatial item recommendation\nin this paper. Geo-SAGE considers both user personal interests and the\npreference of the crowd in the target region, by exploiting both the\nco-occurrence pattern of spatial items and the content of spatial items. To\nfurther alleviate the data sparsity issue, Geo-SAGE exploits the geographical\ncorrelation by smoothing the crowd's preferences over a well-designed spatial\nindex structure called spatial pyramid. We conduct extensive experiments to\nevaluate the performance of our Geo-SAGE model on two real large-scale\ndatasets. The experimental results clearly demonstrate our Geo-SAGE model\noutperforms the state-of-the-art in the two tasks of both out-of-town and\nhome-town recommendations.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 09:44:11 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Wang", "Weiqing", ""], ["Yin", "Hongzhi", ""], ["Chen", "Ling", ""], ["Sun", "Yizhou", ""], ["Sadiq", "Shazia", ""], ["Zhou", "Xiaofang", ""]]}, {"id": "1503.03653", "submitter": "Chang Yao", "authors": "Chang Yao, Divyakant Agrawal, Gang Chen, Beng Chin Ooi, Sai Wu", "title": "Adaptive Logging for Distributed In-memory Databases", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new type of logs, the command log, is being employed to replace the\ntraditional data log (e.g., ARIES log) in the in-memory databases. Instead of\nrecording how the tuples are updated, a command log only tracks the\ntransactions being executed, thereby effectively reducing the size of the log\nand improving the performance. Command logging on the other hand increases the\ncost of recovery, because all the transactions in the log after the last\ncheckpoint must be completely redone in case of a failure. In this paper, we\nfirst extend the command logging technique to a distributed environment, where\nall the nodes can perform recovery in parallel. We then propose an adaptive\nlogging approach by combining data logging and command logging. The percentage\nof data logging versus command logging becomes an optimization between the\nperformance of transaction processing and recovery to suit different OLTP\napplications. Our experimental study compares the performance of our proposed\nadaptive logging, ARIES-style data logging and command logging on top of\nH-Store. The results show that adaptive logging can achieve a 10x boost for\nrecovery and a transaction throughput that is comparable to that of command\nlogging.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 09:53:17 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2015 05:30:24 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Yao", "Chang", ""], ["Agrawal", "Divyakant", ""], ["Chen", "Gang", ""], ["Ooi", "Beng Chin", ""], ["Wu", "Sai", ""]]}, {"id": "1503.03753", "submitter": "Senjuti Basu Roy", "authors": "Senjuti Basu Roy, Laks V. S. Lakshmanan, Rui Liu", "title": "From Group Recommendations to Group Formation", "comments": "14 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been significant recent interest in the area of group\nrecommendations, where, given groups of users of a recommender system, one\nwants to recommend top-k items to a group that maximize the satisfaction of the\ngroup members, according to a chosen semantics of group satisfaction. Examples\nsemantics of satisfaction of a recommended itemset to a group include the\nso-called least misery (LM) and aggregate voting (AV). We consider the\ncomplementary problem of how to form groups such that the users in the formed\ngroups are most satisfied with the suggested top-k recommendations. We assume\nthat the recommendations will be generated according to one of the two group\nrecommendation semantics - LM or AV. Rather than assuming groups are given, or\nrely on ad hoc group formation dynamics, our framework allows a strategic\napproach for forming groups of users in order to maximize satisfaction. We show\nthat the problem is NP-hard to solve optimally under both semantics.\nFurthermore, we develop two efficient algorithms for group formation under LM\nand show that they achieve bounded absolute error. We develop efficient\nheuristic algorithms for group formation under AV. We validate our results and\ndemonstrate the scalability and effectiveness of our group formation algorithms\non two large real data sets.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 17:34:43 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Roy", "Senjuti Basu", ""], ["Lakshmanan", "Laks V. S.", ""], ["Liu", "Rui", ""]]}, {"id": "1503.04169", "submitter": "Dung Nguyen", "authors": "Dung Nguyen, Molham Aref, Martin Bravenboer, George Kollias, Hung Q.\n  Ngo, Christopher R\\'e, Atri Rudra", "title": "Join Processing for Graph Patterns: An Old Dog with New Tricks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Join optimization has been dominated by Selinger-style, pairwise optimizers\nfor decades. But, Selinger-style algorithms are asymptotically suboptimal for\napplications in graphic analytics. This suboptimality is one of the reasons\nthat many have advocated supplementing relational engines with specialized\ngraph processing engines. Recently, new join algorithms have been discovered\nthat achieve optimal worst-case run times for any join or even so-called beyond\nworst-case (or instance optimal) run time guarantees for specialized classes of\njoins. These new algorithms match or improve on those used in specialized\ngraph-processing systems. This paper asks can these new join algorithms allow\nrelational engines to close the performance gap with graph engines?\n  We examine this question for graph-pattern queries or join queries. We find\nthat classical relational databases like Postgres and MonetDB or newer graph\ndatabases/stores like Virtuoso and Neo4j may be orders of magnitude slower than\nthese new approaches compared to a fully featured RDBMS, LogicBlox, using these\nnew ideas. Our results demonstrate that an RDBMS with such new algorithms can\nperform as well as specialized engines like GraphLab -- while retaining a\nhigh-level interface. We hope this adds to the ongoing debate of the role of\ngraph accelerators, new graph systems, and relational systems in modern\nworkloads.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2015 17:53:48 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2015 03:12:37 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Nguyen", "Dung", ""], ["Aref", "Molham", ""], ["Bravenboer", "Martin", ""], ["Kollias", "George", ""], ["Ngo", "Hung Q.", ""], ["R\u00e9", "Christopher", ""], ["Rudra", "Atri", ""]]}, {"id": "1503.04344", "submitter": "Safia  Abbas", "authors": "Safia Abbas", "title": "Deposit subscribe Prediction using Data Mining Techniques based Real\n  Marketing Dataset", "comments": null, "journal-ref": null, "doi": "10.5120/19293-0725", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, economic depression, which scoured all over the world, affects\nbusiness organizations and banking sectors. Such economic pose causes a severe\nattrition for banks and customer retention becomes impossible. Accordingly,\nmarketing managers are in need to increase marketing campaigns, whereas\norganizations evade both expenses and business expansion. In order to solve\nsuch riddle, data mining techniques is used as an uttermost factor in data\nanalysis, data summarizations, hidden pattern discovery, and data\ninterpretation. In this paper, rough set theory and decision tree mining\ntechniques have been implemented, using a real marketing data obtained from\nPortuguese marketing campaign related to bank deposit subscription [Moro et\nal., 2011]. The paper aims to improve the efficiency of the marketing campaigns\nand helping the decision makers by reducing the number of features, that\ndescribes the dataset and spotting on the most significant ones, and predict\nthe deposit customer retention criteria based on potential predictive rules.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2015 20:23:14 GMT"}], "update_date": "2015-06-24", "authors_parsed": [["Abbas", "Safia", ""]]}, {"id": "1503.04385", "submitter": "Kisor Ray", "authors": "Kisor Ray", "title": "Design and Implementation of Database Independent Auto Sequence Numbers", "comments": "03 pages, 02 figures", "journal-ref": "International Journal of Computer Trends and Technology,Volume-20\n  Number-2,2015", "doi": "10.14445/22312803/IJCTT-V20P111", "report-no": null, "categories": "cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Developers across the world use autonumber or auto sequences field of the\nbackend databases for developing both the desktop and web based data centric\napplications which is easier to use at the development and deployment purpose\nbut can create a lot of problems under varied situations. This paper examines\nhow a database independent autonumber could be developed and reused solving all\nthe problems as well as providing the same degree of easy to use features of\nautonumber offered by modern Relational Database Systems.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2015 04:09:26 GMT"}], "update_date": "2015-04-01", "authors_parsed": [["Ray", "Kisor", ""]]}, {"id": "1503.04831", "submitter": "Olaf Hartig", "authors": "Olaf Hartig and Giuseppe Pirro", "title": "A Context-Based Semantics for SPARQL Property Paths over the Web\n  (Extended Version)", "comments": "29 pages, Extended version of a paper published in ESWC 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As of today, there exists no standard language for querying Linked Data on\nthe Web, where navigation across distributed data sources is a key feature. A\nnatural candidate seems to be SPARQL, which recently has been enhanced with\nnavigational capabilities thanks to the introduction of property paths (PPs).\nHowever, the semantics of SPARQL restricts the scope of navigation via PPs to\nsingle RDF graphs. This restriction limits the applicability of PPs on the Web.\nTo fill this gap, in this paper we provide formal foundations for evaluating\nPPs on the Web, thus contributing to the definition of a query language for\nLinked Data. In particular, we introduce a query semantics for PPs that couples\nnavigation at the data level with navigation on the Web graph. Given this\nsemantics we find that for some PP-based SPARQL queries a complete evaluation\non the Web is not feasible. To enable systems to identify queries that can be\nevaluated completely, we establish a decidable syntactic property of such\nqueries.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2015 20:10:16 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Hartig", "Olaf", ""], ["Pirro", "Giuseppe", ""]]}, {"id": "1503.04864", "submitter": "Fay\\c{c}al Hamdi", "authors": "Fay\\c{c}al Hamdi, Nathalie Abadie, B\\'en\\'edicte Bucher and\n  Abdelfettah Feliachi", "title": "GeomRDF: A Geodata Converter with a Fine-Grained Structured\n  Representation of Geometry in the Web", "comments": "12 pages, 2 figures, the 1st International Workshop on Geospatial\n  Linked Data (GeoLD 2014) - SEMANTiCS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, with the advent of the web of data, a growing number of\nnational mapping agencies tend to publish their geospatial data as Linked Data.\nHowever, differences between traditional GIS data models and Linked Data model\ncan make the publication process more complicated. Besides, it may require, to\nbe done, the setting of several parameters and some expertise in the semantic\nweb technologies. In addition, the use of standards like GeoSPARQL (or ad hoc\npredicates) is mandatory to perform spatial queries on published geospatial\ndata. In this paper, we present GeomRDF, a tool that helps users to convert\nspatial data from traditional GIS formats to RDF model easily. It generates\ngeometries represented as GeoSPARQL WKT literal but also as structured\ngeometries that can be exploited by using only the RDF query language, SPARQL.\nGeomRDF was implemented as a module in the RDF publication platform Datalift. A\nvalidation of GeomRDF has been realized against the French administrative units\ndataset (provided by IGN France).\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2015 21:35:18 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Hamdi", "Fay\u00e7al", ""], ["Abadie", "Nathalie", ""], ["Bucher", "B\u00e9n\u00e9dicte", ""], ["Feliachi", "Abdelfettah", ""]]}, {"id": "1503.04957", "submitter": "Andrea Burattin", "authors": "Andrea Burattin, Fabrizio Maria Maggi, Alessandro Sperduti", "title": "Conformance Checking Based on Multi-Perspective Declarative Process\n  Models", "comments": null, "journal-ref": null, "doi": "10.1016/j.eswa.2016.08.040", "report-no": null, "categories": "cs.SE cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process mining is a family of techniques that aim at analyzing business\nprocess execution data recorded in event logs. Conformance checking is a branch\nof this discipline embracing approaches for verifying whether the behavior of a\nprocess, as recorded in a log, is in line with some expected behaviors provided\nin the form of a process model. The majority of these approaches require the\ninput process model to be procedural (e.g., a Petri net). However, in turbulent\nenvironments, characterized by high variability, the process behavior is less\nstable and predictable. In these environments, procedural process models are\nless suitable to describe a business process. Declarative specifications,\nworking in an open world assumption, allow the modeler to express several\npossible execution paths as a compact set of constraints. Any process execution\nthat does not contradict these constraints is allowed. One of the open\nchallenges in the context of conformance checking with declarative models is\nthe capability of supporting multi-perspective specifications. In this paper,\nwe close this gap by providing a framework for conformance checking based on\nMP-Declare, a multi-perspective version of the declarative process modeling\nlanguage Declare. The approach has been implemented in the process mining tool\nProM and has been experimented in three real life case studies.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2015 09:09:30 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Burattin", "Andrea", ""], ["Maggi", "Fabrizio Maria", ""], ["Sperduti", "Alessandro", ""]]}, {"id": "1503.05157", "submitter": "Jeremy Debattista", "authors": "Jeremy Debattista, Santiago Londo\\~no, Christoph Lange, S\\\"oren Auer", "title": "Quality Assessment of Linked Datasets using Probabilistic Approximation", "comments": "15 pages, 2 figures, To appear in ESWC 2015 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing application of Linked Open Data, assessing the quality of\ndatasets by computing quality metrics becomes an issue of crucial importance.\nFor large and evolving datasets, an exact, deterministic computation of the\nquality metrics is too time consuming or expensive. We employ probabilistic\ntechniques such as Reservoir Sampling, Bloom Filters and Clustering Coefficient\nestimation for implementing a broad set of data quality metrics in an\napproximate but sufficiently accurate way. Our implementation is integrated in\nthe comprehensive data quality assessment framework Luzzu. We evaluated its\nperformance and accuracy on Linked Open Datasets of broad relevance.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2015 18:39:22 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Debattista", "Jeremy", ""], ["Londo\u00f1o", "Santiago", ""], ["Lange", "Christoph", ""], ["Auer", "S\u00f6ren", ""]]}, {"id": "1503.05294", "submitter": "Kisor Ray", "authors": "Kisor Ray, Sourav Bag, Saumen Sarkar", "title": "Easy and Fast Design and Implementation of PostgreSQL based image\n  handling application", "comments": "05 pages, 04 figures, 02 tables, International Journal of Advanced\n  Research in Computer Science and Software Engineering, Volume 5, Issue 2,\n  February 2015, ISSN 2277 128X", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modern computing, RDBMS are great to store different types of data. To a\ndeveloper, one of the major objectives is to provide a very low cost and easy\nto use solution to an existing problem. While commercial databases are more\neasy to use along with their new as well as documented features come with\ncomplicated licensing cost, free open source databases are not that\nstraightforward under many situations. This paper shows how a completely free\nadvanced open source RDBMS like PostgreSQL could be designed and modified to\nstore and retrieve high quality images in order to use them along with a\nfrontend application.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2015 07:23:23 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Ray", "Kisor", ""], ["Bag", "Sourav", ""], ["Sarkar", "Saumen", ""]]}, {"id": "1503.05656", "submitter": "Yodsawalai Chodpathumwan", "authors": "Ali Vakilian, Yodsawalai Chodpathumwan, Arash Termehchy and Amir\n  Nayyeri", "title": "Cost-Effective Conceptual Design Using Taxonomies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that annotating named entities in unstructured and\nsemi-structured data sets by their concepts improves the effectiveness of\nanswering queries over these data sets. As every enterprise has a limited\nbudget of time or computational resources, it has to annotate a subset of\nconcepts in a given domain whose costs of annotation do not exceed the budget.\nWe call such a subset of concepts a {\\it conceptual design} for the annotated\ndata set. We focus on finding a conceptual design that provides the most\neffective answers to queries over the annotated data set, i.e., a {\\it\ncost-effective conceptual design}. Since, it is often less time-consuming and\ncostly to annotate general concepts than specific concepts, we use information\non superclass/subclass relationships between concepts in taxonomies to find a\ncost-effective conceptual design. We quantify the amount by which a conceptual\ndesign with concepts from a taxonomy improves the effectiveness of answering\nqueries over an annotated data set. If the taxonomy is a tree, we prove that\nthe problem is NP-hard and propose an efficient approximation and\npseudo-polynomial time algorithms for the problem. We further prove that if the\ntaxonomy is a directed acyclic graph, given some generally accepted hypothesis,\nit is not possible to find any approximation algorithm with reasonably small\napproximation ratio for the problem. Our empirical study using real-world data\nsets, taxonomies, and query workloads shows that our framework effectively\nquantifies the amount by which a conceptual design improves the effectiveness\nof answering queries. It also indicates that our algorithms are efficient for a\ndesign-time task with pseudo-polynomial algorithm being generally more\neffective than the approximation algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 06:23:47 GMT"}, {"version": "v2", "created": "Sat, 6 Jan 2018 23:24:31 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Vakilian", "Ali", ""], ["Chodpathumwan", "Yodsawalai", ""], ["Termehchy", "Arash", ""], ["Nayyeri", "Amir", ""]]}, {"id": "1503.06060", "submitter": "Dominique Gay", "authors": "Romain Guigour\\`es, Dominique Gay, Marc Boull\\'e, Fabrice Cl\\'erot,\n  Fabrice Rossi", "title": "Country-scale Exploratory Analysis of Call Detail Records through the\n  Lens of Data Grid Models", "comments": "Submitted to Industrial Track of ECML/PKDD 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Call Detail Records (CDRs) are data recorded by telecommunications companies,\nconsisting of basic informations related to several dimensions of the calls\nmade through the network: the source, destination, date and time of calls. CDRs\ndata analysis has received much attention in the recent years since it might\nreveal valuable information about human behavior. It has shown high added value\nin many application domains like e.g., communities analysis or network\nplanning. In this paper, we suggest a generic methodology for summarizing\ninformation contained in CDRs data. The method is based on a parameter-free\nestimation of the joint distribution of the variables that describe the calls.\nWe also suggest several well-founded criteria that allows one to browse the\nsummary at various granularities and to explore the summary by means of\ninsightful visualizations. The method handles network graph data, temporal\nsequence data as well as user mobility data stemming from original CDRs data.\nWe show the relevance of our methodology for various case studies on real-world\nCDRs data from Ivory Coast.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2015 13:13:42 GMT"}], "update_date": "2015-03-23", "authors_parsed": [["Guigour\u00e8s", "Romain", ""], ["Gay", "Dominique", ""], ["Boull\u00e9", "Marc", ""], ["Cl\u00e9rot", "Fabrice", ""], ["Rossi", "Fabrice", ""]]}, {"id": "1503.06483", "submitter": "Kamran Kowsari", "authors": "Kamran Kowsari, Maryam Yammahi, Nima Bari, Roman Vichr, Faisal Alsaby,\n  Simon Y. Berkovich", "title": "Construction of FuzzyFind Dictionary using Golay Coding Transformation\n  for Searching Applications", "comments": null, "journal-ref": null, "doi": "10.14569/IJACSA.2015.060313", "report-no": null, "categories": "cs.DB cs.AI cs.DS cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Searching through a large volume of data is very critical for companies,\nscientists, and searching engines applications due to time complexity and\nmemory complexity. In this paper, a new technique of generating FuzzyFind\nDictionary for text mining was introduced. We simply mapped the 23 bits of the\nEnglish alphabet into a FuzzyFind Dictionary or more than 23 bits by using more\nFuzzyFind Dictionary, and reflecting the presence or absence of particular\nletters. This representation preserves closeness of word distortions in terms\nof closeness of the created binary vectors within Hamming distance of 2\ndeviations. This paper talks about the Golay Coding Transformation Hash Table\nand how it can be used on a FuzzyFind Dictionary as a new technology for using\nin searching through big data. This method is introduced by linear time\ncomplexity for generating the dictionary and constant time complexity to access\nthe data and update by new data sets, also updating for new data sets is linear\ntime depends on new data points. This technique is based on searching only for\nletters of English that each segment has 23 bits, and also we have more than\n23-bit and also it could work with more segments as reference table.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2015 21:46:12 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Kowsari", "Kamran", ""], ["Yammahi", "Maryam", ""], ["Bari", "Nima", ""], ["Vichr", "Roman", ""], ["Alsaby", "Faisal", ""], ["Berkovich", "Simon Y.", ""]]}, {"id": "1503.06548", "submitter": "Debajyoti Mukhopadhyay Prof.", "authors": "Sumitkumar Kanoje, Varsha Powar, Debajyoti Mukhopadhyay", "title": "Using MongoDB for Social Networking Website", "comments": "3 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media is a biggest successful buzzword used in the recent time. Its\nsuccess opened various opportunities for the developers. Developing any\napplication requires storage of large data into databases. Many databases are\navailable for the developers, Choosing the right one make development easier.\nMongoDB is a cross platform document oriented, schema-less database eschewed\nthe traditional table based relational database structure in favor of JSON like\ndocuments. This article discusses various pros and cons encountered with the\nuse of the MongoDB so that developers would be helped while choosing it wisely.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2015 08:15:46 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Kanoje", "Sumitkumar", ""], ["Powar", "Varsha", ""], ["Mukhopadhyay", "Debajyoti", ""]]}, {"id": "1503.07241", "submitter": "Narayanan Sundaram", "authors": "Narayanan Sundaram, Nadathur Rajagopalan Satish, Md Mostofa Ali\n  Patwary, Subramanya R Dulloor, Satya Gautam Vadlamudi, Dipankar Das and\n  Pradeep Dubey", "title": "GraphMat: High performance graph analytics made productive", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the growing importance of large-scale graph analytics, there is a need\nto improve the performance of graph analysis frameworks without compromising on\nproductivity. GraphMat is our solution to bridge this gap between a\nuser-friendly graph analytics framework and native, hand-optimized code.\nGraphMat functions by taking vertex programs and mapping them to high\nperformance sparse matrix operations in the backend. We get the productivity\nbenefits of a vertex programming framework without sacrificing performance.\nGraphMat is in C++, and we have been able to write a diverse set of graph\nalgorithms in this framework with the same effort compared to other vertex\nprogramming frameworks. GraphMat performs 1.2-7X faster than high performance\nframeworks such as GraphLab, CombBLAS and Galois. It achieves better multicore\nscalability (13-15X on 24 cores) than other frameworks and is 1.2X off native,\nhand-optimized code on a variety of different graph algorithms. Since GraphMat\nperformance depends mainly on a few scalable and well-understood sparse matrix\noperations, GraphMatcan naturally benefit from the trend of increasing\nparallelism on future hardware.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2015 00:10:50 GMT"}], "update_date": "2015-03-26", "authors_parsed": [["Sundaram", "Narayanan", ""], ["Satish", "Nadathur Rajagopalan", ""], ["Patwary", "Md Mostofa Ali", ""], ["Dulloor", "Subramanya R", ""], ["Vadlamudi", "Satya Gautam", ""], ["Das", "Dipankar", ""], ["Dubey", "Pradeep", ""]]}, {"id": "1503.07759", "submitter": "Edvard Pedersen", "authors": "Edvard Pedersen and Lars Ailo Bongo", "title": "Large-scale Biological Meta-database Management", "comments": "10 pages, 6 figures, 4 tables", "journal-ref": null, "doi": "10.1016/j.future.2016.02.010", "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Up-to-date meta-databases are vital for the analysis of biological data.\nHowever,the current exponential increase in biological data leads to\nexponentially increasing meta-database sizes. Large-scale meta-database\nmanagement is therefore an important challenge for production platforms\nproviding services for biological data analysis. In particular, there is often\na need either to run an analysis with a particular version of a meta-database,\nor to rerun an analysis with an updated meta-database. We present our GeStore\napproach for biological meta-database management. It provides efficient storage\nand runtime generation of specific meta-database versions, and efficient\nincremental updates for biological data analysis tools. The approach is\ntransparent to the tools, and we provide a framework that makes it easy to\nintegrate GeStore with biological data analysis frameworks. We present the\nGeStore system, an evaluation of the performance characteristics of the system,\nand an evaluation of the benefits for a biological data analysis workflow.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2015 15:07:16 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2015 15:16:44 GMT"}, {"version": "v3", "created": "Mon, 22 Feb 2016 11:43:18 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Pedersen", "Edvard", ""], ["Bongo", "Lars Ailo", ""]]}, {"id": "1503.07881", "submitter": "Yonathan Perez", "authors": "Yonathan Perez, Rok Sosic, Arijit Banerjee, Rohan Puttagunta, Martin\n  Raison, Pararth Shah, Jure Leskovec", "title": "Ringo: Interactive Graph Analytics on Big-Memory Machines", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": "10.1145/2723372.2735369", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Ringo, a system for analysis of large graphs. Graphs provide a way\nto represent and analyze systems of interacting objects (people, proteins,\nwebpages) with edges between the objects denoting interactions (friendships,\nphysical interactions, links). Mining graphs provides valuable insights about\nindividual objects as well as the relationships among them.\n  In building Ringo, we take advantage of the fact that machines with large\nmemory and many cores are widely available and also relatively affordable. This\nallows us to build an easy-to-use interactive high-performance graph analytics\nsystem. Graphs also need to be built from input data, which often resides in\nthe form of relational tables. Thus, Ringo provides rich functionality for\nmanipulating raw input data tables into various kinds of graphs. Furthermore,\nRingo also provides over 200 graph analytics functions that can then be applied\nto constructed graphs.\n  We show that a single big-memory machine provides a very attractive platform\nfor performing analytics on all but the largest graphs as it offers excellent\nperformance and ease of use as compared to alternative approaches. With Ringo,\nwe also demonstrate how to integrate graph analytics with an iterative process\nof trial-and-error data exploration and rapid experimentation, common in data\nmining workloads.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2015 20:01:55 GMT"}], "update_date": "2015-03-30", "authors_parsed": [["Perez", "Yonathan", ""], ["Sosic", "Rok", ""], ["Banerjee", "Arijit", ""], ["Puttagunta", "Rohan", ""], ["Raison", "Martin", ""], ["Shah", "Pararth", ""], ["Leskovec", "Jure", ""]]}, {"id": "1503.08115", "submitter": "Francesco Pagano", "authors": "Francesco Pagano", "title": "A Distributed Approach to Privacy on the Cloud", "comments": "PhD Thesis in Computer Science at University of Milan - Italy 2012\n  March 6th", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing adoption of Cloud-based data processing and storage poses a\nnumber of privacy issues. Users wish to preserve full control over their\nsensitive data and cannot accept it to be fully accessible to an external\nstorage provider. Previous research in this area was mostly addressed at\ntechniques to protect data stored on untrusted database servers; however, I\nargue that the Cloud architecture presents a number of specific problems and\nissues. This dissertation contains a detailed analysis of open issues. To\nhandle them, I present a novel approach where confidential data is stored in a\nhighly distributed partitioned database, partly located on the Cloud and partly\non the clients. In my approach, data can be either private or shared; the\nlatter is shared in a secure manner by means of simple grant-and-revoke\npermissions. I have developed a proof-of-concept implementation using an\nin-memory RDBMS with row-level data encryption in order to achieve fine-grained\ndata access control. This type of approach is rarely adopted in conventional\noutsourced RDBMSs because it requires several complex steps. Benchmarks of my\nproofof-concept implementation show that my approach overcomes most of the\nproblems.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2015 15:58:31 GMT"}], "update_date": "2015-03-30", "authors_parsed": [["Pagano", "Francesco", ""]]}, {"id": "1503.08400", "submitter": "Zimu Yuan", "authors": "Zimu Yuan, Shusheng Guo", "title": "Online Query Scheduling on Source Permutation for Big Data Integration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data integration could involve a large number of sources with\nunpredictable redundancy information between them. The approach of building a\ncentral warehousing to integrate big data from all sources then becomes\ninfeasible because of so large number of sources and continuous updates\nhappening. A practical approach is to apply online query scheduling that\ninquires data from sources at runtime upon receiving a query. In this paper, we\naddress the Time-Cost Minimization Problem for online query scheduling, and\ntackle the challenges of source permutation and statistics estimation to\nminimize the time cost of retrieving answers for the real-time receiving query.\nWe propose the online scheduling strategy that enables the improvement of\nstatistics, the construction of source permutation and the execution of query\nworking in parallel. Experimental results show high efficiency and scalability\nof our scheduling strategy.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2015 09:00:20 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Yuan", "Zimu", ""], ["Guo", "Shusheng", ""]]}, {"id": "1503.08407", "submitter": "Zimu Yuan", "authors": "Zimu Yuan, Zhiwei Xu", "title": "CIUV: Collaborating Information Against Unreliable Views", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real world applications, the information of an object can be obtained\nfrom multiple sources. The sources may provide different point of views based\non their own origin. As a consequence, conflicting pieces of information are\ninevitable, which gives rise to a crucial problem: how to find the truth from\nthese conflicts. Many truth-finding methods have been proposed to resolve\nconflicts based on information trustworthy (i.e. more appearance means more\ntrustworthy) as well as source reliability. However, the factor of men's\ninvolvement, i.e., information may be falsified by men with malicious\nintension, is more or less ignored in existing methods. Collaborating the\npossible relationship between information's origins and men's participation are\nstill not studied in research. To deal with this challenge, we propose a method\n-- Collaborating Information against Unreliable Views (CIUV) --- in dealing\nwith men's involvement for finding the truth. CIUV contains 3 stages for\ninteractively mitigating the impact of unreliable views, and calculate the\ntruth by weighting possible biases between sources. We theoretically analyze\nthe error bound of CIUV, and conduct intensive experiments on real dataset for\nevaluation. The experimental results show that CIUV is feasible and has the\nsmallest error compared with other methods.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2015 09:30:58 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Yuan", "Zimu", ""], ["Xu", "Zhiwei", ""]]}, {"id": "1503.08482", "submitter": "Spyros Blanas", "authors": "Spyros Blanas and Surendra Byna", "title": "Towards Exascale Scientific Metadata Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in technology and computing hardware are enabling scientists from\nall areas of science to produce massive amounts of data using large-scale\nsimulations or observational facilities. In this era of data deluge, effective\ncoordination between the data production and the analysis phases hinges on the\navailability of metadata that describe the scientific datasets. Existing\nworkflow engines have been capturing a limited form of metadata to provide\nprovenance information about the identity and lineage of the data. However,\nmuch of the data produced by simulations, experiments, and analyses still need\nto be annotated manually in an ad hoc manner by domain scientists. Systematic\nand transparent acquisition of rich metadata becomes a crucial prerequisite to\nsustain and accelerate the pace of scientific innovation. Yet, ubiquitous and\ndomain-agnostic metadata management infrastructure that can meet the demands of\nextreme-scale science is notable by its absence.\n  To address this gap in scientific data management research and practice, we\npresent our vision for an integrated approach that (1) automatically captures\nand manipulates information-rich metadata while the data is being produced or\nanalyzed and (2) stores metadata within each dataset to permeate\nmetadata-oblivious processes and to query metadata through established and\nstandardized data access interfaces. We motivate the need for the proposed\nintegrated approach using applications from plasma physics, climate modeling\nand neuroscience, and then discuss research challenges and possible solutions.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2015 19:13:18 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Blanas", "Spyros", ""], ["Byna", "Surendra", ""]]}, {"id": "1503.08636", "submitter": "Kisor Ray", "authors": "Kisor Ray, Santanu Ghosh, Mridul Das, Bhaswati Ray", "title": "Design & Implementation Approach for Error Free Clinical Data Repository\n  for the Medical Practitioners", "comments": "04 pages, 04 Figures, International Journal of Computer Trends and\n  Technology, Volume-21 Number-2,2015, ISSN 2231-2803", "journal-ref": null, "doi": "10.14445/22312803/IJCTT-V21P113", "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The modern treatment of any disease is heavily dependent on the medical\ndiagnosis. Clinical data obtained through the diagnostics tests need to be\ncollected and entered into the computer database in order to make a clinical\ndata repository. In most of the cases, manual entry is an absolute necessity.\nHowever, manual entry can cause errors also, leading to wrong diagnosis. This\npaper explains how data could be entered free of error to reduce the chances of\nwrong diagnosis by designing and implementation of a simple database driven\napplication.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2015 11:00:35 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Ray", "Kisor", ""], ["Ghosh", "Santanu", ""], ["Das", "Mridul", ""], ["Ray", "Bhaswati", ""]]}, {"id": "1503.08946", "submitter": "Florin Rusu", "authors": "Weijie Zhao, Yu Cheng, Florin Rusu", "title": "Workload-Driven Vertical Partitioning for Effective Query Processing\n  over Raw Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional databases are not equipped with the adequate functionality to\nhandle the volume and variety of \"Big Data\". Strict schema definition and data\nloading are prerequisites even for the most primitive query session. Raw data\nprocessing has been proposed as a schema-on-demand alternative that provides\ninstant access to the data. When loading is an option, it is driven exclusively\nby the current-running query, resulting in sub-optimal performance across a\nquery workload. In this paper, we investigate the problem of workload-driven\nraw data processing with partial loading. We model loading as fully-replicated\nbinary vertical partitioning. We provide a linear mixed integer programming\noptimization formulation that we prove to be NP-hard. We design a two-stage\nheuristic that comes within close range of the optimal solution in a fraction\nof the time. We extend the optimization formulation and the heuristic to\npipelined raw data processing, scenario in which data access and extraction are\nexecuted concurrently. We provide three case-studies over real data formats\nthat confirm the accuracy of the model when implemented in a state-of-the-art\npipelined operator for raw data processing.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2015 08:17:48 GMT"}, {"version": "v2", "created": "Sat, 9 May 2015 07:08:50 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Zhao", "Weijie", ""], ["Cheng", "Yu", ""], ["Rusu", "Florin", ""]]}]